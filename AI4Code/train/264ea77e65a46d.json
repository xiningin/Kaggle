{"cell_type":{"aafbc120":"code","f0e37e62":"code","7d8c4cb2":"code","3258b34e":"code","6b286097":"code","ee4ba375":"code","97dede8e":"code","60184d4c":"code","99173abd":"code","679d5795":"code","5718dc06":"code","f84be09d":"markdown","f61c9bb3":"markdown","95649907":"markdown","6a0b5480":"markdown","ce5e22e8":"markdown"},"source":{"aafbc120":"import pandas as pd\n\n# Load full data\ndf = pd.read_csv(\"..\/input\/titanic-ful-data\/T_full.csv\")\ndf.head()","f0e37e62":"# Keep the original copy\ndf_original=df.copy()","7d8c4cb2":"#OHE\nimport category_encoders as ce\n# Create an object of the OHE\nOHE = ce.OneHotEncoder(cols=['Sex','Cabin','Embarked','Title'],use_cat_names=True)\n# Encode the Categorical Variable\ndf = OHE.fit_transform(df)","3258b34e":"#Scalte the data\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Create an Object of the StandardScaler\nscaler = StandardScaler()\n\n# Fit with the ITem_Mrp\nscaler.fit(np.array(df.Age).reshape(-1,1))\nscaler.fit(np.array(df.Fare).reshape(-1,1))\nscaler.fit(np.array(df.Ticket).reshape(-1,1))\n\n# Transform the data\ndf.Age = scaler.transform(np.array(df.Age).reshape(-1,1))\ndf.Fare = scaler.transform(np.array(df.Fare).reshape(-1,1))\ndf.Ticket = scaler.transform(np.array(df.Ticket).reshape(-1,1))","6b286097":"#data separation\ntrain = df.loc[df.train_or_test.isin(['train'])]\ntest = df.loc[df.train_or_test.isin(['test'])]","ee4ba375":"#Drop the columns which are not required\ntrain=train.drop(['train_or_test','PassengerId','Name'],axis=1) \ntest=test.drop(['train_or_test','PassengerId','Name','Survived'],axis=1)","97dede8e":"X = train.drop('Survived',1) \ny = train.Survived","60184d4c":"# Check the shape of the train & test data\ntrain.shape,test.shape\n\n# Model Building - 1\nfrom sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3)\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score","99173abd":"#Create the model object\nmodel = LogisticRegression()\n\n#train the model\nmodel.fit(x_train,y_train)\n\n#predict on validation data\npred_cv = model.predict(x_cv)\n\n#check accuracy\naccuracy_score(y_cv,pred_cv)","679d5795":"#final prediction\npred_test = model.predict(test)","5718dc06":"## Logstic Regression with StratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\n\ni=1 \nkf = StratifiedKFold(n_splits=8,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]\n    \n    model = LogisticRegression(random_state=1)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n    pred_test2 = model.predict(test) \n    pred=model.predict_proba(xvl)[:,1]","f84be09d":"### 2. Model Building - LogisticRegression with StratifiedKFold","f61c9bb3":"### 1. Model Building - LogisticRegression","95649907":"### EDA","6a0b5480":"#### _1. Data Cleaning is done where every required .. trimmied unwated spaces,removed wild characters etc.._\n#### _2. Imputed the Missing Values_\n#### _3. Feature extraction is also done, like Ticket,Cabin ( Since Cabin more number of missing values and cannot afford drop the variable categorised into \"Cabin - Yes\",\"Cabin - No\"_\n#### _4. Extracted Title from the name._\n\n#### All the above mentioned steps or EDA part is done in excel before loading the clean data","ce5e22e8":"## Titanic Challenge - How I Reached Top16% Without Using Basic Python Packages"}}