{"cell_type":{"9f4cde48":"code","1c1ec056":"code","97b99448":"code","fb411141":"code","16e48d7e":"code","7741af3f":"code","9e5b00d8":"code","84c8a52c":"code","d1fcedf3":"code","854877b5":"code","5e8c206a":"code","8d24d6a6":"code","d1d527ac":"code","569a3b04":"code","3c4f2a60":"code","bf51aa3e":"code","df3f423a":"code","b9428e06":"code","99d69177":"code","9f73019e":"code","878cab4f":"code","290de1f8":"code","317820ef":"code","7f88cb31":"code","f5e9983c":"code","c045b588":"code","44fddf9b":"code","dc1f9ee2":"markdown","5f0705e2":"markdown","09d2a243":"markdown","c81799ff":"markdown","483c3092":"markdown","825c6e18":"markdown","4b44371d":"markdown","36c04f13":"markdown","4fded14a":"markdown","19940763":"markdown","fde49289":"markdown"},"source":{"9f4cde48":"import numpy as np\nimport scipy.stats as s\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport plotly as py\nsns.set_style('darkgrid')\nfrom tqdm.notebook import tqdm_notebook\n","1c1ec056":"raw_data = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","97b99448":"raw_data.head(10)","fb411141":"raw_data.describe()","16e48d7e":"raw_data.info()","7741af3f":"raw_data.isnull().sum()","9e5b00d8":"raw_data['sex'].replace(to_replace=['male','female'],value=[-1,1],inplace=True)\nraw_data['smoker'].replace(to_replace=['yes','no'],value=[-1,1],inplace=True)","84c8a52c":"raw_data['region'].replace(to_replace=np.unique(raw_data.region),value=[0,1,2,3],inplace=True)\n","d1fcedf3":"fig,axes = plt.subplots(1,2,figsize=(15,6))\n\nsns.kdeplot(raw_data.age,shade=True,ax=axes[0])\nsns.distplot(raw_data.age,label=\"Age\",hist=True,ax=axes[1])\nfig.show()","854877b5":"fig,axes = plt.subplots(1,2,figsize=(15,6))\n\nsns.kdeplot(raw_data.bmi,shade=True,ax=axes[0])\nsns.distplot(raw_data.bmi,label=\"Age\",hist=True,ax=axes[1])\nfig.show()","5e8c206a":"fig,axes = plt.subplots(1,3,figsize=(19,5))\nsns.distplot(raw_data[(raw_data.smoker == -1)][\"charges\"],ax=axes[0])\naxes[0].set_title('Distribution of charges for smokers')\n\nsns.distplot(raw_data[(raw_data.smoker == 1)]['charges'],ax=axes[1])\naxes[1].set_title('Distribution of charges for non-smokers')\n\nsns.countplot(x=\"smoker\",data=raw_data,ax=axes[2])\naxes[2].set_title(\"Countplot of Smokers v\/s Non Smokers\")\nfig.text(0.35,1,\"Smokers v\/s Non Smokers\",{'fontname':'Serif', 'weight':'bold','color': 'black', 'size':35})\nfig.show()","8d24d6a6":"fig,axes = plt.subplots(1,3,figsize=(19,5))\nsns.distplot(raw_data[(raw_data.sex == -1)][\"charges\"],ax=axes[0])\naxes[0].set_title('Distribution of charges for Males')\n\nsns.distplot(raw_data[(raw_data.sex == 1)]['charges'],ax=axes[1])\naxes[1].set_title('Distribution of charges for Females')\n\nsns.countplot(x=\"smoker\",data=raw_data,ax=axes[2])\naxes[2].set_title(\"Countplot of Males v\/s Females\")\nfig.text(0.35,1,\"Males v\/s Females\",{'fontname':'Serif', 'weight':'bold','color': 'black', 'size':35})\nfig.show()","d1d527ac":"fig,axes = plt.subplots(1,5,figsize=(26,9))\nfor i in range(0,4):\n    sns.distplot(raw_data[(raw_data.region == raw_data.region.unique()[i])][\"charges\"],ax=axes[i])\n    axes[i].set_title(f'Distribution of charges for {raw_data.region.unique()[i]}')\n\nsns.countplot(x=\"region\",data=raw_data,ax=axes[4])\naxes[2].set_title(\"Countplot of People Regionwise\")\nfig.text(0.35,1,'Regionwise Analysis of Charges',{'fontname':'Serif', 'weight':'bold','color': 'black', 'size':35})\nfig.show()","569a3b04":"c = ['age','bmi','children','charges']\nfig,axes = plt.subplots(1,4,figsize=(26,7))\nfor i in range(0,4):\n    sns.boxplot(x = c[i],data = raw_data,ax=axes[i])","3c4f2a60":"corr = raw_data.corr()\nsns.heatmap(corr,annot=True)","bf51aa3e":"raw_data['bmi'] = (raw_data['bmi'] - raw_data['bmi'].mean()) \/ raw_data['bmi'].std()\n","df3f423a":"def train_test_split(df, split_ratio = 0.8,seed = 42):\n    \"\"\"\n    Split the dataset into train and test dataset.\n    Input:\n    df-> dataset to be split.\n    split_ratio -> ratio to split the dataset.\n    seed -> random state to use for the random shuffling of the dataset.\n    Output:\n    trainX,trainY-> train dataset containing features and train labels.\n    testX,testY -> test dataset containig features and test labels.\n    \"\"\"\n    \n    trainX = df.sample(frac=split_ratio,random_state = seed)\n    testX = df.drop(trainX.index)\n    trainX = trainX.reset_index(drop=True)\n    testX = testX.reset_index(drop=True)\n    trainY = trainX.charges.values\n    trainY = trainY.reshape(trainY.shape[0],1)\n    testY = testX.charges.values\n    testY = testY.reshape(testY.shape[0],1)\n    trainX.drop(\"charges\",axis=1,inplace=True)\n    trainX.drop('children',axis=1,inplace=True)\n    trainX.drop('sex',axis=1,inplace=True)\n    trainX.drop('region',axis=1,inplace=True) \n    testX.drop('children',axis=1,inplace=True)\n    testX.drop('sex',axis=1,inplace=True)\n    testX.drop('region',axis=1,inplace=True)\n    testX.drop(\"charges\",axis=1,inplace=True)\n    return trainX,trainY,testX,testY\n","b9428e06":"trainX,trainY,testX,testY = train_test_split(raw_data)","99d69177":"def data_preparation(train,test):\n    '''\n    Appends 1 to both train and test dataset so that the theta_0 and theta_1 can be combined into theta.\n    train-> train dataset\n    test -> test dataset\n    '''\n    train = np.array(train)\n    test = np.array(test)\n    trainX = np.array(list(map(lambda x: np.append([1],x) , train)))\n    testX = np.array(list(map(lambda x: np.append([1],x) , test)))\n    return trainX,testX","9f73019e":"trainX,testX = data_preparation(trainX,testX)","878cab4f":"def mse(train,theta_final,train_labels):\n    '''\n    Calculates the mean squared error for the algorithm.\n    Input:\n    train-> train dataset\n    theta_final -> Final features\n    train_labels-> training labels(correct answers)\n    Return:\n    Value of Mean Sqaured error. \n    '''\n    m = train.shape[0]\n    Y_hat = np.matmul(train,theta_final)\n    return (1\/(2*m)) * np.sum(np.square(Y_hat - train_labels))\n","290de1f8":"\ndef delta(train,train_labels,p):\n    '''\n    Calculate the derivative of the loss function to required to perform gradient descent.\n    Inputs:\n    train-> train_dataset\n    train_labels-> ground truth labels for train dataset.\n    p -> theta (so as to get the final gradient)\n    Return: Value of the derivative.\n    '''\n    m = train.shape[0]\n    y_hat = np.dot(train, p)\n    a = np.dot(train.T,(y_hat - train_labels))\n    return  (2\/m)*a","317820ef":"theta_initial = np.zeros((trainX.shape[1],1))\ntheta_final =  np.zeros((trainX.shape[1],1))\nm = trainX.shape[0]\niterations = []\nresidual_points = [0]\nlr = 10 ** (-1)","7f88cb31":"i = 0\nfor p in tqdm_notebook(range(140478)):\n    theta_final = theta_initial - (lr) * delta(trainX,trainY,theta_initial) * (1\/ m)\n    E = int(mse(trainX,theta_final,trainY))\n    residual_points.append(E)\n    if residual_points[i] == residual_points[i-1] :\n        break\n\n    iterations.append(i)\n    theta_initial = theta_final\n    i +=1","f5e9983c":"def R2_Statistics(theta_final):\n    sst = np.sum((testY-testY.mean())**2)\n    ssr = np.sum((np.matmul(testX,theta_final)-testY)**2)\n    r2 = 1-(ssr\/sst)\n    return(r2)","c045b588":"R2_Statistics(theta_final)","44fddf9b":"plt.scatter(x = iterations,y=residual_points[1:])","dc1f9ee2":"#### It is supervised machine learning algorithm used for perofroming regression tasks.\n- **Regression**: Means to predict a Quantitive variable.\\\n- **Loss Function**: \n    - ### Mean Squarred Error : It  measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.\n- For optimisation of the loss:\n    - Gradient Descent: It is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used in machine learning to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.\n- For checking up the measure of fit :\n    - R2 Statistics: R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable .\n    ","5f0705e2":"# Data Visualisation","09d2a243":"If you like my work please upvote and please also check out my other notebooks:\n- [SurvivingTheTitanic](https:\/\/www.kaggle.com\/govindsrathore\/survivingthetitanic)\n- [HeartAttackAnalysis](https:\/\/www.kaggle.com\/govindsrathore\/heart-attack-analysis-prediction-91-acc)\n- [PneumoniaChestXray](https:\/\/www.kaggle.com\/govindsrathore\/vgg-transfer-learning-data-augmentation-94-acc)","c81799ff":"### R2 Statistics is the way of calculating the fit for our model .\n$$R2 = 1 - \\frac{RSS}{TSS}$$\nHere,\n- **RSS : Residual error sum of squares =**   $\\sum_{i=1}^{N} (predictedy_i - mean_y)^2$\n \n- **TSS : Total sum of squares =**  $\\sum_{i=1}^{N} (y_i - mean_y)^2 $","483c3092":"# Gradient Descent \n![](https:\/\/miro.medium.com\/max\/1400\/1*OG1d4edy5BFYeQ0yHjBOJA.gif)\n\n### Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used in machine learning to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.","825c6e18":"Here , It is clearly visible that the residual error is decreasing with each iterations","4b44371d":"# Linear Regression","36c04f13":"Line :\n$Y = \\theta_0 + \\theta_1.X$\n\n#### MSE(Mean Sqaurred Error):\nIt is the average of squared error occurred between the predicted values and actual values. It can be written as:\n$$ MSE = \\frac{1}{2.n}\\sum_{i=0}^{n} (Y_{true} - Y_{pred})^2$$","4fded14a":"### Now if R2 is closer to 1 that means our model is accurately explain the variability of data and if it is closer to 0 then it mean it is not a good fit\n$$ 0 \\leq R2 \\leq 1 $$","19940763":"## Age","fde49289":"![](https:\/\/thumbs.gfycat.com\/GlisteningUntriedIberianchiffchaff-size_restricted.gif)"}}