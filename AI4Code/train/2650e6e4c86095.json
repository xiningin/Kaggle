{"cell_type":{"1f6e57af":"code","5cad5b19":"code","502b2f65":"code","98dd1c63":"code","4047892d":"code","d16dbc0a":"code","ec3f13ad":"code","70de6468":"code","e15c3e14":"code","3a96cce6":"code","ef1039cd":"code","96a903ef":"markdown"},"source":{"1f6e57af":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\nfrom tqdm import tqdm \nfrom sklearn.preprocessing import LabelEncoder\nprint(os.listdir(\"..\/input\"))","5cad5b19":"# Read data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","502b2f65":"#Glimse data\nprint(train.shape)\ntrain.head()","98dd1c63":"print(test.shape)\ntest.head()","4047892d":"df_combine = pd.concat([train,test], axis = 0)\nid_name = 'Id'\ntarget_name = 'Target'\ncols = [f_ for f_ in df_combine.columns if df_combine[f_].dtype == 'object' and f_ != id_name ]\nprint(cols)\n\n# mean encoding\nfor col in tqdm(cols):\n    lb = LabelEncoder()\n    lb.fit(df_combine[col])\n    train[col] = lb.transform(train[col].astype(str))\n    test[col] = lb.transform(test[col].astype(str))\n    \ndel lb\ngc.collect()\n    ","d16dbc0a":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms']\n    df['r4h1_percent_in_male'] = df['r4h1'] \/ df['r4h3']\n    df['r4m1_percent_in_female'] = df['r4m1'] \/ df['r4m3']\n    df['r4h1_percent_in_total'] = df['r4h1'] \/ df['hhsize']\n    df['r4m1_percent_in_total'] = df['r4m1'] \/ df['hhsize']\n    df['r4t1_percent_in_total'] = df['r4t1'] \/ df['hhsize']\n   \n    \nextract_features(train)\nextract_features(test)","ec3f13ad":"cols_to_drop = [id_name ,target_name]\nX_train = train.drop(cols_to_drop, axis = 1)\nY_train = train[target_name]\nX_train.fillna(0)\n\nimport lightgbm as lgb\nlgb = lgb.LGBMClassifier(class_weight='balanced',drop_rate=0.9, min_data_in_leaf=100, max_bin=255,\n                                 n_estimators=500,min_sum_hessian_in_leaf=1,importance_type='gain',learning_rate=0.1,bagging_fraction = 0.85,\n                                 colsample_bytree = 1.0,feature_fraction = 0.1,lambda_l1 = 5.0,lambda_l2 = 3.0,max_depth =  9,\n                                 min_child_samples = 55,min_child_weight = 5.0,min_split_gain = 0.1,num_leaves = 45,subsample = 0.75)  \n\n\n","70de6468":"from sklearn.model_selection import KFold\n\nk_folds = 5\nkf = KFold(n_splits = k_folds, shuffle = True)\n\nfor train_index, test_index in kf.split(X_train, Y_train):\n    x_train, x_val = X_train.iloc[train_index] , X_train.iloc[test_index]\n    y_train, y_val = Y_train.iloc[train_index] , Y_train.iloc[test_index]\n    lgb.fit(x_train, y_train , eval_set= [(x_val, y_val)], early_stopping_rounds = 400)","e15c3e14":"X_test = test.drop('Id', axis = 1)\nX_test.fillna(0)\n\npres = lgb.predict(X_test)","3a96cce6":"id_test = test['Id']\nsubs = pd.DataFrame()\nsubs['Id'] = id_test\nsubs['Target'] = pres\nsubs.to_csv('mean_encoding_target.csv', index = False)","ef1039cd":"subs.head()","96a903ef":"--# This is a BEGINNER'S kernel, please go easy on me. Your comments, suggestions and tips will be highly appreciated and if you liked this kernel please upvote. Thank you very much.\n\n### Reference notebook\n    https:\/\/www.kaggle.com\/opanichev\/lgb-as-always?scriptVersionId=4642703"}}