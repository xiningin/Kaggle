{"cell_type":{"768c56c7":"code","4c37e3d8":"code","e8cd4b1e":"code","b9c4ef6e":"code","851c4fc7":"code","e41d97fa":"code","04771961":"code","b43bbc8e":"code","c11a21d3":"code","c35b3183":"code","e39419cc":"code","c1037928":"code","97de99e2":"code","11d516a6":"code","4aa3d704":"code","79896999":"code","65a1d710":"code","e98b56e6":"code","1da3b281":"code","3f645ab5":"code","ba00b8af":"code","e7ddb90b":"code","d0095ad2":"code","909bd9b7":"code","157387e5":"code","04e9cec5":"code","fcae1a41":"code","0c27b3b3":"code","5940dff7":"code","d3e3cffc":"code","13216c94":"code","408adca8":"code","16fb40ca":"code","d76f75a6":"code","d9241f85":"code","24637f5d":"code","698ab46b":"code","e73175e5":"code","26f080c4":"code","29bb06e1":"code","ee6f8f89":"code","1e61c9e3":"code","26fa8555":"code","9ab06efa":"code","aa3239f0":"code","d4796569":"code","f3563bba":"code","a15a5d23":"code","b278cd12":"code","d8cff239":"code","1d72170f":"code","d16c6c66":"code","2978cfa4":"code","9a042b1f":"code","42140f27":"markdown","c9a8970f":"markdown","a56d7ebc":"markdown","d72d5d15":"markdown","c99ae629":"markdown"},"source":{"768c56c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","4c37e3d8":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","e8cd4b1e":"train.sample(6)","b9c4ef6e":"train.info()","851c4fc7":"plt.scatter(train.GrLivArea,train.SalePrice, c=\"blue\", marker = \"s\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n#Per check, 2 outliers which has greater living area but sold at very less price. Which needs to be removed","e41d97fa":"train = train[train.GrLivArea < 4500]","04771961":"plt.scatter(train.LotArea,train.SalePrice, c=\"blue\", marker = \"s\")\nplt.xlabel(\"LotArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n#Similarly LotArea>150000 consider as outlier and remove from dataset","b43bbc8e":"train = train[train.LotArea < 150000]","c11a21d3":"#Analysis on Target Variable\nfrom scipy import stats\nfrom scipy.stats import norm\n\nsns.distplot(train['SalePrice'], fit= norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Per check SalePrice is highly right skewed with large kurtosis\n\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","c35b3183":"#In order for better prediction, we need to fit the target variable into Normal Dist, hence we go for log transformation\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\n#Now, the target variable is more or less similar to Norm Dist...","e39419cc":"#Data Cleaning Starts here\nntrain = train.shape[0]\nntest = test.shape[0]\nytrain = train.SalePrice.values\ndata = pd.concat((train, test)).reset_index(drop = True)\ndata.drop(['SalePrice'], axis = 1, inplace = True)\ndata.shape","c1037928":"print((data.values == 'Abnorml').sum())\ncol_idx = pd.np.argmax(data.values == 'Abnorml', axis=1).max()\ndata.iloc[:, col_idx].value_counts()","97de99e2":"print((ytrain == 'Abnorml'))","11d516a6":"missing = []\nfor col in data:\n    count = data[col].isnull().sum(axis = 0)\n    if count:\n        missing.append(col)\n        print(\"%s : %d\" %(col, count))","4aa3d704":"data_na = (data.isnull().sum() \/ len(data))*100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending = False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : data_na})\nmissing_data","79896999":"f, ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=data_na.index, y = data_na)\nplt.xlabel('Features')\nplt.ylabel(\"% of NAN Values\")","65a1d710":"corrmat = train.corr()\ncorrmat.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corrmat.SalePrice)\nplt.subplots(figsize=(15,12))\nsns.heatmap(corrmat)","e98b56e6":"#PoolQC - Indicates houses with No POOL, hence for missing values we will fill with NONE\ndata['PoolQC'] = data['PoolQC'].fillna('None')","1da3b281":"#MiscFeature - Similar to above no Miscellaneous Feature available for that house, hence fill NONE\ndata['MiscFeature'] = data['MiscFeature'].fillna('None')","3f645ab5":"#Alley - NA means no Passage access\ndata['Alley'] = data['Alley'].fillna('None')","ba00b8af":"#Fence - NA indicates no fence , hence NONE\ndata['Fence'] = data['Fence'].fillna('None')","e7ddb90b":"#FirePlaceQu - NA means no seperate FirePlace available, hence NONE\ndata['FireplaceQu'] = data['FireplaceQu'].fillna('None')","d0095ad2":"#Frontage is numerical data, it has high chance depend on neighbors area, hence we go for median imputation method\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median))","909bd9b7":"#Replace NA with None since for categorical data\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')","157387e5":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","04e9cec5":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)","fcae1a41":"#NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","0c27b3b3":"data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])","5940dff7":"#Utilities - This feature doesnt help in modelling, hence removed from data\ndata = data.drop(['Utilities'], axis = 1)","d3e3cffc":"data[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")","13216c94":"data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])","408adca8":"data['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])","16fb40ca":"data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])","d76f75a6":"data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","d9241f85":"data['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","24637f5d":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = train.select_dtypes(include = ['object']).columns\nnumerical_features = train.select_dtypes(exclude = ['object']).columns\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))","698ab46b":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(data[c].values))\n    data[c] = lbl.transform(list(data[c].values))","e73175e5":"data['TotalSF'] = data['TotalBsmtSF']+data['1stFlrSF']+data['2ndFlrSF']","26f080c4":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndata_numeric = data.dtypes[data.dtypes != 'object'].index\ndata_skew = data[data_numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\n\nskewness = pd.DataFrame({'Skew': data_skew})\nskewness","29bb06e1":"skewness = skewness[abs(skewness)>0.75]\nfrom scipy.special import boxcox1p\nskewness_features = skewness.index\nlam = 0.15\nfor fld in skewness_features:\n  data[fld] = boxcox1p(data[fld], lam)  ","ee6f8f89":"train = data[:ntrain]\ntest = data[:ntest]","1e61c9e3":"data.info()","26fa8555":"numeric =  data.describe().columns","9ab06efa":"#We will apply K-Fold Cross Validation for each training models\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nn_folds = 5\ndef rmse_kfold(model):\n    kf = KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train[numeric].values, ytrain, scoring = \"neg_mean_squared_error\", cv = kf ))\n    return(rmse)","aa3239f0":"from sklearn import linear_model\nfrom sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nlm = linear_model.LinearRegression()\nscore = rmse_kfold(lm)\nprint('\\n OLS Score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","d4796569":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmse_kfold(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f3563bba":"enet = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0005, l1_ratio=.9,random_state = 1))\nscore = rmse_kfold(enet)\nprint(\"\\nElastic Net Score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a15a5d23":"from sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmse_kfold(KRR)\nprint(\"\\nKRR Score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b278cd12":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmse_kfold(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d8cff239":"import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = rmse_kfold(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#Having issue with install xgboost package","1d72170f":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmse_kfold(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d16c6c66":"from sklearn.metrics import mean_squared_error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","2978cfa4":"model_lgb.fit(train[numeric], ytrain)\nlgb_train_pred = model_lgb.predict(train[numeric])\nlgb_pred = np.expm1(model_lgb.predict(test[numeric].values))\nprint(rmsle(ytrain, lgb_train_pred))","9a042b1f":"print(lgb_pred)","42140f27":"****Training & Prediction****","c9a8970f":"**So, we achieved 7% error in testing data with Lgb Model fitting**","a56d7ebc":"Transforming some numerical variables that are really categorical****","d72d5d15":"****Modelling****","c99ae629":"****Imputing Missed Values for each features one by one depending on their data type"}}