{"cell_type":{"88a17c86":"code","c25126a3":"code","675176d4":"code","65e8c0cf":"code","bdf3873d":"code","bc3bb55d":"code","433b5072":"code","ab5eae97":"code","c13f7a0e":"code","6058e9ac":"code","73be4af0":"code","284a0d60":"code","817f1903":"code","fba6f892":"code","f214df0e":"code","6a0ff830":"code","a22647fa":"code","c7d57b3d":"code","2b5bc6a4":"code","8b539f93":"code","26df0ca0":"code","dfb64700":"code","769a3a91":"code","9ed9ed21":"code","5265a4fb":"code","2f7f6073":"code","e07d3049":"code","d503b504":"code","b521df26":"code","f718a0a8":"code","9087f8e2":"code","d4953441":"code","68cba21e":"code","02e4d626":"code","cbe19b3c":"code","b4309a84":"code","24787f3a":"code","df6df8ce":"code","98d65a6f":"code","76db6c67":"code","ec26f778":"code","96649227":"code","9ff4c7d5":"code","b6e28a36":"code","b784cf24":"code","13552c12":"code","49f95ee7":"code","31e5cc02":"markdown","c7b4325a":"markdown","d10c8e80":"markdown","ba3bb7f4":"markdown","65cb65d1":"markdown","1af39cb8":"markdown","77718cb3":"markdown","7556024c":"markdown","67237954":"markdown","a6ca0800":"markdown","914811fd":"markdown","cbda7729":"markdown","b424ab5d":"markdown","dca752fd":"markdown","492efbb4":"markdown","eba3d246":"markdown","4e868e0f":"markdown","657b3aca":"markdown","7b100755":"markdown","322753ac":"markdown","37345518":"markdown","a05f096c":"markdown","2821f114":"markdown","9a1ca5ad":"markdown","0f675cc3":"markdown","59aec1d3":"markdown","c7026367":"markdown"},"source":{"88a17c86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c25126a3":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","675176d4":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","65e8c0cf":"train_data.info()\ntrain_data.columns.values","bdf3873d":"train_data.describe()    # Gives us a overview of each column (the number of arrays, mean, std , ....)","bc3bb55d":"# matplotlib inline # only in a Jupyter notebook\nimport matplotlib.pyplot as plt\ntrain_data.hist(bins=50, figsize=(20, 15))\nplt.show()","433b5072":"import seaborn as sns\n\ntrain_data.isnull().sum()\n\nsns.heatmap(train_data.isnull(), cbar = False).set_title(\"Missing values heatmap\")\n# print(train_data.isnull().any().sum())","ab5eae97":"train_data.nunique()","c13f7a0e":"features1 = [\"PassengerId\", \"Pclass\", \"Age\"]\nsns.relplot(\n    x=\"value\", y=\"Survived\", col=\"variable\", data=train_data.melt(id_vars=\"Survived\", value_vars=features1), facet_kws=dict(sharex=False),\n);\nfeatures2 = [\"SibSp\", \"Parch\", \"Fare\"]\nsns.relplot(\n    x=\"value\", y=\"Survived\", col=\"variable\", data=train_data.melt(id_vars=\"Survived\", value_vars=features2), facet_kws=dict(sharex=False),\n);","6058e9ac":"# header = [\"Percentage of SURVIVED people in train data\",\"Percentage of DIED people in train data\"]\ntrain_data[\"Survived\"].value_counts() \/ len(train_data[\"Survived\"])","73be4af0":"sns.barplot(x='Pclass', y='Survived', data=train_data)","284a0d60":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', height=2.5, aspect=2)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","817f1903":"survived = 'survived'\nnot_survived = 'not survived'\n# fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_data[train_data['Sex']=='female']\nprint(\"Pecentage of survived women:\")\nprint(women.Survived.sum() \/ train_data.shape[0])\nmen = train_data[train_data['Sex']=='male']\nprint(\"Pecentage of survived men:\")\nprint(men.Survived.sum() \/ train_data.shape[0])\n\ntrain_data.groupby([\"Pclass\",\"Sex\"]).Survived.mean().to_frame\n","fba6f892":"def plot_swar_survivors(dataset, feature1, feature2, title, fize = (155)):\n    fig, ax = plt.subplots(figsize=(18,5))\n    # Turn off grid on the left axis\n    ax.grid(True)\n    plt.xticks(list(range(0,100,2)))\n    sns.swarmplot(y=feature1, x=feature2, hue=\"Survived\", data=train_data).set_title(title)\n\n                  \nplot_swar_survivors(train_data, 'Sex', 'Age', \"Survival swarmplot for Age and Gender\")\nplot_swar_survivors(train_data, 'Age', 'Pclass', \"Survival swarmplot for Age and Pclass\")","f214df0e":"train_data['Family'] = train_data.Parch + train_data.SibSp\ntest_data['Family'] = test_data.Parch + test_data.SibSp\ntrain_data['Is_Alone'] = train_data.Family == 0    # When someone doesn't have a family, he\/she will be alive\n# train_data.head\ntest_data['Is_Alone'] = test_data.Family == 0 ","6a0ff830":"print(train_data.shape)\nprint(test_data.shape)\n","a22647fa":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","c7d57b3d":"# train_data.Cabin = train_data.Cabin.fillna('NA')\nimport re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) \n    \n# we can now drop the cabin feature\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)","2b5bc6a4":"data = [train_data, test_data]\n\nfor dataset in data:\n    mean = train_data[\"Age\"].mean()\n    std = test_data[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_data[\"Age\"].astype(int)\n\nprint(train_data.shape)\nprint(test_data.shape)","8b539f93":"train_data['Embarked'].describe()","26df0ca0":"common_value = 'S'\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n    \nprint(train_data.shape)\nprint(test_data.shape)","dfb64700":"sns.heatmap(train_data.isnull(), cbar = False).set_title(\"Missing values heatmap\")\n# The following picture shows there is no missing value any more.\ntrain_data.info()\n\n# Above you can see that \u2018Fare\u2019 is a float and we have to deal with 4 categorical features: ...\n# ... Name, Sex, Ticket and Embarked. Lets investigate and transfrom one after another.","769a3a91":"print(train_data.shape)\nprint(test_data.shape)","9ed9ed21":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n    \nprint(train_data.shape)\nprint(test_data.shape)","5265a4fb":"## Embarked:  Convert \u2018Embarked\u2019 feature into numeric.\n\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)\n    \nprint(train_data.shape)\nprint(test_data.shape)","2f7f6073":"## Ticket: Since the Ticket attribute has 681 unique tickets, it will be a bit tricky ...\n# ... to convert them into useful categories. So we will drop it from the dataset.\n\ntrain_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)\n\nprint(train_data.shape)\nprint(test_data.shape)","e07d3049":"# Fare: Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \nprint(train_data.shape)\nprint(test_data.shape)","d503b504":"# Name: We will use the Name feature to extract the Titles from the Name, ...\n# ... so that we can build a new feature out of that.\n\ndata = [train_data, test_data]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)  # A new feature named title has been added","b521df26":"train_data[\"Is_Alone\"] = train_data[\"Is_Alone\"].astype(int)\ntest_data[\"Is_Alone\"] = test_data[\"Is_Alone\"].astype(int)","f718a0a8":"#Convert Ages to a list of groups\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \nprint(train_data['Age'].value_counts())\ntrain_data","9087f8e2":"data = [train_data, test_data]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \ntrain_data","d4953441":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['Family'] + 1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n    \ntrain_data","68cba21e":"test_data","02e4d626":"y = train_data.Survived\nX = train_data.drop([\"PassengerId\", \"Survived\"], axis = 1)\nX_test  = test_data.drop([\"PassengerId\"], axis=1).copy()\n\n# pip install feature-engine\n# pip install arcticdata=='1.4'\n# import arcticdata.FeatureEngineering as fe\n# X = fe.scale(X)\n# X_test = fe.scale(X_test)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n# transform data\nX = scaler.fit_transform(X)\nX_test = scaler.fit_transform(X_test)\n\n# features = X.columns.values\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features])\nX_train = X.copy()\ny_train = y.copy()\n# X_train","cbe19b3c":"X_test","b4309a84":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier(max_iter=20, tol=None)\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, y_train)\n\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)","24787f3a":"from sklearn.ensemble import RandomForestClassifier\n\n# random_forest = RandomForestClassifier(n_estimators=400)\nrandom_forest = RandomForestClassifier(n_estimators=100, max_depth=30)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)","df6df8ce":"# from sklearn.linear_model import LogisticRegression\n\n# logreg = LogisticRegression()\n# logreg.fit(X_train, y_train)\n\n# Y_pred = logreg.predict(X_test)\n\n# acc_log = round(logreg.score(X_train, y_train) * 100, 2)","98d65a6f":"#KNN \nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)","76db6c67":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB() \ngaussian.fit(X_train, y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)","ec26f778":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron(max_iter=20)\nperceptron.fit(X_train, y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)","96649227":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(max_iter=500)\nlinear_svc.fit(X_train, y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)","9ff4c7d5":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)","b6e28a36":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN',\n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn,\n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","b784cf24":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=13)\n## 1\npca.fit(X_train)\n#print(pca.explained_variance_ratio_)\nx = pca.transform(X_train)\nX_train = pd.DataFrame(x)\n## 2\npca.fit(X_test)\n#print(pca.explained_variance_ratio_)\nx=pca.transform(X_test)\nX_test = pd.DataFrame(x)\n\nbest_model = RandomForestClassifier(n_estimators=200,bootstrap=False,max_depth=None,\n                                    max_features='sqrt',min_samples_leaf=1,min_samples_split=5)\nbest_model.fit(X_train, y_train)\nY_prediction = best_model.predict(X_test)\n\nbest_model.score(X_train, y_train)\nacc_random_forest = round(best_model.score(X_train, y_train) * 100, 2)\nprint(acc_random_forest)","13552c12":"Y_prediction.shape","49f95ee7":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_prediction})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","31e5cc02":"**Decision Tree Classifier**","c7b4325a":"**Encoding**\n\nSince the string data does not go well with the machine learning algorithms, I needed to convert the non-numeric data to numeric data. I used LabelEncoder to encode the \u2018Sex\u2019 column. The label encoder would substitute \u2018male\u2019 values with some number and \u2018female\u2019 values with some different number.","d10c8e80":"The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, the count of Age is 714, not 891). The std row shows the standard deviation, which measures how dispersed the values are. **The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the peaople have a Age lower than 20.125, while 50% are lower than 28 and 75% are lower than 38.** These are often called the 25th percentile (or first quartile), the median, and the 75th percentile (or third quartile).\n\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).","ba3bb7f4":"**Creating new Features** \n\nI will add two new features to the dataset, that I compute out of other features.","65cb65d1":"**Age:**\nNow we can tackle the issue with the age features missing values. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","1af39cb8":"**Logistic Regression:**","77718cb3":"**Gaussian Naive Bayes:**","7556024c":"**Building Machine Learning Models**","67237954":"**K Nearest Neighbor:**","a6ca0800":"**2. Pclass**\n\nHere we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","914811fd":"**New Feature**\n\nSimilar to the SibSp, this feature contained the number of parents or children each passenger was touring with. A maximum of 9 parents\/children traveled along with one of the traveler. I added the number of \u2018Parch\u2019 and \u2018SibSp\u2019 values to store in a new column named \u2018Family\u2019.\n\nMoreover, the chances of survival skyrocketed when a traveler traveled alone. Created another column, Is_Alone and assigned True if the value in \u2018Family\u2019 column was 0.","cbda7729":"**Stochastic Gradient Descent (SGD):**","b424ab5d":"****Features****\n\n**1. Survived**","dca752fd":"**Random Forest:**","492efbb4":"**Fare:**\n\nFor the \u2018Fare\u2019 feature, we need to do the same as with the \u2018Age\u2019 feature. But it isn\u2019t that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \u201cqcut()\u201d function, that we can use to see, how we can form the categories.","eba3d246":"**Embarked**\n\nEmbarked implies where the traveler mounted from. There are three possible values for Embark \u2014 Southampton, Cherbourg, and Queenstown. More than 70% of the people boarded from Southampton. Just under 20% boarded from Cherbourg and the rest boarded from Queenstown. People who boarded from Cherbourg had a higher chance of survival than people who boarded from Southampton or Queenstown.\n\n![](https:\/\/miro.medium.com\/max\/700\/1*yPOI3nhGpudAJgaOy8OalQ.png)","4e868e0f":"**3. Age**","657b3aca":"**Linear Support Vector Machine:**","7b100755":"**Which is the best Model ?**","322753ac":"**Perceptron**","37345518":"**Data Preprocessing**\n\n**Determine Missing Values:**   We need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n\nLet\u2019s take a more detailed look at what data is actually missing:\nThe Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the \u2018Age\u2019 feature, which has 177 missing values. **The \u2018Cabin\u2019 feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.**","a05f096c":"**Fare**\n\nBy splitting the fare amount into four categories, it was obvious that there was a strong association between the charge and the survival. The higher a tourist paid, the higher would be his chances to survive.\n\n![](https:\/\/miro.medium.com\/max\/700\/1*_OYSiUvP57QMoIYO0S0G7A.png)","2821f114":"**Creating Categories:**\n\nWe will now create categories within the following features:\n\n**Age:**\nNow we need to convert the \u2018age\u2019 feature. First we will convert it from float into integer. Then we will create the new \u2018AgeGroup\u201d variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don\u2019t want for example that 80% of your data falls into group 1.","9a1ca5ad":"**Embarked:** Since the Embarked feature has only 2 missing values, we will just fill these with the most common one.","0f675cc3":"**Submission**","59aec1d3":"**Missing values**\n\nThe dataset had a couple of columns that were missing values. Missing values are obvious in the following heatmap.","c7026367":"**Cabin:**\n\nFirst I thought, we have to delete the \u2018Cabin\u2019 variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the letter refers to the deck. Therefore we\u2019re going to extract these and create a new feature (deck), that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G."}}