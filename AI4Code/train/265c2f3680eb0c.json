{"cell_type":{"f458e317":"code","80faac89":"code","69b9ec7d":"code","999a2297":"code","6694be4d":"code","35b58bc5":"code","d3944b6c":"code","72d68d6f":"code","b270165f":"code","2f23364d":"code","bab8c2e6":"code","d4632d32":"code","71015214":"markdown","898df96c":"markdown","154a1823":"markdown","f9124441":"markdown","0d451e9e":"markdown","70d10fda":"markdown","35e67076":"markdown","02a14f30":"markdown","f84809d0":"markdown","99abd945":"markdown","4639e77f":"markdown","d9d56845":"markdown"},"source":{"f458e317":"import collections\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torchtext as tt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nRUN_TEST = True\n\nTRAIN_CSV = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_CSV = \"..\/input\/nlp-getting-started\/test.csv\"\nOUT_CSV = \"..\/working\/sub-nlp-getting-started.csv\"\n\ntorch.manual_seed(9)\ntorch.cuda.manual_seed(9)\ntorch.cuda.manual_seed_all(9)\n","80faac89":"def transform_text(dataframe):\n\n    dataframe[\"keyword\"] = dataframe[\"keyword\"].replace(np.nan, '', regex=True)\n    dataframe[\"text\"] = dataframe[\"keyword\"] + \" \" + dataframe[\"text\"]\n    return dataframe","69b9ec7d":"def build_vocab(toker, dataframe):\n\n    counter = collections.Counter()\n    for (_, row) in dataframe.iterrows():\n        counter.update(toker(row[\"text\"]))\n    vocab = tt.vocab.Vocab(counter, min_freq=1)\n\n    return vocab","999a2297":"def build_vocab_test():\n\n    toker = tt.data.utils.get_tokenizer('basic_english')\n    dataframe = pd.read_csv(TRAIN_CSV)\n    vocab = build_vocab(toker, dataframe)\n\n    assert len(vocab) == 23547\n    #print(vocab.stoi)\n\nif RUN_TEST:\n    build_vocab_test()","6694be4d":"class CustomDataSet(Dataset):\n\n    def __init__(self, dataframe, toker, vocab, labelled=True):\n\n        self.data = dataframe\n        self.labelled = labelled\n        self.len = len(self.data)\n\n        self.text_pipeline = lambda x: [vocab[token] for token in toker(x)]\n\n        # create new field _text to hold numbered text\n        _text = []\n        for (_, row) in self.data.iterrows():\n            _text.append(self.text_pipeline(row[\"text\"]))\n        self.data = self.data.assign(_text=_text)\n\n    def __len__(self):\n\n        return self.len\n\n    def __getitem__(self, idx):\n\n        _id = self.data.loc[idx, \"id\"]\n        text = self.data.loc[idx, \"_text\"]\n        text = torch.tensor(text)\n        label = -1\n        if self.labelled:\n            label = self.data.loc[idx, \"target\"]\n        return {\"id\": _id, \\\n                \"text\": text, \"label\": label}","35b58bc5":"def collate_batch(batch):\n\n    _batch = {}\n    ids = []\n    texts = []\n    offsets = [0]\n    labels = []\n\n    for item in batch:\n        ids.append(item[\"id\"])\n        texts.append(item[\"text\"])\n        labels.append(item[\"label\"])\n        offsets.append(item[\"text\"].size(0))\n\n    _batch[\"id\"] = torch.tensor(ids)\n    _batch[\"text\"] = torch.cat(texts)\n    _batch[\"offset\"] = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    _batch[\"label\"] = torch.tensor(labels).float().unsqueeze(dim=1)\n\n    return _batch","d3944b6c":"\ndef custom_data_set_test():\n\n    toker = tt.data.utils.get_tokenizer('basic_english')\n    dataframe = pd.read_csv(TRAIN_CSV)\n    vocab = build_vocab(toker, dataframe)\n    dataset = CustomDataSet(dataframe, toker, vocab)\n    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n\n    batch = next(iter(dataloader))\n    assert len(batch[\"label\"]) == 64\n\nif RUN_TEST:\n    custom_data_set_test()\n","72d68d6f":"def build_data_loaders(toker, vocab, train_df, test_df, batch_size=64, val_size=0.1):\n\n    train_ds = CustomDataSet(train_df, toker, vocab)\n\n    # split\n    val_ds_size = int(len(train_ds) * val_size)\n    sizes = [len(train_ds) - val_ds_size, val_ds_size]\n    train_ds, val_ds = random_split(train_ds, sizes)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n            collate_fn=collate_batch)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True,\n            collate_fn=collate_batch)\n\n    test_ds = CustomDataSet(test_df, toker, vocab, labelled=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n            collate_fn=collate_batch)\n\n    return train_loader, val_loader, test_loader","b270165f":"\ndef build_data_loaders_test():\n\n    toker = tt.data.utils.get_tokenizer('basic_english')\n    dataframe = pd.read_csv(TRAIN_CSV)\n    vocab = build_vocab(toker, dataframe)\n    dataset = CustomDataSet(dataframe, toker, vocab)\n    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n    test_dataframe = pd.read_csv(TEST_CSV)\n    train_loader, val_loader, test_loader = build_data_loaders(toker, vocab,\n            dataframe, test_dataframe, 64, 0.1)\n\n    assert dataloader\n    batch = next(iter(train_loader))\n    assert len(batch[\"label\"]) == 64\n    batch = next(iter(val_loader))\n    assert len(batch[\"label\"]) == 64\n    batch = next(iter(test_loader))\n    assert len(batch[\"label\"]) == 64\n\nif RUN_TEST:\n    build_data_loaders_test()\n","2f23364d":"\nclass MyModel(nn.Module):\n\n    def __init__(self, vocab_len, emdim):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_len, emdim)\n        self.fc0 = nn.Linear(emdim, 1)\n\n    def forward(self, text, offsets):\n        x = self.embedding(text, offsets)\n        return self.fc0(x)\n\n\ndef train(model, loss_fn, optimizer, data_loader):\n\n    model.train()\n\n    loss_cnt = 0\n    loss = 0\n    acc_cnt = 0\n    acc = 0\n\n    for batch in data_loader:\n\n        text, offset, label = batch[\"text\"], batch[\"offset\"], batch[\"label\"]\n\n        out = model(text, offset)\n\n        acc_cnt += len(offset)\n        acc += torch.sum(label == (out > 0.5).float())\n\n        loss = loss_fn(out, label)\n\n        loss_cnt += 1\n        loss += loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return loss \/ loss_cnt, acc \/ acc_cnt\n\n\ndef evaluate(model, data_loader):\n\n    model.eval()\n\n    acc_cnt = 0\n    acc = 0\n\n    with torch.no_grad():\n\n        for batch in data_loader:\n\n            text, offset, label = batch[\"text\"], batch[\"offset\"], batch[\"label\"]\n\n            out = model(text, offset)\n\n            acc_cnt += len(offset)\n            acc += torch.sum(label == (out > 0.5).float())\n\n    return acc \/ acc_cnt\n","bab8c2e6":"def create_submission(out_csv, model, data_loader):\n\n    model.eval()\n\n    # write header\n    output = pd.DataFrame({\"id\": [], \"target\": []})\n    output.to_csv(out_csv, index=False)\n\n    offset = 0\n\n    with torch.no_grad():\n\n        for batch in data_loader:\n\n            _id, text, offset = batch[\"id\"], batch[\"text\"], batch[\"offset\"]\n\n            out = model(text, offset)\n            out = (out > 0.5).squeeze().int()\n\n            # append entries\n            output = pd.DataFrame({\"id\": _id, \"target\": out})\n            output.to_csv(OUT_CSV, mode='a', header=False, index=False)","d4632d32":"\ndef main():\n\n    n_epochs = 12\n    batch_size = 32\n    emsize = 128\n\n    print(\"Prepare data\")\n    toker = tt.data.utils.get_tokenizer('basic_english')\n    train_df = pd.read_csv(TRAIN_CSV)\n    train_df = transform_text(train_df)\n    test_df = pd.read_csv(TEST_CSV)\n    test_df = transform_text(test_df)\n    vocab = build_vocab(toker, train_df)\n    train_loader, val_loader, test_loader = build_data_loaders(toker, vocab, \\\n            train_df, test_df, batch_size, 0.1)\n\n    print(\"Setup model\")\n    model = MyModel(len(vocab), emsize)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n\n    print(f\"Train: epochs={n_epochs}\")\n    for epoch in range(1, n_epochs+1):\n\n        loss, acc = train(model, loss_fn, optimizer, train_loader)\n        acc_val = evaluate(model, val_loader)\n\n        scheduler.step()\n\n        print (f\"Epoch {epoch:3d} | Train loss {loss:.6f} acc {acc:.4f} | \"\n               f\"Validation acc {acc_val:.4}\")\n\n    # create submission\n    print(f\"Creating submission: {OUT_CSV}\")\n    create_submission(OUT_CSV, model, test_loader)\n    print(\"Done.\")\n\nif __name__ == \"__main__\":\n    main()\n","71015214":"Build train, validation and test torch.utils.data.DataLoader","898df96c":"Create a torch.utils.data.Dataset class. This also takes care of changing the\ntext field to numbers using the vocab.","154a1823":"Define function to build a torchtext.vocab.Vocab from pd.DataFrame's text field.","f9124441":"Change the batch format so the data is received in easy to use format for the\nmodel.\nThis function is used when the DataLoader is created.","0d451e9e":"Define main that uses above function\/classes.","70d10fda":"Run basic test for build_data_loaders()","35e67076":"Define the Model, and functions to train and evaluate","02a14f30":"Transform text, just cleanup keyword and add to text","f84809d0":"Run basic test of build_vocab() (Also see how it is used)","99abd945":"Helper function to create submissions.","4639e77f":"# Pytorch with EmbeddingBag layer\nThis is a basic example to learn about the pytorch API.\nThe focus isn't on getting the best accuracy but to be be useful for someone\nlearning pytorch. I made another attempt using transformers that gets a\nmuch better accuracy,\nhttps:\/\/www.kaggle.com\/pinkaxe\/bert-with-transformers-library\n\nThis uses a model with a pytorch EmbeddingBag layer. It just prepends the\nkeyword to the text and doesn't transform the text in any other way(eg.\nremove links)\n\nThe EmbeddingBag layer is easier to use than the Embedding layer and useful\nwith short sentences where one can handle it with a standard neural network\nrather than a RNN.\n\nThere is a good explanation here how they differ\nhttps:\/\/jamesmccaffrey.wordpress.com\/2021\/04\/14\/explaining-the-pytorch-embeddingbag-layer\/\n\nThere is also a pytorch tutorial\nhttps:\/\/pytorch.org\/tutorials\/beginner\/text_sentiment_ngrams_tutorial.html\n\nThe program\n\n- builds a vocabulary file from the text fields\n- read and transforms the csv files text field into numbers using the\n  vocabulary file. It creates Datasets and DataLoaders from this that returns\n  the data ready for the neural network.\n- Train and evaluate with a model using an EmbeddingBag layer\n\nFirst function\/classes are defined and then a main function that uses them.","d9d56845":"Run basic test of CustomDataSet()"}}