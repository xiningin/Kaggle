{"cell_type":{"98a08a48":"code","65dfcac0":"code","b84b3f33":"code","192d951d":"code","4639bcfe":"code","15ec178d":"code","40ef35e9":"code","ba1b274a":"code","45e23166":"code","6c39d78d":"code","1f315f97":"code","5a0e1264":"code","61950ba1":"code","7304cd32":"code","bba7099c":"code","cc69c713":"code","0cdef655":"code","77a94c5d":"markdown","af3fea5e":"markdown","b3599920":"markdown","b03059c9":"markdown","15eac7ed":"markdown","805b5232":"markdown","46e09e08":"markdown","69f54f31":"markdown","63b8f59e":"markdown","9f40289c":"markdown","920eb1ac":"markdown","8953ae3f":"markdown","463bfb8d":"markdown","b84d5741":"markdown","0b0d09c8":"markdown","73903e8e":"markdown","b61bb612":"markdown"},"source":{"98a08a48":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom collections import Counter\nimport datetime\nimport wordcloud\nimport json\nfrom datetime import datetime\nimport math","65dfcac0":"df = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\nimport json\ncat_list=json.loads( open('..\/input\/youtube-new\/US_category_id.json').read())['items']\ncat_dict = {}\nfor w in cat_list:\n    cat_dict[int(w['id'])] = w['snippet']['title']","b84b3f33":"df['category']= df['category_id'].apply(lambda a:cat_dict[a])\ndf['n_tags'] =df['tags'].apply(lambda w: w.count('|')+1)\ndf['trending_date_object'] = df['trending_date'].apply(lambda d:datetime.strptime(d,\"%y.%d.%m\"))\ndf['publish_time_object'] = df['publish_time'].apply(lambda d:datetime.strptime(d,\"%Y-%m-%dT%H:%M:%S.%fZ\"))\ndf['trending_delta_object']= df['trending_date_object']-df['publish_time_object']\ndf['trending_delta'] = df['trending_delta_object'].apply(lambda w:math.ceil(w.days*24+w.seconds\/3600))","192d951d":"df.head().transpose()","4639bcfe":"num_attribs = ['views','likes','dislikes','comment_count','n_tags','trending_delta']\ncat_attribs= ['tags','description','title','publish_time','trending_date',\n             'comments_disabled','ratings_disabled','video_error_or_removed','category',\n             'channel_title','trending_delta_object',\n              'trending_date_object','publish_time_object']","15ec178d":"rem_count = 100\nrem_attribs = ['views','likes','dislikes','comment_count']\nfor r in rem_attribs:\n    d = df[r].copy()\n    d.loc[list(np.random.randint(0,len(df),rem_count))] =np.NaN\n    df[r] = d","40ef35e9":"df.info()","ba1b274a":"#Description has null values \ndf[\"description\"] = df[\"description\"].fillna(value=\"\")","45e23166":"df[rem_attribs] =df[rem_attribs].fillna(df[rem_attribs].mean())","6c39d78d":"df.info()","1f315f97":"from pandas.plotting import scatter_matrix\ndone = []\nfor i in num_attribs:\n    for j in num_attribs:\n        if (i!=j and (j,i) not in done):\n            print(i,j)\n            done.append((i,j))\n            plt.figure(figsize=(8,5))\n            plt.scatter(df[i],df[j])\n            plt.title(i+\" \"+j)\n            plt.xlabel(i)\n            plt.ylabel(j)","5a0e1264":"num_attribs","61950ba1":"for i,w in enumerate(num_attribs):\n    _max = df[w].max()\n    _min = df[w].min()\n    _a,_b = df[w].describe()[['25%','75%']]\n    _width=  2*(_b-_a)\/(len(df)**(1\/3))\n    bins =math.floor( (_max-_min)\/_width)\n    print(bins,w)\n    plt.figure(figsize=(8,5))\n    plt.title(w)\n    df[w].hist(bins=bins)\n    if i==0:\n        plt.xlim(0,0.25e8)\n        plt.ylim(0,2500)\n    elif i==1:\n        plt.xlim(0,500000)\n        plt.ylim(0,2000)\n    elif i==2:\n        plt.xlim(0,10000)\n        plt.ylim(0,4000)\n    elif i==3:\n        plt.xlim(0,50000)\n        plt.ylim(0,3500)\n    elif i==5:\n        plt.xlim(0,3000)\n        plt.ylim(0,2000)\n    ","7304cd32":"num_attribs","bba7099c":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.base import BaseEstimator,TransformerMixin\nscaled_data = MinMaxScaler().fit_transform( df[num_attribs])","cc69c713":"class Trim(BaseEstimator,TransformerMixin):\n    def __init__(self,t):\n        self.t = t\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X =np.sort(X,axis=0)\n        return np.apply_along_axis(trim,axis=0,arr=X,t=self.t)\n    \ndef trim(data,t=0.15):\n    return data[ int(len(data)*t):int(len(data)*(1-t)) ]\n","0cdef655":"from sklearn.pipeline import Pipeline\np = Pipeline([\n    ('trim',Trim(0.15)),\n    ('scale',MinMaxScaler())\n])\nscaled_data = p.fit_transform(df[num_attribs])\n\nplt.figure(figsize=(15,10))\n_=plt.boxplot(scaled_data)\n_=plt.xticks([1,2,3,4,5,6],labels=num_attribs)","77a94c5d":"# Data Cleaning","af3fea5e":"adding explicit null values to use for imputing values with mean","b3599920":"## Adding useful features","b03059c9":"## Numerical ","15eac7ed":"*View a gist of the dataset*","805b5232":"### Box Plots","46e09e08":"## Categorical Data Cleaning","69f54f31":"## Preperation for data cleaning\n","63b8f59e":"# Visualizing Data","9f40289c":"fill null values in description column with \"\"","920eb1ac":"**Cat Attribs which have missing values**\n* Description - Has *36912* missing values\n\n**Num Attribs which have missing values**\n* Views has *100* missing values (was et explicity before)\n* Likes has *100* missing values (was et explicity before)\n* Dislikes has *100* missing values (was et explicity before)\n* Comment_count has *100* missing values (was et explicity before)\n","8953ae3f":"Trimming data since there are a lot of outliers","463bfb8d":"### Scatter Plot","b84d5741":"## Import US vidoes Dataset","0b0d09c8":"## Numerical Data Cleaning","73903e8e":"### Histograms","b61bb612":"# Loading and Preparing"}}