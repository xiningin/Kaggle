{"cell_type":{"7cf55a07":"code","90878f95":"code","1823b0d5":"code","0600ad0a":"code","7c71bb93":"code","c4a94ba0":"code","9f0df049":"code","c8f75484":"code","1fadf131":"code","b7a62bf1":"code","9d5f1216":"code","00f44826":"code","dda3f769":"code","645c3422":"code","8c368d43":"code","f88ca1e4":"code","98add9a3":"code","e483795f":"code","5ed728c3":"code","53ca7c08":"code","8c386f63":"markdown","0fee41c4":"markdown","89094f15":"markdown","897678b3":"markdown","a4a37980":"markdown","919ffc63":"markdown","806ecaf6":"markdown","e0c7aa22":"markdown","59be7989":"markdown","a2b38997":"markdown","ab4d807e":"markdown","aa2f73f4":"markdown"},"source":{"7cf55a07":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\n\nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","90878f95":"X_train = pd.read_csv(\"..\/input\/X_train.csv\")\nX_test = pd.read_csv(\"..\/input\/X_test.csv\")\ny_train = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","1823b0d5":"X_train.head()","0600ad0a":"plt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()","7c71bb93":"X_train.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_train = X_train.values.reshape((3810, 128, 10))","c4a94ba0":"X_test.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_test = X_test.values.reshape((3816, 128, 10))","9f0df049":"for j in range(2):\n    plt.figure(figsize=(15, 5))\n    plt.title(\"Target : \" + y_train['surface'][j], size=15)\n    for i in range(10):\n        plt.plot(X_train[j, :, i], label=i)\n    plt.legend()\n    plt.show()","c8f75484":"encode_dic = {'fine_concrete': 0, \n              'concrete': 1, \n              'soft_tiles': 2, \n              'tiled': 3, \n              'soft_pvc': 4,\n              'hard_tiles_large_space': 5, \n              'carpet': 6, \n              'hard_tiles': 7, \n              'wood': 8}","1fadf131":"decode_dic = {0: 'fine_concrete',\n              1: 'concrete',\n              2: 'soft_tiles',\n              3: 'tiled',\n              4: 'soft_pvc',\n              5: 'hard_tiles_large_space',\n              6: 'carpet',\n              7: 'hard_tiles',\n              8: 'wood'}","b7a62bf1":"y_train = y_train['surface'].map(encode_dic).astype(int)","9d5f1216":"class Attention(Layer):\n    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias: eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None: a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","00f44826":"def make_model():\n    inp = Input(shape=(128, 10))\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\n    x = Attention(128)(x)\n    x = Dense(9, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","dda3f769":"def k_folds(X, y, X_test, k=5):\n    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    \n    for i, (train_idx, val_idx) in  enumerate(folds):\n        print(f\"Fold {i+1}\")\n        model = make_model()\n        model.fit(X[train_idx], y[train_idx], batch_size=64, epochs=75, \n                  validation_data=[X[val_idx], y[val_idx]], verbose=0)\n        \n        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n        score = accuracy_score(pred_val, y[val_idx])\n        y_oof[val_idx] = pred_val\n        \n        print(f'Scored {score:.3f} on validation data')\n        \n        y_test += model.predict(X_test)\n        \n    return y_oof, y_test                                                                          ","645c3422":"y_oof, y_test = k_folds(X_train, y_train, X_test, k=5)","8c368d43":"print(f'Local CV is {accuracy_score(y_oof, y_train): .4f}')","f88ca1e4":"def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(15, 15))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","98add9a3":"plot_confusion_matrix(y_train, y_oof, encode_dic.keys())","e483795f":"y_test = np.argmax(y_test, axis=1)","5ed728c3":"sub['surface'] = y_test\nsub['surface'] = sub['surface'].map(decode_dic)\nsub.head()","53ca7c08":"sub.to_csv('submission.csv', index=False)","8c386f63":"### Confusion Matrix","0fee41c4":"### $k$-Folds","89094f15":"### Input","897678b3":"###  Attention Layer\nBecause that's fancy","a4a37980":"### Model","919ffc63":"## Modeling","806ecaf6":"### Submission","e0c7aa22":"### Load Data","59be7989":"# Deep Learning Starter\n\nIn this kernel, I directly feed the data into a **Recurrent Neural Network**. For fancyness, I added an **Attention Mechanism**.\n\nBecause of reproductibility issues, results are very unstable. The solution is to move to PyTorch but I wanted to produce something quickly.","a2b38997":"## Make Data for the Network","ab4d807e":"### Thanks for reading ! \n##### Please leave an upvote, it is always appreciated!","aa2f73f4":"### Ouput\n\nWe encode our targets"}}