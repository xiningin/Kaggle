{"cell_type":{"9be1eb71":"code","9f1847bf":"code","3729331c":"code","008dcab3":"code","023f60ba":"code","6e77d4f0":"code","72946766":"code","6a2679d0":"code","5e03b121":"code","a5d6ab4c":"code","0d90ac6e":"code","99af9350":"code","31b0c598":"code","ce156303":"code","5339bd54":"code","fd2eb8b6":"code","0ec7dde4":"code","0cff7666":"code","849b9313":"code","c2f551c4":"markdown","a66bf804":"markdown","e49273cc":"markdown","5cc96e38":"markdown"},"source":{"9be1eb71":"#lets import the libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom math import ceil\n\n#plots\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n#algorithms\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import parallel_coordinates\n\n#Advanced Optimization\nfrom scipy import optimize as op","9f1847bf":"#lets load the dataset\niris = pd.read_csv('..\/input\/data-science-machine-learning-and-ai-using-python\/Iris.csv')\niris.head()","3729331c":"#lets visualize the species\n#plot the species with respect to sepal length\nsepalPlt = sb.FacetGrid(iris, hue='Species',size=6).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\")\nplt.legend(loc='upper left')","008dcab3":"#plot the species with respect to petal length\npetalPlt = sb.FacetGrid(iris, hue='Species',size=6).map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\")\nplt.legend(loc='upper left')","023f60ba":"#let splot parallel coordinates of the petal and sepal\nparallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")","6e77d4f0":"#lets setup the data for training our model\nspecies = ['Iris-setosa','Iris-versicolor','Iris-virginica']\n\n#number of examples\nm = iris.shape[0]\n\n#features\nn = 4\n\n#number of classes\nk = 3\n\nX = np.ones((m, n+1))\nY = np.array((m,1))\n\nX[:,1] = iris['PetalLengthCm']\nX[:,2] = iris['PetalWidthCm']\nX[:,3] = iris['SepalLengthCm']\nX[:,4] = iris['SepalWidthCm']\n\n#lets provide labels\nY = iris['Species']\n\n#mean normalization\nfor j in range(n):\n  X[:,j] = (X[:,j] - X[:,j].mean())\n\n#lets split dataset\nX_train,X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2, random_state=11)","72946766":"#Logistic Regression\ndef sigmoid(z):\n  return 1.0\/(1 + np.exp(-z))\n\n#Regularised cost functions\ndef regCostFunction(theta, X, Y, _lambda = 0.1):\n  m = len(Y)\n  h = sigmoid(X.dot(theta))\n  reg = (_lambda\/(2*m) * np.sum(theta **2))\n\n  return ((1\/m) * (-Y.T.dot(np.log(h)) -(1-Y).T.dot(np.log(1-h))) + reg)\n\ndef regGradient(theta, X, Y, _lambda = 0.1):\n  m,n = X.shape\n  theta = theta.reshape((n,1))\n  Y = Y.reshape((m,1))\n  h = sigmoid(X.dot(theta))\n  reg = _lambda * theta \/ m\n\n  return ((1\/m) * X.T.dot(h-Y)) + reg\n\n#Optimal Theta\ndef logisticRegression(X,Y,theta):\n  res = op.minimize(fun = regCostFunction, x0 = theta, args = (X,Y), method='TNC', jac=regGradient)\n\n  return res.x","6a2679d0":"#Lets train our model\nall_theta = np.zeros((k, n+1))\n\n#one vs all\ni = 0\nfor flower in species:\n  #set the labels 0 and 1\n  tmp_y = np.array(Y_train == flower, dtype= int)\n  optTheta = logisticRegression(X_train, tmp_y, np.zeros((n + 1, 1)))\n  all_theta[i] = optTheta\n  i += 1","5e03b121":"#lets make predictions\nP = sigmoid(X_test.dot(all_theta.T))  #probability for each flower\np = [species[np.argmax(P[i, :])] for i in range(X_test.shape[0])]\n\n\n#lets print the accuracy\nprint(\"Test Accuracy : \", accuracy_score(Y_test, p) * 100, \"%\")","a5d6ab4c":"#lets import the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","0d90ac6e":"#lets initialise the dataset\nboston = pd.read_csv('..\/input\/data-science-machine-learning-and-ai-using-python\/Boston.csv')\n\n#lets view the dataset\nboston.head()","99af9350":"#lets visualize the dataset using scatter plot\nx = boston['rm']\ny = boston['medv']\n\n#plot the scatter plot\nplt.scatter(x,y, color='g')\nplt.xlabel('Avg rooms per dwelling')\nplt.ylabel('Median values of the home')","31b0c598":"#now lets define the feature variable and target variable of our dataset\nX = pd.DataFrame(x)  #feature variable\nY = pd.DataFrame(y)  #target variable","ce156303":"#lets divide the data into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y,test_size=0.20)","5339bd54":"#Building the model with Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(criterion='mse',random_state=100, max_depth=4, min_samples_leaf = 1)\n\n#train the model\nregressor.fit(X_train, Y_train)","fd2eb8b6":"#let visulaize the tree using graphviz\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(regressor, out_file = 'regression_tree.dot')","0ec7dde4":"#lets predict the values\ny_pred = regressor.predict(X_test)","0cff7666":"#lets print the values\nprint(y_pred[4:9])\nprint(Y_test[4:9])","849b9313":"#lets find out the rmse value\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_pred, Y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","c2f551c4":"**Decision Tree**\n\nA tree shaped algorithm to find the coarse of action. Each node in the tree represnt a action.","a66bf804":"# **Day 9**\n\n**Logistic Regression**\n\nIt is one of the popular ML Algorithm used in the case of predicting various categorical datasets","e49273cc":"**Problem Statement**\n\nUse Ml to Predict the selling price of houses baesd on some economic factors by using Decision Tree Model","5cc96e38":"**Problem Statement**\n\nClassification of Iris Flower's using Logistic Regression"}}