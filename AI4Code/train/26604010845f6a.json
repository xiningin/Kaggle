{"cell_type":{"e2338897":"code","daaadc41":"code","9d7ad8a5":"code","cc65eb4e":"code","26fc2a6b":"code","13351151":"code","86ecbb55":"code","7726beec":"code","eccb7b46":"code","154076d3":"code","67af0279":"code","e4b5f0f3":"code","e5b2d64d":"code","4be398fa":"code","473da45b":"code","20e5fe0f":"code","3e2f5f3c":"code","4304ba45":"code","c31affe5":"code","9f3869b9":"code","7509323d":"code","ca5cccce":"code","1d915d3e":"code","d00bfdfe":"code","5affb9d6":"code","7ea37b34":"code","ee3ec5ce":"code","140cafba":"code","ff8b0dc0":"code","0212e65a":"code","9ddd4545":"code","2131745f":"code","8521487d":"code","65e884e8":"code","dd64f305":"code","c7df35c8":"code","1d617b93":"code","fc571d6a":"code","5578e1a0":"code","1128394c":"markdown","9af509b8":"markdown","e4c1ea39":"markdown","4b6cc2a5":"markdown","b0f3ea3f":"markdown","1989545c":"markdown","2238fcff":"markdown","2ef76c4a":"markdown","67931c07":"markdown","708f345d":"markdown","ff5a5870":"markdown","87b43271":"markdown","1df86a4f":"markdown","2b55547a":"markdown","99dbef1d":"markdown","e9b054ff":"markdown","681f6620":"markdown","5221d2fe":"markdown","55251670":"markdown","3313ea55":"markdown","fc5d678b":"markdown","06cb0475":"markdown","a8bc4073":"markdown","d28e3eb6":"markdown","a64c9bff":"markdown","8c8d4685":"markdown","834fd44a":"markdown"},"source":{"e2338897":"#importing library\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline","daaadc41":"train = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","9d7ad8a5":"# print shape of dataset with rows and columns\nprint(train.shape)","cc65eb4e":"# print the top5 records\ntrain.head()","26fc2a6b":"# to print the full summary\ntrain.info()","13351151":"categorical_features=[feature for feature in train.columns if train[feature].dtypes=='O' or train[feature].dtypes== 'int64']\nprint(categorical_features)","86ecbb55":"discrete_feature=[feature for feature in train.columns if len(train[feature].unique())<25 and feature not in ['Class']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","7726beec":"# step make the list of features which has missing values\nfeatures_with_na=[features for features in train.columns if train[features].isnull().sum()>1]\n# print the missing features list\nprint(len(features_with_na))","eccb7b46":"sns.countplot(train['Class'])\nplt.show()\nprint('Percent of fraud transaction: ',len(train[train['Class']==1])\/len(train['Class'])*100,\"%\")\nprint('Percent of normal transaction: ',len(train[train['Class']==0])\/len(train['Class'])*100,\"%\")","154076d3":"# Lets analyse the continuous values by creating histograms to understand the distribution\ndata=train.copy()\ndata.drop(columns='Class', inplace = True)\n\nfor feature in data.columns:\n    train[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","67af0279":"#figure_factory module contains dedicated functions for creating very specific types of plots\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nclass_0 = train.loc[train['Class'] == 0][\"Time\"]\nclass_1 = train.loc[train['Class'] == 1][\"Time\"]\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","e4b5f0f3":"plt.figure(figsize = (14,14))\nplt.title('Feature correlation')\ncorr = train.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","e5b2d64d":"# Transaction amount \ndata=train.copy()\ndata.drop(columns=['Class'], inplace = True)\nfor i in data.columns:\n  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n  s = sns.boxplot(ax = ax1, x=\"Class\", y=i, hue=\"Class\",data=train, palette=\"PRGn\",showfliers=True)\n  s = sns.boxplot(ax = ax2, x=\"Class\", y=i, hue=\"Class\",data=train, palette=\"PRGn\",showfliers=False)\n  plt.show();","4be398fa":"#Features density plot\ncol = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n       'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n       'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',\n       'V28']\n\ni = 0\nt0 = train.loc[train['Class'] == 0]\nt1 = train.loc[train['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,30))\n\nfor feature in col:\n    i += 1\n    plt.subplot(7,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\", color='b')\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\", color='r')\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","473da45b":"pca_vars = ['V%i' % k for k in range(1,29)]\nplt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=t0[pca_vars].skew(), color='darkgreen')\nplt.xlabel('Column')\nplt.ylabel('Skewness')\nplt.title('V1-V28 Skewnesses for Class 0')","20e5fe0f":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=t1[pca_vars].skew(), color='darkgreen')\nplt.xlabel('Column')\nplt.ylabel('Skewness')\nplt.title('V1-V28 Skewnesses for Class 1')","3e2f5f3c":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(train, hue=\"Class\", height = 6).map(plt.scatter, \"Time\", \"Amount\").add_legend()\nplt.show()","4304ba45":"FilteredData = train[['Time','Amount', 'Class']]\ncountLess = FilteredData[FilteredData['Amount'] < 2500]\ncountMore = train.shape[0] - len(countLess)\npercentage = round((len(countLess)\/train.shape[0])*100,2)\nClass_1 = countLess[countLess['Class'] == 1]\nprint('Total number for transaction less than 2500 is {}'.format(len(countLess)))\nprint('Total number for transaction more than 2500 is {}'.format(countMore))\nprint('{}% of transactions having transaction amount less than 2500' .format(percentage))\nprint('{} fraud transactions in data where transaction amount is less than 2500' .format(len(Class_1)))","c31affe5":"sns.boxplot(x = \"Class\", y = \"Amount\", data = train)\nplt.ylim(0, 5000)\nplt.show()","9f3869b9":"Amount_0 = train.loc[train['Amount'] == 0]\nprint(Amount_0['Class'].value_counts())","7509323d":"from sklearn.preprocessing import StandardScaler, RobustScaler\ndata1=train.copy()\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ntrain['scaled_amount'] = std_scaler.fit_transform(train['Amount'].values.reshape(-1,1))\ntrain['scaled_time'] = std_scaler.fit_transform(data1['Time'].values.reshape(-1,1))\n\ntrain.drop(['Amount', 'Time'], axis=1, inplace = True)\nscaled_amount = train['scaled_amount']\nscaled_time = train['scaled_time']\n\ntrain.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ntrain.insert(0, 'scaled_amount', scaled_amount)\ntrain.insert(1, 'scaled_time', scaled_time)\nprint(train.head())","ca5cccce":"population = data1[data1['Class'] == 0].Amount\nsample = data1[data1['Class'] == 1].Amount\nsampleMean = sample.mean()\npopulationStd = population.std()\npopulationMean = population.mean()\nz_score = (sampleMean - populationMean) \/ (populationStd \/ sample.size ** 0.5)\nz_score","1d915d3e":"print(train.head())","d00bfdfe":"from sklearn.model_selection import train_test_split\nX = train.drop(['Class'], axis=1)\nY = train['Class']\n# This is explicitly used for with data imbalance\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\nprint(X.shape, Y.shape)","5affb9d6":"print('X train shape: ', X_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('y train shape: ', y_train.shape)\nprint('y test shape: ', y_test.shape)","7ea37b34":"print(y_test.value_counts())","ee3ec5ce":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\n# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, confusion_matrix, classification_report, matthews_corrcoef, cohen_kappa_score, accuracy_score, average_precision_score, roc_auc_score, precision_recall_fscore_support\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import  f_classif\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier","140cafba":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"BaggingClassifier\": BaggingClassifier(n_estimators=10, random_state=0),\n    \"SGDClassifier\" : SGDClassifier(),\n    \"GradientBoostingClassifier\" : GradientBoostingClassifier(),\n    \"xgb\" : XGBClassifier()\n}","ff8b0dc0":"def plot(df):\n  fraud = df[df['class']==1]\n  normal = df[df['class']==0]\n  fraud.drop(['class'],axis=1,inplace=True)\n  normal.drop(['class'],axis=1,inplace=True)\n  fraud = fraud.set_index('classifier')\n  normal = normal.set_index('classifier')\n  plt.figure(figsize = (8,2))\n  sns.heatmap(fraud.iloc[:, :], annot=True, cmap=sns.light_palette((210, 90, 60), input=\"husl\"),linewidth=2)\n  plt.title('class 1')\n  plt.show()\n  plt.figure(figsize = (8,2))\n  sns.heatmap(normal.iloc[:, :], annot=True, cmap=sns.light_palette((210, 90, 60), input=\"husl\"),linewidth=2)\n  plt.title('class 0')\n  plt.show()","0212e65a":"def roc_curve(y_test, rdict):\n  sns.set_style('whitegrid')\n  plt.figure()\n  i=0\n  fig, ax = plt.subplots(4,2,figsize=(16,30))\n  for key,val in rdict.items():\n    fpr, tpr, thresholds = metrics.roc_curve( y_test, val,\n                                                  drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( y_test, val)\n    i+= 1\n    plt.subplot(4,2,i)\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title(key)\n    plt.legend(loc=\"lower right\")\n  plt.show()","9ddd4545":"def training(models, x, y, x_t, y_t):\n    conf = []\n    comp = []\n    rdict = {}\n    for key, model in models.items():\n      model = model.fit(x, y)\n      y_pred = model.predict(x_t)\n      rdict[key] = y_pred\n      tn, fp, fn, tp = confusion_matrix(y_t, y_pred).ravel()\n      precision, recall, fscore, support = metrics.precision_recall_fscore_support(y_t, y_pred)\n      r1 = {'Classifier': key, 'TN': tn, 'TP': tp, 'FN': fn, 'FP': fp}\n      conf.append(r1)\n      MCC = matthews_corrcoef(y_t, y_pred)\n      AUROC = roc_auc_score(y_t, y_pred)\n      Cohen_kappa = cohen_kappa_score(y_t, y_pred)\n      accuracy = metrics.accuracy_score(y_t, y_pred)\n      r2 = {'classifier': key,'matthews_corrcoef':MCC,'Cohen_kappa':Cohen_kappa,'accuracy': accuracy,'AUROC':AUROC, 'precision': precision[0],'recall':recall[0],'f1':fscore[0], 'class':0}\n      r3 = {'classifier': key,'matthews_corrcoef':MCC,'Cohen_kappa':Cohen_kappa,'accuracy': accuracy,'AUROC':AUROC, 'precision': precision[1],'recall':recall[1],'f1':fscore[1], 'class':1}\n      comp.append(r2)\n      comp.append(r3)\n    r11 = (pd.DataFrame(conf).to_markdown())\n    r12 = pd.DataFrame(comp)\n    print(f'\\n\\nRoc curve \\n\\n')\n    roc_curve(y_t, rdict)\n    print(f'\\n\\n confusion matrixs comparison \\n\\n')\n    print(r11)\n    print(f'\\n\\n Performance comparison \\n\\n')\n    plot(r12)\n    ","2131745f":"training(classifiers, X_train, y_train, X_test, y_test)","8521487d":"bestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score'] \nfeatureScores_df = featureScores.sort_values(['Score', 'Specs'], ascending=[False, True])  #naming the dataframe columns\nprint(featureScores_df)","65e884e8":"col = ['V17', 'V14', 'V12','V10','V16','V3','V7','V11','V4','V18','V1','V9','V5','V2','V6','V21','V19','V20','V8','V27','scaled_time','V28','V24']","dd64f305":"training(classifiers, X_train[col], y_train, X_test[col], y_test)","c7df35c8":"xgb = XGBClassifier()\n# X_train[col], y_train, X_test[col], y_test\nxgb.fit(X_train[col],y_train)\ny_pred_final = xgb.predict(X_test[col])","1d617b93":"submission = pd.DataFrame({'ID':X_test['V17'],'Prediction':y_pred_final})","fc571d6a":"submission.shape","5578e1a0":"submission.to_csv('\/kaggle\/working\/submission.csv',index=False)","1128394c":"## Analysis  - Are fraudulent transactions of higher value than normal transactions\n\nif fraudulent transactions are in general of higher value than normal transactions or not. To check this lets do a hypothesis test. Lets define our Null and Alternative hypothesis.\n\n>$H_{0}$ : Fraudulent transactions are of similar or higher value as normal transactions\n\n>$H_{A}$ : Fraudulent transactions are of lower value as normal transactions\n\nFor the hypothesis test I will be performing a Z-test, with the valid transactions acting as the population. Though a T-test can also be performed but given that our sample set (fraudulent transactions) is of size 492 there shouldn't be any difference, as for sample set >= 30 the t distribution and z distribution are nearly the same.\n\nWe will be performing the test for 99% significance level, this means that we should get a z-score of atleast 2.58 or higher. formula for z-score, z\u2212score=(x\u00af\u2212\u03bc)\/S.D\n\n\nWhere\n\n* x\u00af  : mean of the sample\n* \u03bc  : population mean\n* S.D : Standard deviation of sampling diatribution\n\nThe standard deviation of sampling distribution in our case is given by the formula :  \u03c3\/\u221an , where  \u03c3  is the Standard deviation of the population and n is the sample size\n\nfor more info refer this [article](https:\/\/towardsdatascience.com\/hypothesis-testing-the-what-why-and-how-867d382b99ca)","9af509b8":"# Outliers","e4c1ea39":"There are 1,825 transactions that has 0 amount, 27 of them are fraud. One of the observation is 0 pending charge by a person is a verification method to verify the fraud.","4b6cc2a5":"In Data Analysis We will Analyze To Find out the below points\n* Categorical Variables\n* Cardinality of Categorical Variables \n* Missing Values\n* Univariant analysis of all The Numerical Variables\n* Distribution of the Numerical Variables\n* Feature Correlation\n* Outliers\n* Relationship between independent and dependent feature\n","b0f3ea3f":"As the z-score is more than 2.58 we reject the Null hypothesis. So there is a 99% chance that the amount spend on fraudulent transactions are on average significantly lower than normal.","1989545c":"\n# Problem statement - Identify fraudulent credit card transactions.\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Make your machine learning model learn to detect these anomalous transactions\n","2238fcff":"# Model Building","2ef76c4a":"We don't have any missing values ","67931c07":"Correlation is **Positive** when the values **increase together**, and Correlation is **Negative** when one value **decreases as the other increases**\n\nCorrelation can have a value:\n\n* 1 is a perfect positive correlation (between 'Amount' and V7, 'Amount' And 'V20')\n* 0 is no correlation (**between features V1-V28**)\n* -1 is a perfect negative correlation (between 'Time' and V3, 'Amount' and V2, 'Amount' and V5)","708f345d":"## Observations:\n\n* From the above two plots it is clearly visible that there are frauds only on the transactions which have transaction amount approximately less than 2500. Transactions which have transaction amount approximately above 2500 have no fraud.\n* As per with the time, the frauds in the transactions are evenly distributed throughout time.","ff5a5870":"# Data Analysis","87b43271":"Normalization is important in PCA since it is a variance maximizing exercise. It projects your original data onto directions which maximize the variance.\n\nsince V1, V2, \u2026 V28 are the principal components obtained with PCA we can clearly see that from above plot that from V1 to V28 variables are normalized. Variable 'Time' and 'Amount' is not normalized.","1df86a4f":"From above plot we can see fradulant transaction has more skewness than normal transaction","2b55547a":"In our dataset we have only one categorical features '**Class**' , k is the original number of unique values in your data column. High cardinality means a lot of unique values ( a large k). A column with hundreds of zip codes is an example of a high cardinality feature. High cardinality theme bird. High dimensionality means a matrix with many dimensions. So we don't have high cardinality present in the dataset.","99dbef1d":"#Data Unbalance","e9b054ff":"#Features correlation","681f6620":"Now we're ready to build machine learning models to predict whether a transaction is fraudulent. We'll train the following models:\n\n* Logistic regression\n* Support vector classifier\n* Desicision Tree\n* Random forest\n* Bagging classifier","5221d2fe":"## **Conclusion**\n\n* The performance of the model slightly increased after removing the few parameters like Amount, V13, V15, V22, V23, V25.\n* As per the Hypothesis testing a the random forest and bagging classifier performed much better than the Logistic regression, support vector machine and Decision Tree, with and without any hyperparameter tweaking!and from the training result we can conclude that amount variable is not impacting the prediction.\n* XGB classifier performed much better than the rest all the algorithm without any hyperparameter tweaking!","55251670":"Observation:\n\n* V3, V4, V10, V11, V17-V19 have clearly separated distributions for Class values 0 and 1\n* V1, V2, V7, V9, V12, V14, V16, V18 have partially saperated distribution for Class 0 and 1\n* V13, V15, ,V20, V22-V28 have almost similar distribution for Class 0 and 1\n* V5, V6, V8, V21 have quite similar distribution for Class 0 and 1\n\nIn general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.","3313ea55":"Only 492 (or 0.172%) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable Class.","fc5d678b":"## Model training with Entire dataset","06cb0475":"I checked if any column is having more than 25 unique values so we classified it as discrete feature, But it seem that we have only continous values only.","a8bc4073":"Observations:\n\n* There are 284358 transactions which has a transaction amount less than 2500. Means 99.84% of transactions have transaction amount less than 2500\n* total number of fraud transactions in whole data are 492. It has been calculated that total number of fraud transactions in data where transaction amount is less than 2500 is also 492. Therefore, all 100% fraud transactions have transaction amount less than 2500 and there is no fraud transaction where transaction amount is more than 2500.\n* From above box plot we can easily infer that there are no fraud transactions occur above the transaction amount of 2500. All of the fraud transactions have transaction amount less than 2500. However, there are many transactions which have a transaction amount greater than 2500 and all of them are genuine.","d28e3eb6":"## Model Training with only selected feature","a64c9bff":"Above Box plot say fradulant transaction has more outlier than non fradulant transaction. Since our data is highly imbalance and we have less amount frudulant trasaction so traforming outlier leads to loss of infomation. we will use outlier as it is.","8c8d4685":"**Credit Card Transactions Time Density Plot** visualises the distribution of '**Not Fraud**' and '**Fraud**' transaction over a continuous interval or time period. \n\nFraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time.\n\nSo 'Time' feature can't tell whether the trasaction is Fraudulent transactions or not.","834fd44a":"## Data Transformation"}}