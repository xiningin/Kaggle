{"cell_type":{"e956a508":"code","4555ae1b":"code","bc966852":"code","3866c9fd":"code","375a2e9c":"code","d547c29b":"code","4ea1720a":"code","37de8c9c":"code","92d07179":"code","a305f016":"code","8dbd3c05":"code","5be4f69c":"code","bd6cf96c":"code","2f8f8775":"code","8883aceb":"code","565434f7":"code","36f9d12a":"code","24feb2f1":"code","5b038430":"code","1b5d57d4":"code","ee978879":"code","4ba5eaed":"code","0b565420":"code","80e9c436":"markdown"},"source":{"e956a508":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport csv\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout, Conv1D, GlobalMaxPool1D","4555ae1b":"# read csv file\ntweets = []\nlabels = []\n\nwith open('..\/input\/nlp-getting-started\/train.csv','r') as csv_file:\n    reader = csv.reader(csv_file, delimiter=',')\n    next(reader)\n    for row in reader:\n        tweets.append(str(row[3]))\n        labels.append(int(row[4]))\n\ndataset_size = len(tweets)\nprint('Dataset size = ',dataset_size)","bc966852":"# punctuation characters\nimport string\nprint(string.punctuation)","3866c9fd":"# english stop words\nimport nltk\nstopwords_list=nltk.corpus.stopwords.words('english')","375a2e9c":"# exclude negation, superlative, \nexclude_list = ['but', 'against', 'on', 'off', 'over', 'all', 'any', 'most', 'no', 'nor', 'not', 'so', 'too', 'very', \"don't\", \"aren't\",\n                     \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \n                     \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\"]\n\nfor word in exclude_list:\n    stopwords_list.remove(word)\n\nprint(stopwords_list)","d547c29b":"import re\n# clean tweets function\n\ndef clean_tweets(tweets):\n    \n    cleaned_tweets = []\n    ps = PorterStemmer()\n    \n    for tweet in tweets:\n#         delete urls\n        clean_tweet = re.sub(r'https?:\/\/(\\w+\\.)(\\w+\/\\w+)?','',tweet.lower())\n#         delete dates in different formats \"xx\/xx\/xxxx xx\/xx\/xx xx\/xx xx-xx-xxxx xx.xx.xxxx\n        clean_tweet = re.sub(r'\\(?\\d\\d?.\\d\\d?(.\\d\\d)?(\\d\\d)?\\)?','',clean_tweet)\n#         delete time in different formats\n        clean_tweet = re.sub(r'\\(?@?\\d\\d?:\\d\\d(\\sPM)?','',clean_tweet)\n#         delete @someone\n        clean_tweet = re.sub(r'@\\w*[\\s$]',' ',clean_tweet)\n#         delete numbers\n        clean_tweet = re.sub(r'\\d+',' ',clean_tweet)\n#         delete unique alphanumeric character\n        clean_tweet = re.sub(r'\\s\\w\\s',' ',clean_tweet)\n#         delete strange words with or without ponctuation\n        clean_tweet = re.sub(r'(\u02c6|\\s)\\S+[\\.\\?\\\"\\\\\\$\\\u02c6\\*\\+\\-_@&=\u00f7;,%\/\\d]+[\u02c6\\s\\.]*[\\s$]',' ',clean_tweet)\n        clean_tweet = re.sub(r'\\s\\W\\S*[\\s$]',' ',clean_tweet)\n#         delete stop words ans stemming remaining words \n        clean_tweet = ' '.join([word for word in clean_tweet.split() if word not in stopwords_list])        \n\n        cleaned_tweets.append(clean_tweet)\n        \n    return cleaned_tweets","4ea1720a":"# clean trainig dataset\ncleaned_tweets = clean_tweets(tweets)","37de8c9c":"# show train sample before and after cleaning\nstart = 250\nend = 270\n\nfor id in range(start,end):\n    print('original : ',tweets[id])\n    print('_cleaned : ',cleaned_tweets[id],'\\n')","92d07179":"# check the max_length of the majority of cleaned tweets\nlength_090 = length_100 = length_110 = 0\n\nfor x in cleaned_tweets:\n    if len(x)<90: length_090+=1        \n    if len(x)<100: length_100+=1\n    if len(x)<110: length_110+=1\n\nprint('length_090 =', length_090*100\/dataset_size)\nprint('length_100 =', length_100*100\/dataset_size)\nprint('length_110 =', length_110*100\/dataset_size)\n","a305f016":"# nlp preprocessing params\ntrain_size = 7000\nvocab_size = 10000\noov_tok = '<OOV>'\nembedding_dim=32\ntrunc_type = 'pre'\npadding_type = 'pre'\nmax_length = 100","8dbd3c05":"# split dataset to train and validation sets\ntrain_tweets = np.array(cleaned_tweets[:train_size])\ntrain_labels = np.array(labels[:train_size])\nvalid_tweets = np.array(cleaned_tweets[train_size:])\nvalid_labels = np.array(labels[train_size:])","5be4f69c":"# tokenization\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_tweets)\nword_index = tokenizer.word_index\nprint(len(word_index))","bd6cf96c":"train_sequences = tokenizer.texts_to_sequences(train_tweets)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nvalid_sequences = tokenizer.texts_to_sequences(valid_tweets)\nvalid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","2f8f8775":"print(word_index)","8883aceb":"model_conv = tf.keras.Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_length),\n    Dropout(0.2),\n    Conv1D(128, 7, activation='relu'),\n    GlobalMaxPool1D(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-4)\n\nmodel_conv.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel_conv.summary()\n\nhistory1 = model_conv.fit(\ntrain_padded,\ntrain_labels,\nepochs=30,\nvalidation_data=(valid_padded, valid_labels),\nbatch_size=32)","565434f7":"# Preparing test data\n\n# read csv file\ntest_tweets = []\ntest_id = []\n\n\nwith open('..\/input\/nlp-getting-started\/test.csv','r') as csv_file:\n    reader = csv.reader(csv_file, delimiter=',')\n    next(reader)\n    for row in reader:\n        test_tweets.append(str(row[3]))\n        test_id.append(row[0])\n        \n# clean test tweets\ncleaned_test_tweets = clean_tweets(test_tweets)\n\ntest_sequences = tokenizer.texts_to_sequences(cleaned_test_tweets)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","36f9d12a":"# show test sample before and after cleaning\n\nstart = 100\nend = 110\n\nfor id in range(start,end):\n    print('original : ', test_tweets[id])\n    print('_cleaned : ', cleaned_test_tweets[id],'\\n')","24feb2f1":"# Prediction\ntest_pred = model_conv.predict(test_padded)\nprint(test_pred.shape)\ntest_pred","5b038430":"test_pred_bool = test_pred.copy().astype(int)\nfor index in range(len(test_pred)): \n    if test_pred[index]>0.6:\n        test_pred_bool[index]=1\n    else:\n        test_pred_bool[index]=0","1b5d57d4":"# prepare submission file\nprediction_file = pd.DataFrame(columns=['id','target'])\nprediction_file['id']= test_id\nprediction_file['target']= test_pred_bool","ee978879":"# show test prediction sample\nprediction_file.head()\nstart = 0\nend = 10\nfor index in range(start,end):\n    print(test_tweets[index])\n    print(test_pred_bool[index],'\\n')","4ba5eaed":"print('Positive tweets prediction = ', len(prediction_file[prediction_file['target']==1]))\nprint('Negative tweets prediction = ', len(prediction_file[prediction_file['target']==0]))","0b565420":"prediction_file.to_csv('submission.csv', index=False)","80e9c436":"### use regex to clean tweets\n\n### Special Characters\n#### In the regex flavors discussed in this tutorial, there are 12 characters with special meanings: \n##### the caret ^\n##### the dollar sign $\n##### the vertical bar or pipe symbol |\n##### the opening parenthesis (\n##### the closing parenthesis )\n##### the opening square bracket [\n##### the opening curly brace {\n\n##### \\w  represents any alphanumeric characters (including underscore)\n##### \\d  represents any digit\n##### .   represents ANY character (do not confuse it with a period )\n##### abc literally matches characters abc in a string\n##### [abc] matches either a or b or c (characters within the brackets)\n##### ?   after a character indicates that the character is optional\n##### *   after a character indicates it can be repeated 0 or more times\n##### +   after a character indicates it can be repeated 1 or more times\n##### \\   is used to escape special characters"}}