{"cell_type":{"e97f5a0f":"code","3b78569b":"code","d782fbf1":"code","bb989f70":"code","04674cd9":"code","ad9bc205":"code","8069440c":"code","866113e5":"code","8c6fdda4":"code","553d37e0":"code","1e917308":"code","85b14a8b":"code","aaa80081":"code","8a45f041":"code","54ce910d":"code","18b4d5cb":"code","60b764c5":"code","585edfb8":"code","d61ed704":"code","b4a082a9":"markdown","d4d5b272":"markdown","468d582c":"markdown","dd448fee":"markdown","3dcd7675":"markdown","bd77ba78":"markdown","b5a65b05":"markdown","c15ab1be":"markdown","24182246":"markdown","33b3555c":"markdown","98c52f3d":"markdown"},"source":{"e97f5a0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b78569b":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nkeras.backend.set_image_data_format('channels_last')","d782fbf1":"mnist_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nmnist_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n# Preserve original dataset\nX_train = mnist_train.drop('label', axis=1).copy()\nX_test = mnist_test.copy()\nY_train = mnist_train['label'].copy()","bb989f70":"# Shapes of training and test data\nprint(f\"Training Set Size: {X_train.shape}\")\nprint(f\"Test Set Size: {X_test.shape}\")\nprint(f\"Label Size: {Y_train.shape}\")","04674cd9":"# Normalize values\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","ad9bc205":"# Reshape to 28 x 28\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)","8069440c":"# Display Sample Image\nfig, ax = plt.subplots(figsize=(10, 10))\nplt.imshow(X_train[1, :, :, 0], cmap='Greys', interpolation='nearest')\nplt.title(\"Sample Image\")\nplt.show()","866113e5":"# Split between train and validation set\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)","8c6fdda4":"# Get one hot encoding\nY_train = keras.utils.to_categorical(Y_train, num_classes=10)\nY_val = keras.utils.to_categorical(Y_val, num_classes=10)","553d37e0":"# Build CNN Model\ndef CNN():\n    model = keras.Sequential()\n    # CONV > CONV > BN > RELU > MAXPOOLING > DROPOUT\n    model.add(layers.Conv2D(32, (3, 3), (1, 1), padding='valid', input_shape=(28, 28, 1), name='conv2d_1_1'))\n    model.add(layers.Conv2D(32, (3, 3), (1, 1), padding='same', name='conv2d_1_2'))\n    model.add(layers.BatchNormalization(name='bn_1'))\n    model.add(layers.Activation('relu', name='relu_1'))\n    model.add(layers.MaxPooling2D((2, 2), (2, 2), padding='valid', name='mp2d_1'))\n    model.add(layers.Dropout(0.2, name='drop_1'))\n    # CONV > CONV > BN > RELU > MAXPOOLING > DROPOUT\n    model.add(layers.Conv2D(64, (3, 3), (1, 1), padding='valid', name='conv2d_2_1'))\n    model.add(layers.Conv2D(64, (3, 3), (1, 1), padding='same', name='conv2d_2_2'))\n    model.add(layers.BatchNormalization(name='bn_2'))\n    model.add(layers.Activation('relu', name='relu_2'))\n    model.add(layers.MaxPooling2D((2, 2), (2, 2), padding='valid', name='mp2d_2'))\n    model.add(layers.Dropout(0.2, name='drop_2'))\n    # FLATTEN > DENSE > CLASSIFICATION\n    model.add(layers.Flatten())\n    model.add(layers.Dense(100, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n    \n    return model","1e917308":"model = CNN()","85b14a8b":"model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])","aaa80081":"model.summary()","8a45f041":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=64, epochs=80, verbose=1)","54ce910d":"plt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()","18b4d5cb":"def predict(model, X, imgs):\n    s = int(np.sqrt(imgs))\n    fig, ax = plt.subplots(s, s, sharex=True, sharey=True, figsize=(15, 15))\n    ax = ax.flatten()\n    preds = model.predict(X[:imgs])\n    for i in range(imgs):\n        y_pred = np.argmax(preds[i])\n        img = X[i].reshape(28, 28)\n        ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n        ax[i].set_title(f'p: {y_pred}')","60b764c5":"predict(model, X_test, 25)","585edfb8":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)","d61ed704":"y_pred = pd.Series(y_pred, name='Label')\nsub = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), y_pred], axis=1)\nsub.to_csv('mnist_submission.csv', index=False)","b4a082a9":"### Feature Normalization and Preprocessing\n\nHere we will scale the values to be between 0 and 1. We won't do feature normalization where we make sure we have 0 mean since each feature are of the same scale. Dividing by 255.0 is enough for the task here. We would then reshape the training and test set to meet the requirements of TensorFlow and Keras.","d4d5b272":"### Graphing Accuracy\n\nBelow we will see how our model went by graphing accuracy with validation accuracy, and our training loss with validation loss.","468d582c":"# Digit Recognizer\n\nWelcome! This notebook shows my process on building a Convolutional Neural Network for the MNIST Dataset. The model architecture is considered simple, but quite effective for the score it provides. There are some functions here to see what predictions are being processed in by the Network and other functions that help visualize and group the different components.","dd448fee":"## Model Architecture\n\nBelow is a function defining a CNN model with 3 main blocks of Convolutional layers. The first two blocks follow the same structure:\n1. Apply a Conv2D layer with 3x3 kernel size and valid padding, then another Conv2D layer with 3x3 kernel but with same padding to keep the same dimensions\n2. Apply a BatchNormalization layer to avoid layers being too depended from one another and allowing each activation to have 0 mean\n3. A RELU activation and a MaxPooling2D layer with 2x2 kernel size and stride=2\n4. An element-wise Dropout layer applied to MaxPooling2D keeping 80% of the activation units\n\nThe final 2 layers consists of a Fully Connected Layer that uses a Softmax activation for classification.","3dcd7675":"### Displaying Sample Images\n\nBelow we are displaying the images within the dataset. Play around with the first index to explore what kind of images we are given within the dataset.","bd77ba78":"### Predictions\n\nBelow is a function to help see whether we have trained the model properly. \"imgs\" is a parameter to see the first x number of images in our test dataset.","b5a65b05":"### Data Conversion\n\nBelow we are going to split the training set into a 99:1 training and validation set. 99% of the training set will be used to train the model and the other 1% will be used to monitor our accuracy and loss on data we have not seen in the training set. The labels \"Y_train\" and \"Y_val\" will be one hot encoded with 10 classes.","c15ab1be":"## Load the Data\n\nHere we load the data from Kaggle and separate the labels from the training set and copy the original dataset into a new variable.","24182246":"## Training and Prediction\n\nWe will train the model for 80 epochs, with a batch size of 64.","33b3555c":"### Model Compilation\n\nHere we will use an Adam optimizer with a Cross entropy loss function.","98c52f3d":"## Submission\n\nWe create the full prediction and place the predictions into the requested format."}}