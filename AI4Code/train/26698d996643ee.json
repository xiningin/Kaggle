{"cell_type":{"55eef00d":"code","e17697dc":"code","005de87f":"code","150a90f7":"code","4338ddbf":"code","763ef0d1":"code","98bd5969":"code","55c634b5":"code","2a55b25b":"code","ff0d2b66":"code","c4629035":"code","d1c6d49a":"code","95cce892":"code","cc5b77ae":"code","09543916":"code","caf96c0a":"code","ab37b765":"code","859bc56f":"code","9320bccc":"code","41e78d2a":"code","a7178f44":"code","f507d378":"code","d55cf8bd":"code","233055bd":"code","bc7f9fcb":"code","03e5d67c":"code","478f7cf0":"code","4a81ae6b":"code","5e0f983d":"code","69bb8ca8":"code","2f0ebe8e":"code","6ad593a1":"markdown","08ec532a":"markdown","6c272cc2":"markdown","8286b42e":"markdown","44e99b63":"markdown","679827d4":"markdown","d011b96b":"markdown","ba9bddb3":"markdown","b8b8a8a7":"markdown","43203465":"markdown","a73ad01a":"markdown","ed872ace":"markdown","c2781232":"markdown","0849c150":"markdown","1a55fa6c":"markdown","ef4e998a":"markdown","6bbbcd08":"markdown","2e34b114":"markdown","0705dec0":"markdown"},"source":{"55eef00d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import normalize\nfrom sklearn.svm import SVC\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e17697dc":"#retrieve data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n# regroup data \nall_data = pd.concat([train, test], ignore_index=True)\n","005de87f":"#data info\ntrain.info()\nprint(\"-\" *60)\ntest.info()","150a90f7":"# show summary statistics\ntrain.describe()\ntest.describe()","4338ddbf":"plt.hist(train.Age)\nplt.xlabel('Age')\n","763ef0d1":"train[['Age', 'Survived']].groupby('Age').count().plot.bar(figsize=(25,5))","98bd5969":"# fill NaN with a random value close to median\nall_data.Age = all_data.Age.fillna(random.randrange(all_data.Age.median()-5, all_data.Age.median()+5, 2))\n","55c634b5":"fig,ax = plt.subplots(1,2,figsize=(12,5))\nsns.barplot(x='Sex', y='Survived', data= train, ax=ax[0])\nsns.countplot('Sex', hue='Survived', data= train, ax=ax[1])","2a55b25b":"fig, ax = plt.subplots(1, 3, figsize=(18,5))\nsns.countplot(x='Pclass', data=train, ax=ax[0])\nax[0].set_title('Number of passengers by class')\nsns.countplot(x='Pclass', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Survival counts per class')\nsns.barplot(x='Pclass', y='Survived', data=train, ax=ax[2])\nax[2].set_title('Survival rate per class')","ff0d2b66":"fig, ax = plt.subplots(1, 3, figsize=(15,5))\nsns.countplot('Embarked', data=train, ax=ax[0])\nax[0].set_title('Number of passengers by port of embarcation ')\nsns.countplot('Embarked', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Survival count by port')\nsns.barplot('Embarked', 'Survived', data=train, ax=ax[2])\nax[2].set_title('Survival rate by port')","c4629035":"all_data.Embarked = all_data.Embarked.fillna('S')","d1c6d49a":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nsns.countplot('Parch', data=train, ax=ax[0])\nax[0].set_title('Nb of Passengers by Nb of children')\nsns.countplot('Parch', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Survival count by Nb of children')","95cce892":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nsns.countplot('SibSp', data=train, ax=ax[0])\nax[0].set_title('Nb of Passengers by Nb of siblings or\/and spouse')\nsns.countplot('SibSp', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Survival count by Nb of siblings or\/and spouse')","cc5b77ae":"plt.hist(train.Fare)\nplt.title('Fare Distribution')\nplt.xlabel('Fare')\nplt.ylabel('Nb of passengers')","09543916":"# handle missing values\nall_data.Fare = all_data.Fare.fillna(all_data.Fare.median())\n","caf96c0a":"# know what columns still contain missing values\nall_data.columns[all_data.isna().any()]","ab37b765":"all_data['Family_size'] = all_data.Parch + all_data.SibSp\nall_data['Parch_sq'] = all_data.Parch ** 2\nall_data['SibSp_sq'] = all_data.SibSp ** 2\nall_data['Family_size_sq'] = all_data.Family_size ** 2\nall_data['Parch_sq2'] = all_data.Parch ** 4\nall_data['SibSp_sq2'] = all_data.SibSp ** 4\nall_data['Family_size_sq2'] = all_data.Family_size ** 4\nall_data['Family_size_sq2'] = all_data.Family_size ** 10","859bc56f":"all_data = all_data.join(pd.get_dummies(all_data[['Sex', 'Embarked', 'Cabin', 'Ticket']]))\nall_data = all_data.drop(['Sex','Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)","9320bccc":"all_data['Age_cat']=0\nall_data.loc[all_data['Age']<=16,'Age_cat']=0\nall_data.loc[(all_data['Age']>16)&(all_data['Age']<=32),'Age_cat']=1\nall_data.loc[(all_data['Age']>32)&(all_data['Age']<=48),'Age_cat']=2\nall_data.loc[(all_data['Age']>48)&(all_data['Age']<=64),'Age_cat']=3\nall_data.loc[all_data['Age']>64,'Age_cat']=4\nall_data = all_data.drop(['Age'], axis=1)\n","41e78d2a":"all_data['Fare_cat']=0\nall_data.loc[all_data['Fare']<=7.775,'Fare_cat']=0\nall_data.loc[(all_data['Fare']>7.775)&(all_data['Fare']<=8.662),'Fare_cat']=1\nall_data.loc[(all_data['Fare']>8.662)&(all_data['Fare']<=14.454),'Fare_cat']=2\nall_data.loc[(all_data['Fare']>14.454)&(all_data['Fare']<=26.0),'Fare_cat']=3\nall_data.loc[(all_data['Fare']>26.0)&(all_data['Fare']<=52.369),'Fare_cat']=4\nall_data.loc[all_data['Fare']>52.369,'Fare_cat']=5\nall_data = all_data.drop(['Fare'], axis=1)","a7178f44":"new_train = all_data.iloc[:891]\nnew_test = all_data.iloc[891:]","f507d378":"feature_columns = list(new_train.columns)\nfeature_columns.remove('Survived')\nfeature_columns.remove('PassengerId')","d55cf8bd":"X = new_train[feature_columns]\ny = new_train.Survived\n# split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size= 0.2, random_state=1)","233055bd":"model_rf = RandomForestClassifier(criterion='entropy', max_leaf_nodes=100, random_state=1)\nmodel_rf = BaggingClassifier(base_estimator=model_rf, random_state=1)\nmodel_rf.fit(train_X, train_y)\npredictions = model_rf.predict(val_X)\naccuracy = accuracy_score(predictions, val_y) * 100\nprint(\"Accuracy:\", accuracy)","bc7f9fcb":"model_dt = DecisionTreeClassifier(criterion='gini', max_leaf_nodes=100, random_state=1)\nmodel_dt = BaggingClassifier(base_estimator=model_dt, random_state=1)\nmodel_dt.fit(train_X, train_y)\npredictions = model_dt.predict(val_X)\naccuracy = accuracy_score(predictions, val_y) * 100\nprint(\"Accuracy:\", accuracy)","03e5d67c":"model_xgb = XGBClassifier(objective='binary:logistic', eta=0.01, max_depth=50, gamma=1)\n#model_xgb = GridSearchCV(model_xgb,{'max_depth': [2, 4, 6], 'eta': [0.01, 0.1, 0.2]}, verbose=1)\nmodel_xgb.fit(train_X, train_y)\npredictions = model_xgb.predict(val_X)\naccuracy = accuracy_score(predictions, val_y) * 100\nprint(\"Accuracy:\", accuracy)","478f7cf0":"model_svc = SVC(C=10)\nmodel_svc.fit(train_X, train_y)\npredictions = model_svc.predict(val_X)\naccuracy = accuracy_score(predictions, val_y) * 100\nprint(\"Accuracy:\", accuracy)","4a81ae6b":"model_nb = GaussianNB()\nmodel_nb.fit(train_X, train_y)\npredictions = model_nb.predict(val_X)\naccuracy = accuracy_score(predictions, val_y) * 100\nprint(\"Accuracy:\", accuracy)","5e0f983d":"# retrain model on all training data\nmodel_rf.fit(X, y)\nmodel_dt.fit(X, y)\nmodel_xgb.fit(X, y)\nmodel_svc.fit(X, y)\nmodel_nb.fit(X, y)","69bb8ca8":"# submit results\npredictions1 = model_rf.predict(new_test[feature_columns])\npredictions2 = model_dt.predict(new_test[feature_columns])\npredictions3 = model_xgb.predict(new_test[feature_columns])\npredictions4 = model_svc.predict(new_test[feature_columns])\npredictions = (predictions1 + predictions2 + predictions3 + predictions4) \/ 4\npredictions = [int(round(x)) for x in predictions]\n\nsubmission = pd.DataFrame({'PassengerId':new_test['PassengerId'],'Survived':predictions})\n\n#Visualize the first 20 rows\nsubmission.head(20)","2f0ebe8e":"filename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","6ad593a1":"## 3- Pclass","08ec532a":"# Exploratory Data Analysis & Data Cleaning","6c272cc2":"### Encode categorical features","8286b42e":"### New features","44e99b63":"### 4- SVM","679827d4":"## 7- Fare","d011b96b":"## 1- Age ","ba9bddb3":"## 4- Embarked","b8b8a8a7":"### 5- Naive Bayes","43203465":"## 2- Sex","a73ad01a":"### 2- Decision Trees","ed872ace":"## 5- Parch","c2781232":"The majority of passengers embarked from Southampton so we can fill the missing values with S","0849c150":"### Binning","1a55fa6c":"# Feature Engineering","ef4e998a":"### 1- Random Forest","6bbbcd08":"## 6- SibSp","2e34b114":"### 3- XGBoost","0705dec0":"# Predictive Modeling "}}