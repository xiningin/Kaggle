{"cell_type":{"6fe6a75a":"code","d27364c8":"code","db3f13a2":"code","3e6355d2":"code","07cf05c0":"code","2d0a1fe8":"code","c8673e35":"code","8de88d4e":"code","26f53374":"code","a96f5d98":"code","a2967ac8":"code","02c20204":"code","6001be86":"code","4f6647aa":"code","856339b7":"code","bed3d44d":"markdown","55815bfe":"markdown","d5fe7b37":"markdown","ce3d8984":"markdown","27f223c5":"markdown"},"source":{"6fe6a75a":"import os\nimport sys\nimport cv2\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\n%matplotlib inline","d27364c8":"root_dir = '..\/input\/hpa-single-cell-image-classification'\ntrain_df = pd.read_csv(root_dir+'\/train.csv')\ntrain_df.head()","db3f13a2":"train_df.shape","3e6355d2":"label_dict = {\n    0: 'Nucleoplasm',\n    1: 'Nuclear membrane',\n    2: 'Nucleoli',\n    3: 'Nucleoli fibrillar center',\n    4: 'Nuclear speckles',\n    5: 'Nuclear bodies',\n    6: 'Endoplasmic reticulum',\n    7: 'Golgi apparatus',\n    8: 'Intermediate filaments',\n    9: 'Actin filaments',\n    10: 'Microtubules',\n    11: 'Mitotic spindle',\n    12: 'Centrosome',\n    13: 'Plasma membrane',\n    14: 'Mitochondria',\n    15: 'Aggresome',\n    16: 'Cytosol',\n    17: 'Vesicles and punctate cytosolic patterns',\n    18: 'Negative' \n}\n\nreverse_labels = {y:x for x,y in label_dict.items()}","07cf05c0":"ohe_labels = train_df['Label'].str.get_dummies(sep='|')\nohe_labels = ohe_labels.rename(columns={str(x): y for x,y in label_dict.items()})\nohe_labels = pd.concat([train_df['ID'], ohe_labels], axis=1)\n#ohe_labels = train_df['ID'].append(ohe_labels)\nohe_labels.head()","2d0a1fe8":"fig = px.bar(ohe_labels.drop('ID', axis=1).sum())\nfig.show()","c8673e35":"sample_img = train_df.sample(1)\nsample_img, sample_label = sample_img.values[0]","8de88d4e":"sample_img_r = cv2.imread(root_dir+'\/train\/'+sample_img+'_red.png', cv2.IMREAD_GRAYSCALE)\nsample_img_g = cv2.imread(root_dir+'\/train\/'+sample_img+'_green.png', cv2.IMREAD_GRAYSCALE)\nsample_img_b = cv2.imread(root_dir+'\/train\/'+sample_img+'_blue.png', cv2.IMREAD_GRAYSCALE)\nsample_img_y = cv2.imread(root_dir+'\/train\/'+sample_img+'_yellow.png', cv2.IMREAD_GRAYSCALE)\n\nfig, ax = plt.subplots(2, 2, figsize=(10,10))\n\nax[0][0].imshow(sample_img_r, cmap='gray')\nax[0][0].set_title('red');\nax[1][0].imshow(sample_img_g, cmap='gray')\nax[1][0].set_title('green');\nax[0][1].imshow(sample_img_b, cmap='gray')\nax[0][1].set_title('blue');\nax[1][1].imshow(sample_img_y, cmap='gray')\nax[1][1].set_title('yellow');\nfig.suptitle(', '.join([label_dict[int(x)] for x in sample_label.split('|')]));","26f53374":"# for fun's sake :)\nfig, ax = plt.subplots(2, 2, figsize=(10,10))\n\nax[0][0].imshow(np.stack([sample_img_r, sample_img_g, sample_img_b], axis=-1))\nax[1][0].imshow(np.stack([sample_img_r, sample_img_g, sample_img_y], axis=-1))\nax[0][1].imshow(np.stack([sample_img_r, sample_img_y, sample_img_b], axis=-1))\nax[1][1].imshow(np.stack([sample_img_y, sample_img_b, sample_img_r], axis=-1))","a96f5d98":"def correlate(arr1, arr2):\n    product = np.mean((arr1 - arr1.mean()) * (arr2 - arr2.mean()))\n    stds = arr1.std() * arr2.std()\n    if stds == 0:\n        return 0\n    else:\n        product \/= stds\n        return product","a2967ac8":"channels = {'red': sample_img_r,\n           'green': sample_img_g,\n           'blue': sample_img_b,\n           'yellow': sample_img_y}\n\nfor i, j in itertools.combinations(list(channels.keys()), 2):\n    print(f'Correlation {i} - {j}: {correlate(channels[i], channels[j])}')","02c20204":"for ch in list(channels.keys()):\n    print(f'{ch}: \\n min: {channels[ch].min()}, max: {channels[ch].max()}, \\\n    mean: {channels[ch].mean()}, median: {np.median(channels[ch])}, std: {channels[ch].std()}')","6001be86":"def plot_channel(channel, thresh=10, blur_kernel=5, draw_box=False):\n    channel_mask = np.zeros(channels[channel].shape)\n    if blur_kernel>0:\n        img = cv2.medianBlur(channels[channel], blur_kernel)\n    else:\n        img = channels[channel]\n    _, thresh_img = cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if draw_box is False:\n        channel_mask = cv2.drawContours(mask, contours, -1, 255, 2)\n    else:\n        for cont in contours:\n            px ,py, w, h = cv2.boundingRect(cont)\n            channel_mask = cv2.drawContours(channel_mask, [cont], 0, 255, 2)\n            channel_mask = cv2.rectangle(channel_mask, (px ,py), (px+w, py+h), 255, 2)\n\n    fig, ax = plt.subplots(1, 2, figsize=(8,11))\n\n    ax[0].imshow(channels[channel], cmap='gray')\n    ax[0].set_title(f'{channel} image')\n    ax[1].imshow(channel_mask, cmap='gray')\n    ax[1].set_title(f'{channel} mask ({len(contours)} contours)')\n    \nplot_channel('red', draw_box=True)\nplot_channel('green', draw_box=True)\nplot_channel('blue', draw_box=True)\nplot_channel('yellow', draw_box=True)","4f6647aa":"#fig, ax = plt.subplots()\npx.bar(ohe_labels.sum(axis=1).value_counts())","856339b7":"data = []\ncols = ohe_labels.drop('ID', axis=1).columns\nfig = go.Figure(data=[\n    go.Bar(name='Frequency of labels', x=cols, \n           y=[ohe_labels[x].sum() for x in cols],\n          marker_color='limegreen')\n])\n\nfig.show()","bed3d44d":"From previous competition <a href='https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification'>Human Protein Atlas Image Classification<\/a>\n> All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references.","55815bfe":"Common sence tells us we should locate the same number of nuclei and cells. Competition's data description confirms we should pay particular attention to blue channel if we want to locate precisely a cell. Just to see how we can approach this task, let's see if we can find contours of those and compare to other channels.","d5fe7b37":"H0 #1: Green channel alone carries significant predictive power.<br>\nH0 #2: there is some correlation between yellow-green and red-blue channels","ce3d8984":"Easier said than done, huh? As for blue channel luckely there is not particular issue of false positives but merging contours is.","27f223c5":"Let's turn the gaze into some statictics of labels co-occurence and might be we will discover something interesting along the way."}}