{"cell_type":{"c22526f6":"code","cd9951ec":"code","2e2a3d7d":"code","bd5748e8":"code","6f61a42d":"code","40fed728":"code","a3cb0a67":"code","2e971737":"code","0aeb6972":"code","863d72e9":"code","70f0871d":"code","24e69b0c":"code","4cd98287":"code","33f96516":"code","eda460a4":"code","4abd0921":"code","f8404d7e":"code","17afade6":"code","11aee625":"markdown","6cc13107":"markdown","be1612f8":"markdown","190e7df5":"markdown","20e020ec":"markdown","ae4ba74b":"markdown","0b436d2f":"markdown","856854fe":"markdown","34b0ef3f":"markdown","9c221c8b":"markdown","73ce31d7":"markdown","1fc4d858":"markdown"},"source":{"c22526f6":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nimport imblearn\n\nfrom pathlib import Path\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.utils import resample\n\nfrom xgboost import XGBClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\n\n\n\n\n\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\n\n# data explaination link https:\/\/www.kaggle.com\/ruthgn\/bank-marketing-data-set\n# https:\/\/www.kaggle.com\/ruthgn\/client-accept-decline-classifier-eda-feateng-more\/notebook","cd9951ec":"def get_data():\n    df = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bank_marketing\/bank-direct-marketing-campaigns.csv')\n# kaggle data\n#     df = pd.read_csv('..\/input\/bank-marketing-data-set\/bank-direct-marketing-campaigns.csv')\n    \n    \n    df.columns =['age', 'job', 'marital_status', 'education', 'credit_default', 'home_loan', 'personal_loan',\n       'contact_type', 'month_contacted', 'day_of_week_contaced', 'campaign_num_contacts', 'days_passed', 'previous_camp_contacts',\n       'previous_outcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n       'euribor3m', 'nr.employed', 'y']\n    \n    return df\n","2e2a3d7d":"df = get_data()","bd5748e8":"to_check = ['age', 'job', 'marital_status', 'education', 'credit_default', 'home_loan', 'personal_loan',\n       'contact_type', 'month_contacted', 'day_of_week_contaced', 'campaign_num_contacts', 'days_passed', 'previous_camp_contacts', 'previous_outcome', 'y']\n\nfor i in to_check:\n    print(i, '\\n', df[i].value_counts(),'\\n', '\\n')\n","6f61a42d":"def clean_please(df):\n    # creating new column with details of the items we are replacing\n    df['not_contacted'] = df['days_passed'] == 999\n    df['not_contacted'] = df['not_contacted']*1\n    \n    # replacing 999 with the mode of the column\n    df['days_passed'] = df['days_passed'].replace(999, np.nan)\n    df['days_passed'] = df['days_passed'].fillna(int(df['days_passed'].mode()))\n    \n    return df\n    ","40fed728":"nominal = ['job', 'marital_status', 'credit_default', 'home_loan', 'personal_loan','contact_type','education', 'month_contacted', 'day_of_week_contaced','previous_outcome']","a3cb0a67":"def one_hot(df):\n    \n    # Split data for one-hot-encoding\n    X = df.drop('y', axis=1)\n    y = df['y']\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=44)\n\n    # Apply one-hot encoder to each feature column with categorical data\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)\n    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[nominal]))\n    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[nominal]))\n\n    # One-hot encoding removed index, let's put it back\n    OH_cols_train.index = X_train.index\n    OH_cols_valid.index = X_valid.index\n\n    # Get appropriate column names for new one-hot encoding\n    OH_cols_train.columns = OH_encoder.get_feature_names_out(nominal)\n    OH_cols_valid.columns = OH_encoder.get_feature_names_out(nominal)\n\n    # Remove old categorical columns (to replace with one-hot encoding)\n    num_X_train = X_train.drop(nominal, axis=1)\n    num_X_valid = X_valid.drop(nominal, axis=1)\n\n    # Add one-hot encoded columns to numerical features\n    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n    # Merge the splits back\n    X = pd.concat([OH_X_train, OH_X_valid])\n    y = pd.concat([y_train, y_valid])\n\n    # Convert `y` to boolean values\n    y = y.map(dict(yes=1, no=0))\n \n    # Join `X` and `y` back into one\n    df = X.join(y)\n\n    return df","2e971737":"# running the function to get the encoded data\ndf = one_hot(df)","0aeb6972":"def test_train(df):\n    \n    X = df.copy()\n    y = X.pop('y')\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)\n    \n    df_train = X_train.join(y_train)\n    df_test = X_test.join(y_test)\n\n    return X_train, X_test, y_train, y_test","863d72e9":"# running the function to get the train and test data\nX_train, X_test, y_train, y_test = test_train(df)","70f0871d":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n        print(\"\")\n        print(\"\")\n    else:\n        print('Confusion matrix, without normalization')\n        print(\"\")\n        print(\"\")\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual Value')\n    plt.xlabel('Predicted Value')","24e69b0c":"def svm_please(X_train, X_test, y_train, y_test):\n\n    clf = SVC(kernel='linear')\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred,labels=[0,1])\n    print('ROC AUC score:', roc_auc_score(y_test, y_pred))\n    print(classification_report (y_test, y_pred))\n    \n    plot_confusion_matrix(cm, classes =['no', 'yes'],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n    \n    \n    ","4cd98287":"df = get_data()\ndf = clean_please(df)\ndf = one_hot(df)\nX_train, X_test, y_train, y_test = test_train(df)\n\nsvm_please(X_train, X_test, y_train, y_test)\n","33f96516":"def xgbc_run_please(X_train, X_test, y_train, y_test\n                    \n    xgbc = XGBClassifier()\n    print(xgbc)\n    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n           colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n           max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n           n_estimators=100, n_jobs=1, nthread=None,\n           objective='multi:softprob', random_state=0, reg_alpha=0,\n           reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n           subsample=1, verbosity=1) \n\n    xgbc.fit(X_train, y_train)\n    scores = cross_val_score(xgbc, X_train, y_train, cv=5)\n    print(\"Mean cross-validation score: %.2f\" % scores.mean())\n\n    kfold = KFold(n_splits=10, shuffle=True)\n    kf_cv_scores = cross_val_score(xgbc, X_train, y_train, cv=kfold )\n    print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n\n    y_pred = xgbc.predict(X_test)\n    \n    cm = confusion_matrix(y_test, y_pred,labels=[0,1])\n    print('ROC AUC score:', roc_auc_score(y_test, y_pred))\n    print(classification_report (y_test, y_pred))\n    \n    plot_confusion_matrix(cm, classes =['no', 'yes'],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)","eda460a4":"# running the XGBoost classifier\n\ndf = get_data()\ndf = clean_please(df)\ndf = one_hot(df)\nX_train, X_test, y_train, y_test = test_train(df)\nxgbc_run_please(X_train, X_test, y_train, y_test)\n\n","4abd0921":"# Separate majority and minority classes\n\ndef resample_please(df, down_samples, up_samples):\n    \n    df_majority = df.loc[df['y'] == 0]\n    df_minority = df.loc[df['y'] == 1]\n\n    # Downsample majority class\n    df_majority_downsampled = resample(df_majority, \n                                     replace=False,    \n                                     n_samples= down_samples)\n    #Upsample minority class\n    df_minority_upsampled = resample(df_minority, \n                                     replace=True,     \n                                     n_samples= up_samples)\n\n    # Combine minority class with downsampled majority class\n    df = pd.concat([df_majority_downsampled, df_minority_upsampled])\n    \n    print(df.shape)\n    print(df.y.value_counts())\n    return df","f8404d7e":"df = get_data()\ndf = clean_please(df)\ndf = one_hot(df)\ndf = resample_please(df, down_samples = 10000, up_samples = None)\n\nX_train, X_test, y_train, y_test = test_train(df)\nsvm_please(X_train, X_test, y_train, y_test)\n","17afade6":"df.columns","11aee625":"Let's decide which types of categorical variables we have, for now lets treat them all as normative variables, maybe we go back and improve the score by categorising ordinal vars","6cc13107":"# Feature Engineering","be1612f8":"plan\n1. look at data\n2. clean data\n3. baseline model\n4. feature engineering\n4. rebalancing (from imblearn.under_sampling import RandomUnderSampler and oversampling)\n5. tuning\n6. explaining","190e7df5":"Resampling doesnt seem to help","20e020ec":"Checking the data to see if it looks reasonable\n\nto do based on below\n1. days_passed = 999 do smth, replace with mode and note them in an additional column\n2. get rid of non existant outcomes","ae4ba74b":"Let's prepare to get a baseline model","0b436d2f":"One Hot Encoding","856854fe":"Running the work so far to get baseline model, including the SVM model","34b0ef3f":"Confusion Matrix code","9c221c8b":"## I will try an SVM model for the baseline","73ce31d7":"## XGBoost Classifier Baseline","1fc4d858":"## Let's balance the dataset\n\nLets downsample the 0's so it matches the 1's"}}