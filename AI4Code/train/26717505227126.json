{"cell_type":{"170723a6":"code","2a2ed976":"code","1c707a68":"code","a9d255ec":"code","029576c2":"code","4e098e9c":"code","b290264d":"code","02804bc4":"code","d4953f08":"code","a4cc325e":"code","96d14850":"code","afe6af62":"code","e603138f":"code","e4a2d7a1":"code","48a63223":"code","bc53f584":"code","24599a9f":"code","093eab12":"code","9c284d98":"code","d737f739":"code","1f680632":"code","4ed5faea":"code","48be030c":"code","bdf52af7":"markdown","e349e5f0":"markdown","f283ded4":"markdown","d821abfd":"markdown","c0de3d23":"markdown","a7f873c8":"markdown","3f99cbb7":"markdown","2c4b987c":"markdown","a48873e2":"markdown","e6254e61":"markdown","a839a992":"markdown"},"source":{"170723a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2a2ed976":"# laod data\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ntrain_df.head()","1c707a68":"# set index\ntrain_df.set_index('Id', inplace=True)\ntest_df.set_index('Id', inplace=True)\nlen_train_df = len(train_df)\nlen_test_df = len(test_df)","a9d255ec":"corrmat = train_df.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>=0.3]\nplt.figure(figsize=(13,10))\ng = sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","029576c2":"# split y_label\ntrain_y_label = train_df['SalePrice']\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)","4e098e9c":"# concat train & test\nboston_df = pd.concat((train_df, test_df), axis=0)\nboston_df_index = boston_df.index\nprint(len(boston_df))\nboston_df.head()","b290264d":"# check null \ncheck_null = boston_df.isna().sum() \/ len(boston_df)\n\n# columns of null ratio >= 0.5\ncheck_null[check_null >= 0.5]","02804bc4":"# remove columns of null ratio >= 0.5\nremove_cols = check_null[check_null >= 0.5].keys()\nboston_df = boston_df.drop(remove_cols, axis=1)\n\nboston_df.head()","d4953f08":"# split object & numeric\nboston_obj_df = boston_df.select_dtypes(include='object')\nboston_num_df = boston_df.select_dtypes(exclude='object')","a4cc325e":"print('Object type columns:\\n',boston_obj_df.columns)\nprint('Numeric type columns:\\n',boston_num_df.columns)","96d14850":"boston_dummy_df = pd.get_dummies(boston_obj_df, drop_first=True)\nboston_dummy_df.index = boston_df_index\nboston_dummy_df.head()","afe6af62":"from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='mean')\nimputer.fit(boston_num_df)\nboston_num_df_ = imputer.transform(boston_num_df)","e603138f":"boston_num_df = pd.DataFrame(boston_num_df_, columns=boston_num_df.columns, index=boston_df_index)\nboston_num_df.head()","e4a2d7a1":"boston_df = pd.merge(boston_dummy_df, boston_num_df, left_index=True, right_index=True)\nprint(len(boston_df))\nboston_df.head()","48a63223":"train_df = boston_df[:len_train_df]\ntest_df = boston_df[len_train_df:]\nprint('train set length: ',len(train_df))\nprint('test set length: ',len(test_df))","bc53f584":"train_df['SalePrice'] = train_y_label","24599a9f":"from sklearn.model_selection import train_test_split\nX_train = train_df.drop(['SalePrice'], axis=1)\ny_train = train_df['SalePrice']\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n\nX_test = test_df\ntest_id_idx = test_df.index","093eab12":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nparam = {\n    'max_depth':[3,4,5],\n    'n_estimators':[250,300,330],\n    'colsample_bytree':[0.3,0.5,0.7],\n    'colsample_bylevel':[0.3,0.5,0.7],\n    'gamma':[0.1,0,4,0.7]\n}\nmodel = xgb.XGBRegressor()\n\nxgb_grid = GridSearchCV(estimator=model, param_grid=param, cv=5, \n                           scoring='neg_mean_squared_error',\n                           n_jobs=-1)\n\n\nxgb_grid.fit(X_train, y_train)\nprint(xgb_grid.best_params_)\nprint(xgb_grid.best_estimator_)","9c284d98":"xgb_grid.score(X_train, y_train)","d737f739":"test_y_pred = xgb_grid.predict(X_test)","1f680632":"id_pred_df = pd.DataFrame()\nid_pred_df['Id'] = test_id_idx\nid_pred_df['SalePrice'] = test_y_pred","4ed5faea":"id_pred_df.to_csv('submission.csv', index=False)","48be030c":"id_pred_df","bdf52af7":"## Check Object & Numeric variables","e349e5f0":"## Load Data","f283ded4":"## Change object type data to dummy variables","d821abfd":"## Training by XGBRegressor","c0de3d23":"## Merge numeric_df & dummies_df","a7f873c8":"## Check NaN ratio and Remove null ratio >= 0.5","3f99cbb7":"## model score of train set","2c4b987c":"## Split train & test set","a48873e2":"## Variables Corrleation >= 0.3","e6254e61":"## Predict test set","a839a992":"## Impute NaN of numeric data to 'mean'"}}