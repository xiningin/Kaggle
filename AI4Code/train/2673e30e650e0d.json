{"cell_type":{"5da1d1ef":"code","7b5e6eae":"code","f679d1d0":"code","345de0ae":"code","46e5f39c":"code","975f4645":"code","47ffc231":"code","948cd9f1":"code","014ba969":"code","6c09c2c0":"code","6ccfc3a8":"code","fcbfe1a9":"code","b7389c26":"code","8d92c0a0":"code","bb507469":"code","afd58017":"code","27a08ecf":"code","3fdaea4b":"code","4b4fd18a":"code","94dbd43f":"code","45465304":"code","3719e593":"code","2d8126c2":"code","54d55784":"code","48299434":"code","490a64a8":"code","85b7558c":"code","d3932955":"code","f1f5edce":"code","5abd00b3":"code","c09d1bdc":"code","4704f9d3":"code","8fca9c6e":"markdown","b6c4e712":"markdown","af7e8beb":"markdown","1f5a55c0":"markdown","545764e9":"markdown","0a707446":"markdown","cf01a6d9":"markdown","9e067846":"markdown","e328a35c":"markdown","32532ad8":"markdown","9394b303":"markdown","739b1292":"markdown","d218d0ab":"markdown","b560c5a6":"markdown","180c0565":"markdown","84db094f":"markdown","ef1db9a6":"markdown","9a826c98":"markdown","b9cc4dd9":"markdown","c73aa417":"markdown","e1d2ffb2":"markdown","c2a9f4a7":"markdown","ad62ca9d":"markdown","5dd833cf":"markdown","7db64c31":"markdown","cca11d38":"markdown","c1a7da4a":"markdown","4650f306":"markdown"},"source":{"5da1d1ef":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE","7b5e6eae":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","f679d1d0":"df.head()","345de0ae":"df.shape","46e5f39c":"df.dtypes","975f4645":"df.isnull().values.any()","47ffc231":"df['Class'].unique()","948cd9f1":"df.describe()","014ba969":"scale=StandardScaler()\ndf[['Amount','Time']]=scale.fit_transform(np.array(df[['Amount','Time']]))","6c09c2c0":"df.hist(figsize=(25,25))\nplt.show()","6ccfc3a8":"corrmat=df.corr()\nfig=plt.figure(figsize=(12,12))\nsns.heatmap(corrmat,vmax=1.0,square=True,linewidths=0.1)\nplt.show()","fcbfe1a9":"col=df.columns.tolist()","b7389c26":"x=df.drop(columns='Class') #input featuures\ny=df['Class']  #target or output feature","8d92c0a0":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=5)\nprint('xtrain: {0}, ytrain: {1}'.format(xtrain.shape,ytrain.shape))\nprint('xtest: {0}, ytest: {1}'.format(xtest.shape,ytest.shape))","bb507469":"fraud=(ytrain==1).sum()\nvalid=(ytrain==0).sum()\nprint('No. of fraud cases:',fraud)\nprint('No. of valid cases:',valid)","afd58017":"resample=SMOTE(random_state=5)\nx_train,y_train=resample.fit_sample(xtrain,ytrain)","27a08ecf":"print(x_train.shape,y_train.shape)","3fdaea4b":"re_fraud=(y_train==1).sum()\nre_valid=(y_train==0).sum()\nprint('No. of fraud cases after resampling:',re_fraud)\nprint('No. of valid cases after resampling:',re_valid)","4b4fd18a":"import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\nfrom mlxtend.plotting import plot_confusion_matrix","94dbd43f":"epoch=10\nact_func='relu'\nlr=0.0001\ndropout_rate=0.5\nbatchsize=32\nloss_func='binary_crossentropy'","45465304":"model=Sequential()\nmodel.add(Dense(60,input_shape=(30,),activation=act_func))\nmodel.add(Dropout(0.5,seed=0))\nmodel.add(Dense(35,activation=act_func))\nmodel.add(Dropout(0.5,seed=0))\nmodel.add(Dense(1,activation='sigmoid'))","3719e593":"model.summary()","2d8126c2":"opt=Adam(learning_rate=lr)","54d55784":"model.compile(optimizer=opt, loss=loss_func, metrics=['accuracy'] )","48299434":"history=model.fit(x_train, y_train, batch_size=batchsize, epochs=epoch, validation_split=0.2, shuffle=True)","490a64a8":"print(history.history.keys()) ","85b7558c":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Model accuracy (Training vs Validation)')\nplt.legend(['train', 'validation'],loc='lower right')\nplt.show()","d3932955":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.title('Model loss (Training vs Validation)')\nplt.legend(['train', 'validation'],loc='upper right')\nplt.show()","f1f5edce":"loss, acc=model.evaluate(xtest,ytest,batch_size=batchsize)","5abd00b3":"print('Loss: ',loss)\nprint('Accuracy: ',acc)","c09d1bdc":"ypred=model.predict_classes(xtest)\nlabel=['valid','fraud']\ncon_mat=confusion_matrix(ytest,ypred)\nplt.figure()\ncm=plot_confusion_matrix(con_mat, class_names=['Valid','Fraud'],colorbar=True)\nax = plt.gca()\nax.set_ylim([1.4,-0.4])\nplt.show()","4704f9d3":"model.save('Fraud_detection.h5')","8fca9c6e":"The standardized data of all the features can be visualized below:","b6c4e712":"##### To check for any missing values","af7e8beb":"<a id=\"predict\"><\/a>\n### 6. Prediction","1f5a55c0":"From the confusion matrix, we can see that the model has predicted 85195 valid cases and 126 fraud cases correctly. The number of false predictions are very small. ","545764e9":"#### Compile model","0a707446":"The dataset has 31 features and 284807 samples","cf01a6d9":"From the above table, we can infer that time and amount features have high range of values as compared to other feartures. So these features are standardized around their mean using StandardScaler().","9e067846":"#### Training Loss vs Validation Loss","e328a35c":"<a id=\"data\"><\/a>\n### 2. Data Preparation","32532ad8":"#### Download and unzip the dataset","9394b303":"<a id=\"intro\"><\/a>\n### 1. Introduction","739b1292":"# Credit Card Fraud Detection\n\n## Contents\n\n### [1. Introduction](#intro)\n\n### [2. Data Preparation](#data)\n   * **Import the required libraries**\n   * **Download and unzip the dataset**\n   * **Split the dataset**\n   \n### [3. Exploratory Analysis](#explore)\n\n### [4. Model Architecture](#cnn)\n   * **Set hyperparameters**\n   * **Define the model**\n   * **Set optimizer** \n   * **Compile model**\n   * **Train model**\n\n### [5. Model Evaluation](#eval)\n   * **Training Accuracy vs Validation Accuracy**\n   * **Training Loss vs Validation Loss**\n   * **Model Accuracy**\n   * **Observations**\n\n### [6. Prediction](#predict)\n\n### [7. Save Model to Disk](#save)\n  ","d218d0ab":"#### Split the dataset","b560c5a6":"<a id=\"save\"><\/a>\n### 7. Save Model to Disk","180c0565":"<a id=\"cnn\"><\/a>\n### 4. Model Architecture","84db094f":"#### Set optimizer ","ef1db9a6":"#### About the dataset\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n#### Problem statement\nClassify the transactions as **fraud (1)** and **legitimate (0)**.\n\n#### Dataset link: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\/","9a826c98":"<a id=\"eval\"><\/a>\n### 5. Model Evaluation","b9cc4dd9":"#### Train model","c73aa417":"#### Import the required libraries","e1d2ffb2":"Now there are equal number of fraud(1) and valid(0) classes in the dataset","c2a9f4a7":"#### Set hyperparameters","ad62ca9d":"#### Model Accuracy","5dd833cf":"#### Observations:\nIn the model accuracy and model loss plots of the trained data, we can see that the model is optimized with a validation accuracy of 100% and a minimal loss of 0.005.\nAlso the test accuracy is 99.8% with a loss of 0.026. ","7db64c31":"#### Define the model","cca11d38":"#### Training Accuracy vs Validation Accuracy","c1a7da4a":"This dataset is highly imbalanced as the classes are biased. This is solved by applying SMOTE oversampling technique to balance the classes. ","4650f306":"<a id=\"explore\"><\/a>\n### 3. Exploratory Analysis"}}