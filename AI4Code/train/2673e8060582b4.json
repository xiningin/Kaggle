{"cell_type":{"01756d76":"code","eed2a90f":"code","e306efe5":"code","8ca8ac95":"code","e6db4df1":"code","7c7ad523":"code","08810b9b":"code","244373d6":"code","0781c607":"code","8876e1cb":"code","1fe0ebf8":"code","9366a016":"code","f909f50a":"code","0dbd6586":"code","3e5461c7":"code","54e505e1":"code","8464f645":"code","fdbc67d7":"code","aa0f0478":"code","2b02c0d7":"code","c8f1e8ac":"code","07adf4a6":"code","f8f5d66b":"code","45dc4d96":"code","44916f70":"code","f248caf6":"code","6b76f405":"code","0ea37ccf":"code","c82c5913":"code","2266fa46":"code","4550db77":"code","fa215688":"code","3b0b1580":"markdown","099e3531":"markdown","f935fb79":"markdown","790c001c":"markdown","718a525e":"markdown","d3fe6c13":"markdown","2c67b89a":"markdown","2f2a86fd":"markdown","a8d97424":"markdown","478d8dab":"markdown","fd011a91":"markdown","539d9c55":"markdown","a225aa23":"markdown","e19656f3":"markdown","8f176a14":"markdown","f7d1d807":"markdown","e0726cde":"markdown","824f8e6a":"markdown","5a07cf9f":"markdown","2618566e":"markdown","a5ed921b":"markdown","d5ebdc6c":"markdown","cf69f594":"markdown","e55ad29d":"markdown","dff64372":"markdown","979f1576":"markdown"},"source":{"01756d76":"# Python <stdio.h>\nimport warnings\nimport numpy as np\nimport pandas as pd\n# Data Visualization\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Prediction\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\n# Clustering\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n\n# Erase Warnings\nwarnings.filterwarnings(\"ignore\")","eed2a90f":"# Import all the data\ndata = pd.read_csv('..\/input\/top50spotify2019\/top50.csv',encoding = \"ISO-8859-1\");\n# Delete Extra column\ndata.drop(['Unnamed: 0'], axis=1, inplace=True)","e306efe5":"# Define predictors set\nx = data[['Beats.Per.Minute', 'Energy', 'Danceability', 'Loudness..dB..', 'Liveness', 'Valence.', 'Length.', 'Acousticness..', 'Speechiness.']]\n# Define target variable\ny = data[['Popularity']]\n# Define info variables\ninfo = data[['Track.Name', 'Artist.Name', 'Genre']]","8ca8ac95":"# Spearman correlation calc\nsc = pd.concat([x,y], axis=1).corr(method='spearman')","e6db4df1":"# Generate a mask for the upper triangle\ntriangle_mask = np.zeros_like(sc, dtype=np.bool)\ntriangle_mask[np.triu_indices_from(triangle_mask)] = True\n\n# Plot\nplt.figure(figsize = (25,10))\nsns.heatmap(data = sc, linewidths=.1, linecolor='black', vmin = -1, vmax = 1, mask = triangle_mask, annot = True,\n            cbar_kws={\"ticks\":[-1,-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8,1]});\nplt.yticks(rotation=45);","7c7ad523":"plt.figure(figsize = (25,10));\nsns.countplot(x=\"Genre\", data=info);\nplt.ylabel('Songs Count');\nplt.xticks(rotation=45, ha='right');","08810b9b":"for i in range(0,len(info)):\n    if info.loc[i,'Genre'] in ['canadian pop','dance pop','pop','panamanian pop','australian pop','boy band']:\n        info.loc[i,'Genre2'] = 'Pop'\n    elif info.loc[i,'Genre'] in ['dfw rap','country rap']:\n        info.loc[i,'Genre2'] = 'Rap'\n    elif info.loc[i,'Genre'] in ['reggaeton flow','reggaeton']:\n        info.loc[i,'Genre2'] = 'Reggaeton'\n    elif info.loc[i,'Genre'] in ['canadian hip hop','atl hip hop']:\n        info.loc[i,'Genre2'] = 'Hip hop'\n    elif info.loc[i,'Genre'] in ['trap music','pop house','edm','big room','electropop','brostep']:\n        info.loc[i,'Genre2'] = 'Eletronic'    \n    elif info.loc[i,'Genre'] in ['latin','r&b en espanol']:\n        info.loc[i,'Genre2'] = 'Latin'\n    elif info.loc[i,'Genre'] in ['escape room']:\n        info.loc[i,'Genre2'] = 'Escape room'        ","244373d6":"plt.figure(figsize = (25,10));\nsns.countplot(x=\"Genre2\", data=info);\nplt.ylabel('Songs Count');\nplt.xlabel('New Genres');","0781c607":"# Create aux dataframe for plot\ndf = pd.concat([info,y],axis=1)","8876e1cb":"# Plot\nplt.figure(figsize = (25,10));\nsns.barplot(data=df,x='Genre2',y='Popularity');\nplt.xticks(rotation=45, ha='right');","1fe0ebf8":"# Plot\nplt.figure(figsize = (25,10));\nsns.barplot(data=df,x='Genre',y='Popularity');\nplt.xticks(rotation=45, ha='right');","9366a016":"# Defining some variables\nmetricsRFR = []       # Prediction Scores for Random Forest Regressor\nmetricsSVR = []       # Prediction Scores for Support Vector Regressor\nmetricsKNR = []       # Prediction Scores for K Nearest Neighbours Regressor\n\n# Preprocessing init\nss = StandardScaler()\n\n# Models init\nsvr = SVR()\nrfr = RandomForestRegressor()\nknr = KNeighborsRegressor(n_neighbors=3)\n\n# Cross-Validation 10 Fold\ncv = KFold(n_splits=10, random_state=1206, shuffle=True)\n\n# Loop into all 10 Folds\nfor train_index, test_index in cv.split(x):\n    # Define train and test to simplify our life\n    x_train, x_test, y_train, y_test = x.loc[train_index,:], x.loc[test_index,:], y.loc[train_index,:], y.loc[test_index,:]\n    # Fit and Transform our train set using StandardScaler\n    x_train = ss.fit_transform(x_train)\n    # Transform our test set based on x_train\n    x_test = ss.transform(x_test)\n    # Fit Models\n    rfr.fit(x_train, y_train)\n    svr.fit(x_train, y_train)\n    knr.fit(x_train, y_train)\n    # Predict using our models\n    y_pred_rfr = rfr.predict(x_test)\n    y_pred_svr = svr.predict(x_test)\n    y_pred_knr = knr.predict(x_test)\n    # Calculate and append Prediction Mean Square Error\n    metricsRFR.append(mean_squared_error(y_test.values.ravel(), y_pred_rfr))\n    metricsSVR.append(mean_squared_error(y_test.values.ravel(), y_pred_svr))\n    metricsKNR.append(mean_squared_error(y_test.values.ravel(), y_pred_knr))\n\n# Print the results\nprint('RFR had Averaged MSE: %.2f\\nSVR had Averaged MSE: %.2f\\nKNR had Averaged MSE: %.2f'\n      % (np.mean(metricsRFR), np.mean(metricsSVR), np.mean(metricsKNR)))","f909f50a":"# Defining some variables\nmetricsRFR = []       # Prediction Scores for Random Forest Regressor\nmetricsSVR = []       # Prediction Scores for Support Vector Regressor\nmetricsKNR = []       # Prediction Scores for K Nearest Neighbours Regressor\nrfrMSE = []           # Aux variable to keep MSE track for 3,4 and 5 features in Random Forest Regressor\nsvrMSE = []           # Aux variable to keep MSE track for 3,4 and 5 features in Support Vector Regressor\nknrMSE = []           # Aux variable to keep MSE track for 3,4 and 5 features in K Nearest Neighbours Regressor\n# Preprocessing init\nss = StandardScaler()\n\n# Models init\nsvr = SVR()\nrfr = RandomForestRegressor()\nknr = KNeighborsRegressor(n_neighbors=3)\n\n# Cross-Validation 10 Fold\ncv = KFold(n_splits=10, random_state=1206, shuffle=True)\n\n# Loop into all 10 Folds\nfor train_index, test_index in cv.split(x):\n    # Define train and test to simplify our life\n    x_train, x_test, y_train, y_test = x.loc[train_index,:], x.loc[test_index,:], y.loc[train_index,:], y.loc[test_index,:]\n    # Fit and Transform our train set using StandardScaler\n    x_train = ss.fit_transform(x_train)\n    # Transform our test set based on x_train\n    x_test = ss.transform(x_test)\n    # Apply Feature Selection to use only 3,4 or 5 features\n    for nFeat in [3,4,5]:\n        # Init FS object to nFeat predictors variables\n        fs = SelectKBest(score_func=mutual_info_regression, k=nFeat)\n        # Choose our selected features in train\/test set\n        x_train_fs = fs.fit_transform(x_train, y_train.values.ravel())\n        x_test_fs = fs.transform(x_test)\n        # Fit Models\n        rfr.fit(x_train_fs, y_train.values.ravel())\n        svr.fit(x_train_fs, y_train.values.ravel())\n        knr.fit(x_train_fs, y_train.values.ravel())\n        # Predict using our models\n        y_pred_rfr = rfr.predict(x_test_fs)\n        y_pred_svr = svr.predict(x_test_fs)\n        y_pred_knr = knr.predict(x_test_fs)\n        # Save MSE\n        rfrMSE.append(mean_squared_error(y_true=y_test.values.ravel(), y_pred=y_pred_rfr))\n        svrMSE.append(mean_squared_error(y_true=y_test.values.ravel(), y_pred=y_pred_svr))\n        knrMSE.append(mean_squared_error(y_true=y_test.values.ravel(), y_pred=y_pred_knr))\n\n    # Append Prediction Mean Square Error\n    metricsRFR.append(rfrMSE)\n    metricsSVR.append(svrMSE)\n    metricsKNR.append(knrMSE)\n    # Reset our MSE lists\n    rfrMSE = []\n    svrMSE = []\n    knrMSE = []","0dbd6586":"# Calculate AVG MSE for each selected group feature in each model\navgRFR = np.mean(metricsRFR, axis=0)\navgSVR = np.mean(metricsSVR, axis=0)\navgKNR = np.mean(metricsKNR, axis=0)\n# Print results\nprint('--- AVG MSE for Random Forest Regressor')\nprint('3 Features: %.2f\\n4 Features: %.2f\\n5 Features: %.2f\\n' % (avgRFR[0], avgRFR[1], avgRFR[2]))\nprint('--- AVG MSE for Support Vector Regressor')\nprint('3 Features: %.2f\\n4 Features: %.2f\\n5 Features: %.2f\\n' % (avgSVR[0], avgSVR[1], avgSVR[2]))\nprint('--- AVG MSE for K Nearest Regressor')\nprint('3 Features: %.2f\\n4 Features: %.2f\\n5 Features: %.2f\\n' % (avgKNR[0], avgKNR[1], avgKNR[2]))","3e5461c7":"# Scale our x data\nss = StandardScaler()\n# Fit and transform our data using StandardScaler\nx_ss = ss.fit_transform(x)","54e505e1":"# Create a PC space with 2 components only\npca = PCA(n_components=2)\n# Fit and Transform X to PC dimension\npc = pca.fit_transform(x_ss)","8464f645":"# Print Explained Variance Ratio\nprint('PC1 explained %.2f ratio and PC2 explained %.2f ratio of total variance.' %\n      (pca.explained_variance_ratio_[0], pca.explained_variance_ratio_[1]))","fdbc67d7":"# Define our range of clusters based on genre size\nn_clusters = range(2, len(info.Genre.unique())) \n# Create a list to append silhouette_avg values for each K to plot\nsilhouette_avg = []\n\n# Loop to find optimal K\nfor K in n_clusters:\n    # Create KNN model\n    km = KMeans(n_clusters=K, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n    # Fit and predict with K clusters using Principal Components\n    pred = km.fit_predict(pc)\n    # Calculate silhouette score\n    silhouette_avg.append(silhouette_score(pc, pred))\n\n# Plot the results to define our optimal K\nplt.figure(figsize=(15,10))\nplt.plot(n_clusters, silhouette_avg, marker='o');\nplt.xticks(n_clusters);\nplt.xlabel('Number of clusters');\nplt.ylabel('Silhouette Averaged Score');","aa0f0478":"# Create our KMeans model using optimal K\nkm = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n# Fit and predict with 3 clusters using Principal Components\npred = km.fit_predict(pc)","2b02c0d7":"# Create a Dataframe for PC with target var\ndf = pd.DataFrame(data = pc, columns = ['PC1', 'PC2'])\n# Create a column for our clusters (KMeans prediction)\ndf['Clusters']=pd.Series(pred)\n# Join with target info\/popularity dataset\ndf = pd.concat([df, y, info], axis = 1)\n# A sample of our new dataframe\ndf.head(3)","c8f1e8ac":"# Plot using seaborn our clusterization\nplt.figure(figsize=(20,10));\nsns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Clusters\", data=df, palette=['green','blue','red'], s= 50);","07adf4a6":"# Define subplots to get a side by side view\nfig, ax = plt.subplots(1,2, figsize=(15,5));\n# Plot KMeans \np1 = sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Clusters\", data=df, palette=['green','blue','red'],  ax=ax[0], s= 50);\n# Put KMeans legend outside\np1.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1);\n# Plot with genre colors \np2 = sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Genre2\", data=df, ax=ax[1], s=50);\n# Put again legend outside\np2.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1);\n# Insert a safe space between the plots\nfig.tight_layout()","f8f5d66b":"# Plot using seaborn our clusterization\nplt.figure(figsize=(20,10));\nsns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Clusters\", data=df, palette=['green','blue','red'], size=\"Popularity\", sizes=(10, 300), edgecolor=\"black\");","45dc4d96":"# Create a PC space with 2 components only\npca = PCA(n_components=3)\n# Fit and Transform X to PC dimension\npc = pca.fit_transform(x_ss)","44916f70":"pca.explained_variance_ratio_","f248caf6":"# Print Explained Variance Ratio\nprint('PC1 explained %.2f\\nPC2 explained %.2f\\nPC3 explained %.2f\\nFor total of %.2f Explained Variance Ratio.' %\n      (pca.explained_variance_ratio_[0], pca.explained_variance_ratio_[1], pca.explained_variance_ratio_[2],sum(pca.explained_variance_ratio_)))","6b76f405":"# Define our range of clusters based on genre size\nn_clusters = range(2, len(info.Genre.unique())) \n# Create a list to append silhouette_avg values for each K to plot\nsilhouette_avg = []\n\n# Loop to find optimal K\nfor K in n_clusters:\n    # Create KNN model\n    km = KMeans(n_clusters=K, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n    # Fit and predict with K clusters using Principal Components\n    pred = km.fit_predict(pc)\n    # Calculate silhouette score\n    silhouette_avg.append(silhouette_score(pc, pred))\n\n# Plot the results to define our optimal K\nplt.figure(figsize=(15,10))\nplt.plot(n_clusters, silhouette_avg, marker='o');\nplt.xticks(n_clusters);\nplt.xlabel('Number of clusters');\nplt.ylabel('Silhouette Averaged Score');","0ea37ccf":"# Create our KMeans model using optimal K\nkm = KMeans(n_clusters=11, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n# Fit and predict with 3 clusters using Principal Components\npred = km.fit_predict(pc)","c82c5913":"# Create a Dataframe for PC with targets\ndf = pd.DataFrame(data = pc, columns = ['PC1', 'PC2','PC3'])\n# Create a column for our clusters (KMeans prediction)\ndf['Clusters']=pd.Series(pred)\n# Join with target info\/popularity dataset\ndf = pd.concat([df, y, info], axis = 1)\n# A sample of our new dataframe\ndf.head(3)","2266fa46":"# Plot 3D - KNN Clusters\npx.scatter_3d(df, x='PC1', y='PC2', z='PC3', color='Clusters')","4550db77":"# Plot 3D - Genres with Popularity size\nfig = px.scatter_3d(df, x='PC1', y='PC2', z='PC3', color='Clusters', size='Popularity', symbol='Genre2')\nfig.update_layout(legend=dict(x=-.1, y=0.5))","fa215688":"# Define ranges for TSNE hyperparameters\nperplexities = [5,10,15,30,40,50]\nlearning = [200, 800, 2000]\n# Create subplots \nfig, ax = plt.subplots(len(perplexities),len(learning), figsize=(20,15))\n\n# Manual Grid Search for best hyper parameters through loop\nfor p in range(0,len(perplexities)):\n    for l in range(0,len(learning)):\n        # Create TSNE model\n        tsne = TSNE(n_components=2, n_iter=2000, perplexity=perplexities[p], learning_rate=learning[l])\n        # Fit and Transform with X scaled for PC\n        x_emb = tsne.fit_transform(x_ss)\n        # Turn X embeddeb into a dataframe to a easy plot\n        x_emb = pd.DataFrame(data = x_emb, columns = ['XE1', 'XE2'])\n        # Join with target categories\n        x_emb = pd.concat([x_emb, info], axis = 1)\n        # Plot 2D data\n        sns.scatterplot(x='XE1', y='XE2', hue=\"Genre2\", data=x_emb, ax=ax[p][l]);\n        ax[p][l].set_title((\"Perplexity=%d | Learning=%d\" % (perplexities[p],learning[l])));\n        ax[p][l].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n# Improve our layout\nplt.tight_layout()","3b0b1580":"## Principal Components Analysis with K Means in a 3D plot\nThe pipeline for this:\n- Create a PCA for 3 components (3D plot)\n- Analyze explained ratio and compare with PCA with 2 components\n- Create a K Means using silhouette analysis to find optimal K in a range from 2 to N clusters, where N means the maximum number of genres in our dataset\n- Analyze the results (using genre categorical column and popularity numerical column)","099e3531":"Now, let's see the same plot side by side with other plot where the colors are define by the **Genre2** column:","f935fb79":"Compared with 2 components that explained 43% now we have 57% that could be considerate a good improvement.\n\n### Model Development - KMeans in 3D","790c001c":"Only **Loudness** with **Energy** have a high positive correlation, but not that significant (most of the academic papers consider a correlation higher than *0.7* as significant). Because the lackness of strong monotonous correlations, I believe that the creation of linear predictive models becomes unnecessary.\n\n## How many songs are located in each genre for Spotify Top 50?\nFor this, I will create a count plot with *seaborn*:","718a525e":"### Understanding Silhouette Averaged Score\nThis is one of the methods that helps to choose an optimal K cluster and is under the range *-1 to +1* where near *+1* indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. That explanations was get from [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html).\n\nSo, the optimal K that will be chosen is **K = 3** that has the higher metric.\n\n### KMeans 2D Analysis","d3fe6c13":"# Predictive Analysis\n\nIs it possible to predict **Popularity** based on X features?\n\nGiven the small dataset, I will implement a Cross-Validation 10 Folds using *sklearn* for our models creation. The predictive models chosen are:\n- SVM (Support Vector Machine)\n- RF (Random Forest)\n- KNN (K Nearest Neighbours)\n\nBecause of correlation analysis in EDA, I will not create any Linear model as explained before.\n\nThe model developments has the follow pipeline:\n- Apply CV 10 Folds in our data\n- Standardize our train set and then our test set based on previous transformation\n- Fit our models with defaults hyperparameters\n- Predict with our models and get the MSE metric\n\nThe model chosen will be the one with the best average prediction metric (MSE) over the cross validation process.\n\n## Model Development","2c67b89a":"With this plot, it is possible to see that PCA\/KMeans approach was unsuccessful to clusterize and separate base on popularity and genre as genre is scattered in the plot. This result will be discussed in **Conclusions**.","2f2a86fd":"Based on those MSE results, I did not succeed to get an useful model using the *default* parameters and all features to predict Popularity in Spotify. Now I will try other approach: apply a simple feature selection before our models creation. I will do this **because given the small dataset (50 samples), would be preferred to have less variables to reduce our complexity**. For this, I will repeate the above code and apply some changes trying to see our models performance with 3,4 and 5 features.\n\n## Model Development with Feature Selection","a8d97424":"Considering the previous two graphs, I believe that the low average popularity of the Pop genre shown in the first one is due to the fact that the *average metric* penalizes low values and some of the smaller genres pertaining to it such as australian pop, canadian pop and boy band have the lowest values of popularity (presented in the last plot) thus lowering the final average.\nAlso, as the other genres created have much less music, they end up benefiting (with less music, less chances to have a low popularity song).","478d8dab":"Based on this graph, we can take some infos:\n- Even with most of the songs, **Pop** shows the lowest popularity average over all genres.\n- Except for **Pop**, all average popularities are very close with **Rap** showing the higher\n\nI will discuss more this result after the next plot using the original genre distribution","fd011a91":"Again popularity has not able to divide into different clusters, having mixed values.\n\nLet's see if a 3D plot is able to verify a clusterization for **Genres** and **Popularity Size**.","539d9c55":"# Objective\n\nMy objectives with this notebook are:\n- A brief Exploratory Data Analysis (EDA)\n- Predictive Models for popularity\n- Clustering visualization with insights for popularity and genres\n\nThe main results will be discuss at **Conclusions**\n\n## Importing Libraries\n","a225aa23":"Based on those results, our models did not improved at all using less variables after a feature selection (A good MSE is close to zero). I will discuss in **Conclusions** possibles reasons for this.","e19656f3":"# A brief Exploratory Data Analysis (EDA)\n\n## Is there any collinearity in our features between each other or with popularity?\nLet's find out using Spearman Correlation ranging from -1 to 1, where:\n- *-1* Implies a strong negative correlation\n- 0 Implies no correlation\n- +1 Implies a strong positive correlation","8f176a14":"Unfortunately, TSNE could not separate our feature in clusters and get info about Genre. Given that, I will do an analysis about the results found here in **Conclusions**. ","f7d1d807":"So, summing up our explained variance ratio using 2 components we got **43%** of explanation.\n\n### Model Development - KMeans in 2D","e0726cde":"# Conclusions\n- Popularity data does not present any major difference between the samples, showing a short variance range and therefore indifferent (EDA presents that proof). This could be responsable for the fact that this target variable did not help in Prediction\/Clustering Analysis.\n    + **In Prediction, our models failed to get a good MSE even after a feature selection because our target variable did not change at all.**\n    + **In Clustering, as we could see in KMeans plot Popularity size was practically the same for most of the samples.**\n- However, as the database refers to spotify's top 50, it was expected that popularity would not change as much.\n- Probably, this dataset is too small (only 50 samples) to get the desired answers for my questions (predict Songs popularity using those attributes and get clusters from those features to findo info about Song genres). I believe that this same analysis using a large dataset could have much better results.\n- I understand that using default hyperparameters in Prediction Analysis I could improve my results, but based in previous statements I don't think that would change at all my results. That was the reason for me to not try a TSNE 3D.\n","824f8e6a":"As we can see, **Pop** style in general contains most of 50 Top Songs. Because we have a small dataset and a lot of Genres I will simplify future genre analysis by creating a new *Genre* column, joining close styles into big groups:\n- Pop: canadian pop, dance pop, pop, panamanian pop, australian pop and boy band\n- Rap: dfw rap and country rap\n- Reggaeton: reggaeton flow and reggaeton\n- Hip hop: canadian hip hop and atl hip hop\n- Eletronic: trap music,pop house, edm, big room, electropop and brostep\n- Latin: latin and r&b en espanol\n- Escape room: escape room\n\n**PS**: I did know, but there is a discussion related to Escape Room genre that was created by Spotify algorithm (AWESOME). \n\nTo do that, I will create this column manually:","5a07cf9f":"# Clustering Analysis\nIs it possible to clusterize our data using those 9 numeric features and get info about *genre* and *popularity*? Let's find out using PCA with KNN and tSNE. \n\n## Principal Components Analysis with K Means in a 2D plot\nThe pipeline for this:\n- Create a PCA for 2 components (2D plot)\n- Analyze explained ratio\n- Create a K Means using silhouette analysis to find optimal K in a range from 2 to N clusters, where N means the maximum number of genres in our dataset\n- Analyze the results (using genre categorical column and popularity numerical column)\n\n### Scale our features before any PCA analysis","2618566e":"## Preparing our dataset","a5ed921b":"### Model Development - PCA with 2 components","d5ebdc6c":"Now it is possible to see more clear that Pop Genres dominates this dataset follow by Eletronic and Latin styles.\n\n## How is distributed average popularity over the genres?\nLet's find out using again *seaborn* in our new column:","cf69f594":"## TSNE in a 2D Plot\n\nTo find a better result using TSNE, I will evaluate different values for **perplexity\/learning rate** that are hyperparameters.\n\n**PS**: For TSNE **I will not** use popularity size, given the fact that I realize the it's range doesn't show any major difference between the points as we saw previously in KNN plots.","e55ad29d":"Based on this plot, there isn't **any clustering for defined genres**, because they are very mixed over the clusters.\n\nNow, let's see one plot where a marker size is defined by **Popularity**:","dff64372":"Now, the same plot will be create","979f1576":"Besides my optimism, using 3 components decreased the Silhouette Score maximum value and increase the clusters complexity. Let's see if this increase in number of clusters could give any information related to **Genre** or **Popularity**.\n\nSo, the optimal K that will be chosen is **K = 11** that has the higher metric.\n\n### KMeans 3D Analysis"}}