{"cell_type":{"26f5a12d":"code","54cb730f":"code","760fc8b9":"code","f99c0cba":"code","c601a8ed":"code","98d14d19":"code","4c36ce56":"code","a0efeada":"code","611c5fb6":"code","11fba12f":"code","fbc3e2d6":"code","f929cdea":"code","b604cb3c":"code","18588810":"code","76afa309":"code","af308424":"code","7fe5f2c4":"code","e267d2f6":"code","c86a4aca":"code","50dfe73d":"code","c4c7a872":"code","95f7f22f":"code","9212eee8":"code","247cb9f1":"code","6ff0d4dd":"code","8ee1eb66":"code","658a8e48":"code","9165f284":"code","9a44e2a7":"code","a79b4832":"code","3ded1921":"code","056b319d":"code","dce6238d":"code","2fad6ab9":"code","26dd3fca":"code","72078a0a":"markdown","f30c0bbc":"markdown","1c6f80ae":"markdown","0afae2cc":"markdown","96bc2e3e":"markdown","629f0a33":"markdown","702433b9":"markdown","01b35c0c":"markdown","55b2a233":"markdown","5fe72893":"markdown","81c67bbc":"markdown","4ce69d73":"markdown","2aa67c20":"markdown","56f2182e":"markdown","ab356e80":"markdown","0157feba":"markdown","427fb276":"markdown","bb2e88af":"markdown","257f419d":"markdown","7b9aafcd":"markdown","cc0e9882":"markdown","441a9287":"markdown","8323f1ec":"markdown","956b4ddf":"markdown","cfdd9587":"markdown","89f25c6e":"markdown","fd95910a":"markdown","d9e37a99":"markdown","dfe93102":"markdown","697bcdd1":"markdown"},"source":{"26f5a12d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54cb730f":"#Load Train data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#Load test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","760fc8b9":"train_data.head()","f99c0cba":"test_data.head()","c601a8ed":"train_data.dtypes","98d14d19":"train_data.columns","4c36ce56":"train_data.shape","a0efeada":"test_data.shape","611c5fb6":"#Check for the number of data values that are NaN in each column\ntrain_data.isnull().sum()\n","11fba12f":"import seaborn as sns\nsns.heatmap(train_data.corr())","fbc3e2d6":"train_data.plot.hist(x='Age',y='Survived',bins=5)\nplt.show()","f929cdea":"train_data['Age'].fillna(train_data.mean()['Age'],inplace=True)\ntrain_data.isnull().sum()","b604cb3c":"test_data['Age'].fillna(test_data.mean()['Age'],inplace=True)\ntest_data['Fare'].fillna(test_data.mean()['Fare'],inplace=True)\ntest_data.isnull().sum()","18588810":"train_data['Pclass'].unique()","76afa309":"one=train_data.loc[train_data.Pclass == 1]['Survived']\nrate_one = sum(one)\/len(one)\n\nprint(\"% of people who survived with Class 1:\", rate_one)\n\ntwo=train_data.loc[train_data.Pclass == 2]['Survived']\nrate_two = sum(two)\/len(two)\n\nprint(\"% of people who survived with Class 1:\", rate_two)\n\nthree=train_data.loc[train_data.Pclass == 3]['Survived']\nrate_three = sum(three)\/len(three)\n\nprint(\"% of people who survived with Class 1:\", rate_three)\n","af308424":"train_data['Embarked'].unique()","7fe5f2c4":"temp=train_data\ntemp.dropna(axis=0,inplace=True)\n\nS=temp.loc[temp.Embarked == 'S']['Survived']\nrate_S = sum(S)\/len(S)\n\nprint(\"% of people who survived with Embarked : S -\", rate_S)\n\nC=temp.loc[temp.Embarked == 'C']['Survived']\nrate_C = sum(C)\/len(C)\n\nprint(\"% of people who survived with Embarked : C -\", rate_C)\n\nQ=temp.loc[temp.Embarked == 'Q']['Survived']\nrate_Q = sum(Q)\/len(Q)\n\nprint(\"% of people who survived with Embarked : Q -\", rate_Q)\n\n\n\n","e267d2f6":"train_data.plot.hist(x='Sex',y='Survived',bins=2)\nplt.xlabel(\"Sex\")","c86a4aca":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","50dfe73d":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","c4c7a872":"#SibSp\ntrain_data['SibSp'].unique()","95f7f22f":"train_data.plot.hist(x='SibSp',y='Survived',bins=8)\nplt.xlabel(\"SibSp\")\nplt.ylabel(\"Survived\")","9212eee8":"#Parch\ntrain_data['Parch'].unique()","247cb9f1":"train_data.plot.hist(x='Parch',y='Survived',bins=6)\nplt.xlabel(\"Parch\")\nplt.ylabel(\"Survived\")","6ff0d4dd":"y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\"]\nX = pd.get_dummies(train_data[features])\n","8ee1eb66":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,random_state=20,test_size=0.2)","658a8e48":"\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=11, max_depth=5, random_state=1)\n\n\nrf.fit(x_train,y_train)\naccuracy=rf.score(x_test,y_test)\n\nprint(\"Random Forest accuracy is :{}\".format(accuracy))\n","9165f284":"from sklearn.naive_bayes import MultinomialNB\n# Instantiate the classifier\nmnb = MultinomialNB()\n\n# Train classifier\nmnb.fit( x_train,y_train)\naccuracy=mnb.score(x_test,y_test)\nprint(\"Naive bayes accuracy is :{}\".format(accuracy))","9a44e2a7":"\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=29)            #n_neighbors optimal value should be suqare root of n\nknn.fit(x_train,y_train)\ny_pred_knn=knn.predict(x_test)\n\n#finding accuracy and confusion matrix\naccuracy=accuracy_score(y_pred_knn,y_test)\nprint(\"KNN accuracy is :{}\".format(accuracy))","a79b4832":"#SVM\nfrom sklearn import svm    \t\t\t\nC = 0.6  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(x_train, y_train)\n#svc = svm.LinearSVC(C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(x_train, y_train)\n# SVC with polynomial (degree 3) kernel\npoly_svc = svm.SVC(kernel='poly', degree=2, C=C).fit(x_train, y_train)\naccuracy1=svc.score(x_test,y_test)\nprint(\"SVM accuracy is :{}\".format(accuracy1))\naccuracy2=rbf_svc.score(x_test,y_test)\nprint(\"SVM rbf accuracy is :{}\".format(accuracy2))\naccuracy3=poly_svc.score(x_test,y_test)\nprint(\"SVM poly accuracy is :{}\".format(accuracy3))","3ded1921":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth = 6,random_state = 99, max_features = None, min_samples_leaf = 5)\ndtree.fit(x_train,y_train)\naccuracy=dtree.score(x_test,y_test)\nprint(\"Decision tree accuracy is :{}\".format(accuracy))","056b319d":"#Choose best value for n_estimators\nfrom sklearn.model_selection import validation_curve\nparam_range = np.arange(1, 250, 2)\ntrain_scoreNum, test_scoreNum = validation_curve(RandomForestClassifier(),X = x_train, y = y_train, \n                                param_name = 'n_estimators', scoring=\"accuracy\",\n                                param_range = param_range, cv = 3,n_jobs=-1)\nplt.plot(train_scoreNum)","dce6238d":"#Choose best value for max_depth\nfrom sklearn.model_selection import validation_curve\nparam_range = np.arange(5, 30, 1)\ntrain_scoreNum, test_scoreNum = validation_curve(RandomForestClassifier(),X = x_train, y = y_train, \n                                param_name = 'max_depth', scoring=\"accuracy\",\n                                param_range = param_range, cv = 3,n_jobs=-1)\nplt.plot(train_scoreNum)","2fad6ab9":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=20, max_depth=8, random_state=1)\n\nrf.fit(x_train,y_train)\nrf.score(x_test,y_test)","26dd3fca":"model = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=1)\nmodel.fit(X, y)\nX_test = pd.get_dummies(test_data[features])\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('titan_predictions_1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","72078a0a":" **As an initial step, load the train and test data to the model.**","f30c0bbc":"***K-Nearest Neighbors Classifier***","1c6f80ae":"*Check how 'SibSp' and 'Parch' are related to \"Survived\"*","0afae2cc":"**We have both numerical and categorical data in the given dataset.**\n\n**Numerical Data:**\n* Age, Fare - continuous\n* SibSp, Parch - discrete\n\n**Categorical Data:**\n* Sex, Embarked, Pclass\n","96bc2e3e":"Let us use this model, which gives 84.91% accuracy to predict the results on the test_data.","629f0a33":"***Naive Bayes Classifier***","702433b9":"Since there is no great impact from 'Embarked', let us ignore the feature.","01b35c0c":"*We can check how each numerical feature is correlated to 'Survival' using a heatmap.*","55b2a233":"The columns 'Age', 'Cabin' and 'Embarked' have missing values. Let's check if they are higly correlated with the target variable 'Survived'. If not, they can be ignored.","5fe72893":"*Let's now replace the missing values in 'Age' by the mean of values.*","81c67bbc":"Now, we can say that n_estimators = 20 and max_depth = 8 are the best parameters in this case for maximum accuracy on the training data set.","4ce69d73":"***Clearly, Random Forest Clasifier gives the highest accuracy of 83.7988%***","2aa67c20":"*We can now say that Women have higher chances of Survival than Men. Hence, 'Sex' plays a very important role too.*","56f2182e":"Let us now tune the hyperparameters of the Random Forest Classifier for better resutls.","ab356e80":"**Now, let us apply various machine learning algorithms on the dataset.**","0157feba":"**Now, let's visualize which features are more important for the prediction**","427fb276":"Therefore, we need to consider 'Pclass'","bb2e88af":"**Before that, we need to clean the data. Let's first handle the missing values**","257f419d":"**Once the data is loaded, let's analyze it.**","7b9aafcd":"***Suppor Vectore Machine with different kernels***","cc0e9882":"Clearly, from the above histogram, it is clear that people with Age<20 and >80 have high chances of Survival. Hence, Age plays a massive role in deciding if a person has Survived. ","441a9287":"Split the training data to test and train so as to find the best algorithm based on accuracy.","8323f1ec":"*The Features in consideration are 'Pclass','Sex','Age','SibSp' and 'Parch'*","956b4ddf":"*Check how 'Embarked' is related to \"Survived\"*","cfdd9587":"***Random Forest Classifier***","89f25c6e":"*Check how 'Pclass' is related to \"Survived\"*","fd95910a":"Therefore, we wil cpnsider 'SibSp' and 'parch' for the model.","d9e37a99":"***Decision Tree Classifier***","dfe93102":"*Check how 'Age' is related to \"Survived\"*","697bcdd1":"*Check how 'Sex' is related to Survived*"}}