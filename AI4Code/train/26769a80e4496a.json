{"cell_type":{"a9940309":"code","26d82618":"code","d1de75e7":"code","bee50f62":"code","714db5f8":"code","17a1511e":"code","91e23e5d":"code","4880c409":"code","ba3a57bc":"code","1ca030c9":"code","427938c6":"code","5fcaef8d":"code","3ece2f54":"code","545bd7f1":"code","b59f1c9f":"code","d18dac7e":"code","b8fc7cfe":"code","b3afa898":"code","c1d38d21":"code","9fc0461f":"code","c8aa5d17":"code","a875538d":"code","f3aa884b":"code","4be81a83":"code","1188db12":"code","d7212bd9":"markdown","7c578cc8":"markdown","5a426c53":"markdown","69d71a16":"markdown","0c4d4d98":"markdown","62a9c022":"markdown","05f3a979":"markdown","75ccef56":"markdown","fd873393":"markdown","c40cc2a1":"markdown","bf5fdf41":"markdown","d36471fa":"markdown","28c20bf8":"markdown","015dbbb7":"markdown","7d9b7fa0":"markdown","02438b39":"markdown","24bb896d":"markdown","4d6dca25":"markdown","a59cabb5":"markdown","40bbdcb0":"markdown"},"source":{"a9940309":"import os\nimport glob\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport cv2\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Input, Dropout, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom skimage.feature import greycomatrix, greycoprops\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split","26d82618":"train_dir = '..\/input\/chest-xray-covid19-pneumonia\/Data\/train\/*'\ntest_dir = '..\/input\/chest-xray-covid19-pneumonia\/Data\/test\/*'\ntrain2_covid_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/COVID'","d1de75e7":"SIZE = 140\nBATCH_SIZE = 64\nTARGET_SIZE = (SIZE,SIZE)\nEPOCH_NUM = 140","bee50f62":"categories_dict = {\n  0: \"PNEUMONIA\",\n  1: \"NORMAL\",\n  2: \"COVID19\"\n}","714db5f8":"train_images = []\ntrain_labels = [] \nlabel = 0\n\n#Importing the first training dataset\n\nfor directory_path in glob.glob(train_dir):\n    assert categories_dict[label] == os.path.normpath(directory_path).split(os.path.sep)[-1]\n    print(categories_dict[label])\n    counter = 1\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        if(counter%200==0): print(counter,\"images loaded\")\n        img = cv2.imread(img_path, 0)\n        img = cv2.resize(img, TARGET_SIZE)\n        train_images.append(img)\n        train_labels.append(label)\n        counter+=1\n        if(counter%1500==0): break\n    \n    print(counter,\"images loaded\")\n    label +=1\n    \n#Importing the additional training dataset\n\nprint(\"additional\",categories_dict[2],\"data\")    \naddit_counter = 1\nfor img_path in glob.glob(os.path.join(train2_covid_dir, \"*.png\")):\n    if(addit_counter%200==0): print(addit_counter,\"images loaded\")\n    img = cv2.imread(img_path, 0)\n    img = cv2.resize(img, TARGET_SIZE)\n    train_images.append(img)\n    train_labels.append(2)\n    addit_counter+=1\n    if(addit_counter%1000==0): break\nprint(addit_counter,\"images loaded\")\n\nx_train = np.array(train_images)\ny_train = to_categorical(train_labels, 3)","17a1511e":"test_images = []\ntest_labels = []\nlabel = 0\n\n#importing the testing dataset\n\nfor directory_path in glob.glob(test_dir):\n    assert categories_dict[label] == os.path.normpath(directory_path).split(os.path.sep)[-1]\n    print(categories_dict[label])\n    counter = 1\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        if(counter%100==0): print(counter, \"images loaded\")\n        img = cv2.imread(img_path, 0)\n        img = cv2.resize(img, TARGET_SIZE)\n        test_images.append(img)\n        test_labels.append(label)\n        counter+=1\n    \n    print(counter,\"images loaded\")\n    label +=1\n\ntest_images = np.array(test_images)\ntest_labels = to_categorical(test_labels, 3)","91e23e5d":"train_test_split(train_images, train_labels)\ntrain_images, val_images, train_labels, val_labels = train_test_split(x_train, y_train, test_size=0.15, random_state=69)","4880c409":"print(\"train:\",train_images.shape[0],\", test:\",test_images.shape[0],\", val:\",val_images.shape[0])","ba3a57bc":"def feature_extractor(images):\n    image_dataset = pd.DataFrame()\n    for image in images:   \n        df = pd.DataFrame()\n        \n        #greycomatrix(image, distances, angles, levels=256, symmetric=False, normed=False)\n        #distances - List of pixel pair distance offsets.\n        #angles - List of pixel pair angles in radians.\n        \n        #5 configuration for the grey-level co-occurrence matrix calculation\n        dists = [[1],[3],[5],[3],[3]]\n        angles = [[0],[0],[0],[np.pi\/4],[np.pi\/2]]\n        \n        for n ,(dist, angle) in enumerate(zip(dists, angles)):\n        \n            GLCM = greycomatrix(image, dist, angle)       \n            GLCM_Energy = greycoprops(GLCM, 'energy')[0]\n            df['Energy'+str(n)] = GLCM_Energy\n            GLCM_corr = greycoprops(GLCM, 'correlation')[0]\n            df['Corr'+str(n)] = GLCM_corr       \n            GLCM_diss = greycoprops(GLCM, 'dissimilarity')[0]\n            df['Diss_sim'+str(n)] = GLCM_diss       \n            GLCM_hom = greycoprops(GLCM, 'homogeneity')[0]\n            df['Homogen'+str(n)] = GLCM_hom       \n            GLCM_contr = greycoprops(GLCM, 'contrast')[0]\n            df['Contrast'+str(n)] = GLCM_contr\n\n        image_dataset = image_dataset.append(df)\n        \n    return image_dataset","1ca030c9":"train_extr_features = feature_extractor(train_images)","427938c6":"test_extr_features = feature_extractor(test_images)","5fcaef8d":"val_extr_features = feature_extractor(val_images)","3ece2f54":"type(train_images)\n# convert from integers to floats\ntrain_images_norm = train_images.astype('float32')\ntest_images_norm = test_images.astype('float32')\nval_images_norm = val_images.astype('float32')\n# normalize to the range 0-1\ntrain_images_norm \/= 255.0\ntest_images_norm \/= 255.0\nval_images_norm \/= 255.0","545bd7f1":"pd.set_option(\"display.max_columns\", None)\ntrain_extr_features","b59f1c9f":"def build_cnn():\n    model = keras.Sequential([\n        keras.Input(shape=(140,140,1), name='Original_Images'),\n        keras.layers.Conv2D(input_shape=(140,140,1), filters=32, kernel_size=11, \n                            strides=1, activation='relu', name='Conv1'),\n        keras.layers.Conv2D(input_shape=(130,130,32), filters=32, kernel_size=11, \n                            strides=1, activation='relu', name='Conv2'),\n        keras.layers.MaxPool2D(pool_size=(5, 5), strides=2),\n        keras.layers.Conv2D(input_shape=(58,58,32), filters=64, kernel_size=9, \n                            strides=1, activation='relu', name='Conv3'),\n        keras.layers.MaxPool2D(pool_size=(5, 5), strides=2),\n        keras.layers.Conv2D(input_shape=(23,23,64), filters=128, kernel_size=8, \n                            strides=1, activation='relu', name='Conv4'),\n        keras.layers.Conv2D(input_shape=(16,16,128), filters=256, kernel_size=9, \n                            strides=1, activation='relu', name='Conv5'),\n        keras.layers.Conv2D(input_shape=(8,8,256), filters=256, kernel_size=8, \n                            strides=1, activation='relu', name='Conv6'),    \n\n        keras.layers.Flatten(),\n        keras.layers.Dense(8, activation=tf.keras.activations.relu, name='Dense')\n    ])\n    print(model.summary())\n    return model","d18dac7e":"def build_mlp():\n    model = keras.Sequential([\n        keras.Input(shape=25, name='Extracted_Traditional_Features'),\n        keras.layers.Dense(8, activation=tf.keras.activations.relu, name='Dense1'),\n        keras.layers.Dense(4, activation=tf.keras.activations.relu, name='Dense2')\n    ])\n    print(model.summary())\n    return model","b8fc7cfe":"mlp = build_mlp()\ncnn = build_cnn()\n\ncombinedInput = concatenate([mlp.output, cnn.output])\n\nx = Dense(8, activation=\"relu\")(combinedInput)\nx = Dense(3, activation=\"softmax\")(x)\n\nmodel = Model(inputs=[mlp.input, cnn.input], outputs=x)\n","b3afa898":"opt = keras.optimizers.Adam(learning_rate=0.005)","c1d38d21":"model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","9fc0461f":"\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\ncb = [\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.1,\n        patience=10,\n        mode='auto',\n        min_delta=0.0002,\n        cooldown=5,\n        min_lr=10e-8,\n        verbose=1,\n    )\n]\n\n","c8aa5d17":"dataset_inputs = tf.data.Dataset.from_tensor_slices((train_extr_features, tf.expand_dims(train_images_norm, axis=-1)))\ndataset_label = tf.data.Dataset.from_tensor_slices(train_labels)\n\ndataset = tf.data.Dataset.zip((dataset_inputs, dataset_label)).batch(BATCH_SIZE).repeat()\nSTEP_SIZE_TRAIN= train_images_norm.shape[0]\/\/BATCH_SIZE\n# fit model\nhistory = model.fit(dataset, \n                    validation_data=([val_extr_features, tf.expand_dims(val_images_norm, axis=-1)], val_labels),\n                    epochs = EPOCH_NUM, steps_per_epoch=STEP_SIZE_TRAIN, callbacks=cb)\n\n","a875538d":"model.save_weights('.\/kentang_net')","f3aa884b":"test_inputs = tf.data.Dataset.from_tensor_slices((test_extr_features, tf.expand_dims(test_images_norm, axis=-1)))\ntest_labelz = tf.data.Dataset.from_tensor_slices(test_labels)\n\ntest_dataset = tf.data.Dataset.zip((test_inputs, test_labelz)).batch(BATCH_SIZE).repeat()\nSTEP_SIZE_TEST= test_images_norm.shape[0]\/\/BATCH_SIZE\n\nscore = model.evaluate(test_dataset, batch_size=BATCH_SIZE, steps=STEP_SIZE_TEST )\nprint(f'Test loss: {score[0]} \/ Test accuracy: {score[1]}')","4be81a83":"import matplotlib.pyplot as plt\nhistory.history.keys()","1188db12":"plt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d7212bd9":"**Splitting data into train and validation dataset**","7c578cc8":"**Intertwining the 2 feature extractor**","5a426c53":"# Data Loading","69d71a16":"# TODO: \n* ZCA whitening\n* Per-pixel mean subtraction\n* Dataset expansion\n\n**Todo (experimental):**\n* Gabor filter","0c4d4d98":"# \"Traditional\" Feature Extraction with GLCM","62a9c022":"> *if you are looking for the best method to classify Pneumonia and Covid19 on chest radiograph, this current version of the notebook might not give the best accuracy on the case study. The main focus of this notebook is to implement the method explained on the given paper.*","05f3a979":"# Testing","75ccef56":"![image.png](attachment:5569684c-3e98-461a-a4f0-2cd1fe1fc63d.png)\n# **Ello and welcome, fellow tensorflonian (\uffe3y\u25bd,\uffe3)\u256d \ud83e\udd0d**","fd873393":"This notebook is \"inspired\" from the work in the paper \"*[Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron\u202c](https:\/\/www.hindawi.com\/journals\/CIN\/2018\/2061516\/)*\" by ZhiFei Lai and HuiFang Deng. The idea of this paper is to combine traditional feature extraction method, such as Gray-Level Co-occurrence matrix (GLCM) and , with high-level feature extraction, such as Deep Convolutional Neural Network, into biomedical informatics field. This paper achieved an overall classification accuracy of 90.1% and 90.2%, respectively, which are higher than the compared methods on the paper.","c40cc2a1":"**References:**\n* [Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron\u202c (by ZhiFei Lai and HuiFang Deng)](https:\/\/www.hindawi.com\/journals\/cin\/2018\/2061516\/#abstract)\n* [load dataset (machinelearningmastery)](https:\/\/machinelearningmastery.com\/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras\/)\n* [loading mixed data (pyimagesearch)](https:\/\/www.pyimagesearch.com\/2019\/02\/04\/keras-multiple-inputs-and-mixed-data\/)\n* [Traditional Features extraction with GLCM (DigitalSreeni)](https:\/\/github.com\/bnsreenu\/python_for_microscopists\/blob\/master\/200_image_classification_using_GLCM.py)","bf5fdf41":"If you are paying attention to the [Chest X-ray (Covid-19 & Pneumonia)](https:\/\/www.kaggle.com\/prashant268\/chest-xray-covid19-pneumonia) dataset, there are only xxx number of Covid19 training data, compared to the xxx number of the pneumonia training data. Thus, we definitely (but not mandatory) need to balance number of data between each class. The [COVID-19 Radiography Database](https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database) will gives us some more Covid19 x-ray data for the training.","d36471fa":"# Importing Dependencies\nWe will be using Tensorflow for the network, and Scikit-learn for the GLCM computation.","28c20bf8":"# Building the Network","015dbbb7":"# Plotting the metrics","7d9b7fa0":"**Multilayer Perceptron to learn the numerical feature data extracted from the GLCM**","02438b39":"**Reduce Learning Rate on Plateau**, to reduce the learning rate gradually if there is no significant improvement on the performance of the network. ","24bb896d":"**Convolutional Neural Network to extract the high-level features**","4d6dca25":"Proposed method from the paper:\n\n![Architecture](https:\/\/static-01.hindawi.com\/articles\/cin\/volume-2018\/2061516\/figures\/2061516.fig.001.svgz)","a59cabb5":"In this version of the notebook, we are using Adam as the optimizer. Please kindly check the older version of the notebook to see how other optimizer, such as SGD and RMSprop, performs on the network.","40bbdcb0":"# Learning"}}