{"cell_type":{"ec794419":"code","e9362045":"code","ae47eeff":"code","12d7a938":"code","3892b14f":"code","140b74f9":"code","3448eeba":"code","67cf2c26":"code","7b2731a1":"code","357de7f6":"code","6ea7a547":"code","7dae72ce":"code","f866ba86":"code","976c7da3":"code","1b24babe":"code","ccab9210":"code","dcfd61d9":"code","357822ef":"code","021a8a4b":"code","68c669f9":"code","fb693e3a":"code","9adf6f73":"code","96d38959":"markdown","87475898":"markdown","f98862c9":"markdown","e23c1346":"markdown","f03294f6":"markdown","187a08c0":"markdown","1bfdb550":"markdown","bef31d0e":"markdown","494795df":"markdown","91513182":"markdown","c20bb805":"markdown","f65a0e4d":"markdown","85717a12":"markdown","fb3a6130":"markdown","769177c4":"markdown","68e6e238":"markdown","21196961":"markdown","287d3b76":"markdown","2a4175c6":"markdown"},"source":{"ec794419":"!pip install tqdm torch gym gym[Box_2D] pyglet toolz seaborn box2d-py scikit-learn","e9362045":"import numpy as np \nimport gym\n\nimport seaborn as sns\nfrom tqdm.notebook import tqdm \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom toolz import curry, do\nfrom functools import partial\nfrom copy import deepcopy\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.spatial import distance","ae47eeff":"import torch\nimport torch.nn as nn\n\nGPU = False\n\ndevice = \"cuda:0\" if GPU and torch.cuda.is_available() else 'cpu'\nprint(f\"device is {device}\")","12d7a938":"class LinearPolicy(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearPolicy, self).__init__()\n\n        self.net = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, output_dim)\n        ).to(device)\n\n    def forward(self, x):\n        action_values = self.net(x)\n\n        return action_values","3892b14f":"class Agent():\n    def __init__(self, policy):\n        self.policy = policy\n        \n    def act(self, observation):\n        action_values = self.policy.forward(torch.Tensor(observation).to(device))\n        action = torch.argmax(action_values).item()\n\n        return action","140b74f9":"def episode(env:gym.Env, agent:Agent, visualize=False):\n    rewards = 0\n\n    observation = env.reset()\n    done = False\n    \n    while not done:\n        if visualize:\n            env.render() \n            \n        action = agent.act(observation)\n        observation, reward, done, info = env.step(action)\n        rewards += reward\n\n    \n    return rewards","3448eeba":"env = gym.make(\"CartPole-v0\")\n\npolicy = LinearPolicy(sum(env.observation_space.shape), env.action_space.n)\nagent = Agent(policy)\n\nprint(f\"Sum rewards: {episode(env, agent, visualize=False)}\")","67cf2c26":"class GA:\n    def __init__(self, pop_size, survivors_selector, mutator, initializer, fitness):\n        self.pop_size = pop_size\n        self.initializer = initializer\n        self.mutator = mutator\n        self.fitness = fitness\n        self.survivors_selector = survivors_selector\n        \n        self.population = None \n        self.population_fitness = None\n        self.elite = None\n        self.elite_fitness = None\n        \n    def next_generation(self):\n        if not self.population:\n            self.population = [self.initializer() for _ in range(self.pop_size)]\n        else:\n            survivors = self.survivors_selector(self.population, self.population_fitness)\n            parents = np.random.choice(survivors, self.pop_size - 1, replace=True) # 1 for the elite\n            \n            children = [do(self.mutator, deepcopy(parent)) for parent in parents]\n            self.population = [self.elite] + children # elitism - elite stays as is without mutations\n            \n        self.population_fitness = [self.fitness(specimen) for specimen in tqdm(self.population)]\n        \n        elite_idx = np.argmax(self.population_fitness)\n        self.elite = self.population[elite_idx]\n        self.elite_fitness = self.population_fitness[elite_idx]","7b2731a1":"@curry\ndef fitness(robustness, env, specimen):\n    episode_rewards = [episode(env, specimen) for _ in range(robustness)]\n    \n    return sum(episode_rewards) \/ len(episode_rewards)","357de7f6":"def initializer(env):\n    policy = LinearPolicy(sum(env.observation_space.shape), env.action_space.n)\n    agent = Agent(policy)\n    \n    return agent","6ea7a547":"@curry\ndef mutator(mutation_strength, agent):\n    genome = nn.utils.parameters_to_vector(agent.policy.parameters())\n    noise = torch.randn(genome.shape).to(device) * mutation_strength\n    \n    genome += noise\n    torch.nn.utils.vector_to_parameters(genome, agent.policy.parameters())","7dae72ce":"@curry\ndef survivor_selection(truncation_len, population, fitnesses):\n        # Sort the populaiton by the fitness\n        fitness_pop = sorted(zip(fitnesses, population), reverse=True, key=lambda fit_pop: fit_pop[0])\n        pop = [specimen for fit, specimen in fitness_pop]\n        \n        return pop[:truncation_len]","f866ba86":"popsize = 50\nmutation_strength = 0.003\ntruncation_length = 10\nfit_robustness = 5\nn_generations = 20\nenv = gym.make(\"Acrobot-v1\")","976c7da3":"torch.autograd.set_grad_enabled(False) # we do not use backprop, se we can disable autograd to save some runtime and memory\n\nfitness_log = []\nga = GA(\n    pop_size = popsize,\n    initializer = partial(initializer, env),\n    fitness = fitness(fit_robustness, env),\n    survivors_selector = survivor_selection(truncation_length),\n    mutator = mutator(mutation_strength)\n)\n\nfor gen in tqdm(range(n_generations)):\n    ga.next_generation()\n    print(f\"generation {gen} fitness {ga.elite_fitness}\")\n    fitness_log.append(ga.elite_fitness)","1b24babe":"sns.lineplot(data={\n    \"generation\": [gen for gen in range(n_generations)],\n    \"fitness\": fitness_log\n    },\n    x=\"generation\",\n    y=\"fitness\",\n)","ccab9210":"class NS:\n    def __init__(self, pop_size, survivors_selector, mutator, initializer, behaviour_characteristic, novelty_measure,\n                 archive_pr):\n        self.pop_size = pop_size\n        self.initializer = initializer\n        self.mutator = mutator\n        self.behaviour_characteristic = behaviour_characteristic\n        self.novelty_measure = novelty_measure\n        self.survivors_selector = survivors_selector\n        self.archive_pr = archive_pr\n\n        self.population = None\n        self.population_novelties = None\n        self.archive = None\n\n    def next_generation(self):\n        if not self.population:\n            self.population = [self.initializer() for _ in range(self.pop_size)]\n        else:\n            survivors = self.survivors_selector(self.population, self.population_novelties)\n            parents = np.random.choice(survivors, self.pop_size, replace=True)\n\n            self.population = [do(self.mutator, deepcopy(parent)) for parent in parents]\n\n        bcs = [self.behaviour_characteristic(specimen) for specimen in self.population]\n\n        # measure the novelty vs the archive + current generation\n        archive_ = np.concatenate([self.archive, np.array(bcs)]) if self.archive is not None else np.array(bcs)\n        self.population_novelties = [self.novelty_measure(bc, archive_) for bc in bcs]\n\n        # update the archive\n        newly_archived = [bc for bc in bcs if np.random.rand() < self.archive_pr]\n        if newly_archived:\n            newly_archived = np.stack(newly_archived)\n            self.archive = np.concatenate([self.archive, newly_archived]) if self.archive is not None else newly_archived\n\n","dcfd61d9":"@curry\ndef last_observation_behaviour_characteristic(env, agent):\n    timestep = 0\n    observation = env.reset()\n    done = False\n    \n    while not done:\n        action = agent.act(observation)\n        observation, reward, done, info = env.step(action)\n        timestep += 1\n    \n    return np.array(observation + [timestep \/ 200])","357822ef":"@curry\ndef knn_novelty(n, bc, archive):\n    knn = NearestNeighbors(n_neighbors=n+1) # +1 because the archive will contain bc itself, which will add a 0 distance\n    knn.fit(archive)\n    \n    distances, indices = knn.kneighbors(bc.reshape(1, -1))\n    avg = np.sum(distances) \/ n\n    \n    return avg","021a8a4b":"popsize = 50\nmutation_strength = 0.8\ntruncation_length = 6\nn_neighbors = 40\narchive_pr = 0.1\nn_generations = 100\nfit_robustness = 5\nenv = gym.make(\"MountainCar-v0\")","68c669f9":"torch.autograd.set_grad_enabled(False) # we do not use backprop, se we can disable autograd to save some runtime and memory\n\nfitness_log = []\nns = NS(\n    pop_size = popsize,\n    initializer = partial(initializer, env),\n    survivors_selector = survivor_selection(truncation_length),\n    mutator = mutator(mutation_strength),\n    behaviour_characteristic = last_observation_behaviour_characteristic(env),\n    novelty_measure = knn_novelty(n_neighbors),\n    archive_pr = archive_pr\n)\n\nfit = fitness(fit_robustness, env)\nlog = {\n    'fitness': [],\n    'novelty': [],\n    'generation': [i for i in range(n_generations)]\n}\n\nfor gen in tqdm(range(n_generations)):\n    ns.next_generation()\n    \n    # for measuring only - the algorithm does not learn from that!\n    fitnesses = [fit(specimen) for specimen in ns.population]\n    \n    elite_fitness = max(fitnesses)\n    mean_novelty = sum(ns.population_novelties) \/ len(ns.population_novelties)\n    \n    print(f\"generation: {gen} fitness: {elite_fitness} mean novelty: {mean_novelty}\")\n    \n    log['fitness'].append(max(elite_fitness, log['fitness'][-1]) if log['fitness'] else elite_fitness)\n    log['novelty'].append(mean_novelty)","fb693e3a":"sns.lineplot(data=\n    log,\n    x=\"generation\",\n    y=\"fitness\",\n)","9adf6f73":"sns.lineplot(data=\n    log,\n    x=\"generation\",\n    y=\"novelty\",\n)","96d38959":"An agent will use this model and act in an environment. Here, the agent will greedily choose the action with the highest value. In some cases it is beneficial to select the actions randomly with their values as weights, but we'll simplify here. ","87475898":"### Install and import dependencies","f98862c9":"## Define an episode\nAn episode is a complete run in our simulation. We will create a method that sums the rewards an agent got during an episode.\nSadly, gym cannot render inside Jupyter so do not visualize this inside a notebook.","e23c1346":"For the novelty measure, we will use the euclidean distance of the $\\text{k-nearest-neighbors}$ of each $BC$.","f03294f6":"For survivor selection, we will use truncated selection - only the $k$ most fit individuals will survive.","187a08c0":"Increasing the population size and the robustness is likely to increase the results (at the cost of longer running time), as does fine-tuning the mutation strength. Still, this quick demonstration achieved nice results.","1bfdb550":"$BC$ is domain specific. In some domains, the last observation is a good behavior characteristic - but one should understand what it encodes and what is being achieved. For example, in the [LunarLander](https:\/\/gym.openai.com\/envs\/LunarLander-v2\/), the observation contains informatin about location, rotation, velocity, \"legs\" touching the ground, etc. It does not tell us anything about how much fuel we consumed, for example, using just the last state will not hold information about the trajectory itself. It is up to the researcher to come up with a measure that tells the innovation in a specific domain.\n<p>\nFor spatial-based rewards (such as learning to walk in [BipedalWalker](https:\/\/gym.openai.com\/envs\/BipedalWalker-v2\/)), using the location of the last state is a good start. Note, however, that in \"escaping\" environments like [MountainCar](https:\/\/gym.openai.com\/envs\/MountainCar-v0\/), the last location is identitcal in all solutions that reach the end! Since we would also like to get there *quickly* - it is beneficial to also be innovative with the *timestep* of the last state. So, as a $BC$, we will use the last *observation* + last *timestep*.","bef31d0e":"## Create a policy model and the model\nFor this notebook, we will learn to estimate the [Q-function](https:\/\/en.wikipedia.org\/wiki\/Q-learning), meaning - at every state, our policy will return the \"*value*\" of each possible action. Note that in this context, *value* does not necessarily mean the expected sum of rewards - it is suffice to value the actions such that beneficial actions have more *value*.\n<p>\nWe will create a neural network that takes in the current state and returns the values of each possible action (in that state). Feel free to play with different architectures yourself :)","494795df":"Initialization - we'll just pass an agent with a default network.\nIt might be worthwhile to try different parameter initialization (Xavier, etc) but for now this will do.","91513182":"Mutation is slightly trickier. We'll sample a random noise vector and add it to the policy's parameters network. A mutation-strength parameter (denoted $\\sigma$) controls the amplitude of this noise. To do this, we'll convert the network parameters to a vector, add the noise, and convert back from a vector.","c20bb805":"## Run an experiment\nFeel free to play with these hyperparameters!","f65a0e4d":"This is a good scheme, but we are still missing the evolutionary operators. \nLet's start with fitness. To reduce randomness, we will run a number of episodes and return their mean.","85717a12":"## Awesome! What's next?\n\nEvolutionary Computation is a fascinating topic, and this notebook is just the tip of the iceberg. <br>\nYou can explore different genetic operators, try different algorithms (such as [Evolution Strategy](https:\/\/arxiv.org\/abs\/1703.03864)), explore different network architectures (or even [evolve them!](https:\/\/www.researchgate.net\/profile\/Jeff-Clune\/publication\/330203191_Designing_neural_networks_through_neuroevolution\/links\/5e7243fc92851c93e0ac18ea\/Designing-neural-networks-through-neuroevolution.pdf)), create different behaviour characteristics, search the *action space* itself with [Safe Mutations](https:\/\/arxiv.org\/pdf\/1712.06563.pdf), and so much more!\n<p>\nLet me know in the comments what great things you achieved!\n    \n![](https:\/\/cdn.substack.com\/image\/fetch\/f_auto,q_auto:good,fl_progressive:steep\/https:%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f95d3fe-13e7-4220-84b1-9c12d760f233_950x535.jpeg)","fb3a6130":"In this notebook we will learn how to utilize the Genetic algorithm to solve RL problems. This notebook loosely follows [this paper](https:\/\/arxiv.org\/abs\/1712.06567)","769177c4":"## Novelty Search","68e6e238":"## Genetic Algorithm \nWe will define a [Genetic Algorithm](https:\/\/en.wikipedia.org\/wiki\/Genetic_algorithm) to optimize the weights of the network. You can use a package (like [LEAP](https:\/\/pypi.org\/project\/leap-ec\/)), but to keep things simple we'll create a very basic implementation, and define the operators later.\n<p>\nGA works by keeping a population of candidate solutions. At each generation, the candidates are evaluated by some *fitness* measure. Depending on the scheme, some less-fit candidates die off, and the best performing ones get to procreate. Their children will accumulate random mutations and the process repeats. \n<p>\nOur algorithm will evolve the policy network (or more specifically, it's parameters). The genetic operators will be described a bit later. \nNote that the paper - and thus, this notebook - does not use crossover (sexual reproduction) and the learning is done only through mutations. The reason behind this decision is to be able to represent each individual as the seeds for the random noise (the mutation function), which will compress the representation and allow for easier parallelization. To keep things simple, we will not use this representation scheme so refer to paper to learn more!\n<p>\n There are many different flavors to this algorithm and we will not cover them all here, so feel free to try things yourself!\n    ","21196961":"Let's see that it works","287d3b76":"Let's run the an experiment again, but this time with NS:","2a4175c6":"This simple implementation easily solves simple environments. However, in some environments the reward signal is deceptive and might lead the algorithm to a local optimum. For example, in the [MountainCar](https:\/\/gym.openai.com\/envs\/MountainCar-v0\/) environment, getting to the destination is difficult, and all agents that don't get the destination get the same reward (-200). So, an agent that \"almost\" gets there is equivalent with an agent that isn't even close - how can we learn from this reward signal? Worse yet, in the [LunarLander](https:\/\/gym.openai.com\/envs\/LunarLander-v2\/) environment, a policy that crashes straight down will get some points for landing in the landing pad. However, slighly using an engine will still cause the lander to crash, but this time outside the landing pad! So, learning to use the engines actually reduces fitness, and the learning converges to a sub-optimal solution!\n<p>\nWe can thus conclude that unless we are close to the solution, the reward function is misleading. The idea behind novelty search is that if the reward function does not guide us to the solution, it should be discarded. Instead, we measure the population based on novelty - how \"different\" it performed from other agents. We thus introduce a $\\text{behavior characteristic}$ ($BC$ for short) that we can compare. Note that this $BC$ is domain specific. We keep an archive that holds past $BC$s, and randomly append to it. Each agent's $BC$ will be compared to the archived $BC$s, and the $BC$s of the other agents in this generation.\n<p>\nRemember - there are only so-many unique ways to fall down before the only innovation possible is to walk properly!"}}