{"cell_type":{"4d7580cf":"code","ebd09d3b":"code","ec01a8c1":"code","ab687ebb":"code","6b8e6030":"code","c03b1f9b":"code","824c0bd0":"code","eeb64cf1":"code","7ec3aeff":"code","9b4a56fe":"code","62c7001b":"code","eaae481c":"code","c8281324":"code","240b3ba1":"code","2d077733":"code","d4aaa794":"code","fee72b27":"code","0ea3cd6e":"markdown","d7732c0d":"markdown","21e17cdd":"markdown","1410852e":"markdown","422d8ffa":"markdown","63bb601b":"markdown","b1afd6ec":"markdown","1d51694f":"markdown","80edd18f":"markdown"},"source":{"4d7580cf":"import os\nimport gc\ngc.enable()\nimport time\nimport random\nimport warnings\n\nimport feather\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nwarnings.filterwarnings('ignore')\n\nSEED = 42\nnp.random.seed(SEED)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\n\nsns.set_style(\"darkgrid\")\nmpl.rcParams['figure.dpi'] = 600\n%matplotlib inline","ebd09d3b":"%%time\ntrain_df = feather.read_dataframe('..\/input\/tpsnov21\/train.feather')\ntest_df = feather.read_dataframe('..\/input\/tpsnov21\/test.feather')\n\nprint('Quick view of training data: ')\ntrain_df.head()","ec01a8c1":"TARGET = 'target'\nFEATURES = [col for col in train_df.columns if col not in ['id', TARGET]]\nprint(f'Training data:\\n\\t Number of rows: {train_df.shape[0]}, Number of columns: {train_df.shape[1]}')\nprint(f'Testing data:\\n\\t Number of rows: {test_df.shape[0]}, Number of columns: {test_df.shape[1]}')","ab687ebb":"print('Basic statistics of training data:')\ntrain_df[FEATURES+[TARGET]].describe()","6b8e6030":"print('Basic statistics of testing data:')\ntest_df[FEATURES].describe()","c03b1f9b":"print(f'Number of missing values in training data: {train_df.isna().sum().sum()}')\nprint(f'Number of missing values in testing data: {test_df.isna().sum().sum()}')","824c0bd0":"print(\"Feature distribution: \")\nncols = 5\nnrows = int(len(FEATURES) \/ ncols + (len(FEATURES) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 80), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = FEATURES[r*ncols+c]\n        sns.kdeplot(x=train_df[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.kdeplot(x=test_df[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","eeb64cf1":"print(\"Target Distribution: \")\n\ntarget_df = pd.DataFrame(train_df[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\n\nfig, ax = plt.subplots(1, 1, figsize=(25, 8), facecolor='#EAEAF2')\nsns.barplot(y=TARGET, x='count', data=target_df, palette=['#58D68D', '#DE3163'], ax=ax, orient='h')\nax.set_xlabel('Count', fontsize=16)\nax.set_ylabel('Target', fontsize=16)\nplt.show()","7ec3aeff":"for col in tqdm(FEATURES.copy()):\n    train_df[col + '_bin'] = train_df[col].apply(lambda x: 1 if np.cbrt(x) > 0 else 0)\n    test_df[col + '_bin'] = test_df[col].apply(lambda x: 1 if np.cbrt(x) > 0 else 0)\n    FEATURES.append(col + '_bin')","9b4a56fe":"train_df[\"mean\"] = train_df[FEATURES].mean(axis=1)\ntrain_df[\"std\"] = train_df[FEATURES].std(axis=1)\ntrain_df[\"min\"] = train_df[FEATURES].min(axis=1)\ntrain_df[\"max\"] = train_df[FEATURES].max(axis=1)\n\ntest_df[\"mean\"] = test_df[FEATURES].mean(axis=1)\ntest_df[\"std\"] = test_df[FEATURES].std(axis=1)\ntest_df[\"min\"] = test_df[FEATURES].min(axis=1)\ntest_df[\"max\"] = test_df[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","62c7001b":"train_df.head()","eaae481c":"def format_time(seconds):\n    \"\"\"\n    Formates time in human readable form\n\n    Args:\n        seconds: seconds passed in a process\n    Return:\n        formatted string in form of MM:SS or HH:MM:SS\n    \"\"\"\n    h = int(seconds \/\/ 3600)\n    m = int((seconds % 3600) \/\/ 60)\n    s = int(seconds % 60)\n    result = ''\n    _h = ('0' + str(h)) if h < 10 else str(h)\n    result += (_h + ' hr ') if h > 0 else ''\n    _m = ('0' + str(m)) if m < 10 else str(m)\n    result += (_m + ' min ') if m > 0 else ''\n    _s = ('0' + str(s)) if s < 10 else str(s)\n    result += (_s + ' sec')\n    return result","c8281324":"scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nfor col in FEATURES:\n    train_df[col] = scaler.fit_transform(train_df[col].to_numpy().reshape(-1,1))\n    test_df[col] = scaler.transform(test_df[col].to_numpy().reshape(-1,1))\n    \nX = train_df[FEATURES].to_numpy().astype(np.float32)\nY = train_df[TARGET].to_numpy().astype(np.float32)\nX_test = test_df[FEATURES].to_numpy().astype(np.float32)\n\ndel train_df, test_df\ngc.collect()","240b3ba1":"from tensorflow.keras import layers\nimport tensorflow as tf\n\ndef create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURES:\n        inputs[feature_name] = layers.Input(\n            name=feature_name, shape=(), dtype=tf.float32\n        )\n    return inputs\n\ndef encode_inputs(inputs, encoding_size):\n    encoded_features = []\n    for i in range(inputs.shape[1]):\n        encoded_feature = tf.expand_dims(inputs[:, i], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features\n\nclass GatedLinearUnit(layers.Layer):\n    def __init__(self, units):\n        super(GatedLinearUnit, self).__init__()\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)\n    \nclass GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate=0.15):\n        super(GatedResidualNetwork, self).__init__()\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"swish\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x\n\nclass VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate):\n        super(VariableSelection, self).__init__()\n        self.grns = list()\n        # Create a GRN for each feature independently\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        # Create a GRN for the concatenation of all the features\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, input in enumerate(inputs):\n            x.append(self.grns[idx](input))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs\n    \ndef create_model(encoding_size, dropout_rate=0.15):\n    inputs = layers.Input(len(FEATURES))\n    feature_list = encode_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n        feature_list\n    )\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","2d077733":"from collections import defaultdict\nimport tensorflow as tf\n\noof_df = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nN_FOLDS = 5\nENCODING_SIZE = 32\nEPOCHS = 500\nVERBOSE = 1\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print(f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    \n    tic = time.time()\n    \n#     clf = tf.keras.Sequential([\n#         tf.keras.layers.Input(len(FEATURES)),\n#         tf.keras.layers.Dense(256, activation='swish'),\n#         tf.keras.layers.Dropout(0.3),\n#         tf.keras.layers.Dense(128, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(64, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(32, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(16, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(1, activation='sigmoid'),\n#     ])\n    clf = create_model(ENCODING_SIZE)\n    \n    clf.compile(loss='binary_crossentropy', \n                optimizer='adam', \n                metrics=[tf.keras.metrics.AUC(name='auc'), 'acc'])\n    \n    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, \n                               patience=4, verbose=VERBOSE)\n\n    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, \n                       verbose=VERBOSE, mode=\"min\", \n                       restore_best_weights=True)\n    \n    clf.fit(x_train, y_train, \n            epochs=EPOCHS, batch_size=2048,\n            validation_data=(x_val, y_val),\n            validation_batch_size=len(y_val),\n            callbacks=[es, lr],\n            shuffle=True,\n            verbose=VERBOSE)\n    \n    preds = np.squeeze(clf.predict(x_val, batch_size=len(y_val)))\n    oof_df[f'nn'].extend(preds)\n    test_df[f'nn'] += (np.squeeze(clf.predict(X_test, batch_size=2048) \/ N_FOLDS))\n\n    score = metrics.roc_auc_score(y_val, preds)\n    print(f\"MODEL: nn\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n\n    del clf\n    gc.collect()\n        \n    del x_train, x_val, y_train, y_val\n    gc.collect()\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","d4aaa794":"score = metrics.roc_auc_score(oof_df[TARGET], oof_df['nn'])\nprint(f'Overall ROC AUC of: {score}')","fee72b27":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsubmission[TARGET] = test_df['nn']\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","0ea3cd6e":"## Data","d7732c0d":"## Data preprocessing","21e17cdd":"## Utils","1410852e":"## Submission","422d8ffa":"## Imports","63bb601b":"## Modeling","b1afd6ec":"Here we use Gated Residual Networks (GRN) and Variable Selection Networks (VSN), proposed by Bryan Lim et al. in [Temporal Fusion Transformers (TFT) for Interpretable Multi-horizon Time Series Forecasting](https:\/\/arxiv.org\/abs\/1912.09363), for structured data classification. GRNs give the flexibility to the model to apply non-linear processing only where needed. VSNs allow the model to softly remove any unnecessary noisy inputs which could negatively impact performance. Together, those techniques help improving the learning capacity of deep neural network models.\n\n![image.png](attachment:2a3641d6-fc08-4daf-a5a7-613da4bc723a.png)\n\nImplementation is taken from [here](https:\/\/keras.io\/examples\/structured_data\/classification_with_grn_and_vsn\/)","1d51694f":"## Feature Engineering","80edd18f":"## Introduction\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated on area under the ROC Curve between the predicted probability and target."}}