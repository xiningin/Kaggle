{"cell_type":{"3f6b40a4":"code","74860c32":"code","6de75691":"code","d4213c92":"code","8c566c20":"code","a71e9384":"code","61074979":"code","1ee6d47c":"code","caf7fcb8":"code","b07be868":"code","734c7109":"code","71c5c2de":"code","66c2559a":"code","c725a621":"code","74da39ea":"code","c4103bb6":"code","dda82047":"code","99f7022b":"code","663515a4":"code","76e45435":"code","004004b8":"code","7fec8d55":"code","af613a62":"code","c1b2a411":"code","878c6fa1":"code","aacd4f4c":"code","9f7e6ea1":"code","e19f4463":"code","458a8af3":"code","e6598cdc":"code","3fe037da":"code","e79afdbc":"code","fbabeb9d":"code","980e987a":"code","ffbb40b9":"code","abf79787":"code","7135c2a7":"code","d51af7fa":"code","229b3b0b":"code","3586a751":"code","6c6d55f5":"code","033197b1":"code","bac230c7":"code","7d411309":"code","6e85652e":"code","2717798b":"code","0ac399db":"code","41bf1229":"code","c9b80e76":"code","e0faca8f":"code","e7686a15":"code","abb750b3":"code","889c2553":"code","d0ed456d":"code","b23f5728":"code","89472431":"code","39f2fa46":"code","0c8d0ac1":"code","cc6509d0":"code","aad9f0f2":"code","344924e2":"code","92e1ea4f":"code","dcbf1565":"code","539420f5":"code","e203890f":"code","7d8dcbac":"code","6ff30a70":"code","4a009e08":"code","c2238104":"code","f961d2ab":"code","9ece8901":"code","8ae6987f":"code","1119b37a":"code","1b87bdcc":"code","b7b89d5b":"code","bb1b7174":"code","38836e07":"code","7b0215e7":"code","d1313d16":"code","7e3ac9f4":"code","df6b79d7":"code","6028dfbb":"code","32d84e74":"code","dd18de85":"code","8f0f96bc":"code","57c640ac":"code","639e07e9":"code","00f4847b":"code","ebef4ef9":"code","fba58c02":"code","74e17e98":"code","20d7c038":"code","88fc6aa0":"markdown","1f7ba726":"markdown","ece9edc5":"markdown","20562cb9":"markdown","7c93b0d8":"markdown","61bbd14e":"markdown","ead45fb1":"markdown","6d4b3e55":"markdown"},"source":{"3f6b40a4":"# Import packages and read in data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom ast import literal_eval\nfrom datetime import datetime\n\ntrain = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\nsub=test.copy()","74860c32":"print (train.shape)\nprint (test.shape)","6de75691":"train.isnull().sum()","d4213c92":"test.isnull().sum()","8c566c20":"data=train.append(test)\ndata.reset_index(inplace=True)\ndata.drop('index',axis=1,inplace=True)\ndata.head(3)","a71e9384":"data.shape","61074979":"data.describe(include='all')","1ee6d47c":"data[data['revenue']==data['revenue'].max()]","caf7fcb8":"data[data['budget']==data['budget'].max()]","b07be868":"data.head()","734c7109":"# Let's see how much missing data we have here first\ndata.isnull().sum()","71c5c2de":"# There are severl columns with high percentage of missing values. I may consider to drop them afterward.\nprint ('The percentage of missing value of each column:')\nprint ('*'*50)\nprint (round(data.isnull().sum()\/data.shape[0]*100,2))","66c2559a":"# For some categorical data, I'd like to fill in na with mode. I check the mode of them first to see if it is reasonable.\nprint (data['genres'].mode()[0])\nprint (data['spoken_languages'].mode()[0])\nprint (data['production_companies'].mode()[0])\nprint (data['production_countries'].mode()[0])","c725a621":"# There is only one missing value in 'release_date'. I'll google it and fill na manually. \ndata[data['release_date'].isnull()]","74da39ea":"# The most ideal way to fill in missing value of 'spoken_language' is to use 'original_language'. However, the data \n# format is quite different, may lead to redundant process. There are only 7 of them are not English. So I'll stick\n# to using mode to fill na for 'spoken_languages'.\ndata[data['spoken_languages'].isnull()]['original_language'].value_counts()","c4103bb6":"# Since I will convert 'cast' and 'crew' into no. of cast\/crew after feature engineering, I just fill in 0 here. \ndata['cast'].fillna('0',inplace=True)\ndata['crew'].fillna('0',inplace=True)\n\n# As mentioned before, these four columns fill in with mode.\ndata['genres'].fillna(data['genres'].mode()[0],inplace=True)\ndata['production_countries'].fillna(data['production_countries'].mode()[0],inplace=True)\ndata['production_companies'].fillna(data['production_companies'].mode()[0],inplace=True)\ndata['spoken_languages'].fillna(data['spoken_languages'].mode()[0],inplace=True)\n\n# Google says this movie's release date is 3\/20\/01. I choose to believe Google.\ndata['release_date'].fillna('3\/20\/01',inplace=True)\n\n# For the continuous variable, fill in with mean value.\ndata['runtime'].fillna(data['runtime'].mean(),inplace=True)\n\n# Just using 'original_title' to fill in 'title'.\ndata['title'].fillna(data['original_title'],inplace=True)","dda82047":"# Beautiful!! We have sorted most of the missing values.\ndata.isnull().sum()","99f7022b":"# Convert 'belongs_to_collection' to binary value: is or is not serial movie.\ndata['belongs_to_collection'].fillna(0,inplace=True)\ndata['belongs_to_collection']=data['belongs_to_collection'].apply(lambda x:1 if x!=0 else x)","663515a4":"# Almost all of movies are released. This variable maybe is not so useful.\ndata['status'].value_counts()","76e45435":"# I'm not gonna dig into analysing 'words' stuffs. These are the variables I'm not gonna use in my model. \nnotusing=['Keywords',\n         'homepage',\n         'id',\n         'imdb_id',\n         'original_language',\n         'original_title',\n         'overview',\n         'poster_path',\n         'status',\n         'tagline']\ndata.drop(notusing,axis=1,inplace=True)","004004b8":"data.head()","7fec8d55":"# Now let's create some functions dealing with the columns with json-like values.\ndef find_name(string):\n    s=eval(string) # list of dict\n    l=[]\n    for i in s:\n        l.append(i['name'])\n    return l\n\ndef find_language(string):\n        t=eval(string)\n        l=[]\n        for i in t:\n            l.append(i['iso_639_1'])\n        return l\n\ndef find_actors(string):\n    if eval(string)==0:\n        return 0\n    else:\n        t=eval(string)\n        l=[]\n        for i in t:\n            l.append(i['name'])\n        return l","af613a62":"# Apply the functions to those json-like columns.\ndata['cast']=data['cast'].apply(find_actors)\ndata['crew']=data['crew'].apply(find_actors)\ndata['genres']=data['genres'].apply(find_name)\ndata['production_companies']=data['production_companies'].apply(find_name)\ndata['production_countries']=data['production_countries'].apply(find_name)\ndata['spoken_languages']=data['spoken_languages'].apply(find_language)\n\n# I converted 'cast' and 'crew' into the no. of cast\/crew, which is doable after the previous process.\ndata['no_of_cast']=data['cast'].apply(lambda x:len(x) if x!=0 else 0)\ndata['no_of_crew']=data['crew'].apply(lambda x:len(x) if x!=0 else 0)\n\ndata.drop(['cast','crew'],axis=1,inplace=True)\n\ndata.head()","c1b2a411":"# Most of the movies containing 1 to 3 genres. Some have more. \nprint ('Movies with each no. of genres')\nprint ('*'*50)\nprint (data['genres'].apply(lambda x:len(x)).value_counts())","878c6fa1":"# Convert the 'genres' into dummy variables.\n# The logic behind this transformation can be found here. https:\/\/stackoverflow.com\/questions\/29034928\/pandas-convert-a-column-of-list-to-dummies\n# It is quite clear to me, doing me a huge favor.\ndata=pd.get_dummies(data['genres'].apply(pd.Series).stack()).sum(level=0).merge(data,left_index=True,right_index=True)","aacd4f4c":"data.head()","9f7e6ea1":"# Firtly, calculate the mean budget of each genre.\n\nlist_of_genres=[]\nfor i in data['genres']:\n    for j in i:\n        if j not in list_of_genres:\n            list_of_genres.append(j)\n\nd={}\nfor i in list_of_genres:\n    genre=i\n    mean_budget=data.groupby(i)['budget'].mean()\n    d[genre]=mean_budget[1]\n    \npd.Series(d).sort_values()","e19f4463":"# This part is just for inspection. To see how many countries\/companies\/languages in total\nlist_of_companies=[]\nfor i in data['production_companies']:\n    for j in i:\n        if j not in list_of_companies:\n            list_of_companies.append(j)\n\nlist_of_countries=[]\nfor i in data['production_countries']:\n    for j in i:\n        if j not in list_of_countries:\n            list_of_countries.append(j)\nlen(list_of_countries)\n\nlist_of_language=[]\nfor i in data['spoken_languages']:\n    for j in i:\n        if j not in list_of_language:\n            list_of_language.append(j)\nlen(list_of_language)\n\nprint ('The total number of company occurs is {}'.format(len(list_of_companies)))\nprint ('The total number of country occurs is {}'.format(len(list_of_countries)))\nprint ('The total number of language occurs is {}'.format(len(list_of_language)))","458a8af3":"# Replace the 0 budget with nan.\ndata['budget'].replace(0,np.nan,inplace=True)\ndata[data['budget'].isnull()][['budget','genres']].head(10)","e6598cdc":"# This function will calculate the mean budget value of that movie.\n# For example, for the index 4 movie, the function will calculate the mean of the mean budget of Action and the mean\n# budget of Thriller. \n\ndef fill_budget(l):\n    el=[]\n    for i in l:\n        if d[i] not in el:\n            el.append(d[i])\n    return (np.mean(el))","3fe037da":"data['budget'].fillna(data['genres'].apply(fill_budget),inplace=True)","e79afdbc":"# Most of the movies are produced by 1 to 3 companies. Some have more. \nprint ('Movies with each no. of production company')\nprint ('*'*50)\ndata['production_companies'].apply(lambda x:len(x)).value_counts()","fbabeb9d":"# Most of the movies was shoot in under 2 countries. Some have more. \nprint ('Movies with each no. of production_countries')\nprint ('*'*50)\ndata['production_countries'].apply(lambda x:len(x)).value_counts()","980e987a":"# Surprisingly, the budget doesn't have much to do with how many countries the movie was shoot, but how many companies\n# involved.\n\ndata['no_of_country']=data['production_countries'].apply(lambda x:len(x))\ndata['no_of_company']=data['production_companies'].apply(lambda x:len(x))\ndata[['budget','no_of_country','no_of_company']].corr()","ffbb40b9":"data['release_date'].head()","abf79787":"# If we just apply the datetime function of panda, there will be some year like 2048, 2050, 2072 happen. I'm pretty\n# sure the time traveling has not been invented yet. The year has to be handled first. If it is greater than 18, it \n# must be 19xx.\n\ndef fix_year(x):\n    year=x.split('\/')[2]\n    if int(year)>18:\n        return x[:-2]+'20'+year\n    else:\n        return x[:-2]+'19'+year\ndata['release_date']=data['release_date'].apply(fix_year)\ndata['release_date']=pd.to_datetime(data['release_date'],infer_datetime_format=True)","7135c2a7":"# There still might be some ambiguities like 11, 15, 09. How does computer know it refers to 2011 or 1911? It don't. \n# So eventually I decided not to use the 'year'. But the month and date and weekday can still be really informative. \n\n#data['year']=data['release_date'].dt.year\ndata['month']=data['release_date'].dt.month\ndata['day']=data['release_date'].dt.day\ndata['weekday']=data['release_date'].dt.weekday\n\n# Mapping weekday and month.\ndata['weekday']=data['weekday'].map({0:'Mon',1:'Tue',2:'Wed',3:'Thur',4:'Fri',5:'Sat',6:'Sun'})\ndata['month']=data['month'].map({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})","d51af7fa":"data[['release_date','month','day','weekday']].head()","229b3b0b":"data.drop(['release_date'],axis=1,inplace=True)\ndata.iloc[:5,20:]","3586a751":"# There are nearly 90 movies are English. \n# I'd like to know will the movie is\/is not foreign language affect the revenue?\nl=[]\nfor i in data['spoken_languages']:\n    if 'en' in i:\n        l.append(i)\n\nlen(l)\/data.shape[0]","6c6d55f5":"# Convert 'spoken_languages' into binary variable 'language_en'.\ndef en_or_not(l):\n    if 'en' in l:\n        return 1\n    else:\n        return 0\ndata['language_en']=data['spoken_languages'].apply(en_or_not)\ndata.drop('spoken_languages',axis=1,inplace=True)","033197b1":"# Same situation in 'production_countries'. Nearly 80% movies were shoot in USA.\nu=[]\nfor i in data['production_countries']:\n    if 'United States of America' in i:\n        u.append(i)\n        \nlen(u)\/data.shape[0]","bac230c7":"# Convert 'production_countries' into binary variable 'produce_in_USA'\ndef usa_or_not(l):\n    if 'United States of America' in l:\n        return 1\n    else:\n        return 0\ndata['produce_in_USA']=data['production_countries'].apply(usa_or_not)\ndata.drop('production_countries',axis=1,inplace=True)","7d411309":"top_company=pd.read_csv('..\/input\/top-company-list\/top_company.csv')\ntop_company=top_company['Production Company'].tolist()\ntop_company[:5]","6e85652e":"data.iloc[:,20:].head()","2717798b":"#data['top_director']=data['director'].apply(lambda x:1 if x in top_director else 0)\ndef get_top_company(l):\n    n=0\n    for i in l:\n        if i in top_company:\n            n+=1\n    return n\n\ndata['top_production_company']=data['production_companies'].apply(get_top_company)\ndata.drop('production_companies',axis=1,inplace=True)","0ac399db":"# To avoid the model being affected by the scale of each variable, normalise the continuous variable.\n\ndata['budget']=data['budget'].apply(lambda x:(x-np.min(data['budget']))\/(np.max(data['budget']-np.min(data['budget']))))\ndata['popularity']=data['popularity'].apply(lambda x:(x-np.min(data['popularity']))\/(np.max(data['popularity']-np.min(data['popularity']))))\ndata['runtime']=data['runtime'].apply(lambda x:(x-np.min(data['runtime']))\/(np.max(data['runtime']-np.min(data['runtime']))))","41bf1229":"# Set the index to movie title, and we are ready to go!!\n\ndata.set_index('title',inplace=True)\ndata.head()","c9b80e76":"plt.figure(figsize=(20,12))\nplt.title('Violin plot of revenue of each month',fontsize=20)\nsns.violinplot(x=data[data['revenue'].notnull()]['month'],y=data[data['revenue'].notnull()]['revenue'],scale='count')","e0faca8f":"plt.figure(figsize=(20,12))\nplt.title('Violin plot of revenue of each weekday',fontsize=20)\nsns.violinplot(x=data[data['revenue'].notnull()]['weekday'],y=data[data['revenue'].notnull()]['revenue'],scale='count')","e7686a15":"data.drop('genres',axis=1,inplace=True)\n\n# Month and weekday need to be converted to dummy variables.\ndata=pd.get_dummies(data)","abb750b3":"data.iloc[:5,20:29]","889c2553":"# For the EDA, I only use the training dataset.\n\nTrain=data[data['revenue'].notnull()]\nTrain.head(5)","d0ed456d":"# Does a movie is serial or not matter?\nprint (Train.groupby('belongs_to_collection')['revenue'].mean())\nTrain.groupby('belongs_to_collection')['revenue'].mean().plot.barh()","b23f5728":"Train['belongs_to_collection'].value_counts()","89472431":"sns.swarmplot(x=Train['belongs_to_collection'],y=Train['revenue'])","39f2fa46":"list_of_genres","0c8d0ac1":"# Similar to creating a series of mean budget of each genre, here we create 'revenue'.\n\ng={}\nfor i in list_of_genres:\n    mean_rev=Train.groupby(i)['revenue'].mean()\n    g[i]=mean_rev[1]\n\ng","cc6509d0":"plt.figure(figsize=(20,8))\npd.Series(g).sort_values().plot.barh()\nplt.title('Mean revenue of each genre',fontsize=20)\nplt.xlabel('Revenue',fontsize=20)","aad9f0f2":"print (pd.DataFrame(Train.groupby('language_en')['revenue'].mean()))\n\nplt.figure(figsize=(10,4))\nTrain.groupby('language_en')['revenue'].mean().sort_values().plot.barh()\nplt.title('Mean revenue of is or is not foreign film.',fontsize=20)\nplt.xlabel('Revenue',fontsize=20)","344924e2":"plt.figure(figsize=(10,4))\nsns.swarmplot(x=Train['language_en'],y=Train['revenue'])\nplt.title('Swarm plot of is or is not foreign film',fontsize=20)","92e1ea4f":"print (pd.DataFrame(Train.groupby('produce_in_USA')['revenue'].mean()))\n\nplt.figure(figsize=(10,4))\nTrain.groupby('produce_in_USA')['revenue'].mean().sort_values().plot.barh()\nplt.title('Mean revenue of shoot in USA or not')\nplt.xlabel('revenue')","dcbf1565":"plt.figure(figsize=(10,4))\nsns.swarmplot(x=Train['produce_in_USA'],y=Train['revenue'])\nplt.title('Swarm plot of movie produced in USA or not',fontsize=20)","539420f5":"plt.figure(figsize=(10,4))\nplt.title('Mean revenue of each no. of top production company',fontsize=20)\nTrain.groupby('top_production_company')['revenue'].mean().plot.bar()\nplt.xlabel('No. of top production company',fontsize=20)\nplt.ylabel('Revenue',fontsize=20)","e203890f":"plt.figure(figsize=(10,4))\nplt.title('Swarm plot of mean revenue of each no. of top production company',fontsize=20)\nplt.xlabel('No. of top production company',fontsize=20)\nplt.ylabel('Revenue',fontsize=20)\n\nsns.swarmplot(x=Train['top_production_company'],y=Train['revenue'])","7d8dcbac":"plt.figure(figsize=(8,8))\nplt.scatter(Train['runtime'],Train['revenue'])\nplt.title('Scatter plot of runtime vs revenue',fontsize=20)\nplt.xlabel('runtime',fontsize=20)\nplt.ylabel('Revenue',fontsize=20)","6ff30a70":"plt.figure(figsize=(8,8))\nplt.scatter(Train['budget'],Train['revenue'])\nplt.title('Scatter plot of budget vs revenue',fontsize=20)\nplt.xlabel('budget',fontsize=20)\nplt.ylabel('Revenue',fontsize=20)","4a009e08":"plt.figure(figsize=(8,8))\nplt.scatter(Train['popularity'],Train['revenue'])\nplt.title('Scatter plot of popularity vs revenue',fontsize=20)\nplt.xlabel('popularity',fontsize=20)\nplt.ylabel('Revenue',fontsize=20)","c2238104":"month=['Jan','Feb','Mar','Apr','May','Jun','July','Aug','Sep','Oct','Nov','Dec']\nm={}\nfor i in month:\n    mean=Train.groupby('month_'+i)['revenue'].mean()\n    m[i]=mean[1]\npd.Series(m)","f961d2ab":"for i in month:\n    print (i,Train['month_'+i].value_counts()[1])","9ece8901":"plt.figure(figsize=(20,8))\npd.Series(m).plot.bar()\nplt.title('Mean revenue of each month',fontsize=20)\nplt.xlabel('Revenue',fontsize=20)","8ae6987f":"plt.figure(figsize=(20,8))\nTrain.groupby('day')['revenue'].mean().sort_values().plot.bar()\nplt.title('Mean revenue of each day',fontsize=20)\nplt.xlabel('Revenue',fontsize=20)","1119b37a":"w={}\nweekday=['Mon','Tue','Wed','Thur','Fri','Sat','Sun']\nfor i in weekday:\n    mean=Train.groupby('weekday_'+i)['revenue'].mean()\n    w[i]=mean[1]\nw","1b87bdcc":"for i in weekday:\n    print (i,Train['weekday_'+i].value_counts()[1])","b7b89d5b":"plt.figure(figsize=(20,8))\npd.Series(w).plot.bar()\nplt.title('Mean revenue of each weekday',fontsize=20)\nplt.xlabel('Revenue',fontsize=20)","bb1b7174":"plt.figure(figsize=(10,4))\nsns.distplot(Train['budget'])\nplt.title('Distribution of budget')","38836e07":"plt.figure(figsize=(10,4))\nsns.distplot(Train['revenue'])\nplt.title('Distribution of revenue')","7b0215e7":"plt.figure(figsize=(10,4))\nsns.distplot(data['popularity'])\nplt.title('Distribution of popularity')","d1313d16":"Train.iloc[:5,20:30]","7e3ac9f4":"num_var=['budget','popularity','runtime','no_of_cast','no_of_crew','language_en','produce_in_USA',\n         'revenue','no_of_country','no_of_company','top_production_company','belongs_to_collection']\ncorr_table=Train[num_var].corr()\ncorr_table['revenue'].sort_values(ascending=False)","df6b79d7":"plt.figure(figsize=(10,10))\nsns.heatmap(corr_table,vmax=True,annot=True,square=True,cmap='YlGnBu')\nplt.title('Heatmap of the correlation between each numerical variable')\nsns.set(font_scale=1.5)","6028dfbb":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score, r2_score","32d84e74":"rf = RandomForestRegressor(n_estimators=2000,\n                           min_samples_split=15, \n                           min_samples_leaf=5, \n                           oob_score=True, \n                           n_jobs=-1, \n                           random_state=101)\nX=Train.loc[:,Train.columns!='revenue']\ny=Train['revenue']\nX_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=101)","dd18de85":"rf.fit(X_train,y_train)","8f0f96bc":"pred = rf.predict(X_test)\nexplained_variance_score(y_test,pred)","57c640ac":"pd.DataFrame({'variable':X.columns.tolist(),\n              'importance':rf.feature_importances_}).sort_values(by='importance',\n                                                                 ascending=False).head(10)","639e07e9":"rf.oob_score_","00f4847b":"r2_score(y_test,pred)","ebef4ef9":"Test=data[data['revenue'].isnull()]","fba58c02":"Test['revenue']=rf.predict(Test.loc[:,Test.columns!='revenue']).astype(int)","74e17e98":"sub['revenue']=rf.predict(Test.loc[:,Test.columns!='revenue']).astype(int)","20d7c038":"sub=sub[['id','revenue']]\nsub.to_csv('submission.csv',index=False)","88fc6aa0":"### Deal with release date","1f7ba726":"# Feature Engineering","ece9edc5":"### The way I fill in missing value in 'budget', which is 0, is fill in with the mean budget of the genres that movie contains. I calculate mean budget of each genre and then put it back to the missing budget. ","20562cb9":"### All done!!! We're good to do some exploratory analysis!","7c93b0d8":"# Machine Learning Model","61bbd14e":"## Normalisation","ead45fb1":"### Data Descriptive Analysis","6d4b3e55":"# EDA"}}