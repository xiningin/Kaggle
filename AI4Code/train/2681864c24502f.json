{"cell_type":{"a96f2693":"code","49ac7656":"code","474632ee":"code","1dbaa483":"code","4dcda9a8":"code","3221b842":"code","606d224b":"code","2f22d0a8":"code","0e4bb5bd":"code","1278ad3a":"code","88df867a":"code","3f6247bf":"code","4a9e69d9":"code","a18f63c2":"code","d176b2fb":"code","f104c322":"code","eed8f798":"code","48ecdba6":"code","73ce5800":"code","d3c53cd9":"code","c62c7a11":"code","ae88fb46":"code","85a87dbf":"code","81d1f554":"code","15e1bea0":"code","0b9f1258":"code","525b133c":"code","9ac84a34":"code","46be306e":"markdown","e1e4e942":"markdown","d1a23573":"markdown","2c97e1c7":"markdown","4d976640":"markdown","d05de405":"markdown","77513388":"markdown","60795baf":"markdown","27aac7fe":"markdown","1f882d45":"markdown","78073f49":"markdown","7451e5c9":"markdown","99111ce5":"markdown","4bcd307e":"markdown","f08d6e69":"markdown","cef4e46f":"markdown","baa0439c":"markdown","6479816f":"markdown","d0f2ec28":"markdown","14e5f05e":"markdown","a2043c11":"markdown","0f6d2c3a":"markdown","35ff873a":"markdown","ec5f1f08":"markdown"},"source":{"a96f2693":"# Import various libraires \nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score, GridSearchCV,RandomizedSearchCV\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport tensorflow as tf\n\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import precision_recall_curve\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.base import BaseSampler\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n                                     NearMiss,\n                                     InstanceHardnessThreshold,\n                                     CondensedNearestNeighbour,\n                                     EditedNearestNeighbours,\n                                     RepeatedEditedNearestNeighbours,\n                                     AllKNN,\n                                     NeighbourhoodCleaningRule,\n                                     OneSidedSelection)\n","49ac7656":"cc_dataset = pd.read_csv(\"..\/input\/creditcard.csv\")","474632ee":"cc_dataset.head()","1dbaa483":"cc_dataset.describe()","4dcda9a8":"cc_dataset.isnull().sum()","3221b842":"print (\"Fraud\")\nprint (cc_dataset.Time[cc_dataset.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (cc_dataset.Time[cc_dataset.Class == 0].describe())","606d224b":"#Data Visualization for checking the distribution for Genuine cases & Fraud cases for each feature\n\nv_feature = cc_dataset.iloc[0:30]\nplt.figure(figsize=(15,18))\nplt.suptitle('Distribution of Features - Genuine vs Fraud Class',fontsize=14)\ngs = gridspec.GridSpec(6,5)\n\nfor i, col in enumerate(v_feature):\n    j = i%5\n    k = int(i\/5)\n    if i >= 30:\n        break\n    ax = plt.subplot(gs[k, j])\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==0],hist=True, kde=True, \n                 bins=30,kde_kws={'linewidth': 2},color='green',label='Genuine Class')\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==1],hist=True, kde=True, \n                 bins=30,kde_kws={'linewidth': 2},color='red',label='Fraud Class')\n\nhandles, labels = ax.get_legend_handles_labels()\nplt.figlegend(labels, loc='upper right')\nplt.tight_layout(pad=4)\nplt.show()","2f22d0a8":"print(cc_dataset.groupby('Class')['Amount'].describe())","0e4bb5bd":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 30\n\nax1.hist(cc_dataset.Amount[cc_dataset.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(cc_dataset.Amount[cc_dataset.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","1278ad3a":"cc_dataset.groupby('Class')['Amount'].apply(lambda x: x[x >= 105].count())","88df867a":"X_orig = cc_dataset.iloc[:,0:-1]\ny_orig = cc_dataset['Class']\nX_orig.columns\n\nfrom sklearn.decomposition import PCA\n\n#Train Test split - By default train_test_split does STRATIFIED split based on label (y-value).\nx_train,x_test,y_train,y_test = train_test_split(X_orig,y_orig,test_size=0.333,random_state=8125)\n\ntrain = pd.concat([x_train, y_train],axis=1)\ntest = pd.concat([x_test, y_test],axis=1)\n\n# Instanciate a PCA object for the sake of easy visualisation\npca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nX = pca.fit_transform(x_train)\ny = y_train","3f6247bf":"# Original Dataset Plot \n\nf, ax0 = plt.subplots(1, 1)\nc0 = ax0.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class Genuine\",\n                 alpha=0.5)\nc1 = ax0.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class Fraud\",\n                 alpha=0.5)\nax0.set_title('Original set')\n# make nice plotting\nax0.spines['top'].set_visible(False)\nax0.spines['right'].set_visible(False)\nax0.get_xaxis().tick_bottom()\nax0.get_yaxis().tick_left()\nax0.spines['left'].set_position(('outward', 10))\nax0.spines['bottom'].set_position(('outward', 10))\nax0.set_xlim([-75000, 75000])\nax0.set_ylim([-100, 2500])\nplt.tight_layout(pad=2)\nplt.show()","4a9e69d9":"# Common Function for Resampling Techniques\ndef plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[y_res == 0, 0], X_res[y_res == 0, 1], label=\"Genuine Class\", alpha=0.5)\n    ax.scatter(X_res[y_res == 1, 0], X_res[y_res == 1, 1], label=\"Fraud Class\", alpha=0.5)\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    ax.set_xlim([-75000, 75000])\n    ax.set_ylim([-100, 2500])\n    return Counter(y_res)","a18f63c2":"###############################################################################\n# Plots for various Undersampling Techniques\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(12, 8))\n \n\nax_arr = (ax1, ax2, ax3, ax4)\nfor ax, sampler in zip(ax_arr, (\n        RandomUnderSampler(),\n        EditedNearestNeighbours(),\n        RepeatedEditedNearestNeighbours(),\n        AllKNN(allow_minority=True)\n        )):\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title(\"Resampling using {}\".format(\n        sampler.__class__.__name__))\n\nfig.legend((c0, c1), ('Genuine Class', 'Fradulent Class'), loc='lower center',\n         ncol=4, labelspacing=0.1)\n\nplt.tight_layout(pad=2)\nplt.show()","d176b2fb":"# Plot for Oversampling Techniques\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n\n\nax_arr = (ax1, ax2, ax3, ax4)\nfor ax, sampler in zip(ax_arr, (\n        SMOTE(random_state=0),\n        BorderlineSMOTE(random_state=0, kind='borderline-1'),\n        BorderlineSMOTE(random_state=0, kind='borderline-2'),\n        SVMSMOTE(random_state=0),\n        )):\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\n\nfig.legend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n         ncol=4, labelspacing=0.1)\nplt.tight_layout(pad=2)\nplt.show()","f104c322":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n\nax_arr = (ax1, ax2)\nfor ax, sampler in zip(ax_arr, (\n        SMOTETomek(random_state=42),\n        SMOTEENN(random_state=42)\n        )):\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\n\nfig.legend((c0, c1), ('Genuine Class', 'Fradulent Class'), loc='lower center',\n         ncol=4, labelspacing=0.1)\nplt.tight_layout(pad=2)\nplt.show()","eed8f798":"#Drop all of the features that have very similar distributions between the two types of transactions.\ncc_dataset = cc_dataset.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)","48ecdba6":"cc_dataset.head()","73ce5800":"sns.boxplot(x=\"Class\", y=\"Amount\", data=cc_dataset)\ntrain.groupby('Class').Amount.describe()","d3c53cd9":"#split the data set again\ny = cc_dataset['Class']\nX = cc_dataset.drop('Class',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y,random_state=8125)\nX_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.333, stratify=y_train,random_state=8125)\ntrain = pd.concat([X_train, y_train],axis=1)","c62c7a11":"pd.options.mode.chained_assignment = None\ndef set_amount_threshold(x):\n    if x > 2000: return 1\n    else: return 0\n    \nX_train['Large_Amount'] = X_train['Amount'].apply(set_amount_threshold)\nX_validate['Large_Amount'] = X_validate['Amount'].apply(set_amount_threshold)\nX_test['Large_Amount'] = X_test['Amount'].apply(set_amount_threshold)","ae88fb46":"from sklearn.linear_model import LogisticRegressionCV\nlogCV = LogisticRegressionCV(Cs=[0.01,0.1,1,10,100], scoring='average_precision',verbose=0,\n                             max_iter=1000,random_state=4092)\nlogCV.fit(X_train,y_train)\nprint(\"The best parameter C is\",logCV.C_[0])","85a87dbf":"from sklearn.metrics import average_precision_score, precision_recall_curve\npred_prob = logCV.predict_proba(X_validate)\ny_score = pred_prob[:,1]\naverage_precision = average_precision_score(y_validate, y_score)\noriginal_precision, original_recall, original_thresholds = precision_recall_curve(y_validate, y_score)\nplt.step(original_recall, original_precision, color='red', alpha=0.5,linewidth=1.5,label='Original logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1), fontsize = 'large')","81d1f554":"# Resampling with SMOTE\n\nsmt = SMOTE(random_state=4001,kind='regular')\nX_smt_res, y_smt_res = smt.fit_sample(X_train, y_train)\npd.value_counts(y_smt_res)","15e1bea0":"# Apply Logistic regression on the Resampled Dataset\n\nfrom sklearn.linear_model import LogisticRegression\nlog_smt_res = LogisticRegression(C=1.0, verbose=0, random_state=5001)\nlog_smt_res.fit(X_smt_res,y_smt_res)","0b9f1258":"pred_prob = log_smt_res.predict_proba(X_validate)\novsm_score = pred_prob[:,1]\naverage_precision = average_precision_score(y_validate, ovsm_score)\novsm_precision, ovsm_recall, ovsm_thresholds = precision_recall_curve(y_validate, ovsm_score)\nplt.step(original_recall, original_precision, color='red', alpha=0.5,linewidth=1.5,label='Original logistic')\nplt.step(ovsm_recall, ovsm_precision, color='cyan', alpha=0.5,linewidth=1.5,label='SMOTE logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1), fontsize = 'large')","525b133c":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data\ny_predx = model.predict(X_validate)\npredictions = [round(value) for value in y_predx]\n# evaluate predictions\naccuracy = accuracy_score(y_validate, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\ntest = pd.concat([X_test, y_test],axis=1)","9ac84a34":"average_precision = average_precision_score(y_validate, y_predx)\nxgb_precision, xgb_recall, xgb_thresholds = precision_recall_curve(y_validate, y_predx)\nplt.step(original_recall, original_precision, color='red', alpha=0.5,linewidth=1.5,label='Original logistic')\nplt.step(ovsm_recall, ovsm_precision, color='cyan', alpha=0.5,linewidth=1.5,label='SMOTE logistic')\nplt.step(xgb_recall, xgb_precision, color='darkblue', alpha=0.5,linewidth=1.5,label='XBoostClassifier')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1), fontsize = 'large')","46be306e":"## Model Selection \n\nVarious models are trained and we selected the final model based on PR Curves\n\n* Precision-Recall (PR) Curves \u2013 These curves summarize the trade-off between True Positive Rate and the positive predictive value for a predictive model with respect to different probability thresholds. PR curves are generally used for moderate to large class imbalance. ROC curves don\u2019t give a real picture of an algorithm\u2019s performance if the class distribution is imbalanced. PR Curve gives accurate prediction of future classification performance as they evaluate the fraction of true positives with respect to positive predictions ","e1e4e942":"### Simple Logistic Regression with Cross Validation estimator","d1a23573":"XGBoost Classifier gives us an accuracy of 99.95 percent which very good to be accepted as the final Model. PR curve also shows that XGboost classifier has near ideal plot for our dataset. Hence, it is the most effective and accurate classifier even without resampling the input dataset.","2c97e1c7":"## Analysis of the Feature Distribution for identifying the Fraudlent Transactions\n\n* ### Time Feature\n\n    The 'Time' feature looks pretty similar across both types of transactions. But there is significant difference in both types. The fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. Fraudulent transaction numbers are higher at off-hour time. So, we cannot ignore the Time feature.\n    \n    \n* ### Similar Data Distribution for Fraud vs Genuine Class\n\n    Few features like V8, V13, V15, V20, V23, V24, V25, V26, V27 and V28 have very similar distribution for both the classes. These features wont be much useful for us to distinguish between Fraud and Genuine Class.\n    \n    \n* ### Amount Feature\n\n    Amount Feature is one of the most important factors to identify Fradulent Transaction and require further analysis\n    \n    \n","4d976640":"### Undersampling Techniques\n\nHere, the Marjority Class i.e. Genuine Class is undersampled. Few of the techniques evaluated are as mentioned below:\n\n* Random under-sampling under-samples the majority class\/classes randomly with or without replacement. It randomly selects a small sub-set of dataset from the entire population. This can be memory efficient but doesn\u2019t perform well with the actual data as it can ignore useful information.\n* K Nearest Neighbor(kNN) method to undersample based on minority class closet to k majority class or identifying near misses\n* Edited Nearest Neighbors(ENN) removes the majority class data points whose class label differs from the class label of at least two of its three nearest neighbors.\n* Repeated Edited Nearest Neighbors(RENN) applies repeatedly the Edited Nearest Neighbors (ENN) algorithm until all the instances have majority class labeled with same class as their neighbors.  \n","d05de405":"The goal for this analysis is to predict credit card fraud in the transactional data. I will be using tensorflow to build the predictive model. \n\nThe sections of this analysis include: \n\n - Exploring the Credit Card Fraud Dataset (Highly Imbalanced Dataset)\n - Data Cleansing and Visualization\n - Building an effective Model for Credit Card Fraud Detection System\n - Exploring various Model Evaluation techniques","77513388":"## Exploring the Data","60795baf":"The dataset is highly imbalanced with Fraud class consisting of 0.172% of the total transactions","27aac7fe":"### Data Resampling \n\nCredit Card Dataset being highly imbalanced dataset, it is required to resample the dataset so that it can be effectively used for Model training purpose. Here we will first take the PCA for all the input columns and try to visualize the various available Resampling Techniques like Undersampling, Oversampling and Hybrid Sampling techniques","1f882d45":"### PR Curve for the Resampled Logistic Regression","78073f49":"About 75% of the Fraudulent transactions are less than 105.89. \nThe maximum value of Fraudulent Transction is 2125.87 . \nThis means that we can ignore the transactions greater than 2125.87 to be fradulent transactions","7451e5c9":"## Plot for Oversampling Techniques","99111ce5":"# Predicting Credit Card Fraud","4bcd307e":"It clearly shows that Fradulent transactions have maximum amount nearly 2130.00. However, the outliers with higher amount of transaction can be Fraudulent transaction so these cannot be ignored. Hence, we will remap the amount greater than 2130.00 as Class - Fraud","f08d6e69":"The distribution of Fraud and Genuine class clearly makes it difficult to identify the pattern in 2D Plots","cef4e46f":"The Credit Card Fraud Dataset contains numerical input variables which are the result of a PCA transformation due to confidentiality issues. Features V1, V2, ... ,V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. ","baa0439c":"## Plot for Hybrid Samplers","6479816f":"### Inferences from the Various Undersampling Techniques:\n\n* **Random Undersampler** - undersamples the Genuine class to an extent that both Genuine and Fraud Class becomes equally distributed. This techinique might result in overfitting while we train the model.\n\n* **Editted Nearest Neighbour, Repeated Editted Nearest Neighbour & AllkNN Undersamplers** - These undersampling techniques doesnt improve much on the Class Distribution and would not help us during training our model.","d0f2ec28":"### XGBoost Classifier for training the Credit Card Dataset","14e5f05e":"Data has been converted into a balanced Dataset","a2043c11":"### Inference from Oversampling and Hybrid Techniques\n\nThe visualized data clearly shows that resampling using SMOTE, SMOTE + Tomek and SMOTE + ENN have converted the imbalanced Dataset to a balanced Dataset with larger amount of training samples. From our previous runs on various models, we would be using SMOTE as our Resampling techinque before training the model","0f6d2c3a":"### PR Curve Compare for various models","35ff873a":"No missing values, that makes things a little easier.\nLet's see how time compares across fraudulent and normal transactions.","ec5f1f08":"There is slight improvement using SMOTE Logistics in comparision with Original Logistic Regression"}}