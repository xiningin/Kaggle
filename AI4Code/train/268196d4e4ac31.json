{"cell_type":{"6cc06a53":"code","4cc6e774":"code","0b9efbfd":"code","28c3524f":"code","8855931a":"code","5159e32d":"code","69dc1125":"code","b9031c81":"code","fbcbfe1c":"code","6d50cfd5":"code","ada536a0":"code","0bb474a7":"code","dd71b108":"code","318cc10a":"code","e3976b39":"code","be4c17a6":"code","e12d7efa":"code","b6ebba9f":"code","51a49b11":"code","4c4afac8":"code","ad8fb830":"code","5222c59a":"code","db3a51eb":"code","278f378e":"code","f5b7b9bf":"code","029ab421":"code","86a78879":"code","79381442":"code","0a5cf0f4":"code","351dd011":"code","afaa2d8c":"code","91eebf19":"code","b1ae2740":"code","32abe9a4":"code","4546ca5b":"code","54647775":"code","fc824056":"code","39374179":"code","889aec8b":"markdown","65e520a6":"markdown","b2209515":"markdown","84ea9dfb":"markdown","3a8aaf41":"markdown","fdf3b40b":"markdown","f3d6b8d3":"markdown","5d8544ab":"markdown","74cb8914":"markdown","19143b89":"markdown","dbce53ed":"markdown","fe4b67e2":"markdown","f1d2b7de":"markdown","3f278640":"markdown","8220e2e2":"markdown","2a94d2c1":"markdown","1f55d97a":"markdown","c0a3419a":"markdown"},"source":{"6cc06a53":"import pandas as pd\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","4cc6e774":"g1 = pd.read_csv(\"\/kaggle\/input\/solar-power-generation-data\/Plant_1_Generation_Data.csv\")\ng2 = pd.read_csv(\"\/kaggle\/input\/solar-power-generation-data\/Plant_2_Generation_Data.csv\")\nw1 = pd.read_csv(\"\/kaggle\/input\/solar-power-generation-data\/Plant_1_Weather_Sensor_Data.csv\")\nw2 = pd.read_csv(\"\/kaggle\/input\/solar-power-generation-data\/Plant_2_Weather_Sensor_Data.csv\")\n\n# I prefer to work with lowercase column names\nfor df in [g1, g2, w1, w2]:\n    df.columns = [col.lower() for col in df.columns]\n    print(df.info(), \"\\n\")","0b9efbfd":"# convert date-times to pandas datetime format\ng1['date_time']= pd.to_datetime(g1['date_time'],format='%d-%m-%Y %H:%M')\ng2['date_time']= pd.to_datetime(g2['date_time'],format='%Y-%m-%d %H:%M:%S')\nw1['date_time']= pd.to_datetime(w1['date_time'],format='%Y-%m-%d %H:%M:%S')\nw2['date_time']= pd.to_datetime(w2['date_time'],format='%Y-%m-%d %H:%M:%S')\n\nfor df in [g1, g2, w1, w2]:\n    print(df.date_time.head(), \"\\n\")","28c3524f":"# check for null values\nfor df in [g1, g2, w1, w2]:\n    print(df.isna().sum(), \"\\n\")","8855931a":"# there are no nulls, but are there date-times that are completely missing from some sensors?\nCounter(g1.source_key)","5159e32d":"# insert rows for missing date-times to be filled with NAs\n# https:\/\/stackoverflow.com\/questions\/62690513\/python-pandas-insert-rows-for-missing-dates-time-series-in-groupby-dataframe\n\n# store original dataframe shapes to validate row counts after inserting nulls for missing time steps\noriginal_rowcounts = [df.shape[0] for df in [g1, g2, w1, w2]]\n\n# define a function to do the time step insertions\ndef insert_missing_date_times(x):\n    \"\"\"\n    Re-indexes a Pandas series to a date-time sequence with a frequency of 15 minutes.\n    Any missing time steps in the original series index will be inserted.\n    \"\"\"\n    return x.reindex(pd.date_range(x.index.min(), x.index.max(), freq='15min', name='date_time'))\n\n# apply the insertion function to each dataframe\ng1_new = g1.set_index('date_time').groupby('source_key').apply(insert_missing_date_times).drop('source_key', axis=1)\ng1_new.reset_index(inplace=True)\ng2_new = g2.set_index('date_time').groupby('source_key').apply(insert_missing_date_times).drop('source_key', axis=1)\ng2_new.reset_index(inplace=True)\nw1_new = w1.set_index('date_time').groupby('source_key').apply(insert_missing_date_times).drop('source_key', axis=1)\nw1_new.reset_index(inplace=True)\nw2_new = w2.set_index('date_time').groupby('source_key').apply(insert_missing_date_times).drop('source_key', axis=1)\nw2_new.reset_index(inplace=True)\n\n# now that the indices have been updated, it is safe to overwrite the originals and drop the temporary dfs\ng1 = g1_new\ng2 = g2_new\nw1 = w1_new\nw2 = w2_new\ndel g1_new, g2_new, w1_new, w2_new\n\n# verify that the math works out: there should be just as many rows added as missing values\nvalidated_rowcounts = [\n    df.isna().sum().max() + original_rowcounts[idx] == df.shape[0] for idx, df in enumerate(\n        [g1, g2, w1, w2]\n    )\n]\nprint(validated_rowcounts)  # should all be True\n\n# check the number of missing values\nprint(\"\\n\")\nfor df in [g1, g2, w1, w2]:\n    print(df.isna().sum(), \"\\n\")\n","69dc1125":"# prepare the weather data for merging\nfor df in [w1, w2]:\n    df.rename(columns={\"source_key\": \"weather_sensor_key\"}, inplace=True)\n    df.drop(\"plant_id\", axis=1, inplace=True)\n\n# merge the data\nplant1 = pd.merge(g1, w1, how='left', on=['date_time'])\nplant2 = pd.merge(g2, w2, how='left', on=['date_time'])\n\n# count nulls and inspect the data\nprint(plant1.isna().sum(), \"\\n\")\nprint(plant2.isna().sum(), \"\\n\")\n\nplant1.head()","b9031c81":"# before filling null values, figure out when sunrise\/sunset occur\np1_times_of_no_light = plant1[plant1['irradiation']==0.0].groupby('date_time')['source_key'].count().reset_index()\np1_times_of_no_light['hour'] = p1_times_of_no_light['date_time'].dt.hour\n\np2_times_of_no_light = plant2[plant2['irradiation']==0.0].groupby('date_time')['source_key'].count().reset_index()\np2_times_of_no_light['hour'] = p2_times_of_no_light['date_time'].dt.hour\n\nprint(\"plant1 hours of no sunlight \\n\", p1_times_of_no_light.groupby('hour')['source_key'].count(), \"\\n\")\nprint(\"plant2 hours of no sunlight \\n\", p2_times_of_no_light.groupby('hour')['source_key'].count(), \"\\n\")","fbcbfe1c":"# create additional time features\nfor df in [plant1, plant2]:\n    df['date'] = df['date_time'].dt.date\n    df['hour'] = df['date_time'].dt.hour\n    df['day'] = df['date_time'].dt.day\n    df['weekday'] = df['date_time'].dt.day_name()\n    df['month'] = df['date_time'].dt.month\n    df['year'] = df['date_time'].dt.year\n\n# inspect\nplant1.head()","6d50cfd5":"# determine how total yield is calculated so that its nulls can be filled\ngrouped = plant1.groupby(['source_key', 'date']).last().reset_index()\n# this should be mostly True\ngrouped['total_yield'] - grouped['daily_yield'] == grouped['total_yield'].shift(1)","ada536a0":"# fill the null values created by inserting the missing time steps\n\n# start by filling missing values outside the hours of sunlight with 0 (except for temperatures)\n# daily_yield is cumulative, so only fill it before the sun rises\nfor df in [plant1, plant2]:\n    df.loc[(df['dc_power'].isna()) & ((df['hour'] < 5) | (df['hour'] > 18)), 'dc_power'] = 0\n    df.loc[(df['ac_power'].isna()) & ((df['hour'] < 5) | (df['hour'] > 18)), 'ac_power'] = 0\n    df.loc[(df['irradiation'].isna()) & (df['hour'] < 5), 'daily_yield'] = 0\n    df.loc[(df['irradiation'].isna()) & ((df['hour'] < 5) | (df['hour'] > 18)), 'irradiation'] = 0\n\n# fill missing daily_yield after sunset with the last non-null value for that day\nfor df in [plant1, plant2]:\n    fill_values = df[~df['daily_yield'].isna()].groupby([\"source_key\", \"date\"])[\"daily_yield\"].last().reset_index()\n    fill_values.rename(columns={\"daily_yield\": \"daily_yield_fill_value_after_sunset\"}, inplace=True)\n    df = pd.merge(df, fill_values, how='left', on=['source_key', 'date'])\n    df.loc[(df['daily_yield'].isna()) & (df['hour'] > 18), 'daily_yield'] = df.loc[(df['daily_yield'].isna()) & (df['hour'] > 18), 'daily_yield_fill_value_after_sunset']\n    df.drop(\"daily_yield_fill_value_after_sunset\", axis=1, inplace=True)\n\n# linearly interpolate other missing values, but ignore total_yield because it = previous total yield + daily yield\nfor df in [plant1, plant2]:\n    df['dc_power'].interpolate(method='linear', axis=0, inplace=True)\n    df['ac_power'].interpolate(method='linear', axis=0, inplace=True)\n    df['daily_yield'].interpolate(method='linear', axis=0, inplace=True)\n    df['module_temperature'].interpolate(method='linear', axis=0, inplace=True)\n    df['ambient_temperature'].interpolate(method='linear', axis=0, inplace=True)\n    df['irradiation'].interpolate(method='linear', axis=0, inplace=True)\n\n# fill missing total_yield by getting the most recent non-null value and adding the current daily yield\nfor df in [plant1, plant2]:\n    final_non_null_total_yields = df[~df['total_yield'].isna()].groupby([\"source_key\", \"date\"])[\"total_yield\"].last().reset_index()\n    final_daily_yields = df.groupby([\"source_key\", \"date\"])[\"daily_yield\"].last().reset_index()\n    fill_values = pd.merge(final_non_null_total_yields, final_daily_yields, how='left', on=['source_key', 'date'])\n    fill_values['total_yield_fill_value'] = fill_values['total_yield'] + fill_values['daily_yield']\n    df = pd.merge(df, fill_values.drop(['total_yield', 'daily_yield'], axis=1), how='left', on=['source_key', 'date'])\n    df['total_yield'].fillna(df['total_yield_fill_value'], inplace=True)\n    df.drop('total_yield_fill_value', axis=1, inplace=True)\n\n# if there are still any missing total_yields, fill them with the most recent non-null value up to that time\n# also fill missing plant IDs this way, since they are all the same\nfor df in [plant1, plant2]:\n    df.sort_values(['source_key', 'date_time'], ascending=True, inplace=True)\n    df['total_yield'].fillna(method='ffill', inplace=True)\n    df['plant_id'].fillna(method='ffill', inplace=True)\n\n# finally, ensure there are no more missing values\nfor df in [plant1, plant2]:\n    print(df.isna().sum(), \"\\n\")\n","0bb474a7":"sns.set(rc={'figure.figsize':(11.7, 8.27)})","dd71b108":"# plant 1\nfor col in plant1.columns[plant1.dtypes == 'float']:\n    sns.distplot(plant1[col], kde=False)\n    plt.title(f\"Plant 1 Distribution of {col}\")\n    plt.show()","318cc10a":"# plant 2\nfor col in plant2.columns[plant2.dtypes == 'float']:\n    sns.distplot(plant2[col], kde=False)\n    plt.title(f\"Plant 2 Distribution of {col}\")\n    plt.show()","e3976b39":"# plant 1 by hour of the day\nfor col in plant1.columns[plant1.dtypes=='float']:\n    grouped = plant1.groupby('hour').mean().reset_index()\n    sns.scatterplot(data=grouped, x='hour', y=col)\n    plt.title(f\"Plant 1 Distribution of {col} by Hour of the Day\")\n    plt.show()","be4c17a6":"# plant 2 by hour of the day\nfor col in plant2.columns[plant1.dtypes=='float']:\n    grouped = plant2.groupby('hour').mean().reset_index()\n    sns.scatterplot(data=grouped, x='hour', y=col)\n    plt.title(f\"Plant 2 Distribution of {col} by Hour of the Day\")\n    plt.show()","e12d7efa":"# fix unreasonable daily_yield values\nfor df in [plant1, plant2]:\n    df.loc[(df['daily_yield'] > 0) & (df['hour'] < 5), 'daily_yield'] = 0\n\n# re-scale plant 1's dc power values to make them comparable to plant 2's\nplant1['dc_power'] \/= 10.","b6ebba9f":"#calculate other useful features\n\nfor df in [plant1, plant2]:\n    df.sort_values(['source_key', 'date_time'], ascending=True, inplace=True)\n    df.rename(columns={\"daily_yield\": \"cumulative_daily_yield\"}, inplace=True)\n    \n    # 15 minute level features (1 value per row)\n    \n    df['dc_ac_ratio'] = np.where(df['ac_power'] == 0, 0, df['dc_power']\/df['ac_power'])\n    df['yield'] = df['cumulative_daily_yield'].diff().fillna(0)\n    # fix differences at the boundaries\n    source_key_mask = df['source_key'] != df['source_key'].shift(1)\n    day_mask = df['date'] != df['date'].shift(1)\n    df.loc[source_key_mask, 'yield'] = 0\n    df.loc[day_mask, 'yield'] = 0\n    \n    # hour level features (1 value per inverter per hour)\n    \n    df['avg_hourly_dc_power'] = df.groupby(['source_key', 'hour'])['dc_power'].transform(func=np.mean)\n    df['avg_hourly_ac_power'] = df.groupby(['source_key', 'hour'])['ac_power'].transform(func=np.mean)\n    df['hourly_yield'] = df.groupby(['source_key', 'hour'])['yield'].transform(func=np.sum)\n    \n    # day level features (1 value per inverter per day)\n    \n    df['avg_daily_dc_power'] = df.groupby(['source_key', 'date'])['dc_power'].transform(func=np.mean)\n    df['avg_daily_ac_power'] = df.groupby(['source_key', 'date'])['ac_power'].transform(func=np.mean)\n    df['avg_daily_dc_ac_ratio'] = df.groupby(['source_key', 'date'])['dc_ac_ratio'].transform(func=np.mean)\n    # daily_yield is cumulative, so the final value, when grouped by inverter and date, is the total daily yield\n    df['total_daily_yield'] = df.groupby(['source_key', 'date'])['cumulative_daily_yield'].transform(func='last')\n    \n    # inverter level features (1 value per inverter)\n    \n    # average daily yield should be calculated using the daily total, so there should be 1 average per inverter\n    df['avg_daily_yield'] = df.groupby(['source_key'])['total_daily_yield'].transform(func=np.mean)\n","51a49b11":"# manually spot check the features\nplant1[plant1['hour']>6].head(15)","4c4afac8":"for col in plant1.columns[plant1.dtypes=='float']:\n    sns.distplot(plant1[col], kde=False)\n    plt.title(f\"Plant 1 Distribution of {col}\")\n    plt.show()","ad8fb830":"for col in plant2.columns[plant1.dtypes=='float']:\n    sns.distplot(plant2[col], kde=False)\n    plt.title(f\"Plant 2 Distribution of {col}\")\n    plt.show()","5222c59a":"# inspect the outliers in yield and hourly_yield\nplant1[plant1['hourly_yield'] < -2000].head(15)","db3a51eb":"plant2[plant2['hourly_yield'] < -2000].head(15)","278f378e":"# see previous notes about why total_yield and ac_power related features are being removed\nfeatures_to_keep = [\n    \"source_key\", \"plant_id\", \n    \"date_time\", \"date\", \"hour\", \"day\", \"weekday\", \"month\", \"year\",    \n    'dc_power', 'cumulative_daily_yield', \n    'ambient_temperature', 'module_temperature', 'irradiation',\n    'avg_hourly_dc_power',\n    'avg_daily_dc_power', \n    'total_daily_yield', 'avg_daily_yield',\n    'yield', 'hourly_yield',\n]\nplant1.drop([c for c in plant1.columns if c not in features_to_keep], axis=1, inplace=True)\nplant2.drop([c for c in plant2.columns if c not in features_to_keep], axis=1, inplace=True)","f5b7b9bf":"sns.set(rc={'figure.figsize':(11.7, 8.27)})","029ab421":"# view an inverter's DC output over time\nsns.lineplot(data=plant1[plant1['source_key']=='YxYtjZvoooNbGkE'], x='date_time', y='dc_power')\nplt.title(\"DC Power Output over Time\")\nplt.show()","86a78879":"# plots for all inverters\n\np1_grouped = plant1.groupby(['source_key', 'date']).last().reset_index()\np2_grouped = plant2.groupby(['source_key', 'date']).last().reset_index()\n\nsns.lineplot(data=p1_grouped, x='date', y='total_daily_yield', hue='source_key')\nplt.title(\"Plant 1 Daily Yield for all Inverters Over Time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\nsns.lineplot(data=p2_grouped, x='date', y='total_daily_yield', hue='source_key')\nplt.title(\"Plant 2 Daily Yield for all Inverters Over Time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\np1_mean_grouped = plant1.groupby('date').mean().reset_index()\np2_mean_grouped = plant2.groupby('date').mean().reset_index()\n\nsns.lineplot(data=p1_mean_grouped, x='date', y='irradiation', label=\"Plant 1\")\nsns.lineplot(data=p2_mean_grouped, x='date', y='irradiation', label=\"Plant 2\")\nplt.title(\"Daily Mean Irradiation Over Time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\np1_mean_grouped = plant1.groupby(['source_key', 'date']).mean().reset_index()\np2_mean_grouped = plant2.groupby(['source_key', 'date']).mean().reset_index()\n\nsns.lineplot(data=p1_mean_grouped, x='date', y='dc_power', hue='source_key')\nplt.title(\"Plant 1 Daily DC Power for all Inverters Over Time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\nsns.lineplot(data=p2_mean_grouped, x='date', y='dc_power', hue='source_key')\nplt.title(\"Plant 2 Daily DC Power for all Inverters Over Time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","79381442":"# inverter performance might vary over time bc of weather & other factors\n# but what about overall performance?  for all hours of the day for the entire period, how do the inverters perform?\np1_grouped = plant1.groupby(['source_key', 'hour']).last().reset_index()\np2_grouped = plant2.groupby(['source_key', 'hour']).last().reset_index()\n\nsns.lineplot(data=p1_grouped, x='hour', y='avg_hourly_dc_power', hue='source_key')\nplt.title(\"Plant 1 Average DC Output for each Inverter by Hour of the Day\")\nplt.ylabel(\"Output (kW)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\nsns.lineplot(data=p2_grouped, x='hour', y='avg_hourly_dc_power', hue='source_key')\nplt.title(\"Plant 2 Average DC Output for each Inverter by Hour of the Day\")\nplt.ylabel(\"Output (kW)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","0a5cf0f4":"sns.pairplot(data=plant1[['module_temperature', 'ambient_temperature', 'irradiation', 'yield', 'dc_power']])\nplt.show()","351dd011":"sns.pairplot(data=plant2[['module_temperature', 'ambient_temperature', 'irradiation', 'yield', 'dc_power']])\nplt.show()","afaa2d8c":"a = plant1[plant1['dc_power'] == 0].groupby('source_key')['date_time'].count().reset_index().rename(columns={\"date_time\": \"nbr_zeros_dc_power\"}).sort_values('nbr_zeros_dc_power', ascending=False)\nb = plant1.groupby(['source_key'])[['source_key', 'avg_daily_yield']].mean().reset_index().sort_values('avg_daily_yield', ascending=False)\nplant1_efficiency = pd.merge(a, b, how='inner', on='source_key')\n\na = plant2[plant2['dc_power'] == 0].groupby('source_key')['date_time'].count().reset_index().rename(columns={\"date_time\": \"nbr_zeros_dc_power\"}).sort_values('nbr_zeros_dc_power', ascending=False)\nb = plant2.groupby(['source_key'])[['source_key', 'avg_daily_yield']].mean().reset_index().sort_values('avg_daily_yield', ascending=False)\nplant2_efficiency = pd.merge(a, b, how='inner', on='source_key')\n\nfig = px.scatter(\n    plant1_efficiency, x='nbr_zeros_dc_power', y='avg_daily_yield', \n    color='source_key', hover_data=['source_key'],\n    title=\"Plant 1 Inverter Avg Daily Yield by Nbr Occurrences of 0 DC Output\"\n)\nfig.update_layout(shapes = [\n    {'type': 'line', 'yref': 'paper', 'xref': 'paper', 'y0': 0, 'y1': 1, 'x0': 0, 'x1': 1}\n])\nfig.show()\n\nfig = px.scatter(\n    plant2_efficiency, x='nbr_zeros_dc_power', y='avg_daily_yield', \n    color='source_key', hover_data=['source_key'],\n    title=\"Plant 2 Inverter Avg Daily Yield by Nbr Occurrences of 0 DC Output\"\n)\nfig.update_layout(shapes = [\n    {'type': 'line', 'yref': 'paper', 'xref': 'paper', 'y0': 0, 'y1': 1, 'x0': 0, 'x1': 1}\n])\nfig.show()\n","91eebf19":"# filter out possibly faulty inverters by finding them in the plots above\nplant1_good = plant1[plant1['avg_daily_yield']>4000].copy()  # easier way to filter these out than by name (see graph)\nplant2_good = plant2[plant2['avg_daily_yield']>4700].copy()\n\n# regress dc_power on irradiation and module_temperature - they are linearly related, so a regression is appropriate\n# do not fit an intercept, since there should be no power without irradiation\nlr_model = LinearRegression(fit_intercept=False, normalize=True)\nlr_model.fit(plant1_good[['module_temperature', 'irradiation']], plant1_good['dc_power'])\nprint(\"Plant 1 R^2\", lr_model.score(plant1_good[['module_temperature', 'irradiation']], plant1_good['dc_power']))\nlr_preds = lr_model.predict(plant1_good[['module_temperature', 'irradiation']])\nresiduals = plant1_good['dc_power'] - lr_preds\nplant1_good['lower_dc_power_than_expected'] = np.where(residuals < 0, 1, 0)\nplant1_good['1day_ma_of_lower_dc_power_than_expected'] = plant1_good.groupby('date')['lower_dc_power_than_expected'].transform(np.mean)\n\nlr_model = LinearRegression(fit_intercept=False, normalize=True)\nlr_model.fit(plant2_good[['module_temperature', 'irradiation']], plant2_good['dc_power'])\nprint(\"Plant 2 R^2\", lr_model.score(plant2_good[['module_temperature', 'irradiation']], plant2_good['dc_power']))\nlr_preds = lr_model.predict(plant2_good[['module_temperature', 'irradiation']])\nresiduals = plant2_good['dc_power'] - lr_preds\nplant2_good['lower_dc_power_than_expected'] = np.where(residuals < 0, 1, 0)\nplant2_good['1day_ma_of_lower_dc_power_than_expected'] = plant2_good.groupby('date')['lower_dc_power_than_expected'].transform(np.mean)\n\nsns.scatterplot(data=plant1_good, x='date_time', y='dc_power', hue='lower_dc_power_than_expected')\nplt.title(\"Plant 1 DC Power when Lower than Expected\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlim(plant1_good.date_time.min(), plant1_good.date_time.max())\nplt.show()\n\nsns.scatterplot(data=plant1_good, x='date_time', y='dc_power', hue='1day_ma_of_lower_dc_power_than_expected')\nplt.title(\"Plant 1 Daily Average DC Power Lower than Expected\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlim(plant1_good.date_time.min(), plant1_good.date_time.max())\nplt.show()\n\nsns.scatterplot(data=plant2_good, x='date_time', y='dc_power', hue='lower_dc_power_than_expected')\nplt.title(\"Plant 2 DC Power when Lower than Expected\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlim(plant2_good.date_time.min(), plant2_good.date_time.max())\nplt.show()\n\nsns.scatterplot(data=plant2_good, x='date_time', y='dc_power', hue='1day_ma_of_lower_dc_power_than_expected')\nplt.title(\"Plant 2 Daily Average DC Power Lower than Expected\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlim(plant2_good.date_time.min(), plant2_good.date_time.max())\nplt.show()","b1ae2740":"# get a list of inverters, so that 1 can be used as an example\nset(plant1.source_key)","32abe9a4":"class InverterModel:\n    def __init__(self, df, source_key):\n        self.df = df\n        self.source_key = source_key\n        self.N = None\n        self.T = 16  # use T previous time steps to predict the next one (there are 4 per hour)\n        self.D = 2  # irradiation and module_temperature are the features to be used\n        self.model = None\n        self.r = None\n        self.predictions = None\n        self.validation_predictions = None\n        self.anomalies = None\n\n    def _prepare_input_data(self):\n        \"\"\"\n        Prepares input data for a LSTM, for a given source_key.  \n        Input will be a N x T x D array, where:\n            N = number of samples, or rows\n            T = number of time steps to predict into the future\n            D = number of features\n        The data will be normalized, and the first half will be used as the training set,\n        while the second half will be used as the validation set.  This method assumes \n        that the dataset has already been sorted by date_time.\n        \"\"\"\n        # remove the times when irradiation = 0\n        dsub = self.df[\n            (self.df['irradiation'] > 0) & (self.df['source_key']==self.source_key)\n        ][['dc_power', 'irradiation', 'module_temperature']].values\n\n        # store the number of samples\n        self.N = len(dsub)\n\n        # normalize the data\n        # use the first half of the dataset for training and the second half for validation\n        # so fit the scaler to the first half, but transform all of it\n        normalizer = MinMaxScaler()\n        normalizer.fit(dsub[:-self.N\/\/2])\n        dsub = normalizer.transform(dsub)\n\n        # reshape the data in preparation for input\n        X = []\n        Y = []\n        for t in range(dsub.shape[0] - self.T):\n            x = dsub[t:t+self.T, 1:]  # stop at time t+T, because t+T is the target\n            X.append(x)\n            y = dsub[t+self.T, 0]\n            Y.append(y)\n\n        # store arrays as object attributes, so they can be referenced in other methods\n        self.X = np.array(X)\n        self.Y = np.array(Y)\n\n        # ensure that the X inputs are shape (N-T, T, D) and the Y inputs are shape (N,)\n        assert self.X.shape == (len(dsub)-self.T, self.T, self.D)\n        assert self.Y.shape[0] == len(dsub)-self.T\n\n    def _fit_lstm(self):\n        \"\"\"\n        Builds and trains an LSTM model on the given dataset.  The first half of the \n        dataset is used for training, and the second half is used for validation.\n        \"\"\"\n        # build LSTM model\n        i = Input(shape=(self.T, self.D))\n        x = LSTM(10)(i)\n        x = Dense(5)(x)\n        x = Dense(1)(x)\n        self.model = Model(i, x)\n        self.model.compile(\n            loss='mse',\n            optimizer=Adam(lr=0.05),\n        )\n\n        # train the LSTM\n        # use the first half of the dataset for training and the second half for validation\n        self.r = self.model.fit(\n            self.X[:-self.N\/\/2], self.Y[:-self.N\/\/2],\n            batch_size=32,\n            epochs=100,\n            validation_data=(self.X[-self.N\/\/2:], self.Y[-self.N\/\/2:]),\n            verbose=0,\n        )\n\n    def _plot_lstm_loss(self):\n        \"\"\"\n        Plots the LSTM's training and validation loss.\n        \"\"\"\n        plt.plot(self.r.history['loss'], label='Training Loss')\n        plt.plot(self.r.history['val_loss'], label='Validation Loss')\n        plt.legend()\n        plt.title(\"Training and Validation Loss by Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.show()\n    \n    def _one_step_forecast(self, show_plot=False):\n        \"\"\"\n        Makes one-step forecast using true targets\n        \"\"\"\n        outputs = self.model.predict(self.X)\n        print(\"Prediction shape:\", outputs.shape)\n        self.predictions = outputs[:,0]\n\n        if show_plot:\n            plt.plot(self.Y, label='targets')\n            plt.plot(self.predictions, label='predictions')\n            plt.title(\"One Step Forecast\")\n            plt.legend()\n            plt.show()\n    \n    def _multi_step_forecast(self, show_plot=False):\n        \"\"\"\n        Makes multi-step forecast using true targets from validation set\n        \"\"\"\n        validation_target = self.Y[-self.N\/\/2:]\n        self.validation_predictions = []\n\n        # first validation input\n        last_x = self.X[-self.N\/\/2]  # array of shape (T, D)\n\n        while len(self.validation_predictions) < len(validation_target):\n            # get predictions and turn the 1x1 array into a scaler\n            pred = self.model.predict(last_x.reshape(1, self.T, self.D))[0,0]\n\n            # update the predictions list\n            self.validation_predictions.append(pred)\n\n            # make the new input\n            last_x = np.roll(last_x, -1)\n            last_x[-1] = pred\n\n        if show_plot:\n            plt.plot(validation_target, label='True Value')\n            plt.plot(self.validation_predictions, label='Forecasted Value')\n            plt.title(f\"{self.T} Step Forecast for the Validation Set\")\n            plt.legend()\n            plt.show()\n    \n    def _find_anomalies(self, anomaly_type=\"low\", show_plot=False):\n        \"\"\"\n        Finds the anomalies in the one-step ahead predictions, using a \n        95% prediction interval.  The anomaly_type specifies whether to \n        focus on samples where the true value was outside of the prediction interval.\n        For instance, we may only wish to see samples where output was lower than \n        expected.\n        \"\"\"\n        # calculate the prediction interval to find anomalies\n        sum_errors = np.sum((self.Y-self.predictions)**2)\n        stdev = np.sqrt(1\/(len(self.Y)-2) * sum_errors)\n        \n        # 95% prediction interval = 1.96 standard deviations\n        interval = 1.96 * stdev\n        lower, upper = self.predictions - interval, self.predictions + interval\n        \n        # samples where the true value was outside of the prediction interval\n        if anomaly_type == \"high\":\n            self.anomalies = np.where(self.Y > upper, 1, 0)\n        elif anomaly_type == \"low\":\n            self.anomalies = np.where(self.Y < lower, 1, 0)\n        else:\n            self.anomalies = np.where(((self.Y > upper) | (self.Y < lower)), 1, 0)\n\n        if show_plot:\n            plt.plot(self.Y, label='targets')\n            #plt.plot(upper, label='predictions')\n            for idx, a in enumerate(list(self.anomalies)):\n                if a == 1:\n                    plt.plot((idx), (self.Y[idx]), 'o', color='red')\n            plt.legend()\n            plt.title(\"Anomalous DC Power Outputs\")\n            plt.show()\n\n        print(f\"{np.sum(self.anomalies)} total anomalies\")\n\n    def forecast(self, forecast_type='single_step', show_plot=False):\n        \"\"\"\n        Executes all methods in order, and forecasts the given number of steps ahead.\n        \"\"\"\n        self._prepare_input_data()\n        self._fit_lstm()\n        if show_plot:\n            self._plot_lstm_loss()\n        if forecast_type == 'single_step':\n            self._one_step_forecast(show_plot=show_plot)\n        else:\n            self._multi_step_forecast(show_plot=show_plot)\n        self._find_anomalies(show_plot=show_plot)\n\n# run 1 inverter as a test\/example\nInverterModel(df=plant1, source_key='1BY6WEcLGh8j5v7').forecast(show_plot=True)\n\n# fit a model to each inverter\np1_inverters = list(set(plant1.source_key))\np2_inverters = list(set(plant2.source_key))\n\ninverter_models = dict.fromkeys(p1_inverters + p2_inverters)\n'''\nfor i in p1_inverters:\n    inverter_models[i] = InverterModel(df=plant1, source_key=i).forecast(show_plot=False)\nfor i in p2_inverters:\n    inverter_models[i] = InverterModel(df=plant2, source_key=i).forecast(show_plot=False)\n'''","4546ca5b":"# univariate modeling of avg daily DC power, for all inverters, at the day level\nall_inverters = []\nfor i in p1_inverters:\n    dsub = plant1[plant1['source_key']==i].groupby('date')['avg_daily_dc_power'].last().reset_index(drop=True).values\n    all_inverters.append(dsub)\n\n# create a N x T x D matrix, where:\n#  N = number of samples (34 days)\n#  T = number of time steps to predict into the future\n#  D = number of features (22 inverters)\nall_inverters = np.array(all_inverters).T\n\n# store the number of samples\nN = len(all_inverters)\n\n# normalize the data\n# use the first half of the dataset for training and the second half for validation\n# so fit the scaler to the first half, but transform all of it\nnormalizer = MinMaxScaler()\nnormalizer.fit(all_inverters[:-N\/\/2])\nall_inverters = normalizer.transform(all_inverters)\n\n# reshape the data in preparation for input\nT = 1  # use T previous time steps to predict the next one (there are 4 per hour)\nD = all_inverters.shape[1]\nX = []\nY = []\nfor t in range(all_inverters.shape[0] - T):\n    x = all_inverters[t:t+T]  # stop at time t+T, because t+T is the target\n    X.append(x)\n    y = all_inverters[t+T]\n    Y.append(y)\n\nX = np.array(X)\nY = np.array(Y)\n\n# ensure that the X inputs are shape (N, T, D) and the Y inputs are shape (N,)\nprint(X.shape)\nprint(Y.shape)","54647775":"# build LSTM model\ni = Input(shape=(T, D))\nx = LSTM(10)(i)\nx = Dense(5)(x)\nx = Dense(1)(x)\nmodel = Model(i, x)\nmodel.compile(\n  loss='mse',\n  optimizer=Adam(lr=0.05),\n)\n\n# train the LSTM\n# use the first half of the dataset for training and the second half for validation\nr = model.fit(\n  X[:-N\/\/2], Y[:-N\/\/2],\n  batch_size=32,  # data is small enough to fit into 1 batch\n  epochs=100,\n  validation_data=(X[-N\/\/2:], Y[-N\/\/2:]),\n)","fc824056":"# Plot the loss\nplt.plot(r.history['loss'], label='Training Loss')\nplt.plot(r.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.title(\"Training and Validation Loss by Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","39374179":"# One-step forecast using true targets\noutputs = model.predict(X)\nprint(outputs.shape)\npredictions = outputs[:,0]\n\nplt.plot(np.mean(Y, axis=1), label='targets')\nplt.plot(predictions, label='predictions')\nplt.title(\"Plant 1's Avg Daily DC Power Output Forecasts\")\nplt.legend()\nplt.show()","889aec8b":"Total yield = previous total yield + daily yield at the end of the day, or at least, it seems like that is how it is calculated.  There are rows where this equation does not hold, so there may be adjustments made to yield that are not included as features in this dataset, so there is no way to know for certain.  Nevertheless, I will assume the equation holds mostly true for now, especially since I am just using it to fill a few missing values.\n\nTo fill the missing values, I will fill anything outside the hours of sunlight with 0, except for temperatures.  Then I will fill missing daily yields after sunset with the last non-null value for that day.  Then I will linearly interpolate other missing values, because the observations are ordered by time and follow roughly linear trends throughout the day.  Linear interpolation may chop off the parabolic peaks in these patterns, corresponding to noon\/max sunlight, but there are so few missing values, it should not matter very much.  I just need them to be filled close to what they would be.\n\nAfter interpolation, there should be no more missing values.","65e520a6":"My suspicions about the plausibility of forecasting output with the features in this dataset seem to have proven accurate, as the LSTM model is overfitting the training set.  The model has picked up on the daily cycles of sunlight, but its performance on the validation set is poor.  \n\nWhen the model is used to make multi-step forecasts (15 minute forecasts for the next 4 hours), it performs horribly.  This may be due to the influence of weather, which cannot possibly be predicted with the provided data. \n\nNow I will move on to forecasting the average daily DC power output for each plant.  ","b2209515":"#### Insights from these Plots\n\nPlant 1 has some inverters that had no yield some days, and some whose dc power dropped lower than the rest. Those could be signs of faulty inverters.\n\nPlant 2 has a lot more variation.  A couple interverters had no yield some days, and one had no dc power output for several consecutive days.  \n\nSunlight was similar for both plants, with some minor variation.\n\n#### Things to Explore, Based on These Insights\n\nInverter performance might vary over time because of weather and other factors, but what about overall performance?  It would be interesting to see how the inverters perform by hour of the day, for the entire time period.","84ea9dfb":"There are days that have daily_yields above 0 after midnight and before the sun has risen.  This is not possible and indicates a sensor reading error.  I will set these values to 0.\n\nDC Power seems to be 10x higher for plant 1 than for plant 2.  So I will divide plant 1's values by 10 to make them comparable.  \n\nTotal yield is a weird field.  The dataset's documentation says it is supposed to be the total yield for an inverter up to a certain point in time.  So unless the inverter is brand new, it should never have a total yield of 0.  Plant 1 has none of these, but plant 2 has plenty of inverters with total yield > 0 one day, but that are suddenly 0 the next day.  It doesn't make sense.  Also, if the total_yield feature measures total yield up to a point in time, then it should equal the previous day's total yield, plus the current day's final daily yield.  Looking at the data, that math works out most of the time, but it often does not. There seem to be adjustments made that are not recorded as a feature in this dataset, causing them to show up as sudden drops in total yield.  Since it is uncertain how total yield calculated, and why it varies, it is an unreliable field.  I will keep it in the data, but will avoid using it for modeling.\n\n**TLDR: Total yield is an unreliable feature, so I will avoid using it for modeling.**","3a8aaf41":"## Data Cleaning\n\nSteps to take:\n1. Convert date-times to Pandas datetime format.\n2. Check for Null values and decide what to do with them.\n3. Check for missing time steps (all sensors should have readings for the same datetimes), and insert Null values if there are missing time steps.\n4. If Nulls are inserted for missing time steps, decide what to do with them.\n5. Look for univariate outliers and decide what to do with them.","fdf3b40b":"# Solar Power Generation: Anomaly Detection & Forecasting\n\nThis notebooks shows how important data cleaning and exploration is.  Simply throwing this data into an AutoML program for forecasting would be reckless, because there are too many gotchas in the way the data is measured and variables that are not included in this dataset but that are critical for solar power forecasting.  For example, there are no nulls, but that does not mean there are no missing observations.  \n\nThe notebook focuses on data cleaning and exploration, missing value imputation, visualization, anomaly and outlier detection, and forecasting.  However, it should be emphasized that the dataset has significant limitations when used for forecasting.  So it really should only be used for anomaly and outlier detection, or in other words, looking for faulty equipment.","f3d6b8d3":"### EDA on the New Features\n\nLooking at the distributions above, a few things become clear.  \n\n1. Many features will need to be transformed to have more normal distributions before modeling.  However, the large number of zero values is the main cause of the skewed distributions.  I will ponder that before making any transformations...\n2. The new yield and hourly_yield features have some extreme outliers.\n3. Plant 2 had days when no power was generated.\n4. dc_ac_ratio is either 0 or 1, meaning it cannot be used to find faulty inverters, because the inverter either converts 100% of DC to AC or nothing at all.  That also means that either dc_power or ac_power can be used, while the other can be ignored.  The same can be said for all features derived off the relationship between dc_power and ac_power.  **I will keep the dc_power features, drop the ac_power features and all dc\/ac ratio features.**\n\n**Note about zero values:**\nThere are many zero values in power generation due to hours with no sunlight, or days where no power was generated.  Scaling or standardizing the data, in preparation for modeling, will be hard with all of these zeros.  Instead, I will drop the rows where there is no sunlight, because who cares about forecasting or looking for anomalies when there is no sunlight?  There would be no power produced at all during these times.  Dropping these rows should remove most of the zeros and reduce or remove the skew in the distributions.  **I will not do anything about the skewed distributions now, but if I use models that require assuming normality, I will address them then.**\n\n### Why not combine the data for both plants?\n\nSome relationships between features, like the one between irradiation and yield, would be the same for both plants.  In these cases, it would make sense to combine the data and model the combined data so there are more samples to learn from.  Other relationships, however, are unique to inverters.  So combining the data would yield no benefit for the feature relationships specific to each inverter.  For example, a time series model fitted to one inverter would most likely be useless when applied to another inverter (unless the yield is so dependent on irradiation that the time series essentially captures irradiation patterns).  The time series models I plan to use for anomaly detection and forecasting will be specific to each inverter.  I may use a neural network, and at that time, I will combine the inputs, but until then, I will keep them separate.","5d8544ab":"Now it should be easy to combine the generator and weather datasets, since the datetimes should match.","74cb8914":"## Feature Engineering\n\nIt would be nice to calculate features that capture the DC power, AC power, and yield at different levels.  Now that there are time features in the dataset, I can calculate features at the following levels: \n* the 15 minute level (1 value per row)\n* the hour level (1 value per inverter per hour)\n* the day level (1 value per inverter per day)\n* the inverter level (1 value per inverter)  \n\nAfterwards, I can do some more EDA on the new features.","19143b89":"Now I will identify outliers due to measurement error.  The tricky part here will be distinguishing between outliers that are indications of possible problems with the inverter versus sensor reading errors.  Right now I only want to identify the latter.  Those might be more extreme outliers or points that do not make any sense at all.  A visual spot check should reveal if there are any of these.\n\nSomething important to remember here is that sensor readings should be consistent overall, but they should be consistent by hour of the day as well.  So I will also need to check distributions by hour of the day.  It is possible that extreme outliers could be masked in the larger distribution, but become noticeable when viewed from the hourly level.","dbce53ed":"## Exploratory Data Analysis","fe4b67e2":"#### Insights from These Plots\n\nFrom these charts, it is apparent that:\n1. Temperature and irradiation are all strongly correlated.  No surprise there.\n2. The distribution of yield is strange, and because of it, it is hard to spot any trends.  It looks like dc_power might be a much more reliable indicator of output.  There are likely other factors affecting yield that are not in this dataset.  That would explain why yields sometimes drop during the day, as if they're being adjusted for some unobserved phenomenon. \n3. Plant 2 has so much variation that it hides the trend of power output to be related to temperature and irradiation.  \n\n**After exploring the data, I have decied to use dc power as the target variable for forecasting, instead of yield.**  I will also use it for anomaly detection.\n\n## Anomaly Detection\n\nA good inverter should be consistently productive.  So I need to look at features that measure productivity and consistency.  That can be average daily yield by standard deviation in daily yield, or the number of times the inverter stopped working, as counted by the number of zeros in DC power output.  I will use the latter, since periodic outages are signs of a faulty inverter.","f1d2b7de":"#### Insights from These Plots\n\nPlant 1 has a couple inverters with lower dc power, on average, than the rest.\n\nPlant 2 has so much variation.  On average, plant 1 seems to have higher performing inverters, at least in terms of consistency and dc output.  It's interesting that plant 2 has a higher yields, on average, despite lower dc output.  \n\n#### Things to Explore Next\n\nI want to look at bivariate relationships.","3f278640":"#### Insights from These Plots\n\nThe inverters in the top left of these plots are the best performers.  The ones in the bottom right are the worst. \n\n**The inverters in the bottom right section of these plots could be faulty and need replacing.**\n\n#### Things to Look for Next\n\nThe next step is to look for times when all inverters performed poorly and irradiation was good.  This could indicate that the panels are all dirty at those times, and should be cleaned.","8220e2e2":"It looks like the sun is shining between the hours of 5am and 6pm.  So any null values outside of this range can safely be set to 0.  To make it easier to fill these values, I will create a bunch of time features first.","2a94d2c1":"**The dark purple bands in these plots show when all of the inverters for a plant, on average, performed worse than expected.  It could indicate that the panels needed to be cleaned.**\n\nInterestingly, there are times when the performance was worse than expected, and then suddenly turned around.  That could indicate that the panels were cleaned.  Signs pointing to this are the colors getting darker purple over time, then quickly reversing to light pink.\n\nIn the absence of labeled data or knowledge of when (or if) the panels were cleaned during this time period, guessing when panels are dirty based on changes in collective inverter performance is the best we can do.\n\n## Forecasting Future Output\n\nThe problem with forecasting solar power generation is that is is so dependent on weather.  Fitting an autoregressive model to yield or output seems silly, because by definition, autoregression fits a variable to its previous values.  How can you predict power generation without knowing anything about the weather?  The weather data is not very helpful here either.  Yes, we can say that the number of hours of sunlight tomorrow should be similar to what they are today, but that is assuming there are no clouds.  There is no way to predict how cloud cover might impact irradiation, with the features in this dataset.  So there is no clean way to forecast power generation.  Nevertheless, I will fit a model and see what happens.  \n\n#### Approach\n\nI explained before why yield was an unreliable feature.  So DC power will be my target variable. \n\nThe first model I will fit is a model that forecasts DC Power, at the 15 minute level, for each inverter.  I will use irradiation and module temperature as features.  A simple regression model that predicts DC power output for given values of irradiation and module temperature would be useful, but it would be more useful to forecast future outputs.  So the model will make 1 step forecasts (forecasts of the DC power output over the next 15 minutes), and it will be specific to an inverter.   \n\nThis first model of DC power can be used to find outliers in the DC power output of an individual inverter.  By calculating the 95% prediction intervals, outliers can be found by comparing the true DC power outputs to the predicted value intervals.  Since we only really care about faulty inverters, the lower interval will be of the most importance.  So observations that are lower than expected (predicted) will be plotted, and any trends will become visible.  The trends may show whether an inverter is performing worse than expected for consecutive time steps, which may indicate that the inverter is faulty and needs replacing.  However, when making any judgements on inverter faultiness using the model, it will be important to remember that the benchmark for \"good\" performance is the inverter's own historical performance.  So if the inverter performed poorly from the start of the dataset, the model may not show anything useful, because it will have trained exclusively on poor performance. \n\nAnother thing that would be useful for a power plant to know is what its expected daily output would be, going into the future.  Each inverter has an average DC output for each day (the average DC power output of every 15 minute interval while there is sunlight).  A model could be fitted to these to forecast the average DC power output for the entire plant.  So I will fit a second model to forecast the average DC power output, for all of a plant's inverters, for the next day.  ","1f55d97a":"All the model seems to be doing is learning a moving average.  That is not too surprising, as the model cannot learn anything about the weather, so predicting something close to the average output likely yields the lowest mean squared error.  **What this means is that unless a power plant has reliable weather forecasting available, the best it may be able to do when forecasting its output is to use climate data for irradiation, and use the historical average output as its forecast.**","c0a3419a":"There are date-times that are missing from some sensors.  I will fill these with NAs, and determine how to handle them."}}