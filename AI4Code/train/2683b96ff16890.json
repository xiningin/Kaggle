{"cell_type":{"a8b2bb59":"code","6471d68a":"code","1e6ae567":"code","9f50993c":"code","542f4e28":"code","45075a42":"code","26b1a329":"code","c74428b1":"code","0c6a1c79":"code","04ffdd52":"code","e6f3c913":"code","8902fe21":"code","4013dbdd":"code","b4fadfd9":"code","8860c627":"code","9b44b749":"code","a1fac7ee":"code","0c46b596":"code","f6c0d311":"code","2fdd1a4f":"code","c22dc8c7":"code","4070308b":"code","ab630125":"markdown","dbdfa23e":"markdown","6cc22731":"markdown","add09351":"markdown","2aad0991":"markdown","db32d084":"markdown","c132a20a":"markdown","ae14d12a":"markdown","69f1f0fb":"markdown","d2762495":"markdown","66d7eaee":"markdown","2e872db6":"markdown","b09826c0":"markdown","bc182d73":"markdown","3209f29b":"markdown","fd1e68a9":"markdown","7f823bda":"markdown","9984b4b9":"markdown","334e8538":"markdown"},"source":{"a8b2bb59":"import os\nprint(os.listdir(\"..\/input\"))","6471d68a":"import numpy as np\nimport pandas as pd","1e6ae567":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9f50993c":"df = pd.read_csv('..\/input\/spam.csv', encoding='latin-1')[['v1', 'v2']]\ndf.columns = ['label', 'message']\ndf.head()","542f4e28":"df.groupby('label').describe()","45075a42":"sns.countplot(data=df, x='label')","26b1a329":"import string\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer as Stemmer\ndef process(text):\n    # lowercase it\n    text = text.lower()\n    # remove punctuation\n    text = ''.join([t for t in text if t not in string.punctuation])\n    # remove stopwords\n    text = [t for t in text.split() if t not in stopwords.words('english')]\n    # stemming\n    st = Stemmer()\n    text = [st.stem(t) for t in text]\n    # return token list\n    return text","c74428b1":"# Testing\nprocess('It\\'s holiday and we are playing cricket. Jeff is playing very well!!!')","0c6a1c79":"# Test with our dataset\ndf['message'][:20].apply(process)","04ffdd52":"from sklearn.feature_extraction.text import TfidfVectorizer","e6f3c913":"tfidfv = TfidfVectorizer(analyzer=process)\ndata = tfidfv.fit_transform(df['message'])","8902fe21":"mess = df.iloc[2]['message']\nprint(mess)","4013dbdd":"print(tfidfv.transform([mess]))","b4fadfd9":"j = tfidfv.transform([mess]).toarray()[0]\nprint('index\\tidf\\ttfidf\\tterm')\nfor i in range(len(j)):\n    if j[i] != 0:\n        print(i, format(tfidfv.idf_[i], '.4f'), format(j[i], '.4f'), tfidfv.get_feature_names()[i],sep='\\t')","8860c627":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nspam_filter = Pipeline([\n    ('vectorizer', TfidfVectorizer(analyzer=process)), # messages to weighted TFIDF score\n    ('classifier', MultinomialNB())                    # train on TFIDF vectors with Naive Bayes\n])","9b44b749":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.20, random_state = 21)","a1fac7ee":"spam_filter.fit(x_train, y_train)","0c46b596":"predictions = spam_filter.predict(x_test)","f6c0d311":"count = 0\nfor i in range(len(y_test)):\n    if y_test.iloc[i] != predictions[i]:\n        count += 1\nprint('Total number of test cases', len(y_test))\nprint('Number of wrong of predictions', count)","2fdd1a4f":"x_test[y_test != predictions]","c22dc8c7":"from sklearn.metrics import classification_report\nprint(classification_report(predictions, y_test))","4070308b":"def detect_spam(s):\n    return spam_filter.predict([s])[0]\ndetect_spam('Your cash-balance is currently 500 pounds - to maximize your cash-in now, send COLLECT to 83600.')","ab630125":"** Write a method to return normailzed text in form of tokens (lemmas)**","dbdfa23e":"**Fit and transform SMS corpus**","6cc22731":"**Clean and normalize text**<br>\nIt will be done in following steps:<br>\n1. Remove punctuations\n2. Remove all stopwords\n3. Apply [stemming](https:\/\/en.wikipedia.org\/wiki\/Stemming) (converting to normal form of word). <br>\n   For example, 'driving car' and 'drives car' becomes drive car<br>","add09351":"Function to predict whether passed message is ham or spam","2aad0991":"**Use classification report to get more details**","db32d084":"**Having messages in form of vectors, we are ready to train our classifier. <br>We will use Naive Bayes which is well known classifier while working with text data. \n<br>Before that we will use pipeline feature of sklearn to create a pipeline of TfidfVectorizer followed by Classifier.**\n<br>Input will be message passed to first stage TfidfVectorizer which will transform it and pass it to Naive Bayes Classifier to get output label","c132a20a":"**Read csv file**","ae14d12a":"# Spam Filter using Naive Bayes Classifier","69f1f0fb":"**Import libraries**","d2762495":"**Lets check what values it gives for a message**","66d7eaee":"**A better view**","2e872db6":"Looking at precision column (for ham, it is 1.00), we can say that all number of wrong predictions (in output of [18]) came from spam predicted as ham. It is ok and cost of predicting spam as ham is negligible to that of predicting ham as spam.","b09826c0":"**Predict for test cases**","bc182d73":"** Lets move directly to creating spam filter <br>\nOur approach:\n**\n1. Clean and Normalize text\n2. Convert text into vectors (using bag of words model) that machine learning models can understand\n3. Train and test Classifier","3209f29b":"**Train spam_filter**","fd1e68a9":"**Perform train test split**","7f823bda":"**Describe dataset and visualize ham\/spam count**","9984b4b9":"**Check for wrong predictions that were classified as ham**","334e8538":"**Convert each message to vectors that machine learning models can understand.<br>We will do that using bag-of-words model**\n<br>We will use TfidfVectorizer. It will convert collection of text documents (SMS corpus) into 2D matrix.\n<br>One dimension represent documents and other dimension repesents each unique word in SMS corpus .\n.\n<br>If **n<sup>th<\/sup> term t has occured p times in m<sup>th<\/sup> document**, (m, n) value in this matrix will be TF-IDF(t), <br><center>where [TF-IDF(t)](https:\/\/en.wikipedia.org\/wiki\/Tf\u2013idf) = Term Frequency (TF) * Inverse Document Frequency (IDF)<\/center>\n<br>Term Frequency (TF) is a measure of how frequent a term occurs in a document.<br>\n<br><center>TF(t)= Number of times term t appears in document (p) \/ Total number of terms in that document<\/center>\n<br>Inverse Document Frequency (IDF) is measure of how important term is. For TF, all terms are equally treated. But, in IDF, for words that occur frequently like 'is' 'the' 'of' are assigned less weight. While terms that occur rarely that can easily help identify class of input features will be weighted high.<br>\n<br><center>Inverse Document Frequency, IDF(t)= log<sub><i>e<\/i><\/sub>(Total number of documents \/ Number of documents with term t in it)<\/center>\n<br>At end we will have for every message, vectors normalized to unit length equal to size of vocalbulary (number of unique terms from entire SMS corpus)"}}