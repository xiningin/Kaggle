{"cell_type":{"25a00f6c":"code","4001c197":"code","d1619ec8":"code","8d5b5b60":"code","e292924d":"code","0d5084e2":"code","631e3fe5":"code","63743f07":"code","4e4bbfa7":"code","8eeadc26":"code","0624b950":"code","e1726e21":"code","d089e334":"code","9d074c23":"code","1970bbe4":"code","fa3ce090":"code","a8dbdd5e":"code","09866206":"code","9f649d02":"code","2089e78a":"code","a013730d":"code","8ec591c1":"code","bca22e02":"code","d986bc90":"code","a08d061f":"code","323416ee":"code","e4b0fe99":"code","c1810f11":"code","9f2486bb":"code","ee7f113c":"code","c7b819c0":"code","55aebaf4":"code","f1a198e5":"code","d8c66170":"code","e658db88":"code","fdcf08f8":"code","2a1d136a":"code","8e412c50":"code","eb9cef28":"code","f31feed2":"code","e519a8b4":"code","0768c57c":"code","26437ed8":"code","73157418":"code","be4355cd":"code","90b59f6d":"code","9b7aca2d":"code","c9565231":"code","62ad97c1":"code","f1ffc650":"code","6a5a4cb6":"code","b104e467":"code","45401f32":"code","7b0719cf":"code","fbfac6fa":"code","8487fd72":"code","6914a470":"code","c4b0aa07":"code","9dfef394":"code","3fdce33f":"code","e137c833":"code","3a10ea9d":"code","49c8fa37":"code","9babe4d6":"code","879c124e":"code","2b7b6bbb":"code","aeac126c":"code","4f616bcd":"code","727708e2":"code","d1c6e96f":"code","bd85fb97":"code","968d9c4a":"code","e74ebbf5":"code","fda63817":"code","bb7b21a7":"code","9600fe0c":"code","140c7739":"code","a5aa3920":"code","cd9efb43":"code","f49ae92b":"code","c307ad60":"code","35972a01":"code","09ce4550":"code","c7d2b044":"code","88956f58":"code","365bd070":"code","838e4d08":"code","7899a770":"code","2f30e653":"code","df1596e9":"code","8c3d36b6":"code","3ccca521":"code","afc40626":"code","dee4cb1e":"code","6ba64d8e":"code","68461f78":"code","9342e8ad":"code","38edc249":"code","5155adad":"code","14b2395c":"code","9f619fc1":"code","2378d694":"code","48b12f49":"code","b3e54548":"code","766930c3":"code","a0b49e3e":"code","4a64cd50":"code","429bf108":"code","d169ad64":"code","e4d95130":"code","bc6338b8":"code","e8df8a25":"code","924cf89d":"code","1322d9f9":"code","55ba3823":"code","5a61292a":"code","7a8c6502":"code","d700ca17":"code","62e89f22":"code","a6a749da":"code","bda22a84":"code","598ecbb3":"code","2bd9f331":"code","32636128":"code","7176d28d":"code","a489746c":"code","b305da78":"code","5c6afa1e":"code","eab93d6d":"code","6cd16813":"code","73ce2206":"code","9aa93a73":"code","1137d077":"code","0f39c201":"code","f09f6c65":"code","b73efc57":"code","9956a8b8":"code","9e63ae5f":"code","6e77d85e":"code","00c2a8bb":"code","8924e5c4":"code","06baa717":"code","e58e1a3b":"code","6727b98e":"code","06ce4332":"code","9fff31ad":"code","66dcaa86":"code","d4944a45":"code","86864095":"code","46613d05":"code","4983a483":"code","7437fb11":"code","24029ed1":"code","25be0200":"code","20dcd50c":"code","0d632b64":"code","be7cd550":"code","ef82fed5":"code","c1442994":"code","80a22cd2":"code","b68f0468":"code","23f5d8d1":"code","2184b8ff":"code","40ecd009":"code","036b5b66":"code","7cbfe213":"code","132cb5e9":"code","2c8fa2b2":"code","db2c505b":"code","3e8fdf1b":"code","4a01a914":"code","a0163c08":"code","a3535a3b":"code","37c53e7e":"code","e9666c3b":"code","861375ab":"code","d36c0494":"code","a0557c6a":"code","02c1dd25":"code","68c421a1":"markdown","483d7918":"markdown","f7636753":"markdown","862ec390":"markdown","eacd4654":"markdown","582d0818":"markdown","4b62150f":"markdown","8c3d1df4":"markdown","bb45926c":"markdown","665eb5e2":"markdown","65225809":"markdown","ff5c950f":"markdown","6970eae4":"markdown","89fa8243":"markdown","71f68cbe":"markdown","b71e7369":"markdown","f25428c8":"markdown","fa3b27c3":"markdown","06942023":"markdown","3e22c33d":"markdown","79471e72":"markdown","561868f7":"markdown","4b442dff":"markdown","41f2f3da":"markdown","090b4b4b":"markdown","e3f03e44":"markdown","2171d421":"markdown","c9096b55":"markdown","fecd37ac":"markdown","516cb0a8":"markdown","6b54ae18":"markdown","cae7d022":"markdown","0c7124c1":"markdown","0cdd5b49":"markdown","7dbbb0d9":"markdown","c2c23aee":"markdown","6576e674":"markdown","3e097f68":"markdown","444f1c34":"markdown","d308afec":"markdown","95553562":"markdown","6c5fe3a0":"markdown","5245e2f6":"markdown","36db9228":"markdown","60b03212":"markdown","a88be6ae":"markdown","04ad2dbe":"markdown","3e510737":"markdown","93a8a980":"markdown","e243936b":"markdown","edf2d553":"markdown","e925c27f":"markdown","21567bec":"markdown","6332b645":"markdown","17f8241b":"markdown","fec1db1f":"markdown","eaf61781":"markdown","2e796c39":"markdown","e4283e68":"markdown","b6b82786":"markdown","5fed4783":"markdown","7f9f1895":"markdown","173949fc":"markdown","9f14b880":"markdown","90efa9c5":"markdown","92448de2":"markdown","f24e0841":"markdown","c34ae0b0":"markdown","6fe9348c":"markdown","6a541c41":"markdown","f17a9d1d":"markdown","8f58a72b":"markdown","adb9ba77":"markdown","802509ca":"markdown","8326c269":"markdown","7f29d4d5":"markdown","8e10673b":"markdown","9acf7d7b":"markdown","1e701075":"markdown","7dd62c20":"markdown","8f8d38cf":"markdown","16c5849a":"markdown","ba789b21":"markdown","48735e60":"markdown","a518f6f0":"markdown","ab906f30":"markdown","35990b17":"markdown","badad874":"markdown","613897ba":"markdown","5422b0f0":"markdown","2ec03b2c":"markdown","822a2718":"markdown","21ba84ec":"markdown","35985b8c":"markdown","da296179":"markdown","117115f9":"markdown","e24cfe30":"markdown","4a06cada":"markdown","305c5087":"markdown","52c69621":"markdown","f2b9508f":"markdown","d87b0df1":"markdown","82dd517a":"markdown","ff7a3d2d":"markdown","a3430dac":"markdown","6fdd7777":"markdown","826cd9d5":"markdown","abba4dba":"markdown","226ea2b6":"markdown","c8a6677f":"markdown","27b64af2":"markdown","e074d8c5":"markdown","edcdfe36":"markdown","dc01e571":"markdown","aaa24f61":"markdown","23bd230a":"markdown","ab49fcae":"markdown","afe76fdb":"markdown","e0b952fb":"markdown","02297d94":"markdown","b39c753f":"markdown","d86acc4f":"markdown","3f1b9135":"markdown","812ec9d8":"markdown","99bbb540":"markdown","f3006fff":"markdown","24b50d5c":"markdown","53f1150e":"markdown","b97ccd9c":"markdown","8663eac1":"markdown","3a47ff4a":"markdown","a973ac44":"markdown","4b6e66a3":"markdown","19052dfd":"markdown","ccc3aba1":"markdown","007673d8":"markdown","e4c9b973":"markdown","441257fb":"markdown"},"source":{"25a00f6c":"from IPython.display import Image\nprint(\"Model's best score to predict monthly energy released\")\nImage(\"..\/input\/results\/Models scores Energy.png\")","4001c197":"from IPython.display import Image\nprint (\"Original data vs predicted\")\nImage(\"..\/input\/results\/Energy -  original vs predicted.png\")","d1619ec8":"from IPython.display import Image\nprint(\"Model's best score to predict monthly energy released\")\nImage(\"..\/input\/results\/Models scores bvalues.png\")","8d5b5b60":"Image(\"..\/input\/results\/b values -  original vs predicted.png\")","e292924d":"import numpy as np\nimport pandas as pd\n\n# map creation\nimport cartopy.crs as ccrs\nimport cartopy\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n\n# data visualization \nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\n\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# feature reduction\nfrom sklearn.decomposition import PCA\n\n#---- Machine learning\n# data preparation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n# model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\n# hyperparameter tunnig\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# # Import necessary modules for neutral network\n# import keras\n# from keras.layers import Dense, BatchNormalization\n# from keras.models import Sequential\n# from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n\n# Model evaluation\nimport math\nfrom sklearn import metrics\nfrom statsmodels.graphics.api import abline_plot","0d5084e2":"plt.figure(figsize=(7,7))\n\n# SHOW LOCATION OF THE GEYSER GEOTHERMAL FIELD\nax1 = plt.axes(projection=ccrs.PlateCarree())\nax1.set_extent([-123, -121, 37,39], crs=ccrs.PlateCarree())\n\n# add color\nax1.add_feature(cfeature.OCEAN.with_scale('10m'))\nax1.add_feature(cfeature.LAND)\nax1.add_feature(cfeature.RIVERS)\nax1.coastlines()\n\n# add grid\ngl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--')\ngl.xlabels_top = False\ngl.ylabels_right = False\ngl.xlocator = mticker.FixedLocator([-122.5, -121.5])\ngl.ylocator = mticker.FixedLocator([38.5, 37.5,37])\ngl.xformatter = LONGITUDE_FORMATTER\ngl.yformatter = LATITUDE_FORMATTER\ngl.xlabel_style = {'size': 13, 'color': 'gray', 'weight': 'bold'}\ngl.ylabel_style = {'size': 13, 'color': 'gray', 'weight': 'bold'}\n\n# San Francisco\/Coordinates\nax1.scatter(x =-122.45, y=37.7, s=2000,c='black')\nax1.text(-122.3, 37.6, 'San Francisco', size=16)\n\n# San Francisco\/Coordinates\nax1.scatter(x =-122.801046, y=38.821042, s=2000,c='green')\nax1.text(-122.62, 38.8, 'The Geysers', size=16)\nax1.text(-122.68, 38.7, 'geothermal field', size=16)\n\n# set title\nax1.set_title('Induced seismic events location',size=15)\n\nplt.show()","631e3fe5":"# step1: Load the seismic catalog \ncatalogue = pd.read_csv(r'..\/input\/water-injection-induced-seismic-events\/seismic catalogue 2003 2016.csv')\nprint('They are {} induced-seismic events from 2003 to 2016 in our study area.'.format(catalogue.shape[0]))\ncatalogue.head(2)","63743f07":"# set date as index\ncatalogue['date'] = pd.to_datetime(catalogue['date'])\ncatalogue = catalogue.set_index('date')\ncatalogue.tail(2)","4e4bbfa7":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n    # x-data for the ECDF: x \n    x = np.sort(data)\n    # y-data for the ECDF: y  The y data of the ECDF go from 1\/n to 1 in equally spaced increments. \n    y = np.arange(1,n+1) \/ n\n    \n    return x, y","8eeadc26":"# define figure size\nfig = plt.figure(figsize=(5,5))\n\n# figure title\nfig.suptitle('Empirical Cumulative Distribution Function', fontsize=18)\n\nmags = catalogue['Ml']\nax1 = plt.plot(*ecdf(mags),marker='.',linestyle = 'none')\nax1 = plt.xlabel('magnitude')\nax1 = plt.ylabel('ECDF')\nax1 = plt.text(1.5, 0.2, 'max magnitude {}'.format(mags.max()),fontsize=14)\nax1 = plt.text(1.7, 0.3, 'Nb. events {}'.format(len(mags)),fontsize=14)","0624b950":"# define fonction to compute b-value with confident interval\ndef b_value(mags, mt):\n    \"\"\"Compute the b-value.\"\"\"\n    # Extract magnitudes above completeness threshold: m\n    m = mags[mags >= mt]\n    # Compute b-value: b\n    b = (np.mean(m)-mt)*np.log(10)\n    return b","e1726e21":"# Because there are plenty of earthquakes above magnitude 1, \n# we can use mt = 1 as our completeness threshold.\nmt = 1\n\n# Compute b-value and confidence interval\nb = b_value(mags, mt)\n\nprint('The b-value is {0:.2f}'.format(b))","d089e334":"# create new column for month and year\ncatalogue['years_months'] = catalogue.index.to_period('M')\n\n#  calculate b_value each month\nyears_months = catalogue['years_months'].unique()\nlist_b= []\nfor year_month in years_months:\n    df = catalogue[catalogue['years_months']==year_month]\n    mags = df['Ml']\n    b = b_value(mags, mt)\n    list_b.append(b)\n\ndf_b_value = pd.DataFrame({'years_months':years_months,'b_value':list_b})    \ndf_b_value.tail(2)","9d074c23":"# drop last month (march 2016)\ndf_b_value = df_b_value[:-1]\n# convert period to date\ndf_b_value['years_months'] = df_b_value['years_months'].values.astype('datetime64[M]')\n# set 'years_months' as index\ndf_b_value = df_b_value.set_index('years_months')\ndf_b_value.head(3)","1970bbe4":"# define figure size\nfig = plt.figure(figsize=(10,5))\n\n# figure \nfig.suptitle('Evolution b-value', fontsize=18)\nax1 = plt.plot(df_b_value.index,df_b_value['b_value'],marker = 'o',color='red')\nax1 = plt.xlabel('year',size=14)\nax1 = plt.ylabel('b-value',size=14)\nplt.show()","fa3ce090":"# calculate the amount of energy released  by an earthquake\ncatalogue['Energy'] = 10**(11.8 + 1.5*catalogue['Ml'])\ncatalogue.head(2)\n\n# calculate the monthly amount of seismic energy releases\nmonthly_energy_released = catalogue.groupby(['years_months'])['Energy'].sum()\nmonthly_energy_released = pd.DataFrame(monthly_energy_released)\n\n# convert period to date\nmonthly_energy_released = monthly_energy_released.reset_index()\nmonthly_energy_released['Date'] = monthly_energy_released['years_months'].values.astype('datetime64[M]')\n\n# set date as index\nmonthly_energy_released = monthly_energy_released.set_index('Date')\n\n# drop unwanted column\nmonthly_energy_released = monthly_energy_released.drop(columns=['years_months'])\n\n# drop 3 last month (March 2016 only one days and outliers)\nmonthly_energy_released = monthly_energy_released[:-1]\n\nmonthly_energy_released.tail(2)","a8dbdd5e":"# define figure size\nfig = plt.figure(figsize=(10,5))\n\n# figure \nfig.suptitle('Monthly seismic energy released', fontsize=18)\nax1 = plt.plot(monthly_energy_released.index,monthly_energy_released['Energy'])\nax1 = plt.xlabel('year',size=14)\nax1 = plt.ylabel('Total energy per month',size=14)\nplt.show()","09866206":"# create nodes' coordinates for interpolation\nx= np.linspace(-122.86, -122.74, 50)\n# select the first two and two number of the list\nx_min = np.mean(x[0:2])\nx_max = np.mean(x[-2:])\n\n# calculate the nodes coordinate between these extremes\ny= np.linspace(38.76, 38.88, 50)\ny_min = np.mean(y[0:2])\ny_max = np.mean(y[-2:])\n\n# calulate the center coordinate of each bin\nx_mean = np.linspace(x_min, x_max, 49)\ny_mean = np.linspace(y_min, y_max, 49)\n# create list with coordinates\nlist_mean_coord = []\nfor longitude in x_mean:\n    for latitude in y_mean:\n        list_mean_coord.append([longitude,latitude])\n        \ndf_mean_coord = pd.DataFrame(list_mean_coord,columns=['mean_long','mean_lat'])","9f649d02":"import matplotlib.tri as tri\n\ndef plot_snapchot(df,year):\n    # -----------------------\n    # Interpolation on a grid\n    # -----------------------\n    # A contour plot of irregularly spaced data coordinates\n    # via interpolation on a grid.\n    xmin = df['mean_long'].min()\n    xmax = df['mean_long'].max()\n    ymin = df['mean_lat'].min()\n    ymax = df['mean_lat'].max()\n\n    npts = df.shape[0]\n    ngridx = 50\n    ngridy = 50\n\n    # Create grid values first.\n    X_int = np.linspace(xmin, xmax, ngridx)\n    Y_int = np.linspace(ymin, ymax, ngridy)\n\n    # Perform linear interpolation of the data (x,y)\n    # on a grid defined by (xi,yi)\n    triang = tri.Triangulation(df['mean_long'], df['mean_lat'])\n    interpolator = tri.LinearTriInterpolator(triang, df['count'])\n    Xi, Yi = np.meshgrid(X_int, Y_int)\n    Z_int  = interpolator(Xi, Yi)\n\n    # define figure size\n#     fig, ax = plt.subplots(figsize=(6,4))\n    \n    ax = plt.contourf(X_int, Y_int, Z_int, levels=50, cmap=\"RdBu_r\")\n    plt.title('{}'.format(year), fontsize=14)\n    x = [-122.84, -122.8,-122.76]\n    plt.xticks(x)\n    \n    # fig.colorbar()\n    plt.colorbar(ax)\n\n    return ax","2089e78a":"def count_meq(year):\n    data = catalogue[catalogue['year']==year]\n    # df with only coordinate\n    data = data[['Longitude','Latitude']]\n    data = data.reset_index()\n    data = data.drop('date',axis=1)\n    # bin the data into equally spaced groups\n    x_cut = pd.cut(data.Longitude, np.linspace(-122.86, -122.74, 50), right=False)\n    y_cut = pd.cut(data.Latitude, np.linspace(38.76, 38.88, 50), right=False)\n\n    # group and count\n    result = data.groupby([x_cut,y_cut]).count()\n\n    # rename columns and flatten df\n    result.columns = ['countx','count']\n    result = result.reset_index()\n    # select only count\n    count = result[['count']]\n    # fill NaN value with zero\n    count = count.fillna(0)\n    # append count\n    df_result = pd.concat([df_mean_coord,count],axis=1)\n    return df_result","a013730d":"# extract year from index\ncatalogue['year'] = pd.DatetimeIndex(catalogue.index).year\n\n# use function count number of seismic events per bins per year\ndf_result_2003 = count_meq(2003)\ndf_result_2004 = count_meq(2004)\ndf_result_2005 = count_meq(2005)\ndf_result_2006 = count_meq(2006)\ndf_result_2007 = count_meq(2007)\ndf_result_2008 = count_meq(2008)\ndf_result_2009 = count_meq(2009)\ndf_result_2010 = count_meq(2010)\ndf_result_2011 = count_meq(2011)\ndf_result_2012 = count_meq(2012)\ndf_result_2013 = count_meq(2013)\ndf_result_2014 = count_meq(2014)\ndf_result_2015 = count_meq(2015)\ndf_result_2016 = count_meq(2016)\n","8ec591c1":"fig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\nplt.suptitle('Density maps (number of eartquake per bin) for the year: ',y=0.92,size=16)\n\n# color based on trend\nax1 = fig.add_subplot(5,3,1)\nax1 = plot_snapchot(df_result_2003,'2003')\n\nax2 = fig.add_subplot(5,3,2)\nax2 = plot_snapchot(df_result_2004,'2004')\n\nax1 = fig.add_subplot(5,3,3)\nax1 = plot_snapchot(df_result_2005,'2005')\n\nax1 = fig.add_subplot(5,3,4)\nax1 = plot_snapchot(df_result_2006,'2006')\n\nax1 = fig.add_subplot(5,3,5)\nax1 = plot_snapchot(df_result_2007,'2007')\n\nax2 = fig.add_subplot(5,3,6)\nax2 = plot_snapchot(df_result_2008,'2008')\n\nax1 = fig.add_subplot(5,3,7)\nax1 = plot_snapchot(df_result_2009,'2009')\n\nax1 = fig.add_subplot(5,3,8)\nax1 = plot_snapchot(df_result_2010,'2010')\n\nax1 = fig.add_subplot(5,3,9)\nax1 = plot_snapchot(df_result_2011,'2011')\n\nax1 = fig.add_subplot(5,3,10)\nax1 = plot_snapchot(df_result_2012,'2012')\n\nax1 = fig.add_subplot(5,3,11)\nax1 = plot_snapchot(df_result_2013,'2013')\n\nax1 = fig.add_subplot(5,3,12)\nax1 = plot_snapchot(df_result_2014,'2014')\n\nax1 = fig.add_subplot(5,3,13)\nax1 = plot_snapchot(df_result_2015,'2015')\n\nax1 = fig.add_subplot(5,3,14)\nax1 = plot_snapchot(df_result_2016,'2016')","bca22e02":"###-------- LOAD THE INJECTION DATA\n# Load monthly amount of injected water: \"Gross Injected (1000kg)\"\ndf_GIW_per_well = pd.read_csv(r'..\/input\/water-injection-induced-seismic-events\/Total Gross Injected Water.csv',sep=';')\n# Load monthly injection rate: \"Water Injection Rate (1000 kg\/hr)\"\ndf_RIW_per_well = pd.read_csv(r'..\/input\/water-injection-induced-seismic-events\/Total Water Injection Rate.csv',sep=';')\n# Load number of days per month where \ndf_days_per_well = pd.read_csv(r'..\/input\/water-injection-induced-seismic-events\/Total days_inj.csv',sep=';')\n\n# set date as index\ndf_GIW_per_well['Date'] = pd.to_datetime(df_GIW_per_well['Date'])\ndf_GIW_per_well = df_GIW_per_well.set_index('Date')\n\ndf_RIW_per_well['Date'] = pd.to_datetime(df_RIW_per_well['Date'])\ndf_RIW_per_well = df_RIW_per_well.set_index('Date')\n\ndf_days_per_well['Date'] = pd.to_datetime(df_days_per_well['Date'])\ndf_days_per_well = df_days_per_well.set_index('Date')\n\n# select the period as during which the induced seismic events are monitored \ndf_GIW_per_well = df_GIW_per_well.loc['2003-05-01':'2016-02-01']\ndf_RIW_per_well = df_RIW_per_well.loc['2003-05-01':'2016-02-01']\ndf_days_per_well = df_days_per_well.loc['2003-05-01':'2016-02-01']\n\ndf_GIW_per_well.tail(2)","d986bc90":"# plot the data\ndf_GIW_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","a08d061f":"df_RIW_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","323416ee":"df_days_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","e4b0fe99":"# first we drop all the columns where 'Gross Injected (1000kg)' is 0 everywhere\nprint('Number of columns before dropping well not used for injection during the selected period: {}'.format(df_GIW_per_well.shape[1]))\ndf_GIW_per_well = df_GIW_per_well.loc[:, df_GIW_per_well.any()]\nprint('final number of wells used: {}'.format(df_GIW_per_well.shape[1]))\nprint('')","c1810f11":"# then we make sure to keep the same columns in the two other df\ncolumns_to_keep = df_GIW_per_well.columns\ndf_RIW_per_well = df_RIW_per_well.drop(columns=[col for col in df_RIW_per_well if col not in columns_to_keep])\ndf_days_per_well = df_days_per_well.drop(columns=[col for col in df_days_per_well if col not in columns_to_keep])\n\n# add prefix\ndf_GIW_per_well = df_GIW_per_well.add_prefix('GIW_')\ndf_RIW_per_well = df_RIW_per_well.add_prefix('RIW_')\ndf_days_per_well = df_days_per_well.add_prefix('days_')","9f2486bb":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15,8),sharex=True)\n# fig.subplots_adjust(hspace=0.4,wspace=0.3)\n\ndf_GIW_per_well.plot(ax=axes[0],legend=None)\naxes[0].set_ylabel('GIW',size=14)\ndf_RIW_per_well.plot(ax=axes[1],legend=None)\naxes[1].set_ylabel('RIW',size=14)\ndf_days_per_well.plot(ax=axes[2],legend=None)\naxes[2].set_ylabel('days',size=14)\naxes[2] = plt.xlabel('year',size=14)\n\nplt.show()","ee7f113c":"for col in df_RIW_per_well.columns:\n    df_RIW_per_well[col][df_RIW_per_well[col] < 0] = 0\n    \ndf_RIW_per_well.plot(legend=None)\nplt.ylabel('injection rate (x1000kg\/hr)')\nplt.show()\n\nprint('Now, on the initial 73 injection wells only {} were injected during this period'\\\n      .format(df_RIW_per_well.shape[1]))","c7b819c0":"# create empty df:\ndf_features_vs_sismicity = pd.DataFrame()\n\n# add column with total amount of water injected per month in the area\ntotal_vol_inj = df_GIW_per_well.sum(axis=1)\n# add the monthly seismic energy released in this area\ndf_features_vs_sismicity = pd.concat([monthly_energy_released,df_b_value,total_vol_inj],axis=1)\ndf_features_vs_sismicity.columns = ['Energy','b_value','GIW_sum']\n\ndf_features_vs_sismicity.head(2)","55aebaf4":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.title('Seasonal evolutions of the amount of injected water and the seismic activity ',size=18, y=1.06) \n \n\nax.plot(df_features_vs_sismicity.index, df_features_vs_sismicity[\"GIW_sum\"],color='darkblue')\nax.set_xlabel('year',fontsize=15)\nax.set_ylabel('Total injected water (1000kg)', color='darkblue',fontsize=15)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params('y', colors='darkblue')\n\nax2 = ax.twinx()  #specify that the two lines share the same x-axis\nax2.plot(df_features_vs_sismicity.index,df_features_vs_sismicity['Energy'],color='green')\nax2.set_ylabel('Energy seismic', color='green',fontsize=15)\nax2.tick_params('y', colors='green')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","f1a198e5":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.title('Seasonal evolutions of the amount of injected water and b_value ',size=18, y=1.06) \n \n\nax.plot(df_features_vs_sismicity.index, df_features_vs_sismicity[\"GIW_sum\"],color='darkblue')\nax.set_xlabel('year',fontsize=15)\nax.set_ylabel('Total injected water (1000kg)', color='darkblue',fontsize=15)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params('y', colors='darkblue')\n\nax2 = ax.twinx()  #specify that the two lines share the same x-axis\nax2.plot(df_features_vs_sismicity.index,df_features_vs_sismicity['b_value'],color='red')\nax2.set_ylabel('b_value', color='red',fontsize=15)\nax2.tick_params('y', colors='red')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","d8c66170":"# create dictionnary to write shift version on injection data\ndic_GIW_shift = {}\n# create dictionnary to write shorter version on seismic catalogue\ndic_shorter_catalogue={}\ndic_shorter_b_value={}\ndic_GIW_SEISMIC={}\n\nlist_coef_GIW_Energy = []\nlist_coef_GIW_b_value = []\n\nfor i in range (0,6):\n    # Creating a time-shifted dictionnary with the injection data and remove nan values\n    dic_GIW_shift['GIW_shift{}'.format(i)] = df_features_vs_sismicity[\"GIW_sum\"].shift(i).dropna()\n    # create a shorter version of seismic data to fit the lenght of the lagged versions\n    if i == 0:\n        dic_shorter_catalogue['shorter_catalogue{}'.format(i)] = df_features_vs_sismicity['Energy']\n        dic_shorter_b_value['shorter_b_value{}'.format(i)] = df_features_vs_sismicity['b_value']\n    else:\n        dic_shorter_catalogue['shorter_catalogue{}'.format(i)] = df_features_vs_sismicity['Energy'].iloc[:-i]\n        dic_shorter_b_value['shorter_b_value{}'.format(i)] = df_features_vs_sismicity['b_value'].iloc[:-i]\n    # store time-shifted series with the associated seismic moment  \n    dic_GIW_SEISMIC['{}'.format(i)] = pd.concat([dic_GIW_shift['GIW_shift{}'.format(i)],\n                                                 dic_shorter_catalogue['shorter_catalogue{}'.format(i)],\n                                                 dic_shorter_b_value['shorter_b_value{}'.format(i)]],\n                                                 axis=1,join='inner')\n   # calculate the correlation coefficient \n    coef_GIW_Energy = dic_GIW_shift['GIW_shift{}'.format(i)].corr(dic_shorter_catalogue['shorter_catalogue{}'.format(i)] )\n    list_coef_GIW_Energy.append(coef_GIW_Energy)\n    coef_GIW_b_value = dic_GIW_shift['GIW_shift{}'.format(i)].corr(dic_shorter_b_value['shorter_b_value{}'.format(i)] )\n    list_coef_GIW_b_value.append(coef_GIW_b_value)","e658db88":"list_month = list(range (0,6))\n\nfig = plt.figure(2,figsize=(16,10))\n\nax1 = fig.add_subplot(2,2,1)\n# plot coef for the lagged version\nax1.plot(list_month,list_coef_GIW_Energy,marker='o',linestyle = '-')\n\n# Label axes and show plot\nax1 = plt.xlabel('lagged version (month)',fontsize=15)\nax1 = plt.xticks(fontsize=14)\nax1 = plt.ylabel('correlation coefficient',fontsize=15)\nax1 = plt.yticks(fontsize=14)\nax1 = plt.title('correlation coefficient between seismic activity and \\nseveral lagged versions of the injection data',fontsize=16,y=1.05)\n\nax2 = fig.add_subplot(2,2,2)\n# plot coef for the lagged version and a linear-regression-line\nax2.scatter(dic_GIW_shift['GIW_shift2'],dic_shorter_catalogue['shorter_catalogue2'],marker='o')\nm, b = np.polyfit(dic_GIW_shift['GIW_shift2'],dic_shorter_catalogue['shorter_catalogue2'], 1)\nax2 = plt.plot(dic_GIW_shift['GIW_shift2'],m*dic_GIW_shift['GIW_shift2']+b,color='k')\n# add coef correlation at location x and y\nax2 = plt.text(2500000, 2e18, 'R=0.34',fontsize=15)\n# # Label axes and show plot\nax2 = plt.xlabel('2 Months lagged version of injection data',fontsize=15)\nax2 = plt.xticks(fontsize=14)\nax2 = plt.ylabel('monthly seismic energy released',fontsize=15)\nax2 = plt.yticks(fontsize=14)\nax2 = plt.title('Injection data (2 months shifted) versus \\n seismic activity',fontsize=16,y=1.05)\n\n\nplt.show()    ","fdcf08f8":"list_month = list(range (0,6))\n\nfig = plt.figure(2,figsize=(16,10))\n\nax1 = fig.add_subplot(2,2,1)\n# plot coef for the lagged version\nax1.plot(list_month,list_coef_GIW_b_value,marker='o',linestyle = '-', color= 'red')\n\n# Label axes and show plot\nax1 = plt.xlabel('lagged version (month)',fontsize=15)\nax1 = plt.xticks(fontsize=14)\nax1 = plt.ylabel('correlation coefficient',fontsize=15)\nax1 = plt.yticks(fontsize=14)\nax1 = plt.title('correlation coefficient between b_value and \\nseveral lagged versions of the injection data',fontsize=16,y=1.05)\n\nax2 = fig.add_subplot(2,2,2)\n# plot coef for the lagged version and a linear-regression-line\nax2.scatter(dic_GIW_shift['GIW_shift2'],dic_shorter_b_value['shorter_b_value2'],marker='o')\nm, b = np.polyfit(dic_GIW_shift['GIW_shift2'],dic_shorter_b_value['shorter_b_value2'], 1)\nax2 = plt.plot(dic_GIW_shift['GIW_shift2'],m*dic_GIW_shift['GIW_shift2']+b,color='k')\n# add coef correlation at location x and y\n# ax2 = plt.text(2500000, 52, 'R=0.34',fontsize=15)\n# # Label axes and show plot\nax2 = plt.xlabel('2 Months lagged version of injection data',fontsize=15)\nax2 = plt.xticks(fontsize=14)\nax2 = plt.ylabel('monthly b_value',fontsize=15)\nax2 = plt.yticks(fontsize=14)\nax2 = plt.title('Injection data (2 months shifted) versus \\n b_value',fontsize=16,y=1.05)\n\n\nplt.show()    ","2a1d136a":"def plot_distribution(df,variable):\n    ax = sns.displot(data=df, x=variable, kde=True)\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(df[variable])\n    print( '\\n mu = {:.2E} and sigma = {:.2E}\\n'.format(mu, sigma))\n\n    #Now plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2E} and $\\sigma=$ {:.2E} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title('moment distribution')\n\n    #Get also the QQ-plot\n    fig = plt.figure()\n    res = stats.probplot(df[variable], plot=plt)\n    plt.show()\n    \n    #skewness and kurtosis\n    print(\"Skewness: %f\" % df[variable].skew())\n    print(\"Kurtosis: %f\" % df[variable].kurt())","8e412c50":"plot_distribution(df_b_value,'b_value')","eb9cef28":"plot_distribution(monthly_energy_released,'Energy')","f31feed2":"# calculate log b_value\ndf_b_value['log_b_value'] = np.log1p(df_b_value['b_value'])\n# calculate log Energy\nmonthly_energy_released['log_Energy'] = np.log1p(monthly_energy_released['Energy'])\n\n#skewness and kurtosis\nprint(\"Skewness log_b_value: %f\" % df_b_value['log_b_value'].skew())\nprint(\"Kurtosis log_b_value: %f\" % df_b_value['log_b_value'].kurt())\nprint('')\nprint(\"Skewness log_Energy: %f\" % monthly_energy_released['log_Energy'].skew())\nprint(\"Kurtosis log_Energy: %f\" % monthly_energy_released['log_Energy'].kurt())\n","e519a8b4":"# calcul correlation between the 2 target variables\nmonthly_energy_released['log_Energy'].corr(df_b_value['b_value'])","0768c57c":"def extract_features(df):\n    df_features = pd.DataFrame()\n    df_features[\"sum\"] = df.sum(axis=1)\n    df_features[\"max\"] = df.max(axis=1)\n    df_features[\"mean\"] = df.mean(axis=1)\n    df_features[\"std\"] = df.std(axis=1)\n    df_features[\"skew\"] = df.skew(axis=1)\n    df_features[\"kurtosis\"] = df.kurtosis(axis=1)\n    df_features[\"5pct\"] = df.quantile(.05,axis=1)\n    df_features[\"10pct\"] = df.quantile(.1,axis=1)\n    df_features[\"15pct\"] = df.quantile(.15,axis=1)\n    df_features[\"20pct\"] = df.quantile(.2,axis=1)\n    df_features[\"25pct\"] = df.quantile(.25,axis=1)\n    df_features[\"30pct\"] = df.quantile(.3,axis=1)\n    df_features[\"35pct\"] = df.quantile(.35,axis=1)\n    df_features[\"40pct\"] = df.quantile(.4,axis=1)\n    df_features[\"45pct\"] = df.quantile(.45,axis=1)\n    df_features[\"50pct\"] = df.quantile(.50,axis=1)\n    df_features[\"55pct\"] = df.quantile(.55,axis=1)\n    df_features[\"60pct\"] = df.quantile(.60,axis=1)\n    df_features[\"65pct\"] = df.quantile(.65,axis=1)\n    df_features[\"70pct\"] = df.quantile(.70,axis=1)\n    df_features[\"75pct\"] = df.quantile(.75,axis=1)\n    df_features[\"80pct\"] = df.quantile(.80,axis=1)\n    df_features[\"85pct\"] = df.quantile(.85,axis=1)\n    df_features[\"90pct\"] = df.quantile(.90,axis=1)\n    df_features[\"95pct\"] = df.quantile(.95,axis=1)\n    return df_features","26437ed8":"def create_lag(df):\n    df_shift1 = pd.DataFrame()\n    df_shift1 = df.shift(1)\n    df_shift1 = df.add_prefix('S1_')\n    df_shift2 = pd.DataFrame()\n    df_shift2 = df.shift(2)\n    df_shift2 = df.add_prefix('S2_')\n    df_shift3 = pd.DataFrame()\n    df_shift3 = df.shift(3)\n    df_shift3 = df_shift3.add_prefix('S3_')\n    df_shift4 = pd.DataFrame()\n    df_shift4 = df.shift(4)\n    df_shift4 = df_shift4.add_prefix('S4_')\n\n    df_lags = pd.concat([df,df_shift1,df_shift2,df_shift3, df_shift4], axis=1)\n    df_lags = df_lags.dropna() \n    return df_lags","73157418":"# create new dataframe with new features\ndf_fearures_GIW = extract_features(df_GIW_per_well)\ndf_fearures_RIW = extract_features(df_RIW_per_well)\ndf_fearures_days = extract_features(df_days_per_well)\n# add prefix\ndf_fearures_GIW = df_fearures_GIW.add_prefix('GIW_')\ndf_fearures_RIW = df_fearures_RIW.add_prefix('RIW_')\ndf_fearures_days =df_fearures_days.add_prefix('days_')\n# concat df\ndf_features = pd.concat([df_fearures_GIW,df_fearures_RIW,df_fearures_days],axis=1)\ndf_features.shape","be4355cd":"# add lag versions :\ndf_features_lag = create_lag(df_features)\ndf_features_lag.shape","90b59f6d":"# feature reduction\npca = PCA(n_components=0.99)\ndf_features_lag_reduced =  pd.DataFrame(pca.fit_transform(df_features_lag))\ndf_features_lag_reduced = df_features_lag_reduced.add_prefix('PCA_FEATURE_AXE_')\ndf_features_lag_reduced.shape","9b7aca2d":"df_GIW_per_well_lag = create_lag(df_GIW_per_well)\ndf_RIW_per_well_lag = create_lag(df_RIW_per_well)\ndf_days_per_well_lag = create_lag(df_days_per_well)\ndf_GIW_per_well_lag.shape","c9565231":"# scale the 3 dataframes with: GIW, RIW, days \nscaler = StandardScaler()\ndf_GIW_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_GIW_per_well_lag))\ndf_GIW_per_well_lag_scaled = df_GIW_per_well_lag_scaled.add_prefix('GIW_')\ndf_RIW_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_RIW_per_well_lag))\ndf_RIW_per_well_lag_scaled = df_RIW_per_well_lag_scaled.add_prefix('RIW_')\ndf_days_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_days_per_well_lag))\ndf_days_per_well_lag_scaled = df_days_per_well_lag_scaled.add_prefix('days_')\n\n# concat the 3 df\ndf_injection_data_scaled = pd.concat([df_GIW_per_well_lag_scaled,\n                                      df_RIW_per_well_lag_scaled,\n                                      df_days_per_well_lag_scaled],axis=1)\ndf_injection_data_scaled.head(2)\n","62ad97c1":"# feature reduction\npca = PCA(n_components=0.99)\ndf_injection_data_scaled_reduced = pd.DataFrame(pca.fit_transform(df_injection_data_scaled))\ndf_injection_data_scaled_reduced = df_injection_data_scaled_reduced.add_prefix('PCA_inj_AXE_')\ndf_injection_data_scaled_reduced.shape\n","f1ffc650":"print(pca.explained_variance_ratio_)","6a5a4cb6":"# create list to select features that correlates the best wiht seismic energy\nlist_variable_with_high_corr_Ener=[]\n# create list to select features that correlates the best wiht b-value\nlist_variable_with_high_corr_bval=[]\n","b104e467":"def plot_correlation(df,var1,var2,color):\n    \"\"\"\n    plot data and a linear regression model fit. \n    Parameters\n    ----------\n    df : dataframe\n    var1: 'column_name' of the variable 1 in df\n    var2: 'column_name' of the variable 2 in df\n    color : color scatter points\n    Returns\n    -------\n    Figure  \n    \"\"\"\n    # transform var1 and var2 into numpy array:\n    xm = np.array(df[var1])\n    ym = np.array(df[var2])\n    # get regression line properties:\n    slope, intercept, r_value, p_value, std_err = stats.linregress(xm, ym)\n    # Plot linear regression with 95% confidence interval and the regression coefficient\n    sns.regplot(x=var1,y=var2,data=df,fit_reg=True,color = color,\n                line_kws={'label':\"R={:.2f}\".format(r_value),\"color\": \"black\"}) \n    # axes and title properties\n    plt.xlabel(var1,fontsize=15)\n    plt.ylabel(var2,fontsize=15)\n    # plot legend\n    plt.legend(prop={'size': 15})","45401f32":"# resize targets (remove last 4 raws) to match size explanatory variables (4 rows missing due to drop NaN value after shift)\ntarget_Ener = monthly_energy_released['log_Energy'][:-4].reset_index()\ntarget_Ener = target_Ener.drop(columns = ['Date'],axis=1)\n\ntarget_bval = df_b_value['log_b_value'][:-4].reset_index()\ntarget_bval = target_bval.drop(columns = ['years_months'],axis=1)\n\n# injection data vs Energy: \ndf_injection_data_scaled_Ener = pd.concat([df_injection_data_scaled,target_Ener], axis=1)\ndf_pca_injection_data_scaled_Ener = pd.concat([df_injection_data_scaled_reduced,target_Ener], axis=1)\n\n# injection data vs b-value:\ndf_injection_data_scaled_bval = pd.concat([df_injection_data_scaled,target_bval], axis=1)\ndf_pca_injection_data_scaled_bval = pd.concat([df_injection_data_scaled_reduced,target_bval], axis=1)\n\n# features vs Energy:\ndf_features_lag2 = df_features_lag.reset_index().drop(columns = ['Date'],axis=1)\ndf_features_Ener = pd.concat([df_features_lag2,target_Ener],axis=1)\ndf_pca_features_Ener = pd.concat([df_features_lag_reduced,target_Ener], axis=1)\n\n# features vs b-value:\ndf_features_bval = pd.concat([df_features_lag2,target_bval],axis=1)\ndf_pca_features_bval = pd.concat([df_features_lag_reduced,target_bval], axis=1)\n\n# finally create one df with all the data\ndf_all_data = pd.concat([df_injection_data_scaled,df_injection_data_scaled_reduced,\n                         df_features_lag2,df_features_lag_reduced,                         \n                         target_Ener,target_bval], axis=1)\ndf_all_data = df_all_data.set_index(df_features_lag.index)","7b0719cf":"def calcul_sort_save_corr(df,variable,threshold=0.35):\n    # calcul, sort and save in df the correlation coefficient\n    corr = pd.DataFrame(df[df.columns[0:]].corr()[variable][:-1])\n    corr = pd.DataFrame(abs(corr[variable]).sort_values(ascending=False))\n\n    # select coef corr > 0.3 and append corresponding variables to a list\n    highest_corr = corr[corr[variable]>threshold]\n    print(corr.head(10))\n    return highest_corr","fbfac6fa":"corr_injection_Ener = calcul_sort_save_corr(df_injection_data_scaled_Ener,'log_Energy')","8487fd72":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between raw injection data and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_injection_data_scaled_Ener,'days_205','log_Energy','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_injection_data_scaled_Ener,'RIW_222','log_Energy','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_injection_data_scaled_Ener,'GIW_58','log_Energy','red')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_correlation(df_injection_data_scaled_Ener,'GIW_10','log_Energy','black')\n\nplt.show()","6914a470":"corr_pca_injection_Ener = calcul_sort_save_corr(df_pca_injection_data_scaled_Ener,'log_Energy',threshold=0.26)","c4b0aa07":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from raw injection data and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_4','log_Energy','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_1','log_Energy','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_6','log_Energy','red')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_21','log_Energy','black')\n\nplt.show()","9dfef394":"corr_features_Ener = calcul_sort_save_corr(df_features_Ener,'log_Energy')","3fdce33f":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between extracted features and seismic energy', fontsize=18,y=0.95)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_features_Ener,'S4_days_90pct','log_Energy','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_features_Ener,'S4_days_std','log_Energy','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_features_Ener,'S4_days_kurtosis','log_Energy','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_features_Ener,'S4_days_skew','log_Energy','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_features_Ener,'S2_GIW_95pct','log_Energy','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_features_Ener,'S1_GIW_95pct','log_Energy','darkblue')\n\nplt.show()","e137c833":"corr_pca_features_Ener = calcul_sort_save_corr(df_pca_features_Ener,'log_Energy')","3a10ea9d":"# define figure size\nfig = plt.figure(figsize=(18,3))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from extracted features and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(1,2,1)\nax1 = plot_correlation(df_pca_features_Ener,'PCA_FEATURE_AXE_0','log_Energy','blue')\nax2 = fig.add_subplot(1,2,2)\nax2 = plot_correlation(df_pca_features_Ener,'PCA_FEATURE_AXE_2','log_Energy','green')\n\nplt.show()","49c8fa37":"corr_injection_bval = calcul_sort_save_corr(df_injection_data_scaled_bval,'log_b_value',threshold = 0.75)","9babe4d6":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between raw injection data and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_injection_data_scaled_bval,'days_162','log_b_value','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_injection_data_scaled_bval,'days_210','log_b_value','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_injection_data_scaled_bval,'days_234','log_b_value','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_injection_data_scaled_bval,'days_186','log_b_value','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_injection_data_scaled_bval,'GIW_234','log_b_value','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_injection_data_scaled_bval,'GIW_186','log_b_value','darkblue')\n\nplt.show()","879c124e":"corr_pca_injection_bval = calcul_sort_save_corr(df_pca_injection_data_scaled_bval,'log_b_value',threshold = 0.70)","2b7b6bbb":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from raw injection data and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_0','log_b_value','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_2','log_b_value','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_3','log_b_value','red')\n\nplt.show()","aeac126c":"corr_features_bval = calcul_sort_save_corr(df_features_bval,'log_b_value',threshold = 0.74)","4f616bcd":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between extracted features and b-value', fontsize=18,y=.95)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_features_bval,'S4_days_skew','log_b_value','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_features_bval,'S3_days_skew','log_b_value','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_features_bval,'S4_days_kurtosis','log_b_value','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_features_bval,'S4_days_sum','log_b_value','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_features_bval,'S4_days_mean','log_b_value','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_features_bval,'S3_days_sum','log_b_value','darkblue')\n\nplt.show()","727708e2":"corr_pca_features_bval = calcul_sort_save_corr(df_pca_features_bval,'log_b_value',threshold = 0.70)","d1c6e96f":"# define figure size\nfig = plt.figure(figsize=(18,3))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from extracted features and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(1,2,1)\nax1 = plot_correlation(df_pca_features_bval,'PCA_FEATURE_AXE_3','log_b_value','blue')","bd85fb97":"print(df_features_Ener['log_Energy'].sort_values(ascending=True).head(3))","968d9c4a":"positon_outliers = [16]\n\n# remove outlier in df with Energy\ndf_injection_data_scaled_Ener = df_injection_data_scaled_Ener.drop(df_injection_data_scaled_Ener.index[positon_outliers])\ndf_pca_injection_data_scaled_Ener = df_pca_injection_data_scaled_Ener.drop(df_pca_injection_data_scaled_Ener.index[positon_outliers])\ndf_features_Ener = df_features_Ener.drop(df_features_Ener.index[positon_outliers])\ndf_pca_features_Ener = df_pca_features_Ener.drop(df_pca_features_Ener.index[positon_outliers])\n\n# remove outlier in df with b_value\ndf_injection_data_scaled_bval = df_injection_data_scaled_bval.drop(df_injection_data_scaled_bval.index[positon_outliers])\ndf_pca_injection_data_scaled_bval = df_pca_injection_data_scaled_bval.drop(df_pca_injection_data_scaled_bval.index[positon_outliers])\ndf_features_bval = df_features_bval.drop(df_features_bval.index[positon_outliers])\ndf_pca_features_bval = df_pca_features_bval.drop(df_pca_features_bval.index[positon_outliers])\n\n# remove outlier in df with all the data\ndf_all_data = df_all_data.drop(df_all_data.index[positon_outliers])\n","e74ebbf5":"# re-calculate coefficient of correlations:\ncorr_injection_Ener = calcul_sort_save_corr(df_injection_data_scaled_Ener,'log_Energy',threshold=0.30)","fda63817":"corr_pca_injection_Ener = calcul_sort_save_corr(df_pca_injection_data_scaled_Ener,'log_Energy',threshold=0.28)","bb7b21a7":"corr_features_Ener = calcul_sort_save_corr(df_features_Ener,'log_Energy',threshold=0.30)","9600fe0c":"corr_pca_features_Ener = calcul_sort_save_corr(df_pca_features_Ener,'log_Energy',threshold=0.26)","140c7739":"corr_injection_bval = calcul_sort_save_corr(df_injection_data_scaled_bval,'log_b_value',threshold=0.65)","a5aa3920":"corr_pca_injection_bval = calcul_sort_save_corr(df_pca_injection_data_scaled_bval,'log_b_value',threshold=0.65)","cd9efb43":"corr_features_bval = calcul_sort_save_corr(df_features_bval,'log_b_value',threshold=0.65)","f49ae92b":"corr_pca_features_bval = calcul_sort_save_corr(df_pca_features_bval,'log_b_value',threshold=0.65)","c307ad60":"# concat df with meaningful features\nmeaningful_features_Energy = pd.concat([corr_injection_Ener,corr_pca_injection_Ener,\n                                        corr_features_Ener,corr_pca_features_Ener],axis=0)\nmeaningful_features_Energy = pd.DataFrame(abs(meaningful_features_Energy['log_Energy']).sort_values(ascending=False)).dropna()\nprint(meaningful_features_Energy.shape)\nmeaningful_features_Energy.head(5)","35972a01":"# concat df with meaningful features\nmeaningful_features_bval = pd.concat([corr_injection_bval,corr_pca_injection_bval,\n                                        corr_features_bval,corr_pca_features_bval],axis=0)\nmeaningful_features_bval = pd.DataFrame(abs(meaningful_features_bval['log_b_value']).sort_values(ascending=False)).dropna()\nprint(meaningful_features_bval.shape)\nmeaningful_features_bval.head(5)","09ce4550":"# select only variables highly correlated with targets\nX_Ener = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_Energy.index]]\nX_Ener.head(2)","c7d2b044":"# select only variables highly correlated with targets\nX_bval = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_bval.index]]\nX_bval.head(3)","88956f58":"target_Ener = df_all_data['log_Energy'].values\ntarget_bval = df_all_data['log_b_value'].values","365bd070":"X_train_Ener, X_test_Ener, y_train_Ener, y_test_Ener = train_test_split(X_Ener, target_Ener,test_size = .3, random_state=0)\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(X_bval, target_bval,test_size = .3, random_state=0)","838e4d08":"def get_best_score(grid):\n    best_score = grid.best_score_\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    return best_score\n\ndef model_evaluation(y_test,prediction):\n    r2 = round(metrics.r2_score(y_test, prediction), 2)\n    abs_perc_error = np.mean(np.abs((y_test-prediction)\/prediction))\n    mean_abs_err = metrics.mean_absolute_error(y_test, prediction)\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction))\n    print(\"R2 (explained variance):\",r2 )\n    print(\"Mean Absolute Perc Error (\u03a3(|y-pred|\/y)\/n):\", abs_perc_error)\n    print(\"Mean Absolute Error (\u03a3|y-pred|\/n):\", \"{:,f}\".format(mean_abs_err))\n    print(\"Root Mean Squared Error (sqrt(\u03a3(y-pred)^2\/n)):\", \"{:,f}\".format(rmse))\n    ## residuals\n#     prediction = prediction.reshape(len(prediction),1)\n    residuals = y_test - prediction\n    if abs(max(residuals)) > abs(min(residuals)):\n        max_error = max(residuals)  \n    else:\n        max_error = min(residuals) \n    max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))\n    # max_true = y_test[max_idx]\n    max_pred = prediction[max_idx]\n    print(\"Max Error:\", \"{}\".format(max_error))\n    \n    ## Plot predicted vs true\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n    ax[0].scatter(prediction, y_test, color=\"black\")\n    abline_plot(intercept=0, slope=1, color=\"red\", ax=ax[0])\n    # ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[0].grid(True)\n    ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")\n    ax[0].legend()\n\n    ## Plot predicted vs residuals\n    ax[1].scatter(prediction, residuals, color=\"red\")\n    ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[1].grid(True)\n    ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n    ax[1].hlines(y=0, xmin=np.min(prediction), xmax=np.max(prediction))\n    ax[1].legend()\n    plt.show()\n\n    print('The model explains {}% of the variance of the target variable.'.format(r2*100))\n    print('On average, predictions have an error of {:,.2f}, or they\u2019re wrong by {:,.2f}%.'.format(mean_abs_err,(abs_perc_error)*100)) \n#     print('the average difference between the predicted value and the actual value is {:,.2f}%: '.format(abs_perc_error*100))\n    print('The biggest error on the test set was over {:,.2f}.'.format(max_error))\n","7899a770":"def plot_nb_feature_vs_score(df):\n    # PLOT RESULT:\n    df.plot('number_feat', 'best_score')\n    # Returns index of minimun best_score\n    index = df[['best_score']].idxmax() \n    # get the number of features used to have the best score \n    print(df['number_feat'][index])\n    \ndef plot_feature_importance(importance,n,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names[:n],'feature_importance':feature_importance[:n]}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(15,10))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","2f30e653":"def select_model(model_ini,thres, X_train,X_test):   \n    selection = SelectFromModel(model_ini, threshold=thres, prefit=True)\n    n_features = selection.transform(X_train).shape[1]\n    selected_vars = list(X_train.columns[selection.get_support()])\n    X_train_selected = X_train[selected_vars]\n    X_test_selected = X_test[selected_vars]\n    return X_train_selected,X_test_selected","df1596e9":"def perform_grid_search(model,random_grid,X_train, y_train,thres):\n    random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 300, cv = 4, \n                               verbose=2, random_state=12,scoring ='explained_variance')\n    # Fit the random search model\n    random.fit(X_train, y_train)\n    # print output\n    print('-'*60)\n    print('Results from Grid Search with threshold = {}'.format(thres))\n    print(\"Best score:\",random.best_score_)\n    print(\"with the following parameters :\\n\",random.best_params_)","8c3d36b6":"lasso_score = []\nnumber_feature = []\nfor i in range (1,X_Ener.shape[1]):\n    # select diferent \n    columns = meaningful_features_Energy.index[:i]\n    X = X_Ener[columns].values\n    y = target_Ener\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n    \n    lasso = Lasso(normalize=True)\n    parameters = {'alpha': [1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,2],\n              'tol':[1e-6,1e-5,1e-4,1e-3,1e-2]}\n    grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=0, scoring = 'explained_variance')\n    grid_lasso.fit(X_train, y_train)\n    \n    sc_lasso = get_best_score(grid_lasso)\n    lasso_score.append(sc_lasso)\n    number_feature.append(i)\n\nresult_lasso =  pd.DataFrame(zip(number_feature,lasso_score),columns = ['number_feat', 'best_score'])","3ccca521":"plot_nb_feature_vs_score(result_lasso)","afc40626":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# Lasso regression model and gridsearch\ngrid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_lasso.fit(X_train, y_train)\n\nsc_lasso = get_best_score(grid_lasso)","dee4cb1e":"## best fit: \nlasso = Lasso(alpha= 0.01, normalize=True,tol = 1e-05)\n## fit the model. \nlasso.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = lasso.predict(X_test)\n\n#add score to list\nr2_Lasso = metrics.r2_score(y_test, prediction)\nmse_Lasso = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","6ba64d8e":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=12, verbose=1 , scoring ='explained_variance')\ngrid_linear.fit(X_train, y_train)\n\nsc_linear = get_best_score(grid_linear)","68461f78":"model_linreg = LinearRegression(copy_X= True, fit_intercept= True, normalize= False)\nmodel_linreg.fit(X_train,y_train)\nprediction = model_linreg.predict(X_test)\n\n#add score to list\nr2_linreg = metrics.r2_score(y_test, prediction)\nmse_linreg = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","9342e8ad":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nridge = Ridge(normalize=True)\nparameters = {'alpha':[1e-6,1e-5,1e-4,1e-3,1e-2,0.1,0.5,1], \n              'tol':[1e-9,1e-7,1e-6,1e-5,1e-4]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_ridge.fit(X_train, y_train)\n\nsc_ridge = get_best_score(grid_ridge)","38edc249":"## best fit: \nridge_bf = Ridge(alpha= 1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = ridge_bf.predict(X_test)\n\n#add score to list\nr2_ridge = metrics.r2_score(y_test, prediction)\nmse_ridge = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","5155adad":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# Lasso regression model and gridsearch\nelastic_reg = ElasticNet()\nparameters = {\n    'alpha': [1e-6,1e-5, 1e-4, 5e-3,1e-3,5e-2, 1e-2, 5e-1, 1e-1,1.0],\n    'l1_ratio': [1e-6,1e-5, 1e-4,5e-3, 1e-3,5e-2, 1e-2, 5e-1,1e-1,1.0],\n    'tol': [1e-6,1e-5, 1e-4,1e-3]}\ngrid_elas = GridSearchCV(elastic_reg, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_elas.fit(X_train, y_train)\n\nsc_elas = get_best_score(grid_elas)","14b2395c":"## best fit: \nelastic = ElasticNet(alpha= 1.0,l1_ratio= 0.005,tol=1e-05)\n## fit the model. \nelastic.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = elastic.predict(X_test)\n\n#add score to list\nr2_elas = metrics.r2_score(y_test, prediction)\nmse_elas = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","9f619fc1":"# data selection for decision tree methods\ndf_all_data2 = df_all_data.drop(columns=['log_Energy','log_b_value'], axis =1)\nX_train_Ener, X_test_Ener, y_train_Ener, y_test_Ener = train_test_split(df_all_data2, target_Ener,test_size = .3, random_state=0)\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(df_all_data2, target_Ener,test_size = .3, random_state=0)\n","2378d694":"# initial random forest regressor model, fit it \nmodel_rf_ini = RandomForestRegressor(n_estimators= 100,random_state = 0)\nmodel_rf_ini.fit(X_train_Ener, y_train_Ener)\n# extract and plot important features\nplot_feature_importance(model_rf_ini.feature_importances_,60,X_train_Ener.columns,'Random Forest Regressor: ')","48b12f49":"# Create the random grid\nrandom_grid_rf = {\n                # measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","b3e54548":"# # do first random search with threshold = 0.0002\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0002, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0002)","766930c3":"# # do first random search with threshold = 0.0004\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0004, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0004)","a0b49e3e":"# # do first random search with threshold = 0.0006\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0006, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0006)","4a64cd50":"# # do first random search with threshold = 0.0008\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0008, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0008)","429bf108":"# # do first random search with threshold = 0.0009\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0009, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0009)","d169ad64":"# # # do first random search with threshold = 0.001\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.001, X_train_Ener, X_test_Ener)\n# # # initiat model\n# model_rf = RandomForestRegressor()\n# # # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.001)","e4d95130":"# # do first random search with threshold = 0.0011\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0011, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0011)","bc6338b8":"# # do first random search with threshold = 0.0012\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0012, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0012)","e8df8a25":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0013)","924cf89d":"# # do first random search with threshold = 0.0014\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0014, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0014)","1322d9f9":"# # do first random search with threshold = 0.0015\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0015, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0015)","55ba3823":"# # do first random search with threshold = 0.00175\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.00175, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.00175)","5a61292a":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\n# model_rf = RandomForestRegressor()\n\n# param_grid = {\n#     'bootstrap': [True],\n#     'criterion': ['mae'],\n#     'max_depth': range(45,55,1),\n#     'max_features': ['log2'],\n#     'min_samples_leaf': [1,2,3,4,5],\n#     'min_samples_split': [5,6,7,8,9],\n#     'n_estimators': [100],\n#     'random_state': [0]\n# }\n\n# grid_search = GridSearchCV(estimator = model_rf, param_grid = param_grid, cv = 12, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_Ener)\n# grid_search.best_params_","7a8c6502":"# do first random search with threshold = 0.02\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 45,max_features = 'log2',\n                                min_samples_leaf = 2, min_samples_split = 6, n_estimators = 100,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_Ener)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_Ener, prediction)\nmse_rf = metrics.mean_squared_error(y_test_Ener, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)","d700ca17":"# do first random search with threshold = 0.02\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 45,max_features ='log2',\n                                min_samples_leaf = 2, min_samples_split = 6, n_estimators = 3000,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_Ener)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_Ener, prediction)\nmse_rf = metrics.mean_squared_error(y_test_Ener, prediction)\n\nprint(r2_rf)","62e89f22":"# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_Ener, label=y_train_Ener)\n\n# Instantiate the initial regressor model: model_gbm_ini\nmodel_gbm_ini = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,eval_metric='rmse',seed=42)\n\n# Fit andomized_mse to the data\nmodel_gbm_ini.fit(X_train_Ener,y_train_Ener)","a6a749da":"plot_feature_importance(model_gbm_ini.feature_importances_,60,X_train_Ener.columns,'xgboost regression: ')","bda22a84":"# initial the parameters for random search\nrandom_grid_gbm = {\n    'colsample_bytree': [0.6,0.7,0.8,0.9,1],\n    'max_depth': range(1,11,1),\n    'eta' : [0.01,0.05,0.1, 0.15, 0.2],\n    'alpha': [0,0.3,0.6,0.9,1],\n    'min_child_weight': [1],\n    'scale_pos_weight' : [1],\n    'n_estimators' :  [100],\n     'seed' : [42]\n    \n}","598ecbb3":"def perform_grid_search_xgb(model,random_grid,X_train, y_train,thres):\n    # Create the DMatrix: \n    train_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n    random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 300, cv = 4, \n                               verbose=2, random_state=12,scoring ='explained_variance')\n    # Fit the random search model\n    random.fit(X_train, y_train)\n    # print output\n    print('-'*60)\n    print('Results from Grid Search with threshold = {}'.format(thres))\n    print(\"Best score:\",random.best_score_)\n    print(\"with the following parameters :\\n\",random.best_params_)","2bd9f331":"# # do first random search with threshold = 0.001\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.001, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.001)","32636128":"# # do first random search with threshold = 0.002\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.002, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.002)","7176d28d":"# # do first random search with threshold = 0.003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.003, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.003)","a489746c":"# # do first random search with threshold = 0.004\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.004, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.004)","b305da78":"# # do first random search with threshold = 0.005\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.005, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.005)","5c6afa1e":"# # do first random search with threshold = 0.006\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.006, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.006)","eab93d6d":"# # do first random search with threshold = 0.007\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.007)","6cd16813":"# # do first random search with threshold = 0.008\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.008, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.008)","73ce2206":"# # do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.009, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.009)","9aa93a73":"# #### do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# gbm_param_grid = {\n#     'colsample_bytree': [0.65,0.67,0.69, 0.7,0.72,0.74],\n#     'max_depth': range(1,4,1),   \n#     'eta' : [0.13,0.14,0.15, 0.16, 0.17],\n#     'alpha': [0.4,0.5,0.6,0.7,0.8],\n#     'min_child_weight': [1,1.5],\n#     'scale_pos_weight' : [1,1.5],\n#     'n_estimators' :  [100],\n#      'seed' : [42],\n    \n# }\n\n# grid_search = GridSearchCV(estimator = model_gbm, param_grid = gbm_param_grid, cv = 4, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_Ener)\n# grid_search.best_params_\n# grid_search.best_score_","1137d077":"import xgboost as xgb\n\n# do first random search with threshold = 0.035\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_selected, label=y_train_Ener)\n\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.67,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)","0f39c201":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.07,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_Ener, prediction), 2)\nprint(r2)","f09f6c65":"# decrease subsample from 1 to 0.5\nimport xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.07,subsample=0.5,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_Ener, prediction), 2)\nprint(r2)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)\n\n#add score to list\nr2_xgb = metrics.r2_score(y_test, prediction)\nmse_xgb = metrics.mean_squared_error(y_test, prediction)","b73efc57":"# create list with the model score\nlist_score_r2 = [r2_linreg,r2_ridge,r2_Lasso,r2_elas,r2_rf,r2_xgb]\nlist_score_mse = [mse_linreg,mse_ridge,mse_Lasso,mse_elas,mse_rf,mse_xgb]\n# create list with model name\nlist_regressors = ['linear','Ridge','Lasso','ElaNet','RF','xgboost']\n\n# create dictionnary and dataframe\ndic_score = {'model': list_regressors,\n            'score_R2':list_score_r2,\n            'score_mse':list_score_mse,}\n\ndic_score = pd.DataFrame(dic_score)\ndic_score","9956a8b8":"# Plot the predictions for each model\nfig, axes = plt.subplots(2,figsize=(15,5))\nax = plt.subplot(1,2,1)\nax = sns.pointplot(x = \"model\", y = \"score_R2\", data = dic_score) \nax.set_ylabel('Score (R2)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n     ax.text(ind,dic_score['score_R2'][ind]+0.002,'{:.5f}'.format(dic_score['score_R2'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nax2 = plt.subplot(1,2,2)\nax2 = sns.pointplot(x = \"model\", y = \"score_mse\", data = dic_score) \nax2.set_ylabel('Score (MSE)', size=20, labelpad=12.5)\nax2.set_xlabel('Model', size=20, labelpad=12.5)\nax2.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n     ax2.text(ind,dic_score['score_mse'][ind]+0.002,'{:.5f}'.format(dic_score['score_mse'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nplt.title(\"Models' scores\", size=20)\nplt.savefig('Models scores Energy2.png')\n","9e63ae5f":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n## best fit: \nridge_bf = Ridge(alpha= 1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nlog_prediction = ridge_bf.predict(X_test)\nprediction =  np.exp(log_prediction)\noriginal_data = np.exp(y_test)\n\n","6e77d85e":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(10,5))\n      \nplt.plot(original_data,'-ob',label='original data')\nplt.plot(prediction,'-dr',label='prediction')\nplt.ylabel('monthly seimic energy',size=16)\nplt.legend()\n\nplt.title(\"Original data vs predicted\", size=20)\n\nplt.savefig('Energy -  original vs predicted.png')","00c2a8bb":"# select only variables highly correlated with targets\nX_bval = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_bval.index]]\n# select target: log_b_values\ntarget_bval = df_all_data['log_b_value'].values\n# split the data\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(X_bval, target_bval,test_size = .3, random_state=0)","8924e5c4":"lasso_score = []\nnumber_feature = []\nfor i in range (1,X_bval.shape[1]):\n    # select diferent \n    columns = meaningful_features_bval.index[:i]\n    X = X_bval[columns].values\n    y = target_bval\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n    \n    lasso = Lasso(normalize=True)\n    parameters = {'alpha': [1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,2],\n              'tol':[1e-6,1e-5,1e-4,1e-3,1e-2]}\n    grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=0, scoring = 'explained_variance')\n    grid_lasso.fit(X_train, y_train)\n    \n    sc_lasso = get_best_score(grid_lasso)\n    lasso_score.append(sc_lasso)\n    number_feature.append(i)\n\nresult_lasso =  pd.DataFrame(zip(number_feature,lasso_score),columns = ['number_feat', 'best_score'])","06baa717":"plot_nb_feature_vs_score(result_lasso)","e58e1a3b":"# # select columns that give best results\n# columns = meaningful_features_bval.index[:62]\n# X = X_bval[columns].values\n# y = target_bval\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# # Lasso regression model and gridsearch\n# grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=1, scoring = 'explained_variance')\n# grid_lasso.fit(X_train, y_train)\n\n# sc_lasso = get_best_score(grid_lasso)","6727b98e":"## best fit: \nlasso = Lasso(alpha= 0.0001, normalize=True,tol = 0.01)\n## fit the model. \nlasso.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = lasso.predict(X_test)\n\n#add score to list\nr2_Lasso = metrics.r2_score(y_test, prediction)\nmse_Lasso = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","06ce4332":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=12, verbose=1 , scoring ='explained_variance')\ngrid_linear.fit(X_train, y_train)\n\nsc_linear = get_best_score(grid_linear)","9fff31ad":"model_linreg = LinearRegression(copy_X= True, fit_intercept= True, normalize= False)\nmodel_linreg.fit(X_train,y_train)\nprediction = model_linreg.predict(X_test)\n\n#add score to list\nr2_linreg = metrics.r2_score(y_test, prediction)\nmse_linreg = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","66dcaa86":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nridge = Ridge(normalize=True)\nparameters = {'alpha':[1e-6,1e-5,1e-4,1e-3,1e-2,0.1,0.5,1], \n              'tol':[1e-9,1e-7,1e-6,1e-5,1e-4]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_ridge.fit(X_train, y_train)\n\nsc_ridge = get_best_score(grid_ridge)","d4944a45":"## best fit: \nridge_bf = Ridge(alpha= 0.1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = ridge_bf.predict(X_test)\n\n#add score to list\nr2_ridge = metrics.r2_score(y_test, prediction)\nmse_ridge = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","86864095":"# # select columns that give best results\n# columns = meaningful_features_bval.index[:62]\n# X = X_bval[columns].values\n# y = target_bval\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# # Lasso regression model and gridsearch\n# elastic_reg = ElasticNet()\n# parameters = {\n#     'alpha': [1e-6,1e-5, 1e-4, 5e-3,1e-3,5e-2, 1e-2, 5e-1, 1e-1,1.0],\n#     'l1_ratio': [1e-6,1e-5, 1e-4,5e-3, 1e-3,5e-2, 1e-2, 5e-1,1e-1,1.0],\n#     'tol': [1e-6,1e-5, 1e-4,1e-3]}\n# grid_elas = GridSearchCV(elastic_reg, parameters, cv=12, verbose=1, scoring = 'explained_variance')\n# grid_elas.fit(X_train, y_train)\n\n# sc_elas = get_best_score(grid_elas)","46613d05":"## best fit: \nelastic = ElasticNet(alpha= 0.05,l1_ratio= 1e-6,tol=1e-06)\n## fit the model. \nelastic.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = elastic.predict(X_test)\n\n#add score to list\nr2_elas = metrics.r2_score(y_test, prediction)\nmse_elas = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","4983a483":"# initial random forest regressor model, fit it \nmodel_rf_ini = RandomForestRegressor(n_estimators= 100,random_state = 0)\nmodel_rf_ini.fit(X_train_bval, y_train_bval)\n# extract and plot important features\nplot_feature_importance(model_rf_ini.feature_importances_,60,X_train_bval.columns,'Random Forest Regressor: ')","7437fb11":"# Create the random grid\nrandom_grid_rf = {\n                # measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split  \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","24029ed1":"# # do first random search with threshold = 0.002\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.002, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.002)","25be0200":"# # do first random search with threshold = 0.004\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.004, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.004)","20dcd50c":"# # do first random search with threshold = 0.006\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.006, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.006)","0d632b64":"# # do first random search with threshold = 0.008\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.008, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.008)","be7cd550":"# # do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.009, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.009)","ef82fed5":"# # do first random search with threshold = 0.01\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.01, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.01)","c1442994":"# # do first random search with threshold = 0.012\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.012, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.012)","80a22cd2":"# # do first random search with threshold = 0.014\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.014, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.014)","b68f0468":"# # do first random search with threshold = 0.016\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.016, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.016)","23f5d8d1":"# # do first random search with threshold = 0.018\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.018, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.018)","2184b8ff":"# measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split  \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","40ecd009":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_bval, X_test_bval)\n\n# model_rf = RandomForestRegressor()\n\n\n\n# param_grid = {\n#     'bootstrap': [True],\n#     'criterion': ['mse','mae'],\n#     'max_depth': range(75,155,5),\n#     'max_features': ['log2'],\n#     'min_samples_leaf': [1,2,3,4],\n#     'min_samples_split': [2,3,4],\n#     'n_estimators': [100],\n#     'random_state': [0]\n# }\n\n# grid_search = GridSearchCV(estimator = model_rf, param_grid = param_grid, cv = 6\n#                            , verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_bval)\n# grid_search.best_params_","036b5b66":"# do first random search with threshold = 0.08\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.008, X_train_bval, X_test_bval)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 75,max_features = 'log2',\n                                min_samples_leaf = 1, min_samples_split = 2, n_estimators = 100,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_bval)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_bval, prediction)\nmse_rf = metrics.mean_squared_error(y_test_bval, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_bval,prediction)","7cbfe213":"import xgboost as xgb\n\n# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_bval, label=y_train_bval)\n\n# Instantiate the initial regressor model: model_gbm_ini\nmodel_gbm_ini = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,eval_metric='rmse',seed=42)\n\n# Fit andomized_mse to the data\nmodel_gbm_ini.fit(X_train_bval,y_train_bval)","132cb5e9":"plot_feature_importance(model_gbm_ini.feature_importances_,60,X_train_bval.columns,'xgboost regression: ')","2c8fa2b2":"# # do first random search with threshold = 0.0001\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0001, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0001)","db2c505b":"# do first random search with threshold = 0.0002\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0002, X_train_bval, X_test_bval)\n\n# Instantiate the regressor: gbm\nmodel_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# perform gridsearchCH\nperform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0002)","3e8fdf1b":"# # do first random search with threshold = 0.0003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0003, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0003)","4a01a914":"# do first random search with threshold = 0.0004\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0004, X_train_bval, X_test_bval)\n\n# Instantiate the regressor: gbm\nmodel_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# perform gridsearchCH\nperform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0004)","a0163c08":"# # do first random search with threshold = 0.0005\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0005, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0005)","a3535a3b":"# #### do first random search with threshold = 0.0003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0003, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# gbm_param_grid = {\n#     'colsample_bytree': [0.75,0.77,0.79, 0.8,0.82,0.84],\n#     'max_depth': range(1,3,1),   \n#     'eta' : [0.16,0.18,0.2, 0.22, 0.24],\n#     'alpha': [0,0.1,0.2,0.3],\n#     'min_child_weight': [1],\n# #     'scale_pos_weight' : [1],\n#     'n_estimators' :  [100],\n#      'seed' : [42],\n    \n# }\n\n# grid_search = GridSearchCV(estimator = model_gbm, param_grid = gbm_param_grid, cv = 4, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_bval)\n# grid_search.best_params_\n# grid_search.best_score_","37c53e7e":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1,\n                        max_depth = 1, eta = 0.2,colsample_bytree = 0.8,\n                        alpha = 0.0,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_bval)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_bval, prediction), 2)\nprint(r2)","e9666c3b":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1,\n                        max_depth = 1, eta = 0.1,colsample_bytree = 0.6,\n                        alpha = 0.0,seed=42,scale_pos_weight=1,subsample = 0.8)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_bval)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n#add score to list\nr2_xgb = metrics.r2_score(y_test_bval, prediction)\nmse_xgb = metrics.mean_squared_error(y_test_bval, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_bval,prediction)","861375ab":"# create list with the model score\nlist_score_r2 = [r2_linreg,r2_ridge,r2_Lasso,r2_elas,r2_rf,r2_xgb]\nlist_score_mse = [mse_linreg,mse_ridge,mse_Lasso,mse_elas,mse_rf,mse_xgb]\n# create list with model name\nlist_regressors = ['linear','Ridge','Lasso','ElaNet','RF','xgboost']\n\n# create dictionnary and dataframe\ndic_score = {'model': list_regressors,\n            'score_R2':list_score_r2,\n            'score_mse':list_score_mse,}\n\ndic_score_bval = pd.DataFrame(dic_score)\ndic_score_bval","d36c0494":"# Plot the predictions for each model\nfig, axes = plt.subplots(2,figsize=(15,5))\nax = plt.subplot(1,2,1)\nax = sns.pointplot(x = \"model\", y = \"score_R2\", data = dic_score_bval,color='green') \nax.set_ylabel('Score (R2)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score_bval.index: \n     ax.text(ind,dic_score_bval['score_R2'][ind]+0.002,'{:.5f}'.format(dic_score_bval['score_R2'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nax2 = plt.subplot(1,2,2)\nax2 = sns.pointplot(x = \"model\", y = \"score_mse\", data = dic_score_bval,color='green') \nax2.set_ylabel('Score (MSE)', size=20, labelpad=12.5)\nax2.set_xlabel('Model', size=20, labelpad=12.5)\nax2.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score_bval.index: \n     ax2.text(ind,dic_score_bval['score_mse'][ind]+5e-5,'{:.5f}'.format(dic_score_bval['score_mse'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nplt.title(\"Models' scores \", size=20)\nplt.savefig('Models scores bvalues.png')\n","a0557c6a":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n## best fit: \nridge_bf = Ridge(alpha= 0.1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nlog_prediction = ridge_bf.predict(X_test)\nprediction =  np.exp(log_prediction)\noriginal_data = np.exp(y_test)","02c1dd25":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(10,5))\n      \nplt.plot(original_data,'-ob',label='original data')\nplt.plot(prediction,'-dr',color='green',label='prediction')\nplt.ylabel('monthly b-values',size=16)\nplt.legend()\n\nplt.title(\"Original data vs predicted\", size=20)\n\nplt.savefig('b values -  original vs predicted.png')","68c421a1":"<a id=\"4.3.2\"><\/a>\n### 4.3.2: prepare dataframe to calculate correlation coefficient","483d7918":"#### visualize original data vs prediction with RIDGE","f7636753":"## <a id=\"INJECTION_DATA\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2: INJECTION DATA<\/center><\/h3>","862ec390":"<a id=\"6.3.2\"><\/a>\n### 6.3.2: xgboost\n#### Features importance","eacd4654":"<a id=\"5.4\"><\/a>\n## 5.4: Decision tree methods ","582d0818":"Results from Grid Search with threshold = 0.0006\n\nBest score: **0.2249491184242004**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mae', 'bootstrap': True}\n","4b62150f":"#### GridSearchCV with threshold = 0.0013 (best score)","8c3d1df4":"#### Run best fit Lasso","bb45926c":"#### Function for features extraction","665eb5e2":"#### Run gridsearch with threshold 0.0003 ","65225809":"Results from Grid Search with threshold = 0.0015\n\nBest score: **0.25165823131968784**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 200, 'criterion': 'mae', 'bootstrap': True}","ff5c950f":"Results from Grid Search with threshold = 0.004\nBest score: **0.8717680156094305**\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False","6970eae4":"<a id=\"4.2.3\"><\/a>\n## 4.2.3: Injection data: scaling, lag version, and feature reduction","89fa8243":"best score : **0.40611976681211237**\n\n{'alpha': 0.4,\n 'colsample_bytree': 0.67,\n 'eta': 0.14,\n 'max_depth': 2,\n 'min_child_weight': 1.5,\n 'n_estimators': 100,\n 'scale_pos_weight': 1,\n 'seed': 42}","71f68cbe":"result gridsearch1:\n`bootstrap`: True,`criterion`: 'mae', `max_depth`: 75, `max_features`: 'log2', `min_samples_leaf`: 1,\n`min_samples_split`: 2,`n_estimators`: 100,`random_state`: 0","b71e7369":"<a id=\"6.1\"><\/a>\n## 6.1: Features selection and data split for linear models","f25428c8":"<a id=\"4.3\"><\/a>\n## 4.3: correlations between features and target variables","fa3b27c3":"#### feature selection and gridsearch","06942023":"Results from Grid Search with threshold = 0.001\n\nBest score: **0.23901575756703272**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0.6}","3e22c33d":"#### Best result with Elastic","79471e72":"### Function to perform GridsearchCV","561868f7":"#### Function for adding lag versions","4b442dff":"[1: SEISMIC DATA](#SEISMIC_DATA)\n\n- [1.1: Load the data](#1.1)\n- [1.2: Simple Statistical seismology](#1.2)\n    - [1.2.1: Plot the ECDF of the Earthquake magnitudes](#1.2)\n    - [1.2.2: Computing the b-value](#1.2.2) \n- [1.3: Seicmic activity evolution from 2003 to 2016](#1.3)\n    - [1.3.1: b-value evolution](#1.3)\n    - [1.3.2: monthly seismic energy released](#1.3.2)\n    - [1.3.3: Density maps of induced-earthquakes](#1.3.3)\n    \n[2: INJECTION DATA](#INJECTION_DATA)\n- [2.1: Load and prepare the data](#2.1)\n\n[3: INJECTION DATA VS INDUCED SEISMICITY](#INJ_VS_SEISM)\n- [3.1: injection vs seismic energy](#3.1)\n- [3.2: Injection vs b_value](#3.2)\n- [3.3: lag periods](#3.3)\n\n[4: DATA PREPARATION](#Data_Preparation)\n- [4.1: Dependent variables:](#4.1)\n    - [4.1.1: Check the asymmetry of the probability distribution](#4.1.1)\n    - [4.1.2: Log transform skewed targets:](#4.1.2)\n   \n- [4.2: Independent variables](#4.2) \n    - [4.2.1: define functions used for data preparation](#4.2)\n    - [4.2.2: Features extraction](#4.2.2)\n    - [4.2.3: Injection data: scaling, lag version, and feature reduction](#4.2.3)\n    \n    \n- [4.3: correlations between features and target variables](#4.3)\n    - [4.3.1: function used to plot correlations](#4.3.1)\n    - [4.3.2: prepare dataframe to calculate correlation coefficient](#4.3.2)\n    - [4.3.3:  correlation between seismic energy and:](#4.3.3)\n    - [4.3.4:  correlation between b value and:](#4.3.4)\n    \n - [4.5: Outliers detection:](#4.5)\n    \n[5: MACHINE LEARNING: prediction monthy seismic energy](#ML)\n- [5.1: Features selection and data split for linear models](#5.1)\n- [5.2: functions used for machine learning](#5.2)\n- [5.3: Linear model](#5.3)\n    - [5.3.1: Feature selection with Lasso](#5.3)\n    - [5.3.2: Linear regression](#5.3.2)\n    - [5.3.3: Ridge model](#5.3.3)\n    - [5.3.4: ElasticNet regression](#5.3.4)\n    \n    \n- [5.4: Decision tree methods](#5.4)\n    - [5.4.1: Random Forest Regressor](#5.4.1)\n    - [5.4.2: xgboost regression](#5.4.2)\n    \n- [5.5: SUMMARY PREDICTION ENERGY](#5.5)\n\n[6: MACHINE LEARNING: prediction monthy b-value](#ML2)\n- [6.1: Features selection and data split for linear models](#6.1)\n- [6.2: Linear model](#6.2)\n    - [6.2.1: Feature selection with Lasso](#6.2)\n    - [6.2.2: Linear regression](#6.2.2)\n    - [6.2.3: Ridge model](#6.2.3)\n    - [6.2.4: ElasticNet regression](#6.2.4)\n    \n- [6.3: Decision tree methods](#6.3)\n    - [6.3.1: Random Forest Regressor](#6.3)\n    - [6.3.2: xgboost](#6.3.2)\n    \n- [6.4: Summary -- predition monthly b-value --](#6.4)","41f2f3da":"<a id=\"5.2\"><\/a>\n## 5.2: functions used for machine learning\n### function to evaluate model prediction","090b4b4b":"#### PCA axes from injection data :","e3f03e44":"#### features obtained with PCA","2171d421":"The model has a better performance on the training set than on the testing set.\nso we will try to lower:\n\n- `colsample_bytree` (the ratio of features used),\n- `subsample` (the ratio of the training instances used),\n- `eta` (the learning rate of our GBM (i.e. how much we update our prediction with each successive tree).\n    \nand to increase:\n\n- `gamma` (the minimum loss reduction required to make a further split),\n- `min_child_weight` (the minimum sum of instance weight needed in a leaf)","c9096b55":"Results from Grid Search with threshold = 0.008\n\nBest score: **0.8817541154750542**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': False}","fecd37ac":"The seismicity at the Geysers follows the Gutenberg-Richter law quite well, with a b-around 1.","516cb0a8":"Both target variables have:\n- a distribution close from the normal distribution.\n- a low skewness. (means that the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.)\n\n- low Kurtosis. (means that the data tends to have heavy tails, or outliers)","6b54ae18":"#### distribution b_value","cae7d022":"Results from Grid Search with threshold = 0.0005\n\nBest score: **0.8773872353186494**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0}","0c7124c1":"<a id=\"6.4\"><\/a>\n## 6.4: Summary -- predition monthly b-value --","0cdd5b49":"Results from Grid Search with threshold = 0.002\n\nBest score: **0.869887463593535**\n\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': False}","7dbbb0d9":"## <a id=\"ML\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>5: MACHINE LEARNING: prediction monthy seismic energy<\/center><\/h3>","c2c23aee":"### gridsearchCH with threshold = 0.008 (best score)","6576e674":"Results from Grid Search with threshold = 0.0003\n\nBest score: **0.891920367880493**\nResults from Grid Search with threshold = 0.0003\n\nBest score: 0.891920367880493\n\nwith the following parameters : {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 1, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 1, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}","3e097f68":"<a id=\"2.1\"><\/a>\n## 2.1: Load and prepare the data\n\nInitially, I have downloaded 73 files for the 73 injection wells. In each file I have selected four columns:\n- date (YY\/MM),\n- \"Gross Injected (1000kg)\", which represent the volume of water injected during the month,\n- \"Water Injection Rate (1000 kg\/hr)\", \n- \"Days\", which is the number of days during the month were water was injected.\n\nThen, three files were generated:\n- \"Total Gross Injected Water\", with the \"Gross Injected (1000kg)\" for the 73 wells from 1969-05-01\tto 2021-02-01\n- \"Total Water Injection Rate\", with the \"Water Injection Rate (1000 kg\/hr)\" for the 73 wells from 1969-05-01\tto 2021-02-01\n- \"Total days_inj\", with the \"days\" for the 73 wells from 1969-05-01 to 2021-02-01\n\nIn these three files, the columns names are made with the API well numbers. An API is an \"unique, permanent, numeric identifier\" assigned to each well drilled for oil and gas in the United States.\n","444f1c34":"**Result gridsearch:**\n{'bootstrap': True,\n 'criterion': 'mae',\n 'max_depth': 45,\n 'max_features': 'log2',\n 'min_samples_leaf': 2,\n 'min_samples_split': 6,\n 'n_estimators': 100,\n 'random_state': 0}","d308afec":"#### Best result with linear model","95553562":"Here, we define the grid for random searh that going to be used for each model.","6c5fe3a0":"<a id=\"4.3.3\"><\/a>\n### 4.3.3:  correlation between seismic energy and:\n#### Raw injection data :","5245e2f6":"#### Best result with Ridge","36db9228":"Results from Grid Search with threshold = 0.004\n\nBest score: **0.3554615103664501**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 8, 'eta': 0.05, 'colsample_bytree': 0.9, 'alpha': 0.3}","60b03212":"<a id=\"5.4.1\"><\/a>\n### 5.4.1: Random Forest Regressor\n- First, we evaulate the importance of features.\n- Then, we create several models trained on different input features and we perform a RandomizedSearchCV on each model. Each model is made of 100 trees to speed the calculation.\n- Finally, we select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","a88be6ae":"<a id=\"5.3.2\"><\/a>\n### 5.3.2: Linear regression\n#### GridSearchCV() with number features = 22","04ad2dbe":"<a id=\"3.1\"><\/a>\n### 3.1: injection vs seismic energy","3e510737":"#### Run gridsearch with right number of features","93a8a980":"<a id=\"1.2.2\"><\/a>\n###  1.2.2: Computing the b-value ","e243936b":"# Injection induced seismic events at The Geysers geothermal field: 2003-2016\n\nThe Geysers is the world's largest geothermal field, containing a complex of 18 geothermal power plants, drawing steam from more than 350 wells, located in the Mayacamas Mountains approximately 72 miles north of San Francisco, California. Geysers produced about 20% of California's renewable energy in 2019.\n\nThe first commercial geothermal power plant in The Geysers in California was put into operation in September 1960  to tap natural steam. But, in the late 1980s, it was found that the flow of steam across the geothermal field had reduced and the reservoir was not recharging quickly enough to meet the required steam supply. As a result, inefficient power plants were shut down.\n\nThe geothermal reservoir is now recharged by injecting recycled wastewater from the city of Santa Rosa and the Lake County sewage treatment plants. 18 million gallons of treated wastewater is supplied each day. \n\nThe injection of cold water into this hot geothermal reservoir induced seismic events. A dense seismic network was installed to monitor the induced seismicity from 2003 to 2016 (data available here: http:\/\/ncedc.org\/egs\/catalog-search.html).\n\nHere, I have collected the injection data from 73 injection wells present in the Northwest of the geothermal field (data available here:https:\/\/www.conservation.ca.gov\/calgem\/Pages\/WellFinder.aspx) to try to investigate the relation between induced seismic events and water injection. \n","edf2d553":"<a id=\"5.3\"><\/a>\n## 5.3: Linear model\n### 5.3.1: Feature selection with Lasso","e925c27f":"**Import librairies**","21567bec":"#### Run gridsearch with right number of features","6332b645":"Results from Grid Search with threshold = 0.0002\n\nBest score: **0.8875462686991474**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 1, 'alpha': 0}","17f8241b":"<a id=\"4.2.2\"><\/a>\n## 4.2.2: Features extraction","fec1db1f":"<a id=\"6.2\"><\/a>\n## 6.2: Linear model\n### 6.2.1: Feature selection with Lasso","eaf61781":"#### features obtained with PCA","2e796c39":"Results from Grid Search with threshold = 0.0002\n\nBest score: **0.21704545681030946**\n\nwith the following parameters :\n\n{'random_state': 0, 'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","e4283e68":"<a id=\"1.3.3\"><\/a>\n### 1.3.3: Density maps of induced-earthquakes","b6b82786":"Results from Grid Search with threshold = 0.005\n\nBest score: **0.3678609550293852**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.6, 'alpha': 0.3}","5fed4783":"### Function for features selection","7f9f1895":"<a id=\"4.3.1\"><\/a>\n### 4.3.1: function used to plot correlations","173949fc":"Results from Grid Search with threshold = 0.0008\n\nBest score: **0.2327851268229801**\n\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': True}","9f14b880":"The model has a better performance on the training set than on the testing set.\nso we will try to lower:\n\n- `colsample_bytree` (the ratio of features used),\n- `subsample` (the ratio of the training instances used),\n- `eta` (the learning rate of our GBM (i.e. how much we update our prediction with each successive tree).\n    \nand to increase:\n\n- `gamma` (the minimum loss reduction required to make a further split),\n- `min_child_weight` (the minimum sum of instance weight needed in a leaf)","90efa9c5":"#### extracted features:","92448de2":"Results from Grid Search with threshold = 0.0009\n\nBest score: **0.23715507366773927**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","f24e0841":"Results from Grid Search with threshold = 0.008\n\nBest score: **0.3577037794408484**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.1, 'colsample_bytree': 0.7, 'alpha': 0}","c34ae0b0":"#### feature selection and gridsearch","6fe9348c":"<a id=\"5.1\"><\/a>\n## 5.1: Features selection and data split for linear models","6a541c41":"Results from Grid Search with threshold = 0.001\n\nBest score: **0.2386073321504737**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mae', 'bootstrap': True}","f17a9d1d":"Results from Grid Search with threshold = 0.00175\n\nBest score: **0.24612827184727618**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","8f58a72b":"Results from Grid Search with threshold = 0.0004\n\nBest score: **0.8814383131990927**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.1, 'colsample_bytree': 0.7, 'alpha': 0}","adb9ba77":"<a id=\"5.5\"><\/a>\n## 5.5: Summary -- predition monthly seismic energy released --","802509ca":"The model overfit the data. To reduce over fitting we can try to tune these parameters:\n\n- `n_estimators`: The more trees, the less likely the algorithm is to overfit. \n- `max_features`: This defines how many features each tree is randomly assigned, it could be lowered \n- `max_depth`: lowering this parameter reduce the complexity of the learned models, and so the over fitting risk. \n- `min_samples_leaf`: increase to obtain similar effect as the max_depth parameter.\n","8326c269":"Results from Grid Search with threshold = 0.002\n\nBest score:**0.3211045305680925**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 6, 'eta': 0.05, 'colsample_bytree': 0.7, 'alpha': 0.6}","7f29d4d5":"Results from Grid Search with threshold = 0.0012\n\nBest score: **0.254835044417125**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'mae', 'bootstrap': True}","8e10673b":"#### Best result with Elastic","9acf7d7b":"<a id=\"4.3.4\"><\/a>\n### 4.3.4:  correlation between b value and:\n#### raw injection data","1e701075":"Results from Grid Search with threshold = 0.006\n\nBest score: **0.3678609550293852**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.6, 'alpha': 0.3}","7dd62c20":"Results from Grid Search with threshold = 0.0013\n\nBest score: **0.26921830705419325**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'mae', 'bootstrap': True}","8f8d38cf":"#### feature selection","16c5849a":"Results from Grid Search with threshold = 0.018\n\nBest score: **0.8446134637581928**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","ba789b21":"### Visualize prediction made with Ridge","48735e60":"#### Best result with Ridge","a518f6f0":"Results from Grid Search with threshold = 0.0004\n\nBest score: **0.2181101677673559**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mae', 'bootstrap': True}","ab906f30":"<a id=\"1.1\"><\/a>\n## 1.1: Load the data","35990b17":"### run best gbm","badad874":"Results from Grid Search with threshold = 0.004\n\nBest score: **0.8717680156094305**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","613897ba":"See that the last month has only one day!","5422b0f0":"<a id=\"6.2.4\"><\/a>\n### 6.2.4: ElasticNet regression\n#### GridSearchCV() with number feature = 62","2ec03b2c":"<a id=\"6.2.2\"><\/a>\n### 6.2.2: Linear regression\n#### GridSearchCV() with number features = 22","822a2718":"#### Run gridsearchCV with threshold = 0.007","21ba84ec":"Results from Grid Search with threshold = 0.009\n\nBest score: **0.3547285700797372**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.05, 'colsample_bytree': 0.8, 'alpha': 0}","35985b8c":"The highest coefficient correlation was found after 2 months, meaning that seismicity peak occurs 2 months after peak injection.  Same with b-values.","da296179":"R2 evolves from 0.8 to 0.18","117115f9":"<a id=\"3.2\"><\/a>\n### 3.2: Injection vs b_value","e24cfe30":"<a id=\"5.3.4\"><\/a>\n### 5.3.4: ElasticNet regression\n#### GridSearchCV() with number feature = 22","4a06cada":"**Results for prediction monthly b-value**","305c5087":"We can clearly observe a saisonanlity in the amount of injected water (more injected water during the winter months and less during the summer months), in the amount of seismic energy released every month, and in the b_value. However, it seems there is a lag period between injection pick, seismicity pick, and b_value pick. ","52c69621":"Results from Grid Search with threshold = 0.009\n\nBest score: **0.8611314245563841**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': False}","f2b9508f":"<a id=\"1.3.2\"><\/a>\n### 1.3.2: monthly seismic energy released","d87b0df1":"<a id=\"3.3\"><\/a>\n### 3.3: lag periods","82dd517a":"Results from Grid Search with threshold = 0.006\n\nBest score: **0.8723766584804427**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': False}","ff7a3d2d":"Result gridsearchCV:\n\nBEST SCORE  = **0.891920367880493**, with:\n\n{'alpha': 0,\n 'colsample_bytree': 0.8,\n 'eta': 0.2,\n 'max_depth': 1,\n 'min_child_weight': 1,\n 'n_estimators': 100,\n 'seed': 42}","a3430dac":"We can observe that there is a negative value for the water injection rate (RIW). It is not possible, so we will replace it by '0'. ","6fdd7777":"## <a id=\"SEISMIC_DATA\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1: SEISMIC DATA<\/center><\/h3>","826cd9d5":"<a id=\"4.1\"><\/a>\n## 4.1: Dependent variables:\n### 4.1.1: Check the asymmetry of the probability distribution \n#### b-value","abba4dba":"Results from Grid Search with threshold = 0.0014\n\nBest score: **0.22909710132256744**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'mae', 'bootstrap': False}","226ea2b6":"## ABSTRACT:\n**Objectives:**\n\nPredict the **monthly amount of seismic energy released** and the **monthly b-value**.\n\n**Data:**  \nA seismic catalogue and the injection data for 73 injection wells, that include:\n- monthly amount of injected water,\n- monthly average injection rate,\n- number of day in a month where water ws injected.\n\n**Model evaluation**\n\nSeveral models are used (Lasso, linear, Ridge, Elasticnet, Random forest and xgboost). These model are scored using the explained variance (r2), and the model with the best score is selected for the final prediction.\n\n\n**Results for prediction monthly amount of seismic energy released**\n","c8a6677f":"<a id=\"1.2\"><\/a>\n## 1.2: Simple Statistical seismology\n### 1.2.1: Plot the ECDF of the Earthquake magnitudes","27b64af2":"Results from Grid Search with threshold = 0.0011\n\nBest score: **0.25370632639103463**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","e074d8c5":"Results from Grid Search with threshold = 0.012\n\nBest score: **0.8681300519859287**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'mse', 'bootstrap': False}","edcdfe36":"### Functions for plotting features' importance","dc01e571":"<a id=\"4.1.2\"><\/a>\n### 4.1.2: Log transform skewed targets:\n**Ideally, we want our skewness value to be around 0 and kurtosis less than 3.** ","aaa24f61":"Results from Grid Search with threshold = 0.007\n\nBest score: **0.3717616957479021**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 0.7, 'alpha': 0.6}","23bd230a":"It is an ongoing project...","ab49fcae":"We can see that many injection wells were not used during this period. they were either not drilled yet or abandonned, so we can remove them.","afe76fdb":"<a id=\"5.4.2\"><\/a>\n### 5.4.2: xgboost regression\nAs before we:\n- First, evaulate the importance of features.\n- Then, create several models trained on different input features and we perform a RandomizedSearchCV on each model.\n- Finally, select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","e0b952fb":"#### Seismic energy released with Mw>4","02297d94":"<a id=\"6.2.3\"><\/a>\n### 6.2.3: Ridge model\n#### GridSearchCV() with number features = 62","b39c753f":"Results from Grid Search with threshold = 0.014\n\nBest score: **0.857859455823619**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","d86acc4f":"<a id=\"6.3\"><\/a>\n## 6.3: Decision tree methods \n### 6.3.1: Random Forest Regressor\n- First, we evaulate the importance of features.\n- Then, we create several models trained on different input features and we perform a RandomizedSearchCV on each model.\n- Finally, we select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","3f1b9135":"<a id=\"4.5\"><\/a>\n## 4.5: Outliers detection:","812ec9d8":"<a id=\"1.3\"><\/a>\n## 1.3: Seicmic activity evolution from 2003 to 2016 \n### 1.3.1: b-value evolution","99bbb540":"## <a id=\"Data_Preparation\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>4: DATA PREPARATION<\/center><\/h3>","f3006fff":"Results from Grid Search with threshold = 0.0001\n\nBest score: **0.8864982237714087**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}","24b50d5c":"Results from Grid Search with threshold = 0.016\n\nBest score: **0.8472248203819742**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","53f1150e":"Other variables didn't help","b97ccd9c":"<a id=\"5.3.3\"><\/a>\n### 5.3.3: Ridge model\n#### GridSearchCV() with number features = 22","8663eac1":"#### Best result with linear model","3a47ff4a":"<a id=\"4.2\"><\/a>\n## 4.2: Independent variables  \n### 4.2.1: define functions used for data preparation","a973ac44":"## <a id=\"INJ_VS_SEISM\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3: INJECTION DATA VS INDUCED SEISMICITY<\/center><\/h3>","4b6e66a3":"**Study area location**","19052dfd":"#### Extracted features","ccc3aba1":"## <a id=\"ML2\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>6: MACHINE LEARNING: prediction monthy b-value<\/center><\/h3>","007673d8":"Results from Grid Search with threshold = 0.003\n\nBest score: **0.33907105956175704**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 0.8, 'alpha': 0.6}","e4c9b973":"Results from Grid Search with threshold = 0.01\n\nBest score: **0.8629705092133909**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","441257fb":"#### Run best fit Lasso"}}