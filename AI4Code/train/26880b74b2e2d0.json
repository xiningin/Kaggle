{"cell_type":{"0707a07c":"code","43010c3b":"code","8b4e4f71":"code","10e446da":"code","dfb88c9f":"code","4eaf7180":"code","3b9cde23":"code","f3cafbbb":"code","69db8bc9":"code","e0aa8da4":"code","a080650d":"code","851a718e":"code","14ed3fa9":"code","6871a52c":"code","98f65fbd":"code","c119e32f":"code","a502fb14":"code","b912f88d":"code","a7ea697b":"code","4ba7bc94":"code","efaceac7":"code","c427e7d1":"code","631b8f8b":"code","1cc2d810":"code","ee91ddf2":"code","743b68cb":"code","96148230":"code","e789e36d":"code","3e53610f":"code","4295b4a9":"code","b87a1a60":"code","2217825f":"code","07698157":"code","250bdd7c":"code","8cef1b33":"code","7218e411":"code","bda8ee96":"code","2597548f":"code","a48edbbb":"code","e140aa61":"code","d6faed72":"code","5228c864":"code","640c5747":"code","8fb00d1d":"code","8b441412":"code","991e8657":"code","741f2e36":"code","08dba811":"code","926ce35f":"code","33bfbf02":"code","7b39f2be":"code","3d127025":"code","bdd8ae9a":"code","08153e2e":"code","6b286bbc":"code","0c04fbf7":"code","c08e44fe":"code","9cdbda0c":"code","f716a933":"code","d1c191d8":"code","b414cdc7":"code","f29ae1b3":"markdown","0e93fa52":"markdown","4362458a":"markdown","bdd70315":"markdown","1e9cb037":"markdown","033b1f2f":"markdown","af7bcc50":"markdown","929a8e3c":"markdown","49d7d948":"markdown","f5dc37ec":"markdown","7e8c967e":"markdown","fa9f4a8a":"markdown","ff420dd4":"markdown","fc00108a":"markdown","e7d2dc78":"markdown","0bd90871":"markdown","55f17511":"markdown","00dd7b29":"markdown","654cefa1":"markdown","62b64dcd":"markdown","01a9c30f":"markdown","81171051":"markdown","8c4ecb75":"markdown","e4b5f9ed":"markdown","99d147be":"markdown","74d0245c":"markdown","44bb36be":"markdown","c1f16cf2":"markdown","882c7fe2":"markdown","2daf0e02":"markdown","2a9e0b85":"markdown","569ab4f2":"markdown","3857a487":"markdown","1d41a8b5":"markdown","43f880ea":"markdown","fbf9e28b":"markdown","c348e225":"markdown","6658c1ce":"markdown","21a2509c":"markdown","d760949d":"markdown","b1281871":"markdown","b1a2e641":"markdown","9e91f430":"markdown","25434915":"markdown","4891c5b6":"markdown","99b97748":"markdown","6f3da7e5":"markdown","b0c8af64":"markdown","ede2e0d3":"markdown","c725ef73":"markdown","3c27ccc9":"markdown","48a22c08":"markdown","573b6611":"markdown","97e6733f":"markdown","88459c01":"markdown","c59c689c":"markdown","37c92cbd":"markdown","c48c562b":"markdown","82f6e334":"markdown","4e77ec0c":"markdown","ce871456":"markdown","c6b337e7":"markdown","c4e71c1a":"markdown","a7c65751":"markdown","015bd5a8":"markdown","f33821e4":"markdown","e250b998":"markdown","cbff7f21":"markdown","424dddb2":"markdown","18c716db":"markdown","c782a451":"markdown","f8f62113":"markdown","d5d7fffe":"markdown","f8b29cd3":"markdown","43800fc2":"markdown","a7b4a428":"markdown","a7774d83":"markdown","2622a448":"markdown","e000a737":"markdown","39b59c56":"markdown"},"source":{"0707a07c":"# Using this for auto-compeletion..\n%config Completer.use_jedi = False","43010c3b":"import os\nfrom typing import Text, List, Dict, Set, Tuple, Optional, Union\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = [18, 10]\nplt.style.use('fivethirtyeight')\n%matplotlib inline","8b4e4f71":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","10e446da":"train_data.head(10)","dfb88c9f":"test_data.head()","4eaf7180":"train_data.info()","3b9cde23":"train_data.describe()","f3cafbbb":"train_data['Age'].isnull().value_counts()","69db8bc9":"train_data['Age'].value_counts().head(10)","e0aa8da4":"age_highest_freq = train_data['Age'].value_counts().head(10).index.values\nfreq_of_highes_age = train_data['Age'].value_counts().head(10).values\nage_probabilities = list(map(lambda value: (1\/sum(freq_of_highes_age)) * value,\n                             freq_of_highes_age))","a080650d":"train_data['Age'] = train_data['Age'].apply(lambda value: value if not np.isnan(value) \n                                            else np.random.choice(age_highest_freq,\n                                                                  p=age_probabilities))","851a718e":"train_data.info()","14ed3fa9":"train_data['Embarked'].isnull().value_counts()","6871a52c":"freq = train_data['Embarked'].value_counts().values[0]\ntrain_data['Embarked'].fillna(freq, inplace=True)","98f65fbd":"train_data['Embarked'].isnull().value_counts()","c119e32f":"train_data['Embarked'].unique()","a502fb14":"train_data['Embarked'].replace(to_replace=644,\n                               value=train_data['Embarked'].value_counts().index.values[0],\n                               inplace=True)","b912f88d":"train_data['Embarked'].unique()","a7ea697b":"train_corr = train_data.corr()\ntrain_corr","4ba7bc94":"plt.figure(figsize=(18,10))\nsns.heatmap(train_corr, annot=True)\nplt.show()","efaceac7":"TITLES = ('Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n          'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n          'Don', 'Jonkheer')","c427e7d1":"def patterns(name: Text, titles: Set) -> Optional[Text]:\n    for title in titles:\n        if title in name:\n            return title\n    return \"Untitled\"","631b8f8b":"train_data['Title'] = train_data['Name'].apply(lambda name: patterns(name, TITLES))","1cc2d810":"train_data","ee91ddf2":"def squeeze_title(dataframe: pd.DataFrame) -> Text:\n    title = dataframe['Title']\n    if title in ('Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col'):\n        return 'Mr'\n    elif title in ('Countess', 'Mme'):\n        return 'Mrs'\n    elif title in ('Mlle', 'Ms'):\n        return 'Miss'\n    elif title == 'Dr':\n        if dataframe['Sex'] == 'male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","743b68cb":"train_data['Title'] = train_data.apply(squeeze_title, axis=1) \ntrain_data","96148230":"CaPc = train_data.groupby('Pclass').count()\nCaPc","e789e36d":"plt.figure(figsize=(18,10))\nsns.jointplot(x='Pclass', y='Cabin', data=CaPc, dropna=True)\nplt.show()","3e53610f":"cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\ntrain_data['Deck'] = train_data['Cabin'].apply(lambda letter:\n                                               patterns(str(letter), cabin_list))\ntrain_data.drop(columns='Cabin', inplace=True)","4295b4a9":"train_data.head()","b87a1a60":"fig = plt.figure(figsize=(15,21))\nSS_SUR = fig.add_subplot(221)\nPC_SUR = fig.add_subplot(222)\nSS_PC = fig.add_subplot(2,2,(3,4))\n\nsns.barplot(x='Survived', y='SibSp', data=train_data, ax=SS_SUR)\nsns.barplot(x='Survived', y='Parch', data=train_data, ax=PC_SUR)\n\nSS_PC.title.set_text(\"Correlation between `SibSp` & `Parch`\")\nsns.regplot(x='SibSp', y='Parch', data=train_data, ax=SS_PC)\n\nplt.show()","2217825f":"train_data['Family_size'] = train_data['SibSp'] + train_data['Parch']\ntrain_data.tail()","07698157":"train_data[[\"Pclass\", \"Fare\"]].loc[train_data['Fare'] == 0,:]","250bdd7c":"train_data[['Pclass', 'Fare']].corr()","8cef1b33":"plt.figure(figsize=(18,10))\nsns.regplot(x='Pclass', y='Age', data=train_data)\nplt.show()","7218e411":"tester = train_data.copy()\ntester[['Survived', 'Died']] = pd.get_dummies(tester['Survived'])\nage_sur = tester[['Age', 'Survived', 'Died']].groupby('Age').sum()\nage_sur.reset_index(inplace=True)\nage_sur.sort_values(by=['Died', 'Survived'], ascending=False, inplace=True)\nage_sur.head(10)","bda8ee96":"plt.figure(figsize=(18,10))\nsns.regplot(x='Age', y='Fare', data=train_data)\nplt.show()","2597548f":"plt.figure(figsize=(18,10))\nsns.catplot(x='Sex', y='Title', data=train_data)\nplt.show()","a48edbbb":"train_data.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)","e140aa61":"COLUMNS = ['Title', 'Sex', 'Age', 'SibSp', 'Parch', 'Family_size', 'Pclass',\n           'Fare', 'Deck', 'Embarked', 'Survived']\ntrain_data = train_data[COLUMNS]\ntrain_data","d6faed72":"plt.figure(figsize=(18,10))\nsns.heatmap(train_data.corr(), cmap='Blues', annot=True)\nplt.show()","5228c864":"y_label = train_data['Survived']\nX_features = train_data.iloc[:,:-1]\nX_features","640c5747":"folds = 4\nX_train, X_test, y_train, y_test = train_test_split(X_features,\n                                                    y_label,\n                                                    test_size=(1\/folds),\n                                                    random_state=42,\n                                                    stratify=y_label)","8fb00d1d":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import (StandardScaler, OneHotEncoder)\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import f1_score","8b441412":"NUMERICAL_COLUMNS = [\"Age\", \"SibSp\", \"Parch\", \"Family_size\", \"Fare\"]\nCATEGORICAL_COLUMNS = [\"Title\", \"Sex\", \"Pclass\", \"Deck\", \"Embarked\"]","991e8657":"transformers = make_column_transformer(\n                                       (StandardScaler(), NUMERICAL_COLUMNS),\n                                       (OneHotEncoder(handle_unknown='ignore'),\n                                                      CATEGORICAL_COLUMNS))","741f2e36":"class ClfSwitcher(BaseEstimator):\n    \n    def __init__(self, \n                 estimator = DecisionTreeClassifier()):\n        \"\"\"\n        Custom Estimator is a custom class that helps you to switch\n        between classifiers.\n        \n        Args:\n            estimator: sklearn object \u2013 classifier\n        \"\"\"\n        self.estimator = estimator\n    \n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n    \n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n    \n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n    \n    def score(self, X, y):\n        return self.estimator.score(X, y)","08dba811":"pipeline = Pipeline([\n    ('transformer', transformers),\n    ('clf', ClfSwitcher())\n])\n\nparameters = [\n    {\n        'clf__estimator': [DecisionTreeClassifier()],\n        'clf__estimator__criterion': ['gini', 'entropy'],\n\n    },\n    {\n        'clf__estimator': [ExtraTreesClassifier()],\n        'clf__estimator__n_estimators': [100, 250],\n        'clf__estimator__criterion': ['gini', 'entropy'],\n\n    },\n    {\n        'clf__estimator': [RandomForestClassifier()],\n        'clf__estimator__n_estimators': [100, 250],\n        'clf__estimator__criterion': ['gini', 'entropy'],\n    },\n    {\n        'clf__estimator': [SVC()],\n        'clf__estimator__kernel': ['rbf', 'sigmoid'],\n        'clf__estimator__C': [1e-3, 1e-2, 1e-1, 1.0, 10., 1e+2, 1e+3],\n        'clf__estimator__degree': [3, 4, 5, 6]\n    },\n    {\n        'clf__estimator': [LogisticRegression()],\n        'clf__estimator__penalty': ['l1', 'l2'],\n        'clf__estimator__tol': [1e-4, 1e-3, 1e-2],\n        'clf__estimator__C': [1e-3, 1e-2, 1e-1, 1.0, 10., 1e+2, 1e+3],\n        'clf__estimator__solver': ['lbfgs', 'liblinear']   \n    }\n    \n]","926ce35f":"cv = KFold(n_splits=(folds - 1))","33bfbf02":"gscv = GridSearchCV(pipeline,\n                    parameters,\n                    cv=cv,\n                    scoring='r2',\n                    n_jobs=12,\n                    verbose=3)\ngscv.fit(X_train, y_train)","7b39f2be":"gscv.best_params_","3d127025":"model = pipeline.set_params(**gscv.best_params_)","bdd8ae9a":"model.fit(X_train, y_train)","08153e2e":"y_hat = model.predict(X_test)","6b286bbc":"model.score(X_test, y_test)","0c04fbf7":"cm = confusion_matrix(y_test, y_hat, labels=[1, 0])\ncm","c08e44fe":"confusion_plot = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                        display_labels=['Survived', 'Died'])\n\nfig, ax = plt.subplots(1,1,figsize=(12, 8))\nax.grid(False)\nconfusion_plot.plot(cmap='Blues', ax=ax)\nax.set_title('Confusion Matrix Between True Label & Predicted Label')\nplt.show()","9cdbda0c":"f1_score(y_test, y_hat)","f716a933":"def prediction_fn(data_dir: Text, model) -> pd.DataFrame:\n    \"\"\"\n    A Function created for producing a batch prediction\n    for titanic model.\n    \n    Args:\n        data_dir [Text]: directory to the test dataset to use it in the prediction.\n        model: model pipeline that is going to be used in the prediction.\n    \n    Returns:\n        pd.DataFrame: predicted dataframe.\n    \"\"\"\n    # read dataset\n    test_data = pd.read_csv(data_dir, index_col=False)\n    \n    # pre-processing & Feature Engineering\n    ## Age\n    age_highest_freq = test_data['Age'].value_counts().head(10).index.values\n    freq_of_highes_age = test_data['Age'].value_counts().head(10).values\n    age_probabilities = list(map(lambda value: (1\/sum(freq_of_highes_age)) * value,\n                                 freq_of_highes_age))\n    test_data['Age'] = test_data['Age'].apply(lambda value: value if not np.isnan(value) \n                                                else np.random.choice(age_highest_freq,\n                                                                      p=age_probabilities))\n    ## Embarked\n    freq = test_data['Embarked'].value_counts().values[0]\n    test_data['Embarked'].fillna(freq, inplace=True)\n    test_data['Embarked'].replace(to_replace=644,\n                                   value=test_data['Embarked'].value_counts().index.values[0],\n                                   inplace=True)\n    ## Fare\n    test_data['Fare'].fillna(method='bfill', inplace=True)\n    ## Title\n    test_data['Title'] = test_data['Name'].apply(lambda name: patterns(name, TITLES))\n    test_data['Title'] = test_data.apply(squeeze_title, axis=1) \n    \n    ## Cabin\n    test_data['Deck'] = test_data['Cabin'].apply(lambda letter:\n                                                   patterns(str(letter), cabin_list))\n    test_data.drop(columns='Cabin', inplace=True)\n    \n    ## Family Size\n    test_data['Family_size'] = test_data['SibSp'] + test_data['Parch']\n    \n    # Drop the useless features (Only keep PassengerId for merging  it with the predictions)\n    passId = test_data['PassengerId']\n    test_data.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)\n    \n    # Model predictions\n    predictions = model.predict(test_data)\n    \n    # Building the dataframe of the predction values\n    predictions = pd.DataFrame(predictions, columns=['Survived'])\n    predictions = pd.merge(passId, predictions, left_index=True, right_index=True)\n    \n    return predictions","d1c191d8":"import warnings\nwarnings.filterwarnings('ignore')\nprediction = prediction_fn('..\/input\/titanic\/test.csv', model)\nprediction","b414cdc7":"prediction.to_csv('gender_submission.csv', index=False, index_label=False)\nprint(\"You have sucessfully exported the predictions!\")","f29ae1b3":"## Dependencies","0e93fa52":"![](https:\/\/miro.medium.com\/max\/1400\/1*VeHSAW_AxHz-GYwexWfdQg.jpeg)\n\nRef: [Titanic Survival Analysis Using R](https:\/\/chanida-limt.medium.com\/titanic-survival-prediction-c421aac8da32)","4362458a":"Copyright [2021] [Data Scientist & ML Engineer: [Ahmed](https:\/\/www.kaggle.com\/dsxavier\/)]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.","bdd70315":"Before continuing **Feature Engineering** \u2013 all the features are stable now. We need to expand our vision more to understand the data that we have before applying any additional FE. What is going to help us in our mission of understanding the dataset is EDA.","1e9cb037":"Now, Let's clear our dataset from any useless features and prepare the dataset for our next phase which is **Train & Test Split**.","033b1f2f":"To work on `Cabin`, we need to understand what these values related to, the letters before each number. So that, we need to understand the infrastructure of the ship. \n\nAfter Reading more about how `Cabin` is related to passengers in Titanic \u2013 you can find this article in **dummies** \u2013 [Duites and Cabins for Passengers on the Titanic](https:\/\/www.dummies.com\/article\/academics-the-arts\/history\/20th-century\/suites-and-cabins-for-passengers-on-the-titanic-180677). \n\nThis article explains the correlation between the `Cabin` and the `Passenger Class (Pclass)`: we were not able to observe that from the correlation diagram since `Cabin` has lot of `NaN` values.","af7bcc50":"According to **BBC**: \n> *The first half of the ship reaches the botton first. Two minutes later, the back half of the Titanic joins it on the floor of the Atlantic.*","929a8e3c":"The Correlation after **Feature Engineering** is efficient!","49d7d948":"Now, we're fine-tune the model and see what is the best model, besides what are the best hyperparameters.","f5dc37ec":"## [Data Dictionary](https:\/\/www.kaggle.com\/c\/titanic\/data#:~:text=should%20look%20like.-,data%20dictionary,-Variable)\n","7e8c967e":"<span style='color:CornflowerBlue;font-size:16px'>Feature 05: SibSp & Parch<\/span>","fa9f4a8a":"We noticed that; `Age` has `NaN` values, but if we compared between `Age` and `Cabin`, you can see that `Age` can be handled compared with `Cabin`.\n\n- <span style='color:green;'><b>Notice:<\/b><\/span> We're going to handle `Cabin` at the end, so we can get the benefit from it, too.","ff420dd4":"One of the critical things I read that written by *Prof. Andrew Neg* in his book **Machine Learning Yearning** \u2013 the proportion between your train, dev, and test data has to be on the same line of the distribution, you can't have a different proportion between your train, and test data and you expect to give you high accuracy.\n\n`stratify` parameter in `train_test_split` accomplish that goal for you. When it splits the label, it will make sure that the proportion of the training label is similar to the proportion of the testing label.","fc00108a":"### 2. Train & Test Split","e7d2dc78":"As we can see, `Age` and `Fare` have **Weak Linear Correlation**, which means; I can't apply feature cross on them even if I want.","0bd90871":"<u>Quoted from *TRIANGLEINEQUALITY*<\/u>\n> *You may be interested to know that \u2018Jonkheer\u2019 is a male honorific for Dutch nobility. Also interesting is that I was tempted to just send \u2018Dr\u2019 -> \u2018Mr\u2019, but decided to check first, and there was indeed a female doctor aboard! It seems 1912 was further ahead of its time than Doctor Who!*\n\n> *Curious, I looked her up: her name was Dr. Alice Leader, and she and her husband were physicians in New York city*.[$^1$](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)\n\n![](https:\/\/i0.wp.com\/www.encyclopedia-titanica.org\/images\/leader_af.jpg)","55f17511":"Since this project is all about classification, we're going to set up the best probabilistic models that can help us to predict the highest accuracy we can obtain.\n\nBut first, we want to make sure that our datasets have transformed & normalized, correctly. So, we're going to build a transformed pipeline to add our features into it.Since this project is all about classification, we're going to set up the best probabilistic models that can help us to predict the highest accuracy we can obtain.\n\nBut first, we want to make sure that our datasets have transformed & normalized, correctly. So, we're going to build a transformed pipeline to add our features into it.","00dd7b29":"### 5. Model Predictions","654cefa1":"<p style='text-align:center;'>Thanks for reaching this level of expermenting\nthe idea of<\/p>\n<p style='text-align:center;'><b>Titanic - How Data Scientist applies Analysis & ML<\/b><\/p>\n<p style='text-align:center;'>Data Scientist & ML Engineer: <a href='https:\/\/www.linkedin.com\/in\/drxavier997\/'>Ahmed<\/a><\/p>\n<p style='text-align:center;'>Created at: 2022-01-13","62b64dcd":"To have a fair probability between all the `NaN` values replacements, we need to normalize the frequency of our `Age` feature so the total sum of the probability is equal 1. .","01a9c30f":"We can see that `Embarked` has a number inside its values. Let's change this number of the max frequency value inside this feature.","81171051":"To fill the `NaN` values in the `Age` column, we can get the highest frequency of `Age` and start to fulfil it randomly instead of the `NaN` values. \n\nNote: \n- We're going to select the highest 10 frequencies of our `Age` class and their frequencies and then, we're going to assign them, randomly.","8c4ecb75":"### 6. Model Evalutaion","e4b5f9ed":"The idea behind slicing `Name` helps for observing patterns from the name of the passengers and converting them into a categorical feature.\n\n- <span style='color:green;'><b>Read This article:<\/b><\/span> <a href=\"https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/\">Basic Feature Engineering with the Titanic Data<\/a>","99d147be":"### 7. Export Predictions","74d0245c":"### (A) Import Libraries","44bb36be":"Let's split our train dataset into a train, and test datasets [75, 25].","c1f16cf2":"<table>\n<thead>\n  <tr>\n      <th><a href='#Table-of-Contents'>Table of Contents<\/a><\/th>\n    <th><\/th>\n    <th><\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n      <td><a href='#Dependencies'>Dependencies<\/a><br><\/td>\n    <td><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#(A)-Import-Libraries'>(A) Import Libraries<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n      <td><a href='#Workflow-Pipeline'>Workflow Pipeline<\/a><\/td>\n    <td><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#1.-Data-Preprocessing'>1. Data Preprocessing<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n    <td><\/td>\n      <td><a href='#(A)-Data-Wrangling'>(A) Data Wrangling<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n    <td><\/td>\n      <td><a href='#(B)-Exploratory-Data-Analysis'>(B) Exploratory Data Analysis<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#2.-Train-&-Test-Split'>2. Train & Test Split<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#3.-Algorithm-Setup'>3. Algorithm Setup<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#4.-Model-Fitting'>4. Model Fitting<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#5.-Model-Predictions'>5. Model Predictions<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#6.-Model-Evalutaion'>6. Model Evalutaion<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td><\/td>\n      <td><a href='#7.-Export-Predictions'>7. Export Predictions<\/a><\/td>\n    <td><\/td>\n  <\/tr>\n<\/tbody>\n<\/table>","882c7fe2":"According to **dummies**:\n\n- Titanic's `Passenger first-class` Cabins:\n\n> *First-class accommodations were located amidships, <u>where the rocking of the ship was less keenly felt and passengers were less likely to get seasick<\/u>. They were decorated opulently in different period styles: Queen Anne, Louis XVI, and Georgian.*\n\n- Titanic's `Passenger second-class` Cabins:\n\n> *passengers slept in berths built into the walls of the cabins. At two to four berths per cabin, privacy was hard to come by, although a passenger could close the curtain around his or her berth. <u>Each second-class cabin had a washbasin and a chamber pot to be used in case of seasickness<\/u>.*\n\n- Titanic's `Passenger third-class` Cabins:\n\n> *passengers slept on bunk beds in crowded quarters at six to a narrow cabin. Like second-class passengers, they shared bathrooms, but the number of people sharing a bathroom was much higher in third class: Only two bathtubs were available for all 710 third-class passengers, one for the men and one for the women.*","2daf0e02":"First, we're going to explore the titanic dataset and start do some data wrangling and EDA ","2a9e0b85":"Let's evaluate our model using **Single Evaluation Metric** and **Multi-evaluation Metrics**","569ab4f2":"Let's check if there's any correlation between the `Age` and `Fare`","3857a487":"Now, Let's do some **Feature Engineering**","1d41a8b5":"**What's the `Age` most of ppl `Died`?**","43f880ea":"The correlation between the features and the label beside the features within itself needs more work so we can discover the patterns.","fbf9e28b":"Group the data by `Pclass`","c348e225":"Let's go back to the FE","6658c1ce":"For formatting purposes \u2013 we want to shift our `Survive` Label to the end of the table. Also, we need to add each Feature corresponding to its correlations. For the sake of curiosity, let's check the correlation between the new features and the old ones after deleting unnecessary features","21a2509c":"#### (B) Exploratory Data Analysis","d760949d":"Young age are the most people died, specially `Age` of **24**, maybe this has been effected by filling the missing value!","b1281871":"Let's visualize the `# of siblings \/ spouses aboard the Titanic (SibSp)` and `# of parents \/ children aboard the Titanic (Parch)` with `Survived` Class & the Linear Correlation between them","b1a2e641":"<span style='color:CornflowerBlue;font-size:16px'>Feature 01: Age<\/span>","9e91f430":"Let's check the values inside the feature","25434915":"The first half of the ship is the place where most first-class cabins exist. So, using the `Cabin` feature may help us if we extract the letters from the `Cabin` observation will help us as it represents the Deck.","4891c5b6":"### 3. Algorithm Setup","99b97748":"## Workflow Pipeline","6f3da7e5":"<span style='color:CornflowerBlue;font-size:16px'>Feature 06: Sex & Title<\/span>","b0c8af64":"## An Overview","ede2e0d3":"### 4. Model Fitting","c725ef73":"# Titanic - Machine Learning from Disaster\n---","3c27ccc9":"`Embarked` Feature has only 1 missing value, we can fill this value with the mode value of the `Embarked` feature.","48a22c08":"We need to check the `Title` with the `Sex` to see if our FE works well","573b6611":"<span style='color:CornflowerBlue;font-size:16px'>Feature 06: Age & Pclass & Fare<\/span>","97e6733f":"Equation for that is;\n\n# $X_{norm} = \\frac{{1}}{\\sum_{i}^{N}{x_i}}\\times{x_j}$","88459c01":"Now, That's good but not perfect \u2013 we want to squeeze the number of categories in the feature. We can iterate throw all the observations and check the type of title this person has, but we may have a problem! One of the titles is \"*Dr*\". This title can fit both [\"Men\", \"Women\"] \u2013 we will have to add one more feature to help us decide whether this person is a \"*Male*\" or \"*Female*\", we will use `Sex` for this job.","c59c689c":"First, we need to check the correlation between the features \u2013 this might be the first step to help us identify which feature will be really helpful in predictions.","37c92cbd":"But we still can face the problem of overfitting..but how!? We're going to face the problem of overfitting because we split our data, randomly. We need to ensure that even if we sent weak hyperparameter to our probabilistic models, it wouldn't go throw overfitting because of <u>skew distribution<\/u>.\n\nTherefore, I always prefer to add a procedure called **[cross-validation](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics))**","c48c562b":"##### **Feature Engineering**","82f6e334":"### 1. Data Preprocessing","4e77ec0c":"<span style='color:CornflowerBlue;font-size:16px'>Feature 04: Cabin & Pclass<\/span>","ce871456":"Now, let's test the model we trained using our test data we split from the main train dataset.","c6b337e7":"## Table of Contents","c4e71c1a":"First, we're going to start with building the transformers pipeline","a7c65751":"These data are likely missed. We can't cross `Fare` with `Pclass`.","015bd5a8":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","f33821e4":"1. Single-number Evaluation Metric","e250b998":"<span style='color:CornflowerBlue;font-size:16px'>Feature 03: Name<\/span>","cbff7f21":"First, Let's check if the anyone from *First-class* or the *Second-class* has `Fare =0.0`. If so, we can't involve price with us.","424dddb2":"Then, we're going to the Models' pipeline. If you want to have multiple models in one pipeline, we can do that by building a [custom estimator](https:\/\/stackoverflow.com\/questions\/48507651\/multiple-classification-models-in-a-scikit-pipeline-python#:~:text=consider%20checking%20out%20similar%20questions%20here%3A).","18c716db":"<span style='color:CornflowerBlue;font-size:16px'>Feature 02: Embarked<\/span>","c782a451":"#### (A) Data Wrangling","f8f62113":"---","d5d7fffe":"2. Multi-evaluation Metrics","f8b29cd3":"`NaN` Cabins will be replaced with `Unknown`","43800fc2":"---","a7b4a428":"According to *Prof. Andrew Neg* in a book of **Machine Learning Yearning** \u2013 If you want to relay on accurate accuracy for your classification model, you should relay on <u>Multi-evaluation metric<\/u> [ Precision, Recall ]. You can combine them into <u>single-number evaluation metric<\/u> which `F1-score `.","a7774d83":"We can see that \u2013 `P-class` and `Fare` have **Negative Linear Correlation**. (Note that we're trying to check the linear correlation although this is a categorical feature, we're doing that for the sake of seeing the effection on the `Fare` feature).","2622a448":"We can see, there're **positive Linear Correlation** between `SibSp` and `Parch`. Maybe this shows us \u2013 most people who had family relationships in the ship are more likely to risk themselves to rescue their family, especially if they're siblings\/spouses.\n\nWhy not combine them!? Both features are likely to fit the same idea. It will be really efficient when we're going to use decision tree classification.","e000a737":"<table>\n<thead>\n  <tr>\n    <th>Variable<\/th>\n    <th>Definition<\/th>\n    <th>Key<\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td>survival<\/td>\n    <td>Survival<\/td>\n    <td>0 = No, 1 = Yes<\/td>\n  <\/tr>\n  <tr>\n    <td>pclass<\/td>\n    <td>Ticket class<\/td>\n    <td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n  <\/tr>\n  <tr>\n    <td>sex<\/td>\n    <td>Sex<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>Age<\/td>\n    <td>Age in years<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>sibsp<\/td>\n    <td># of siblings \/ spouses aboard the Titanic<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>parch<\/td>\n    <td># of parents \/ children aboard the Titanic<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>ticket<\/td>\n    <td>Ticket number<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>fare<\/td>\n    <td>Passenger fare<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>cabin<\/td>\n    <td>Cabin number<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>embarked<\/td>\n    <td>Port of Embarkation<\/td>\n    <td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n  <\/tr>\n<\/tbody>\n<\/table>","39b59c56":"Lastly, we're going to create a function that apply all the above steps and provide to us the predictions of the `test.csv`."}}