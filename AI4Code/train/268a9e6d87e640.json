{"cell_type":{"1e1cff66":"code","f9d8fd17":"code","55d8389d":"code","f6c3236f":"code","e0bf88dc":"code","184115c7":"code","4ed65757":"code","4aaf0328":"code","e0b21e6c":"code","69dd2d2e":"code","1171667d":"code","102f5573":"code","7840a392":"code","8c3a0c29":"code","08697948":"code","75daee67":"code","d9f96741":"code","0736c93c":"code","30add08f":"code","c448c94e":"code","82cbf006":"code","ce0fcf0a":"code","e048780a":"code","b3cb4693":"code","2d2ffb9c":"code","bc283120":"markdown","946962c4":"markdown","adc070da":"markdown","24c178d8":"markdown","f8f9fdbc":"markdown","9bd708a9":"markdown","75b2ff42":"markdown","97742806":"markdown","b5ce3bfe":"markdown","9f8109a5":"markdown","92dccc88":"markdown","14a43027":"markdown","67881ce0":"markdown","89a86ae5":"markdown","418eec0f":"markdown","86611f40":"markdown","4d3f2ddf":"markdown","ca04038a":"markdown","ffa2344d":"markdown","b507195e":"markdown","352e8f14":"markdown","36b66aff":"markdown","6e4dd79e":"markdown","bb46d8dc":"markdown"},"source":{"1e1cff66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\n#ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf-8\"))\n\n# Any results you write to the current directory are saved as output.","f9d8fd17":"# read csv(comma separated value) into data\ndata = pd.read_csv(\"..\/input\/column_2C_weka.csv\")","55d8389d":"data.columns  #Columns in our data ","f6c3236f":"data.head()","e0bf88dc":"data.tail()","184115c7":"#Now let's look at the data of our data\ndata.info()","4ed65757":"data.describe()","4aaf0328":"colors = ['cyan' if i == 'Normal' else 'orange' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns !='class'],\n                                       c = colors,\n                                       figsize =[15,15],\n                                       diagonal ='hist',    # histogram of each features\n                                       alpha = 0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor=\"black\"\n                          )\nplt.savefig('graph4.png')\nplt.show()","e0b21e6c":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","69dd2d2e":"# We list them separately according to 'Abnormal' and 'Normal' properties\nN = data[data['class']==\"Normal\"]\nA = data[data['class']==\"Abnormal\"]\nprint(\"NORMAL\")\nN.info()\nprint()\nprint(\"ABNORMAL\")\nA.info()","1171667d":"# Scatter Plot\nplt.figure(figsize=[12,8])\nplt.scatter(N.pelvic_radius, N.pelvic_incidence, color=\"cyan\", label=\"Normal\")\nplt.scatter(A.pelvic_radius, A.pelvic_incidence, color=\"orange\", label=\"Abnormal\")\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"pelvic_incidence\")\nplt.legend() # To show labels\nplt.savefig('graph3.png')\nplt.show()","102f5573":"data['class'] = [1 if each=='Normal' else 0 for each in data['class']]\ndata_class = data['class']   # This is what we do for convenience\ny = data_class.values\nx_d = data.drop([\"class\"], axis=1)   # We will use other features except Class","7840a392":"# Normalization\nx = (x_d - np.min(x_d)) \/ (np.max(x_d) - np.min(x_d))","8c3a0c29":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)","08697948":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=22)\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)","75daee67":"print(\" {} nn score : {}\".format(22, knn.score(x_test, y_test)))","d9f96741":"score_list = []\nfor each in range(1,30):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n#Plot\nplt.figure(figsize=[13,8])\nplt.plot(range(1,30),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.savefig('graph2.png')\nplt.show()","0736c93c":"# Model complexity\nrand = np.arange(1,30)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(rand):\n    # k from 1 to 30(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # fit with knn\n    knn.fit(x_train, y_train)\n    train_accuracy.append(knn.score(x_train, y_train))           # train accuracy\n    test_accuracy.append(knn.score(x_test, y_test))              # test accuracy\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(rand, test_accuracy, label='Testing Accuracy' , color='red')\nplt.plot(rand, train_accuracy, label='Training Accuracy', color='black')\nplt.legend()\nplt.title(' K Value vs Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('K Values')\nplt.xticks(rand)\nplt.savefig('graph.png')\nplt.show()\n\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy), 1+test_accuracy.index(np.max(test_accuracy))))","30add08f":"data1 = A  # We have previously defined A to Class Abnormal.\nx = np.array(data1.loc[:, 'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:, 'sacral_slope']).reshape(-1,1)\n\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x, y=y)\nplt.xlabel(\"pelvic incidence\")\nplt.ylabel(\"sacral slope\")\nplt.show()","c448c94e":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score    \n#Regression\nreg = LinearRegression()\n# Fit\nreg.fit(x,y)\n\n# Prediction: \npred_space = np.linspace(min(x), max(x)).reshape(-1,1)\ny_head = reg.predict(pred_space)\n\n# r2 score with LinearRegression\nprint('R^2 score: ',reg.score(x, y))\n# r2 score with metrics\nprint('R^2 score metrics: ', r2_score(y, reg.predict(x)))\n\n# Plot regression line and scatter\nplt.subplots(figsize=(12,10))\nplt.plot(pred_space, y_head, color='red', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.savefig('graph6.png')\nplt.show()","82cbf006":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train, y_train)\n\n#test\nprint(\"Accuracy of SVM Score : \", svm.score(x_test, y_test))","ce0fcf0a":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n#test\nprint(\"Accuracy of Naive Score : \", nb.score(x_test, y_test))","e048780a":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\n#test\nprint(\"Accuracy of Decision Tree Score : \", dtc.score(x_test, y_test))","b3cb4693":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100, random_state=1)\n#n_estimators =100 -> Determines how many trees we have\nrfc =rfc.fit(x_train, y_train)\n#test\nprint(\"Random Forest Score\", rfc.score(x_test, y_test))","2d2ffb9c":"from sklearn.metrics import confusion_matrix\ny_pred = rfc.predict(x_test)\ny_true = y_test\n\ncm = confusion_matrix(y_true, y_pred)\n\n# confusion matrix visualization\n\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, linewidths = 0.5, linecolor = 'green', fmt=\".0f\", ax=ax)\nplt.savefig('graph7.png')\nplt.show()","bc283120":"<a id=\"1\"><\/a> <br>\n# 2) Exploratory Data Analysis (EDA)","946962c4":"# Cihan Yatbaz\n###  03 \/ 11 \/ 2018\n\n\n\n1.  [Introduction:](#0)\n2. [Exploratory Data Analysis (EDA) :](#1)\n3. [K-NEAREST NEIGHBORS ( KNN ) :](#2)\n    1. [KNN  :](#3)\n    2. [Model Complexity  :](#4)  \n4. [REGRESSION :](#5)\n5. [Support Vector Machine ( SVM) :](#6)\n6. [Naive Bayes :](#7)\n7. [Decission Tree :](#8)\n8. [Random Forest :](#9)\n9. [Confusion Matrix :](#10)\n10. [CONCLUSION :](#11)\n","adc070da":"### Let's try to find the best k value with loop now","24c178d8":"<a id=\"9\"><\/a> <br>\n# 8) Random Forest\n* We have multiple decision trees in a random forest and Random forest giving us the most correct results by doing the transaction with decision tree while we process.\n* As a result, it improves the accuracy and reliability of the model","f8f9fdbc":"### In the figure above, how can we predict whether a point we want to find is Normal or Abnormal? Let's learn together.\n* First we set a point.\n* Then we choose the value 3 closest to this point.\n* If the Abnormal number is higher in these values, it is Abnormal in your new value. If the number of Normals is high, our new value becomes normal.\n","9bd708a9":"### If k value has a value of 22 or 26, our test results will give the best result.","75b2ff42":"<a id=\"10\"><\/a> <br>\n# 9) Confusion Matrix\n* Confusion Matrix : With the result, it shows how many mistakes we have from A and B.\n* Confusion Matrix enables us to visualize these results","97742806":"* Now we are doing normalization. Because if some of our columns have very high values, they will suppress other columns and do not show much.\n* Formulel : (x- min(x)) \/ (max(x) - min(x))","b5ce3bfe":"* Now we reserve 70% of the values as 'train' and 30% as 'test'.","9f8109a5":"<a id=\"7\"><\/a> <br>\n# 6) Naive Bayes\n* Determines the probability that the point in the selected circle can be A or B.","92dccc88":"<a id=\"3\"><\/a> <br>\n## A) KNN","14a43027":"We have Abnormal and Normal values in our data and let's see them now","67881ce0":"### Scatter Matrix Plot","89a86ae5":"<a id=\"4\"><\/a> <br>\n## B) Model Complexity\n* We can use Model Complexity to find the best value.  This way we can easily find the best result by comparing the Value and Accuracy.","418eec0f":"<a id=\"6\"><\/a> <br>\n# 5) Support Vector Machine ( SVM)\n*  Specifies the line that will pass between 2 values in the table and tries to keep margin highest","86611f40":"<a id=\"5\"><\/a> <br>\n# 4) REGRESSION \n* Let's create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable","4d3f2ddf":"#### 0 = Abnormal values       &      1 = Normal values\n* In our test, there are 60 true, 6 false results for Abnormal.\n* In our test, there are 20 true, 7 false results for Normal.","ca04038a":"<a id=\"11\"><\/a> <br>\n> # CONCLUSION                                                                                                                                                      \nThank you for your votes and comments                                                                                                                                              \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**","ffa2344d":"Let's create our KNN model\n* n_neighbours = k  ----->   We determine the k value and try to increase the accuracy of the result","b507195e":"<a id=\"8\"><\/a> <br>\n# 7) Decission Tree\n* It allows us to differentiate between different classes with Splits","352e8f14":"<a id=\"0\"><\/a> <br>\n# 1) Introduction\n\nWe will be working on this kernel Biomechanical features of orthopedic patients data. This kernel will do some inspections with KNN. Firstly we will examine dataset. Let's start ","36b66aff":"<a id=\"2\"><\/a> <br>\n# 3) K-NEAREST NEIGHBORS ( KNN )","6e4dd79e":"### Count plot\nNow let's see how many of our Normal and Abnormal values exist","bb46d8dc":"### Which value belongs to the selected point?"}}