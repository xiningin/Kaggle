{"cell_type":{"40a0ffe7":"code","f2cf5135":"code","d6144797":"code","7dcf27d3":"code","de65bfc3":"code","30c1c68e":"code","488e082d":"code","dd5d08ef":"code","5f568f0e":"code","ba618693":"code","7db6ded4":"code","54fe835e":"code","86a1472e":"code","ddcbe1c8":"code","85879f59":"code","af45a59e":"code","17307618":"code","1c1afc11":"code","e9a36a94":"code","1ebf67b2":"code","f61289ab":"code","c0be79b5":"code","96adf35a":"code","7a8fb86b":"code","05ae2549":"code","17886fea":"code","3099fb1c":"markdown","ad81132e":"markdown","57fed34c":"markdown","cc2f09f5":"markdown","442fe51e":"markdown","4ad8749b":"markdown","538377f4":"markdown","ea65f8f5":"markdown","c7719202":"markdown","e48036ee":"markdown","266eb529":"markdown","1af7cfd4":"markdown","f3d1343b":"markdown","dfe346e2":"markdown","c11432f1":"markdown","0a1b6df8":"markdown","e633e479":"markdown","feb2954e":"markdown","d297a58a":"markdown","62ac28a6":"markdown","e8512717":"markdown","13bd476a":"markdown"},"source":{"40a0ffe7":"#Kaggle's way of welcoming us \nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f2cf5135":"#Loading the iris dataset in dataframe\nirisMaster = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\n\n#Visualizing the top 5 records\nirisMaster.head()","d6144797":"#Getting the data summary\nirisMaster.info()","7dcf27d3":"print(\"Total Rows : \",irisMaster.shape[0])\nprint(\"Total Columns : \",irisMaster.shape[1])","de65bfc3":"irisMaster[\"Species\"].value_counts()","30c1c68e":"#Statistical summary of the dataset\nirisMaster.describe().T","488e082d":"#Dropping the nominal feature \"Id\"\niris = irisMaster.drop(\"Id\", axis = 1)\niris.head()","dd5d08ef":"sns.set_style(\"whitegrid\");\nsns.pairplot(data = iris, hue = \"Species\", height = 4)\nplt.show()","5f568f0e":"#Visualizing the Distribution & Box plots of all the four features\nfig, ax = plt.subplots(4, 2, figsize = (14,14))\nsns.distplot(iris[\"SepalLengthCm\"] , color=\"skyblue\", ax=ax[0, 0])\nsns.boxplot(iris[\"SepalLengthCm\"] , color=\"skyblue\", ax=ax[1, 0])\nsns.distplot(iris[\"SepalWidthCm\"] , color=\"olive\", ax=ax[0, 1])\nsns.boxplot(iris[\"SepalWidthCm\"] , color=\"skyblue\", ax=ax[1, 1])\nsns.distplot(iris[\"PetalLengthCm\"] , color=\"gold\", ax=ax[2, 0])\nsns.boxplot(iris[\"PetalLengthCm\"] , color=\"skyblue\", ax=ax[3, 0])\nsns.distplot(iris[\"PetalWidthCm\"] , color=\"teal\", ax=ax[2, 1])\nsns.boxplot(iris[\"PetalWidthCm\"] , color=\"skyblue\", ax=ax[3, 1])\nplt.suptitle(\"Distribution + Box and Whiskers Plots of all the features\")\nplt.legend()\nplt.show()","ba618693":"iris_Setosa = iris[iris[\"Species\"] == \"Iris-setosa\"];\niris_Virginica = iris[iris[\"Species\"] == \"Iris-virginica\"];\niris_Versicolor = iris[iris[\"Species\"] == \"Iris-versicolor\"];\n\nprint(\"Setosa: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Setosa[\"SepalLengthCm\"]), np.std(iris_Setosa[\"SepalLengthCm\"]))\nprint(\"Setosa: Mean and SD Deviation of Petal Width : \", np.mean(iris_Setosa[\"SepalWidthCm\"]), np.std(iris_Setosa[\"SepalWidthCm\"]))\nprint(\"Setosa: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Setosa[\"PetalLengthCm\"]), np.std(iris_Setosa[\"PetalLengthCm\"]))\nprint(\"Setosa: Mean and SD Deviation of Petal Width : \", np.mean(iris_Setosa[\"PetalWidthCm\"]), np.std(iris_Setosa[\"PetalWidthCm\"]))\nprint(\"\\n\")\nprint(\"Virginica: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Virginica[\"SepalLengthCm\"]), np.std(iris_Virginica[\"SepalLengthCm\"]))\nprint(\"Virginica: Mean and SD Deviation of Petal Width : \", np.mean(iris_Virginica[\"SepalWidthCm\"]), np.std(iris_Virginica[\"SepalWidthCm\"]))\nprint(\"Virginica: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Virginica[\"PetalLengthCm\"]), np.std(iris_Virginica[\"PetalLengthCm\"]))\nprint(\"Virginica: Mean and SD Deviation of Petal Width : \", np.mean(iris_Virginica[\"PetalWidthCm\"]), np.std(iris_Virginica[\"PetalWidthCm\"]))\nprint(\"\\n\")\nprint(\"Versicolor: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Versicolor[\"SepalLengthCm\"]), np.std(iris_Versicolor[\"SepalLengthCm\"]))\nprint(\"Versicolor: Mean and SD Deviation of Petal Width : \", np.mean(iris_Versicolor[\"SepalWidthCm\"]), np.std(iris_Versicolor[\"SepalWidthCm\"]))\nprint(\"Versicolor: Mean and SD Deviation of Sepal Length : \", np.mean(iris_Versicolor[\"PetalLengthCm\"]), np.std(iris_Versicolor[\"PetalLengthCm\"]))\nprint(\"Versicolor: Mean and SD Deviation of Petal Width : \", np.mean(iris_Versicolor[\"PetalWidthCm\"]), np.std(iris_Versicolor[\"PetalWidthCm\"]))","7db6ded4":"fig, ax = plt.subplots(2,2, figsize = (12,14))\nsns.boxplot(x='Species',y = \"SepalLengthCm\", data=iris, ax=ax[0, 0])\nsns.boxplot(x='Species',y = \"SepalWidthCm\", data=iris, ax=ax[0,1])\nsns.boxplot(x='Species',y = \"PetalLengthCm\", data=iris, ax=ax[1, 0])\nsns.boxplot(x='Species',y = \"PetalWidthCm\", data=iris, ax=ax[1, 1])\nplt.suptitle(\"Box and Whiskers Plots - Specieswise Distribution\")\nplt.legend()\nplt.show()","54fe835e":"irisCorr = iris.corr()\nirisCorr","86a1472e":"irisCovar = iris.cov()\nirisCovar","ddcbe1c8":"sns.heatmap(irisCorr, annot = True, cmap = \"YlGnBu\", linewidth = 0.1)\nplt.show()","85879f59":"sns.heatmap(irisCovar, annot = True, cmap = \"YlGnBu\", linewidth = 0.1)\nplt.show()","af45a59e":"#Let us explore how Petal Width and Petal Length features are distributed\nsns.FacetGrid(iris, hue = \"Species\", height = 5).map(plt.scatter, \"PetalWidthCm\", \"PetalLengthCm\").add_legend();\nplt.show()","17307618":"from sklearn.decomposition import PCA  \n\n#Input Features in 4-Dimensions in X variable, Preparing the target in Y variable\nX_iris = iris.drop('Species', axis=1)\ny_iris = iris['Species']\n\nmodel = PCA(n_components=2) # hyperparameters setting\nmodel.fit(X_iris)                      \nX_iris_2D = model.transform(X_iris)  # Transform the data to two dimensions","1c1afc11":"iris['PCA1'] = X_iris_2D[:, 0]\niris['PCA2'] = X_iris_2D[:, 1]\nsns.lmplot(\"PCA1\", \"PCA2\", hue='Species', data=iris, fit_reg=False);","e9a36a94":"from sklearn.mixture import GaussianMixture      \nmodel = GaussianMixture(n_components=3, covariance_type='full')  # hyperparameters\nmodel.fit(X_iris)                    \ny_gmm = model.predict(X_iris)        # determine the cluster labels","1ebf67b2":"iris['cluster'] = y_gmm\nsns.lmplot(\"PCA1\", \"PCA2\", data=iris, hue='Species', col='cluster', fit_reg=False);","f61289ab":"#Cross Validation & Train, Test set preparation\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\n#Performance Measurements\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Importing various classifiers\n#Linear Classifiers\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Non-linear classifiers\nfrom sklearn.naive_bayes import GaussianNB # Naive Bayes Classifier\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn.svm import SVC  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algorithm\n\n#Ensemble Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n","c0be79b5":"#Splitting the dataset into train and test sets in the ration 70:30\nX_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size = 0.30, random_state=42)","96adf35a":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7a8fb86b":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RFC', RandomForestClassifier()))\nprint(models)\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: Mean: %f SD: (%f)' % (name, cv_results.mean(), cv_results.std()))","05ae2549":"#Compare Algorithms\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","17886fea":"# Make predictions on validation dataset\n\npredict_results = []\nnames = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    predict_results.append(predictions)\n    names.append(name)\n    print('%s: ' % (name))\n    # Evaluate predictions\n    print(accuracy_score(y_test, predictions))\n    confMat = confusion_matrix(y_test, predictions)\n    print(confMat)\n    plt.matshow(confMat, cmap = \"Greys\");\\\n    plt.title(name)\n    print(classification_report(y_test, predictions))","3099fb1c":"# **Action Plan:**\n\n\n 1. Problem Statement\n 2. Select a Performance Measure\n 3. Get the Data\n 4. EDA - Discover and Visualize the Data to gain Insights\n    - Dimensionality Reduction Check with PCA\n    - Unsupervised Algorithm - Clustering Analysis\n 5. Split the data into Train and Test sets\n 6. ML Model Building\n        - Select and Train a Model\n        - Evaluate the Model on Test set\n          - Naive Bayes Classifier\n          - Logistic Regression\n          - k-Nearest Neighbors (kNN) Classifier\n          - Support Vector Machine (SVM)\n          - Decision Tree Classifier \n          - Random Forest Classifier (Ensemble method)\n 7. Conclusion","ad81132e":"**Observations**:\n1. Dataset has a total of 150 records. There are no columnwise missing values. \n2. Total of 6 columns namely Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species\n3. Species is a Categorical column - This is also going to be the classification prediction model's output.\n4. Column 'Id'is a nominal feature - not of much use to us.\n5. Remaining 4 columns are going to be the major features for our analysis and model building (SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm).\n6. All the above 4 features are of dtype - float64.\n7. Target variable - \"Species\" is 'Balanced' with 50 species in each of the three classes.","57fed34c":"## Dimensionality Reduction Check using PCA\nPetalLengthCm and PetalWidthCm are strong positively correlated. Let us check if we can reduce the number of dimensions from existing four to a lesser number.","cc2f09f5":"* From the pairplots, we can see that PetalLengthCm and PetalWidthCm are good features to be in our Model, since they provide distinguishable plots for Setosa and approximately distinguishable plots for Versicolor and Virginica.\n* Also, the KDE (Kernel Density Estimates) plot of PetalLengthCm vs PetalWidthCm shows how the distribution of Setosa is clearly separable from the other two species.  ","442fe51e":"Observations:\n- SepalWidthCm KDE curve displays \"Normal Distribution\" properties.\n- PetalLengthCm and PetalWidthCm are having \"bimodal\" properties are not clearly NOT Normally distributed. \n> *This might be because of the Setosa distribution \"mixed\" (Gaussian Mixture) with Versicolor and Virginica Species. *\n- PetalLengthCm and PetalWidthCm are left-skewed (negative skew). No Outliers are observed in both the attributes. \n- SepalWidthCm and SepalLengthCm shows minimal skewness - same can be confirmed from KDE plots also.\n- Few Outliers are observed in SepalWidthCm","4ad8749b":"# 5. Split the data into Train and Test sets","538377f4":"## Unsupervised : Clustering Alogorithm for Gaussian Mixture Models","ea65f8f5":"# 4. EDA - Discover and Visualize the Data to gain Insights","c7719202":"Clustering algorithms try to cluster the data points without considering the 'label' data. From the output plots, we can clearly see that the iris species here can be easily classified into three different clusters.","e48036ee":"Conclusion:\nEven after the PCA transformation is applied, still the features in 2-Dimensions are fairly well separated. Therefore, we will be going ahead with all the four features as initially concluded. ","266eb529":"# 1. Problem Statement\n* Classify the iris dataset based on the available features Sepal Length, Sepal Width, Petal Length, Petal Width. \n* Gain insights from the data.\n* Identify the optimal Classifier ML model for the same.\n\n## About Iris Data set:\n\n**Data set Source:** \nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/iris\n\n**Data Set Information:**\n\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\n**Predicted attribute:** class of iris plant.\n\n\n**Attribute Information:**\n\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class:\n-- Iris Setosa\n-- Iris Versicolour\n-- Iris Virginica","1af7cfd4":"# 2. Select a Performance Measure\n\n- Confusion Matrix - True Positive Rate (TPR), False Negative Rate (FNR) etc.\n- Accuracy Score","f3d1343b":"## Class Distribution - Check for Class imbalance","dfe346e2":"## Multivariate Analysis","c11432f1":"## Univariate Analysis","0a1b6df8":"**Observations:**\n1. PetalLengthCm and PetalWidthCm are strong positively correlated.\n2. However, SepalLengthCm and SepalWidthCm are not correlated.\n3. SepalLengthCm is showing positive correlation with remaining three features.\n4. However, SepalWidthCm is showing weak correlations with remaining three features.\n\n**Conclusion:** It is better to include all the four attributes in our model.","e633e479":"Most of the values are in main diagonal of Confusion Matrix. It shows the model is performing well.","feb2954e":"# Welcome to the Hello World of ML Models :) Playing with Iris Data set with Supervised & Unsupervised algorithm based classifiers. ","d297a58a":"# 7. Conclusion:\n\n## Due to the simple nature of the dataset with lack of complexities, most of the classifiers has produced close-to-one accuracy results, Support Vector Machine Classifier is preferred due to its mean and spread values from cross-validation process. ","62ac28a6":"Observations:\n- Mean of Petal Width for all the three species are on lower side compared to others. ","e8512717":"# 6. ML Model Building & Evaluation","13bd476a":"# 3. Get the Data"}}