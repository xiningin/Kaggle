{"cell_type":{"10eb8167":"code","cb2f5ce1":"code","b17c8aa6":"code","ec0643ed":"code","659180dd":"code","0516a7e1":"code","079f11cd":"code","257dc479":"code","5ab4e397":"code","406be582":"code","a0b64bd5":"code","03d22182":"code","2d7fcfd8":"code","94903724":"code","404c1de0":"code","a233fc89":"code","b10a684f":"code","9f95ec16":"code","bf894c46":"code","ca54ac35":"code","181279f1":"code","3c759ba8":"code","89032fef":"code","58966b5c":"code","8bb14494":"code","cbd0b018":"code","aa854c2e":"markdown","e0bfce3f":"markdown","2af58c0a":"markdown","171705e8":"markdown","2de97e0e":"markdown","c4d5a2f0":"markdown","8efcf0d1":"markdown","974706f1":"markdown","4097bde2":"markdown","8e175cc2":"markdown"},"source":{"10eb8167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cb2f5ce1":"def load_data(file):\n    file = '..\/input\/'+file+'.csv'\n    return pd.read_csv(file)\n\ndf_train_set = load_data(\"train\")\ndf_test_set = load_data(\"test\")","b17c8aa6":"df_train_set.info()","ec0643ed":"df_train_set.head(5)","659180dd":"pd.pivot_table(df_train_set, values=\"PassengerId\", index=\"Pclass\", columns=\"Survived\",aggfunc='count',\n               margins=True)","0516a7e1":"func = lambda x: 100*x.count()\/df_train_set.shape[0]\n\npd.pivot_table(df_train_set, values=\"PassengerId\", index=[\"Pclass\"], columns=\"Survived\", aggfunc=func,\n               margins=True, fill_value=0)","079f11cd":"pd.pivot_table(df_train_set, values=\"PassengerId\", index=\"Embarked\", columns=\"Survived\",aggfunc='count',\n               margins=True)","257dc479":"df_train_set_final = df_train_set","5ab4e397":"# Remove the two rows that are missing embarkation information\ndf_train_set_final = df_train_set_final.dropna(subset=[\"Embarked\"])\n\n# Remove the columns Cabin and Ticket information for this analysis\ndf_train_set_final.drop(columns=['Cabin','Ticket'], inplace=True)","406be582":"corr_matrix_train = df_train_set_final.corr()","a0b64bd5":"corr_matrix_train[\"Survived\"].sort_values(ascending=False)","03d22182":"df_train_set_final.boxplot(by='Survived', column=['Fare'], grid = False)","2d7fcfd8":"df_train_set_final.info()","94903724":"df_train_set_final.describe()","404c1de0":"# Creating a DataFrame Selector that will pull either categorical or numerical columns\n# Data Frame selector\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n\tdef __init__(self, attribute_names):\n\t\tself.attribute_names = attribute_names\n\tdef fit(self, X, y=None):\n\t\treturn self\n\tdef transform(self, X):\n\t\treturn X[self.attribute_names].values","a233fc89":"# Copying the labels into a dataset\n\ny_train = df_train_set_final[\"Survived\"]","b10a684f":"from sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# One hot encoder for converting the categorical values into binary values\ncat_encoder = OneHotEncoder(sparse=False)\n\n# Using median strategy to replace the missing values for Age column\nImputer = SimpleImputer(strategy=\"median\")\n\n# List of numerical and categorical attributes\nnum_attribs = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\ncat_attribs = [\"Sex\",\"Embarked\",\"Pclass\"]\n\nnum_pipeline = Pipeline([\n        ('selector', DataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\ncat_pipeline = Pipeline([\n('selector', DataFrameSelector(cat_attribs)),\n('cat_encode', OneHotEncoder(sparse=False)),\n])\nfull_pipeline = FeatureUnion(transformer_list=[\n(\"num_pipeline\", num_pipeline),\n(\"cat_pipeline\", cat_pipeline),\n])\n\n","9f95ec16":"# Applying the full pipeline on the training set\n\nX_train = full_pipeline.fit_transform(df_train_set_final)","bf894c46":"X_train","ca54ac35":"X_train.shape","181279f1":"from sklearn.svm import SVC\n\n#svm_clf = SVC(gamma=\"auto)\nsvm_clf = SVC(gamma=\"auto\", C=1, degree=1, kernel='rbf')\n\nsvm_clf.fit(X_train, y_train)","3c759ba8":"X_test = full_pipeline.fit_transform(df_test_set)\ny_pred = svm_clf.predict(X_test)","89032fef":"from sklearn.model_selection import cross_val_score\n\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()","58966b5c":"X_test","8bb14494":"submission = pd.DataFrame({\n    \"PassengerId\": df_test_set[\"PassengerId\"],\n    \"Survived\": y_pred\n})\n\n\nsubmission.head(5)\n","cbd0b018":"submission.to_csv('submission.csv', index=False)","aa854c2e":"** Getting information on the data attributes **","e0bfce3f":"** From the cross validation exercise we see a 82% accuracy **","2af58c0a":"* One way to handle the missing \"Age\" rows is to replace the age with median age *","171705e8":"** Loading training and testing data **","2de97e0e":"Handling Data quality issues","c4d5a2f0":"Checking for correlations among the variables with Survival","8efcf0d1":"1. The training set has 891 rows with 12 columns\n2. Data has 5 object datatypes, 2 float datatypes and 5 integer datatypes\n3. Of 891 rows only 204 have cabin data filled (this is a data quality issue that has to be addressed)\n4. Embarked data is present for all the rows except 2 rows. We will handle this issue in below code\n5. Similary Age attribute is only present for 714 rows","974706f1":"Make a copy of the training dataset","4097bde2":"** For the first model lets use SVM model **","8e175cc2":"** Lets create a pipelines to handle both the numeric and categorical data attributes **"}}