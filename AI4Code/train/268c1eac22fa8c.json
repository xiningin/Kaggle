{"cell_type":{"bc50f69c":"code","94f389d2":"code","40a402fb":"code","fc32fce8":"code","56690030":"code","4ea4fe25":"code","898511d7":"code","3e851b49":"code","a02ba1fa":"code","b749805f":"code","15aaaac2":"code","964bef11":"code","ba7afc56":"code","fd918d3d":"code","500a89c2":"code","01f7a251":"code","e3a81cf9":"code","02ebb220":"code","d63b7ef4":"code","d1bf55c1":"code","45294c76":"code","564bfdda":"code","6a8a2831":"markdown","59b8e0f6":"markdown","58e8c4b7":"markdown","eddc3c87":"markdown","773df9f2":"markdown","1d02693f":"markdown","f380bdf2":"markdown","b0104bbf":"markdown","a461ff5c":"markdown","60b133fd":"markdown","3eb6fdcc":"markdown","5e96f827":"markdown","4ef1f1cd":"markdown","98b1eefa":"markdown","1fab531a":"markdown","b9073a66":"markdown","9c7e6b13":"markdown","25cb5476":"markdown","6daa47ad":"markdown"},"source":{"bc50f69c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","94f389d2":"from langdetect import detect\nfrom tqdm.auto import tqdm\ntqdm.pandas()","40a402fb":"import glob, json\n\ndef load_docs(base_path):\n    loaded_docs = []\n    file_paths = glob.glob(base_path)\n    file_names = [os.path.basename(path) for path in file_paths]\n    for filepath in tqdm(file_paths):\n        doc = \"\"\n        with open(filepath) as f:\n            d = json.load(f)\n            for paragraph in d[\"body_text\"]:\n                doc += \" \"+paragraph[\"text\"].lower()\n            loaded_docs.append(doc)\n    return loaded_docs","fc32fce8":"medx_docs = load_docs(\"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/*.json\")\n","56690030":"import collections\n\ndef count_langs(docs):\n    langs = []\n    non_english_idx = []\n    broken_idx = []\n    for idx, doc in enumerate(tqdm(docs)):\n        try:\n            lang = detect(doc)\n            if lang != 'en':\n                non_english_idx.append((lang, idx))\n        except Exception as e: \n            print(f\"Error detecing lang for doc {idx}: {e}\")\n            broken_idx.append(idx)\n        langs.append(lang)\n    counts = collections.Counter(langs)\n    #print(f\"non-english (lang, idx): {non_english_idx}\")\n    #print(f\"broken (idx): {broken_idx}\")\n    for idx in broken_idx:\n        print(f\"broken {idx}: {docs[idx]}\")\n        print()\n    return counts, non_english_idx\n","4ea4fe25":"def print_non_english(docs, indices):\n    for idx in indices:\n        lang = idx[0]\n        doc_idx = idx[1]\n        if lang != 'fr' and lang != 'es' and lang != 'de':\n            #there appear to be 200-300 french and spanish documents each, so skip those\n            #the rest are just a few, so might as well just take a look just for interest\n            print(f\"{doc_idx}, {lang}: {docs[doc_idx][:1000]}\")\n            print()\n    ","898511d7":"medx_counts, medx_nes = count_langs(medx_docs)\nmedx_counts.most_common()","3e851b49":"comuse_docs = load_docs(\"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/*.json\")\n","a02ba1fa":"comuse_counts, comuse_nes = count_langs(comuse_docs)\ncomuse_counts.most_common()","b749805f":"custom_docs = load_docs(\"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/*.json\")\n","15aaaac2":"custom_counts, custom_nes = count_langs(custom_docs)\ncustom_counts.most_common()","964bef11":"noncom_docs = load_docs(\"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/*.json\")\n","ba7afc56":"noncom_counts, noncom_nes = count_langs(noncom_docs)\nnoncom_counts.most_common()","fd918d3d":"total_count = collections.Counter()\ntotal_count += medx_counts\ntotal_count += comuse_counts\ntotal_count += custom_counts\ntotal_count += noncom_counts\ntotal_count.most_common()","500a89c2":"print_non_english(medx_docs, medx_nes)","01f7a251":"print_non_english(comuse_docs, comuse_nes)","e3a81cf9":"print_non_english(custom_docs, custom_nes)","02ebb220":"print_non_english(noncom_docs, noncom_nes)","d63b7ef4":"def print_lang(docs, indices, lang_to_print, max):\n    count = 0\n    for idx in indices:\n        lang = idx[0]\n        doc_idx = idx[1]\n        if lang != lang_to_print:\n            continue\n        print(f\"{doc_idx}, {lang}: {docs[doc_idx][:1000]}\")\n        print()\n        count += 1\n        if count >= max:\n            break\n    ","d1bf55c1":"print_lang(custom_docs, custom_nes, \"fr\", 5)","45294c76":"print_lang(custom_docs, custom_nes, \"es\", 5)","564bfdda":"print_lang(custom_docs, custom_nes, \"de\", 5)","6a8a2831":"## Medrxiv, outliers","59b8e0f6":"Having run this kernel multiple times, the actual indices of the \"broken\" documents changes, so cannot really re-use the indices but rather seems better to run it each time. Or make a filtered dataset..","58e8c4b7":"## Spanish","eddc3c87":"## CommUse, outliers","773df9f2":"## Biorxiv \/ Medrxiv","1d02693f":"# Languages","f380bdf2":"## French","b0104bbf":"But I found at least many Spanish and French docs that seem to contain something meaningful. So, if you have a nice solution to translate the valid non-English docs, would be happy to hear...","a461ff5c":"## NonComm Use","60b133fd":"Some of the non-English identified also seem a little bit messed up, likely causing identification as something different. Such as 'cy' [seems to be](https:\/\/www.loc.gov\/standards\/iso639-2\/php\/code_list.php) for Welsh :).","3eb6fdcc":"## Custom License, Outliers","5e96f827":"# Top Languages","4ef1f1cd":"# Some Valid: French, Spanish, Deutch, ...","98b1eefa":"## German","1fab531a":"## Custom License","b9073a66":"## Comm_Use","9c7e6b13":"Recently I was doing some preprocessing on the documents in this set, and noticed there are different languages in the documents. Found a [question](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139146) on the forum about how many languages, with a pointer to a nice [language detection library](https:\/\/pypi.org\/project\/langdetect\/). Thanks to @dannellyz for the pointer. So this kernel is a quick look at how many docs there are in different languages.\n\nAlso helps to identify some documents that seem to be corrupt or otherwise invalid. In the end, it seems there is maybe around 1-2% of documents in non-English languages. I may skip them from my further analysis unless I find a nice way to translate them..\n\nAnway..","25cb5476":"## NonComm, outliers","6daa47ad":"Thats all folks, ..."}}