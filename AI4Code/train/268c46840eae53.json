{"cell_type":{"f75c55ce":"code","f5ca8e77":"code","15082e01":"code","b4f88d58":"code","7003dc67":"code","dc250b99":"code","7235ad37":"code","dc02a238":"code","b0ea497e":"code","fef93563":"code","fa790781":"markdown","af4af7c5":"markdown","e95dcb6d":"markdown","8fc3d17a":"markdown","8bf1a142":"markdown"},"source":{"f75c55ce":"\"\"\"\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\ntoday = pd.to_datetime('2016-01-01')\n\ntr = pd.read_csv('..\/input\/train.csv').set_index('id')\nte = pd.read_csv('..\/input\/test.csv').set_index('id')\ndf = pd.concat([tr, te])\ndf['price_per_living'] = np.log1p(df['price']\/df['sqft_living'])\ndf = df.drop('price', axis=1)\ntr_idx = tr.index.tolist()\nte_idx = te.index.tolist()\ndel tr, te; gc.collect();\n\ndf['was_renovated'] = (df['yr_renovated'] != 0).astype('uint8')\nnot_renovated = df[df['was_renovated'] == 0].index\ndf.loc[not_renovated, 'yr_renovated'] = df.loc[not_renovated, 'yr_built']\ndf['date'] = pd.to_datetime(df['date'].str[:8])\ndf['yr_built'] = pd.to_datetime({'year': df['yr_built'], 'month': [1]*len(df), 'day': [1]*len(df)})\ndf['yr_renovated'] = pd.to_datetime({'year': df['yr_renovated'], 'month': [1]*len(df), 'day': [1]*len(df)}, errors='coerce')\n\ndf['today-D-date'] = (today - df['date']).dt.days\ndf['today-D-yr_renovated'] = (today - df['yr_renovated']).dt.days\ndf['today-D-yr_built'] = (today - df['yr_built']).dt.days\ndf['date-D-yr_built'] = (df['date'] - df['yr_built']).dt.days\ndf['yr_renovated-D-yr_built'] = (df['yr_renovated'] - df['yr_built']).dt.days\ndf = df.drop(['date', 'yr_built', 'yr_renovated'], axis=1)\n\ndf['room_count'] = df['bedrooms'] + df['bathrooms']\ndf['sqft_living_per_rooms'] = df['sqft_living'] \/ (df['room_count']+1)\ndf['sqft_lot_per_rooms'] = df['sqft_lot'] \/ (df['room_count']+1)\ndf['room_per_floors'] = df['room_count'] \/ df['floors']\n\ndf['sqft_living_per_floors'] = df['sqft_living'] \/ df['floors']\ndf['sqft_lot_per_floors'] = df['sqft_lot'] \/ df['floors']\n\ndf['sqft_living_per_bedrooms'] = df['sqft_living'] \/ (df['bedrooms']+1)\ndf['sqft_lot_per_bedrooms'] = df['sqft_lot'] \/ (df['bedrooms']+1)\n\ndf['bedroom_per_floors'] = df['bedrooms'] \/ df['floors']\n\ndf['sqft_lot-D-sqft_living'] = df['sqft_lot'] - df['sqft_living']\ndf['sqft_lot-R-sqft_living'] = df['sqft_lot'] \/ df['sqft_living']\n\ndf['sqft_living15-D-sqft_living'] = df['sqft_living15'] - df['sqft_living']\ndf['sqft_living15-R-sqft_living'] = df['sqft_living15'] \/ df['sqft_living']\n\ndf['sqft_lot15-D-sqft_lot'] = df['sqft_lot15'] - df['sqft_lot']\ndf['sqft_lot15-R-sqft_lot'] = df['sqft_lot15'] \/ df['sqft_lot']\n\ndf['rooms_mul']=df['bedrooms']*df['bathrooms']\ndf['total_score']=df['condition']+df['grade']+df['view']\n\ndf['has_basement'] = (df['sqft_basement']>0).astype('uint8')\ndf['has_attic'] = ((df['floors'] % 1) != 0).astype('uint8')\n\nfor k in range(4, 60, 4):\n    km = KMeans(32, n_jobs=8)\n    df['clustering_'+str(k)] = km.fit_predict(df[['lat', 'long']])\n    df['clustering_'+str(k)] = df['clustering_'+str(k)].astype(str)\n    \ndf['zipcode'] = df['zipcode'].astype('str')\ndf['zipcode-3'] = df['zipcode'].str[2:3]\ndf['zipcode-4'] = df['zipcode'].str[3:4]\ndf['zipcode-5'] = df['zipcode'].str[4:5]\ndf['zipcode-34'] = df['zipcode'].str[2:4]\ndf['zipcode-45'] = df['zipcode'].str[3:5]\ndf['zipcode-35'] = df['zipcode'].str[2:3] + df['zipcode'].str[4:5]\n\ncols =  ['zipcode']\nfor col in cols:\n    val_count = df[col].value_counts()\n    agg_cols = ['price_per_living']\n    temp = df.groupby(col)[agg_cols].agg('mean').rename({k: str(col)+'_mean_'+str(k) for k in agg_cols}, axis=1)\n    df = df.merge(temp, how='left', on=col)\n    \ndef haversine_array(lat1, lng1, lat2, lng2): \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    AVG_EARTH_RADIUS = 6371 # in km \n    lat = lat2 - lat1 \n    lng = lng2 - lng1 \n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 \n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n    return h\n\nlat2, long2 = df['lat'].values, df['long'].values\ndist = pd.DataFrame([haversine_array(df.loc[i, 'lat'], df.loc[i, 'long'], lat2, long2) for i in df.index], index=df.index, columns=df.index, dtype='float32')\nrel_cols = ['price_per_living', 'sqft_living', 'condition', 'view', 'grade']\nfor k in [1, 3, 10]:\n    near = dist[(dist>0)&(dist<k)]\n    neighbors = [near.loc[i].dropna().index.tolist() for i in near.index]\n    cols = ['{}_{}'.format(col, k) for col in rel_cols]\n    tmp = pd.DataFrame([df.loc[near, rel_cols].mean().values for near in neighbors], index=df.index, columns=cols)\n    df = pd.concat([df, tmp], axis=1)\nfor col in rel_cols:\n    df[col+'_10'] = df[col+'_10'].fillna(df[col+'_10'].mean())\n    df[col+'_3'] = df[col+'_3'].fillna(df[col+'_10'])\n    df[col+'_1'] = df[col+'_1'].fillna(df[col+'_3'])\n    \ndf = pd.get_dummies(df)\nskewness = pd.Series()\nfor col in [col for col in df.columns if col.startswith('sqft')]:\n    if (df[col]<0).sum() == 0:\n        skewness.loc[col] = df[col].skew()\nskew_col = skewness[skewness>1].index.tolist()\ndf[skew_col] = np.log1p(df[skew_col])\nfeats = [col for col in df.columns if col != 'price_per_living']\ndf[feats] = StandardScaler().fit_transform(df[feats])\n\nx_data = df.loc[tr_idx, feats]\ny_data = df.loc['price_per_living']\ntest = df.loc[te_idx, feats]\n\"\"\"","f5ca8e77":"import pandas as pd\nimport numpy as np\nfrom ast import literal_eval\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")","15082e01":"org_tr = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\norg_te = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\nx_data = pd.read_csv('..\/input\/kakr-2nd-comp-processed-dataset\/x_data.csv', index_col='id')\ny_data = pd.Series.from_csv('..\/input\/kakr-2nd-comp-processed-dataset\/y_data.csv')\ntest = pd.read_csv('..\/input\/kakr-2nd-comp-processed-dataset\/test.csv', index_col='id')\nparams = pd.read_csv('..\/input\/kakr-2nd-comp-processed-dataset\/params.csv')\nparams['params'] = params['params'].apply(literal_eval)\nparams['feats'] = params['feats'].apply(literal_eval)","b4f88d58":"print(x_data.shape)\nx_data.head()","7003dc67":"test.head()","dc250b99":"params","7235ad37":"def rmse(pred, true):\n    return -np.sqrt(np.mean((pred-true)**2))\n\ndef oof(algo, params, cols):\n    prediction = np.zeros(len(x_data))\n    test_prediction = np.zeros(len(test))\n    for t, v in KFold(5, random_state=0).split(x_data):\n        x_train = x_data[cols].iloc[t]\n        x_val = x_data[cols].iloc[v]\n        y_train = y_data.iloc[t]\n        y_val = y_data.iloc[v]\n        if algo.startswith('lgb'):\n            model = LGBMRegressor(**params)\n        elif algo == 'xgb':\n            model = XGBRegressor(**params)\n        elif algo == 'cb':\n            model = CatBoostRegressor(**params)\n        elif algo == 'ridge':\n            model = Ridge(**params)\n        elif algo == 'svm':\n            model = LinearSVR(**params)\n        elif algo == 'rbf':\n            model = SVR(**params)\n        elif algo == 'knn':\n            model = KNeighborsRegressor(**params)\n        if algo in ['ridge', 'svm', 'rbf', 'knn']:\n            model.fit(x_train, y_train)\n        else:\n            model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False, early_stopping_rounds=100)\n        prediction[v] = np.expm1(model.predict(x_val))\n        test_prediction += np.expm1(model.predict(test[cols]))\/5\n    score = rmse(prediction*org_tr['sqft_living'], org_tr['price'])\n    print(score)\n    return prediction, test_prediction","dc02a238":"test_predictions = []\nval_predictions = []\nfor i in params.index.tolist():\n    row = params.loc[i]\n    if row['algo'] == 'cb':\n        row['params']['task_type'] = 'CPU'\n    elif row['algo'].startswith('lgb'):\n        row['params']['device_type'] = 'cpu'\n        row['params'].pop('metric')\n    val_pred, test_pred = oof(row['algo'], row['params'], row['feats'])\n    val_predictions.append(val_pred)\n    test_predictions.append(test_pred)","b0ea497e":"val_predictions_t = np.array(val_predictions).transpose()\nlr = LinearRegression()\nlr.fit(val_predictions_t, np.expm1(y_data))\nrmse(lr.predict(val_predictions_t)*org_tr['sqft_living'], org_tr['price'])","fef93563":"test_prediction = lr.predict(np.array(test_predictions).transpose())*org_te['sqft_living']\nsubmission = pd.DataFrame({'id': org_te['id'], 'price': test_prediction})\nsubmission.to_csv('submissions.csv', index=False)","fa790781":"# Stacking\n\nstacking\uc740 \uac04\ub2e8\ud55c linear regression \ubaa8\ub378\uc744 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4. (https:\/\/www.kaggle.com\/harangdev\/stacking-simple-average-vs-linear-regression \ucc38\uace0)","af4af7c5":"# Feature Engineering\n\n* price\ub97c target\uc73c\ub85c \ub193\uc9c0 \uc54a\uace0, price\/sqft_living\uc744 target\uc73c\ub85c \uc124\uc815\n* sqft\ub85c \uc2dc\uc791\ud558\ub294 feature\ub4e4\uacfc target\uc5d0 \ub300\ud558\uc5ec log \ubcc0\ud658 \uc2e4\ud589 (https:\/\/www.kaggle.com\/harangdev\/skewed-feature-target-log \ucc38\uace0)\n* yr_renovated\uac00 0\uc778\uc9c0\uc758 \uc5ec\ubd80\ub85c was_renovated \uc5f4 \ub9cc\ub4e4\uace0, \uae30\uc874 yr\\_renovated \uc5f4\uc5d0\uc11c\ub294 0\uc744 \uac01 \ud589\uc758 yr_built\ub85c \ub300\uccb4\ud568\n* date feature\ub4e4\uc740 \ubaa8\ub450 timedelta\ub85c \ubc14\uafc8\n* \uc5ec\ub7ec feature\ub4e4\uc744 \uc870\ud569\n* KMeans clustering\uc744 \uc801\uc6a9\ud558\uc5ec \uba87 \ubc88\uc9f8 clustering\uc5d0 \ud574\ub2f9\ud558\ub294\uc9c0\n* zipcode\ub97c \ubd84\ud574\n* zipcode \ud65c\uc6a9 mean target encode\n* \ud2b9\uc815 \uc8fc\ud0dd\uacfc \uac70\ub9ac\uac00 \uac00\uae4c\uc6b4 \uc8fc\ud0dd\ub4e4\uc758 \ud3c9\uade0 feature\uac12 \ubc0f target\uac12\n* standard scaling\n\ngeo data feature engineering\uacfc \uad00\ub828\ud574\uc11c\ub294 \ud5c8\ud0dc\uba85\ub2d8\uc758 https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering \ub97c \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uce90\uae00 \ucee4\ub110\uc5d0\uc11c\ub294 \uba54\ubaa8\ub9ac \ubb38\uc81c\ub85c \uac00\uae4c\uc6b4 \uc8fc\ud0dd\uc744 \ucc3e\ub294 \ucf54\ub4dc\uac00 \uc2e4\ud589\uc774 \uc548\ub418\ubbc0\ub85c, \ub9cc\ub4e4\uc5b4\ub193\uc740 \ub370\uc774\ud130\ub97c \ub85c\ub4dc\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130\ub97c \ub9cc\ub4dc\ub294 \ucf54\ub4dc\ub294 \uc8fc\uc11d\ucc98\ub9ac\ud558\uc5ec \uc544\ub798\uc5d0 \uc801\uc5b4\ub193\uc558\uc2b5\ub2c8\ub2e4.","e95dcb6d":"# \ucd5c\uc885 test \uc608\uce21\uac12 \uc81c\ucd9c","8fc3d17a":"# \ud559\uc2b5\n\n\uc54c\uace0\ub9ac\uc998\uc740 lightgbm(gbdt), lightgbm(goss), lightgbm(rf), catboost, xgboost, ridge, knn, svr(linear), svr(rbf)\uc744 \uc37c\uc2b5\ub2c8\ub2e4.\n\noof \uae30\ubc95\uc744 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.","8bf1a142":"# \ubaa8\ub4c8\uacfc \ucc98\ub9ac\ub41c \ub370\uc774\ud130 \ub85c\ub4dc\n\nfeature\ub97c \ucc98\uc74c\uc5d0 \ubb34\uc791\uc815 \ub9ce\uc774 \uc0dd\uc131\ud558\uc600\uae30 \ub54c\ubb38\uc5d0 lightgbm \ubaa8\ub378\uc758 feature\\_importances\\_ \uc18d\uc131\uc744 \ud65c\uc6a9\ud55c rfe(recurrent feature elimination)\uc73c\ub85c feature \uc218\ub97c \uc904\uc778 \ub4a4 \ud559\uc2b5\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. (https:\/\/www.kaggle.com\/harangdev\/feature-selection \ucc38\uace0) rfe\ub97c \ucd1d 10\ubc88 \ubc18\ubcf5\ud558\uc5ec 10\uac1c\uc758 feature set\uc744 \ubf51\uc544\ub0b4\uace0, hyperparameter optimization\uc744 \ud560 \ub54c \uac19\uc774 \ucd5c\uc801\ud654\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.\n\n[optuna](https:\/\/optuna.org\/)\ub85c \ud558\uc774\ud37c\ub9c8\ub77c\ubbf8\ud130 \ucd5c\uc801\ud654\ub97c \uc2e4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4. \uac01 \uc54c\uace0\ub9ac\uc998\ubcc4\ub85c \ucd5c\uc801\uc758 \ubaa8\ub378 \ud558\ub098\uc529\uc744 \ubf51\uace0, \uc774\ub97c \uc81c\uc678\ud558\uace0 \uc131\ub2a5\uc774 \uc81c\uc77c \uc88b\uc740 4\uac1c\uc758 \ubaa8\ub378\uc744 \ucd94\uac00\ub85c \ubf51\uc558\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub807\uac8c \uc120\ud0dd\ub41c params\uc640 feats\ub294 params.csv \ud30c\uc77c\ub85c \ucee4\ub110 \ub370\uc774\ud130\uc14b\uc5d0 \uc62c\ub838\uc2b5\ub2c8\ub2e4."}}