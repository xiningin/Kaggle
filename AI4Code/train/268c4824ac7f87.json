{"cell_type":{"c01dd442":"code","09a823d7":"code","b5e5a1fb":"code","d7a45c31":"code","156ae922":"code","177b6d59":"code","9dfc7d33":"code","2a73d553":"code","0d245dd2":"code","28234ff1":"code","4b67f9be":"code","820219da":"code","e40abb49":"code","d8959aad":"code","7d9c8e9d":"code","94b7af98":"code","fdb74768":"code","abb86fdd":"code","94f4ebac":"code","b4073355":"code","56eec0f1":"code","392b398c":"code","a6d3d98a":"code","1e6f8219":"code","e9788af6":"code","df02aafa":"markdown","3786f0b3":"markdown","110198da":"markdown","5d09a540":"markdown","5c5a940f":"markdown","6a314ff9":"markdown","3532d6b1":"markdown","e0b4a648":"markdown","b86e894b":"markdown","174d77a7":"markdown"},"source":{"c01dd442":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport optuna\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09a823d7":"%%time\n\npath='\/kaggle\/input\/tabular-playground-series-oct-2021\/'\ndf_training = pd.read_csv(path+'train.csv')\n#READING TEST DATASET AND SUBMISSION FILE\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_submission.csv')","b5e5a1fb":"df_training.head()","d7a45c31":"df_training.shape","156ae922":"for column in df_training.columns:\n    if df_training[column].dtype == \"float64\":\n        df_training[column]=pd.to_numeric(df_training[column], downcast=\"float\")\n    if df_training[column].dtype == \"int64\":\n        df_training[column]=pd.to_numeric(df_training[column], downcast=\"integer\")","177b6d59":"df_training.head()","9dfc7d33":"for column in df_test.columns:\n    if df_test[column].dtype == \"float64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"float\")\n    if df_test[column].dtype == \"int64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"integer\")","2a73d553":"df_test.head()","0d245dd2":"X = df_training.drop('target', axis=1).copy()\nX.head()","28234ff1":"y = df_training['target'].copy()\ny.head()","4b67f9be":"X_test = df_test.copy()\nX_test.head()","820219da":"#new_f1= df_training.groupby(['f179'])['target'].mean()\n#X[\"new_f1\"] = X[\"f22\"]*X[\"f53\"]\n#new_f2= df_training.groupby(['f69'])['target'].mean()\n#new_f3= df_training.groupby(['f58'])['target'].mean()\n#new_f4= df_training.groupby(['f136'])['target'].mean()\n#new_f5= df_training.groupby(['f156'])['target'].mean()\n#new_f6= df_training.groupby(['f78'])['target'].mean()\n#new_f7= df_training.groupby(['f8'])['target'].mean()\n#new_f8= df_training.groupby(['f214'])['target'].mean()\n#feature_eng = [new_f1,new_f2,new_f3,new_f4,new_f5,new_f6,new_f7,new_f8]\n\n#newdf = pd.DataFrame(feature_eng)\n#newdf.head()","e40abb49":"features = [\"f179\", \"f69\",\"f58\", \"f136\", \"f214\",\"f156\", \"f78\", \"f8\"]\nuseful_features = [\"f22\",\"f179\",\"f69\",\"f156\",\"f58\",\"f136\",\"f214\"]","d8959aad":"#useful_features = [\"f22\", \"f179\", \"f69\",\n                   #\"f58\", \"f214\", \"f78\", \"f136\", \"f156\", \"f8\", \"f3\", \"f77\", \"f200\", \"f92\", \"f185\", \"f142\", \"f115\", \"f284\"]\n#n_features=['f22', 'f179', 'f69', 'f58', 'f78', 'f44', 'f138', 'f139', 'f144' , 'f146', 'f157', 'f158', 'f160', 'f53', 'f170']\n#useful_features_n = ['f22', 'f53', 'f170']\n#n_features=['f22', 'f30', 'f53', 'f56', 'f126', 'f163']\nX[\"mean\"] = X[features].mean(axis=1)\nX[\"std\"] = X[features].std(axis=1)\nX['variance']=X[features].var(axis=1)\nX[\"min\"] = X[features].min(axis=1)\nX[\"max\"] = X[features].max(axis=1)\n\nX_test[\"mean\"] = X_test[features].mean(axis=1)\nX_test[\"std\"] = X_test[features].std(axis=1)\nX_test['variance']=X_test[features].var(axis=1)\nX_test[\"min\"] = X_test[features].min(axis=1)\nX_test[\"max\"] = X_test[features].max(axis=1)\n\n\n#X_test[\"mean\"] = X_test.mean(axis=1)\n#X_test[\"std\"] = X_test.std(axis=1)\n#X_test['variance']=X_test.var(axis=1)\n\n#X_test[\"mean\"] = X_test[useful_features_n].mean(axis=1)\n#X_test[\"std\"] = X_test[useful_features].std(axis=1)\n#X_test[\"min\"] = X_test[useful_features].min(axis=1)\n#X_test[\"max\"] = X_test[useful_features].max(axis=1)\n#X_test[\"new_f1\"] = X_test[\"f22\"]*X_test[\"f53\"]\n#X_test[\"new_f2\"] = X_test[\"f22\"]*X_test[\"f170\"]\n#X_test[\"new_f3\"] = X_test[\"f170\"]*X_test[\"f53\"]\n\n\n#X['std'] = X.std(axis=1)\n#X['min'] = X.min(axis=1)\n#X['max'] = X.max(axis=1)\n\n#X_test['std'] = X_test.std(axis=1)\n#X_test['min'] = X_test.min(axis=1)\n#X_test['max'] = X_test.max(axis=1)\n\n","7d9c8e9d":"X.shape","94b7af98":"from sklearn.cluster import KMeans\nfeature_cols = [col for col in df_test.columns.tolist()]\nn_clusters_1 = 6\ncd_feature = True # cluster distance instead of cluster number\ncluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters_1)]\nkmeans = KMeans(n_clusters=n_clusters_1, n_init=50, max_iter=500, random_state=42)\n\nif cd_feature:\n    # train\n    X_cd = kmeans.fit_transform(df_training[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=df_training.index)\n    df_training = df_training.join(X_cd)\n    # test\n    X_cd = kmeans.transform(df_test[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=df_test.index)\n    df_test = df_test.join(X_cd)\n    \nelse:\n    # train\n    df_training[\"cluster\"] = kmeans.fit_predict(train[useful_features])\n    # test\n    df_test[\"cluster\"] = kmeans.predict(test[useful_features])\n    \n    # one-hot encode\n    ohe = OneHotEncoder()\n    X_ohe = ohe.fit_transform(np.array(df_training[\"cluster\"]).reshape(-1,1)).toarray()\n    T_ohe = ohe.transform(np.array(df_test[\"cluster\"]).reshape(-1,1)).toarray()\n\n    X_ohe = pd.DataFrame(X_ohe, columns=cluster_cols, index=train.index)\n    T_ohe = pd.DataFrame(T_ohe, columns=cluster_cols, index=test.index)\n\n    df_training = pd.concat([train, X_ohe],axis=1)\n    df_test = pd.concat([test, T_ohe],axis=1)\n\nfeature_cols += cluster_cols\ndf_training.head()","fdb74768":"X[\"cluster1\"] = df_training['cluster1']\nX[\"cluster2\"] = df_training['cluster2']\nX[\"cluster3\"] = df_training['cluster3']\nX[\"cluster4\"] = df_training['cluster4']\nX[\"cluster5\"] = df_training['cluster5']\nX[\"cluster6\"] = df_training['cluster6']\n","abb86fdd":"features_n = [\"f22\",\"f179\", \"f69\",\"f58\", \"f136\", \"f214\",\"f156\", \"f78\", \"f8\",\"min\",\n              \"max\",\"mean\",\"std\",\"variance\",\"cluster1\",\"cluster2\",\"cluster3\",\"cluster4\",\"cluster5\",\"cluster6\"]\n","94f4ebac":"X[\"new_f1\"] = (X[\"cluster1\"])\/(X[\"cluster2\"])\nX[\"new_f2\"] = (X[\"cluster3\"])\/(X[\"cluster4\"])\nX[\"new_f3\"] = (X[\"cluster5\"])\/(X[\"cluster6\"])","b4073355":"features_n_f = [\"f22\",\"f179\", \"f69\",\"f58\", \"f136\", \"f214\",\"f156\", \"f78\", \"f8\",\"min\",\n              \"max\",\"mean\",\"std\",\"variance\",\"cluster1\",\"cluster2\",\"cluster3\",\"cluster4\",\n              \"cluster5\",\"cluster6\",\"new_f1\",\"new_f2\",\"new_f3\"]\n","56eec0f1":"X[features_n_f].head()","392b398c":"#{'colsample_bytree': 0.5805849806605896,\n#'gamma': 2.467408277656332, 'max_depth': 8.0, 'min_child_weight': 7.0, 'reg_alpha': 156.0, 'reg_lambda': 0.2993899607673447}\nparams = {\n   # 'max_depth': 6,\n    'max_depth': 8,\n    'n_estimators': 9500,\n    'learning_rate': 0.007279718158350149,\n    #'learning_rate': 0.05,\n    'subsample': 0.7,\n    #'colsample_bytree': 0.2,\n    'colsample_bytree': 0.5805849806605896,\n    'colsample_bylevel': 0.6000000000000001,\n    #'min_child_weight': 56.41980735551558,\n    'min_child_weight': 7.0,\n    #'reg_lambda': 75.56651890088857,\n    'reg_lambda': 0.2993899607673447,\n    #'reg_alpha': 0.11766857055687065,\n    'reg_alpha': 156.0,\n    #'gamma': 0.6407823221122686\n    'gamma': 2.467408277656332\n    }","a6d3d98a":"%%time\nkf = StratifiedKFold(n_splits=7, shuffle=True, random_state=0)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X[features_n_f], y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","1e6f8219":"ss = dt.fread('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\n","e9788af6":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('.\/first_sub_paratun_feat_selec.csv', index=False)\nss.head()","df02aafa":"For parameter tuning, please refer to the following notebook: \n\nhttps:\/\/www.kaggle.com\/adizafar\/xgboost-parameter-tuning-bayesian-optimization","3786f0b3":"For details about feature selection, please see my notebooks:\n\nFeature Selection using Random Forest: https:\/\/www.kaggle.com\/adizafar\/feature-importance-via-random-forest\n\nFeature Selection using Genetic Algorithm:https:\/\/www.kaggle.com\/adizafar\/feature-selection-using-genetic-algorithm","110198da":"# Modeling","5d09a540":"# Parameter Tuning using Bayesian Optimization","5c5a940f":"# Feature Selection\n","6a314ff9":"# XGBoost-Feature Selection- Feature Engineering - Parameter Tuning","3532d6b1":"# Feature Engineering","e0b4a648":"# Submission","b86e894b":"For feature engineering, please see the reference notebook\n\nI have utilized the work of: https:\/\/www.kaggle.com\/motchan\/tps-oct-2021-kmeans","174d77a7":"# Memory Reduction"}}