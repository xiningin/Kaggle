{"cell_type":{"8258fcba":"code","25ba9eb0":"code","662106ad":"code","8ec82d3a":"code","0d8e40ee":"code","a30ce3e4":"code","d752b7c5":"code","42a2db74":"code","c2612d0f":"code","7dc30ce2":"code","e14de3b2":"code","3f26ce03":"code","5b3a486c":"code","405d97f3":"code","b53614b2":"code","74bdffea":"code","d995898b":"markdown","ab446e47":"markdown","788016d2":"markdown","f483dcdc":"markdown","3665dfe3":"markdown"},"source":{"8258fcba":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25ba9eb0":"import pandas as pd\n\npath = \"\/kaggle\/input\/digit-recognizer\/\"\ndataset = pd.read_csv(path+\"train.csv\")\n\n# Show the first 10 rows of the dataset\ndataset.head(3)\n\n# To check profile of the dataset\n#dataset.describe(include='all')  ","662106ad":"X = dataset.iloc[:,1:785]\nY = dataset.iloc[:,0]\n\n# X is an input data and Y is the labelled target.\nprint(\"Input\")\nprint(X)\nprint(\"Target\")\nprint(Y)","8ec82d3a":"# Preparing training and test dataset\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n\nprint(\"X_train shape\")\nprint(X_train.shape)\n\nprint(\"X_test shape\")\nprint(X_test.shape)\nX_test_ori = X_test.copy()\n\nprint(\"y_train shape\")\nprint(y_train.shape)\n\nprint(\"y_test shape\")\nprint(y_test.shape)\n\n\n#The input values need to be standardised (normalised)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nprint(X_train)\nprint(X_train.shape)\n\n\n# Just want to check the size of training and test dataset\n# N_total = len(X)\n# N_train = len(X_train)\n# N_test = len(X_test)\n# print(\"N_total \"+str(N_total))\n# print(\"Check the total value: \"+str(N_train)+\" + \"+str(N_test)+\" = \"+str(N_train + N_test))\n\nNc = len(dataset.columns)\nprint(\"Number of columns \"+str(Nc))\n\n# This step is required since we will have a multiclass output. Since, we have 10 possible output values, \n# the ConvNet output should be stored as matrix with 10 columns.  \n\nfrom keras.utils import to_categorical\ntrain_labels = to_categorical(y_train)\n\nprint(y_train)\nprint(train_labels)\n\ny_train = to_categorical(y_train)\n\nshowASingleTestedImage = False","0d8e40ee":"# Create ANN architecture. The input is 784 nodes and the output is 10 nodes. Therefore, the nodes between \n# input and output is around (784+10)\/2 = 397 ~ 400 nodes.\n\nfrom keras import optimizers\ninputSz = Nc-1\n\nmodel = Sequential()\n\nmodel.add(Dense(400, activation='relu', kernel_initializer='random_normal', input_dim=inputSz)) \nmodel.add(Dense(200, activation='relu', kernel_initializer='random_normal'))\nmodel.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\nmodel.add(Dense(10, activation='softmax', kernel_initializer='random_normal'))\nmodel.summary()\n\nmodel.compile(optimizer ='adam',loss='categorical_crossentropy', metrics =['accuracy'])\n","a30ce3e4":"# Starting to train the dataset\n\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\"best_model1.hdf5\", monitor='loss', verbose=1,\n    save_best_only=True, mode='auto', period=1)\n\nbatch_size=20 \nepochs= 30\n\n\n#Fitting the data to the training dataset\nhistory = model.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_split=0.2)\n\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","d752b7c5":"# Save the trained model and its weights\nsaveTheModel = False\n\nif (saveTheModel==True):\n    model_json = model.to_json()\n    with open(\"model1.json\", \"w\") as json_file:\n        json_file.write(model_json)\n\n    # serialize weights to HDF5\n    model.save_weights(\"weight_of_model1.h5\")\n    print(\"Saved model to disk\")\n","42a2db74":"N_total = len(X)\nprint(N_total)\nN_train = len(X_train)\nprint(N_train)\nN_test = len(X_test)\nprint(N_test)\n\nprint(\"N_total \"+str(N_total))\nprint(\"Check the total value: \"+str(N_train)+\" + \"+str(N_test)+\" = \"+str(N_train + N_test))\n\n\nprint(X_test.shape)\nprint(y_test.shape)\n\nprint(y_test.iloc[0])","c2612d0f":"X_test_lab = X_test.copy()\nX_test_lab = sc.fit_transform(X_test_lab)\nN = len(X_test_lab)\n\ny_lab = model.predict(X_test_lab)\n\n\ndef formatList(a):\n    N = len(a)\n    s = \"\"\n    for i in range(N):\n        vS = \"%2.5f\"%a[i]\n        s = s +\" \"+vS\n    return s\n\nclassIdx = []\n#CM = np.zeros((10,10),dtype =\"int\")\nfor i in range(N):\n    yItem = y_lab[i,:]\n    classIdx.append(np.argmax(yItem))\n    #print(\"[\"+str(i)+\"] --------- char: \"+str(y_test.iloc[i])+\" >>> \"+str(classIdx[i])+\" -----------\")   \n    #print(formatList(y[i,:]))\n\ny_pred = classIdx.copy()    \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(cm)\n\nclassName = []\nfor i in range(10):\n    className.append(str(i))\n\ncmap=plt.cm.Reds\nfig, ax = plt.subplots()\nim = ax.imshow(cm, interpolation='nearest', cmap=cmap)\nax.figure.colorbar(im, ax=ax)\n\nax.figure.set_size_inches(8,6,True)\n\nax.set(xticks=np.arange(cm.shape[1]),\n  yticks=np.arange(cm.shape[0]),\n  xticklabels=className, yticklabels=className,\n  title='',\n  ylabel='True character',\n  xlabel='Predicted character')\n\n# Rotate the tick labels and set their alignment.\n#plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n#  rotation_mode=\"anchor\")\n\nfmt = 'd'\nthresh = cm.max() \/ 2.\nfor i in range(cm.shape[0]):\n  for j in range(cm.shape[1]):\n    ax.text(j, i, format(cm[i, j], fmt),\n    ha=\"center\", va=\"center\",\n    color=\"white\" if cm[i, j] > thresh else \"black\")\n\nfig.tight_layout()\n\n\nprint(y_test.shape)\nprint(y_test.iloc[2])\n#plt.imshow(cm, cmap='binary')\n\n","7dc30ce2":"#Another statistical parameters:\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\n\nacc   = accuracy_score(y_test,y_pred)\nprec  = precision_score(y_test,y_pred, average =None)\nrecl  = recall_score(y_test,y_pred, average =None)\nf1_sc = f1_score(y_test,y_pred, average =None)\ncohkp = cohen_kappa_score(y_test,y_pred)\n#roc   = roc_auc_score(y_test,y_pred)\n\nprint(\"Accuracy          : %3.4f \"%(acc))\nprint(\"Average precision : %3.4f \"%(np.average(prec)))\nprint(\"Average recall    : %3.4f \"%(np.average(recl)))\nprint(\"Average F1-score  : %3.4f \"%(np.average(f1_sc)))\nprint(\"Cohen-Kappa score : %3.4f \"%(cohkp))\n\n","e14de3b2":"# Sample Images\nimport random as rd\n\n\ndef convListToImg(list1):\n    N = len(list1)\n    N2 = int(np.sqrt(N))\n    img = np.zeros((N2,N2), dtype =\"uint8\")\n    idx = 0\n    for i in range(N2):\n        for j in range(N2):\n            img[i,j] = list1[idx]\n            idx = idx + 1\n    return img\n\nidxList = []\nfor i in range(10):\n    idxList.append(rd.randrange(N_test))\n#print(idxList)\n\nix = 0\nfig, axs = plt.subplots(2,5,figsize=(13,6))  # Width: 12 Height: 6\nfor i in range(2):\n    for j in range(5):\n        ixx = idxList[ix]\n        list1 = X_test_ori.iloc[ixx,0:785]\n        img = convListToImg(list1)\n        axs[i,j].set_title(\"[\"+str(ixx)+\"] Label:\"+str(y_test.iloc[ixx])+\" >> \"+str(y_pred[ixx]))\n        axs[i,j].imshow(img, cmap=\"gray\")\n        ix = ix + 1\n\n","3f26ce03":"# Load test dataset from unused dataset in training stages\ntestDataset = pd.read_csv(path+\"test.csv\")\ntestDataset.head(10)","5b3a486c":"X_test= testDataset.iloc[:,0:785]\nX_test_ori = X_test.copy()\nX_test = sc.fit_transform(X_test)\nN = len(X_test)\n\n\n\ny_pred_noLab = model.predict(X_test)\n\ndef formatList(a):\n    N = len(a)\n    s = \"\"\n    for i in range(N):\n        vS = \"%2.5f\"%a[i]\n        s = s +\" \"+vS\n    return s\n\nfinal_detected_class = []\nfor i in range(N):\n    yItem = y_pred_noLab[i,:]\n    final_detected_class.append(np.argmax(yItem))\n    print(\"[\"+str(i)+\"] --------- char: \"+str(final_detected_class[i])+\"------------\")    \n    print(formatList(y_pred_noLab[i,:]))\n\n","405d97f3":"# Show a single image from test dataset\nimport matplotlib.pyplot as plt\n\n\nif (showASingleTestedImage==True):\n\n    print(\"Give the index! Max: \"+str(N))\n    testIdx = int(input())\n\n\n    list1 = X_test_ori.iloc[testIdx,0:785]\n    #print(list1)\n    img = convListToImg(list1)\n\n    plt.imshow(img,cmap=\"gray\")\n    plt.title(\"Detected as \"+str(final_detected_class[testIdx]))\n    plt.show()\n","b53614b2":"import random as rd\nidxList = []\nfor i in range(12):\n    idxList.append(rd.randrange(N))\nprint(idxList)\n\nix = 0\nfig, axs = plt.subplots(2,5,figsize=(13,6))\nfor i in range(2):\n    for j in range(5):\n        list1 = X_test_ori.iloc[idxList[ix],0:785]\n        img = convListToImg(list1)\n        axs[i,j].set_title(\"[\"+str(idxList[ix])+\"] >> \"+str(final_detected_class[idxList[ix]]))\n        axs[i,j].imshow(img, cmap=\"gray\")\n        ix = ix + 1\n\n","74bdffea":"# This block is used to store the prediction result as CSV file\n\ncsvFileName = \"sample_submission.csv\"\ncsvFile = open(csvFileName,\"w+\")\nN_detClass = len(final_detected_class)\nheaderStr = \"ImageId,Label\"\ncsvFile.write(headerStr+\"\\n\")\nfor i in range(N_detClass):\n    strVal = \"\"\n    strVal = str(i)\n    strVal = strVal +\",\"+ str(final_detected_class[i]) \n    csvFile.write(strVal+\"\\n\")\n\ncsvFile.close()\n\nprint(\"File \"+csvFileName+\" has been saved\")\n\n","d995898b":"**Step 3**. The dataset consists of 785 columns. The 1st column is the character class. There are 10 characters that will be recognised. The value could be from \"0\" to \"9\". The following 784 columns, the 2nd to 784th, are obtained by flattening a sample image with size 28 x 28 pixels.\n\n**Langkah 3**. The dataset terdiri atas 785 kolom. Kolom yang pertama melambangkan kelas atau jenis karakter yang akan dikenali. Terdapat 10 jenis karakter berbeda yang akan dikenali. Nilai jenis karakter berupa angka antara \"0\" sampai dengan \"9\". Kolom kedua sampai dengan kolom-kolom selanjutnya, 784 kolom, diperoleh dengan melakukan flattening terhadap citra berukuran 28 x 28 piksel. Variabel X menyimpan data piksel citra karakter sedangkan Y untuk mewakili jenis karakter.","ab446e47":">  # **Preparing result for Submission**","788016d2":"# We will test the model by using labelled dataset","f483dcdc":"**Step 1**. Load dataset that will be used in the analysis. The MNIST characters (0 to 9) are stored as CSV file. There are two CSV files available. The first file, train.csv, is used to train the model whereas the second file, test.csv, is required in model testing. A number character is represented by a single record in the CSV file. \n\n**Langkah 1**. Load dataset yang akan digunakan. Dataset karakter angka MNIST (angka 0 s.d 9) disimpan dalam bentuk file CSV. Terdapat dua file CSV, file pertama *train.csv* merupakan data untuk proses training dan *test.csv* digunakan untuk menguji model hasil training. Satu baris record di file *train.csv* melambangkan satu jenis angka.","3665dfe3":"**Langkah 2.** Data yang tersimpan di file CSV akan dibaca dengan menggunakan fungsi yang tersedia di library Pandas. Variabel *path* diperlukan untuk menyimpan path direktori lokasi penyimpanan file CSV. Dengan menggunakan fungsi *read_csv* maka data yang ada di dalam file *train.csv* akan tersimpan di variabel *dataset*. Jika kita ingin menampilkan tiga baris data teratas dari train.csv maka berikan perintah *dataset.head(3)*. Perintah untuk menjelaskan profil data secara umum adalah dengan menuliskan *dataset.describe(include='all')*"}}