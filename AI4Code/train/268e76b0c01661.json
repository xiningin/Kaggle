{"cell_type":{"c99caffd":"code","f4dea6cb":"code","7d1a20e5":"code","be0fcb57":"code","c42b86a9":"code","1e6c8356":"code","c9ecf1f3":"code","5c8df038":"code","f24b189e":"code","180297ad":"code","61d14b8a":"code","3c8cbd31":"code","d1629dde":"code","b2c18fbe":"code","1569721b":"code","ea13d404":"code","6f4e14f5":"code","d579374f":"code","37c77d24":"code","bbbbfa72":"markdown","d7b7c0f6":"markdown","cc3c6316":"markdown","4397f478":"markdown","791d5ee6":"markdown","e4f0ad82":"markdown","6f69527d":"markdown","71dca54d":"markdown","23bd42b2":"markdown","eeb342ab":"markdown","0f4a816f":"markdown","161d94c8":"markdown"},"source":{"c99caffd":"# load necessary modules\n%pylab inline\nimport pandas as pd","f4dea6cb":"# Let's load the iris dataset\niris = pd.read_csv('..\/input\/iris\/Iris.csv', index_col= 'Id')\niris.head()","7d1a20e5":"print(f'We have {iris.shape[0]} observations and {iris.shape[1]} features')","be0fcb57":"X = iris.iloc[:, 0:4].values # take the first four features\ny = iris.Species.values # b","c42b86a9":"# define custom colors\nmycolors = {'Iris-setosa': 'C0', 'Iris-versicolor': 'C1', 'Iris-virginica': 'C3'}\ncolors = iris.Species.map(mycolors)","1e6c8356":"# There is more variance in the SepalLenght than in the PetalLenght\nnp.var(iris.SepalLengthCm), np.var(iris.PetalLengthCm)","c9ecf1f3":"# The covariance between \nnp.cov(iris.SepalLengthCm, iris.PetalLengthCm)","5c8df038":"# The covariance between SepalLength and PetalWidth\nnp.cov(iris.SepalLengthCm, iris.PetalWidthCm)\n","f24b189e":"def covariance_matrix(X):\n    \"\"\"\n    Computes the covariance matrix of X as\n    cov(X) = (X - X0).T * (X - X0) \/ (n-1)\n    \n    Before we compute the Z-score of the matrix\n    Arguments\n    ---------\n    X (NumPy array)\n    \n    \"\"\"\n    n = X.shape[0]\n    Z = np.divide(X - X.mean(axis=0), X.std(axis=0))\n    \n    return np.matmul(Z.T, Z)\/(n-1)","180297ad":"S = covariance_matrix(X)\nS","61d14b8a":"# center the data around the mean of the observation\nZ_score = np.divide(X- X.mean(axis=0), X.std(axis=0)) # substract mean and divide by variance\nZ_score.shape # we keep the same, only remove mean\nS = np.cov(Z_score, rowvar=False, bias=False) # col variable true\nS","3c8cbd31":"# compute eigenvalues and eigenvectors form the covariance matrix.\neVal, W = np.linalg.eig(S) # returns eigenvalues, eigenvectors","d1629dde":"# calculate the percentage of max variance from eigenValues\nvar = eVal\/np.cumsum(eVal).max()\nvar[0]*100, var[1]*100","b2c18fbe":"T = np.matmul(Z_score, W[:,0:2]) # matrix multiplication with the two first components\nplt.scatter(T[:,0], T[:,1], c= colors, s = 14)\nplt.xlabel(f'PC$_1$ = {var[0]*100:2.1f} %', fontsize = 14);\nplt.ylabel(f'PC$_2$ = {var[1]*100:2.1f} %', fontsize = 14);","1569721b":"# u matrix of the left singular vectors\n# s vectors with the singular values\n# vh matrix of the right singular vectors\nU, s, Vt = np.linalg.svd(Z_score) # perform SVD\nW2 = Vt.T[:,:2] # get two principal components\n\nT = Z_score.dot(W2) # transform data in the new dimension (as np.matmul)\n\nplt.scatter(T[:,0], T[:,1], s = 14, color=colors);","ea13d404":"# load the data againg (just to remember)\niris = pd.read_csv('..\/input\/iris\/Iris.csv', index_col= 'Id')\nX = iris.iloc[:, 0:4].values # take the first four features","6f4e14f5":"# Center the data around the mean (perform z-transform)\nfrom sklearn.preprocessing import StandardScaler\nZ = StandardScaler().fit_transform(X)","d579374f":"# encode different types of iris as integers\nfrom sklearn.preprocessing import LabelEncoder\nspecies = LabelEncoder().fit_transform(iris.Species)\n","37c77d24":"from sklearn.decomposition import PCA\nmyPCA = PCA(n_components = 2, svd_solver = 'full')\nT = myPCA.fit_transform(Z)\n\nvar = myPCA.explained_variance_ratio_*100\nplt.scatter(T[:,0], T[:,1], c = species, s = 14)\nplt.xlabel(f'PC$_1$ = {var[0]:2.1f} %', fontsize = 14);\nplt.ylabel(f'PC$_2$ = {var[1]:2.1f} %', fontsize = 14);","bbbbfa72":"We can get the proportion of the variance explained with the eigenValues.","d7b7c0f6":"Let's assume now that our random variables are SepalLengthCm and PetalLengthCm. Let's calculate their variances.\n","cc3c6316":"# Princial Components Analysis with simple math\n\nThis notebook explain the basis and mathematical foundations of dimensionality reduction with principal components analysis (PCA). It also uses the iris dataset as example. \n","4397f478":"<A id=\"subsection1.2\"><\/A>\n## 1.2 Variance and covariance\n\n<P>PCA look for the linear combinations that show as much variation as possible. Variables that have little variation doesn't give much information about the samples (e.g., observations in a data matrix that have similar features). What PCA does is to reduce the number of dimensions to those with the largest variance.<\/P>\n\n### Variance\n\nThe variance ($\\sigma^2$) is the dispersion of a random variable. For a random variable $a$, the variance is given by:\n\n\n\\begin{equation}\n\\sigma^2_a = \\frac{1}{n-1} \\sum_{i=1}^n (a_i - \\bar{a})^2,\n\\end{equation}\n\nwhere $a_i$ is the i-th measurement (feature) of the only variable, and $\\bar{a}$ is the mean of n observations.\n\n","791d5ee6":"<A id=\"section2\"><\/A>\n# 2. Matrix decomposition\n\n**Matrix decomposition - or factorization - is the decomposition of a matrix into the product of other matrices.**\n\n<A id=\"subsection2.1\"><\/A>\n## 2.1 Eigendecomposition\n\n<P>Because the covariance matrix is a square symmetric matrix, it can be diagonalised choosing a new orthogonal coordinate system. We make it diagonal to have the correlation between variables zero. The new orthogonal coordinate system is given by the eigenvectors, and the corresponding eigenvectors will be located in the diagonal. An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.<\/P>\n\n\\begin{equation}\n\\text{A}\\vec{v}= \\lambda \\vec{v} \n\\end{equation}","e4f0ad82":"<a id=\"section1\"><\/a>\n# 1. Introduction\n\n\n<P>Principal Components Analysis (PCA) is a technique to reduce the number of dimensions of a dataset. It is also used for visualization in two or three dimensions. Classic examples of dimensionality reduction are  data compression in computer vision or the elimination of noise in electrical engineering. The resulted dimensions are the *linear combinations* of essential features encounted in our dataset. <\/P>\n\n<A id=\"subsection1.1\"><\/A>\n## 1.1 Multivariate data\n\n<P>Large datasets contain not only a large number of samples (observations) but also several measurements (also called features,  dimensions, or variables). What we want to achieve with PCA is to reduce the **dimensions** to a combination of the essentials that explains most of the data. How much of this data we want to represent is given by the variance. Indeed, is the co-variance between the dimensions what is computed to be reduced to the essentials.<\/P>\n\n\\begin{equation}\n\\text{X} \\in \\mathbb{R}^{n \\times m}= \\begin{bmatrix} a_1\\cdots a_{1m}\\\\a_2  \\cdots a_{2m} \\\\\\vdots \\\\ a_n \\cdots a_{nm}\\end{bmatrix}\n\\end{equation}\n\n\n<P>Here, we have $n$ observations (in rows), and $m$ dimensions (in cols) in a $n \\times m$ matrix. PCA tries to find relationships between measurements that are strictly linear thereby reducing $m$. <\/P>","6f69527d":"<A id=\"subsection2.2\"><\/A>\n## 2.2 Single value decomponsition (SVD)\n\nThe SVD allows us to describe a data matrix $\\text{X}$ as a matrix-vector product in a three-step process:\n\n\\begin{equation}\n \\text{X} = \\text{U}\\text{S} \\text{V}^T,\n\\end{equation}\n\nWhere $\\text{U}$ is the matrix of left singular vectors, $\\text{S}$ is a diagonal matrix of singular values and $\\text{V}^T$ the matrix of right singular vectors.  The columns of $\\text{U}$ are called the left-singular vectors of $\\text{X}$ while the columns of $\\text{V}$ are the right singular vectors of $\\text{X}$. ","71dca54d":"# Table of Contents\n\n* [1. Introduction](#section1)\n    - [1.1 Multivariate data](#subsection1.1)\n    - [1.2 Variance and covariance](#subsection1.2)\n* [2. Matrix decomposition](#section2)\n    - [2.1 Eigendecomposition](#subsection2.1)\n    - [2.2 Singular value decomposition (SVD)](#subsection2.2)\n* [3. Conclusion](#section3)","23bd42b2":"If we now project the covariance matrix in the new axis, we can see the reduction of the dimensions. ","eeb342ab":"If we center the data around the mean of the features, we can rewrite the expression of the covariance in matrix form:\n\\begin{equation}\n\\Sigma = \\frac{X^T X}{n-1}\n\\end{equation}\n\nWe can also divide every feature by the standard deviation to look for larger covariances","0f4a816f":"### Covariance\nIf we have several variables we need to compute the covariance. The covariance is the tendency of two variables (features) to vary together. It can be positive, negative, or zero. Besides, large values represent a larger dependency, whereas smaller variations are noise.  \n\nLets assume now we have two variables: $x =  a_1 \\cdots a_{1n}$ and $y = a_2 \\cdots a_{2n}.$ - these are two columns of the matrix $\\text{X}$ corresponding to two variables (features or measurements) from the dataset. Their covariance is given by:\n\n\\begin{equation}\n\\operatorname{cov}(x, y) = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\end{equation}\n\nFor m-features, we can express it in a $m \\times m$ covariance matrix:\n\n\\begin{equation}\n\\Sigma = \\begin{bmatrix} \\text{var}(x) & \\text{cov}(x,y) \\\\ \\text{cov}(y,x) &\\text{var(y)} \\end{bmatrix}\n\\end{equation}","161d94c8":"<A id=\"section3\"><\/A>\n# 3. Conclusion\n\nSingular-value decomponsition is recommended is because is faster to compute than the covariance matrix. That's the algorimth behind PCA. In fact, if the dimensionality of the data is too high, there is no way you can even store the covariance matrix in memory. For a manageable size of dimensions, eigendecomposition of the covariance matrix is still possible.\n\nWe will describe bellow the way to do it with [sklearn.decomposition](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n"}}