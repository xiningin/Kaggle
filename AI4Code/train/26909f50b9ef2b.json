{"cell_type":{"3d8019ad":"code","b92c5e11":"code","a6a789f2":"code","b75eca28":"code","2d5c19e6":"code","21c4f37b":"code","77ad4307":"code","21f6b67b":"code","d5167c53":"code","8312ccc9":"code","8405ac4b":"code","b3687f0d":"code","c4a8dc54":"code","c59cc221":"code","7a2ef9c6":"code","edc549ea":"code","ce0d5561":"code","ee2005c7":"code","d6217fa5":"code","9fc53f57":"code","74423c26":"code","17c4f94f":"code","3fa8deb1":"code","847290dd":"code","125edd3e":"code","55733cd6":"code","d5237673":"code","b18fc925":"code","98a96ef6":"code","344ad888":"code","4f4c16d3":"code","f4c105c2":"markdown","be428aa3":"markdown","ffb98ca8":"markdown","3256e3ee":"markdown","fe90f65a":"markdown","8aee8ff6":"markdown","7ebe6d27":"markdown","d28f2117":"markdown","8d1347db":"markdown","a97aabce":"markdown","2123231c":"markdown","a686ba8b":"markdown","7bedc6eb":"markdown","3fc2f7cc":"markdown","635151bd":"markdown","27f3456a":"markdown"},"source":{"3d8019ad":"import pandas as pd\nimport os","b92c5e11":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv')\nprint(df.shape)\ndf.sample(10)","a6a789f2":"print(df.Species.unique())\nprint(df.info())","b75eca28":"print(df.describe())","2d5c19e6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline","21c4f37b":"for specie in df.Species.unique():\n    df_spe=df.query('Species==@specie')\n    print(specie)\n    for col in df_spe.columns:\n        if(col =='Species'):continue\n        df_spe.boxplot(col)\n        plt.show()","77ad4307":"df.query('Species==\"Roach\" & (Weight ==0 | Weight>350)')","21f6b67b":"df= df.drop([54])\ndf.query('Species ==\"Roach\"').describe().T","d5167c53":"df.iloc[40,1]=df.query('Species ==\"Roach\"').describe().T['25%'].Weight","8312ccc9":"df.query('Species ==\"Roach\"').describe().T","8405ac4b":"for col in df.columns:\n    if(col =='Species'):continue\n    df.boxplot(col)\n    plt.show()","b3687f0d":"df.query('Weight> 1500')","c4a8dc54":"df= df.query('Weight<= 1500')","c59cc221":"df.query('Weight> 1500')","7a2ef9c6":"sb.pairplot(df, kind='scatter', hue='Species');","edc549ea":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","ce0d5561":"# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'species'. \ndf['Species']= label_encoder.fit_transform(df['Species']) \n\ndf['Species'].unique()","ee2005c7":"X=df.drop(['Weight'] , axis=1, inplace=False)\nX.head()","d6217fa5":"y= df[df.columns[1:2]]","9fc53f57":"lg = LinearRegression()\nlstSeed=[]\nlstRMSQ=[]\nlstRSq=[]\nfor seed in range(0,150,10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n    lg.fit(X_train, y_train) #training the algorithm\n    pred = lg.predict(X_test)\n    root_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,pred))\n    r_sq = metrics.r2_score(y_test,pred)\n    lstRSq.append(r_sq)\n    lstSeed.append(seed)\n    lstRMSQ.append(root_mean_sq)","74423c26":"df_metric=pd.DataFrame({\n    'Seed': lstSeed, \n    'RMSQ': lstRMSQ,\n    'RSQ': lstRSq})\ndf_metric.head()","17c4f94f":"ax=df_metric.plot('Seed', 'RMSQ',legend=False)\nax2 = ax.twinx()\ndf_metric.plot('Seed', 'RSQ', ax=ax2,color=\"r\",legend=False)\nax.figure.legend()\nplt.show()","3fa8deb1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\nlg.fit(X_train, y_train) #training the algorithm\npred = lg.predict(X_test)\nprint('root mean sq:',np.sqrt(metrics.mean_squared_error(y_test,pred)))\nprint('r squared:',metrics.r2_score(y_test,pred))","847290dd":"X=df.drop(['Weight'] , axis=1, inplace=False)\ny= df[df.columns[1:2]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\nX_train_inter = np.ones((X_train.shape[0],1))\nX_train = np.concatenate((X_train_inter, X_train), axis = 1)\n\nX_test_inter = np.ones((X_test.shape[0],1))\nX_test = np.concatenate((X_test_inter, X_test), axis = 1)\nprint(X_train.shape)\nprint(X_test.shape)","125edd3e":"def computeCost(X,y,theta):\n    #number of training examples\n    m= len(y)\n    hypothesis= X.dot(theta)\n    #Take a summation of the squared values\n    delta=np.sum(np.square(hypothesis-y))\n    J=(1\/(2*m))*delta\n    return J\n\ndef gradientDescent(X, y, theta, alpha, num_iters):\n    #number of training examples\n    m, n = np.shape(X)\n    x_t = X.transpose()\n    J_history = np.zeros((num_iters, 1))\n    for i in range(num_iters):\n        hypothesis = np.dot(X,theta)-y\n        gradient = np.dot(x_t, hypothesis) \/ m\n        #update the theta\n        theta = theta- alpha*gradient\n        J_history[i]=np.sum(hypothesis**2) \/ (2*m)\n    return theta,J_history\n\ndef predict(x_test,theta):\n    n = len(x_test)\n    predicted_vals=[]\n    for i in range(0,n):\n        predicted_vals.append(np.matmul(theta.T,x_test[i,:]))\n    return predicted_vals\n\ndef runEpoch(X,y,theta,alpha,iterations,epochs):\n    dicGradient={}\n    dicRMSQ={}\n    dicRSQ={}\n    dicJ_Hist={}\n    J_hist=[]\n    X_t_act, X_valid, y_t_act, y_valid = train_test_split(X, y, test_size=0.2, random_state=10)\n    for epoch in range(epochs):\n        print('Running Epoch {}'.format(epoch))\n        theta,J_History=gradientDescent(X_t_act,y_t_act,theta,alpha,iterations)\n        dicGradient[epoch]=(theta,J_History)\n        J_hist.extend(J_History)\n        pred_vals=predict(X_valid,theta)\n        root_mean_sq = np.sqrt(metrics.mean_squared_error(y_valid,pred_vals))\n        r_sq = metrics.r2_score(y_valid,pred_vals)\n        dicRMSQ[epoch]=root_mean_sq\n        print('Epoch {0}: RMSQ {1}'.format(epoch,root_mean_sq))\n        dicRSQ[epoch]=r_sq\n    key_min = min(dicRMSQ.keys(), key=(lambda k: dicRMSQ[k]))\n    return dicGradient[key_min][0],J_hist","55733cd6":"n=X_train.shape[1]\ntheta=np.zeros((n, 1))\ntheta,J_History=runEpoch(X_train,y_train,theta,0.00065,4000,25)\nprint(theta)\nplt.plot(J_History);\nplt.show();","d5237673":"pred_vals=predict(X_test,theta)\npreds=[]\nfor pred in pred_vals:\n    preds.append(abs(pred[0]))","b18fc925":"root_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,preds))\nr_sq = metrics.r2_score(y_test,preds)\nprint('root mean sq:',root_mean_sq)\nprint('r squared:',r_sq)","98a96ef6":"def normalEquation(X,y):\n    x_trans=X.T\n    inv=np.linalg.pinv(np.dot(x_trans,X))\n    theta=np.dot(np.dot(inv,x_trans),y)\n    return theta","344ad888":"theta_ne= normalEquation(X_train,y_train)\nprint(theta_ne)","4f4c16d3":"pred_vals=predict(X_test,theta_ne)\npreds=[]\nfor pred in pred_vals:\n    preds.append(abs(pred[0]))\nroot_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,preds))\nr_sq = metrics.r2_score(y_test,preds)\nprint('root mean sq:',root_mean_sq)\nprint('r squared:',r_sq)","f4c105c2":"Looking at the plot we can choose the seed value to be 10","be428aa3":"So, we will drop the record with Index 54, since in this record all the parameters are outliers.","ffb98ca8":"In context of this data set we can see that the gradient descent implementation gives a bit better result than Scikit library or using Normal equation, this may be attributed to tuning parameters available with gradient descent.\nBut gradient descent has drawback with respect to Normal Equation, that it has to go through lot more iterations (time consuming) and we need to choose learning rate.","3256e3ee":"Looking at the Specie wise box plots, we observe that for \"Roach\" specie, there are two records which are having anamoly.\n    1. In one record the Weight is zero.\n    2. In second record, the Weight, Length1, Length2, Length3, Height and Width measurements are all outliers.","fe90f65a":"Using the fish market data set we will try to create a model to estimate fish weight, this model will be based on multiple linear regression.\n\nWe will be trying out three approaches:\n    1. Using the Scikit library.\n    2. Using the gradient descent implementation which gives us a way to fine tune hyper parameters like learning rate.\n    3. Using Normal equation implementation, this equation is helpful in a way that is does not require us to go through iterations and chossing learning rate as is the case with gradient descent.","8aee8ff6":"<a id='intro'><\/a>\n### Introduction","7ebe6d27":"<a id='eda'><\/a>\n### Exploratory Data Analysis","d28f2117":"<a id='conclusions'><\/a>\n### Conclusions","8d1347db":"<a id='lg-ne'><\/a>\n### Multiple Linear Regression Using Normal Equation\n\nNormal equation, will for some linear regression (usually when features are less than aprroximately 10000) problems gives us a much better way to solve the optimal value of $\\theta$\n\n**Normal Equation**\n\n$\\theta = (X^{T}X)^{-1}X^{T}y$","a97aabce":"Now we will look at the attributes in a consolidated way.","2123231c":"<a id='lg-skt'><\/a>\n### Multiple Linear Regression Using Scikit Learn Library","a686ba8b":"Here we observe that there are three outliers in 'Weight' attribute.","7bedc6eb":"Species -->\tspecies name of fish\n\nWeight -->\tweight of fish in Gram g\n\nLength1 --> vertical length in cm\n\nLength2 -->\tdiagonal length in cm\n\nLength3 -->\tcross length in cm\n\nHeight -->\theight in cm\n\nWidth -->\tdiagonal width in cm","3fc2f7cc":"<a id='lg-grd'><\/a>\n### Multiple Linear Regression Using Gradient Descent Implementation\n\nIn this section we will explicitly implement gradient descent and cost function, we will tune various parameters like learning rate, iterations etc.","635151bd":"For the record where the weight is zero, looking at this record we see that the Length1,Length2,Length3 and width for this record lies around the 1st Quartile, and since this is just one record, we chose to replace the weight 0 with the 1st quartile value of weight.","27f3456a":"# Estimating Fish Weight- Using Multiple Linear Regression\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#eda\">Exploratory Data Analysis<\/a><\/li>\n<li><a href=\"#lg-skt\">Multiple Linear Regression Using Scikit Learn Library<\/a><\/li>\n<li><a href=\"#lg-grd\">Multiple Linear Regression Using Gradient Descent Implementation<\/a><\/li>\n<li><a href=\"#lg-ne\">Multiple Linear Regression Using Normal Equation<\/a><\/li>\n<li><a href=\"#conclusions\">Conclusions<\/a><\/li>\n<\/ul>"}}