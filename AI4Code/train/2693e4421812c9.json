{"cell_type":{"517d73ae":"code","8264b5f1":"code","159d599f":"code","3081e0b5":"code","1de025fd":"code","8f9e9e9f":"code","79b73d32":"code","036ec7d5":"code","e656aadc":"code","0bc81add":"markdown","f11c99d7":"markdown","ac29c8de":"markdown","1ad16446":"markdown","79f29500":"markdown","20b5c5b3":"markdown","e7be0262":"markdown","237877e2":"markdown","4deedd62":"markdown","465f3d5e":"markdown","a858aa82":"markdown","ff61cef9":"markdown"},"source":{"517d73ae":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering_new.ex5 import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n\ndf = pd.read_csv(\"..\/input\/fe-course-data\/ames.csv\")","8264b5f1":"features = [\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\",]\n\nprint(\"Correlation with SalePrice:\\n\")\nprint(df[features].corrwith(df['SalePrice']))","159d599f":"X = df.copy()\ny = X.pop(\"SalePrice\")\nX = X.loc[:, features]\n\n# `apply_pca`, defined above, reproduces the code from the tutorial\npca, X_pca, loadings = apply_pca(X)\nprint(loadings)","3081e0b5":"# View the solution (Run this cell to receive credit!)\nq_1.check()","1de025fd":"X = df.copy()\ny = X.pop(\"SalePrice\")\n\nX[\"Feature1\"] = X['GrLivArea'] + X['TotalBsmtSF']\nX[\"Feature2\"] = X['YearRemodAdd'] * X['TotalBsmtSF']\n\nscore = score_dataset(X, y)\nprint(f\"Your score: {score:.5f} RMSLE\")\n\n\n# Check your answer\nq_2.check()","8f9e9e9f":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","79b73d32":"sns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=2,\n);","036ec7d5":"# You can change PC1 to PC2, PC3, or PC4\ncomponent = \"PC1\"\n\nidx = X_pca[component].sort_values(ascending=False).index\ndf.loc[idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features]","e656aadc":"# View the solution (Run this cell to receive credit!)\nq_3.check()","0bc81add":"**This notebook is an exercise in the [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering) course.  You can reference the tutorial at [this link](https:\/\/www.kaggle.com\/ryanholbrook\/principal-component-analysis).**\n\n---\n","f11c99d7":"# 3) Outlier Detection\n\nDo you notice any patterns in the extreme values? Does it seem like the outliers are coming from some special subset of the data?\n\nAfter you've thought about your answer, run the next cell for the solution and some discussion.","ac29c8de":"-------------------------------------------------------------------------------\n\nYour goal in this question is to use the results of PCA to discover one or more new features that improve the performance of your model. One option is to create features inspired by the loadings, like we did in the tutorial. Another option is to use the components themselves as features (that is, add one or more columns of `X_pca` to `X`).\n\n# 2) Create New Features\n\nAdd one or more new features to the dataset `X`. For a correct solution, get a validation score below 0.140 RMSLE. (If you get stuck, feel free to use the `hint` below!)","1ad16446":"As you can see, in each of the components there are several points lying at the extreme ends of the distributions -- outliers, that is.\n\nNow run the next cell to see those houses that sit at the extremes of a component:","79f29500":"We'll rely on PCA to untangle the correlational structure of these features and suggest relationships that might be usefully modeled with new features.\n\nRun this cell to apply PCA and extract the loadings.","20b5c5b3":"# Keep Going #\n\n[**Apply target encoding**](https:\/\/www.kaggle.com\/ryanholbrook\/target-encoding) to give a boost to categorical features.","e7be0262":"# 1) Interpret Component Loadings\n\nLook at the loadings for components `PC1` and `PC3`. Can you think of a description of what kind of contrast each component has captured? After you've thought about it, run the next cell for a solution.","237877e2":"# Introduction #\n\nIn this exercise, you'll work through several applications of PCA to the [*Ames*](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) dataset.","4deedd62":"Run this cell to set everything up!","465f3d5e":"Let's choose a few features that are highly correlated with our target, `SalePrice`.\n","a858aa82":"-------------------------------------------------------------------------------\n\nThe next question explores a way you can use PCA to detect outliers in the dataset (meaning, data points that are unusually extreme in some way). Outliers can have a detrimental effect on model performance, so it's good to be aware of them in case you need to take corrective action. PCA in particular can show you anomalous *variation* which might not be apparent from the original features: neither small houses nor houses with large basements are unusual, but it is unusual for small houses to have large basements. That's the kind of thing a principal component can show you.\n\nRun the next cell to show distribution plots for each of the principal components you created above.","ff61cef9":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/221677) to chat with other Learners.*"}}