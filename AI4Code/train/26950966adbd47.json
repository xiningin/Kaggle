{"cell_type":{"7901ff92":"code","a7c883aa":"code","75274cc8":"code","ca346bc3":"code","491f60f0":"code","22810865":"code","530cdab9":"code","a5700225":"code","af5ab81c":"code","3ba29266":"code","57dd4bda":"code","f07e8eef":"code","08685be7":"code","96ff3e47":"code","72aea1e4":"code","4067201d":"code","1f46138e":"code","c83c27c3":"code","42f93bd0":"code","d8880fb8":"code","a0d87bf2":"code","ecc440bb":"code","50dfc2e7":"code","63649da4":"code","510af296":"code","728f362b":"markdown","4b992642":"markdown","627d4c7b":"markdown","2ed70b29":"markdown","a60dd561":"markdown","970c3233":"markdown","979e7158":"markdown","5be5ea24":"markdown"},"source":{"7901ff92":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nimport torch\nimport pandas as pd","a7c883aa":"def encode_file(tokenizer, data_path, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n    \"\"\"\n    Returns list[torch.tensor] of tokenized outputs from the input file separated per line\n    \"\"\"\n    examples = []\n    with open(data_path, \"r\") as f:\n        for text in f.readlines():\n            tokenized = tokenizer.batch_encode_plus(\n                [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n            )\n            # We keep dimension 0 as a singleton since `model.generate` requires dimensionality of (BS x SL)\n            examples.append(tokenized['input_ids']) # 1 x SL\n    return examples\n\ndef get_span_from_ids(input_ids, t5):\n    whole_input_str = tokenizer.decode(input_ids.squeeze())\n    input_str = whole_input_str.split('context: ')[-1]\n    question_str = whole_input_str.split('context: ')[0]\n    \n    # Return whole input string if neutral\n    if \"Which section was neutral?\" in whole_input_str:\n        #print('Neutral found! Returning input string ...')\n        return input_str.strip()\n    #print(input_str)\n    \n    # Predict\n    generated_ids = t5.generate(\n        input_ids=input_ids,\n        num_beams=1,\n        max_length=80,\n        repetition_penalty=2.5\n    ).squeeze()\n    predicted_span = tokenizer.decode(generated_ids)\n    # Make sure that the predicted span only has words contained in the context input\n    input_str_list = input_str.split()\n    predicted_span_list = predicted_span.split()\n    predicted_span_filtered = \" \".join([s for s in predicted_span_list if s in input_str_list])\n    return predicted_span_filtered\n\ndef process_span(pred_span, input_ids):\n    whole_input_str = tokenizer.decode(input_ids.squeeze())\n    input_str = whole_input_str.split('context:')[-1].strip()\n    question_str = whole_input_str.split('context:')[0].strip()\n    \n    if \"question: neutral\" in whole_input_str:\n        #print('Neutral found! Returning input string ...')\n        final_span = input_str\n    else:\n        input_str_list = input_str.split()\n        predicted_span_list = pred_span.split()\n        predicted_span_filtered = \" \".join([s for s in predicted_span_list if s in input_str_list])\n        # Simple heuristic given that blank answers are typically for short contexts\n        if predicted_span_filtered != '':\n            final_span = predicted_span_filtered\n        else:\n            final_span = input_str\n\n    return final_span.replace(' \u2047 ', '`').replace('\"', '')\n\ndef get_span_from_ids_batch(input_ids, t5):\n    \"\"\"\n    Returns batch of predicted spans (str)\n    \"\"\"\n    generated_ids = t5.generate(\n        input_ids=input_ids,\n        num_beams=4,\n        max_length=80,\n        length_penalty=2,\n        early_stopping=True,\n        #repetition_penalty=2.5,\n    )\n    predicted_spans = [tokenizer.decode(ids) for ids in generated_ids]\n    return predicted_spans\n\ndef post_process(selected):\n    return \" \".join(set(selected.lower().split()))","75274cc8":"test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')#.iloc[:200]\nprocessed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\nprocessed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n\nwith open('..\/working\/test.source', 'w') as f:\n    f.write(processed_input_str_test)\n","ca346bc3":"!head -20 ..\/working\/test.source","491f60f0":"tokenizer = T5Tokenizer.from_pretrained('..\/input\/t5-qa-training-short-question-pytorch\/')\nt5 = T5ForConditionalGeneration.from_pretrained('..\/input\/t5-5-epochs-sentiment-extraction\/')","22810865":"# Note we don't do any padding so no sequence length constraint is applied\ntest_input_ids = encode_file(tokenizer, '..\/working\/test.source', 80, pad_to_max_length=True, return_tensors='pt')","530cdab9":"len(test_input_ids)","a5700225":"# Checking max len\n#lens = [len(test_input_ids[i].squeeze()) for i in range(len(test_input_ids))]\n#max(lens)","af5ab81c":"test_input_ids[6]","3ba29266":"test_input_ids[6].size()","57dd4bda":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","f07e8eef":"input_ids_tensor = torch.cat(test_input_ids).to(device)\ninput_dataset = TensorDataset(input_ids_tensor)\ninput_dataloader = DataLoader(\n    input_dataset,\n    batch_size=16,\n)\ninput_dataloader = iter(input_dataloader)","08685be7":"#generated_ids = t5.generate(\n#    input_ids=next(input_dataloader)[0],\n#    num_beams=1,\n#    max_length=80,\n#    #repetition_penalty=2.5\n#)","96ff3e47":"#tokenizer.decode(generated_ids[2])","72aea1e4":"t5.to(device)\nt5.eval()","4067201d":"for param in t5.parameters():\n    param.requires_grad = False","1f46138e":"test_preds = []\nfor ex in tqdm(input_dataloader):\n    test_preds += get_span_from_ids_batch(ex[0], t5)","c83c27c3":"# Final processing\ntest_preds = [process_span(s, ids) for s, ids in zip(test_preds, test_input_ids)]","42f93bd0":"test_preds","d8880fb8":"#tokenizer.decode(tokenizer.encode('Cramps . . .'))","a0d87bf2":"len(test_preds)","ecc440bb":"sub = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nsub.shape","50dfc2e7":"sub['selected_text'] = test_preds\nsub.selected_text = sub.selected_text.map(post_process)","63649da4":"sub.head()","510af296":"sub.to_csv('submission.csv', index=False)","728f362b":"Save predictions","4b992642":"## Read the model","627d4c7b":"## Sample prediction\n\nCareful, these go through the generator. Only run for testing","2ed70b29":"## Read the data","a60dd561":"## T5 Question Answering Inference (PyTorch)\n\n[T5](https:\/\/arxiv.org\/abs\/1910.10683) is a recent approach to do doing sequence to sequence modeling that specifically required input text, and output text, also called a *text-to-text* approach. I've been deeply interested in this model the moment I read about it.\n\nI believe that the combination of *text-to-text* as a universal interface for NLP tasks paired multi-task learning (single model learning multiple tasks) will have huge impact on how deep learning is applied in practice. This competition is my first attemt at utilizing T5 for a real world dataset so I hope it helps you guys use it for your own purposes!","970c3233":"## Prepare data","979e7158":"## Make predictions\n\nFrom 2 hours on CPU, forward pass takes ~5min on GPU w\/ batch size of 16","5be5ea24":"## Setup data as DataLoader"}}