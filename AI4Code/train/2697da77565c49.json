{"cell_type":{"0e80d28c":"code","9e97589a":"code","0af94e5d":"code","bf2ee7a4":"code","e7acf5fb":"code","18bd7235":"code","ad054500":"code","d99fbe3c":"code","0f3335a0":"code","1e33531d":"code","a5c86947":"code","ffeca7d6":"code","297aba84":"code","5b4ccbf4":"code","4709f6b8":"code","39193352":"code","99a84e2b":"code","c6f273ae":"code","e4a4e885":"code","e75c48ab":"code","8c6b5ea5":"code","09c9d1aa":"code","ee9b1301":"code","8fa0789f":"code","a1660837":"code","17a2ccae":"code","f884abe4":"code","94731121":"code","27dbeeb8":"code","2295d07e":"code","730b0088":"code","936b3cbb":"code","b11f0c36":"code","d57e5638":"code","ca05efee":"code","18224260":"code","2d945e38":"code","fc50ddae":"code","268748ea":"code","367a8d95":"code","3969ccd4":"code","7a231ed9":"code","e30f7fce":"code","a24c6466":"code","64e85911":"code","8934190b":"code","e1887efc":"code","10b5b646":"code","3c08adef":"markdown","49364f2b":"markdown","1fa40a40":"markdown","e907a4bf":"markdown","c5cc28c0":"markdown","300b4496":"markdown","707f5815":"markdown","eca5c786":"markdown","e1536b11":"markdown","f311362c":"markdown","6af05f76":"markdown","e6ecec4c":"markdown","7257cf67":"markdown","26d99621":"markdown","476066ed":"markdown","97f8d023":"markdown","6156d5b1":"markdown","abcb8971":"markdown","56ce9247":"markdown"},"source":{"0e80d28c":"import pandas as pd\nimport numpy as np\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline","9e97589a":"!pip install featurewiz","0af94e5d":"from featurewiz import FE_kmeans_resampler, FE_find_and_cap_outliers, EDA_find_outliers\nfrom featurewiz import FE_convert_all_object_columns_to_numeric, split_data_n_ways, FE_create_categorical_feature_crosses\nfrom featurewiz import FE_create_time_series_features, FE_concatenate_multiple_columns\nfrom featurewiz import simple_XGBoost_model\nimport featurewiz as FW","bf2ee7a4":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', 500)","e7acf5fb":"from load_kaggle import load_kaggle","18bd7235":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","ad054500":"target = 'target'\n#df[target] = (df[target] - np.mean(df[target]))\/np.std(df[target])\n#train[target] = np.log(train[target].values)\nidcols = ['id']\nfeatures = [x for x in list(test) if x not in idcols]","d99fbe3c":"train = train[features+[target]]\ndf = train.copy(deep=True)\nprint(train.shape)\ntrain.head(1)","0f3335a0":"df[target].hist()","1e33531d":"train.loc[train[target]<=4] = 3.2","a5c86947":"y_preds = FW.simple_XGBoost_model(X_XGB=train[features], Y_XGB=train[target], X_XGB_test=test[features], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='glmm', verbose=0)","ffeca7d6":"y_preds1, model = y_preds","297aba84":"### Base model above with no feature engg gets you ~0.88 score which is a very nice score.\nsubm[target] = y_preds1\nsubm.to_csv('submission.csv',index=False)\nsubm.head()","5b4ccbf4":"!pip install autoviml","4709f6b8":"from autoviml.Auto_ViML import Auto_ViML","39193352":"#!pip install autoviz","99a84e2b":"#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#filename = \"\"\n#sep = \",\"\n#dft = AV.AutoViz(\n#    filename,\n#    sep=\",\",\n#    depVar=target,\n#    dfte=train,\n#    header=0,\n#    verbose=0,\n#    lowess=False,\n#    chart_format=\"svg\",\n#    max_rows_analyzed=30000,\n#    max_cols_analyzed=30,\n#)","c6f273ae":"### Step 1: we create numeric interaction variables first ###\nintxn_vars = [('cont1','cont4'), ('cont4','cont6'),('cont4','cont13')]","e4a4e885":"train = FW.FE_create_interaction_vars(train, intxn_vars)\ntest = FW.FE_create_interaction_vars(test, intxn_vars)\ntrain.head(2)","e75c48ab":"### we must bin the above newly created discrete variables into 4 or 6 buckets. We will choose 6 for now\nintx_cols = train.columns.tolist()[-3:]\nintx_dict = dict(zip(intx_cols, [6]*3))\ntrain, test = FW.FE_discretize_numeric_variables(train,intx_dict,test=test, strategy='gaussian')\nprint(train.shape, test.shape)\ntrain.head(1)","8c6b5ea5":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","09c9d1aa":"y_preds = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='glmm', verbose=0)","ee9b1301":"y_preds1, model = y_preds","8fa0789f":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","a1660837":"### step 2: we bin the following numeric variables using gaussian mixture models\nbin_these = {'cont1': 8, 'cont2':5, 'cont4':3,    'cont12':2, 'cont13':2}\ntrain, test = FW.FE_discretize_numeric_variables(train,bin_these,test=test, strategy='gaussian')\nprint(train.shape, test.shape)","17a2ccae":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","f884abe4":"output = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='glmm', verbose=0)","94731121":"y_preds1, model = output","27dbeeb8":"### The CV scores are not bad - let's keep these binned variables and add to them in next steps ##","2295d07e":"### step 3: next we create feature crosses of these categorical variables ###\ntrain = FW.FE_create_categorical_feature_crosses(train, ['cat4','cat5','cat6'])\ntest = FW.FE_create_categorical_feature_crosses(test, ['cat4','cat5','cat6'])\nprint(train.shape, test.shape)","730b0088":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","936b3cbb":"y_preds = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='glmm', verbose=0)","b11f0c36":"y_preds1, model = y_preds","d57e5638":"### Absolutely no improvement - but we will keep these vars as long as performance is same! ####","ca05efee":"### step 4: create groupby aggregates of the following numerics \nagg_nums = ['cont5','cont7','cont2']\ngroupby_vars = ['cat5','cat4']\ntrain_add, test_add = FW.FE_add_groupby_features_aggregated_to_dataframe(train[agg_nums+groupby_vars], agg_types=['mean','std'],\n                                groupby_columns=groupby_vars,\n                                ignore_variables=[] , test=test[agg_nums+groupby_vars])","18224260":"train_copy = train.join(train_add.drop(groupby_vars+agg_nums, axis=1))\ntest_copy = test.join(test_add.drop(groupby_vars+agg_nums, axis=1))\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head(2)","2d945e38":"preds = [x for x in list(test_copy) if x not in idcols]\nlen(preds)","fc50ddae":"output = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='glmm', verbose=0)","268748ea":"y_preds1, model = output","367a8d95":"###### step 5: log transform these columns ##########\nlog_cols = {'cont7':'log', 'cont4':'log'}\ntrain_copy = FW.FE_transform_numeric_columns(train_copy, log_cols)\ntest_copy = FW.FE_transform_numeric_columns(test_copy, log_cols)\ntrain_copy.head(2)","3969ccd4":"train_best, test_best = FW.featurewiz(train_copy, target, test_data=test_copy,verbose=2)","7a231ed9":"def left_subtract(l1,l2):\n    lst = []\n    for i in l1:\n        if i not in l2:\n            lst.append(i)\n    return lst\n","e30f7fce":"cats = train_copy.select_dtypes(include=\"object\").columns.tolist()\nlen(cats)","a24c6466":"sel_nums =  ['cont0', 'cont1', 'cont2', 'cont3', 'cont5', 'cont6', 'cont8', 'cont9', 'cont10', 'cont11', 'cont1_discrete', 'cont2_discrete', 'cont4_discrete', 'cont5_by_cat4_std', 'cont5_by_cat5_std', 'cont7_by_cat4_std', 'cont2_by_cat4_std', 'cont12_discrete', 'cont13_discrete', 'cont7_log', 'cont4_log']\npreds = sel_nums+cats\nprint(len(preds))","64e85911":"### using reduced list of variables, the score actually drops 2% points! wow #######\ny_preds = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Regression', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","8934190b":"y_preds1, model = y_preds","e1887efc":"subm = test[idcols]\n#subm = pd.DataFrame()\nsubm[target] = y_preds1\nsubm.head()","10b5b646":"subm.to_csv(target+'_Feb_submission2.csv',index=False)","3c08adef":"#output = split_data_n_ways(df,target, n_splits=2)","49364f2b":"# Autoviml got about 0.8746 in the Kaggle rankings. #######\n###  This is slightly lower than 0.8845 that Autoviml got a month ago but it is about same as featurewiz\n### The good news is that AutoviML and Featurewiz now produce results on a 300K dataset fast\n### It takes less than 2 mins for Autoviml and Featurewiz to crunch this dataset! That's a huge leap.","1fa40a40":"combs = ['Item_Category','Subcategory_1','Subcategory_2']\ntrain = FE_concatenate_multiple_columns(train, combs)\ntest = FE_concatenate_multiple_columns(test, combs)","e907a4bf":"train,_ = FW.FE_split_one_field_into_many(train, field='Product', splitter='-', filler='missing')\ntest,_ = FW.FE_split_one_field_into_many(test, field='Product', splitter='-', filler='missing')\ntrain.head(1)","c5cc28c0":"# This notebook got ~0.8456 in Feb TPS using the Featurewiz library (see below)\n##  Turn on the GPU Accelerator in this Notebook to get the fastest Results below using XGBoost","300b4496":"####\nm, feats, trainm, testm = Auto_ViML(train_copy[preds+[target]], target, test_copy[preds],\n                            sample_submission='',\n                            scoring_parameter='', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=True, Binning_Flag=False,\n                            Add_Poly=0, Stacking_Flag=True,Imbalanced_Flag=False,\n                            verbose=1)","707f5815":"### Add my \"Utility Script\" named Load_kaggle from the File Menu above.###","eca5c786":"train = FE_create_time_series_features(train, 'Date')\ntest = FE_create_time_series_features(test, 'Date')\ntrain.head(1)","e1536b11":"# Just use this one line of code to get ~0.8456 score in ~2 mins! But it can be improved using AutoViz insights (se below)","f311362c":"# Let's use Auto_ViML with GPU to see if we can do better","6af05f76":"# Select the best features created using Featurewiz","e6ecec4c":"#### Lastly convert all object columns to numeric ############\ntrain_copy, test_copy = FE_convert_all_object_columns_to_numeric(train_copy,test_copy)\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head()","7257cf67":"train = FE_find_and_cap_outliers(train,[target], verbose=1)\n#test = FE_find_and_cap_outliers(test,nums,verbose=0)","26d99621":"## AutoViz tells us to do the following using Featurewiz ####\n\nTabular Playground Series - Feb 2021\n\n1. numeric interaction vars and then bin them\n('cont1','cont4'), ('cont4','cont6'),('cont4','cont13')\n\n2. bin the following:\n'cont1': 8, 'cont2':5, 'cont4':3,    'cont12':2, 'cont13':2, \n\n3. interaction cat vars - feature crosses\n\n\n4. groupby vars\n'cont5' by 'cat4', 'cont2' by 'cat5', 'cont7' by 'cat5'\n\n\n5. log transform these\n'cont7':'log', 'cont4':'log',","476066ed":"### The CV scores are less with new features. ####### So it is not worth adding these features\n### <Let us discard the new interaction variables and go back to the old train, test data > ","97f8d023":"y_preds2 = testm['target_predictions'].values\ny_preds2","6156d5b1":"# Install Featurewiz to perform feature engineering and selection","abcb8971":"## Use AutoViz to gain some insights - here's what I learnt from looking at AutoViz charts\n","56ce9247":"## Goal: Use Featurwiz to build a better ranking model in TPS\n1.  Big_Mart Sales Prediction Score: 1147  -- Rank 250 out of 41,361 = That's a Top <1% Rank!!\n1.  Loan Status Predictions Score 0.791  -- Rank 850 out of 67,424 - Top 1.25% Rank\n1.  Machine Hack Flight Ticket Score 0.9389 -- Rank 165 out of 2723 - Top 6% Rank!\n1.  Machine Hack Data Scientist Salary class Score 0.417 -- Rank 58 out of 1547 - Top 3.7% Rank! (Autoviml Score was 0.329 -- less than 0.417 of Featurewiz+Simple even though an NLP problem!)\n1.  MCHACK Book Price NLP Score 0.7336 -- Rank 104 Autoviml NLP problem and should have done better"}}