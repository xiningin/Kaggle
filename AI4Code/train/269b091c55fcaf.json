{"cell_type":{"5ad8c4d5":"code","c696f330":"code","e689f377":"code","f04729e3":"code","aecf0f92":"code","d91cfd12":"code","46b34c14":"code","196a4827":"code","1d9b5d9d":"code","fc90c277":"code","c953ad1e":"code","bcb18227":"code","b5d7efca":"code","18c032a4":"code","5be02b3e":"code","bce15cfa":"code","de979e5d":"code","56aa95a2":"code","4811c1d8":"code","54e70be7":"code","eb5d091c":"markdown","f585ab5b":"markdown","51560dfa":"markdown","bcb1395f":"markdown","89593e06":"markdown","bbb4f83a":"markdown","fd542671":"markdown","4ab7bdd7":"markdown","6e1ba830":"markdown","f66391bd":"markdown","9fe4d665":"markdown"},"source":{"5ad8c4d5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (Shift+Enter) will list the files in the input directory\nimport os\npath = '..\/input'\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.","c696f330":"train_path = f'{path}\/train.csv'\ntest_path = f'{path}\/test.csv'\ntrain_df = pd.read_csv(train_path)","e689f377":"print(train_df.shape)\ntrain_df.head()","f04729e3":"target = train_df['target']\ntrain_df = train_df.drop(['ID_code'], axis = 1).astype('float16')","aecf0f92":"target.value_counts().plot.bar()\nprint('%age value of 0s target variable:', target.value_counts()[0]\/len(target) * 100)\nprint('%age value of 1s target variable:', target.value_counts()[1]\/len(target) * 100)","d91cfd12":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nx_pca = pca.fit_transform(train_df)\nprint(pca.explained_variance_ratio_)\nprint(sum(pca.explained_variance_ratio_))\n\nx_pca = pd.DataFrame(data = x_pca)\nplt.scatter(x = x_pca[0], y = x_pca[1], data = x_pca, c = target.values)\nplt.xlabel('pc1')\nplt.ylabel('pc2')\nplt.title('representation of classes with pca')","46b34c14":"pca = PCA().fit(train_df)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","196a4827":"def density_feature_plot(df, features, grid_size = (8,8)):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(grid_size[0],grid_size[1],figsize=(16,16))\n    \n    t0 = df.loc[df['target'] == 0]\n    t1 = df.loc[df['target'] == 1]\n\n    for feature in features:\n        i += 1\n        plt.subplot(grid_size[0],grid_size[1],i)\n        sns.kdeplot(t0[feature], bw=0.5,label=0)\n        sns.kdeplot(t1[feature], bw=0.5,label=1)\n        plt.xlabel(feature, fontsize=9)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n    plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.tight_layout()\n    plt.show();","1d9b5d9d":"features = train_df.columns.values[2:66]\ndensity_feature_plot(train_df, features)","fc90c277":"features = train_df.columns.values[66:130]\ndensity_feature_plot(train_df, features)","c953ad1e":"features = train_df.columns.values[130:166]\ndensity_feature_plot(train_df, features, (6,6))","bcb18227":"features = train_df.columns.values[166:]\ndensity_feature_plot(train_df, features, (6,6))","b5d7efca":"train_df.iloc[:, 2:100].plot(kind='box', figsize=[16,8])","18c032a4":"# Plot last 100 features.\ntrain_df.iloc[:, 100:].plot(kind='box', figsize=[16,8])","5be02b3e":"corr_df = train_df.corr()","bce15cfa":"import seaborn as sns\nsns.set(style=\"white\")\nmask = np.zeros_like(corr_df.iloc[:,1:], dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df.iloc[:,1:], mask=mask, cmap=cmap, vmax=.2, center=0,\n            square=True, linewidths=.5)","de979e5d":"corr_target=corr_df.loc[corr_df.target>0.05]['target'].iloc[1:] # slight +ve co-rrelation\ncorr_target.plot(kind='bar')","56aa95a2":"corr_target=corr_df.loc[corr_df.target < -0.05]['target'].iloc[1:] \ncorr_target.plot(kind='bar') # slight -ve co-rrelation","4811c1d8":"pd.DataFrame(train_df.isnull().sum()).T","54e70be7":"train_df.duplicated().sum()","eb5d091c":"### check for missing ang duplicate rows","f585ab5b":"No missing values and the duplicate rows","51560dfa":"### Outliers in the data","bcb1395f":"Let's see on high level, if data is separable in 2d\/3d using PCA\/tSNE","89593e06":"There are significan[](http:\/\/)t no.of outliers in the data, they should be treated accordingly","bbb4f83a":"only 13% of variance is explained in 3 principle axis. PCA was unable to capture the variance into 2\/3 dimesions meaning that there is high varaince in the data given. So linear models might not perform well on this data.","fd542671":"very less co-rrelation among the features","4ab7bdd7":"### Co-relation among variables","6e1ba830":"We can observe that there is a considerable number of features with significant different distribution for the two target values.\nFor example, var_0, var_1, var_2, var_5, var_9, var_13, var_21, var_26, var_44, var_76, var_86, var_99, var_106, var_109, var_139, var_174, var_198.","f66391bd":"Train Data distrubution:\nLet us see, how distrubution of data varies b\/w two targets","9fe4d665":"> ### co-rrelation with target variable"}}