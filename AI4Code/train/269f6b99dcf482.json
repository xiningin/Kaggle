{"cell_type":{"11a0fa85":"code","c9840452":"code","96dc88ff":"code","890c7654":"code","38bcfa75":"code","77fda8a2":"code","c4fb01fb":"code","7c728cfb":"code","026f74db":"code","1290d8b5":"code","e96efc4f":"code","60690bdb":"code","82fc6f78":"code","5f27ceaf":"code","8e4b5e1e":"code","92234519":"code","d2b37014":"code","bc871dba":"code","3d133d45":"code","468941f2":"code","befcdc6f":"code","7fff6286":"code","1ddd8b83":"code","a7716c1c":"code","540ecfcf":"code","9efd8828":"code","50d3f4b3":"code","93f00d92":"code","87f191f1":"code","65991c49":"code","2a7774b2":"code","36a35e10":"code","31dc7a8e":"code","37b90545":"code","b1319224":"code","5f377027":"code","ae73484a":"code","5e28d004":"code","ebd350e2":"code","393c3835":"code","2c7cd7a9":"code","662469c7":"code","56a9355d":"code","0d0eec03":"code","c44ac7ca":"code","5cda1913":"code","ad132049":"markdown","24cb1133":"markdown","2ceb2028":"markdown","1627b0f0":"markdown","587b5fe7":"markdown","3dadfc33":"markdown","b75c1bb6":"markdown","82ff2539":"markdown","28c2c1b0":"markdown","459064de":"markdown","7b21ac01":"markdown","d8e347b6":"markdown","e328aff7":"markdown","97822fb5":"markdown","4056ed04":"markdown","0c97ecd8":"markdown","e2f96bbb":"markdown","9bfb1cee":"markdown","18345fa3":"markdown","4fcfa753":"markdown","67145e23":"markdown","fc5954ce":"markdown"},"source":{"11a0fa85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c9840452":"# Imports\nimport math\n\n\nimport scipy.stats\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns","96dc88ff":"# Read the data\nX = pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/test.csv', index_col='Id')\nz = pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/sample_submission.csv')\n\n","890c7654":"# How is our data shaping up?\nprint('Our training dataset has {} rows and {} columns.'.format(X.shape[0], X.shape[1]))\nprint('Our test dataset has {} rows and {} columns.'.format(X_test_full.shape[0], X_test_full.shape[1]))","38bcfa75":"X.head()","77fda8a2":"X.describe()","c4fb01fb":"# How expensive are houses?\nprint('The cheapest house sold for ${:,.0f} and the most expensive for ${:,.0f}'.format(\n    X.SalePrice.min(), X.SalePrice.max()))\nprint('The average sales price is ${:,.0f}, while median is ${:,.0f}'.format(\n    X.SalePrice.mean(), X.SalePrice.median()))\n","7c728cfb":"X.SalePrice.hist(bins=75, rwidth=.8, figsize=(14,4))\nplt.title('How expensive are houses?')\nplt.show()","026f74db":"# When were the houses built?\nprint('Oldest house built in {}. Newest house built in {}.'.format(\n    X.YearBuilt.min(), X.YearBuilt.max()))\n","1290d8b5":"X.YearBuilt.hist(bins=14, rwidth=.9, figsize=(12,4))\nplt.title('When were the houses built?')\nplt.show()","e96efc4f":"# When where houses sold?\nX.groupby(['YrSold','MoSold']).count().plot(kind='barh', figsize=(14,21.8))\nplt.title('When where houses sold?')\nplt.show()","60690bdb":"# Where are houses?\n\nX.groupby('Neighborhood').count().plot(kind='barh', figsize=(14,21.85))\nplt.title('What neighborhoods are houses in?')\nplt.show()","82fc6f78":"sns.pairplot(X[[\"SalePrice\", \"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\", \"FullBath\", \"BedroomAbvGr\", \"TotRmsAbvGrd\"]], diag_kind=\"kde\")","5f27ceaf":"X.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)","8e4b5e1e":"from sklearn.model_selection import train_test_split\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","92234519":"# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","d2b37014":"len(numeric_cols)","bc871dba":"# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","3d133d45":"X_train.isnull().sum() #to get all columns have or not missing values","468941f2":"from sklearn.impute import SimpleImputer\n\n\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Fill in the lines below: imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","befcdc6f":"# Preprocessed training and validation features\nfinal_imputer = SimpleImputer(strategy='median')\nfinal_X_train = pd.DataFrame(final_imputer.fit_transform(X_train))\nfinal_X_valid = pd.DataFrame(final_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nfinal_X_train.columns = X_train.columns\nfinal_X_valid.columns = X_valid.columns","7fff6286":"from xgboost import XGBRegressor\nfrom ml_metrics import rmse\n# Define the model\nmodel = XGBRegressor(n_estimators=350,max_depth=15,learning_rate=.1,random_state=1,\n                     n_jobs=10, subsample=1,min_child_weight=0.6) # Your code here\n\n# Fit the model\nmodel.fit(final_X_train, y_train)\n\n# Get validation predictions and MAE\npreds_valid = model.predict(final_X_valid)\n\n\nprint(\"RMSE (Your appraoch):\")\nprint(rmse(y_valid, preds_valid))","1ddd8b83":"feature_columns = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath',\n                   'BedroomAbvGr', 'TotRmsAbvGrd']\nX_up = X[feature_columns]\n\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X_up, y, random_state=1)\n","a7716c1c":"train_X.columns","540ecfcf":"X_up.columns","9efd8828":"my_imputer_up = SimpleImputer()\nimputed_X_up_train = pd.DataFrame(my_imputer_up.fit_transform(train_X))\nimputed_X_up_valid = pd.DataFrame(my_imputer_up.transform(val_X))\n\n# Fill in the lines below: imputation removed column names; put them back\nimputed_X_up_train.columns = train_X.columns\nimputed_X_up_valid.columns = val_X.columns","50d3f4b3":"# Preprocessed training and validation features\nfinal_imputer_up = SimpleImputer(strategy='median')\nfinal_X_train_up = pd.DataFrame(final_imputer_up.fit_transform(train_X))\nfinal_X_valid_up = pd.DataFrame(final_imputer_up.transform(val_X))\n\n# Imputation removed column names; put them back\nfinal_X_train_up.columns = train_X.columns\nfinal_X_valid_up.columns = val_X.columns","93f00d92":"model_up = XGBRegressor(n_estimators=350,max_depth=15,random_state=5,learning_rate=.1,\n                        n_jobs=10, subsample=1,min_child_weight=0.6) # Your code here\n\n# Fit the model\nmodel_up.fit(final_X_train_up, train_y)\n\n# Get validation predictions and MAE\npreds_valid = model_up.predict(final_X_valid_up)\n\n\nprint(\"RMSE (Your appraoch):\")\nprint(rmse(val_y, preds_valid))","87f191f1":"from sklearn.model_selection import GridSearchCV","65991c49":"Select_parameters = {'min_samples_leaf':[1,2,3,5,6,4,8,9], 'random_state':[1,2,3,4,5,6,9,8],\n                     'n_estimators':[100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000]}","2a7774b2":"gsearch = GridSearchCV(estimator=XGBRegressor(),\n                       param_grid = Select_parameters, \n                       scoring='neg_mean_absolute_error',\n                       n_jobs=4,cv=3)\n\n","36a35e10":"gsearch.fit(X_train,y_train)\n","31dc7a8e":"gsearch.best_params_, gsearch.best_score_","37b90545":"\n\ngsearch.best_params_.get('min_samples_leaf')\n\n","b1319224":"gsearch.best_params_.get('random_state')\n","5f377027":"gsearch.best_params_.get('n_estimators')","ae73484a":"final_model_up = XGBRegressor(n_estimators=gsearch.best_params_.get('n_estimators'),  learning_rate=0.1,\n                           max_depth=16,random_state=gsearch.best_params_.get('random_state'),\n                           subsample=1.0,min_samples_leaf=gsearch.best_params_.get('min_samples_leaf'),\n                           n_jobs=4)","5e28d004":"final_model_up.fit(final_X_train, y_train)\n\n","ebd350e2":"#  preds_test = final_model_up.predict(final_X_valid_up)\n","393c3835":"preds_test = final_model_up.predict(X_test)","2c7cd7a9":"y.head()","662469c7":"# print the top few validation predictions\nprint(final_model_up.predict(X_valid.head(10)))\n","56a9355d":"\n\n# print the top few actual prices from validation data\nprint(y.head(10))","0d0eec03":"len(final_X_train_up)","c44ac7ca":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_full.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\nprint('done')","5cda1913":"# # Save test predictions to file\n# output = pd.DataFrame({'Id': X_test.index,\n#                        'SalePrice': preds_test})\n# output.to_csv('submission.csv', index=False)\n# print('done')","ad132049":"# Data processing","24cb1133":"The location of the house is expected to play a key role in determining the price of the house.\nLooks like a good chunk of houses are in North Ames, Collect Creek, and Old Town, with few houses in Bluestem, Northpark Villa and Veenker.\nThese areas also appear to contain public facilities such as schools and hospitals.\nThis led to a rise in the price of houses where no other areas","2ceb2028":"Not much action in the 80s apparently. Looks like majority of houses were built in the 50s and after.","1627b0f0":"It is self-evident that when you want to buy a new house there are many things that must be inquired about such as the size of the house, the number of rooms, the number of bathrooms, how many floors of the house, the size of each floor and on which contains rooms, bathrooms and living rooms .... And other basic stuff.\nThe following figure shows the impact of the price of the house in different factors.","587b5fe7":"## General Quastion about Dataset","3dadfc33":"#### A feature that plays a key role in the price of a home\n* LotArea: Lot size in square feet\n* YearBuilt: Original construction date\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* FullBath: Full bathrooms above grade\n* Bedroom: Number of bedrooms above basement level\n\n","b75c1bb6":"> # Preliminary investigation","82ff2539":"# Remove rows with missing target, separate target from predictors","28c2c1b0":"-------------------------------------------------------------------------------------------------","459064de":"### About dataset:\n#### The Ames housing dataset was the basis for the Kaggle house prices competition. The object of the competition was to predict the sale price of a house based on a set of features such as the number of bedrooms, the neighbourhood within Ames, etc. It is worth looking into it with Tableau to do some initial exploratory data analysis.\n","7b21ac01":"#### Everything looks good here, with no missing values. The typical house price in Ames is roughly 150k.House prices range from 100k to 200k","d8e347b6":"# Separate Numerical columns from Categorical columns ","e328aff7":"### Inspect your predictions and actual values from validation data.","97822fb5":"#  Build model","4056ed04":"# Grid Search","0c97ecd8":"# Contact with missing values by Simple Imputer","e2f96bbb":"# Break off validation set from training data","9bfb1cee":"If the time of sale during the year affects the selling price of homes, it is also likely that the location of the home will affect its price.","18345fa3":"\n## High Level Overview\n\nLet's begin by getting high-level look at our data.\n","4fcfa753":"We see a strong seasonal pattern in house sales, with peaks in June and July(summer season)","67145e23":"## Improve the model by Feature","fc5954ce":"## Imports and Data Load"}}