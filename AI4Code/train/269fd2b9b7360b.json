{"cell_type":{"f7207215":"code","aa9c758a":"code","03867275":"code","0cdf65cb":"code","1c0cbed8":"code","b470b893":"code","38d6558f":"code","e3576510":"code","4899fa07":"code","cdf986e6":"code","ae0837cd":"code","0b5383a1":"code","be3aa637":"code","4091d0e8":"code","2c035850":"code","3e404650":"code","40e30547":"code","3ae736a3":"code","28bcaebe":"code","9b0015fb":"code","d6db997e":"code","06285002":"code","3a338bc3":"code","bf390d76":"code","0a2372de":"code","2f8be863":"code","ea57d2f4":"code","4eaca69e":"code","fe39eabc":"code","3676a1a7":"code","71fd8fc0":"code","67dacd4f":"code","d71bab8f":"code","c5182d7a":"code","b2c37f7e":"code","f5a7c6ca":"code","8f259914":"code","46ff5c71":"code","899b3dfc":"code","3125b593":"code","fb9085b1":"code","6423599c":"code","dd475042":"code","c7ef369e":"code","9b683c30":"code","df139f4b":"code","456f17c8":"code","5c279b45":"code","c7aa4e49":"code","c7cec441":"code","f1c88c65":"code","4e028c9f":"code","94feff20":"code","17d82478":"code","e08475be":"code","9755a055":"code","253b2855":"code","6c57d3d4":"code","fc8fba15":"code","d7a578ec":"code","8c112b79":"code","b22f906f":"markdown","a0ec908d":"markdown","93a20731":"markdown","057c9040":"markdown","a9ef0eae":"markdown"},"source":{"f7207215":"#Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as mplt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n# pd.pandas.set_option('display.max_rows',None)\npd.pandas.set_option('display.max_columns',None)","aa9c758a":"df_train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","03867275":"\ndf_train = pd.concat([df_train,df_test],ignore_index=True)","0cdf65cb":"print(df_train.shape)\ndf_train.head(10)\n","1c0cbed8":"feature_num_nanvalues = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>=1 and df_train[feature].dtypes!='O']\nfor feature in feature_num_nanvalues:\n    print(\"{}:{}% missing values\".format(feature,np.round(df_train[feature].isnull().mean(),4)))","b470b893":"# Replace Missing Numerical Values\ndef replace_numerical_values(dataset,feature_num_nanvalues):\n    data = dataset.copy()\n    for feature in feature_num_nanvalues:\n        median_value = data[feature].median()\n         ## create a new feature to capture nan values\n        data[feature+'nan']=np.where(data[feature].isnull(),1,0)\n        data[feature].fillna(median_value,inplace=True)\n    return data\n    \ndf_train = replace_numerical_values(df_train,feature_num_nanvalues)\ndf_train[feature_num_nanvalues].isnull().sum()\ndf_train.shape","38d6558f":"feature_cate_nanValues = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>=1 and df_train[feature].dtypes=='O']\nfor feature in feature_cate_nanValues:\n    print(\"{}:{}% missing values\".format(feature,np.round(df_train[feature].isnull().mean(),4)))","e3576510":"#Replace Categorical Values\ndef replace_cat_feature(dataset,feature_nan):\n    data = dataset.copy()\n    data[feature_nan]= data[feature_nan].fillna(\"Missing\")\n    return data\ndf_train = replace_cat_feature(df_train,feature_cate_nanValues)\ndf_train[feature_cate_nanValues].isnull().sum()","4899fa07":"df_train.head()\ndf_train.shape","cdf986e6":"print(df_train.SalePrice.skew())\nmplt.hist(df_train.SalePrice,color='blue')\nmplt.show()","ae0837cd":"#To improve linearity of the target depended data\ntarget=np.log(df_train.SalePrice)\nprint(\"Target skew is\",target.skew())\nmplt.hist(target,color='blue')\nmplt.show()\n#histogram\n#sns.distplot(df_train['SalePrice']);","0b5383a1":"\ndf_train.plot(x='GrLivArea', y='SalePrice', kind='scatter')","be3aa637":"#optional: plot MEDV vs RM using the plot method of the dataframe (using kind=\"scatter\").\ndf_train.plot(x='LotArea', y='SalePrice', kind='scatter')","4091d0e8":"\ndf_train.plot(x='TotalBsmtSF', y='SalePrice', kind='scatter')","2c035850":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = mplt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","3e404650":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = mplt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nmplt.xticks(rotation=90);","40e30547":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nmplt.show();","3ae736a3":"## Temporal Variables (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    df_train[feature]=df_train['YrSold']-df_train[feature]\n    \ndf_train.head()","28bcaebe":"df_train[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","9b0015fb":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\nfor feature in num_features:\n    df_train[feature]=np.log(df_train[feature])","d6db997e":"df_train.head()","06285002":"cat_feature = [feature for feature in df_train.columns if df_train[feature].dtype=='O']\ncat_feature\nprint(len(cat_feature))\ndf_train.shape\n","3a338bc3":"# let's have a look at how many labels each variable has\nfor feature in df_train.columns:\n    if df_train[feature].dtype=='O':\n        print(feature,':',len(df_train[feature].unique()),'label')\n","bf390d76":"# Exterior1st\ndf_train.Exterior1st.value_counts().sort_values(ascending=False).head(10)","0a2372de":"df_train.isnull().sum().sum()\n","2f8be863":"#Integer conversions (Label Encoder)\nfrom sklearn.preprocessing import LabelEncoder\nlc = LabelEncoder()\n\nfor i in cat_feature:\n     df_train[i] = lc.fit_transform(df_train[i])\n    \ndf_train.shape\n# df_train.head(10)","ea57d2f4":"df_train.head()","4eaca69e":"df_train[cat_feature].shape","fe39eabc":"X = df_train.drop('SalePrice', axis='columns')\nX.head()","3676a1a7":"# column_trans.fit_transform(X)\ndf_train.isnull().sum()","71fd8fc0":"## Always remember there way always be a chance of data leakage so we need to split the data first and then apply feature\n## Engineering\nfrom sklearn.model_selection import train_test_split\nX = df_train.drop('SalePrice', axis='columns')\ny = df_train['SalePrice']\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","67dacd4f":"## for feature slection\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n### Apply Feature Selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\nfeature_sel_model.fit(X,y)\n\n","d71bab8f":"feature_sel_model.get_support()","c5182d7a":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = X.columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(np.sum(feature_sel_model.estimator_.coef_ == 0)))","b2c37f7e":"selected_feat\nX=X[selected_feat]\nX","f5a7c6ca":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5,random_state=0)\nX_train.shape,X_test.shape","8f259914":"from sklearn.linear_model import LinearRegression\n\n\n\nlr = LinearRegression().fit(X_train, y_train)\n\n\nprint(lr.score(X_test, y_test))\n","46ff5c71":"coeff_df = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df","899b3dfc":"y_pred = lr.predict(X_test)","3125b593":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(25)\nprint(df1)","fb9085b1":"df1.plot(kind='bar',figsize=(10,8))\nmplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nmplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nmplt.show()","6423599c":"\nprint('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))","dd475042":"df_train['SalePrice'].head()","c7ef369e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\n\nlm = LinearRegression()\n\n#Next we do cross validation, which splits apart our training data and fits the model on different samples and \n# gives scores for each sample to get the best fit model before we test it on the testing data.\n\nscores = cross_val_score(lm, X_train, y_train, cv = 5)    #cv is the number of folds, scores will give an array of scores\n\nprint (scores, np.mean(scores), np.std(scores))\n\n#To get predictions (y_hat) and check them all in one using cross validation\n\npredictions = cross_val_predict(lm, X_test, y_test, cv = 5)     #y_test is needed here in predictions to get scores for each fold of cv\n\n\n#If this is good, continue to fit the model on the data\n\n\nlm.fit(X_train, y_train)\n\ny_hat = lm.predict(X_test)      #this gives me my predictions\n\nlm.score(X_test, y_test)     #this tells me my model performance\n\nprint('Score Value: \\n', lm.score(X_test, y_test))\n# The coefficients\n#print('Coefficients: \\n', lr.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, y_hat))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.4f'\n      % r2_score(y_test, predictions))\n\n","9b683c30":"# Building the Model ------------------------------------------------------------------------\n\n# Fitting regressior to the Training set\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=1)),\n    ('model', Ridge())\n]\n\nridge_pipe = Pipeline(steps)\nridge_pipe.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = ridge_pipe.predict(X_test)\n\n\nprint(ridge_pipe.score(X_test,y_test))","df139f4b":"from sklearn.pipeline import make_pipeline\npipe = make_pipeline(StandardScaler(), Ridge())\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)\n","456f17c8":"from sklearn.ensemble import GradientBoostingRegressor\n# Fit regression model\nreg = GradientBoostingRegressor(random_state=0)\nreg.fit(X_train, y_train)\n# reg.predict(X_test)\n\n# y=df_test['SalePrice']\nreg.predict(X_test)\nprint(\"Score value:%.3f\"%reg.score(X_test, y_test))\ny_hat = reg.predict(X_test) \nprint('Mean squared error: %.3f'\n      % mean_squared_error(y_test,y_hat))\nprint('Root Mean Squared Error:%.3f'%np.sqrt(mean_squared_error(y_test, y_hat)))\n","5c279b45":"\naf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndataset=pd.DataFrame({'SalePrice':y_hat})\ndf = pd.concat([af['Id'],dataset],axis=1)\n\ndf.head()\n\ndf.to_csv('..\/input\/house-prices-advanced-regression-techniques\/final_submission.csv', index=False)","c7aa4e49":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_hat})\ndf1 = df.head(25)\ndf1.plot(kind='bar',figsize=(10,8))\nmplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nmplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nmplt.show()","c7cec441":"from sklearn.ensemble import GradientBoostingClassifier\n\nparam_grid={'n_estimators':[100,500], \n            'learning_rate': [0.1,0.05,0.02],\n            'max_depth':[4], \n            'min_samples_leaf':[3], \n            'max_features':[1.0] } \ntuning = GridSearchCV(estimator=GradientBoostingRegressor(), cv=3, param_grid=param_grid, \n    n_jobs=4)\ntuning.fit(X_train,y_train)\ntuning.predict(X_test)\nprint(\"Score value:%.3f\"%tuning.score(X_test, y_test))\ny_hat = tuning.predict(X_test) \nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, y_hat))\n","f1c88c65":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n# import xgboost as xgb\n# import lightgbm as lgb","4e028c9f":"# Cross-validation with k-folds\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","94feff20":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","17d82478":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)","e08475be":"\n\n# Performing Grid Search with specific alpha values for Ridge\nalphas = np.array([1,0.1,0.01,0.001,0.0001])\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\ngrid.fit(X_train, y_train)\nprint(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)\n\n","9755a055":"\n\n# Performing Grid Search with specific of alpha values for Lasso\nalphas = np.array([1,0.1,0.01,0.001,0.0001])\nmodel = Lasso()\ngrid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\ngrid.fit(X_train, y_train)\nprint(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)\n\n","253b2855":"# Performing Grid Search specific alpha values for ElasticNet\nalphas = np.array([1,0.1,0.01,0.001,0.0001])\nmodel = ElasticNet()\ngrid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\ngrid.fit(X_train, y_train)\nprint(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","6c57d3d4":"# Applying the searched values of Alpha and l1_ratio\nmodel1 = Ridge(alpha=0.923990115671)\nmodel2 = Lasso(alpha=0.968154257543)\nmodel3 = ElasticNet(alpha=0.001, l1_ratio=0.99411629749)\nmodel1.fit(X_train, y_train)\nmodel2.fit(X_train, y_train)\nmodel3.fit(X_train, y_train)\npred1 = model1.predict(X_test)\npred2 = model2.predict(X_test)\npred3 = model3.predict(X_test)","fc8fba15":"print(\"Score value:%.3f\"%model1.score(X_test, y_test))\nprint(\"Score value:%.3f\"%model2.score(X_test, y_test))\nprint(\"Score value:%.3f\"%model3.score(X_test, y_test))","d7a578ec":"finalpred = 0.33*pred1 + 0.33*pred2 + 0.34*pred3\n#sample = sample.rename_axis('Id').reset_index()\naf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndataset=pd.DataFrame({'SalePrice':finalpred})\ndf = pd.concat([af['Id'],dataset],axis=1)\n#af = af.rename_axis('Id').reset_index()\n\n#af['SalePrice'] = y_hat\n# sample['Id'] = df_test['Id']\ndf.head()\n\ndf.to_csv('..\/input\/house-prices-advanced-regression-techniques\/final_submission.csv', index=False)","8c112b79":"from sklearn import ensemble\n\n#lr =  ensemble.RandomForestRegressor(n_estimators = 100, oob_score = True, n_jobs = -1,random_state =50,max_features = \"sqrt\", min_samples_leaf = 50)\n#lr = linear_model.LinearRegression()\nlr = ensemble.GradientBoostingRegressor()\n\n\nmodel = lr.fit(X_train, y_train)\n\n\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\n","b22f906f":"### Data Cleaning and Preprocessing","a0ec908d":"### Data Exploration and Analysis","93a20731":"## Numerical Variables\nSince the numerical variables are skewed we will perform log normal distribution","057c9040":"### Data Collection","a9ef0eae":"### Preprocessing Data 2"}}