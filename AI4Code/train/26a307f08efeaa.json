{"cell_type":{"3a9e875e":"code","c2066558":"code","36e79121":"code","96f752d9":"code","e59b05ff":"code","0fcacc68":"code","e82da9ae":"code","5e7c1432":"code","aa3743b8":"code","84e0da8e":"code","2bebc7ee":"code","197f2aca":"code","310bd851":"code","ba0a84df":"code","5d495660":"code","f36ef2a7":"code","f402a981":"code","c84fd672":"code","bf225813":"code","e889de59":"code","4ae97cc6":"code","ca526d55":"code","68b52e7d":"code","44424774":"code","09521f38":"code","0fb8b99d":"code","4c11ca58":"code","a1c2530e":"code","9752ce62":"code","90c850de":"code","c894ddd7":"code","091ccd6d":"code","2293a828":"code","abeb324f":"markdown","ca827fd7":"markdown","fdb6be80":"markdown","2c10e9b1":"markdown","7c53a275":"markdown"},"source":{"3a9e875e":"# import packages\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wordcloud as wc\nimport scipy.sparse as sparse\nimport xgboost as xgb\nimport sklearn\n\nfrom xgboost import XGBRegressor\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import KFold,train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nsaving=False","c2066558":"# load data\n\n# 4803 movies\ntmdb_5000_movies=pd.read_csv(\"..\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv\",index_col=\"id\")\ntmdb_5000_credits=pd.read_csv(\"..\/input\/tmdb-movie-metadata\/tmdb_5000_credits.csv\",index_col=\"movie_id\")\ntmdb_5000=pd.merge(tmdb_5000_movies,tmdb_5000_credits,left_index=True,right_index=True)\ntmdb_5000.dropna(subset=[\"release_date\"],inplace=True)\ntmdb_5000.replace(0,np.nan,inplace=True)\n\n# 27278 movies, 138493 users, 20000263 ratings\nmovielens_rating=pd.read_csv(\"..\/input\/movielens-20m-dataset\/rating.csv\")\nmovielens_link=pd.read_csv(\"..\/input\/movielens-20m-dataset\/link.csv\")\nmovielens_movie=pd.read_csv(\"..\/input\/movielens-20m-dataset\/movie.csv\")\nmovielens_score=pd.read_csv(\"..\/input\/movielens-20m-dataset\/genome_scores.csv\")\nmovielens_tag=pd.read_csv(\"..\/input\/movielens-20m-dataset\/genome_tags.csv\")\nmovielens_dict={v:k for k,v in movielens_link.movieId.to_dict().items()}\nmovielens_rating.movieId=movielens_rating.movieId.apply(movielens_dict.get)\nmovielens_rating.userId-=1\nmovielens_score.movieId=movielens_score.movieId.apply(movielens_dict.get)\nmovielens_score.tagId-=1\nmovielens_tag.tagId-=1","36e79121":"# helper functions to deal with multi-hot features\ndef group_indices(series,index=\"id\"):\n    d={}\n    for i in range(series.size):\n        l=eval(series.iloc[i])\n        for x in l:\n            d.setdefault(x[index],[])\n            d[x[index]].append(i)\n    return d\n\ndef get_groups(series,index=\"name\"):\n    s=set()\n    for i in range(series.size):\n        l=eval(series.iloc[i])\n        for x in l:s.add(x[index])\n    return list(s)\n\ndef multi_count(series,index=\"id\"):\n    return {k:len(v) for (k,v) in group_indices(series,index).items()}\n\ndef expand_multi_feature(df,column,index=\"id\"):\n    groups=group_indices(df[column],index=index)\n    result=pd.DataFrame()\n    for name,indices in groups.items():\n        rows=df.iloc[indices].copy()\n        rows[column]=name\n        result=result.append(rows)\n    return result\n\ndef multi_groupby(df,column,index=\"id\"):\n    return expand_multi_feature(df,column,index).groupby(column)","96f752d9":"# numbers of movies released in each decade\ndef count_pie(series,filename):\n    counts=series.value_counts()\n    counts=counts\/counts.sum()\n    labels=['' if num<0.01 else str(year) for (year,num) in counts.items()]\n    f, ax = plt.subplots(figsize=(8, 8))\n    explode = [0.02 if counts.iloc[i] < 100 else 0.001 for i in range(counts.size)]\n    plt.pie(counts,labels=labels,autopct=lambda x:'{:1.0f}%'.format(x) if x > 1 else '',explode=explode)\n    if saving:plt.savefig(filename,dpi=150)\n    plt.show()\n\ndef count_decade_pie(df,filename):\n    count_pie(df.release_date.dropna().apply(lambda x:str(int(x[:4])\/\/10*10)+'s'),filename)\n    \ncount_decade_pie(tmdb_5000,filename=\"pie_decade.png\")","e59b05ff":"# numbers of movies of different genres and keywords\ndef multi_bar(series,filename):    \n    sns.set(style=\"whitegrid\")\n    count=multi_count(series,\"name\")\n    count=sorted(count.items(),key=lambda x:x[1],reverse=True)\n    count=dict(count[:30])\n    f, ax = plt.subplots(figsize=(10, 6))\n    plt.xticks(rotation=85, fontsize=15)\n    plt.bar(count.keys(),count.values(),align=\"center\")\n#     plt.pie(count.values(),labels=count.keys())\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=100)\n    plt.show()\n\nmulti_bar(tmdb_5000.genres,filename=\"bar_genres.png\")\nmulti_bar(tmdb_5000.keywords,filename=\"bar_keywords.png\")","0fcacc68":"# wordcloud of genres and keywords\ndef multi_wordcloud(series,filename):\n    w=wc.WordCloud(background_color=\"white\",margin=20,width=800,height=600,prefer_horizontal=0.7,max_words=50,scale=2)\n    count=multi_count(series,\"name\")\n    w.generate_from_frequencies(count)\n    if saving:w.to_file(filename)\n    f, ax = plt.subplots(figsize=(16, 8))\n    plt.axis('off')\n    plt.imshow(w)\n    plt.show()\n\nmulti_wordcloud(tmdb_5000.genres,filename=\"wordcloud_genres.png\")\nmulti_wordcloud(tmdb_5000.keywords[tmdb_5000.release_date.apply(lambda x:int(x[:4])\/\/10*10)==1930],filename=\"wordcloud_keywords_1930.png\")\nmulti_wordcloud(tmdb_5000.keywords[tmdb_5000.release_date.apply(lambda x:int(x[:4])\/\/10*10)==1950],filename=\"wordcloud_keywords_1950.png\")\nmulti_wordcloud(tmdb_5000.keywords[tmdb_5000.release_date.apply(lambda x:int(x[:4])\/\/10*10)==1980],filename=\"wordcloud_keywords_1980.png\")\nmulti_wordcloud(tmdb_5000.keywords[tmdb_5000.release_date.apply(lambda x:int(x[:4])\/\/10*10)==2000],filename=\"wordcloud_keywords_2000.png\")","e82da9ae":"# change of percentage of movies of each genres released in each decade\ndef multi_stackplot_10year(df,value,filename):\n    sns.set(style=\"whitegrid\")\n    df=df[[\"release_date\",value]].dropna()\n    df.release_date=df.release_date.apply(lambda x:int(x[:4])\/\/10*10)\n    df=df.sort_values(\"release_date\")\n    years=pd.Series(range(df[\"release_date\"].min(),df[\"release_date\"].max()+1,10),name=\"year\")\n    counts=[]\n    groups=[]\n    for (name,col) in multi_groupby(df,\"genres\",\"name\"):\n        groups.append(name)\n        counts.append(pd.merge(years.to_frame(),col.groupby(\"release_date\").size().rename(\"count\").to_frame(),\n                               how=\"left\",left_on=\"year\",right_on=\"release_date\").fillna(0).astype(int)[\"count\"])\n    counts=np.array(counts)\n    counts=counts.transpose(0,1)\n    counts=counts\/counts.sum(0)\n    f, ax = plt.subplots(figsize=(16, 8))\n    plt.stackplot(years,counts,labels=groups)\n    plt.xticks(years)\n    plt.xlim(years.iloc[0],years.iloc[-1])\n    plt.ylim(0,1)\n    plt.legend()\n    if saving:plt.savefig(filename,dpi=200)\n    plt.show()\n\nmulti_stackplot_10year(tmdb_5000,\"genres\",filename=\"stackplot_genres.png\")","5e7c1432":"# distribution of popularity and runtime groupby genres\ndef plotby_box(df,x,y,filename,yscale=\"linear\"):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    sns.boxenplot(data=expand_multi_feature(df,x,\"name\"),x=x,y=y)\n    plt.yscale(yscale)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \ndef plotby_violin(df,x,y,filename,yscale=\"linear\"):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    sns.violinplot(data=expand_multi_feature(df,x,\"name\"),x=x,y=y)\n    plt.yscale(yscale)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \nplotby_box(tmdb_5000,\"genres\",\"popularity\",yscale=\"log\",filename=\"genres_popularity.png\")\nplotby_violin(tmdb_5000,\"genres\",\"runtime\",yscale=\"linear\",filename=\"genres_runtime.png\")","aa3743b8":"# average vote of each genres' movies\ndef plotby_bar(df,x,y,filename):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    sns.barplot(data=expand_multi_feature(df,x,\"name\"),x=x,y=y)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \nplotby_bar(tmdb_5000,\"genres\",\"vote_average\",filename=\"genres_vote.png\")","84e0da8e":"# average budget and revenue of each genres' movies\ndef plotby_2bar(df,x,y,filename):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    multi_groupby(df.dropna(subset=y),x,\"name\")[y].mean().sort_values(y,ascending=False).plot(kind=\"bar\",ax=ax)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n\nplotby_2bar(tmdb_5000,\"genres\",[\"revenue\",\"budget\"],filename=\"genres_budget_revenue.png\")","2bebc7ee":"# return on investment of each genres' movies\ndef plotby_portion(df,x,y,filename):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n#     sns.barplot(data=multi_groupby(df,x,\"name\")[y].mean().transpose())\n    group=multi_groupby(df.dropna(subset=y),x,\"name\")\n    (group[y[0]].mean()\/group[y[1]].mean()).sort_values(ascending=False).plot(kind=\"bar\")\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(\"ROI\",fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \nplotby_portion(tmdb_5000,\"genres\",[\"revenue\",\"budget\"],filename=\"genres_ROI.png\")","197f2aca":"# define transformers\nclass SparseMatrixTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self,row=None,col=None,value=None,shape=None):\n        self.row=row\n        self.col=col\n        self.value=value\n        self.shape=shape\n    \n    def fit(self,X,y=None):\n        self.shape=(X[self.row].max()+1,X[self.col].max()+1)\n        return self\n    \n    def transform(self,X,y=None):\n        if type(X) is pd.DataFrame:\n            return sparse.csr_matrix(sparse.coo_matrix((X[self.value],(X[self.row],X[self.col])),self.shape))\n        else:\n            return sparse.csr_matrix(sparse.coo_matrix((X[:,2],(X[:,0],X[:,1])),self.shape))\n    \nuser_movie_transformer=SparseMatrixTransformer(row=\"userId\",col=\"movieId\",value=\"rating\").fit(movielens_rating)\nmovie_tag_transformer=SparseMatrixTransformer(row=\"movieId\",col=\"tagId\",value=\"relevance\",shape=(user_movie_transformer.shape[1],movielens_tag.tagId.max()+1))","310bd851":"# base class for recommendation system\nclass RecommenderMixin(BaseEstimator,TransformerMixin):\n    def __init__(self,n_rec=10):\n        self.n_rec=n_rec\n    \n    def evaluate(self,X_train,X_valid,silent=False):\n        n_users,n_items=X_train.shape\n        X=(X_train+X_valid).T\n        item_rating=np.array([row.data.mean() if row.nnz else 0 for row in X])\n        item_rating[np.isnan(item_rating)]\n        mean_rating=X.data.mean()\n        if not silent:print(\"predicting...\")\n        recommend=self.predict(X_train)\n        if not silent:print(\"evaluating...\")\n        intersect=np.asarray(X_valid[np.arange(n_users).repeat(self.n_rec),recommend.flatten()].reshape(n_users,-1))\n        hit=(intersect!=0).sum()\n        precision=hit\/recommend.size\n        recall=hit\/X_valid.nnz\n        coverage=np.unique(recommend).size\/n_items\n        hit_rating=(intersect[intersect!=0]).mean()\n        indices=recommend[(intersect!=0)]\n        recom_rating=np.average(item_rating[indices])\n        print(\"accuracy=%.4f\"%precision)\n        print(\"recall=%.4f\"%recall)\n        print(\"cover=%.4f\"%coverage)\n        print(\"user's vote to the hit movies=%.4f\"%(hit_rating))\n        print(\"average vote to the hit movies=%.4f\"%(recom_rating))\n        print(\"average vote to all movies=%.4f\"%(mean_rating))\n        \n    def fit_predict(self,X,y=None):\n        return self.fit(X,y).predict(X)","ba0a84df":"# item based colaborative filtering\n# calculating the similarity between movies based on users' vote to the movies\nclass ItemBasedCF(RecommenderMixin):\n    def __init__(self,n_rec=10,n_sim=20,file_name=None,baseline=3.0):\n        super().__init__(n_rec=n_rec)\n        self.baseline=baseline\n        if file_name:\n            self.similar_matrix=sparse.load_npz(file_name)\n            self.n_sim=self.similar_matrix.shape[1]\n        else:\n            self.n_sim=n_sim\n            self.similar_matrix=None\n        \n    def save(self,file_name):\n        sparse.save_npz(file_name,self.similar_matrix)\n\n    def fit(self,X,y=None,silent=False):\n        if not silent:print(\"calculating the similar matrix...\")\n        n_users,n_items=X.shape\n        user_movie=X.copy()\n        user_movie.data-=self.baseline\n        norms=sparse.linalg.norm(user_movie,axis=0)\n        norms[norms==0]=1\n        user_movie*=sparse.diags(1\/norms)\n        movie_sim_matrix=user_movie.T*user_movie\n        if not silent:print(\"sparsifying similar matrix...\")\n        row=np.arange(n_items).repeat(self.n_sim)\n        col=np.array([np.argpartition(row.toarray().flatten(),-self.n_sim)[-self.n_sim:].copy() for row in movie_sim_matrix]).flatten()\n        self.similar_matrix=sparse.csr_matrix(sparse.coo_matrix((np.asarray(movie_sim_matrix[row,col]).flatten(),(row,col)),(n_items,n_items)))\n        if not silent:print(\"fitting done.\")\n        return self\n    \n    def transform(self,X,y=None):\n        user_movie=X.copy()\n        user_movie.data-=self.baseline\n        return user_movie*self.similar_matrix\n    \n    def predict(self,X):\n        watched_movies=X.copy()\n        watched_movies.data.fill(np.inf)\n        movie_favor=self.transform(X)-watched_movies\n        return np.array([np.argpartition(row.toarray().flatten(),-self.n_rec)[-self.n_rec:].copy() for row in movie_favor])\n    \n# itemCF=ItemBasedCF(file_name=\"ItemBasedCF.npz\")","5d495660":"# content based recommendation system\n# calculating the similarity between movies based their relevance to each tags\nclass ContentBasedRS(RecommenderMixin):\n    def __init__(self,n_rec=10,n_sim=100,file_name=None,baseline=3.0):\n        super().__init__(n_rec=n_rec)\n        self.baseline=baseline\n        if file_name:\n            self.similar_matrix=sparse.load_npz(file_name)\n            self.n_sim=self.similar_matrix.shape[1]\n        else:\n            self.n_sim=n_sim\n            self.similar_matrix=None\n        \n    def save(self,file_name):\n        sparse.save_npz(file_name,self.similar_matrix)\n\n    def fit(self,X,y=None,silent=False):\n        n_items,n_features=X.shape\n        if not silent:print(\"sparsifying relevance matrix...\")\n        row=np.arange(n_items).repeat(self.n_sim)\n        col=np.array([np.argpartition(row.toarray().flatten(),-self.n_sim)[-self.n_sim:].copy() for row in X]).flatten()\n        relevance_matrix=sparse.csr_matrix(sparse.coo_matrix((np.asarray(X[row,col]).flatten(),(row,col)),(n_items,n_features)))\n        if not silent:print(\"calculating the similar matrix...\")\n        movie_sim_matrix=relevance_matrix*relevance_matrix.T\n        if not silent:print(\"sparsifying similar matrix...\")\n        row=np.arange(n_items).repeat(self.n_sim)\n        col=np.array([np.argpartition(row.toarray().flatten(),-self.n_sim)[-self.n_sim:].copy() for row in movie_sim_matrix]).flatten()\n        self.similar_matrix=sparse.csr_matrix(sparse.coo_matrix((np.asarray(movie_sim_matrix[row,col]).flatten(),(row,col)),(n_items,n_items)))\n        if not silent:print(\"fitting done.\")\n        return self\n    \n    def transform(self,X,y=None):\n        user_movie=X.copy()\n        user_movie.data-=self.baseline\n        return user_movie*self.similar_matrix\n    \n    def predict(self,X):\n        watched_movies=X.copy()\n        watched_movies.data.fill(np.inf)\n        movie_favor=self.transform(X)-watched_movies\n        return np.array([np.argpartition(row.toarray().flatten(),-self.n_rec)[-self.n_rec:].copy() for row in movie_favor])\n    \n# contentRS=ContentBasedCF(file_name=\"ContentBasedRS.npz\")","f36ef2a7":"# fitting content based recommendation system\ncontentRS=ContentBasedRS()\n%time contentRS.fit(movie_tag_transformer.transform(movielens_score))\nif saving:contentRS.save(\"ContentBasedRS.npz\")","f402a981":"# fitting item based colaborative filtering\nitemCF=ItemBasedCF()\n%time itemCF.fit(user_movie_transformer.transform(movielens_rating))\nif saving:itemCF.save(\"ItemBasedCF.npz\")","c84fd672":"# example of the most similar movies to a specified movie\ndef movie_similar(rs,movie):\n    index=rs.similar_matrix[movie].indices\n    return pd.merge(movielens_movie.iloc[index].reset_index(drop=True),pd.DataFrame({\"similar\":rs.similar_matrix[movie,index].toarray().flatten()}),left_index=True,right_index=True).sort_values(\"similar\",ascending=False).reset_index(drop=True)\n    \nmovie_similar(itemCF,17506)","bf225813":"# example of movies recommended to a specified movie\ndef user_recommend(rs,user):\n    return movielens_movie.iloc[rs.predict(user_movie_transformer.transform(movielens_rating)[[user]]).flatten()].reset_index(drop=True)\n    \nuser_recommend(itemCF,5)","e889de59":"# see the already watched movies of a specified movie\ndef user_watched(user):\n    watched_movies=movielens_rating[movielens_rating.userId==user]\n    return pd.DataFrame({\"title\":movielens_movie.title.iloc[watched_movies.movieId],\"genres\":movielens_movie.genres.iloc[watched_movies.movieId],\"rating\":watched_movies.rating.values})\n\nuser_watched(5)","4ae97cc6":"# evaluating and comparing the recommendation system\nrating_train,rating_valid=train_test_split(movielens_rating[[\"userId\",\"movieId\",\"rating\"]])\nrating_train=user_movie_transformer.transform(rating_train)\nrating_valid=user_movie_transformer.transform(rating_valid)\n%time ItemBasedCF().fit(rating_train).evaluate(rating_train,rating_valid)\n%time ContentBasedRS().fit(movie_tag_transformer.transform(movielens_score)).evaluate(rating_train,rating_valid)","ca526d55":"data=tmdb_5000.dropna(subset=[\"vote_count\"]).copy()\ndata[\"release_year\"]=data.release_date.apply(lambda x:int(x[:4]))\ndata=data[data.vote_count>=100].copy()","68b52e7d":"# check multi-hot features\npd.DataFrame({\n    \"cast\":pd.Series(multi_count(tmdb_5000.cast)).describe(),\n    \"crew\":pd.Series(multi_count(tmdb_5000.crew)).describe(),\n    \"companies\":pd.Series(multi_count(tmdb_5000.production_companies)).describe(),\n    \"countries\":pd.Series(multi_count(tmdb_5000.production_countries,index=\"iso_3166_1\")).describe(),\n    \"languages\":pd.Series(multi_count(tmdb_5000.spoken_languages,index=\"iso_639_1\")).describe(),\n    \"keywords\":pd.Series(multi_count(tmdb_5000.keywords)).describe(),\n    \"genres\":pd.Series(multi_count(tmdb_5000.genres)).describe(),\n}).round(2).astype(object)","44424774":"# correlation between numeric features\ndef plot_corr(df,filename):\n    plt.subplots(figsize=(12, 9))\n    sns.heatmap(df.corr(),annot=True,linewidths=.5,annot_kws={\"fontsize\":15})\n    plt.yticks(rotation=0,fontsize=15)\n    plt.xticks(rotation=0,fontsize=15)\n    if saving:plt.savefig(filename,dpi=150)\n    plt.show()\n\nplot_corr(data[[\"vote_average\",\"budget\",\"popularity\",\"revenue\",\"runtime\",\"release_year\"]],filename=\"corr.png\")","09521f38":"# see the scatter of features\ndef plotscatter(df,x,y,hue,filename,xlim=None,ylim=None,xscale=None,yscale=None):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 15))\n    sns.scatterplot(data=expand_multi_feature(df,hue,\"name\"),x=x,y=y,hue=hue)\n    plt.yticks(fontsize=20)\n    plt.xticks(fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if xlim:plt.xlim(xlim)\n    if xscale:plt.xscale(xscale)\n    if ylim:plt.ylim(ylim)\n    if yscale:plt.yscale(yscale)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n\ndef plotjoint(df,x,y,filename,xlim=None,ylim=None,xscale=None,yscale=None):\n    sns.set(style=None,font_scale=2)\n    grid=sns.jointplot(data[x],data[y],kind=\"hex\",color=\"#4CB391\",height=15,ratio=10,xlim=xlim,ylim=ylim)\n    if saving:grid.savefig(filename)\n        \n    \nplotscatter(data,\"revenue\",\"vote_average\",\"genres\",xlim=1e6,xscale=\"log\",filename=\"scatter_revenue_vote.png\")\nplotscatter(data,\"popularity\",\"vote_average\",\"genres\",xlim=(-0.5,100),filename=\"scatter_popularity_vote.png\")\nplotscatter(data,\"runtime\",\"vote_average\",\"genres\",filename=\"scatter_runtime_vote.png\")\nplotjoint(data,\"release_year\",\"vote_average\",filename=\"scatter_year_vote.png\",ylim=(4,8.5))\n\nplotscatter(data,\"budget\",\"revenue\",\"genres\",xlim=1e6,ylim=1e6,xscale=\"log\",yscale=\"log\",filename=\"scatter_budget_revenue.png\")\nplotscatter(data,\"popularity\",\"revenue\",\"genres\",xlim=(0,100),ylim=1e5,yscale=\"log\",filename=\"scatter_popularity_revenue.png\")","0fb8b99d":"# selected features\nlabel_name=\"vote_average\"\nnumeric_features=[\"budget\",\"runtime\",\"popularity\",\"revenue\",\"release_year\"]\nmulti_hot_features=[\"genres\"]","4c11ca58":"# define transformers\nclass MultiHotEncoder(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        self.categories=[]\n    \n    def fit(self,X,y=None):\n        self.categories=[]\n        for name,series in X.iteritems():\n            df=to_split(series).str.get_dummies()\n            df.columns=name+\"_\"+df.columns\n            self.categories+=list(df.columns)\n        return self\n    \n    def transform(self,X,y=None):\n        result=pd.DataFrame(index=X.index)\n        for name,series in X.iteritems():\n            df=to_split(series).str.get_dummies()\n            df.columns=name+\"_\"+df.columns\n            result=result.join(df)\n        for i,cate in enumerate(self.categories):\n            if result.columns[i]!=cate:\n                result.insert(i,cate,0)\n        return result.reset_index(drop=True)\n\n    \ndef to_split(series,sep=\"|\",index=\"name\",threshold=0):\n    count=multi_count(series,index)\n    result=pd.Series(index=series.index)\n    for i in range(series.size):\n        l=eval(series.iloc[i])\n        result.iloc[i]=\"\"\n        for x in l:\n            if count[x[index]]>=threshold:\n                result.iloc[i]+=x[index]+sep\n    return result\n\nclass DataFrameTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self,transformers):\n        self.transformers=transformers\n        \n    def fit(self,X,y=None):\n        for i in self.transformers:\n            if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n        return self\n    \n    def transform(self,X,y=None):\n        result=pd.DataFrame(index=range(X.shape[0]))\n        for i in self.transformers:\n            if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n            elif i[1]==\"drop\":continue\n            else:x=i[1].transform(X[i[2]])\n            if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n            result=result.join(x)\n        return result","a1c2530e":"preprocessor=DataFrameTransformer([\n    (\"numeric\",\"passthrough\",numeric_features),\n])\nxgb_params={\"silent\":True,\"eta\":0.01,'subsample': 0.8,'colsample_bytree': 0.8,\"max_depth\":5}\ndmat=xgb.DMatrix(preprocessor.fit_transform(data),data[label_name])\nxgb.cv(xgb_params,dmat,num_boost_round=20000,early_stopping_rounds=500,nfold=4,seed=42,feval=lambda preds,dmat:(\"R2score\",sklearn.metrics.r2_score(dmat.get_label(),preds))).iloc[[-1]]","9752ce62":"preprocessor=DataFrameTransformer([\n    (\"numeric\",\"passthrough\",numeric_features),\n    (\"multihot\",MultiHotEncoder(),multi_hot_features),\n])","90c850de":"xgb_params={\"silent\":True,\"eta\":0.01,'subsample': 0.8,'colsample_bytree': 0.7,\"max_depth\":7}\ndmat=xgb.DMatrix(preprocessor.fit_transform(data),data[label_name])\nresult=xgb.cv(xgb_params,dmat,num_boost_round=20000,early_stopping_rounds=500,nfold=4,seed=42,feval=lambda preds,dmat:(\"R2score\",sklearn.metrics.r2_score(dmat.get_label(),preds)))\nresult.iloc[[-1]]","c894ddd7":"X=preprocessor.fit_transform(data)\ny=data[label_name]\ntrain_X,valid_X,train_y,valid_y=train_test_split(X,y)\ntrain_mat=xgb.DMatrix(train_X,train_y)\nvalid_mat=xgb.DMatrix(valid_X,valid_y)\nmodel=xgb.train(xgb_params,train_mat,num_boost_round=result.shape[0])","091ccd6d":"xgb.to_graphviz(model)","2293a828":"def plot_importance(model,filename,importance_type=\"weight\"):\n    f,ax=plt.subplots(figsize=(10,10))\n    xgb.plot_importance(model,height=0.5,ax=ax,importance_type=importance_type)\n    if saving:plt.savefig(filename,dpi=150,bbox_inches=\"tight\")\n    plt.show()\n    \nplot_importance(model,importance_type=\"gain\",filename=\"boost_tree_importance.png\")","abeb324f":"## Prediction","ca827fd7":"Let's predict the average vote of the movies.","fdb6be80":"## Recommendation","2c10e9b1":"## Visualization","7c53a275":"# Movie Visualization, Recommendation, Prediction"}}