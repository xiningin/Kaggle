{"cell_type":{"c61e2658":"code","520fec3e":"code","9358b773":"code","a18e2919":"code","576b709d":"code","5fe27237":"code","33913325":"code","5e99f67a":"code","7f831276":"code","dc2c7cf2":"code","672baccb":"code","5de35738":"code","5fc68920":"code","ceeb913f":"code","df080fa8":"code","cb97ec8c":"code","26e6851e":"code","eecf8e0a":"code","ff998ec4":"code","3530d224":"code","7f68823f":"code","68d939d5":"code","0d49b045":"code","4e81ccaf":"code","cfae969e":"code","f52e712f":"code","96e8d7ef":"code","48cae05d":"code","676fac3e":"code","9d06c790":"code","60a3b9a2":"code","618db294":"code","9b6d5801":"code","82ef6a4c":"code","cbb163a2":"code","63bd6ada":"code","e8c843b0":"code","6cd5b759":"code","b992a120":"code","e96e00b1":"code","c1e9cd9c":"code","922e6706":"code","4d3f1288":"code","d3d8c046":"code","9197d103":"code","c38efd96":"code","d354640f":"code","529d72db":"code","860914ba":"code","20d52b58":"code","fcd908a3":"code","41048e79":"code","34f8c994":"code","d96f29b8":"code","d114834a":"code","874ff9ad":"code","e55a991c":"code","13c80c91":"code","9e3bad3a":"code","71f0fb29":"code","033bb1d1":"code","2015be39":"code","0f709a90":"code","9055064c":"markdown","a8354fad":"markdown","d03f8fff":"markdown","632b8977":"markdown","2a5f48fa":"markdown","267bfb12":"markdown","b7bc7a2f":"markdown","b9c760f5":"markdown","14db6516":"markdown","aa845770":"markdown","4d012758":"markdown","12a94232":"markdown","16aaa3b9":"markdown","ecc708df":"markdown","d496b693":"markdown","76376f15":"markdown","61fdb47c":"markdown","ce95ba87":"markdown","1873bc1b":"markdown","1933cdef":"markdown","2caa0461":"markdown","4aa64e35":"markdown","308507dc":"markdown","5f44f465":"markdown","26483bbe":"markdown","a4edeea4":"markdown","ee8417b6":"markdown","cb4ad3a6":"markdown","3fc3c752":"markdown","8ef43e28":"markdown"},"source":{"c61e2658":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","520fec3e":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)  \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline \nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')","9358b773":"data=pd.read_csv('\/kaggle\/input\/food-demand-forecasting\/train.csv')\ncenter=pd.read_csv('\/kaggle\/input\/food-demand-forecasting\/fulfilment_center_info.csv')\nmeal=pd.read_csv('\/kaggle\/input\/food-demand-forecasting\/meal_info.csv')\ntest=pd.read_csv('\/kaggle\/input\/food-demand-forecasting\/test.csv')\n#\/kaggle\/input\/food-demand-forecasting\/sample_submission.csv\n","a18e2919":"print(\"The Shape of Demand dataset :\",data.shape)\nprint(\"The Shape of Fulmilment Center Information dataset :\",center.shape)\nprint(\"The Shape of Meal information dataset :\",meal.shape)\nprint(\"The Shape of Test dataset :\",test.shape)\n","576b709d":"data.head()\n","5fe27237":"test['num_orders']=123456 ### Assigning random number for Target Variable of Test Data.\n","33913325":"test.head()\n","5e99f67a":"center.head()\n","7f831276":"meal.head()\n","dc2c7cf2":"data=pd.concat([data,test],axis=0)\n","672baccb":"data=data.merge(center,on='center_id',how='left')\n","5de35738":"data=data.merge(meal,on='meal_id',how='left')\n","5fc68920":"data.isnull().sum()\n","ceeb913f":"#Discount Amount\ndata['discount amount']=data['base_price']-data['checkout_price']\n\n#Discount Percent\ndata['discount percent'] = ((data['base_price']-data['checkout_price'])\/data['base_price'])*100\n\n#Discount Y\/N\ndata['discount y\/n'] = [1 if x>0 else 0 for x in (data['base_price']-data['checkout_price'])]\n\ndata=data.sort_values(['center_id', 'meal_id', 'week']).reset_index()\n\n#Compare Week Price\ndata['compare_week_price'] = data['checkout_price'] - data['checkout_price'].shift(1)\n","df080fa8":"data['compare_week_price'][data['week']==1]=0\n","cb97ec8c":"data=data.sort_values(by='index').reset_index().drop(['level_0','index'],axis=1)\n","26e6851e":"# Compare Week Price Y\/N\ndata['compare_week_price y\/n'] = [1 if x>0 else 0 for x in data['compare_week_price']]\n","eecf8e0a":"data.head()","ff998ec4":"data.isnull().sum()","3530d224":"train=data[data['week'].isin(range(1,146))]\ntest=data[data['week'].isin(range(146,156))]\n","7f68823f":"print(\"The Shape of Train dataset :\",train.shape)\nprint(\"The Shape of Test dataset :\",test.shape)\n","68d939d5":"plt.figure(figsize=(16,14))\nsns.heatmap(train.corr(),annot=True, square=True, cmap='Reds')\n","0d49b045":"fig=plt.figure(figsize=(4,7))\nplt.title('Total No. of Orders for Each Center type',fontdict={'fontsize':13})\nsns.barplot(y='num_orders', x='center_type', data=train.groupby('center_type').sum()['num_orders'].reset_index(),palette='autumn');\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Center Type',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","4e81ccaf":"train['center_id'].nunique()\n","cfae969e":"fig=plt.figure(figsize=(10,8))\nplt.title('Top 20 Centers with Highest No. of Orders',fontdict={'fontsize':14})\nsns.barplot(y='num_orders', x='center_id', data=train.groupby(['center_id','center_type']).num_orders.sum().sort_values(ascending=False).reset_index().head(20),palette='YlOrRd_r',order=list(train.groupby(['center_id','center_type']).num_orders.sum().sort_values(ascending=False).reset_index().head(20)['center_id']));\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Center ID',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","f52e712f":"fig=plt.figure(figsize=(4,7))\nplt.title('Total No. of Centers under Each Center type',fontdict={'fontsize':13})\nsns.barplot(y=train.groupby(['center_id','center_type']).num_orders.sum().reset_index()['center_type'].value_counts(), x=train.groupby(['center_id','center_type']).num_orders.sum().reset_index()['center_type'].value_counts().index,palette='autumn');\nplt.ylabel('No. of Centers',fontdict={'fontsize':12})\nplt.xlabel('Center Type',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","96e8d7ef":"sns.set_style(\"white\")\nplt.figure(figsize=(8,8))\nsns.scatterplot(y=train['base_price']-train['checkout_price'],x=train['num_orders'],color='coral')\nplt.ylabel('Discount (Base price - Checkout Price)')\nsns.despine(bottom = True, left = True)\n","48cae05d":"fig=plt.figure(figsize=(16,7))\nsns.set_style(\"whitegrid\")\nplt.title('Pattern of Orders',fontdict={'fontsize':14})\n\nsns.pointplot(x=train.groupby('week').sum().reset_index()['week'],y=train.groupby('week').sum().reset_index()['num_orders'],color='coral')\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Week',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","676fac3e":"plt.figure(figsize=(6,6))\ncolors = ['coral','#FFDAB9','yellowgreen','#6495ED']\nplt.pie(train.groupby(['cuisine']).num_orders.sum(),\n    labels=train.groupby(['cuisine']).num_orders.sum().index,\n    shadow=False,\n    colors=colors,\n    explode=(0.05, 0.05, 0.03,0.05),\n    startangle=90, \n    autopct='%1.1f%%',pctdistance=0.6,\n    textprops={'fontsize': 12})\nplt.title('Total Number of Orders for Each Category')\nplt.tight_layout()\nplt.show()\n","9d06c790":"fig=plt.figure(figsize=(11,8))\nsns.set_style(\"white\")\n\nplt.xticks(rotation=90,fontsize=12)\nplt.title('Total Number of Orders for Each Category',fontdict={'fontsize':14})\nsns.barplot(y='num_orders', x='category', data=train.groupby('category').num_orders.sum().sort_values(ascending=False).reset_index(),palette='YlOrRd_r');\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Category',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","60a3b9a2":"fig=plt.figure(figsize=(18,8))\nsns.set_style(\"white\")\nplt.xticks(rotation=90,fontsize=12)\nplt.title('Total Number of Orders for Each Cuisine-Category',fontdict={'fontsize':14})\n\nsns.barplot(x='category',y='num_orders',data=train.groupby(['cuisine','category']).sum().sort_values(by='num_orders', ascending=False).reset_index(),hue='cuisine',palette='YlOrRd_r')\n\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Cuisine-Category',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","618db294":"list(data.groupby('region_code').num_orders.sum().sort_values(ascending=False).reset_index().values[:,0])\n","9b6d5801":"fig=plt.figure(figsize=(8,8))\nsns.set_style(\"white\")\nplt.xticks(fontsize=13)\nplt.title('Total Number of Orders for Each Region',fontdict={'fontsize':14})\nsns.barplot(y='num_orders', x='region_code', data=data.groupby('region_code').num_orders.sum().sort_values(ascending=False).reset_index(),palette='YlOrRd_r',order=list(data.groupby('region_code').num_orders.sum().sort_values(ascending=False).reset_index().values[:,0]));\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Region',fontdict={'fontsize':12})\nplt.xticks()\nsns.despine(bottom = True, left = True);\n","82ef6a4c":"fig=plt.figure(figsize=(18,8))\nsns.set_style(\"white\")\nplt.xticks(rotation=90,fontsize=13)\nplt.title('Total Number of Orders for each Meal ID',fontdict={'fontsize':14})\nsns.barplot(y='num_orders', x='meal_id', data=data.groupby('meal_id').num_orders.sum().sort_values(ascending=False).reset_index(),palette='YlOrRd_r',order=list(data.groupby('meal_id').num_orders.sum().sort_values(ascending=False).reset_index()['meal_id'].values));\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('Meal',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","cbb163a2":"fig=plt.figure(figsize=(18,8))\nsns.set_style(\"white\")\nplt.xticks(rotation=90,fontsize=13)\nplt.title('Total Number of Orders for each City',fontdict={'fontsize':14})\nsns.barplot(y='num_orders', x='city_code', data=train.groupby('city_code').num_orders.sum().sort_values(ascending=False).reset_index(),palette='YlOrRd_r',order=list(train.groupby('city_code').num_orders.sum().sort_values(ascending=False).reset_index()['city_code'].values));\nplt.ylabel('No. of Orders',fontdict={'fontsize':12})\nplt.xlabel('City',fontdict={'fontsize':12})\nsns.despine(bottom = True, left = True);\n","63bd6ada":"city4={590:'CH1', 526:'CH2', 638:'CH3'}\ndata['city_enc_4']=data['city_code'].map(city4)\ndata['city_enc_4']=data['city_enc_4'].fillna('CH4')\n","e8c843b0":"data['city_enc_4'].value_counts()\n","6cd5b759":"data.head()\n","b992a120":"data.isnull().sum()","e96e00b1":"datax=data.copy()\ndatax.head()\n","c1e9cd9c":"datax['center_id']=datax['center_id'].astype('object')\ndatax['meal_id']=datax['meal_id'].astype('object')\ndatax['region_code']=datax['region_code'].astype('object')\n","922e6706":"obj=datax[['center_id','meal_id','region_code','center_type','category','cuisine','city_enc_4']]\nnum=datax.drop(['center_id','meal_id','region_code','center_type','category','cuisine','city_enc_4'],axis=1)\n","4d3f1288":"encode1=pd.get_dummies(obj,drop_first = True)\n","d3d8c046":"datax=pd.concat([num,encode1],axis=1)\n","9197d103":"datax.head()\n","c38efd96":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n","d354640f":"train=datax[datax['week'].isin(range(1,136))]\ntest=datax[datax['week'].isin(range(136,146))]\n\nX_train=train.drop(['id','num_orders','week'],axis=1)\ny_train=train['num_orders']\n\nX_test=test.drop(['id','num_orders','week'],axis=1)\ny_test=test['num_orders']\n\nreg = LinearRegression()\nreg.fit(X_train,y_train)\nprint('Train Score :',reg.score(X_train,y_train))\nprint('Test Score :',reg.score(X_test,y_test))\n\ny_pred = reg.predict(X_test)\nprint('R squared :',(r2_score(y_test,y_pred)))\nprint('RMSE :',np.sqrt(mean_squared_error(y_test,y_pred)))\n","529d72db":"sc=StandardScaler()\n\ncat=datax.drop(['checkout_price','base_price','discount amount','discount percent','compare_week_price'],axis=1)\nnum=datax[['checkout_price','base_price','discount amount','discount percent','compare_week_price']]\nscal= pd.DataFrame(sc.fit_transform(num),columns=num.columns)\ndatas=pd.concat([scal,cat],axis=1)\n\ntrain=datas[datas['week'].isin(range(1,136))]\ntest=datas[datas['week'].isin(range(136,146))]\n\n\nX_train=train.drop(['id','num_orders','week'],axis=1)\ny_train=np.log(train['num_orders']) #Applying Log Transformation on the Target Feature\n\nX_test=test.drop(['id','num_orders','week'],axis=1)\ny_test=np.log(test['num_orders']) #Applying Log Transformation on the Target Feature\n\nreg = LinearRegression()\nreg.fit(X_train,y_train)\nprint('Train Score :',reg.score(X_train,y_train))\nprint('Test Score :',reg.score(X_test,y_test))\n\ny_pred = reg.predict(X_test)\nprint('R squared :',(r2_score(y_test,y_pred)))\nprint('RMSLE :',np.sqrt(mean_squared_error(y_test,y_pred)))\n","860914ba":"datay=datas.copy()\n","20d52b58":"datay['Quarter']=(datas['week']\/13).astype('int64')\n","fcd908a3":"datay['Quarter'] = datay['Quarter'].map({0:'Q1',\n                         1:'Q2',\n                         2:'Q3',\n                         3:'Q4',\n                         4:'Q1',\n                         5:'Q2',\n                         6:'Q3',\n                         7:'Q4',\n                         8:'Q1',\n                         9:'Q2',\n                         10:'Q3',\n                         11:'Q4'})","41048e79":"datay['Quarter'].value_counts()\n","34f8c994":"datay['Year']=(datas['week']\/52).astype('int64')\n","d96f29b8":"datay['Year'] = datay['Year'].map({0:'Y1',\n                         1:'Y2',\n                         2:'Y3'})","d114834a":"objy=datay[['Quarter', 'Year']]\nnumy=datay.drop(['Quarter', 'Year'],axis=1)\n\nencode1y=pd.get_dummies(objy,drop_first = True)\nencode1y.head()\n\ndatay=pd.concat([numy,encode1y],axis=1)\n","874ff9ad":"datay.head()","e55a991c":"datay['num_orders']=np.log1p(datay['num_orders'])\n","13c80c91":"train=datay[datay['week'].isin(range(1,146))]\ndef outliers_3(col):\n    q3=round(train[col].quantile(0.75),6)\n    q1=round(train[col].quantile(0.25),6)\n    iqr=q3-q1\n    lw = q1 - (3*iqr)\n    hw = q3 + (3*iqr)  \n    uo=train[train[col]>hw].shape[0]\n    lo=train[train[col]<lw].shape[0]\n    print('Number of Upper Outliers :',uo)\n    print('Number of Lower Outliers :',lo)\n    print('Percentage of Outliers :',((uo+lo)\/train.shape[0])*100)\n","9e3bad3a":"\noutliers_3('num_orders')\n","71f0fb29":"datay.head()","033bb1d1":"train=datay[datay['week'].isin(range(1,136))]\ntest=datay[datay['week'].isin(range(136,146))]\n\n\nX_train=train.drop(['id','num_orders','week','discount amount','city_code'],axis=1)\ny_train=train['num_orders']\n\nX_test=test.drop(['id','num_orders','week','discount amount','city_code'],axis=1)\ny_test=test['num_orders'] \n\nreg = LinearRegression()\nreg.fit(X_train,y_train)\nprint('Train Score :',reg.score(X_train,y_train))\nprint('Test Score :',reg.score(X_test,y_test))\n\npredictions = reg.predict(X_test)\nprint('R squared :',(r2_score(y_test,y_pred)))\nprint('RMSLE :',np.sqrt(mean_squared_error(y_test,y_pred)))\n","2015be39":"Result=pd.DataFrame(predictions)\nResult=np.expm1(Result).astype('int64')\nSubmission = pd.DataFrame(columns=['id', 'num_orders'])\nSubmission['id'] = test['id']\nSubmission['num_orders'] = Result.values\nSubmission.to_csv('My submission.csv', index=False)\nprint(\"Your submission was successfully saved\")\n","0f709a90":"Submission.head()","9055064c":"Type_A has the most number of orders because, Type_A has the most number of Centers - 43 Centers.\n\n","a8354fad":"We could see that Beverages are the food category which has the higest number of orders and Biriyani is the food category with least number of orders.\n\n","d03f8fff":"**Encoding All Categorical Features**","632b8977":"# **Base Model**\n\nBuilding base model by splitting the last 10 week of the train dataset as test.\n","2a5f48fa":"# **Feature Engineering**\n\nFeature engineering is the process of using domain knowledge of the data to create features that improves the performance of the machine learning models.\n\nWith the given data, We have derived the below features to improve our model performance.\n\n* Discount Amount : This defines the difference between the \u201cbase_Price\u201d and \u201ccheckout_price\u201d.\n* Discount Percent : This defines the % discount offer to customer.\n* Discount Y\/N : This defines whether Discount is provided or not - 1 if there is Discount and 0 if there is no Discount.\n* Compare Week Price : This defines the increase \/ decrease in price of a Meal for a particular center compared to the previous week.\n* Compare Week Price Y\/N : Price increased or decreased - 1 if the Price increased and 0 if the price decreased compared to the previous week.\n* Quarter : Based on the given number of weeks, derived a new feature named as Quarter which defines the Quarter of the year.\n* Year : Based on the given number of weeks, derived a new feature named as Year which defines the Year.\n\n","267bfb12":"**Train Test Split**","b7bc7a2f":"# **Data Transformation**\n\n* Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n* In our data, the target variable \u2018num_orders\u2019 is not normally distributed. Using this without applying any transformation techniques will downgrade the performance of our model.\n* Therefore, we have applied Logarithm transformation on our Target feature \u2018num_orders\u2019 post which the data seems to be more approximate to normal distribution.\n* After Log transformation, We have observed 0% of Outlier data being present within the Target Variable \u2013 num_orders using 3 IQR Method.\n\n\n# **Evaluation Metric**\n\nThe evaluation metric for this competition is 100*RMSLE where RMSLE is Root of Mean Squared Logarithmic Error across all entries in the test set.\n\n# **Approach** \n\n* Simple Linear Regression model without any feature engineering and data transformation which gave a RMSE : 194.402\n* Without feature engineering and data transformation, the model did not perform well and could'nt give a good score.\n* Post applying feature engineering and data transformation (log and log1p transformation), Linear Regression model gave a   RMSLE score of 0.634.","b9c760f5":"**Copying to New DataFrame**","14db6516":"**Data Preprocessing**","aa845770":"**Deriving New Features**","4d012758":"# **Data Pre-Processing**\n\n* There are no Missing\/Null Values in any of the three datasets.\n* Before proceeding with the prediction process, all the three datasheets need to be merged into a single dataset. Before performing the merging operation, primary feature for combining the datasets needs to be validated.\n* The number of Center IDs in train dataset is matching with the number of Center IDs in the Centers Dataset i.e 77 unique records. Hence, there won't be any missing values while merging the datasets together.\n* The number of Meal IDs in train dataset is matching with the number of Meal IDs in the Meals Dataset i.e 51 unique records. Hence, there won't be any missing values while merging the datasets together.\n* As checked earlier, there were no Null\/Missing values even after merging the datasets.\n","12a94232":"When we analysed the trend of order placed over the weeks, we could see that the highest number of orders were received in week 48 and the lowest in week 62.\n\n","16aaa3b9":"#  **Problem Statement**\n\nThe data set is related to a meal delivery company which operates in multiple cities. They have various fulfilment centers in these cities for dispatching meal orders to their customers.\nThe dataset consists of historical data of demand for a product-center combination for weeks 1 to 145.\nWith the given data and information, the task is to predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations, so that these fulfilment centers stock the necessary raw materials accordingly.","ecc708df":"Italian Cuisine has the highest number of orders with Continental cuisine being the least.\n\n","d496b693":"The are are 77 Fullfilment Centers in total.\n\n","76376f15":"Also when we checked the number of orders with respect to City, we could see that City - 590 has the highest number of orders - 18.5M orders which is almost 10M orders higher than the City with second highest number of orders - City 526 - 8.6M orders.\n\n","61fdb47c":"Initially, when we checked, which Center Type has the highest number of Orders, We found that Center Type_A has the highest number of orders, but now when we check individually, we could see that Center 13 of Type_B has the highest number of Orders. Let\u2019s analyze the reason behind that.\n\n","ce95ba87":"We created a new feature: Discount which is the difference of base price and checkout price and tried to find out if there is any relationship between the discount and the number of orders. But surprisingly there are no good correlation between the discount and the number of orders.\n\n","1873bc1b":"\n**Demand forecasting is a key component to every growing online business. Without proper demand forecasting processes in place, it can be nearly impossible to have the right amount of stock on hand at any given time. A food delivery service has to deal with a lot of perishable raw materials which makes it all the more important for such a company to accurately forecast daily and weekly demand.**\n\n\n**Too much inventory in the warehouse means more risk of wastage, and not enough could lead to out-of-stocks \u2014 and push customers to seek solutions from your competitors. In this challenge, get a taste of demand forecasting challenge using a real dataset.**","1933cdef":"# **Libraries Used**\n\npandas, numpy, scikit learn, matplotlib, seaborn, xgboost, lightgbm, catboost\n\n","2caa0461":"Similary when we checked which specific cuisne-food category has the highest number of orders, we could see that Indian-Rice Bowl has the highest number of orders and Indian-Biriyani has the least.\n\n","4aa64e35":"# **Data Dictionary**\n\nThe dataset consists of three individual datasheets, the first dataset contains the historical demand data for all centers, the second dataset contains the information of each fulfillment center and the third dataset contains the meal information.\n\nWeekly Demand data (train.csv):\nContains the historical demand data for all centers. The Train dataset consists of 9 variables and records of 423727 unique orders. test.csv contains all the following features except the target variable. The Test dataset consists of 8 variables and records of 32573 unique orders.\n\nfulfilment_center_info.csv:\nContains information for each fulfilment center. The dataset consists of 5 variables and records of 77 unique fulfillment centers.\n\nmeal_info.csv:\nContains information for each meal being served\n\n\n\n","308507dc":"Linear Model 2 : Applying Standard Scaling & Log Transformation\n","5f44f465":"Meal ID 2290 has the higest number of Orders. There is not much significant differences between number of orders for different Meal IDs.\n\n","26483bbe":"# ****Business Benefits****\nThe replenishment of raw materials is done only on weekly basis and since the raw material is perishable, the procurement planning is of utmost importance.\nTherefore predicting the Demand helps in reducing the wastage of raw materials which would result in the reduced cost of operation. Increased customer satisfaction by timely fulfilling their expectations and requirements.\n\n","a4edeea4":"Also when we checked the number of orders with respect to Region, we could see that Region - 56 has the highest number of orders - 60.5M orders which is almost 35M orders higher than the Region with second highest number of orders - Region 34 - 24M orders.\n\n","ee8417b6":"# **Encoding City**\n\nAs per our observation from our barchart of the City against the number of orders. There the high significant difference between the Top 3 cities which has the highest number of orders. Therefore, in our first approach we will encode the City with Highest No. of Orders as CH1, City with 2nd Highest No. of Orders as CH2 and City with 3rd Highest No. of Orders as CH3 and rest all of the cities which does not have much significant differences between the number of orders as CH4.\n\n","cb4ad3a6":"**Copying to New DataFrame**","3fc3c752":"**Applying Log Transformation on the Target Feature**","8ef43e28":"Type A has the highest number of Orders placed and Type C has lowest"}}