{"cell_type":{"f5f8432a":"code","04ccb131":"code","b4700c76":"code","7595af54":"code","e0bdddec":"code","aaee3ab3":"code","1d68d49f":"code","ac556104":"code","30bbdbda":"code","e58fff07":"code","a3d9a42c":"code","757c2c57":"code","beaf6740":"code","95d9e437":"code","3a1fa4d4":"code","a184a799":"code","56f9aa82":"code","22b0f8a8":"code","32a9e2d6":"code","293e828d":"code","fc8c3ba2":"code","ce41525f":"code","1b15ca8a":"code","a1dd6bed":"code","10fcef8a":"code","6c1e2fa5":"code","d5d7550b":"code","ef3904ae":"code","982484bb":"code","f7f83893":"code","6c8a931c":"code","9ca40748":"code","ff81554e":"code","cddcaa3a":"code","5cdb4ee5":"code","d4a5a7d3":"code","6f6c10d1":"code","e1a370a9":"code","d69cd496":"code","4c4a1bac":"code","de68fc74":"code","13bdaf08":"code","1b027424":"code","9dd6a9ca":"code","2fb4c368":"code","b408c2c7":"code","78a3cb1e":"code","4e74112a":"code","ab81eedf":"code","571ed7e3":"code","c694f9a3":"code","ea3b5655":"code","c8c0f139":"code","b1a92e2e":"code","95d51b5d":"code","68fde29f":"code","feced782":"code","ba016391":"code","ce9c66f4":"code","4777a04f":"code","8a5f82b2":"code","50860da3":"code","933f7182":"code","7e0a487e":"code","ec599876":"code","d1a7e3ec":"code","22c75fe7":"code","74611bed":"code","39e05605":"code","51cac226":"code","faa6e95e":"code","3e870f46":"code","72775cd7":"code","997c42bc":"code","d97aba42":"code","345311ac":"code","0c87b337":"code","d06e6635":"code","f17daf2c":"code","571cf0c9":"code","6b1724b1":"code","83860508":"code","664cace4":"code","6ccad6b1":"markdown","4b937a4d":"markdown","d9d0202e":"markdown","e99c727c":"markdown","31f2234b":"markdown","1e8ddeef":"markdown","26ff2f9a":"markdown","e1d2b34a":"markdown","56e77e07":"markdown","f9597483":"markdown","78d363c4":"markdown","2d6f0f13":"markdown","ea21f585":"markdown","8f6b6d50":"markdown","e09fc66c":"markdown","079632c7":"markdown","f9a0890f":"markdown","a962976e":"markdown","49bf9a93":"markdown","f1f5ef03":"markdown","6a39c82f":"markdown","44809232":"markdown","874f8897":"markdown","a04ded3b":"markdown","ee9b4fec":"markdown","62e119e3":"markdown","63d12617":"markdown","2b030d1e":"markdown","3a56fa61":"markdown","c00beb53":"markdown","d7ef8643":"markdown","63f6e821":"markdown","31f2dec7":"markdown","5ef1348d":"markdown","9a9fdba8":"markdown","5895501b":"markdown","edb84039":"markdown","e1c12304":"markdown","b5a624cd":"markdown","117a4b62":"markdown","21af6d9d":"markdown","84193561":"markdown","d8b8c53b":"markdown","17142527":"markdown","973cc05b":"markdown","bd5f6dea":"markdown","acd074d9":"markdown","f057ce7f":"markdown","53c77943":"markdown"},"source":{"f5f8432a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04ccb131":"# Importing csv file\ndf=pd.read_csv(\"..\/input\/global-superstore\/GlobalSuperstoreData.csv\")\ndf.head()","b4700c76":"# To know the shape of the data\ndf.shape","7595af54":"\n# To know the  stastical datset\ndf.describe()","e0bdddec":"\n# to know the datatypes of the column\ndf.info()","aaee3ab3":"# Converting order date to time format \ndf['Order Date'] = pd.to_datetime(df['Order Date'], format='%d-%m-%Y')\n# Introducing the month feature to aggregte the values month-wise later\ndf['Month'] = df['Order Date'].dt.strftime('%Y-%m')\ndf['Month'] = pd.to_datetime(df['Month'],format=\"%Y-%m\")\n# indexing to Order date\ndf = df.set_index('Order Date')\n\ndf.head(2)","1d68d49f":"# Create new column Seg_mark\ndf[\"Seg_mark\"]=df[\"Market\"].str.cat(df[\"Segment\"],sep=\" \")","ac556104":"df.head()","30bbdbda":"df[\"Seg_mark\"].value_counts()","e58fff07":"# Droping both Segment and Market column \ndf.drop([\"Segment\"],inplace=True,axis=1)\ndf.drop([\"Market\"],inplace=True,axis=1)","a3d9a42c":"df.head(2)","757c2c57":"# Grouping by Market-Segment to calculate total profit for each market segment\ndf_grouped = df.groupby(['Seg_mark'])['Profit'].sum()\ndf_grouped = pd.DataFrame(df_grouped)\ndf_grouped","beaf6740":"# Lets see it the other way\npivot = pd.pivot_table(df,index = 'Month', columns = 'Seg_mark', values = 'Profit', aggfunc = 'sum')\npivot\n","95d9e437":"# Splitting train and test \ntrain_pivot = pivot[:42]\ntest_pivot = pivot[42:]","3a1fa4d4":"### Finding the most profitable Market Segment using Coeffecient of Variation\n\nCoV = train_pivot.std(axis = 0)\/train_pivot.mean(axis = 0)\nCoV = pd.DataFrame(CoV, columns = ['CoV'])\nCoV = CoV.sort_values(by = 'CoV')\nCoV\n\n\n\n\n\n","a184a799":"# Filtering the Seg_mark for APAC Consumer5\ndf = df.loc[df['Seg_mark'] == 'APAC Consumer']\ndf.head()","56f9aa82":"#Preperation of dataset with month and sales colmumn\ndf = df.groupby('Month')['Sales'].sum()\ndf = pd.DataFrame(df)\ndf","22b0f8a8":"df.shape","32a9e2d6":"#Splitting train and test datset\ntrain_len = 42\ntrain = df[0:train_len] # first 42 months as training set\ntest = df[train_len:]","293e828d":"# reading the train dataset\ntrain.head()","fc8c3ba2":"#Shape of train datset\ntrain.shape","ce41525f":"#Shape of test dataset\ntest.shape\n","1b15ca8a":"import matplotlib.pyplot as plt\nimport seaborn as sns","a1dd6bed":"# Lets try plotting the series first to see the possible patterns like trend and seasonality\n\ndf.plot(figsize = [14,7])\nplt.legend(loc = 'best')\nplt.title('Retail Gaint Sales')\nplt.show(block=False)","10fcef8a":"### Outlier detection\n\nimport seaborn as sns\nfig = plt.subplots(figsize=(12, 2))\nax = sns.boxplot(x=df['Sales'],whis=1.5)","6c1e2fa5":"fig = df.Sales.hist(figsize = (12,4))","d5d7550b":"from pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 12, 8\ndecomposition = sm.tsa.seasonal_decompose(df.Sales, model='additive') # additive seasonal index\nfig = decomposition.plot()\nplt.show()","ef3904ae":"decomposition = sm.tsa.seasonal_decompose(df.Sales, model='multiplicative') # multiplicative seasonal index\nfig = decomposition.plot()\nplt.show()","982484bb":"### Naive method\n\ny_hat_naive = test.copy()\ny_hat_naive['naive_forecast'] = train['Sales'][train_len-1]\n","f7f83893":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\nplt.legend(loc='best')\nplt.title('Naive Method')\nplt.show()\n","6c8a931c":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_naive['naive_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_naive['naive_forecast'])\/test['Sales'])*100,2)\n\nresults = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","9ca40748":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Sales'].mean()","ff81554e":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Simple average forecast')\nplt.legend(loc='best')\nplt.title('Simple Average Method')\nplt.show()","cddcaa3a":"\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['avg_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_avg['avg_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple average method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","5cdb4ee5":"y_hat_sma = df.copy()\nma_window = 6\ny_hat_sma['sma_forecast'] = df['Sales'].rolling(ma_window).mean()\ny_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]","d4a5a7d3":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\nplt.legend(loc='best')\nplt.title('Simple Moving Average Method')\nplt.show()\n","6f6c10d1":"rmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sma['sma_forecast'][train_len:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sma['sma_forecast'][train_len:])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults\n","e1a370a9":"### Simple exponential smoothing\n\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nmodel = SimpleExpSmoothing(train['Sales'])\nmodel_fit = model.fit(smoothing_level=0.2,optimized=False)\nmodel_fit.params\ny_hat_ses = test.copy()\ny_hat_ses['ses_forecast'] = model_fit.forecast(6)","d69cd496":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ses['ses_forecast'], label='Simple exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Simple Exponential Smoothing Method')\nplt.show()","4c4a1bac":"#Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ses['ses_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ses['ses_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults","de68fc74":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\n# model_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\nmodel_fit = model.fit( optimized=True)\nprint(model_fit.params)\ny_hat_holt = test.copy()\ny_hat_holt['holt_forecast'] = model_fit.forecast(6)","13bdaf08":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()","1b027424":"#Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_holt['holt_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_holt['holt_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","9dd6a9ca":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwa['hw_forecast'] = model_fit.forecast(6)","2fb4c368":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","b408c2c7":"# Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwa['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwa['hw_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","78a3cb1e":"y_hat_hwm = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwm['hw_forecast'] = model_fit.forecast(6)","4e74112a":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwm['hw_forecast'], label='Holt Winters\\'s mulitplicative forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Mulitplicative Method')\nplt.show()","ab81eedf":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwm['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwm['hw_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","571ed7e3":"### Stationarity vs non-stationary time series\n\ndf['Sales'].plot(figsize=(12, 4))\nplt.legend(loc='best')\nplt.title('Retail Gaint Sales')\nplt.show(block=False)","c694f9a3":"from statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(df['Sales'])\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","ea3b5655":"from statsmodels.tsa.stattools import kpss\nkpss_test = kpss(df['Sales'])\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])","c8c0f139":"\nfrom scipy.stats import boxcox\ndata_boxcox = pd.Series(boxcox(df['Sales'], lmbda=0), index = df.index)","b1a92e2e":"plt.figure(figsize=(12,4))\nplt.plot(data_boxcox, label='After Box Cox tranformation')\nplt.legend(loc='best')\nplt.title('After Box Cox transform')\nplt.show()","95d51b5d":"### Differencing to remove trend\n\ndata_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), df.index)\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox_diff, label='After Box Cox tranformation and differencing')\nplt.legend(loc='best')\nplt.title('After Box Cox transform and differencing')\nplt.show()","68fde29f":"data_boxcox_diff.dropna(inplace=True)","feced782":"data_boxcox_diff.tail()","ba016391":"### Augmented Dickey-Fuller (ADF) test\n\nadf_test = adfuller(data_boxcox_diff)\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","ce9c66f4":"### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\n\nkpss_test = kpss(data_boxcox_diff)\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])","4777a04f":"from statsmodels.graphics.tsaplots import plot_acf\nplt.figure(figsize=(12,4))\nplot_acf(data_boxcox_diff, ax=plt.gca(), lags = 30)\nplt.show()","8a5f82b2":"# Import plot_pacf function from statsmodels library\n\nfrom statsmodels.graphics.tsaplots import plot_pacf","50860da3":"# Plot the Partial Autocorrelation function (PACF)\n\nplt.figure(figsize=(12,4))\nplot_pacf(data_boxcox_diff, ax=plt.gca(), lags = 2)\nplt.show()","933f7182":"#Splitting data into train and test \ntrain_data_boxcox = data_boxcox[:train_len]\ntest_data_boxcox = data_boxcox[train_len:]\ntrain_data_boxcox_diff = data_boxcox_diff[:train_len-1]\ntest_data_boxcox_diff = data_boxcox_diff[train_len-1:]\n\ntrain_data_boxcox_diff","7e0a487e":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(train_data_boxcox_diff, order=(1, 0, 0)) \nmodel_fit = model.fit()\nprint(model_fit.params)","ec599876":"### Recover original time series\ny_hat_ar = data_boxcox_diff.copy()\ny_hat_ar['ar_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox_diff'].cumsum()\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ar['ar_forecast'] = np.exp(y_hat_ar['ar_forecast_boxcox'])","d1a7e3ec":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ar['ar_forecast'][test.index.min():], label='Auto regression forecast')\nplt.legend(loc='best')\nplt.title('Auto Regression Method')\nplt.show()","22c75fe7":"#Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ar['ar_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ar['ar_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive (AR) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","74611bed":"model = ARIMA(train_data_boxcox_diff, order=(0, 0, 1)) \nmodel_fit = model.fit()\nprint(model_fit.params)","39e05605":"y_hat_ma = data_boxcox_diff.copy()\ny_hat_ma['ma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox_diff'].cumsum()\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ma['ma_forecast'] = np.exp(y_hat_ma['ma_forecast_boxcox'])","51cac226":"plt.figure(figsize=(12,4))\nplt.plot(df['Sales'][:train_len], label='Train')\nplt.plot(df['Sales'][train_len:], label='Test')\nplt.plot(y_hat_ma['ma_forecast'][test.index.min():], label='Moving average forecast')\nplt.legend(loc='best')\nplt.title('Moving Average Method')\nplt.show()","faa6e95e":"# Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ma['ma_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ma['ma_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Moving Average (MA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","3e870f46":"model = ARIMA(train_data_boxcox_diff, order=(1, 0, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","72775cd7":"y_hat_arma = data_boxcox_diff.copy()\ny_hat_arma['arma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox_diff'].cumsum()\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arma['arma_forecast'] = np.exp(y_hat_arma['arma_forecast_boxcox'])","997c42bc":"#Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot( df['Sales'][:train_len-1], label='Train')\nplt.plot(df['Sales'][train_len-1:], label='Test')\nplt.plot(y_hat_arma['arma_forecast'][test.index.min():], label='ARMA forecast')\nplt.legend(loc='best')\nplt.title('ARMA Method')\nplt.show()","d97aba42":"# Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arma['arma_forecast'][train_len-1:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arma['arma_forecast'][train_len-1:])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive moving average (ARMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","345311ac":"model = ARIMA(train_data_boxcox, order=(1, 1, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","0c87b337":"y_hat_arima = data_boxcox_diff.copy()\ny_hat_arima['arima_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox_diff'].cumsum()\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arima['arima_forecast'] = np.exp(y_hat_arima['arima_forecast_boxcox'])","d06e6635":"# Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_arima['arima_forecast'][test.index.min():], label='ARIMA forecast')\nplt.legend(loc='best')\nplt.title('Autoregressive integrated moving average (ARIMA) method')\nplt.show()","f17daf2c":"# Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arima['arima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arima['arima_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive integrated moving average (ARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","571cf0c9":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train_data_boxcox, order=(1, 1, 1), seasonal_order=(1, 1, 0, 12)) \nmodel_fit = model.fit()\nprint(model_fit.params)","6b1724b1":"y_hat_sarima = data_boxcox_diff.copy()\ny_hat_sarima['sarima_forecast_boxcox'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_sarima['sarima_forecast'] = np.exp(y_hat_sarima['sarima_forecast_boxcox'])\n","83860508":"# Plot train, test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sarima['sarima_forecast'][test.index.min():], label='SARIMA forecast')\nplt.legend(loc='best')\nplt.title('Seasonal autoregressive integrated moving average (SARIMA) method')\nplt.show()","664cace4":"# Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sarima['sarima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sarima['sarima_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Seasonal autoregressive integrated moving average (SARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","6ccad6b1":"## Auto regression method (AR)","4b937a4d":"We have 48 months of data in total. Now, lets take the last 6 months as test data ands the rest as train data. Then lets calculate the CoV to find out the most profitable Market Segment.","d9d0202e":"There are 21 market segmenta  are there","e99c727c":"# Build and evaluate time series forecast","31f2234b":"Seems like APAC Consumer is the clear winner as it has the least CoV of all the Market Segments. Now, from this point onwards, I would forecast the sales for this Market-Segment.","1e8ddeef":"Both the assumptions have been satisfied, lets proceed with model building.","26ff2f9a":"### Exponential smoothing methods","e1d2b34a":"### Time series Decomposition","56e77e07":"Recover original time series","f9597483":"### Conclusion\n\nAfter applying each of the above techniques and forecasting the Sales for the next 6 months, \nwe can conclude that Holt Winter's Additive Method amongst the smoothing techniques and SARIMA Method \namongst the autoregressive methods were able to best predict the sales closest to the actual values, \nalso their MAPE values is the least amongst the lot. This gives \nus conclusive evidence supporting the fact that these 2 methods provide the best models in this case.","78d363c4":"Looks much more stationary than before. Good to proceed with the 2nd assumption now","2d6f0f13":"Holt's Exponential captures trend as well","ea21f585":"### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test","8f6b6d50":"### Time-Series Analysis","e09fc66c":"Recover original time series","079632c7":"No outlier  is present","f9a0890f":"Multiplicative seasonal decomposition","a962976e":"Additive seasonal decomposition","49bf9a93":"Calculate RMSE and MAPE","f1f5ef03":"Recover original time series forecast","6a39c82f":"Amongst the autoregressive models, SARIMA method gave us the best plot and least MAPE value, \nsince there was seasonality in the data to deal with. We can therefore prefer SARIMA amongst all \nthe autoregressive methods.","44809232":"### Box Cox transformation to make variance constant","874f8897":"### Preparing Data to Calculate Coefficient of Variation (CoV)\n\nThe coefficient of variation is a ratio of the standard deviation to mean. When we carefully observe the dataset, we can see that among the available market segments, the mean of the profits for these market segments varies quite a bit. Also, the standard deviation for the profits varies in a similar manner and it is meaningless to compare standard deviations across all the market segments. As a better metric to compare the variance between the segments, we will use the coefficient of variation. It normalises the standard deviation and gives us a comparative figure on the basis of which we can identify the most profitable market segment.\n\nThe data first of all will be prepared in order to calculate CoV","a04ded3b":"### Holt's method with trend","ee9b4fec":"### Augmented Dickey-Fuller (ADF) test","62e119e3":"### Data Preparation","63d12617":"### Auto regression moving average method (ARMA)","2b030d1e":"### Handling datetime column","3a56fa61":"### Simple moving average method","c00beb53":"There are 2 assumptions that are taken before applying Auto Regressive methods.\n\nStationarity: The time series need to be stationary. It should have a constant mean and variance thorughout, along with a consistent covariance at a fixed lag. It should not show any trend or seasonality patterns.\n\nAuto-correlation: The variables are dependent and are influenced by their own lagged values.\nTwo formal tests for stationarity based on Hypothesis testing are ADF and KPSS tests.\n","d7ef8643":"### Seasonal auto regressive integrated moving average (SARIMA)","63f6e821":"### Auto regressive integrated moving average (ARIMA)","31f2dec7":"The consistently incresing trend is quite evident. Also, the seasonality extract shows the dip in Sales every year in the month of July. The middle months show higher sales in general.","5ef1348d":"### Partial Auto-Correaltion Function (PACF)\n\nPACF captures only the direct relationships between the variables and the lagged values.","9a9fdba8":"Looks like APAC Consumer is the most profitable of all market segments","5895501b":"### Holt Winters' additive method with trend and seasonality","edb84039":"### Autocorrelation function (ACF)\n\n\nThis function tells us about the correlation between an observation with its lagged values. \nIt helps in determining which lag of the observation is influencing the most. \nAlso, ACF captures both direct and indirect relationships between the observations\n\n","e1c12304":"Clearly, since the time-series data has seasonality as we saw during the decomposition, \nHolt Winters Methods perform the best as they capture the seasonal component, giving the least errors. \nWe can prefer the Holt Winters' Additive method amongst all the smoothing techniques applied on the\ndata. But lets further proceed to applying some autoregressive techniques as well.","b5a624cd":"Seems like it has an upward trend and some seasonal components as well. Also, as seen from the above plot, there are no missing values\n","117a4b62":"### Simple average method","21af6d9d":"# Auto Regressive methods","84193561":"### Holt Winter's multiplicative method with trend and seasonality","d8b8c53b":"As we saw through above two tests, the time series is not stationary. Now, we need to make it \nstationary. \nWe will use Box-cox transformation to make the variance constant and use first order differencing to \nmake the mean constant.","17142527":"## Simple time series methods","973cc05b":"Calculate RMSE and MAPE","bd5f6dea":"The given series is converted to stationary","acd074d9":"Recover original time series forecast","f057ce7f":"Calculate RMSE and MAPE","53c77943":"### Moving average method (MA)"}}