{"cell_type":{"2c1b19ec":"code","091cc26d":"code","f2751e0e":"code","d8a24d34":"code","291a13fa":"code","33eea2cb":"code","6049b5eb":"code","eb09d364":"code","b1f9a01e":"code","8e5afdcf":"code","4693e3b5":"code","36e9e1b7":"code","9f830273":"markdown","f07e5d46":"markdown","2d7e479f":"markdown","2ceeed8b":"markdown","47595837":"markdown","902791bb":"markdown"},"source":{"2c1b19ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","091cc26d":"df1 = pd.read_csv('..\/input\/besedo\/train.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf1.head()","f2751e0e":"df = pd.read_csv('..\/input\/besedo\/train_trimmed_encod.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf.head()","d8a24d34":"#Codes by Pooja Jain https:\/\/www.kaggle.com\/jainpooja\/av-guided-hackathon-predict-youtube-likes\/notebook\n\ntext_cols = ['ad_content_title', 'ad_content_body', 'ad_location_region', 'ad_location_city']\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['|']), random_state = 42)\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(text_cols):\n  op = wc.generate(str(df1[c]))\n  _ = axes[i].imshow(op)\n  _ = axes[i].set_title(c.upper(), fontsize=24)\n  _ = axes[i].axis('off')\n\n#_ = fig.delaxes(axes[4])","291a13fa":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","33eea2cb":"df[numerical_nan].isna().sum()","6049b5eb":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","eb09d364":"from scipy import linalg\nimport matplotlib","b1f9a01e":"def isotonic_regression(y, weight=None, y_min=None, y_max=None, callback=None):\n    \"\"\"Solve the isotonic regression model::\n\n        min sum w[i] (y[i] - y_[i]) ** 2\n\n        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max\n\n    where:\n        - y[i] are inputs (real numbers)\n        - y_[i] are fitted\n        - w[i] are optional strictly positive weights (default to 1.0)\n\n    Parameters\n    ----------\n    y : iterable of floating-point values\n        The data.\n\n    weight : iterable of floating-point values, optional, default: None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : optional, default: None\n        If not None, set the lowest value of the fit to y_min.\n\n    y_max : optional, default: None\n        If not None, set the highest value of the fit to y_max.\n\n    Returns\n    -------\n    `y_` : list of floating-point values\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    if weight is None:\n        weight = np.ones(len(y), dtype=y.dtype)\n    if y_min is not None or y_max is not None:\n        y = np.copy(y)\n        weight = np.copy(weight)\n        C = np.dot(weight, y * y) * 10  # upper bound on the cost function\n        if y_min is not None:\n            y[0] = y_min\n            weight[0] = C\n        if y_max is not None:\n            y[-1] = y_max\n            weight[-1] = C\n\n    active_set = [(weight[i] * y[i], weight[i], [i, ])\n                  for i in range(len(y))]\n    current = 0\n    counter = 0\n    while current < len(active_set) - 1:\n        value0, value1, value2 = 0, 0, np.inf\n        weight0, weight1, weight2 = 1, 1, 1\n        while value0 * weight1 <= value1 * weight0 and \\\n                        current < len(active_set) - 1:\n            value0, weight0, idx0 = active_set[current]\n            value1, weight1, idx1 = active_set[current + 1]\n            if value0 * weight1 <= value1 * weight0:\n                current += 1\n\n            if callback is not None:\n                callback(y, active_set, counter, idx1)\n                counter += 1\n\n        if current == len(active_set) - 1:\n            break\n\n        # merge two groups\n        value0, weight0, idx0 = active_set.pop(current)\n        value1, weight1, idx1 = active_set.pop(current)\n        active_set.insert(current,\n                          (value0 + value1,\n                           weight0 + weight1, idx0 + idx1))\n        while value2 * weight0 > value0 * weight2 and current > 0:\n            value0, weight0, idx0 = active_set[current]\n            value2, weight2, idx2 = active_set[current - 1]\n            if weight0 * value2 >= weight2 * value0:\n                active_set.pop(current)\n                active_set[current - 1] = (value0 + value2, weight0 + weight2,\n                                           idx0 + idx2)\n                current -= 1\n\n    solution = np.empty(len(y))\n    if callback is not None:\n        callback(y, active_set, counter+1, idx1)\n        callback(y, active_set, counter+2, idx1)\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    return solution","8e5afdcf":"import pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\n\ndef cb(y, active_set, counter, current):\n    solution = np.empty(len(y))\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(9.5,6.5)\n\n    color = y.copy()\n    pl.scatter(np.arange(len(y)), solution, s=50, cmap=pl.cm.Spectral, vmin=50, c=color)\n    pl.scatter([np.arange(len(y))[current]], [solution[current]], s=200, marker='+', color='red')\n    pl.xlim((0, 40))\n    pl.ylim((50, 300))\n    pl.savefig('isotonic_%03d.png' % counter)\n    pl.show()\n\nn = 40\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\n\ny_ = isotonic_regression(y, callback=cb)","4693e3b5":"import pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\ny = np.array([0]*50+[1]*50)\nrs = check_random_state(0)\nx = np.random.random(size=(n,)) #you can interpret it as the outputs of the SVM or any other model\n\nres = sorted(list(zip(x,y)), key = lambda x: x[0]) \nx = []\ny = []\nfor i,j in res:\n    x.append(i)\n    y.append(j)\nx= np.array(x)\ny= np.array(y)\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n###############################################################################\n# plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = pl.figure()\npl.plot(x, y, 'r.', markersize=12)\npl.plot(x, y_, 'g.-', markersize=12)\npl.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\npl.gca().add_collection(lc)\npl.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\npl.title('Isotonic regression')\n\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(9.5,6.5)\npl.savefig('inverse_isotonic.png')\npl.show()","36e9e1b7":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke was Here.')","9f830273":"<font color=\"#EC7063\">Encoded Version: pd.gotEncoded_dummies<\/font>","f07e5d46":"![](https:\/\/github.com\/ddd-crew\/ddd-starter-modelling-process\/raw\/master\/resources\/ddd-starter-modelling-process.jpg)https:\/\/github.com\/ddd-crew\/ddd-starter-modelling-process","2d7e479f":"\n<div class=\"alert alert-block alert-success\">\n    In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations such that the fitted line is non-decreasing (or non-increasing) everywhere, and lies as close to the observations as possible. It has applications in statistical inference. One might use it to fit an isotonic curve to the means of some set of experimental results when an increase in those means according to some particular ordering is expected. A benefit of isotonic regression is that it is not constrained by any functional form, such as the linearity imposed by linear regression, as long as the function is monotonic increasing.https:\/\/en.wikipedia.org\/wiki\/Isotonic_regression\n\n<\/div>","2ceeed8b":"Dataset's Title is DDDDDD. Since there is no explanation I infered that's due to the 6 (six) files\/data? It could mean troubleshooting or many other things. Though I found some more interesting DDD collection of D: \n\n\n<font color=\"#EC7063\">Domain-Driven Design<\/font>\n\n\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain.\nhttps:\/\/en.wikipedia.org\/wiki\/Domain-driven_design\n\n<font color=\"#EC7063\">Warning<\/font>\n\nDon't expect to read any DDD in this Kaggle Notebook. Is that for beginners? I'm not in that level. I'm below of that beginner's level. How below: a sort of Pfizer's vaccine storage temperature Below.   ","47595837":"#Codes by Kundan Jha https:\/\/www.kaggle.com\/kundnjha\/isotonic-regression","902791bb":"### <b><mark style=\"background-color: #9B59B6\"><font color=\"white\">Isotonic or Monotonic Regression<\/font><\/mark><\/b>"}}