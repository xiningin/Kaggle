{"cell_type":{"a6585563":"code","20476df3":"code","8577d2a4":"code","15c31193":"code","24cb1b54":"code","97322e18":"code","0c263bb2":"code","5f4adae6":"code","7829595a":"code","2683d8e6":"code","47b4c851":"code","1e257724":"code","7e942c84":"code","b1bcf49d":"code","eb6f280e":"code","28e035b2":"code","cb72a73e":"code","c3c7f69c":"code","e87ea4b4":"code","306f420f":"markdown","019df7b5":"markdown","1eedeb58":"markdown","fc79e17e":"markdown","612e38f9":"markdown","0979c74b":"markdown","f0e87ff2":"markdown","2f2a9dc5":"markdown","86dee465":"markdown","5e5633ce":"markdown","4cadf016":"markdown","efb8c387":"markdown","ebb1dfb1":"markdown","e1bf908c":"markdown","718c8bf1":"markdown","a61084c8":"markdown","713efb7e":"markdown","90bf5f12":"markdown","e48d2880":"markdown","d779d8d8":"markdown"},"source":{"a6585563":"%tensorflow_version 2.x\nimport tensorflow\ntensorflow.__version__","20476df3":"import random\nrandom.seed(0)","8577d2a4":"import warnings\nwarnings.filterwarnings(\"ignore\")","15c31193":"from google.colab import drive\ndrive.mount('\/content\/drive\/')","24cb1b54":"project_path = '\/content\/drive\/My Drive\/'","97322e18":"import h5py\n\n# Open the file as readonly\nh5f = h5py.File(project_path + 'SVHN_single_grey1.h5', 'r')\n\n# Load the training, test and validation se\nX_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n# Close this file\nh5f.close()","0c263bb2":"print(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","5f4adae6":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ncolumns=10\nrows=10\n\nfig=plt.figure(figsize=(8, 8))\n\nfor i in range(1,columns*rows+1):\n  img=X_train[i]\n  fig.add_subplot(rows,columns,i)\n  print(y_train[i],end='   ')\n  if i % columns == 0:\n    print (\"\")\n  plt.imshow(img,cmap='gray')\n\nplt.show()","7829595a":"# Importing OpenCV module for the resizing function\nimport cv2\nimport numpy as np\n\n# Create a resized dataset for training and testing inputs with corresponding size\n# Here we are resizing it to 28X28 (same input size as MNIST)\nX_train_resized=np.zeros((X_train.shape[0],28,28))\nfor i in range(X_train.shape[0]):\n  #using cv2.resize to resize each train example to 28X28 size using Cubic interpolation\n  X_train_resized[i,:,:]=cv2.resize(X_train[i],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n\nX_test_resized = np.zeros((X_test.shape[0], 28, 28))\nfor i in range(X_test.shape[0]):\n  #using cv2.resize to resize each test example to 28X28 size using Cubic interpolation\n  X_test_resized[i,:,:] = cv2.resize(X_test[i], dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n  \n# We don't need the original dataset anynmore so we can clear up memory consumed by original dataset\ndel(X_train, X_test)","2683d8e6":"X_train = X_train_resized.reshape(X_train_resized.shape[0], 28, 28, 1)\nX_test = X_test_resized.reshape(X_test_resized.shape[0], 28, 28, 1)","47b4c851":"del(X_train_resized, X_test_resized)","1e257724":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train \/= 255\nX_test \/= 255","7e942c84":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\n\nprint(\"Images in X_train:\", X_train.shape[0])\nprint(\"Images in X_test:\", X_test.shape[0])","b1bcf49d":"from tensorflow.keras.utils import to_categorical\n\ny_train=to_categorical(y_train,num_classes=10)\ny_test=to_categorical(y_test,num_classes=10)","eb6f280e":"print(\"Label: \", y_train[2])\nplt.imshow(X_train[2].reshape(28,28), cmap='gray')","28e035b2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n\n# Initialize the model\nmodel = Sequential()\n\n# Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'relu' \nmodel.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)))\n\n# Add a Convolutional Layer with 64 filters of size 3X3 and activation function as 'relu' \nmodel.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\n\n# Add a MaxPooling Layer of size 2X2 \nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Apply Dropout with 0.2 probability \nmodel.add(Dropout(rate=0.2))\n\n# Flatten the layer\nmodel.add(Flatten())\n\n# Add Fully Connected Layer with 128 units and activation function as 'relu'\nmodel.add(Dense(128, activation=\"relu\"))\n\n#Add Fully Connected Layer with 10 units and activation function as 'softmax'\nmodel.add(Dense(10, activation=\"softmax\"))","cb72a73e":"for l in model.layers:\n  print(l.name)","c3c7f69c":"for l in model.layers:\n  if 'dense' not in l.name:\n    l.trainable=False\n  if 'dense' in l.name:\n    print(l.name + ' should be trained') ","e87ea4b4":"model.load_weights(project_path + 'cnn_mnist_weights-1.h5')","306f420f":"### Resize all the train and test inputs to 28X28, to match with MNIST CNN model's input size\n\n","019df7b5":"### Load the dataset","1eedeb58":"### Print the shape of training and testing data","fc79e17e":"### Load pre-trained weights from MNIST CNN model\n- load the file named `cnn_mnist_weights.h5`","612e38f9":"Let's load the dataset now","0979c74b":"As we are using google colab, we need to mount the google drive to load the data file","f0e87ff2":"# Ignore the warnings","2f2a9dc5":"## The Street View House Numbers (SVHN) Dataset\n\nSVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n\n- 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 0.\n- 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data\n- Comes in two formats:\n  1. Original images with character level bounding boxes.\n  2. MNIST-like 32-by-32 images centered around a single character (many of the images do contain some distractors at the sides).\n\n- The dataset that we will be using in this notebook contains 42000 training samples and 18000 testing samples","86dee465":"### Initialize a sequential model\n- define a sequential model\n- add 2 convolutional layers\n    - no of filters in first layer: 32\n    - no of filters in second layer: 64\n    - kernel size: 3x3\n    - activation: \"relu\"\n    - input shape: (28, 28, 1) for first layer\n- add a max pooling layer of size 2x2\n- add a dropout layer\n    - dropout layers fight with the overfitting by disregarding some of the neurons while training\n    - use dropout rate 0.2\n- flatten the data\n    - add Flatten later\n    - flatten layers flatten 2D arrays to 1D array before building the fully connected layers\n- add 2 dense layers\n    - number of neurons in first layer: 128\n    - number of neurons in last layer: number of classes\n    - activation function in first layer: relu\n    - activation function in last layer: softmax\n    - we may experiment with any number of neurons for the first Dense layer; however, the final Dense layer must have neurons equal to the number of output classes","5e5633ce":"### Let's visualize our dataset","4cadf016":"Firstly, let's select TensorFlow version 2.x in colab","efb8c387":"### Reshape train and test sets into compatible shapes\n- Sequential model in tensorflow.keras expects data to be in the format (n_e, n_h, n_w, n_c)\n- n_e= number of examples, n_h = height, n_w = width, n_c = number of channels\n- do not reshape labels","ebb1dfb1":"### Normalize data\n- we must normalize our data as it is always required in neural network models\n- we can achieve this by dividing the RGB codes with 255 (which is the maximum RGB code minus the minimum RGB code)\n- normalize X_train and X_test\n- make sure that the values are float so that we can get decimal points after division","e1bf908c":"# Initialize the random number generator","718c8bf1":"### Building the CNN \n- Define the layers of model with same size as the CNN used for MNIST Classification","a61084c8":"### Make only dense layers trainable\n- freeze the initial convolutional layer weights and train only the dense (FC) layers\n- set trainalble = False for all layers other than Dense layers","713efb7e":"### One-hot encode the class vector\n- convert class vectors (integers) to binary class matrix\n- convert y_train and y_test\n- number of classes: 10\n- we are doing this to use categorical_crossentropy as loss","90bf5f12":"### Print shape of data and number of images\n- print shape of X_train\n- print number of images in X_train\n- print number of images in X_test","e48d2880":"Add path to the folder where your dataset is present","d779d8d8":"We can delete X_train_resized and X_test_resized variables as we are going to use X_train and X_test variables going further"}}