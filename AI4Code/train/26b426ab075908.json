{"cell_type":{"2f655cf3":"code","c67aa975":"code","c3baeb64":"code","c5f280fe":"code","9e591095":"code","9867c98b":"code","04787852":"code","4a796e8b":"code","22b4f7ef":"code","2ecc6d10":"code","f0900708":"code","8ed2ae40":"code","af93f504":"code","c9d27d93":"code","3e56a5c3":"code","7bbc6df4":"code","6e69f3f9":"code","e81c04c5":"code","7a4aca74":"code","8e072fe6":"code","711ea3d7":"code","ee0d33ed":"code","df9a2994":"code","b33585e0":"code","59cf547f":"code","ee4c7f16":"code","27af9fee":"code","b4f3b36c":"code","0c593130":"code","a71925ea":"code","93ecd18a":"code","a67b053a":"code","e9640712":"code","ed7e4779":"code","d1a4993a":"code","4111d20b":"code","c477b45d":"code","7224c7bb":"code","8de5a644":"code","a4505b54":"code","3fbad91f":"code","eacc340d":"code","9a05303f":"code","93c9de7f":"code","b8bc7ba6":"code","08210e91":"markdown","0a0555d5":"markdown","89957042":"markdown","d54b80eb":"markdown","ac28c78b":"markdown","11ac1204":"markdown","1c7bbc18":"markdown","3dd1541d":"markdown","58302dfd":"markdown","c9b7a777":"markdown","9ef8a210":"markdown","c4b25cc1":"markdown","33feb431":"markdown","b2efee7f":"markdown","c3fac512":"markdown","b36754f7":"markdown","83b1b0f0":"markdown","db8e22e5":"markdown","d7fe9f73":"markdown","cf3da78c":"markdown","17e6f079":"markdown","e12d95ac":"markdown","06880464":"markdown","db4b3e35":"markdown","f00e8144":"markdown","837d6689":"markdown"},"source":{"2f655cf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c67aa975":"train=pd.read_csv(r\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(r\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission=pd.read_csv(r\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","c3baeb64":"train.head()","c5f280fe":"train.shape","9e591095":"train.isnull().sum()","9867c98b":"test.isnull().sum()","04787852":"train['target'].value_counts()","4a796e8b":"import seaborn as sns","22b4f7ef":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","2ecc6d10":"train[['text','target']].head(20)","f0900708":"#lets look separately how non disaster tweet(0) look like\n\nfor i in range(10):\n    res=train[train['target']==0]['text'].values[i] \n    print(res)","8ed2ae40":"#lets look separately how  disaster tweet(1) look like\n\nfor i in range(10):\n    res=train[train['target']==1]['text'].values[i] \n    print(res)","af93f504":"pd.DataFrame(train['keyword'].value_counts()[:20])","c9d27d93":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')","3e56a5c3":"pd.DataFrame(train['location'].value_counts()).head(20)","7bbc6df4":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","6e69f3f9":"import re","e81c04c5":"# Applying a first round of text cleaning techniques since we have seen the text is having noise\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","7a4aca74":"import nltk","8e072fe6":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","711ea3d7":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","ee0d33ed":"from nltk.corpus import stopwords","df9a2994":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","b33585e0":"from nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n","59cf547f":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","ee4c7f16":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","27af9fee":"from sklearn.feature_extraction.text import CountVectorizer","b4f3b36c":"#  count vectorizer\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])","0c593130":"from sklearn.feature_extraction.text import TfidfVectorizer","a71925ea":"#Tf-Idf\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])\n","93ecd18a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score","a67b053a":"#fitting the model to countvectorizer\nclf=LogisticRegression(C=1.0)\nscores=cross_val_score(clf,train_vectors,train['target'],cv=7,scoring=\"f1\")\nscores","e9640712":"scores.mean()","ed7e4779":"clf.fit(train_vectors,train['target'])","d1a4993a":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","4111d20b":"scores.mean()","c477b45d":"from sklearn.naive_bayes import MultinomialNB\n# Fitting a simple Naive Bayes on Counts\nclf_NB=MultinomialNB()\nscores=cross_val_score(clf_NB,train_vectors,train['target'],cv=5,scoring=\"f1\")\nscores","7224c7bb":"scores.mean()","8de5a644":"clf_NB.fit(train_vectors,train[\"target\"])","a4505b54":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores =cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","3fbad91f":"scores.mean()","eacc340d":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","9a05303f":"sample_submission['target']=clf_NB.predict(test_vectors)","93c9de7f":"sample_submission.head()","b8bc7ba6":"sample_submission.to_csv(\"submission.csv\", index=False)","08210e91":"Disaster tweets look like some serious tweets","0a0555d5":"# TF-IDF","89957042":"**Make Submission**","d54b80eb":"Non disaster tweets looks like so casual tweets","ac28c78b":"Here we can observe that the location feature contains city names as well as country names","11ac1204":"Importing data sets","1c7bbc18":"### Lets look at keyword feature in train dataset","3dd1541d":"**Stemming And Lemmatization**","58302dfd":"# Tokenization","c9b7a777":"> **StopWord Removal**","9ef8a210":"**Logistic Regression**","c4b25cc1":"Tokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance","33feb431":"***Exploring the target column***","b2efee7f":"Now lets look at text and target feature in the train data","c3fac512":"**Even though the location feature  has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning.**","b36754f7":"I found Naive Bayes classification of countvectorizer is giving good accuracy,I decided to predict test records using it.","83b1b0f0":"# Bag of Words","db8e22e5":"We can see that keyword and location features have missing values in both test and train data","d7fe9f73":"**Naive Bayes classifier**","cf3da78c":"# **Lets Build the Model now**","17e6f079":"# Data cleaning","e12d95ac":"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques.","06880464":"# Data Analysis","db4b3e35":"*Lets check whether the data have missing values or not.*","f00e8144":"The train data is classified into 0 and 1 and we can also say that the target column is balanced properly with almost equal no of 0's and 1's therefore there is no chance of overfitting. ","837d6689":"It appears the countvectorizer gives a better performance than TFIDF in this case."}}