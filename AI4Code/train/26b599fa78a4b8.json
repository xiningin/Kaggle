{"cell_type":{"66a034fa":"code","19e1bda4":"code","31fb7097":"code","c441505d":"code","dceede50":"code","d510a8a9":"code","5bbc7c7a":"code","f9159168":"code","78c315cb":"code","324e43a9":"code","a83a1ec2":"code","6a9fe09d":"code","29b767d7":"code","cfb8b3db":"code","841ea8c5":"code","7c74b3d0":"code","54b61f18":"code","c23029f6":"code","6a2c3014":"code","3b924623":"code","70d34da6":"code","98f6604e":"code","75ba148d":"code","48ec26b6":"code","8c6aaa13":"code","b20f9d91":"code","c750af0b":"code","f8208893":"code","fb063257":"code","a194ddcc":"code","49b2eeb2":"markdown","bf2fedf6":"markdown","8c200898":"markdown","cc2c745e":"markdown","a21a4354":"markdown","579287b7":"markdown","6a0a687f":"markdown","5d66f3f8":"markdown","89c522c3":"markdown","f3dccdbd":"markdown","ba6b8483":"markdown","f53935e7":"markdown","d9712851":"markdown","2f185a04":"markdown","50cc0563":"markdown"},"source":{"66a034fa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport torch\nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import SubsetRandomSampler\nfrom tqdm import tqdm_notebook","19e1bda4":"def load_dataset(path):\n    df=pd.read_csv(path)\n    print(\"Dataset Contains : {} rows and {} columns\".format(df.shape[0],df.shape[1]))\n    return df","31fb7097":"print(\"Loading train dataset :\")\ndftrain=load_dataset('..\/input\/train.csv')\nprint(\"Loading test dataset :\")\ndftest=load_dataset('..\/input\/test.csv')","c441505d":"print(\"Label datatype : {} \\nImage datatype : {}\".format(dftrain.label.dtype,dftrain.pixel0.dtype))\nprint(\"Test Image datatype : {}\".format(dftest.pixel0.dtype))","dceede50":"maxp=dftrain.iloc[:,1:].max().max()\nminp=dftrain.iloc[:,1:].min().min()\nprint(\"Pixel range : {}-{}\".format(minp,maxp))","d510a8a9":"class MnistDataset(torch.utils.data.Dataset):\n    def __init__(self,data,transform=None):\n        self.data=data\n        self.transform=transform\n        \n    def __getitem__(self,idx):\n        \n        label=self.data.iloc[idx,0]\n        img=self.data.iloc[idx,1:].values.astype(np.uint8).reshape(28,28)\n        if(self.transform):\n            img=self.transform(img)\n        return img,label\n        \n    def __len__(self):\n        return len(self.data)","5bbc7c7a":"batch_size=16\nvalid_split=0.1\nis_cuda=torch.cuda.is_available()\n\n# preprocessing on image\n# as of now I'm only mean normalizing the image\n# todo - convert the image in range (0,1)\n\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntransform_valid = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\n\ntrainset=MnistDataset(dftrain,transform_train)\nvalidset=MnistDataset(dftrain,transform_valid)\n\nindex= list(range(len(dftrain)))\nnp.random.shuffle(index)\n\nsplit=int(len(dftrain)*valid_split)\nvalid_index,train_index=index[:split],index[split:]\n\ntrainsampler=SubsetRandomSampler(train_index)\nvalidsampler=SubsetRandomSampler(valid_index)\n\ntrainloader=DataLoader(trainset,sampler=trainsampler,batch_size=batch_size)\nvalidloader=DataLoader(validset,sampler=validsampler,batch_size=batch_size)\n\nprint(f\"Train Length {len(train_index)}\")\nprint(f\"Valid Lenght {len(valid_index)}\")\nprint(f\"Total {len(train_index)+len(valid_index)}\")","f9159168":"for x,y in trainloader:\n    plt.figure(figsize=(5,5))\n    for i in range(3):\n        plt.subplot(1,3,i+1)\n        plt.imshow(x[i].view(28,28))\n        plt.title(str(y[i]))\n    break","78c315cb":"for x,y in validloader:\n    plt.figure(figsize=(5,5))\n    for i in range(3):\n        plt.subplot(1,3,i+1)\n        plt.imshow(x[i].view(28,28))\n        plt.title(str(y[i]))\n    break","324e43a9":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self,data,transform=None):\n        self.data=data\n        self.transform=transform\n        \n    def __getitem__(self,idx):\n        \n        img=self.data.iloc[idx,:].values.astype(np.uint8).reshape(28,28)\n        if(self.transform):\n            img=self.transform(img)\n        return img\n        \n    def __len__(self):\n        return len(self.data)","a83a1ec2":"transform_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntestset=TestDataset(dftest,transform_test)\ntestloader=DataLoader(testset,batch_size=1)","6a9fe09d":"i=0\nplt.figure(figsize=(5,5))\nfor x in testloader:\n    plt.subplot(1,3,i+1)\n    plt.imshow(x[0].view(28,28))\n    i+=1\n    if(i==3):\n        break","29b767d7":"def train(net,criterion,optimiser,num_epoch=10):\n    train_loss=[]\n    valid_loss=[]\n    for epoch in tqdm_notebook(range(num_epoch)):\n\n        net.train()    \n        print(f\"Epoch : {epoch} :-\")\n        for x,y in trainloader:\n            if(is_cuda):\n                x,y=x.cuda(),y.cuda()\n            out=net(x)\n            optimiser.zero_grad()\n            loss=criterion(out,y)\n            loss.backward()\n            optimiser.step()\n        train_loss.append(loss)\n        print(f\"\\tTrain Loss : {loss}\")\n\n        net.eval()\n        for x,y in validloader:\n            if(is_cuda):\n                x,y=x.cuda(),y.cuda()\n            out=net(x)\n            loss=criterion(out,y)\n        valid_loss.append(loss)\n        print(f\"\\tValid Loss : {loss}\")\n\n        torch.cuda.empty_cache()\n        \n    return net,train_loss,valid_loss","cfb8b3db":"def plot_losses(train_loss,valid_loss):\n    plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    plt.plot(range(len(train_loss)),train_loss)\n    plt.title(\"Training Loss\")\n    plt.subplot(1,2,2)\n    plt.plot(range(len(valid_loss)),valid_loss)\n    plt.title(\"Validation Loss\")","841ea8c5":"def predict_output(net):\n    pred=[]\n    net.eval()\n    for x in tqdm_notebook(testloader):\n        if(is_cuda):\n            x=x.cuda()\n        p=net(x)\n        pred.append(torch.argmax(p).data.cpu().numpy())\n    return pred","7c74b3d0":"class Net(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1=nn.Sequential(\n            nn.Conv2d(1,5,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.conv2=nn.Sequential(\n            nn.Conv2d(5,10,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc=nn.Sequential(\n            nn.Linear(7*7*10,50),\n            nn.ReLU(),\n            nn.Linear(50,10)\n        )\n        \n    def forward(self,x):\n        \n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","54b61f18":"net=Net()\nif(is_cuda):\n    net=net.cuda()\n    \ncriterion=nn.CrossEntropyLoss()\noptimiser=optim.SGD(net.parameters(),lr=0.001)","c23029f6":"net,train_loss,valid_loss=train(net,criterion,optimiser,num_epoch=50)","6a2c3014":"plot_losses(train_loss,valid_loss)","3b924623":"pred=predict_output(net)\ndfpred=pd.DataFrame({\"ImageId\":list(range(1,len(dftest)+1)),\"Label\":pred})\ndfpred.to_csv(\"pred3.csv\",index=False)","70d34da6":"class Net_bn(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1=nn.Sequential(\n            nn.Conv2d(1,5,3,padding=1),\n            nn.BatchNorm2d(5),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.conv2=nn.Sequential(\n            nn.Conv2d(5,10,3,padding=1),\n            nn.BatchNorm2d(10),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc=nn.Sequential(\n            nn.Linear(7*7*10,50),\n            nn.ReLU(),\n            nn.Linear(50,10)\n        )\n        \n    def forward(self,x):\n        \n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","98f6604e":"net_bn=Net_bn()\nif(is_cuda):\n    net_bn=net_bn.cuda()\n    \ncriterion=nn.CrossEntropyLoss()\noptimiser=optim.SGD(net_bn.parameters(),lr=0.001)","75ba148d":"net_bn,train_loss,valid_loss=train(net_bn,criterion,optimiser,num_epoch=50)","48ec26b6":"plot_losses(train_loss,valid_loss)","8c6aaa13":"pred=predict_output(net_bn)\ndfpred=pd.DataFrame({\"ImageId\":list(range(1,len(dftest)+1)),\"Label\":pred})\ndfpred.to_csv(\"pred4.csv\",index=False)","b20f9d91":"class Net_deep(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1=nn.Sequential(\n            nn.Conv2d(1,5,3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(5,5,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.conv2=nn.Sequential(\n            nn.Conv2d(5,10,3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(10,10,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc=nn.Sequential(\n            nn.Linear(7*7*10,100),\n            nn.ReLU(),\n            nn.Linear(100,10)\n        )\n        \n    def forward(self,x):\n        \n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","c750af0b":"net_deep=Net_deep()\nif(is_cuda):\n    net_deep=net_deep.cuda()\n    \ncriterion=nn.CrossEntropyLoss()\noptimiser=optim.SGD(net_deep.parameters(),lr=0.001)","f8208893":"net_deep,train_loss,valid_loss=train(net_deep,criterion,optimiser,num_epoch=50)","fb063257":"plot_losses(train_loss,valid_loss)","a194ddcc":"pred=predict_output(net_deep)\ndfpred=pd.DataFrame({\"ImageId\":list(range(1,len(dftest)+1)),\"Label\":pred})\ndfpred.to_csv(\"pred5.csv\",index=False)","49b2eeb2":"### Function to plot losses","bf2fedf6":"### What Am I doing !\nMNIST is one of the most fundamental dataset and a simple `fully connected` network gives more than `80%` accuracy.\n\nIn this kernel I'll use CNN and try to get more than `95%` accuracy. This kernel I'm writing for learning purpose, so I'm detailing things so that \nIt might help other beginners\n\n### These are the things I'll be exploring in this kernel:\n\n1. Pytorch custom  dataset class and dataloader functionality for parallel data stream.\n2. Self defined CNN architecture(simple one) with batch normalization(I never tried it, don't know it'd be useful or not for MNIST)\n4. Proper Validation and test functionality (I never tried validation dataset, recently I came to know its usefulness).","8c200898":"### Visualizing test dataset","cc2c745e":"# 2. Convolution Model With BatchNorm","a21a4354":"Since test dataset doen't contains labes, I'm defining different class for loading test data","579287b7":"I'm taking out the functionality which will be common across various models and wrapping them up in functions.","6a0a687f":"# 1. Convolution Model without Batchnorm","5d66f3f8":"# Helper Functions","89c522c3":"### Main training function","f3dccdbd":"# Let's import and visualise the dataset","ba6b8483":"# Defining Test Dataset Class","f53935e7":"### Function for output prediciton using trained model","d9712851":"### Let's see if trainloader and validloader is getting right data","2f185a04":"# Pytoch Dataset Class","50cc0563":"# 3. More Deeper Convolution Network"}}