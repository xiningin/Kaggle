{"cell_type":{"9927265e":"code","7be82ea8":"code","0fd91110":"code","5b2e347d":"code","c498ea18":"code","4197c226":"code","a8a983b4":"code","2cfcd84b":"code","c89dead5":"code","56f58ab3":"code","242e8ac2":"code","f4b50a12":"code","ed0b88b1":"code","ccc6526b":"code","376c55bf":"code","7990537f":"code","37a22af1":"code","91e85d86":"code","a9165d90":"code","249f816c":"code","40122215":"code","d7351515":"code","b42fb174":"code","3e1af76d":"code","857d5c31":"code","c2006960":"code","76576172":"code","25b3e71d":"code","e061bf2a":"code","70fa20a2":"code","feffecef":"code","cfc44b23":"code","e0a9c8ab":"code","c1cb229e":"code","2bb33922":"code","c7d515d3":"code","cdc3aa42":"code","75d54494":"markdown","4afde26a":"markdown","40ea02ce":"markdown","3234b616":"markdown","41137585":"markdown","67541d62":"markdown","fbdf8db6":"markdown","36bd54c7":"markdown","ff42e232":"markdown","85658f0b":"markdown","463b7ac8":"markdown","c62b069e":"markdown","aa983f80":"markdown","75fc3316":"markdown","0cdb842b":"markdown","5443d589":"markdown","9a6269e9":"markdown","8885bb67":"markdown","5df3db97":"markdown","184cc0ba":"markdown","bb18692e":"markdown","e661342b":"markdown","a74452f4":"markdown","282ad417":"markdown","6271030a":"markdown","46b8c380":"markdown","ad80b3de":"markdown","76e567d6":"markdown","77bbf750":"markdown","1fb0e67a":"markdown","57c5f550":"markdown","979465af":"markdown","36a197cb":"markdown","7a84c300":"markdown","48b3f487":"markdown","37324bfd":"markdown","d13f780c":"markdown","56701529":"markdown","bd3b58a4":"markdown","a35149e1":"markdown","0ad78e16":"markdown","b5d7da54":"markdown","381660c8":"markdown","c5a779e8":"markdown","1b8c2171":"markdown","15dba75e":"markdown","ae615cdb":"markdown","d1a89cdb":"markdown","d3e042aa":"markdown"},"source":{"9927265e":"%matplotlib inline","7be82ea8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0fd91110":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import KFold, cross_validate","5b2e347d":"import tensorflow as tf\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten\nfrom tensorflow.keras.optimizers import RMSprop, Adam","c498ea18":"import optuna\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_contour","4197c226":"diabetes_data = pd.read_csv(\"..\/input\/early-diabetes-classification\/diabetes_data.csv\", sep = \";\")\ndiabetes_data.head(3)","a8a983b4":"diabetes_data.info()","2cfcd84b":"diabetes_data.describe().T","c89dead5":"sns.histplot(data=diabetes_data, x = \"age\", bins=\"auto\")\nmean_age = diabetes_data[\"age\"].mean()\nplt.axvline(mean_age, c = \"r\", label = \"mean age\")\nplt.title(\"Distribution of Age\")\nplt.legend()\nplt.show()","56f58ab3":"sns.countplot(x = \"class\", data = diabetes_data)\nplt.title(\"Counts of samples per class\")\nplt.show()","242e8ac2":"sns.catplot(x = \"class\", y = \"age\", data = diabetes_data, hue = \"obesity\", kind = \"violin\", split = True)\nplt.title(\"Counts of samples per class per obesity\")\nplt.show()","f4b50a12":"sns.countplot(x = \"class\", data = diabetes_data, hue = \"gender\")\nplt.title(\"Counts of samples per class per gender\")\nplt.legend(loc = 'upper center')\nplt.show()","ed0b88b1":"X, y = diabetes_data.drop(\"class\", axis = 1), diabetes_data[[\"class\"]]","ccc6526b":"X = pd.get_dummies(X)\nX.head(3)","376c55bf":"gb_class = GradientBoostingClassifier(random_state = 42)","7990537f":"kf = KFold(n_splits = 5, shuffle = True, random_state = 42)","37a22af1":"scores = cross_validate(gb_class, X, y, cv = kf, scoring = \"accuracy\", return_train_score = True, n_jobs = -1)","91e85d86":"scores","a9165d90":"accuracy = scores['test_score'].mean()\nprint(f\"Base Accuracy: {round(accuracy, 2) * 100}%.\")","249f816c":"def objective(trial, X, y, cv, scoring):\n  \"\"\"\n  An objective function to tune hyperparameters of Gradient Boosting Classifier.\n  Args:\n    trial: an Optuna trial\n    X: DataFrame object, features\n    y: Series object, Labels\n    cv: k folds to cross-validate\n    scoring: String, evaluation metric\n  Return:\n    Mean test accuracy\n  \"\"\"\n\n  params = {\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5000, step = 100),\n    \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.3, log = True),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.9, step = 0.1),\n    \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n    \"random_state\": 42,\n    }\n  # Perform cross validation\n  gb_class = GradientBoostingClassifier(**params)\n\n  # Compute scores\n  scores = cross_validate(gb_class, X, y, cv = cv, scoring = scoring, n_jobs = -1)\n  accuracy = scores[\"test_score\"].mean()\n\n  return accuracy","40122215":"study = optuna.create_study(direction = \"maximize\")\n\nkf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\nfunc = lambda trial: objective(trial, X, y, cv = kf, scoring = \"accuracy\")","d7351515":"%%time\n# Start optimizing with 100 trials\nstudy.optimize(func, n_trials = 100)","b42fb174":"print(f\"The highest accuracy reached by this study: {(study.best_value) * 100}%.\")","3e1af76d":"print(\"Best params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t{key}: {value}\")","857d5c31":"plot_optimization_history(study)","c2006960":"plot_param_importances(study)","76576172":"plot_contour(study)","25b3e71d":"N_TRAIN_EXAMPLES = 60000\nN_TEST_EXAMPLES = 10000\nBATCH_SIZE = 128\nCLASSES = 10\nEPOCHS = 10","e061bf2a":"def objective(trial):\n    \"\"\"\n    An objective function to tune hyperparameters of Conv2D layer of a Neural Network.\n    Args:\n        trial: an Optuna trial\n    Return:\n    Accuracy on test data\n    \"\"\"\n    # Clear cluster from previous Keras session\n    clear_session()\n\n    # Load, preprocess, and split data\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    img_x, img_y = x_train.shape[1], x_train.shape[2]\n    x_train = x_train.reshape(-1, img_x, img_y, 1)[:N_TRAIN_EXAMPLES].astype(\"float32\") \/ 255.0\n    x_test = x_test.reshape(-1, img_x, img_y, 1)[:N_TEST_EXAMPLES].astype(\"float32\") \/ 255.0\n    y_train = y_train[:N_TRAIN_EXAMPLES]\n    y_test = y_test[:N_TEST_EXAMPLES]\n    input_shape = (img_x, img_y, 1)\n\n    # Build model\n    model = Sequential([\n              Conv2D(\n                  filters = trial.suggest_categorical(\"filters\", [64, 128]),\n                  kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5]),\n                  strides = trial.suggest_categorical(\"strides\", [1, 2]),\n                  padding = trial.suggest_categorical(\"padding\", [\"valid\", \"same\"]),\n                  activation = trial.suggest_categorical(\"activation\", [\"relu\", \"linear\"]),\n                  kernel_regularizer = trial.suggest_categorical(\"kernel_regularizer\", [\"l1\", \"l2\", \"l1_l2\"]),\n                  input_shape = input_shape\n              ),\n              Conv2D(filters = 64, kernel_size = 3, activation = \"relu\", strides = 1),\n              Flatten(),\n              Dense(CLASSES, activation = \"softmax\")\n          ])\n\n    # Compile  model with a sampled learning rate\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n    model.compile(loss = \"sparse_categorical_crossentropy\",\n                            optimizer = Adam(learning_rate = learning_rate),\n                            metrics = [\"accuracy\"])\n\n    # Train model            \n    model.fit(x_train, y_train, \n                        validation_data = (x_test, y_test),\n                        shuffle = True,\n                        batch_size = BATCH_SIZE,\n                        epochs = EPOCHS)\n              \n    # Evaluate model accuracy on the test set\n    score = model.evaluate(x_test, y_test)\n    return score[1]","70fa20a2":"study_nn = optuna.create_study(direction = \"maximize\")","feffecef":"study_nn.optimize(objective, n_trials = 100) # timeout = 600","cfc44b23":"print(f\"Number of finished trials: {len(study_nn.trials)}\")","e0a9c8ab":"trial = study_nn.best_trial\nprint(\"Highest accuracy: {:.2f}%.\".format(trial.value * 100))","c1cb229e":"print(\"Params:\")\nfor key, value in trial.params.items():\n    print(f\"\\t{key}: {value}\")","2bb33922":"plot_optimization_history(study_nn)","c7d515d3":"plot_param_importances(study_nn)","cdc3aa42":"plot_contour(study_nn)","75d54494":"## Chapter 2. Hyperparameter tuning of a Neural Network  <a class=\"anchor\"  id=\"chapter2\"><\/a>","4afde26a":"Mean age of people whose medical data were observed is slightly less than 50 years. There are as many younger participants (e.g., between 35 and 50) as older ones (e.g., between 50 and 65).","40ea02ce":"I use `Early Diabetes Classification` dataset (available in Kaggle [here](https:\/\/www.kaggle.com\/andrewmvd\/early-diabetes-classification)) to explore how *Optuna* works. \n\nThis dataset contains 520 observations with 17 characteristics, collected using direct questionnaires and diagnosis results from patients in a Hospital in Sylhet, Bangladesh. Due to the presence of a relatively long asymptomatic phase, early detection of diabetes is always desired for a clinically meaningful outcome. Around 50% of all people suffering from diabetes are undiagnosed because of its long-term asymptomatic phase. However, it is assumed that certain symptoms (or a combination thereof) might suggest developing diabetes. Datasets features hold information about the presence of a symptom, or not. Diabetic patients are placed in class 1, and those without the illness - in class 0.\n\nThe dataset was loaded and saved in `diabetes_data`. Its head rows are displayed thereafter.","3234b616":"`gender` holds categorical values. Machine Learning algorithms and Neural Networks expect to process and compute numeric values. Therefore, the categorical features are one-hot encoded (via `pd.get_dummies()`) (see the last 2 columns). It should be noted here that normalization is not necessary since all values are within one and the same range.","41137585":"## 1.3. Preprocess Data <a class=\"anchor\"  id=\"subsection3\"><\/a>","67541d62":"Due to the small number of samples (520), validation and test data should be set aside. Otherwise, the model would be trained on a much smaller number of samples (e.g. 418 if 80% are used for training). \n\n`cross_validate` is used to evaluate model performance during training, to avoid overfitting, and to monitor training. It accepts the Machine Learning algorithm, features, labels, number of folds to cross validate (on each iteration, one fold of random samples is withheld for validation), and an evaluation metric. \n\n*n_jobs* indicates the number of jobs to run in parallel. Training the estimator and computing the score are parallelized over the cross validation splits. A value set to -1 means that all available processors are used simultaneously for cross validation.","fbdf8db6":"Gradient Boosting Classifier builds an additive model in a forward stage-wise fashion. It allows for optimization of arbitrary differentiable loss functions. Gradient Boosting is a little bit powerful algorithm for diabetes dataset but for the sake of this exercise, I explore how `Optuna` looks into the defined search space to tune its hyperparameters.\n\nGradient Boosting is instantiated with its default values. The hyperparameters which values are tuned later, and their defaults are given below but explained later. \n\n- n_estimators = 100\n- learning_rate = 0.1\n- max_depth = 3\n- subsample = 1.0\n- max_features = None","36bd54c7":"### 1.4.2  Tune `GradientBoostingClassifier` hyperparameters with `Optuna` <a class=\"anchor\"  id=\"sub_sub2\"><\/a>","ff42e232":"It is interesting to note that diabetes was observed in similar number obese and non-obese individuals. ","85658f0b":"Thereafter, the study optimizes the objective function by doing 100 trials. Each trial runs the objective function, which fits the data for 10 epochs. Summary is displayed at the end, including: accuracy, hyperparameters and their values for this trial, the best trial so far and its accuracy. \n\n*Note: There are warnings which do not affect model training and validation. Colab does not display them*.","463b7ac8":"`Optuna` is an automatic hyperparameter optimization software framework, designed for Machine Learning. It uses the terms *study* and *trial*.\nThe *trial* is a single execution of the objective function. It is a process of evaluating an objective function. On the other hand, a *study* is an optimization based on the objective function. It corresponds to an optimization task, i.e., a set of trials.\n\nThe objective function returns the value which should be optimized. In this case, this is the evaluation metric showing model accuracy. Arguments of the objective function are `Optuna`'s trial, features, labels, number of folds for cross validation, and scoring metrics.\n\nThe first thing to do in an objective function is to create a search space using built-in `Optuna` methods. For diabetes example, I create a (not so) small search space of Gradient Boosting hyperparameters.\n\nThe search space is a dictionary. trial object\u2019s `suggest_*` functions are used to create possible values to search over. These functions require at least the hyperparameter name, minimum and maximum of the range to search over, or possible categories for categorical hyperparameters. To make the space smaller, suggest_float and suggest_int could have additional step or log arguments.\n\nThe hyperparameters and their min and max values are stored in a dictionary `params`. The objective function will look into the best combination of values of 5 hyperparameters:\n\n- `n_estimators` (default = 100) shows the number of boosting stages to perform.\n\n- the `learning_rate` (default = 0.1) shrinks the contribution of each tree by its value. There is a trade-off between learning_rate and n_estimators.\n\n- `max_depth` (default = 3) shows the maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. \n\n- `subsample` (default = 1.0) defines the fraction of samples to be used for fitting the individual base learners.\n\n- `max_features` (default=None) indicates the number of features to consider when looking for the best split. Possible values: *auto*, *sqrt*, *log2*.\n","c62b069e":"There are not many `Optuna` examples or tutorials with Neural Networks. I found only the Keras and TensorFlow examples in Optuna's [GitHub repository](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/keras\/keras_simple.py). Below, I play with Keras example but with another (Fashion-MNIST) dataset. The task is to optimize filter and kernel size of a Convolutional layer, stride and layer activation.\n\nFashion-MNIST is a dataset consisting of a training set of 60,000 images and a test set of 10,000 images. Each example is a 28x28 grayscale image, associated with a label from 10 classes (0 - T-shirt\/top, 1 -\tTrouser, 2 - Pullover, 3 - Dress, 4 - Coat, 5 - Sandal, 6 - Shirt, 7 - Sneaker, 8 - Bag, 9 - Ankle boot).\n\nFashion-MNIST image data is loaded via Keras API.\n\nConstants are defined in advance: number of train and validation samples (60,000 and 10,000, respectively), batch size (128), classes (10), and epochs (10).","aa983f80":"##### **Plot contours**\n\nThe function below plots the parameter relationship as contour plot in a study. It should be borne in mind that if a parameter contains missing values, a trial with missing values is not plotted.","75fc3316":"`Optuna` study is created by calling `create_study()`. It works with its default settings (see [here](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.create_study.html#optuna.study.create_study)) save *direction*. The latter defines direction of optimization. In this case, accuracy should be *maximized*.\n\nCross validation is performed on 5 folds prepared by `KFold()`.\n\n`func` passed to `study` gets all possible combinations to look over. The `objective` function creates an in-memory study.","0cdb842b":"## Imports  <a class=\"anchor\"  id=\"imports\"><\/a>","5443d589":"# Optimize model performance with `Optuna`\n\nThe general objective of Machine Learning algorithms is to minimize a loss function (e.g., the distance between two points) and to maximize a success metric (e.g., Mean Squared Error, Accuracy, Precision, etc.). A sample of a dataset with many variables is located on a point somewhere in a multidimensional space. Algorithms should find the optimal point in this space (e.g., the global minimum) where the loss function reaches its minimum value. \n\nIn fact, to find a combination of hyperparameters that returns the best results is one of the greatest challenges in Machine Learning. Luckily, `scikit learn` and `pyspark` have built in functions for hyperparameter tuning ([`GridSearchCV()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) and [`pyspark.ml.tuning()`](https:\/\/spark.apache.org\/docs\/latest\/ml-tuning.html)). \n\nTensorFlow in general, and Keras in particular, do not provide such a solution (to the best of my knowledge). Thus, it could be assumed that even the best performing Neural Network might return better outputs (i.e., the search space could be broader and might offer another minimum) with another combination of hyperparameters.\n\nA couple of months ago [Why Is Everyone at Kaggle Obsessed with Optuna For Hyperparameter Tuning](https:\/\/towardsdatascience.com\/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c) grabbed my attention. I reviewed [Optuna](https:\/\/optuna.readthedocs.io\/en\/stable\/index.html) documentation and couldn't believe that I didn't come across it so far. It is an automatic hyperparameter optimization software framework designed to help Machine Learning.\n\nThis Notebook explores how `Optuna` works with a Scikit Learn algorithm and with a Neural Network. To avoid excessive Exploratory Data Analysis and data preprocessing, I use simple datasets. ","9a6269e9":"## Table of Contents\n\n* [Imports](#imports)\n* [Chapter 1. Hyperparameter tuning of a Machine Learning algorithm](#chapter1)\n    * [Section 1.1. Load Dataset](#subsection1)\n    * [Section 1.2. Exploratory Data Analysis](#subsection2)\n    * [Section 1.3. Preprocess Data](#subsection3)\n    * [Section 1.4. Modelling](#subsection4)\n      * [1.4.1 Predict class with default values of `GradientBoostingClassifier`](#sub_sub1)\n      * [1.4.2 Tune `GradientBoostingClassifier` hyperparameters with `Optuna`](#sub_sub2)\n    * [Section 1.5. Use visuals to get more insights](#subsection5)\n* [Chapter 2. Hyperparameter tuning of a Neural Network ](#chapter2)\n    * [Section 2.1. Load data, build, train and evaluate model](#subsection6)\n    * [Section 2.2. Tune Neural Network hyperparameters with `Optuna`](#subsection7)\n    * [Section 2.3. Use visuals to get more insights](#subsection8)\n* [Conclusion](#conclusion)","8885bb67":"## 1.5. Use visuals to get more insights <a class=\"anchor\"  id=\"subsection5\"><\/a>","5df3db97":"There are more men than women in the dataset. Almost equal number of male patients were diagnosed with and without diabetes. However, diabetes was observed in slightly more women than men.","184cc0ba":"The dataset contains 520 entries described with 16 features (17th column holds the class). There are not missing values and all values are integers except the ones in `gender` column.","bb18692e":"## 2.1. Load data, build, train and evaluate model <a class=\"anchor\"  id=\"subsection6\"><\/a>","e661342b":"## 2.3. Use visuals to get more insights <a class=\"anchor\"  id=\"subsection8\"><\/a>","a74452f4":"Gradient Boosting slightly overfits the data. Its accuracy on three folds reaches 100%. Nonetheless, its performance on validation samples is good as well \u2013 model accuracy ranges between 95% and 98%. Thus, the Base model's accuracy is exactly 97%.","282ad417":"## Chapter 1. Hyperparameter tuning of a Machine Learning algorithm  <a class=\"anchor\"  id=\"chapter1\"><\/a>","6271030a":"#### **Plot contours**\n\nContour plot is not very informative here, either. Nonetheless, it could be seen that 64 neurons with ReLU activation (top row, second column) is coloured in grey \u2013 an indicator for a higher objective value (accuracy).","46b8c380":"To summarize, `Optuna` finds the combination of hyperparameters that returns the best performing model (similarly to `GridSearchCV()` of Scikit-learn). Its `visualization` module provides utility functions for plotting the optimization process using `plotly` and `matplotlib`. \n\nTo optimize hyperparameters of Machine Learning algorithms looks a straightforward and not so complex task. However, tuning a Neural Network is much more time-consuming endeavour.","ad80b3de":"## 1.1. Load Dataset <a class=\"anchor\"  id=\"subsection1\"><\/a>","76e567d6":"## 1.2. Exploratory Data Analysis <a class=\"anchor\"  id=\"subsection2\"><\/a>","77bbf750":"All deep learning steps are wrapped in the objective function: data load, data preprocessing, train-test split, model build, model compile, model train, and model evaluate.\n\n`Optuna` is used only to tune the first (Convolutional) layer, and to find the optimal learning rate.","1fb0e67a":"The best trial in this study is No. 93. It reached 98.27% accuracy (higher than the Base value) with hyperparameters as shown below:","57c5f550":"## 1.4. Modelling <a class=\"anchor\"  id=\"subsection4\"><\/a>","979465af":"## Conclusion  <a class=\"anchor\"  id=\"conclusion\"><\/a>","36a197cb":"A study is created to maximize accuracy (score).","7a84c300":"100 trials were successfully passed.","48b3f487":"People with diabetes outnumber healthy ones. There are 200 samples of non-sick people, and 320 where diabetes was confirmed.","37324bfd":"The most accurate Neural Network has 64 units in its first Convolutional layer, each of size 5 x 5 pixels; \"same\" padding preserves image size. Non-linearity is provided by \"ReLU\" activation function. Loss is regularized with Ridge regression (L2 regularization), and the learning rate is diminished to 0.00098.","d13f780c":"#### **Plot optimization history**\nThe plot below shows how the objective value (accuracy) varied over each epoch. Four times it fell to 0.1; all other trials accuracy fluctuated around 90%.","56701529":"#### **Plot hyperparameters importance**\n\nIt is not surprising that the learning rate is the most important hyperparameter for Neural Network performance. Interestingly, kernel size also played a role. All other hyperparameters had insignificant impact for the final result.","bd3b58a4":"##### **Plot hyperparameters importance**\n\nThe code line below plots hyperparameters importances during optimization. In this case, number of estimators and the learning rate are the most important, followed by  trees maximum depth. Maximum features and subsample are not so important.","a35149e1":"All numeric features but `age` contain binary values - 0 or 1. In other words, either the symptom was observed (1, True), or not (0, False).","0ad78e16":"To conclude, `Optuna` is a great library for hyperparameters tuning. It is very similar to `GridSearchCV()`, which could be used with Scikit Learn algorithms. TensorFlow and Keras, to the best of my knowledge, do not offer similar function, which makes finding the best Neural Network quite a painful and time-consuming task. `Optuna` addresses this weakness. \n\nA matter for further (personal) research is to find out how to use `Optuna` to tune hyperparameters of more than one layer. Furthermore, the number and type of layers is also a hyperparameter, which `Optuna` functions seem not capable to optimize.  ","b5d7da54":"##### **Plot optimization history**\n\n`plot_optimization_history` plots optimization history of all trials in a study. The blue dots show accuracy on each trial, and the red line - the best value attained.","381660c8":"## 2.2. Tune Neural Network hyperparameters with `Optuna` <a class=\"anchor\"  id=\"subsection7\"><\/a>","c5a779e8":"All features are stored in `X`, and the label (class) in `y`.","1b8c2171":"Visuals (e.g., plots, diagrams, tables) help for getting more and better insights into the data. ","15dba75e":"### 1.4.1 Predict class with default values of `GradientBoostingClassifier` <a class=\"anchor\"  id=\"sub_sub1\"><\/a>","ae615cdb":"This function is optimized by calling `optimize` over the `study`. It accepts a callable (function) that implements the objective function.  *n_trials* define the number of optimization trials.\n\nRunning an optimization study returns the following artefacts for each trial:\n- Trial number and if it was finished or not\n- The optimized value (in this case, accuracy)\n- A dictionary of parameters used in the trial to reach optimized value\n- Best trial so far.\n\n*Note: Kaggle Notebooks display warnings which were not raised in Colab. Nonetheless, all trials were passed successfully, regardless the excessive number of warnings*.","d1a89cdb":"The last trial reached the highest accuracy of 91.44%.","d3e042aa":"All samples are shuffled and split into 5 folds to be used for cross validation."}}