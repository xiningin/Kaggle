{"cell_type":{"339a6f6b":"code","ec3af2fd":"code","1353869c":"code","826d5ffd":"code","beb23f5c":"code","687d857d":"code","6c893531":"code","df219001":"code","c1233cbd":"code","0cdc47ad":"code","1580cabe":"code","fcf1591e":"code","04f32006":"code","64280f4f":"code","e7ad171c":"code","c604b03f":"code","e2b4976d":"code","a91a7eda":"code","62c3245d":"code","505456ec":"code","4e550cf0":"code","b9c3a052":"code","29cbcb5e":"markdown","c3e209ea":"markdown","8befc751":"markdown","df9a02b5":"markdown","d05c5b46":"markdown","7a90cd00":"markdown","afce4a35":"markdown","a5b583ea":"markdown","4a047b5e":"markdown","327ceed0":"markdown","78be7bde":"markdown","f6085dd2":"markdown","20b8766e":"markdown","cbff7da1":"markdown","7a8a3f03":"markdown","ab05b13e":"markdown","49cf6683":"markdown","7587268f":"markdown","829ab6ad":"markdown","d9ff09f7":"markdown"},"source":{"339a6f6b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('punkt')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nstop = set(stopwords.words('english'))\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom collections import Counter, defaultdict\n\nimport spacy\n#!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# For customizing our plots.\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n#plotly\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot, plot\n\n# Core packages for general use throughout the notebook.\nimport random\nimport warnings\nimport time\nimport datetime\nimport re\nimport string\n\n# Setting some options for general use.\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')","ec3af2fd":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","1353869c":"display(train.sample(5))","826d5ffd":"display(test.sample(5))","beb23f5c":"print(train.shape)\nprint(test.shape)","687d857d":"colors = ['lightsalmon', 'lightskyblue']\nfig = go.Figure([go.Bar(x=train['target'].value_counts().index, y=train['target'].value_counts().values,\n                        marker_color=colors)])\nfig.update_layout(title_text='Distribution of Tweets', title_x=0.5)\nfig.show()","6c893531":"# total word count\ntrain['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\ntest['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n\n# total unique word count\ntrain['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stopwords count\ntrain['stopword_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest['stopword_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain['char_count'] = train['text'].apply(lambda x: len(str(x)))\ntest['char_count'] = test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain['punctuation_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","df219001":"train.head(2)","c1233cbd":"meta_features = ['word_count','unique_word_count','stopword_count','url_count','mean_word_length',\n                 'char_count','punctuation_count','hashtag_count','mention_count']\n\nDISASTER_TWEETS = train['target'] == 1\n\nfor i, feature in enumerate(meta_features):\n    \n    a = f'{feature} Target Distribution in Training Set'\n    b = f'{feature} Training & Test Set Distribution'\n    fig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], subplot_titles=[a, b])\n    \n    hist_data = [train.loc[~DISASTER_TWEETS][feature], train.loc[DISASTER_TWEETS][feature]]\n\n    group_labels = ['Not Disaster', 'Disaster']\n    \n    fig2 = ff.create_distplot(hist_data, group_labels)\n    \n    fig.add_trace(go.Histogram(fig2['data'][0],\n                           marker_color='mediumslateblue'\n                          ), row=1, col=1)\n\n    fig.add_trace(go.Histogram(fig2['data'][1],\n                               marker_color='mediumvioletred'\n                              ), row=1, col=1)\n\n    fig.add_trace(go.Scatter(fig2['data'][2],\n                             line=dict(color='mediumslateblue', width=0.5)\n                            ), row=1, col=1)\n\n    fig.add_trace(go.Scatter(fig2['data'][3],\n                             line=dict(color='mediumvioletred', width=0.5)\n                            ), row=1, col=1)\n    \n\n    \n    hist_data = [train[feature], test[feature]]\n\n    group_labels = ['Training', 'Testing']\n    \n    fig2 = ff.create_distplot(hist_data, group_labels)\n    \n    fig.add_trace(go.Histogram(fig2['data'][0],\n                           marker_color='mediumturquoise'\n                          ), row=1, col=2)\n\n    fig.add_trace(go.Histogram(fig2['data'][1],\n                               marker_color='indianred'\n                              ), row=1, col=2)\n\n    fig.add_trace(go.Scatter(fig2['data'][2],\n                             line=dict(color='mediumturquoise', width=0.5)\n                            ), row=1, col=2)\n\n    fig.add_trace(go.Scatter(fig2['data'][3],\n                             line=dict(color='indianred', width=0.5)\n                            ), row=1, col=2)\n    \n    fig.show()\n\n","0cdc47ad":"lis = [\n    train[train['target'] == 0]['text'],\n    train[train['target'] == 1]['text']\n]","1580cabe":"cnt = 1\na = 'Non Disaster Tweets'\nb = 'Disaster Tweets'\nfig = make_subplots(rows=1, cols=2, subplot_titles=[a, b])\ncolors = ['darkturquoise', 'deepskyblue']\nfor i, j in zip(lis, colors):\n    \n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n    \n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:50]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    fig.add_trace(go.Bar(x=x, y=y, marker_color=colors[cnt-1]),\n                  row=1, col=cnt)\n    cnt+=1\n    \nfig.update_layout(\n    title=\"Most Common Unigrams\", title_x=0.5,\n    xaxis_title=\"Word\",\n    yaxis_title=\"Count\",\n)\nfig.show()","fcf1591e":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrain['text_clean'] = train['text'].apply(lambda x: remove_url(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_emoji(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_html(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_punct(x))","04f32006":"train['tokenized'] = train['text_clean'].apply(word_tokenize)\n\ntrain['lower'] = train['tokenized'].apply(lambda x: [word.lower() for word in x])\n\ntrain['stopwords_removed'] = train['lower'].apply(lambda x: [word for word in x if word not in stop])\n\ntrain['pos_tags'] = train['stopwords_removed'].apply(nltk.tag.pos_tag)\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \ntrain['wordnet_pos'] = train['pos_tags'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\nwnl = WordNetLemmatizer()\n\ntrain['lemmatized'] = train['wordnet_pos'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrain['lemmatized'] = train['lemmatized'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['lemma_str'] = [' '.join(map(str, l)) for l in train['lemmatized']]","64280f4f":"lis = [\n    train[train['target'] == 0]['lemma_str'],\n    train[train['target'] == 1]['lemma_str']\n]","e7ad171c":"cnt = 1\nfig = make_subplots(rows=1, cols=2)\ncolors = ['darkturquoise', 'deepskyblue']\nfor i, j in zip(lis, colors):\n    \n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n    \n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    fig.add_trace(go.Bar(x=x, y=y, marker_color=colors[cnt-1]),\n                  row=1, col=cnt)\n    cnt+=1\n    \nfig.update_layout(\n    title=\"Most Common Unigrams\", title_x=0.5,\n    xaxis_title=\"Word\",\n    yaxis_title=\"Count\",\n)\nfig.show()","c604b03f":"def ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    cnt = 1\n    fig = make_subplots(rows=1, cols=2)\n    colors = ['darkturquoise', 'deepskyblue']\n    for i, j in zip(lis, colors):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        fig.add_trace(go.Bar(x=x, y=y, marker_color=colors[cnt-1]),\n                  row=1, col=cnt)\n        cnt+=1\n        \n    fig.update_layout(\n        title=\"Most Common Unigrams\", title_x=0.5,\n        xaxis_title=\"Word\",\n        yaxis_title=\"Count\",\n    )\n    fig.show()","e2b4976d":"ngrams(2, 'Most Common Bigrams')","a91a7eda":"ngrams(3, 'Most Common Trigrams')","62c3245d":"def plotly_wordcloud(text, title):\n    wc = WordCloud(stopwords = set(STOPWORDS),\n                   max_words = 200,\n                   max_font_size = 100)\n    wc.generate(text)\n    \n    word_list=[]\n    freq_list=[]\n    fontsize_list=[]\n    position_list=[]\n    orientation_list=[]\n    color_list=[]\n\n    for (word, freq), fontsize, position, orientation, color in wc.layout_:\n        word_list.append(word)\n        freq_list.append(freq)\n        fontsize_list.append(fontsize)\n        position_list.append(position)\n        orientation_list.append(orientation)\n        color_list.append(color)\n        \n    # get the positions\n    x=[]\n    y=[]\n    for i in position_list:\n        x.append(i[0])\n        y.append(i[1])\n            \n    # get the relative occurence frequencies\n    new_freq_list = []\n    for i in freq_list:\n        new_freq_list.append(i*100)\n    new_freq_list\n    \n    trace = go.Scatter(x=x, \n                       y=y, \n                       textfont = dict(size=new_freq_list,\n                                       color=color_list),\n                       hoverinfo='text',\n                       hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                       mode='text',  \n                       text=word_list\n                      )\n    \n    layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                        'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    \n    fig.update_layout(\n        title=title, title_x=0.5,\n    )\n    \n    return fig","505456ec":"text = ' '.join(train[train[\"target\"]==1]['lemma_str'])\nplotly_wordcloud(text, 'Disaster Tweets')","4e550cf0":"text = ' '.join(train[train[\"target\"]==0]['lemma_str'])\nplotly_wordcloud(text, 'Non Disaster Tweets')","b9c3a052":"# Loading NER.\nnlp = en_core_web_sm.load() \n\ndef plot_named_entity_barchart(text):\n    \n    \"\"\"A function for extracting named entities and comparing them\"\"\"\n    \n    def _get_ner(text):\n        doc = nlp(text)\n        return [X.label_ for X in doc.ents]\n\n    ent = text.apply(lambda x: _get_ner(x))\n    ent = [x for sub in ent for x in sub]\n    counter = Counter(ent)\n    count = counter.most_common()\n\n    x, y = map(list, zip(*count))\n    \n    fig.add_trace(go.Bar(x=x, y=y, marker_color=colors[cnt-1]),\n                  row=1, col=cnt)\n    \ncnt = 1\na = 'Non Disaster Tweets'\nb = 'Disaster Tweets'\nfig = make_subplots(rows=1, cols=2, subplot_titles = [a, b])\ncolors = ['darkorange', 'cadetblue']\nfor i, j in zip(lis, colors):\n\n    def _get_ner(i):\n        doc = nlp(i)\n        return [X.label_ for X in doc.ents]\n\n    ent = i.apply(lambda x: _get_ner(x))\n    ent = [x for sub in ent for x in sub]\n    counter = Counter(ent)\n    count = counter.most_common()[:15]\n\n    x, y = map(list, zip(*count))\n    \n    fig.add_trace(go.Bar(x=x, y=y, marker_color=colors[cnt-1]),\n                  row=1, col=cnt)\n    \n    cnt+=1\n\nfig.update_layout(\n    title=\"Common Named-Entity Counts\", title_x=0.5,\n    xaxis_title=\"Word\",\n    yaxis_title=\"Count\",\n)\n\nfig.show()","29cbcb5e":"## Load the data","c3e209ea":"If you learn something or find it interesting do upvote\ud83d\udc4d","8befc751":"In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.","df9a02b5":"## Named Entity Recognition Analysis","d05c5b46":"## Exploratory Data Analysis","7a90cd00":"### Most Common Words","afce4a35":"Before cleaning the data we can see that most of the commmon words are useless and have no meaning so text cleaning is to be performed.","a5b583ea":"## Import Libraries","4a047b5e":"### Target Distribution","327ceed0":"This helps us to analyse how our data is distributed between the 2 classes - disaster and non disaster","78be7bde":"When we look at the NER results we can get lots of great insights. We can see that in disaster tweets countries, cities, states are much more common than non disaster ones. Again [](http:\/\/)nationality or religious or political group names are more likely to be mentioned in disaster tweets. These are great indicators for us...","f6085dd2":"### Meta Features","20b8766e":"## Data Cleaning","cbff7da1":"With the help of meta features we can figure out the difference between diaster and non disaster tweets.<br>\nSome of the meta features are:\n- word_count: number of words in text\n- unique_word_count: number of unique words in text\n- stopword_count: number of stop words in text\n- url_count number: of urls in text\n- mean_word_length: average character count in words\n- char_count number: of characters in text\n- punctuation_count: number of punctuations in text\n- hashtag_count: number of hashtags (#) in text\n- mention_count: number of mentions (@) in text","7a8a3f03":"## References:\n#### This kernel includes ideas from kernels below.\n- https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\/notebook\n- https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model\/notebook\n- https:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers\/notebook","ab05b13e":"As Data Cleaning is complete lets focus on Most Common Words","49cf6683":"Common words usually help us to get to know our data. For example lets say disaster and non disaster were not known to you so using common words we can get to know what the class is about","7587268f":"**Steps followed to clean the data:**\n- Removed urls, emojis, html tags and punctuations,\n- Tokenized the tweet base texts,\n- Lower cased clean text,\n- Removed stopwords,\n- Applied part of speech tags,\n- Converted part of speeches to wordnet format,\n- Applying word lemmatizer,\n- Converted tokenized text to string again.","829ab6ad":"## WordCloud","d9ff09f7":"## Motivation:\nI basically wanted to learn Exploratory Data Analysis in order to perform EDA quite easily on the datasets that I work with in the future. So I took up a dataset and learnt from quite other good notebooks and produced my own findings. I hope you too will learn from this notebook. <br>\n### Please upvote if you like the work..."}}