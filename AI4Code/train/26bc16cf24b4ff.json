{"cell_type":{"a56dd0da":"code","b241bd32":"code","0738adea":"code","3b831ed6":"code","eefa0d49":"code","65b67be2":"code","7543651d":"code","1bd3d4f9":"code","581ad65c":"code","52b77d37":"code","69576a8e":"code","f91a76bf":"code","908be3c3":"code","f9edd5d7":"code","dc892559":"code","004c650e":"code","b42b4b43":"code","a87bfb5e":"code","ff8967ad":"code","8b93e951":"code","af2ec685":"code","bbd74af3":"code","6495dc1b":"code","a658c71f":"code","bba6a202":"code","03c51427":"code","6f2aa1b5":"code","bacb6db7":"code","767c9862":"markdown","8d030acb":"markdown","7561ffd1":"markdown","6cfe004a":"markdown","014fed70":"markdown","1bb1ea5f":"markdown","fe2c1be2":"markdown","10a9f184":"markdown","6d7bdbc0":"markdown","8083b762":"markdown","9de01c35":"markdown","992ae44c":"markdown","f8477a45":"markdown","eca3bed4":"markdown","de36fb5c":"markdown","06d9d0df":"markdown","3b0e2f53":"markdown","3dcb7ed2":"markdown","a71fcb32":"markdown","b511d500":"markdown","62730eb7":"markdown","5c1b9283":"markdown","79f57739":"markdown","30351335":"markdown","e5da31f3":"markdown","0388c87d":"markdown","e96becb0":"markdown","b87c0ea8":"markdown","9eb7a5a2":"markdown","b76cd8de":"markdown","7bc88c4f":"markdown","666c9341":"markdown","db51c6ba":"markdown","2a052efc":"markdown","b151597d":"markdown","f66a154a":"markdown","a6846321":"markdown","11488c77":"markdown","a0c5327f":"markdown","81e46ef0":"markdown","ae237075":"markdown","2900e069":"markdown","665046f7":"markdown"},"source":{"a56dd0da":"# Importing standard packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List, Tuple\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, r2_score","b241bd32":"def logistic_func(z: List) -> np.array:\n        \n    \"\"\" Calculate logistic\/sigmoid function . \"\"\"\n\n    return 1\/(1+np.exp(-np.array(z)))","0738adea":"#\u00a0Plot logistic function\nplt.figure(figsize=(8,6))\nx = np.linspace(-6, 6, 1000)\ny = logistic_func(x)\n#\u00a0Plot decision boundary\nx1, y1 = [-6, 6], [0.5, 0.5]\nplt.xlabel(r'$x$', size=20)\nplt.ylabel(r'$y$', size=20)\nplt.title('Logistic Function (Sigmoid)')\nplt.grid(alpha=0.5)\nplt.plot(x, y);\nplt.plot(x1, y1, marker = 'o');\nplt.legend(labels=[\"Sigmoid Function\", \"Decision Boundary\"]);","3b831ed6":"#\u00a0Import dataset\ndf = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\n# Display dataframe\ndf","eefa0d49":"# Check for Nulls\ndf.info()","65b67be2":"#\u00a0Check for NaNs \ndf.isna().sum()","7543651d":"# Check for duplicates\ndf.duplicated().sum()","1bd3d4f9":"#\u00a0Overall statistics\ndf.describe()","581ad65c":"#\u00a0Bar chart of class ratio \ntarget_pd = pd.DataFrame(index = [\"No Disease\",\"Disease\"], columns= [\"Quantity\", \"Percentage\"])\n# No disease\ntarget_pd.loc[\"No Disease\"][\"Quantity\"] = len(df[df.columns[-1]][df[df.columns[-1]]==0].dropna())\ntarget_pd.loc[\"No Disease\"][\"Percentage\"] = target_pd.iloc[0,0]\/len(df[df.columns[-1]])*100\n# Disease\ntarget_pd.loc[\"Disease\"][\"Quantity\"] = len(df[df.columns[-1]][df[df.columns[-1]]==1].dropna())\ntarget_pd.loc[\"Disease\"][\"Percentage\"] = target_pd.iloc[1,0]\/len(df[df.columns[-1]])*100\n# Plot barchart\nfig = plt.figure(figsize = (10, 5))\nplt.bar(list(target_pd.index), target_pd.iloc[:,0], color ='maroon',width = 0.4)\nplt.ylabel(\"Number of cases\")\nplt.title(\"Distribution of disease and non-disease cases\");\n#\u00a0Print the dataframe\ntarget_pd","52b77d37":"# Histogram of features (check for skew)\nfig=plt.figure(figsize=(20,20))\nfor i, feature in enumerate(df.columns):\n    ax=fig.add_subplot(8,4,i+1)\n    df[feature].hist(bins=20,ax=ax,facecolor='black')\n    ax.set_title(feature+\" Distribution\",color='DarkRed')\n    ax.set_yscale('log')\nfig.tight_layout()  ","69576a8e":"# Check for correlation\nfig=plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot = True, cmap=\"tab20c\");\nfig.tight_layout()  ","f91a76bf":"#\u00a0Drop NaNs and duplicates\ndf = df.dropna().drop_duplicates().reset_index(drop=True)","908be3c3":"#\u00a0Overall statistics\ndf.describe()","f9edd5d7":"#\u00a0Bar chart of class ratio \ntarget_pd = pd.DataFrame(index = [\"No Disease\",\"Disease\"], columns= [\"Quantity\", \"Percentage\"])\n# No disease\ntarget_pd.loc[\"No Disease\"][\"Quantity\"] = len(df[df.columns[-1]][df[df.columns[-1]]==0].dropna())\ntarget_pd.loc[\"No Disease\"][\"Percentage\"] = target_pd.iloc[0,0]\/len(df[df.columns[-1]])*100\n# Disease\ntarget_pd.loc[\"Disease\"][\"Quantity\"] = len(df[df.columns[-1]][df[df.columns[-1]]==1].dropna())\ntarget_pd.loc[\"Disease\"][\"Percentage\"] = target_pd.iloc[1,0]\/len(df[df.columns[-1]])*100\n# Plot barchart\nfig = plt.figure(figsize = (10, 5))\nplt.bar(list(target_pd.index), target_pd.iloc[:,0], color ='maroon',width = 0.4)\nplt.ylabel(\"Number of cases\")\nplt.title(\"Distribution of disease and non-disease cases\");\n#\u00a0Print the dataframe\ntarget_pd","dc892559":"# Histogram of features (check for skew)\nfig=plt.figure(figsize=(20,20))\nfor i, feature in enumerate(df.columns):\n    ax=fig.add_subplot(8,4,i+1)\n    df[feature].hist(bins=20,ax=ax,facecolor='black')\n    ax.set_title(feature+\" Distribution\",color='DarkRed')\n    ax.set_yscale('log')\nfig.tight_layout()  ","004c650e":"# Check for correlation\nfig=plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot = True, cmap=\"tab20c\");\nfig.tight_layout()  ","b42b4b43":"#\u00a0Create X (features) and y (target) dataset\nX = df[df.columns[:-1]]\ny = df[df.columns[-1]]","a87bfb5e":"def feature_scaling(X: pd.DataFrame) -> pd.DataFrame:\n    \n    \"\"\" Normalises the features in X (dataframe) and returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1. \"\"\"\n    \n    # Return normalised data\n    return (X - np.mean(X, axis=0))\/np.std(X, axis=0, ddof=0)","ff8967ad":"# Create normalised data\nX = feature_scaling(X)\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/4, random_state=42, stratify=y)\n# Re-index\nX_train = X_train.reset_index(drop=True) \ny_train = y_train.reset_index(drop=True) \nX_test = X_test.reset_index(drop=True) \ny_test = y_test.reset_index(drop=True) ","8b93e951":"# Add ones to the dataframes\nm,n = X_train.values.shape\no,p = X_test.values.shape\nX_train = pd.concat((pd.DataFrame(np.ones((m, 1)), columns= ['Bias']),X_train),axis=1)\nX_test = pd.concat((pd.DataFrame(np.ones((o, 1)), columns= ['Bias']),X_test),axis=1)","af2ec685":"def LR_CG(X: pd.DataFrame, y: pd.Series, theta: np.array) -> Tuple[Dict, float]:\n    \n    \"\"\" Calculate the cost and gradients of the logistic model via gradient descent. \"\"\"\n\n    n = X.shape[0]\n    h = self.logistic_func(X @ theta)\n    # Cost function (add minus for appropriacy)\n    cost = -1\/n * (y.T @ np.log(h) + (1 - y).T @ np.log(1-h)) \n    # Derivatives\n    dtheta = 1\/n * (X.T @ (h-y))\n    # Store gradients in a dictionary\n    grads = {\"dtheta\": dtheta}\n    return grads, cost","bbd74af3":"def BGD(X: pd.DataFrame, y: pd.Series, num_iterations: int, theta: np.array) -> Tuple[Dict, Dict, float]:\n    \n    \"\"\" Perform batch gradient descent on theta parameters for the logistic regression to find minimum cost function. \"\"\"\n\n    costs = []\n    for i in range(num_iterations):\n        # Calculate cost and gradients \n        grads, cost = LR_CG(X, y, theta)\n        # Retrieve derivatives from grads\n        dtheta = grads[\"dtheta\"]\n        # Updating estimate parameters\n        theta = theta - learning_rate * dtheta  \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        # Save parameters and gradients in dictionary\n        params = {\"theta\": theta}\n        grads = {\"dtheta\": dtheta}\n    return params, grads, costs","6495dc1b":"def predict(X: pd.DataFrame, y: pd.Series, theta_estimate: np.array, threshold: float) -> pd.Series:\n    \n    \"\"\" Compute regression predictions for the logistic model. \"\"\"\n\n    n = X.shape[0]\n    #\u00a0Initialise parameters\n    y_pred = np.zeros(n)\n    # Compute vector hypothesis predicting the probabilities\n    h = logistic_func(X @ theta_estimate)\n    # Convert probabilities y_log_prob to binary predictions\n    y_pred[h>=threshold] = 1\n    y_pred[h<threshold] = 0\n    return y_pred\n","a658c71f":"class LogisticRegression():\n    \n    def __init__(self, theta: np.array, num_iterations: int, learning_rate: float, threshold: float):\n    \n        \"\"\" Initialise parameters. \"\"\"\n       \n        self.num_iterations = num_iterations\n        self.learning_rate = learning_rate\n        self.threshold = threshold\n    \n    def fit(self, X: pd.DataFrame, y: pd.Series) -> np.array:\n    \n        \"\"\" Fit logistic regression model. \"\"\"\n\n        # Batch Gradient Descent\n        params, grads, costs = self.BGD(X, y)\n        # Retrieve theta parameters from dictionary \"parameters\"\n        return params[\"theta\"]\n    \n    def predict(self, X: pd.DataFrame, theta_estimate: np.array) -> pd.Series:\n    \n        \"\"\" Compute regression predictions for the logistic model. \"\"\"\n\n        n = X.shape[0]\n        #\u00a0Initialise parameters\n        y_pred = np.zeros(n)\n        # Compute vector hypothesis predicting the probabilities\n        h = self.logistic_func(X @ theta_estimate)\n        # Convert probabilities y_log_prob to binary predictions\n        y_pred[h>=self.threshold] = 1\n        y_pred[h<self.threshold] = 0\n        return y_pred\n    \n    def logistic_func(self, z: List) -> np.array:\n        \n        \"\"\" Calculate logistic\/sigmoid function . \"\"\"\n        \n        return 1\/(1+np.exp(-np.array(z)))\n\n    def LR_CG(self, X: pd.DataFrame, y: pd.Series, theta: np.array) -> Tuple[Dict, float]:\n    \n        \"\"\" Calculate the cost and gradients of the logistic model via gradient descent. \"\"\"\n        \n        n = X.shape[0]\n        h = self.logistic_func(X @ theta)\n        # Cost function (add minus for appropriacy)\n        cost = -1\/n * (y.T @ np.log(h) + (1 - y).T @ np.log(1-h)) \n        # Derivatives\n        dtheta = 1\/n * (X.T @ (h-y))\n        # Store gradients in a dictionary\n        grads = {\"dtheta\": dtheta}\n        return grads, cost\n\n    def BGD(self, X: pd.DataFrame, y: pd.Series) -> Tuple[Dict, Dict, float]:\n    \n        \"\"\" Perform batch gradient descent on theta parameters for the logistic regression to find minimum cost function. \"\"\"\n    \n        costs = []\n        # Initialise theta\n        theta = np.zeros(shape=(X.shape[1]), dtype=np.float32)\n        for i in range(self.num_iterations):\n            # Calculate cost and gradients \n            grads, cost = self.LR_CG(X, y, theta)\n            # Retrieve derivatives from grads\n            dtheta = grads[\"dtheta\"]\n            # Updating estimate parameters\n            theta = theta - self.learning_rate*dtheta  \n            # Record the costs\n            if i % 100 == 0:\n                costs.append(cost)\n            # Save parameters and gradients in dictionary\n            params = {\"theta\": theta}\n            grads = {\"dtheta\": dtheta}\n        return params, grads, costs","bba6a202":"# Create parameters\nparams = {\"theta\": np.zeros(shape=(X_train.shape[1]), dtype=np.float32), \"num_iterations\": 1000, \"learning_rate\": 0.1, \"threshold\": 0.5}\n# Instantiate model\nheart_disease_model = LogisticRegression(**params)","03c51427":"# Fit model to training dataset to obtain estimates\ntheta_train = heart_disease_model.fit(X_train, y_train)\n# Obtain predictions for training and test dataset\ny_pred_train = heart_disease_model.predict(X_train, theta_train)\ny_pred_test = heart_disease_model.predict(X_test, theta_train)\n#\u00a0Calculate R2 score on training and test dataset\nprint(\"Theta estimates are: \\n{}\".format(theta_train))\nprint(\"Train dataset accuracy: {}\".format(np.float(sum(y_pred_train==y_train)) \/ float(len(y_train))))\nprint(\"Test dataset accuracy: {}\".format(np.float(sum(y_pred_test==y_test)) \/ float(len(y_test))))","6f2aa1b5":"# Print confusion matrix\ncm_test = confusion_matrix(y_test, y_pred_test)\nax = sns.heatmap(cm_test, annot=True, fmt=\".1f\")\nax.set(xlabel=\"Predicted Labels\", ylabel=\"True Labels\");","bacb6db7":"#\u00a0Print train metric report\npd.DataFrame(classification_report(y_test, y_pred_test, output_dict=True))","767c9862":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","8d030acb":"For most machine learning models, we would like them to have low bias and low variance - that is, the model should perform well on the training set (low bias) and also the test set, alongside with other new random test sets (low variance). Therefore, to test for bias and variance of our model, we shall split the dataset into training and test set. We will not be tuning any hyperparameters (and thus do not need a validation set).  We will not be tuning any hyperparameters (and thus do not need a validation set). \n\nFor these functions, the $X$ dataset (of features) should have a column 1's as the first column to account for the bias term\/intercept co-efficient. Before this occurs, one should check the order of magnitude of the features - if they differ hugely, one must apply feature scaling. Having looked at the data however, it is clear that the order of magnitude of some of the features are very different, so we must perform feature scaling. ","7561ffd1":"We can see not much has changed from the overall statistics of the data after pre-processing which is excellent! We can no proceed with machine learning models.\n\n**Note:** Instead of just removing the NaNs, we could have imputed them with different techniques. ","6cfe004a":"<center> <h1>\ud83d\udcc9 Logistic Regression \ud83d\udcc9 <\/h1> <\/center>","014fed70":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","1bb1ea5f":"Let us look at the information obtained **after** applying the pre-processing steps:","fe2c1be2":"Some comments about the code implementations:\n\n1. Here we have created an entire python class for linear regression to make it similar to a machine learning pipeline format. If you would like to just use the functions by themselves, simply take them from the individual sections.\n2. For the batch gradient descent function alone, we must feed an initialised theta parameter - usually this can be just zeros. \n3. We have kept with vectorised notation as it is much faster using matrices than loops.\n4. Some of the code inputs have been hidden to make the notebook neater. You may look at the code by clicking on the 'show' button. ","10a9f184":"# Data Processing","6d7bdbc0":"## Extra","8083b762":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","9de01c35":"# Background","992ae44c":"Recall that the parameters of our model are the $\\theta_j$ values. These are\nthe values you will adjust to minimize cost $G(\\underline{\\theta})$. One way to do this is to\nuse the batch gradient descent algorithm. In batch gradient descent, each\niteration performs the update\n\n$$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial G(\\boldsymbol\\theta)}{\\partial \\theta_j} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n\nWith each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $G(\\boldsymbol \\theta)$ (see p.g. 10 [Logistic Regression](https:\/\/web.stanford.edu\/~jurafsky\/slp3\/5.pdf)).","f8477a45":"<p> <center> This notebook is in <span style=\"color: green\"> <b> Active <\/b> <\/span> state of development! <\/center> <\/p>  \n<p> <center> Be sure to checkout my other notebooks for <span style=\"color: blue\"> <b> knowledge, insight and laughter <\/b> <\/span>! \ud83e\udde0\ud83d\udca1\ud83d\ude02<\/center> <\/p> ","eca3bed4":"Types of Logistic Regression:\n\n1. Binary (Pass\/Fail)\n2. Multi (Cats, Dogs, Sheep)\n3. Ordinal (Low, Medium, High)","de36fb5c":"Recall that the cost function (known as cross entropy) in logistic regression is:\n\n$$ G(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} Cost\\left(h_\\theta\\left( x^{(i)} \\right), y^{(i)} \\right)$$\nwhere\n$$ Cost\\left(h_\\theta\\left( x \\right), y \\right) = \\log\\left(h_\\theta\\left( x \\right) \\right) \\qquad \\text{if} \\ y=1$$\n$$ Cost\\left(h_\\theta\\left( x \\right), y \\right) = -\\log\\left(1-h_\\theta\\left( x \\right) \\right) \\qquad \\text{if} \\ y=0$$\n\nCompressing these events together into one (by treating the predictions as Bernoulli random variable events), we obtain:\n\n$$ G(\\boldsymbol{\\theta}) = \\pm \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n\n**Note:** Depending on visualisation, we'd like our cost function to be positive so in this case we will take $G(\\boldsymbol \\theta)$ to take on the negative argument.","06d9d0df":"# Aim","3b0e2f53":"Our current prediction\/logistic function returns a probability score between 0 and 1. In order to map this to a discrete class (true\/false, cat\/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 0.\n\n$$p\u22650.5,\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60=1$$\n$$p<0.5,\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60=0$$\n\nFor example, if our threshold was 0.5 and our prediction function returned 0.7, we would classify this observation as positive (class 1). If our prediction was 0.2 we would classify the observation as negative (class 0). For logistic regression with multiple classes we could select the class with the highest predicted probability.","3dcb7ed2":"The aim is to provide, from scratch, code implementations for linear regression problems. This will involve both the main functions needed to solve a linear regression and some additional utility functions as well.\n\n**Note**: We will not be diving into in-depth exploratory data analysis, feature engineering etc... in these notebooks and so will not be commenting extensively on things such as skewness, kurtosis, homoscedasticity etc...","a71fcb32":"# Model Testing and Results","b511d500":"Logistic regression, despite its name, is a linear model for classification rather than regression. The reason why the term logistic *regression* is used becomes obvious once we examine the logistic function (often also called sigmoid function):\n$$g(z) = \\frac{1}{1+e^{-z}}$$\n\nRecall that the logistic regression hypothesis is defined as:\n\n$$ h_\\theta(\\underline{x}) = g(\\underline{x}^T\\underline{\\theta})$$\n\nwhere the function $g$ is the sigmoid function.","62730eb7":"It is easier to use vectorised notation to code this:\n\n$$ h_\\theta\\left( x \\right) = g\\left(X \\theta \\right) \\qquad \\text{logistic regression hypothesis} $$\n$$ G(\\boldsymbol{\\theta}) = \\pm \\frac{1}{n} \\cdot \\left(y^T log\\left(h\\right) - \\left(1-y\\right)^T log\\left(1-h\\right) \\right) \\qquad \\text{cost function} $$\n$$ \\frac{\\partial G(\\boldsymbol\\theta)}{\\partial \\theta} = \\frac{1}{n} \\cdot X^T(h-y) \\qquad \\text{gradient function} $$","5c1b9283":"## Import Modules","79f57739":"**Note:** The bottom labels are the true labels and the side labels are the predicted labels.","30351335":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","e5da31f3":"# Summary","0388c87d":"## Logistic Function (Sigmoid)","e96becb0":"# Data Visualisation","b87c0ea8":"## Vectorised notation","9eb7a5a2":"Through partial differentiation, the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\nelement (for $j = 0, 1, \\cdots , n$) is defined as follows:\n\n$$ \\frac{\\partial G(\\boldsymbol\\theta)}{\\partial \\theta_j} = \\frac{1}{n} \\sum_{i=1}^n \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n\n**Note:** While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$.","b76cd8de":"Thanks for reading this notebook. If there are any mistakes or things that need more clarity, feel free to respond in the comment section and I will be happy to reply.\n\nAs always, please leave an upvote - it would also be helpful if you cite this documentation if you are going to use any of the code. \ud83d\ude0a\n\n#CodeWithSid","7bc88c4f":"## Splitting dataset","666c9341":"# Data Collection","db51c6ba":"- It is clear that once training is complete and we use the trained model to predict the test set, logistic regression performs excellently on predicting patients that will not have a 10 year risk of heart disease (high F Score for label 0) but performs poorly and misclassifies alot of the patients who do have a 10 year risk of heart disease (low F Score for label 1 due to low recall). \n- The accuracy is very high for both training and test dataset- this is not a good representation of the model's strength however as the dataset is heavily imabalanced and thus should not be used solely as the metric of importance. \n- From the analysis above, logistic regression is perhaps therefore not the best model to use for this type of dataset (if you want to be able to correctly classify patients who do have a 10 year risk of heart disease) - either using all of the features is not a good representation to classify patients as having a 10 year heart disease or due to the high multi-dimensional feature set, we need a more 'segregative' model e.g. Random Forest or KNN. ","2a052efc":"# Batch Gradient Descent","b151597d":"<center> <img src=\"https:\/\/i.redd.it\/84w2yii8tcs31.png\" width=\"400\" height=\"400\" \/> <\/center>","f66a154a":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","a6846321":"## Decision Boundary","11488c77":"Depending on the type of data, certain metrics will be more important than others. Some examples are:\n1. Maximise recall of model i.e. reduce the number of misclassifications of fraud cases (Type 2 errors\/False Negatives).\n2. Maximise precision of model i.e. reduce the number of misclassifications of non-fraud cases (Type 1 errors\/False Positives)\n\nWe would like a balanced trade off between these metrics - this is encompassed in the F Score. \n\n**Note:** Maximising accuracy of the model can also be valuable information as it tells us how many datapoints we are correctly classifying however if we had a large imbalanced dataset, this is not necessarily reflective of our needs. ","a0c5327f":"Let us look at the information obtained **before** applying the pre-processing steps:","81e46ef0":"# Gradient Function","ae237075":"# Cost Function","2900e069":"# Regression Predictions","665046f7":"# Full Logistic Regression Model"}}