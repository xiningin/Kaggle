{"cell_type":{"67484e26":"code","516e3911":"code","3558c47d":"code","3a60dc71":"code","cdd0b8d6":"code","2f45ceff":"code","65929403":"code","bdc87f54":"code","5ede3530":"code","6c40dce4":"code","08bdaad9":"code","ce0f5fc6":"code","7c3e50ec":"code","565658a8":"code","16d5a7f5":"code","97566e64":"code","4892f6ae":"code","8d8083c3":"code","573fbd39":"code","6cac3024":"code","7ed0c567":"code","25b92140":"code","e6cd0692":"code","bb541b12":"code","6e58c192":"code","288d1507":"code","d7a2d599":"code","8a150a1f":"code","0326f1a0":"code","3769cf6e":"code","8de6a993":"code","ba632bef":"code","ccdc67b7":"code","b4f4aac8":"code","1e995712":"code","393c83f2":"code","fac82008":"code","2b6fba91":"code","6d66d851":"code","e97439b1":"code","281635c9":"markdown","453c2974":"markdown","9e153d9b":"markdown","e95e7912":"markdown","313b39f7":"markdown","2e947c7f":"markdown","df1983c3":"markdown","6b4a0bcd":"markdown","02e3afdc":"markdown","dca5ea14":"markdown","a06d6a9b":"markdown","08a12035":"markdown","50634fe7":"markdown","aa97acdf":"markdown","53e998bc":"markdown","81fdce0e":"markdown","733982ba":"markdown","40b31df2":"markdown","bbdb2037":"markdown","c0017773":"markdown","88e5a22b":"markdown","3b8cb66d":"markdown","38619b42":"markdown","53a7731b":"markdown","611cfcb4":"markdown","8bf2b630":"markdown","475411cf":"markdown","6849daf5":"markdown","4fe850ec":"markdown"},"source":{"67484e26":"from IPython.core.display import display, HTML\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import boxcox, probplot, norm, shapiro\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nfrom sklearn.cluster import KMeans\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","516e3911":"#Main function for plot. Usefull for other case of clustering.\n\ndef comprueba_normalidad(df, return_type='axes', title='Comprobaci\u00f3n de normalidad'):\n    '''\n    '''\n    fig_tot = (len(df.columns))\n    fig_por_fila = 3.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+2 ) )\n    c = 0 \n    shapiro_test = {}\n    lambdas = {}\n    for i, col in enumerate(df.columns):\n        ax = plt.subplot(num_filas, fig_por_fila, i+1)\n        probplot(x = df[df.columns[i]], dist=norm, plot=ax)\n        plt.title(df.columns[i])\n        shapiro_test[df.columns[i]] = shapiro(df[df.columns[i]])\n    plt.suptitle(title)\n    plt.show()\n    shapiro_test = pd.DataFrame(shapiro_test, index=['Test Statistic', 'p-value']).transpose()\n    return shapiro_test","3558c47d":"os.listdir()","3a60dc71":"XY = pd.read_csv('..\/input\/uci-wholesale-customers-data\/Wholesale customers data.csv')","cdd0b8d6":"XY.head(2)","2f45ceff":"XY.describe()","65929403":"XY.info()","bdc87f54":"XY.isnull().sum()","5ede3530":"# Mapeo los datos\nXY['Channel'] = XY['Channel'].map({1:'Horeca', 2:'Retail'})\nXY['Region'] = XY['Region'].map({3:'Other Region', 2:'Oporto', 1: 'Lisboa'})","6c40dce4":"XY_cuants = XY[['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']].copy()","08bdaad9":"XY_normalizado = (XY_cuants-XY_cuants.mean())\/XY.std()\n# This function, let as see a more ordered graph. \n# try not to use it yourself and see how the graph changes ","ce0f5fc6":"plt.figure(figsize=(14,6))\nax = sns.boxplot(data=XY_normalizado)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","7c3e50ec":"plt.figure(figsize=(14,6))\nax = sns.boxplot(data=XY)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')\n#For this case there are not so much difference. But always its a good idea tried it. ","565658a8":"## Representation of the distributions of the variables using histograms.","16d5a7f5":"plt.figure(figsize=(18,20))\nn = 0\nfor i, column in enumerate(XY_cuants.columns):\n    n+=1\n    plt.subplot(5, 5, n)\n    sns.distplot(XY_cuants[column], bins=30)\n    plt.title('Distribuci\u00f3n var {}'.format(column))\nplt.show()","97566e64":"matriz_correlaciones = XY.corr(method='pearson')\nn_ticks = len(XY.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), XY.columns, rotation='vertical')\nplt.yticks(range(n_ticks), XY.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Matriz de correlaciones de Pearson')","4892f6ae":"shapiro_test = comprueba_normalidad(XY_cuants, title='Normalidad variables originales')","8d8083c3":"shapiro_test","573fbd39":"bc = PowerTransformer(method='box-cox')\nX_cuants_boxcox = bc.fit_transform(XY_cuants)\nX_cuants_boxcox = pd.DataFrame(X_cuants_boxcox, columns=XY_cuants.columns)","6cac3024":"shapiro_test = comprueba_normalidad(X_cuants_boxcox, title='Normalidad variables transformadas')","7ed0c567":"# looks perfect \u00a1","25b92140":"shapiro_test","e6cd0692":"plt.figure(figsize=(18,20))\nn = 0\nfor i, column in enumerate(X_cuants_boxcox.columns):\n    n+=1\n    plt.subplot(4, 4, n)\n    sns.distplot(X_cuants_boxcox[column], bins=30)\n    plt.title('Distribuci\u00f3n var {}'.format(column))\nplt.show()","bb541b12":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_cuants_boxcox)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","6e58c192":"for k in list(X_cuants_boxcox.columns):\n    IQR = np.percentile(X_cuants_boxcox[k],75) - np.percentile(X_cuants_boxcox[k],25)\n    \n    limite_superior = np.percentile(X_cuants_boxcox[k],75) + 1.5*IQR\n    limite_inferior = np.percentile(X_cuants_boxcox[k],25) - 1.5*IQR\n    \n    X_cuants_boxcox[k] = np.where(X_cuants_boxcox[k] > limite_superior,limite_superior,X_cuants_boxcox[k])\n    X_cuants_boxcox[k] = np.where(X_cuants_boxcox[k] < limite_inferior,limite_inferior,X_cuants_boxcox[k])","288d1507":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_cuants_boxcox)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","d7a2d599":"#No Outliers now. Ok, next step. ","8a150a1f":"#In df one the two initial categorical variables and the transformed numeric variables\ndf  =  pd.concat([XY[['Channel','Region']],X_cuants_boxcox],axis=1)\ndf[:3]","0326f1a0":"df = pd.get_dummies(df,columns=['Channel','Region'],drop_first=True)\ndf[:3]","3769cf6e":"scaler = MinMaxScaler(feature_range=(0, 1))\nX_escalado = scaler.fit_transform(df)\nX_escalado = pd.DataFrame(X_escalado,columns=df.columns)\nX_escalado.head()","8de6a993":"# Now, with the next code, we are looking for the best number of cluster for our dataset.","ba632bef":"cluster_range = range(1,20)\ncluster_wss=[] \nfor cluster in cluster_range:\n    model = KMeans(cluster)\n    model.fit(X_escalado)\n    cluster_wss.append(model.inertia_)","ccdc67b7":"plt.figure(figsize=[10,6])\nplt.title('Curva WSS para encontrar el valor \u00f3ptimo de cl\u00fasters o grupos')\nplt.xlabel('# grupos')\nplt.ylabel('WSS')\nplt.plot(list(cluster_range),cluster_wss,marker='o')\nplt.show()","b4f4aac8":"model = KMeans(n_clusters=6,random_state=0)\nmodel.fit(X_escalado)","1e995712":"#Original Dataset with the predictions\ndf_total = XY.copy()\ndf_total['cluster']=model.predict(X_escalado)\ndf_total[:2]","393c83f2":"df_total.cluster.value_counts().plot(kind='bar', figsize=(10,4))\nplt.title('Conteo de clientes por grupo')\nplt.xlabel('Grupo')\n_ = plt.ylabel('Conteo')","fac82008":"#Here, we coud see our clients inside of a cluster","2b6fba91":"descriptivos_grupos = df_total.groupby(['cluster'],as_index=False).mean()\ndescriptivos_grupos","6d66d851":"df_total.groupby('cluster').mean().plot(kind='bar', figsize=(15,7))\nplt.title('Gasto medio por producto en cada cl\u00faster')\nplt.xlabel(u'N\u00famero de cl\u00faster')\n_ = plt.ylabel('Valor medio de gasto')","e97439b1":"df_total[:2]","281635c9":"Variable normalization is the process in which a variable is transformed to follow a normal or Gaussian distribution.\n\nIn general, we will only want to normalize the data if we are going to use a machine learning algorithm or a statistical technique that assumes that the data is distributed in a Gaussian or normal way. For example, student's t tests, ANOVAs, linear regressions, logistic regressions, linear discriminant analysis (LDA), k-means, etc.\n\nAmong the ways to transform a variable to normal are methods such as the Box-Cox transformation or the Yeo-Johnson method.","453c2974":"# I explain the groups using the means of each variable per group: \u00b6","9e153d9b":"\nAnother treatment that we must do is to treat outliers or atypical values.","e95e7912":"\n## Pre-scaling the data:","313b39f7":"The normality statistic is very high in all variables now, so we continue with this transform.","2e947c7f":"<h1><center> Segmentation using K-means clustering:","df1983c3":"I also get a dataframe with the means of the variables in each group. This would represent each of the groups.\n\nThis is very necessary since the actions that the objective of this problem would be to do actions to each of the groups separately. For this, it is very important to know what each group is like, in order to act differently.","6b4a0bcd":"<h1><center> Outliers:","02e3afdc":"___\n<h1><center>  Clustering<\/center><\/h1>\n\n___\n\n## Customer Segmentation of a Wholesale Distributor\n\n### Description :\n\nThe objective of the problem is to separate the customers of a wholesale distributor into groups that are as homogeneous as possible but differ as much as possible in order to carry out different targeted actions for each of the groups.\n\nWe will use the * Wholesale customers * dataset. This dataset can be downloaded from the following path from the University of California Irvine (** Url: ** https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers)\n\n### Dataset Description:\n\nThe dataset has ** 8 descriptive variables X **.\n\nThe total number of samples is 440 clients.\n\n** Independent variables X: **\n\n\n1. FRESH: annual expense (CU) on fresh products (Continuous);\n1. MILK: annual expense (CU) on dairy products (Ongoing);\n1. GROCERY: annual expense (CU) on grocery products (Ongoing);\n1. FROZEN: annual expenditure (CU) on frozen products (Continuous)\n1. DETERGENTS_PAPER: Annual expenditure (CU) on detergents and paper products (Ongoing)\n1. DELICATESSEN: annual expense (CU) on delicatessen products (Continuous);\n1. CHANNEL: Customer channel - Horeca (Hotel \/ Restaurant \/ Caf\u00e9) or Retail channel (Nominal)\n1. REGION: Client region - Lisnon, Porto or Others (Nominal)\n\n**More details:**\n\nThere are two categorical or nominal variables, \"REGION\" and \"CHANNEL\".\n\nREGION Frequency\n* Lisbon 77\n* Oporto 47\n* Other Region 316\n\nCHANNEL Frequency\n* Horeca 298\n* Retail 142","dca5ea14":"All variables are statistically significantly not distributed as a normal.\n\n** Shapiro-Wilk test: ** If the p-value is less than a significance level $ \\ alpha $, it is concluded that the distribution does not come from a normal one.","a06d6a9b":"As we are going to apply a K-means algorithm later, the data must meet a series of hypotheses.\n\n* The K-means assumes that the data have a ** normal distribution **.\n* Also, it is very prone to ** outliers **.\n\nTherefore, we must transform the variables so that they follow a normal distribution and treat the outliers.","08a12035":"We must scale the data when we use methods based on distance measurements, such as SVMs, K-NNs, or K-means. In these algorithms, a \"1\" unit change in a numeric variable is given equal importance regardless of the variable.\n\nFor example, we can look at prices in different currencies. A dollar is much more than a Yen, so if there are two products in different currencies, the algorithm will give the same importance to an increase of one Yen as that of a dollar.\n\nIn this case, since everything is spent on products in the same currency, it would not be strictly necessary. However, when scaling we are comparing ranges of variables. That is, customer C is one of those who spends more or spends less on a product.","50634fe7":"\n### I predict and get customers with your prediction","aa97acdf":"## Thats was all. Any questions or suggestion will be welcome. ","53e998bc":"I create a dataframe with all the variables and a new one that is the prediction of the assigned cluster:","81fdce0e":"## I create dummies of categorical variables","733982ba":"# Data normalization:","40b31df2":"\nNow, the distributions look Gaussian.","bbdb2037":"I save in a variable ** X_cuants ** only the numeric variables, since I am going to represent them and apply some transformation on them.","c0017773":"The following graphs represent the <a href='https:\/\/es.wikipedia.org\/wiki\/Gr%C3%A1fico_Q-Q'>Q-Q Plot<\/a>, which is a graph that compares between two distributions. In this case, each of the variables with a normal distribution. If they follow the same distribution, the points fall close to the red line.","88e5a22b":" Now I transform the variables with a Box-Cox transform.","3b8cb66d":"<h1><center> Representation of the correlation Matrix.","38619b42":"<h1><center> GRAPHICS <\/center><\/h1>","53a7731b":"\nThe graph assumes the optimal point when the curve creates a bend. In this case it would be about 4-6 groups.","611cfcb4":"<h1><center> First Step <\/center><\/h1>","8bf2b630":"Finally, for each of the groups, I obtain their average expenditure on each product.\n\nAs an annotation ... behavior could also be analyzed by dividing the groups into their two categorical variables and analyzing the average expenses, in this way segmentations would be made by channel, geography and customer characteristics.","475411cf":"We will choose the number of groups at 6, but what is usually done is to try several and see if the final results make sense from a business point of view, as I will comment later.","6849daf5":"<h1><center> Data Transformation to find the hypothesis.","4fe850ec":"# Now, we have to obtain the characteristic of each group to find the hide informtion inside our DF and give value to our analysis."}}