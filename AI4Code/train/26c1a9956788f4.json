{"cell_type":{"05073739":"code","df986de0":"code","587de88c":"code","bc91954a":"code","0000770d":"code","41ccde1c":"code","be01227e":"code","2755eab6":"code","794fa136":"code","224dea83":"code","73e96a1e":"code","d9158187":"code","25e9fbf9":"code","722fe7ed":"code","6b2d800d":"code","1bdba436":"code","4c236acb":"code","9078ece0":"code","aaf9b44a":"code","9e6f27c3":"code","ea0c1946":"code","5019d2dd":"code","4d8027fc":"code","7559ee73":"code","2ad443b5":"code","5a10a4be":"code","840025c3":"code","067dd2f2":"code","fa0207f4":"code","f0af7a48":"code","ef568005":"code","a4df8a70":"code","cd428327":"code","fe49ee22":"code","3f37c170":"code","49884a68":"code","66b5ee4b":"code","acbf1821":"code","639bca7d":"code","6f474250":"code","26f478eb":"code","288f14e2":"code","c2f0981a":"code","1308f6fb":"code","19560181":"code","c4837823":"code","7b712ff7":"code","1db80341":"code","1ca7eb84":"markdown","17c81648":"markdown","b2d5970f":"markdown","8978f554":"markdown","34b786c9":"markdown","29caff77":"markdown","c306408d":"markdown","819ca4ee":"markdown","5e3787bc":"markdown","37073b91":"markdown","49703918":"markdown","69c56c01":"markdown","fc161d8a":"markdown","f94de1c8":"markdown","b4ed19ed":"markdown","e829175f":"markdown","620b4ffb":"markdown","2aa18402":"markdown","4506cb1b":"markdown","01a197bf":"markdown","40e07350":"markdown","ffbb6199":"markdown","5746ff13":"markdown"},"source":{"05073739":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom tqdm import tqdm,trange,tqdm_notebook # predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.\n#from tqdm import tqdm.notebook.tqdm","df986de0":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","587de88c":"train.head()","bc91954a":"test.head()","0000770d":"train.info()","41ccde1c":"train = train.dropna() # to remove missing values","be01227e":"print('The number of training data points are: ',train.shape[0])","2755eab6":"train.shape","794fa136":"print('The number of testing data points are: ',test.shape[0])","224dea83":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nsns.countplot(train['sentiment'])","73e96a1e":"plt.figure(figsize=(10,6))\nsns.countplot(test['sentiment'])","d9158187":"ax = (pd.Series(train['sentiment']).value_counts(normalize=True, sort=False)*100).plot.bar()\nax.set(ylabel=\"Percent\")\nplt.show()","25e9fbf9":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","722fe7ed":"ax = (pd.Series(test['sentiment']).value_counts(normalize=True, sort=False)*100).plot.bar()\nax.set(ylabel=\"Percent\")\nplt.show()","6b2d800d":"def plotWordClouds(df_text,sentiment):\n    text = \" \".join(str(tmptext) for tmptext in df_text)\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(text)\n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('WordCloud - ' + sentiment)\n    plt.show()     ","1bdba436":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)","4c236acb":"subtext = train[train['sentiment']=='positive']['selected_text']\nplotWordClouds(subtext,'positive')","9078ece0":"subtext = train[train['sentiment']=='neutral']['selected_text']\nplotWordClouds(subtext,'neutral')","aaf9b44a":"subtext = train[train['sentiment']=='negative']['selected_text']\nplotWordClouds(subtext,'negative')","9e6f27c3":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","ea0c1946":"train.head(5)","5019d2dd":"#def remove_stopword(x):\n#    return [y for y in x if y not in stopwords.words('english')]\n#train['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","4d8027fc":"from collections import Counter\ntrain['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(30))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","7559ee73":"from plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=900, height=800,color='Common_words')\nfig.show()","2ad443b5":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","5a10a4be":"import nltk \na = set('I love my girlfriend so much'.lower().split())\nb = set('My girlfriend love me always'.lower().split())\n \nprint (\"Jaccard similarity of above two sentences is\",1-nltk.jaccard_distance(a, b))","840025c3":"pip install utils\n","067dd2f2":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport utils","fa0207f4":"import numpy as np\nimport torch\n\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","f0af7a48":"import utils","ef568005":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"..\/input\/bert-base-uncased\/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"..\/input\/tweet-train-folds\/train_folds.csv\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}\/vocab.txt\", \n        lowercase=True)","a4df8a70":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \"\"\"\n    Processes the tweet and outputs the features necessary for model training and inference\n    \"\"\"\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    # Finds the start and end indices of the span\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    # Assign a positive label for the characters within the selected span (based on the start and end indices)\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    # Encode the tweet using the set tokenizer (converted to ids corresponding to word pieces)\n    tok_tweet = tokenizer.encode(tweet)\n    # Save the ids and offsets (character level indices)\n    # Ommit the first and last tokens, which should be the [CLS] and [SEP] tokens\n    input_ids_orig = tok_tweet.ids[1:-1]\n    tweet_offsets = tok_tweet.offsets[1:-1]\n    \n    # A token is considered \"positive\" when it has at least one character with a positive label\n    # The indices of the \"positive\" tokens are stored in `target_idx`.\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    # Store the indices of the first and last \"positive\" tokens\n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699\n    }\n    \n    # Prepend the beginning of the tweet with the [CLS] token (101), and tweet sentiment, represented by the `sentiment_id`\n    # The [SEP] token (102) is both prepended and appended to the tweet\n    # You can find the indices in the vocab file: https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/bert-base-uncased-vocab.txt\n    input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n    token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n    targets_start += 3\n    targets_end += 3\n\n    # Pad sequence if its length < `max_len`\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    # Return processed tweet as a dictionary\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","cd428327":"class TweetDataset:\n    \"\"\"\n    Dataset which stores the tweets and returns them as processed features\n    \"\"\"\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","fe49ee22":"class TweetModel(transformers.BertPreTrainedModel):\n    \"\"\"\n    Model class that combines a pretrained bert model with a linear later\n    \"\"\"\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # Load the pretrained BERT model\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n        # Set 10% dropout to be applied to the BERT backbone's output\n        self.drop_out = nn.Dropout(0.1)\n        # 768 is the dimensionality of bert-base-uncased's hidden representations\n        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        # The output will have two dimensions (\"start_logits\", and \"end_logits\")\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # Return the hidden states from the BERT backbone\n        _, _, out = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        ) # bert_layers x bs x SL x (768 * 2)\n\n        # Concatenate the last two hidden states\n        # This is done since experiments have shown that just getting the last layer\n        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n        # Sample explanation: https:\/\/bert-as-service.readthedocs.io\/en\/latest\/section\/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n        # Apply 10% dropout to the last 2 hidden states\n        out = self.drop_out(out) # bs x SL x (768 * 2)\n        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n        logits = self.l0(out) # bs x SL x 2\n\n        # Splits the tensor into start_logits and end_logits\n        # (bs x 2) -> (bs x SL x 1), (bs x SL x 1)\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1) # (bs x SL)\n        end_logits = end_logits.squeeze(-1) # (bs x SL)\n\n        return start_logits, end_logits","3f37c170":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    \"\"\"\n    Return the sum of the cross entropy losses for both the start and end logits\n    \"\"\"\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","49884a68":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    \"\"\"\n    Trains the bert model on the twitter data\n    \"\"\"\n    # Set model to training mode (dropout + sampled batchnorm is activated)\n    model.train()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    # Set tqdm to add loading screen and set the length\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    # Train the model on each batch\n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        # Move ids, masks, and targets to gpu while setting as torch.long\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        # Reset gradients\n        model.zero_grad()\n        # Use ids, masks, and token types as input to the model\n        # Predict logits for each of the input tokens for each batch\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        ) # (bs x SL), (bs x SL)\n        # Calculate batch loss based on CrossEntropy\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        # Calculate gradients based on loss\n        loss.backward()\n        # Adjust weights based on calculated gradients\n        optimizer.step()\n        # Update scheduler\n        scheduler.step()\n        \n        # Apply softmax to the start and end logits\n        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n        # This is similar to the characteristics of \"probabilities\"\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        \n        # Calculate the jaccard score based on the predictions for this batch\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet, # Full text of the px'th tweet in the batch\n                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n                offsets=offsets[px] # Offsets for each of the tokens for the px'th tweet in the batch\n            )\n            jaccard_scores.append(jaccard_score)\n        # Update the jaccard score and loss\n        # For details, refer to `AverageMeter` in https:\/\/www.kaggle.com\/abhishek\/utils\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        # Print the average loss and jaccard score at the end of each batch\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)","66b5ee4b":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \"\"\"\n    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n    \"\"\"\n    \n    # A span's start index has to be greater than or equal to the end index\n    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    # Combine into a string the tokens that belong to the predicted span\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n        # than the beginning offset of the following token, add a space.\n        # Basically, add a space when the next token (word piece) corresponds to a new word\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    # Calculate the jaccard score between the predicted span, and the actual span\n    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n    # https:\/\/www.kaggle.com\/abhishek\/utils\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    \"\"\"\n    Evaluation function to predict on the test set\n    \"\"\"\n    # Set model to evaluation mode\n    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n    # Reference: https:\/\/github.com\/pytorch\/pytorch\/issues\/5406\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    \n    # Turns off gradient calculations (https:\/\/datascience.stackexchange.com\/questions\/32651\/what-is-the-use-of-torch-no-grad-in-pytorch)\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        # Make predictions and calculate loss \/ jaccard score for each batch\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            # Move tensors to GPU for faster matrix calculations\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            # Predict logits for start and end indexes\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            # Calculate loss for the batch\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            # Apply softmax to the predicted logits for the start and end indexes\n            # This converts the \"logits\" to \"probability-like\" scores\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            # Calculate jaccard scores for each tweet in the batch\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            # Update running jaccard score and loss\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            # Print the running average loss and jaccard score\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    \n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg","acbf1821":"def run(fold):\n    \"\"\"\n    Train model for a speciied fold\n    \"\"\"\n    # Read training csv\n    dfx = pd.read_csv(config.TRAINING_FILE)\n\n    # Set train validation set split\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    # Instantiate TweetDataset with training data\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    # Instantiate DataLoader with `train_dataset`\n    # This is a generator that yields the dataset in batches\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    # Instantiate TweetDataset with validation data\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    # Instantiate DataLoader with `valid_dataset`\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=2\n    )\n\n    # Set device as `cuda` (GPU)\n    device = torch.device(\"cuda\")\n    # Load pretrained BERT (bert-base-uncased)\n    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n    # Output hidden states\n    # This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n    model_config.output_hidden_states = True\n    # Instantiate our model with `model_config`\n    model = TweetModel(conf=model_config)\n    # Move the model to the GPU\n    model.to(device)\n\n    # Calculate the number of training steps\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    # Get the list of named parameters\n    param_optimizer = list(model.named_parameters())\n    # Specify parameters where weight decay shouldn't be applied\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    # Define two sets of parameters: those with weight decay, and those without\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    # Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    # Create a scheduler to set the learning rate at each training step\n    # \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https:\/\/pytorch.org\/docs\/stable\/optim.html)\n    # Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    # Apply early stopping with patience of 2\n    # This means to stop training new epochs when 2 rounds have passed without any improvement\n    es = utils.EarlyStopping(patience=2, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(3):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break","639bca7d":"run(fold=0)","6f474250":"run(fold=1)","26f478eb":"run(fold=3)","288f14e2":"run(fold=4)","c2f0981a":"df_test = test","1308f6fb":"df_test.head()","19560181":"df_test.loc[:, \"selected_text\"] = df_test.text.values","c4837823":"device = torch.device(\"cuda\")\nmodel_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\nmodel_config.output_hidden_states = True","7b712ff7":"# Load each of the five trained models and move to GPU\nmodel1 = TweetModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"model_4.bin\"))\nmodel5.eval()","1db80341":"final_output = []\n\n# Instantiate TweetDataset with the test data\ntest_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n)\n\n# Instantiate DataLoader with `test_dataset`\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1\n)\n\n# Turn of gradient calculations\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    # Predict the span containing the sentiment for each batch\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        # Predict start and end logits for each of the five models\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        # Get the average start and end logits across the five models and use these as predictions\n        # This is a form of \"ensembling\"\n        outputs_start = (\n            outputs_start1 \n            + outputs_start2 \n            + outputs_start3 \n            + outputs_start4 \n            + outputs_start5\n        ) \/ 5\n        outputs_end = (\n            outputs_end1 \n            + outputs_end2 \n            + outputs_end3 \n            + outputs_end4 \n            + outputs_end5\n        ) \/ 5\n        \n        # Apply softmax to the predicted start and end logits\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n        # Convert the start and end scores to actual predicted spans (in string form)\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            _, output_sentence = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            final_output.append(output_sentence)\n","1ca7eb84":"## Most Common Words Used","17c81648":"Now the Bert will begin it's magic to get into the closed results of this competetion. I am using the Bert model from Abhishek Thakur's Bag.\n@Lorenzo Ampil's Notebook helped me to understand the deep learning part.","b2d5970f":"# WORK IN PROGRESS.................","8978f554":"### Defining loss function is necessary in the aspect of machine learning !\n\n**What is Loss Function ?**\nit's a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they're pretty good, it'll output a lower number.\n\nDo you Understand ?\nYes I am sure you know it !","34b786c9":"SO we understand the similarity, now I hope we have understand in which way we have to pass so that we can evaluate this problem.","29caff77":"# <a id='basic'> Processing Data <a\/>","c306408d":"# <a id='basic'> Import Library<\/a>","819ca4ee":"So from the Above Wordcloud we understand the basic sense about positive negative and Neutral words.","5e3787bc":"## Visualize the words and Phrase through WordCloud, I am sure It will be Fun","37073b91":"### In this competetion our agenda is need to pick out the part of the tweet (word or phrase) that reflects the sentiment.","49703918":"# <a id='basic'>Training <a\/>","69c56c01":"# <a id='basic'> Tweet Sentiment Extraction<a\/>\n### Extract support phrases for sentiment labels","fc161d8a":"### Removing Stopwords","f94de1c8":"![image.png](attachment:image.png)","b4ed19ed":"## Here we will use Pytorch for further process.\nPyTorch is an open-source deep learning framework which is used for implementing network architectures like RNN, CNN, LSTM, etc and other high-level algorithms.","e829175f":"# <a id='basic'> Evaluation Function <a\/>","620b4ffb":"# <a id='basic'> Exploratory data analysis <a\/>","2aa18402":"# Utils Script","4506cb1b":"# <a id = 'basic'>Evaluation Metric<a\/>\n\nThe metric in this competition is the word-level Jaccard score. A good description of Jaccard similarity for strings is here.\nA Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below.","01a197bf":"#### I know understanding this formulae is little difficult, no worry, you will get it gradually :-)\n","40e07350":"# <a id='basic'> Training Function <a\/>","ffbb6199":"## Understanding Jaccard similarity with two simple sentence","5746ff13":"# <a id='basic'> Model <a\/>"}}