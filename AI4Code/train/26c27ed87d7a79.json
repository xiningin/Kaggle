{"cell_type":{"73dbc0e8":"code","26b06c08":"code","d65d6616":"code","c6d9056e":"code","2e39c327":"code","552229f0":"code","d83b684e":"code","5c186587":"code","8ebe946a":"code","42c5e800":"code","8e3eed38":"code","a38ee3ba":"code","a3f85587":"code","cdd75ca7":"code","d1a13527":"code","72be3100":"code","6cd39df5":"code","048c6fc3":"code","2e7c31e7":"code","a111221d":"code","32e1577e":"code","34ed5608":"markdown","a68f0589":"markdown","1cc337cd":"markdown","4a0e0168":"markdown","6a484df5":"markdown","26e8807b":"markdown","aae99ff9":"markdown","fcba27a1":"markdown","da3cb5c1":"markdown","2ce57e9d":"markdown","59eb9f29":"markdown","81f4bf87":"markdown","dd93e060":"markdown","276b00b9":"markdown","c96a7c14":"markdown","46ebbbd3":"markdown","0792aa07":"markdown"},"source":{"73dbc0e8":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","26b06c08":"### 1> set several paths\nPATH_TRAIN = '..\/input\/nlp-getting-started\/train.csv'\nPATH_TEST = '..\/input\/nlp-getting-started\/test.csv'\n\n### 2> read_csv\ndf_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)","d65d6616":"df_train.head()","c6d9056e":"df_train.info()","2e39c327":"df_train.target.value_counts()","552229f0":"df_train.loc[df_train['target'] == 1, 'text'].str.split()","d83b684e":"def make_list_by_target(target):\n    corpus = []\n    \n    for x in df_train.loc[df_train['target'] == target, 'text'].str.split():\n        # x\uc5d0\ub294 \uac01 idx\ubcc4 text_data\uac00 \ub4e4\uc5b4\uac04\ub2e4.\n        for i in x:\n            # i\uc5d0\ub294 \uac01 text_data\ubcc4 \ub2e8\uc5b4\ub4e4\uc774 \ub4e4\uc5b4\uac04\ub2e4. \n            corpus.append(i)\n            \n    return corpus","5c186587":"from nltk.corpus import stopwords","8ebe946a":"stop = set(stopwords.words('english'))","42c5e800":"from collections import defaultdict","8e3eed38":"corpus = make_list_by_target(0)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\nplt.title('With no disaster')\nplt.bar(x, y, color = 'red')","a38ee3ba":"corpus = make_list_by_target(1)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\n\nplt.title('With disaster')\nplt.bar(x, y, color='green')","a3f85587":"import string","cdd75ca7":"plt.figure(figsize=(10, 5))\ncorpus = make_list_by_target(0)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color = 'red')","d1a13527":"plt.figure(figsize=(10, 5))\ncorpus = make_list_by_target(1)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color='green')","72be3100":"from collections import Counter","6cd39df5":"counter = Counter(make_list_by_target(0))\nmost_common = counter.most_common()\n\nx = list()\ny = list()\n\nfor word, count in most_common[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y, y=x, saturation=1)","048c6fc3":"counter = Counter(make_list_by_target(1))\nmost_common = counter.most_common()\n\nx = list()\ny = list()\n\nfor word, count in most_common[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y, y=x, saturation=1)","2e7c31e7":"from sklearn.feature_extraction.text import CountVectorizer","a111221d":"def get_top_tweet_bigrams(corpus, n=10):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    \n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","32e1577e":"plt.figure(figsize=(10, 5))\ntop_tweet_bigrams = get_top_tweet_bigrams(df_train['text'])[:10]\n\nx, y = map(list, zip(*top_tweet_bigrams))\n\nsns.barplot(x=y, y=x)","34ed5608":"------","a68f0589":"------\n------","1cc337cd":"### 4> non-disaster data (target == 1) \uc911\uc5d0\uc11c \ub2e8\uc5b4\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac","4a0e0168":"## 2) What stopwords are used frequently?","6a484df5":"------\n\n## 5) Check Bigram","26e8807b":"----------\n\n## 4) Check Corpus\n\n### 1> disaster data (target == 0) \uc911\uc5d0\uc11c corpus\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac","aae99ff9":"### 2> stopword\n\nstopword (\ubd88\uc6a9\uc5b4): \uc790\uc8fc \ub4f1\uc7a5\ud558\uc9c0\ub9cc \ubd84\uc11d\uc744 \ud558\ub294 \uac83\uc5d0 \uc788\uc5b4\uc11c\ub294 \ud070 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\ub294 \ub2e8\uc5b4","fcba27a1":"\ud574\ub2f9 \ube48\ub3c4\uac00 \uc720\uc0ac\ud558\ub2c8 \ucd94\ud6c4 \uad6c\ub450\uc810\uc744 \uadf8\ub0e5 \uc81c\uac70\ud574\ub3c4 \ub420 \uac83 \uac19\ub2e4.","da3cb5c1":"-------","2ce57e9d":"#### \uacb0\ub860: \ubcf4\ud1b5 \uc7ac\ub09c \uc0c1\ud669\uc5d0\uc11c\ub294 \ubcf8\uc778\uc758 \uc704\uce58('in')\uc744 \uc124\uba85\ud558\ub824\uace0 \ud55c\ub2e4.","59eb9f29":"### 2> disaster data (target == 1) \uc911\uc5d0\uc11c corpus\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac","81f4bf87":"### 1> \ud574\ub2f9 target \uac12\uc744 \uac00\uc9c0\ub294 text\ub4e4 \uc911 \uc0ac\uc6a9\ub418\ub294 \ub2e8\uc5b4\ub4e4\uc744 \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4dc\ub294 \ud568\uc218","dd93e060":"--------\n\n## 3) Check Punctuation (\uad6c\ub450\uc810 \ud655\uc778)\n\n### 1> disaster data (target == 0) \uc911\uc5d0\uc11c \uad6c\ub450\uc810\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac","276b00b9":"## 1) the distribution of target","c96a7c14":"# 1. EDA\n\n## 0) get DataFrame","46ebbbd3":"### 2> non-disaster data (target == 1) \uc911\uc5d0\uc11c \uad6c\ub450\uc810\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac","0792aa07":"### 3> disaster data (target == 0) \uc911\uc5d0\uc11c \ub2e8\uc5b4\uc758 \uc0ac\uc6a9 \ube48\ub3c4 \uc870\uc0ac"}}