{"cell_type":{"3cdbc6aa":"code","c0fc27e6":"code","9525e464":"code","342ccd26":"code","74cf99aa":"code","5b84e9d9":"code","1da899ba":"code","f46ea10e":"code","3449e035":"code","082d1810":"code","dbed9ae5":"code","c77e7f0e":"code","a7b6479b":"code","8ed5a71b":"code","0523d262":"code","efc7e0bd":"code","85dfeaca":"code","322adf49":"code","4690e60c":"markdown","e46b48d3":"markdown"},"source":{"3cdbc6aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","c0fc27e6":"!git clone https:\/\/github.com\/yunjey\/pytorch-tutorial.git","9525e464":"%cd pytorch-tutorial\/tutorials\/03-advanced\/image_captioning\/\n!pip install -r requirements.txt","342ccd26":"!wget https:\/\/www.dropbox.com\/s\/ne0ixz5d58ccbbz\/pretrained_model.zip .","74cf99aa":"!unzip pretrained_model.zip","5b84e9d9":"!wget https:\/\/www.dropbox.com\/s\/26adb7y9m98uisa\/vocap.zip .","1da899ba":"!mkdir models data","f46ea10e":"!cp encoder-5-3000.pkl models\/encoder-2-1000.ckpt","3449e035":"!cp decoder-5-3000.pkl models\/decoder-2-1000.ckpt","082d1810":"!unzip vocap.zip -d data","dbed9ae5":"!ls ..\/..\/..\/..\/..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg","c77e7f0e":"!pip install pycocotools > \/dev\/null","a7b6479b":"import matplotlib.pyplot as plt\nimport numpy as np \nimport argparse\nimport pickle \nimport os\nimport torch\nfrom torchvision import transforms \nfrom build_vocab import Vocabulary\nfrom model import EncoderCNN, DecoderRNN\nfrom PIL import Image\n \n \n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n \ndef load_image(image_path, transform=None):\n    image = Image.open(image_path)\n    image = image.resize([224, 224], Image.LANCZOS)\n    \n    if transform is not None:\n        image = transform(image).unsqueeze(0)\n    \n    return image\n \ndef show_text(image_path):\n    # Image preprocessing\n    transform = transforms.Compose([\n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary wrapper\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n \n    # Build models\n    encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean\/variance)\n    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n \n    # Load the trained model parameters\n    encoder.load_state_dict(torch.load(encoder_path))\n    decoder.load_state_dict(torch.load(decoder_path))\n \n    # Prepare an image\n    image = load_image(image_path, transform)\n    image_tensor = image.to(device)\n    \n    # Generate an caption from the image\n    feature = encoder(image_tensor)\n    sampled_ids = decoder.sample(feature)\n    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n    \n    # Convert word_ids to words\n    sampled_caption = []\n    for word_id in sampled_ids:\n        word = vocab.idx2word[word_id]\n        sampled_caption.append(word)\n        if word == '<end>':\n            break\n    sentence = ' '.join(sampled_caption)\n    \n    # Print out the image and the generated caption\n    image = Image.open(image_path)\n    plt.imshow(np.asarray(image))\n    plt.show()\n    print (sentence)\n\n\nencoder_path ='models\/encoder-2-1000.ckpt'\ndecoder_path ='models\/decoder-2-1000.ckpt'\nvocab_path ='data\/vocab.pkl'\n    \n# Model parameters (should be same as paramters in train.py)\nembed_size=256\nhidden_size=512\nnum_layers=1\n    \nimport glob\nfiles = sorted(glob.glob('..\/..\/..\/..\/..\/input\/petfinder-pawpularity-score\/train\/*.jpg'))\nfor i, image_path in enumerate (files):\n    sentence = show_text(image_path)\n    if i == 5:\n        break","8ed5a71b":"train_df = pd.read_csv('..\/..\/..\/..\/..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain_df.head()","0523d262":"def generate_text(image_path):\n    # Image preprocessing\n    transform = transforms.Compose([\n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary wrapper\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n \n    # Build models\n    encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean\/variance)\n    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n \n    # Load the trained model parameters\n    encoder.load_state_dict(torch.load(encoder_path))\n    decoder.load_state_dict(torch.load(decoder_path))\n \n    # Prepare an image\n    image = load_image(image_path, transform)\n    image_tensor = image.to(device)\n    \n    # Generate an caption from the image\n    feature = encoder(image_tensor)\n    sampled_ids = decoder.sample(feature)\n    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n    \n    # Convert word_ids to words\n    sampled_caption = []\n    for word_id in sampled_ids:\n        word = vocab.idx2word[word_id]\n        sampled_caption.append(word)\n        if word == '<end>':\n            break\n    sentence = ' '.join(sampled_caption)\n    return sentence","efc7e0bd":"file_ids = []\nsentences = []\nfiles = sorted(glob.glob('..\/..\/..\/..\/..\/input\/petfinder-pawpularity-score\/train\/*.jpg'))\nfor i, image_path in enumerate (files):\n    file_id = image_path.split('\/')[-1].split('.')[0]\n    sentence = generate_text(image_path)[8:-6]\n    \n    file_ids.append(file_id)\n    sentences.append(sentence)\n        \nres = pd.DataFrame()\nres['Id'] = file_ids\nres['Description'] = sentences\n\nprint(res.shape)\nres.head()","85dfeaca":"res.to_csv('..\/..\/..\/..\/..\/working\/PetFinder_ImageCaptionedDescriptionDataset.csv', index=False)","322adf49":"# EOF","4690e60c":"#### Wow it's noisy ... haha.\n\n#### We need more vocab.  )^o^(","e46b48d3":"## This is just Idea to generate text data with pretrained Image Captioning Model\n\n- To improve your model with this method, you maybe have to prepare more vocab and more training\n\n- If you think this is interesting, please upvote this notebook \ud83d\ude09"}}