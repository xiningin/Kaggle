{"cell_type":{"e9845c54":"code","7adff360":"code","fa10cd51":"code","c256dc60":"code","aa4c3aed":"code","03936c0d":"code","26e170b3":"code","be9cec69":"code","7ce9f492":"code","6c532f72":"code","bf597dca":"code","dff3522d":"code","81d82a32":"code","9530adc9":"code","93ea182f":"code","2ce56db2":"code","de831fa6":"code","dfe9b23b":"code","f1c7b8d3":"code","9c3bc010":"code","e420dca9":"code","5c5534c1":"code","f100fb1b":"code","e375f655":"code","5ec1b286":"code","a25e7f55":"code","9e8a3bf5":"code","50dd1c06":"code","17252543":"code","3b0efbda":"code","e3841ca0":"code","5a0cf877":"code","bc7cda94":"code","f728f802":"markdown","26c7d8fe":"markdown","2b6f0685":"markdown","e13dabcd":"markdown","00ace923":"markdown","f27f3083":"markdown","b557d279":"markdown","5dde19df":"markdown","8b8401dc":"markdown","dd31e925":"markdown","5735e6bb":"markdown","7ec91a05":"markdown","74a40135":"markdown","2345c0ae":"markdown","8bb09471":"markdown","6a4e8d2b":"markdown","5d6c632f":"markdown","d7e39a6c":"markdown","3656e8fc":"markdown","500b4ca8":"markdown","213aadc3":"markdown","f475673a":"markdown","0d05f367":"markdown","dd6faff3":"markdown","94d1f764":"markdown","ad457805":"markdown","c61487a3":"markdown","ebfbc371":"markdown"},"source":{"e9845c54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7adff360":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.isna().sum()","fa10cd51":"df['Survived'].value_counts(normalize=True)","c256dc60":"feat = df.copy()\nfeat.set_index('PassengerId',inplace=True)\n\nfrom sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean') \nfeat['Age'] = imp_mean.fit_transform(feat[['Age']]) #Age missing values will go with the mean. Depending on model performance we may change this strategy later\nimp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent') \nfeat['Embarked'] = imp_mode.fit_transform(feat[['Embarked']])\nfeat = feat[feat['Cabin'].isnull()]\nfeat.drop(columns=['Cabin','Name','Ticket'], inplace=True)\n\nfeat.isna().sum()","aa4c3aed":"df_cabin = df.copy()\ndf_cabin.set_index('PassengerId',inplace=True)\ndf_cabin['Age'] = imp_mean.fit_transform(df_cabin[['Age']]) #Age missing values will go with the mean. Depending on model performance we may change this strategy later\ndf_cabin['Embarked'] = imp_mode.fit_transform(df_cabin[['Embarked']])\ndf_cabin.dropna(inplace=True)\ndf_cabin.drop(columns=['Name','Ticket'], inplace=True)\ndf_cabin.isna().sum()","03936c0d":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\ndf_cabin['Cabin'] = df_cabin['Cabin'].str[0]\ndf_cabin['Sex'] = le.fit_transform(df_cabin['Sex'])\ndf_cabin['Embarked'] = le.fit_transform(df_cabin['Embarked'])\ndf_cabin['Cabin'] = le.fit_transform(df_cabin['Cabin'])\n\ndf_cabin.head()","26e170b3":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ny_cabin = df_cabin['Cabin']\nX_cabin = StandardScaler().fit_transform(df_cabin.drop(columns=['Cabin','Survived']))\n\nX_cabin_train, X_cabin_test, y_cabin_train, y_cabin_test = train_test_split(X_cabin, y_cabin, random_state=7)","be9cec69":"from sklearn.ensemble import GradientBoostingClassifier\n\ncabinGBC = GradientBoostingClassifier(random_state=7, max_depth=5, n_estimators = 1000)\ncabinGBC.fit(X_cabin_train, y_cabin_train)","7ce9f492":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_cabin_hat = hummerGBC.predict(X_cabin_test)\nconfusion = confusion_matrix(y_cabin_test, y_cabin_hat)\nsns.heatmap(confusion, annot=True)","6c532f72":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_cabin_test, y_cabin_hat, zero_division=0))","bf597dca":"feat['Sex'] = le.fit_transform(feat['Sex'])\nfeat['Embarked'] = le.fit_transform(feat['Embarked'])\n\ndf_cabin.head()","dff3522d":"feat.head()","81d82a32":"X_feat = feat.drop(columns='Survived')\nfeat['Cabin'] = cabinGBC.predict(X_feat)\nfeat.head()","9530adc9":"df1 = feat.append(df_cabin)\ndf1.sort_index(inplace=True)\ndf1.head()","93ea182f":"from sklearn.preprocessing import KBinsDiscretizer\n\ndf1['Child'] = df1['Age']<17 #Creating Child Variable. Will define as younger than 17\ndf1['Child'] = le.fit_transform(df1['Child'])\ndf1['Family'] = df1['SibSp'] + df1['Parch']\ndf1['Alone'] = df1['Family'] ==0\ndf1['Alone'] = le.fit_transform(df1['Alone'])\n\nest = KBinsDiscretizer(n_bins=5, encode = 'ordinal', strategy='uniform')\ndf1['Age_bin'] = est.fit_transform(df1[['Age']])\ndf1.head()","2ce56db2":"sns.heatmap(df1.corr())","de831fa6":"y = df1['Survived']\nX = StandardScaler().fit_transform(df1.drop(columns=['Survived','Child','SibSp','Parch','Age', 'Alone']))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)","dfe9b23b":"#from sklearn.model_selection import GridSearchCV\n\n\"\"\"param_grid = {'learning_rate' = [0.01,0.1,0.5,1,2],\n             'n_estimators' = [50,100,500,1000,2000],\n             }\n\"\"\"\n\nhummerGBC = GradientBoostingClassifier(random_state=7, max_depth=5, n_estimators = 1000)\nhummerGBC.fit(X_train, y_train)\nprint('GBC Score:', hummerGBC.score(X_test, y_test))\n      \nfrom sklearn.ensemble import RandomForestClassifier\nhummerRFC = RandomForestClassifier(random_state=7, max_depth=7, n_estimators = 1000)\nhummerRFC.fit(X_train, y_train)\nprint('RFC Score:', hummerRFC.score(X_test, y_test))\n\nfrom sklearn.ensemble import AdaBoostClassifier\nhummerADA = AdaBoostClassifier(random_state=7, n_estimators = 7000, learning_rate = 0.1)\nhummerADA.fit(X_train, y_train)\nprint('ADA Score:', hummerADA.score(X_test, y_test))","f1c7b8d3":"from sklearn.metrics import confusion_matrix\n\ny_hat = hummerGBC.predict(X_test)\nconfusion = confusion_matrix(y_test, y_hat)\nsns.heatmap(confusion, annot=True)","9c3bc010":"from sklearn.metrics import f1_score\n\nf1_score(y_test, y_hat, average='binary')","e420dca9":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(hummerGBC, random_state=7).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = df1.drop(columns=['Survived','Child','SibSp','Parch','Age','Alone']).columns.tolist())","5c5534c1":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder()\ndf2 = df1.drop(columns=['Survived','Child','SibSp','Parch','Age', 'Alone', 'Fare'])\ndf2 = enc.fit_transform(df2).toarray()\n\ny_NB = df1['Survived']\nX_NB = df2\n\nX_NB_train, X_NB_test, y_NB_train, y_NB_test = train_test_split(X_NB, y_NB, random_state=7)\n\nfrom sklearn.naive_bayes import CategoricalNB\n\nhummerNB = CategoricalNB()\nhummerNB.fit(X_NB_train, y_NB_train)\nhummerNB.score(X_NB_test, y_NB_test)","f100fb1b":"y_NB_hat = hummerNB.predict(X_NB_test)\nconfusion = confusion_matrix(y_NB_test, y_NB_hat)\nsns.heatmap(confusion, annot=True)","e375f655":"f1_score(y_NB_test, y_NB_hat, average='binary')","5ec1b286":"df_pred = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_pred.isna().sum()","a25e7f55":"feat_pred = df_pred.copy()\nfeat_pred.set_index('PassengerId',inplace=True)\n\nfeat_pred['Age'] = imp_mean.fit_transform(feat_pred[['Age']]) #Age missing values will go with the mean. Depending on model performance we may change this strategy later\nfeat_pred['Fare'] = imp_mean.fit_transform(feat_pred[['Fare']])\nfeat_pred['Embarked'] = imp_mode.fit_transform(feat_pred[['Embarked']])\nfeat_pred = feat_pred[feat_pred['Cabin'].isnull()]\nfeat_pred.drop(columns=['Cabin','Name','Ticket'], inplace=True)\n\nfeat_pred.isna().sum()","9e8a3bf5":"le.fit(feat_pred['Sex'])\nfeat_pred['Sex'] = le.transform(feat_pred['Sex'])\nle.fit(feat_pred['Embarked'])\nfeat_pred['Embarked'] = le.transform(feat_pred['Embarked'])\nX_feat_pred = feat_pred\nfeat_pred['Cabin'] = cabinGBC.predict(X_feat_pred)\n\nfeat_pred.head()","50dd1c06":"df_cabin_pred = df_pred.copy()\ndf_cabin_pred.set_index('PassengerId',inplace=True)\ndf_cabin_pred['Age'] = imp_mean.fit_transform(df_cabin_pred[['Age']]) #Age missing values will go with the mean. Depending on model performance we may change this strategy later\ndf_cabin_pred['Embarked'] = imp_mode.fit_transform(df_cabin_pred[['Embarked']])\ndf_cabin_pred.dropna(inplace=True)\ndf_cabin_pred.drop(columns=['Name','Ticket'], inplace=True)\ndf_cabin_pred.isna().sum()","17252543":"df_cabin_pred['Cabin'] = df_cabin_pred['Cabin'].str[0]\ndf_cabin_pred['Sex'] = le.fit_transform(df_cabin_pred['Sex'])\ndf_cabin_pred['Embarked'] = le.fit_transform(df_cabin_pred['Embarked'])\ndf_cabin_pred['Cabin'] = le.fit_transform(df_cabin_pred['Cabin'])\n\ndf_cabin_pred.head()","3b0efbda":"df_pred1 = feat_pred.append(df_cabin_pred)\ndf_pred1.sort_index(inplace=True)\ndf_pred1.head()","e3841ca0":"df_pred1['Child'] = df_pred1['Age']<17 #Creating Child Variable. Will define as younger than 17\ndf_pred1['Child'] = le.fit_transform(df_pred1['Child'])\ndf_pred1['Family'] = df_pred1['SibSp'] + df_pred1['Parch']\ndf_pred1['Alone'] = df_pred1['Family'] ==0\ndf_pred1['Alone'] = le.fit_transform(df_pred1['Alone'])\n\nest = KBinsDiscretizer(n_bins=5, encode = 'ordinal', strategy='uniform')\ndf_pred1['Age_bin'] = est.fit_transform(df_pred1[['Age']])\ndf_pred1.head()","5a0cf877":"X_pred = StandardScaler().fit_transform(df_pred1.drop(columns=['Child','SibSp','Parch','Age', 'Alone']))","bc7cda94":"X_pred = StandardScaler().fit_transform(X_pred)\ny_hat = hummerGBC.predict(X_pred)\nsubmission = pd.DataFrame({'PassengerId':df_test['PassengerId'],'Survived':y_hat})\nsubmission.to_csv('hummer1.csv', index=False)","f728f802":"# Data cleaning","26c7d8fe":"## Confusion matrix","2b6f0685":"# Joining datasets cabin and not cabin","e13dabcd":"# Model evaluation","00ace923":"# Feature engineering - Siblings, Spouse, Parents and Child","f27f3083":"# Re-Standardizing and split with feature selected","b557d279":"# Model Fitting","5dde19df":"# Cabin standard","8b8401dc":"# Predicting Cabin Level in Train Dataset","dd31e925":"# Non Cabin Data Cleaning and NAN Treatment","5735e6bb":"# Label encoding","7ec91a05":"# Cabin encoding","74a40135":"Event though precision is low the error cabins are close. I expect that it will perform well in the entire model.","2345c0ae":"# Cabin Data Cleaning and NAN Treatment","8bb09471":"## F1-Score","6a4e8d2b":"# Correlation visualization - Trying to identify correlation with target and inter-correlation","5d6c632f":"## F1-Score","d7e39a6c":"# Majority Class Baseline","3656e8fc":"## Permutation Importance","500b4ca8":"## Confusion Matrix","213aadc3":"# Define X and y test","f475673a":"# Model Evaluation","0d05f367":"# Data Load","dd6faff3":"# Import prediction data","94d1f764":"# Prepare a NB Model Feature Set","ad457805":"# Cabin Model Fit","c61487a3":"## F1 Score","ebfbc371":"# Cabin Model Evalutation\n\n## Confusion Matrix"}}