{"cell_type":{"4905f76b":"code","fbfdb54a":"code","2ad406c0":"code","e0615ddb":"code","b70b7233":"code","8ba14f2f":"code","65e1fb62":"code","ee659dc8":"code","8394d14e":"code","a7962bc0":"code","1e54b55e":"code","a7bed97a":"code","b868c898":"code","98aefa12":"code","582ddcc6":"code","93f8e51f":"code","aac6afb8":"markdown","f1829a69":"markdown","e462648c":"markdown","e69e71a2":"markdown","7af0a287":"markdown","640dc3a9":"markdown","5f6b4445":"markdown","9a954cf3":"markdown"},"source":{"4905f76b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport optuna\nfrom optuna.trial import TrialState\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbfdb54a":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n\ntrain_df[\"nan_count\"] = train_df.isnull().sum(axis=1)\ntest_df[\"nan_count\"] = test_df.isnull().sum(axis=1)\n\nfeature_cols = [col for col in train_df.columns if col not in ['id', 'claim']]\n\nX, y = train_df[feature_cols], train_df['claim']\ntest = test_df[feature_cols]","2ad406c0":"train_df.head()","e0615ddb":"data_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n    ('scaler', StandardScaler())\n])","b70b7233":"params_xgb = {'max_depth': 3,\n              'n_estimators': 7781,\n              'min_child_weight': 7, \n             'learning_rate': 0.051757426392409905, \n             'gamma': 9.213719370277009e-06, \n             'lambda': 0.007979791084907156, \n             'alpha': 0.030419732577886008, \n             'subsample': 0.6768773088432918, \n             'colsample_bytree': 0.6616897821787362, \n             'grow_policy': 'lossguide',\n             'random_state': 41,\n             'verbosity' : 0}\n\nif  torch.cuda.is_available():\n    _device = 'gpu'\nelse:\n    _device = 'cpu'\n    \nif _device == 'cpu':\n    params_xgb['tree_method'] = 'hist'\n    params_xgb['prediction'] = 'cpu_predictior'\nelse:\n    params_xgb['tree_method'] = 'gpu_hist'\n    params_xgb['prediction'] = 'gpu_predictior'\n    \nmodel_xgb = XGBRegressor(**params_xgb)\n\npipeline_xgb = Pipeline(steps=[\n    ('transformer', data_transformer),\n    ('model', model_xgb)\n])","8ba14f2f":"# pipeline_xgb.fit(X, y)\n# predictions = pipeline_xgb.predict(test)\n# print(predictions[:10])","65e1fb62":"if  torch.cuda.is_available():\n    _device = 'gpu'\nelse:\n    _device = 'cpu'\nparams_lgbm = {'learning_rate': 0.09497708539776868,\n          'n_estimators': 3168,\n          'max_depth': 2,\n          'reg_alpha': 0.015322467962858254,\n          'reg_lambda': 1.7933044558463738e-06,\n          'device' : _device,\n          'random_state': 41,\n          'verbosity' : 0}\n\nmodel_lgbm = LGBMClassifier(**params_lgbm)\n\npipeline_lgbm = Pipeline(steps=[\n    ('transformer', data_transformer),\n    ('model', model_lgbm)\n])","ee659dc8":"# pipeline_lgbm.fit(X, y)\n# predictions = pipeline_lgbm.predict(test)\n# print(predictions[:10])","8394d14e":"def cross_validation_predict(pipe, X, y, test_data, name, do_add=True, n_folds = 10):\n    global train_df, test_df\n    valid_predictions = {}\n    submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n    submission.columns = ['id', 'pred_' + name]\n    \n    kfold = StratifiedKFold(n_splits = n_folds, random_state=41, shuffle=True)\n\n    predictions = [0.0]*test.shape[0]\n    for idx in kfold.split(X=X, y=y):\n        train_idx, val_idx = idx[0], idx[1]\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        pipe.fit(X_train, y_train)\n\n        val_preds = pipe.predict(X_val)\n        print(roc_auc_score(y_val, val_preds))\n        if do_add:\n            valid_predictions.update(dict(zip(val_idx, val_preds)))\n        preds = pipe.predict(test_data)\n        predictions += preds \/ kfold.n_splits\n        \n    if do_add:\n        valid_predictions = pd.DataFrame.from_dict(valid_predictions, orient=\"index\").reset_index()\n        valid_predictions.columns = [\"id\", \"pred_\" + name]\n        train_df = train_df.merge(valid_predictions, on=\"id\", how=\"left\")\n\n        submission['pred_' + name] = preds\n        test_df = test_df.merge(submission, on=\"id\", how=\"left\")\n\n        \n    return predictions","a7962bc0":"predictions = cross_validation_predict(pipeline_lgbm, X, y, test, \"lgbm\", n_folds = 10)\n\nprint(predictions[:10])","1e54b55e":"test_df.head()","a7bed97a":"predictions = cross_validation_predict(pipeline_xgb, X, y, test, \"xgb\", n_folds = 10)\n\nprint(predictions[:10])","b868c898":"train_df.head()","98aefa12":"def blender_predict(model, useful_features):\n    global train_df, test_df\n    X_train = train_df[useful_features]\n    y_train = train_df.claim\n    test = test_df[useful_features]\n    \n    model.fit(X_train, y_train)\n    predictions = model.predict(test)\n    \n    return predictions","582ddcc6":"from sklearn.linear_model import LinearRegression\nuseful_features = ['pred_xgb', 'pred_lgbm']\n\npredictions = blender_predict(LinearRegression(), useful_features)\nprint(predictions[:10])\n\nX_new = train_df[useful_features]\ny_new = train_df.claim\ntest_new = test_df[useful_features]\npredictions_2 = cross_validation_predict(LinearRegression(), X_new, y_new, test_new, \"none\", False)\nprint(predictions_2[:10])","93f8e51f":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmission.claim = predictions_2\nsubmission.to_csv(\"submission.csv\", index=False)","aac6afb8":"# **K-fold or Crossvlidation Function**\nThis function uses crossvalidation to create predictions for all training and test data to blenda afterwards.","f1829a69":"This cell makes two predictions, one with blending on the full dataset and another with K-fold.","e462648c":"# **Pipeline - LGBM**","e69e71a2":"# **Blending function**","7af0a287":"This Cell uses the whole training data to train the XGB model and makes predictions. This is commented out at the moment as the purpose of this pipeline is to create a pipeline starting from scratch and ending with blending.","640dc3a9":"# **Pipeline - XGB**\nThe hyperparameters were tuned separately with optuna.","5f6b4445":"The next four cells create data for blending using the pipelines and crossvalidation function.","9a954cf3":"# **Data Transformation**\nFirst step is to impute missing values and then scale the features as the features have very different ranges."}}