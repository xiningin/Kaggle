{"cell_type":{"524a76dc":"code","5a291724":"code","0f0e4db1":"code","985f9bbe":"code","6df4a461":"code","d1299170":"code","d43d4d46":"code","b637ee7f":"code","f5203f86":"code","05bde37c":"code","f80fa155":"code","b14473a4":"code","bae31729":"code","9f479b6e":"code","fb4c942d":"code","bdc58263":"code","46b4b443":"code","b3c79126":"code","6656360c":"code","d65df82d":"code","b3d91ba5":"code","eb0f1b53":"code","aad218a7":"code","436f892b":"code","70c4716a":"code","7529c7ca":"code","7c4e848e":"code","149380e9":"code","51f10b37":"code","a511eac9":"code","d8101ae0":"code","c34f27da":"code","44314de0":"code","85e030ee":"markdown","60e39604":"markdown","a3bbbffc":"markdown","be226927":"markdown","455222e0":"markdown","987fdfab":"markdown","0fafddfe":"markdown","2ddf1172":"markdown","75ddf19a":"markdown","c2b854af":"markdown","d0732442":"markdown","e8994d3a":"markdown","31c3f13f":"markdown","d59d0c16":"markdown","9f125fe9":"markdown","4430fdb4":"markdown","be928ae2":"markdown","15fa184d":"markdown","cd02afcb":"markdown"},"source":{"524a76dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# import the used lib\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sklearn\nfrom matplotlib import pyplot as plt\nimport json\nimport glob\nfrom wordcloud import WordCloud\nimport re\nimport nltk\nimport time\n","5a291724":"paths = os.listdir('\/kaggle\/input\/')\nprint(paths)","0f0e4db1":"clean_data_path = f'\/kaggle\/input\/preprocess\/clean_covid_data.csv'\ndf_covid = pd.read_csv(clean_data_path)","985f9bbe":"df_covid.describe(include='all')","6df4a461":"from nltk.corpus import stopwords\nimport scipy.misc\nfrom matplotlib.pyplot import imread","d1299170":"df_title = df_covid.loc[:, [\"title\"]].dropna()\ndf_title.info()\ndf_abstract = df_covid.loc[:, [\"abstract\"]].dropna()\ndf_abstract.info()","d43d4d46":"def lower_case(x):\n    return x.lower()\n\ndf_title[\"title\"] = df_title['title'].apply(lambda x: lower_case(x))\ndf_title[\"title\"] = df_title['title'].apply(lambda x: x.strip())\ndf_title[\"title\"] = df_title['title'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_title[\"title\"] = df_title['title'].apply(lambda x: re.sub(' +',' ',x))\ntitles = ' '.join(df_title[\"title\"])\n\n\ndf_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: lower_case(x))\ndf_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: x.strip())\ndf_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: re.sub(' +',' ',x))\nabstracts = ' '.join(df_abstract[\"abstract\"])\n\nprint(titles[:100])\nprint(abstracts[:200])","b637ee7f":"df_title = None\ndf_abstract = None","f5203f86":"stopword = stopwords.words('english')  # remove the stop words\n\nwordcloud_title = WordCloud(max_font_size=None, background_color='white', \n                      collocations=False, stopwords=stopword,\n                      width=1000, height=1000).generate(titles)\n\nwordcloud_abstract = WordCloud(max_font_size=None, background_color='white', \n                      collocations=False, stopwords=stopword,\n                      width=1000, height=1000).generate(abstracts)\n\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.imshow(wordcloud_title)\nplt.title('Common Words in Title')\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.imshow(wordcloud_abstract)\nplt.title('Common Words in Abstract')\nplt.show()","05bde37c":"\nwnl = nltk.stem.WordNetLemmatizer()\n\nword_count = {}\n\ndef Pos_tag(text, publish_time):\n    token = nltk.word_tokenize(text)\n    pos = nltk.pos_tag(token)\n    try:\n        timeStruct = time.strptime(publish_time, \"%Y-%m-%d\")\n    except:\n        timeStruct = time.strptime(publish_time, \"%Y\")\n    return token, pos, timeStruct.tm_year\n\n# 'NN*', 'VB*'\ndef add2vocab(pos_tag):\n    for w, p in pos_tag:\n        if (re.match('NN',p) or re.match('VB',p)) and w not in stopword and w.isalnum() and len(w) > 1:\n            w = w.lower()\n            if re.match('NN',p):\n                w = wnl.lemmatize(w, pos='n')  \n            if re.match('VB',p):\n                w = wnl.lemmatize(w, pos='v')\n            if w in word_count:\n                word_count[w] += 1\n            else:\n                word_count[w] = 1  ","f80fa155":"df_subset = df_covid.loc[:, ['abstract','publish_time']].dropna()\nall_pos = []\nall_year = []\nfor idx, (abstract, publish_time) in df_subset.iterrows():\n    token, pos, year = Pos_tag(abstract, publish_time)\n    add2vocab(pos)\n    all_pos.append(pos)\n    all_year.append(year)","b14473a4":"# vocab = [k for k,v in word_count if v > 5]\nword_count_sort = sorted(word_count.items(), key=lambda d: d[1], reverse=True)\nvocab = [k for k,v in word_count_sort[:100]]\ncount = [v for k,v in word_count_sort[:100]]\nprint(vocab)","bae31729":"plt.figure(figsize=(10,10))\nplt.barh(range(len(vocab[:50])), count[:50], height=0.3, color='steelblue', alpha=0.8)  \nplt.yticks(range(len(vocab[:50])), vocab)\n# plt.xlim(30,47)\nplt.xlabel(\"frequency\")\nplt.title(\"Most Frequent words\")\n# for x, y in enumerate(count):\n#     plt.text(y + 0.2, x - 0.1, '%s' % y)\nplt.show()","9f479b6e":"max_year = max(all_year)\nmin_year = min(all_year)\nprint('articles are from %d year to %d year.' % (min_year, max_year))\npublish_count = np.zeros(max_year-min_year+1)\nfor y in all_year:\n    publish_count[y-min_year] += 1\nyear_list = list(range(min_year, max_year+1))","fb4c942d":"word2ix = {word:ix for ix, word in enumerate(vocab)}\nmatrix = np.zeros((max_year-min_year+1) * len(vocab)).reshape(max_year-min_year+1, len(vocab))\nfor pos, year in zip(all_pos, all_year):\n    for w,p in pos:\n        if re.match('NN',p):\n            w = wnl.lemmatize(w, pos='n')\n            if w in vocab:\n                matrix[year-min_year][word2ix[w]] += 1\n        elif re.match('VB',p):\n            w = wnl.lemmatize(w, pos='v')\n            if w in vocab:\n                matrix[year-min_year][word2ix[w]] += 1","bdc58263":"# sub_axix = filter(lambda x:x%200 == 0, x_axix)\nsmall_matrix = matrix[:-1,:20].copy()  # 1957-2019, top20 words\nplt.figure(figsize=(15,10))\nplt.title('Words Trend')\nsize1, size2 = small_matrix.shape\nyear_num = year_list[:-1]\ncolors = ['g', 'r', 'b', 'k', 'y', 'c', 'm']\nfor idx in range(size2):\n    plt.plot(year_num, list(small_matrix[:, idx]), color=colors[idx%7], label=vocab[idx])\nplt.plot(year_num, publish_count[:-1], 'r*', label='publications')\nplt.legend() # \u663e\u793a\u56fe\u4f8b\n\nplt.xlabel('year')\nplt.ylabel('word frequency')\nplt.show()","46b4b443":"small_matrix = None","b3c79126":"small_matrix = matrix[-20:-1,:20].copy()  # 2000-2019, top20 words\nsmall_count = publish_count[-20:-1]\nfor idx in range(small_matrix.shape[0]):\n    small_matrix[idx,:] = small_matrix[idx,:]\/small_count[idx]\nplt.figure(figsize=(15,10))\nplt.title('Words Trend')\nsize1, size2 = small_matrix.shape\nyear_num = year_list[-20:-1]\nprint(year_num)\ncolors = ['g', 'r', 'b', 'k', 'y', 'c', 'm']\nfor idx in range(size2):\n    plt.plot(year_num, list(small_matrix[:, idx]), color=colors[idx%7], label=vocab[idx])\nplt.legend() \n\nplt.xlabel('year')\nplt.ylabel('word frequency')\nplt.show()","6656360c":"matrix = None\nsmall_matrix = None","d65df82d":"df_cluster = df_covid.loc[:, [\"paper_id\",\"title\",\"abstract\"]].dropna()\ndf_cluster[\"abstract\"] = df_cluster['abstract'].apply(lambda x: lower_case(x))\ndf_cluster[\"abstract\"] = df_cluster['abstract'].apply(lambda x: x.strip())\ndf_cluster[\"abstract\"] = df_cluster['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_cluster[\"abstract\"] = df_cluster['abstract'].apply(lambda x: re.sub(' +',' ',x))","b3d91ba5":"corpus = list(df_cluster['abstract'])\nprint(len(corpus))","eb0f1b53":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nfrom time import time\n\nX = None\n\ndef prepare_text_vector(opts):\n    \n    # #############################################################################\n    # Transform the text to vector\n    \n    global X\n    t0 = time()\n    if opts.use_hashing:\n        if opts.use_idf:\n            # Perform an IDF normalization on the output of HashingVectorizer\n            hasher = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english', alternate_sign=False,\n                                       norm=None)\n            vectorizer = make_pipeline(hasher, TfidfTransformer())\n        else:\n            vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                           stop_words='english',\n                                           alternate_sign=False, norm='l2')\n    else:\n        vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                     min_df=2, stop_words='english',\n                                     use_idf=opts.use_idf)\n    X = vectorizer.fit_transform(corpus)\n    X = X.toarray()\n\n    print(\"done in %fs\" % (time() - t0))\n    print(\"n_samples: %d, n_features: %d\" % X.shape)\n    print()\n\n    # #############################################################################\n    # Performing dimensionality reduction\n    \n    if opts.n_components:\n        if opts.use_pca:\n            print(\"Performing dimensionality reduction using SparsePCA\")\n            t0 = time()\n            pca_sk = SparsePCA(n_components=3)\n            X = pca_sk.fit_transform(X)\n            print(\"done in %fs\" % (time() - t0))\n        else:\n            print(\"Performing dimensionality reduction using LSA\")\n            t0 = time()\n            # Vectorizer results are normalized, which makes KMeans behave as\n            # spherical k-means for better results. Since LSA\/SVD results are\n            # not normalized, we have to redo the normalization.\n            svd = TruncatedSVD(opts.n_components)\n            normalizer = Normalizer(copy=False)\n            lsa = make_pipeline(svd, normalizer)\n\n            X = lsa.fit_transform(X)\n\n            print(\"done in %fs\" % (time() - t0))\n\n            explained_variance = svd.explained_variance_ratio_.sum()\n            print(\"Explained variance of the SVD step: {}%\".format(\n                int(explained_variance * 100)))\n    print()\n    ","aad218a7":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\n\ndef cluster_text(opts):  \n    \n    # #############################################################################\n    # Kmeans clustering\n\n    # y_preds = KMeans(n_clusters=5, random_state=0, n_jobs=4, verbose=10).fit_predict(pca_result)\n    if opts.minibatch:\n        estimators = [('k_means_3', MiniBatchKMeans(n_clusters=3, init='k-means++', n_init=1,\n                                                    init_size=1000, batch_size=1000, verbose=opts.verbose), '221'),\n                      ('k_means_4', MiniBatchKMeans(n_clusters=4, init='k-means++', n_init=1,\n                                                    init_size=1000, batch_size=1000, verbose=opts.verbose), '222'),\n                      ('k_means_5', MiniBatchKMeans(n_clusters=5, init='k-means++', n_init=1,\n                                                    init_size=1000, batch_size=1000, verbose=opts.verbose), '223'),\n                      ('k_means_6', MiniBatchKMeans(n_clusters=6, init='k-means++', n_init=1,\n                                                    init_size=1000, batch_size=1000, verbose=opts.verbose), '224'),]\n    else:\n        estimators = [('k_means_3', KMeans(n_clusters=3, init='k-means++', max_iter=100, \n                                           n_init=1, verbose=opts.verbose), '221'),\n                      ('k_means_4', KMeans(n_clusters=4, init='k-means++', max_iter=100, \n                                           n_init=1, verbose=opts.verbose), '222'),\n                      ('k_means_5', KMeans(n_clusters=5, init='k-means++', max_iter=100, \n                                           n_init=1, verbose=opts.verbose), '223'),\n                      ('k_means_6', KMeans(n_clusters=6, init='k-means++', max_iter=100, \n                                           n_init=1, verbose=opts.verbose), '224'),]\n    fignum = 1\n    titles = ['Covid-19 Articles\\' Abstract - Clustered (K-Means) 3 clusters', \n              'Covid-19 Articles\\' Abstract - Clustered (K-Means) 4 clusters', \n              'Covid-19 Articles\\' Abstract - Clustered (K-Means) 5 clusters', \n              'Covid-19 Articles\\' Abstract - Clustered (K-Means) 6 clusters']\n    fig = plt.figure(fignum, figsize=(20, 15))\n    t0 = time()\n    for name, est, fig_idx in estimators:\n        ax = fig.add_subplot(int(fig_idx),projection='3d')\n        print(\"Clustering sparse data with %s\" % est)\n        est.fit(X)\n        labels = est.labels_\n        centers = est.cluster_centers_\n        ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=10, c=labels.astype(np.float), edgecolor='y')\n        ax.w_xaxis.set_ticklabels([])\n        ax.w_yaxis.set_ticklabels([])\n        ax.w_zaxis.set_ticklabels([])\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_zlabel('z')\n        ax.set_title(titles[fignum - 1])\n        ax.dist = 12\n        fignum = fignum + 1\n    print(\"done in %0.3fs\" % (time() - t0))\n    fig.show()","436f892b":"from optparse import OptionParser\n\nopt = OptionParser()\nopt.add_option(\"--use_hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nopt.add_option(\"--n_features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nopt.add_option(\"--use_idf\",\n              action=\"store_false\", default=True,\n              help=\"Use Inverse Document Frequency feature weighting.\")\n\nopt.add_option(\"--n_components\", type=\"int\", default=3,\n               help=\"Preprocess documents with latent semantic analysis.\")\nopt.add_option(\"--use_pca\", default=True, \n               help=\"if True use PCA else use SVD\")\nopt.add_option(\"--minibatch\", action=\"store_false\", default=False,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\n\nopt.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n","70c4716a":"opt.use_hashing = True\nopt.n_features = 2**10\nopt.use_idf = False\nopt.n_components = 3\nopt.use_pca = True\nopt.minibatch = False\nopt.verbose = False\nprepare_text_vector(opt)\nprint(X.shape)\ncluster_text(opt)","7529c7ca":"opt.use_hashing = True\nopt.n_features = 2**12\nopt.use_idf = False\nopt.n_components = 3\nopt.use_pca = True\nopt.minibatch = False\nopt.verbose = False\nprepare_text_vector(opt)\nprint(X.shape)\ncluster_text(opt)","7c4e848e":"opt.use_hashing = True\nopt.n_features = 2**10\nopt.use_idf = True\nopt.n_components = 3\nopt.use_pca = True\nopt.minibatch = False\nopt.verbose = False\nprepare_text_vector(opt)\nprint(X.shape)\ncluster_text(opt)","149380e9":"opt.use_hashing = True\nopt.n_features = 2**10\nopt.use_idf = False\nopt.n_components = 3\nopt.use_pca = False\nopt.minibatch = False\nopt.verbose = False\nprepare_text_vector(opt)\nprint(X.shape)\ncluster_text(opt)","51f10b37":"opt.use_hashing = True\nopt.n_features = 2**10\nopt.use_idf = False\nopt.n_components = 3\nopt.use_pca = False\nopt.minibatch = False\nopt.verbose = False\nprepare_text_vector(opt)\nprint(X.shape)","a511eac9":"est = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1, verbose=opt.verbose)\nest.fit(X)\nlabels = est.labels_\ncenters = est.cluster_centers_\ncenter_idx = []\nfor center in centers:\n    min_idx = 0\n    min_dis = np.inf\n    for idx, loc in enumerate(X):\n        dis = ((center - loc)**2).sum()\n        if dis < min_dis:\n            min_dis = dis\n            min_idx = idx\n    center_idx.append(min_idx)\nprint(center_idx)\n\npd.set_option('display.width',400)\nfor c_ix in center_idx:\n    tmp = df_cluster.iloc[c_ix:c_ix+1]\n    title = list(tmp['title'])\n    print(' '.join(title).replace('<br>', ' '))\n#     abstract = list(tmp['abstract'])\n#     print(' '.join(abstract))\ncluster0_idx = [idx for idx, x in enumerate(labels) if x == 0]\ncluster1_idx = [idx for idx, x in enumerate(labels) if x == 1]\ncluster2_idx = [idx for idx, x in enumerate(labels) if x == 2]\nprint(len(cluster0_idx))\nprint(len(cluster1_idx))\nprint(len(cluster2_idx))","d8101ae0":"cluster0_abstract = list(df_cluster.iloc[cluster0_idx].loc[:, 'abstract'])\ncluster1_abstract = list(df_cluster.iloc[cluster1_idx].loc[:, 'abstract'])\ncluster2_abstract = list(df_cluster.iloc[cluster2_idx].loc[:, 'abstract'])\nassert len(cluster0_abstract) == len(cluster0_idx) and len(cluster1_abstract) == len(cluster1_idx) and len(cluster2_abstract) == len(cluster2_idx)\nabstract0 = ' '.join(cluster0_abstract)\nabstract1 = ' '.join(cluster1_abstract)\nabstract2 = ' '.join(cluster2_abstract)","c34f27da":"wordcloud_abstract0 = WordCloud(max_font_size=None, background_color='white', \n                      collocations=False, stopwords=stopword,\n                      width=1000, height=1000).generate(abstract0)\nwordcloud_abstract1 = WordCloud(max_font_size=None, background_color='white', \n                      collocations=False, stopwords=stopword,\n                      width=1000, height=1000).generate(abstract1)\nwordcloud_abstract2 = WordCloud(max_font_size=None, background_color='white', \n                      collocations=False, stopwords=stopword,\n                      width=1000, height=1000).generate(abstract2)\n\nplt.figure(figsize=(25,15))\nplt.subplot(1,3,1)\nplt.axis(\"off\")\nplt.imshow(wordcloud_abstract0)\nplt.title('Common Words in Abstract Cluster 1')\nplt.subplot(1,3,2)\nplt.axis(\"off\")\nplt.imshow(wordcloud_abstract1)\nplt.title('Common Words in Abstract Cluster 2')\nplt.subplot(1,3,3)\nplt.axis(\"off\")\nplt.imshow(wordcloud_abstract2)\nplt.title('Common Words in Abstract Cluster 3')\nplt.show()","44314de0":"w0 = [w for w,f in list(wordcloud_abstract0.words_.items())[:10]]\nprint(', '.join(w0))\nw1 = [w for w,f in list(wordcloud_abstract1.words_.items())[:10]]\nprint(', '.join(w1))\nw2 = [w for w,f in list(wordcloud_abstract2.words_.items())[:10]]\nprint(', '.join(w2))","85e030ee":"As we can see from the clustering diagram that using the SVD can get better performance.\nFor faster and easier, we cluster into 3 categories and use the set \"hashvector with idf (1024), SVD, kmeans\".","60e39604":"The three words \"virus\", \"infection\" and \"cell\" almost appear in every abstract. ","a3bbbffc":"Now let's look the rate trend.","be226927":"We use idf in hashvector, let's see the cluster performance of \"hashvector with idf (1024), pca, kmeans\".","455222e0":"I have preprocess the data in https:\/\/www.kaggle.com\/qingliu67\/data-preprocess, and I just use the data.","987fdfab":"## 2. Analysis the Trend","0fafddfe":"There are 9489 articles, 7654 articles and 5358 articles in the three categories. And we can find that the research directions of these three clustering centers are different. ","2ddf1172":"The code is modified based on the[ sciki-learn document.](https:\/\/scikit-learn.org\/stable\/auto_examples\/text\/plot_document_clustering.html#sphx-glr-download-auto-examples-text-plot-document-clustering-py)","75ddf19a":"We can see that most frequent words in title are \"virus\", \"infection\", \"desease\", \"respiratory\" and in abstract are \"infection\", \"cell\", \"protein\", \"viru\", \"cell\".","c2b854af":"Now we use the SVD to do dimensionality reduction, let's see the cluster performance of \"hashvector with idf (1024), SVD, kmeans\".","d0732442":"First, we look the word frequency trend.","e8994d3a":"As we can see, the cluster1 is about ","31c3f13f":"The abstract of a paper covers the main content, so only the abstract is used to count word frequency, which is also to save time.","d59d0c16":"Now let's see the cluster performance of \"hashvector (1024), pca, kmeans\".","9f125fe9":"As can be seen from the figure, there are some unrelated words. In the following work, I will try to filter out more relevant words. ","4430fdb4":"# Analyze the Data","be928ae2":"## 1. See the Title and Abstract's Words Cloud","15fa184d":"## 3. Use Hashvector to Cluster","cd02afcb":"We change to vetor size to 4096, let's see the cluster performance of \"hashvector (4096), pca, kmeans\"."}}