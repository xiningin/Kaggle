{"cell_type":{"6929eb0c":"code","79d25cd1":"code","f5d1577a":"code","35f62d90":"code","3217171f":"code","2c789fa9":"code","6310a554":"code","f9cf6e91":"code","5fcd4c7e":"code","2d27724d":"code","69850569":"code","28866cb7":"code","0a64afed":"code","6f006206":"code","e0420497":"code","14fcacc6":"code","7af9a5d0":"code","dfd8f632":"code","38de34a9":"code","1928ddd6":"code","fa29f74b":"code","caddc0de":"code","890e8d34":"code","08c76fa4":"code","a14b7846":"code","1b3990ce":"code","e32fcc12":"code","9f6b96cf":"code","98de456a":"code","7c7c5f38":"code","faffa7d6":"code","b37059c9":"code","44ce9d12":"code","26cdfcc9":"code","306ba38f":"code","8c1f3ddb":"code","031396cb":"code","5ee08398":"code","858ff7b9":"markdown","0e030f5f":"markdown","a05f6e56":"markdown","c581c855":"markdown","33463382":"markdown","0ed67775":"markdown"},"source":{"6929eb0c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy.stats import stats\nfrom scipy.stats import zscore\nfrom scipy.stats.mstats import winsorize\nimport warnings\nwarnings.filterwarnings(\"ignore\")","79d25cd1":"df=pd.read_csv(\"..\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv\",index_col=0 ).reset_index() \n\n# First 5 rows in the dataset:\ndisplay(df.head())","f5d1577a":"# Dataset's variables type:\nprint(df.info(),\"\\n\")","35f62d90":"# Count of unique values in the dataset:\nprint(\"Count of Unique Values:\\n \",df.nunique(),\"\\n\")","3217171f":"# Count of rows and columns in the dataset:\nprint(\"Count of row and column: \",\"\\n\",df.shape)","2c789fa9":"# Count of  unique \"Topic\" column\nprint(\"Count of  unique Topic column: \",\"\\n\\n\", df[\"Topic\"].value_counts(), \"\\n\")\n# There are 12 diffrent subjects\n\ndisplay(pd.crosstab(df[\"NationalITy\"],df[\"Topic\"]).reset_index())\n# According to nationality count of Topic","6310a554":"# \"Topic\" column should be \"category\" variable type. So,\ndf[\"Topic\"]=df[\"Topic\"].astype('category')\n\n# \"StageID\" also,\ndf[\"StageID\"]=df[\"StageID\"].astype('category')\n\n# \"StudentAbsenceDays\"  also,\ndf[\"StudentAbsenceDays\"]=df[\"StudentAbsenceDays\"].astype('category')\n\n# \"Class\"  also,\ndf[\"Class\"]=df[\"Class\"].astype('category')\n\ndf.info()","f9cf6e91":"# sum of null values for each variables:\nprint(\"Count of null values: \\n\" ,df.isna().sum(),\"\\n\")","5fcd4c7e":"# Let's to find out success of student,add a new column including weighted variables.So if the number of raises a student's hand is 20, \n# weighted raisedhands would be 2 in a new column.\n\ndf[\"weighted_rh\"]=pd.cut(x=df[\"raisedhands\"], bins=[-1,25,50,75,100], labels=[2,3,4,5])\ndf[\"weighted_vr\"]=pd.cut(x=df[\"VisITedResources\"], bins=[-1,25,50,75,100], labels=[2,3,4,5])\ndf[\"weighted_av\"]=pd.cut(x=df[\"AnnouncementsView\"], bins=[-1,25,50,75,100], labels=[2,3,4,5])\ndf[\"weighted_dis\"]=pd.cut(x=df[\"Discussion\"], bins=[-1,25,50,75,100], labels=[2,3,4,5])\n\n# All weighted variables are category variable type so we should change the type to 'int64'\n\ndf[\"weighted_rh\"]=df[\"weighted_rh\"].astype('int64')\ndf[\"weighted_vr\"]=df[\"weighted_vr\"].astype('int64')\ndf[\"weighted_av\"]=df[\"weighted_av\"].astype('int64')\ndf[\"weighted_dis\"]=df[\"weighted_dis\"].astype('int64')\n\n# For each student add a new column called \"studentsucces\"\n\ndf[\"studentsuccess\"]=((df[\"raisedhands\"]*df[\"weighted_rh\"])+(df[\"VisITedResources\"]*df[\"weighted_vr\"])+\n                      (df[\"AnnouncementsView\"]*df[\"weighted_av\"])+(df[\"Discussion\"]*df[\"weighted_dis\"]))\/(df[\"weighted_rh\"]+df[\"weighted_vr\"]+df[\"weighted_av\"]+df[\"weighted_dis\"])\n\n\ndf.head()","2d27724d":"# Statistical variables for each column:\ndf.describe()\n\n# Standart deviation is more than half of the mean, so these continous variables can not have normal distribution \n# But to make sure it can be analyzed by statistical distributions","69850569":"# Let's visualize the dataset to analyze it better \n\nplt.figure(figsize=(17,10), dpi=100)\nplt.subplot(2,3,1)\nplt.title(\"Student Success for Topic\")\nsns.barplot(df[\"Topic\"],y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.xticks(rotation=45)\nplt.ylim(0,85)\n\nplt.subplot(2,3,2)\nplt.title(\"Student Success for NationalITy\")\nsns.barplot(df[\"NationalITy\"],y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.xticks(rotation=45)\nplt.ylim(0,85)\n\nplt.subplot(2,3,3)\nplt.title(\"Student Success for Student Absence Day\")\nsns.barplot(df[\"StudentAbsenceDays\"],y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.ylabel(\"Student  Success\")\n    \nplt.subplot(2,3,4)\nsns.barplot(x=df[\"ParentschoolSatisfaction\"], y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.xticks(rotation=55)\n\nplt.subplot(2,3,5)\nsns.barplot(x=df[\"Relation\"], y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.xticks(rotation=45)\n\nplt.subplot(2,3,6)\nsns.barplot(x=df[\"StageID\"], y=df[\"studentsuccess\"], data=df, palette=\"Greens\")\nplt.xticks(rotation=45)\n\nplt.show()\n","28866cb7":"# According to nationality student's success rates for each topic\n\ndf.groupby(\"Topic\")[\"studentsuccess\"].mean().sort_values().reset_index()\n\ntopic_natio=df.groupby(by=[\"NationalITy\",\"Topic\"])[\"studentsuccess\"].mean().reset_index()\ntopic_natio.head()\n\nfig=px.bar(topic_natio, x=\"NationalITy\", y=\"studentsuccess\", color=\"Topic\")\nfig.show()","0a64afed":"plt.figure(figsize=(16,4), dpi=100)\n\ncolumn=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]\nfor i in range(len(column)):\n    plt.subplot(1,5,i+1)\n    plt.title(\"{}\".format(column[i]))\n    plt.boxplot(df[column[i]], whis=1.5 )\nplt.show()        \n        \n# We do not have any outliers values.","6f006206":"zscores=zscore(df[\"raisedhands\"])\nzscores=pd.DataFrame(zscores)\n\nfor threshold in range(1,5):\n    print(\"Threshold value is {}: \".format(threshold))\n    print(\"--\"*10)\n    print(\"Count of outliers: \",len(np.where(zscores>threshold)[0]),\"\\n\")\n\n# To make sure, we can do zscores. For each whiskers, count of outliers. Default whiskers value is 1.5 so we can say that there are no outliers","e0420497":"# PERCENTILE METHOD\n\ncolumn=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]\nIQR=[]\n\nfor i in range(len(column)):        \n    q75,q25=np.percentile(df[column[i]], [75,25])\n    IQR.append(q75-q25)\n\ndf_interqu=pd.DataFrame(columns=column)\ndf_interqu=df_interqu.append({\"raisedhands\":IQR[0],\n                              \"VisITedResources\":IQR[1],\n                              \"AnnouncementsView\":IQR[2],\n                              \"Discussion\":IQR[3],\n                              \"studentsuccess\":IQR[4]}, ignore_index=True)\ndisplay(df_interqu)","14fcacc6":"plt.figure(figsize=(18,5),dpi=100)\n\ncolumn=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\"]\n\nplt.figure(figsize=(18,5), dpi=100)\nfor i in range(len(column)):\n    plt.subplot(1,4,i+1)\n    sns.scatterplot(y=df[column[i]], x=df[\"studentsuccess\"], data=df)\n\nplt.show()\n\n# There is a relationship between studentsuccess and other variables but studentsucess and visitedresources is more than others ","7af9a5d0":"# Correlation matrix:\n\ncorrelation=df[[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]].corr()\ncorrelation\n","dfd8f632":"# Correlaton result is changing for each topic so we can grouping:\n\ncorr_topic=df.groupby(\"Topic\")[[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]].corr()\ncorr_topic.head(20)","38de34a9":"corr_english=df[df[\"Topic\"]==\"English\"][[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]].corr()\nsns.heatmap(corr_english, annot=True, linewidth=0.5, fmt='.4g')\n\nplt.show()","1928ddd6":"# Is there any relationship between \"Parentschoolsatisfaction\" and \"studentsuccess\"? To test it, we can apply ttest in scipy.stats libraries\n# If p value is less than 0.05, we can accept HA, on the other hand we can say \"there is an important difference between Parentschoolsatisfaction\n# and studentsuccess\".\n\npd.options.display.float_format= '{:.15f}'.format\n\ndf_satisfied=df[\"ParentschoolSatisfaction\"].unique()\n\nfor var in [\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]:\n    comparison=pd.DataFrame(columns=[\"Satisfied\",\"Not_Satisfied\",\"statistic\",\"p_value\"])\n    print(\"Comparison for {}\".format(var), end='')\n    for i in range(0, len(df_satisfied)):\n        for j in range(i+1, len(df_satisfied)):\n            ttest=stats.ttest_ind(df[df[\"ParentschoolSatisfaction\"]==df_satisfied[i]][var],\n                                  df[df[\"ParentschoolSatisfaction\"]==df_satisfied[j]][var])\n           \n            Satisfied=df_satisfied[i]\n            Not_Satisfied=df_satisfied[j]\n            statistic=ttest[0]\n            p_value=ttest[1]\n            \n            comparison=comparison.append({\"Satisfied\":Satisfied,\n                                         \"Not_Satisfied\":Not_Satisfied,\n                                         \"statistic\":statistic,\n                                          \"p_value\":p_value}, ignore_index=True)\n    display(comparison)            \n","fa29f74b":"# to test whether there is a difference between the two categorical variables, we can make chiquare in the scipy.stats libraries.\n# If p value is less than 0.05, we can accept HA, on the other hand we can say \"there is an important difference between the two categorical variables\n\npd.options.display.float_format= '{:.15f}'.format\nnatio_topic=pd.crosstab(df[\"NationalITy\"],df[\"Topic\"])\n\nprint(stats.chisquare(natio_topic, axis=None))","caddc0de":"pd.options.display.float_format='{:.15f}'.format\nnatio_stfcn=pd.crosstab(df[\"ParentschoolSatisfaction\"],df[\"NationalITy\"])\n\nprint(stats.chisquare(natio_stfcn, axis=None))","890e8d34":"print(\"Raisedhands & success:\",\"\\n\",stats.ttest_ind(df[\"raisedhands\"], df[\"studentsuccess\"], equal_var=False),\"\\n\")\nprint(\"Visited resources & success:\",\"\\n\",stats.ttest_ind(df[\"VisITedResources\"], df[\"studentsuccess\"], equal_var=False),\"\\n\")\nprint(\"Announcement view & success:\",\"\\n\",stats.ttest_ind(df[\"AnnouncementsView\"], df[\"studentsuccess\"], equal_var=False),\"\\n\")\nprint(\"Discussion & success:\",\"\\n\",stats.ttest_ind(df[\"Discussion\"], df[\"studentsuccess\"], equal_var=False))","08c76fa4":"# \u0131s there any differences between relation and studentsuccess?\n\npd.options.display.float_format= '{:.15f}'.format\nrelation=df[\"Relation\"].unique()\n\n\nfor var in [\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]: #\u00f6nce s\u00fcrekli de\u011fi\u015fkenler\n    relation_df=pd.DataFrame(columns=[\"group_1\",\"group_2\",\"statistic\",\"p_value\"])\n    print(\"{}:\".format(var),end='')\n    for i in range(0,len(relation)): # kategorik de\u011fi\u015fken\n        for j in range(i+1,len(relation)):\n            ttest=stats.ttest_ind(df[df[\"Relation\"]==relation[i]][var],\n                                  df[df[\"Relation\"]==relation[j]][var])\n            \n            group_1=relation[i]\n            group_2=relation[j]\n            statistic=ttest[0]\n            p_value=ttest[1]\n            \n            relation_df=relation_df.append({\"group_1\":group_1,\n                                            \"group_2\":group_2,\n                                            \"statistic\":statistic,\n                                            \"p_value\":p_value}, ignore_index=True)\n    display(relation_df)","a14b7846":"# Is there any differences between StudentAbsenceDays and studentsuccess?\n\ndf_absence=df[\"StudentAbsenceDays\"].unique()\n\nfor var in [\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]:\n    print(\"{}: \". format(var),end=' ')\n    absence_comparison=pd.DataFrame(columns=[\"group_1\",\"group_2\",\"statistic\",\"p_value\"])\n    for i in range(0,len(df_absence)):\n        for j in range(i+1,len(df_absence)):\n            ttest=stats.ttest_ind(df[df[\"StudentAbsenceDays\"]==df_absence[i]][var],\n                                 df[df[\"StudentAbsenceDays\"]==df_absence[j]][var])\n            \n            absence_comparison=absence_comparison.append({\"group_1\":df_absence[i],\n                                                         \"group_2\":df_absence[j],\n                                                         \"statistic\":ttest[0],\n                                                         \"p_value\":ttest[1]}, ignore_index=True)\n    display(absence_comparison)","1b3990ce":"# Lets use histogram in seaborn libraries to test whether there is a normal distribution.\n\nplt.figure(figsize=(19,4), dpi=100)\n\ncolumn=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]\n\nfor i in range(len(column)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column[i]])\n    plt.ylim(0,0.0175)\nplt.show()    \n    ","e32fcc12":"# JARQUE - BERA TEST:\n\nfrom scipy.stats import jarque_bera\n\npd.options.display.float_format='{:.8f}'.format\n\nvariables=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]\ndistributions=pd.DataFrame(columns=[\"variable\",\"jarque_bera_stats\",\"jarque_bera_pvalue\"])\n\nfor i in variables:\n    jb=jarque_bera(df[i])\n    distributions=distributions.append({\"variable\":i,\n                                       \"jarque_bera_stats\":jb[0],\n                                       \"jarque_bera_pvalue\":jb[1]}, ignore_index=True)\n\ndistributions\n\n# H0 --> Data has normal distribution\n# H1 --> Data does not have normal distribution\n\n# We accept H1 Hypothesis because of p value is less than 0.05","9f6b96cf":"# NORMAL TEST:\n\nfrom scipy.stats import normaltest\npd.options.display.float_format='{:.6f}'.format\n\nvariables=[\"raisedhands\",\"VisITedResources\",\"AnnouncementsView\",\"Discussion\",\"studentsuccess\"]\ndistributions=pd.DataFrame(columns=[\"variable\",\"normal_test_stats\",\"normal_test_pvalue\"])\n\nfor i in variables:\n    norm=normaltest(df[i])\n    distributions=distributions.append({\"variable\":i,\n                                       \"normal_test_stats\":norm[0],\n                                       \"normal_test_pvalue\":norm[1]}, ignore_index=True)\n\ndistributions\n\n# H0 --> Data has normal distribution\n# H1 --> Data does not have normal distribution\n\n# We accept H1 Hypothesis because of p value is less than 0.05","98de456a":"from sklearn.preprocessing import normalize\n\ndf[\"normalized_studentsuccess\"]=normalize(np.array(df[\"studentsuccess\"]).reshape(1,-1).reshape(-1,1))\n\nnormal_features=[\"studentsuccess\",\"normalized_studentsuccess\"]\n\nprint(\"Minimum Value\\n-------------\")\nprint(df[normal_features].min(),\"\\n\")\nprint(\"Maximum Value\\n-------------\")\nprint(df[normal_features].max())\n","7c7c5f38":"sns.distplot(np.log(df[\"studentsuccess\"]));\n\n# Studentsuccess variable does not have normal distribution with logaritmic function.","faffa7d6":"# Hedef de\u011fi\u015fkeni a\u00e7\u0131klamada kullanaca\u011f\u0131m \u00f6zellikler kategorik de\u011fi\u015fken oldu\u011fu i\u00e7in s\u00fcrekli de\u011fi\u015fkene d\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi gerekmektedir.\n\ndf[\"absence_days\"]=pd.get_dummies(df[\"StudentAbsenceDays\"], drop_first=True)\ndf[\"mother_father\"]=pd.get_dummies(df[\"Relation\"], drop_first=True)\ndf[\"satisfaction\"]=pd.get_dummies(df[\"ParentschoolSatisfaction\"], drop_first=True)\ndf[\"male_female\"]=pd.get_dummies(df[\"gender\"], drop_first=True)\ndf[\"semester\"]=pd.get_dummies(df[\"Semester\"], drop_first=True)","b37059c9":"df.head()","44ce9d12":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","26cdfcc9":"df_new=df[[\"absence_days\",\"mother_father\",\"satisfaction\",\"male_female\",\"semester\",\"studentsuccess\"]]\n\nX=df_new.values","306ba38f":"X=StandardScaler().fit_transform(df_new)\n\npca=PCA(n_components=6)\nprincipalComponents=pca.fit_transform(X)","8c1f3ddb":"exp_var=pca.explained_variance_ratio_\ncumsum_var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n\nprint(exp_var,\"\\n\")\nprint(cumsum_var)","031396cb":"plt.plot(exp_var);","5ee08398":"plt.plot(cumsum_var);","858ff7b9":"# Data Visualization","0e030f5f":"# Principal Component Analysis","a05f6e56":"# Normality Test","c581c855":"# T-test","33463382":"# Relationships Between Numeric Variables","0ed67775":"# Normalization"}}