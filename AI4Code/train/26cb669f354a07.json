{"cell_type":{"cefe67c9":"code","30e43434":"code","71aeb72b":"code","55fca297":"code","d750b794":"code","aa1a4e30":"code","940fb093":"code","7439722c":"code","af3ff227":"code","6264ae78":"code","b6c2f441":"code","abe1de13":"code","88caaaf4":"code","b8e417c7":"code","e497bbaa":"code","920a5f62":"code","f56f502b":"code","999c4ac1":"markdown","d256990a":"markdown","fe48ff1f":"markdown","a216de67":"markdown","a30db31f":"markdown","3b8180ad":"markdown","be36e8f7":"markdown","f35020f0":"markdown","676782f8":"markdown","37212c1a":"markdown","92f729bf":"markdown","695a00d6":"markdown"},"source":{"cefe67c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial import distance\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","30e43434":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","71aeb72b":"train_data.head()","55fca297":"train_data.describe()","d750b794":"train_labels = train_data.Cover_Type.values\ntest_id = test_data.Id.values\n\ntrain_data.drop(['Soil_Type7', 'Soil_Type15', 'Id', 'Cover_Type'], axis=1, inplace=True)\ntest_data.drop(['Soil_Type7', 'Soil_Type15', 'Id'], axis=1, inplace=True)\n","aa1a4e30":"print(train_data.shape, test_data.shape)","940fb093":"%timeit pairwise_distances(train_data[:150], metric = 'euclidean')\n%timeit distance.cdist(train_data[:150], train_data[:150], 'euclidean')","7439722c":"min_max_scaler = MinMaxScaler() # If you did not use the scaler, you will get higher accuracy\ntrain_data = min_max_scaler.fit_transform(train_data)\ntest_data = min_max_scaler.fit_transform(test_data)\n\ndistance_matrix = pairwise_distances(train_data, metric = 'euclidean')\nprint(distance_matrix.shape)","af3ff227":"sorted_distance_index = np.argsort(distance_matrix, axis=1).astype(np.uint16)\nprint(sorted_distance_index)","6264ae78":"sorted_distance_labels = train_labels[sorted_distance_index].astype(np.uint8)\nprint(sorted_distance_labels)","b6c2f441":"max_k = 100\nk_matrix = np.empty((len(sorted_distance_labels), 0), dtype=np.uint8)\nfor k in range (1, max_k+1):\n    k_along_rows = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=sorted_distance_labels[:, 1:k+1]).reshape(len(sorted_distance_labels), -1)\n    k_matrix = np.hstack((k_matrix, k_along_rows))\nprint(k_matrix)","abe1de13":"k_truth_table = np.where(k_matrix == train_labels[:, None], 1, 0)\nprint(k_truth_table)\nprint(k_truth_table.shape)","88caaaf4":"accuracy_per_k = np.sum(k_truth_table, axis=0)\/len(k_truth_table)\nbest_accuracy = np.amax(accuracy_per_k)\nbest_k = np.argmax(accuracy_per_k) + 1 # real k = index + 1\nprint('Best K: {0}, Best Accuracy: {1:4.2f}%'.format(best_k, best_accuracy*100))\nplt.plot(range(1, max_k+1), accuracy_per_k)\nplt.title('Classification accuracy vs Choice of K')\nplt.xlabel('K')\nplt.ylabel('Classification Accuracy')\nplt.show()","b8e417c7":"print(\"RAM needed for the distance matrix = {:.2f} GB\".format(len(train_data)*len(test_data) * 64 \/ (8 * 1024 * 1024 * 1024)))","e497bbaa":"# Those variables are no longer needed, Free up some RAM instead\ndel k_truth_table\ndel k_matrix\ndel sorted_distance_labels\ndel sorted_distance_index\ndel distance_matrix","920a5f62":"# ALERT: This code takes some time, it took 8 minutes on a powerful PC but with relatively low RAM usage (around 6.8G)\ndef classify(unknown, dataset, labels, k):\n    classify_distance_matrix = pairwise_distances(unknown, dataset, metric='euclidean')\n    nearest_images = np.argsort(classify_distance_matrix)[:, :k]\n    nearest_images_labels = labels[nearest_images]\n    classification = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=nearest_images_labels[:, :k])\n    return classification.astype(np.uint8).reshape(-1, 1)\n\npredict = np.empty((0, 1), dtype=np.uint8)\nchunks = 15\nlast_chunk_index = 0\nfor i in range(1, chunks+1):\n    new_chunk_index = int(i * len(test_data) \/ chunks)\n    predict = np.concatenate((predict, classify(test_data[last_chunk_index : new_chunk_index], train_data, train_labels, best_k)))\n    last_chunk_index = new_chunk_index\n    print(\"Progress = {:.2f}%\".format(i * 100 \/ chunks))","f56f502b":"submission = pd.DataFrame({\"Id\": test_id, \"Cover_Type\": predict.ravel()})\nsubmission.to_csv('submission.csv', index=False)","999c4ac1":"We will use K=1 which should yield an accuracy around 86.8%, however the size of the test_data is too big we need to claculate the size of the distance matrix first to avoid running out of RAM. Note that the array would be cast as float64 before we can convert it.","d256990a":"Note that the nearest neighbour (first column) is actually the point itself so we will have to consider filtering the first column from our calculations.\nWe have to know the labels of these indexes, luckily, we can use numpy direct indexing for this one.","fe48ff1f":"Seems good! Now we need to create a distance matrix but first let's see how much RAM is needed for this given that the array needs to be casted as float64 at the beginning..","a216de67":"Perfect! Now we have the prediction for every point over all the Ks from 1 to 100. Let's find the best one. If the prediction was correct we replace the value by 1, otherwise 0.","a30db31f":"No way we would be able to to store it in 1 numpy array... so we will loop over chunks of the test_data, classify them and reuse the same variable names..","3b8180ad":"pairwise_distances is faster.","be36e8f7":"Let's try to find the best K for this case. We will use the leave-one-out approach, so we will try to classify every point in the training data using all other points over a range of K values and determine the best one.\nWe need to create a distance matrix for the train_data.","f35020f0":"Let's see what's really inside","676782f8":"We need to decide the maximum k we would like to try for this case. I think 100 is more than enough.\nWe will build an array where the rows are the data points (indexes) and the columns are the Ks, we would then see the classification of every point in the training set over all the Ks using np.bincount and np.argmax to find the most common element in the nearest neighbors.","37212c1a":"Using the argsort, we can find the indexes of the sorted distance matrix (indexes of nearest neighbours)","92f729bf":"It appears there are 2 useless columns: Soil_Type7 & Soil_Type15 (std = 0) so we can drop them from both train and test data.","695a00d6":"We will run a quick benchmark to see which function should we use."}}