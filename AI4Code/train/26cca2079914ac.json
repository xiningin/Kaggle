{"cell_type":{"961ec16a":"code","e294aad8":"code","c337543d":"code","e8980932":"code","f5f73f75":"code","97abf502":"code","7b2c2821":"code","562931f6":"code","0829cef5":"code","d2724a5a":"code","261e702c":"code","82742e13":"code","b2159fd4":"code","d78e0e8b":"code","0995c77d":"code","8b8f4d09":"code","6d92b955":"code","fe7effb5":"code","a2a422d5":"code","e5cd4421":"code","6cba508f":"code","cadf219c":"code","43542f33":"code","cee32f0a":"code","7b99b7d3":"code","ef62f407":"code","93b33075":"code","43fc9fc9":"code","b943d7cf":"code","c568d041":"code","77a0e07a":"code","1599574f":"code","9ee13d73":"code","73b8d379":"code","3f2d13b2":"code","d312f327":"code","6263a34f":"code","c3f071bc":"code","9d2b0cb0":"code","0478bb66":"code","9bb8fdcc":"code","e5fa3eb3":"code","0ccd8bb8":"code","4e40b0bf":"code","36d83aa6":"code","b1ec3b8c":"code","628567f8":"code","8beb7758":"code","a2ae0177":"code","6fd8ab8d":"code","052ec017":"code","2bc8752e":"code","2d924c1d":"code","f25b38bc":"markdown","6d1117d6":"markdown","132d1385":"markdown","52321bb3":"markdown","51b8d217":"markdown","71ed01a6":"markdown","294135bd":"markdown","3df19039":"markdown","fb2a89b1":"markdown","60579136":"markdown","02195bb4":"markdown","8de204b7":"markdown","d7a9f549":"markdown","dd11149a":"markdown","142c4ba3":"markdown","ecde60e3":"markdown","307447dd":"markdown","9b6befb6":"markdown","f21a185f":"markdown","bc72b541":"markdown","2651b7d5":"markdown","38631b20":"markdown","d9ab3188":"markdown","309740d7":"markdown","aee9834f":"markdown","04795f82":"markdown","c626da20":"markdown","d91c603d":"markdown","86c3bf40":"markdown","769c982c":"markdown","e3a3349f":"markdown","4b79724f":"markdown","3d388fcb":"markdown"},"source":{"961ec16a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for data visualization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e294aad8":"# loading data\ndf = pd.read_csv(os.path.join(dirname, filename))\ndf.shape","c337543d":"df.head()","e8980932":"df.info()","f5f73f75":"#checking the categories for object data types\nprint(df['international plan'].unique())\nprint(df['voice mail plan'].unique())","97abf502":"df.describe(include='all')","7b2c2821":"\n#check for missing values\ndf.isnull().sum()","562931f6":"\n# check for duplicated rows\ndf.duplicated().sum()","0829cef5":"# check for outliers\ndf.skew()","d2724a5a":"\ndf['number vmail messages'].describe()","261e702c":"df['number vmail messages'][df['number vmail messages']>0].describe()","82742e13":"# New categorical feature = \n                # if numofvmailmessage <1 = No VM plan\n                # if numofvmailmessage >1 and <38 = Normal users\n                # if numofvmailmessage >38 and <53= High Frequency users","b2159fd4":"df['vmail_messages'] = pd.cut(df['number vmail messages'],bins=[0,1,38,52],\n                             labels=['No VM plan','Normal Users','High Frequency users'],\n                             include_lowest=True)\ndf.head(20)","d78e0e8b":"cor = df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(cor.round(3),annot=True,cmap='coolwarm')\nplt.show()","0995c77d":"df.columns","8b8f4d09":"numerics =['account length','number vmail messages',\n       'total day minutes', 'total day calls', 'total day charge',\n       'total eve minutes', 'total eve calls', 'total eve charge',\n       'total night minutes', 'total night calls', 'total night charge',\n       'total intl minutes', 'total intl calls', 'total intl charge',\n       'customer service calls']\nxnum = df[numerics]\ny = df['churn']\nfrom sklearn.feature_selection import f_classif\nfval,pval = f_classif(xnum,y)\nfor i in range(len(numerics)):print(numerics[i],pval[i])","6d92b955":"\ncategories = ['state','area code','phone number', 'international plan',\n       'voice mail plan','vmail_messages']\n\ny = df['churn']\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import chi2\nfor col in categories:\n    xcat = LabelEncoder().fit_transform(df[col]).reshape(-1,1)\n    cval,pval = chi2(xcat,y)\n    print(col,pval)","fe7effb5":"#selecting important features based on previous analysis\nx = df[['international plan','vmail_messages','total day minutes','total eve minutes',\n     'total night minutes','total intl minutes','customer service calls']]\ny = df['churn']","a2a422d5":"x.head()","e5cd4421":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler\npreprocessor = ColumnTransformer([('ohe',OneHotEncoder(),[1]),\n                                ('ode',OrdinalEncoder(),[0]),\n                                 ('sc',StandardScaler(),[2,3,4,5,6])],remainder='passthrough')","6cba508f":"x_new = preprocessor.fit_transform(x)\npd.DataFrame(x_new).head()","cadf219c":"# train test split\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(x_new,y,test_size=0.2,random_state=5)\nprint(x.shape)\nprint(xtrain.shape)\nprint(xtest.shape)\nprint(y.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","43542f33":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(class_weight='balanced')\nmodel.fit(xtrain,ytrain)\n","cee32f0a":"# performance analysis\nfrom sklearn import metrics\nypred = model.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred))","7b99b7d3":"# performance analysis on train data\nypred2 = model.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","ef62f407":"# preprocessing pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler\npreprocessor = ColumnTransformer([('ohe',OneHotEncoder(),[1]),\n                                ('ode',OrdinalEncoder(),[0])],\n                                remainder=\"passthrough\")\nx_new = preprocessor.fit_transform(x)\npd.DataFrame(x_new)","93b33075":"# train test split\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(x_new,y,test_size=0.2,random_state=5)\nprint(x.shape)\nprint(xtrain.shape)\nprint(xtest.shape)\nprint(y.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","43fc9fc9":"# decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel2 = DecisionTreeClassifier(random_state=5,class_weight={0:0.5,1:0.5})\nmodel2.fit(xtrain,ytrain)","b943d7cf":"import graphviz\nfrom sklearn import tree\n\nfname = ['International plan', 'vmail_NO_Plan','vmail_Normal','vmail_HF', 'Total day minutes',\n       'Total eve minutes', 'Total night minutes', 'Total intl minutes',\n       'Customer service calls']\ncname = ['Not Leaving','Leaving']\ngraphdata = tree.export_graphviz(model2,feature_names=fname,class_names=cname,\n                                filled=True,rounded=True)\ngraph = graphviz.Source(graphdata)\ngraph\n","c568d041":"# performance analysis\nypred2 = model2.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","77a0e07a":"\n# performance analysis on train data\nypred2 = model2.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","1599574f":"param_grid = {\"max_depth\":np.arange(3,25,2),\n              \"min_samples_leaf\":np.arange(3,50,2),\n              \"min_samples_split\":np.arange(10,120,5)}\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=5),\n                          param_grid=param_grid,n_jobs=-1,\n                          scoring='recall',verbose=True,cv=5)\ngrid_search.fit(x_new,y)","9ee13d73":"print(grid_search.best_score_)\nprint(grid_search.best_params_)","73b8d379":"# Controlling overfitting\nmodel2 = DecisionTreeClassifier(criterion='gini',random_state=5,\n                               max_depth=8,min_samples_leaf=5,min_samples_split=20)\nmodel2.fit(xtrain,ytrain)\n","3f2d13b2":"# performance analysis On test data\nypred2 = model2.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","d312f327":"# performance analysis on train data\nypred2 = model2.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","6263a34f":"model2.feature_importances_\nfor i in range(len(fname)):print(fname[i],model2.feature_importances_[i])","c3f071bc":"from sklearn.ensemble import RandomForestClassifier\nmodel4 = RandomForestClassifier(n_estimators=100,random_state=5,\n                               max_depth=8,oob_score=True)\n#train the model\nmodel4.fit(xtrain,ytrain)","9d2b0cb0":"# performance analysis On test data\nypred2 = model4.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","0478bb66":"# performance analysis on train data\nypred2 = model4.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","9bb8fdcc":"#check OOB (out of bag) score\nmodel4.oob_score_","e5fa3eb3":"from sklearn.ensemble import AdaBoostClassifier\nmodel5 = AdaBoostClassifier(n_estimators=120,random_state=5,learning_rate=0.2)\nmodel5.fit(xtrain,ytrain)\n","0ccd8bb8":"# performance analysis On test data\nypred2 = model5.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","4e40b0bf":"\n# performance analysis on train data\nypred2 = model5.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","36d83aa6":"\n# Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel6 = GradientBoostingClassifier(learning_rate=0.1,n_estimators=150,random_state=5)\nmodel6.fit(xtrain,ytrain)\n","b1ec3b8c":"\n# performance analysis On test data\nypred2 = model6.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","628567f8":"# performance analysis on train data\nypred2 = model6.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))\n","8beb7758":"from xgboost import XGBClassifier\nmodel7 = XGBClassifier(learning_rate=0.005,n_estimators=120,max_depth=8)\nmodel7.fit(xtrain,ytrain)","a2ae0177":"# performance analysis On test data\nypred2 = model7.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","6fd8ab8d":"# performance analysis on train data\nypred2 = model7.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","052ec017":"from mlxtend.classifier import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n\nlog_model = LogisticRegression()\ndt_model = DecisionTreeClassifier(random_state=5,max_depth=10)\nrf_model = RandomForestClassifier(n_estimators=100,random_state=5,max_depth=10)\ngb_model = GradientBoostingClassifier(learning_rate=0.01,n_estimators=120,random_state=5)\n\n\nmodel8 = StackingClassifier(classifiers=[dt_model,rf_model,gb_model],\n                           meta_classifier=log_model)\nmodel8.fit(xtrain,ytrain)","2bc8752e":"# performance analysis On test data\nypred2 = model8.predict(xtest)\nprint(\"Accuracy : \",metrics.accuracy_score(ytest,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytest,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytest,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytest,ypred2))","2d924c1d":"# performance analysis on train data\nypred2 = model8.predict(xtrain)\nprint(\"Accuracy : \",metrics.accuracy_score(ytrain,ypred2))\nprint(\"Recall : \",metrics.recall_score(ytrain,ypred2))\nprint(\"F1 score : \",metrics.f1_score(ytrain,ypred2))\nprint(\"Precision : \",metrics.precision_score(ytrain,ypred2))","f25b38bc":"### Correlation Analysis","6d1117d6":"#### No dulicated rows observed","132d1385":"# 6. Apply Machine Learning algorithm - Logistic regression\u00b6","52321bb3":"\n## Random Forest","51b8d217":"## Feature Selection","71ed01a6":"## Overfitting\n\n`performance of model on test data = low`\n\n`performance of model on train data = high`\n\n**Reasons for overfitting**\n- Noisy features or noisy data\n- lack of data - lack of number of observations\/rows\n- features are having very complex\/nonlinear relation with label\n- the algorithm used is very complex\/nonlinear\n\n\n**Ways to handle overfitting situation**\n- drop noisy\/irrelevant features\n- keep the features simple, may be convert the numeric to categorical\n- collect more data - more rows, NO BENEFIT from collecting more features\n- Try a less complex algorithm\n- In case of decision tree, decrease the value of max_depth, increase min_samples_leaf and min_samples_split\n","294135bd":"#### encoding categorical features","3df19039":"## Decision Tree Classifier","fb2a89b1":"1. At the data level\n    * Extract more features\n    * collect more data - features \/ samples\n    * perform better preprocessing\n    * improve feature selection process - drop irrelevant features\n2. At the modelling level\n    * Tune the hyperparameters of the algorithm to improve its performance\n    * Change the ML algorithm used for modelling\n    * combine multiple algorithms to make predictions","60579136":"Oservation - \n* Voice mail plan and numer vmail messages seems to have similar information, as those who will be having voice mail plan will be having the value of number vmail messages higher than 0\n* total day charges and total day minutes should be correlated as there would be a multiplier of per minute charge used to calculate total day charge from total day minutes, same also applied for total eve minutes, total night minutes and total internation mins\n* Phone number, state and area code seems to be identifier so may not have any quantitative or qualittaive info","02195bb4":"# Standard Lifecycle for any data science project to be followed\n\n1. Domain Exploration\n    * Understand the business process, how business functions\n    * Identify few problems and the beleif based solutions\n    * Identify opportunities where business relies on expertise of an SME.\n2. Data Collection and Data Exploration\n    * understand the data structure\n    * Ask for data dictionary to the business\n    * explore data to identify issues with data quality and patterns in general with in data\n3. Data cleaning\n    * handle unwanted columns\n    * handle missing values\n    * handle duplicate entries\n    * handle outliers and un natural values\n4. Feature Engineering\n    * Feature Extraction\n        * use data engieering and statistics to create new useful features from existing data\n    * Feature Selection - select best features which are relevant to predict the label\n        * EDA ( Exploratory Data Analytics ) using Data Visualization\n        * EDA using statistics\n        * Wrapper Methods\n        * Embedded Methods\n        * Filter methods\n5. Preprocessing of data\n    * encoding of data\n    * scaling of features\n    * splitting data into train and test sets\n6. Apply ML to build a predictive model\n    * Use a ML algorithm and train using train set\n7. Performance Analysis - How good the model is?\n    * check performance of model using train data and test data\n8. Optimization & Tuning\n    * improve the performance of the ML algorithm\n9. Deploy the model to production\n    * Export the model as a portable file\n    * deploy it as REST service\n10. Monitoring the perfomance of a model in production","8de204b7":"## Underfitting\n`performance of model on test data = low`\n\n`performance of model on train data = low`\n\n\n**Reasons for underfitting**\n- lack of informative features\n- lack of a powerful algorithm, as the existing features may have silghtly complex\/nonlinear relation with the target and the current algorithm is not able to learn\n- presence of noisy observations\n\n\n**Ways to handle underfitting situation**\n- colllect\/ create more features, perform feature extraction\n- collect more columns, NO BENFIT from collecting rows\n- Try a more powerful\/complex predictive algorithm\n- In case of deicision tree, increase the value of max_depth, decrease the value of min_samples_leaf and min_samples_split\n- perform better data cleaning, handling outliers etc.","d7a9f549":"#### No Missing values observed","dd11149a":"## Hyperparameter Tuning for decision tree using Gridsearch","142c4ba3":"# 3. Data Cleaning","ecde60e3":"### Visualizing the tree model","307447dd":"### Observation - Recall is not satisfactory\nSo we will now try other algorithms","9b6befb6":"* Number vmail messages - we can take action while performing feature extraction\n* Total intl class - we will take action during correlation analysis\n* Customer service calls - the skew is almost 1, thus we can go ahead without an action","f21a185f":"# 2. Data Exploration","bc72b541":"## Best fitting\n\n`performance of model on test data = high`\n\n`performance of model on train data = high`","2651b7d5":"## Gradient Boosting Trees","38631b20":"## Adaboost","d9ab3188":"### Feature selection using Chi Square Test\n\n- Used to compare the distribution of categories of a categorical feature in two or more groups\n- in nutshell to compare whether a categorical attribute has some relationship with the other categorical attribute\n\n* H0 = Null Hypothesis = the categorical attribute has uniform distribution in two or more groups\n* Ha = Alternate hypothesis = the categorical attribute has different distribution in two or more groups\n\nWe always analyse the pvalue, consider 95% as confidence interval, significance level = 5% i.e.0.05\n\n`if pvalue >0.05 = accept the Null hypothesis - feature is not important`\n`if pvalue <0.05 = reject the Null hypothesis - feature is important`","309740d7":"# 8. Optimization and Tuning\n","aee9834f":"## XGBosst","04795f82":"# 7. Performance Analysis","c626da20":"Observation\n* Total day minutes & total day charge - correlation 1 >> we can drop one of these\n* Total eve minutes & total eve charge - correlation 1 >> we can drop one of these\n* Total night minutes & total night charge - correlation 1 >> we can drop one of these\n* Total intl minutes & total intl charge - correlation 1 >> we can drop one of these","d91c603d":"## Stacking\nUsing Decisoin Tree, Random Forest, Gradient Boosting as base learners, logistic regression as meta learner","86c3bf40":"\n#### ANOVA is used for comparing the distribution of a numeric variable in two or more groups\n* Ho = Null Hypothesis = the distribution of the varible in multiple groups is uniform\n* Ha = Alternate Hypothesis = the distribution of the variable in multiple groups in different\n    \n    we analyse the pvalue, lets say for confidence interval of 95%, significance level = 5%\n\n`if pvalue>0.05 = accept the null hypothesis and the feature is NOT important`\n`if pvalue <0.05 = reject the null hypothesis and the feature is important`","769c982c":"### Feature selection using ANOVA","e3a3349f":"### Feature importances","4b79724f":"# 4. Feature Engineering\n## Feature Extraction","3d388fcb":"# 5. Preprocessing"}}