{"cell_type":{"67abec31":"code","f0651793":"code","ef484749":"code","5bf25a4f":"code","8865e335":"code","ae30c230":"code","dc1e0ef9":"code","432a84e4":"code","1096df97":"code","1ad44fa7":"code","9418ee3b":"code","290eaa87":"code","f55d3189":"code","19ecdb71":"code","01a14795":"code","001810f7":"code","caed2770":"code","b4e55422":"code","ad0fb3ac":"code","ba405780":"code","05073cef":"code","e022d0e5":"code","94df9037":"code","aff6442c":"markdown","d9b04a19":"markdown","643b8b93":"markdown","1d584e2b":"markdown","5f097934":"markdown","0bd24a4d":"markdown","71d06f65":"markdown","4c982813":"markdown","c4a2668a":"markdown","4ee083cb":"markdown","5697fb6f":"markdown","35495a81":"markdown","2d3d7bf9":"markdown","c0063d23":"markdown","021b0f29":"markdown","9be7d0b4":"markdown"},"source":{"67abec31":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","f0651793":"input_dir = \"\/kaggle\/input\/nlp-getting-started\"\ntrain = pd.read_csv(os.path.join(input_dir, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_dir, 'test.csv'))\ndf=pd.concat([train,test])","ef484749":"key_metrics= {'samples' : len(train),\n             'samples_per_class' : train['target'].value_counts().median(),\n             'median_of_samples_lengths': np.median(train['text'].str.split().map(lambda x: len(x))),\n             }\nkey_metrics = pd.DataFrame.from_dict(key_metrics, orient='index').reset_index()\nkey_metrics.columns = ['metric', 'value']\nkey_metrics","5bf25a4f":"green = '#52BE80'\nred = '#EC7063'\nsns.countplot(train['target'], palette=[green, red])\n# PS : here: https:\/\/htmlcolorcodes.com\/fr\/tableau-de-couleur\/ => to a cheerful color table where you can get an infinity of color codes  ","8865e335":"blue = \"#5DADE2\"\ndef get_sentences_len_histogram(fig_size = (10,6), _class=None, color=blue):\n    \"\"\"make histogram of lengths of the tweets\n    * _class : consider all the dataset\n    - _class=0 only non disaster tweets\n    - _class=1 only disaster tweets\"\"\"\n    f, ax = plt.subplots(figsize=fig_size)\n    if str(_class)=='None':\n        tweets_len=train['text'].str.len()\n        ax.set_title('tweets length')\n        ax.hist(tweets_len,color=color)\n    else: \n        tweets_len=train[train['target']==_class]['text'].str.len()\n        ax.set_title(f'{_class} disaster tweets length ')\n        ax.hist(tweets_len,color=color)\n\n    return ax\n\ndef get_words_count_histogram(fig_size = (10,6), _class=None, color=blue):\n    \"\"\"make histogram of lengths of the tweets\n    * _class : consider all the dataset\n    - _class=0 only non disaster tweets\n    - _class=1 only disaster tweets\"\"\"\n    f, ax = plt.subplots(figsize=fig_size)\n    if str(_class)=='None':\n        tweets_len=train['text'].str.split().map(lambda x: len(x))\n        ax.set_title('words counts by tweet')\n        ax.hist(tweets_len,color=color)\n    else: \n        tweets_len=train[train['target']==_class]['text'].str.split().map(lambda x: len(x))\n        ax.set_title(f'words counts by  \"{_class}\" disaster tweets ')\n        ax.hist(tweets_len,color=color)\n\n    return ax","ae30c230":"get_sentences_len_histogram()","dc1e0ef9":"# two subplots\nax1 = get_sentences_len_histogram(_class=0, color=green)\nax1 = get_sentences_len_histogram(_class=1, color=red)","432a84e4":"get_words_count_histogram()","1096df97":"# two subplots\nax1 = get_words_count_histogram(_class=0, color=green)\nax1 = get_words_count_histogram(_class=1, color=red)","1ad44fa7":"from string import digits \n\ndef tweets_cleaning(x, remove_emojis=True, remove_stop_words=True):\n    \"\"\"Apply function to a \"\"\"\n    x = x.lower().strip()\n    # romove urls\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    x = url.sub(r'',x)\n    # remove html tags\n    html = re.compile(r'<.*?>')\n    x = html.sub(r'',x)\n    # remove punctuation\n    operator = str.maketrans('','',string.punctuation) #????\n    x = x.translate(operator)\n    # remove digits\n    remove_digits = str.maketrans('', '', digits) \n    x = ' '.join([i.translate(remove_digits) for i in x.split()])\n    \n    if remove_emojis:\n        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n    if remove_stop_words:\n        x = ' '.join([word for word in x.split(' ') if word not in stop])\n    return x","9418ee3b":"df['cleaned_tweets'] = df['text'].apply(tweets_cleaning)","290eaa87":"train = df[~df['target'].isna()]\ntrain['target'] = train['target'].astype(int)\ntest = df[df['target'].isna()]","f55d3189":"# lets keep a part a validation subset that will be used by our model\nX_train, X_val, y_train, y_val = train_test_split(train, train['target'], test_size=0.2, random_state=42)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_val.shape)","19ecdb71":"from tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\n\n# Vectorization parameters\n# Limit on the number of features. We use the top 20K features.\nTOP_K = 20000\n\n# Limit on the length of text sequences. \n# Sequences longer than this will be truncated.\n# and less than it will be padded\nMAX_SEQUENCE_LENGTH = 50\n\nclass CustomTokenizer:\n    def __init__(self, train_texts):\n        self.train_texts = train_texts\n        self.tokenizer = Tokenizer(num_words=TOP_K)\n        \n#    @property\n#    def max_len(self):\n        # Get max sequence length.\n#        max_length = len(max(self.train_texts , key=len))\n#        return min(max_length, MAX_SEQUENCE_LENGTH)\n        \n    def train_tokenize(self):\n        # Get max sequence length.\n        max_length = len(max(self.train_texts , key=len))\n        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n    \n        # Create vocabulary with training texts.\n        self.tokenizer.fit_on_texts(self.train_texts)\n        \n    def vectorize_input(self, tweets):\n        # Vectorize training and validation texts.\n        \n        tweets = self.tokenizer.texts_to_sequences(tweets)\n        # Fix sequence length to max value. Sequences shorter than the length are\n        # padded in the beginning and sequences longer are truncated\n        # at the beginning.\n        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post',padding='post')\n        return tweets\n    \ntokenizer = CustomTokenizer(train_texts = X_train['cleaned_tweets'])\n# fit o the train\ntokenizer.train_tokenize()\ntokenized_train = tokenizer.vectorize_input(X_train['cleaned_tweets'])\ntokenized_val = tokenizer.vectorize_input(X_val['cleaned_tweets'])\ntokenized_test = tokenizer.vectorize_input(test['cleaned_tweets'])","01a14795":"import tqdm\nimport requests\nimport zipfile\nURL = \"http:\/\/nlp.stanford.edu\/data\/glove.42B.300d.zip\"\n\n\n\ndef fetch_data(url=URL, target_file='glove.zip', delete_zip=False):\n    #if the dataset already exists exit\n    if os.path.isfile(target_file):\n        print(\"datasets already downloded :) \")\n        return\n\n    #download (large) zip file\n    #for large https request on stream mode to avoid out of memory issues\n    #see : http:\/\/masnun.com\/2016\/09\/18\/python-using-the-requests-module-to-download-large-files-efficiently.html\n    print(\"**************************\")\n    print(\"  Downloading zip file\")\n    print(\"  >_<  Please wait >_< \")\n    print(\"**************************\")\n    response = requests.get(url, stream=True)\n    #read chunk by chunk\n    handle = open(target_file, \"wb\")\n    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n        if chunk:  \n            handle.write(chunk)\n    handle.close()  \n    print(\"  Download completed ;) :\") \n    #extract zip_file\n    zf = zipfile.ZipFile(target_file)\n    print(\"1. Extracting {} file\".format(target_file))\n    zf.extractall()\n    if delete_zip:\n        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n        os.remove(path=zip_file)\n        \nfetch_data()","001810f7":"glove_file = \"glove.42B.300d.txt\"\n\n\nEMBEDDING_VECTOR_LENGTH = 50 # <=200\ndef construct_embedding_matrix(glove_file, word_index):\n    embedding_dict = {}\n    with open(glove_file,'r') as f:\n        for line in f:\n            values=line.split()\n            # get the word\n            word=values[0]\n            if word in word_index.keys():\n                # get the vector\n                vector = np.asarray(values[1:], 'float32')\n                embedding_dict[word] = vector\n    ###  oov words (out of vacabulary words) will be mapped to 0 vectors\n\n    num_words=len(word_index)+1\n    #initialize it to 0\n    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n\n    for word,i in tqdm.tqdm(word_index.items()):\n        if i < num_words:\n            vect=embedding_dict.get(word, [])\n            if len(vect)>0:\n                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n    return embedding_matrix\n\nembedding_matrix =  construct_embedding_matrix(glove_file, tokenizer.tokenizer.word_index)\nprint(embedding_matrix.shape)","caed2770":"#https:\/\/datascience.stackexchange.com\/questions\/45165\/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","b4e55422":"model=Sequential()\noptimzer=Adam(clipvalue=0.5)\n\nembedding=Embedding(len(tokenizer.tokenizer.word_index)+1, EMBEDDING_VECTOR_LENGTH, embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n\nmodel.add(embedding)\nmodel.add(Dropout(0.2))\n#model.add(Dense(30, activation='relu'))#, kernel_constraint=maxnorm(3)))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer=optimzer, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])","ad0fb3ac":"history=model.fit(tokenized_train,y_train, \n                  batch_size=32, epochs=20, \n                  validation_data=(tokenized_val,y_val), \n                  verbose=2)","ba405780":"\nloss, accuracy, f1_score, precision, recall = model.evaluate(tokenized_val, y_val, verbose=0)","05073cef":"print(f'F1 score : {\"%.3f\"%f1_score}')\nprint(f'precision : {\"%.3f\"%precision}')\nprint(f'Recall : {\"%.3f\"%recall}')","e022d0e5":"# generate scores\ntest['scores'] = model.predict(tokenized_test)\n# generate deisions\ntest['predction'] = np.round(test['scores']).astype(int)\ntest = test.drop('target', axis=1)","94df9037":"test.head()","aff6442c":"In addition to the accuracy, we will add the precision, recall and F1-score as backend metrics to binary Keras Classifier model: we have to calculate them manually, because these metrics are not supported by keras since 2.0 version.","d9b04a19":"### fit the model","643b8b93":"## Text cleaning:\n\nwe can apply some basic data cleaning that are recurrent in tweeters such as removing punctuation, html tags urls and emojis, spelling correction,..","1d584e2b":"### Make new predictions:","5f097934":"\n### Model evaluation","0bd24a4d":"Some plots are always more comprehensive than numbers, so lets visulize and discover some useful insights\n### Target class distribution with a bar plot","71d06f65":"Then we will create an embedding matrix we will map each word index to its corresponding embedding vector:","4c982813":"## Construct an embedding Matrix\u00a0\ud83e\uddf1\nFirst of we will download Glove pre-trained embedding from the official site:","c4a2668a":"Lets create the model","4ee083cb":"The Distribution of samples length is mainly concentrated between 120 to 140 characters","5697fb6f":"### Collect some key metrics:\n\nLets compute the following metrics for the training dataset, to get a global view of the dataset\n\n* Number of samples: Total number of examples \n* Number of samples per class: Number of samples per class (rean disaster \/non real disaster).\n* Number of words per sample: Median or mean number of words in one sample.\n* Frequency distribution of words: Distribution showing the frequency (number of occurrences) of each word in the dataset.\n* Distribution of sample length: Distribution showing the number of words per sample in the dataset.\n\n((( source https:\/\/developers.google.com\/machine-learning\/guides\/text-classification\/step-2)))","35495a81":"### Histogram of number of words for each tweet: \n#### a. in all the dataset\n","2d3d7bf9":"### Histogram of text sentences lengths: \n#### a. in all the dataset","c0063d23":"### train validation test split:","021b0f29":"Inspiration from [this notebook](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove) ","9be7d0b4":"## Text pre-processing\n\n1. Tokenization\u00a0\nIt consists in dividing the texts into words or smaller sub-texts, allowing us to determine the \"vocabulary\" of the dataset (set of unique tokens present in the data). Usually we use word-level representation. For our exemple we will use NLTK Tokenizer()\n2. Word indexing:\nConstruct a vocablary_index mapper based on word frequency: the index would be inversely proportional to the word occurrence frequency in the overall dataset. the most frequent world would have index=1.. And every single word would get a unique\n"}}