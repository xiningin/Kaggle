{"cell_type":{"dd3a506e":"code","0bdd4b45":"code","07e614a2":"code","a77b858c":"code","6fb98b62":"code","d5af8a68":"code","86c76e71":"code","3a1b62d8":"code","702b58e9":"code","45294808":"code","490fc6e3":"code","107513aa":"code","790ddcc6":"code","f55739aa":"code","1889bdc3":"code","098fb1d6":"code","bdd1d6ec":"code","e4c6ee2f":"code","14ff8637":"code","9caaa6b0":"code","8e5455f4":"code","7798e7d0":"code","24e82f07":"code","0c577672":"code","2813048a":"code","69a2f76b":"code","ecf71894":"code","a8ac44e1":"code","27ebe17c":"code","d9edcefa":"code","f2fed901":"code","0b43cd83":"code","75ecc388":"code","c8150836":"code","8ffa2ec6":"code","a2441668":"code","1a59e40b":"code","71a99239":"markdown","ae1ea3cb":"markdown","54bc261d":"markdown","53e7b89b":"markdown","094381c5":"markdown","3367c1be":"markdown","9cadc83e":"markdown","d289b80d":"markdown","fc83feab":"markdown","0167e790":"markdown","9f568de9":"markdown","c53fddc1":"markdown","6c275089":"markdown","10d8caca":"markdown","f1a19ab6":"markdown","41870b55":"markdown","dd8d8e54":"markdown","2100f7d6":"markdown","dfd644d2":"markdown","1b787422":"markdown","0ea0ea0c":"markdown","8f2405c1":"markdown","da10fd99":"markdown","e4797d3b":"markdown","4cd143fb":"markdown","56b7ad5d":"markdown","eb258686":"markdown","39fc2a49":"markdown"},"source":{"dd3a506e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0bdd4b45":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","07e614a2":"df = pd.read_csv('\/kaggle\/input\/prostate-cancer\/Prostate_Cancer.csv')\nprint('Dataset :',df.shape)\nx = df.iloc[:, [0, 1, 2, 3]].values\ndf.info()\ndf[0:10]","a77b858c":"df['diagnosis_result'] = df['diagnosis_result'].replace(['B'],'0')\ndf['diagnosis_result'] = df['diagnosis_result'].replace(['M'],'1')","6fb98b62":"df[['diagnosis_result']] = df[['diagnosis_result']].apply(pd.to_numeric, errors ='ignore')\ndf.info()","d5af8a68":"df['diagnosis_result'].value_counts()","86c76e71":"df.diagnosis_result.value_counts()[0:30].plot(kind='bar')\nplt.show()","3a1b62d8":"df = df[['radius','texture','perimeter','area', 'smoothness','diagnosis_result']] #Subsetting the data\ncor = df.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","702b58e9":"sns.set_style(\"ticks\")\nsns.pairplot(df,hue=\"diagnosis_result\",size=3);\nplt.show()","45294808":"from sklearn.model_selection import train_test_split\nY = df['diagnosis_result']\nX = df.drop(columns=['diagnosis_result'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","490fc6e3":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","107513aa":"from sklearn.linear_model import LogisticRegression\n\n# We defining the model\nlogreg = LogisticRegression(C=10)\n\n# We train the model\nlogreg.fit(X_train, Y_train)\n\n# We predict target values\nY_predict1 = logreg.predict(X_test)","790ddcc6":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","f55739aa":"# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(score_logreg)","1889bdc3":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict1)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","098fb1d6":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True),n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","bdd1d6ec":"# The confusion matrix\nsvmcla_cm = confusion_matrix(Y_test, Y_predict2)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svmcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","e4c6ee2f":"# Test score\nscore_svmcla = svmcla.score(X_test, Y_test)\nprint(score_svmcla)","14ff8637":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict2)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","9caaa6b0":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","8e5455f4":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","7798e7d0":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","24e82f07":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict3)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","0c577672":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","2813048a":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","69a2f76b":"# Test score\nscore_dtcla = dtcla.score(X_test, Y_test)\nprint(score_dtcla)","ecf71894":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict4)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","a8ac44e1":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","27ebe17c":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","d9edcefa":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","f2fed901":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict5)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","0b43cd83":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","75ecc388":"# The confusion matrix\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","c8150836":"# Test score\nscore_knncla= knncla.score(X_test, Y_test)\nprint(score_knncla)","8ffa2ec6":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict6)\n\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","a2441668":"Testscores = pd.Series([score_logreg, score_svmcla, score_nbcla, score_dtcla, score_rfcla, score_knncla],index=['Logistic Regression Score', 'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', 'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores)","1a59e40b":"from sklearn.metrics import roc_curve\n\n# Logistic Regression Classification\nY_predict1_proba = logreg.predict_proba(X_test)\nY_predict1_proba = Y_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY_predict2_proba = svmcla.predict_proba(X_test)\nY_predict2_proba = Y_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY_predict4_proba = dtcla.predict_proba(X_test)\nY_predict4_proba = Y_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY_predict6_proba = knncla.predict_proba(X_test)\nY_predict6_proba = Y_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","71a99239":"**ROC curve**","ae1ea3cb":"K-Nearest neighbors is a technique that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). This technique is non-parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. All the training data used in the test phase. This makes the training faster and the test phase slower and more costlier. In this technique, the number of neighbors k is usually an odd number if the number of classes is 2. For finding closest similar points, find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance.","54bc261d":"**Distribution of diagnosis 68 benign  38 malignant with Cancer**","53e7b89b":"![CANCER](https:\/\/www.mountelizabeth.com.sg\/images\/default-source\/default-album\/surgery-prostate-cancer.jpg?sfvrsn=e70a871e_4)","094381c5":"**What is Prostate cancer?**\n\nProstate cancer is a form of cancer that develops in the prostate gland.\n\n**In Brief:**\n\nA cancer in a man's prostate, a small walnut-sized gland that produces seminal fluid.\nA man's prostate produces the seminal fluid that nourishes and transports sperm.\nSymptoms include difficulty with urination, but sometimes there are no symptoms at all.","3367c1be":"Logistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input  (X)  belongs to the default class  (Y=1) )  (P(X)=P(Y=1|X)) . In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nStarted from make an instance of the model setting the default values. Specify the inverse of the regularization strength in 10. Trained the logistic regression model with the training data, and then applied such model to the test data.","9cadc83e":"# IMPORTING LIBRARIES","d289b80d":"# 3. Naive bayes classification","fc83feab":"**Diagnosis of prostate cancer is based on Type of tumour found in the medical test.**","0167e790":"Based on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let  N  be the number of test cases,  M  is the number of variables in the classifier.\n* Let  m  be the number of input variables to be used to determine the decision in a given node;  m<M .\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose  m  variables on which to base the decision.\n* Calculate the best partition of the training set from the  m  variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100.","9f568de9":"Data for training and testing To select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20%, assumed it ideal ratio between training and testing","c53fddc1":"**In this study, we tried to predict Prostate Cancer using 6 different algorithm:**\n<ol>\n    <li>Logistic regression classification<\/li>\n    <li>SVM (Support Vector Machine) classification<\/li>\n    <li>Naive bayes classification<\/li>\n    <li>Decision tree classification<\/li>\n    <li>Random forest classification<\/li>\n    <li>K-Nearest Neighbor classification<\/li>\n<\/ol>\n\n**Predictor variable use in classifying Prostate and its features**\n<ol>\n    <li>radius (mean of distances from center to points on the perimeter)<\/li>\n    <li>texture (standard deviation of gray-scale values)<\/li>\n    <li>perimeter<\/li>\n    <li>area<\/li>\n    <li>smoothness (local variation in radius lengths)<\/li>\n<\/ol>","6c275089":"# 6. K-Nearest Neighbor classification","10d8caca":"We measure the study of 7 different algorithms using a confusion matrix.","f1a19ab6":"The naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem  (P(X|Y)=P(Y|X)P(X)P(Y)) , we can find the probability of  X  happening, given that  Y  has occurred. Here,  Y  is the evidence and  X  is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors\/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes.","41870b55":"# 5. Random forest classification","dd8d8e54":"**Beningn Tumour: Noncancerous**\n\nIf the cells are not cancerous, the tumor is benign. It won't invade nearby tissues or spread to other areas of the body (metastasize). A benign tumor is less worrisome unless it is pressing on nearby tissues, nerves, or blood vessels and causing damage.1\n\n\ufeff Fibroids in the uterus or lipomas are examples of benign tumors. \nBenign tumors may need to be removed by surgery.\n\n**Malignant Tumour: Cancerous**\n\nMalignant means that the tumor is made of cancer cells, and it can invade nearby tissues. Some cancer cells can move into the bloodstream or lymph nodes, where they can spread to other tissues within the body\u2014this is called metastasis.\n\nNow For Conveinent Purpose Diagnosis Result in the Table Can be Changed to following:\n\nM = '1' Which Indicates diagnosis of Prostate Cancer \nB = '0' which Indicates patient doesn't have Prostate Cancer and not Harmful. ","2100f7d6":"# 4. Decision Tree classification","dfd644d2":"# 1. Logistic regression classification","1b787422":"# 2. SVM (Support Vector Machine) classification","0ea0ea0c":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation.","8f2405c1":"From a Comparison of classification techniques, we plotting ROC to illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.","da10fd99":"# DIAGNOSIS Explanation","e4797d3b":"# Comparison of classification techniques","4cd143fb":"# Exploratory Data Analysis","56b7ad5d":"SVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function)  y=f(x)  between some high-dimensional input vector  x  and scalar output  y . It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set  D=(xi,yi)\u2208X\u00d7Y,i=1 ,  l , where  l  stands for the number of the training data pairs and is therefore equal to the size of the training data set  D , additionally,  yi  is denoted as  di , where  d  stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes","eb258686":"# VISUALIZATION","39fc2a49":"A decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively."}}