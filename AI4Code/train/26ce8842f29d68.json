{"cell_type":{"077d2f4f":"code","6a5e8c25":"code","76aa7ffa":"code","5bc7f220":"code","442270ff":"code","2d522c58":"code","d2537afc":"code","41f5e344":"code","e3fcf149":"code","21e6398a":"code","aa460385":"code","6a8d5f89":"code","b774b24c":"code","550e8498":"code","b8c6617f":"code","ba62c906":"code","257ef7d9":"code","f827bff9":"code","fb83bc62":"code","05e81309":"code","7ef7c4fa":"code","be780fcf":"code","af2399d4":"code","9888c06d":"code","577dd964":"code","7c87ffef":"code","e7a49970":"code","b268bfde":"code","d853c552":"code","84a7afbc":"code","828b01d6":"code","903946f6":"code","fb33bb1b":"code","a180546e":"code","bc5c3a9c":"code","2fc7f89a":"code","5679c657":"code","3b36667a":"code","d38a43a4":"code","ee13ebdb":"code","c2a4ffa1":"code","7ca0c848":"code","1d402028":"code","757970d8":"code","d8aa43bb":"code","8c6e2d9b":"code","06f00c79":"code","379999cb":"code","7ff7f874":"code","2eed9dcf":"code","da6aba79":"code","9ebb67eb":"code","53dc5729":"code","df95c8b2":"code","8efcab66":"code","0e2e920a":"code","66e8a80e":"code","de815d58":"code","c15c2993":"code","cf420dc9":"code","11a2aa4d":"code","e6998563":"code","ad6adeb0":"code","b3694af2":"code","a549dcb1":"code","92315ce9":"code","550f445d":"code","aeb990e0":"code","c4eea704":"code","87711dc8":"code","e02ee674":"code","bf96aeaf":"code","69cd59e1":"code","b5e0a50b":"code","41e7d40e":"code","7f657aff":"code","dc3d25f1":"code","1dcb4dbf":"code","4ae46415":"code","24e9a01a":"code","aefe5a01":"code","ad8bc76c":"markdown","816ab150":"markdown","8dcc89ca":"markdown","221cace0":"markdown","215d02bb":"markdown","8d11e2e3":"markdown","fe3e1220":"markdown","ef06b7da":"markdown","be64fe5e":"markdown","c68efdc1":"markdown","6b68322d":"markdown","3e516ac3":"markdown","f91488c4":"markdown","ec826dc8":"markdown","13bf762e":"markdown","89eda05a":"markdown","982214d5":"markdown","8a19e3b4":"markdown","df713593":"markdown","0bccc1f6":"markdown","48d0fb51":"markdown","4f928649":"markdown","afa9bd90":"markdown","cd29cfda":"markdown","a450e7ec":"markdown","b2b1efd3":"markdown","35a6b367":"markdown","682de89f":"markdown","ffca46dc":"markdown","efafb7a1":"markdown","69df0f54":"markdown","09ac97d6":"markdown","3889f641":"markdown","5b7378d5":"markdown","51fd8700":"markdown","4639266b":"markdown","13ba9fce":"markdown","96e5f5f8":"markdown","0f589067":"markdown","210d9bb9":"markdown","41569607":"markdown","ef163e51":"markdown","434e7362":"markdown","d9cadf45":"markdown","43e94c89":"markdown","785a24d5":"markdown","232e1a2f":"markdown","e6af6c88":"markdown","945e7ae0":"markdown","5de73414":"markdown","b3738419":"markdown","32efe068":"markdown","19c0de4c":"markdown","474c2a6b":"markdown","4137766b":"markdown","2a2c6b2c":"markdown","77b6cf8a":"markdown","ee51189a":"markdown","c0613e27":"markdown"},"source":{"077d2f4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6a5e8c25":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom scipy.stats import boxcox\nfrom sklearn.model_selection import StratifiedKFold","76aa7ffa":"# this is not to skip some columns when showing the dataframe\npd.set_option('display.max_columns', None)\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head(2)","5bc7f220":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head(2)","442270ff":"train.shape","2d522c58":"test.shape","d2537afc":"train.info()","41f5e344":"test.info()","e3fcf149":"train.isnull().sum()","21e6398a":"test.isnull().sum()","aa460385":"# This code allocates the passengers who survived and those who didn't into 2 different variables.\nsurvived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n# This code prints the number of survived and dead passengers along with the percentage.\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))\/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","6a8d5f89":"#we will use scater plot to see outlier .. \nfig, ax = plt.subplots()\nax.scatter(x = train['Age'], y = train['Survived'])\nplt.ylabel('Survived', fontsize=13)\nplt.xlabel('Age', fontsize=13)\nplt.show()","b774b24c":"# dropping that one old grandma from the boat\ntrain = train.drop(train[(train['Age']>79) & (train['Survived']>0.8)].index)","550e8498":"fig, ax = plt.subplots()\nax.scatter(x = train['Fare'], y = train['Survived'])\nplt.ylabel('Survived', fontsize=13)\nplt.xlabel('Fare', fontsize=13)\nplt.show()","b8c6617f":"train = train.drop(train[(train['Fare']>400) & (train['Survived']>0.8)].index)","ba62c906":"target = train['Survived']\n#concatinating the two dataframes\nwhole_df = pd.concat([train,test],keys=[0,1], sort = False)\n# Sanity check to ensure the total length matches \nwhole_df.shape[0] == train.shape[0]+test.shape[0]","257ef7d9":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (18, 6))\n\n# This is a plot to show the missing values in the merged dataset\nsns.heatmap(whole_df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='copper')\nax.set_title('Dataset')\n","f827bff9":"sns.barplot(x='Pclass', y='Survived', data=whole_df)","fb83bc62":"# This code shows the number of survived and dead passengers based on their class.\nsns.barplot(x='Pclass',y='Survived',hue='Sex',data=whole_df,order=None,hue_order=None)","05e81309":"whole_df['Fare'].fillna(whole_df[whole_df['Pclass'] == 3]['Fare'].mean(), inplace=True)","7ef7c4fa":"plt.figure(figsize=(15,10))\nsns.heatmap(whole_df.drop('PassengerId',axis=1).corr(), vmax=0.6, annot=True)","be780fcf":"sns.boxplot(data=whole_df, y='Age', x='Pclass')","af2399d4":"sns.boxplot(data=whole_df, y='Age', x='SibSp')","9888c06d":"sns.boxplot(data=whole_df, y='Age', x='Parch')","577dd964":"\ndef adjust_age(df,age_s):\n    #convert age feature to a numpy array\n    age_array = age_s.to_numpy()\n    result = []\n    \n    # in case cannot find similar group we use default to fill\n    default = int(df[\"Age\"].median())\n    # iterate every row\n    for i,val in enumerate(age_array):\n        # if empty\n        if np.isnan(val):\n            # get the median age of the group with similar attributes\n            median_age = df[(df['Pclass']==df.iloc[i]['Pclass'])&(df['SibSp']==df.iloc[i]['SibSp'])&(df['Parch']==df.iloc[i]['Parch'])]['Age'].median()\n            \n            try:\n                # age will be nan if no similar group found\n                result.append(int(median_age))\n            except:\n                # append the default if no median_age found\n                result.append(default)\n        else:\n            result.append(int(val))\n\n    return result\n\n# filling age using our function        \nwhole_df['Age'] = adjust_age(whole_df,whole_df['Age'])\n\n","7c87ffef":"\n# Rechecking the condition of our dataset\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (18, 6))\nsns.heatmap(whole_df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='copper')\nax.set_title('Whole dataset')\n","e7a49970":"whole_df['Cabin'].fillna(value=\"NA\",inplace=True)","b268bfde":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (18, 6))\n\n# train data \nsns.heatmap(whole_df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='copper')\nax.set_title('Train data')\n","d853c552":"train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","84a7afbc":"whole_df['Fare'] = pd.qcut(whole_df['Fare'], 13)\n","828b01d6":"sns.countplot(data=whole_df, y='Fare',hue='Survived')","903946f6":"# This plots the count of people from different ages who survived and died\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10, 8))\nsns.countplot(data=whole_df[(whole_df['Age'] >= 0)&(whole_df['Age'] <= 70)], y='Age', hue='Survived',ax=ax)","fb33bb1b":"\ndef categorize_age(val):\n    if val < 1:\n        return \"infant\"\n    # Child case\n    if val >= 1 and val <= 14:\n        # classify as child\n        return \"child\"\n    #youth case\n    elif val >= 15 and val <= 24:\n        return \"youth\"\n    # limited to 63 to keep seniors pure\n    elif val >= 25 and val <= 63:\n        return \"adult\"\n    # seniors case (all dead)\n    elif val >= 64:\n        return \"senior\"\n    print(\"Something is wrong!\")\n","a180546e":"# applying the new function to age\nwhole_df['Age'] = whole_df['Age'].apply(categorize_age)\n# plotting the survival of each age category\nsns.countplot(data=whole_df, y='Age',hue='Survived')","bc5c3a9c":"# +1 is because im adding the person as well. 1 means alone\nwhole_df['famsize'] = whole_df['Parch']+whole_df['SibSp']+1","2fc7f89a":"\n#whole_df['famsize'] = np.log1p(whole_df['famsize'])\nsns.distplot(whole_df['famsize'])\nwhole_df['famsize'].skew()","5679c657":"#sns.countplot(data=train, y='famsize',hue='Survived')\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15, 8))\nsns.barplot(data=whole_df, y='Survived',x='famsize',hue='Age',ax=ax)","3b36667a":"sns.countplot(data=whole_df, y='famsize',hue='Survived')","d38a43a4":"def classify_fam_size(famsize):\n    # highest count and high death rate\n    if famsize == 1:\n        return \"solo\"\n    # higher survival rate than death rate\n    elif famsize in [2,3,4]:\n        return \"small-family\"\n    # noticably higher death rate compared to the last groups\n    elif famsize in [5,6]:\n        return \"mid-family\"\n    elif famsize > 6:\n        return \"large-family\"\n\nwhole_df['famsize'] = whole_df['famsize'].apply(classify_fam_size)","ee13ebdb":"whole_df['Cabin'].unique()","c2a4ffa1":"def take_section(code):\n        return code[0]\nwhole_df['Cabin'] = whole_df['Cabin'].apply(take_section)","7ca0c848":"whole_df['Cabin'].unique()","1d402028":"sns.countplot(data=whole_df, x='Cabin',hue='Pclass')","757970d8":"sns.barplot(data=whole_df, x='Cabin',y='Survived',hue='Pclass')","d8aa43bb":"#grouping cabins based on class\ndef group_cabin(code):\n    # adding T because there is 1 person in T and has a class same as ppl\n    # in A,C and B\n    if code in ['A','C','B','T']:\n        return \"ACBT\"\n    elif code in [\"F\",\"G\"]:\n        return \"FG\"\n    elif code in ['E','D']:\n        return 'DE'\n    else:\n        return code\n    \nwhole_df['Cabin'] = whole_df['Cabin'].apply(group_cabin)","8c6e2d9b":"sns.barplot(data=whole_df, x='Cabin',y='Survived')","06f00c79":"# Getting the title of each person\nwhole_df['title'] = whole_df['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())","379999cb":"whole_df['title'].unique()","7ff7f874":"sns.countplot(data=whole_df, y='title',hue='Survived')","2eed9dcf":"sns.barplot(data=whole_df, y='title',x='Survived', hue='Sex')","da6aba79":"def standardise_names(name):\n    if name in ['Mrs','Miss','Ms', 'Lady','Mlle','the Countess','Dona']:\n        return \"female_titles\"\n    elif name in ['Mr','Master']:\n        return name\n    else:\n        return 'others'\nwhole_df['title'] = whole_df['title'].apply(standardise_names)","9ebb67eb":"sns.countplot(data=whole_df, y='title',hue='Survived')","53dc5729":"tickets = {}\n# grouping training set by ticket and finding the survival ratio\nfor ticket, df in whole_df.xs(0).groupby('Ticket'):\n    # not putting high survival rate for solo ppl\n    if df.shape[0]>1:\n        tickets[ticket] = df['Survived'].sum()\/ df.shape[0]","df95c8b2":"# setting the calculated survival rate based on the ticket\n# if ticket not precalculated then set 0.5 (idk)\ndefault = 0.5\nwhole_df['ticket_sr'] = whole_df['Ticket']\nwhole_df['ticket_sr'] = whole_df['ticket_sr'].apply(lambda x: tickets[x] if x in tickets.keys() else default)","8efcab66":"sns.countplot(data=whole_df, hue='famsize',y='ticket_sr',orient='h')","0e2e920a":"sns.barplot(data=whole_df, y='ticket_sr',x='Survived',orient='h')","66e8a80e":"whole_df.drop(labels= ['Name','Ticket','SibSp','Parch'], axis=1,inplace=True)\nwhole_df = pd.get_dummies(whole_df,columns=['Pclass','Sex',\"Age\",'Fare','Cabin','Embarked','title','famsize'],drop_first=True)","de815d58":"# global setting to show all the features\npd.set_option('display.max_columns', None)\nwhole_df.head(3)","c15c2993":"corr = whole_df.drop('PassengerId',axis=1).corr()\ncorr = corr[(corr['Survived'] > 0.1) | (corr['Survived'] < -0.1)]\ncorr['Survived'].sort_values(ascending=False).drop('Survived').plot(kind='bar')","cf420dc9":"# separate the sets back\ntrain,test = whole_df.xs(0),whole_df.xs(1)","11a2aa4d":"# preparing X and target\ny = train['Survived'].astype(int)\nX = train.drop(labels=['Survived','PassengerId'], axis=1)","e6998563":"# This one was used during testing the different models but they were all removed to make the notebook cleaner\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=24)","ad6adeb0":"# prepping the test set to submit to kaggle\nX_final,IDs =  test.drop(labels=['PassengerId','Survived'], axis=1), test['PassengerId']","b3694af2":"# special libraries for ML\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold, KFold\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier","a549dcb1":"thresh = 0.1\ncorr = train.drop('PassengerId',axis=1).corr()\ncorr = corr[(corr['Survived'] > thresh) | (corr['Survived'] < -thresh)].drop('Survived')\nselected_features = corr.sort_values('Survived').index\nselected_features","92315ce9":"# make visualization of the score of different models\ndef make_benchmark(X,y, scoring='accuracy', cv = 5):\n    # using stratified because we have more deaths than survivors\n    folds = StratifiedKFold(n_splits=cv)\n    # different models for benchmark\n    models = [SVC(gamma='auto'),\n            DecisionTreeClassifier(),KNeighborsClassifier(),\n            ExtraTreeClassifier(),LogisticRegression(max_iter=10000,solver='lbfgs'),\n            RidgeClassifier(),AdaBoostClassifier(),\n            BaggingClassifier(),RandomForestClassifier()]\n    # names of the models\n    names = [\"SVM\",\"Decision Tree\",\"KNN\",\"Extra Trees\",\"Logistic Regression\",\n             \"Ridge classifier\",\"AdaBoost\",\"Bagging Classifier\",\"Random Forest\"]\n    # cross val mean score for each model\n    scores = [np.mean(cross_val_score(model, X, y, cv=folds, scoring=scoring)) for model in models]\n    \n\n    return pd.DataFrame({\"model_name\":names,\n                         \"model\": models,\n                         scoring+\"_score\":scores})","550f445d":"# we will use this later on gridsearch\nfolds = StratifiedKFold(n_splits=5)","aeb990e0":"# make benchmark\nbench = make_benchmark(X,y,cv=5)\n#plot benchmark\nsns.barplot(x='accuracy_score',y='model_name',data=bench.sort_values('accuracy_score',ascending=False)).set_title(\"Models Score Benchmark\")","c4eea704":"# do benchmark but using features selected previously (has corr > 0.1)\n\nbench = make_benchmark(X[selected_features],y,cv=5)\nsns.barplot(x='accuracy_score',y='model_name',data=bench.sort_values('accuracy_score',ascending=False)).set_title(\"Models Score Benchmark\")","87711dc8":"## WARNING: this will take more than 10 minutes\n## IF you still want to do so please uncomment the block\n\n'''rf = RandomForestClassifier(random_state=24)\nparams = {'criterion':['gini','entropy'],\n          'max_features':['auto'],\n          'n_estimators':[150,200,300],\n          'max_depth': [3,5,7,10,14],\n          'class_weight':[ \"balanced\", \"balanced_subsample\"],\n          'min_samples_split':[20,15,10]}\n\ngsrf = GridSearchCV(rf , params,cv=KFold(5),verbose=2)\ngsrf.fit(X,y)\ngsrf.best_estimator_'''","e02ee674":"# different numbers of estimators effect on random forest\nfolds = StratifiedKFold(n_splits=5)\nx_axis, y_axis = [],[]\nfor i in range(250,800,50):\n    rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n                           criterion='entropy', max_depth=7, max_features='auto',\n                           max_leaf_nodes=None, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=i,\n                           n_jobs=None, oob_score=False, random_state=24, verbose=0,\n                           warm_start=False)\n    \n    y_axis.append(np.mean(cross_val_score(rf, X, y, cv=folds,scoring='accuracy')))\n    x_axis.append(i)\n\nsns.lineplot(x=x_axis, y=y_axis)","bf96aeaf":"#effect of  different depth on RandomForest\n\nfolds = StratifiedKFold(n_splits=5)\nx_axis, y_axis = [],[]\nfor i in range(3,30,1):\n    #our model\n    rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n                           criterion='entropy', max_depth=i, max_features='auto',\n                           max_leaf_nodes=None, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=700,\n                           n_jobs=None, oob_score=False, random_state=24, verbose=0,\n                           warm_start=False)\n    # add score for plotting\n    y_axis.append(np.mean(cross_val_score(rf, X, y, cv=folds,scoring='accuracy')))\n    x_axis.append(i)\n\nsns.lineplot(x=x_axis, y=y_axis)","69cd59e1":"#effect of  different depth on RandomForest\n\nfolds = StratifiedKFold(n_splits=5)\nx_axis, y_axis = [],[]\nfor i in np.linspace(0.1,1.,10):\n    #our model\n    rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n                           criterion='entropy', max_depth=9, max_features=i,\n                           max_leaf_nodes=None, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=700,\n                           n_jobs=None, oob_score=False, random_state=24, verbose=0,\n                           warm_start=False)\n    # add score for plotting\n    y_axis.append(np.mean(cross_val_score(rf, X, y, cv=folds,scoring='accuracy')))\n    x_axis.append(i)\n\nsns.lineplot(x=x_axis, y=y_axis)","b5e0a50b":"folds = StratifiedKFold(n_splits=5)\nx_axis, y_axis = [],[]\nfor i in np.linspace(0.0001,0.3,15):\n    #our model\n    rf = RandomForestClassifier(bootstrap=True, ccp_alpha=i, class_weight='balanced',\n                           criterion='entropy', max_depth=9, max_features=.8,\n                           max_leaf_nodes=None, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=700,\n                           n_jobs=None, oob_score=False, random_state=24, verbose=0,\n                           warm_start=False)\n    # add score for plotting\n    y_axis.append(np.mean(cross_val_score(rf, X, y, cv=folds,scoring='accuracy')))\n    x_axis.append(i)\n\nsns.lineplot(x=x_axis, y=y_axis)","41e7d40e":"folds = StratifiedKFold(n_splits=5)\nx_axis, y_axis = [],[]\nfor i in range(1,50,3):\n    #our model\n    rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.02, class_weight='balanced',\n                           criterion='entropy', max_depth=9, max_features=.8,\n                           max_leaf_nodes=None, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=i, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=700,\n                           n_jobs=None, oob_score=False, random_state=24, verbose=0,\n                           warm_start=False)\n    # add score for plotting\n    y_axis.append(np.mean(cross_val_score(rf, X, y, cv=folds,scoring='accuracy')))\n    x_axis.append(i)\n\nsns.lineplot(x=x_axis, y=y_axis)","7f657aff":"rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.00,\n                       class_weight='balanced_subsample', criterion='gini',\n                       max_depth=9, max_features=0.2, min_samples_leaf=4,\n                       min_samples_split=15,n_estimators=700,\n                       random_state=24, verbose=0)","dc3d25f1":"np.mean(cross_val_score(rf, X, y, cv=folds, scoring='accuracy')) ","1dcb4dbf":"rf.fit(X,y)","4ae46415":"y_hat = rf.predict(X_final)","24e9a01a":"submission_df = pd.DataFrame({\"PassengerId\":IDs,\n                              \"Survived\": y_hat})","aefe5a01":"submission_df.to_csv('submit.csv',index=False)","ad8bc76c":"These are the performance of different models out of the box (except logistic reg because it wasnt converging).\n\nAfter a lot of trials of fitting diffirent models, we noticed that although many of them gives us a high cross validation score, they overfit the data as kaggle score is a lot lower than the CV score. The best model that acheived a decent score is Random Forest which has almost equal accuracy when fitted on the whole dataset or selected features ","816ab150":"There are many people who dont have a close family member onboard","8dcc89ca":"## High correlation features in the training set","221cace0":"we can see that all infants survived while all seniors died. also, aduls have the worst chance of survival","215d02bb":"It seems that the more we increased the number of samples required to make a leaf branch, the worse the accuracy gets. However, we cannot conclude the result as we think it might be solving the overfitting issue.","8d11e2e3":"### Models benchmark","fe3e1220":"#### Names","ef06b7da":"### Using tickets to make ticket survival rate\n\nWe are thinking that people who are close\/friends\/family will buy the ticket together. Therefore, in case of an accident they will try to help each other which results in either the majority of the group surviving or dying. Hence, if most of the group survived based on train dataset, we are assuming that the person in the test set (no label) will also survive given that they have the same ticket as the survivor group in training set. ","be64fe5e":"### Finding the effect of max_features on accuracy\nNote that we stabalized the n_estimator and depth","c68efdc1":"1. From the plot above, people who are in the first class have the highest survival probability compared to 2 and 3.\n\nWe can see a negative correlation between the label and Pclass","6b68322d":"taking 20% of features to consider when looking for the best split yeilds the best accuracy while taking 80% yeilds us the lowest. we will use 80% as we are afraid of overfitting","3e516ac3":"Above 80% of women in the first two classes (1,2) and overall, women have a higher chance to survive than men.\n\nSince we have few missing enteries of the Fare price in test data, we can fill it with the average price depending on the class.","f91488c4":"### Finding the effect of RF depth on accuracy\nNote: we stabalized the n_estimator of 700","ec826dc8":"PassengerId: An unique index for passenger rows. It starts from 1 for first row and increments by 1 for every new rows.\n\nSurvived: Shows if the passenger survived or not. 1 stands for survived and 0 stands for not survived.\n\nPclass: Ticket class. 1 stands for First class ticket. 2 stands for Second class ticket. 3 stands for Third class ticket.\n\nName: Passenger's name. Name also contain title. \"Mr\" for man. \"Mrs\" for woman. \"Miss\" for girl. \"Master\" for boy.\n\nSex: Passenger's sex. It's either Male or Female.\n\nAge: Passenger's age. \"NaN\" values in this column indicates that the age of that particular passenger has not been recorded.\n\nSibSp: Number of siblings or spouses travelling with each passenger.\n\nParch: Number of parents of children travelling with each passenger.\n\nTicket: Ticket number.\n\nFare: How much money the passenger has paid for the travel journey.\n\nCabin: Cabin number of the passenger. \"NaN\" values in this column indicates that the cabin number of that particular passenger has not been recorded.\n\nEmbarked: Port from where the particular passenger was embarked\/boarded.","13bf762e":"## Removing uncessesary Features and Dummify","89eda05a":"grouping sizes which have similar survival rates together","982214d5":"## Feature Exploration and Filling","8a19e3b4":"## Machine Learning Experimentation\nthis section is for\n* testing multiple machine learning algorithms on dataset\n    * the goal is to **find best model**\n    * value **test accuracy** over **train accuracy**\n    * if test acc is way higher than train acc this means overfitting","df713593":"These are the top features that correlate with the Survival rate. female titles, ticket survival rate, Mr title and sex male all play a drastic role in predicting the survival rate","0bccc1f6":"### Filling Age\n\nTo fill age, we are looking for features with correlations with Age, to use","48d0fb51":"#### Removing Fare outlier","4f928649":"We can see that the less the ticket survival rate is, the less chance of survival the ticket holder has","afa9bd90":"Note that Cabins with starting letter of C,A and B are purely contain first class travellers. Since E and D have similar survived\/dead distributions and both counts are very small, we are grouping them together","cd29cfda":"## Model Optimization\nthis section is for\n* experimenting with different hyperparameters of the model\n    - the goal is to find a set of hyperparameters that yeild the best result","a450e7ec":"It seems like intruducing a litle bit of pruning helps increase accuracy, then the accuracy drops down with the increase of pruning","b2b1efd3":"For Pclass and Age, the first class tend to have older people when compared to second class. and second class has older people when compared to third classs","35a6b367":"## Exploring and Cleaning the dataframe","682de89f":"### Age\n\nFor Age, it will be categorized as below\n* infant: < 1 year\n* Child: 1 - 14 years\n* youth: 15 - 24 years\n* adult: 25 - 63 years\n* senior 64 and above","ffca46dc":"From the above graph we think trees in the forest wont expand more than depth of 20. The best depth they yeilds the best accuracy is 9","efafb7a1":"After grouping, Mr has the highest chance of death while female_titles have the highest chance of survival. ","69df0f54":"We will merge all Female titles together. However, for male titles we will keep Mr and Master separate as they have contradicting survival rates. we will group other titles in a group called other","09ac97d6":"## Attempting to fill missing values","3889f641":"People onboard who has less siblings\/spouse onboard tend to be older.","5b7378d5":"It appears that the more the fare, the higher the survival probability as seen from the orange bar. On the other hand, the cheaper the fare, the more probable that a person will not survive.","51fd8700":"### Getting features with high correlation","4639266b":"### Filling Embarked","13ba9fce":"Here it seems the most correlated features are Pclass, SibSp, Parch and Fare","96e5f5f8":"### Parch + SibSp\n\nWe adding these two give us an estimate of the family size","0f589067":"We have more Not Survived samples than Survived samples. This will affect how we sample data for training the model.","210d9bb9":"### do benchmark but using selected features","41569607":"It seems that Cabin is the most missing feature we have","ef163e51":"### Finding the effect of number of base estimators on accuracy\n the model is the best model in gridsearch above","434e7362":"### Finding the effect of number of minimum samples to make a leaf on accuracy\nstabalized hyperparameters:\n* n_estimators\n* depth\n* max features\n* pruning (CCPAlpha)","d9cadf45":"### Changing famsize feature to be categorical\n\nBased on the distribution below, sizes with similar distributions will be grouped together ","43e94c89":"## Detecting and removing outliers\n\n#### Removing Age outlier","785a24d5":"NOTE: missing values in Survived belongs to the test data as it doesnt have a label","232e1a2f":"### The percentage of survivors in Train","e6af6c88":"### Cabin\n\nListing all unique values in Cabin\n\nWe can group them by the first letter","945e7ae0":"### Filling Cabin\n\nFor This feature, we wanted to remove it due to the large amount of missing data in both train and test. However, we think people in the first class cabins might have a higher priority compared to second and third. Hence, we are filling empty values with 'NA'","5de73414":"* The best chance of survival for children is for those who have a fammily size of 2 - 4\n\n* infants and seniors survival rate are unaffected by family size\n\n* youth survival rate tends to have a slight positive correlation with the increase of family size\n","b3738419":"## Checking out how many null values in both datasets","32efe068":"## Machine Learning prepping\nThis section is for\n* separating the dataset into **X** (features) and **y** (label)\n* split the data into training and testing set\n    - research **optimal train to test ratio**\n* Breaking down data into test and train \n* feature selection","19c0de4c":"#### Survival based on Class","474c2a6b":"## Merging dataframes\n\nAfter we are done with cleaning the train data, now we merge both train and test to fill missing features and do feature engineering.","4137766b":"# Feature engineering\n\nhere we make other features and turn existing ones in a way which we think it will help the model in making predictions\n\n### Fare\n\nConverting the continuos range to categorical thru binning","2a2c6b2c":"## Finalizing random forest","77b6cf8a":"It seems like the lesser the depth the higher accuracy we get. However, we are affraid that this the RF is overfitting. Hence we went with 700","ee51189a":"The older the traveller, the more probable that they will have more Parents\/children onboard. \n\nWe will fill age by looking at the average age of people in the same class and have the same number of Siblings\/Spouse and Parentch\\children","c0613e27":"### Finding the effect of CCPAlpha on accuracy (Pruning)\nstabalized hyperparameters:\n* n_estimators\n* depth\n* max features"}}