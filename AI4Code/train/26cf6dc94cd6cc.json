{"cell_type":{"1b43257d":"code","8fc2260c":"code","1844aa59":"code","4ffd9aaf":"code","1e5dbad8":"code","3522d72c":"code","49a3591e":"code","36eb1f9b":"code","ba81d841":"code","dd2f87b8":"code","c0b8b78c":"code","14ea946a":"code","4778198d":"code","4ceb9e7e":"code","e720efae":"code","71381a20":"code","f98fc872":"markdown","17896572":"markdown","132c7425":"markdown","49d264c5":"markdown","16baeb25":"markdown","86224f46":"markdown"},"source":{"1b43257d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fc2260c":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","1844aa59":"import pickle\nimport pandas as pd\nimport numpy as np\n\nX = []\ny = []\n\nwith open('..\/input\/test-labels-ner\/sentences.pkl', 'rb') as f:\n    X = pickle.load(f)\n    \nwith open('..\/input\/test-labels-ner\/bio_labels.pkl', 'rb') as f:\n    y = pickle.load(f)\n","4ffd9aaf":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","1e5dbad8":"!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/","3522d72c":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.","49a3591e":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\nprint(f'No. raw training rows: {len(train)}')","36eb1f9b":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","ba81d841":"# papers = {}\n# for paper_id in train['Id'].unique():\n#     with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n#         paper = json.load(f)\n#         papers[paper_id] = paper","dd2f87b8":"c=0\nco = 0","c0b8b78c":"with open('train_ner.json', 'w') as f:\n    for i in range(len(X)):\n        words = X[i]\n        nes = y[i]\n#         row_json = {'tokens' : words, 'tags' : nes}\n#         co = co+1\n#         json.dump(row_json, f)\n#         f.write('\\n')\n        if('B' in nes):\n            row_json = {'tokens' : words, 'tags' : nes}\n            co = co+1\n            json.dump(row_json, f)\n            f.write('\\n')\n#         else:\n#             c = c+1\n#             if(c>100):\n#                 c=0\n#                 co=co+1\n#                 row_json = {'tokens' : words, 'tags' : nes}\n#                 json.dump(row_json, f)\n#                 f.write('\\n')\n                \n                ","14ea946a":"del X\ndel y\n# will run into memory issues otherwise","4778198d":"print(co)","4ceb9e7e":"# len(X)","e720efae":"!pip install --upgrade git+https:\/\/github.com\/intake\/filesystem_spec\n# comment out if you want to run without internet","71381a20":"!python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n--model_name_or_path 'bert-base-cased' \\\n--train_file '.\/train_ner.json' \\\n--validation_file '.\/train_ner.json' \\\n--num_train_epochs 1  \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 8 \\\n--save_steps 15000 \\\n--output_dir '.\/output' \\\n--report_to 'none' \\\n--seed 123 \\\n--do_train","f98fc872":"# use version 8 for a bigger sample","17896572":"Lets load the NER labeled data set","132c7425":"you can play around with how much data you want to use i would suggest to use all of it but then again i was testing this approach so using all of it didnt make sense as it would require a lot of time and resources","49d264c5":"Training begins","16baeb25":"# Find the dataset with model @ - https:\/\/www.kaggle.com\/ash1706\/ner-model","86224f46":"This notebook trains a NER BERT BASE model \nCredits to https:\/\/www.kaggle.com\/tungmphung\/pytorch-bert-for-named-entity-recognition fro such a amazing notebook\nand all the owners of the dataset i am using\nalso a great NER data processing notebook - https:\/\/www.kaggle.com\/devashishprasad\/bio-labeled-dataset"}}