{"cell_type":{"90582499":"code","b3bf49e1":"code","9b6668f6":"code","b6b1b711":"code","447a1926":"code","0826f32b":"code","9394afbf":"code","e57cbe12":"code","629cdeb8":"code","58efce18":"code","fe77fb00":"code","fed3eb3f":"code","5cecf280":"code","e87307b0":"code","8920eb66":"code","0813a549":"code","1a3278bf":"code","d03633be":"code","ace149aa":"code","83471038":"code","46f2964f":"markdown","0bfb4f6e":"markdown","838db578":"markdown","b2a7aa32":"markdown","9da0a212":"markdown","743d1acc":"markdown","fbe9e932":"markdown","06e01b5b":"markdown","e7ba8b2f":"markdown"},"source":{"90582499":"import numpy as np \nimport cv2\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport string\nfrom mlxtend.plotting import plot_decision_regions\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\n\nprint(os.listdir(\"..\/input\"))\ndim = 100","b3bf49e1":"def getYourFruits(fruits, data_type, print_n=False, k_fold=False):\n    images = []\n    labels = []\n    val = ['Training', 'Test']\n    if not k_fold:\n        path = \"..\/input\/*\/fruits-360\/\" + data_type + \"\/\"\n        for i,f in enumerate(fruits):\n            p = path + f\n            j=0\n            for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n                image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n                image = cv2.resize(image, (dim, dim))\n                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n                images.append(image)\n                labels.append(i)\n                j+=1\n            if(print_n):\n                print(\"There are \" , j , \" \" , data_type.upper(), \" images of \" , fruits[i].upper())\n        images = np.array(images)\n        labels = np.array(labels)\n        return images, labels\n    else:\n        for v in val:\n            path = \"..\/input\/*\/fruits-360\/\" + v + \"\/\"\n            for i,f in enumerate(fruits):\n                p = path + f\n                j=0\n                for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n                    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n                    image = cv2.resize(image, (dim, dim))\n                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n                    images.append(image)\n                    labels.append(i)\n                    j+=1\n        images = np.array(images)\n        labels = np.array(labels)\n        return images, labels\n    \ndef getAllFruits():\n    fruits = []\n    for fruit_path in glob.glob(\"..\/input\/*\/fruits-360\/Training\/*\"):\n        fruit = fruit_path.split(\"\/\")[-1]\n        fruits.append(fruit)\n    return fruits\n    ","9b6668f6":"#Choose your Fruits\nfruits = [ 'Cocos','Pineapple'] #Binary classification\n\n#Get Images and Labels \nX_t, y_train =  getYourFruits(fruits, 'Training', print_n=True, k_fold=False)\nX_test, y_test = getYourFruits(fruits, 'Test', print_n=True, k_fold=False)\n\n#Get data for k-fold\nX,y = getYourFruits(fruits, '', print_n=True, k_fold=True)\n\n#Scale Data Images\nscaler = StandardScaler()\nX_train = scaler.fit_transform([i.flatten() for i in X_t])\nX_test = scaler.fit_transform([i.flatten() for i in X_test])\nX = scaler.fit_transform([i.flatten() for i in X])","b6b1b711":"def plot_image_grid(images, nb_rows, nb_cols, figsize=(15, 15)):\n    assert len(images) == nb_rows*nb_cols, \"Number of images should be the same as (nb_rows*nb_cols)\"\n    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=figsize)\n    \n    n = 0\n    for i in range(0, nb_rows):\n        for j in range(0, nb_cols):\n            axs[i, j].axis('off')\n            axs[i, j].imshow(images[n])\n            n += 1        ","447a1926":"print(fruits[y_train[0]])\nplot_image_grid(X_t[0:100], 10, 10)","0826f32b":"print(fruits[y_train[490]])\nplot_image_grid(X_t[490:590], 10, 10)","9394afbf":"def getClassNumber(y):\n    v =[]\n    i=0\n    count = 0\n    for index in y:\n        if(index == i):\n            count +=1\n        else:\n            v.append(count)\n            count = 1\n            i +=1\n    v.append(count)        \n    return v\n\ndef plotPrincipalComponents(X, dim):\n    v = getClassNumber(y_train)\n    colors = 'b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple'\n    markers = ['o', 'x' , 'v', 'd']\n    tot = len(X)\n    start = 0 \n    if(dim == 2):\n        for i,index in enumerate(v):\n            end = start + index\n            plt.scatter(X[start:end,0],X[start:end,1] , color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n            start = end\n        plt.xlabel('PC1')\n        plt.ylabel('PC2')\n    \n    if(dim == 3):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        for i,index in enumerate(v):\n            end = start + index\n            ax.scatter(X[start:end,0], X[start:end,1], X[start:end,2], color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n            start = end\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_zlabel('PC3')\n\n\n    plt.legend(loc='lower left')\n    plt.xticks()\n    plt.yticks()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = unique_labels(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=fruits, yticklabels=fruits,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return cm,ax","e57cbe12":"pca = PCA(n_components=2)\ndataIn2D = pca.fit_transform(X_train)\nplotPrincipalComponents(dataIn2D, 2)","629cdeb8":"pca = PCA(n_components=3)\ndataIn3D = pca.fit_transform(X_train)\nplotPrincipalComponents(dataIn3D, 3)","58efce18":"def showPCA(image,X2, X10, X50):\n    fig = plt.figure(figsize=(15,15))\n    ax1 = fig.add_subplot(1,4,1)\n    ax1.axis('off')\n    ax1.set_title('Original image')\n    plt.imshow(image)\n    ax1 = fig.add_subplot(1,4,2)\n    ax1.axis('off') \n    ax1.set_title('50 PC')\n    plt.imshow(X50)\n    ax1 = fig.add_subplot(1,4,3)\n    ax1.axis('off') \n    ax1.set_title('10 PC')\n    plt.imshow(X10)\n    ax2 = fig.add_subplot(1,4,4)\n    ax2.axis('off') \n    ax2.set_title('2 PC')\n    plt.imshow(X2)\n    plt.show()\n\ndef computePCA(n, im_scaled, image_id):\n    pca = PCA(n)\n    principalComponents = pca.fit_transform(im_scaled)\n    im_reduced = pca.inverse_transform(principalComponents)\n    newImage = scaler.inverse_transform(im_reduced[image_id])\n    return newImage\n\ndef showVariance(X_train):\n    #Compute manually the principal components\n    cov_matr=np.dot(X_train, X_train.T)\n    eigval,eigvect=np.linalg.eig(cov_matr)\n\n    index=np.argsort(eigval)[::-1] #take in order the index of ordered vector (ascending order)\n\n    #eigvect[:,i] is associated to eigval[i] so \n    eigvect=eigvect[:,index]\n    eigval=eigval[index]\n\n    n_PC=[]\n    var_explained=[]\n    var_temp=[]\n    var_tmp=0\n    for i in range(10):\n        var_tmp=var_tmp+eigval[i]\n        n_PC.append(i)\n        var_temp.append(eigval[i]\/(eigval.sum())*100)\n        var_explained.append(var_tmp\/(eigval.sum())*100)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n\n    ind = np.arange(10)    \n    width = 0.35         # the width of the bars\n    p1 = ax.bar(ind, var_temp, width, color='b')\n    p2 = ax.bar(ind + width, var_explained, width, color='r')\n\n    ax.legend((p1[0], p2[0]), ('Individual explained variance', 'Cumulative explained variance'))\n\n    ax.set_title('Variance explained using PCs')\n    ax.set_xticks(ind + width \/ 2)\n    ax.set_xticklabels(('1', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n\n    plt.xlabel('Number of PC')\n    plt.ylabel('Variance exaplained in %')\n\n    ax.autoscale_view()\n\n    plt.show()","fe77fb00":"image_id = 2\nimage = X_t[image_id]\n\n#Compute PCA\nX_2 = computePCA(2, X_train,image_id)\nX_10 = computePCA(10, X_train,image_id)\nX_50 = computePCA(50, X_train,image_id)\n\n#Reshape in order to plot images\nX2 = np.reshape(X_2, (dim,dim,3)).astype(int)\nX10 = np.reshape(X_10, (dim,dim,3)).astype(int)\nX50 = np.reshape(X_50, (dim,dim,3)).astype(int)\n\n#Plot\nshowPCA(image, X2, X10, X50)","fed3eb3f":"showVariance(X_train)","5cecf280":"svm = SVC(gamma='auto', kernel='linear', probability=True)\nsvm.fit(X_train, y_train) \ny_pred = svm.predict(X_test)\n\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with SVM: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred,classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = svm.predict_proba(X_test)\nprobs = probs[:, 1]\nsvm_fpr, svm_tpr, thresholds = metrics.roc_curve(y_test, probs)\nsvm_auc = metrics.roc_auc_score(y_test, probs)\n","e87307b0":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = knn.predict_proba(X_test)\nprobs = probs[:, 1]\nknn_fpr, knn_tpr, thresholds = metrics.roc_curve(y_test, probs)\nknn_auc = metrics.roc_auc_score(y_test, probs)\n","8920eb66":"#CHANGING VALUES OF N\naccuracy_train = []\naccuracy_test = []\n\nfor i in range(1,15):   #check all possible values for 1 to 15\n    k_nn = KNeighborsClassifier(n_neighbors=i)\n    k_nn.fit(X_train,y_train)\n    pred_i = k_nn.predict(X_test)\n    accuracy_train.append(k_nn.score(X_train,y_train)*100)\n    accuracy_test.append(k_nn.score(X_test,y_test)*100)\n    \naccuracy_train_array=np.asarray(accuracy_train)\naccuracy_test_array=np.asarray(accuracy_test)\n    \nplt.figure(figsize=(10,6))\nplt.plot(range(1,15),accuracy_train_array, label='Training_Accuracy', color='green')\nplt.plot(range(1,15),accuracy_test_array, label='Testing_Accuracy', color='red')\nplt.legend()\nplt.title('Accuracy vs K value')\nplt.xlabel('K')\nplt.ylabel('Accuracy%')\n\nplt.show()","0813a549":"tree = DecisionTreeClassifier()\ntree = tree.fit(X_train,y_train)\ny_pred = tree.predict(X_test)\n\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = tree.predict_proba(X_test)\nprobs = probs[:, 1]\ntree_fpr, tree_tpr, thresholds = metrics.roc_curve(y_test, probs)\ntree_auc = metrics.roc_auc_score(y_test, probs)","1a3278bf":"# CHANGING MAX_DEPTH\nscore_train=[]\nscore_test=[]\n\nfor i in range(1,10):\n    dtree_md = DecisionTreeClassifier(max_depth=i)\n    dtree_md.fit(X_train,y_train)\n    \n    score_train.append(dtree_md.score(X_train,y_train)*100)\n    score_test.append(dtree_md.score(X_test,y_test)*100)\n    \nscore_train_array=np.asarray(score_train)\nscore_test_array=np.asarray(score_test)\nplt.figure(figsize=(10,6))\nplt.plot(range(1,10),score_train_array,color='green', label=\"Training_accuracy\")\nplt.plot(range(1,10),score_test_array,color='red',label=\"Testing_accuracy\")\n\nplt.legend()\nplt.xlabel('max_depth')\nplt.ylabel('Accuracy%')\nplt.show()","d03633be":"#ROC CURVE\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(svm_fpr, svm_tpr, 'b', marker='.', label = 'SVM = %0.3f' % svm_auc )\nplt.plot(knn_fpr, knn_tpr, 'g', marker='.', label = 'K-NN = %0.3f' % knn_auc)\nplt.plot(tree_fpr, tree_tpr, 'r', marker='.',label = 'DECISION TREE = %.3f' % tree_auc)\nplt.legend(loc = 'lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","ace149aa":"fruits = ['Mango Green', 'Banana' , 'Strawberry','Tangelo', 'Kiwi' ,'Orange', 'Lemon', 'Cocos' , 'Pineapple' , 'Peach', 'Mandarine']\n#fruits = getAllFruits() #Be sure to have enough free memory\n\n#Get Images and Labels\nX, y =  getYourFruits(fruits, 'Training')\nX_test, y_test = getYourFruits(fruits, 'Test')\n\n#Scale Data Images\nscaler = StandardScaler()\nX_train = scaler.fit_transform([i.flatten() for i in X])\nX_test = scaler.fit_transform([i.flatten() for i in X_test])","83471038":"#SVM\nmodel = SVC(gamma='auto', kernel='linear')\nmodel.fit(X_train, y) \ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with SVM: {0:.2f}%\".format(precision))\n\n#K-NN\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y)\ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\n\n#DECISION TREE\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train,y)\ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))","46f2964f":"**in lsat execution  it is taking to much time so we stop it \n**\n ","0bfb4f6e":"### DATA IN 2D","838db578":"## PCA EXAMPLE","b2a7aa32":"# VISUALIZATION OF DATA\nLet's see now how one of our samples appears","9da0a212":"#### LINEAR SVM","743d1acc":"# DECISION TREE\n","fbe9e932":"## VARIANCE EXPLAINED USING PC","06e01b5b":"### DATA IN 3D","e7ba8b2f":"# MULTI-CLASS CLASSIFICATION "}}