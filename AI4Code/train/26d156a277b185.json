{"cell_type":{"f2c498ed":"code","781e5c62":"code","ed93bcfc":"code","65dc6021":"code","8ade1715":"code","80e55282":"code","ba1d8157":"code","88b06de7":"code","451ad9c3":"code","007d9ac7":"code","bebeb944":"code","25018b49":"code","389946cd":"code","f1fae124":"code","88faafa1":"code","8a64a66a":"code","7c75938b":"code","f8ecaff1":"code","da0d6285":"markdown","04d69b4b":"markdown","9920bdb3":"markdown","8a7153dd":"markdown","09f2844b":"markdown","57a550fb":"markdown","221da0a9":"markdown","eec76e14":"markdown","33631069":"markdown","75b6da2a":"markdown","5664a05d":"markdown","125272cf":"markdown","c11aab2b":"markdown","78211f23":"markdown","4781a105":"markdown"},"source":{"f2c498ed":"from sklearn.preprocessing import StandardScaler\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))","781e5c62":"df = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head()","ed93bcfc":"df.info()","65dc6021":"df.rename(index=str, columns={'Annual Income (k$)': 'Income',\n                              'Spending Score (1-100)': 'Score'}, inplace=True)\ndf.head()","8ade1715":"# Let's see our data in a detailed way with pairplot\nX = df.drop(['CustomerID', 'Gender'], axis=1)\nsns.pairplot(df.drop('CustomerID', axis=1), hue='Gender', aspect=1.5)\nplt.show()","80e55282":"from sklearn.cluster import KMeans\n\nclusters = []\n\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(X)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(3, 140000), xytext=(3, 50000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","ba1d8157":"# 3 cluster\nkm3 = KMeans(n_clusters=3).fit(X)\n\nX['Labels'] = km3.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 3))\nplt.title('KMeans with 3 Clusters')\nplt.show()","88b06de7":"# Let's see with 5 Clusters\nkm5 = KMeans(n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans with 5 Clusters')\nplt.show()","451ad9c3":"fig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.swarmplot(x='Labels', y='Income', data=X, ax=ax)\nax.set_title('Labels According to Annual Income')\n\nax = fig.add_subplot(122)\nsns.swarmplot(x='Labels', y='Score', data=X, ax=ax)\nax.set_title('Labels According to Scoring History')\n\nplt.show()","007d9ac7":"# Let's see with 5 Clusters\nkm5 = KMeans(init= 'k-means++',n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans++ with 5 Clusters')\nplt.show()","bebeb944":"fig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.swarmplot(x='Labels', y='Income', data=X, ax=ax)\nax.set_title('Labels According to Annual Income')\n\nax = fig.add_subplot(122)\nsns.swarmplot(x='Labels', y='Score', data=X, ax=ax)\nax.set_title('Labels According to Scoring History')\n\nplt.show()","25018b49":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='complete').fit(X)\n\nX['Labels'] = agglom.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('Agglomerative (complete linkage) with 5 Clusters')\nplt.show()","389946cd":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(X)\n\nX['Labels'] = agglom.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('Agglomerative (average linkage) with 5 Clusters')\nplt.show()","f1fae124":"from scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \n\ndist = distance_matrix(X, X)\nprint(dist)","88faafa1":"Z = hierarchy.linkage(dist, 'complete')\nplt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size=12, orientation='right')","8a64a66a":"Z = hierarchy.linkage(dist, 'average')\nplt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size =12, orientation = 'right')","7c75938b":"from sklearn.cluster import DBSCAN \n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\n\nX['Labels'] = db.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]))\nplt.title('DBSCAN with epsilon 11, min samples 6')\nplt.show()\n","f8ecaff1":"fig = plt.figure(figsize=(20,15))\n\n##### KMeans #####\nax = fig.add_subplot(221)\n\nkm5 = KMeans(n_clusters=5).fit(X)\nX['Labels'] = km5.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('KMeans with 5 Clusters')\n\n#### KMeans++ #####\nax = fig.add_subplot(222)\nkm5 = KMeans(init= 'k-means++',n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), ax=ax, s=60)\nax.set_title('KMeans++ with 5 Clusters')\n\n\n##### Agglomerative Clustering #####\nax = fig.add_subplot(223)\n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(X)\nX['Labels'] = agglom.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('Agglomerative with 5 Clusters')\n\n\n##### DBSCAN #####\nax = fig.add_subplot(224)\n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\nX['Labels'] = db.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'], s=60,\n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]), ax=ax)\nax.set_title('DBSCAN with epsilon 11, min samples 6')","da0d6285":"### Elbow method to find suitable number of clusters","04d69b4b":"## Visual plots of the four algorithms\n","9920bdb3":"## K-Means++\n\nLet's do the same with K-Means++ for 5 clusters","8a7153dd":"## Density Based Clustering (DBSCAN)\n\nMost of the traditional clustering techniques, such as k-means, hierarchical can be used to group data without supervision. \n\nHowever, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor.\nAdditionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nIn this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n\n### Modeling\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster.","09f2844b":"# Mall Customer Segmentation Data\n\nWe are going to experiment using these clustering methods: \n\n1. K-Means <br>\n2. K-Means++ <br>\n2. Hierarchical <br>\n3. DBSCAN","57a550fb":"## K-Means ","221da0a9":"## Hierarchical Clustering\n\n## Agglomerative\n\nWe will be looking at a clustering technique, which is <b>Agglomerative Hierarchical Clustering<\/b>. Agglomerative is the bottom up approach which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n\nThe <b> Agglomerative Clustering <\/b> class will require two inputs:\n<ul>\n    <li> <b>n_clusters<\/b>: The number of clusters to form as well as the number of centroids to generate. <\/li>\n    <li> <b>linkage<\/b>: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. <\/li>\n    <ul> \n        <li> Values experimented: 'complete' and 'average' <\/li> \n    <\/ul>\n<\/ul>","eec76e14":"We observe that k-means and agglomerative clustering performed better than DBSCAN for this dataset","33631069":"From the above plot we see that gender has no direct relation to segmenting customers. That's why we can drop it and move on with other features which is why we will X parameter from now on.","75b6da2a":"As we can see DBSCAN doesn't perform very well because the density in our data is not that strong. Label -1 means outliers so it will appear most as outliers. We may have performed better if we had had a bigger data.","5664a05d":"\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nA <b>distance matrix<\/b> contains the <b> distance from each point to every other point of a dataset <\/b>.","125272cf":"We used __complete__ linkage for our case, let's change it to __average__ linkage to see how the dendogram changes.","c11aab2b":"Elbow method tells us to select the cluster when there is a significant change in inertia. As we can see from the graph, we can say this may be either 3 or 5. Let's see both results in graph and decide.\n\n###  Creating the Visual Plots for 3 and 5 clusters","78211f23":"A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where customers are viewed as singleton clusters. \nBy moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. ","4781a105":"By judging from the plots we could say that 5 cluster seems better than the 3 ones. As this is a unsupervised problem we can't really know for sure which one is the best in real life but by looking at the data it's safe to say that 5 would be our choice. \n\nThe 5 clusters are observed:\n\n1) low income and high spending  \n2) high income and low spending  \n3) high income and high spending  \n4) low income and low spending  \n5) mid income and mid spending  \n\n\nAlso let's see the 5 cluster more clearly with swarmplot:"}}