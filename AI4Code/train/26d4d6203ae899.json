{"cell_type":{"8c157270":"code","a16f67b9":"code","ff1bc845":"code","2bbe7998":"code","80668f8f":"code","e5e09304":"code","ae752553":"code","f483f983":"code","41731cb6":"code","e188930d":"code","da098fc7":"code","307f8f26":"code","bb70828e":"code","6aba40d5":"code","dd8d4e2f":"markdown","b89c9228":"markdown","8d97b63b":"markdown","b4d440f5":"markdown","0a3f5c47":"markdown","5112b77c":"markdown"},"source":{"8c157270":"import os\nimport unicodedata\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport sklearn as sl\n\ndf = pd.read_csv(\"..\/input\/medium-articles-dataset\/medium_data.csv\")\n\ndef normalize_text(text : str) -> str:\n    \"\"\" Normalize the unicode string\n        :param text: text data\n        :retrns clean_text: clean text\n    \"\"\"\n    if text != np.nan:\n        clean_text = unicodedata.normalize(\"NFKD\",text)\n    else:\n        clean_text = text\n    return clean_text\n\ndef create_wc(text : str) -> int:\n    \"\"\" Count words in a text\n        :param text: String to check the len\n        :retirns wc: Word count\n    \"\"\"\n    wc = 0\n    norm_text = text.lower()\n    wc = len(norm_text.split(\" \"))\n    return wc\n\ndf['clean_title'] = df.title.apply(lambda x: normalize_text(x) if x!= np.nan else x)\ndf['clean_subtitle'] = df.subtitle.apply(lambda x: normalize_text(x) if x!= np.nan and type(x) == str else x)\ndf['title_wc'] = df.title.apply(lambda x: create_wc(x) if x!= np.nan else 0)\ndf['subtitle_wc'] = df.subtitle.apply(lambda x: create_wc(x) if x!= np.nan and type(x) == str else 0)","a16f67b9":"df.head()","ff1bc845":"df.info()","2bbe7998":"print(\"Value Counts for each publication:\")\nprint(df.publication.value_counts())\n\nsns.set(style=\"ticks\", color_codes=True)\nplt.style.use('fivethirtyeight')\n\ng = sns.catplot(x=\"claps\", y=\"publication\", kind=\"box\", data=df, order=df.publication.value_counts().iloc[:25].index)\ng.set(xlim=(-25, 2500))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,250,500,750,1000,1250,1500,1750,2000,2250,2500], minor=True)\nplt.title(\"Claps vs. Publication - Ordered by Most Common Publications\")\nplt.show()\n\ndf_ordered = df.groupby(\"publication\").median().sort_values(by = 'claps', ascending=False)\n\ng = sns.catplot(x=\"claps\", y=\"publication\", kind=\"box\", data=df, order=df_ordered.iloc[:25].index)\ng.set(xlim=(-25, 2500))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,250,500,750,1000,1250,1500,1750,2000,2250,2500], minor=True)\nplt.title(\"Claps vs. Publication - Ordered by Median Claps\")\nplt.show()","80668f8f":"plt.figure(figsize=(30,10))\ng = sns.scatterplot(x=\"claps\", y=\"reading_time\", data=df, legend='full')\ng.set(xlim=(-25, 1050))\nplt.title(\"Claps vs. Reading Time\")\nplt.show()","e5e09304":"print(\"Value Counts for each reading time:\")\nprint(df.reading_time.value_counts())\nprint('\\n')\n\nplt.style.use('fivethirtyeight')\n\ndef bin_time(row):\n    read_time = row['reading_time']\n    if read_time <= 3:\n        return 'Very Short (0-3 minutes)'\n    elif read_time > 3 and read_time <= 7:\n        return 'Short (4-7 minutes)'\n    elif read_time > 7 and read_time <= 12:\n        return 'Substantial (8-12 minutes)'\n    elif read_time > 12:\n        return 'Long (13+ minutes)'\n\ndf['reading_time_bin'] = df.apply(lambda row: bin_time(row), axis=1)\nprint(\"Value Counts for each reading time bin:\")\nprint(df.reading_time_bin.value_counts())\n\n\ndf_ordered = df.groupby(\"reading_time_bin\").median().sort_values(by = 'claps', ascending=False)\n\ng = sns.catplot(x=\"claps\", y=\"reading_time_bin\", kind=\"box\", data=df, order=df_ordered.iloc[:25].index)\ng.set(xlim=(-25, 1050))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,100,200,300,400,500,600,700,800,900,1000], minor=True)\nplt.title(\"Claps vs. Reading Time - Ordered by Median Claps\")\nplt.show()\n","ae752553":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom PIL import Image\n\nprint(torch.__version__)\ntorch.cuda.is_available()","f483f983":"from tqdm.notebook import tqdm\n\ndf_img = df.dropna(subset=['image']).reset_index()\n\nimage_data = np.zeros([df_img.shape[0], 1, 15, 50], dtype=float)\nimage_labels = np.zeros([df_img.shape[0]], dtype=float)\n\nfor i in tqdm(range(df_img.shape[0])):\n    img_file = df_img.loc[i, 'image']\n    if img_file[-1] == '.':\n        img_file = img_file.strip('.')\n    img = Image.open('\/kaggle\/input\/medium-articles-dataset\/images\/' + img_file).convert('LA').resize((50,15),resample=Image.BILINEAR)\n    img_data = np.asarray(img)\n    image_data[i,0,:,:] = img_data[:,:,0]\n    num_clap = df_img.loc[i, 'claps']\n    if num_clap < 50:\n        image_labels[i] = 0\n    elif num_clap >= 50 and num_clap < 100:\n        image_labels[i] = 1\n    elif num_clap >= 100 and num_clap < 400:\n        image_labels[i] = 2\n    else:\n        image_labels[i] = 3\n    ","41731cb6":"train_cut = int(np.floor(image_data.shape[0] * 5\/7))\nval_cut = int(np.floor(image_data.shape[0] * 1\/7))\n        \nimage_data = np.float32(image_data)\nimage_labels = np.float32(image_labels)\nimage_data = image_data\/255.\ntrain_dat = image_data[0:train_cut]\ntrain_labels = image_labels[0:train_cut]\nval_dat = image_data[train_cut:train_cut + val_cut]\nval_labels = image_labels[train_cut:train_cut + val_cut]\ntest_dat = image_data[train_cut + val_cut:]\ntest_labels = image_labels[train_cut + val_cut:]","e188930d":"class MNIST_Net(nn.Module):\n    \"\"\" \n    Original Model\n    \"\"\"\n    def __init__(self,p=0.5,minimizer='Adam',step_size=0.001):\n        super(MNIST_Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d(p)\n        self.fc1 = nn.Linear(640, 64)\n        self.fc2 = nn.Linear(64, 4)\n        if minimizer == 'Adam':\n            self.optimizer = torch.optim.Adam(self.parameters(), lr = step_size)\n        else:\n            optimizer = torch.optim.SGD(self.parameters(), lr = step_size, momentum=0.9)\n         \n        self.criterion=nn.CrossEntropyLoss()\n            \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 640)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n    \n    def get_acc_and_loss(self, data, targ):\n        output = self.forward(data)\n        loss = self.criterion(output, targ)\n        pred = torch.max(output,1)[1]\n        correct = torch.eq(pred,targ).sum()\n        \n        return loss,correct\n        \n    def run_grad(self,data,targ):\n    \n        loss, correct=self.get_acc_and_loss(data,targ)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        return loss, correct","da098fc7":"def run_epoch(net,epoch,train,batch_size, num=None, ttype=\"train\"):\n    \n    error_rates_batch = []\n    epoch_indices = []\n    net.train()\n    if ttype=='train':\n        t1=time.time()\n        n=train[0].shape[0]\n        if (num is not None):\n            n=np.minimum(n,num)\n        ii=np.array(np.arange(0,n,1))\n        tr=train[0][ii]\n        y=train[1][ii]\n        train_loss=0; train_correct=0\n        with tqdm(total=len(y)) as progress_bar:\n            for j in np.arange(0,len(y),batch_size):\n                data=torch.torch.from_numpy(tr[j:j+batch_size]).to(device)\n                targ=torch.torch.from_numpy(y[j:j+batch_size]).type(torch.long).to(device)\n                loss, correct = net.run_grad(data,targ) \n                train_loss += loss.item()\n                train_correct += correct.item()\n\n                error_rates_batch.append(1 - correct.item() \/ batch_size)\n                epoch_indices.append(epoch + j\/len(y))\n                \n                progress_bar.set_postfix(loss=loss.item())\n                progress_bar.update(data.size(0))\n        train_loss \/= len(y)\n        print('\\nTraining set epoch {}: Avg. loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(epoch,\n            train_loss, train_correct, len(y),\n            100. * train_correct \/ len(y)))\n        \n    return error_rates_batch, epoch_indices\n        \n    \n","307f8f26":"def net_test(net,val,batch_size,ttype='val'):\n\n    error_rates_batch = []\n    batch_indices = []\n    net.eval()\n    with torch.no_grad():\n                test_loss = 0\n                test_correct = 0\n                vald=val[0]\n                yval=val[1]\n                for j in np.arange(0,len(yval),batch_size):\n                    data=torch.torch.from_numpy(vald[j:j+batch_size]).to(device)\n                    targ = torch.torch.from_numpy(yval[j:j+batch_size]).type(torch.long).to(device)\n                    loss,correct=net.get_acc_and_loss(data,targ)\n\n                    test_loss += loss.item()\n                    test_correct += correct.item()\n\n                    error_rates_batch.append(1 - correct.item()\/batch_size)\n                    batch_indices.append(j\/len(yval))\n\n                test_loss \/= len(yval)\n                SSS='Validation'\n                if (ttype=='test'):\n                    SSS='Test'\n                print('\\n{} set: Avg. loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(SSS,\n                    test_loss, test_correct, len(yval),\n                    100. * test_correct \/ len(yval)))\n    return error_rates_batch, batch_indices\n","bb70828e":"import time\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\ndef create_model(net_id, model_name):\n    batch_size=64\n    step_size=.001\n    num_epochs=20\n    numtrain=50000\n    minimizer=\"Adam\"\n    dropout_p=0.5\n    dim=28\n    nchannels=1\n\n    # use GPU when possible\n    train = (train_dat, train_labels)\n    val = (val_dat, val_labels)\n    test = (test_dat, test_labels)\n    if net_id == 0:\n        net = MNIST_Net(p = dropout_p, minimizer=minimizer,step_size=step_size)\n    net.to(device)\n    #define optimizer\n\n    ers_train = []\n    eis_train = []\n    ers_val = []\n    eis_val = []\n    \n    for i in range(num_epochs):\n        [error_rates, epoch_indices] = run_epoch(net,i,train,batch_size, num=numtrain, ttype=\"train\")\n        ers_train.extend(error_rates)\n        eis_train.extend(epoch_indices)\n        [error_rates, batch_indices] = net_test(net,val,batch_size)\n        ers_val.extend(error_rates)\n        eis_val.extend([x+i for x in batch_indices])\n\n    plt.plot(eis_train, ers_train)\n    plt.title(\"Training Set\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error Rate\")\n    plt.show()\n\n    plt.plot(eis_val, ers_val)\n    plt.title(\"Validation Set\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Eror_Rate\")\n    plt.show()\n\n    net_test(net,test,batch_size,ttype='test')\n\n    #torch.save(net.state_dict(), datadir+'models\/'+model_name)\n\n    return net","6aba40d5":"my_net = create_model(0, \"original_model\")","dd8d4e2f":"## Publication vs. Claps","b89c9228":"It might be useful to bin the reading times into categories and then observe the data","8d97b63b":"# Data Cleaning + Initial Augmentation\nThanks to Jaganadh Gopinadhan (https:\/\/www.kaggle.com\/jaganadhg) for his Kernel wherein he cleaned the title and subtitle columns as well as added the title count and subtitle count columns (https:\/\/www.kaggle.com\/jaganadhg\/clapmediumclap). All code in the following block is courtesy of Jaganadh","b4d440f5":"# EDA\nFirst, let's do an overview of the data","0a3f5c47":"## Reading Time vs. Claps\n","5112b77c":"Let's see how the claps are distributed across different categories:"}}