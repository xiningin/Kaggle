{"cell_type":{"2246f565":"code","a395d572":"code","d8d09b66":"code","41e7e2bf":"code","59bfc3db":"code","2a44897c":"code","ab2407d4":"code","4eeab1f1":"code","3b7fe881":"code","ed7b0d98":"code","6a7dd13a":"code","93c11f54":"code","bd0e8991":"code","cd6896fc":"code","85f3e5fd":"code","f0807cff":"code","f6b72e76":"code","69b9b9ec":"code","8a6051b7":"code","b31de8f9":"code","ec9b56a2":"code","afbec7ff":"code","c45c87e4":"code","37754074":"code","165b98b5":"code","21d1170f":"code","b650b580":"code","ac617a6a":"code","db957914":"markdown","2352fc36":"markdown","bd559959":"markdown","6378ee71":"markdown","28188d05":"markdown","379ef781":"markdown","9df6fd89":"markdown","15b3852d":"markdown"},"source":{"2246f565":"!pip install imutils","a395d572":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport random \nimport cv2\nimport imutils\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras import backend as K\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization","d8d09b66":"dir = \"..\/input\/handwritten-characters\/Train\/\"\ntrain_data = []\nimg_size = 32\nnon_chars = [\"#\",\"$\",\"&\",\"@\"]\nfor i in os.listdir(dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 4000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        train_data.append([img,i])","41e7e2bf":"len(train_data)","59bfc3db":"val_dir = \"..\/input\/handwritten-characters\/Validation\/\"\nval_data = []\nimg_size = 32\nfor i in os.listdir(val_dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(val_dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 1000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        val_data.append([img,i])","2a44897c":"len(val_data)","ab2407d4":"random.shuffle(train_data)\nrandom.shuffle(val_data)","4eeab1f1":"train_X = []\ntrain_Y = []\nfor features,label in train_data:\n    train_X.append(features)\n    train_Y.append(label)","3b7fe881":"val_X = []\nval_Y = []\nfor features,label in val_data:\n    val_X.append(features)\n    val_Y.append(label)","ed7b0d98":"LB = LabelBinarizer()\ntrain_Y = LB.fit_transform(train_Y)\nval_Y = LB.fit_transform(val_Y)","6a7dd13a":"train_X = np.array(train_X)\/255.0\ntrain_X = train_X.reshape(-1,32,32,1)\ntrain_Y = np.array(train_Y)","93c11f54":"val_X = np.array(val_X)\/255.0\nval_X = val_X.reshape(-1,32,32,1)\nval_Y = np.array(val_Y)","bd0e8991":"print(train_X.shape,val_X.shape)","cd6896fc":"print(train_Y.shape,val_Y.shape)","85f3e5fd":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(32,32,1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(35, activation='softmax'))","f0807cff":"model.summary()","f6b72e76":"model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])","69b9b9ec":"history = model.fit(train_X,train_Y, epochs=50, batch_size=32, validation_data = (val_X, val_Y),  verbose=1)","8a6051b7":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","b31de8f9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","ec9b56a2":"def sort_contours(cnts, method=\"left-to-right\"):\n    reverse = False\n    i = 0\n    if method == \"right-to-left\" or method == \"bottom-to-top\":\n        reverse = True\n    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n        i = 1\n    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n    key=lambda b:b[1][i], reverse=reverse))\n    # return the list of sorted contours and bounding boxes\n    return (cnts, boundingBoxes)","afbec7ff":"def get_letters(img):\n    letters = []\n    image = cv2.imread(img)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n    dilated = cv2.dilate(thresh1, None, iterations=2)\n\n    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n    # loop over the contours\n    for c in cnts:\n        if cv2.contourArea(c) > 10:\n            (x, y, w, h) = cv2.boundingRect(c)\n            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        roi = gray[y:y + h, x:x + w]\n        thresh = cv2.threshold(roi, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n        thresh = cv2.resize(thresh, (32, 32), interpolation = cv2.INTER_CUBIC)\n        thresh = thresh.astype(\"float32\") \/ 255.0\n        thresh = np.expand_dims(thresh, axis=-1)\n        thresh = thresh.reshape(1,32,32,1)\n        ypred = model.predict(thresh)\n        ypred = LB.inverse_transform(ypred)\n        [x] = ypred\n        letters.append(x)\n    return letters, image\n\n#plt.imshow(image)","c45c87e4":"def get_word(letter):\n    word = \"\".join(letter)\n    return word","37754074":"letter,image = get_letters(\"..\/input\/handwriting-recognition\/train_v2\/train\/TRAIN_00003.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","165b98b5":"letter,image = get_letters(\"..\/input\/handwriting-recognition\/train_v2\/train\/TRAIN_00023.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","21d1170f":"letter,image = get_letters(\"..\/input\/handwriting-recognition\/train_v2\/train\/TRAIN_00030.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","b650b580":"letter,image = get_letters(\"..\/input\/handwriting-recognition\/validation_v2\/validation\/VALIDATION_0005.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","ac617a6a":"letter,image = get_letters(\"..\/input\/handwriting-recognition\/test_v2\/test\/TEST_0007.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","db957914":"If you liked this notebook, then do **Upvote** as it will keep me motivated in creating such kernels ahead. **Thanks!!**","2352fc36":"## Conclusion \nThis notebook is an illustration of how a character segmentation and classification approach can be used for offline handwritten text extraction. In order to improve the model, the model should be trained on the complete dataset, this notebook was trained on slightly less number of images due to session constraints. Also, for applying this method to a complete paragraph, following approach can be used, **line segmentation >> word segmentation >> character segmentation >> classification >> post-processing**. ","bd559959":"# *Offline Handwritten Text Recognition*\n\n### The purpose of this notebook is to give a brief idea and a basic approach for offline handwritten text recognition by using segmentation and classification. ","6378ee71":"## References\n1. [https:\/\/www.pyimagesearch.com\/2020\/08\/24\/ocr-handwriting-recognition-with-opencv-keras-and-tensorflow\/](http:\/\/) \n2. [https:\/\/www.pyimagesearch.com\/2015\/04\/20\/sorting-contours-using-python-and-opencv\/](http:\/\/)","28188d05":"## What is Offline Handwritten Text Recognition?\nOffline handwriting recognition involves the automatic conversion of text in an image into letter codes that are usable within computer and text-processing applications. In simple terms, it is the text extraction from your handwritten notebooks\/pages. Why called offline? The point being that there is an online text recognition system, which is referred for text that is digitally generated by using tools like stylus, apple pencil, etc.","379ef781":"## Drawbacks\n1. The recognition part is dependent on the contour detection code, so if the opencv library is not able to find the character contour, then this method will fail.\n2. There could be a lot of variation in a single handwritten letter in terms of writing style, therefore a lot more examples are needed for training this model.\n3. This model will not work for connected texts like a cursive handwritten word.","9df6fd89":"## Approach\n\n* **Step1** :  Build a digit(0-9) + A-Z characters classifier using a CNN architecture.\n* **Step2** :  Apply character segmentation for the handwritten word image.\n* **Step3** :  Classify each segmented letter and then get the final word in the image.","15b3852d":"## Recognition and Post-Processing \n1. The sort contours function is used to get the correct order of individual characters for correct output extraction. In this case for extracting a single word, a left to right sorting of individual characters is needed.\n2. The get letters function fetches the list of letters and get word function gets the individual word. "}}