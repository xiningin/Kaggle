{"cell_type":{"69ef7f7e":"code","4e4be3a9":"code","755c4dcb":"code","9484c3d9":"code","f6915d3e":"code","242b5067":"code","197ccbf8":"code","2da0a2b1":"code","f1395c61":"code","7f006a07":"code","79485cd9":"code","a68c494c":"code","6ddca04e":"code","eb87a92b":"code","a7411016":"code","490f0c92":"code","1d22ef34":"code","d1aaaa93":"code","e148bc51":"code","1912e83b":"code","1cca1e49":"code","60169b5a":"code","3c6415e2":"code","9c1b5e95":"code","5c7b20fa":"code","6f710a94":"code","f8022579":"code","88159945":"code","9af30784":"code","4406ad1f":"code","e1423e59":"code","bd7cbe07":"code","a3a0fa56":"code","d1a8740d":"code","11f42453":"code","5c8680ad":"code","2f75d975":"code","9065dca8":"code","65e06be7":"code","750020ba":"code","73acbda7":"code","5a7e7d69":"code","133629b6":"code","34642357":"code","2945ef01":"code","bca076c4":"code","ab8d0703":"code","c9a992a4":"code","8d3e201d":"code","29fa1aec":"code","8ff0fc9c":"code","e3f70acb":"code","fe36369b":"code","27496cba":"code","da79cac2":"code","f539bf0e":"code","b6227701":"code","93344652":"code","02d6042c":"code","08d928fb":"code","d954d06e":"code","14b7e36b":"code","e856b3d4":"code","45ec343e":"code","b7241060":"code","93833312":"code","6cdb3a83":"code","0656c565":"code","b539ea13":"code","0b13b74a":"code","76abdc86":"code","8f55bf5e":"code","a362a872":"code","9bde0ab6":"code","fb2a01f1":"code","a2d75907":"code","7a66e77e":"code","7edc437c":"code","b96ac9ad":"code","f2a35cb9":"code","5c8056bb":"code","9f953a2f":"code","220d9000":"code","c2886b0a":"code","cec4bc55":"code","65e8c80d":"code","e9fd8e41":"code","d11fd513":"code","4137be92":"code","2928dadf":"code","32ebc946":"code","2e276e84":"code","39f4198d":"code","282ab3cf":"code","456320f8":"code","2bb337ca":"code","ce3e6e06":"code","768251f7":"code","7b366726":"code","2fcc87fa":"code","7e4218f2":"code","d1a5e551":"code","673179dd":"code","a8bea23d":"code","1002e1c0":"code","2fe0557d":"code","d1ba6bcf":"code","31160c34":"code","7db0bf2f":"code","74521f8c":"code","c300b921":"code","df63562b":"code","4edf6ea3":"code","c943c5da":"code","8d12dc72":"code","a9e27fdc":"code","0a6af3ce":"code","938b597e":"code","8fb57bc7":"code","f3f41385":"code","70ebbc93":"code","c580c242":"code","34886e9b":"code","16fcb00e":"code","d47b0abe":"code","9551a3f7":"code","a4c7a86a":"code","b5c2058c":"code","51ec660a":"code","9b62603f":"code","3f05043d":"code","27ec6de9":"code","ff4663fe":"code","e34ed411":"code","d2286687":"code","cc396809":"code","09ba303a":"code","5f8aba08":"code","156620b7":"code","00cc593a":"code","19ec3885":"code","60b92a16":"code","16b75c7f":"code","e48cdecf":"code","30928ffe":"code","cc91ecf5":"code","ddf8b0cb":"code","e71f9e04":"code","f93ccc11":"code","c13aa097":"code","fd835ccf":"code","7f8eea90":"code","bd2147be":"code","3bfc5dcb":"code","0e909029":"code","6adda89a":"code","a770f3dd":"code","b099c069":"code","51f6ccfb":"code","bc641b5e":"code","162484ba":"code","d4542a6f":"code","2df79564":"code","f9509554":"code","bdfc6753":"code","e8b9f22f":"code","1fe8ced3":"code","62085275":"code","987fc311":"code","854861e4":"code","1848c3f2":"code","36af42ee":"code","8ad87933":"code","26277b09":"code","f1676dd0":"code","a7d50ff8":"code","ebe2f1bc":"code","f46e66e6":"code","d03b1c21":"code","9b3cd0e7":"code","ab2581be":"markdown","97efc6e5":"markdown","9370d3d9":"markdown","ebd27c34":"markdown","edf225ea":"markdown","3a671ade":"markdown","9f13377c":"markdown","825ce803":"markdown","eb74095f":"markdown","a887f682":"markdown","06476f73":"markdown","e0b72d37":"markdown","2a5b22ef":"markdown","4d442e2a":"markdown","8a11c04b":"markdown","8f658967":"markdown","4494ad33":"markdown","0da5b8dd":"markdown","81815cc3":"markdown","cb6eb451":"markdown"},"source":{"69ef7f7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4e4be3a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(25,25))\n\nimport pandas_profiling as pp\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.57\nimport fastai\n\nfrom fastai import *\nfrom fastai.tabular import *\n\n# from torchvision.models import *\n# import pretrainedmodels\n\n# from utils import *\nimport sys\n\nfrom fastai.callbacks.hooks import *\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n!pip3 install catboost\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\n\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom hyperopt import STATUS_OK\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')","755c4dcb":"df1=pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\ndf2=pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv')\ndf3=pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')","9484c3d9":"df1.head(3)","f6915d3e":"df2.head(3)","242b5067":"df3.head(3)","197ccbf8":"df2 = df2[df2.id!='1997-08-20']\ndf2 = df2[df2.id!='2012-09-29']\ndf2 = df2[df2.id!='2014-01-01']","2da0a2b1":"df2['id'] = df2['id'].astype(int)","f1395c61":"df2 = df2[df2['original_language']=='en']\ndf2.shape","7f006a07":"df2=df2.merge(df1, on='id')\ndf2=df2.merge(df3, on='id')","79485cd9":"df2.shape","a68c494c":"df2 = df2.dropna(subset=['budget','revenue', 'poster_path', 'genres'], axis=0)\ndf2.shape","6ddca04e":"df2 = df2[(df2[['budget','revenue', 'poster_path', 'genres']] != 0).all(axis=1)]\ndf2.shape","eb87a92b":"df2.head(3)","a7411016":"C= df2['vote_average'].mean()\nC","490f0c92":"m= df2['vote_count'].quantile(0.9)\nm","1d22ef34":"q_movies = df2.copy().loc[df2['vote_count'] >= m]\nq_movies.shape","d1aaaa93":"# IMDB formula\ndef weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # Calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","e148bc51":"q_movies['score'] = q_movies.apply(weighted_rating, axis=1)","1912e83b":"#Sort movies based on score calculated above\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#Print the top 15 movies\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)","1cca1e49":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng = sns.barplot(x='title', y = 'score', data=q_movies.head(50))\ng.set_title(\"Movies and Scores\", fontsize=22)\ng.set_xlabel(\"Movies\", fontsize=18)\ng.set_ylabel('Score', fontsize=18)\nplt.show()","60169b5a":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y='score',  data=q_movies.head(50))\nplt.legend(title='Movies and Score', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y='runtime', data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"runtime\", fontsize=17)\ng1.set_title(\"Movie Score Runtime Wise\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","3c6415e2":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y='score',  data=q_movies.head(50))\nplt.legend(title='Movies and Score', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['budget'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"Budget\", fontsize=17)\ng1.set_title(\"Movie Score Budget Wise\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","9c1b5e95":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y='score',  data=q_movies.head(50))\nplt.legend(title='Movies and Score', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['revenue'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"Revenue\", fontsize=17)\ng1.set_title(\"Movie Score Revenue Wise\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","5c7b20fa":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['revenue'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Revenues vs Budget', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['budget'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"budget\", fontsize=17)\ng1.set_title(\"Movie Revenue vs Budget\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"revenue\", fontsize=17)\nplt.show()","6f710a94":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['score'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Score vs Popularity', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['popularity'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"popularity\", fontsize=17)\ng1.set_title(\"Movie Score vs Popularity\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","f8022579":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['score'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Score vs Votes', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['vote_count'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"votes\", fontsize=17)\ng1.set_title(\"Movie Score vs Votes\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","88159945":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['score'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Score vs VoteAvg', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['vote_average'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"votes\", fontsize=17)\ng1.set_title(\"Movie Score vs VoteAvg\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Score\", fontsize=17)\nplt.show()","9af30784":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['vote_count'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Vote count vs Vote avg', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['vote_average'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"vote-avg\", fontsize=17)\ng1.set_title(\"Movie Votes \/ Count\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"vote-count\", fontsize=17)\nplt.show()","4406ad1f":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\ng1 = sns.barplot(x='title', y=q_movies['runtime'].astype(float),  data=q_movies.head(50))\nplt.legend(title='Movies Runtime vs Budget', loc='best')\ngt = g1.twinx()\ngt = sns.pointplot(x='title', y=q_movies['budget'].astype(float), data=q_movies.head(50), color='black', legend=False, ci=70, scale=0.5)\ngt.set_ylabel(\"budget\", fontsize=17)\ng1.set_title(\"Movie Runtime vs Budget\", fontsize=19)\ng1.set_xlabel(\"Movie\", fontsize=17)\ng1.set_ylabel(\"Runtime\", fontsize=17)\nplt.show()","e1423e59":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = df2.overview.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","bd7cbe07":"df_rating = pd.read_csv(\"..\/input\/the-movies-dataset\/ratings_small.csv\", parse_dates=['timestamp'])\ndf_rating.head()","a3a0fa56":"df_movie = df2[['id', 'title', 'genres']]\ndf_movie.rename(columns = {'id':\"movieId\"}, inplace=True)\n\nprint(\"df shape before removing NaN:\".ljust(15), df_movie.shape)\ndf_movie.dropna(inplace=True)\nprint(\"df shape after removing NaN:\".ljust(15), df_movie.shape)","d1a8740d":"df_movie.head()","11f42453":"R_df = df_rating.pivot(index = 'userId', columns='movieId', values='rating').fillna(0)\nR_df.head()","5c8680ad":"R = R_df.as_matrix()\nuser_ratings_mean = np.mean(R, axis = 1)\nR_demeaned = R - user_ratings_mean.reshape(-1, 1)","2f75d975":"from scipy.sparse.linalg import svds\nU, sigma, Vt = svds(R_demeaned, k = 50)","9065dca8":"sigma = np.diag(sigma)","65e06be7":"all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\npreds_df = pd.DataFrame(all_user_predicted_ratings, columns = R_df.columns)","750020ba":"preds_df.head()","73acbda7":"def recommend_movies(predictions_df, userId, movies_df, original_ratings_df, num_recommendations=5):\n    \n    # Get and sort the user's predictions\n    user_row_number = userId - 1 # UserID starts at 1, not 0\n    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)\n    \n    # Get the user's data and merge in the movie information.\n    user_data = original_ratings_df[original_ratings_df.userId == (userId)]\n    user_full = (user_data.merge(df_movie, how = 'left', left_on = 'movieId', right_on = 'movieId').\n                     sort_values(['rating'], ascending=False)\n                 )\n\n    print ('User {0} has already rated {1} movies.'.format(userId, user_full.shape[0]))\n    print ('Recommending the highest {0} predicted ratings movies not already rated.'.format(num_recommendations))\n    \n    # Recommend the highest predicted rating movies that the user hasn't seen yet.\n    recommendations = (df_movie[~df_movie['movieId'].isin(user_full['movieId'])].\n         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n               left_on = 'movieId',\n               right_on = 'movieId').\n         rename(columns = {user_row_number: 'Predictions'}).\n         sort_values('Predictions', ascending = False).\n                       iloc[:num_recommendations, :-1]\n                      )\n\n    return user_full, recommendations\n\nalready_rated, predictions = recommend_movies(preds_df, 8, df_movie, df_rating, 5)","5a7e7d69":"already_rated.dropna().head(5)","133629b6":"predictions","34642357":"df2.overview[9]","2945ef01":"print('dataset shape before removing NaN:', df2.shape)\ndf2.dropna(subset = ['overview', 'release_date', 'production_companies'], inplace=True)\nprint('dataset shape after removing NaN:', df2.shape)","bca076c4":"df2.head(2)","ab8d0703":"from ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres', 'production_companies']\nfor feature in features:\n    df2[feature] = df2[feature].apply(literal_eval)","c9a992a4":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","8d3e201d":"def get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing\/malformed data\n    return []","29fa1aec":"df2['director'] = df2['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres', 'production_companies']\nfor feature in features:\n    df2[feature] = df2[feature].apply(get_list)","8ff0fc9c":"df2[['title', 'cast', 'director', 'keywords', 'genres', 'production_companies']].head(3)","e3f70acb":"def clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x)\n        else:\n            return ''","fe36369b":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres', 'production_companies']\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(clean_data)","27496cba":"def create_soup1(x):\n    return ' '.join(x['keywords'])  + ' ' + ' '.join(x['genres'])\n\ndf2['soup1'] = df2.apply(create_soup1, axis=1)","da79cac2":"def create_soup2(x):\n    return  ' '.join(x['cast']) + ' ' + x['director']  + ' ' + ' '.join(x['production_companies'])\n\ndf2['soup2'] = df2.apply(create_soup2, axis=1)","f539bf0e":"df2.head(3)","b6227701":"df2[['title', 'overview', 'soup1', 'soup2']].head(4)","93344652":"df2['score'] = df2.apply(weighted_rating, axis=1)\ndf2['score'].min(), df2['score'].max()","02d6042c":"df2['score_rank'] = df2.score.apply(lambda x: \"bad_movie\" if x <=7 else \"good_movie\")\ndf2['vote_avg_rank'] = df2.vote_average.apply(lambda x: \"low_average\" if x <=7 else \"high_average\")","08d928fb":"df2.score_rank.value_counts(normalize=True)","d954d06e":"df2.vote_avg_rank.value_counts(normalize=True)","14b7e36b":"df2.loc[df2['score'].idxmax()]['title']","e856b3d4":"df2.loc[df2['score'].idxmin()]['title']","45ec343e":"df2.sort_values(['score'],ascending=False)['title'][:50]","b7241060":"df2.sort_values(['vote_average'],ascending=False)['title'][:50]","93833312":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\n\nsns.distplot(df2['score'], hist=True, kde=True, \n             bins=int(180\/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})","6cdb3a83":"df2['budget'] = df2['budget'].astype(float)\ndf2['revenue'] = df2['revenue'].astype(float)\ndf2['runtime'] = df2['runtime'].astype(float)\ndf2['popularity'] = df2['popularity'].astype(float)\ndf2['score'] = df2['score'].astype(float)\ndf2['vote_average'] = df2['vote_average'].astype(float)\ndf2['vote_count'] = df2['vote_count'].astype(float)","0656c565":"df2['release_date'] = pd.to_datetime(df2['release_date'])","b539ea13":"df2['hit_flop'] = np.where((df2['revenue']\/df2['budget']) >1.0 , 'hit', 'flop')\n                           \ndf2.hit_flop.value_counts()","0b13b74a":"a = pd.Series([item for sublist in df2.genres for item in sublist])\ndf_genres = a.groupby(a).size().rename_axis('genres').reset_index(name='f')","76abdc86":"df_genres","8f55bf5e":"df2.status.value_counts()","a362a872":"df2.head(2)","9bde0ab6":"df_reduce = df2[['title', 'overview', 'soup1', 'soup2', \n                 'budget', 'revenue', 'popularity', 'runtime',\n                 'adult', 'release_date', 'score', 'director', 'vote_count', 'status', 'original_language', 'score_rank', 'hit_flop']]","fb2a01f1":"df_reduce.head(2)","a2d75907":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.callbacks.tracker import *\nfrom fastai.text import *\nfrom fastai.tabular import *","7a66e77e":"data_lm = (TextList.from_df(df_reduce[['overview']])\n                    .split_by_rand_pct(0.1)\n                   .label_for_lm()\n                   .databunch(bs=64)\n          )","7edc437c":"data_lm.show_batch()","b96ac9ad":"data_lm.save('data_lm.pkl')","f2a35cb9":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.6).to_fp16()","5c8056bb":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","9f953a2f":"learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7))","220d9000":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(skip_end=5)","c2886b0a":"learn.fit_one_cycle(10, 1e-4, moms=(0.8,0.7))\nlearn.recorder.plot_losses()","cec4bc55":"learn.save_encoder('fine_tuned_enc')","65e8c80d":"TEXT = \"A story of \"\nN_WORDS = 20\nN_SENTENCES = 2\n\nfor temp in [0.1,0.5,1,1.5,2,2.5]: \n    print(learn.predict(TEXT, N_WORDS, temperature=temp))\n    print('-'*10)","e9fd8e41":"from fastai.callbacks import *\nauroc = AUROC()","d11fd513":"def get_val_idxs(train,n_splits=20):\n    np.random.seed(42)\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    train_idxs, valid_idxs = next(cv.split(train))\n    return train_idxs,valid_idxs","4137be92":"ymin = 1\nymax = 9.5\n\nclass scaledSigmoid(nn.Module):\n    def forward(self, input):\n        return torch.sigmoid(input) * (ymax - ymin) + ymin","2928dadf":"txt_cols=['overview', 'soup1', 'soup2', 'title', 'director']\n\ntrain_idxs,val_idxs = get_val_idxs(df_reduce[['overview', 'soup1', 'soup2', 'title', 'director', 'hit_flop']], n_splits=20)\ntrain_idxs,val_idxs\ntrain_idxs.shape,val_idxs.shape\n\ndep_var = 'hit_flop'\n\ndata_txt = (TextList.from_df(df_reduce[['overview', 'soup1', 'soup2', 'title', 'director', 'hit_flop']], \n                             cols = txt_cols, vocab=data_lm.vocab)\n                            .split_by_idx(val_idxs)\n                            .label_from_df(cols=dep_var) #,label_cls=FloatList)\n                            .databunch(bs=64))","32ebc946":"data_txt.show_batch()","2e276e84":"learn = text_classifier_learner(data_txt,AWD_LSTM,metrics=[accuracy, auroc], \n                                    loss_func=LabelSmoothingCrossEntropy())\n    \nlearn.load_encoder('fine_tuned_enc')\nlearn.model","39f4198d":"# learn.model[1].add_module(\"sSig\", module= scaledSigmoid())","282ab3cf":"learn.freeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","456320f8":"learn.freeze()\nlearn.fit_one_cycle(2, 5e-2, moms=(0.8, 0.7))","2bb337ca":"learn.save('tmp1')\n_=learn.load('tmp1')\nlearn.freeze_to(-2)\n\nlearn.fit_one_cycle(3, slice(1e-03\/(2.6**4),1e-03), moms=(0.8, 0.7))","ce3e6e06":"learn.save('tmp2')\n_=learn.load('tmp2')\nlearn.freeze_to(-3)\n\nlearn.fit_one_cycle(3, slice(1e-04\/(2.6**4),1e-04), moms=(0.8, 0.7))","768251f7":"learn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-05\/(2.6**4),1e-05), moms=(0.8, 0.7))","7b366726":"val_df = df_reduce.loc[val_idxs].copy()\nval_df.head(10)","2fcc87fa":"one_item = val_df.loc[132]\none_item.overview","7e4218f2":"learn.predict(one_item)","d1a5e551":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","673179dd":"def get_val_idxs(train,n_splits=20):\n    np.random.seed(42)\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    train_idxs, valid_idxs = next(cv.split(train))\n    return train_idxs,valid_idxs","a8bea23d":"df_reduce['year'] = df_reduce['release_date'].dt.year\ndf_reduce['month'] = df_reduce['release_date'].dt.month\ndf_reduce['day'] = df_reduce['release_date'].dt.day\ndf_reduce['weekday'] = df_reduce['release_date'].dt.weekday\ndf_reduce['half_year'] = df_reduce['month'].apply(lambda x: 'first_half' if x <=6 else 'second_half')","1002e1c0":"df_reduce.head(3)","2fe0557d":"df_g_budget = df_reduce.groupby(['year'], as_index=False)['budget'].mean()\ndf_g_budget = pd.DataFrame(df_g_budget)\ndf_g_budget.reset_index(drop=True, inplace=True)\ndf_g_budget.rename(columns={\"budget\": \"budget_avg\"}, inplace=True)\n\ndf_g_revenue = df_reduce.groupby(['year'], as_index=False)['revenue'].mean()\ndf_g_revenue = pd.DataFrame(df_g_revenue)\ndf_g_revenue.reset_index(drop=True, inplace=True)\ndf_g_revenue.rename(columns={\"revenue\": \"revenue_avg\"}, inplace=True)\n\ndf_g_revenue.head(3)","d1ba6bcf":"gc.collect()\ndf_reduce['budget_rev'] = df_reduce.year.map(df_g_budget.set_index('year').budget_avg)\ndf_reduce['revenue_rev'] = df_reduce.year.map(df_g_revenue.set_index('year').revenue_avg)","31160c34":"df_reduce['budget_2'] = np.where(df_reduce['budget']==0., df_reduce.budget_rev, df_reduce.budget)\ndf_reduce['revenue_2'] = np.where(df_reduce['revenue']==0., df_reduce.revenue_rev, df_reduce.revenue)","7db0bf2f":"df_reduce.head(5)","74521f8c":"df_reduce['budget'] = df_reduce['budget'].astype(float)\ndf_reduce['revenue'] = df_reduce['revenue'].astype(float)\ndf_reduce['budget_2'] = df_reduce['budget_2'].astype(float)\ndf_reduce['revenue_2'] = df_reduce['revenue_2'].astype(float)\ndf_reduce['budget_rev'] = df_reduce['budget_rev'].astype(float)\ndf_reduce['revenue_rev'] = df_reduce['revenue_rev'].astype(float)\ndf_reduce['runtime'] = df_reduce['runtime'].astype(float)\ndf_reduce['popularity'] = df_reduce['popularity'].astype(float)\ndf_reduce['score'] = df_reduce['score'].astype(float)\ndf_reduce['vote_count'] = df_reduce['vote_count'].astype(float)","c300b921":"df_reduce.shape","df63562b":"cat_names=['adult', 'director',  'status', 'original_language', 'year', 'month', 'day', 'weekday', 'half_year']\ncont_names= [ 'budget_2', 'revenue_2', 'runtime' ]\n\nprint(f'# of continuous feas: {len(cont_names)}')\nprint(f'# of categorical feas: {len(cat_names)}')\n\ndep_var = 'hit_flop'\n\nprocs = [FillMissing,Categorify, Normalize]\n\ntxt_cols=['overview', 'soup1', 'soup2', 'title']\nprint(txt_cols[0])\n\nlen(cat_names) + len(cont_names) + 4 + 1 + 1 + 4 + 4 == df_reduce.shape[1]","4edf6ea3":"df_reduce.shape","c943c5da":"train_idxs,val_idxs = get_val_idxs(df_reduce,n_splits=20)\ntrain_idxs,val_idxs\ntrain_idxs.shape,val_idxs.shape","8d12dc72":"def get_tabular_databunch(df_reduce,bs=100,val_idxs=val_idxs):\n    return (TabularList.from_df(df_reduce, cat_names, cont_names, procs=procs)\n                            .split_by_idx(val_idxs)\n                            .label_from_df(cols=dep_var)#,label_cls=FloatList)\n                            .databunch(bs=bs))","a9e27fdc":"def get_text_databunch(df_reduce,bs=100,val_idxs=val_idxs):\n    return (TextList.from_df(df_reduce, cols = txt_cols, vocab=data_lm.vocab)\n                            .split_by_idx(val_idxs)\n                            .label_from_df(cols=dep_var)#,label_cls=FloatList)\n                            .databunch(bs=bs))","0a6af3ce":"def get_tabular_learner(data,params,seed=42):\n    \n    return tabular_learner(data,metrics=[accuracy, auroc],loss_func=LabelSmoothingCrossEntropy(), **params)\n\ndef get_text_learner(data,params,seed=42):\n\n    learn = text_classifier_learner(data,AWD_LSTM,metrics=[accuracy, auroc], \n                                    loss_func=LabelSmoothingCrossEntropy(),**params)\n    \n    learn.load_encoder('fine_tuned_enc') \n    return learn","938b597e":"from fastai.text import *\nfrom fastai.tabular import *\n\n\nclass ConcatDataset(Dataset):\n    def __init__(self, x1, x2, y): self.x1,self.x2,self.y = x1,x2,y\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i): return (self.x1[i], self.x2[i]), self.y[i]\n    \ndef tabtext_collate(batch):\n    x,y = list(zip(*batch))\n    x1,x2 = list(zip(*x)) # x1 is (cat,cont), x2 is numericalized ids for text\n    x1 = to_data(x1)\n    x1 = list(zip(*x1))\n    x1 = torch.stack(x1[0]), torch.stack(x1[1])\n    x2, y = pad_collate(list(zip(x2, y)), pad_idx=1, pad_first=True)\n    return (x1, x2), y\n\nclass ConcatModel(nn.Module):\n    def __init__(self, mod_tab, mod_nlp, layers, drops): \n        super().__init__()\n        self.mod_tab = mod_tab\n        self.mod_nlp = mod_nlp\n        lst_layers = []\n        activs = [nn.ReLU(inplace=True),] * (len(layers)-2) + [None]\n        for n_in,n_out,p,actn in zip(layers[:-1], layers[1:], drops, activs):\n            lst_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n        self.layers = nn.Sequential(*lst_layers)\n\n    def forward(self, *x):\n        x_tab = self.mod_tab(*x[0])\n        x_nlp = self.mod_nlp(x[1])[0]\n        x = torch.cat([x_tab, x_nlp], dim=1)\n        return self.layers(x)    \n\n\n\ndef get_tabtext_learner(data,tab_learner,text_learner,lin_layers,ps):\n    tab_learner.model.layers = tab_learner.model.layers[:-2] # get rid of related output layers\n\n    text_learner.model[-1].layers =text_learner.model[-1].layers[:-3] # get rid of related output layers\n    \n    lin_layers = lin_layers+ [tab_learner.data.train_ds.c]\n    model = ConcatModel(tab_learner.model, text_learner.model, lin_layers, ps)\n    \n    loss_func = tab_learner.loss_func\n\n    # assign layer groups for gradual training (unfreezing group)\n    layer_groups = [nn.Sequential(*flatten_model(text_learner.layer_groups[0])),\n                    nn.Sequential(*flatten_model(text_learner.layer_groups[1])),\n                    nn.Sequential(*flatten_model(text_learner.layer_groups[2])),\n                    nn.Sequential(*flatten_model(text_learner.layer_groups[3])),\n                    nn.Sequential(*(flatten_model(text_learner.layer_groups[4]) + \n                                    flatten_model(model.mod_tab) +\n                                    flatten_model(model.layers)))] \n    learner = Learner(data, model, loss_func=loss_func, layer_groups=layer_groups,metrics = tab_learner.metrics)\n    return learner\n\ndef predict_one_item(learner,item,tab_db,text_db, **kwargs):\n    '''\n    learner: tabular text learner\n    item: pandas series\n    Return raw prediction from model and modified prediction (based on y.analyze_pred)\n    '''\n    tab_oneitem = tab_db.one_item(item,detach=True,cpu=True)\n    text_oneitem= text_db.one_item(item,detach=True,cpu=True)\n    _batch = [( ([tab_oneitem[0][0][0],tab_oneitem[0][1][0]],text_oneitem[0][0]), tab_oneitem[1][0] )]\n    tabtext_onebatch = tabtext_collate(_batch)\n\n    # send to gpu\n    tabtext_onebatch = to_device(tabtext_onebatch,None)\n\n    # taken from fastai.basic_train Learner.predict function\n    res = learner.pred_batch(batch=tabtext_onebatch)\n    raw_pred,x = grab_idx(res,0,batch_first=True),tabtext_onebatch[0]\n\n    ds = learner.data.single_ds\n    pred = ds.y.analyze_pred(raw_pred, **kwargs)\n    return pred, raw_pred","8fb57bc7":"def get_databunches(bs=64):\n    # get tabtext databunch, tabular databunch (for tabular model) and text databunch (for text model)\n    tab_db = get_tabular_databunch(df_reduce[cat_names + cont_names+ [dep_var]])\n    text_db = get_text_databunch(df_reduce[txt_cols +[dep_var]])\n    \n    train_ds = ConcatDataset(tab_db.train_ds.x, text_db.train_ds.x, tab_db.train_ds.y)\n    valid_ds = ConcatDataset(tab_db.valid_ds.x, text_db.valid_ds.x, tab_db.valid_ds.y)\n    \n    train_sampler = SortishSampler(text_db.train_ds.x, key=lambda t: len(text_db.train_ds[t][0].data), bs=bs\/\/2)\n    valid_sampler = SortSampler(text_db.valid_ds.x, key=lambda t: len(text_db.valid_ds[t][0].data))\n\n#     train_dl = DataLoader(train_ds, bs\/\/2, sampler=train_sampler)\n    train_dl = DataLoader(train_ds, bs\/\/2, sampler=train_sampler,shuffle=False)\n    valid_dl = DataLoader(valid_ds, bs, sampler=valid_sampler)\n    data = DataBunch(train_dl, valid_dl, device=defaults.device, collate_fn=tabtext_collate)\n    return data,tab_db,text_db","f3f41385":"data,tab_db,text_db = get_databunches(bs=64)","70ebbc93":"text_db.show_batch()","c580c242":"tab_db.show_batch()","34886e9b":"tab_params={\n    'layers':[500],\n    'emb_drop': 0.3,\n    'y_range': [1,9.5],\n    #'use_bn': True,    \n    }\n\ntext_params={\n    #     'lin_ftrs':[1000],\n    #     'ps': [0.001,0,0],\n        'bptt':70,\n        'max_len':20*70,\n        'drop_mult': 1., \n         #'use_bn': True,    \n    }","16fcb00e":"tab_learner = get_tabular_learner(tab_db,tab_params)\ntext_learner = get_text_learner(text_db,text_params)","d47b0abe":"text_learner.model","9551a3f7":"lin_layers=[500]\nps=[0.3]","a4c7a86a":"# 50 is the default lin_ftrs in AWD_LSTM\nlin_layers[-1]+= 50 if 'lin_ftrs' not in text_params else text_params['lin_ftrs']","b5c2058c":"lin_layers","51ec660a":"# first layer = tabular data layer + 50 (from LSTM)\n# second layer = as per your choice\n\nlin_layers=[500+50]\nps=[0.3]\nlin_layers","9b62603f":"# be careful here. If no lin_ftrs is specified, the default lin_ftrs is 50\nlearner = get_tabtext_learner(data,tab_learner,text_learner,lin_layers ,ps)","3f05043d":"learner.model","27ec6de9":"len(learner.layer_groups)\nlearner.layer_groups","ff4663fe":"learner.freeze()\nlearner.lr_find()","e34ed411":"learner.recorder.plot(skip_end=1)","d2286687":"learner.fit_one_cycle(3, 5e-2, moms=(0.8, 0.7))","cc396809":"learner.save('tmp1')","09ba303a":"_=learner.load('tmp1')\nlearner.freeze_to(-2)\n\nlearner.fit_one_cycle(3, slice(1e-03\/(2.6**4),1e-03), moms=(0.8, 0.7))","5f8aba08":"learner.save('tmp2')","156620b7":"_=learner.load('tmp2')\nlearner.freeze_to(-3)\n\nlearner.fit_one_cycle(5, slice(1e-04\/(2.6**4),1e-04), moms=(0.8, 0.7))","00cc593a":"learner.save('tmp3')\n_=learner.load('tmp3')\nlearner.unfreeze()\n\nlearner.fit_one_cycle(5, slice(1e-05\/(2.6**4),1e-05), moms=(0.8, 0.7))","19ec3885":"learner.save('final')\n_ = learner.load('final')","60b92a16":"val_df = df_reduce.loc[val_idxs].copy()","16b75c7f":"val_df.head(10)","e48cdecf":"one_item = val_df.loc[80]\none_item","30928ffe":"pred,raw_pred = predict_one_item(learner,one_item,tab_db,text_db)","cc91ecf5":"pred,raw_pred","ddf8b0cb":"from fastai.collab import *\nratings_1 = pd.read_csv(\"..\/input\/the-movies-dataset\/ratings_small.csv\", parse_dates=True)\nratings_2 = pd.read_csv(\"..\/input\/the-movies-dataset\/ratings.csv\", parse_dates=True)\nratings_1.shape, ratings_2.shape","e71f9e04":"ratings_1.head()","f93ccc11":"ratings_2.head()","c13aa097":"df2.head()","fd835ccf":"df_ratings_movies = pd.merge(df2, ratings_1, left_on='id', right_on='movieId', how='inner')\ndf_ratings_movies.shape","7f8eea90":"df_ratings_movies.head(3)","bd2147be":"df_ratings_movies = df_ratings_movies[['userId', 'movieId', 'timestamp', 'title', 'rating']]\ndf_ratings_movies.head(3)","3bfc5dcb":"data = CollabDataBunch.from_df(df_ratings_movies, seed = 42, valid_pct=0.2, user_name='userId', \n                               item_name='title', rating_name='rating')\n\ndata.show_batch()","0e909029":"ratings_1.rating.min(), ratings_1.rating.max()","6adda89a":"learner = collab_learner(data, n_factors=50, y_range=(0., 5.5), wd=1e-1)","a770f3dd":"learner.lr_find()","b099c069":"learner.recorder.plot(suggestion=True)","51f6ccfb":"learner.fit_one_cycle(5, 1e-2)","bc641b5e":"learner = collab_learner(data, use_nn=True, emb_szs = {'userId': 50, 'title': 50},\n                         layers = [256, 128], y_range=(0., 5.5))","162484ba":"learner.lr_find()\nlearner.recorder.plot(suggestion=True)","d4542a6f":"learner.fit_one_cycle(5, 1e-3, wd=1e-1)","2df79564":"learner.predict(df_ratings_movies.iloc[0])","f9509554":"df_ratings_movies.iloc[0]","bdfc6753":"learner.get_preds(ds_type=DatasetType.Valid)","e8b9f22f":"learner.model","1fe8ced3":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","62085275":"sample_txt = df2['overview'][0]\nsample_txt","987fc311":"doc = nlp(sample_txt)\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","854861e4":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]","1848c3f2":"get_entities(sample_txt)","36af42ee":"from tqdm import tnrange, tqdm_notebook\nentity_pairs = []\n\nfor i in tqdm_notebook(df2['overview']):\n  entity_pairs.append(get_entities(i))","8ad87933":"entity_pairs[10:20]","26277b09":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n  k = len(matches) - 1\n\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","f1676dd0":"import gc\ngc.collect()\nrelations = [get_relation(i) for i in tqdm_notebook(df2['overview'])]","a7d50ff8":"pd.Series(relations).value_counts()[:50]","ebe2f1bc":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","f46e66e6":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","d03b1c21":"gc.collect()","9b3cd0e7":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"takes\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","ab2581be":"# Collaborative Filtering using SVD","97efc6e5":"# Prediction of Score of a Movie","9370d3d9":"## Importing Libraries and Data from Kaggle","ebd27c34":"# Demographic Filtering","edf225ea":"Let's plot the wordcloud of movie plots","3a671ade":"## Word Cloud","9f13377c":"Lets create a dataframe with all important features and ignore others","825ce803":"## Language Model","eb74095f":"Lets first train the Language Model","a887f682":"# Importing Libraries and Data","06476f73":"In this Notebook (which is still unfinished), I will try to build a recommender system.\n\nThere are multiple items which I will be trying to accomplish in this Notebook:\n\n1. Demographic Filtering\n2. EDA\n3. Using Fastai NLP model to predict the score of a movie based on its brief plot description. This failed miserably.\n4. Concatenating Fastai NLP and Tabular model to predict the score of a movie based on plot and other details. This resulted in somewhat better results (around 83% AUROC).\n5. Using Fastai Collaborative Filtering model to predict what an user will like if similar users liked different movies in past.\n6. Using Spotify Annoy to find similar movies (this is still unfinished).\n","e0b72d37":"# NLP Model","2a5b22ef":"# Plots","4d442e2a":"## Prediction time","8a11c04b":"# Collaborative Filtering","8f658967":"# Spacy and Graphs","4494ad33":"# Introduction","0da5b8dd":"## Fastai - Tabular + Text","81815cc3":"## Reading the Data","cb6eb451":"## Text Classification Model + Tabular Learner Model"}}