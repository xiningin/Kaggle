{"cell_type":{"631ad9d3":"code","7f7f6076":"code","37063f52":"code","dd47a483":"code","0df9ad31":"code","5383fd33":"code","337e5fac":"code","7ffa8fc2":"code","21e2f4fa":"code","dbbbccb5":"code","0e0b1235":"code","937c971a":"code","faead061":"code","72fa12f4":"code","018bb321":"code","6ec61dd1":"code","b5798771":"code","fbf76a76":"code","09939634":"code","4c4c032e":"code","82bad3d7":"markdown","894e8e16":"markdown","70b23d4f":"markdown"},"source":{"631ad9d3":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom time import time\nimport json\n\nimport re\nimport string\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom gensim.models import FastText\n\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping\n\nimport tensorflow as tf\n\nfrom keras.layers import Input, Dense, Embedding, Flatten, Dropout, SpatialDropout1D # General\nfrom keras.layers import CuDNNLSTM, Bidirectional # LSTM-RNN\nfrom keras.optimizers import Adam\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","7f7f6076":"df = pd.read_csv('..\/input\/ndsc-beginner\/train.csv')","37063f52":"df.head()","dd47a483":"table = str.maketrans('','', string.punctuation)\n\ndef removeNumbersAndPunctuations(text):\n    text = text.translate(table)\n    text = re.sub(r'\\d+', '', text)\n    return text","0df9ad31":"df['title'] = df['title'].apply(removeNumbersAndPunctuations)","5383fd33":"X, y = {}, {}\ntokenizers = {}\nMAX_NB_WORDS = 20000\n\nfor typ in ['mobile', 'fashion', 'beauty']:    \n    sdf = df[df['image_path'].str.contains(typ)]\n    \n    X[typ], y[typ] = {}, {}\n    X[typ]['train'], X[typ]['test'], y[typ]['train'], y[typ]['test'] = train_test_split(sdf['title'], sdf['Category'], test_size=0.16, random_state=42)\n\n    tok = Tokenizer(num_words=MAX_NB_WORDS, lower=True) \n    tok.fit_on_texts(X[typ]['train'])\n    tokenizers[typ] = tok\n    \n\nvalaccs = {}\nvaltruth = {}\nvalpreds = {}","337e5fac":"def getModel(X, y, typ, tokenizer):\n    print(f'Processing for {typ} dataset...')\n    \n    # Split dataset into Train and Test    \n    X_train, X_test, y_train, y_test = X['train'], X['test'], y['train'], y['test']\n    \n    # Load embeddings\n    print(f'Loading word embeddings for {typ} dataset...')\n    embeddings_index = {}\n    f = open(f'..\/input\/creating-fasttext-embeddings\/ftembeddings300{typ}.txt', encoding='utf-8')\n    for line in tqdm(f):\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print(f'found {len(embeddings_index)} word vectors for {typ} dataset')\n    \n    # One-hot y datasets\n    y_train = pd.get_dummies(y_train)\n    y_test = pd.get_dummies(y_test)\n    \n    # Constants\n    NUM_CATEGORIES = y['train'].nunique()\n    MAX_SEQUENCE_LENGTH = 30\n    MAX_NB_WORDS = 20000\n    EMBED_DIM = 300\n    HIDDEN = 256\n    \n    print(f'Creating sequence matrices for {typ}')\n    # Create Sequence Matrices\n    tok = tokenizer\n    word_index = tok.word_index\n    \n    sequences = tok.texts_to_sequences(X_train)\n    train_dtm = sequence.pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)\n\n    test_sequences = tok.texts_to_sequences(X_test)\n    test_dtm = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)\n    \n    # Prepare embedding matrix\n    print(f'Preparing embedding matrix for {typ} dataset...')\n    words_not_found = []\n    NUM_WORDS = min(MAX_NB_WORDS, len(word_index))\n    embedding_matrix = np.zeros((NUM_WORDS, EMBED_DIM))\n    for word, i in word_index.items():\n        if i >= NUM_WORDS:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if (embedding_vector is not None) and len(embedding_vector) > 0:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n        else:\n            words_not_found.append(word)\n    \n    WEIGHTS = embedding_matrix\n    model = RNN_Model(NUM_CATEGORIES, NUM_WORDS, MAX_SEQUENCE_LENGTH, EMBED_DIM, HIDDEN, WEIGHTS)\n\n    print(f'Training for {typ} dataset with frozen embedding layer...')\n    \n    ea = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n    adam = Adam(lr=0.001, decay=0.000049, epsilon=1e-8)\n    \n    model.compile(loss='categorical_crossentropy',optimizer=adam, metrics=['categorical_accuracy'])\n    model.fit(train_dtm, y_train, batch_size=128, epochs=20, validation_data=(test_dtm,y_test), verbose=1, callbacks=[ea])    \n    \n    print(f'Training for {typ} dataset with unfrozen embedding layer...')\n    \n    ea2 = EarlyStopping(monitor='val_categorical_accuracy', patience=3, restore_best_weights=True)\n    adam2 = Adam(lr=0.001, decay=0.00006, epsilon=1e-8)\n    \n    model.layers[1].trainable = True\n    model.compile(loss='categorical_crossentropy',optimizer=adam2, metrics=['categorical_accuracy'])\n    model.fit(train_dtm, y_train, batch_size=128, epochs=20, validation_data=(test_dtm,y_test), verbose=1, callbacks=[ea2])\n    \n    valaccs[typ] = model.evaluate(test_dtm, y_test)\n    valtruth[typ] = [np.argmax(truth) + y_test.columns[0] for truth in y_test.values]\n    valpreds[typ] = [np.argmax(pred)+ y_test.columns[0] for pred in model.predict(test_dtm)]\n    return model","7ffa8fc2":"def RNN_Model(NUM_CATEGORIES, NUM_WORDS, MAX_SEQUENCE_LENGTH, EMBED_DIM, HIDDEN, WEIGHTS):\n    text_sequence = Input(shape=(MAX_SEQUENCE_LENGTH,), name='TEXT_SEQUENCE_INPUT')\n    \n    rnn_layer = Embedding(NUM_WORDS, EMBED_DIM, weights=[WEIGHTS], trainable=False, name='EMBEDDING')(text_sequence)\n    rnn_layer = SpatialDropout1D(0.5, name='EMBEDDING_DROPOUT')(rnn_layer)\n    rnn_layer = Bidirectional(CuDNNLSTM(HIDDEN, return_sequences=True), name='BILSTM_LAYER1')(rnn_layer)\n    rnn_layer = Bidirectional(CuDNNLSTM(HIDDEN, return_sequences=True), name='BILSTM_LAYER2')(rnn_layer)\n    rnn_layer = Flatten()(rnn_layer)\n    rnn_layer = Dropout(0.4,name='RNN_DROPOUT')(rnn_layer)\n\n    output = Dense(NUM_CATEGORIES, activation='softmax', name='OUTPUT')(rnn_layer)\n    model = Model(inputs=text_sequence, outputs=output)\n    \n    return model","21e2f4fa":"models = {}\nfor typ in ['mobile', 'fashion', 'beauty']:\n    models[typ] = getModel(X[typ], y[typ], typ, tokenizers[typ])","dbbbccb5":"y_truth, y_pred = [], []\nfor key in ['mobile', 'fashion', 'beauty']:\n    y_truth.extend(valtruth[key])\n    y_pred.extend(valpreds[key])","0e0b1235":"count = 0\nfor t,p in zip(y_truth, y_pred):\n    if t == p:\n        count += 1\nprint('Accuracy:', count\/len(y_pred))","937c971a":"with open('..\/input\/ndsc-beginner\/categories.json', 'rb') as handle:\n    catNames = json.load(handle)\n\ncatNameMapper = {}\nfor category in catNames.keys():\n    for key, value in catNames[category].items():\n        catNameMapper[value] = key","faead061":"catNameLabelsSorted = ['SPC', 'Icherry', 'Alcatel', 'Maxtron', 'Strawberry', 'Honor', 'Infinix', 'Realme', \n                       'Sharp', 'Smartfren', 'Motorola', 'Mito', 'Brandcode', 'Evercoss', 'Huawei', \n                       'Blackberry', 'Advan', 'Lenovo', 'Nokia', 'Sony', 'Asus', 'Vivo', 'Xiaomi', 'Oppo', \n                       'Iphone', 'Samsung', 'Others Mobile & Tablet', 'Big Size Top', 'Wedding Dress', \n                       'Others', 'Crop Top ', 'Big Size Dress', 'Tanktop', 'A Line Dress', 'Party Dress', \n                       'Bodycon Dress', 'Shirt', 'Maxi Dress', 'Blouse\\xa0', 'Tshirt', 'Casual Dress', \n                       'Lip Liner', 'Setting Spray', 'Contour', 'Other Lip Cosmetics', 'Lip Gloss', 'Lip Tint', \n                       'Face Palette', 'Bronzer', 'Highlighter', 'Primer', 'Blush On', 'Concealer', 'Lipstick', \n                       'Foundation', 'Other Face Cosmetics', 'BB & CC Cream', 'Powder']","72fa12f4":"catNamePred = list(map(lambda x: catNameMapper[x], y_pred))\ncatNameActual = list(map(lambda x: catNameMapper[x], y_truth))","018bb321":"confMat = confusion_matrix(catNamePred, catNameActual, labels=catNameLabelsSorted)","6ec61dd1":"fig, ax = plt.subplots(figsize=(30,30))\nsns.heatmap(confMat, annot=True, fmt='d', xticklabels=catNameLabelsSorted, yticklabels=catNameLabelsSorted)\nplt.ylabel('PREDICTED')\nplt.xlabel('ACTUAL')\nplt.show()","b5798771":"test_data = pd.read_csv('..\/input\/ndsc-beginner\/test.csv')\ntest_data['title'] = test_data['title'].apply(removeNumbersAndPunctuations)\ntest_data['typ'] = test_data['image_path'].apply(lambda x: x[:x.index('_')])","fbf76a76":"s = {'beauty': 0, 'fashion': 17, 'mobile': 31}\npreds = []\nfor ind, row in test_data.iterrows():\n    tok = tokenizers[row['typ']]\n    sequences = tok.texts_to_sequences([row['title']])\n    test_dtm = sequence.pad_sequences(sequences,maxlen=30)\n    preds.append(np.argmax(models[row['typ']].predict(test_dtm)) + s[row['typ']])","09939634":"test_data['Category'] = preds","4c4c032e":"df_submit = test_data[['itemid', 'Category']].copy()\ndf_submit.to_csv('submission_svc.csv', index=False)","82bad3d7":"## Submission","894e8e16":"## Remove Numbers and Punctuations","70b23d4f":"### Create RNN Model"}}