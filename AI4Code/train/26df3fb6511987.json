{"cell_type":{"5ae5e0de":"code","192886fb":"code","3a3f5b5f":"code","a3bfff94":"code","86585cbc":"code","88fd1a94":"code","a895c0f9":"code","9d337321":"code","08e59502":"code","3c41b095":"code","a8e0ab8d":"code","d6572d22":"code","e57dc48d":"code","0b16cbf6":"code","d4773166":"code","b5b20153":"code","a3e427cc":"code","c313313f":"markdown","5bd3dc7a":"markdown","307ba9ea":"markdown","7dbee931":"markdown","ce5681b7":"markdown","d19b2623":"markdown","b435ca1e":"markdown","35cc0b4d":"markdown","cf2d125c":"markdown","28480fb6":"markdown","47deff70":"markdown"},"source":{"5ae5e0de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2\nimport seaborn as sns\nsns.set_context(\"talk\")\n# sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nstyle.use('fivethirtyeight')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom functools import partial\nimport scipy as sp\nfrom sklearn import metrics\nimport os, sys, gc\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","192886fb":"# train = pd.read_csv(\"\/kaggle\/input\/liverpool-ion-switching\/train.csv\")\n# test = pd.read_csv(\"\/kaggle\/input\/liverpool-ion-switching\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/data-without-drift\/train_clean.csv\")\nbs = 50_000\ntest = pd.read_csv(\"\/kaggle\/input\/data-without-drift\/test_clean.csv\")\n# submission = pd.read_csv(\"\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv\")","3a3f5b5f":"print(train.shape)\ntrain.head()","a3bfff94":"print(test.shape)\ntest.head()","86585cbc":"def feature_engineering(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df_index = ((df.time * bs \/\/ 5) - 1).values\n    df['batch'] = df_index \/\/ bs\n    df['batch'] = df['batch'].astype(np.int16)\n    df['signal'] = df['signal'].astype(np.float16)\n    df['batch_index'] = df_index  - (df.batch * bs)\n    df['batch_slices'] = df['batch_index']  \/\/ (0.1 * bs)\n    df['batch_slices'] = df['batch_slices'].astype(np.int16)\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean().astype(np.float16)\n        d['std'+c] = df.groupby([c])['signal'].std().astype(np.float16)\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n            df[v] = df[v].astype(np.float16)\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        df[c+'_msignal'] = df[c+'_msignal'].astype(np.float16)\n        \n    return df\n\ntrain = feature_engineering(train)\ntest = feature_engineering(test)","88fd1a94":"print(test.shape)\ntest.head()","a895c0f9":"class OptimizedRounderF1(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize f1 score\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -metrics.f1_score(y, X_p, average='macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","9d337321":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\nclass StratifiedGroupKFold(object):\n    \"\"\"\n    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        labels_num = np.max(y) + 1\n        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n        y_distr = Counter()\n        groups = X[group].values\n        for label, g in zip(y, groups):\n            y_counts_per_group[g][label] += 1\n            y_distr[label] += 1\n\n        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n        groups_per_fold = defaultdict(set)\n\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(self.n_splits)])\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n        \n        groups_and_y_counts = list(y_counts_per_group.items())\n        random.Random(self.random_state).shuffle(groups_and_y_counts)\n\n        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n            best_fold = None\n            min_eval = None\n            for i in range(self.n_splits):\n                fold_eval = eval_y_counts_per_fold(y_counts, i)\n                if min_eval is None or fold_eval < min_eval:\n                    min_eval = fold_eval\n                    best_fold = i\n            y_counts_per_fold[best_fold] += y_counts\n            groups_per_fold[best_fold].add(g)\n\n        all_groups = set(groups)\n        for i in range(self.n_splits):\n            train_groups = all_groups - groups_per_fold[i]\n            test_groups = groups_per_fold[i]\n\n            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n\n            yield train_idx, test_idx","08e59502":"def get_oof_ypred(model, x_val, x_test, modelname=\"linear\", task=\"binary\"):  \n    \"\"\"\n    get oof and target predictions\n    \"\"\"\n    sklearns = [\"xgb\", \"catb\", \"linear\", \"knn\"]\n\n    if task == \"binary\": # classification\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n            oof_pred = oof_pred[:, 1]\n            y_pred = y_pred[:, 1]\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n            # NN specific\n            if modelname == \"nn\":\n                oof_pred = oof_pred.ravel()\n                y_pred = y_pred.ravel()        \n\n    elif task == \"multiclass\":\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n        oof_pred = np.argmax(oof_pred, axis=1)\n        y_pred = np.argmax(y_pred, axis=1)\n\n    elif task == \"regression\": # regression\n        oof_pred = model.predict(x_val)\n        y_pred = model.predict(x_test)\n\n        # NN specific\n        if modelname == \"nn\":\n            oof_pred = oof_pred.ravel()\n            y_pred = y_pred.ravel()\n\n    return oof_pred, y_pred","3c41b095":"from sklearn import linear_model\n\ndef lin_model(cls, train_set, val_set):\n    \"\"\"\n    Linear model hyperparameters and models\n    \"\"\"\n\n    params = {\n            'max_iter': 8000,\n            'fit_intercept': True,\n            'random_state': cls.seed\n        }\n\n    if cls.task == \"regression\":\n        # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html\n        model = linear_model.Ridge(**{'alpha': 220, 'solver': 'lsqr', 'fit_intercept': params['fit_intercept'],\n                                'max_iter': params['max_iter'], 'random_state': params['random_state']})\n    elif (cls.task == \"binary\") | (cls.task == \"multiclass\"):\n        # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n        model = linear_model.LogisticRegression(**{\"C\": 1.0, \"fit_intercept\": params['fit_intercept'], \n                                \"random_state\": params['random_state'], \"solver\": \"lbfgs\", \"max_iter\": params['max_iter'], \n                                \"multi_class\": 'auto', \"verbose\":0, \"warm_start\":False})\n                                \n    model.fit(train_set['X'], train_set['y'])\n\n    # feature importance (for multitask, absolute value is computed for each feature)\n    if cls.task == \"multiclass\":\n        fi = np.mean(np.abs(model.coef_), axis=0).ravel()\n    else:\n        fi = model.coef_.ravel()\n\n    return model, fi","a8e0ab8d":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n\nclass RunModel(object):\n    \"\"\"\n    Model Fitting and Prediction Class:\n\n    train_df : train pandas dataframe\n    test_df : test pandas dataframe\n    target : target column name (str)\n    features : list of feature names\n    categoricals : list of categorical feature names\n    model : lgb, xgb, catb, linear, or nn\n    task : options are ... regression, multiclass, or binary\n    n_splits : K in KFold (default is 3)\n    cv_method : options are ... KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n    group : group feature name when GroupKFold or StratifiedGroupKFold are used\n    parameter_tuning : bool, only for LGB\n    seed : seed (int)\n    scaler : options are ... None, MinMax, Standard\n    verbose : bool\n    \"\"\"\n\n    def __init__(self, train_df, test_df, target, features, categoricals=[],\n                model=\"lgb\", task=\"regression\", n_splits=4, cv_method=\"KFold\", \n                group=None, parameter_tuning=False, seed=1220, scaler=None, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.target = target\n        self.features = features\n        self.categoricals = categoricals\n        self.model = model\n        self.task = task\n        self.n_splits = n_splits\n        self.cv_method = cv_method\n        self.group = group\n        self.parameter_tuning = parameter_tuning\n        self.seed = seed\n        self.scaler = scaler\n        self.verbose = verbose\n        self.cv = self.get_cv()\n        self.y_pred, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n\n    def train_model(self, train_set, val_set):\n\n        # compile model\n        if self.model == \"lgb\": # LGB             \n            model, fi = lgb_model(self, train_set, val_set)\n\n        elif self.model == \"xgb\": # xgb\n            model, fi = xgb_model(self, train_set, val_set)\n\n        elif self.model == \"catb\": # catboost\n            model, fi = catb_model(self, train_set, val_set)\n\n        elif self.model == \"linear\": # linear model\n            model, fi = lin_model(self, train_set, val_set)\n\n        elif self.model == \"knn\": # knn model\n            model, fi = knn_model(self, train_set, val_set)\n\n        elif self.model == \"nn\": # neural network\n            model, fi = nn_model(self, train_set, val_set)\n        \n        return model, fi # fitted model and feature importance\n\n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        if self.model == \"lgb\":\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        else:\n            if (self.model == \"nn\") & (self.task == \"multiclass\"):\n                n_class = len(np.unique(self.train_df[self.target].values))\n                train_set = {'X': x_train, 'y': onehot_target(y_train, n_class)}\n                val_set = {'X': x_val, 'y': onehot_target(y_val, n_class)}\n            else:\n                train_set = {'X': x_train, 'y': y_train}\n                val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n\n    def calc_metric(self, y_true, y_pred): # this may need to be changed based on the metric of interest\n        if self.task == \"multiclass\":\n            return f1_score(y_true, y_pred, average=\"macro\")\n        elif self.task == \"binary\":\n            return roc_auc_score(y_true, y_pred) # log_loss\n        elif self.task == \"regression\":\n            return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def get_cv(self):\n        # return cv.split\n        if self.cv_method == \"KFold\":\n            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df)\n        elif self.cv_method == \"StratifiedKFold\":\n            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target])\n        elif self.cv_method == \"TimeSeriesSplit\":\n            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n            return cv.split(self.train_df)\n        elif self.cv_method == \"GroupKFold\":\n            cv = GroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n        elif self.cv_method == \"StratifiedGroupKFold\":\n            cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n\n    def fit(self):\n        # initialize\n        oof_pred = np.zeros((self.train_df.shape[0], ))\n        y_vals = np.zeros((self.train_df.shape[0], ))\n        y_pred = np.zeros((self.test_df.shape[0], ))\n\n        # group does not kick in when group k fold is used\n        if self.group is not None:\n            if self.group in self.features:\n                self.features.remove(self.group)\n            if self.group in self.categoricals:\n                self.categoricals.remove(self.group)\n        fi = np.zeros((self.n_splits, len(self.features)))\n\n        # scaling, if necessary\n        if self.scaler is not None:\n            # fill NaN\n            numerical_features = [f for f in self.features if f not in self.categoricals]\n            self.train_df[numerical_features] = self.train_df[numerical_features].fillna(self.train_df[numerical_features].median())\n            self.test_df[numerical_features] = self.test_df[numerical_features].fillna(self.test_df[numerical_features].median())\n            self.train_df[self.categoricals] = self.train_df[self.categoricals].fillna(self.train_df[self.categoricals].mode().iloc[0])\n            self.test_df[self.categoricals] = self.test_df[self.categoricals].fillna(self.test_df[self.categoricals].mode().iloc[0])\n\n            # scaling\n            if self.scaler == \"MinMax\":\n                scaler = MinMaxScaler()\n            elif self.scaler == \"Standard\":\n                scaler = StandardScaler()\n            df = pd.concat([self.train_df[numerical_features], self.test_df[numerical_features]], ignore_index=True)\n            scaler.fit(df[numerical_features])\n            x_test = self.test_df.copy()\n            x_test[numerical_features] = scaler.transform(x_test[numerical_features])\n            if self.model == \"nn\":\n                x_test = [np.absolute(x_test[i]) for i in self.categoricals] + [x_test[numerical_features]]\n            else:\n                x_test = x_test[self.features]\n        else:\n            x_test = self.test_df[self.features]\n\n        # fitting with out of fold\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            # train test split\n            x_train, x_val = self.train_df.loc[train_idx, self.features], self.train_df.loc[val_idx, self.features]\n            y_train, y_val = self.train_df.loc[train_idx, self.target], self.train_df.loc[val_idx, self.target]\n\n            # fitting & get feature importance\n            if self.scaler is not None:\n                x_train[numerical_features] = scaler.transform(x_train[numerical_features])\n                x_val[numerical_features] = scaler.transform(x_val[numerical_features])\n                if self.model == \"nn\":\n                    x_train = [np.absolute(x_train[i]) for i in self.categoricals] + [x_train[numerical_features]]\n                    x_val = [np.absolute(x_val[i]) for i in self.categoricals] + [x_val[numerical_features]]\n\n            # model fitting\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model, importance = self.train_model(train_set, val_set)\n            fi[fold, :] = importance\n            y_vals[val_idx] = y_val\n\n            # predictions\n            oofs, ypred = get_oof_ypred(model, x_val, x_test, self.model, self.task)\n            oof_pred[val_idx] = oofs.reshape(oof_pred[val_idx].shape)\n            y_pred += ypred.reshape(y_pred.shape) \/ self.n_splits\n\n            # check cv score\n            print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], oof_pred[val_idx])))\n\n        # feature importance data frame\n        fi_df = pd.DataFrame()\n        for n in np.arange(self.n_splits):\n            tmp = pd.DataFrame()\n            tmp[\"features\"] = self.features\n            tmp[\"importance\"] = fi[n, :]\n            tmp[\"fold\"] = n\n            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n        gfi = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n\n        # outputs\n        loss_score = self.calc_metric(y_vals, oof_pred)\n        if self.verbose:\n            print('Our oof loss score is: ', loss_score)\n        return y_pred, loss_score, model, oof_pred, y_vals, fi_df\n\n    def plot_feature_importance(self, rank_range=[1, 50]):\n        # plot feature importance\n        _, ax = plt.subplots(1, 1, figsize=(10, 20))\n        sorted_df = self.fi_df.sort_values(by = \"importance_mean\", ascending=False).reset_index().iloc[self.n_splits * (rank_range[0]-1) : self.n_splits * rank_range[1]]\n        sns.barplot(data=sorted_df, x =\"importance\", y =\"features\", orient='h')\n        ax.set_xlabel(\"feature importance\")\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        return sorted_df","d6572d22":"features = test.columns.values.tolist()\ndropcols = ['batch', 'batch_slices', 'batch_slices2', 'open_channels', 'time']\nfeatures = [f for f in features if f not in dropcols]\ntarget = 'open_channels'\ncategoricals = []\ngroup = \"batch\"","e57dc48d":"model = RunModel(train, test, target, features, categoricals=categoricals,\n            model=\"linear\", task=\"regression\", n_splits=4, cv_method=\"StratifiedGroupKFold\", \n            group=group, parameter_tuning=False, seed=1220, scaler=\"Standard\", verbose=True)\nprint(\"Training done\")","0b16cbf6":"N = 2\nreg_scores = np.zeros(N)\noptRfs = []\nfor i in np.arange(N):\n    optRf = OptimizedRounderF1()\n    optRf.fit(model.oof, model.y_val)\n    coefficientsf = optRf.coefficients()\n    opt_valsf = optRf.predict(model.oof, coefficientsf)\n    reg_score = f1_score(model.y_val, opt_valsf, average='macro')\n    print(f\"CV score round {i} (reg) = {reg_score}\")\n    optRfs.append(optRf)\n    reg_scores[i] = reg_score\nbst_i = np.argmax(reg_scores)\noptRf = optRfs[bst_i]\ncoefficientsf = optRf.coefficients()\nprint(coefficientsf)\nopt_valsf = optRf.predict(model.oof, coefficientsf)\nopt_predsf = optRf.predict(model.y_pred, coefficientsf)","d4773166":"print(f\"CV score = {model.score}\")","b5b20153":"del train, test\ngc.collect()\nsubmit = pd.read_csv(\"\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv\")\nsubmit[\"open_channels\"] = opt_predsf\nsubmit[\"open_channels\"] = submit[\"open_channels\"].astype(int)\nsubmit.to_csv('submission_knnreg.csv', index=False, float_format='%.4f')\nprint(\"submission file saved!\")","a3e427cc":"submit[\"open_channels\"].hist(alpha=0.5)","c313313f":"# Utilities","5bd3dc7a":"# Feature engineering","307ba9ea":"# CV score","7dbee931":"# Submit","ce5681b7":"# models\nThis is a part of my model pipeline which is under development, so please ignore some redundancy. For the sake of time, I use a linear model.","d19b2623":"# Fitting","b435ca1e":"# StratifiedGroupKFold\nThe following cell is an example of its implementation.","35cc0b4d":"# Load data","cf2d125c":"# Optimize thresholds for F1","28480fb6":"# Libraries","47deff70":"There is a growing interest in which validation strategy to employ (like [this discussion](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/138850), as is always the case with a tough kaggle competition. When it comes to whether StratifiedKFold or GroupKFold is better, there is actually the third good option: StratifiedGroupKFold. Here I show you an implementation and how to use it.\n\nAs a dataset, I use a cleaned signal uploaded [here](https:\/\/www.kaggle.com\/cdeotte\/data-without-drift)."}}