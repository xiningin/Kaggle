{"cell_type":{"45c09fd0":"code","8a054f1b":"code","7c675588":"code","5fa63f52":"code","8e85b185":"code","e0139613":"code","f37769b4":"code","0389ceec":"code","23adcfee":"code","9ce6967c":"code","657c3372":"code","3d400ed9":"code","89232c11":"code","ce848466":"code","da439dff":"code","ea397d7a":"code","052f9253":"code","083fc90b":"code","564be8f3":"code","7020374d":"code","2ba994fd":"code","dc3600e1":"code","98ca8e93":"code","cbb024d7":"code","8471885d":"code","71fad217":"markdown","2ae8217e":"markdown","8ab46b2d":"markdown","03031524":"markdown","09034859":"markdown","b9dc44b0":"markdown"},"source":{"45c09fd0":"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, Normalizer, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB \nimport catboost","8a054f1b":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","7c675588":"def how_much_empty(*args): # calculate the percentage of empty values\n    print('column '.ljust(13,' '), end='\\t')\n    for i in range(len(args)):\n        print(i+1,'df ', end='\\t\\t')\n    print('\\n', '='*20*len(args), end='\\n\\n')\n    \n    cols_df = []\n    for df in args:\n        cols_df.append([(df[col].isnull().sum()\/len(df))*100 for col in df.columns])    \n    \n    for i in range(len(cols_df[0])):\n        print(str(df.columns[i]).ljust(13,' '), end = '')\n        for j in cols_df:\n            print(str(round(j[i], 3)).rjust(6,' '), end = '%\\t\\t') \n        print('\\n')","5fa63f52":"how_much_empty(train)","8e85b185":"def pipelining_preprocessor(df1, dropcolls = None, target = None): # return processed  dataFrame\n    df = df1.copy()\n    \n    if target:\n        y = df[target]\n        df.drop(target, axis=1, inplace=True)\n    \n    if dropcolls:\n        df.drop(dropcolls, axis=1, inplace=True)\n    \n    categoric = df.select_dtypes(include='object').columns\n    numeric = df.select_dtypes(exclude='object').columns\n    \n    imputer_numeric = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), \n                                      ('normalizer', Normalizer()), \n                                     ])\n    imputer_categoric = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='Missing')), \n                                        ('onehot', OneHotEncoder(drop='if_binary')), \n                                       ])\n    \n    preprocessor = ColumnTransformer(transformers=[('imputer_numeric', imputer_numeric, numeric), ('imputer_categoric', imputer_categoric, categoric)], \n                                     n_jobs=-1, verbose=True)\n    \n    if target:\n        return (pd.DataFrame(preprocessor.fit_transform(df)), y)\n    else:\n        return pd.DataFrame(preprocessor.fit_transform(df))","e0139613":"def pipelining_preprocessor2(df1, dropcolls = None, target = None): # return processed  dataFrame\n    df = df1.copy()\n    \n    if target:\n        y = df[target]\n        df.drop(target, axis=1, inplace=True)\n    \n    if dropcolls:\n        df.drop(dropcolls, axis=1, inplace=True)\n    \n    def normalizer(df):\n        return pd.DataFrame(Normalizer().fit_transform(df), columns = df.columns)\n    \n    def fill_empty_by_pop(df): # filler for empty values in cols by most popular values (order columns by original)\n        df_obj = df.loc[:, df.dtypes == 'object']\n        pop_obj = df_obj.describe().loc['top',:]\n        \n        df_digits = df.loc[:, df.dtypes != 'object']\n        pop_digits = df_digits.median()\n        \n        \n#         return df_obj.fillna('Unknown').join(normalizer(df_digits.fillna(pop_digits)))[df.columns]\n        return df_obj.fillna(pop_obj).join(normalizer(df_digits.fillna(pop_digits)))[df.columns]\n    \n    ohe = pd.get_dummies(fill_empty_by_pop(df), drop_first=True, prefix_sep=': ',)\n    \n    if target:\n        return ohe, y\n    else:\n        return ohe","f37769b4":"X, y = pipelining_preprocessor2(train, \n                                dropcolls = ['PassengerId', 'Name', 'Cabin', 'Ticket'], \n                                target='Survived')","0389ceec":"X_test = pipelining_preprocessor2(test, dropcolls = ['PassengerId', 'Name', 'Cabin', 'Ticket'])","23adcfee":"X","9ce6967c":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)","657c3372":"full_pool = catboost.Pool(X, y)","3d400ed9":"params = {'learning_rate': [0.87, 0.9, 0.93],\n         }\n\ncat_model = catboost.CatBoostClassifier(early_stopping_rounds=25, \n                                        eval_metric='Accuracy',\n                                        loss_function='Logloss', \n#                                         task_type = 'GPU', \n                                        depth=4, \n                                        l2_leaf_reg=12, \n                                        max_ctr_complexity = 1\n                                       )","89232c11":"grid_search_results = cat_model.grid_search(params, full_pool, \n                                            partition_random_seed=2, cv = skf, \n                                            plot=True, \n                                            verbose=False)","ce848466":"grid_search_results['params']","da439dff":"rf_model = RandomForestClassifier(n_estimators=1000,\n                                  random_state=2, \n                                  max_features = 'sqrt', \n                                  oob_score = True, \n                                  criterion='gini', \n                                  n_jobs=-1,)\n\nparams = {'max_depth': range(19, 23)}","ea397d7a":"forest_results = GridSearchCV(rf_model, \n                              params, cv=skf, \n                              n_jobs=-1, verbose=1)\n\n# forest_results.fit(X, y)","052f9253":"# forest_results.best_params_","083fc90b":"cat_final = catboost.CatBoostClassifier(early_stopping_rounds=25, \n                                        eval_metric='Accuracy',\n                                        loss_function='Logloss', \n#                                         task_type = 'GPU', \n                                        learning_rate = 0.9, \n                                        depth=4, \n                                        l2_leaf_reg=12, \n                                        max_ctr_complexity = 1\n                                       )\n\nrf_final = RandomForestClassifier(n_estimators=1000,\n                                  random_state=2, \n                                  max_features = 'sqrt', \n                                  oob_score = True, \n                                  criterion='gini', \n                                  max_depth=25, \n                                  n_jobs=-1,)\n\nfinal_model_hard = VotingClassifier(estimators=[('rf', rf_final), \n                                           ('cat', cat_final), \n                                           ('LogisticRegression', LogisticRegression(max_iter = 30000)), \n                                           ('SVC', SVC()), \n                                           ('GaussianNB', GaussianNB()), \n                                           ('DecisionTreeClassifier', DecisionTreeClassifier()), \n                                           ], voting = 'hard')\n\nfinal_model_soft = VotingClassifier(estimators=[('rf', rf_final), \n                                           ('cat', cat_final), \n                                           ('LogisticRegression', LogisticRegression(max_iter = 30000)), \n                                           ('SVC', SVC(probability=True)), \n                                           ('GaussianNB', GaussianNB()), \n                                           ('DecisionTreeClassifier', DecisionTreeClassifier()), \n                                           ], voting = 'soft')\n\nfinal_model_lil = VotingClassifier(estimators=[('rf', rf_final), \n                                           ('cat', cat_final), \n                                           ('DecisionTreeClassifier', DecisionTreeClassifier()), \n                                           ], voting = 'hard')","564be8f3":"cross_val_score(final_model_hard, X, y, cv=skf, n_jobs=-1).mean()","7020374d":"cross_val_score(final_model_soft, X, y, cv=skf, n_jobs=-1).mean()","2ba994fd":"cross_val_score(final_model_lil, X, y, cv=skf, n_jobs=-1).mean()","dc3600e1":"cross_val_score(rf_final, X, y, cv=skf, n_jobs=-1).mean()","98ca8e93":"cross_val_score(cat_final, X, y, cv=skf, n_jobs=-1).mean()","cbb024d7":"final_model_soft.fit(X,y)\ny_pred = final_model_soft.predict(X_test)\n\n# rf_final.fit(X,y)\n# y_pred = rf_final.predict(X_test)","8471885d":"output = pd.DataFrame({'PassengerId': pd.read_csv('..\/input\/titanic\/test.csv').PassengerId, 'Survived': y_pred})\noutput.to_csv('submission_ensemble_soft.csv', index=False)\noutput","71fad217":"# Preprocessing","2ae8217e":"<hr>\n\n# Modeling","8ab46b2d":"First of all we need to drop \"Cabin\" because a lot of missing values. Also we need to drop cols \"PassengerId\" and \"Name\", because that features has no usefull information. Also try to drop \"Ticket\". \n\nThen we need to fill empty values in other columns","03031524":"## Gradient Boosting (CatBoost)","09034859":"## Analyzis of features","b9dc44b0":"## Pipeline making with fillna and OneHotEncoding"}}