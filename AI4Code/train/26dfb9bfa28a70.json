{"cell_type":{"ed581810":"code","e1cab06c":"code","f7ec3102":"code","ddc43e55":"code","6c71dc89":"code","cd9a99b9":"code","fbd68b62":"code","7b74ed66":"code","9ca2ece6":"code","1d47875d":"markdown","5aa84f6c":"markdown","f577ecde":"markdown","e6690cee":"markdown","7df46ab9":"markdown","d72734f2":"markdown","22ffd358":"markdown","f1a79920":"markdown"},"source":{"ed581810":"import matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as dset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\nimport os\nfrom torchvision import transforms\nfrom sklearn.metrics import roc_curve, auc","e1cab06c":"data_transforms = transforms.Compose([\n            transforms.RandomRotation(90, resample=PIL.Image.BILINEAR),\n            transforms.RandomApply((transforms.RandomHorizontalFlip(.5), \n                                    transforms.RandomVerticalFlip(.5)), p=0.5),\n            transforms.ToTensor(), \n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n\n","f7ec3102":"trainset = dset.ImageFolder('\/kaggle\/input\/skin-cancer-malignant-vs-benign\/data\/train', \n                            transform=data_transforms)\ntestset = dset.ImageFolder('\/kaggle\/input\/skin-cancer-malignant-vs-benign\/data\/test', \n                           transform=data_transforms)\ndata_size = len(trainset.samples)\nvalidation_split = .2\nsplit = int(np.floor(validation_split * data_size))\nindices = list(range(data_size))\nnp.random.shuffle(indices)\nbatch_size = 8\ntrain_indices, val_indices = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n                                           sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                         sampler=val_sampler)\n\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)","ddc43e55":"class Flattener(nn.Module):\n    def forward(self, x):\n        batch_size, *_ = x.shape\n        return x.view(batch_size, -1)","6c71dc89":"USE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","cd9a99b9":"def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n    loss_history = []\n    train_history = []\n    val_history = []\n\n    for epoch in range(num_epochs):\n        model.train() # Enter train mode\n        \n        loss_accum = 0\n        correct_samples = 0\n        total_samples = 0\n        for i_step, (x, y) in enumerate(train_loader):\n          \n            x_gpu = x.to(device)\n            y_gpu = y.to(device)\n            prediction = model(x_gpu)    \n            loss_value = loss(prediction, y_gpu)\n            optimizer.zero_grad()\n            loss_value.backward()\n            optimizer.step()\n            \n            _, indices = torch.max(prediction, 1)\n            correct_samples += torch.sum(indices == y_gpu)\n            total_samples += y.shape[0]\n            \n            loss_accum += loss_value\n\n        ave_loss = loss_accum \/ i_step\n        train_accuracy = float(correct_samples) \/ total_samples\n        val_accuracy = compute_accuracy(model, val_loader)\n        \n        loss_history.append(float(ave_loss))\n        train_history.append(train_accuracy)\n        val_history.append(val_accuracy)\n        \n        print(f\"Average loss: {ave_loss}, \\\n              Train accuracy: {train_accuracy}, \\\n              Val accuracy: {val_accuracy}\")\n        \n    return loss_history, train_history, val_history\n        \ndef compute_accuracy(model, loader, auc=False):\n    model.eval() \n    correct_samples = 0\n    total_samples = 0\n    y_true, y_pred = [], []\n    for (x, y) in loader:\n      \n        x_gpu = x.to(device)\n        y_gpu = y.to(device)\n        prediction = model(x_gpu)    \n        indices = torch.argmax(prediction, 1)\n        fpr, tpr, _ = roc_curve(y_gpu.cpu().detach().numpy(), indices.cpu().detach().numpy())\n\n        correct_samples += torch.sum(indices == y_gpu)\n        total_samples += y_gpu.shape[0]\n        accuracy = float(correct_samples) \/ total_samples\n        torch.cuda.get_device_name(0)\n        if auc:\n            y_true.extend(y_gpu.cpu().detach().numpy())\n            y_pred.extend(indices.cpu().detach().numpy())\n    if auc:\n        return accuracy, y_true, y_pred\n    else: \n        return accuracy\n    \n    \n\n    return accuracy, y_true, y_pred\n\ndef loss_accuracy_plot(loss_history, train_history, val_history):\n    plt.figure(figsize=(10, 9))\n    plt.subplot(311)\n#     plt.ylim(bottom=0)\n    plt.title(\"Loss\")\n    plt.plot(loss_history)\n    plt.subplot(312)\n    plt.ylim((.5, 1))\n    plt.title(\"Train\/validation accuracy\")\n    plt.plot(train_history, label=\"train\")\n    plt.plot(val_history, label=\"validation\")\n\n    plt.legend();","fbd68b62":"device = torch.device('cuda')\n# torch.set_num_threads(16)\nprint(torch.cuda.get_device_name(0))\nnn_model = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(4),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(24),    \n            Flattener(),\n            nn.Linear(64*2*2, 2),\n          )\ndevice = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\nnn_model.type(torch.cuda.FloatTensor)\n\nnn_model.to(device)\n\nloss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n\n# optimizer = optim.SGD(nn_model.parameters(), lr=1e-3, weight_decay=1e-4)\noptimizer = optim.Adam(nn_model.parameters(), lr=0.001, \n                       betas=(0.9, 0.999), eps=1e-08, \n                       weight_decay=0.0001, amsgrad=False)","7b74ed66":"loss_history, train_history, val_history = train_model(nn_model, \n                                                       train_loader, \n                                                       val_loader, \n                                                       loss, \n                                                       optimizer, 30)\nloss_accuracy_plot(loss_history, train_history, val_history)\nprint('Finished!')\n\ntest_accuracy, y_true, y_pred = compute_accuracy(nn_model, test_loader, auc=True)\n\nfpr, tpr, _ = roc_curve(y_true, y_pred)\nauc_rate = auc(fpr, tpr)\n\n\nprint(f'Final accuracy on the test set is: {test_accuracy}; auc is: {auc_rate}')","9ca2ece6":"test_accuracy, y_true, y_pred = compute_accuracy(nn_model, test_loader, auc=True)\n\nfpr, tpr, _ = roc_curve(y_true, y_pred)\nauc_rate = auc(fpr, tpr)\n\n\nprint(f'Final accuracy on the test set is: {test_accuracy}; auc is: {auc_rate}')","1d47875d":"Using pyTorch datasets read the train\/test samples and split train set to train and validation sets. ","5aa84f6c":"Curves, accuracy: ","f577ecde":"Make Flattener object","e6690cee":"Creating the model, defining loss and optimizer for training procedure. ","7df46ab9":"Function for training model, computing accuracy and plotting train\/val curves.","d72734f2":"Importing libraries","22ffd358":"## Classification using the CNN pyTorch","f1a79920":"Make the transformer that includes RandomRotation, ToTensor and Normalize steps. "}}