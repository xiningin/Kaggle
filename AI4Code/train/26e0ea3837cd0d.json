{"cell_type":{"00df8fab":"code","5fac8e2a":"code","69795702":"code","11f4142e":"code","a8ef7abe":"code","ed2b101f":"code","15b9e652":"code","6ff1db51":"code","0c08b41f":"code","6f1f6a25":"code","ae2e00ae":"code","d224d21e":"markdown","75b15076":"markdown","ee2ff4ec":"markdown","d117818c":"markdown","39d1a677":"markdown","adad1163":"markdown","48990a7b":"markdown","d286c09e":"markdown","50aa6bc9":"markdown"},"source":{"00df8fab":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","5fac8e2a":"np.random.seed(0)\n\nn_samples = 400\n\ndef true_fun(X_1, X_2 , X_3):\n    \n    y_temp_1 = np.cos(1.5 * np.pi * X_1)\n    y_temp_2 = y_temp_1 * X_2 * X_2\n    y_temp_3 = X_3 * 0.5\n    \n    result = 2 * y_temp_1 + y_temp_2 + y_temp_3\n    return result\n\nX_1 = np.sort(np.random.rand(n_samples))\nX_2 = np.sort(np.random.rand(n_samples)) +  np.random.randn(n_samples) * 0.2\n\ndata = pd.DataFrame(X_1, columns= {\"X_1\"})\ndata[\"X_2\"] = X_2\ndata[\"X_3\"] = 1\n\ndata[\"X_3\"].loc[data[\"X_2\"] >0.4] = 0.5\ny = true_fun(data[\"X_1\"], data[\"X_2\"], data[\"X_3\"]) + np.random.randn(n_samples) * 0.2\n\ndata[\"y\"] = y\n","69795702":"sns.pairplot(data, kind=\"reg\" , diag_kind=\"kde\")","11f4142e":"X = data.drop(\"y\" , axis = 1)\ny = data[\"y\"]","a8ef7abe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","ed2b101f":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\npredictions = lm.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nprint (lm.score(X_train,y_train))\nprint (lm.score(X_test,y_test))","15b9e652":"data[\"X_4\"] = data[\"X_1\"] * data[\"X_1\"]","6ff1db51":"X = data.drop(\"y\" , axis = 1)\ny = data[\"y\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","0c08b41f":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\npredictions = lm.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nprint (lm.score(X_train,y_train))\nprint (lm.score(X_test,y_test))","6f1f6a25":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\npolynomial_features = PolynomialFeatures(degree=3,include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\npipeline.fit(X_train,y_train)\npredictions = pipeline.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\n\nprint (pipeline.score(X_train,y_train))\nprint (pipeline.score(X_test,y_test))","ae2e00ae":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100, max_features = 4)\nrf.fit(X_train,y_train)\nprint(regr.feature_importances_)\npredictions = rf.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nprint (rf.score(X_train,y_train))\nprint (rf.score(X_test,y_test))","d224d21e":"### Split the Data","75b15076":"### A bit of feature engeeniring","ee2ff4ec":"### PolynimialFeatures solution","d117818c":"### Random Foresest Regressor","39d1a677":"#### Database\n\n* the data are created using a True function. So the solution is already in the function and there would no need to use any ML techniques. This is a study using the dummy data to check the impact of polynomial features on the performance of a linear regression and also to study the features importance in RandomForest  Regressor.","adad1163":"### Conclusion\n\n* PolynomialFeatures method captures the 3 grade degree of the function that maps X to y\n* RandomForests Regressor captures in particular the 4th feature which makes sense if I look how that feature influences the output","48990a7b":"### Plot the data","d286c09e":"* In this notebook:\n    * Custom dataset creation for playing with different scenarios\n    * Comparison of the result of Linear Regression, Feature engeeniring (X^2), Polynomial Regression, and RandomForest regression ","50aa6bc9":"## Training the Model\n\n#### Linear Regression"}}