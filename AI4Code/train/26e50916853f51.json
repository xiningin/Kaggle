{"cell_type":{"f18ece6b":"code","9745e726":"code","00d7206b":"code","38ce1991":"code","6cdad00d":"code","b7435e49":"code","74b373b9":"code","3572e18f":"code","1cd03e6a":"code","7284d3d7":"code","604e24c5":"code","6a5c1aec":"code","aed478be":"code","1d8c446b":"code","86757e97":"code","9a0833fe":"code","6f681631":"code","57423834":"code","998ff763":"code","8664f3ae":"code","ceda50b7":"code","d577e5bd":"code","310eb8bb":"code","056fbb25":"code","e8b6e7be":"code","4989387a":"code","b815cd47":"code","de5822a9":"code","62d285d6":"code","94513271":"markdown","0747eec5":"markdown","d4d6cfee":"markdown","4930e2b5":"markdown","465e968f":"markdown","169445be":"markdown","6ec6bb2d":"markdown","a7daa101":"markdown"},"source":{"f18ece6b":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9745e726":"data = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.head()","00d7206b":"data.shape","38ce1991":"data.info()","6cdad00d":"data.describe()","b7435e49":"data.isnull().sum()","74b373b9":"data.drop(['id', 'Unnamed: 32'], inplace=True, axis=1)","3572e18f":"data[['diagnosis']].value_counts()","1cd03e6a":"ax = data[['diagnosis']].value_counts().plot(kind='bar', figsize=(8, 6), title=\"Diagnosis Counts\")\nax.set_xlabel(\"Benign & Malignant\")\nax.set_ylabel(\"Frequency\")","7284d3d7":"data['diagnosis'] = data['diagnosis'].map( {'B': 1, 'M': 0} )","604e24c5":"plt.figure(figsize=(16,9))\nsns.heatmap(data.corr(), annot=True)\nplt.title(\"Correlation between Features\", fontsize=23)\nplt.show()","6a5c1aec":"data[data.columns[0:]].corr()['diagnosis'][:].sort_values(ascending=False)","aed478be":"fig, axs = plt.subplots( figsize=(15,8))\ndata.hist(ax=axs)\nplt.tight_layout()","1d8c446b":"def pop(df, values, axis=1):\n    if axis == 0:\n        if isinstance(values, (list, tuple)):\n            popped_rows = df.loc[values]\n            df.drop(values, axis=0, inplace=True)\n            return popped_rows\n        elif isinstance(values, (int)):\n            popped_row = df.loc[values].to_frame().T\n            df.drop(values, axis=0, inplace=True)\n            return popped_row\n        else:\n            print('values parameter needs to be a list, tuple or int.')\n    elif axis == 1:\n        # current df.pop(values) logic here\n        return df.pop(values)","86757e97":"poped_values = pop(data, [0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19], axis=0)\npoped_values","9a0833fe":"feature_cols = [c for c in data.columns if c not in ['diagnosis']]","6f681631":"X = data[feature_cols]\ny = data['diagnosis']\n\ntest_sample = poped_values[feature_cols]\ntest_result = poped_values['diagnosis']","57423834":"# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=23, max_iter=3000,tol=30.295954819192826))\nfeature_sel_model.fit(data[feature_cols], data['diagnosis'])","998ff763":"feature_sel_model.get_support()","8664f3ae":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = data[feature_cols].columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((data[feature_cols].shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint(selected_feat)\n#print('features with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_ == 0)))","ceda50b7":"X = X[selected_feat]\ny = data['diagnosis'] \n\ntest_sample = poped_values[selected_feat]\ntest_result = poped_values['diagnosis']","d577e5bd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","310eb8bb":"first_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy',\n                                  random_state = 42)\nfirst_model.fit(X_train, y_train)\n\npred_y = first_model.predict(X_test)\n\npreds = first_model.predict(X_train)\n\nprint(\"Accuracy:\", accuracy_score(y_test, pred_y))","056fbb25":"new_preds = first_model.predict(test_sample)\nprint(\"Accuracy:\", accuracy_score(test_result, new_preds))","e8b6e7be":"gbrt = GradientBoostingClassifier(random_state = 0, max_depth = 1)\ngbrt.fit(X_train, y_train)\n\n\nprint(\"Accuracy on training set:\", gbrt.score(X_train, y_train))\nprint(\"Accuracy on test set:\", gbrt.score(X_test, y_test))","4989387a":"second_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n\nsecond_model.fit(X_train, y_train)\n\npred_n = second_model.predict(X_test)\n\nprint(accuracy_score(y_test, pred_n))","b815cd47":"#Test 2\nnext_preds = first_model.predict(test_sample)\nprint(\"Accuracy:\", accuracy_score(test_result, next_preds))","de5822a9":"#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nfirst_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy',\n                                  random_state = 42)\nfirst_model.fit(X_train, y_train)\n\npred_y = first_model.predict(X_test)\n\npreds = first_model.predict(X_train)\n\nprint(\"Accuracy:\", accuracy_score(y_test, pred_y))","62d285d6":"#Using normalization\n\n\nXx = (X - np.min(X))\/(np.max(X)-np.min(X)).values\n\nX_train, X_test, y_train, y_test = train_test_split(Xx, y, test_size=0.3)\n\n\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 200)\nprint(\"test accuracy: {}% \".format((logreg.fit(X_train, y_train).score(X_test, y_test))*100))\nprint(\"train accuracy: {}%\".format((logreg.fit(X_train, y_train).score(X_train, y_train))*100))","94513271":"## Exploratory Data Analysis","0747eec5":"## POPING OUT 19 ROWS FROM THE DATAFRAME","d4d6cfee":"## Model 2","4930e2b5":"# Testing our Popped Data on Our Model","465e968f":"## Model 4","169445be":"# Model_3","6ec6bb2d":"## Feature Selection - Dimentionality Reduction","a7daa101":"### Modelling"}}