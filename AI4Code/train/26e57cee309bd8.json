{"cell_type":{"90f35357":"code","a8ae4e5b":"code","1341c259":"code","67145531":"code","cecf44fc":"code","5d4ccd4e":"code","58e29e41":"code","fe9d92ef":"code","7eb2a99e":"code","1a0bc940":"code","8c9e708f":"code","8b6559b4":"code","d555f372":"code","e8558c55":"code","53767d03":"code","a517d9bd":"code","f8a03edc":"code","47933e69":"code","6e725d43":"code","4706ac47":"code","c592b8e4":"code","2e1fef8e":"code","df5387a6":"code","128ec5e6":"code","e3a4ec61":"code","b0eb60b0":"code","b5d18a35":"code","5bcb3291":"code","8411e5b7":"code","776eca96":"code","9593a107":"code","a44f519c":"code","90278cbf":"code","decb4e7f":"code","4c314bb5":"code","d9c650c1":"code","31b52464":"code","d6540a07":"code","40d14474":"code","a8943f93":"code","09712737":"code","24d72ffb":"code","a200a383":"code","af26e0c1":"code","e8ac4e37":"code","c3635056":"code","af2d49c3":"code","e03cf9c1":"code","e45e5b35":"code","0064e7f4":"code","63695e0b":"code","9a766fc0":"code","51191858":"code","d5a43d18":"code","1f428b44":"markdown","6b09e989":"markdown","aab16a29":"markdown","ea5c6b77":"markdown","4428b3ae":"markdown","d72ece49":"markdown","a8a0034b":"markdown","04e144cb":"markdown","d998cbb8":"markdown","b68f8ad3":"markdown","3b19938a":"markdown","14183311":"markdown","d7ecd486":"markdown","d33837a8":"markdown","bfc6ba21":"markdown","14e7d8b0":"markdown","d5d0fab8":"markdown","8880b5aa":"markdown","6883b821":"markdown","a4b878cf":"markdown","5de6684e":"markdown","21c79c70":"markdown","de450b04":"markdown","eb6c39aa":"markdown","3be38831":"markdown","6f1ba5e7":"markdown","5f6fcc7d":"markdown","dcb09bdb":"markdown","f0d8fe89":"markdown","750c6964":"markdown","ce653627":"markdown","f22352e7":"markdown","d913d244":"markdown","a7593650":"markdown","c8454a89":"markdown","82f429c6":"markdown","1ec2ba99":"markdown","4f0bde06":"markdown"},"source":{"90f35357":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport numpy as np\nplt.style.use('fivethirtyeight')\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom mpl_toolkits.basemap import Basemap\nimport folium\nimport folium.plugins\nfrom matplotlib import animation,rc\nimport io\nimport base64\nimport pandas as pd\nfrom IPython.display import HTML, display\nimport warnings\nwarnings.filterwarnings('ignore')\nimport codecs\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8ae4e5b":"terror=pd.read_csv('\/kaggle\/input\/gtd\/globalterrorismdb_0718dist.csv',encoding='ISO-8859-1')","1341c259":"terror.shape","67145531":"columns=terror.columns\nfor i in columns:\n    print(i,'has',terror[i].nunique(),'unique columns')","cecf44fc":"terror.info(verbose=True,null_counts = False)","5d4ccd4e":"terror.columns.values","58e29e41":"terror.info(verbose=True,null_counts = False)","fe9d92ef":"terror.describe()","7eb2a99e":"df_cat=terror.select_dtypes(include='object',exclude='float')\ndf_cat.head(2)","1a0bc940":"df_cat.notnull().sum()\/len(df_cat)*100","8c9e708f":"round(100*(terror.isnull().sum()\/len(terror.index)),2).values","8b6559b4":"round(100*(terror['nkill'].isnull().sum()\/len(terror['nkill'].index)),2)","d555f372":"terror.isnull().sum().values","e8558c55":"terror.dropna(axis=1,inplace=True)","53767d03":"terror.shape","a517d9bd":"terror.isnull().sum()","f8a03edc":"terror.columns","47933e69":"df_cat=terror.select_dtypes(include='object',exclude='float')\ndf_cat.head(2)","6e725d43":"df_cat.columns","4706ac47":"from sklearn.preprocessing import LabelEncoder\nterror[['country_txt', 'region_txt', 'attacktype1_txt', 'targtype1_txt',\n       'gname', 'weaptype1_txt', 'dbsource']]=terror[['country_txt', 'region_txt', 'attacktype1_txt', 'targtype1_txt',\n       'gname', 'weaptype1_txt', 'dbsource']].apply(LabelEncoder().fit_transform)","c592b8e4":"terror.head()","2e1fef8e":"terror.cov()","df5387a6":"terror.corr()","128ec5e6":"plt.figure(figsize=(30,25))\nsns.heatmap(terror.corr(),annot=True)\nplt.show()","e3a4ec61":"from sklearn.preprocessing import StandardScaler","b0eb60b0":"sc = StandardScaler()\ndata = pd.DataFrame(sc.fit_transform(terror))\ndataframe=pd.DataFrame(data)","b5d18a35":"from sklearn.decomposition import PCA","5bcb3291":"pca=PCA()\npca.fit(data)","8411e5b7":"data_pca= pca.transform(data)\ndata_pca.shape","776eca96":"data_pca1=data_pca.copy()","9593a107":"pca.components_","a44f519c":"cumsum=np.cumsum(pca.explained_variance_ratio_)\ncumsum","90278cbf":"plt.figure(figsize=(10,12))\n\nplt.plot(range(1,30), cumsum, color='k', lw=2)\n\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\n\nplt.axvline(8, c='b')\nplt.axhline(0.9, c='r')\n\nplt.show()","decb4e7f":"pca = PCA(n_components=8)\npca.fit(data)\ndata_pca = pd.DataFrame(pca.transform(data))\ndata_pca.shape","4c314bb5":"data_pca","d9c650c1":"sns.pairplot(data_pca,diag_kind='kde',palette=\"Set2\")","31b52464":"from sklearn.cluster import KMeans\ncluster_range = range(1,10)\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters, n_init=10, max_iter=100)\n    clusters.fit(data_pca)\n    \n    cluster_errors.append(clusters.inertia_)\n    \npd.DataFrame({'num_clusters':cluster_range, 'Error': cluster_errors})","d6540a07":"clusters.inertia_","40d14474":"plt.figure(figsize=(10,5))\nplt.plot(cluster_range, cluster_errors, marker = \"o\" )\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error')\nplt.xticks(cluster_range)\nplt.show()","a8943f93":"kmeans = KMeans(2, n_init=5, max_iter=100)\nkmeans.fit(data_pca)\ndata_pca['label'] = kmeans.labels_\ndata_pca","09712737":"kmeans.inertia_","24d72ffb":"## from __future__ import print_function\n%matplotlib inline\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX=data_pca.values\n\nrange_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.Spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.Spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","a200a383":"from sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D","af26e0c1":"# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\n\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(data_pca)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1],X_reduced[:, 2], c=kmeans.labels_,\n           cmap=plt.cm.Set1, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","e8ac4e37":"kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2345)\nkmeans.fit(data_pca)\ncentroids = kmeans.cluster_centers_","c3635056":"centroids","af2d49c3":"centroid_df = pd.DataFrame(centroids, columns = list(data_pca) )","e03cf9c1":"centroid_df","e45e5b35":"kmeans.inertia_","0064e7f4":"from sklearn import metrics\nmetrics.silhouette_score(data_pca,kmeans.labels_)","63695e0b":"from sklearn.cluster import AgglomerativeClustering \nm2 = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='single')\nm2.fit(dataframe)","9a766fc0":"d1=pd.DataFrame(data_pca1)","51191858":"terror.corr().head(7)","d5a43d18":"d1.corr().head(3)","1f428b44":" **silhoute analysis is the appropriate metrics to support the model built(Kmeans, agglomerative)**\n\n**The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.**\n\n\n**From the above graphs and silhouette scores, we can tell that the the best silhouette score is when the k= 3,(0.26100908194691824) hence our initial assumption of k being 2 is inappropriate, because in case of that the silhouette is lower(0.23585264431806544) that that of 3.\nHence, optimal k is 3.**","6b09e989":"## Lets perform k means and aggolomerative clustering with optimal clusters","aab16a29":"### ELBOW METHOD:\nThe basic idea behind partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation [or total within-cluster sum of square (WCSS)] is low.\nIn order to have a low WCSS, the clusters should be more compact.\n\nDetermining the optimal number of clusters in a data set is a fundamental issue in partitioning clustering, such as k-means clustering, which requires the user to specify the number of clusters k to be generated.\n\nOne method to determine the optimal k is using the elbow method.\n\n- Plot the curve of wss according to the number of clusters k.\n- The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.","ea5c6b77":"**In the picture, though there is a certain degree of overlap, the points belonging to same category are distinctly clustered and region bound. This proves that the data captured in the first two Principal Components is informative enough to discriminate the categories from each other.**","4428b3ae":"**Based on the correlation table we can apply PCA by seeing the correlation among the independent variables or features.**\n\n\n**Here, the correlation among them is >0.5 which implies that there exists multicollinearity or noise in the data set. To remove this noise we use PCA and reduce the dimensions among the features.**\n\n\n**This reduces the noise or multicollinearity in the data set.**","d72ece49":"### For performing Unsupervised learning we will first see the covariance and correlation of the dataset to know about the multicollinearity","a8a0034b":"**Multicollinearity makes it hard to interpret your coefficients, and it reduces the power of your model to identify independent variables that are statistically significant.\nThese are definitely serious problems.\nClearly the PCA has helped in reducing the multicollinearity of the data, eg: iyear and event id previously had 0.99 correlation, which is now -1.789848e-15.It has also helped in the dimensionality reduction of our dataset.**","04e144cb":"The goal of clustering is to determine the intrinsic grouping in a set of unlabeled data.","d998cbb8":"let us plot a correlation plot for seeing the multicolinearity in the data","b68f8ad3":"We can see that here a lot of columns have a lot of null values going as high as 99.99 and as low as no nulls,since this data is based on real life attacks and incidents it is better for us if we try to drop the instead of  Null Value imputation, as it is risky and might lead to misleading results and interpretations.\n\nThis is also one of the limitations of the dataset,For example: the column \u2018nkill\u2019 has 10313 null values.(5.68%) If we impute these with mean, median or k nearest neighbors imputer it will depict that at every instance of an attack, there were fatalities. While in reality there were no fatalities reported in that particular event.","3b19938a":"Label encoding the categorical columns:","14183311":"github repository that was made on the same data after framing a problem statement: https:\/\/github.com\/meenujha\/Global-Terrorism-Database-Capstone","d7ecd486":"### Agglomerative clustering(Hierarchial Clustering)\n**Agglomerative clustering is a hierarchical cluster technique that builds nested clusters with a bottom-up approach where each data point starts in its own cluster and as we move up, the clusters are merged, based on a distance matrix.**\n\n**Agglomerative clustering: This is a \"bottom-up\" approach. each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.**\n\n**Can explain optimal clusters by the dendrogram**\n\n**affinity is nothing but method to calculate the distances, can be any of these:euclidean,manhattan,chebyshev,minkowski,jaccard.**\n\n**There are several ways to measure the distance between clusters in order to decide the rules for clustering,and they are often called *Linkage Methods*.**\n\n\n**linkages are 4 types:complete,single average,centroid and wards.**","d33837a8":"seems like the k or the optimal number of clusters is 2.","bfc6ba21":"# Global Terrorism Database\n![image.png](attachment:image.png)","14e7d8b0":"Lets make a pairplot to see the minimal number of clusters in the data:","d5d0fab8":"The total sum of squared distances of every data point from respective centroid is also called inertia. Let us print the inertia value for all K values.","8880b5aa":"**the below code will give us an account of the different k values and their respective silhouette scores.\nSilhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.**","6883b821":"### Dropping the nulls:","a4b878cf":"## KMEANS Clustering","5de6684e":"lets cluster again and check the inertia","21c79c70":"- The k-means algorithm aims to partition a set of objects, based on their attributes\/features, into k clusters, where k is a predefined constant. \n- The algorithm defines k centroids, one for each cluster. The centroid of a cluster is formed in such a way that it is closely related, in terms of similarity (where similarity can be measured by using different methods such as Euclidean distance or Extended Jacquard) to all objects in that cluster.\n- Technically, what k-means is interested in, is the variance. It minimizes the overall variance, by assigning each object to the cluster such that the variance is minimized. ","de450b04":"Data is Positively skewed or right skewed","eb6c39aa":"From above we can tell that the columns with strong correlation amongst themselves are:weaptype1 and attack_type1,attact_type1_txt,attack_type1 and attact_type1_txt,INT_ANY and INT_IDEO.","3be38831":"# Understanding the data","6f1ba5e7":"8 components explain 90% of the data according to  the graph.","5f6fcc7d":"## Principal Component Analysis","dcb09bdb":"**Scaling data is mandatory for pca.**\n\n**This is because the PCA calculates a new projection of your data set and the new axis are based on the standard deviation of your variables.\nSo a variable with a high standard deviation will have a higher weight for the calculation of axis than a variable with a low standard deviation.**\n\n**To avoid this,we normalize our data,so that all variables have the same standard deviation, thus all variables have the same weight and our PCA can now calculate the relevant axis.**","f0d8fe89":"### Evaluation of PCA results:","750c6964":"The above are our principal components,now lets calculate the cummulative sum and know how many components represent what number ","ce653627":"The dendrogram code was taking too long to get executed here and hence i did it in my jupyter notebook.\nThe results are as below","f22352e7":"#### WHY PCA?","d913d244":"# Checking for Nulls:","a7593650":"Compared to most types of criminal violence, terrorism poses special challenges to a nation and exhausts all of its resources in prevention of it including the loss of life. While the human cost is devastating, the economic impact may be larger than most realize. \n\nTerrorism is one of the parameters that tourists check for, before visiting a country and hence if a Nation has more prevalent terrorism, chances are, despite its fascinating tourist attractions, it might end up in little to no Tourism. \n\nIn response, there has been growing interest in researching about terrorism, their motives and most vulnerable target groups that are attacked.\n\nI have made tableau visualisations on the same on my Tableau profile, make sure you check it out here : https:\/\/cutt.ly\/xfLmyuy\n\nI have also made a separate analysis of the adversely affected countries, sad part, India is one of them : https:\/\/cutt.ly\/jfLmxfp\n\n\nSince the data has no well defined problem statement and large size, in this kernel i will be using the unsupervised learning techniques in order to see the hidden patterns in data and perform clustering.\n\nConsider upvoting if you liked the kernel,on that note, lets get started.\n","c8454a89":"## Lets see the categorical columns in the data, before we perform the USL on it.","82f429c6":"**Hence we conclude that the optimal K-value from Kmeans and Agglomerative Clustering is 3.**","1ec2ba99":"![image.png](attachment:image.png)","4f0bde06":"## Unsupervised Learning\n\nUnsupervised learning is the training of machine using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance. Here the task of machine is to group unsorted information according to similarities, patterns and differences without any prior training of data. \n\nAs the name suggests in unsupervised learning,there will be no supervision,that means no training will be given to the machine.\nMachine is restricted to find the hidden structure in unlabeled data by them-selves.\n\n\nIt allows the model to work on its own to discover patterns and information that was previously undetected. It mainly deals with unlabelled data.\n\n\nUSL is used for unlabelled data and is of two types : Clustering and association.\n\n1.Clustering:clustering is grouping a set of objects in such a manner that objects in the same group are more similar than to those object belonging to other groups.(will discuss a few examples here)\n\n\n2.Association:It is the process of measuring the degree of association between any 2 items.\neg: amazon recommendation engine: people who bought a stool, also bought a rope and a fan. ;)"}}