{"cell_type":{"e4161663":"code","67febe24":"code","4528f73a":"code","9ce7e655":"code","f6c7ca52":"code","c192298c":"code","51a96eb3":"code","25988a9b":"code","0fa0ba13":"code","996e6499":"code","c99be36c":"code","60f8bbc6":"code","7afe2c01":"markdown","bbaef343":"markdown","b8f18a5d":"markdown"},"source":{"e4161663":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67febe24":"#Ensemble Model Library\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport lightgbm as lgb","4528f73a":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings    \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","9ce7e655":"def lgbm_train(fullSet, params, start_range, end_range):\n    test_columns = [f\"d_{val}\" for val in range(start_range -7, end_range+1)]\n\n    test_set = fullSet[fullSet.d.isin(test_columns)]\n    train_set = fullSet[~fullSet.d.isin(test_columns)]\n    \n    del fullSet\n    \n    print('phase 1')\n    \n    X_train = train_set.drop(['id', 'd', 'demand'], axis=1)\n    y_train = train_set['demand']\n        \n    del train_set\n    \n    X_train.weekday = X_train.weekday.astype('uint8')\n    test_set.weekday = test_set.weekday.astype('uint8')\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=532)\n    gc.collect()\n    \n    X_train.reset_index(drop=True, inplace=True)\n    y_train.reset_index(drop=True, inplace=True)\n\n    test_set.reset_index(drop=True, inplace=True)\n\n    X_val.reset_index(drop=True, inplace=True)\n    y_val.reset_index(drop=True, inplace=True)\n    \n    print('phase 2')\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n    \n    del X_train, y_train, X_val, y_val\n    \n    model = lgb.train(params, train_set, valid_sets=[train_set, val_set], verbose_eval = 2) \n    \n    for i in test_columns:\n        X_i = test_set[test_set.d == i]\n        X_i_feat = X_i.drop(['id', 'd', 'demand'], axis=1)\n        X_i = X_i[['id', 'd']]\n\n        X_i_feat.reset_index(drop=True, inplace=True)\n        X_i.reset_index(drop=True, inplace=True)\n        y_i = pd.Series(model.predict(X_i_feat))\n\n        X_i = pd.concat([X_i, y_i], axis=1)\n        X_i.columns = ['id', 'd', 'demand']\n\n        test_set = test_set.merge(X_i, how='left', on=['id', 'd'])\n        test_set.demand_x = np.where(test_set.d == i, test_set.demand_y, test_set.demand_x)\n        test_set.drop('demand_y', axis=1, inplace=True)\n        test_set.columns = ['id', 'dept_id', 'cat_id', 'd', 'demand', 'weekday', 'snap_CA',\n               'snap_TX', 'sell_price_x', 'lag_t25', 'lag_t30', 'lag_t7', 'lag_t1',\n               'lag_t2', 'rolling_mean7', 'rolling_std7', 'rolling_mean2',\n               'rolling_std2', 'rolling_pricemean7', 'pricelag_t1',\n               'demand_price_mean7', 'demand_price_mean2']\n\n        test_set['lag_t7'] = test_set.groupby(['id'])['demand'].shift(7)\n        test_set['lag_t1'] = test_set.groupby(['id'])['demand'].shift(1)\n        test_set['lag_t2'] = test_set.groupby(['id'])['demand'].shift(2)\n        test_set['rolling_mean7'] = test_set.groupby(['id'])['demand'].shift(1).rolling(7).mean()\n        test_set['rolling_std7'] = test_set.groupby(['id'])['demand'].shift(1).rolling(7).std()\n        test_set['rolling_mean2'] = test_set.groupby(['id'])['demand'].shift(1).rolling(2).mean()\n        test_set['rolling_std2'] = test_set.groupby(['id'])['demand'].shift(1).rolling(2).std()\n\n        test_set['demand_price_mean7'] = test_set['rolling_mean7']\/test_set['rolling_pricemean7']\n        test_set['demand_price_mean2'] = test_set['rolling_mean2']\/test_set.groupby(['id'])['sell_price_x'].shift(1).rolling(2).mean()  \n    \n    test_columns = [f\"d_{val}\" for val in range(start_range, end_range+1)]\n    test_set = test_set[test_set.d.isin(test_columns)]\n    test_set = test_set[['id', 'd', 'demand']]\n    \n    test_set = test_set.pivot(index='id', columns='d')\n    \n    demand = test_set.demand\n    test_set.drop('demand', axis=1, inplace=True)\n    \n    test_set = pd.concat([test_set, demand], axis=1)\n    test_set.reset_index(drop=False, inplace = True)\n    \n    return test_set","f6c7ca52":"val_set = pd.read_csv('..\/input\/creating-features\/validation_set.csv')\nval_set, _ = reduce_mem_usage(val_set)","c192298c":"params = {'boosting_type': 'gbdt',\n        'n_jobs': -1,\n        'seed': 42,\n        'objective': 'regression',\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 1, \n        'colsample_bytree': 0.85,\n        'colsample_bynode': 0.85,\n        'min_data_per_leaf': 25,\n        'num_leaves': 45,\n        'lambda_l1': 0.5,\n        'lambda_l2': 0.5,\n        'num_iterations': 150,\n         }","51a96eb3":"valSubmission = lgbm_train(val_set, params, 1914, 1941)","25988a9b":"valSubmission.to_csv('valSubmission.csv', index=False)","0fa0ba13":"del val_set, valSubmission\ngc.collect()","996e6499":"eval_set = pd.read_csv('..\/input\/creating-features\/evaluation_set.csv')\neval_set, _ = reduce_mem_usage(eval_set)","c99be36c":"evalSubmission = lgbm_train(eval_set, params, 1942, 1969)","60f8bbc6":"evalSubmission.to_csv('evalSubmission.csv', index=False)","7afe2c01":"# Create Training Pipeline","bbaef343":"# Train Evaluation Set","b8f18a5d":"# Train Validation Set"}}