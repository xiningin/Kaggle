{"cell_type":{"5513a82e":"code","48149a07":"code","37dd23b5":"code","3f10ffbe":"code","9ee1a99a":"code","123acc64":"code","448a6e9f":"code","656a4294":"code","6cd569b3":"code","0c05106e":"code","b3ebb961":"code","f066a5e0":"code","bd635af7":"code","379d890c":"code","f486e4c0":"code","6e0c50de":"code","d936cee5":"code","56b50ec4":"code","e41cd14e":"code","16e52eb9":"code","1ed4fc13":"markdown","5d450f9a":"markdown","4d0fa677":"markdown","57f028e9":"markdown","ef065373":"markdown","8fb8cc95":"markdown","e53cf561":"markdown","83f6521b":"markdown","06eadf1f":"markdown","765f7d51":"markdown"},"source":{"5513a82e":"from tqdm import tqdm\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport tsfresh.feature_extraction.feature_calculators as tff\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","48149a07":"train = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","37dd23b5":"def extract_features(X_all, x, seg_id):\n    \n    X_all.loc[seg_id, 'ave'] = x.mean()\n    X_all.loc[seg_id, 'std'] = x.std()\n    X_all.loc[seg_id, 'max'] = x.max()\n    X_all.loc[seg_id, 'min'] = x.min()\n    X_all.loc[seg_id, 'abs_energy'] = np.dot(x, x)\n    X_all.loc[seg_id, 'sum_of_reoccurring_data_points'] = tff.sum_of_reoccurring_data_points(x)\n    X_all.loc[seg_id, 'sum_of_reoccurring_values'] = tff.sum_of_reoccurring_values(x)\n    X_all.loc[seg_id, 'count_above_mean'] = tff.count_above_mean(x)\n\n    return X_all","3f10ffbe":"rows = 150_000\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX_train_data = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min', 'abs_energy'])\nttf_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n\nfor seg_id in tqdm(range(segments)):\n    seg = train.iloc[seg_id*rows:seg_id*rows+rows]\n    x = seg['acoustic_data'].values.astype(float)\n    ttf_train.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n\n    X_train_data = extract_features(X_train_data, x, seg_id)","9ee1a99a":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\n","123acc64":"X_test_data = pd.DataFrame(columns=X_train_data.columns, dtype=np.float64, index=submission.index)","448a6e9f":"for seg_id in tqdm(X_test_data.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv', dtype={'acoustic_data': np.int16})\n    \n    x = seg['acoustic_data'].values.astype(float)\n    X_test_data = extract_features(X_test_data, x, seg_id)","656a4294":"X = pd.concat([X_train_data, X_test_data])\ny = np.append(np.zeros((X_train_data.shape[0], )), np.ones(X_test_data.shape[0], ))","6cd569b3":"n_fold = 5\nkf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","0c05106e":"lgb_classifier_params = {'num_leaves': 100,\n                         'min_data_in_leaf': 120,\n                         'objective': 'binary',\n                         'max_depth': -1,\n                         'learning_rate': 0.1,\n                         \"boosting\": \"gbdt\",\n                         \"metric\": 'auc',\n                         \"verbosity\": -1,\n                         }","b3ebb961":"f1s_valid = []\nf1s_train = []\nall_correctly_recognized = []\nfor fold_n, (train_index, valid_index) in enumerate(kf.split(X, y)):\n    print('\\nFold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    model = lgb.LGBMClassifier(**lgb_classifier_params, n_estimators=10000, n_jobs=-1)\n    model.fit(X_train, y_train, \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              verbose=100, early_stopping_rounds=200)\n\n    y_pred_valid = np.where(model.predict(X_valid, num_iteration=model.best_iteration_) > 0.5, 1, 0)\n    y_pred_train = np.where(model.predict(X_train, num_iteration=model.best_iteration_) > 0.5, 1, 0)\n\n    f1s_valid.append(f1_score(y_valid, y_pred_valid))\n    f1s_train.append(f1_score(y_train, y_pred_train))","f066a5e0":"print('CV mean train score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1s_train), np.std(f1s_train)))","bd635af7":"print('CV mean valid score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1s_valid), np.std(f1s_valid)))","379d890c":"print(classification_report(y_valid, y_pred_valid))","f486e4c0":"print(classification_report(y_valid, y_pred_valid))","6e0c50de":"features_importance = pd.DataFrame(sorted(zip(model.feature_importances_, X_train.columns.values)),  columns=['Value', 'Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=features_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","d936cee5":"def compare_splits_feature(column, quantile=1.0):\n    \n    q = X[column].quantile(quantile)\n    plt.figure(figsize=(13, 7))\n    plt.title(column)\n    plt.ylabel('Value')\n    plt.xlabel('Split')\n    sns.violinplot(x='split', y=column, data=X[X[column] < q])","56b50ec4":"X['split'] = y\ncompare_splits_feature('sum_of_reoccurring_data_points', 0.9)\ncompare_splits_feature('ave')\ncompare_splits_feature('count_above_mean')","e41cd14e":"pointer = np.where(valid_index < len(X_train_data))[0]\nvalid_ids_from_train_data = valid_index[pointer]\nX_valid_from_train_data = X_train_data.iloc[valid_ids_from_train_data, :]\ncorrect = y_valid[pointer] == y_pred_valid[pointer]\ncorrectly_recognized = np.where(correct == True)[0]","16e52eb9":"recognized = np.zeros((len(X_valid_from_train_data), ))\nrecognized[correctly_recognized] = 1\nX_valid_from_train_data['correctly_recognized'] = recognized\nX_valid_from_train_data['time_to_failure'] = ttf_train.iloc[valid_ids_from_train_data, :].values\nplt.figure(figsize=(13, 7))\nplt.title('Correctly recognized training examples vs time_to_failure')\nplt.ylabel('Value')\nplt.xlabel('Split')\nsns.violinplot(x='correctly_recognized', y='time_to_failure',\n               data=X_valid_from_train_data, ) #scale='count'\nplt.xticks([0, 1], ['False', 'True'])","1ed4fc13":"The datasets can be distinguished quite good.\nLet's check which features are most important","5d450f9a":"Most of the samples from the training data are recognized correctly. I'm disappointed because:\n- Plots show there are long samples in incorrectly recognized signals\n- The distribution of mistakes time to failure is wide. The peak doesn't tell me anything\n\nWork in progress","4d0fa677":"Classification report for the last fold","57f028e9":"### Dear Kagglers\n\nIf you like my kernel maybe you'll like my other work for this competition:\nhttps:\/\/www.kaggle.com\/davids1992\/watching-frequencies-in-the-data\n\nUpvotes are my motivation :)","ef065373":"The same with the test set","8fb8cc95":"Now we can prepare the cross-validation, where the training examples have label 0, and test examples have label 1.","e53cf561":"I'm going to use some popuular features, together with some different properties of the signal.","83f6521b":"## Adversarial validation\n\nBecause of the obvious problems with the validation in this competition, I was thinking about preparing [adversarial validation](http:\/\/fastml.com\/adversarial-validation-part-one\/)\n\n*Work in progress!*","06eadf1f":"We can visualize the distribution of the most interesting features","765f7d51":"There are some different properties in the test and training data. I'd like to understand them better! The next step is to analyze what training samples are similar to the test samples, and from which part of the earthquake they come.\n\nWe saved correctly recognized indices in correctly_recognized list. Let's plot what part of the signal was found correct.\n\nI base the analysis on the last validation split."}}