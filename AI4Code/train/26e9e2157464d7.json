{"cell_type":{"953e2105":"code","ef95dc47":"code","ea43e013":"code","9cb42419":"code","6f3dea00":"code","ca9c946d":"code","f540785a":"code","b218cdff":"code","a02870ab":"code","7d9cf5fc":"code","942124c9":"code","430f4a6f":"code","64a58808":"code","db79a61b":"code","38b4e1f4":"code","5c88d845":"code","fe039bc2":"code","81792780":"code","0552396f":"code","3bcc494c":"code","a378804c":"code","d94d7271":"code","1639a432":"code","ee09306f":"code","26aebd79":"code","ce9259ea":"code","e7edc48a":"code","2da9d281":"code","28fa537d":"code","5076d04c":"code","fd47a127":"code","d6e60104":"code","8bc264b1":"code","0b8ad80e":"code","d020024b":"code","33d23427":"code","27b54d03":"code","6304588b":"code","dfb5b7e9":"code","13a8e17d":"code","11f1068a":"code","06db1d37":"code","dd49e17b":"code","3fa34999":"code","9e2171cf":"code","84895d5d":"code","46e5b099":"code","8a0d7e3c":"code","a3dd1947":"code","fe962672":"code","248f4792":"code","5102f2c8":"code","7126d0ba":"code","5180f0c7":"code","cef683c0":"code","bfc222b0":"code","d9cd069b":"code","30033631":"code","6dbf5452":"code","5cf0234d":"code","29bbd71d":"code","59da563e":"markdown","1cddf2b3":"markdown","e3e3fd7f":"markdown","388d790a":"markdown","9b15974c":"markdown","df49a257":"markdown","6881dda3":"markdown","3fa15307":"markdown"},"source":{"953e2105":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef95dc47":"!unzip \"\/content\/drive\/My Drive\/Analytics_Vidya_cv\/av_independence_day\/train1.zip\"\n!unzip \"\/content\/drive\/My Drive\/Analytics_Vidya_cv\/av_independence_day\/test1.zip\"","ea43e013":"train = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv\")\ntest = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv\")","9cb42419":"train.head()","6f3dea00":"abstracts = train['ABSTRACT']\ntitles = train['TITLE']","ca9c946d":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\n\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords","f540785a":"train.shape","b218cdff":"filt = train['ABSTRACT'] != \"\"\ntrain = train[filt]\ntrain = train.dropna()","a02870ab":"train['ABSTRACT'][123]","7d9cf5fc":"print(\"Computer Science:\" + str(train[\"Computer Science\"][123]))\nprint(\"Physics:\" + str(train[\"Physics\"][123]))\nprint(\"Mathematics:\" + str(train[\"Mathematics\"][123]))\nprint(\"Statistics:\" + str(train[\"Statistics\"][123]))\nprint(\"Quantitative Biology:\" + str(train[\"Quantitative Biology\"][123]))\nprint(\"Quantitative Finance:\" + str(train[\"Quantitative Finance\"][123]))","942124c9":"labels = train[[\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]]\nlabels.head()","430f4a6f":"fig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 16\nfig_size[1] = 6\nplt.rcParams[\"figure.figsize\"] = fig_size\nlabels.sum(axis = 0).plot.bar()","64a58808":"import nltk\nnltk.download('stopwords')\ndef preprocess_text(text):\n    #remove punctuations and numbers\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    \n    #single character removal\n    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n    \n    #removing multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    new_text = \"\"\n    for word in text.split():\n        if word not in stopwords.words(\"english\"):\n            new_text = new_text + ' ' + word\n    \n    return new_text","db79a61b":"X = []\ntexts = list(train[\"ABSTRACT\"])\nfor t in texts:\n    X.append(preprocess_text(t))\ny = labels.values","38b4e1f4":"X[4]","5c88d845":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42, shuffle = True)","fe039bc2":"tokenizer = Tokenizer(num_words = 10000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index)+1\n\nmax_len = 200\n\nX_train = pad_sequences(X_train, padding = 'post', maxlen = max_len)\nX_test = pad_sequences(X_test, padding = 'post', maxlen = max_len)","81792780":"vocab_size","0552396f":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros","3bcc494c":"embeddings_dictionary = dict()\n\nglove_file = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt', encoding = \"utf8\")","a378804c":"for line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype = 'float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()","d94d7271":"y_test","1639a432":"embedding_matrix = zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","ee09306f":"inputs = Input(shape = (max_len,))\nembedding_layer = Embedding(vocab_size, 100, weights = [embedding_matrix], trainable = False)(inputs)\nLSTM_1 = LSTM(256)(embedding_layer)\ndense_1 = Dense(6, activation = 'sigmoid')(LSTM_1)\nmodel = Model(inputs = inputs, outputs = dense_1)\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])","26aebd79":"model.summary()","ce9259ea":"from keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","e7edc48a":"history = model.fit(X_train, y_train, batch_size = 128, epochs = 20, verbose = 1)","2da9d281":"score = model.evaluate(X_test, y_test, verbose = 1)\n\nprint(\"Loss : \", score[0])\nprint(\"Accuracy : \", score[1])","28fa537d":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(history.history['loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","5076d04c":"X_t = []\ntest_abs = list(test['ABSTRACT'])\nfor t in test_abs:\n    X_t.append(preprocess_text(t)) ","fd47a127":"X_t = tokenizer.texts_to_sequences(X_t)\nX_t = pad_sequences(X_t, padding = 'post', maxlen = max_len)\npred = model.predict(X_t)","d6e60104":"pred[0]","8bc264b1":"for i in range(pred.shape[0]):\n    for j in range(pred.shape[1]):\n        if pred[i][j] >= 0.45:\n          pred[i][j] = 1\n        else:\n          pred[i][j] = 0","0b8ad80e":"output = pd.DataFrame()\noutput['ID'] = test['ID']\noutput['Computer Science'] = pred[:, 0]\noutput['Physics'] = pred[:, 1]\noutput['Mathematics'] = pred[:, 2]\noutput['Statistics'] = pred[:, 3]\noutput['Quantitative Biology'] = pred[:, 4]\noutput['Quantitative Finance'] = pred[:, 5]","d020024b":"courses = [[\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]]\nfor column in courses:\n    output[column] = output[column].astype(int)","33d23427":"output.head()","27b54d03":"output.to_csv(\"Keras LSTM.csv\", index = False)","6304588b":"train = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv\")\ntest = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv\")","dfb5b7e9":"labels = train[[\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]]\nlabels.head()","13a8e17d":"filt = train['ABSTRACT'] != \"\"\ntrain = train[filt]\ntrain = train.dropna()","11f1068a":"import nltk\nnltk.download('stopwords')\ndef preprocess_text(text):\n    #remove punctuations and numbers\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    \n    #single character removal\n    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n    \n    #removing multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    new_text = \"\"\n    for word in text.split():\n        if word not in stopwords.words(\"english\"):\n            new_text = new_text + ' ' + word\n    \n    return new_text","06db1d37":"X = []\ntexts = list(train[\"ABSTRACT\"])\nfor t in texts:\n    X.append(preprocess_text(t))\ny = labels.values","dd49e17b":"traintmp = train.drop(['TITLE'],axis=1)","3fa34999":"df = traintmp\ndf['labels'] = list(zip(df['Computer Science'].tolist(), df.Physics.tolist(), df.Mathematics.tolist(), df.Statistics.tolist(),  df['Quantitative Biology'].tolist(), df['Quantitative Finance'].tolist()))\n# traintmp","9e2171cf":"df1 = df.drop([\"Computer Science\",\"Physics\", \"Mathematics\", \"Statistics\",\"Quantitative Biology\", \n         \"Quantitative Finance\", \"ID\"], axis=1)","84895d5d":"df1","46e5b099":"from sklearn.model_selection import train_test_split\ntrain_df, eval_df = train_test_split(df1, test_size=0.2)","8a0d7e3c":"!pip install simpletransformers","a3dd1947":"from simpletransformers.classification import MultiLabelClassificationModel","fe962672":"model = MultiLabelClassificationModel('roberta', 'roberta-base', num_labels=6, args={'train_batch_size':2, 'gradient_accumulation_steps':16, 'learning_rate': 3e-5, 'num_train_epochs': 3, 'max_seq_length': 200})","248f4792":"!pip install transformers==2.11.0","5102f2c8":"model.train_model(train_df)","7126d0ba":"result, model_outputs, wrong_predictions = model.eval_model(eval_df)","5180f0c7":"X_t = []\ntest_abs = list(test['ABSTRACT'])\nfor t in test_abs:\n    X_t.append(preprocess_text(t)) ","cef683c0":"preds, outputs = model.predict(X_t)","bfc222b0":"outputs[6]","d9cd069b":"preds[6]\n# df1 = df.drop([\"Computer Science\",\"Physics\", \"Mathematics\", \"Statistics\",\"Quantitative Biology\", \n#          \"Quantitative Finance\", \"ID\"], axis=1)","30033631":"sub_df = pd.DataFrame(preds,columns=[\"Computer Science\",\"Physics\", \"Mathematics\", \"Statistics\",\"Quantitative Biology\", \n         \"Quantitative Finance\"])","6dbf5452":"sub_df['ID'] = test['ID']","5cf0234d":"sub_df = sub_df[[\"ID\",\"Computer Science\",\"Physics\", \"Mathematics\", \"Statistics\",\"Quantitative Biology\", \n         \"Quantitative Finance\"]]","29bbd71d":"sub_df.to_csv('transformer(roberta3).csv', index=False)","59da563e":"Plotting the count of each topic","1cddf2b3":"train[\"ABSTRACT\"] = pd.Series(X)","e3e3fd7f":"We observe that the columns are in order of the occurences of the courses","388d790a":"# Loading the data","9b15974c":"Removing any empty string","df49a257":"# Creating the Text classification model","6881dda3":"# ROBERTA TRANSFORMER","3fa15307":"I will be using GLoVe word embeddings to convert text into numerals"}}