{"cell_type":{"37e0d768":"code","dfe02d3d":"code","389b77af":"code","7aded1bb":"code","aa08dc36":"code","a22cfe5e":"code","1c8a423e":"code","30e3fc42":"code","b693c8f4":"code","defe09fb":"code","9c22557a":"code","163837b5":"code","13824358":"code","547f5468":"code","e0533511":"code","b7f6ece3":"code","9e4b3172":"code","728e60ea":"code","2bb70652":"code","7ea025aa":"code","e95d00f8":"code","f63108c1":"code","36efed10":"code","b1538905":"code","b497889f":"code","f53cef60":"code","ab480f88":"code","15b106d5":"code","712002cb":"code","1259f567":"code","ed1b0653":"code","a5e1182e":"code","b177de41":"code","9c2cfb40":"code","8351c36e":"code","a1d08a9b":"code","2501dd60":"code","fe6dd98b":"code","61148dc9":"code","00a788c4":"code","d14ecac8":"code","d0186124":"code","3a64a22e":"code","562cbf3a":"code","d6eecb08":"code","dc878e86":"code","f0aa459b":"code","82789281":"code","03a34faa":"code","7894e6a8":"code","84e55fce":"code","420b6acb":"code","a5e8f471":"code","d094b775":"code","e9d40767":"code","7aba690e":"code","af695bdd":"code","142190c6":"markdown"},"source":{"37e0d768":"## Python\nimport os\nimport random\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n## Package\nimport glob \nimport keras\nimport IPython.display as ipd\nimport librosa\n\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.tools as tls\nimport seaborn as sns\nimport scipy.io.wavfile\nimport tensorflow as tf\npy.init_notebook_mode(connected=True)\n\n\n## Keras\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\nfrom keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import np_utils\nfrom keras.utils import to_categorical\n\nfrom keras.models import Model, Sequential\nfrom keras import optimizers\nfrom keras.layers import Input, Conv1D, Conv2D,BatchNormalization, MaxPooling1D,MaxPooling2D, LSTM, Dense, Activation, Layer,Reshape\n\nfrom keras.utils import to_categorical\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\n\n## Sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n## Rest\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom tqdm import tqdm_notebook as tqdm\n\ninput_duration=3\n# % pylab inline","dfe02d3d":"# Data Directory\n# Please edit according to your directory change.\nRavdess_path = \"..\/input\/ravdess-emotional-speech-audio\/\"\ndir_list = os.listdir(Ravdess_path)\ndir_list.sort()\nprint (dir_list)","389b77af":"# Create DataFrame for Data intel\nravdess_db = pd.DataFrame(columns=['path','source','actor', 'gender', 'emotion','emotion_lb'])\ncount = 0\n\nfor i in dir_list:\n    if i == 'audio_speech_actors_01-24':\n        continue\n    file_list = os.listdir(Ravdess_path + i)\n    for f in file_list:\n        nm = f.split('.')[0].split('-')\n        path = data_path + i + '\/' + f\n        src = int(nm[1])\n        actor = int(nm[-1])\n        emotion = int(nm[2])\n        source = \"Ravdess\"\n\n        if int(actor)%2 == 0:\n            gender = \"female\"\n        else:\n            gender = \"male\"\n\n        if nm[3] == '01':\n            intensity = 0\n        else:\n            intensity = 1\n\n        if nm[4] == '01':\n            statement = 0\n        else:\n            statement = 1\n\n        if nm[5] == '01':\n            repeat = 0\n        else:\n            repeat = 1\n\n        if emotion == 1:\n            lb = \"neutral\"\n        elif emotion == 2:\n            lb = \"calm\"\n        elif emotion == 3:\n            lb = \"happy\"\n        elif emotion == 4:\n            lb = \"sad\"\n        elif emotion == 5:\n            lb = \"angry\"\n        elif emotion == 6:\n            lb = \"fearful\"\n        elif emotion == 7:\n            lb = \"disgust\"\n        elif emotion == 8:\n            lb = \"surprised\"\n        else:\n            lb = \"none\"\n\n        ravdess_db.loc[count] = [path,source,actor, gender, emotion,lb]\n        count += 1","7aded1bb":"ravdess_db.head()","aa08dc36":"#ravdess_db.to_csv(\"\/content\/gdrive\/My Drive\/Colab Notebooks\/list.csv\")\nravdess_db.to_csv(\"list.csv\")","a22cfe5e":"ravdess_db['split'] =  np.where((ravdess_db.actor ==23) | (ravdess_db.actor ==24), 'Test', \n                                (np.where((ravdess_db.actor ==21) | (ravdess_db.actor ==22),'Val','Train')))","1c8a423e":"ravdess_db['split'].value_counts()","30e3fc42":"ravdess_db.drop(ravdess_db.index[ravdess_db['emotion_lb'] == 'surprised'], inplace = True)\nravdess_db.loc[ravdess_db.emotion_lb=='calm',['emotion','emotion_lb']]= 1,'neutral'","b693c8f4":"ravdess_db.emotion_lb.value_counts()","defe09fb":"dataset_db = ravdess_db.copy()","9c22557a":"dataset_db.emotion_lb = dataset_db.gender+\"_\"+dataset_db.emotion_lb","163837b5":"dataset_db.index=range(len(dataset_db.index))","13824358":"#dataset_db.to_csv(\"\/content\/gdrive\/My Drive\/Colab Notebooks\/list2.csv\")\ndataset_db.to_csv(\"list2.csv\")","547f5468":"dataset_db.emotion_lb.value_counts()\ndataset_db.sort_values(by=['path'], inplace=True)\ndataset_db.head()","e0533511":"dataset_db.index = range(len(dataset_db.index))","b7f6ece3":"dataset_db.shape","9e4b3172":"audio_duration = 3\nsampling_rate = 22050*2\ninput_length = sampling_rate * audio_duration\nn_mfcc = 20","728e60ea":"data_sample= np.zeros(input_length)\nMFCC = librosa.feature.mfcc(data_sample, sr=sampling_rate, n_mfcc=n_mfcc)","2bb70652":"MFCC.shape","7ea025aa":"dataset_db.split.value_counts()","e95d00f8":"from tqdm import tqdm_notebook as tqdm","f63108c1":"signal, sample_rate = librosa.load(modelling_db.path[0], res_type='kaiser_fast',sr=sampling_rate)\nsignal,index = librosa.effects.trim(signal,top_db = 25)\nsignal = scipy.signal.wiener(signal)\n\nif len(signal) > input_length:\n    signal = signal[0:input_length]\nelif  input_length > len(signal):\n    max_offset = input_length - len(signal)  \n    signal = np.pad(signal, (0, max_offset), \"constant\")","36efed10":"signal = np.array(signal).reshape(-1,1)\n","b1538905":"signal.shape","b497889f":"audios= np.empty(shape=(dataset_db.shape[0],128, MFCC.shape[1], 1))\n\ncount=0\nfor i in tqdm(range(len(dataset_db))):\n    signal, sample_rate = librosa.load(dataset_db.path[i], res_type='kaiser_fast',sr=sampling_rate)\n    signal,index = librosa.effects.trim(signal,top_db = 25)\n    signal = scipy.signal.wiener(signal)\n    \n    if len(signal) > input_length:\n        signal = signal[0:input_length]\n    elif  input_length > len(signal):\n        max_offset = input_length - len(signal)  \n        signal = np.pad(signal, (0, max_offset), \"constant\")\n\n    melspec = librosa.feature.melspectrogram(signal, sr=sample_rate, n_mels=128,n_fft=2048,hop_length=512)   \n    logspec = librosa.amplitude_to_db(melspec)\n    logspec = np.expand_dims(logspec, axis=-1)\n    audios[count,] = logspec \n    count+=1","f53cef60":"audios.shape","ab480f88":"import h5py\n#with h5py.File('\/content\/gdrive\/My Drive\/Colab Notebooks\/Ravdess_audio_Mel_spec.h5', 'w') as hf:\nwith h5py.File('Ravdess_audio_Mel_spec.h5', 'w') as hf:\n    hf.create_dataset(\"Ravdess_audio_Mel_spec\",  data=audios)","15b106d5":"import h5py\n#with h5py.File('\/content\/gdrive\/My Drive\/Colab Notebooks\/Ravdess_audio_Mel_spec.h5', 'r') as hf:\nwith h5py.File('Ravdess_audio_Mel_spec.h5', 'r') as hf:\n  audios = hf['Ravdess_audio_Mel_spec'][:]","712002cb":"librosa.display.specshow(audios[0].reshape(128,259))","1259f567":"x_train = audios[(dataset_db['split'] == 'Train')]\ny_train = dataset_db.emotion_lb[(dataset_db['split'] == 'Train')]\n\nprint(x_train.shape,y_train.shape)","ed1b0653":"x_test = audios[(dataset_db['split'] == 'Val')]\ny_test = dataset_db.emotion_lb[(dataset_db['split'] == 'Val')]\n\nprint(x_test.shape,y_test.shape)","a5e1182e":"x_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)\n\nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))","b177de41":"x_traincnn = x_train\nx_testcnn = x_test","9c2cfb40":"x_traincnn.shape,x_testcnn.shape,y_train.shape,y_test.shape","8351c36e":"# CNN I\/P Config\nnum_classes = len(np.unique(np.argmax(y_train, 1)))\ninput_shape = x_traincnn.shape[1:]\nlearning_rate = 0.0001\ndecay = 1e-6\nmomentum = 0.9\n\n#LSTM Configuration\nnum_lstm = 256\n","a1d08a9b":"input_shape","2501dd60":"model = Sequential(name='Audio_CNN_2D')\n\n# LFLB1\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last',input_shape=input_shape))\nmodel.add(BatchNormalization())\nmodel.add(Activation('elu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(filters=128,kernel_size=(3,3), strides=(1,1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('elu'))\nmodel.add(MaxPooling2D(pool_size=(4,4), strides=(4,4)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('elu'))\nmodel.add(MaxPooling2D(pool_size=(4,4), strides=(4,4)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('elu'))\nmodel.add(MaxPooling2D(pool_size=(4,4), strides=(4,4)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n# FC\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\n# Model compilation\nopt = optimizers.Adam(lr=0.001, beta_1=0.9,  beta_2=0.999, amsgrad=False)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])","fe6dd98b":"model.summary()","61148dc9":"#Train Config\n\nbatch_size = 16\nnum_epochs = 100\n\n# Model Training\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.000001)\n# Please change the model name accordingly.\n#mcp_save = ModelCheckpoint('\/content\/gdrive\/My Drive\/Colab Notebooks\/Models\/Audio_2DCNN_4L_R2.h5', save_best_only=True, monitor='val_categorical_accuracy', mode='max')\nmcp_save = ModelCheckpoint('Audio_2DCNN_4L_R2.h5', save_best_only=True, monitor='val_categorical_accuracy', mode='max')\ncnnhistory=model.fit(x_traincnn, y_train, batch_size=batch_size, epochs=num_epochs,validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])\n","00a788c4":"max(cnnhistory.history['val_categorical_accuracy'])","d14ecac8":"# Plotting the Train Valid Loss Graph\n\nplt.plot(cnnhistory.history['categorical_accuracy'])\nplt.plot(cnnhistory.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d0186124":"# Plotting the Train Valid Loss Graph\n\nplt.plot(cnnhistory.history['loss'])\nplt.plot(cnnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3a64a22e":"# Saving the model.json\n\nimport json\nmodel_json = model.to_json()\n#with open(\"\/content\/gdrive\/My Drive\/Colab Notebooks\/Models\/Audio_2DCNN_4L_R2.json\", \"w\") as json_file:\nwith open(\"Audio_2DCNN_4L_R2.json\", \"w\") as json_file:\n    json_file.write(model_json)\n","562cbf3a":"# loading json and creating model\nfrom keras.models import model_from_json\n#json_file = open('\/content\/gdrive\/My Drive\/Colab Notebooks\/Models\/Audio_2DCNN_4L_R2.json', 'r')\njson_file = open('Audio_2DCNN_4L_R2.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)","d6eecb08":"from keras.models import load_model\n\n# Returns a compiled model identical to the previous one\n#loaded_model.load_weights('\/content\/gdrive\/My Drive\/Colab Notebooks\/Models\/Audio_2DCNN_4L_R2.h5')\nloaded_model.load_weights('Audio_2DCNN_4L_R2.h5')","dc878e86":"#new_model = load_model('\/content\/gdrive\/My Drive\/Colab Notebooks\/Models\/Audio_2DCNN_4L.h5')\nnew_model = load_model('Audio_2DCNN_4L.h5')","f0aa459b":"# evaluate loaded model on test data\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","82789281":"x_test_data = audios[(dataset_db['split'] == 'Test')]\ny_test_data = dataset_db.emotion_lb[(dataset_db['split'] == 'Test')]\n\nprint(x_test_data.shape,y_test_data.shape)","03a34faa":"preds = loaded_model.predict(x_test_data,batch_size=16,verbose=1)\npreds1=preds.argmax(axis=1)\nabc = preds1.astype(int).flatten()\npredictions = (lb.inverse_transform((abc)))","7894e6a8":"preddf = pd.DataFrame({'predictedvalues': predictions})\npreddf[:10]","84e55fce":"actualdf = pd.DataFrame({'actualvalues': y_test_data})\nactualdf[:10]\nactualdf.index = range(len(actualdf.index))","420b6acb":"finaldf = pd.concat([actualdf,preddf],axis=1)\nfinaldf.head()","a5e8f471":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (9,6), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    ","d094b775":"from sklearn.metrics import accuracy_score\ny_true = finaldf.actualvalues\ny_pred = finaldf.predictedvalues\naccuracy_score(y_true, y_pred)*100","e9d40767":"from sklearn.metrics import f1_score\nf1_score(y_true, y_pred, average='macro') *100","7aba690e":"from sklearn.metrics import confusion_matrix\nc = confusion_matrix(y_true, y_pred)\nc","af695bdd":"class_names=sorted(set(finaldf.actualvalues))\nprint_confusion_matrix(c, class_names)\n","142190c6":"### RAVDESS DATASET"}}