{"cell_type":{"c8f7b562":"code","67ed4e66":"code","63f4b53b":"code","4661aea3":"code","b7bd582c":"code","31a03a6b":"code","fa572e1e":"code","97541ee7":"code","e7af75f9":"code","46ba3a8b":"code","aa1a9bb0":"code","acff1feb":"code","01774717":"code","9405fb08":"code","b8e6d663":"code","4bee9e8d":"code","ce35fe4d":"code","b4bd1bb3":"code","b902032e":"code","93a01851":"code","e33ceb91":"code","0e92accd":"code","62680255":"code","da7f1670":"code","5749f2a7":"code","171bf44b":"code","4cdcd398":"code","5f1b16fd":"code","1d78a476":"code","dc691221":"code","90f197a3":"code","bf61c76a":"code","fdf805ac":"markdown","84e01740":"markdown","0e43b62d":"markdown"},"source":{"c8f7b562":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport nltk\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('..\/input\/fraud-email-dataset\/fraud_email_.csv')\ndataset.head()","67ed4e66":"print (\"Number of Columns = \", dataset.shape[1])\nprint (\"Number of rows = \", dataset.shape[0])","63f4b53b":"requiredColumns = dataset.columns.values\nfor x in requiredColumns:\n    if(dataset[x].isnull().sum() > 0):\n        print (x)\n    ","4661aea3":"from nltk.corpus import stopwords\nimport string\n\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\",'...','nbsp','br','\/div','div'])\n\ndef CleanText(givenText):\n    reqText = givenText.lower()\n    reqText = re.sub(r\"=2e\", \"\", reqText)\n    reqText = re.sub(r\"=2c\", \"\", reqText)\n    reqText = re.sub(r\"\\=\", \"\", reqText)\n    reqText = re.sub(r\"news.website.http\\:\\\/.*\\\/.*502503.stm.\", \"\", reqText)\n    reqText = re.sub(r\"http:\/\/www.forcetacticalarmy.com\",\"\",reqText)\n    reqText = re.sub(r\"\\'s\", \" \", reqText)\n    reqText = re.sub(r\"\\'\", \" \", reqText)\n    reqText = re.sub(r\":\", \" \", reqText)\n    reqText = re.sub(r\"_\", \" \", reqText)\n    reqText = re.sub(r\"-\", \" \", reqText)\n    reqText = re.sub(r\"\\'ve\", \" have \", reqText)\n    reqText = re.sub(r\"can't\", \"can not \", reqText)\n    reqText = re.sub(r\"n't\", \" not \", reqText)\n    reqText = re.sub(r\"i'm\", \"i am \", reqText)\n    reqText = re.sub(r\"\\'re\", \" are \", reqText)\n    reqText = re.sub(r\"\\'d\", \" would \", reqText)\n    reqText = re.sub(r\"\\d\", \"\", reqText)\n    reqText = re.sub(r\"\\b[a-zA-Z]\\b\",\"\", reqText)\n    reqText = re.sub(r\"[\\,|\\.|\\&|\\;|<|>]\",\"\", reqText)\n    reqText = re.sub(r\"\\S*@\\S*\", \" \", reqText)\n    reqText = reqText.replace('_','')\n    sentenceWords = []\n    requiredWords = nltk.word_tokenize(reqText)\n    for word in requiredWords:\n        if word not in oneSetOfStopWords and word not in string.punctuation:\n            sentenceWords.append(word)\n    reqText = \" \".join(sentenceWords)     \n    return reqText\n\n","b7bd582c":"print (dataset.shape)\ndataset = dataset[dataset['Text'].notnull()]\nprint (dataset.shape)","31a03a6b":"%%time\nnewDataset = dataset[dataset['Text'].notnull()][:5000]\nnewDataset['cleaned_text'] = newDataset.Text.apply(lambda x: CleanText(x))\nnewDataset.head()","fa572e1e":"newDataset['Class'].value_counts()","97541ee7":"%%time\nfrom nltk.corpus import stopwords\nimport string\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\",'...','nbsp','br','\/div','div'])\n\ntotalWords = []\ncleanedSentences = newDataset['cleaned_text'].values\nfor x in range(0,len(cleanedSentences)):\n    tempWords = nltk.word_tokenize(cleanedSentences[x])\n    for a in tempWords:\n        totalWords.append(a)\nwordfreqdist = nltk.FreqDist(totalWords)\nmostcommon = wordfreqdist.most_common(100)\nprint(mostcommon)\n","e7af75f9":"import string\nimport re\nfrom os import listdir\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","46ba3a8b":"reqSentences = [list(x.split(\" \")) for x in cleanedSentences]","aa1a9bb0":"from gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\n\nplt.figure(figsize=(12,12))\n# train model\n\nmodel = Word2Vec(reqSentences, min_count=1)\n# save model\nmodel.save('model.bin')\n# fit a 2d PCA model to the vectors\nX = model[model.wv.vocab]\n#print (X[0])\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n# create a scatter plot of the projection\nplt.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\nplt.show()\n","acff1feb":"cleanedSentences = newDataset['cleaned_text'].values\nlabels = newDataset['Class'].values\n# integer encode the documents\nvocab_size = 100\nencoded_docs = [one_hot(d, vocab_size) for d in cleanedSentences]\n# pad documents to a max length of 4 words\nmax_length = 100\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# define the model\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 32, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nmodel.summary()\n# fit the model\nhistory = model.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))\n","01774717":"# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# integer encode and pad documents\ndef encode_docs(tokenizer, max_length, docs):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(docs)\n    # pad sequences\n    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n    return padded\n# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    return model","9405fb08":"np.random.seed(7)\ntrainDocuments = newDataset['cleaned_text'].values[:500]\nytrain = newDataset['Class'].values[:500]\nXtrain, Xtest, ytrain,ytest = train_test_split(trainDocuments, ytrain, test_size=0.2, random_state=1)\ntokenizer = create_tokenizer(Xtrain)\nvocabSize = len(tokenizer.word_index) + 1\n\nprint('Vocabulary size: %d' % vocabSize)\n# calculate the maximum sequence length\nmax_length = max([len(s.split()) for s in trainDocuments])\nprint('Maximum length: %d' % max_length)\n\nXtrain = encode_docs(tokenizer, max_length, Xtrain)\nXtest = encode_docs(tokenizer, max_length, Xtest)\n# define model\nmodel = define_model(vocabSize, max_length)\n# fit network\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\n_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\nprint('Train Accuracy: %f' % (acc*100))\n# evaluate model on test dataset\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))","b8e6d663":"def predict_sentiment(line,  tokenizer, max_length, model):\n    # clean review\n    padded = encode_docs(tokenizer, max_length, [line])\n    # predict sentiment\n    yhat = model.predict(padded, verbose=0)\n    # retrieve predicted percentage and label\n    print (\"The prediction - \", yhat)\n    percent_pos = yhat[0,0]\n    if round(percent_pos) == 0:\n        return (1-percent_pos), 'NEGATIVE'\n    return percent_pos, 'POSITIVE'","4bee9e8d":"text1 = ['Everyone', 'enjoy', 'film', 'I', 'love', 'recommended']\npercent, sentiment = predict_sentiment(text1, tokenizer, max_length, model)\nprint('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text1, sentiment, percent*100))\n# test negative text\ntext2 = ['This', 'bad', 'movie', 'Do', 'watch', 'It', 'sucks']\npercent, sentiment = predict_sentiment(text2, tokenizer, max_length, model)\nprint('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text2, sentiment, percent*100))","ce35fe4d":"testingWords = []\nwords = nltk.word_tokenize(text1)\nfor word in words:\n    if word not in oneSetOfStopWords and word not in string.punctuation:\n        testingWords.append(word)\nprint (testingWords)","b4bd1bb3":"dataset[dataset['Text'].notnull()][:10]","b902032e":"sentences = newDataset['cleaned_text'].values\nlabels = newDataset['Class'].values","93a01851":"from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(sentences, labels, test_size = 0.25)","e33ceb91":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(Xtrain))\nlist_tokenized_train = tokenizer.texts_to_sequences(Xtrain)\nlist_tokenized_test = tokenizer.texts_to_sequences(Xtest)","0e92accd":"maxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","62680255":"import gensim.models.keyedvectors as word2vec\nword2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\nembed_size = 300","da7f1670":"reqSentences = [row.split(\" \") for row in sentences]","5749f2a7":"import gensim\nmodel = gensim.models.Word2Vec(\n    reqSentences,\n    size=150,\n    window=5,\n    min_count=1,\n    workers=10,\n    iter=10)","171bf44b":"model['business']","4cdcd398":"model.most_similar('dear')","5f1b16fd":"model.most_similar('money')","1d78a476":"embed_size = 150\nembeddings_index = dict()\nfor word in model.wv.vocab:\n    embeddings_index[word] = model.wv.word_vec(word)\nprint('Loaded %s word vectors.' % len(embeddings_index))\ngc.collect()\n#We get the mean and standard deviation of the embedding weights so that we could maintain the \n#same statistics for the rest of our own random generated weights. \nall_embs = np.stack(list(embeddings_index.values()))\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\nnb_words = len(tokenizer.word_index)\n#We are going to set the embedding size to the pretrained dimension as we are replicating it.\n#the size will be Number of Words in Vocab X Embedding Size\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\ngc.collect()\n\n#With the newly created embedding matrix, we'll fill it up with the words that we have in both \n#our own dictionary and loaded pretrained embedding. \nembeddedCount = 0\nfor word, i in tokenizer.word_index.items():\n    i-=1\n    #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n    embedding_vector = embeddings_index.get(word)\n    #and store inside the embedding matrix that we will train later on.\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector\n        embeddedCount+=1\nprint('total embedded:',embeddedCount,'common words')\n\ndel(embeddings_index)\ngc.collect()\n\n#finally, return the embedding matrix","dc691221":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gensim.models.keyedvectors as word2vec\nimport gc","90f197a3":"inp = Input(shape=(maxlen, ))\nx = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\nx = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\nprint (model.summary())","bf61c76a":"%%time\nbatch_size = 32\nepochs = 4\nmodel.fit(X_t,ytrain, batch_size=batch_size, epochs=epochs, validation_data=(X_te, ytest), verbose=2)","fdf805ac":"<h2>Using the \"Tokenizer\" and \"Sequential and Dense\" model<\/h2>","84e01740":"<h4>The Embedding layer is defined as the first hidden layer of a network.There are 3 arguments which you must define:<\/h4>\n<ul>\n<li>input dim: This is the size of the vocabulary in the text data. For example, if your data\nis integer encoded to values between 0-10, then the size of the vocabulary would be 11\n    words.<\/li>\n <li>output dim: This is the size of the vector space in which words will be embedded. It\ndefines the size of the output vectors from this layer for each word. For example, it could\nbe 32 or 100 or even larger. Test different values for your problem.<\/li>\n<li>input length: This is the length of input sequences, as you would define for any input\nlayer of a Keras model. For example, if all of your input documents are comprised of 1000\nwords, this would be 1000.<\/li>\n<\/ul>","0e43b62d":"model.similarity('business', 'transaction')"}}