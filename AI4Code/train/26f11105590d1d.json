{"cell_type":{"191264aa":"code","78d6d047":"code","39f9d73c":"code","6bdd7932":"code","2f5bc81f":"code","be57c99f":"code","309b8207":"code","2e0e130f":"code","9c0f0b3b":"code","a8fe4410":"code","60603fb2":"code","62a029da":"code","b0b5af9e":"code","f3332773":"code","69391592":"code","e20c26d0":"code","8b53eeaa":"code","3ea9021f":"code","2a275d95":"code","860facd6":"code","5279e88b":"code","ae16d6c0":"code","8dcbbf3e":"code","c7b3d7de":"code","08d574e1":"code","961101ab":"code","380e8389":"markdown","17708dbe":"markdown","f90ccabf":"markdown","629d9210":"markdown","d64ad849":"markdown","5757f68c":"markdown","6356314e":"markdown","0b35fcdf":"markdown","99a06420":"markdown","16e5248f":"markdown","5a550261":"markdown","d87b07ab":"markdown"},"source":{"191264aa":"# Importa\u00e7\u00e3o das bibliotecas\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport seaborn as sns","78d6d047":"# Base de treino\ntrain = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv',header=0,na_values='?')\ntrain.head()","39f9d73c":"# Base de teste (classifica\u00e7\u00e3o)\ntest = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv',header=0,na_values='?')\ntest.head()","6bdd7932":"# Features\n\n\n# Gr\u00e1fico em barras de cada feature\nfor i in train.columns:\n    sns.displot(y=i,hue='income',data=train,multiple='stack')","2f5bc81f":"# Feature 'Id' \u00e9 \u00fanica para cada objeto, logo \u00e9 desnecess\u00e1ria para o problema\n# Al\u00e9m disso, feature 'education' \u00e9 redundante com 'education.num'\ntrain = train.drop(columns=['Id','education'])","be57c99f":"# Checar se h\u00e1 valores faltantes (na)\ncheck_na = train.isna().any()\ncheck_na","309b8207":"# Obter moda (valor de maior frequ\u00eancia) das features com dados faltantes\nmode_na = train[['workclass','occupation','native.country']].mode()\nmode_na","2e0e130f":"# Preenchimento de dados faltantes pela moda da feature\nfor i in train.columns[check_na]:\n    train[i] = train[i].fillna(mode_na[i][0])\n\n# Checar se ainda h\u00e1 valores faltantes\ntrain.isna().any()","9c0f0b3b":"# Visualiza\u00e7\u00e3o das features em rela\u00e7\u00e3o ao target em gr\u00e1ficos\n# Features (int64)\nnum_columns = train.select_dtypes(['int64']).columns\nfor i in num_columns:\n    sns.catplot(x=\"income\",y=i,kind=\"boxen\",data=train)","a8fe4410":"# A maioria dos gr\u00e1ficos apresenta vis\u00edvel diferen\u00e7a entre 'incomes',\n# com exce\u00e7\u00e3o de 'fnlwgt'. Assim, essa feature tamb\u00e9m pode ser retirada.\ntrain = train.drop(columns=['fnlwgt'])","60603fb2":"# Features (object)\nenc_income = preprocessing.OrdinalEncoder()\ntrain['income'] = enc_income.fit_transform(train['income'].values.reshape((-1,1)))\n\ncat_columns = train.select_dtypes(['object']).columns\nfor i in cat_columns:\n    sns.catplot(x=\"income\",y=i,kind='bar',data=train)","62a029da":"# Vis\u00edvel diferen\u00e7a de 'income' entre valores das 'features'\n# Entretanto, alta varia\u00e7\u00e3o de 'income' para 'native.country', provavelmente\n# devido \u00e0 diferen\u00e7a de amostragem de valores.\n# Troca dos valores diferentes de 'United-States' (moda) por 'Others'\nothers = np.delete(train['native.country'].unique(),[0])\ntrain['native.country'] = train['native.country'].replace(others,'Others')\nsns.catplot(x=\"income\",y='native.country',kind='bar',data=train)","b0b5af9e":"# Outro que possui diferen\u00e7a razo\u00e1vel de amostragem \u00e9 'race'\nothers = np.delete(train['race'].unique(),[0])\ntrain['race'] = train['race'].replace(others,'Others')\nsns.catplot(x=\"income\",y='race',kind='bar',data=train)","f3332773":"# Converter dados do tipo object para int64 (necess\u00e1rio para uso de alguns modelos)\nenc = preprocessing.OrdinalEncoder()\ntrain[cat_columns] = enc.fit_transform(train[cat_columns])\n\ntrain.head()","69391592":"# Refazer o que foi feito na base de treino:\n# Checar se h\u00e1 na\ncheck_na = test.isna().any()\ncheck_na","e20c26d0":"# Trocar na por moda (mesmos casos que na base de treino)\nfor i in test.columns[check_na]:\n    test[i] = test[i].fillna(mode_na[i][0])\n\n# Checar se tudo foi preenchido\ntest.isna().any()","8b53eeaa":"# Drop das features desnecess\u00e1rias\ny_pred = pd.DataFrame()\ny_pred['Id'] = test['Id']\ntest = test.drop(columns=[\"Id\",\"education\",\"fnlwgt\"])\n\n# Troca por 'Others'\nothers = np.delete(test['native.country'].unique(),np.where(test['native.country'].unique()=='United-States'))\ntest['native.country'] = test['native.country'].replace(others,'Others')\nothers = np.delete(test['race'].unique(),np.where(test['race'].unique()=='White'))\ntest['race'] = test['race'].replace(others,'Others')\n\n# Converter dados categ\u00f3ricos em num\u00e9ricos\ntest[cat_columns] = enc.transform(test[cat_columns])\n\ntest.head()","3ea9021f":"import matplotlib as mpl\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing","2a275d95":"# Separa\u00e7\u00e3o em features e targets\ny_train = train['income']\nX_train = train.drop(columns=['income'])\nX_test = test\n\n# Normaliza\u00e7\u00e3o: limita\u00e7\u00e3o em [-1,1]\nmax_abs_scaler = preprocessing.MaxAbsScaler().fit(X_train)\nX = max_abs_scaler.transform(X_train)\n# Normaliza\u00e7\u00e3o: m\u00e9dia nula e vari\u00e2ncia unit\u00e1ria\nscaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)\nX_train[:] = X\n\n# O mesmo para X_test:\nX = max_abs_scaler.transform(X_test)\nX = scaler.transform(X)\nX_test[:] = X","860facd6":"from sklearn.svm import SVC\nkernels = ['linear','poly']\n\nfor i in kernels:\n    print (\"Kernel type: \" + i)\n    model = SVC(kernel=i)\n    score = cross_val_score(model,X_train,y_train,cv=5)\n    print (\"Accuracy = \" + str(np.mean(score)))","5279e88b":"from sklearn.ensemble import RandomForestClassifier\n\nstart = 5\nend = 30\nh = 2\nn_estimators = [10, 50, 100, 200]\ndepth = []\n\nfor nestimators in n_estimators:\n    scores = []\n    for i in range(start,end,h):\n        model = RandomForestClassifier(n_estimators=nestimators, criterion='gini', max_depth=i)\n        score = cross_val_score(model,X_train,y_train,cv=5)\n        scores.append(np.mean(score))\n        depth.append(h*scores.index(max(scores))+start)\n    print(\"Max accuracy: \" + str(max(scores)) + \", estimators: \" + str(nestimators) + \", depth: \" + str(depth[-1]))\n    mpl.pyplot.plot(range(start,end,h), scores, label ='estm:'+str(nestimators))\n    \nmpl.pyplot.xlabel('max depth')\nmpl.pyplot.legend()\nmpl.pyplot.title('Random Forest - Accuracy')","ae16d6c0":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodel = LinearDiscriminantAnalysis()\nscore = cross_val_score(model,X_train,y_train,cv=5)\nprint (\"Accuracy = \" + str(np.mean(score)))","8dcbbf3e":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nscore = cross_val_score(model,X_train,y_train,cv=5)\nprint (\"Accuracy = \" + str(np.mean(score)))","c7b3d7de":"# Treinamento do modelo\nmodel = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=15)\nmodel.fit(X_train,y_train)\n\n# Classifica\u00e7\u00e3o da base de teste\ny_pred['income'] = model.predict(X_test)\ny_pred.head()","08d574e1":"# Transformar os dados para categ\u00f3ricos novamente\ny_pred['income'] = enc_income.inverse_transform(y_pred['income'].values.reshape((-1,1)))\ny_pred.head()","961101ab":"# Submiss\u00e3o\ny_pred.to_csv('submission.csv', index = False)","380e8389":"## Logistic Regression","17708dbe":"# Importa\u00e7\u00e3o das bases e bibliotecas","f90ccabf":"# Classifica\u00e7\u00e3o da Base de Teste e Submiss\u00e3o","629d9210":"# Modelos de Aprendizado de M\u00e1quina\n\nEscolha baseada na m\u00e9trica precis\u00e3o.\n\nValida\u00e7\u00e3o por cross-validation de 5 folds.","d64ad849":"PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es - 2\u00ba semestre de 2021\n\nClassifica\u00e7\u00e3o na base Adult (2\u00aa atividade)","5757f68c":"## Compara\u00e7\u00e3o dos resultados\nComparando a acur\u00e1cia de cada modelo, o melhor foi o Random Forest com 100~200 estimadores. Por\u00e9m, como a diferen\u00e7a entre os modelos de Random Forest \u00e9 pequena, foi escolhido o com 50 estimadores por ser mais simples e ainda manter a acur\u00e1cia.","6356314e":"## Base de Teste: Tratamento dos dados","0b35fcdf":"## SVC","99a06420":"# Base de dados Adult\n\n<table>\n  <tr>\n    <th>Feature<\/th>\n    <th>Definition<\/th>\n    <th>Type<\/th>\n    <th>Values<\/th>\n  <\/tr>\n  <tr>\n    <td>Id<\/td><td>Number of identification of observed individual<\/td><td>int64<\/td><td>unique to each individual<\/td>\n  <\/tr>\n  <tr>\n    <td>age<\/td><td>Age of observed individual<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr> \n  <tr>\n    <td>workclass<\/td><td>Workclass of observed individual<\/td><td>object<\/td><td>Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked<\/td>\n  <\/tr>\n  <tr>\n    <td>fnlwgt<\/td><td>Final weight assigned by the US census bureau to each observation. It's the number of people the census believes\nthe entry represents<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr>\n  <tr>\n    <td>education<\/td><td>Education of observed individual<\/td><td> object<\/td><td>Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool<\/td>\n  <\/tr>\n  <tr>\n    <td>education.num<\/td><td>Number-coded education of observed individual<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr>\n  <tr>\n    <td>marital.status<\/td><td>Marital status of observed individual<\/td><td>object<\/td><td>Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse<\/td>\n  <\/tr>\n  <tr>\n    <td>occupation<\/td><td>Occupation of observed individual<\/td><td>object<\/td><td>Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces<\/td>\n  <\/tr>\n  <tr>\n    <td>relationship<\/td><td>Relationship status of observed individual<\/td><td>object<\/td><td>Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried<\/td>\n  <\/tr>\n  <tr>\n    <td>race<\/td><td>Race of observed individual<\/td><td>object<\/td><td>Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black<\/td>\n  <\/tr>\n  <tr>\n    <td>sex<\/td><td>Sex of observed individual<\/td><td>object<\/td><td>Female, Male<\/td>\n  <\/tr>\n  <tr>\n    <td>capital.gain<\/td><td>Capital gain of observed individual<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr>\n  <tr>\n    <td>capital.loss<\/td><td>Capital loss of observed individual<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr>\n  <tr>\n    <td>hours.per.week<\/td><td>Hours worked per week by observed individual<\/td><td>int64<\/td><td>continuous<\/td>\n  <\/tr>\n  <tr>\n    <td>native.country<\/td><td>Native country of observed individual<\/td><td>object<\/td><td>United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.<\/td>\n  <\/tr>\n<\/table>\n<table>\n  <tr>\n    <th>Target&nbsp;&nbsp;<\/th>\n    <th>Definition<\/th>\n    <th>Type<\/th>\n    <th>Values<\/th>\n  <\/tr>\n  <tr>\n    <td>income&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/td>\n    <td>Income of observed individual<\/td>\n    <td>object&nbsp;<\/td><td>>50K, &lt=50K&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/td>\n  <\/tr>\n<\/table>","16e5248f":"## Random Forest","5a550261":"## Base de Treino: An\u00e1lise e Tratamento dos dados","d87b07ab":"## Linear Discriminant Analysis"}}