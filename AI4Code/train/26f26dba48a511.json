{"cell_type":{"9d6a62dc":"code","3da2b56e":"code","3b3d48dd":"code","57c250d6":"code","376e4907":"code","a25038fe":"code","b75bd0b5":"code","7f64ec19":"code","1782b804":"code","a652960d":"code","35c2712c":"code","e3029443":"code","736b2713":"code","aff8c083":"code","909202c7":"code","8a687c46":"markdown","84f3e4bc":"markdown","636ad13a":"markdown","fd0f4804":"markdown","392a83bb":"markdown","9c54f375":"markdown","06004676":"markdown","94944c41":"markdown","c27c8b9b":"markdown","a886f02c":"markdown","791b48ac":"markdown","98ca0e4d":"markdown","cb8a5d6f":"markdown","dd742a3e":"markdown","3eb04633":"markdown","424295de":"markdown","b6256ed9":"markdown","d47f5131":"markdown","142b7fbf":"markdown","367bece9":"markdown"},"source":{"9d6a62dc":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3da2b56e":"abalone_df = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')\nabalone_df.columns = [\"sex\",\"length\",\"diameter\",\"height\",\"whole_weight\",\"shucked_weight\",\"viscera_weight\",\"shell_weight\",\"rings\"]","3b3d48dd":"sns.heatmap(abalone_df.corr(), cmap='BrBG', annot=True)","57c250d6":"#Drop two columns\nabalone_df = abalone_df.drop([\"whole_weight\", \"diameter\"],axis=1)","376e4907":"#Set M and F as A, use .unique() to show the unique values in the columns\nabalone_df.loc[abalone_df.sex==\"M\",\"sex\"] = \"A\"\nabalone_df.loc[abalone_df.sex==\"F\",\"sex\"] = \"A\"\nabalone_df['sex'].unique()","a25038fe":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\none_hot_sex = pd.DataFrame(enc.fit_transform(abalone_df[['sex']]).toarray())\none_hot_sex.columns = [enc.categories_[0][0],enc.categories_[0][1]]\n#Join the one-hot columns to the whole data frame\nabalone_df = abalone_df.loc[:,abalone_df.columns != 'sex'].join(one_hot_sex)\nabalone_df.head()","b75bd0b5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(abalone_df.loc[:,abalone_df.columns != 'rings'], \n                                                    abalone_df['rings'], test_size=0.15, random_state=42)","7f64ec19":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline","1782b804":"svr = make_pipeline(StandardScaler(), SVR(kernel='rbf', C = 1.5,gamma=\"auto\"))\nada_br = AdaBoostRegressor(svr,random_state=42, n_estimators=4,learning_rate=0.1)\nada_br.fit(X_train,y_train)","a652960d":"test_mae = [mean_absolute_error(y_test,np.around(y_pred)) for y_pred in ada_br.staged_predict(X_test)]\ntrain_mae = [mean_absolute_error(y_train,np.around(y_pred)) for y_pred in ada_br.staged_predict(X_train)]","35c2712c":"print('The test MAE at each stage according are', np.around(test_mae,3))\nprint('The train MAE at each stage according are', np.around(train_mae,3))","e3029443":"plt.plot(range(5)[1:], train_mae,color='b',label = \"train MAE\")\nplt.plot(np.arange(5)[1:], test_mae,color='r',label = \"test MAE\")\nplt.title(\"MAE for training set and test set\")\nplt.xlabel('Model 1 to Model 4')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","736b2713":"#Actual vs predicted values\ndef fitted_vs_actual(model,X_test,X_train,y_true_test,y_true_train,titles):\n    #Make prediction from the adaboost model\n    y_pred_test = np.around(model.predict(X_test))\n    y_pred_train = np.around(model.predict(X_train))\n    #plotting\n    fig, axes = plt.subplots(1, 2,figsize=(15,5))\n    colors = ['green','#01B6B7']\n    y_pred = [y_pred_test,y_pred_train]\n    y_true = [y_true_test,y_true_train]\n    for i in range(2):\n        sns.regplot(x = y_pred[i],y = y_true[i], color=colors[i],ax = axes[i])\n        axes[i].title.set_text(titles[i])\n        axes[i].set(ylabel='actual values',xlabel='predicted values')","aff8c083":"fitted_vs_actual(ada_br,X_test,X_train,y_test,y_train,['Test Data','Training Data'])","909202c7":"def dis_error(model,X_test,X_train,y_test,y_train):\n    fig, axes = plt.subplots(1, 2,figsize=(18,5))\n\n    #The raw error for test\n    y_pred_test = np.around(model.predict(X_test))\n    y_pred_train = np.around(model.predict(X_train))\n    \n    sns.countplot(x = (y_pred_test-y_test).to_numpy(),ax=axes[0])\n    axes[0].title.set_text('Test set raw error')\n    axes[0].set_xlabel('Raw error')\n    axes[0].set_ylabel('Frequency')\n\n    sns.countplot(x=(y_pred_train-y_train).to_numpy(),ax=axes[1])\n    axes[1].title.set_text('Training set raw error')\n    axes[1].set_xlabel('Raw error')\n    axes[1].set_ylabel('Frequency')\ndis_error(ada_br,X_test,X_train,y_test,y_train)","8a687c46":"**Doing better in training set**\n\nThe plots below show the actual values of compressive Strength against the predicted values. The closer the data points to the straight line, the more accuracy the prediction is. \n\nThe model is doing better in the training set, the graph of the right is the training data, the data points are narrower and closer to the line on the middle, whereas the data points are more spreaded in the test set. However, in fact, the performance of the two models are similar. They are doing pretty well, but training data is better.","84f3e4bc":"Perform a random 85\/15 split of the data to form a training set and a test set. It is performed by using train_test_slit from sklearn.","636ad13a":"# Model evaluation","fd0f4804":"# Summary\n\n***Compare the performance of the AdaBoost regressor on the training set and test set***","392a83bb":"As shown below, the sex column is removed, and two binary columns, A and I are added.","9c54f375":"From the line graph above, we can see that both training set and test set have *decreasing MAE* as the number of model increase. This means the subsequence models are 'learning' what the previous model did poorly and adjust it. \n\nWe can see the last model is doing the best since it has learned what the previous models were doing not good enough. It is done by re-assigned the weight to each instance, those instances with higher loss would have a higher weight. The loss in sklearn adaboost is least sqaure error. \n\nEach subsequent estimator is grown from previously grown estimator on the same set of training data with different weight. As a result of various weight, it is hoped that the later estimator can make a better prediction on what the previous model did poorly.","06004676":"*Adaboost hyperparameter*\n\nLearning rate shrinks the contribution of each regressor by learning_rate. There is a trade-off between learning_rate and *n_estimators*. svr itself is doing pretty well, dropping down the learning rate to control this trade-off. Higher number of estimators could lead to overfitting, to avoid it, one of the way is to reduce the *learning rate*. In fact, the result from grid search does suggest that the model should have a lower n_estimator and learning_rate. This might help the adaboost model to train with the data without overfitting.\n\n*SVR hyperparameter*\n\n*C* is SVR is another regularization parameter, higher C lead to a header penality on margin violation. \n\nUsing *kernel* rbf is because it generally do well as it can handle non-linearity as well.\n\n*gamma* parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'.\n\n**Optimisation:**\n\nI tried several ***combinations of hyperparameter in SVR***, as well as the adaboostregressor. For support vector regressor, the parameters searched are:\n\n*C* : 1.5,5 and, 2 ; \n*gamma* : auto, scale. \n\nFor the adaboostrgressor \n*learning_rate* : 0.1,5,1 \n*n_esimators* : 4 or 5. \n\nSearching the combination from the above values, I found the current combintaion has the lowest 3-folds cross validation error by mean squared error(neg_mean_squared_error). So, this combination is selected.\n\nThe parameters:\n\nC = 1.5\n\ngamma = auto\n\nlearning_rate = 0.1\n\nn_estimators = 4\n\nrandom_state is randomly selected, this is setting to ensure that when other run the code here, the result will remain the same.","94944c41":"***Distribution of the raw prediction***\n\n**Accuary predictions are the majority (zero error)**\n\nMost of the raw prediction errors are zero. This means most of the prediction is exactly what the true values is, which is found in both training set and test set. \n\n**Mostly underestimate**\n\nThe model has a wider range of number of ring that were underestimated, which is the negative side. The range of the raw error for test set are from -1 to -11, whereas that of the training set are from -1 to -17. Both dataset have wider range in underestimation than in overestimation. Overestimation for are up to 4 and 5 for test set and training set respectively.","c27c8b9b":"For the text column, relabel the \u2019M\u2019 and \u2019F\u2019 instances in your Python code as \u2019A\u2019 (abbrev. for Adult). You should then use one-hot encoding to convert this column into numerical columns.","a886f02c":"# Model building","791b48ac":"np.around is used to round up the number to integer. \n\nUsing mean_absolute_error to obtain the mean absolute error from the training adn test sets in each model in adaboost. \n\nStaged_predict is used here, which return the predicted values for each model (internediate estimators and the final estimator) in the adaboost.","98ca0e4d":"*The MAE* at each stage, as we have 4 estimators, we have 4 errors in total.","cb8a5d6f":"**Performance on training set is similar to that on test set**\n\nThe AdaBoost regressor performs better in the training set which is expected, although the difference between them are small. The MAE for the test set and training set are 1.478 and 1.433 respectively. It is because the training set is the data that used to train the model, while the test data is the ***unseen data*** to the model. It is reasonable to expect that the model performance on the test set is not as good as that on the training set, which the MAE and the distribution tell us the same thing. For the plots of the actual values vs predicted values, the spread of the data points are similar in both dataset. The model is doing slightly better in the training set as the data points are a little bit closer to the line of the actual value. However, overall speaking, there performance are similar. The MAE is about 1.4 for both set.\n\n**Performance on subsequent model is going better**\n\nThe second thing is that as the number of estimator increase, the performance of the regressor is goining better, regardless which dataset it is. It shows that the model doesn't overfit to the training data, and it performance has improved in the sub-sequent models. As the MAE is dropping from the first model to the last model in the adaboost regressor.","dd742a3e":"Support Vector regressor are sensitive to scale of the data, so I use StandardScaler to scale the data before they fit into the model.","3eb04633":"First, read the data set and assigne appropriate column's names to the data.","424295de":"From the correlations between pairs of features, determine which two features should be removed for the subsequent training and testing steps. ","b6256ed9":"**Inspect the data and perform the following data cleaning**","d47f5131":"ring values(response) are what we want our AdaBoost regressor to predict. And there are 8 attributes in this data.","142b7fbf":"# Train test split","367bece9":"***Dimensions of features before and after data cleaning***\n\nBefore the cleaning, we have 8 features. After the cleaning, we have 7 features that are slightly different."}}