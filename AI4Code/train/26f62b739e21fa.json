{"cell_type":{"409b4710":"code","ce8e23a7":"code","c99f5904":"code","f7c05b8c":"code","217d18c1":"code","85cb4d62":"code","1d9fac0f":"code","8940e410":"code","ec56ee39":"code","7f6adf4d":"code","00078689":"code","09bf4bfd":"code","74407d5d":"code","0e32dcff":"code","84ada8d5":"code","cf08a951":"code","cc51785a":"code","7113777d":"code","54e86da3":"code","b80e3b25":"code","eafd1409":"code","9602a7f8":"code","0cf632d9":"code","77cf737a":"code","b4b9c79c":"code","8d6d942a":"code","86033ced":"code","aa96f0c1":"code","54a30d20":"code","ea58d26d":"code","cd5098df":"code","d3612dee":"code","ab40f6a1":"markdown","895a963a":"markdown","9bfd745e":"markdown","ceb85970":"markdown","b96aa82a":"markdown","b08e577d":"markdown","8237b775":"markdown","58a8a005":"markdown","f5321fca":"markdown","35553f86":"markdown","540edf0e":"markdown","c246310e":"markdown","06ad81e0":"markdown","ab68b128":"markdown","29c07c95":"markdown","068841ea":"markdown","1f275345":"markdown","50485cf9":"markdown","525979ac":"markdown","a1facb8a":"markdown","fbc0a42a":"markdown","885cd3cf":"markdown","d5ce19a2":"markdown","86a4d8ad":"markdown","52c44667":"markdown"},"source":{"409b4710":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ce8e23a7":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ndata = train.append(test)\n","c99f5904":"print('Train has {} files and {} columns'.format(train.shape[0], train.shape[1]))\n","f7c05b8c":"train.info()","217d18c1":"train.head()","85cb4d62":"print('Train has {} files and {} columns'.format(test.shape[0], test.shape[1]))\n","1d9fac0f":"test.head()\n","8940e410":"test.info()\n","ec56ee39":"train.select_dtypes(int).nunique()","7f6adf4d":"test.select_dtypes(int).nunique() \n","00078689":"print('Empty values by column in Train ', (train.isnull().sum()))\n","09bf4bfd":"print('Empty values by column in Test ',(test.isnull().sum()))\n","74407d5d":"data['Title'] = data['Name']\n\nfor name_string in data['Name']:\n    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata.replace({'Title': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = data.groupby('Title')['Age'].median()[titles.index(title)]\n    data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] = age_to_impute\n    \n# Substituting Age values in TRAIN and TEST:\ntrain['Age'] = data['Age'][:891]\ntest['Age'] = data['Age'][891:]","0e32dcff":"data['Family_Size'] = data['Parch'] + data['SibSp']\n\ntrain['Family_Size'] = data['Family_Size'][:891]\ntest['Family_Size'] = data['Family_Size'][891:]","84ada8d5":"drop_column = [ 'Cabin', 'Ticket',  'Parch', 'SibSp', 'Name', 'Embarked']","cf08a951":"train.drop(drop_column, axis = 1, inplace = True)\ntest.drop(drop_column, axis = 1, inplace = True)\n","cc51785a":"mapping = {'male':1, 'female':0}\ntrain['Sex'] = train['Sex'].replace(mapping).astype(np.float64)\ntest['Sex'] = test['Sex'].replace(mapping).astype(np.float64)","7113777d":"\ntest['Fare'].fillna(value = test['Fare'].mode()[0], inplace = True)\n\n","54e86da3":"from collections import Counter\ndef detect_outliers(df,n,features):\n\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers\nOutliers_to_drop = detect_outliers(train,2,[[\"Pclass\", \"Sex\", \"Age\", \"Fare\",\"Family_Size\",\"Survived\"]])\ntrain.loc[Outliers_to_drop]\n\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","b80e3b25":"plt.figure(figsize=[16,18])\nplt.subplot(234)\nplt.hist(x = [train[train['Survived']==1]['Fare'], train[train['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()","eafd1409":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival by class\")\nplt.show()\n","9602a7f8":"def correlation_heat(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heat(train[[\"Pclass\", \"Sex\", \"Age\",  \"Fare\",\"Survived\"]])\n","0cf632d9":"features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_Size\"]\n#Columns to wrok with in Train\nX_train = train[features] #define training features set\ny_train = train[\"Survived\"] \n#Columns to work with in Test\nX_test = test[features] #define testing features set\n","77cf737a":"from sklearn.model_selection import train_test_split \nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) \n\n","b4b9c79c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier\n","8d6d942a":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nimport warnings \nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import f1_score, make_scorer\n\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)","86033ced":"model_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(X_train, y_train, model, name, model_results = None, sort = True):\n    #10 fold cross validation model##\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 10,\n                                scoring = scorer, n_jobs = -1)\n    print('Mean ', round(cv_scores.mean(), 5),'STd', round(cv_scores.std(), 5))\n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model':name,\n                                                'cv_mean': cv_scores.mean(),\n                                                'cv_std' : cv_scores.std()},\n                                                index = [0]), ignore_index = True)\n    return model_results\n\nmodel_results = cv_model(X_train, y_train, LinearSVC(), 'LSVC', \n                         model_results)\n\nmodel_results = cv_model(X_train, y_train, \n                         GaussianNB(), 'GNB', model_results)\n\nmodel_results = cv_model(X_train, y_train, \n                         MLPClassifier(hidden_layer_sizes=(16, 32, 64, 64, 32)),\n                         'MLP', model_results)\n\nmodel_results = cv_model(X_train, y_train, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)\n\nmodel_results = cv_model(X_train, y_train, \n                         RidgeClassifierCV(), 'RIDGE', model_results)\n\nfor n in [5, 10, 20]:\n    print('\\nKNN with {n} neighbors\\n', n)\n    model_results = cv_model(X_train, y_train, \n                             KNeighborsClassifier(n_neighbors = n),\n                             n, model_results)\n    \nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(X_train, y_train, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)\n\nmodel_results = cv_model(X_train, y_train,\n                          RandomForestClassifier(100, random_state=10),\n                              'RF', model_results)\nfrom xgboost import XGBClassifier\nmodel_results = cv_model(X_train, y_train,\n                          XGBClassifier(n_estimators= 100, random_state=10),\n                              'XGB', model_results)\n\nmodel_results.set_index('model', inplace = True)","aa96f0c1":"print(model_results)","54a30d20":"model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']))\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","ea58d26d":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score \n\nxg_clf = XGBClassifier()\nparameters_rf = {'n_estimators' : [200],'learning_rate': [0.1],\n              'max_depth': [4], \"min_child_weight\":[6], \"gamma\":[0], \"subsample\":[0.80] \n                              }\n\n\ngrid_rf = GridSearchCV(xg_clf, parameters_rf, scoring=make_scorer(accuracy_score))\n#make_scorer(accuracy_score)\ngrid_rf.fit(X_training, y_training)\nxg_clf = grid_rf.best_estimator_\n\npred_xg = xg_clf.predict(X_valid)\nacc_xg = accuracy_score(y_valid, pred_xg)\nprint(\"The Score for XGBoost is: \" + str(acc_xg))\nprint(\"The best Score for XGBoost is: \" + str(grid_rf.best_score_))\n\n\n","cd5098df":"\n\n\nrf_clf = RandomForestClassifier()\n\nparameters_rf = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]}\n\ngrid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))\ngrid_rf.fit(X_training, y_training)\n\nrf_clf = grid_rf.best_estimator_\n\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(\"The Score for Random Forest is: \" + str(acc_rf))\n\nprint(\"The best Score for Random Forest is: \" + str(grid_rf.best_score_))","d3612dee":"submission_predictions = rf_clf.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\n         \"Survived\": submission_predictions})\n \nsubmission.to_csv(\"titanicprediction.csv\", index=False)\nprint(submission.shape)","ab40f6a1":"Barplot to determine Survival by Class","895a963a":"to create validation data set","9bfd745e":"**Supervised Learning Estimators\n**\nHere we are detecting wich is the best model to predict the passengers status in test ","ceb85970":"**Splitting data**","b96aa82a":"\n\n\nThe objective of this competition is to know who passengers were likely to survive at Titanic's accident\nWe are exploring differents topics since visualizing, report, and present the problem solving steps and to find the final solution.\nFirst is neccesary to import packages to work with.\n","b08e577d":" **Predict most accuracy output RandomForest Classifier **","8237b775":"To know the first 6 rows of train data","58a8a005":"**Score data**","f5321fca":"Mapping str values to convert into float\n","35553f86":"**Correlation**","540edf0e":"**Predict most accuracy output XGBoost **","c246310e":"**Fill NaN values on Test","06ad81e0":"Create new column joining Parch and SibSp columns ","ab68b128":"Detect outliers","29c07c95":"Export a submission file with the predictions of test Dataset","068841ea":"Importing data","1f275345":"Drop unnecesary columns","50485cf9":"**Clean Data**\nWrangle, prepare, cleanse the data\nDetecting null values on each column ","525979ac":"To know how many different int values are for each column","a1facb8a":"Create a function to stock results of each model","fbc0a42a":"Infer missing data from known data","885cd3cf":"To know the total of columns","d5ce19a2":"**Plotting**","86a4d8ad":"To determine if Fare influence in Survival rate","52c44667":"Working with Title and Name columns to fill out Age Nan values "}}