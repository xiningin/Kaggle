{"cell_type":{"47d2356e":"code","4c2f023c":"code","76cee200":"code","32d46b06":"code","85d884a3":"code","d798206d":"code","cf89957d":"code","e1f7a49e":"code","d4202c21":"code","217e8084":"code","7bbdd9db":"code","10bf9239":"markdown","279a4c72":"markdown","869faf7d":"markdown","77aab0fc":"markdown","cd62d47d":"markdown","95e42e0c":"markdown","f01070c9":"markdown","b2afc866":"markdown","962d1c32":"markdown","24e07d3a":"markdown"},"source":{"47d2356e":"%matplotlib inline\nfrom copy import copy\nimport gc\nimport joblib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport sys\nfrom warnings import simplefilter\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast","4c2f023c":"class CFG:\n    \n    model_name = 'best_model'\n\n    data_dir = Path('..\/input\/commonlitreadabilityprize')\n    train_file = data_dir \/ 'train.csv'\n    test_file = data_dir \/ 'test.csv'\n    sample_file = data_dir \/ 'sample_submission.csv'\n\n    build_dir = Path('.\/build\/')\n    output_dir = build_dir \/ model_name\n    trn_encoded_file = output_dir \/ 'trn.enc.joblib'\n    val_predict_file = output_dir \/ f'{model_name}.val.txt'\n    submission_file = 'submission.csv'\n\n    pretrained_dir = '..\/input\/tfbert-large-uncased'\n\n    id_col = 'id'\n    target_col = 'target'\n    text_col = 'excerpt'\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    max_len = 205\n    n_fold = 5\n    n_est = 10\n    n_stop = 2\n    batch_size = 8\n    seed = 42","76cee200":"train_df = pd.read_csv(CFG.train_file, index_col=CFG.id_col)\ntest_df = pd.read_csv(CFG.test_file, index_col=CFG.id_col)\ny = train_df[CFG.target_col].values\nprint(test_df.shape, y.shape, train_df.shape)\ntrain_df.head()","32d46b06":"class Tokenize:\n    \n    def load_tokenizer():\n        \n        if not os.path.exists(CFG.pretrained_dir + '\/vocab.txt'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n            tokenizer.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained tokenizer')\n            tokenizer = BertTokenizerFast.from_pretrained(CFG.pretrained_dir)\n\n        model_config = BertConfig.from_pretrained(CFG.pretrained_dir)\n        model_config.output_hidden_states = True\n        \n        return tokenizer, model_config","85d884a3":"class BERT:\n    \n    def load_bert(config):\n        \n        if not os.path.exists(CFG.pretrained_dir + '\/tf_model.h5'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\", config=config)\n            bert_model.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained model')\n            bert_model = TFBertModel.from_pretrained(CFG.pretrained_dir, config=config)\n        return bert_model\n\n    def bert_encode(texts, tokenizer, max_len=CFG.max_len):\n        \n        input_ids = []\n        token_type_ids = []\n        attention_mask = []\n\n        for text in texts:\n            token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                             add_special_tokens=True)\n            input_ids.append(token['input_ids'])\n            token_type_ids.append(token['token_type_ids'])\n            attention_mask.append(token['attention_mask'])\n\n        return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","d798206d":"tokenizer, bert_config = Tokenize.load_tokenizer()\n\nX = BERT.bert_encode(train_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)\nX_tst = BERT.bert_encode(test_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)\ny = train_df[CFG.target_col].values\nprint(X[0].shape, X_tst[0].shape, y.shape)","cf89957d":"joblib.dump(X, CFG.trn_encoded_file)","e1f7a49e":"class model:\n\n    def build_model(bert_model, max_len=CFG.max_len):    \n        \n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n        token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n        attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n        sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n        clf_output = sequence_output[:, 0, :]\n        clf_output = Dropout(.1)(clf_output)\n        out = Dense(1, activation='linear')(clf_output)\n\n        model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n        model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\n        return model\n    \n    def scheduler(epoch, lr, warmup=5, decay_start=10):\n        \n        if epoch <= warmup:\n            return lr \/ (warmup - epoch + 1)\n        elif warmup < epoch <= decay_start:\n            return lr\n        else:\n            return lr * tf.math.exp(-.1)","d4202c21":"class Train:\n    \n    def train():\n\n        ls = LearningRateScheduler(model.scheduler)\n        es = EarlyStopping(patience=CFG.n_stop, restore_best_weights=True)\n\n        cv = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\n        p = np.zeros_like(y, dtype=float)\n        p_tst = np.zeros((X_tst[0].shape[0], ), dtype=float)\n        for i, (i_trn, i_val) in enumerate(cv.split(X[0]), 1):\n            print(f'Training CV #{i}:')\n            tf.random.set_seed(CFG.seed + i)\n\n            bert_model = BERT.load_bert(bert_config)\n            clf = model.build_model(bert_model, max_len=CFG.max_len)\n            if i == 1:\n                print(clf.summary())\n            history = clf.fit([x[i_trn] for x in X], y[i_trn],\n                              validation_data=([x[i_val] for x in X], y[i_val]),\n                              epochs=CFG.n_est,\n                              batch_size=CFG.batch_size,\n                              callbacks=[ls])\n            clf.save_weights(f'{CFG.model_name}_cv{i}.h5')\n\n            p[i_val] = clf.predict([x[i_val] for x in X]).flatten()\n            p_tst += clf.predict(X_tst).flatten() \/ CFG.n_fold\n\n            K.clear_session()\n            del clf, bert_model\n            gc.collect()\n        \n        return p, p_tst","217e8084":"p, p_test = Train.train()","7bbdd9db":"print(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(CFG.val_predict_file, p, fmt='%.6f')","10bf9239":"<h1 style=\"border:2px solid Purple;text-align:center\">2. Defining Configs \ud83d\udcac<\/h1>","279a4c72":"<h1 style=\"border:2px solid Purple;text-align:center\">Topics Covered \ud83d\udccc<\/h1>\n\n**1. Importing Libraries \ud83d\udcda**\n\n**2. Defining Configs \ud83d\udcac**\n\n**3. Data Pipeline \ud83d\udcc2**\n\n**4. Training Pipeline \ud83c\udfaf**\n\n**5. Evaluation \ud83d\udd8a**","869faf7d":"Hope you liked the notebook !!","77aab0fc":"<h1 style=\"border:2px solid Purple;text-align:center\">3. Data Pipeline \ud83d\udcc2<\/h1>","cd62d47d":"This notebook is aimed at folks trying to get started with the competition in Tensorflow & BERT. We will be going through the entire training pipeline, so sit tight.\n\nIt is very simple and intuitive to understand. I hope it is useful for everyone !!","95e42e0c":"**Please remember to upvote the notebook, if you liked the content !!** \ud83d\ude03\ud83d\ude03 ","f01070c9":"<h1 style=\"border:2px solid Purple;text-align:center\">1. Importing Libraries \ud83d\udcda<\/h1>","b2afc866":"<h1 style=\"border:2px solid Purple;text-align:center\">4. Training Pipeline \ud83c\udfaf<\/h1>","962d1c32":"<h1 style=\"border:2px solid Purple;text-align:center\">Introduction \ud83c\udf96<\/h1>","24e07d3a":"<h1 style=\"border:2px solid Purple;text-align:center\">5. Evaluation \ud83d\udd8a<\/h1>"}}