{"cell_type":{"ebd61f5a":"code","83d763da":"code","65c919b9":"code","777a5a05":"code","eb0a849e":"code","f9ab0f11":"code","ffad4faa":"code","6c49809c":"code","033eacfc":"code","c315dfe6":"code","0b647a95":"code","88708558":"code","c89f08db":"code","ebbd443f":"code","1eecff29":"code","d6a696b7":"code","c1a28ed6":"code","f1128218":"code","5a913f22":"code","cf615da5":"code","86e2ea13":"code","4167c348":"code","fec4a7ea":"code","67f53dd9":"markdown","52e754ba":"markdown","4064c74a":"markdown","4c5c7435":"markdown","8f69728a":"markdown","115f7225":"markdown","08cb8ef0":"markdown","6c7bd2f3":"markdown","ae33c757":"markdown"},"source":{"ebd61f5a":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ncolnames=['ts',\n              'uid',\n              'id.orig_h',\n              'id.orig_p',\n              'id.resp_h',\n              'id.resp_p',\n              'proto',\n              'service',\n              'duration',\n              'orig_bytes',\n              'resp_bytes',\n              'conn_state',\n              'local_orig',\n              'local_resp',\n              'missed_bytes',\n              'history',\n              'orig_pkts',\n              'orig_ip_bytes',\n              'resp_pkts',\n              'resp_ip_bytes',\n              'label']\n\npd.set_option('display.max_columns', None) # To display all the columns","83d763da":"def load_data(filepath):\n    df = pd.read_table(filepath, names=colnames, skiprows=8)\n    df.drop(df.shape[0]-1, inplace=True)\n    return df","65c919b9":"# rootdir = 'small_dataset_iot23'\nrootdir = '..\/input\/small-dataset-iot23'\n\nall_dfs = []\n\nfor subdir, dirs, files in os.walk(rootdir):\n    for file in files:\n        filepath = os.path.join(subdir, file)\n        if filepath.endswith('conn.log.labeled'):\n            curr_df = load_data(filepath)\n            all_dfs.append(curr_df)\n\nfor df in all_dfs:\n    print(df.shape)","777a5a05":"df_merged = pd.concat(all_dfs, ignore_index=True)\nprint(df_merged.shape)","eb0a849e":"df_merged.head()","f9ab0f11":"df_merged['label'].value_counts()","ffad4faa":"df_merged.loc[(df_merged.label == '-   Malicious   PartOfAHorizontalPortScan'), 'label'] = 'PartOfAHorizontalPortScan'\ndf_merged.loc[(df_merged.label == '(empty)   Malicious   PartOfAHorizontalPortScan'), 'label'] = 'PartOfAHorizontalPortScan'\ndf_merged.loc[(df_merged.label == '-   Malicious   Okiru'), 'label'] = 'Okiru'\ndf_merged.loc[(df_merged.label == '(empty)   Malicious   Okiru'), 'label'] = 'Okiru'\ndf_merged.loc[(df_merged.label == '-   Benign   -'), 'label'] = 'Benign'\ndf_merged.loc[(df_merged.label == '(empty)   Benign   -'), 'label'] = 'Benign'\ndf_merged.loc[(df_merged.label == '-   benign   -'), 'label'] = 'Benign'\ndf_merged.loc[(df_merged.label == '-   Malicious   DDoS'), 'label'] = 'DDoS'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C'), 'label'] = 'C&C'\ndf_merged.loc[(df_merged.label == '(empty)   Malicious   C&C'), 'label'] = 'C&C'\ndf_merged.loc[(df_merged.label == '-   Malicious   Attack'), 'label'] = 'Attack'\ndf_merged.loc[(df_merged.label == '(empty)   Malicious   Attack'), 'label'] = 'Attack'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C-HeartBeat'), 'label'] = 'C&C-HeartBeat'\ndf_merged.loc[(df_merged.label == '(empty)   Malicious   C&C-HeartBeat'), 'label'] = 'C&C-HeartBeat'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C-FileDownload'), 'label'] = 'C&C-FileDownload'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C-Torii'), 'label'] = 'C&C-Torii'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C-HeartBeat-FileDownload'), 'label'] = 'C&C-HeartBeat-FileDownload'\ndf_merged.loc[(df_merged.label == '-   Malicious   FileDownload'), 'label'] = 'FileDownload'\ndf_merged.loc[(df_merged.label == '-   Malicious   C&C-Mirai'), 'label'] = 'C&C-Mirai'\ndf_merged.loc[(df_merged.label == '-   Malicious   Okiru-Attack'), 'label'] = 'Okiru-Attack'","6c49809c":"df_merged['label'].value_counts()","033eacfc":"# These are the columns which are to be removed as they do not provide any beneficial information:\n\n# 'ts' -> timestamp \n# 'uid' -> unique id of connection\n# 'id.orig_h' -> origin IP\n# 'id.orig_p' -> origin port ??\n# 'id.resp_h' -> destination IP\n# 'id.resp_p' -> destination port ??\n# 'local_orig' -> if connection originated locally for origin\n# 'local_resp' -> if connection originated locally for destination\n\n# non_useful_columns = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'local_orig', 'local_resp']\nnon_useful_columns = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'local_orig', 'local_resp', 'history']\n\ndf_merged.drop(non_useful_columns, axis=1, inplace=True)\ndf_merged.head()","c315dfe6":"def subsample(df):\n    df_benign = df[df['label'] == 'Benign']\n    df_malicious = df[df['label'] != 'Benign']\n    \n    print('Before subsampling: ', df_benign.shape, df_malicious.shape)\n    \n    df_malicious = df_malicious.sample(len(df_benign), random_state = 0)\n    df = pd.concat([df_malicious, df_benign])\n    df = df.sample(frac=1, random_state=0)\n    \n    print('After subsampling: ', df_benign.shape, df_malicious.shape)\n    \n    return df","0b647a95":"df_merged = subsample(df_merged)\nprint(df_merged.shape)","88708558":"print(df_merged['label'].value_counts())\ndf_merged['label'].value_counts().plot(kind='bar')\nplt.show()","c89f08db":"# There are some empty values denoted by '-' int the dataframe,\n# They need to be replace with NaN\n\nprint(df_merged.isna().sum())\nprint()\ndf_merged.replace('-', np.nan, inplace=True)\nprint(df_merged.isna().sum())","ebbd443f":"# Make 'proto', 'service', 'conn_state', 'history' as One-Hot Encoded\nprint(df_merged.columns)\n\ndef encode(df, column_name):\n    dummies = pd.get_dummies(df[[column_name]], dummy_na=True) # Also create a NaN column for NaN values\n    res = pd.concat([df, dummies], axis=1)\n    res = res.drop([column_name], axis=1)\n    return res\n\n# columns_to_encode = ['proto', 'service', 'conn_state', 'history']\ncolumns_to_encode = ['proto', 'service', 'conn_state']\n\nfor column in columns_to_encode:\n    df_merged = encode(df_merged, column)\n\nprint(df_merged.columns)","1eecff29":"print(df_merged.shape)\ndf_merged.tail()","d6a696b7":"# Get the list of those columns which contain NaN values and replace NaN values with median\nnan_columns = df_merged.columns[df_merged.isna().any()].tolist()\nprint('Before replacing, the columns containing NaN values:', nan_columns)\n\nfor column in nan_columns:\n    df_merged[column].fillna(df_merged[column].median(), inplace=True)\n\nnan_columns = df_merged.columns[df_merged.isna().any()].tolist()\nprint('After replacing, the columns containing NaN values:', nan_columns)\n\nprint(df_merged.shape)\ndf_merged.tail()","c1a28ed6":"# Remove columns which contain only 0 values\nempty_columns = df_merged.columns[(df_merged == 0).all()]\nprint(empty_columns)\ndf_merged.drop(empty_columns, axis=1, inplace=True)\nprint(df_merged.shape)\ndf_merged.head()","f1128218":"# Reset index\ndf_merged.reset_index(drop=True, inplace=True)","5a913f22":"# Extract X and Y (labels) of dataset\nY = df_merged[['label']]\nprint(Y.shape)\nY.tail()","cf615da5":"X = df_merged.drop(['label'], axis=1)\nprint(X.shape)\nX.tail()","86e2ea13":"x = X.values\nstandard_scaler = StandardScaler()\n\nx_scaled = standard_scaler.fit_transform(x)\n\nX = pd.DataFrame(x_scaled, columns=X.columns)\nX.tail()","4167c348":"Y.loc[(Y.label == 'Benign'), 'label'] = 0 \nY.loc[(Y.label == 'PartOfAHorizontalPortScan'), 'label'] = 1 \nY.loc[(Y.label == 'Attack'), 'label'] = 2\nY.loc[(Y.label == 'DDoS'), 'label'] = 3 \nY.loc[(Y.label == 'C&C'), 'label'] = 4 \nY.loc[(Y.label == 'C&C-Torii'), 'label'] = 5 \nY.loc[(Y.label == 'FileDownload'), 'label'] = 6\nY.loc[(Y.label == 'C&C-FileDownload'), 'label'] = 7","fec4a7ea":"X.to_csv('X.csv', index=False)\nY.to_csv('Y.csv', index=False)","67f53dd9":"# 6) Extract X and Y dataset","52e754ba":"# 7) Normalization [0, 1]\n### Using sklearn's StandardScaler class to normalize 'X' dataframe column-wise","4064c74a":"# 5) Handle empty values and do one-hot encoding","4c5c7435":"# 8) Assign numerical values to output","8f69728a":"# 9) Save the final dataset in a CSV file","115f7225":"# 2) Clean the Labels","08cb8ef0":"# 1) Load the data into a single dataframe","6c7bd2f3":"# 3) Delete unnecessarry columns\n","ae33c757":"# 4) Subsample the data\n## Class Balancing\n### Subsampling -> 50% Benign, 50% Malicious (All malicious combined)"}}