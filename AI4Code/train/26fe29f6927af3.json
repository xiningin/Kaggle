{"cell_type":{"a7a31eea":"code","ebb8c572":"code","d92c9423":"code","258cb569":"code","86c8d997":"code","ab3a32cd":"code","74ceff20":"code","66cc943c":"code","0d01191d":"code","5ba1aa9a":"code","dadf34e5":"code","2dff2d4f":"code","e551bf62":"code","db41fbc5":"code","c39a660b":"code","e4d59e0f":"code","a78f6c81":"code","6a68b638":"code","6a112f48":"code","820f061d":"code","c74d44e7":"code","5defa031":"code","9e1f90dd":"code","a2df88aa":"code","f57df0e7":"code","65e24c6f":"code","1b1dcb80":"code","146ba295":"code","57c1f321":"code","cc84ffff":"code","640583e2":"code","0fb7e127":"markdown","f8938f68":"markdown","a44547ca":"markdown","c3a55419":"markdown","09db79a1":"markdown","e0523b97":"markdown","4c152910":"markdown","777a8e0a":"markdown","f28d2831":"markdown","5f7329d1":"markdown","d2611bab":"markdown","56d8a2ab":"markdown","5e4b097f":"markdown","30b20dec":"markdown","af91f816":"markdown","1966125d":"markdown","132ff1c6":"markdown","93199b98":"markdown","55fb1144":"markdown"},"source":{"a7a31eea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebb8c572":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d92c9423":"loans = pd.read_csv('\/kaggle\/input\/loan-data\/loan_data.csv')","258cb569":"loans.info()","86c8d997":"loans.describe()","ab3a32cd":"loans.head()","74ceff20":"plt.figure(figsize=(10,6))\nloans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Credit.Policy=1')\nloans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='Credit.Policy=0')\nplt.legend()\nplt.xlabel('FICO')","66cc943c":"plt.figure(figsize=(10,6))\nloans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='not.fully.paid=1')\nloans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='not.fully.paid=0')\nplt.legend()\nplt.xlabel('FICO')","0d01191d":"plt.figure(figsize=(11,7))\nsns.countplot(x='purpose',hue='not.fully.paid',data=loans,palette='Set1')","5ba1aa9a":"sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')","dadf34e5":"plt.figure(figsize=(11,7))\nsns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',\n           col='not.fully.paid',palette='Set1')","2dff2d4f":"loans.info()","e551bf62":"cat_feats = ['purpose']","db41fbc5":"final_data = pd.get_dummies(loans,columns=cat_feats,drop_first=True)","c39a660b":"final_data.info()","e4d59e0f":"from sklearn.model_selection import train_test_split","a78f6c81":"X = final_data.drop('not.fully.paid',axis=1)\ny = final_data['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","6a68b638":"from sklearn.tree import DecisionTreeClassifier","6a112f48":"dtree = DecisionTreeClassifier()","820f061d":"dtree.fit(X_train,y_train)","c74d44e7":"predictions = dtree.predict(X_test)","5defa031":"from sklearn.metrics import classification_report,confusion_matrix","9e1f90dd":"print(classification_report(y_test,predictions))","a2df88aa":"print(confusion_matrix(y_test,predictions))","f57df0e7":"from sklearn.ensemble import RandomForestClassifier","65e24c6f":"rfc = RandomForestClassifier(n_estimators=600)","1b1dcb80":"rfc.fit(X_train,y_train)","146ba295":"predictions = rfc.predict(X_test)","57c1f321":"from sklearn.metrics import classification_report,confusion_matrix","cc84ffff":"print(classification_report(y_test,predictions))","640583e2":"print(confusion_matrix(y_test,predictions))","0fb7e127":"** Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid. **","f8938f68":"## Get the Data\n\n** Use pandas to read loan_data.csv as a dataframe called loans.**","a44547ca":"# Import Libraries\n\n**Import the usual libraries for pandas and plotting. You can import sklearn later on.**","c3a55419":"# Setting up the Data\n\nLet's get ready to set up our data for our Random Forest Classification Model!\n\n**Check loans.info() again.**","09db79a1":"** Let's see the trend between FICO score and interest rate. Recreate the following jointplot.**","e0523b97":"** Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy. Check the documentation for lmplot() if you can't figure out how to separate it into columns.**","4c152910":"## Predictions and Evaluation\n\nLet's predict off the y_test values and evaluate our model.\n\n** Predict the class of not.fully.paid for the X_test data.**","777a8e0a":"## Train Test Split\n\nNow its time to split our data into a training set and a testing set!\n\n** Use sklearn to split your data into a training set and a testing set as we've done in the past.**","f28d2831":"**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**","5f7329d1":"## Training a Decision Tree Model\n\nLet's start by training a single decision tree first!\n\n** Import DecisionTreeClassifier**","d2611bab":"# Exploratory Data Analysis\n\nLet's do some data visualization! We'll use seaborn and pandas built-in plotting capabilities, but feel free to use whatever library you want. Don't worry about the colors matching, just worry about getting the main idea of the plot.\n\n** Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**\n\n*Note: This is pretty tricky, feel free to reference the solutions. You'll probably need one line of code for each histogram, I also recommend just using pandas built in .hist()*","56d8a2ab":"** Check out the info(), head(), and describe() methods on loans.**","5e4b097f":"**Now use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a fixed larger dataframe that has new feature columns with dummy variables. Set this dataframe as final_data.**","30b20dec":"## Predictions and Evaluation of Decision Tree\n**Create predictions from the test set and create a classification report and a confusion matrix.**","af91f816":"**Show the Confusion Matrix for the predictions.**","1966125d":"** Create a similar figure, except this time select by the not.fully.paid column.**","132ff1c6":"**Now create a classification report from the results. Do you get anything strange or some sort of warning?**","93199b98":"## Categorical Features\n\nNotice that the **purpose** column as categorical\n\nThat means we need to transform them using dummy variables so sklearn will be able to understand them. Let's do this in one clean step using pd.get_dummies.\n\nLet's show you a way of dealing with these columns that can be expanded to multiple categorical features if necessary.\n\n**Create a list of 1 element containing the string 'purpose'. Call this list cat_feats.**","55fb1144":"## Training the Random Forest model\n\nNow its time to train our model!\n\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**"}}