{"cell_type":{"25621155":"code","946b5109":"code","8bfb91ac":"code","1f366ffe":"code","1101552c":"code","97ddffa6":"markdown"},"source":{"25621155":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport lightgbm as lgb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.tabular.transform import add_cyclic_datepart\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nPATH = '\/kaggle\/input\/recsys-challenge-2015' ","946b5109":"def read_buys(limit=None):\n    buys = pd.read_csv(f\"{PATH}\/yoochoose-buys.dat\",\n                    names=[\"session\", \"timestamp\", \"item\", \"price\", \"qty\"],\n                   parse_dates=[\"timestamp\"])\n    buys = buys.sort_values(by=[\"timestamp\", \"session\"])\n    if limit:\n        buys = buys.iloc[:limit]\n    return buys\n\n\ndef read_clicks(limit=None):\n    print(\"Loading clicks\")\n    filename = f\"{PATH}\/yoochoose-clicks.dat\"\n    df = pd.read_csv(filename,\n                     names=[\"session\", \"timestamp\", \"item\", \"category\"],\n                     parse_dates=[\"timestamp\"],\n                     converters={\"category\": lambda c: -1 if c == \"S\" else c})\n    df = df.sort_values(by=[\"timestamp\", \"session\"])\n    if limit:\n        df = df.iloc[:limit]\n    print(\"Clicks shape %s %s\" % df.shape)\n    return df\n\n\ndef process_clicks(clicks, rolling_days=15):\n    # Compute dwell time for each click\n    print(\"Processing clicks\")\n    clicks['prev_ts'] = clicks.groupby('session')['timestamp'].transform(lambda x: x.shift())\n    clicks['diff_prev'] = clicks[\"timestamp\"] - clicks[\"prev_ts\"] # in minutes\n    clicks[\"dwell\"] = clicks.groupby('session')['diff_prev'].transform(lambda x: x.shift(-1)).dt.seconds\/60\n    clicks = clicks.sort_values(by=[\"session\", \"timestamp\"])\n    print(\"Processed clicks shape %s %s\" % clicks.shape)\n    return clicks\n\n\ndef process_buys(limit=None):\n    # Group into sessions, compute nr of items bought and set label column\n    buys = read_buys(limit=limit)\n    print(\"Processing buys\")\n    print(\"Buys from %s to %s\" % (buys.timestamp.min(), buys.timestamp.max()))\n    grouped = buys.groupby(\"session\")\n    buys_g = pd.DataFrame(index=grouped.groups.keys())\n    buys_g[\"items_bought\"] = grouped.item.count() # quantity may be zero which is weird so dont use it\n    buys_g[\"is_buy\"] = 1 # for easier merge later on\n    buys_g.index.name = \"session\"\n    print(\"Buys grouped by session %s %s\" % buys_g.shape)\n    return buys_g\n\n\ndef get_items_cats_percent(clicks, limit=None):\n    buys = read_buys(limit=limit)\n    # percent bought\n    item_id_bought_pct = buys.item.value_counts(normalize=True)\n    cat_id_viewed_pct = clicks.category.value_counts(normalize=True)\n    item_id_viewed_pct = clicks.item.value_counts(normalize=True)\n\n    return dict(views=dict(item=item_id_viewed_pct, cat=cat_id_viewed_pct), buys=item_id_bought_pct)\n\n\ndef process_sessions(processed_clicks, limit=None):\n    print(\"Preprocessing - Grouping clicks into sessions\")\n    clicks = processed_clicks\n    \n    # Group clicks by session\n    grouped = clicks.groupby(\"session\")\n    sessions = pd.DataFrame(index=grouped.groups.keys())\n    \n    # Session counters\n    sessions[\"total_clicks\"] = grouped.item.count()\n    sessions[\"total_items\"] = grouped.item.unique().apply(lambda x: len(x))\n    sessions[\"total_cats\"] = grouped.category.unique().apply(lambda x: len(x))\n    print(\"Computed counters\")\n    \n    # Session duration\n    sessions[\"max_dwell\"] = grouped.dwell.max()\n    sessions[\"mean_dwell\"] = grouped.dwell.mean()\n    sessions[\"start_ts\"] = grouped.timestamp.min()\n    sessions[\"end_ts\"] = grouped.timestamp.max()\n    sessions[\"total_duration\"] = (sessions[\"end_ts\"] - sessions[\"start_ts\"]).dt.seconds \/ 60\n    print(\"Computed dwell and duration\")\n    \n    # Click rate\n    sessions[\"total_duration_secs\"] = (sessions[\"end_ts\"] - sessions[\"start_ts\"]).dt.seconds\n    sessions[\"click_rate\"] = sessions[\"total_clicks\"] \/ sessions[\"total_duration_secs\"]\n    sessions.click_rate = sessions.click_rate.replace(np.inf, np.nan)\n    sessions.click_rate = sessions.click_rate.fillna(0)\n    del sessions[\"total_duration_secs\"]\n    print(\"Computed click rate\")\n    \n    # Dates\n    #sessions = add_datepart(sessions, \"start_ts\", drop=False)\n    #sessions = add_datepart(sessions, \"end_ts\", drop=False)\n    sessions = add_cyclic_datepart(sessions, \"start_ts\", drop=False)\n    sessions = add_cyclic_datepart(sessions, \"end_ts\", drop=False)\n    print(\"Computed cyclic date parts\")\n    \n    # What is the item and cat most viewed in this session?\n    # How many times were they viewed?\n    sessions[\"cat_most_viewed_n_times\"] = grouped.category.value_counts().unstack().max(axis=1)\n    sessions[\"cat_most_viewed\"] = grouped.category.value_counts().unstack().idxmax(axis=1)\n    sessions[\"item_most_viewed_n_times\"] = grouped.item.value_counts().unstack().max(axis=1)\n    sessions[\"item_most_viewed\"] = grouped.item.value_counts().unstack().idxmax(axis=1)\n    print(\"Computed most viewed item\/cat per session\")\n\n    # For the item most viewed in each session, what is its global buy\/view frequency?\n    freqs = get_items_cats_percent(clicks, limit=limit)\n    cat_views = pd.DataFrame(freqs[\"views\"][\"cat\"])\n    cat_views.columns = [\"cat_views_freqs\"]\n    sessions = sessions.merge(cat_views, how=\"left\", left_on=\"cat_most_viewed\", right_index=True)\n    sessions.cat_views_freqs = sessions.cat_views_freqs.fillna(0)\n    item_views = pd.DataFrame(freqs[\"views\"][\"item\"])\n    item_views.columns = [\"item_views_freqs\"]\n    sessions = sessions.merge(item_views, how=\"left\", left_on=\"item_most_viewed\", right_index=True)\n    sessions.item_views_freqs = sessions.item_views_freqs.fillna(0)\n    item_buys = pd.DataFrame(freqs[\"buys\"])\n    item_buys.columns = [\"item_buys_freqs\"]\n    sessions = sessions.merge(item_buys, how=\"left\", left_on=\"item_most_viewed\", right_index=True)\n    sessions.item_buys_freqs = sessions.item_buys_freqs.fillna(0)\n    print(\"Computed most viewed\/bought freqs\")\n    \n    # Sorting sessions\n    sessions = sessions.sort_values(by=[\"start_ts\"])\n    sessions.index.name = \"session\"\n    \n    print(\"Sessions shape %s %s\" % sessions.shape)\n    print(\"Sessions columns %s \" % sessions.columns)\n    print(\"Sessions from %s to %s\" % (sessions.start_ts.min(), sessions.start_ts.max()))\n    return sessions\n\n\ndef prep(limit=None):\n    print(\"Prepping data for classification\")\n    buys = process_buys(limit=limit)\n    clicks = read_clicks(limit=limit)\n    processed_clicks = process_clicks(clicks)\n    sessions = process_sessions(clicks, limit=limit)\n    \n    print(\"Merging clicks and buys\")\n    X = pd.merge(sessions, buys, how=\"left\", left_index=True, right_index=True)\n    \n    X = X.sort_values(by=[\"start_ts\"])\n    y = X[\"is_buy\"]\n    y = y.fillna(0)\n    \n    X[\"cat_most_viewed\"] = X[\"cat_most_viewed\"].astype(\"float64\")\n    \n    # Delete label\n    del X[\"is_buy\"]\n    \n    # Delete time columns (cant be used as is and we already have the cyclic date parts)\n    del X[\"start_ts\"]\n    del X[\"end_ts\"]\n    \n    return X, y\n\ndef classify(X, y):\n    \n    print(\"Splitting into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25, shuffle=False)\n\n    params = {\n        'boosting_type': 'gbdt', \n        'objective': 'binary', 'metric': 'auc',\n        'learning_rate': 0.03, 'num_leaves': 7, 'max_depth': 3,\n        'min_child_samples':100, # min_data_in_leaf\n        'max_bin': 100, #number of bucked bin for feature values\n        'subsample': 0.9, # subsample ratio of the training instance\n        'subsample_freq':1, # frequence of subsample\n        'colsample_bytree': 0.7, # subsample ratio of columns when constructing each tree.\n        'min_child_weight':0,\n        'min_split_gain':0, # lambda_l1, lambda_l2 and min_gain_to_split to regularization.\n        'nthread':8, 'verbose': 0, \n        'scale_pos_weight': 150 # because training data is extremely unbalanced\n    }\n    \n    print(\"Building datasets for lightgbm\")\n    # prepare model\n    dtrain = lgb.Dataset(X_train,label=y_train,feature_name=X.columns.tolist())\n    dvalid = lgb.Dataset(X_test,label=y_test,feature_name=X.columns.tolist())\n    \n    cats = [\"cat_most_viewed\", \"item_most_viewed\"]\n    evals_results = {}\n    \n    print(\"Starting classification\")\n    model = lgb.train(params, dtrain, valid_sets=[dtrain, dvalid],\n                      #categorical_feature=cats,\n                      valid_names=['train', 'valid'],\n                      evals_result = evals_results, num_boost_round=1000,\n                      #early_stopping_rounds= 100, \n                      verbose_eval=50, feval = None)\n    \n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    y_pred = np.round_(y_pred, 0)\n    print('The accuracy of prediction is:', metrics.accuracy_score(y_test, y_pred))\n    print('The roc_auc_score of prediction is:', metrics.roc_auc_score(y_test, y_pred))\n    print('The null acccuracy is:', max(y_test.mean(), 1 - y_test.mean()))\n    \n    return model","8bfb91ac":"X, y = prep(limit=100000)","1f366ffe":"model = classify(X, y)\n","1101552c":"vs = model.feature_importance()\nks = X.columns\nd = dict(zip(ks, vs))\nsorted(d.items(), key=lambda x: x[1], reverse=True)[:15]","97ddffa6":"> ## Code"}}