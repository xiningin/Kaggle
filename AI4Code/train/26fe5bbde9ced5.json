{"cell_type":{"59a43b13":"code","b0f06ca3":"code","084339d6":"code","0fa37357":"code","7269e33c":"code","0c39c6ec":"code","bfb37f48":"code","60cd832f":"code","38d18ee2":"code","e9df22ad":"code","aea02cc3":"code","a6f9707e":"code","d2b02949":"code","e32cf125":"code","dbc99416":"code","11c7e0f7":"code","64a8ab07":"code","12f67873":"code","88282aca":"code","f9b9957a":"code","6f1673e2":"code","ba037993":"code","468b2572":"code","25452112":"code","ca44cfba":"code","3b04e0ea":"code","514c541c":"code","eabc57c2":"code","c3f9461c":"code","3c1f894a":"code","59a746a2":"code","860b74be":"markdown"},"source":{"59a43b13":"import tensorflow","b0f06ca3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","084339d6":"# Initialize the random number generator\nimport random\nrandom.seed(0)\n\n# Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0fa37357":"# IMPORT DATA\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('\/kaggle\/input\/ner_dataset-1.csv', encoding='latin1')\ndata = data.fillna(method=\"ffill\") # Deal with N\/A","7269e33c":"data.head()","0c39c6ec":"data.shape","bfb37f48":"data.info()","60cd832f":"tags = list(set(data[\"POS\"].values)) # Read POS values","38d18ee2":"tags # List of possible POS values","e9df22ad":"words = list(set(data[\"Word\"].values))\nwords.append(\"DUMMY\") # Add a dummy word to pad sentences.","aea02cc3":"words","a6f9707e":"# Code to read sentences\n\nclass ReadSentences(object): \n    \n    def __init__(self, data):\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","d2b02949":"sentences = ReadSentences(data).sentences # Read all sentences","e32cf125":"sentences[1]","dbc99416":"# Convert words and tags into numbers\nword2id = {w: i for i, w in enumerate(words)}\ntag2id = {t: i for i, t in enumerate(tags)}","11c7e0f7":"word2id","64a8ab07":"tag2id","12f67873":"# Prepare input and output data\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nmax_len = 50\nX = [[word2id[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=len(words)-1)\ny = [[tag2id[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2id[\".\"])","88282aca":"# Convert output to one-hot bit\n\nfrom tensorflow.keras.utils import to_categorical\ny = [to_categorical(i, num_classes=len(tags)) for i in y]","f9b9957a":"y[0]","6f1673e2":"X[0]","ba037993":"# Training and test split by sentences\n\nfrom sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20)","468b2572":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input","25452112":"input = Input(shape=(max_len,)) # Input layer\nmodel = Embedding(input_dim=len(words), output_dim=50, input_length=max_len)(input) # Word embedding layer\nmodel = Dropout(0.1)(model) # Dropout\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model) # Bi-directional LSTM layer\nout = TimeDistributed(Dense(len(tags), activation=\"softmax\"))(model)  # softmax output layer","ca44cfba":"model = Model(input, out) # Complete model","3b04e0ea":"model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # Compile with an optimizer","514c541c":"history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=3, validation_split=0.1, verbose=1) # Train","eabc57c2":"# Demo test on one sample. See how it is mostly correct, but not 100%\n\ni = 1213 # Some test sentence sample\np = model.predict(np.array([X_te[i]])) # Predict on it\np = np.argmax(p, axis=-1) # Map softmax back to a POS index\nfor w, pred in zip(X_te[i], p[0]): # for every word in the sentence\n    print(\"{:20} -- {}\".format(words[w], tags[pred])) # Print word and tag","c3f9461c":"import nltk\nnltk.download('punkt')","3c1f894a":"from nltk import word_tokenize\n\nsentence = nltk.word_tokenize('That was a nice jump')\nX_Samp = pad_sequences(maxlen=max_len, sequences=[[word2id[word] for word in sentence]], padding=\"post\", value=len(words)-1)","59a746a2":"p = model.predict(np.array([X_Samp[0]])) # Predict on it\np = np.argmax(p, axis=-1) # Map softmax back to a POS index\nfor w, pred in zip(X_Samp[0], p[0]): # for every word in the sentence\n    print(\"{:20} -- {}\".format(words[w], tags[pred])) # Print word and tag","860b74be":"### Load the dataset"}}