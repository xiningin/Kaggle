{"cell_type":{"c0b21156":"code","7a4598a9":"code","c8c5cd45":"code","7ae0b7d2":"code","5debd2f3":"code","9a26beda":"code","3baac474":"code","92725b4b":"code","f82ebc46":"code","f457415c":"code","c3ee7b34":"code","5d2c55c5":"code","9324791e":"code","5847fb4b":"code","62d83d33":"code","34806569":"code","ced1661f":"code","6d3f188f":"code","e213cc2b":"code","9b8266d6":"code","3ea78962":"code","b3ae9936":"code","32bc74f7":"code","46433ee2":"code","27fac298":"code","1b39d536":"code","e7572d51":"code","36152a1e":"code","ad7455c4":"code","91ae0c02":"code","f89ed70e":"code","53d6ae1a":"code","41183804":"code","0fffa61b":"code","a046b5c0":"code","d202e758":"code","e9aeccba":"code","09b2c189":"code","1ab56c49":"code","d77777a4":"code","00c7a9e9":"code","3acee90a":"code","855a9918":"code","60e1770e":"markdown","61db972c":"markdown","47975184":"markdown","9c091a35":"markdown","6feedc60":"markdown","791f75a9":"markdown","75913a7a":"markdown","810d499d":"markdown","60923009":"markdown","fa80f09d":"markdown","a9c6ef6b":"markdown","eb291230":"markdown","54e4ac69":"markdown","935fb190":"markdown"},"source":{"c0b21156":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth', 5000)\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\nDIR_NAME = \"..\/input\"\n# Any results you write to the current directory are saved as output.","7a4598a9":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, RepeatedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n\nimport xgboost as xgb","c8c5cd45":"# load csv\ndef load_dir_csv(directory, csv_files=None):\n    if csv_files is None:\n        csv_files = sorted( [ f for f in os.listdir(directory) if f.endswith(\".csv\") ])    \n    csv_vars  = [ filename[:-4] for filename in csv_files ]\n    gdict = globals()\n    for filename, var in zip( csv_files, csv_vars ):\n        print(f\"{var:32s} = pd.read_csv({directory}\/{filename})\")\n        gdict[var] = pd.read_csv( f\"{directory}\/{filename}\" )\n        print(f\"{'shape ':32s} = \" + str(gdict[var].shape))\n        display(gdict[var].head())","7ae0b7d2":"%%time\nload_dir_csv(DIR_NAME, [\"train_transaction.csv\", \"test_transaction.csv\", \"train_identity.csv\", \"test_identity.csv\"])","5debd2f3":"%%time\n# merge to data\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","9a26beda":"print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\nprint('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))","3baac474":"del train_transaction, train_identity, test_transaction, test_identity","92725b4b":"gc.collect()","f82ebc46":"train.head()","f457415c":"test.head()","c3ee7b34":"train.describe()","5d2c55c5":"test.describe()","9324791e":"def is_integer_num(n):\n    if isinstance(n, int):\n        return True\n    if isinstance(n, float):\n        return n.is_integer()\n    return False\n\ndef missing_values_table_specified_value(df, value=0.5): \n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum()\/len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    \n    if is_integer_num(value):\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns['Missing Values'] >= value]\n        print('The number of columns with {} counts missing values is {}.'.format(value, len(mis_val_table_ren_columns)))\n    else:\n        value = value * 100\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns['% of Total Values'] >= value]\n        print('The number of columns with {}% missing values is {}.'.format(value, len(mis_val_table_ren_columns)))\n    return mis_val_table_ren_columns \n\ndef missing_values_table(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","5847fb4b":"missing_values_table_specified_value(train, 0.5).head()","62d83d33":"missing_values_table_specified_value(test, 0.5).head()","34806569":"display(missing_values_table(train), missing_values_table(test))","ced1661f":"train['isFraud'].value_counts()","6d3f188f":"sns.countplot(train['isFraud'])","e213cc2b":"print('{:.4f}% of data that are fraud in train.'.format(train['isFraud'].mean() * 100))","9b8266d6":"quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nprint(quantitative)\nprint('Counts: {}'.format(len(quantitative)))","3ea78962":"qualitative = [f for f in train.columns if train.dtypes[f] == 'object']\nprint(qualitative)\nprint('Counts: {}'.format(len(qualitative)))","b3ae9936":"# Get columns with less than n unique values in the type specified in column_type\ndef unique_data_under_n_columns_list(df, column_type='object', n=0):\n    # If n = 0, return all columns\n    if n == 0:\n        n = df.shape[0]\n    \n    columns_list = df.select_dtypes(include=column_type).columns.tolist()\n    unique_n_list = []\n    for colomn in columns_list:\n        unique_count = len(df[colomn].unique())\n        if unique_count < n:\n            unique_n_list.append(colomn)\n        else:\n            print('{} Is excluded because it has a unique value {} greater than {}.'.format(colomn, n, unique_count))\n    return unique_n_list","32bc74f7":"# Graph data of specified type\ndef visualization_object_data(df, fig_x=10, fig_y=5):\n    # Get columns type of object\n    object_list = unique_data_under_n_columns_list(df, 'object', 65)\n    for colomn in object_list:\n        print('column name:{} \/ unique value(axis x):{} \/ max value:{} \/ min value:{} \/ missing value:{} '.format(colomn, len(df[colomn].unique()), max(df[colomn].value_counts()), min(df[colomn].value_counts()), df[colomn].isnull().sum()))\n        # Calculate the Optimal Horizontal Size of Shapes\n        fig_x = len(df[colomn].unique())\n        order = df[colomn].value_counts(ascending=False).index\n        if fig_x <= 8:\n            fig, ax = plt.subplots(1, 3, figsize=(fig_x*6,fig_y))\n            sns.countplot(x=colomn, ax=ax[0], data=df, order=order)\n            ax[0].set_title('All', fontsize=14)\n            sns.countplot(x=colomn, ax=ax[1], data=df.loc[df['isFraud'] == 1], order=order)\n            ax[1].set_title('isFraud = 1', fontsize=14)\n            sns.countplot(x=colomn, ax=ax[2], data=df.loc[df['isFraud'] == 0], order=order)\n            ax[2].set_title('isFraud = 0', fontsize=14)\n        else:\n            fig, ax = plt.subplots(1, 3, figsize=(32,10))\n            sns.countplot(y=colomn, ax=ax[0], data=df, order=order)\n            ax[0].set_title('All', fontsize=14)\n            sns.countplot(y=colomn, ax=ax[1], data=df.loc[df['isFraud'] == 1], order=order)\n            ax[1].set_title('isFraud = 1', fontsize=14)\n            sns.countplot(y=colomn, ax=ax[2], data=df.loc[df['isFraud'] == 0], order=order)\n            ax[2].set_title('isFraud = 0', fontsize=14)\n\n        plt.show()\n        plt.pause(0.05)","46433ee2":"visualization_object_data(train)","27fac298":"%%time\nX_train = train.drop('isFraud', axis=1)\ny_train = train['isFraud'].copy()\nX_test = test.copy()","1b39d536":"X_train.shape, y_train.shape","e7572d51":"del train, test\ngc.collect()","36152a1e":"%%time\n# Label Encoding\nfor f in qualitative:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n    X_train[f] = lbl.transform(list(X_train[f].values))\n    X_test[f] = lbl.transform(list(X_test[f].values))  ","ad7455c4":"# Check if it is encoded\nprint(len(X_train.select_dtypes(include='object').columns))\nprint(len(X_test.select_dtypes(include='object').columns))","91ae0c02":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","f89ed70e":"%%time\nX_train = reduce_mem_usage(X_train)","53d6ae1a":"%%time\nX_test = reduce_mem_usage(X_test)","41183804":"gc.collect()","0fffa61b":"X_train.head()","a046b5c0":"X_test.head()","d202e758":"X_train.describe()","e9aeccba":"X_test.describe()","09b2c189":"sub = pd.read_csv(DIR_NAME + '\/sample_submission.csv')","1ab56c49":"sub.head()","d77777a4":"NFOLDS = 5\nkf = KFold(n_splits = NFOLDS, shuffle = True)\ny_preds = np.zeros(sub.shape[0])\ny_oof = np.zeros(X_train.shape[0])","00c7a9e9":"%%time\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n\n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=9,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        tree_method='gpu_hist'\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    \n    print('ROC AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    \n    y_preds += clf.predict_proba(X_test)[:,1] \/ NFOLDS","3acee90a":"sub['isFraud'] = y_preds\nsub.to_csv('submission.csv', index=False)","855a9918":"sub.head()","60e1770e":"# Overview\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files identity and transaction, which are joined by TransactionID.\n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\n","61db972c":"## Basic statistics","47975184":"# import data","9c091a35":"# model xgboost","6feedc60":"It is imbalanced data, which has an overwhelming number of 0","791f75a9":"## Looking Fraud","75913a7a":"Test data contains fewer columns with 50% or more missing data than train data.","810d499d":"## Looking qualitative columns","60923009":"# EDA","fa80f09d":"There is data of column name that I don't understand well.  \nFor example, CXX, DXX, MXX, VXX of `transaction.csv`.","a9c6ef6b":"# Boosting Model + FE Importance","eb291230":"## Split into Quantitative and Qualitative data","54e4ac69":"The VXX data seems to have almost zero data.","935fb190":"## Missing value"}}