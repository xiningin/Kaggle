{"cell_type":{"eaa2842b":"code","404944f9":"code","22dce969":"code","daaaa70c":"code","7a0265f4":"code","2355a773":"code","1d12d1b9":"code","8f7c0195":"code","f8ea9b8a":"code","e3cbe8d1":"code","53c17e1f":"code","ce92ec3f":"code","49071bbd":"code","a3e60ad8":"code","6dc28a50":"code","a143e53f":"code","653951b9":"code","305390a6":"code","5555a960":"code","a1d9c673":"code","576f9e2d":"code","59cf0e28":"code","a31a0899":"code","8b0f2f66":"code","e7c9ef33":"code","c5e208ff":"code","e2ebf0c7":"code","f5b389a3":"code","529ff47f":"code","0aa8ff3e":"code","02471cbd":"code","ec3a8387":"code","1cfe53a4":"code","4e19f1bd":"code","3721a524":"code","0486ea77":"code","b79a9227":"code","1d368b7c":"code","365f28c2":"code","642460c6":"code","baf7bd45":"code","60864c22":"code","ea98dd91":"code","aa86312d":"code","ada0281d":"code","88a0bc14":"code","4dd299e1":"code","a98c62fb":"code","327edcb6":"code","3ff3a7f2":"code","d7c11a0b":"code","797e5c7d":"code","aec0c5cd":"code","30442de8":"code","eb452923":"code","c02e684e":"code","d2663e6f":"code","3d2988fa":"code","46687e31":"code","b8b5568f":"code","2903077f":"code","2c13243c":"code","9794d5e0":"code","cb8509cf":"code","3bfb6f5a":"code","2681c376":"code","6e1f4743":"code","02c6d4f4":"code","b40cef7d":"code","03cbf43d":"code","5a394845":"code","4d169a7c":"code","57055bfd":"code","354f5c40":"code","a11c6a5e":"code","46065f5b":"code","e26cf4df":"code","29b98c2f":"code","1eec1267":"code","2bbd00e1":"code","2ce51a7b":"code","209a2bae":"code","728e36ad":"code","b47cecc0":"code","ab1c2dc3":"code","b73498d3":"code","a2c74d05":"code","155a0581":"code","6c979160":"code","4f6a0125":"code","cb52904d":"code","9bc90bfd":"code","ec6b05e0":"code","ac812f9b":"code","555ec751":"code","0330190e":"code","c9d455da":"code","40a8598f":"code","2cfb4bbd":"code","cd9db165":"code","ec835bbd":"code","66006e70":"code","449d2158":"code","e5626450":"code","dc3cdc19":"code","64c16f62":"code","3d8677e7":"code","7af70e48":"code","efe10745":"code","622fe532":"code","bb6ae84e":"code","01715bf0":"code","f6feec6b":"code","e7f293c6":"code","4946c637":"code","92a647ad":"markdown","8a7b7b07":"markdown","2ef61897":"markdown","1b555636":"markdown","1fc7a282":"markdown","1b8e169b":"markdown","84c865d1":"markdown","21356050":"markdown","6d51b867":"markdown","d5366c25":"markdown","5e7ad8d9":"markdown","ce64799f":"markdown","9af20ff7":"markdown","8c281671":"markdown","9933ed0d":"markdown","30ae90b2":"markdown","1378e7d2":"markdown","ce235b50":"markdown","efabefe5":"markdown","e5498240":"markdown","44291421":"markdown","743fe55a":"markdown","66b56490":"markdown","cdb343e3":"markdown","0f540583":"markdown","455d6650":"markdown","fec9c1f7":"markdown","90bed6f7":"markdown","a548a06f":"markdown","cd651a3f":"markdown","6cccd1ab":"markdown","816094d4":"markdown","93ed0083":"markdown","d77a871a":"markdown","201bdbcb":"markdown","ae1691d3":"markdown","a3e3aefb":"markdown","078ed789":"markdown","ba68c4a8":"markdown","0c55a40e":"markdown","baa10980":"markdown","f3084dff":"markdown"},"source":{"eaa2842b":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https:\/\/fortunedotcom.files.wordpress.com\/2014\/07\/new-logos-airbnb.jpg\")","404944f9":"import pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels as statm\nfrom statsmodels.formula.api import ols\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom plotnine import *\n","22dce969":"filedata= pd.read_csv('..\/input\/train.csv')","daaaa70c":"filedata.head(5)","7a0265f4":"len(filedata.columns)","2355a773":"filedata.shape","1d12d1b9":"filedata.info()","8f7c0195":"#check for missing data, and output columns that have missing data\nfor col in filedata:\n    if (filedata[col].isnull().any()):\n        print(col)","f8ea9b8a":"#fills missing data with 0s\n#GO BACK TO THIS, 0 may not be best fill for all missing data\nfiledata=filedata.fillna(0)","e3cbe8d1":"#summary stats on each of the numeric columns\nfiledata.describe()","53c17e1f":"#check all the statistics\nfiledata.describe(include='all')","ce92ec3f":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','uint8']\nnumericdataX = filedata.select_dtypes(include=numerics)\nx= numericdataX['accommodates']\nsns.distplot(x);","49071bbd":"x= numericdataX.iloc[:,1]\nsns.distplot(x);","a3e60ad8":"ggplot(filedata, aes(x='room_type')) + geom_bar(fill = \"red\")","6dc28a50":"ggplot(filedata, aes(x='city')) + geom_bar(fill = \"green\")","a143e53f":"#check categorical data\nfiledata.describe(include=['O'])","653951b9":"#check numeric data\nfiledata.describe()","305390a6":"filedata.columns","5555a960":"regressor = linear_model.LinearRegression()\nfor i in range(1,10): \n    x= np.array(numericdataX.iloc[:,i]).reshape(-1,1)\n    y= np.array(filedata['log_price']).reshape(-1,1)\n    regressor.fit(x,y)\n    plt.figure(figsize=(8,5))\n    plt.subplot(10,1,i)\n    plt.scatter(x,y,color='blue', alpha=0.1)\n    plt.plot(x,regressor.predict(x),color=\"red\")\n    plt.legend()","a1d9c673":"statm.graphics.gofplots.qqplot(numericdataX.iloc[:,6], line='r')","576f9e2d":"statm.graphics.gofplots.qqplot(numericdataX.iloc[:,1], line='r')","59cf0e28":"statm.graphics.gofplots.qqplot(numericdataX.iloc[:,9], line='r')","a31a0899":"def checkCorrelation(data):\n    \"\"\"\n    Plot correlation Matrix for given data\n   :param data: dataset having features\n   :return: return plot representing pearson correlation\n   \"\"\"\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(data.corr(),linewidths=0.25,vmax=1.0,square=True,cmap=\"BuGn_r\", \n    linecolor='w',annot=True)","8b0f2f66":"#return Model \ndef data_model(xdata):\n    \"\"\"\n     fits linear regression model on given data\n    :param xdata: independent variable dataset\n    :return: linear regression model with fit of xdata \n   \"\"\"\n    #add constant to data\n    X = sm.add_constant(xdata)\n    targetY=filedata[['log_price']]\n    y = targetY\n\n    # Fit the linear model\n    model = linear_model.LinearRegression()\n    results = model.fit(X, y)\n    model = sm.OLS(y, X)\n    results = model.fit()\n    return results","e7c9ef33":"def data_summary(xdata):\n    \"\"\"\n    Returns chart having summary of data\n   :param xdata: independent variable dataset\n   :return: summary of data \n   \"\"\"\n    results = data_model(xdata)\n    return results.summary()","c5e208ff":"def crossValidationError(data):\n    \"\"\"\n   Finds cross validation error of model\n   :param X: independent variable dataset\n   :return: float value returns mean squared error\n   \"\"\"\n    numericdataX=data\n    X = np.array(numericdataX.drop(['log_price'],axis=1), dtype=pd.Series)\n    Y = np.array(numericdataX['log_price'], dtype=pd.Series)\n    regr1 = linear_model.LinearRegression()\n    ms_errors= cross_val_score(regr1, X, Y, cv=5, scoring = make_scorer(mean_squared_error))\n    rms_errors = np.sqrt(ms_errors)\n    mean_rms_error = rms_errors.mean()\n    return mean_rms_error","e2ebf0c7":"#Checking correlation in data\ncheckCorrelation(numericdataX)","f5b389a3":"#So as per correlation matrix colums such as latitude, longitude, number_of_reviews and review_scores_rating are not making much impact on log_price\n#as valueof cirrelation is poor\n#lets drop them from our dataset\nnumericdataX=numericdataX.drop(['id','number_of_reviews',\n       'review_scores_rating','latitude',\n       'longitude' ], axis=1)","529ff47f":"# buid model and check summary\ndata_summary(numericdataX)","0aa8ff3e":"# there is also correlation between bathroom and accomodates and bedroom lets only keep acomodates\nnumericdataX = numericdataX.drop(['bathrooms','bedrooms','beds'], axis=1)","02471cbd":"# buid model and check summary\ndata_summary(numericdataX)","ec3a8387":"crossValidationError(numericdataX)","1cfe53a4":"filedata.room_type.value_counts()","4e19f1bd":"#creating dummy variable for column room_type\nnumericdataX=pd.concat([numericdataX,filedata['room_type']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns= ['room_type'],drop_first=True)","3721a524":"numericdataX","0486ea77":"filedata.bed_type.value_counts()","b79a9227":"numericdataX=pd.concat([numericdataX,filedata['bed_type']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns=['bed_type'],drop_first=True)","1d368b7c":"filedata.cancellation_policy.value_counts()","365f28c2":"numericdataX=pd.concat([numericdataX,filedata['cancellation_policy']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns=['cancellation_policy'],drop_first=True)","642460c6":"filedata.city.value_counts()","baf7bd45":"numericdataX=pd.concat([numericdataX,filedata['city']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns=['city'],drop_first=True)","60864c22":"filedata.instant_bookable.value_counts()\nnumericdataX=pd.concat([numericdataX,filedata['instant_bookable']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns=['instant_bookable'],drop_first=True)","ea98dd91":"checkCorrelation(numericdataX)","aa86312d":"data_summary(numericdataX.drop(['log_price'],axis=1))","ada0281d":"filedata.property_type.value_counts()\nnumericdataX=pd.concat([numericdataX,filedata['property_type']], axis=1)\nnumericdataX=pd.get_dummies(numericdataX,columns=['property_type'],drop_first=True)","88a0bc14":"data_summary(numericdataX)","4dd299e1":"crossValidationError(numericdataX)","a98c62fb":"# P value of bed type has poor P value\nnumericdataX = numericdataX.loc[:, ~numericdataX.columns.str.startswith('bed_type_')]","327edcb6":"data_summary(numericdataX.drop(['log_price'],axis=1))","3ff3a7f2":"crossValidationError(numericdataX)","d7c11a0b":"filedata.columns","797e5c7d":"interactionDF= pd.DataFrame()\ninteractionDF['bedrooms']=filedata['bedrooms']\ninteractionDF['beds']=filedata['beds']\ninteractionDF['bathrooms']=filedata['bathrooms']\ninteractionDF['bed*bathroom*bedrooms']=filedata['bedrooms']*filedata['beds']*filedata['bathrooms']\ndata_summary(interactionDF)","aec0c5cd":"numericdataX= pd.concat([numericdataX,interactionDF],axis=1)\ndata_summary(numericdataX)","30442de8":"interactionDF1= pd.DataFrame()\ninteractionDF1['review_scores_rating']=filedata['review_scores_rating']\ninteractionDF1['number_of_reviews']=filedata['number_of_reviews']\ninteractionDF1['reiew_score*Number']=filedata['review_scores_rating']*filedata['number_of_reviews']\ndata_summary(interactionDF1)","eb452923":"numericdataX= pd.concat([numericdataX,interactionDF1],axis=1)","c02e684e":"data_summary(numericdataX)","d2663e6f":"crossValidationError(numericdataX)","3d2988fa":"\n##import h2o\n##from h2o.automl import H2OAutoML\n##h2o.init()","46687e31":"numericdataX","b8b5568f":"filedata","2903077f":"mean_log= np.mean(numericdataX['log_price'])","2c13243c":"classificationData= numericdataX","9794d5e0":"classificationData.loc[ classificationData['log_price'] <= mean_log, 'log_price'] = 0","cb8509cf":"classificationData.loc[ classificationData['log_price'] > mean_log, 'log_price'] = 1","3bfb6f5a":"classificationDataY= classificationData['log_price']\nclassificationDataX=classificationData.drop(['log_price'],axis=1)","2681c376":"from sklearn.linear_model import LogisticRegression","6e1f4743":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(classificationDataX, classificationDataY, test_size = 0.2,random_state=0)","02c6d4f4":"classifier= LogisticRegression()","b40cef7d":"classifier.fit(X_train,y_train)","03cbf43d":"y_pred = classifier.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))","5a394845":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","4d169a7c":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","57055bfd":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\ndef rocAucCurve(classifier):\n    logit_roc_auc = roc_auc_score(y_test, classifier.predict(X_test))\n    fpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\n    plt.figure()\n    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('Log_ROC')\n    plt.show()","354f5c40":"rocAucCurve(classifier)","a11c6a5e":"from sklearn.ensemble import RandomForestClassifier  \nclassifierDT = RandomForestClassifier()  \nclassifierDT.fit(X_train, y_train) ","46065f5b":"y_pred = classifierDT.predict(X_test)  ","e26cf4df":"from sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))  ","29b98c2f":"rocAucCurve(classifierDT)","1eec1267":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint","2bbd00e1":"#Let us see what default parameters our model used\nprint('Parameters currently in use:\\n')\npprint(classifierDT.get_params())","2ce51a7b":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 200]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","209a2bae":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","728e36ad":"grid_search.best_params_","b47cecc0":"random1=RandomForestClassifier(n_estimators=200,max_depth=90, min_samples_split=8, min_samples_leaf=3, max_features=3,bootstrap=True)","ab1c2dc3":"random1.fit(X_train,y_train)\ny_pred = random1.predict(X_test)  \nprint(confusion_matrix(y_test, y_pred))  ","b73498d3":"print(classification_report(y_test, y_pred)) \nrocAucCurve(random1)","a2c74d05":"from sklearn.ensemble import GradientBoostingClassifier","155a0581":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","6c979160":"y_pred = gb.predict(X_test)  \nprint(confusion_matrix(y_test, y_pred))","4f6a0125":"print(classification_report(y_test, y_pred)) \nrocAucCurve(gb)","cb52904d":"learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n    print()","9bc90bfd":"#Learning rate 1  is good \ngb_op = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.75, max_features=2, max_depth = 2, random_state = 0)\ngb_op.fit(X_train,y_train)","ec6b05e0":"y_pred = gb_op.predict(X_test)  \nprint(confusion_matrix(y_test, y_pred))","ac812f9b":"print(classification_report(y_test, y_pred)) \nrocAucCurve(gb_op)","555ec751":"Y= filedata['log_price']","0330190e":"numericdataX= numericdataX.drop(['log_price'],axis=1)","c9d455da":"X_train, X_test, y_train, y_test = train_test_split(numericdataX,Y, test_size=0.2, random_state=0)  ","40a8598f":"from sklearn.ensemble import RandomForestRegressor  \nregressor = RandomForestRegressor()  \nregressor.fit(X_train, y_train)  ","2cfb4bbd":"y_pred = regressor.predict(X_test) ","cd9db165":"df=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})  \ndf","ec835bbd":"def regression_Metrics(y_test, y_pred):  \n    from sklearn import metrics  \n    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","66006e70":"regression_Metrics(y_test,y_pred)","449d2158":"#Let us see what default parameters our model used\nprint('Parameters currently in use:\\n')\npprint(regressor.get_params())","e5626450":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 200]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","dc3cdc19":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","64c16f62":"grid_search.best_params_","3d8677e7":"regressor1=RandomForestRegressor(n_estimators=200,max_depth=90, min_samples_split=10, min_samples_leaf=10, max_features=3,bootstrap=True)","7af70e48":"regressor1.fit(X_train,y_train)\ny_pred = regressor1.predict(X_test) ","efe10745":"regression_Metrics(y_test,y_pred)","622fe532":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\ngb.fit(X_train, y_train)","bb6ae84e":"y_pred = gb.predict(X_test)  ","01715bf0":"regression_Metrics(y_test,y_pred)","f6feec6b":"learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in learning_rates:\n    gb = GradientBoostingRegressor(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n    print()","e7f293c6":"gb_op = GradientBoostingRegressor(n_estimators=20, learning_rate = 1, max_features=2, max_depth = 2, random_state = 0)\ngb_op.fit(X_train,y_train)\ny_pred = gb_op.predict(X_test)","4946c637":"regression_Metrics(y_test,y_pred)","92a647ad":"## Thanks You!!!!! ","8a7b7b07":"## Lets start with H20","2ef61897":"# Load File","1b555636":"Compute precision, recall, F-measure and support","1fc7a282":"## As we can see above tha by adding an interaction term our cross validation error gets lower and we are getting perfect R2 ,Aic and Bic values \n## This is a model we were looking for","1b8e169b":" ### 2. Boosting based tree algorithm (GradientBoosting)\n Lets consider another set of parameters for managing boosting:\n\n- learning_rate\nThis determines the impact of each tree on the final outcome (step 2.4). GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\nLower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\nLower values would require higher number of trees to model all the relations and will be computationally expensive.\n- n_estimators\nThe number of sequential trees to be modeled (step 2)\nThough GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\nsubsample\nThe fraction of observations to be selected for each tree. Selection is done by random sampling.\nValues slightly less than 1 make the model robust by reducing the variance.\nTypical values ~0.8 generally work fine but can be fine-tuned further.\nApart from these, there are certain miscellaneous parameters which affect overall functionality:\n\n- loss\nIt refers to the loss function to be minimized in each split.\nIt can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.\n- init\nThis affects initialization of the output.\nThis can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n- random_state\nThe random number seed so that same random numbers are generated every time.\nThis is important for parameter tuning. If we don\u2019t fix the random number, then we\u2019ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\nIt can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.\n- verbose\nThe type of output to be printed when the model fits. The different values can be:\n0: no output generated (default)\n1: output generated for trees in certain intervals >1: output generated for all trees\n- warm_start\nThis parameter has an interesting application and can help a lot if used judicially.\nUsing this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n- presort \n Select whether to presort data for faster splits.\nIt makes the selection automatically by default but it can be changed if needed.\n\n\n","84c865d1":"Interpretation: Of the entire test set, 81% price wa predicted properly","21356050":"## Classification with Trees","6d51b867":"# Linear Regression","d5366c25":"1.  Linear Regression:\nRoot Mean Squared Error: 0.4721355501041085\n\n2.  Linear Regresssion with trees:\n    - Random Forest\n        -  Mean Absolute Error: 0.3681282451600619\n        - Mean Squared Error: 0.2388446162343491\n        - Root Mean Squared Error: 0.48871731730556583\n\n    -  Boosting \n         - Mean Absolute Error: 0.39340469697589975\n         - Mean Squared Error: 0.2670819491835599\n         - Root Mean Squared Error: 0.5167997186372685\n\n","5e7ad8d9":"## Comparison of different Approaches:","ce64799f":"Here after Tuning the parameters we got AUC as 0.81 as compared to \nRandom Forest not Tuned Parameter","9af20ff7":"# Linear Regression on Airbnb Dataset\nThis is a dataset of AirBnb having 29 columns. Aim of this linear regression is to predict the price of room from given features.","8c281671":"## Linear Regression","9933ed0d":"\nEvaluating the Algorithm\nTo evaluate performance of the regression algorithm, the commonly used metrics are mean absolute error, mean squared error, and root mean squared error. The Scikit-Learn library contains functions that can help calculate these values for us. To do so, use this code from the metrics package:\n","30ae90b2":" 1. Bagging based tree algorithm (Random Forest)","1378e7d2":"# Load Libraries","ce235b50":"The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. \nThe dotted line represents the ROC curve of a purely random classifier;\na good classifier stays as far away from that line as possible (toward the top-left corner).","efabefe5":"# Interaction Term","e5498240":"## Boosting based tree algorithm (GradientBoosting)","44291421":"## Comparison of Classification Models\nLets Compare  Precision and AUC \n1. Logistic Regression\n  - Precision : 81%\n  - AUC : 0.81\n2. Random Forest\n  - Precision : 81%\n  - AUC : 0.81\n3. Gradient Boosting \n - Precision : 77%\n - AUC : 0.77","743fe55a":"## Hyper Parameter Tuning","66b56490":"### Functions","cdb343e3":"As per the comparison Regression with linear regression works well on given Model","0f540583":"6127+ 5905 correct predictions and 1468+1323 incorrect predictions.","455d6650":"## Handling Categorical Variables","fec9c1f7":"## Assumption that data point are linearly disributed ","90bed6f7":"# Multicolinearity","a548a06f":"## B. Classification with Trees\nThe best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance,\n Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model.\n Hyper Parameter\n  - n_estimators = number of trees in the foreset\n  - max_features = max number of features considered for splitting a node\n  - max_depth = max number of levels in each decision tree\n  - min_samples_split = min number of data points placed in a node before the node is split\n  - min_samples_leaf = min number of data points allowed in a leaf node\n  - bootstrap = method for sampling data points (with or without replacement)","cd651a3f":"75% correct Prediction","6cccd1ab":"## A.Classification with Logistic Regression\n## B. Classification with Trees\n               1. Bagging based tree algorithm (Random Forest)\n               2. Boosting based tree algorithm (GradientBoosting)\n","816094d4":"## Q-Q plot\n","93ed0083":"# Exploratory Data Analysis(EDA)","d77a871a":"## A.Classification with Logistic Regression","201bdbcb":"### Hyper Parameters Tuning","ae1691d3":"As per the comparison Logistic Regression and Random Forest Works well on given Model for classification","a3e3aefb":"# Linear Assumptions","078ed789":"Random Hyperparameter Grid\nTo use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:\n    [More Details](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","ba68c4a8":"### Regression with Trees","0c55a40e":"## File structure and content","baa10980":"Evaluating the Algorithm","f3084dff":"The precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_test."}}