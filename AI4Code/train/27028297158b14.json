{"cell_type":{"756afd18":"code","8eec2dc5":"code","012beccb":"code","753058a2":"code","13130148":"code","8608d020":"code","9d0f8326":"code","887914fa":"code","3267f656":"code","1d5ffaab":"code","77e297c5":"code","bf6944e2":"code","64187fc9":"code","ee151054":"code","12b8fd76":"markdown","a45cc71a":"markdown","c8d64b98":"markdown","34bc4461":"markdown","82a0996d":"markdown"},"source":{"756afd18":"#Import libraries\nimport numpy as np\nimport keras\nfrom keras.datasets import fashion_mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, MaxPooling2D, BatchNormalization\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\n\n#MNIST data is downloaded and splitted in train\/test set\n(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()\n\n#First dataset image is visualized\nfirst_image = x_train_orig[0]\nfirst_image = np.array(first_image, dtype='float')\npixels = first_image.reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()","8eec2dc5":"#Reshape and OneHotEncoder\nx_train = x_train_orig.reshape(60000,28,28,1)\nx_test = x_test_orig.reshape(10000,28,28,1)\n\ny_train = to_categorical(y_train_orig, 10)\ny_test = to_categorical(y_test_orig, 10)","012beccb":"#The create_cnn function define a convolutional neural network with corresponding layers.\ndef create_cnn ():\n    model = Sequential ()\n    #Since the first layer of this cnn is a 32 kernel convolution layer, it is passed as an argument the\n    # input_shape parameter, indicating that the input images will be 28x28 dimension and grayscale\n    model.add (Conv2D (32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\n    # The second convolutional layer is created, in this case with 64 kernels\n    model.add (Conv2D (64, (3, 3), activation = 'relu'))\n    #Next, a two-dimensional maxpooling layer is created, indicating a stride = 2\n    model.add (MaxPooling2D ((2, 2), strides = 2))\n    #In this stage we add a Dropout layer that randomly removes a set of neurons from the stage\n    #of training\n    model.add (Dropout (0.25))\n    #The flatten layer transforms a two-dimensional matrix (in this case, our images) into a vector that\n    #can be processed by FC layer (fully connected)\n    model.add (Flatten ())\n    # We added a dense layer of 128 neurons with relu activation to interpret the characteristics.\n    model.add (Dense (128, activation = 'relu'))\n    model.add (Dropout (0.5))\n    #Finally, we add an output layer with softmax activation function with 10 nodes, since the object of this\n    #cnn is the classification among 10 kinds of objects.\n    model.add (Dense (10, activation = 'softmax'))\n    return model","753058a2":"# First, we create the model through the create_cnn function defined above\nmodel = create_cnn ()\n#Adadelta optimizer is defined. It is recommended to leave the default parameters in this optimizer.\nsgd = optimizers.Adadelta (learning_rate = 1.0, rho = 0.95)\nmodel.compile (loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n\n# Next, the model is trained using the test set as a validation set\nhist_model = model.fit (x_train, y_train, validation_data = (x_test, y_test), epochs = 12, batch_size = 128)","13130148":"#An accuracy and loss evolution graph is represented:\nplt.title('Precisi\u00f3n')\nplt.plot(hist_model.history['accuracy'], color='blue', label='train')\nplt.plot(hist_model.history['val_accuracy'], color='orange', label='test')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","8608d020":"#Now the loss function\nplt.title('Loss')\nplt.plot(hist_model.history['loss'], color='blue', label='train')\nplt.plot(hist_model.history['val_loss'], color='orange', label='test')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","9d0f8326":"#Finally, we use the test set to evaluate the model\n_, acc = model.evaluate (x_test, y_test, verbose = 0)\nprint ('%.3f' % (acc * 100.0))","887914fa":"#We use the obtained model to predict the first four dataset images.\nprint(model.predict(x_test[:4]))\ny_test[:4]","3267f656":"def create_mini_vgg ():\n    #First part of the mini-vgg\n    #Sequential model is created\n    model = Sequential ()\n    #Since the first layer of this cnn is a 32 kernel convolution layer, it is passed as an argument the\n    # input_shape parameter, indicating that the input images will be 28x28 dimension and grayscale\n    model.add (Conv2D (32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\n    #After the first convolutional layer, we introduce a normalization layer. in this way we normalize\n    #the entrance to the second convolutional layer. As a consequence, the speed in the\n    #training. The chanDim parameter refers to the axis attribute of the Batch Normalization layer.\n    model.add (BatchNormalization ())\n    # We add the second convolutional layer\n    model.add (Conv2D (32, (3, 3), activation = 'relu'))\n    # Again, we normalize the input to the max pooling layer.\n    model.add (BatchNormalization ())\n    #A max pooling layer is added with stride = 2\n    model.add (MaxPooling2D ((2, 2), strides = 2))\n    #Finally to complete the first part of the cnn mini-vgg, we add a Dropout layer with probability\n    # 0.25\n    model.add (Dropout (0.25))\n    \n    #Second part of the mini-vgg\n    # The first part is replicated, but in this case we double the number of kernels, that is, we went from 32 to 64.\n    model.add (Conv2D (64, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\n    # Again, we normalize the input to the second convolutional layer.\n    model.add (BatchNormalization ())\n    # We add the second convolutional layer with 64 kernels.\n    model.add (Conv2D (64, (3, 3), activation = 'relu'))\n    # Again, we normalize the input to the max pooling layer.\n    model.add (BatchNormalization ())\n    #A max pooling layer is added with stride = 2.\n    model.add (MaxPooling2D ((2, 2), strides = 2))\n    #Finally to complete the first part of the cnn mini-vgg, we add a Dropout layer with probability\n    # 0.25.\n    model.add (Dropout (0.25))\n    \n    #Third part of the mini-vgg\n    #In this part the fully connected layer is created. In this layer take an input set, in this case,\n    #from part 2 of the mini-vgg, and outputs an n-dimensional vector, where n is the number of classes\n    #a classify.\n    #Before adding the FC layer, we add a flatten layer in order to transform the two-dimensional matrix into a vector\n    model.add (Flatten ())\n    # FC layer with 512 neurons with batch normalization and dropout.\n    model.add (Dense (512, activation = 'relu'))\n    model.add (BatchNormalization ())\n    model.add (Dropout (0.5))\n    \n    # Fourth of the mini-vgg\n    #Finally, we add the output layer with 10 nodes and a softmax activation function, in order to find the\n    # distribution of the probability of classifying a given image among 10 classes.\n    model.add (Dense (10, activation = 'softmax'))\n    \n    return model","1d5ffaab":"#First, we create the mini-vgg model through the create_mini_vgg function defined above\nmodel_vgg = create_mini_vgg ()\n#Adadelta optimizer is defined. It is recommended to leave the default parameters in this optimizer.\nsgd = optimizers.Adadelta (learning_rate = 1.0, rho = 0.95)\n#For the compilation of the model the optimizer defined in the previous line and the loss function will be used\n#categorical_crossentropy.\nmodel_vgg.compile (loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n\n# Next, model is trained using the test set as a validation set\nhist_model_vgg = model_vgg.fit (x_train, y_train, validation_data = (x_test, y_test), epochs = 12, batch_size = 128)","77e297c5":"#Representation of train and test set accuracy\nplt.title('Precisi\u00f3n Mini-VGG')\nplt.plot(hist_model_vgg.history['accuracy'], color='blue', label='train')\nplt.plot(hist_model_vgg.history['val_accuracy'], color='orange', label='test')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","bf6944e2":"#Representation of train and test set loss function\nplt.title('Loss Mini-VGG')\nplt.plot(hist_model_vgg.history['loss'], color='blue', label='train')\nplt.plot(hist_model_vgg.history['val_loss'], color='orange', label='test')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","64187fc9":"#The model is evaluated using test set.\n_, acc_vgg = model_vgg.evaluate(x_test, y_test, verbose=0)\nprint('%.3f' % (acc_vgg * 100.0))","ee151054":"#We use the obtained model to predict the first four dataset images.\nprint(model.predict(x_test[:4]))\ny_test[:4]","12b8fd76":"Step 1: Data initialization and loading","a45cc71a":"Step 2: Deep CNN + Dropout model creation, compile, training and evaluation\n\nA deep convolutional neural network will be created with below parameter:\n\n- Two convolution layers of 32 and 64 kernels, size 3x3 and stride 1, both with ReLU activation.\n- A maxpooling layer with a kernel size (2x2) and stride 2.\n- A dropout layer (with probability equal to 0.25).\n- A flatten layer.\n- A dense layer of 128 neurons with reluctance activation.\n- A dropout layer (with probability equal to 0.5).\n- A dense layer with softmax activation.","c8d64b98":"For compilation we will use:\n- keras.losses.categorical_crossentropy as a function of loss\n- keras.optimizers.Adadelta () as optimizacor\n- The metric to optimize will be the accuracy\n- To train the model we will use a batch_size = 128, epochs = 12 and the test set to validate.","34bc4461":"Step 3: Mini-VGG model creation, compile, training and evaluation\n\nA convolutional neural network based on VGG will be implemented. We\u00b4ll use a secuential model divided in 4 parts:\n\n- First: CONV (32 kernels of size 3x3 and stride 1) => RELU => CONV => RELU => POOL (2x2, stride 2). After the activation layers we will add Batch Normalization (chanDim = -1) and after the pooling layer we will add a dropout = 0.25.\n- Second: Same as the first part, but doubling the number of \"feature maps\" or kernels in the convolutional layers.\n- Third: FC (Fully connected of 512 neurons) => RELU. Add Batch Normalization and Dropout = 0.5.\n- Fourth: Finally we add the classifier (softmax with the corresponding number of outputs).","82a0996d":"For compilation we will use:\n\n- keras.losses.categorical_crossentropy as a function of loss\n- keras.optimizers.Adadelta () as optimizacor\n- The metric to optimize will be the accuracy\n- To train the model we will use a batch_size = 128, epochs = 12 and the test set to validate."}}