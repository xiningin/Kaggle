{"cell_type":{"c089fdfa":"code","757972bf":"code","29122492":"code","a89f84b0":"code","e00d111e":"code","48a375d4":"code","cba0bbaf":"code","059fbbc9":"code","dd8f8155":"code","4543ad9c":"code","dda9efe5":"code","feed9915":"code","93d64479":"code","bbd3dd01":"code","19daace9":"code","b42fbc51":"code","0a6fbe99":"code","90b9549c":"code","0a6c3abe":"markdown","41b68f5b":"markdown","7c86fd5c":"markdown","42f24d25":"markdown","140bf2d4":"markdown","7cc6623a":"markdown","7f8d3f62":"markdown","415964c7":"markdown","601b5498":"markdown","abc1caa4":"markdown","1d400cb9":"markdown","7b55b69b":"markdown","f16637bb":"markdown","73b55f9f":"markdown","921bec82":"markdown","8ec52bad":"markdown","da21b610":"markdown"},"source":{"c089fdfa":"import numpy as np\nimport pandas as pd\n\ndataset = pd.read_json(\"\/kaggle\/input\/planesnet\/planesnet\/planesnet.json\")\ndataset.drop(\"scene_ids\", axis=1, inplace=True)\ndataset.drop(\"locations\", axis=1, inplace=True)\ndataset.head()","757972bf":"pixel_columns_name = np.empty(1200, dtype=object)\nchannels = [\"r\", \"g\", \"b\"] \nfor c in channels:\n    for p in range(400):\n        pixel_columns_name[400*channels.index(c)+p] = c+\"_\"+str(p)\n\nimage_data = pd.DataFrame(dataset['data'].values.tolist(), columns=pixel_columns_name)\nlabels = dataset.labels\n\nimage_data = pd.concat([image_data, labels], axis=1)\nimage_data.head()","29122492":"from matplotlib import pyplot as plt\nfrom matplotlib import colors\nimport random\n%matplotlib inline\n\nimg_number = random.randrange(0, len(image_data.labels.tolist()))\nprint(\"Image number:\", img_number, \"with label:\", image_data.labels[img_number])\n\nplt.subplot(1, 3, 1)\nplt.imshow(np.array(image_data.iloc[img_number,0:400].values.tolist()).reshape(20,20), cmap='Reds_r')\nplt.title(\"R\")\nplt.subplot(1, 3, 2)\nplt.imshow(np.array(image_data.iloc[img_number,401:801].values.tolist()).reshape(20,20), cmap='Greens_r')\nplt.title(\"G\")\nplt.subplot(1, 3, 3)\nplt.imshow(np.array(image_data.iloc[img_number,801:1201].values.tolist()).reshape(20,20), cmap='Blues_r')\nplt.title(\"B\")\nplt.show()","a89f84b0":"y = image_data.labels\nimage_data.drop(\"labels\", inplace=True, axis=1)\nX = image_data","e00d111e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","48a375d4":"X_train = X_train.to_numpy().reshape(-1, 20,20, 3) # 20*20, 3 channels (R-G-B)\nX_test = X_test.to_numpy().reshape(-1, 20,20, 3) # 20*20, 3 channels (R-G-B)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","cba0bbaf":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train = X_train \/ 255.\nX_test = X_test \/ 255.","059fbbc9":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","dd8f8155":"X_train,X_valid,y_train,y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=13)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","4543ad9c":"import keras\nimport tensorflow as tf\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\n\nfrom tensorflow.python.client import device_lib \ndevice_lib.list_local_devices() # let's list all available computing devices","dda9efe5":"batch_size = 64\nepochs = 20\nnum_classes = 2 # in fact, each image could or could not contain a plane","feed9915":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),activation='linear',input_shape=(20,20,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D((4, 4),padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(4, 4),padding='same'))\nmodel.add(Conv2D(256, (3, 3), activation='linear',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(MaxPooling2D(pool_size=(4, 4),padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='linear'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(Dense(num_classes, activation='softmax'))","93d64479":"model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\nmodel.summary()","bbd3dd01":"with tf.device('\/GPU:0'):\n    model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_valid, y_valid))","19daace9":"plt.figure(figsize=(7, 7), dpi=80)\nplt.subplot(2,1,1)\nplt.title(\"Training History - Accuracy\")\nplt.plot(range(epochs), model_train.history[\"accuracy\"], label=\"accuracy\", color=\"red\")\nplt.scatter(range(epochs), model_train.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.xticks(range(0,epochs,1))\nmin_y = min(np.min(model_train.history[\"val_accuracy\"]), np.min(model_train.history[\"accuracy\"]))\nplt.yticks(np.linspace(min_y-0.1,1,11))\nplt.legend()\n\n\nplt.subplot(2,1,2)\nplt.title(\"Training History - Loss\")\nplt.plot(range(epochs), model_train.history[\"val_loss\"], label=\"val_loss\", color=\"red\")\nplt.scatter(range(epochs), model_train.history[\"loss\"], label=\"loss\")\nplt.xticks(range(0,epochs,1))\nmax_y = max(np.max(model_train.history[\"val_loss\"]), np.max(model_train.history[\"loss\"]))\nplt.yticks(np.linspace(0,max_y+0.1,11))\nplt.legend()","b42fbc51":"test_eval = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1]*100, \"%\")","0a6fbe99":"from sklearn.metrics import classification_report\n\ny_pred = np.argmax(np.round(model.predict(X_test)), axis=1)\n\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(y_test, y_pred, target_names=target_names))","90b9549c":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\ncf = confusion_matrix(y_test, y_pred)\nsn.heatmap(cf, annot=True)\nplt.title(\"Confusion Matrix\")","0a6c3abe":"Labels meaning:\n* **0**: not planes\n* **1**: planes\n\nLet's split `data` into 1200 columns, one for each `channel_pixel`, so we will have: `r_0...r_399, g_0...g_399, b_0...b_399`","41b68f5b":"## Training\nHere can be used CPU, but using GPU training is significantly faster (~2s\/epoch against ~12s\/epoch).","7c86fd5c":"`data` field contains, for each image, its information: is a list of `400*3=1200` values. \nEvery considered image is compound by a 20x20 pixels. Each pixel has RGB information. So, an image is \"splitted\" in three: R_image, G_image, B_image. Then, `data` is compound like this: \n`[R_image, G_image, B_image]` where each \\\\( x\\_image \\\\) is a sequence of 400 values \\\\( i \\\\) with \\\\( 0\u2264i\u2264255 \\\\).\nIn the following code we split `data` into 1200 columns named like this: \\\\( channel\\_pixelNumber \\\\). So we will have: `r_0, r_1,..., r_399, g_0, ..., g_399, b_0, ..., b_399`.","42f24d25":"## Building Model\n### Model Definition","140bf2d4":"## Testing\nNow is time to evaluate the model on test dataset","7cc6623a":"Let's reshape data for making them fitting into the model that will built later, giving to every image the shape of 3-dimensional object (20x20x3).","7f8d3f62":"Now let's define the model. It's based on CNN with the following kind of layers:\n* **Conv2D**: convulutional layer. In the first layer the input is shaped (20, 20, 3): a 3D-object of 20x20x3, or a matrix 20x20 for each channel of information (R, G, B);\n* **LeakyReLU**: this layer helps to manage non-linearity of data using a non linear activation function;\n* **MaxPooling2D**: used for reduce data dimensionality;\n* **Flatten**: number of units proportional to input;\n* **Dense**: regular dense connected layer.","415964c7":"* The data right are in int8 format. Before using them in the CNN, let's converte them into float32, and rescaling them bringing every pixel value in range \\\\( [0, 1] \\\\).","601b5498":"# Introduction\nConsidering images like these:\n![satellitar view with planes](http:\/\/i.imgur.com\/SkimtmU.png)\n![satellitar view without planes](http:\/\/i.imgur.com\/9mxE7Ca.png)\nwe want to be able to recognize planes in satellitar photos.\nIn the following notebok, will be used *Deep Convolutional Neural Network* in order to achive this goal.\n\nDataset is compound by 32.000 images of which 8.000 are plane images. Every image is a 20x20 RGB pixels image.\n\nN.B.: in this notebook will be used **Tensorflow** backend.","abc1caa4":"Finally, let's represet the **confusion matrix**","1d400cb9":"Now let's extract a **validation set** from the training set","7b55b69b":"Now let's visualize a random image, just to take a look","f16637bb":"## Preprocessing Data\nLet's import data from `.jons` file, then drop `scene_ids` column which is a reference to the entire scene photographed, and `locations` which are not relevant to the purpose.","73b55f9f":"Let's compile the model, setting as loss function *crossentropy* and as metric *accuracy*.","921bec82":"Let's define basic network parameters","8ec52bad":"Exploiting `model_train`, let's visualize learning history","da21b610":"### Defining training and testing dataset"}}