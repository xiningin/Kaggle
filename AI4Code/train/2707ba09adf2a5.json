{"cell_type":{"cad85e5b":"code","c1d2ff1b":"code","075dc826":"code","d1a4d785":"code","529fdc8e":"code","36cdf4e6":"code","c7495cab":"code","073302c0":"code","9847061a":"code","80ef221e":"code","f764f68c":"code","fb51219f":"code","ecd744f5":"code","b2e688a9":"code","14d68ce0":"code","383663f2":"code","d2f66af0":"code","98165531":"code","ae3e9a74":"code","ec22f5bf":"code","f8c67e1b":"code","5d13a840":"code","cd8b632e":"code","efb1cc94":"code","6d6ab8b3":"code","3ade2eba":"code","66eb093c":"code","94c2fc49":"code","8feeb9da":"code","44aef8fd":"code","a31eb278":"code","c11c9776":"code","3e585463":"code","97923198":"code","00a42ae1":"code","b8f99d02":"code","b82a4878":"code","70fd087a":"code","5f562a74":"code","467e75ab":"code","13dfb4d2":"markdown","986b0369":"markdown","ac51d90b":"markdown","61c9a2b8":"markdown","6d67bee9":"markdown","b11fadc3":"markdown","c0cec507":"markdown","7c85911b":"markdown","26e543d1":"markdown","24f0d255":"markdown","1b916748":"markdown","9ee14fb6":"markdown","27466c46":"markdown","f3c7388c":"markdown","ec01e7a9":"markdown","2749bf16":"markdown","4a2390ee":"markdown","f790d666":"markdown","f3939599":"markdown","83f3170f":"markdown","7b5394be":"markdown","1e343608":"markdown","4aab1afd":"markdown","f0081870":"markdown","aaab0e17":"markdown","492ec11a":"markdown","d9b8587d":"markdown","d4059cd2":"markdown","5f11a0d7":"markdown","7d792f70":"markdown","c80c3714":"markdown","44f3e03a":"markdown","83ad6459":"markdown","bb25056e":"markdown","949b9714":"markdown"},"source":{"cad85e5b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","c1d2ff1b":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","075dc826":"df.isnull().sum()","d1a4d785":"df = df.drop('Unnamed: 32',axis=1)","529fdc8e":"df.head()","36cdf4e6":"df = df.drop('id',axis=1)","c7495cab":"df['diagnosis'].unique()","073302c0":"df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == \"M\" else 0)","9847061a":"df.info()","80ef221e":"sns.countplot(x='diagnosis',data=df)","f764f68c":"df['diagnosis'].value_counts()","fb51219f":"len(df[df['diagnosis']==1]) * 100 \/ len(df)","ecd744f5":"df.describe()","b2e688a9":"y = df['diagnosis']              # Target Variable\nX = df.drop('diagnosis',axis=1)  # Independent Variables\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ndf = pd.DataFrame(X,columns=df.columns[1:])\ndf['diagnosis'] = y","14d68ce0":"df.head()","383663f2":"df.describe()","d2f66af0":"x = df.drop('diagnosis',axis=1)\ny = df['diagnosis']\ndata = pd.concat([y,x.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","98165531":"data = pd.concat([y,x.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","ae3e9a74":"data = pd.concat([y,x.iloc[:,20:30]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","ec22f5bf":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True,cmap='viridis')","f8c67e1b":"for i in df.columns:\n    print(\"Features highly related to column {}:\".format(i))\n    related_list = []\n    for j in df.columns:\n        if (i != j) & (abs(df.corr()[i][j]) > 0.9):\n            related_list.append(j)\n    print(related_list)\n    print(\"-\" * 50)","5d13a840":"sns.jointplot(df['texture_worst'],df['texture_mean'],kind='regg',color='purple')","cd8b632e":"sns.jointplot(df['concave points_mean'],df['concave points_worst'],kind='regg')","efb1cc94":"sns.pairplot(df[['radius_mean','radius_se','radius_worst','perimeter_mean','perimeter_se','perimeter_worst','area_mean','area_se','area_worst','diagnosis']],hue='diagnosis')","6d6ab8b3":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n\n#from sklearn.model_selection import train_test_split\n#from sklearn.model_selection import GridSearchCV\n# ","3ade2eba":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=5000,random_state=11)\nrf.fit(X,y)\nfeat_imp = pd.DataFrame(rf.feature_importances_)\nfeat_imp.index = pd.Series(df.iloc[:,:-1].columns)\nfeat_imp = (feat_imp*100).copy().sort_values(by=0,ascending=False)\nfeat_imp = feat_imp.reset_index()\nfeat_imp","66eb093c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nresults_list = []\nfor var in np.arange(feat_imp.shape[0],9,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\n    final_rf = RandomForestClassifier(random_state=11)\n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    model = gscv.fit(X_train,y_train)\n    \n    results_list.append((var, model.best_score_))\n    print(\"Model Created using the top {} variables\".format(var))\n    print(\"F1 Score: {}\".format(model.best_score_))\n    print(\"-\"*30)\n    \n    #print(str(var)+\" variables:  \"+str(model.best_estimator_)+\"  F1 score: \"+str(model.best_score_))","94c2fc49":"from imblearn.over_sampling import SMOTE\nSMOTE_list = []\nfor var in np.arange(feat_imp.shape[0],9,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\n    smote = SMOTE(random_state = 11) \n    X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n    final_rf = RandomForestClassifier(random_state=11)\n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    model = gscv.fit(X_train_smote,y_train_smote)\n    SMOTE_list.append((var, model.best_score_))\n    print(\"SMOTE Model Created using the top {} variables\".format(var))\n    print(\"F1 Score: {}\".format(model.best_score_))\n    print(\"Best Model {}\".format(model.best_estimator_))\n    print(\"-\"*30)","8feeb9da":"x_plot = range(10,31)\ny_results = [] \nfor i in range(20,-1,-1):\n    y_results.append(results_list[i][1])\ny_results\ny_results_SMOTE = [] \nfor i in range(20,-1,-1):\n    y_results_SMOTE.append(SMOTE_list[i][1])\ny_results\ny_1 = y_results\ny_2 = y_results_SMOTE\n\nplt.figure(figsize=(10,6))\nplt.plot(x_plot, y_1, '-b', label='Without SMOTE')\nplt.plot(x_plot, y_2, '-r', label='With SMOTE')\nplt.legend()\nplt.xlabel('Number of Variables')\nplt.ylabel('F1 Score')\nplt.title('Figure Comparing the F1 Scores obtained with and without using SMOTE')\n","44aef8fd":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=5000,random_state=11)\nxgb.fit(X,y)\nfeat_imp_xgb = pd.DataFrame(xgb.feature_importances_)\nfeat_imp_xgb.index = pd.Series(df.iloc[:,:-1].columns)\nfeat_imp_xgb = (feat_imp_xgb*100).copy().sort_values(by=0,ascending=False)\nfeat_imp_xgb = feat_imp_xgb.reset_index()\nfeat_imp_xgb","a31eb278":"SMOTE_list_xgb = []\nfor var in np.arange(feat_imp.shape[0],9,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\n    smote = SMOTE(random_state = 11) \n    X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n    final_xgb = XGBClassifier(random_state=11)\n    gscv = GridSearchCV(estimator=final_xgb,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    model = gscv.fit(X_train_smote,y_train_smote)\n    SMOTE_list_xgb.append((var, model.best_score_))\n    print(\"SMOTE XGB Model Created using the top {} variables\".format(var))\n    print(\"F1 Score: {}\".format(model.best_score_))\n    print(\"Best Model {}\".format(model.best_estimator_))\n    print(\"-\"*30)","c11c9776":"xgb_results = []\nfor i in range(20,-1,-1):\n    xgb_results.append(SMOTE_list_xgb[i][1])\n    \nplt.figure(figsize=(10,6))\nplt.plot(x_plot, xgb_results, '-b')\nplt.xlabel('Number of Variables')\nplt.ylabel('F1 Score')\nplt.title('Figure Comparing the F1 Scores obtained using XGBoost for various numbers of input variables')","3e585463":"X_new = X[feat_imp.iloc[:18,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[100,500,1000,5000],\n    \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\nmodel = gscv.fit(X_train_smote,y_train_smote)\nfinal_rfc_model = model.best_estimator_\n    ","97923198":"rfc_preds = final_rfc_model.predict(X_test)","00a42ae1":"X_new = X[feat_imp.iloc[:26,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\nfinal_xgb = XGBClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_xgb,param_grid={\n   \"n_estimators\":[100,500,1000,5000],\n   \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\nmodel = gscv.fit(X_train_smote,y_train_smote)\nfinal_xgb_model = model.best_estimator_","b8f99d02":"xgb_preds = final_xgb_model.predict(X_test)","b82a4878":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, rfc_preds))","70fd087a":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(y_test, rfc_preds),annot=True)\nplt.ylabel('Actual Class')\nplt.xlabel('Predicted Class')\nplt.title('Predictions Using the Random Forest Classifier')","5f562a74":"print(classification_report(y_test, xgb_preds))","467e75ab":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(y_test, xgb_preds),annot=True)\nplt.ylabel('Actual Class')\nplt.xlabel('Predicted Class')\nplt.title('Predictions Using the XGBoost Classifier')","13dfb4d2":"We can clearly see the positive linear relationships between each of these variables in a pairwise format, highlighting the large values for the correlation shown in the heatmap.\n\n## 3: Feature Selection\n\nWithin this dataset, we have 30 different numerical features which may have an impact on the presence of breast cancer, with some features being more significant than others. In order to determine which features are most important, we shall make use of both the Random Forest Classifier and XGBoost Classifier. First, we must split our dataset into the input features and output variable.","986b0369":"# Predicting the Presence of Breast Cancer\n\nThe goal of this project is to create a model which accurately determines the presence of Breast Cancer. The dataset used within this notebook was found on kaggle.com (https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data), uploaded by UCI Machine Learning. \n\nIn this project, we shall undertake the following tasks:\n\n0. Package and Data Imports\n1. Data Cleaning\n2. Exploratory Data Analysis and Visualisations\n3. Feature Selection\n4. Model Building\n5. Model Evaluation\n\n## 0: Package and Data Imports\n\nLet us begin by importing the packages necessary for our exploratory data analysis and visualisation. We shall import packages required for model building in section 4.","ac51d90b":"We manage to achieve near perfection predictions on the test set using the Random Forest Classifier, with only 1 instance being incorrectly classified. As a result, an accuracy of 99.1% has been achieved. As well as this, we have been able to produce a model with an F1-score of 99%. \n\n### 5.2: XGBoost Classifier\n\nIn this section, we shall repeat the analysis from above using the XGBoost classifier.","61c9a2b8":"We notice that the highest F1 score is achieved using the top 17 variables. There is a relatively large drop in performance between 12 and 11 variables, which is to be expected due to the large difference between the importance values found in the feature importance dataframe.\n\nLet us now investigate the use of SMOTE within this backward elimination process.","6d67bee9":"Let us now use Pandas to import our csv file into a dataframe.","b11fadc3":"We can see that the minimum and maximum values for each column have been reduced and we can therefore conclude that the data has been scaled successfully. Let us now produce boxplots for each feature to determine the effect they have on the presence of breast cancer. We shall split the features into 3 groups of ten to make visualisation easier.","c0cec507":"Let us plot these values to visually determine which model has produced the highest F1 score.","7c85911b":"## 1: Data Cleaning\n\nIn this section we shall attempt to engineer new featres from our dataset, as well as deal with any missing values. \n\n### 1.1: Missing Data\n\nLet us determine whether there are any missing data points within this dataset.","26e543d1":"We notice that there are 4 features that seem to have the most important impact on the presence of breast cancer. As with the Random Forest Classifier above, it is still difficult to determine the optimal number of variables. Since we found that the use of SMOTE improved the F1 score obtained, we shall use it within the backward elimination process using the XGBoost classifier.","24f0d255":"Let us now determine the most important features using a Random Forest Classifier.","1b916748":"We observe that there are no missing data points within any column excepted for the column called \"Unnamed: 32\". It appears like this column in redundant and as a result we shall remove it from the dataset.","9ee14fb6":"### 4.2: XGBoost Classifier\n\nLet us now implement the final version of the XGBoost classifier. In the feature selection section, we found that the optimal number of features was the most important 26 features.","27466c46":"The box plot above shows the range of the mean values obatined for each of the ten features measured within our dataset. We see that for each feature except fractal dimension, the median value for those cells containing breast cancer is much higher than those that do not. Cancerous cells also seem to have a wider inter quartile range for each feature in comparison to non-cancerous cells. \n\nLet us now plot the second group of ten features.","f3c7388c":"It appears that the standard error of the measurements recorded within the dataset are the least influential factor on the presence of breast cancer. However, the results shown above do not provide a clear indication into the optimal number of features for use within our model. Let us implement the backward elimination technique in order to to do this. \n\nWe shall be analysis the performance of the models created using the F1 score metric. This is due to the fact that our target class is slightly unbalanced. As a result of this, we would be able to generate high accuracy by designing a model that simply predicts the majority class. The F1 score metric is more beneficial in this case since it takes into account both false positives and negatives. In order for a high F1 score to be achieved, the model must produce high precision and recall values.","ec01e7a9":"We observe that the use of SMOTE to balance the target classes resulted in a better F1 socre for each number of variables used, as seen in the plot below. ","2749bf16":"We notice that 212 of our datapoints are classified as malignant, which equates to approximately 37% of the total dataset. These classes are relatively balanced and as a result it is not necessary to synthetically produce more instances of the minority class using a technique such as SMOTE.\n\n### 2.2: Independent Variables\n\nLet us now begin investigating our independent variables and their impact on the presence of breast cancer. Let us check the describe method of the dataframe.","4a2390ee":"Let us check the head and describe methods of the dataframe to ensure that the scaler has worked correctly.","f790d666":"For most of the features here, we again observe a difference in the median value for each of the two target classes. \n\nLet us plot the final group of features.","f3939599":"## 5: Model Analysis\n\nIn this section we shall analyse the performance achieved by the random forest and XGBoost classifiers. We shall produce confusion matrices and classification reports for each model. A confusion matrix displays the predictions made by the model against the actual label associated to them, where as the classification report displays values such as accuracy, precision and recall for each class individually.\n\n### 5.1: Random Forest Classifier\n\nWe shall now analyse the predictions made by the random forest classifier. ","83f3170f":"After looking through the list, we are able to notice that there are two groups of relationships within this dataset. We notice that the radius, perimeter and area measurements are all related, which intuitively follows expectation. Under the assumption that the cells are of circular shape, then the mathematical formulas for the perimeter and area of a circle can be used. Both formulas, given by perimeter = Pi x diameter = Pi x 2 x radius, and area = Pi x r x 2, clearly involve the radius and hence the extremely positive correlation.\n\nWe also observe high correlation between the pairs of variables \"texture_worst\" and \"texture_mean\" and \"concave points_worst\" and \"concave points_mean\". Let us produce joint plots of these pairs to highlight the relationship between the variables.","7b5394be":"We observe that some of the variables stored within the dataset have an extremely small range of values that can be taken, where as other have an extremely large range of possible values. In order to make identifying differences between classes, let us scale the data using the Standard Scaler from sci-kit learn.","1e343608":"### 1.2: Feature Extraction\n\nLet us check the info method of our dataframe.","4aab1afd":"The plots above clearly show the extremely positive relationship between these pair of variables. Let us now produce a pair plot of the 9 variables relating to the radius, perimeter and area of the cells.","f0081870":"Let us now check the head of the dataframe to investigate the types of data we have stored for each datapoint.","aaab0e17":"Again we able to observe a significant difference between the median values by class for each of the features shown above. \n\nLet us investigate the relationships between the variables by producing a correlation heatmap using Seaborn.","492ec11a":"We notice that our target column contains two possible values, \"M\" or \"B\". \"M\" stands for malignant and is used when there is presence of breast cancer. \"B\" stands for benign and is used when there are no signs of breast cancer. Let us change these values for use in our machine learning algorithms. We shall record the presence of breast cancer as a 1 and use 0 to denote no presence of breast cancer.","d9b8587d":"We can see that the model created using 26 variables obtained the highest F1 score of 0.97689 using the 'gini' criterion and 1000 estimators. We shall implement the final XGBoost classifier in the model creation section.\n\n## 4: Model Building\n\nIn this section we shall create final models using the Random Forest Classifier and XGBoost classifier using the parameters determined in the previous section. \n\n### 4.1: Random Forest Classifier\n\nWhen selecting features for this model, we found that the top 18 features, along with SMOTE, produced the highest F1 score. Let us create the final random forest classifier.","d4059cd2":"It turns out that we achieve the exact same predictions as the Random Forest Classsifier whilst using the XGBoost classifier.","5f11a0d7":"We can observe that there are variables within the dataset that are almost perfectly correlated. Let us investigate the values that are higher than 0.9.","7d792f70":"We shall now use this model to generate predictions which we shall analyse in the Model Analysis section.","c80c3714":"We notice that there are 10 unique features stored within our dataset, with 3 data points recorded for each. The mean, standard error and \"worst\" or largest of each of these features was computed and stored (UCI Machine Learning). We observe that each of these entries are of the type \"float\" and are therefore all numeric. As a result, there is no scope for feature extraction within this project and this means that feature selection will become significantly important when it comes to model building. \n\nOur data has now been successfully cleaned.\n\n## 2: Exploratory Data Analysis and Visualisations\n\nIn this section we shall attempt to determine which features have the most impact on the presence of breast cancer. \n\n### 2.1: Target Variable\n\nLet us first investigate the distribution of the points within our target variable.","44f3e03a":"The \"diagnosis\" column is our target column. Let us check the different values this column can take.","83ad6459":"The model that achieved the highest F1 score was built using SMOTE with 18 variables and the \"entropy\" criterion. In the model building section below, we shall implement the final Random Forest Classifier model using these parameters. \n\nLet us now repeat the process using the XBGoost classifier. First we shall create a feature importance dataframe.","bb25056e":"We shall now create predictions using this model.","949b9714":"The \"id\" column is simply a unique number for each item within the dataset. Unfortunately, there is no useful information to be gained from this column and as a result it shall be removed from the dataset."}}