{"cell_type":{"56f4828c":"code","61c90b2f":"code","e7747aa6":"code","65ded39c":"code","00e7f8be":"code","2e5d2826":"code","f160afe9":"code","66acd40f":"code","3f99aee2":"code","b8a6213d":"code","77bf1384":"code","e24d7e94":"code","95e3991e":"code","0163efcd":"code","50a91af4":"code","f8137c44":"code","d4031d52":"code","80ebec94":"code","e05cfa50":"code","c86ac39e":"code","08a8c35b":"code","a3ef7623":"code","8b9904e2":"code","f47f345a":"code","31ad5109":"code","a7a97343":"code","115757d8":"code","96d6dd6c":"markdown","acfa069a":"markdown","adaf6f82":"markdown","affc4e55":"markdown","8e625def":"markdown","a5aac373":"markdown","0ca968b5":"markdown","7987db61":"markdown","f1dd73ac":"markdown","890f51c1":"markdown","d5a0ed83":"markdown","6b12b2d2":"markdown","9b23dd84":"markdown","8aac62f6":"markdown","229fc78b":"markdown","d5c4bf4b":"markdown","c7949309":"markdown","b2a8a695":"markdown","d396e529":"markdown","a1a2b08e":"markdown","fdcee55a":"markdown","28b20831":"markdown","e084c865":"markdown","4748220b":"markdown","fa936ae1":"markdown","8732644e":"markdown","bb4129b9":"markdown","361114f9":"markdown","32f58036":"markdown","9508577e":"markdown","da35a25b":"markdown","7090cc29":"markdown","5f502676":"markdown","a9caf89b":"markdown"},"source":{"56f4828c":"# Loading python libraries\n%matplotlib inline\nimport seaborn as sns\ncm = sns.light_palette(\"grey\", as_cmap=True)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nimport shap\nimport os\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter('ignore')","61c90b2f":"############################################################\n# Load the school explorer data and clean it\nSCHOOL = pd.read_csv('..\/input\/data-science-for-good\/2016 School Explorer.csv', index_col=[\"Location Code\"])\nSCHOOL.columns = [x.upper() for x in SCHOOL.columns]\nSCHOOL.index.name = 'DBN'\n\n# Convert to a numeric value (i.e. 3174.99 instead of $3,174.99)\ndollars = [\"SCHOOL INCOME ESTIMATE\"]\nfor col in dollars:\n    SCHOOL[col] = SCHOOL[col].str.replace(',', '')\n    SCHOOL[col] = SCHOOL[col].str[1:].astype(float)\n# Convert these to numerics\npercents = [\"PERCENT ELL\", \"PERCENT ASIAN\", \"PERCENT BLACK\", \"PERCENT HISPANIC\", \n            \"PERCENT BLACK \/ HISPANIC\", \"PERCENT WHITE\", \"STUDENT ATTENDANCE RATE\",\n            \"PERCENT OF STUDENTS CHRONICALLY ABSENT\", \"RIGOROUS INSTRUCTION %\", \n            \"COLLABORATIVE TEACHERS %\", \"SUPPORTIVE ENVIRONMENT %\", \n            \"EFFECTIVE SCHOOL LEADERSHIP %\", \"STRONG FAMILY-COMMUNITY TIES %\", \"TRUST %\"]\nfor col in percents:\n    SCHOOL[col] = SCHOOL[col].str[:-1].astype(float) \/ 100\n# I'm just going to drop these ratings since we already have a % version of each\nratings = [\"RIGOROUS INSTRUCTION RATING\", \"COLLABORATIVE TEACHERS RATING\",\n          \"SUPPORTIVE ENVIRONMENT RATING\", \"EFFECTIVE SCHOOL LEADERSHIP RATING\",\n          \"STRONG FAMILY-COMMUNITY TIES RATING\", \"TRUST RATING\", \"STUDENT ACHIEVEMENT RATING\"]\nSCHOOL = SCHOOL.drop(ratings,axis=1)\n\n# Save school name and address for later use\nSCHOOL_NAMES = SCHOOL[[\"SCHOOL NAME\", \"ADDRESS (FULL)\"]]\n\n# Save latitude and longitude to calculate distance from closest elite school\nDISTANCE = SCHOOL[['LATITUDE', 'LONGITUDE']]\n\n# I didn't attempt to use these variables\nother_data_not_used = [\"SCHOOL INCOME ESTIMATE\", \"OTHER LOCATION CODE IN LCGMS\", \"SCHOOL NAME\", \"SED CODE\", \"ADDRESS (FULL)\", \"GRADES\", 'CITY', 'LATITUDE', 'LONGITUDE', 'ZIP']\nSCHOOL = SCHOOL.drop(other_data_not_used, axis=1)\n\n# Some more simple data cleaning \/ preprocessing\nSCHOOL[[\"ADJUSTED GRADE\", \"NEW?\"]] = SCHOOL[[\"ADJUSTED GRADE\", \"NEW?\"]].replace(\"x\", 1).fillna(0)\nSCHOOL[\"GRADE LOW\"] = SCHOOL[\"GRADE LOW\"].replace(\"0K\", 0).replace(\"PK\", -1).astype(float)\nSCHOOL[\"GRADE HIGH\"] = SCHOOL[\"GRADE HIGH\"].replace(\"0K\", 0).astype(float)\nSCHOOL[\"COMMUNITY SCHOOL?\"] = SCHOOL[\"COMMUNITY SCHOOL?\"].replace(\"Yes\", 1).replace(\"No\", 0).astype(float)\n\n# There are a massive amount of common core Result variable.  I treat these in the following way:\n# 1. Sum up all the 4s scored in ethnic, economic need and ELL sub-categories across all grade levels\n#    separately for both ELA and MATH\n# 2. Divide by the total number of students tested in all these grade levels\n# 3. The result is the total fraction of the student body that both received a 4 and belonged to\n#    that sub-category.  So for example, if my new variable MATH ELL% was 0.1 for a school,\n#    then that 10% of that school's student body was an ELL student who also received a 4 in MATH.\n# 4. Finally, I average ELA and MATH together\nSCHOOL[\"ELA TESTED\"] = SCHOOL[[\"GRADE 3 ELA - ALL STUDENTS TESTED\", \"GRADE 4 ELA - ALL STUDENTS TESTED\", \"GRADE 5 ELA - ALL STUDENTS TESTED\", \"GRADE 6 ELA - ALL STUDENTS TESTED\", \"GRADE 7 ELA - ALL STUDENTS TESTED\", \"GRADE 8 ELA - ALL STUDENTS TESTED\"]].sum(1)\nSCHOOL[\"ELA ALL 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - ALL STUDENTS\", \"GRADE 4 ELA 4S - ALL STUDENTS\", \"GRADE 5 ELA 4S - ALL STUDENTS\", \"GRADE 6 ELA 4S - ALL STUDENTS\", \"GRADE 7 ELA 4S - ALL STUDENTS\", \"GRADE 8 ELA 4S - ALL STUDENTS\"]].sum(1)\nSCHOOL[\"ELA AAALN 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 4 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 5 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 6 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 7 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 8 ELA 4S - AMERICAN INDIAN OR ALASKA NATIVE\"]].sum(1)\nSCHOOL[\"ELA BLACK 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 4 ELA 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 5 ELA 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 6 ELA 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 7 ELA 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 8 ELA 4S - BLACK OR AFRICAN AMERICAN\"]].sum(1)\nSCHOOL[\"ELA LATINO 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - HISPANIC OR LATINO\", \"GRADE 4 ELA 4S - HISPANIC OR LATINO\", \"GRADE 5 ELA 4S - HISPANIC OR LATINO\", \"GRADE 6 ELA 4S - HISPANIC OR LATINO\", \"GRADE 7 ELA 4S - HISPANIC OR LATINO\", \"GRADE 8 ELA 4S - HISPANIC OR LATINO\"]].sum(1)\nSCHOOL[\"ELA ASIAN 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 4 ELA 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 5 ELA 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 6 ELA 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 7 ELA 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 8 ELA 4S - ASIAN OR PACIFIC ISLANDER\"]].sum(1)\nSCHOOL[\"ELA WHITE 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - WHITE\", \"GRADE 4 ELA 4S - WHITE\", \"GRADE 5 ELA 4S - WHITE\", \"GRADE 6 ELA 4S - WHITE\", \"GRADE 7 ELA 4S - WHITE\", \"GRADE 8 ELA 4S - WHITE\"]].sum(1)\nSCHOOL[\"ELA MULTIRACIAL 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - MULTIRACIAL\", \"GRADE 4 ELA 4S - MULTIRACIAL\", \"GRADE 5 ELA 4S - MULTIRACIAL\", \"GRADE 6 ELA 4S - MULTIRACIAL\", \"GRADE 7 ELA 4S - MULTIRACIAL\", \"GRADE 8 ELA 4S - MULTIRACIAL\"]].sum(1)\nSCHOOL[\"ELA ECON 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 4 ELA 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 5 ELA 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 6 ELA 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 7 ELA 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 8 ELA 4S - ECONOMICALLY DISADVANTAGED\"]].sum(1)\nSCHOOL[\"ELA ELL 4%\"] = SCHOOL[[\"GRADE 3 ELA 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 4 ELA 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 5 ELA 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 6 ELA 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 7 ELA 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 8 ELA 4S - LIMITED ENGLISH PROFICIENT\"]].sum(1)\n\nSCHOOL[\"MATH TESTED\"] = SCHOOL[[\"GRADE 3 MATH - ALL STUDENTS TESTED\", \"GRADE 4 MATH - ALL STUDENTS TESTED\", \"GRADE 5 MATH - ALL STUDENTS TESTED\", \"GRADE 6 MATH - ALL STUDENTS TESTED\", \"GRADE 7 MATH - ALL STUDENTS TESTED\", \"GRADE 8 MATH - ALL STUDENTS TESTED\"]].sum(1)\nSCHOOL[\"MATH ALL 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - ALL STUDENTS\", \"GRADE 4 MATH 4S - ALL STUDENTS\", \"GRADE 5 MATH 4S - ALL STUDENTS\", \"GRADE 6 MATH 4S - ALL STUDENTS\", \"GRADE 7 MATH 4S - ALL STUDENTS\", \"GRADE 8 MATH 4S - ALL STUDENTS\"]].sum(1)\nSCHOOL[\"MATH AAALN 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 4 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 5 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 6 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 7 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\", \"GRADE 8 MATH 4S - AMERICAN INDIAN OR ALASKA NATIVE\"]].sum(1)\nSCHOOL[\"MATH BLACK 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 4 MATH 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 5 MATH 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 6 MATH 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 7 MATH 4S - BLACK OR AFRICAN AMERICAN\", \"GRADE 8 MATH 4S - BLACK OR AFRICAN AMERICAN\"]].sum(1)\nSCHOOL[\"MATH LATINO 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - HISPANIC OR LATINO\", \"GRADE 4 MATH 4S - HISPANIC OR LATINO\", \"GRADE 5 MATH 4S - HISPANIC OR LATINO\", \"GRADE 6 MATH 4S - HISPANIC OR LATINO\", \"GRADE 7 MATH 4S - HISPANIC OR LATINO\", \"GRADE 8 MATH 4S - HISPANIC OR LATINO\"]].sum(1)\nSCHOOL[\"MATH ASIAN 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 4 MATH 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 5 MATH 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 6 MATH 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 7 MATH 4S - ASIAN OR PACIFIC ISLANDER\", \"GRADE 8 MATH 4S - ASIAN OR PACIFIC ISLANDER\"]].sum(1)\nSCHOOL[\"MATH WHITE 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - WHITE\", \"GRADE 4 MATH 4S - WHITE\", \"GRADE 5 MATH 4S - WHITE\", \"GRADE 6 MATH 4S - WHITE\", \"GRADE 7 MATH 4S - WHITE\", \"GRADE 8 MATH 4S - WHITE\"]].sum(1)\nSCHOOL[\"MATH MULTIRACIAL 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - MULTIRACIAL\", \"GRADE 4 MATH 4S - MULTIRACIAL\", \"GRADE 5 MATH 4S - MULTIRACIAL\", \"GRADE 6 MATH 4S - MULTIRACIAL\", \"GRADE 7 MATH 4S - MULTIRACIAL\", \"GRADE 8 MATH 4S - MULTIRACIAL\"]].sum(1)\nSCHOOL[\"MATH ECON 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 4 MATH 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 5 MATH 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 6 MATH 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 7 MATH 4S - ECONOMICALLY DISADVANTAGED\", \"GRADE 8 MATH 4S - ECONOMICALLY DISADVANTAGED\"]].sum(1)\nSCHOOL[\"MATH ELL 4%\"] = SCHOOL[[\"GRADE 3 MATH 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 4 MATH 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 5 MATH 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 6 MATH 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 7 MATH 4S - LIMITED ENGLISH PROFICIENT\", \"GRADE 8 MATH 4S - LIMITED ENGLISH PROFICIENT\"]].sum(1)\n\n# I also save the total number of Grade 8 students tested as a rough proxy for stduent enrollment\nnumber_of_ela_grades = (SCHOOL[[\"GRADE 3 ELA - ALL STUDENTS TESTED\", \"GRADE 4 ELA - ALL STUDENTS TESTED\", \"GRADE 5 ELA - ALL STUDENTS TESTED\", \"GRADE 6 ELA - ALL STUDENTS TESTED\", \"GRADE 7 ELA - ALL STUDENTS TESTED\", \"GRADE 8 ELA - ALL STUDENTS TESTED\"]]>1).sum(1)\nnumber_of_math_grades = (SCHOOL[[\"GRADE 3 MATH - ALL STUDENTS TESTED\", \"GRADE 4 MATH - ALL STUDENTS TESTED\", \"GRADE 5 MATH - ALL STUDENTS TESTED\", \"GRADE 6 MATH - ALL STUDENTS TESTED\", \"GRADE 7 MATH - ALL STUDENTS TESTED\", \"GRADE 8 MATH - ALL STUDENTS TESTED\"]]>1).sum(1)\nSCHOOL = SCHOOL.drop(SCHOOL.iloc[:,23:143].columns, axis=1)\n\n# Here they are converted from total number, to fraction of student-body\nelaCC = [\"ELA TESTED\",\"ELA ALL 4%\",\"ELA AAALN 4%\",\"ELA BLACK 4%\",\"ELA LATINO 4%\",\"ELA ASIAN 4%\",\n    \"ELA WHITE 4%\",\"ELA MULTIRACIAL 4%\",\"ELA ECON 4%\",\"ELA ELL 4%\"]\nmathCC = [\"MATH TESTED\",\"MATH ALL 4%\",\"MATH AAALN 4%\",\"MATH BLACK 4%\",\"MATH LATINO 4%\",\n        \"MATH ASIAN 4%\",\"MATH WHITE 4%\",\"MATH MULTIRACIAL 4%\",\"MATH ECON 4%\",\"MATH ELL 4%\"]\nfor col in mathCC[1:]:\n    SCHOOL[col] = SCHOOL[col] \/ SCHOOL[\"MATH TESTED\"].values\nfor col in elaCC[1:]:\n    SCHOOL[col] = SCHOOL[col] \/ SCHOOL[\"ELA TESTED\"].values\n\nCOMMON_CORE4S = pd.DataFrame(data=(SCHOOL[elaCC[1:]].values + SCHOOL[mathCC[1:]].values)\/ 2.0, index=SCHOOL.index, columns=[\"%Students scored 4\", \"%AAALN and scored 4\", \"%BLACK and scored 4\", \"%LATINO and scored 4\", \"%ASIAN and scored 4\", \"%WHITE and scored 4\", \"%MULTIRACIAL and scored 4\", \"%ECON NEED and scored 4\", \"%ELL and scored 4\"])\nCOMMON_CORE4S = COMMON_CORE4S.round(2)\nSCHOOL = SCHOOL.drop(elaCC, axis=1)\nSCHOOL = SCHOOL.drop(mathCC, axis=1)\n\n# We would like to identify schools with these traits, that are either doing well or potentially could do well\ndemographics = [\"ECONOMIC NEED INDEX\", \"PERCENT ELL\", \"PERCENT ASIAN\",\n\"PERCENT BLACK\",\"PERCENT HISPANIC\",\"PERCENT BLACK \/ HISPANIC\",\"PERCENT WHITE\"]\nSCHOOL_DEMOGRAPHICS = SCHOOL[demographics]\nSCHOOL_DEMOGRAPHICS.fillna(SCHOOL_DEMOGRAPHICS.mean(), inplace=True) \nOTHER_SCHOOL_DATA = SCHOOL.drop(demographics, axis=1)","e7747aa6":"SHSAT = pd.read_csv(\"..\/input\/nyc-shsat-test-results-2017\/nytdf.csv\", index_col=\"DBN\")\nSHSAT.columns = [x.upper() for x in SHSAT.columns]\nSHSAT = SHSAT[[\"OFFERSPERSTUDENT\"]]\nSHSAT.columns = [\"OffersPerStudent\"]\n\n# Join with school explorer on the DBN.  \nSHSAT = SHSAT.join(OTHER_SCHOOL_DATA, how='inner').iloc[:,:1]\n\n# convert from sting xx% to numeric 0.xx\nSHSAT[\"OffersPerStudent\"].fillna(\"0%\", inplace=True)\nSHSAT[\"OffersPerStudent\"].replace(\"0\", \"0%\", inplace=True)\nSHSAT[\"OffersPerStudent\"] = SHSAT[\"OffersPerStudent\"].str[:-1].astype(float) \/ 100","65ded39c":"SAFETY = pd.read_csv(\"..\/input\/ny-2010-2016-school-safety-report\/2010-2016-school-safety-report.csv\", index_col=\"DBN\")\nSAFETY = SAFETY.loc[SAFETY.index.dropna()]\nSAFETY.columns = [x.upper() for x in SAFETY.columns]\nEliteSchools = [\"Brooklyn Latin School, The\", \"Brooklyn Technical High School\", \"Bronx High School of Science\", \"High School for Mathematics, Science and Engineeri\", \"High School of American Studies at Lehman College\", \"Queens High School for the Sciences at York Colleg\", \"Staten Island Technical High School\", \"Stuyvesant High School\"]\nELITES = SAFETY.loc[SAFETY['LOCATION NAME'].isin(EliteSchools), ['LOCATION NAME','LATITUDE','LONGITUDE','REGISTER']]\nELITES['REGISTER'] = ELITES['REGISTER'].str.replace(',', '')\nELITES['REGISTER'] = ELITES['REGISTER'].astype(int)\nELITES = ELITES.groupby(ELITES['LOCATION NAME']).mean()\nSAFETY = SAFETY[['MAJOR N','OTH N','NOCRIM N','PROP N','VIO N']]\nSAFETY=SAFETY.groupby(['DBN']).mean()\nSAFETY.columns = [\"MAJOR CRIMES\", \"OTHER CRIMES\", \"NONCRIMINAL CRIMES\", \"PROPERTY CRIMES\", \"VIOLENT CRIMES\"]","00e7f8be":"#This code lifted from last example on this stackoverflow:\n#https:\/\/stackoverflow.com\/questions\/19412462\/getting-distance-between-two-points-based-on-latitude-longitude\ndef distance(origin, destination):\n    \"\"\"\n    Calculate the Haversine distance.\n\n    Parameters\n    ----------\n    origin : tuple of float\n        (lat, long)\n    destination : tuple of float\n        (lat, long)\n\n    Returns\n    -------\n    distance_in_km : float\n\n    Examples\n    --------\n    >>> origin = (48.1372, 11.5756)  # Munich\n    >>> destination = (52.5186, 13.4083)  # Berlin\n    >>> round(distance(origin, destination), 1)\n    504.2\n    \"\"\"\n    lat1, lon1 = origin\n    lat2, lon2 = destination\n    radius = 6371  # km\n\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat \/ 2) * math.sin(dlat \/ 2) +\n         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n         math.sin(dlon \/ 2) * math.sin(dlon \/ 2))\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    d = radius * c\n\n    return d","2e5d2826":"for school in EliteSchools:\n    DISTANCE[school] = 0.0\n    EliteTuple = (ELITES.loc[school, \"LATITUDE\"], ELITES.loc[school, \"LONGITUDE\"])\n    for row in DISTANCE.index:\n        MidTuple = (DISTANCE.loc[row, \"LATITUDE\"], DISTANCE.loc[row, \"LONGITUDE\"])\n        DISTANCE.loc[row, school] = distance(MidTuple, EliteTuple)\nDISTANCE = DISTANCE.round(1)\ndel DISTANCE['LATITUDE']\ndel DISTANCE['LONGITUDE']\nDISTANCE['DISTANCE_AVERAGE_SPECIALIZED_SCHOOL'] = DISTANCE.mean(1)\nDISTANCE['DISTANCE_NEAREST_SPECIALIZED_SCHOOL'] = DISTANCE.min(1)\nDISTANCE = DISTANCE[['DISTANCE_NEAREST_SPECIALIZED_SCHOOL', 'DISTANCE_AVERAGE_SPECIALIZED_SCHOOL']]","f160afe9":"ENROLLMENT = pd.read_csv(\"..\/input\/ny-school-demographics-and-accountability-snapshot\/2006-2012-school-demographics-and-accountability-snapshot.csv\", index_col=[\"DBN\"])\nENROLLMENT = ENROLLMENT.loc[ENROLLMENT[\"schoolyear\"]==20112012, \"grade7\"]\nENROLLMENT.replace('    ', \"0\", inplace=True)\nENROLLMENT = ENROLLMENT.astype('int')\nENROLLMENT.name = \"7thGradeEnrollment\"\nOTHER_SCHOOL_DATA = OTHER_SCHOOL_DATA.join(ENROLLMENT, how='left')","66acd40f":"SCHOOL_NAMES.head()","3f99aee2":"SHSAT.head().style.background_gradient(cmap=cm)","b8a6213d":"SCHOOL_DEMOGRAPHICS.head().style.background_gradient(cmap=cm)","77bf1384":"COMMON_CORE4S.head().style.background_gradient(cmap=cm)","e24d7e94":"SAFETY.head()","95e3991e":"DISTANCE.head()","0163efcd":"OTHER_SCHOOL_DATA.head().style.background_gradient(cmap=cm)","50a91af4":"# First identify a target for the model to work on, offers per student.\ntarget = \"OffersPerStudent\"\n# Next assemble input data: common core 4 percentages, school demographics and other school information\nmodel_input_variables = OTHER_SCHOOL_DATA.join(COMMON_CORE4S[\"%Students scored 4\"]).join(SAFETY, how='left').join(DISTANCE, how='left')\nmodel_input_variables.fillna(-.01, inplace=True) # fill in missing data with an arbitary value.\nmodel_input_variables = SHSAT[[target]].join(model_input_variables, how='inner').iloc[:,1:]\n\nRF = RandomForestRegressor(min_samples_leaf=10, n_jobs=8, n_estimators=100, random_state=0)\n# A simple grid for parameter tuning\nRF_params = {\"max_depth\": [3,6,None],\n              \"max_features\": [0.33,0.67,1.0],\n              \"min_samples_leaf\": [4,9,16]}\nRF_GRID = GridSearchCV(RF, RF_params, n_jobs=2, cv=2)\nRF_GRID.fit(model_input_variables, SHSAT[target])\nRF = RF.set_params(**RF_GRID.best_params_)\nRF.fit(model_input_variables, SHSAT[target])\n# delete variables which are not used or almost unused to keep the model on the simpler side\nmodel_input_variables = model_input_variables.loc[:, RF.feature_importances_>0.01]\nRF.fit(model_input_variables, SHSAT[target])\n# Save the model's predictions as a new variable\nSHSAT[\"PREDICTED\"] = RF.predict(model_input_variables)\nSHSAT[\"PREDICTED\"] = SHSAT[\"PREDICTED\"].round(2)","f8137c44":"importances = pd.Series(index=model_input_variables.columns, data=RF.feature_importances_).sort_values(ascending=True)\nimportances.plot(kind='barh', figsize=(11,7), color=\"orange\");","d4031d52":"SHSAT[\"UNDERPERFORM\"] = SHSAT[\"PREDICTED\"] - SHSAT[target]\n# sort all of our datasets by UNDERPERFORM\nSHSAT = SHSAT.sort_values(\"UNDERPERFORM\", ascending=False)\nSCHOOL_NAMES = SCHOOL_NAMES.loc[SHSAT.index]\nSCHOOL_DEMOGRAPHICS = SCHOOL_DEMOGRAPHICS.loc[SHSAT.index]\nCOMMON_CORE4S = COMMON_CORE4S.loc[SHSAT.index]\nOTHER_SCHOOL_DATA = OTHER_SCHOOL_DATA.loc[SHSAT.index]\nSAFETY = SAFETY.loc[SHSAT.index]\nDISTANCE = DISTANCE.loc[SHSAT.index]\nmodel_input_variables = model_input_variables.loc[SHSAT.index]\n# Calculate shapley explanations to display later\nexplainer = shap.TreeExplainer(RF)\nshap_values = explainer.shap_values(model_input_variables)[:,:-1]\n# recombine all the variables so we can use them all at once if want to.\nall_variables = SCHOOL_NAMES.join(SHSAT, how=\"inner\").join(SCHOOL_DEMOGRAPHICS, how=\"inner\").join(COMMON_CORE4S, how=\"inner\").join(OTHER_SCHOOL_DATA, how=\"inner\").join(SAFETY, how=\"left\").join(DISTANCE, how=\"left\")\n\n# Plot expected vs actual\nsns.lmplot(x=\"PREDICTED\", y=target, data=SHSAT, fit_reg=True, markers='.', size = 6,\n           palette=\"coolwarm\", );\n#           scatter_kws={'s':OTHER_SCHOOL_DATA['APPROXIMATE_ENROLLMENT_PER_GRADE']}); ","80ebec94":"SCHOOL_NAMES.join(SHSAT, how='inner').head().style.background_gradient(cmap=cm)","e05cfa50":"row = SCHOOL_NAMES.index.get_loc(\"03M859\")\nprint(SCHOOL_NAMES.iloc[row])\nindex = model_input_variables.columns + \" (\" + model_input_variables.iloc[row].astype(str) + \")\"\npd.Series(index=index, data=shap_values[row]).sort_values(ascending=True).plot(kind='barh', figsize=(11,7));","c86ac39e":"row = SCHOOL_NAMES.index.get_loc(\"17K590\")\nprint(SCHOOL_NAMES.iloc[row])\nindex = model_input_variables.columns + \" (\" + model_input_variables.iloc[row].astype(str) + \")\"\npd.Series(index=index, data=shap_values[row]).sort_values(ascending=True).plot(kind='barh', figsize=(11,7));","08a8c35b":"SCHOOL_NAMES.join(SCHOOL_DEMOGRAPHICS).loc[[\"03M859\", \"17K590\"]].style.background_gradient(cmap=cm, axis=1)","a3ef7623":"# First define a function to conveniantly calculate attractiveness.\ndef calculate_ATTRACTIVENESS():\n    attract = ECON_NEED_WEIGHT * all_variables[\"ECONOMIC NEED INDEX\"] \n    attract = attract + ELL_WEIGHT * all_variables[\"PERCENT ELL\"]\n    attract = attract + ASIAN_WEIGHT * all_variables[\"PERCENT ASIAN\"]\n    attract = attract + BLACK_WEIGHT * all_variables[\"PERCENT BLACK\"]\n    attract = attract + WHITE_WEIGHT * all_variables[\"PERCENT WHITE\"]\n    attract = attract + HISPANIC_WEIGHT * all_variables[\"PERCENT HISPANIC\"]\n    attract = attract + NONFEEDER_WEIGHT*(all_variables[target].mean()-all_variables[target])\n    attract = attract + AAALN_4_WEIGHT * all_variables[\"%AAALN and scored 4\"]\n    attract = attract + BLACK_4_WEIGHT * all_variables[\"%BLACK and scored 4\"]\n    attract = attract + LATINO_4_WEIGHT * all_variables[\"%LATINO and scored 4\"]\n    attract = attract + ASIAN_4_WEIGHT * all_variables[\"%ASIAN and scored 4\"]\n    attract = attract + WHITE_4_WEIGHT * all_variables[\"%WHITE and scored 4\"]\n    attract = attract + MULTIRACIAL_4_WEIGHT * all_variables[\"%MULTIRACIAL and scored 4\"] \n    attract = attract + ECON_4_WEIGHT * all_variables[\"%ECON NEED and scored 4\"]\n    attract = attract + ELL_4_WEIGHT * all_variables[\"%ELL and scored 4\"]\n    attract = attract \/ (ECON_NEED_WEIGHT + ELL_WEIGHT + ASIAN_WEIGHT + BLACK_WEIGHT + WHITE_WEIGHT + HISPANIC_WEIGHT + NONFEEDER_WEIGHT  + AAALN_4_WEIGHT + BLACK_4_WEIGHT + LATINO_4_WEIGHT + ASIAN_4_WEIGHT + WHITE_4_WEIGHT + MULTIRACIAL_4_WEIGHT + ECON_4_WEIGHT + ELL_4_WEIGHT)\n    attract = attract.clip(lower=0.0)\n    return attract","8b9904e2":"# First, we can assign ATTRACTIVENESS weightings to overall school demographics\nECON_NEED_WEIGHT = 1.0 # How much to weight school's Economic Need index\nELL_WEIGHT = 0.5       # How much to weight school's ELL student percentage\nASIAN_WEIGHT = 0.0     # How much to weight school's Asian student percentage\nBLACK_WEIGHT = 1.0     # How much to weight school's Black student percentage\nWHITE_WEIGHT = 0.0     # How much to weight school's White student percentage\nHISPANIC_WEIGHT = 1.0  # How much to weight school's Hispanic student percentage\nNONFEEDER_WEIGHT = 1.0 # How much to weight % of students who do not receive SHSAT offers\n\n# We can put extra-empahsis on target groups who are already performing well on common core. \nAAALN_4_WEIGHT = 1.0       # How much to weight school's AAALN students with 4s percentage\nBLACK_4_WEIGHT = 2.0       # How much to weight school's Black students with 4s percentage\nLATINO_4_WEIGHT = 2.0      # How much to weight school's Latino students with 4s percentage\nASIAN_4_WEIGHT = 0.0       # How much to weight school's Asian students with 4s percentage\nWHITE_4_WEIGHT = 0.0       # How much to weight school's White students with 4s percentage\nMULTIRACIAL_4_WEIGHT = 1.0 # How much to weight school's Multiracial students with 4s percentage\nECON_4_WEIGHT = 2.0        # How much to weight school's Econ. disadvantaged students with 4s percentage\nELL_4_WEIGHT = 0.5         # How much to weight school's ESL students with 4s percentage\n# Now create the ATTRACTIVENESS score\nall_variables[\"ATTRACTIVENESS\"] = calculate_ATTRACTIVENESS()","f47f345a":"sns.lmplot(x=\"PREDICTED\", y=target, data=all_variables, \n           markers='.', size = 6, fit_reg=False,\n           hue=\"ATTRACTIVENESS\",\n           palette=\"coolwarm_r\",\n           legend=False,);","31ad5109":"# First, we can assign ATTRACTIVENESS weightings to overall school demographics\nECON_NEED_WEIGHT = 1.0 # How much to weight school's Economic Need index\nELL_WEIGHT = 0.0       # How much to weight school's ELL student percentage\nASIAN_WEIGHT = 0.0     # How much to weight school's Asian student percentage\nBLACK_WEIGHT = 0.0     # How much to weight school's Black student percentage\nWHITE_WEIGHT = 0.0     # How much to weight school's White student percentage\nHISPANIC_WEIGHT = 0.0  # How much to weight school's Hispanic student percentage\nNONFEEDER_WEIGHT = 0.0 # How much to weight percentage of students who do not receive SHSAT offeres\n\n# We can get extra-empahsis to target groups who are already performing well on common core. \nAAALN_4_WEIGHT = 0.0       # How much to weight school's AAALN students with 4s percentage\nBLACK_4_WEIGHT = 0.0       # How much to weight school's Black students with 4s percentage\nLATINO_4_WEIGHT = 0.0      # How much to weight school's Latino students with 4s percentage\nASIAN_4_WEIGHT = 0.0       # How much to weight school's Asian students with 4s percentage\nWHITE_4_WEIGHT = 0.0       # How much to weight school's White students with 4s percentage\nMULTIRACIAL_4_WEIGHT = 0.0 # How much to weight school's Multiracial students with 4s percentage\nECON_4_WEIGHT = 1.0        # How much to weight school's Economically disadvantaged students with 4s percentage\nELL_4_WEIGHT = 0.0         # How much to weight school's ESL students with 4s percentage\n\n# recalculate the ATTRACTIVENESS score\nall_variables[\"ATTRACTIVENESS\"] = calculate_ATTRACTIVENESS()\n\n# Plot the new ATTRACTIVENESS\nsns.lmplot(x=\"PREDICTED\", y=target, data=all_variables, \n           markers='.', size = 6, fit_reg=False,\n           hue=\"ATTRACTIVENESS\",\n           palette=\"coolwarm_r\",\n           legend=False,); ","a7a97343":"# First, we can assign ATTRACTIVENESS weightings to overall school demographics\nECON_NEED_WEIGHT = 0.0 # How much to weight school's Economic Need index\nELL_WEIGHT = 0.0       # How much to weight school's ELL student percentage\nASIAN_WEIGHT = 0.0     # How much to weight school's Asian student percentage\nBLACK_WEIGHT = 0.0     # How much to weight school's Black student percentage\nWHITE_WEIGHT = 0.0     # How much to weight school's White student percentage\nHISPANIC_WEIGHT = 0.0  # How much to weight school's Hispanic student percentage\nNONFEEDER_WEIGHT = 2.0 # How much to weight % of students who do not receive SHSAT offeres\n\n# We can get extra-empahsis to target groups who are already performing well on common core. \nAAALN_4_WEIGHT = 1.0       # How much to weight school's AAALN students with 4s percentage\nBLACK_4_WEIGHT = 1.0       # How much to weight school's Black students with 4s percentage\nLATINO_4_WEIGHT = 1.0      # How much to weight school's Latino students with 4s percentage\nASIAN_4_WEIGHT = 0.0       # How much to weight school's Asian students with 4s percentage\nWHITE_4_WEIGHT = 0.0       # How much to weight school's White students with 4s percentage\nMULTIRACIAL_4_WEIGHT = 0.0 # How much to weight school's Multiracial students with 4s percentage\nECON_4_WEIGHT = 1.0        # How much to weight school's Econ. disadvantaged students with 4s percentage\nELL_4_WEIGHT = 1.0         # How much to weight school's ESL students with 4s percentage\n\n# Now create the ATTRACTIVENESS score\nall_variables[\"ATTRACTIVENESS\"] = calculate_ATTRACTIVENESS()\n\n# Plot the new ATTRACTIVENESS\nsns.lmplot(x=\"PREDICTED\", y=target, data=all_variables, \n           markers='.', size = 6, fit_reg=False,\n           hue=\"ATTRACTIVENESS\",\n           palette=\"coolwarm_r\",\n           legend=False,);","115757d8":"recommended_schools = all_variables[[\"SCHOOL NAME\", \"ADDRESS (FULL)\",target,\"PREDICTED\", \"UNDERPERFORM\", \"ATTRACTIVENESS\"]+list(SCHOOL_DEMOGRAPHICS.columns)+list(COMMON_CORE4S.columns[1:] )]\nrecommended_schools[\"ATTRACTIVENESS_UNDERPERFORM_COMBINED\"] = recommended_schools[\"ATTRACTIVENESS\"].rank() + recommended_schools[\"UNDERPERFORM\"].rank()\nrecommended_schools[\"ATTRACTIVENESS\"] = recommended_schools[\"ATTRACTIVENESS\"].round(2)\nrecommended_schools = recommended_schools.sort_values([\"ATTRACTIVENESS_UNDERPERFORM_COMBINED\"], ascending=False)\ndel recommended_schools[\"ATTRACTIVENESS_UNDERPERFORM_COMBINED\"]\nrecommended_schools.to_csv(\"BenS_PASSNYC_Recommendations.csv\")\nrecommended_schools.head(30).style.background_gradient(cmap=cm)","96d6dd6c":"A Random Forest machine learning model comes with some metrics to understand how it worked.  As the model runs, it keeps track of which variables it used to explain the patterns between the inputs and output.  The total amount  each variable was used is called the \"Importance\" and is shown in the chart below.\n\nThe charts shows that Common Core variables dominate the relationship, with attendance variables coming next, and all other variables rest contributing a very small amount.  We can interpret this to mean that Common Core variables best explain why schools receive or do not receive offers per student.","acfa069a":"# Data Prep Results\nThe data is now cleaned and split into a half-dozen portions.","adaf6f82":"### School demographics\nAll scaled from 0 to 1. I'll use this data to calculate the \"attractiveness score\"","affc4e55":"## Expected vs. actual percentages of SHSAT offers for each school.\nNow we can compare the model's expected percentages with the actual percentages and identify underperforming schools.  A plot will help us visualize and understand our results.\n\nThe plot's X-axis is the expected percentage of offers while the Y-axis is the actual SHSAT percentages.  The schools below the line of best fit are underperforming and may be especially receptive to outreach.","8e625def":"### Changing the weights to emphasize a different target population\nYou can change my initial weights to anything you want.  For example, let's suppose someone at PASSNYC is especially focused on economically disadvantaged students who need help getting offers.  Simply set other weights to 0 and the economic need weights high, as I demonstrate here.  Now a different pattern emerges, as the bluer dots identify those schools that are more economically challenged.  There seem to be mix of both under and overperforming schools when attractiveness is defined in this way.","a5aac373":"### Building the machine learning model.\nClicking the black \"code\" button below will show how I built a relatively simple machine learning model known as a \"random forest.\"  For non-technical people, the key thing to understand is that a well-built machine learning model will learn the patterns that exist in the data between the input variables and the output variable (offers per student).\n\n**Technical Details** I follow the very standard approach of using cross-validation to find model parameters. Then fit and score using all of the data points - normally this would be a big no-no, due to overfitting, but with this simple of a model and an explanation objective (rather than a model accuracy objective), I believe we can get away with it.  The alternative would be carving our the small 580 row data set into an even smaller train and test set.","0ca968b5":"### And now, here is why the model expected MEDGAR EVERS COLLEGE PREP to receive SHSAT offers\nAgain, very good Common Core and expecially good attendance rates.","7987db61":"# Data Prep\nThe following couple sections of data prep can be skipped if you're here for high level results only.  The story picks up again in the section entitled *Data Prep Results*. ","f1dd73ac":"### And finally the rest of the school data\nWe will feed this as inputs to the machine learning model.\nSome notes on specific columns:\n* Added 7th grade enrollment from SOCRATA to see if school size had any impact.\n* Converted GRADE LOW of kindergarden to 0 and preschool to -1.\n* Converted \"x\" values for NEW, ADJUSTED GRADE and COMMUNITY SCHOOL to 1.","890f51c1":"### Distance from the Specialized Schools\nCalculated from the Latitude and Longitudes.  Used distance from the nearest specialized school and average distance from all 8 specialized schools. Presumably, if the specialized school was right down the street, you might be more likely to seek an offer.","d5a0ed83":"### Let's compare two of  the schools on this list: Special Music School vs. Medgar Evers College Prep\nThe model expected 26% of SPECIAL MUSIC SCHOOL's enrollment to receive an offer (although none did, a 26% underperformance!).  \n\nThis following chart shows how much each variable contributed to the model's 26% prediction.\n (You can see the raw data values in paranthesis next to each column's name)\n \n We can see that the Common Core scores were very good on average, with a school average of 3.67 ELA and 3.85 for Math.  The model liked those scores a lot, which is why they contributed so much.  Attendance was also very good.","6b12b2d2":"# What I Did\nWhile going through SHSAT resources, particuarly [the Pathways to an Elite Education report](https:\/\/steinhardt.nyu.edu\/research_alliance\/publications\/pathways_to_an_elite_education) and  [Data from the New School Center for NYC Affairs](http:\/\/www.centernyc.org\/high-school-diversity-data\/), I was struck by the numbers of minority and economically disadvantaged students who score highly on Common Core tests, yet do not apply to or receive offers to New York City's elite specialized high schools. Since a major part of PASSNYC's mission is to identify such \"diamonds in the rough,\" I decided to bring a data-oriented approach to this topic.\n\nMy hypothesis is that students with strong Common Core test scores will be easiest to convert into SHSAT test takers and to receive offers. Their existing academic ability means they'll be especially receptive to awareness outreach and convert to Elite High School offers with minimal mentoring or SHSAT-specific test prep from PASSNYC's partners.\n\nApart from the school explorer data, I also used the following data to build my solution:\n* *nyc-shsat-test-results-2017* to measure SHSAT offers\n* *ny-school-demographics-and-accountability-snapshot* from Socrata NYC Open Data  for enrollment information\n* *ny-2010-2016-school-safety-report* from Socrata NYC Open Data for crime statistics and to measure distances from the middle schools to the elite high schools.","9b23dd84":"# Thank You!","8aac62f6":"### School names and addresses","229fc78b":"### Finally, I define attractiveness as the schools where targeted populations from non-feeder schools are already scoring high on Common Core.\nThis is the population that I hypothesize would be most receptive to SHSAT awareness initatives, and most likely to convert to offers with minimal test-prep.","d5c4bf4b":"### School Safety Report of Crimes in each School","c7949309":"### School percentages of SHSAT offers per Student\nThis is the data that the machine-learning model will try to explain.","b2a8a695":"# Putting it All Together: Recommended Outreach Schools\nLet's put this all together give a final recommendation.  \n\nI use the attractiveness as just defined: non-feeder schools with underrepresented groups who score high on Common Core. They fit with my original hypothesis that awareness outreach and test services will be especially effective in those schools.\n\nDisplayed below are the top 30 recommended schools, sorted by the combined rank of attractiveness and underperformance. Scrolling to the right will show more data from each school.\n\n**_None_** of these top 30 recommended schools received a single offer during 2017 (they are not feeder schools), and demographically nearly all of them align with PSSNYC's mission very well.\n\nThe complete list of schools is saved in the file: BenS_PASSNYC_Recommendations.csv\n\n","d396e529":"### Common Core Test Results.\nEach value is the percentage of school's student body, within each subcategory, *that also* scored a 4.  ELA and MATH are simply averaged together and treated equally.\n\nFor example, imagine a school enrollment of 100, inclding 10 Latino students, and 3 of those Latino students scored a 4 on their Common Core (both MATH and ELA). In this example, * %LATINO and scored 4 * would be set to 0.03\n\nThis data will also be used to make an \"attractiveness score\".","a1a2b08e":"### SHSAT Test Results Data","fdcee55a":"# The Challenge\nPASSNYC aims to increase the diversity of students taking the Specialized High School Admissions Test (SHSAT).\nThey've asked the Kaggle communiy to help \"*identify schools where minority and underserved students stand to gain the most from services like after school programs, test preparation, mentoring, or resources for parents.*\"","28b20831":"\n\n### My solution has two steps:   \n\n**Part I** takes this forum post from Max B of PASSNYC as a starting point:\n\n*\"The hypothesis is that using what we know about students\/schools who do take the test, we can find similar students\/schools and rank them on their likelihood\/opportunity of converting into test-takers\"*\n\nI use machine learning to find the relationship between school characteristics and SHSAT offers per student. Comparing  actual SHSAT offers per student with the model's expectations gives a  over or underperformance, and schools that are doing worse than they should be are ripe for outreach.  \n\n**Part II** specifically addresses this portion of the PASSNYC Challenge problem statement: \n\n*\"The best solutions will enable PASSNYC to identify the schools where minority and underserved students stand to gain the most from services\"*\n\nI created an *attractiveness score* for each school in the data set, ** _that PASSNYC can adapt to match their goals_ ** based on each school's share of the groups in PASSNYC's mission (for example the economically disadvanted, minorities, or SHSAT nonfeeder schools). The score can be changed with user input from PASSNYC, depending on which groups they want to emphasize for a particulat type of outreach.\n\n** Putting it all together** by using over \/ underperform rankings in conjuction with the adaptable attractiveness score, schools can be quickly sorted and filtered by ** _both_ untapped academic potential _and_ alignment with PASSNYC's mission**.  \n","e084c865":"### How are these previous two schools alike? How are they different?\nThese two schools share 3 things in common: good Common Core scores, good attendance, but SHSAT specialized high school offers.  Consequantly, they lead in underperforming expectations.\n\nHowever, when we look at demographics, we see they are very different in ethnic composition and economic need.","4748220b":"Thanks to PASSNYC for making this possible and answering my qustions, everyone on the Kaggle forums who commented and gave feedback or inspired me through their work, and Kaggle for taking competitions in an exciting new direction.","fa936ae1":"## Let's dig into a few specific underperforming schools\n\nThe model can tell us exactly why it made any school's prediction.\n\nFor reference, here are the top 5 underperforming schools.","8732644e":"### Socrata NYC Demographics and Accountability Snapshot\n* Used to get total grade 7 enrollment to find out if school size impacts SHSATs.","bb4129b9":"## Re-plot Expected vs. Actual SHSAT percentages, adding ATTRACTIVENESS as the color.\nTo help understand what this looks like, I've replotted the expected vs actual amount of offers.  This time I also added a color to show the new attractiveness score.\n\nMore attractive (economically disadvantaged, underrepresented minorities and non-feeder schools) are in a bluer color.  Many of the attractive schools are also underperfomring . Putting those two together can build a recommendation for PASSNYC .","361114f9":"# Solution Part 2\n### School Attractiveness to PASSNYC's  Mission\nPASSNYC's mission is to increase diversity, serve the economically disadvantaged and counter the feeder school trend.  To that end, I've created parameters to weight these factors relative to one another as the organizers wish.\n\nI've set some initial weights as a starting point to demonstrate my approach, but I want to emphasize that my goal is to provide a *flexible and dynamic* way for PASSNYC to balance these objectives themselves. They know their own mission better than anyone, and their mission and goals are likely to evolve and expand over time. A good solution should be able to adapt to keep up with those changes.","32f58036":"The difference in demographics suggests several things:\n* Quite different outreach may be needed in these different school.  Economic levels are different, not to mention the fact that the students at Special Music School may be interested in a specialized Music high school, rather than SHSAT-entry schools\n* The second school may be a better fit to PASSNYC's stated mission, as the economic conditions are lower and underrepresented minorities much higher.  Which leads to Part 2 of my solution... ","9508577e":"### SCHOOL Explorer Data Prep","da35a25b":"### Socrata NY 2010-2016 School Safety Report\n* Used for crime statistics\n* Also used for the specialized high school Latitudes and Longitudes to calculate distances from them to the middle schools.","7090cc29":"## Setting weights for school attractivness score\nHere are some weights I've initially set with a mix of PASSNYC's target groups as an example. They can be changed to any number from 0 to as high as you wish.  The variables with the highest weights will have the most influence on the school *attractiveness score*.  These weights represent my best guess on how PASSNYC is balancing their mission objectives, with some special emphasis on populations who are already successful on Common Core.\n\n","5f502676":"# Results\nI find that Common Core results and attendance are the best ways to identify schools that should recieve many SHSAT high school offers. \n\nAnd I found dozens of schools with these positive characteristics yet did not receive a single offer to a specialized high school in 2017.  These 30 schools are generally made up of the underrepresented populations that PASSNYC wants to help.\n\nThe top 30 most promising schools are displayed in a table at the very bottom of this report, with the full detailed rankings of all 580 schools saved to a csv file.\n\nSurprisingly, neither crime statistics nor distance to the specialized schools had any impact on whether a school's students received offers.\n","a9caf89b":"# Solution Part 1\n# Which Schools Are Under \/ Overperforming Expectations?\nWith the data ready, we can build a model to relate school characteristics to percentages of students receiving offers from the specialized schools. \n\nMax B again:\n\n*\"The hypothesis is that using what we know about students\/schools who do take the test, we can find similar students\/schools and rank them on their likelihood\/opportunity of converting into test-takers*\"\n\nThe machine learning model below will do exactly that.  It will find the relationships in the data between school characteristics and SHSAT offers.  With that relationship in hand, we can compare school's expected results to actual results, and rank or identify underperforming schools."}}