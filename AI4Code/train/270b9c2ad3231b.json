{"cell_type":{"244eaaa2":"code","3ba872d5":"code","ef07ccf6":"code","784021ca":"code","9bf7c78c":"code","cf85554f":"code","0e03c60c":"code","af9b2aad":"code","35ef574b":"code","4bb5e344":"code","7ea9ce13":"code","5fe7a6af":"code","5429c69f":"code","6214a949":"code","7837b429":"code","b105145c":"markdown","9b795f7d":"markdown","6badf45f":"markdown","132d7c50":"markdown","6cce4977":"markdown","e3ca83f8":"markdown","96d8794a":"markdown"},"source":{"244eaaa2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport os, gc, random\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 140) #\u6700\u5927\u8868\u793a\u5217\u6570\u306e\u6307\u5b9a\n\n\n# fix seed\nseed = 2021\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)","3ba872d5":"from torch.utils.data import Dataset\nfrom torch import nn\n \nclass JSMP_Dataset(Dataset):\n     \n    def __init__(self, file_path, window_size):\n        # valiables\n        self.file_path = file_path\n        self.window_size = window_size\n        \n        # read csv\n        train = pd.read_csv(file_path)\n        \n        # pre processing\n        train = train.query('date > 85').reset_index(drop = True) \n        train.fillna(train.mean(),inplace=True)\n        train['action'] = ((train['resp'].values) > 0).astype(int)\n        \n        resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n        self.features = [c for c in train.columns if \"feature\" in c]\n        self.f_mean = np.mean(train[self.features[1:]].values,axis=0)\n        \n        self.X_train = train.loc[:, train.columns.str.contains('feature')].values\n        self.y_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n        \n        \n        self.X_train = torch.from_numpy(self.X_train).float()\n        self.y_train = torch.from_numpy(self.y_train).float()\n        \n        self.X_weight = torch.from_numpy(train.weight.values)\n        \n        # reduce memory\n        del train\n        gc.collect()\n \n    def __len__(self):\n        return len(self.X_train) - self.window_size\n     \n    def __getitem__(self, i):\n        data = self.X_train[i:(i+ self.window_size), :] \n        label = self.y_train[i + self.window_size - 1]\n        weight = self.X_weight[i + self.window_size - 1]\n \n        return weight, data, label","ef07ccf6":"window_size = 5\nfile_path = '\/kaggle\/input\/jane-street-market-prediction\/train.csv'\nds = JSMP_Dataset(file_path, window_size)","784021ca":"from torch.utils.data.dataset import Subset\nn_samples = len(ds)\ntrain_size = int(n_samples * 0.8)\n\ntrain_ds = Subset(ds, list(range(0, train_size)))\nvalid_ds = Subset(ds, list(range(train_size, n_samples)))\n\nprint('train size:',len(train_ds))\nprint('valid size:',len(valid_ds))\n","9bf7c78c":"batch_size = 4096\n\n# make DataLoder\ntrain_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n\n# dict\ndataloaders_dict = {'train': train_dataloader,\n                    'val'  : valid_dataloader}","cf85554f":"# Check\nbatch_iterator = iter(dataloaders_dict['train'])\nweight, inputs, labels = next(batch_iterator)\nprint(weight.size(), inputs.size(), labels)","0e03c60c":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\n\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)","af9b2aad":"class TCN(nn.Module):\n    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n        super(TCN, self).__init__()\n        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n        \n        self.fc1 = nn.Linear(130 * num_channels[-1], 128)\n        self.dropout1 = nn.Dropout(dropout)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n        self.LeakyReLU1 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        \n        self.fc2 = nn.Linear(128, 128)\n        self.dropout2 = nn.Dropout(dropout)\n        self.batch_norm2 = nn.BatchNorm1d(128)\n        self.LeakyReLU2 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        \n        self.fc3 = nn.Linear(128, output_size)\n        \n    def forward(self, inputs):\n        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n        y1 = torch.flatten(y1, start_dim=1)\n        \n        y1 = self.fc1(y1)\n        y1 = self.batch_norm1(y1)\n        y1 = self.LeakyReLU1(y1)\n        y1 = self.dropout1(y1)\n        \n        y1 = self.fc2(y1)\n        y1 = self.batch_norm2(y1)\n        y1 = self.LeakyReLU2(y1)\n        y1 = self.dropout2(y1)\n        \n        o = self.fc3(y1)\n        return torch.sigmoid(o)","35ef574b":"net = TCN(input_size=5, output_size=5, num_channels=[16, 8, 4, 2], kernel_size=2, dropout=0.5)\nprint(net)","4bb5e344":"# Check\nx = torch.randn(batch_size, window_size, 130)\nprint(x.shape)\n\no = net(x)\nprint(o.shape)","7ea9ce13":"import torch.optim as optim\n\ncriterion = nn.BCELoss() # Binary Cross Entropy\noptimizer = optim.Adam(net.parameters(), lr=0.001)","5fe7a6af":"from tqdm import tqdm\nfrom pytorch_lightning.metrics import Accuracy\n\n\ndef train_model(net, dataloader_dict, criterion, optimizer, num_epochs):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print('use devise:', device)\n    \n    net.to(device)\n    accuracy = Accuracy(compute_on_step=False).to(device)\n    #torch.backends.cudnn.deterministic = True\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('--------------------------')\n        \n        for phase in ['train', 'val']:\n\n            epoch_loss = 0.0\n        \n            for _, inputs, labels in tqdm(dataloader_dict[phase]):\n                \n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # init optimizer:\u52fe\u914d\u30d1\u30e9\u30e1\u30fc\u30bf\u30920\u306b\u3059\u308b\n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n\n                    outputs = net(inputs)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                    \n                    # Calculate Score\n                    y_hat = (torch.median(outputs, axis=1).values > 0.5).long()\n                    y = torch.median(labels, axis=1).values.long()\n                    accuracy(y_hat, y)\n                    \n                    epoch_loss += loss.item() * inputs.size(0)\n                \n            # print Score\n            epoch_accuracy = accuracy.compute()\n            print('{} Loss: {:.4f} Acc:{:.4f}'.format(phase, epoch_loss, epoch_accuracy))\n            \n        # save model\n        if phase == 'val':\n            if epoch == 0:\n                best_val_accuracy = epoch_accuracy\n                best_val_loss = epoch_loss\n                save_accuracy = True\n                save_loss = True\n            elif best_val_accuracy < epoch_accuracy:\n                best_val_accuracy = epoch_accuracy\n                save_accuracy = True\n            elif best_val_loss > epoch_loss:\n                best_val_loss = epoch_loss\n                save_loss = True\n            \n            if save_accuracy:\n                print('Best accuracy score updated. New model was saved.')\n                torch.save(net.state_dict(), '.\/best_accuracy_model.mdl')\n                save_accuracy = False\n            \n            if save_loss:\n                print('Best loss score updated. New model was saved.')\n                torch.save(net.state_dict(), '.\/best_loss_model.mdl')\n                save_loss = False","5429c69f":"num_epochs = 50\ntrain_model(net, dataloaders_dict, criterion, optimizer, num_epochs)","6214a949":"# load model\nnet1 = TCN(input_size=5, output_size=5, num_channels=[16, 8, 4, 2], kernel_size=2, dropout=0.5)\nnet2 = TCN(input_size=5, output_size=5, num_channels=[16, 8, 4, 2], kernel_size=2, dropout=0.5)\n\nnet1.load_state_dict(torch.load('.\/best_accuracy_model.mdl'))\nnet2.load_state_dict(torch.load('.\/best_loss_model.mdl'))\n\nnet1.eval()\nnet2.eval()","7837b429":"th = 0.5\n\nimport janestreet\nenv = janestreet.make_env()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('use devise:', device)\n\nfor i, (test_df, pred_df) in enumerate(env.iter_test()):\n    x_tt = test_df.loc[:, ds.features].values\n    if np.isnan(x_tt[:, 1:].sum()):\n        x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * ds.f_mean\n    \n    # make window data\n    if i == 0:\n        x_window = x_tt.copy()\n    elif i < window_size: \n        x_window = np.concatenate([x_window, x_tt], axis=0)\n    else:\n        x_window = np.concatenate([x_window[1:, :], x_tt], axis=0)\n    \n    if i < window_size - 1:\n        # pass \n        pred_df.action = 0\n    else:\n        # prediction\n        if test_df['weight'].item() > 0:\n            inputs = torch.Tensor(x_window).unsqueeze(0).to(device)\n            outputs = (net1(inputs) + net2(inputs)) \/ 2\n            pred = (torch.median(outputs, axis=1).values > th).long()\n            pred_df.action = pred.item()\n            #print(pred.item())\n        else:\n            pred_df.action = 0\n        \n    env.predict(pred_df)","b105145c":"## Inference","9b795f7d":"## Network(TCN)\nhttps:\/\/github.com\/locuslab\/TCN\/blob\/master\/TCN\/tcn.py","6badf45f":"## Loss Function\/Optim","132d7c50":"## Dataset Class","6cce4977":"## Train","e3ca83f8":"## Train\/Validation Dataset","96d8794a":"## DataLoader"}}