{"cell_type":{"7dee554e":"code","e91d844b":"code","0b0a17d4":"code","e61cd572":"code","72a1dc79":"code","451540af":"code","ea905283":"code","d1b5c6dc":"code","fdbd53e3":"code","e12ab074":"code","d6bdc3bc":"code","255f5c85":"code","b436f420":"code","bfff9dae":"code","ebeb12b4":"code","e7be1a94":"code","0f6d508c":"code","b6ac900d":"code","d632f259":"code","772326d3":"code","44a2d6d2":"code","f926d020":"code","b7f90f79":"code","acaa01f0":"code","446f5130":"code","866035c5":"code","fc0bb059":"code","74884ad2":"code","9595785a":"code","81296ef3":"code","0e78e090":"code","55cffd21":"code","05bd05ae":"code","fd727ea0":"code","7286dee2":"code","85c89ff4":"code","550f985e":"code","590ba568":"code","2ed83d90":"code","30da9ef4":"code","ccc34e03":"code","ec06d074":"code","34c816f5":"code","3145bd01":"markdown","ae928481":"markdown","ca8be230":"markdown","e3ebe2b9":"markdown","1292d9c4":"markdown","848261de":"markdown","018094c8":"markdown"},"source":{"7dee554e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e91d844b":"df=pd.read_csv('..\/input\/phishing-website-dataset\/dataset.csv')","0b0a17d4":"df.head()","e61cd572":"df['Result'].unique()","72a1dc79":"#1-> Legitimate\n#0-> Phishing","451540af":"#df.dtypes","ea905283":"df.shape","d1b5c6dc":"import seaborn as sns\nsns.countplot(df['Result'])","fdbd53e3":"#Unique values for each columns\ncol=df.columns\nfor i in col:\n     if  i!='index':\n        print(i,df[i].unique())","e12ab074":"\n# col=df.columns\n# for column in col:\n#     df[column] = df[column].replace(-1,0)\n    ","d6bdc3bc":"#Replacing -1 with 0 \ndf['Result']=df['Result'].replace(-1,0)","255f5c85":"df.shape","b436f420":"# Print correlation with target variable\nprint(df.corr()['Result'].sort_values())      ","bfff9dae":"plt.figure(figsize=(15, 15))\nsns.heatmap(df.corr(), linewidths=.5)","ebeb12b4":"# df=df[['having_IPhaving_IP_Address',\n# 'URLURL_Length',\n# 'Shortining_Service',\n# 'having_At_Symbol',\n# 'double_slash_redirecting',\n# 'Prefix_Suffix',\n# 'Domain_registeration_length',\n# 'Favicon',\n# 'port',\n# 'HTTPS_token',\n# 'Request_URL',\n# 'Submitting_to_email',\n# 'Abnormal_URL',\n# 'Redirect',\n# 'on_mouseover',\n# 'RightClick',\n# 'popUpWidnow',\n# 'Iframe',\n# 'age_of_domain',\n# 'DNSRecord',\n# 'web_traffic',\n# 'Page_Rank',\n# 'Google_Index',\n# 'Statistical_report',\n# 'Result']]","e7be1a94":"# from sklearn.feature_selection import chi2\n\n# chi_scores = chi2(X,Y)\n# chi_scores","0f6d508c":"# p_values = pd.Series(chi_scores[1],index = X.columns)\n# p_values.sort_values(ascending = False , inplace = True)","b6ac900d":"# p_values.plot.bar()\n","d632f259":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\nX= df.drop(columns='Result')\nY=df['Result']\n\ntrain_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=2)","772326d3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlr=LogisticRegression()\nlr.fit(train_X,train_Y)\npred=lr.predict(test_X)\naccuracy_score(pred,test_Y)","44a2d6d2":"knn=KNeighborsClassifier(n_neighbors=3)\nmodel= knn.fit(train_X,train_Y)\n\nknn_predict=model.predict(test_X)","f926d020":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\naccuracy_score(knn_predict,test_Y)","b7f90f79":"neighbors = np.arange(1, 15)\n\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors= k )\n\n    # Fit the classifier to the training data\n    knn.fit(train_X, train_Y)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(train_X, train_Y)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(test_X, test_Y)\n\n# Generate plot\nplt.figure(figsize=(10,5))\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","acaa01f0":"from sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(train_X, train_Y)\nnaive_predict=clf.predict(test_X)\naccuracy_score(naive_predict,test_Y)","446f5130":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(max_depth=10, random_state=0)\nforest_clf.fit(train_X,train_Y)\nran_pred=forest_clf.predict(test_X)\naccuracy_score(ran_pred,test_Y)","866035c5":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state = 42)\nfrom pprint import pprint\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","fc0bb059":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)\n{'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}","74884ad2":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X,train_Y)","9595785a":"rf_random.best_params_\n","81296ef3":"forest_clf = RandomForestClassifier(bootstrap=False,max_depth=20,max_features='auto',min_samples_leaf=1,min_samples_split=2,n_estimators=1800)\nforest_clf.fit(train_X,train_Y)\nran_pred=forest_clf.predict(test_X)\naccuracy_score(ran_pred,test_Y)","0e78e090":"from sklearn.model_selection import GridSearchCV \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngrid.fit(train_X,train_Y) ","55cffd21":"grid.best_params_","05bd05ae":"from sklearn.svm import SVC\nsvm_clf =SVC(C=100, gamma=0.0001, kernel='rbf')\nsvm_clf.fit(train_X,train_Y)\npred=svm_clf.predict(test_X)\naccuracy_score(pred,test_Y)","fd727ea0":"# from keras.models import Sequential\n# from keras.layers import Dense\n\n# classifier = Sequential() # Initialising the ANN\n\n# classifier.add(Dense(units = 16, activation = 'relu', input_dim = 31))\n# classifier.add(Dense(units = 8, activation = 'relu'))\n# classifier.add(Dense(units = 6, activation = 'relu'))\n# classifier.add(Dense(units = 1, activation = 'sigmoid'))","7286dee2":"# classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')","85c89ff4":"# classifier.fit(train_X,train_Y, batch_size = 1, epochs = 100)","550f985e":"# Y_pred = classifier.predict(test_X)\n# Y_pred = [ 1 if y>=0.5 else 0 for y in Y_pred ]","590ba568":"# accuracy_score(Y_pred,test_Y)","2ed83d90":"# #Dependencies\n# import keras\n# from keras.models import Sequential\n# from keras.layers import Dense\n# # Neural network\n# model = Sequential()\n# model.add(Dense(16, input_dim=21, activation='relu'))\n# model.add(Dense(12, activation='relu'))\n# model.add(Dense(1, activation='softmax'))","30da9ef4":"# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","ccc34e03":"# history = model.fit(train_X,train_Y,batch_size=1,epochs=100)","ec06d074":"# y_pred = model.predict(test_X)","34c816f5":"# accuracy_score(y_pred,test_Y)","3145bd01":"## 1)Logistic Regression","ae928481":"## 3)Naive Bayes","ca8be230":"### Spliting into training and testing","e3ebe2b9":"## 5)SVM","1292d9c4":"## 2)KNeighbors Classfier","848261de":"# Model Building","018094c8":"## 4)Random Forest\n"}}