{"cell_type":{"df616f51":"code","9dea13f4":"code","137f3a1e":"code","5eb698f1":"code","c44f4c3b":"code","49747d2c":"code","c2d072d6":"code","6c439899":"code","d15f7528":"code","24354732":"code","a0208472":"code","7c69344b":"code","60a2af9a":"code","c31753e8":"code","080f465f":"code","2128bfed":"code","1b096f9d":"code","7a987e6f":"code","b7d38ec8":"code","bfd3ef9b":"code","c1e50900":"markdown","ee7a21c1":"markdown","74b0a7ad":"markdown","b8373c34":"markdown","569a9f6c":"markdown","e537cd33":"markdown","d1896ac2":"markdown","6781f524":"markdown","57fa2fc2":"markdown","1033baec":"markdown","b472e5de":"markdown"},"source":{"df616f51":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nfrom pathlib import Path\n","9dea13f4":"class Config:\n    debug = False\n    competition = \"PetFinder\"\n    seed = 42\n    NFOLDS = 5\n    EPOCHS = 10\n    ","137f3a1e":"data_dir = Path('..\/input\/tabular-playground-series-oct-2021') # Change me every month","5eb698f1":"train_df = pd.read_csv(data_dir \/ \"train.csv\",\n#                       nrows=100000\n                      )\n\ntest_df = pd.read_csv(data_dir \/ \"test.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","c44f4c3b":"train_df.info()","49747d2c":"train_df.head()","c2d072d6":"# this function will help to reduce momory \n# data will be smaller with the same value\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","6c439899":"train_df = reduce_mem_usage(train_df)\ntest_df  = reduce_mem_usage(test_df)","d15f7528":"y = train_df.target\n\ntest_df = test_df.drop([\"id\"], axis=1)\nX = train_df.drop([\"id\", \"target\"], axis=1)\nX.head()","24354732":"# Train\/Validation Split\n# x_train, x_valid, y_train, y_valid = train_test_split(X, y,\n#                                                     test_size=0.2,\n#                                                     stratify=None,\n#                                                     random_state=42)\n","a0208472":"features = [col for col in train_df.columns if col not in ('id', 'target')]","7c69344b":"# TODO: Find good defaults\n\nlgbm_params = {\n    'objective': 'binary',\n    'num_leaves': 20,\n    'max_depth': 7,\n    'n_estimators': 200,\n    'metric': 'AUC',\n    'learning_rate': 0.05\n}","60a2af9a":"lgbm_params = {\n    'objective': 'binary', \n    'device_type': 'gpu', \n    'n_estimators': 20000, \n    'learning_rate':  0.01, \n    'min_child_weight': 256,\n    'min_child_samples': 20, \n    'reg_alpha': 10, \n    'reg_lambda': 0.1, \n    'subsample': 0.6, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.4,\n}","c31753e8":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nkf = StratifiedKFold(n_splits=Config.NFOLDS, shuffle=True, random_state=Config.seed)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n#     print(type(train_idx))\n    print(10*\"=\", f\"Fold={fold}\", 10*\"=\")\n\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    \n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    model = lgb.LGBMClassifier(**lgbm_params)\n    model.fit(x_train, y_train,\n                eval_set=[(x_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=100,\n              verbose=0)\n    \n    preds_valid = model.predict_proba(x_valid)[:, -1]\n    # Want probability or classification?\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n\n    auc = roc_auc_score(y_valid, preds_valid)\n    print('auc: ', auc)\n    scores.append(auc)\n    \n    test_preds = model.predict_proba(test_df[features])[:, -1]\n    final_test_predictions.append(test_preds)\n","080f465f":"print(f\"scores -> mean: {np.mean(scores)}, std: {np.std(scores)}\")","2128bfed":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)","1b096f9d":"# predictions = model.predict_proba(test_df)","7a987e6f":"# predictions = predictions[:,1] # Forgot this!","b7d38ec8":"df = pd.DataFrame(np.column_stack(final_test_predictions))\ndf['mean'] = df.mean(axis=1)\ndf","bfd3ef9b":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_2.csv\",index=None)\nsample_submission.to_csv(\"basic_lgbm_cv.csv\",index=None)\nsample_submission","c1e50900":"# Configuration","ee7a21c1":"# Submission","74b0a7ad":"# Save OOF Predictions\n\nSave the dictionary that we created for all the training predictions that were made when each fold was used for validation","b8373c34":"# Load Libraries","569a9f6c":"# Load Data","e537cd33":"# TPS Oct 2021\n\nA \"Getting Started\" LGBM notebook.\n\nSave train_pred_2.csv and test_pred_2.csv for blending.\n\n## Others\n\n- [TPS Oct 2021 - Basic XGBoost CV](https:\/\/www.kaggle.com\/mmellinger66\/tps-oct-2021-basic-xgboost-cv) - train_pred_1.csv \/ test_pred_1.csv for blending.\n\n## Versions\n\n- V9: Try StratifiedKFold\n- V8: Started added Blending code\n- V7: Different parameters\n- V6: KFold\n- V5: ...\n\n# References\n\n- [LGBM-OneModel-ThreeSeeds-Blend](https:\/\/www.kaggle.com\/mlanhenke\/tps-10-lgbm-onemodel-threeseeds-blend)\n- [LightGBM Classifier in Python](https:\/\/www.kaggle.com\/prashant111\/lightgbm-classifier-in-python)\n- [Simple LightGBM Model](https:\/\/www.kaggle.com\/gorosia\/simple-lightgbm-model)\n- [competition part-5: blending 101](https:\/\/www.kaggle.com\/abhishek\/competition-part-5-blending-101)\n","d1896ac2":"# Feature Engineering","6781f524":"# Model","57fa2fc2":"# Extract Target","1033baec":"# Reduce Memory\n\nToo many memory issues.  Got a function to reduce the float and int types by checking the max column value and setting column to minimum necessary type.\n\n- https:\/\/www.kaggle.com\/hrshuvo\/tps-oct-21-xgb-kfold\n- https:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage","b472e5de":"# Predict"}}