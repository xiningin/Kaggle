{"cell_type":{"73bdecc1":"code","84e29457":"code","f3674bb0":"code","2f922be8":"code","f5d8b9a4":"code","61462814":"code","3f753d07":"code","e5ec638f":"code","7954d3cc":"code","47dcd932":"code","db913372":"code","08a89a02":"code","b7a637c1":"markdown","283778b7":"markdown","0dca39cc":"markdown","eabd39e1":"markdown","9ea3c974":"markdown","d071f6e9":"markdown"},"source":{"73bdecc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84e29457":"!pip install mlrose\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\nimport mlrose\nimport time","f3674bb0":"fitness = mlrose.FourPeaks()","2f922be8":"%%time\nmid_size_problem_sa = 600\n\nproblem = mlrose.DiscreteOpt(length = mid_size_problem_sa , fitness_fn = fitness, maximize = True)\n\nprint(\"ExpDecay\")\n\nT = mlrose.ExpDecay() # init_temp=1.0, exp_const=0.005, min_temp=0.001)\n\nstart_time = time.time()\nbest_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\ntotal_time=time.time() - start_time\nprint(best_fitness, total_time)\n\n# T = mlrose.ExpDecay(init_temp=1, exp_const=0.0005, min_temp=0.0001)\n\n# start_time = time.time()\n# best_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\n# total_time=time.time() - start_time\n# print(best_fitness, total_time)\n\n# T = mlrose.ExpDecay(init_temp=1, exp_const=0.00005, min_temp=0.0001)\n\n# start_time = time.time()\n# best_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\n# total_time=time.time() - start_time\n# print(best_fitness, total_time)\n\n# T = mlrose.ExpDecay(init_temp=1, exp_const=0.000005, min_temp=0.0001)\n\n# start_time = time.time()\n# best_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\n# total_time=time.time() - start_time\n# print(best_fitness, total_time)\n\nprint(\"ArithDecay\")\n\nT = mlrose.ArithDecay()\n\nstart_time = time.time()\nbest_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\ntotal_time=time.time() - start_time\nprint(best_fitness, total_time)\n\n# T = mlrose.ArithDecay(init_temp=10, decay=0.95, min_temp=.1)\n\n# start_time = time.time()\n# best_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\n# total_time=time.time() - start_time\n# print(best_fitness, total_time)\n\n# T = mlrose.ArithDecay(init_temp=10, decay=0.095, min_temp=.1)\n\n# start_time = time.time()\n# best_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\n# total_time=time.time() - start_time\n# print(best_fitness, total_time)\n\nprint(\"GeomDecay\")\n\nT = mlrose.GeomDecay()\n\nstart_time = time.time()\nbest_state, best_fitness = mlrose.simulated_annealing(problem, random_state = 1,schedule = T)\ntotal_time=time.time() - start_time\nprint(best_fitness, total_time)","f5d8b9a4":"%%time\nmid_size_problem_rhc = 600\n\nproblem = mlrose.DiscreteOpt(length = mid_size_problem_rhc , fitness_fn = fitness, maximize = True)\n\n\nfor attempts in range(0,30,2):\n    start_time = time.time()\n    best_state, best_fitness = mlrose.random_hill_climb(problem, restarts=0, max_attempts=attempts, curve=False, random_state=1)\n    total_time=time.time() - start_time\n    print(attempts, best_fitness, total_time)","61462814":"%%time\n# Solve problem using the genetic algorithm\n\n\nmid_size_problem_ga = 600\n\nproblem = mlrose.DiscreteOpt(length = mid_size_problem_ga , fitness_fn = fitness, maximize = True)\n\n\n\nfor popsize in range(100,2000,200):\n    start_time = time.time()\n    best_state, best_fitness = mlrose.genetic_alg(problem, random_state = 1, mutation_prob = 0.4, max_attempts = 300, pop_size=popsize)\n    total_time=time.time() - start_time\n    print(popsize, best_fitness, total_time)","3f753d07":"%%time\n# Solve problem using the genetic algorithm\nmid_size_problem_mimic = 600\n\nproblem = mlrose.DiscreteOpt(length = mid_size_problem_mimic , fitness_fn = fitness, maximize = True)\n\nfor popsize in range(100,2000,200):\n    start_time = time.time()\n    best_state, best_fitness = mlrose.mimic(problem, curve=False, pop_size=popsize, keep_pct=0.2, max_attempts=30, random_state=1, fast_mimic=True)\n    total_time=time.time() - start_time\n    print(popsize, best_fitness, total_time)","e5ec638f":"%%time\n# Solve problem using the genetic algorithm\nmid_size_problem_mimic = 600\n\nproblem = mlrose.DiscreteOpt(length = mid_size_problem_mimic , fitness_fn = fitness, maximize = True)\n\n_ , _, sa_curve = mlrose.simulated_annealing(problem, curve=True, random_state = 1,schedule = mlrose.GeomDecay())\n\n_ , _, mimic_curve = mlrose.mimic(problem, curve=True, pop_size=700, keep_pct=0.2, max_attempts=30, random_state=1, fast_mimic=True) # 1100 was initially chosen im halfing it as its crashing the kernel\n\n_ , _, ga_curve = mlrose.genetic_alg(problem, curve=True, random_state = 1, mutation_prob = 0.4, max_attempts = 300, pop_size=300)\n\n_ , _, rhc_curve = mlrose.random_hill_climb(problem, curve=True, restarts=0, max_attempts=50, random_state=1)\n\n\n","7954d3cc":"%%time\nplot1_data={}\nplot1_data['sa']={}\nplot1_data['rhc']={}\nplot1_data['ga']={}\nplot1_data['mimic']={}\nplot1_data['saT']={}\nplot1_data['rhcT']={}\nplot1_data['gaT']={}\nplot1_data['mimicT']={}\n\n# for iter in range(100,1000,100):\n    \nproblem_size = range(100,1100,100)\n# print(problem_size)\nfor ps in problem_size:\n\n    fitness = mlrose.OneMax()\n    problem = mlrose.DiscreteOpt(length = ps , fitness_fn = fitness, maximize = True)\n    \n    start_time = time.time()\n    best_state, best_fitness = mlrose.simulated_annealing(problem, curve=False, random_state = 1,schedule = mlrose.GeomDecay())\n    total_time=time.time() - start_time\n    plot1_data['sa'][ps]=best_fitness\n    plot1_data['saT'][ps]=total_time\n\n    start_time = time.time()\n    best_state, best_fitness = mlrose.mimic(problem, curve=False, pop_size=700, keep_pct=0.2, max_attempts=30, random_state=1, fast_mimic=True) # 1100 was initially chosen im halfing it as its crashing the kernel\n    total_time=time.time() - start_time\n    plot1_data['mimic'][ps]=best_fitness\n    plot1_data['mimicT'][ps]=total_time\n\n    start_time = time.time()\n    best_state, best_fitness = mlrose.genetic_alg(problem, curve=False, random_state = 1, mutation_prob = 0.4, max_attempts = 300, pop_size=300)\n    total_time=time.time() - start_time\n    plot1_data['ga'][ps]=best_fitness\n    plot1_data['gaT'][ps]=total_time\n    \n    start_time = time.time()\n    best_state, best_fitness = mlrose.random_hill_climb(problem, curve=False, restarts=0, max_attempts=50, random_state=1)\n    total_time=time.time() - start_time\n    plot1_data['rhc'][ps]=best_fitness\n    plot1_data['rhcT'][ps]=total_time\n    \n    print(\"PS: \"+ str(ps) +\" done\")","47dcd932":"import matplotlib.pyplot as plt\nplt.title(\"FourPeaks fitness vs iteration for problem size 600\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Fitness\")\n# plt.ylim(0.0, 1.1)\nplt.plot(range(1,len(rhc_curve)+1), rhc_curve, label=\"RHC\", color=\"r\")\nplt.plot(range(1,len(sa_curve)+1), sa_curve, label=\"SA\", color=\"g\")\nplt.plot(range(1,len(ga_curve)+1), ga_curve, label=\"GA\", color=\"b\")\nplt.plot(range(1,len(mimic_curve)+1), mimic_curve, label=\"MIMIC\", color=\"m\")\nplt.legend(loc=\"best\")\nplt.show()","db913372":"y_sa = [plot1_data['sa'][key] for key in sorted(plot1_data['sa'].keys(), reverse=False)]\ny_ga = [plot1_data['ga'][key] for key in sorted(plot1_data['ga'].keys(), reverse=False)]\ny_rhc = [plot1_data['rhc'][key] for key in sorted(plot1_data['rhc'].keys(), reverse=False)]\ny_mimic = [plot1_data['mimic'][key] for key in sorted(plot1_data['mimic'].keys(), reverse=False)]\nplt.title(\"FourPeaks fitness vs problem size\")\nplt.xlabel(\"problem size\")\nplt.ylabel(\"Fitness\")\nplt.plot(range(100,1100,100), y_sa, label=\"RHC\", color=\"r\")\nplt.plot(range(100,1100,100), y_rhc, label=\"SA\", color=\"g\")\nplt.plot(range(100,1100,100), y_ga, label=\"GA\", color=\"b\")\nplt.plot(range(100,1100,100), y_mimic, label=\"MIMIC\", color=\"m\")\nplt.legend(loc=\"best\")\nplt.show()","08a89a02":"y_sa = [plot1_data['saT'][key] for key in sorted(plot1_data['saT'].keys(), reverse=False)]\ny_ga = [plot1_data['gaT'][key] for key in sorted(plot1_data['gaT'].keys(), reverse=False)]\ny_rhc = [plot1_data['rhcT'][key] for key in sorted(plot1_data['rhcT'].keys(), reverse=False)]\ny_mimic = [plot1_data['mimicT'][key] for key in sorted(plot1_data['mimicT'].keys(), reverse=False)]\nplt.title(\"FourPeaks Time vs problem size\")\nplt.xlabel(\"problem size\")\nplt.ylabel(\"Time\")\nplt.plot(range(100,1100,100), y_sa, label=\"RHC\", color=\"r\")\nplt.plot(range(100,1100,100), y_rhc, label=\"SA\", color=\"g\")\nplt.plot(range(100,1100,100), y_ga, label=\"GA\", color=\"b\")\nplt.plot(range(100,1100,100), y_mimic, label=\"MIMIC\", color=\"m\")\nplt.legend(loc=\"best\")\nplt.show()","b7a637c1":"# Generating plot fitness vs iteration","283778b7":"1100 seems fine.","0dca39cc":"GeomDecay seems ok","eabd39e1":"# Generating plot fitness vs problem size","9ea3c974":"not giving good performance but i will take 30","d071f6e9":"300 seems fine"}}