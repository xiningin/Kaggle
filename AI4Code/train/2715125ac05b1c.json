{"cell_type":{"06e0bd8e":"code","66365327":"code","a35a1d98":"code","3604ef40":"code","3a709856":"code","9a5ee732":"code","de2623a3":"code","fea9d277":"code","2f893c79":"code","490882ff":"markdown","23e08bed":"markdown","c9232a46":"markdown","ad42105e":"markdown","5391ddf7":"markdown","e5869f74":"markdown","4abc833e":"markdown"},"source":{"06e0bd8e":"# All the packages that we need to import\nimport numpy as np               # for linear algebra\nimport pandas as pd              # for tabular output\nfrom scipy.stats import rankdata # for ranking the candidates","66365327":"# The given data encoded into vectors and matrices\n\nattributes = np.array([\"GRE\", \"GPA\", \"College ranking\", \"Recommendation Rating\", \"Interview Rating\"])\ncandidates = np.array([\"Alfred\", \"Beverly\", \"Calvin\", \"Diane\", \"Edward\", \"Fran\"])\nraw_data = np.array([\n    [690, 3.1,  9,  7,  4],\n    [590, 3.9,  7,  6, 10],\n    [600, 3.6,  8,  8,  7],\n    [620, 3.8,  7, 10,  6],\n    [700, 2.8, 10,  4,  6],\n    [650, 4.0,  6,  9,  8],\n])\n\nweights = np.array([0.3, 0.2, 0.2, 0.15, 0.15])\n\n# The indices of the attributes (zero-based) that are considered beneficial.\n# Those indices not mentioned are assumed to be cost attributes.\nbenefit_attributes = set([0, 1, 2, 3, 4])\n\n# Display the raw data we have\npd.DataFrame(data=raw_data, index=candidates, columns=attributes)","a35a1d98":"m = len(raw_data)\nn = len(attributes)\ndivisors = np.empty(n)\nfor j in range(n):\n    column = raw_data[:,j]\n    divisors[j] = np.sqrt(column @ column)\n\nraw_data \/= divisors\n\ncolumns = [\"$X_{%d}$\" % j for j in range(n)]\npd.DataFrame(data=raw_data, index=candidates, columns=columns)","3604ef40":"raw_data *= weights\npd.DataFrame(data=raw_data, index=candidates, columns=columns)","3a709856":"a_pos = np.zeros(n)\na_neg = np.zeros(n)\nfor j in range(n):\n    column = raw_data[:,j]\n    max_val = np.max(column)\n    min_val = np.min(column)\n    \n    # See if we want to maximize benefit or minimize cost (for PIS)\n    if j in benefit_attributes:\n        a_pos[j] = max_val\n        a_neg[j] = min_val\n    else:\n        a_pos[j] = min_val\n        a_neg[j] = max_val\n\npd.DataFrame(data=[a_pos, a_neg], index=[\"$A^*$\", \"$A^-$\"], columns=columns)","9a5ee732":"sp = np.zeros(m)\nsn = np.zeros(m)\ncs = np.zeros(m)\n\nfor i in range(m):\n    diff_pos = raw_data[i] - a_pos\n    diff_neg = raw_data[i] - a_neg\n    sp[i] = np.sqrt(diff_pos @ diff_pos)\n    sn[i] = np.sqrt(diff_neg @ diff_neg)\n    cs[i] = sn[i] \/ (sp[i] + sn[i])\n\npd.DataFrame(data=zip(sp, sn, cs), index=candidates, columns=[\"$S^*$\", \"$S^-$\", \"$C^*$\"])","de2623a3":"def rank_according_to(data):\n    ranks = rankdata(data).astype(int)\n    ranks -= 1\n    return candidates[ranks][::-1]","fea9d277":"cs_order = rank_according_to(cs)\nsp_order = rank_according_to(sp)\nsn_order = rank_according_to(sn)\n\npd.DataFrame(data=zip(cs_order, sp_order, sn_order), index=range(1, m + 1), columns=[\"$C^*$\", \"$S^*$\", \"$S^-$\"])","2f893c79":"print(\"The best candidate\/alternative according to C* is \" + cs_order[0])\nprint(\"The preferences in descending order are \" + \", \".join(cs_order) + \".\")","490882ff":"## Step 4 and 5 - Calculating Separation Measures and Similarities to PIS\n\nThe separation or distance between the alternatives can be measured by the $n$-dimensional Euclidean distance. The separation from the PIS $A^*$ and NIS $A^-$ are $S^*$ and $S^-$ respectively.\n\n$$\n\\begin{align}\nS_i^* &= \\sqrt{\\sum_{j = 1}^n \\left(v_{ij} - v^*_j\\right)^2} \\\\\nS_i^- &= \\sqrt{\\sum_{j = 1}^n \\left(v_{ij} - v^-_j\\right)^2} \\\\\n\\end{align}\n$$\n\nwhere $i = 1, 2, \\ldots, m$ and $j = 1, 2, \\ldots, n$.\n\nWe also calculate\n\n$$\nC^*_i = \\frac{S_i^-}{S_i^* + S_i^-},\\text{ where }i = 1, 2, \\ldots, m\n$$","23e08bed":"# Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS)\n\nIt is a multi-criteria decision analysis method that is based on the concept that the chosen alternative should have the shortest geometric distance to the Positive Ideal Solution (PIS) and the longest geometric solution from the Negative Ideal Solution (NIS).","c9232a46":"## Step 2 - Calculating the Weighted Normalized Ratings\n\n$$v_{ij} = w_j r_{ij}$$\n\nwhere $i = 1, 2, \\ldots, m$ and $j = 1, 2, \\ldots, n$.","ad42105e":"## Step 6 - Ranking the candidates\/alternatives\n\nWe choose the candidate with the maximum $C^*$ or rank all the alternatives in descending order according to their $C^*$ values. This process can also be done for the $S^*$ and $S^-$ values.","5391ddf7":"## Step 1 - Normalizing the ratings\n\n$$r_{ij}=\\frac{x_{ij}}{\\sqrt{\\sum_{i = 1}^{m} x_{ij}^2}}$$\n\nwhere $i = 1, 2, \\ldots, m$ and $j = 1, 2, \\ldots, n$.","e5869f74":"## Pre-requisites\n\nFor this problem, we are always provided with the following data:\n1. The ratings in every category for each candidate.\n2. The weights for every category or attribute to be considered.\n\nNote that an attribute can be beneficial attribute (in which case, we will want to maximize it's contribution) or a cost attribute (which we will need to minimize). We call the set of beneficial attributes $J_1$ and that of cost attributes $J_2 = J_1^C$.","4abc833e":"## Step 3 - Identifying PIS ($A^*$) and NIS ($A^-$)\n\n$$\n\\begin{align}\nA^* &= \\left\\{v_1^*, v_2^*, \\ldots, v_n^*\\right\\} \\\\\nA^- &= \\left\\{v_1^-, v_2^-, \\ldots, v_n^-\\right\\} \\\\\n\\end{align}\n$$\n\nAnd we define\n\n$$\n\\begin{align}\nv_j^* &=\n\\begin{cases}\n\\max{(v_{ij})}, \\text{ if} j \\in J_1 \\\\\n\\min{(v_{ij})}, \\text{ if} j \\in J_2\n\\end{cases}\n\\\\\nv_j^- &=\n\\begin{cases}\n\\min{(v_{ij})}, \\text{ if} j \\in J_1 \\\\\n\\max{(v_{ij})}, \\text{ if} j \\in J_2\n\\end{cases}\n\\\\\n\\end{align}\n$$\n\nwhere $i = 1, 2, \\ldots, m$ and $j = 1, 2, \\ldots, n$."}}