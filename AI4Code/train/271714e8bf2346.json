{"cell_type":{"00e43998":"code","551a290c":"code","535057d3":"code","72cf19ad":"code","8efcd5df":"code","cda93976":"code","7e01afe1":"code","387145cd":"code","37115ef5":"code","139a9a74":"code","a05fbbe0":"code","4dad9fd4":"code","9b90ec2d":"code","d46fe803":"code","da0a162e":"code","1fe4f7ad":"code","b6b21179":"code","1b6e9ebd":"code","92073dc5":"code","65e093bb":"code","c00f0d3a":"code","866aa707":"code","a4d72542":"code","92a62e38":"code","865edd3b":"code","496ed02b":"code","249092c1":"code","70bd50b2":"code","9e2a5408":"code","1ea83ed2":"code","34a80d6f":"markdown","cb843de0":"markdown","76630d5b":"markdown","5cf937c6":"markdown","b9935d95":"markdown","aa621b02":"markdown","30fd0ca4":"markdown","605a091a":"markdown","a1f62a5b":"markdown","5a2b274f":"markdown","57f63dcd":"markdown","6450f07e":"markdown"},"source":{"00e43998":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","551a290c":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA","535057d3":"data = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')","72cf19ad":"data.head()","8efcd5df":"data.info","cda93976":"data.describe()","7e01afe1":"#correlation matrix\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(corrmat, vmax=.8, annot=True);","387145cd":"data = data.drop('CUST_ID', axis = 1) \n","37115ef5":"for col in data:\n    data[[col]].hist()\n","139a9a74":"data.isna().sum()","a05fbbe0":"data.loc[(data['MINIMUM_PAYMENTS'].isnull()==True),'MINIMUM_PAYMENTS']=data['MINIMUM_PAYMENTS'].mean()","4dad9fd4":"data.loc[(data['CREDIT_LIMIT'].isnull()==True),'CREDIT_LIMIT']=data['CREDIT_LIMIT'].mean()","9b90ec2d":"data.isna().sum()","d46fe803":"data.boxplot(rot=100, figsize=(40,20))\n","da0a162e":"cols = list(data)\nirq_score = {}\n\nfor c in cols:\n    q1 = data[c].quantile(0.25)\n    q3 = data[c].quantile(0.75)\n    score = q3 - q1\n    outliers = data[(data[c] < q1 - 1.5 * score) | (data[c] > q3 + 1.5 * score)][c]\n    values = data[(data[c] >= q1 - 1.5 * score) | (data[c] <= q3 + 1.5 * score)][c]\n    \n    irq_score[c] = {\n        \"Q1\": q1,\n        \"Q3\": q3,\n        \"IRQ\": score,\n        \"n_outliers\": outliers.count(),\n        \"outliers_avg\": outliers.mean(),\n        \"outliers_stdev\": outliers.std(),\n        \"outliers_median\": outliers.median(),\n        \"values_avg:\": values.mean(),\n        \"values_stdev\": values.std(),\n        \"values_median\": values.median(),\n    }\n    \nirq_score = pd.DataFrame.from_dict(irq_score, orient='index')\n\nirq_score\n","1fe4f7ad":"data.shape\n","b6b21179":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(data))\nprint(z)\n","1b6e9ebd":"threshold = 3\nprint(np.where(z > 3))","92073dc5":"data = data[(z < 3).all(axis=1)]\n","65e093bb":"data.boxplot(rot=90, figsize=(30,10))\n","c00f0d3a":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata = sc.fit_transform(data)\n","866aa707":"data = pd.DataFrame(data)","a4d72542":"data.head()","92a62e38":"data.shape","865edd3b":"# Using the elbow method to find the optimal number of clusters\nfrom sklearn.cluster import KMeans\nwcss=[]\nfor i in range (1,12):\n    kmeans=KMeans(n_clusters=i,init='k-means++',random_state=40)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,12),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","496ed02b":"# Fitting K-Means to the dataset\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny_km = kmeans.fit_predict(data)\nprint(y_km)","249092c1":"labels = kmeans.labels_\nlabels","70bd50b2":"from sklearn.decomposition import PCA\npca = PCA(2)\nprincipalComponents = pca.fit_transform(data)\nx, y = principalComponents[:, 0], principalComponents[:, 1]\nprint(principalComponents.shape)\n\ncolors = {0: 'red', 1: 'blue', 2: 'green', 3: 'yellow', 4: 'purple'}","9e2a5408":"final_data = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \ngroups = final_data.groupby(labels)","1ea83ed2":"fig, ax = plt.subplots(figsize=(15, 10)) \n\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5, color=colors[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n    \nax.set_title(\"Customer Segmentation based on Credit Card usage\")\nplt.show()\n","34a80d6f":"## Feature Scalling","cb843de0":"## Clustering Using Kmeans","76630d5b":"## Data Preprocessing","5cf937c6":"### Using PCA","b9935d95":"## Import packages","aa621b02":"### any Comment .... please write here \n## please UpVote ... Thanks ","30fd0ca4":"## Explore Missing Values","605a091a":"### Dealing With Missing Values","a1f62a5b":"# Build Model","5a2b274f":"## Dealing With Outliers","57f63dcd":"### Using Zscore to handle outliers","6450f07e":"## Explore Outliers"}}