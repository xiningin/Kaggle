{"cell_type":{"1e4f5d44":"code","bdca8229":"code","43ed2b2a":"code","414dc14a":"code","6876f505":"code","cb3327b6":"code","497bc1f9":"code","c6fa503c":"code","9033ec3f":"code","63fb524e":"code","df72eb37":"code","aa4093be":"code","5feb471d":"code","b51b86c5":"code","2b58de24":"code","6a6d05f8":"code","1b879d04":"code","231464cb":"code","a3faf296":"code","e572e061":"code","e903cb6d":"code","a8f7afaf":"code","e3cbd41e":"code","56e7d7bc":"code","5b510a55":"code","b432c4ab":"code","e5bf442b":"code","2cf3369b":"code","820c0693":"code","53f75af4":"code","3b69582a":"code","595cf049":"code","8539930f":"code","742da08c":"code","7b025ba3":"code","164ba3ce":"code","008ce380":"code","b7f5126d":"code","3f8ef8d2":"code","12f2c6b6":"markdown","46f164d4":"markdown","4a2fc6ca":"markdown","42c070f6":"markdown","d2d36552":"markdown","60d0ea10":"markdown","f7ec897b":"markdown","32853c4d":"markdown","2ca6a2ca":"markdown","0ce9345b":"markdown","885e0940":"markdown","946041fc":"markdown","29fe4c71":"markdown","f0bfe39b":"markdown","e7897e72":"markdown","135a5ca0":"markdown","ec6385e4":"markdown","6e19d756":"markdown","6598573a":"markdown","afe396a3":"markdown","a6745579":"markdown","27d2fc08":"markdown","f3492710":"markdown","4279f50f":"markdown","0d06ba40":"markdown","5af5b54b":"markdown","ef4abc20":"markdown","d97053c2":"markdown","547d7dd5":"markdown","d7107c50":"markdown","87c1a1ad":"markdown","2f2c59bc":"markdown","b3238c80":"markdown","464b016c":"markdown","05fd6e30":"markdown","1a90ddbe":"markdown","5b02a1bd":"markdown","16dd39c6":"markdown","c8736f12":"markdown","b6ce9ec6":"markdown","c3adda97":"markdown","344b9bee":"markdown","23950ca7":"markdown","0753c4af":"markdown","6a31dac1":"markdown","b3cabf17":"markdown","8050d994":"markdown"},"source":{"1e4f5d44":"import numpy as np\nimport pandas as pd\nimport os\nimport subprocess as sp\nimport matplotlib.pyplot as plt\nimport warnings\nimport statsmodels.api as sm\nfrom calendar import month_abbr\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bdca8229":"!curl https:\/\/scrippsco2.ucsd.edu\/assets\/data\/atmospheric\/stations\/in_situ_co2\/monthly\/monthly_in_situ_co2_mlo.csv --output co2_data.csv\n!cp co2_data.csv co2_data_new.csv\n!cp ..\/input\/scripps-co2-program-06122021\/monthly_in_situ_co2_mlo.csv co2_data.csv","43ed2b2a":"expected_hash = \"645983b005053fb3ba442ad034966c7f23173975\"\nassert sp.getoutput(\"sha1sum .\/co2_data.csv\")[:len(expected_hash)] == expected_hash, \"File has been somehow corrupted\"","414dc14a":"# Let's load the data !\ndata = pd.read_csv(\".\/co2_data.csv\", comment='\"', skiprows=[55, 56])\n# According to the website, an unavailable data point is represented by the value -99.99\ndata[data == -99.99] = np.NaN\ndata","6876f505":"# I expect that the date columns are not contradictory\nif not (data.iloc[:, 0].values == np.floor(data.iloc[:, 3].values)).all(): warnings.warn(\"Date columns are contradictory !\")\n# I expect that if one value is missing, then every other one the same date are missing\nif not ((data.values == -99.99).sum(axis=-1) == 6).all(): warnings.warn(\"There are entries where some values are missing but not all of them\")","cb3327b6":"# Separate the data that interest us\nmois = data.iloc[:, 1]\nannees = data.iloc[:, 3]\nco2 = data.iloc[:, 4]","497bc1f9":"plt.title(\"C02 concentration (ppm) in athmosphere during time\")\nplt.plot(annees, co2, label=\"CO2 concentration\")\nplt.legend()\nplt.xlabel(\"time\")\nplt.ylabel(\"C02 concentration (ppm)\")\nplt.show()","c6fa503c":"\"\"\"\n    Utils functions to re-use code\n\"\"\"\ndef make_reg(X, y):\n    \"\"\"\n        Make a regression using X to explain y\n    \"\"\"\n    # Remove nans\n    mask_y = np.logical_not(np.isnan(y))\n    mask_x = np.logical_not(np.isnan(y))\n    mask = np.logical_and(mask_x, mask_y)\n    # Create a linear model with a bias\n    reg = sm.OLS(y[mask].values, sm.add_constant(X[mask]))\n    # Fit the model\n    res = reg.fit()\n    return res\n\ndef plot_predictions_vs_real(x, prediction, real):\n    \"\"\"\n        Plot predictions against ground truth on the training data\n    \"\"\"\n    plt.title(\"C02 concentration in athmosphere during time\")\n    plt.plot(x, real, label=\"Measured values\")\n    plt.plot(x, prediction, label=\"Predicted values\")\n    plt.legend()\n    plt.xlabel(\"Years\")\n    plt.ylabel(\"CO2 concentration (ppm)\")\n    plt.show()\n\ndef plot_residuals(pred, y):\n    \"\"\"\n        Plot residuals as a function of time and a lagged plot of residuals\n    \"\"\"\n    residuals = y-pred\n    plt.title(\"Residuals vs time\")\n    plt.plot(residuals, label=\"Residuals\")\n    plt.legend()\n    plt.xlabel(\"Years\")\n    plt.ylabel(\"Residual\")\n    plt.show()\n    plt.title(\"Residuals lagged scatter plot\")\n    plt.scatter(residuals[:-1], residuals[1:], label=\"Residuals\")\n    plt.legend()\n    plt.xlabel(\"residuals at time t\")\n    plt.ylabel(\"residuals at time t + 1\")\n    plt.show()","9033ec3f":"# Now let's use our function to fit our first model\nres = make_reg(annees, co2)\nres.summary()","63fb524e":"co2_pred = res.predict(sm.add_constant(annees))\nplot_predictions_vs_real(annees, co2_pred, co2)","df72eb37":"plot_residuals(co2_pred, co2)","aa4093be":"res = make_reg(annees, np.log(co2))\nres.summary()","5feb471d":"co2_pred = np.exp(res.predict(sm.add_constant(annees)))\nplot_predictions_vs_real(annees, co2_pred, co2)","b51b86c5":"plot_residuals(co2_pred, co2)","2b58de24":"res = make_reg(np.stack((annees, annees**2), axis=-1), co2)\nres.summary()","6a6d05f8":"co2_pred = res.predict(sm.add_constant(np.stack((annees, annees**2), axis=-1)))\nplot_predictions_vs_real(annees, co2_pred, co2)","1b879d04":"plot_residuals(co2_pred, co2)","231464cb":"periodic = co2_pred - co2\nplt.title(\"Periodic component left\")\nplt.plot(annees, periodic)\nplt.xlabel(\"years\")\na = plt.ylabel(\"residuals\")\n","a3faf296":"na_mask = periodic.isna()\nprint(\"Date of missing values:\", list(zip(np.floor(annees[na_mask]).astype(np.int).tolist(), mois[na_mask].tolist())))","e572e061":"print(np.arange(len(periodic))[na_mask])","e903cb6d":"periodic_slice = periodic[76:765]\nassert not periodic_slice.isna().any(), \"Bad indices, there is still nans in the data\"\nperiodic_slice = periodic_slice.values\nprint(\"Proportion of the data used to perform fft: {:.4f}\".format(len(periodic_slice) \/ len(periodic)))","a8f7afaf":"def fourierExtrapolation(x, t, n_harm=4):\n    n = x.size\n    x_freqdom = np.fft.rfft(x)\n    plt.plot(x_freqdom)\n    plt.show()\n    indexes = np.argsort(np.absolute(x_freqdom))\n    indexes_keep = indexes[-(1+n_harm*2):]\n    x_approx = np.zeros_like(x_freqdom)\n    x_approx[indexes_keep] = x_freqdom[indexes_keep]\n    return np.fft.irfft(x_approx, t.max()+1)[t]\n\napprox = fourierExtrapolation(periodic_slice, np.arange(len(periodic_slice)))\nplt.plot(periodic_slice)\nplt.plot(approx)\nplt.show()\nplt.plot(periodic_slice - approx)\nplt.show()","e3cbd41e":"plot = plot_acf(periodic[~na_mask])","56e7d7bc":"# Transform time and co2 series into polynomial and lagged variables\ndef make_ploynomial_and_seasonal(x, ts):\n    return np.stack((ts[:-12], x[12:], x[12:]**2), axis=-1)\n\nx = make_ploynomial_and_seasonal(annees, co2)\nm = ~np.isnan(x).any(axis=-1)\nres = make_reg(x[m], co2[12:][m])\nres.summary()","5b510a55":"co2_pred = res.predict(sm.add_constant(make_ploynomial_and_seasonal(annees, co2)))\nplot_predictions_vs_real(annees[12:], co2_pred, co2[12:])","b432c4ab":"plot_residuals(co2_pred, co2[12:])","e5bf442b":"def make_linear_and_seasonal(x, ts):\n    return np.stack((ts[:-12], x[12:]), axis=-1)\n\nx = make_linear_and_seasonal(annees, co2)\nm = ~np.isnan(x).any(axis=-1)\nres = make_reg(x[m], co2[12:][m])\nres.summary()","2cf3369b":"co2_pred = res.predict(sm.add_constant(make_linear_and_seasonal(annees, co2)))\nplot_predictions_vs_real(annees[12:], co2_pred, co2[12:])","820c0693":"plot_residuals(co2_pred, co2[12:])","53f75af4":"first_test_year = 2019\ntrain_mask = annees < first_test_year\n# Split into train and test set\nannees_train, co2_train = annees[train_mask], co2[train_mask]\nannees_test, co2_test = annees[~train_mask], co2[~train_mask]\n\ndef predictions_plot(preds=None, ci=None, zoom=False):\n    plt.plot(annees_train, co2_train, label=\"train set\")\n    plt.plot(annees_test, co2_test, label=\"true label\")\n    if preds is not None:\n        plt.plot(annees_test, preds, label=\"Predicted\")\n        plt.fill_between(annees_test, ci[:, 0], ci[:, 1], alpha=.3)\n        # Plot MSE to have a numerical metric\n        print(\"MSE={:.4f}\".format(np.mean((preds - co2_test)**2)))\n    if zoom:\n        plt.title(\"Predicted CO2 concentration in ppm (zoomed)\")\n        x_window = [2018, 2022]\n        m = np.logical_and(annees_test >= x_window[0], annees_test <= x_window[1])\n        if preds is not None:\n            data = np.concatenate((co2_test[m].ravel(), preds[m].ravel(), ci[m].ravel()))\n        else:\n            data = co2_test[m]\n        plt.xlim(x_window)\n        plt.ylim([np.nanmin(data)*0.99, np.nanmax(data)*1.01])\n    else:\n        plt.title(\"Predicted CO2 concentration in ppm\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"CO2 concentration (ppm)\")\n    plt.legend()\n    plt.show()\n        \npredictions_plot(zoom=True)","3b69582a":"res = make_reg(annees_train, co2_train)\nx = sm.add_constant(annees_test)\nco2_pred = res.predict(x)\nci = res.get_prediction(x).conf_int()\npredictions_plot(co2_pred, ci)\npredictions_plot(co2_pred, ci, zoom=True)","595cf049":"res = make_reg(annees_train, np.log(co2_train))\nx = sm.add_constant(annees_test)\nco2_pred = np.exp(res.predict(x))\nci = np.exp(res.get_prediction(x).conf_int())\npredictions_plot(co2_pred, ci)\npredictions_plot(co2_pred, ci, zoom=True)","8539930f":"res = make_reg(np.stack((annees_train, annees_train**2), axis=-1), co2_train)\nx = sm.add_constant(np.stack((annees_test, annees_test**2), axis=-1))\nco2_pred = res.predict(x)\nci = res.get_prediction(x).conf_int()\npredictions_plot(co2_pred, ci)\npredictions_plot(co2_pred, ci, zoom=True)","742da08c":"# The autoregressive model needs to predict gradualy as it can be necessary to rely on past predictions\ndef autoregressive_extrapolation(reg, f, annees_test, co2_train, min_lag=12):\n    n = len(annees_test)\n    preds = co2_train.values\n    cis = np.zeros((len(co2_train), 2))\n    cis[:, 0] = co2_train.values\n    cis[:, 1] = co2_train.values\n    while len(annees_test):\n        co2_batch, annees_batch = preds[-min_lag:], annees_test[:min_lag]\n        n_pred = len(annees_batch)\n        annees_test = annees_test[min_lag:]\n        preds = np.concatenate((preds, np.zeros(n_pred)))\n        cis = np.concatenate((\n            cis,\n            np.stack((np.zeros(n_pred), np.zeros(n_pred)), axis=-1)\n        ), axis=0)\n        annees_batch = np.concatenate((np.zeros(min_lag), annees_batch))\n        x = f(\n            annees_batch,\n            preds[-min_lag-n_pred:]\n        )\n        co2_pred = res.predict(sm.add_constant(x))\n        # Lower bound calculated by considering the best case scenario\n        ci_low = res.get_prediction(\n            sm.add_constant(f(annees_batch, cis[-min_lag-n_pred:, 0]))\n        ).conf_int()[:, 0]\n        # Higher bound calculated by considering the worst case scenario\n        ci_high = res.get_prediction(\n            sm.add_constant(f(annees_batch, cis[-min_lag-n_pred:, 1]))\n        ).conf_int()[:, 1]\n        \n        preds[-n_pred:] = co2_pred\n        cis[-n_pred:, 0] = ci_low\n        cis[-n_pred:, 1] = ci_high\n        \n    return preds[-n:], cis[-n:]","7b025ba3":"x = make_ploynomial_and_seasonal(annees_train, co2_train)\nm = ~np.isnan(x).any(axis=-1)\nres = make_reg(x[m], co2_train[12:][m])\n\nco2_pred, ci = autoregressive_extrapolation(res, make_ploynomial_and_seasonal, annees_test, co2_train)\npredictions_plot(co2_pred, ci)\npredictions_plot(co2_pred, ci, zoom=True)","164ba3ce":"x = make_linear_and_seasonal(annees_train, co2_train)\nm = ~np.isnan(x).any(axis=-1)\nres = make_reg(x[m], co2_train[12:][m])\n\nco2_pred, ci = autoregressive_extrapolation(res, make_linear_and_seasonal, annees_test, co2_train)\npredictions_plot(co2_pred, ci)\npredictions_plot(co2_pred, ci, zoom=True)","008ce380":"x = make_ploynomial_and_seasonal(annees, co2)\nm = ~np.isnan(x).any(axis=-1)\nres = make_reg(x[m], co2[12:][m])\n\n# Predict until 2025\nannees_to_predict = [2022, 2023, 2024]\n# Average over the measure date in the month (no big difference)\ndates_prop = np.unique(annees - np.floor(annees)).reshape((-1, 2)).mean(axis=-1)\nannees_pred = np.repeat(annees_to_predict, len(dates_prop)) + np.repeat(np.expand_dims(dates_prop, axis=0), 3, axis=0).ravel()\n# Last months of current year are None so we have to also predict them\nannees_pred = np.concatenate((annees[-3:], annees_pred))\n\nco2_pred, ci = autoregressive_extrapolation(res, make_ploynomial_and_seasonal, annees_pred, co2[:-3])\nplt.title(\"Predicted CO2 concentration in ppm\")\nplt.plot(annees, co2, label=\"train\")\nplt.plot(annees_pred, co2_pred, label=\"predicted\")\nplt.fill_between(annees_pred, ci[:, 0], ci[:, 1], color='orange', alpha=.3)\nplt.xlabel(\"Time\")\nplt.ylabel(\"CO2 concentration (ppm)\")\nplt.legend()\nplt.show()\n\nplt.title(\"Predicted CO2 concentration in ppm (zoomed)\")\nplt.plot(annees, co2, label=\"train\")\nplt.plot(annees_pred, co2_pred, label=\"predicted\")\nplt.fill_between(annees_pred, ci[:, 0], ci[:, 1], color='orange', alpha=.3)\nplt.xlim([annees_pred.min(), annees_pred.max()])\nplt.ylim([ci.min(), ci.max()])\nplt.xlabel(\"Time\")\nplt.ylabel(\"CO2 concentration (ppm)\")\nplt.legend()\nplt.show()","b7f5126d":"new_data = pd.read_csv(\".\/co2_data_new.csv\", comment='\"', skiprows=[55, 56])\nnew_annee = new_data.iloc[:, 3].values\nnew_co2 = new_data.iloc[:, 4].values\nm = new_annee >= annees_pred.min()\nnew_annee = new_annee[m]\nnew_co2 = new_co2[m]\nn_missing = len(annees_pred) - len(new_annee)\nif n_missing > 0:\n    new_annee = np.concatenate((new_annee, annees_pred[-n_missing:]))\n    new_co2 = np.concatenate((new_co2, np.ones(n_missing) * -99.99))\nnew_co2[new_co2 == -99.99] = np.nan","3f8ef8d2":"extrapolation = pd.DataFrame({\n    \"Year\": np.floor(annees_pred).astype(np.int),\n    \"Month\": map(lambda x: month_abbr[int(x)+1], np.floor((annees_pred - np.floor(annees_pred))*12)),\n    \"Co2 Prediction\": co2_pred,\n    \"Co2 True value\": new_co2,\n    \"0.05 ci\": ci[:, 0],\n    \"0.95 ci\": ci[:, 1],\n    \"In Interval\": np.logical_and(ci[:, 0] <= new_co2, new_co2 <= ci[:, 1]),\n    \"Squared error\": (co2_pred - new_co2)**2\n})\nplt.title(\"Predicted CO2 concentration (in ppm) vs measured values\")\nplt.plot(annees, co2, label=\"train\")\nplt.plot(annees_pred, co2_pred, label=\"predicted\")\nplt.fill_between(annees_pred, ci[:, 0], ci[:, 1], color='orange', alpha=.3)\nplt.plot(annees_pred, new_co2, label=\"measured\")\nplt.xlim([annees_pred.min(), annees_pred.max()])\nplt.ylim([ci.min(), ci.max()])\nplt.xlabel(\"Time\")\nplt.ylabel(\"CO2 concentration (ppm)\")\nplt.legend()\nplt.show()\nprint(\"So far, MSE={:.4f}\".format(np.nanmean(extrapolation[\"Squared error\"])))\nextrapolation","12f2c6b6":"## Polynomial + autoregressive model\nGiven what we just said, we will try to improve our best model (the polynomial one) by adding the value of y that we are trying to predict with a lag of $12$ time units:\n\n$$\n    y_t = \\beta_0 + \\beta_1y_{t-12} + \\beta_2t + \\beta_3t^2\n$$","46f164d4":"Same idea with the linear + seasonal. Do note however that the MSE is better for the polynomial than the linear one. At least it doesn't invalidate what we said about the AIC","4a2fc6ca":"The exponential model is less wrong but still underestimates a lot the ground truth. Here again, the confidence interval are wrong probably for violated assumptions reasons (residuals not independant)","42c070f6":"Well, the result isn't very good, we see that there is a clear boundary effect that will hurt if we try to extrapolate.\n\nGiven this result, I want to try to go in another direction to predict the seasonal component.\nI will rather try to adjust the existing model than decomposing the problem","d2d36552":"I want to try to conduct a fourier transform decomposition, but there are some missing datapoints.\nfft expect a uniformly sampled signal.\nThere is multiple things I can do from here:\n1. Apply another harmonic decomposition algorithm that do not expect a uniformly sampled signal\n2. Find an interpolation scheme to \"fill in\" the missing datapoints (that may introduce bias)\n3. Perform my analysis on the biggest slice without missing datapoints\n\nI've made some research to find a non uniform fourier transform algorithm but all I found was approximations, and algorithm where I have no clue how to use them. As I do not have a signal processing expert under the hand to ask questions to.<br>\n[More details here](https:\/\/cims.nyu.edu\/cmcl\/nufft\/nufft.html)\n\nNow to choose between the second and last option I want to have a look at the location of the missing data points","60d0ea10":"Let's see if we can make some assumptions about the data and check them","f7ec897b":"The polynomial model is much better (MSE 10 times smaller than previous one) but as expected, the seasonal component is completly ignored.\nOnce again confidence interval fail probably for the same reasons","32853c4d":"## Fourier decomposition approach","2ca6a2ca":"# Co2 concentration in the atmosphere since 1958\n\n# Introduction\nIn this notebook, I'll use the data of Mauna Loa Observatory, Hawaii provided by the [Scripps CO2 Program](https:\/\/scrippsco2.ucsd.edu\/data\/atmospheric_co2\/primary_mlo_co2_record.html). The goal will be to explain the data using a model, and then try to predict the future concentration in atmosphere of co2 at this place until 2025.","0ce9345b":"## Linear + seasonal model\nAs we saw earlier the coefficient associated to the $t^2$ term is very small. Let's try to simplify the model by removing it:\n$$\n    y_t = \\beta_0 + \\beta_1y_{t-12} + \\beta_2t\n$$","885e0940":"## Polynomial model","946041fc":"# Test on the existing data\nI will now test the considered models trained on a subset of the data, and see their performances on the remaining data. I don't argue that it proves anything, but at best it could prove us wrong.\nAS  the final goal is to predict until 2025 and we are currently at the end of 2021, we will try to predict 2019, 2020, 2021 using past data","29fe4c71":"## Polynomial + lagged variable","f0bfe39b":"Really similar to the previous one","e7897e72":"## Linear model","135a5ca0":"So we clearly see that there is a structure with the shape of a banana in the residuals over time. This is not a good sign as some assumptions of the model are completly false. Furthermore, the impression is confirmed by the lagged plot\n\n*Red flag: structure in the residuals*","ec6385e4":"Let's check that the file has not been modified somehow by checking its hash","6e19d756":"## Exponential model\nVisually, it looks like the curve is following a slite exponential-ish tendancy. Let's try to fit to $\\log(y)$ instead of $y$ itself:\n$$\n    \\log(y_t) = \\beta_0 + \\beta_1t\n$$","6598573a":"We'll consider only a subset of  the variables of the dataset, namely the measured co2 concentration, and the date expressed in a continuous way. The other columns are either redundancies or calculated columns.","afe396a3":"Visually, we can see that the model is very close to ground truth. I'm surprised that the confidence intervals are wrong given that I consider the worst possible case at each step. I belive that it's again due to the violated assumptions of linear models but if anyone have an idea please share it in comments.","a6745579":"# Final prediction","27d2fc08":"the $R^2$ is the same, and the p-values are very small. However, the AIC is slithly higher, so according to this metric it's worth it to keep the quadratic component in t he equation.","f3492710":"I see that I have a slice between April 1964 and October 2021 with no missing values that could be used while dropping the rest. Another argument in favor of the slice option versus the interpolation one is that there are consectuive missing values, meaning that interpolation will be less precise.\n\nSo I've decided to go with the slice option.","4279f50f":"## Autoregressive approach\nAs the fourier gave nothing of interest, i will try to caracterize the seasonal component using an autoregressive model, meaning that i'll use the past values to predict the futur ones.\nThe advantage is that it can express much more than a linear model. The disadvantage is that it's harder to use and when extrapolating, the error blow up in time as it uses past predictions to predict futur ones.","0d06ba40":"## Log model","5af5b54b":"We clearly see that the linear explains well the global trend. However it's clear that the true data is not linear. Maybe plotting the residuals could be helpfull","ef4abc20":"The best AIC so far is the polynomial one, so we will use it to make our predictions ","d97053c2":"## Linear model\nAs it's better to start simple, i'll first only try to explain the data using a linear model:\n$$\n    y_t = \\beta_0 + \\beta_1t\n$$","547d7dd5":"As expected, the linear model is not very good, the true value are really under-estimated. Furthermore, the confidence interval is wrong, probably because the assumptions of the model are violated","d7107c50":"No clear structure other than the seasonal component in the time vs residuals plot. The mean and std appear to be the same during time. \nThis seasonnal component can explain the correlation observed in the lagged plot. \nWe should probably try to describe it","87c1a1ad":"## Linear + lagged variable","2f2c59bc":"The model is now much better ! We have a $R^2$ of $1$ meaning that we explained all the explainable variance. The AIC is way better than the preivous one, meaning that we can believe that we don't overfit. P-value wise, we lost the treshold of $0.001$ but we are still better than $0.01$.","b3238c80":"As in the linear model, we still have a banana structure in the residuals over time. Same for the lag plot .\n\n*Red flag: still structure in the data*","464b016c":"# Models\nNow that the data is loaded and that we had a look at it, we are gonna fit models to it and try to express in the siplest way possible the data.\n\n### Notation\nDuring the notebook I'll describe the models using the following notation:\n* $t$ denotes the date expressed in a continous way\n* $y_t$ is the concentration of CO2 in the atmosphere at time $t$\n* $\\beta_i$ denotes the parameters of the model","05fd6e30":"The $R^2$ of $0.984$ is better than the previous one. The p-values indicate also a strong effect of the explainatory variables. I don't understand why the AIC is so low I thnk it's better to not take it into account for this model.","1a90ddbe":"Once again we have an even better $R^2$. All 3 p-values are very small meaning that all explainatory variable have a linear effect.\nThe AIC of $3373$ is the best one so far, which is a good indication that we are not yet overfitting","5b02a1bd":"The ACF plot show the correlation of $y_t$ and $y_{t-x}$. On this plot we clearly see a sinusoidal component. The wavelength would be around 12 units of time. As the samples are given on the 15 of each month, a period of 12 samples correspond to exactly a year, which is intuitively explainable.","16dd39c6":"Visually there is no big difference with the previous model","c8736f12":"## Load the data\n\nLet's download the last version of the scripps program so that it's reusable for future work.\nBut the data we'll use in this notebook is a static dataset corresponding to the latest data available when the notebook was created for reproducibility","b6ce9ec6":"We see that the $R^2$ is very good ($0.976$). Furthermore, the p-values of the coefficient are very low which is a good sign.\nNow let's visually see the fitte curve","c3adda97":"Visually, the log model looks better than the previous one, but we still see that it is not good enough. On the boundaries of the data, the error is pretty big, which is a problem if we want to extrapolate","344b9bee":"## Polynomial model\nOk so the growth isn't linear or exponential, so I'll now assume that it is quadratic. To see if it's the case, let's fit a polynomial model:\n$$\n    y_t = \\beta_0 + \\beta_1t + \\beta_2t^2\n$$","23950ca7":"# Seasonal component\n*Note of the me from the future: This section was quite a mess and end up on nothing usefull, i left it so that we can follow the path of toughts that led to the end but nothing usefull for the study until the next session*","0753c4af":"Visually the predictions looks very good. Some spikes can be seen sometimes that are slightly off the predictions but other than that nothing to really say.","6a31dac1":"# Conclusion\nWe manage to fit a model using a quadratic growth + a seasonal (yearly) component with a test MSE less than $1$ppm. We failed to provide a valid confidence interval and we believe it may due to violated assumptions of linear models, namely the independance between error samples.\nThere is still periodic comopnents in the residuals that could be reolved by further work.","b3cabf17":"The visual plot looks much better, we have visually the best possible explaintation for the global tendancy","8050d994":"The residuals still have some seasonal component that is not captured by the model, but there is no clear frequency associated to it."}}