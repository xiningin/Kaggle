{"cell_type":{"418d40db":"code","a58d4df7":"code","4dd87a4b":"code","6b9f904c":"code","2aff620c":"code","c48392bc":"code","55f3d953":"code","994a166a":"code","191b62bf":"code","e3dec30e":"code","2306d8de":"code","338a84cd":"code","145d1170":"code","902fdbbb":"code","0bde9b65":"code","697dd248":"code","12bf7d41":"code","2fb85223":"code","0d026efb":"code","4e3d61ef":"code","2a763ccb":"code","947d68f5":"code","8131a62e":"code","ea3ec431":"code","24651561":"code","d4d7230d":"code","175443a8":"code","9121ed60":"code","08877bdd":"code","5def85f3":"code","df93fad7":"code","c26585cf":"code","0b57b9ad":"code","1da4cda1":"code","d56609d1":"code","afaf47af":"code","c49ce57f":"code","fd1a9e5e":"code","f1b91265":"code","a862b113":"code","01365563":"code","21ec7019":"code","58cd0895":"markdown","b2485a1c":"markdown","cd40bd01":"markdown","8963d5a9":"markdown","aae5ce39":"markdown","c0efd99b":"markdown","38f5d228":"markdown","87bf523f":"markdown","eb9fe9b0":"markdown","fe225354":"markdown","0239fb88":"markdown","1430e64d":"markdown","2b32277c":"markdown","dde11afb":"markdown","6d2e2819":"markdown","a676677a":"markdown","b4b8b992":"markdown","548da09b":"markdown","e138dba9":"markdown","c129e675":"markdown","aea3f803":"markdown","3e570bf3":"markdown","13fc9650":"markdown","9d1bec80":"markdown","ce99f073":"markdown","2dce4728":"markdown","45e5d033":"markdown","46d85b2b":"markdown","117a7a3c":"markdown","93ac0f7a":"markdown","cd239926":"markdown","6e77be60":"markdown","db4490b7":"markdown","a1cb6848":"markdown","7d737166":"markdown","c2a01fa3":"markdown","56f19f1d":"markdown","dec7b345":"markdown","7c5be3a4":"markdown","6c5d9bca":"markdown"},"source":{"418d40db":"!pip install numpyro==0.2.4\n!pip install dtaidistance\n!pip install arviz","a58d4df7":"# %load imports.py\nimport os\nfrom collections import namedtuple\nfrom io import StringIO\nimport re\n\nimport requests\n\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport jax\nfrom jax.experimental.ode import odeint, vjp_odeint\nimport jax.numpy as np\nfrom jax.random import PRNGKey\nimport numpy as onp\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\nimport arviz as az\n\nNUM_CHAINS = 1\nnumpyro.set_host_device_count(NUM_CHAINS)\nnumpyro.enable_x64()\n\nplt.style.use('ggplot')\n","4dd87a4b":"# %load fetching.py\nfrom io import StringIO\nimport re\nimport requests\n\n# country codes\ndef fetch_isocodes():\n    isocodes = pd.read_csv('..\/input\/countries-iso-codes\/wikipedia-iso-country-codes.csv')\n    isocodes.columns = isocodes.columns.str.replace(' ', '_').str.lower()\n    isocodes = isocodes.rename({\"english_short_name_lower_case\": 'country_name'}, axis=1)\n    return isocodes\n\n# ACAPS\ndef fetch_acaps(isocodes, url=None):\n    filepath = '..\/input\/acaps-covid19-government-measures\/acaps_covid19_goverment_measures_dataset.xlsx'\n#     if url is not None:\n#         measures = pd.read_excel(url, sheet_name='Database')\n#         measures.to_csv(filepath, index=False)\n    \n    measures = pd.read_excel(filepath, sheet_name='Database')\n    measures.columns = measures.columns.str.lower()\n    measures['date_implemented'] = pd.to_datetime(measures['date_implemented'])\n\n    measures = measures.merge(isocodes, left_on='iso', right_on='alpha-3_code')\n    return measures\n\n# ECDC for names of lactions\ndef fetch_ecdc():\n    ecdc = (pd.read_csv('https:\/\/covid.ourworldindata.org\/data\/owid-covid-data.csv')\n            .assign(date=lambda f: f['date'].pipe(pd.to_datetime))\n           )\n    \n    return ecdc\n\n# Apple mobility\n\ndef fetch_apple(location_code, url=None):\n    filepath = '..\/input\/apple-covid-mobility\/applemobilitytrends.csv'\n    if url is not None:\n        response = requests.get(url)\n        apple_mobility = (pd.read_csv(StringIO(response.content.decode())))\n        apple_mobility.to_csv(filepath, index=False)\n\n    apple_mobility = (pd.read_csv(filepath)\n                      .drop('alternative_name', axis=1)\n                      .set_index(['geo_type', 'region', 'transportation_type'])\n                      .rename_axis(\"date\", axis=1)\n                      .stack()\n                      .rename('change')\n                      .reset_index('date')\n                      .assign(date=lambda f: pd.to_datetime(f['date']))\n                      .set_index('date', append=True)\n                      )\n\n    apple_mobility = (apple_mobility\n                      .reset_index()\n                      .replace({'UK': 'United Kingdom', 'Republic of Korea': 'South Korea', 'Macao': 'Macau'})  # Only missing is Macao\n                      .merge(location_code, left_on='region', right_on='location', how='left')\n                     .assign(change=lambda f: f['change'].div(100).sub(1)))\n\n    apple_mobility = apple_mobility.loc[lambda f: f['iso_code'].notna()].set_index(['iso_code', 'date', 'transportation_type'])['change'].unstack()\n    return apple_mobility\n\ndef fetch_google(isocodes, location_code):\n    google_mobility = pd.read_csv('https:\/\/www.gstatic.com\/covid19\/mobility\/Global_Mobility_Report.csv', parse_dates=['date'])\n\n    def safe_match(pat, text):\n        match = re.match(pat, text)\n        return match.groups()[0] if match else text\n\n    google_mobility.columns = google_mobility.columns.map(lambda col: safe_match(\"(.*)_percent\", col))\n    google_mobility = (google_mobility\n                .merge(isocodes, left_on='country_region_code', right_on='alpha-2_code', how='left')\n                .merge(location_code, left_on='alpha-3_code', right_on='iso_code', how='left'))\n\n    google_mobility = google_mobility.loc[lambda f: f['sub_region_1'].isna()].set_index(['iso_code', 'date']).select_dtypes(float).div(100)\n    return google_mobility\n\n# oxford.columns[mask].str.extract(r'(..)_.*', expand=False)\n\ndef fetch_oxford():\n    oxford = pd.read_csv('https:\/\/github.com\/OxCGRT\/covid-policy-tracker\/raw\/master\/data\/OxCGRT_latest.csv')\n    oxford.columns = oxford.columns.map(str.lower).str.replace(' ', '_')\n    oxford['date'] = pd.to_datetime(oxford['date'], format='%Y%m%d')\n\n    ordinal_columns = oxford.columns[oxford.columns.str.contains(\"^c._.*_.*\")]\n    geographic_columns = oxford.columns[oxford.columns.str.contains('^c._flag')]\n    strip_measure_name = lambda name: name.split('_')[0]\n    measures_ix = dict(zip(ordinal_columns.map(strip_measure_name), ordinal_columns.map(lambda s: '_'.join(s.split('_')[1:]))))\n\n    oxford_long = (oxford\n                   .set_index(['countrycode', 'date'])\n                   [geographic_columns]\n                   .rename(columns=strip_measure_name)\n                   .rename_axis('category', axis=1)\n                   .stack()\n                   .to_frame('flag')\n                   .join(oxford.set_index(['countrycode', 'date'])\n                         [ordinal_columns]\n                         .rename(columns=strip_measure_name)\n                         .replace(0, np.nan)\n                         .rename_axis('category', axis=1)\n                         .stack()\n                         .rename('measure'),\n                         how='outer'\n                        )\n                  )\n\n    oxford_long = (oxford_long\n                   .rename_axis('variable', axis=1)\n                   .stack()\n                   .rename('value'))\n    return oxford_long, measures_ix","6b9f904c":"isocodes = fetch_isocodes()\nacaps = fetch_acaps(isocodes)\necdc = fetch_ecdc()\nlocation_code = ecdc.groupby(['location', 'iso_code']).first().iloc[:, 0].reset_index().iloc[:,:2]\napple = fetch_apple(location_code)\ngoogle = fetch_google(isocodes, location_code)\noxford_long, measures_ix = fetch_oxford()\n\nmobility = google.join(apple, how='outer')","2aff620c":"ix = pd.IndexSlice\n\ndef build_dataset(country, mobility, oxford_long):\n    subset_mobility = mobility.loc[country].copy().sort_index()\n    for col in subset_mobility.columns:\n        first_notna = subset_mobility[col].notna().idxmax()\n        subset_mobility.loc[:first_notna, col] = 0.\n\n    subset_mobility = subset_mobility.fillna(method='ffill').fillna(method='bfill')\n\n    subset_measures = (oxford_long\n                       .unstack(-1)\n                       .loc[country]\n                       .query('flag != flag or flag == 1.')\n                       ['measure']\n                       .unstack()\n                       .reindex(['c' + str(i) for i in range(1, 9)], axis=1))\n\n    common_ix = subset_measures.index.union(subset_mobility.index)\n    subset_measures = subset_measures.reindex(common_ix).fillna(method='ffill').fillna(0)\n    subset_mobility = subset_mobility.reindex(common_ix).fillna(method='ffill').fillna(method='bfill')\n    return subset_measures, subset_mobility","c48392bc":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nall_countries = list(set(mobility.reset_index().iso_code))\ndef run_linear_regression(mobility_category, test_country, selected_countries):\n    errors = []\n    country_used = []\n    meas_train, mob_train = [], []\n    for country in selected_countries :\n        if country == test_country:\n            continue\n        try:\n            meas, mob = build_dataset(country, mobility, oxford_long)\n            mob = mob.apply(lambda x: np.log(1 + x), axis =1)\n            if mob[mobility_category].isnull().sum()<0.2*mob[mobility_category].shape[0] :\n                meas_train.append(meas.interpolate(method='nearest').fillna(method='ffill').fillna(method='bfill'))\n                mob_train.append(mob.interpolate(method='nearest').fillna(method='ffill').fillna(method='bfill'))\n                country_used.append(country)\n        except Exception as e:\n            errors.append(e)\n    print(\"Countries used for learning, \",country_used)\n    meas_train = pd.concat(meas_train)\n    mob_train = pd.concat(mob_train)\n\n    meas_test, mob_test = build_dataset(test_country, mobility, oxford_long)\n    mob_test = mob_test.apply(lambda x: np.log(1 + x), axis =1)\n    mob_test[mobility_category].plot()\n\n    plt.title(\"{} mobility level\".format(mobility_category))\n    plt.show()\n    print(\"Measures conventions are set according to\\nhttps:\/\/www.bsg.ox.ac.uk\/sites\/default\/files\/2020-05\/BSG-WP-2020-032-v5.0_0.pdf, Page 13\")\n    meas_test.plot()\n    plt.title(\"Measures taken by {}\".format(test_country))\n    plt.show()\n    \n    X_train, y_train = meas_train.to_numpy(), mob_train[mobility_category].to_numpy()\n    X_test, y_test = meas_test.interpolate(method='nearest').fillna(method='ffill').fillna(method='bfill').to_numpy(), mob_test[mobility_category].interpolate(method='nearest').fillna(method='ffill').fillna(method='bfill').to_numpy()\n\n    def linear_regression(X_train, y_train, X_test, y_test):\n        # Create linear regression object\n        ##regr = linear_model.LinearRegression()\n        regr = linear_model.Lasso(alpha=0.015)\n\n        # Train the model using the training sets\n        regr.fit(X_train, y_train)\n\n        # Make predictions using the testing set\n        y_pred = regr.predict(X_test)\n\n        # The coefficients\n        print('Coefficients: \\n', regr.coef_)\n        # The mean squared. rolling(error\n        print('Mean squared error: %.2f'\n           % mean_squared_error(y_test, y_pred))\n        # The coefficient of determination: 1 is perfect prediction\n        print('Coefficient of determination: %.2f'\n           % r2_score(y_test, y_pred))\n\n        # Plot outputs\n        plt.plot(np.exp(y_test)-1, color='blue', linewidth=1)\n        plt.plot(np.exp(y_pred)-1, color= 'red', linewidth=1)\n        plt.title(\"{} mobility level inference\".format(mobility_category))\n        plt.show()\n        return regr\n    return linear_regression(X_train, y_train, X_test, y_test), errors","55f3d953":"#First test on a country which implemented drastic measures, lockdown : Italy\n\ntest_country = 'ITA'\n\n# Choose wether to use your own list or all countries\nselected_countries =['FRA', 'ESP', 'DEU', 'GBR', 'DNK']\n\nmobility_category = 'transit'#'grocery_and_pharmacy'\nrun_linear_regression(mobility_category, test_country, selected_countries)","994a166a":"#First test on a country which did not implement drastic measures, no lockdown : Denmark\n\ntest_country = 'DEU'\n\n# Choose wether to use your own list or all countries\nselected_countries =['FRA', 'ESP', 'DNK', 'GBR','ITA']\n\nmobility_category = 'transit'#'grocery_and_pharmacy'\nrun_linear_regression(mobility_category, test_country, selected_countries)","191b62bf":"import numpy as np\nfrom tqdm import tqdm\nfrom dtaidistance import dtw\n\nall_countries = list(set(mobility.reset_index().iso_code) )\nmob_train = {}\nerrors = []\nfor country in all_countries : \n    try:\n        _, mob = build_dataset(country, mobility, oxford_long)\n        mob_train[country]=mob\n    except KeyError: \n        errors.append(country)\nprint(mob_train.keys())\n\nmob_train_selection = []\nmobility_category = 'transit'\nfor my_country in mob_train.keys():\n    if (mob_train[my_country][mobility_category].std()>0 and\n        mob_train[my_country][mobility_category].isnull().sum()<mob_train[my_country].shape[0]*0.2):\n        mob_train_selection.append(my_country)\n\nn=len(mob_train_selection)\nsimilarity_matrix = np.empty([n,n])\nfor i,first_country in tqdm(enumerate(mob_train_selection)):\n    for j,second_country in enumerate(mob_train_selection):\n        if j>=i:\n            first_mob = mob_train[first_country][mobility_category].interpolate(method='nearest').fillna(method='ffill').fillna(method='backfill')\n            second_mob = mob_train[second_country][mobility_category].interpolate(method='nearest').fillna(method='ffill').fillna(method='backfill')\n            distance = dtw.distance(first_mob,second_mob)\n            similarity_matrix[i][j],similarity_matrix[j][i] = distance, distance\n        \n","e3dec30e":"import seaborn as sns\ndf_similarity_matrix = pd.DataFrame(similarity_matrix, columns=mob_train_selection, index=mob_train_selection)\nfig, ax = plt.subplots(figsize=(10,8)) \nsns.heatmap(df_similarity_matrix)","2306d8de":"sns.clustermap(df_similarity_matrix)","338a84cd":"from sklearn import manifold\n\nmds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\nresults = mds.fit(df_similarity_matrix)","145d1170":"adist = df_similarity_matrix.to_numpy()\namax = np.amax(adist)\nadist \/= amax\n\nmds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\nresults = mds.fit(adist)\n\ncoords = results.embedding_\n\nplt.subplots_adjust(bottom = 0.1)\nplt.scatter(\n    coords[:, 0], coords[:, 1], marker = 'o'\n    )\nfor label, x, y in zip(df_similarity_matrix.columns, coords[:, 0], coords[:, 1]):\n    plt.annotate(\n        label,\n        xy = (x, y), xytext = (-20, 20),\n        textcoords = 'offset points', ha = 'right', va = 'bottom',\n        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n\nplt.show()","902fdbbb":"# %load reparameterizations.py\n\ndef reparametrize_beta(mean, sample_size):\n#     v = numpyro.sample(f'sample_size_{i}', dist.Gamma(*reparametrize_gamma(10., 5)))\n    alpha = mean * sample_size\n    beta = (1 - mean) * sample_size\n    return alpha, beta \n\ndef reparametrize_gamma(mean, std):\n    var = std ** 2\n    alpha = mean ** 2 \/ var\n    beta = mean \/ var\n    return alpha, beta\n","0bde9b65":"import jax.numpy as np","697dd248":"def sample_parameters(nb_measures, nb_mobilities):\n    a = numpyro.sample('a', dist.Normal(0., 0.2))\n    alpha = numpyro.sample('alpha', \n                           dist.Gamma(*reparametrize_gamma(1. \/ nb_measures, 1.)), \n                           sample_shape=(nb_measures, nb_mobilities))\n    alpha -= np.log(1.05) \/ nb_measures\n    return a, alpha","12bf7d41":"def multi_model(nb_measures, nb_mobilities, all_countries, all_measures, all_mobilities=None):\n    a, alpha = sample_parameters(nb_measures, nb_mobilities)\n    sigma = numpyro.sample(f'sigma', dist.Exponential(1.), sample_shape=(nb_mobilities,))\n    for i in range(len(all_countries)):\n        country = all_countries[i]\n        measures = all_measures[i]\n        mobility = all_mobilities[i] if all_mobilities is not None else None\n        \n        mu = np.exp(- np.dot(measures, alpha)) + a\n        numpyro.sample(f'y_{country}', dist.Normal(mu, sigma), obs=mobility)","2fb85223":"# oxford_long.reset_index()['countrycode'].unique()","0d026efb":"def make_all_datasets(mobility, oxford_long, country_names, selected_mobilities):\n    all_countries = []\n    all_mobilities = []\n    all_measures = []\n    all_times = []\n    for country, subset in oxford_long.groupby('countrycode'):\n        if country not in country_names:\n            continue\n        subset = subset.reset_index('countrycode', drop=True)\n        all_countries.append(country)\n        subset_measures, subset_mobility = build_dataset(country, mobility, oxford_long)\n        \n        subset_mobility = subset_mobility[selected_mobilities].add(1).rolling(7, center=True).mean().fillna(method='ffill').fillna(method='bfill')\n        all_mobilities.append(np.asarray(subset_mobility))\n        all_measures.append(np.asarray(subset_measures))\n        all_times.append(subset_mobility.index.to_numpy())\n    return all_countries, all_mobilities, all_measures, all_times\n\nSELECTED_COUNTRIES = ['FRA', 'ITA', 'ESP', 'DEU', 'SWE', 'GBR', 'DNK']\nSELECTED_MOBILITIES = ['transit', 'walking', 'driving', 'grocery_and_pharmacy', 'retail_and_recreation', 'workplaces', 'transit_stations']\nall_countries, all_mobilities, all_measures, all_times = make_all_datasets(mobility, oxford_long, SELECTED_COUNTRIES, SELECTED_MOBILITIES)","4e3d61ef":"for i, country in enumerate(all_countries):\n#     if country != 'DEU':\n#         continue\n    measures = all_measures[i]\n    subset_mobility = all_mobilities[i]\n    times = all_times[i]\n    pd.DataFrame(subset_mobility, columns=SELECTED_MOBILITIES, index=times).plot()\n    plt.title(country)","2a763ccb":"nb_measures = all_measures[0].shape[1]\nnb_mobilities = all_mobilities[0].shape[1]\n\nmcmc = MCMC(NUTS(multi_model, dense_mass=True), 500, 500, num_chains=1, progress_bar=True)\nmcmc.run(PRNGKey(0), \n         nb_measures, nb_mobilities,\n         all_countries, all_measures, all_mobilities)\nmcmc.print_summary()","947d68f5":"def plot_mobility_results(mu, pi, y_true, times):\n    plt.plot(times, y_true, \"bx\", label=\"true\")\n    plt.plot(times, mu, \"b--\", label=\"pred\")\n    plt.fill_between(times, pi[0, :], pi[1, :], color=\"b\", alpha=0.3)\n    plt.legend()\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.title('Daily deaths')\n\ndef plot_y_pred(y_pred, data, times, country):\n    mu = np.mean(y_pred, 0)\n    pi = np.percentile(y_pred, (10., 90.), 0)\n    for i in range(data.shape[1]):\n        fig, ax = plt.subplots(figsize=(8, 5))\n        plot_mobility_results(mu[:, i], pi[:, :, i], data[:, i], times)\n        plt.title(SELECTED_MOBILITIES[i] + ' ' + country)","8131a62e":"predictive = Predictive(multi_model, mcmc.get_samples())\npred = predictive(PRNGKey(0), \n           nb_measures, nb_mobilities, \n           all_countries, all_measures)\n\nfor i, country in enumerate(all_countries):\n    if country != 'DEU':\n        continue\n    y_pred = pred[f'y_{country}']\n    times = all_times[i]\n    subset_mobility = all_mobilities[i]\n    plot_y_pred(y_pred, subset_mobility, times, country)","ea3ec431":"%%latex\n\\begin{align}\n\\frac{dS}{dt} &= - \\frac{R_t}{T_{inf}} I S \\\\\n\\frac{dE}{dt} &= \\frac{R_t}{T_{inf}} I S - \\frac{E}{T_{inc}} \\\\\n\\frac{dI}{dt} &= \\frac{E}{T_{inc}} - \\frac{I}{T_{inf}} \\\\\n\\frac{dR}{dt} &= m_a \\frac{I}{T_{inf}} + (1 - c_a)\\frac{H}{T_{hosp}} \\\\\n\\frac{dH}{dt} &= (1 - m_a) \\frac{I}{T_{inf}} + (1 - f_a)\\frac{C}{T_{crit}} - \\frac{H}{T_{hosp}} \\\\\n\\frac{dC}{dt} &= c_a \\frac{H}{T_{hosp}} - \\frac{C}{T_{crit}} \\\\\n\\frac{dD}{dt} &= f_a \\frac{C}{T_{crit}}\n\\end{align}","24651561":"# %load ode.py\ndef build_my_odeint(mobility_data, rtol=1e-5, atol=1e-9, mxstep=500):\n    \"\"\"\n    code based on jax.experimental.ode.build_ode to make it work with mobility data\n    \"\"\"\n    def dz_dt(z, t, r0, r1, t_inc, t_inf, t_hosp, t_crit, m_a, c_a, f_a, *alpha):\n        s = z[0]\n        e = z[1]\n        i = z[2]\n        r = z[3]\n        h = z[4]\n        c = z[5]\n        \n        alpha_ = np.array(alpha)\n        int_t = np.array([t]).astype(int)[0]\n        rt_u = r0 * (1 + mobility_data[int_t]) - r1 * mobility_data[int_t]\n        rt = np.dot(rt_u, alpha_)\n#         rt = alpha0 * rt_u[0] + alpha1 * rt_u[1]\n        \n        ds = - (rt \/ t_inf) * i * s\n        de = (rt \/ t_inf) * i * s - (e \/ t_inc)\n        di = e \/ t_inc - i \/ t_inf\n        dr = m_a * i \/ t_inf + (1 - c_a) * (h \/ t_hosp)\n        dh = (1 - m_a) * (i \/ t_inf) + (1 - f_a) * c \/ t_crit - h \/ t_hosp\n        dc = c_a * h \/ t_hosp - c \/ t_crit\n        dd = f_a * c \/ t_crit\n\n        return np.stack([ds, de, di, dr, dh, dc, dd])\n\n    ct_odeint = jax.custom_transforms(\n        lambda y0, t, *args: odeint(dz_dt, y0, t, *args, rtol=rtol, atol=atol, mxstep=mxstep))\n\n    v = lambda y0, t, *args: vjp_odeint(dz_dt, y0, t, *args, rtol=rtol, atol=atol, mxstep=mxstep)\n\n    jax.defvjp_all(ct_odeint, v)\n\n    return jax.jit(ct_odeint)\n","d4d7230d":"# %load prior_means.py\nParams = namedtuple('Params', \n                    ['r0', 'r1', \n                     't_inc', 't_inf', 't_hosp', 't_crit', \n                     'm_a', 'c_a', 'f_a']\n                   )\n\nPRIOR_MEANS = Params(\n    r0=3.28,\n    r1=0.2, \n    t_inc=5.6, \n    t_inf=7.9, \n    t_hosp=4., \n    t_crit=14., \n    m_a=0.8, \n    c_a=0.1, \n    f_a=0.35)\n","175443a8":"# %load model.py\n\ndef make_target_dist(psi_h, psi_c, psi, \n                     daily_hosp, daily_critical, daily_deaths, \n                     bump_hosp, bump_critical, N):\n    \n    target_dist = dist.GammaPoisson(\n                   np.array([psi_h, psi_c, psi]),\n                   rate=np.stack([psi_h \/ (daily_hosp + bump_hosp), \n                                  psi_c \/ (daily_critical + bump_critical), \n                                  psi \/ daily_deaths]\n                                ).T\n    )\n\n    reloc = dist.transforms.AffineTransform(\n        loc=np.stack([\n            - bump_hosp,\n            - bump_critical,\n            np.zeros(N)]\n        ).T,\n        scale=1.)\n\n    reloc_target_dist = dist.TransformedDistribution(target_dist, [reloc])\n    return reloc_target_dist\n\ndef sample_parameters(nb_mobilities):\n    kappa0 = numpyro.sample('kappa0', dist.TruncatedNormal(0, 0., 0.5))\n    kappa1 = numpyro.sample('kappa1', dist.TruncatedNormal(0, 0, 0.5))\n    r0 = numpyro.sample('r0', dist.TruncatedNormal(0, PRIOR_MEANS.r0, kappa0))\n    r1 = numpyro.sample('r1', dist.TruncatedNormal(0, PRIOR_MEANS.r1, kappa1))\n\n    alpha = numpyro.sample('alpha', dist.Gamma(*reparametrize_gamma(1., 0.5)), sample_shape=(nb_mobilities,))\n    alpha \/= np.sum(alpha)\n    \n    t_inc = numpyro.sample('t_inc', dist.Gamma(*reparametrize_gamma(PRIOR_MEANS.t_inc, .86)))\n    t_inf = numpyro.sample('t_inf', dist.Gamma(*reparametrize_gamma(PRIOR_MEANS.t_inf, 3.)))\n    t_hosp = numpyro.sample('t_hosp', dist.Gamma(*reparametrize_gamma(PRIOR_MEANS.t_hosp, 3.)))\n    t_crit = numpyro.sample('t_crit', dist.Gamma(*reparametrize_gamma(PRIOR_MEANS.t_crit, 3.)))\n    \n    sample_size_m = numpyro.sample('sample_size_m', dist.Gamma(*reparametrize_gamma(7., 2)))\n    sample_size_c = numpyro.sample('sample_size_c', dist.Gamma(*reparametrize_gamma(7., 2)))\n    sample_size_f = numpyro.sample('sample_size_f', dist.Gamma(*reparametrize_gamma(7., 2)))\n    m_a = numpyro.sample('m_a', dist.Beta(*reparametrize_beta(PRIOR_MEANS.m_a, sample_size_m)))\n#     m_a = 0.8\n    c_a = numpyro.sample('c_a', dist.Beta(*reparametrize_beta(PRIOR_MEANS.c_a, sample_size_c)))\n    f_a = numpyro.sample('f_a', dist.Beta(*reparametrize_beta(PRIOR_MEANS.f_a, sample_size_f)))\n    \n    params = (r0, r1, t_inc, t_inf, t_hosp, t_crit, m_a, c_a, f_a)\n    \n    return params, alpha\n    \n\ndef sample_compartment_init(pop_country, country_name=None):\n#     tau = numpyro.sample('tau', dist.Exponential(0.03))\n#     kappa_i0 = numpyro.sample('kappa_i0', dist.TruncatedNormal(0, 0., 0.5))\n    i_init = numpyro.sample(f'i_init_{country_name}', \n                            dist.TruncatedNormal(loc=50., scale=10.)\n#                             dist.Gamma(*reparametrize_gamma(50, 10.))\n#                             dist.Exponential(1. \/ tau)\n                           )\n    i_init \/= pop_country\n    z_init = np.array([1. - i_init, 0., i_init, 0., 0., 0., 0.])\n    return z_init\n    \ndef model(seirhcd_int, N, pop_country, y=None, compartments='d', nb_mobilities=1):\n    ts = np.arange(float(N))\n    params, alpha = sample_parameters(nb_mobilities=nb_mobilities)\n    z_init = sample_compartment_init(pop_country)\n    \n    z = seirhcd_int(z_init, ts, *params, *alpha)\n    \n    daily_deaths = diff_pop(z[:, -1], pop_country)\n    psi = numpyro.sample('psi', dist.TruncatedNormal(scale=5.))\n\n    if compartments == 'd':\n        numpyro.sample('deceased', dist.GammaPoisson(psi, rate=psi \/ daily_deaths), obs=y)\n\n    elif compartments == 'hcd':\n        daily_hosp = diff_pop(z[:, -3], pop_country)\n        daily_critical = diff_pop(z[:, -2], pop_country)\n\n        hosp = z[:, -3] * pop_country\n        critical = z[:, -2] * pop_country\n\n        hosp_m1 = np.hstack(([0.],  hosp[:-1]))\n        critical_m1 = np.hstack(([0.],  critical[:-1]))\n\n        bump_hosp = np.min(np.stack([hosp_m1, BUMP_HOSP * np.ones(N)]), axis=0)\n        bump_critical = np.min(np.stack([critical_m1, BUMP_CRITICAL * np.ones(N)]), axis=0)\n\n        psi_h = numpyro.sample('psi_h', dist.TruncatedNormal(scale=5.))\n        psi_c = numpyro.sample('psi_c', dist.TruncatedNormal(scale=5.))\n        \n        target_dist = make_target_dist(psi_h, psi_c, psi, \n                                       daily_hosp, daily_critical, daily_deaths, \n                                       bump_hosp, bump_critical, N)\n        \n        numpyro.sample('y', target_dist, obs=y)\n\ndef multi_model(all_countries, all_mobilities, all_populations, observations=None):\n    nb_mobilities = all_mobilities[0].shape[1]\n    params, alpha = sample_parameters(nb_mobilities=nb_mobilities)\n    psi = numpyro.sample('psi', dist.TruncatedNormal(scale=5.))\n    i_init = numpyro.sample('i_init', dist.TruncatedNormal(loc=50, scale=10))\n    \n    for i in range(len(all_countries)):\n        country = all_countries[i]\n        mobility_data = all_mobilities[i]\n        seirhcd_int = build_my_odeint(mobility_data)\n        pop_country = all_populations[i]\n        if observations is not None:\n            y = observations[i]\n        else:\n            y = None\n        \n        ts = np.arange(float(mobility_data.shape[0]))\n\n        z_init = np.array([1. - i_init \/ pop_country, 0., i_init \/ pop_country, 0., 0., 0., 0.])\n        z = seirhcd_int(z_init, ts, *params, *alpha)\n\n        daily_deaths = diff_pop(z[:, -1], pop_country)\n\n        numpyro.sample('deceased_' + country, dist.GammaPoisson(psi, rate=psi \/ daily_deaths), obs=y)","9121ed60":"# %load postprocess.py\n\ndef compute_mu_pi_2(y_pred):\n    pop_pred = np.stack([y_pred['hosp'], y_pred['critical'], y_pred['deceased']]).T\n    mu = np.mean(pop_pred, 1)\n    pi = np.percentile(pop_pred.astype(float), (10., 90.), 1)\n    return mu, pi\n\ndef compute_mu_pi(y_pred, observation_name='y'):\n    pop_pred = y_pred[observation_name]\n    mu = np.mean(pop_pred, 0)\n    pi = np.percentile(pop_pred.astype(float), (10., 90.), 0)\n    return mu, pi\n\ndef plot_compartment_results(mu, y_true, times, pi=None):\n    plt.plot(times, y_true, \"bx\", label=\"true\")\n    plt.plot(times, mu, \"b--\", label=\"pred\")\n    if pi is not None:\n        plt.fill_between(times, pi[0, :], pi[1, :], color=\"b\", alpha=0.3)\n    plt.legend()\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.title('Daily deaths')\n\ndef plot_daily_cumulated(mu, pi, data, times, name):\n    fig, ax = plt.subplots(figsize=(8, 5))\n    plot_compartment_results(mu, data, times, pi)\n    plt.title('Daily ' + name)\n\n    fig, ax = plt.subplots(figsize=(8, 5))\n    plot_compartment_results(np.cumsum(mu), np.cumsum(data), times)\n    plt.title('Cumulated ' + name)\n\n# # Control for HCD params\ndef plot_hcd_results(mu, pi, data, times):\n    for i, name in enumerate(['deaths', 'critical', 'hospitalized'], start=1):\n        plot_daily_cumulated(mu[:, -i], pi[:, :, -i], data[:, -i], times, name)\n        \ndef plot_forest(inference_data):\n    az.plot_forest(inference_data, var_names=['t_inc', 't_inf', 't_hosp', 't_crit'], \n                   kind='forestplot', ridgeplot_overlap=3, combined=True, figsize=(9, 3))\n    plt.grid()\n    az.plot_forest(inference_data, var_names=['m_a', 'c_a', 'f_a'], figsize=(9, 2))\n    plt.grid()\n    az.plot_forest(inference_data, var_names=['r0', 'r1'], figsize=(9, 2))\n    plt.grid()\n    az.plot_forest(inference_data, var_names=['alpha'], figsize=(9, 4))\n    plt.grid()\n    az.plot_forest(inference_data, var_names=['i_init'], figsize=(9, 1))\n","08877bdd":"# %load train_test_split.py\n\ndef indexslice(l, ix):\n    for i, item in enumerate(l):\n        if i in ix:\n            yield item\n\ndef mask_ix(l, mask):\n    def aux(l, mask):\n        for item, bl in zip(l, mask):\n            if bl:\n                yield item\n    return list(aux(l, mask))\n\ndef split_train_test(countries_train, countries_test, all_countries):\n    mask_train = [c in countries_train for c in all_countries]\n    mask_test = [c in countries_test for c in all_countries]\n    return mask_train, mask_test\n\nlist(indexslice([1, 2, 3], [0, 2]))\nmask_ix([1, 2, 3], [True, False, False]) ;\n","5def85f3":"isocodes = fetch_isocodes()\nacaps = fetch_acaps(isocodes)\necdc = fetch_ecdc().set_index(['iso_code', 'date'])\nlocation_code = ecdc.groupby(['location', 'iso_code']).first().iloc[:, 0].reset_index().iloc[:,:2]\napple = fetch_apple(location_code)\ngoogle = fetch_google(isocodes, location_code)\nmobility = google.join(apple, how='outer')\npopulations_country = (ecdc.assign(population = lambda f: \n                                   f['new_cases']\n                                   .div(f['new_cases_per_million'])\n                                   .mul(1_000_000))\n                       .groupby('iso_code')\n                       ['population']\n                       .last())\n\ncountry_code_lookup = ecdc.reset_index()[['iso_code', 'location']].drop_duplicates().set_index('location').iloc[:, 0]\ncountry_name_lookup = country_code_lookup.reset_index().set_index('iso_code').iloc[:, 0]","df93fad7":"mobility_categories = [\n        'transit', 'walking', 'driving', \n    'grocery_and_pharmacy', 'retail_and_recreation', 'workplaces', 'transit_stations']\n","c26585cf":"# %load model\/preprocessing.py\n\nix = pd.IndexSlice\n\ndef diff_pop(cumulative, pop_country):\n    daily = np.hstack((np.array([0.]), np.diff(cumulative * pop_country)))\n    daily += 1\n    return daily\n\ndef make_dataset(mobility, ecdc, days_before_deaths):\n    deaths_subset = ecdc['total_deaths']\n    \n    ten_deaths_date = deaths_subset.gt(10).idxmax()\n    begin_date = ten_deaths_date - pd.Timedelta(days_before_deaths, unit='days')\n\n    total_deaths = deaths_subset.loc[begin_date:].to_numpy()\n    times = deaths_subset.loc[begin_date:].index.map(onp.datetime64).to_numpy()\n\n    mobility_subset = mobility[mobility_categories]\n\n    mobility_subset = (mobility_subset\n                .reindex(deaths_subset.loc[begin_date:].index)\n                .fillna(method='ffill')\n                .fillna(method='bfill')\n                #.fillna(mobility.iloc[-1])\n                .rolling('7d').mean())\n    \n    mobility_data = np.asarray(mobility_subset.to_numpy())\n    return total_deaths, times, mobility_data\n\ndef make_all_datasets(mobility, ecdc, populations_country, country_names, days_before_deaths):\n    all_countries = []\n    all_populations = []\n    all_mobilities = []\n    all_deaths = []\n    all_times = []\n    for country, subset in ecdc.groupby('iso_code'):\n        if country not in country_names:\n            continue\n        subset = subset.reset_index('iso_code', drop=True)\n        all_countries.append(country)\n#         try:\n        total_deaths, times, mobility_data = make_dataset(mobility.loc[country], subset, days_before_deaths)\n#         except KeyError:\n#             continue\n        pop_country = populations_country.loc[country]\n        daily_deaths = diff_pop(total_deaths \/ pop_country, pop_country)\n        all_populations.append(pop_country)\n        all_mobilities.append(mobility_data)\n        all_deaths.append(daily_deaths)\n        all_times.append(times)\n    return all_countries, all_populations, all_mobilities, all_deaths, all_times\n","0b57b9ad":"SELECTED_COUNTRIES = [country_code_lookup.loc[name] \n                      for name in \n                      ('Denmark', 'Sweden', 'France', 'Germany', 'United Kingdom', \n                       'Spain', 'Italy'\n                      )\n                     ]\n\nNB_DAYS_BEFORE_TEN_DEATHS = 30\n\nall_countries, all_populations, all_mobilities, all_deaths, all_times = make_all_datasets(mobility,\n                                                                                          ecdc,\n                                                                                          populations_country, \n                                                                                          SELECTED_COUNTRIES, \n                                                                                          NB_DAYS_BEFORE_TEN_DEATHS)","1da4cda1":"for i, country in enumerate(all_countries):\n    pop = all_populations[i]\n    mobility_data = all_mobilities[i]\n    total_deaths = all_deaths[i]\n    times = all_times[i]\n    ax = pd.DataFrame(total_deaths).set_index(times).plot(legend=False, figsize=(8, 5))\n    pd.DataFrame(mobility_data, columns=mobility_categories).set_index(times).plot(ax=ax, secondary_y=True, legend=False)\n    plt.title('Deaths and mobility - ' + country_name_lookup.loc[country])\n    plt.legend(loc='lower left')","d56609d1":"mask_train, mask_test = split_train_test([country_code_lookup.loc[name] for name in ['Spain', 'Italy', 'Germany', 'United Kingdom']], \n                                         [country_code_lookup.loc[name] for name in ['Sweden']], \n                                         all_countries)\n# print(mask_train, mask_test)\nprint('train countries:', mask_ix(all_countries, mask_train))\nprint('test countries:', mask_ix(all_countries, mask_test))","afaf47af":"mcmc_multi = MCMC(NUTS(multi_model, dense_mass=True), 200, 200, num_chains=NUM_CHAINS)\n\nmcmc_multi.run(PRNGKey(0), \n               all_countries=mask_ix(all_countries, mask_train),\n               all_mobilities=mask_ix(all_mobilities, mask_train),\n               all_populations=mask_ix(all_populations, mask_train),\n              observations=mask_ix(all_deaths, mask_train),\n              )\n\nmcmc_multi.print_summary()","c49ce57f":"# %load rt_mobility.py\n\ndef reorder(d, mask_train, mask_teset):\n    return mask_ix(d, mask_train) + mask_ix(d, mask_test)\n\ndef compute_mu_pi_3(data):\n    mu = np.mean(data, axis=0)\n    pi = np.percentile(data, (10, 90), axis=0)\n    return mu, pi\n\ndef compute_rt(ts, mobility_data, r0, r1, alpha):    \n    rt_u = r0 * (1 + mobility_data[ts]) - r1 * mobility_data[ts]\n    rt = np.dot(rt_u, alpha)\n    return rt\n\ndef compute_rt_samples(samples, mobility_data, N):\n    corr_samples = {**samples, **{'alpha': samples['alpha'] \/ samples['alpha'].sum(axis=1)[:, np.newaxis]}}\n    mu, pi = dict(), dict()\n    for name in 'r0', 'r1', 'alpha':\n        mu[name], pi[name] = compute_mu_pi_3(corr_samples[name])\n\n    ts = np.arange(N)\n\n    rt_pi = onp.empty((corr_samples['r0'].shape[0], times.shape[0]))\n    for i in range(corr_samples['r0'].shape[0]):\n        rt_pi[i] = compute_rt(ts, mobility_data, \n                              corr_samples['r0'][i], \n                              corr_samples['r1'][i], \n                              corr_samples['alpha'][i])\n    \n    return rt_pi\n","fd1a9e5e":"samples = mcmc_multi.get_samples()\npredictive = Predictive(multi_model, samples)\n\ny_pred = predictive(PRNGKey(2), \n                    all_countries=reorder(all_countries, mask_train, mask_test),\n                    all_mobilities=reorder(all_mobilities, mask_train, mask_test), \n                    all_populations=reorder(all_populations, mask_train, mask_test)\n                   )","f1b91265":"for i, country in enumerate(reorder(all_countries, mask_train, mask_test)):\n    mu, pi = compute_mu_pi(y_pred, f'deceased_{country}')\n    plot_daily_cumulated(mu, pi, reorder(all_deaths, mask_train, mask_test)[i], reorder(all_times, mask_train, mask_test)[i], \n                         f'deaths - {country}')\n    mobility_data = reorder(all_mobilities, mask_train, mask_test)[i]\n    times = reorder(all_times, mask_train, mask_test)[i]\n    rt_pi = compute_rt_samples(samples, mobility_data, times.shape[0])    \n    mu, pi = compute_mu_pi_3(rt_pi)\n    \n    fig, ax = plt.subplots(figsize=(8, 5))\n    plt.plot(times, mu)\n    plt.xticks(rotation=45)\n    plt.fill_between(times, pi[0], pi[1], alpha=0.3, interpolate=True)\n    plt.title(f'Rt - {country}')\n    \n    plt.tight_layout()","a862b113":"inference_data = az.from_numpyro(mcmc_multi)\nplot_forest(inference_data)","01365563":"# %load compartments.py\n\ndef run_sim_samples(integrator, samples, N, pop_country):\n    ts = np.arange(float(N))\n    res = []\n    for i in range(samples['c_a'].shape[0]):\n        post_params = dict()\n        for param in 'r0, r1, t_inc, t_inf, t_hosp, t_crit, m_a, c_a, f_a, alpha'.split(', '):\n            post_params[param] = samples[param][i]\n    #         post_params[param] = inference_data.posterior[param].values[0, -1]\n\n        i_init = samples['i_init'][i]\n        i_init \/= pop_country\n        z_init = np.array([1. - i_init, 0., i_init, 0., 0., 0., 0.])\n        args = list(post_params.values())[:-1]\n\n        alpha = post_params['alpha']\n        alpha \/= np.sum(alpha)\n        \n        sim_res = integrator(z_init, ts, *args, *alpha)\n        res.append(sim_res)\n\n    res = np.stack(res)\n    return res\n\ndef plot_compartment(pred_data, true_data, pop_country, times):\n    pi = np.percentile(pred_data, (10., 90.), 0)\n    \n    plt.plot(onp.asarray(times), np.mean(pred_data, axis=0) * pop_country, label='pred')\n#     plt.plot(times, true_data, label='true')\n    plt.fill_between(onp.asarray(times), pi[0, :] * pop_country, pi[1, :] * pop_country, interpolate=True, alpha=0.3)\n    plt.legend()\n\ndef plot_hcd(res, pop_country, times, title=None):\n    for i, name in enumerate(['hospitalized', 'critical', 'deceased' ], start=5):\n        plt.subplots()\n#         plot_compartment(res[:, :, -i], hosp_indexed[name].to_numpy(), pop_country)\n        plot_compartment(res[:, :, i], np.zeros(res.shape[1]), pop_country, times)\n        plt.title(name + ' - ' + title)\n    \ndef plot_seir(res, pop_country, times, title=None):\n    for i, name in enumerate(['susceptible', 'exposed', 'infected', 'recovered']):\n        plt.subplots()\n        plot_compartment(res[:, :, i], np.zeros(res.shape[1]), pop_country, times)\n        plt.title(name + ' - ' + title)\n","21ec7019":"for i, country_code in enumerate(reorder(all_countries, mask_train, mask_test)):\n    if country_code != 'ESP':\n        continue\n    all_times_reordered = reorder(all_times, mask_train, mask_test)\n    seirhcd_int = build_my_odeint(reorder(all_mobilities, mask_train, mask_test)[i])\n    res = run_sim_samples(seirhcd_int, samples, \n                          all_times_reordered[i].shape[0], \n                          reorder(all_populations, mask_train, mask_test)[i])\n    plot_seir(res, reorder(all_populations, mask_train, mask_test)[i], all_times_reordered[i], title=country_code)\n    plot_hcd(res, reorder(all_populations, mask_train, mask_test)[i], all_times_reordered[i], title=country_code)","58cd0895":"### Measures description","b2485a1c":"We leverage the _Coronavirus Government Response Tracker_ dataset from Oxford Blavatnik School of Government.\n\nWe are interested in the _Closure and containment_ indicators, which are described in the following section. \nNote that when no measure is in place, the indicator is at zero.","cd40bd01":"### Other priors\n    \n$R_0 \\sim \\mathcal{N}^+(3.28, \\kappa_0)$\n\n$R_1 \\sim \\mathcal{N}^+(0.2, \\kappa_1)$\n\nwhere $\\kappa_0, \\kappa_1 \\sim \\mathcal{N}^+(0, .5)$\n\n\n$T_{inc} \\sim \\mathrm{Gamma}(5.6, .86)$\n\n$T_{inf} \\sim \\mathrm{Gamma}(7.9, 3)$\n\n$T_{hosp} \\sim \\mathrm{Gamma}(4, 3)$\n\n$T_{crit} \\sim \\mathrm{Gamma}(14, 3)$\n\n$m_a \\sim \\mathrm{Beta}(0.8, \\phi_m)$\n\n$c_a \\sim \\mathrm{Beta}(0.1, \\phi_c)$\n\n$f_a \\sim \\mathrm{Beta}(0.35, \\phi_f)$\n\nwhere $\\phi_m, \\phi_c, \\phi_f \\sim \\mathrm{Gamma}(7, 2)$","8963d5a9":"## Data fetching","aae5ce39":"## Inference settings\n\nWe use a multi-country setting where we train the model on a pool of countries and then predict the number of deceased individuals, as well as $R_t$, on another country.\n\nIn order to do this, the model has to learn country-independent parameters, since the only varying factor in countries is the mobility.","c0efd99b":"# Effective reproduction number and death modelling","38f5d228":"<!-- # Related work\n\nRegarding Covid-19 specifically, [Flaxman, Mishra, Gansy, Bhatt et al.](https:\/\/www.imperial.ac.uk\/mrc-global-infectious-disease-analysis\/covid-19\/report-13-europe-npi-impact\/) develop a a semi-mechanistic Bayesian hierarchical model to estimate the impact of interventions on 11 European countries, which they later tweak to [estimate](https:\/\/www.imperial.ac.uk\/mrc-global-infectious-disease-analysis\/covid-19\/report-20-italy\/) the transmission intensity in Italy. [Bryant and Elofsson](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.04.13.20063644v2) enrich the original model with Google mobility datasets.\n\n## Mobility\nThe release of mobility datasets provided by Google and Apple is, to our knowledge, a novelty caused by the severity of the Covid-19 pandemic. Hence, related works leveraging mobility data are mostly recent.\n\nPrior work done on mobility data include an [evaluation](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.04.05.20054288v2) of the effect of a urban mobility index on the virus growth rate, [synthetic intervention](https:\/\/arxiv.org\/abs\/2005.00072) -->","87bf523f":"# Estimating the impact of non-pharmaceutical interventions on COVID-19 infection spread through mobility data\n\nWe aim at answering the question :\n\n**How does is the implementation of different strategies affecting the rates of COVID-19 infection ?**","eb9fe9b0":"#### Mobility, a good proxy for measures\nThe intuition we had, working on this problem, was that mobility could be a useful proxy for measures. In order to verify this hypothesis we decided to take a few approaches. \n- First we compared the mobility timeseries to see if countries with similar policies have similar mobilities\n- Then we learn the relationship between measures and mobility through bayesian inference. \n<!-- or through linear regression -->\n","fe225354":"## Parameterization","0239fb88":"# Conclusion\nWe have shown that there is a relationship between intervention and mobility. This relationship is better seen in countries that have implemented strong policies, as one might expect but can also be detected in countries like Germany and Denmark, which have avoided strong lockdowns. We have performed various regressions to estimate the mobility based on taken measures to showcase this relationship. With the same purpose, we analysed the countries' mobility clusters and found out that the clusters seem to be based on types of policies.\n\nWe then leveraged mobility data in order to estimate the effective reproduction number $R_t$ through time in some european countries that have been struck by the pandemic.\nThe $R_t$ estimator with fixed parameters, i.e. without any country-specific effect, has limitations however. While regularizing the model, this feature puts much constraint and prevents it from learning on a vast pool of countries. Additional degrees of freedom could be implemented in the model to correct this behavior.\n\nAdditionally, there are other effects on $R_t$ that are not taken into account with mobility: widespread testing and isolation of indivuals policy, as well as masks-related policies. These effects will strengthen in the months to come.\n\nBy combining the two approaches we have taken - the first model feeding the second one - one could generate what-if scenarii. These can either look back on the past, or look into the future, in order to assist policiy makers, typically on measures that aim at relieving countries from lockdown.","1430e64d":"# Datasets","2b32277c":"# Summary\nThrough the Covid-19 pandemic, many countries have adopted restriction policies, which have had a direct impact on the population's mobility. Major technology companies have released mobility datasets, that describe theses mobility changes, relative to a baseline, across different categories of places or modes of transportation.\n\nCan mobility enable us to predict the effective reproduction number and thus the infectiosity? Is it possible for governments to forsee the mobility changes induced by their measures and use these predicted mobility levels to anticipate death rate?\n\nWith this approach in mind we detected clustered countries based on the mobility data, revealing patterns and similarities between government measures across different countries. We built models, which predict mobility changes based on non-pharmaceutical interventions (NPIs).\nAdditionally, we built a bayesian model, that leverages an epidemiological compartmental model, in order to estimate the real effective reproduction number $R_t$ through time as a function of mobility.\n\nThis opens the possibility to build an end-to-end pipeline from NPIs to  \ud835\udc45\ud835\udc61  and compartments, e.g. number of hospitalized, critical and deceased individuals, in order to generate what-if scenarios, useful to evaluate governement policies both restrospectively and prospectively","dde11afb":"## Parameterization\nWe use the following parameterizations:\n- (concentration, rate) for the $\\mathrm{GammaPoisson}$ distribution.\n- (mean, std) for the $\\mathrm{Gamma}$ distribution\n- (mean, sample size) for the $\\mathrm{Beta}$ distribution","6d2e2819":"We leverage the _numpyro_ probabilistic programming framework to perform bayesian inference tasks.","a676677a":"## Bayesian regression","b4b8b992":"# Sources\n\n- Imperial team\n    - [Report 13](https:\/\/www.imperial.ac.uk\/mrc-global-infectious-disease-analysis\/covid-19\/report-13-europe-npi-impact\/): 30 March 2020\n    - [Technical update](https:\/\/arxiv.org\/abs\/2004.11342): 23 april 2020\n    - [Code](https:\/\/github.com\/ImperialCollegeLondon\/covid19model) in R and Stan\n    - [website](https:\/\/mrc-ide.github.io\/covid19estimates\/#\/)\n\n\n- R0: [An article estimating it](https:\/\/academic.oup.com\/jtm\/article\/27\/2\/taaa021\/5735319) 13 Feb 2020\n\n\n- Compartment models\n    - [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Compartmental_models_in_epidemiology)\n    - SEIR-HCD [Kaggle kernel](https:\/\/www.kaggle.com\/anjum48\/seir-hcd-model) from @anjum (Datasaurus). Sources of inspiration:\n        - [Neher labs](https:\/\/covid19-scenarios.org\/) \n        - [Gabriel Goh](https:\/\/gabgoh.github.io\/COVID\/index.html) \n\n\n- Probabilistic programming\n    - [numpyro](https:\/\/github.com\/pyro-ppl\/numpyro) ([paper](https:\/\/arxiv.org\/abs\/1912.11554)): probabilistic programming library in python, built on [JAX](https:\/\/github.com\/google\/jax)\n    - [SEIR ode modelling](https:\/\/docs.pymc.io\/notebooks\/ODE_API_introduction.html) in Pymc3 \n    - [Predator-Prey ode](http:\/\/pyro.ai\/numpyro\/examples\/ode.html) in Numpyro \n    - Another [predator-prey](https:\/\/fehiepsi.github.io\/rethinking-numpyro\/16-generalized-linear-madness.html) ode in numpyro\n    - [A bayesian SIR](https:\/\/github.com\/twiecki\/covid19) modelling with Pymc3 with the [video](https:\/\/youtu.be\/C1kWBTj6KvE?t=410)\n    - [Estimating COVID-19's $R_t$ in Real-Time with PYMC3](https:\/\/github.com\/k-sys\/covid-19\/blob\/master\/Realtime%20Rt%20mcmc.ipynb)\n\n\n- Miscellanous\n    - [generation interval](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.03.05.20031815v1)\n    - [masks](https:\/\/www.preprints.org\/manuscript\/202004.0203\/v1)\n\n*Data sources*\n\n- Epidemiology\n    - [Our world in data](https:\/\/github.com\/owid\/covid-19-data\/tree\/master\/public\/data): confirmed cases and deaths from ECDC (European center of Disease Control) + tests (collected by owid)\n\n\n- Mobility\n    - [Apple](https:\/\/www.apple.com\/covid19\/mobility) - 3 categories: walking, driving, transit\n    - [Google](https:\/\/www.google.com\/covid19\/mobility\/) - 6 categories\n\n\n- Government Measures\n<!--     - [ACAPS](https:\/\/data.humdata.org\/dataset\/acaps-covid19-government-measures-dataset) -->\n    - [Oxford](https:\/\/www.bsg.ox.ac.uk\/research\/research-projects\/coronavirus-government-response-tracker)\n","548da09b":"#### Similarity Analysis\n> We see some interesting strong similarities between countries. Like Italy and France that have been hit hard by Covid-19 and enforced a total lockdown. But how can we visualize these typologies better?","e138dba9":"### Compartment initialization\nWe seed the model with the following, assuming $N$ is the country population:\n- $NI_0 \\sim \\mathcal{N}^+(50, 10)$\n- $S_0 = 1 - I_0$\n- $E_0, R_0, H_0, C_0, D_0 = 0$","c129e675":"## Mobility datasets\nWe gathered data from Google and Apple, describing respectively 6 and 3 mobility categories.\n\nGoogle\n- _Retail and recreation_\n    - restaurants, cafes, shopping centers, theme parks, museums, libraries, and movie theaters.\n- _Grocery and pharmacy_\n    - grocery markets, food warehouses, farmers markets, specialty food shops, drug stores, and pharmacies.\n- _Parks_\n    - national parks, public beaches, marinas, dog parks, plazas,and public gardens\n- _Transit stations_\n    - public transport hubs such as subway, bus, and train stations\n- _Workplaces_\n- _Residential_\n\n_The baseline is the median value, for the corresponding day of the week, during the 5-week period Jan 3\u2013Feb 6, 2020._\n\nApple\n- _Driving_\n- _Walking_\n- _Transit_\n\n_relative volume of directions requests per country\/region, sub-region or city compared to a baseline volume on 13 January 2020_","aea3f803":"We display the mean of predictions along with the 10% and 90% quantiles.   \n\nWe train the model simultaneously on Spain (ESP), Italy (ITA), Germany (DEU), United Kingdom (GBR), and test on Sweden (SWE).","3e570bf3":"## Parameters used in the model\n$R_t$ is the effective reproduction number at time t.  \n\nTransition times\n\n    T_inc = average incubation period.\n    T_inf = average infectious period.\n    T_hosp = average time a patient is in hospital before either recovering or becoming critical.\n    T_crit = average time a patient is in a critical state (either recover or die).\n\nFractions\n\n    m_a = fraction of infections that are asymptomatic or mild.\n    c_a = fraction of severe cases that turn critical. \n    f_a = fraction of critical cases that are fatal.\n\n### Priors for these parameters\n- The Conversation [article](https:\/\/theconversation.com\/how-long-are-you-infectious-when-you-have-coronavirus-135295):\n    - Incubation: 5 days\n    - Infectious period: 2 days in incubation, 6 days from symptoms onwards\n    - Illness: 10 days\n    - NB: _For COVID-19, there is emerging evidence to suggest the infectious period may start 1 to 3 days before you develop symptoms_ \n    - [article](https:\/\/www.thelancet.com\/journals\/laninf\/article\/PIIS1473-3099(20)30196-1\/fulltext)\n        - _The peak viral load of patients with MERS-CoV and SARS-CoV infections occurs at around 7\u201310 days after symptom onset_ \n        - _The median interval between symptom onset and hospitalisation was 4 days (range 0\u201313)_\n\n\n- Imperial college MRC Centre for Global Infectious Disease Analytics\n    - T_inc: Gamma(5.1, 0.86)\n        - _The infection-to-onset distribution is Gamma distributed with mean 5.1 days and coefficient of variation 0.86._   \n    - T_inf + T_hosp + T_crit: Gamma(17.8, 0.45)\n        - _The onset-to-death distribution is also Gamma distributed with a mean of 17.8 days and a coefficient of variation 0.45._  \n    - T_inc + T_inf + T_hosp + T_crit\n        - _The infection-to-death distribution is therefore given by: \u03c0\u223cGamma(5.1,0.86) + Gamma(17.8,0.45)_","13fc9650":"### Target\nLet $D_t$ be the number of death at time $t$ for a given country. We model $d_t = \\mathbb{E}[D_t]$\n\nWe sample \n$$D_t \\sim \\mathrm{GammaPoisson}(\\psi, \\frac{\\psi}{d_t})$$\nwhere $\\psi \\sim \\mathcal{N}^+(0, 5)$","9d1bec80":"## Results","ce99f073":"As we can see for the case of Spain (ESP), the model estimates that half of the population has been infected. This is unlikely to be true, as [other estimates](https:\/\/mrc-ide.github.io\/covid19estimates\/#\/details\/Spain) point towards a more reasonable 5%.\n\nHowever, the number of hospitalized individuals could be close to true dynamics. \n\nWe have to keep in mind that this model has not been trained on any of other compartments data, it only optimizes the number of deceased individuals.","2dce4728":"### Mobility analysis\n- Let's cluster the different countries by typology of mobility","45e5d033":"C1 School closing  \nRecord closings of schools and universities\n- 1: recommend closing\n- 2: Require closing (only some levels or categories, eg just high school, or just public schools)\n- 3: Require closing all levelsNo data \n\nC2 Workplace closing  \nRecord closings of workplaces\n- 1: recommend closing (or work from home)\n- 2: require closing (or work from home)for some sectors or categories of workers\n- 3: require closing (or work from home) all-but-essential workplaces (eg grocery stores, doctors)\n\nC3 Cancel public events  \nRecord cancelling public events\n- 1: Recommend cancelling\n- 2: Require cancelling\n\nC4 Restrictions on gatherings  \nRecord the cut-off size for bans on private gatherings\n- 1: Restrictions on very large gatherings (the limit is above 1000 people)\n- 2: Restrictions on gatherings between 100-1000 people\n- 3: Restrictions on gatherings between 10-100 people\n- 4: Restrictions on gatherings of less than 10 people\n\nC5 Close public transport  \nRecord closing of public transport\n- 1: Recommend closing (or significantly reduce volume\/route\/means of transport available)\n- 2: Require closing (or prohibit most citizens from using it)\n\nC6 Stay at home requirements  \nRecord orders to \u201cshelter-in-place\u201d and otherwise confine to home\n- 1: recommend not leaving house\n- 2: require not leaving house with exceptions for daily exercise, grocery shopping, and \u2018essential\u2019 trips\n- 3: Require not leaving house with minimal exceptions (e.g. allowed to leave only once every few days, or only one personcan leave at a time, etc.)\n\nC7 Restrictions on internal movement  \nRecord restrictions on internal movement\n- 1: Recommend closing(or significantly reduce volume\/route\/means of transport)\n- 2: Require closing (or prohibit most people from using it)\n\nC8 International travel controls  \nRecord restrictions on international travel\n- 1: Screening\n- 2: Quarantine arrivals from high-risk regions\n- 3: Ban on high-risk region","46d85b2b":"# SEIR-HCD Model\n\nModel built on compartmental models. Each of the following letters represents a compartment of the population of a country:\n\nS - susceptible  \nE - exposed  \nI - infectious  \nR - recovered  \nH - hospitalized  \nC - critical  \nD - deceased  \n\nWe normalize these count numbers by the total population of the country.","117a7a3c":"## Measures dataset","93ac0f7a":"### Populations of all compartments over time","cd239926":"## Inference","6e77be60":"Let $M_{t, i} \\geq 0$ be here the relative change in mobility, expressed here differently to ease modelling.  \nIn this section, we thus have $M_{t_i} = 1$ when there is _no change_ in mobility, and $M_{t, i} = 0$ when a 100% diminution has occured (theoretic).\n\nFor a mobility category $i$, time $t$, we model $M_{t, i} = \\mathbb{E}[m_{t, i}]$ with:\n\n$$M_{t, i} \\sim \\mathcal{N}(m_{t, i}, \\sigma)$$\nwhere the prior for $\\sigma$ is $\\sigma \\sim \\mathrm{Exponential}(1) $.\n\nWe model:\n\n$$m_{t, i} = \\exp(- \\sum_k \\alpha_{k, i} I_{k, t})$$ \nwith \n- $I_{k, t} \\in \\mathbb{N}$ ordinal indicator of the measure $k$ at time $t$\n- $\\alpha_{k, i} \\geq 0$ to be inferred, with prior $\\alpha_{k, i} \\sim \\mathrm{Gamma}(\\frac{1}{N}, 1) - \\frac{\\log 1.05}{N} $ where $N$ is the number of measures\n\nFor simplicity, we do not include the `residential` category, which has opposite correlation with mobility. We also remove the `parks` series, that is quite noisy in some countries.\n\nWe fit this model on multiple countries with the same parameters, in order to extract a country-independent effect of measures on mobility.\n\n<!--     - how to handle country-specific behaviors e.g. parks in Sweden, Denmark ? Is it \/ should it be implemented in the measure indicator ? -->\n    \n<!-- \n### Approach 2: infer causality ?\n\n- [Article](https:\/\/towardsdatascience.com\/inferring-causality-in-time-series-data-b8b75fe52c46#99db): _Inferring causality in time series data_\n    - [Tigramite](https:\/\/github.com\/jakobrunge\/tigramite): PCMCI - _Causal discovery for time series datasets_\n- Paper [Graphical models for time series](https:\/\/sci-hub.im\/https:\/\/ieeexplore.ieee.org\/document\/5563116): Gaining insight into their computational implementation -->","db4490b7":"#### Cluster Analysis\n> With this new point of view, we are able to detect countries that have indeed taken similar measures leading to similar mobility. For instance this approach shows that Japan and Sweden have similar mobility's profiles. They have both avoided strong lockdown. On the other hand Italy, France and Great Britain belong to the same cluster of drastic mobility loss. Finally we can also detect countries that have decided to handle the crisis with intermediate measures, like Denmark, Germany or Norway.Therefore it seems that this clusterization validates the use of mobility as a convenient proxy for measures.","a1cb6848":"As we can see, the results quality vary from country to country. Some are quite convincing, e.g. Italy and Spain, while for others the predictions are a bit off.  \n\nIn the case of Germany for instance, we can see that the effective reproduction number does not go below 1, which explains that one does not see an inflexion in the curve. ","7d737166":"## Reproduction number\nlet $\\mathrm{m}_{t, i}$ be the reduction of mobility in the category $i$, relatively to a baseline (before the pandemic).  \nHence $\\mathrm{m}_{t, i} > -1$.\n\nWe adopt the strong hypothesis that each mobility category presents the same transmission dynamics. Let $R_{t, i}$ be defined by the following linear relationship:\n$$ R_{t, i} = (1 + \\mathrm{m}_{t, i})R_0 - \\mathrm{m}_{t, i}R_1  $$\nfor any time $t$ through the pandemic, with $R_0$, $R_1$ to be estimated.\n\nWe model the effective reproduction number with a weighted mean for each mobility category: \n\n$$R_t = \\sum_i \\alpha_i R_{t, i}$$\nwith $\\alpha_i$ to be estimated, where\n- $\\sum_i \\alpha_i = 1$ \n- $\\forall i, \\alpha_i > 0$\n\nWe choose the following prior : $\\forall i, \\alpha_i \\sim \\mathrm{Gamma}(1, .5)$","c2a01fa3":"![process](https:\/\/i.imgur.com\/9ZAt3H3.png)","56f19f1d":"##### Data fetching","dec7b345":"# Future work\n\n- We could leverage hospitalization and ICU data and feed it to the model, so that it learns based on observations of H, C and D compartments at once. This would reinforce the confidence one can have on the deceased compartment, and $R_t$, predictions, as well as enable the model to estimate other compartments more accurately. An example of dataset that could be used is the one from [IHME](https:\/\/covid19.healthdata.org\/) that provides estimated numbers.\n\n- We could include [other data sources](https:\/\/citymapper.com\/cmi\/) as additional mobility measurements.\n\n\n- Combining to make scenarios: one could build a framework that has the following properties:\n    - Input : measure, date of implementation, country\n    - Output : mobility, $R_t$, deaths [+ infected, hospitalized, critical] time series\n\nHere are some example questions of interest:\n- _What if the lockdown had been decided one week later ? Two weeks earlier ?_  \n- _What will happen if the lockdown is lifted in one week ? Will we see a spike in hospitals ?_\n\nThese questions are of utmost importance for policy makers.","7c5be3a4":"Let's take a look at the model parameters's distributions:","6c5d9bca":"# Relationship between measures and mobility"}}