{"cell_type":{"6197b971":"code","0b19a658":"code","02f7b480":"code","71f2adc6":"code","5881a178":"code","0fde18cb":"code","db109125":"code","1508d6ec":"code","236e1bba":"code","c592b6d4":"code","e342b7e9":"code","a355448c":"code","3474b013":"code","b9e65c54":"code","fee6ab73":"code","1f6f7475":"code","08a4acc3":"code","c4d325f2":"code","e857a9e7":"code","425da158":"code","5aa5a41c":"code","c9813865":"code","9a7171e9":"code","abffdee0":"code","0ac91b2e":"code","61dfa4aa":"code","ba2466dd":"code","45aa9b7e":"code","f4256bb1":"code","c4141e8d":"code","769e7a25":"code","b90b82fb":"code","ac0d19ea":"code","779201d2":"code","a8c3c659":"code","108e09b6":"code","f9e62252":"code","e3a634d5":"code","eb413013":"code","1362a678":"code","7e10db8c":"code","78247b38":"code","8b4bef3e":"code","4ad3e7a9":"code","cc28c89f":"code","3eed54f4":"code","cbee0d03":"code","9fbd60c5":"code","f8f48fd4":"code","e8f340ce":"code","4a6d1f7f":"code","9897be41":"code","8b6099f7":"code","4bec8a53":"code","63d65376":"code","29b4b5a1":"code","a4a14a45":"code","f3757b79":"code","2240aa67":"code","485b6bbf":"code","f033acf6":"code","aaa85a33":"code","dc292be2":"code","0470b613":"code","80941d05":"code","5018ff8f":"code","2dd45658":"code","c8c3859e":"code","92b5dcc2":"code","62286ecc":"code","5a3ed26c":"code","e2c23034":"code","db831ab7":"code","c41cc6ce":"code","4a545a42":"code","167aa0ee":"code","e6c01462":"code","a59999f8":"code","dbdb8b76":"code","936f12f8":"code","c810a95e":"code","64a6027f":"code","95f7a57f":"code","2f88ae04":"code","c319e571":"code","d39a8439":"code","78427170":"code","d7b27a58":"code","a0604878":"code","0ddbbe86":"code","e8402c1a":"code","c15112fa":"code","ca362c9a":"code","ddfbe331":"code","7e02833b":"code","e337d025":"code","3463328e":"code","2e5b0ee7":"code","335971f8":"code","9d6ed7d8":"code","66d82cf6":"code","9db27395":"code","ca954646":"code","0c948ca9":"code","1b059d13":"code","9392730b":"code","150a2391":"code","535be4d0":"code","75bc09a6":"code","5913a2c3":"code","d99294bc":"code","79408ff8":"code","394116f9":"code","f580ac0a":"code","b6d8c784":"code","39e3517b":"code","b2cd7129":"code","3d040e36":"code","bd5ef488":"code","0342ade8":"code","07479e74":"code","731601c9":"code","355115a5":"code","ca13dbd2":"code","51d50289":"code","67d3dd7e":"code","e0cea9b5":"code","9855206c":"code","2079caa9":"code","e91de963":"code","e86bb5b8":"code","d403b22a":"code","7f31dd9b":"code","790614a0":"code","6c96d41c":"code","97d00765":"code","c9ff4fb2":"code","0ec3c938":"code","896a313d":"code","54abd857":"code","efbe4600":"code","14304629":"code","411d83ae":"code","017f586d":"code","1aa6381e":"code","cddd27d6":"code","04a0259d":"code","fab02048":"code","2718023b":"code","d8f44c80":"code","70fec04d":"code","5ec000b4":"code","99c0c65b":"code","6861f147":"code","08f4061b":"code","1c1529ce":"code","5209f46b":"code","161913a1":"code","7f5ecaec":"code","beec2f27":"code","d2d3c8bd":"code","ef36aeca":"code","11c9db8e":"code","3db6cf3d":"code","608c4c52":"code","50fa481b":"code","a2d69b53":"code","8d3078f0":"code","c716162d":"code","9435b46a":"code","6502b7e1":"code","744ee0ea":"code","2bb17816":"code","21d6d6cd":"code","b76effa8":"code","c1287d44":"code","88fec120":"code","6c690878":"code","94588a94":"code","6f818a4d":"code","c2ad5f88":"code","be1ed958":"code","0a6e0eaa":"code","e78fd5ac":"code","62ab3056":"code","df1e462d":"code","0896918a":"code","d51ea75d":"code","82cf14e3":"code","b5ef2484":"code","af9fbbb4":"code","33db9b19":"code","bf33f100":"code","4b98838a":"code","38ff59a6":"code","1c1e1a22":"code","a9c0fbcd":"code","5d7813fa":"code","a6e31bb5":"code","508e83ef":"code","2ca1f327":"code","360ac738":"markdown","9ac03e4d":"markdown","fd439d56":"markdown","bb873806":"markdown","d3c96f6d":"markdown","e20c19cb":"markdown","35490014":"markdown","ece2f103":"markdown","c197c80e":"markdown","21a75973":"markdown","e0ebef13":"markdown","b2ce4509":"markdown","7073b0f9":"markdown","563ba5a9":"markdown","d2bfc18b":"markdown","a60f62dc":"markdown","8d4f8329":"markdown","b3b8cf51":"markdown","63efe0ab":"markdown","b1aeb424":"markdown","fede2c95":"markdown","f52e27be":"markdown","863418bb":"markdown","e69608ea":"markdown","7fcff58f":"markdown","fbd24861":"markdown","d8d424b1":"markdown","d73d25e9":"markdown","2646ee8d":"markdown","99b2dc7b":"markdown","4854e12f":"markdown","d642c02e":"markdown","c2b7fed7":"markdown","61b2391d":"markdown","ddfe5763":"markdown","e092db64":"markdown","374c6d6e":"markdown","333385d0":"markdown","97877f60":"markdown","ed812040":"markdown","1a447e57":"markdown","0d9604a7":"markdown","9f9cbaa6":"markdown","92ea6e29":"markdown","c34ab474":"markdown","1428f009":"markdown","948e96ec":"markdown","c4d3d21c":"markdown","abee6d41":"markdown","cd6c58d2":"markdown","30369c7a":"markdown","2548dc26":"markdown","3a3f4298":"markdown","3bd6b448":"markdown","a4539d2d":"markdown","9160584f":"markdown","722dd573":"markdown","31f1002c":"markdown","a5a1bac6":"markdown","ffe5eb9d":"markdown","1b8a10c9":"markdown","2ba055da":"markdown","85012f6a":"markdown","9c4ff4d9":"markdown","b315588a":"markdown","ffbdf6a5":"markdown","75ea0bf4":"markdown","d66e4857":"markdown","f04f8698":"markdown","db913c8f":"markdown","7c6cd25c":"markdown","c6aaf590":"markdown","d72463ed":"markdown","4c036bce":"markdown","6a955e15":"markdown","b540655f":"markdown","1b11e859":"markdown","b280d315":"markdown","f76d4e12":"markdown","b8628dd2":"markdown","fc04f55e":"markdown","566b86d7":"markdown","b6fe7b64":"markdown","6e645f2b":"markdown","5a83117b":"markdown","baa051b2":"markdown","2120f58a":"markdown","e51b2174":"markdown","de40f0b3":"markdown","ff0160f7":"markdown","0175ab37":"markdown","40999655":"markdown","4a433707":"markdown","439d1283":"markdown","5ab2f574":"markdown","51161805":"markdown","a33cf66b":"markdown","1949f3ac":"markdown","cc14fc71":"markdown","f4e8412f":"markdown","3ab7842f":"markdown","40bf150b":"markdown","8c5b1c78":"markdown","347397c1":"markdown","960a561f":"markdown","d759bd23":"markdown","9525dcf5":"markdown","a62cd103":"markdown","1c7f9e35":"markdown","2fc2f167":"markdown","2f5ca19d":"markdown","14511e4b":"markdown","9cf95b58":"markdown","56651270":"markdown","89b51f2b":"markdown","1a6fbbfe":"markdown","4331fbf2":"markdown","f3964d9e":"markdown","e4699f4c":"markdown","14e4396f":"markdown"},"source":{"6197b971":"import warnings\nwarnings.filterwarnings('ignore')","0b19a658":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime, timedelta\n\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as plticker\n%matplotlib inline\n\n#Model Building libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve","02f7b480":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","71f2adc6":"Lead_score=pd.read_csv('..\/input\/lead-scoring-dataset\/Lead Scoring.csv')\nLead_score.head()","5881a178":"# Reading the data dictionary file\nLead_score_data_dict=pd.read_excel('..\/input\/lead-scoring-dataset\/Leads Data Dictionary.xlsx')\nLead_score_data_dict","0fde18cb":"# Checking shape of Dataframe\nRow_col=Lead_score.shape\nRow_col","db109125":"# Checking size\nLead_score.size","1508d6ec":"#checking the datatypes\nLead_score.info()","236e1bba":"# Checking the statistical details of numerical columns\nLead_score.describe()","c592b6d4":"# Checking for 'select' Values\nprint(Lead_score['Specialization'].str.contains('Select').value_counts())\nprint(Lead_score['How did you hear about X Education'].str.contains('Select').value_counts())\nprint(Lead_score['Lead Profile'].str.contains('Select').value_counts())\nprint(Lead_score['Country'].str.contains('Select').value_counts())","e342b7e9":"# Converting 'Select' values to NaN.\nLead_score = Lead_score.replace('Select', np.nan)","a355448c":"# Checking for 'select' Values after replacing them with np.nan\nprint(Lead_score['Specialization'].str.contains('Select').value_counts())\nprint(Lead_score['How did you hear about X Education'].str.contains('Select').value_counts())\nprint(Lead_score['Lead Profile'].str.contains('Select').value_counts())\nprint(Lead_score['Country'].str.contains('Select').value_counts())","3474b013":"# Checking for total count and percentage of null values in all columns of the dataframe.\n\ntotal = pd.DataFrame(Lead_score.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(Lead_score.isnull().sum()\/Lead_score.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1)","b9e65c54":"# we will drop the columns having more than 40% NA values.\nLead_score = Lead_score.drop(Lead_score.loc[:,list(round(100*(Lead_score.isnull().sum()\/len(Lead_score.index)), 2)>45)].columns, axis=1)\nLead_score.shape","fee6ab73":"#We can check the number of unique values is a column\n# If the number of unique values <=40: Categorical column\n# If the number of unique values in a columns> 50: Continuous\n\nLead_score.nunique().sort_values()","1f6f7475":"# Dropping the columns\ndrop_cols=['Tags']\nLead_score.drop(labels=drop_cols,axis=1,inplace=True)","08a4acc3":"round((Lead_score['What matters most to you in choosing a course'].value_counts(normalize=True)*100),2)","c4d325f2":"need = Lead_score['What matters most to you in choosing a course'].value_counts().index[:2]\nLead_score['What matters most to you in choosing a course'] = np.where(Lead_score['What matters most to you in choosing a course'].isin(need),Lead_score['What matters most to you in choosing a course'], 'OTHER')\nround((Lead_score['What matters most to you in choosing a course'].value_counts(normalize=True)*100),2)","e857a9e7":"round((Lead_score['Lead Origin'].value_counts(normalize=True)*100),2)","425da158":"need = Lead_score['Lead Origin'].value_counts().index[:4]\nLead_score['Lead Origin'] = np.where(Lead_score['Lead Origin'].isin(need),Lead_score['Lead Origin'], 'OTHER')","5aa5a41c":"round((Lead_score['Lead Origin'].value_counts(normalize=True)*100),2)","c9813865":"round((Lead_score['What is your current occupation'].value_counts(normalize=True)*100),2)","9a7171e9":"need = Lead_score['What is your current occupation'].value_counts().index[:5]\nLead_score['What is your current occupation'] = np.where(Lead_score['What is your current occupation'].isin(need),Lead_score['What is your current occupation'], 'OTHER')","abffdee0":"round((Lead_score['What is your current occupation'].value_counts(normalize=True)*100),2)","0ac91b2e":"round((Lead_score['City'].value_counts(normalize=True)*100),2)","61dfa4aa":"need = Lead_score['City'].value_counts().index[:4]\nLead_score['City'] = np.where(Lead_score['City'].isin(need),Lead_score['City'], 'OTHER')","ba2466dd":"round((Lead_score['City'].value_counts(normalize=True)*100),2)","45aa9b7e":"round((Lead_score['Last Notable Activity'].value_counts(normalize=True)*100),2)","f4256bb1":"need = Lead_score['Last Notable Activity'].value_counts().index[:6]\nLead_score['Last Notable Activity'] = np.where(Lead_score['Last Notable Activity'].isin(need),Lead_score['Last Notable Activity'], 'OTHER')","c4141e8d":"round((Lead_score['Last Notable Activity'].value_counts(normalize=True)*100),2)","769e7a25":"round((Lead_score['Last Activity'].value_counts(normalize=True)*100),2)","b90b82fb":"need = Lead_score['Last Activity'].value_counts().index[:10]\nLead_score['Last Activity'] = np.where(Lead_score['Last Activity'].isin(need),Lead_score['Last Activity'], 'OTHER')","ac0d19ea":"round((Lead_score['Last Activity'].value_counts(normalize=True)*100),2)","779201d2":"round((Lead_score['Specialization'].value_counts(normalize=True)*100),2)","a8c3c659":"need = Lead_score['Specialization'].value_counts().index[:10]\nLead_score['Specialization'] = np.where(Lead_score['Specialization'].isin(need),Lead_score['Specialization'], 'OTHER')","108e09b6":"round((Lead_score['Specialization'].value_counts(normalize=True)*100),2)","f9e62252":"round((Lead_score['Lead Source'].value_counts(normalize=True)*100),2)","e3a634d5":"need = Lead_score['Lead Source'].value_counts().index[:8]\nLead_score['Lead Source'] = np.where(Lead_score['Lead Source'].isin(need),Lead_score['Lead Source'], 'OTHER')","eb413013":"round((Lead_score['Lead Source'].value_counts(normalize=True)*100),2)","1362a678":"round((Lead_score['Country'].value_counts(normalize=True)*100),2)","7e10db8c":"need = Lead_score['Country'].value_counts().index[:10]\nLead_score['Country'] = np.where(Lead_score['Country'].isin(need),Lead_score['Country'], 'OTHERS')","78247b38":"round((Lead_score['Country'].value_counts(normalize=True)*100),2)","8b4bef3e":"round(100*(Lead_score.isnull().sum()\/len(Lead_score.index)), 2)","4ad3e7a9":"# Rest missing values are under 2% so we can drop these rows.\nLead_score.dropna(inplace = True)","cc28c89f":"round(100*(Lead_score.isnull().sum()\/len(Lead_score.index)), 2)","3eed54f4":"row_col=Lead_score.shape\nrow_col","cbee0d03":"# Checking row wise null values\n(Lead_score.isnull().sum(axis=1)*100\/len(Lead_score)).value_counts(ascending=True)","9fbd60c5":"Lead_score.size","f8f48fd4":"# Checking for duplicate values in dataset\nLead_score_dupl=Lead_score.copy()\nLead_score_dupl.drop_duplicates(subset=None,inplace=True)\nLead_score_dupl.size","e8f340ce":"round(100*(row_col[0])\/(Row_col[0]),2)","4a6d1f7f":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\ncols = ['r','c']\n\nLead_score['Converted'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True, colors=cols)\nplt.title('Lead Conversion',fontweight=\"bold\", size=20)\nplt.subplot(1,2,2)\nsns.countplot('Converted',data=Lead_score,palette='PuRd')\nplt.title('Lead Conversion',fontweight=\"bold\", size=20)\nplt.subplots_adjust(right=1)\nplt.show()\n","9897be41":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\n\nsns.countplot(x='Lead Source', data =Lead_score, palette='magma')\nplt.xticks(rotation = 90,fontweight=\"bold\")\nplt.title('Lead Source', fontweight='bold',size=20)\nplt.subplot(1,2,2)\nsns.barplot(x=\"Lead Source\", y=\"Converted\", data=Lead_score,palette='magma')\nplt.title('Lead Source vs Converted',fontweight=\"bold\", size=20)\nplt.ylabel(\"Conversion  Rate\")\nplt.xticks(rotation = 90,fontweight=\"bold\")\nplt.subplots_adjust(right=1)\nplt.show()","8b6099f7":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\ncols = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue','r']\nLead_score['Lead Origin'].value_counts().plot.pie(autopct='%1.1f%%',shadow=True, colors=cols)\nplt.title('Lead Origin',fontweight=\"bold\", size=20)\nplt.subplot(1,2,2)\nsns.barplot(x=\"Lead Origin\", y=\"Converted\", data=Lead_score,palette='magma')\nplt.title('Lead Origin vs Converted',fontweight=\"bold\", size=20)\nplt.ylabel(\"Conversion  Rate\",fontweight=\"bold\", size=20)\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.subplots_adjust(right=1)\nplt.show()","4bec8a53":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.countplot(x = \"Do Not Email\", hue = \"Converted\", data = Lead_score, palette= 'gist_rainbow')\n\nplt.title('Email vs Lead Conversion', fontweight=\"bold\", size=20)\nplt.subplot(1,2,2)\nsns.countplot(x = \"Do Not Call\", hue = \"Converted\", data = Lead_score, palette='cool')\nplt.title('Call vs Lead Conversion',fontweight=\"bold\", size=20)\nplt.subplots_adjust(right=1)\nplt.show()","63d65376":"Lead_score['TotalVisits'].describe(percentiles=[0.05,.25, .5, .75, .90, .95, .99])","29b4b5a1":"percentiles = Lead_score['TotalVisits'].quantile([0.05,0.95]).values\nLead_score['TotalVisits'][Lead_score['TotalVisits'] <= percentiles[0]] = percentiles[0]\nLead_score['TotalVisits'][Lead_score['TotalVisits'] >= percentiles[1]] = percentiles[1]","a4a14a45":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.violinplot(y= \"TotalVisits\", x = \"Converted\", data = Lead_score, palette= 'BuPu')\nplt.xticks(rotation = 90,fontweight=\"bold\")\nplt.title('Total visits to website vs Lead Conversion', fontweight=\"bold\", size=20)\nplt.subplot(1,2,2)\nsns.violinplot(y = \"Total Time Spent on Website\",x = \"Converted\", data = Lead_score, palette='husl')\nplt.title('Total time spent on website vs Lead Conversion',fontweight=\"bold\", size=20)\nplt.subplots_adjust(right=1)\nplt.show()","f3757b79":"plt.figure(figsize=(14, 6))\nsns.countplot(x='Last Activity',hue='Converted', data= Lead_score, palette='summer')\nplt.title('Last Activity vs Lead Conversion',fontweight=\"bold\", size=20)\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.show()","2240aa67":"Lead_score.Country.describe()","485b6bbf":"plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Specialization\", y=\"Converted\", data=Lead_score,palette='ocean')\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.title('Specialization vs Lead Conversion', fontweight='bold', size=20)\n\nplt.show()","f033acf6":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\n\nsns.countplot(x='What is your current occupation', data=Lead_score,palette='husl')\nplt.title('Current Occupation',fontweight=\"bold\", size=20)\nplt.xticks(rotation = 45,fontweight=\"bold\")\n\nplt.subplot(1,2,2)\nsns.barplot(x='What is your current occupation', y=\"Converted\", data=Lead_score,palette='winter')\nplt.title('Current occupation vs Lead Conversion',fontweight=\"bold\", size=20)\nplt.ylabel(\"Conversion  Rate\",fontweight=\"bold\", size=20)\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.subplots_adjust(right=1)\nplt.show()","aaa85a33":"Lead_score['What matters most to you in choosing a course'].describe()","dc292be2":"Lead_score['Search'].value_counts()","0470b613":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\ncols = ['lightskyblue','salmon','yellowgreen', 'gold', 'purple']\nLead_score['City'].value_counts().plot.pie(autopct='%1.1f%%',shadow=True, colors=cols)\nplt.title('City of the Customer',fontweight=\"bold\", size=20)\nplt.subplot(1,2,2)\nsns.barplot(x=\"City\", y=\"Converted\", data=Lead_score,palette='twilight')\nplt.title('City vs Lead Conversion',fontweight=\"bold\", size=20)\nplt.ylabel(\"Conversion  Rate\",fontweight=\"bold\", size=20)\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.subplots_adjust(right=1)\n\nplt.show()","80941d05":"plt.figure(figsize=(14, 6))\nsns.countplot(x='Last Notable Activity', hue=\"Converted\", data=Lead_score,palette='nipy_spectral')\nplt.xticks(rotation = 45,fontweight=\"bold\")\nplt.title('Last notable activity vs Lead Conversion', fontweight='bold', size=20)\n\nplt.show()","5018ff8f":"Lead_score = Lead_score.drop(['Lead Number','Search','Magazine','Newspaper Article','X Education Forums','Newspaper',\n           'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content',\n           'Get updates on DM Content','I agree to pay the amount through cheque','A free copy of Mastering The Interview','Country','Last Notable Activity','Do Not Email', 'Do Not Call'],1)","2dd45658":"Lead_score.shape","c8c3859e":"#Categorical columns\nLead_score.loc[:,Lead_score.dtypes == 'object'].columns","92b5dcc2":"# Create dummy variables using the 'get_dummies'\ndummy = pd.get_dummies(Lead_score[['Lead Origin','Specialization' ,'Lead Source','What is your current occupation','City', 'What matters most to you in choosing a course','Last Activity']], drop_first=False)\n# Add the results to the master dataframe\nLead_final = pd.concat([Lead_score, dummy], axis=1)\nLead_final.head()","62286ecc":"#Drop columns after dummy variable creation\nLead_final = Lead_final.drop(['Lead Origin','Specialization' ,'Lead Source','What is your current occupation', \n'What matters most to you in choosing a course','Last Activity', 'City'],1)","5a3ed26c":"Lead_final.info()","e2c23034":"Lead_final.columns","db831ab7":"#Lets drop the columns having Other in their Category\nLead_final = Lead_final.drop(['Specialization_OTHER','Lead Source_OTHER','City_OTHER', 'City_Other Cities','What is your current occupation_OTHER',\n       'What is your current occupation_Other','What matters most to you in choosing a course_OTHER','Last Activity_OTHER'],1)","c41cc6ce":"#Number of columns after dummy variable creation\nLead_final.shape","4a545a42":"# Putting feature variable to X\nX = Lead_final.drop(['Prospect ID','Converted'], axis=1)\n\nX.head()","167aa0ee":"# Putting response variable to y\ny = Lead_final['Converted']\n\ny.head()","e6c01462":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","a59999f8":"#Rows and columns after split\nprint(X_train.shape)\nprint(X_test.shape)\n","dbdb8b76":"#scaling continuous variables in the dataset\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","936f12f8":"### Checking the Lead Conversion Rate\nconverted = (sum(Lead_final['Converted'])\/len(Lead_final['Converted'].index))*100\nconverted","c810a95e":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","64a6027f":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","95f7a57f":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 23)             # running RFE with 23 variables as output\nrfe = rfe.fit(X_train, y_train)","2f88ae04":"rfe.support_","c319e571":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","d39a8439":"col = X_train.columns[rfe.support_]\ncol","78427170":"X_train.columns[~rfe.support_]","d7b27a58":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","a0604878":"col1=col.drop('What is your current occupation_Housewife',1)\n","0ddbbe86":"#Lets run the model with selected variables\nX_train_sm = sm.add_constant(X_train[col1])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e8402c1a":"col2=col1.drop('What matters most to you in choosing a course_Flexibility & Convenience',1)","c15112fa":"X_train_sm = sm.add_constant(X_train[col2]) #Rerun the model\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","ca362c9a":"col3=col2.drop('Lead Source_Facebook',1)","ddfbe331":"X_train_sm = sm.add_constant(X_train[col3])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","7e02833b":"col4=col3.drop('Lead Source_Referral Sites',1)","e337d025":"X_train_sm = sm.add_constant(X_train[col4])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","3463328e":"col5=col4.drop('Lead Origin_API',1)","2e5b0ee7":"X_train_sm = sm.add_constant(X_train[col5])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","335971f8":"col6=col5.drop('Lead Source_Organic Search',1)","9d6ed7d8":"X_train_sm = sm.add_constant(X_train[col6])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","66d82cf6":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","9db27395":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","ca954646":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","0c948ca9":"y_train_pred_final['predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","1b059d13":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","9392730b":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","150a2391":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col6].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col6].values, i) for i in range(X_train[col6].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","535be4d0":"col7=col6.drop('What matters most to you in choosing a course_Better Career Prospects',1)","75bc09a6":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col7])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","5913a2c3":"vif = pd.DataFrame()\nvif['Features'] = X_train[col7].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col7].values, i) for i in range(X_train[col7].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d99294bc":"col8=col7.drop('Lead Origin_Landing Page Submission',1)","79408ff8":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col8])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","394116f9":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","f580ac0a":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","b6d8c784":"y_train_pred_final['predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","39e3517b":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","b2cd7129":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","3d040e36":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col8].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col8].values, i) for i in range(X_train[col8].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bd5ef488":"X_train_sm = sm.add_constant(X_train[col8])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","0342ade8":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","07479e74":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","731601c9":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","355115a5":"y_train_pred_final['predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","ca13dbd2":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","51d50289":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","67d3dd7e":"plt.figure(figsize=(15,8), dpi=80, facecolor='w', edgecolor='k', frameon='True')\n\ncor = X_train[col8].corr()\nax=sns.heatmap(cor, annot=True, cmap=\"YlGnBu\")\n\nplt.tight_layout()\nplt.show()","e0cea9b5":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","9855206c":"# Let's see the sensitivity of our logistic regression model\nprint('Sensitivity:')\nTP \/ float(TP+FN)","2079caa9":"# Let us calculate specificity\nprint('Specificity:')\nTN \/ float(TN+FP)","e91de963":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","e86bb5b8":"# positive predictive value \nprint (TP \/ float(TP+FP))","d403b22a":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","7f31dd9b":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","790614a0":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","6c96d41c":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","97d00765":"def auc_val(fpr,tpr):\n    AreaUnderCurve = 0.\n    for i in range(len(fpr)-1):\n        AreaUnderCurve += (fpr[i+1]-fpr[i]) * (tpr[i+1]+tpr[i])\n    AreaUnderCurve *= 0.5\n    return AreaUnderCurve","c9ff4fb2":"auc = auc_val(fpr,tpr)\nauc","0ec3c938":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","896a313d":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","54abd857":"# Slightly alter the figure size to make it more horizontal.\n\n#plt.figure(figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k', frameon='True')\nsns.set_style(\"whitegrid\") # white\/whitegrid\/dark\/ticks\nsns.set_context(\"paper\") # talk\/poster\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'], figsize=(10,6))\n# plot x axis limits\nplt.xticks(np.arange(0, 1, step=0.05), size = 12)\nplt.yticks(size = 12)\nplt.show()","efbe4600":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.34 else 0)\n\ny_train_pred_final.head()","14304629":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","411d83ae":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","017f586d":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","1aa6381e":"# Let's see the sensitivity of our logistic regression model\nprint('Sensitivity:')\nTP \/ float(TP+FN)","cddd27d6":"# Let us calculate specificity\nprint('Specificity:')\nTN \/ float(TN+FP)","04a0259d":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","fab02048":"# Positive predictive value \nprint (TP \/ float(TP+FP))","2718023b":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","d8f44c80":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","70fec04d":"precision=confusion[1,1]\/(confusion[0,1]+confusion[1,1])\nprecision","5ec000b4":"recall=confusion[1,1]\/(confusion[1,0]+confusion[1,1])\nrecall","99c0c65b":"precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","6861f147":"recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","08f4061b":"y_train_pred_final.Converted, y_train_pred_final.predicted","1c1529ce":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","5209f46b":"# Slightly alter the figure size to make it more horizontal.\nplt.figure(figsize=(8, 4), dpi=100, facecolor='w', edgecolor='k', frameon='True')\nplt.plot(thresholds, p[:-1], \"b-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","161913a1":"F1 = 2*(precision*recall)\/(precision+recall)\nF1","7f5ecaec":"\nX_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\n","beec2f27":"X_test = X_test[col8]\nX_test.head()","d2d3c8bd":"X_test_sm = sm.add_constant(X_test)\n","ef36aeca":"y_test_pred = res.predict(X_test_sm)","11c9db8e":"y_test_pred[:10]","3db6cf3d":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","608c4c52":"# Let's see the head\ny_pred_1.head()","50fa481b":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","a2d69b53":"# Putting CustID to index\ny_test_df['Prospect ID'] = y_test_df.index","8d3078f0":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","c716162d":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","9435b46a":"y_pred_final.head()\n","6502b7e1":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})","744ee0ea":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['Converted','Prospect ID','Conversion_Prob'], axis=1)","2bb17816":"y_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.34 else 0)","21d6d6cd":"y_pred_final","b76effa8":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","c1287d44":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","88fec120":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","6c690878":"# Let's see the sensitivity of our logistic regression model\nprint('Sensitivity:')\nTP \/ float(TP+FN)","94588a94":"# Let us calculate specificity\nprint('Specificity:')\nTN \/ float(TN+FP)","6f818a4d":"print(FP\/ float(TN+FP))","c2ad5f88":"print (TP \/ float(TP+FP))","be1ed958":"print (TN \/ float(TN+ FN))","0a6e0eaa":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr, thresholds","e78fd5ac":"fpr, tpr, thresholds = metrics.roc_curve( y_pred_final.Converted, y_pred_final.Conversion_Prob, drop_intermediate = False )","62ab3056":"draw_roc(y_pred_final.Converted, y_pred_final.Conversion_Prob)","df1e462d":"def auc_val(fpr,tpr):\n    AreaUnderCurve = 0.\n    for i in range(len(fpr)-1):\n        AreaUnderCurve += (fpr[i+1]-fpr[i]) * (tpr[i+1]+tpr[i])\n    AreaUnderCurve *= 0.5\n    return AreaUnderCurve","0896918a":"# Selecting the test dataset along with the Conversion Probability and final predicted value for 'Converted'\nleads_test_pred = y_pred_final.copy()\nleads_test_pred.head()","d51ea75d":"#  columns from train dataset\nleads_train_pred = y_train_pred_final[['Prospect ID','Converted','Conversion_Prob','final_predicted']]\nleads_train_pred.head()","82cf14e3":"# Concatenating the 2 dataframes train and test along the rows with the append() function\nlead_full_pred = leads_train_pred.append(leads_test_pred)\nlead_full_pred.head()","b5ef2484":"# Inspecting the shape of the final dataframe and the test and train dataframes\nprint(leads_train_pred.shape)\nprint(leads_test_pred.shape)\nprint(lead_full_pred.shape)\n","af9fbbb4":"# Ensuring the LeadIDs are unique for each lead in the finl dataframe\nlen(lead_full_pred['Prospect ID'].unique().tolist())","33db9b19":"# Calculating the Lead Score value\n# Lead Score = 100 * Conversion_Prob\nlead_full_pred['Lead_Score'] = lead_full_pred['Conversion_Prob'].apply(lambda x : round(x*100))\nlead_full_pred.head()","bf33f100":"# Inspecing the max LeadID\nlead_full_pred['Prospect ID'].max()","4b98838a":"# Making the ProspectID column as index\n# We willlater join it with the original_leads dataframe based on index\nlead_full_pred = lead_full_pred.set_index('Prospect ID').sort_index(axis = 0, ascending = True)\nlead_full_pred.head()","38ff59a6":"# Slicing the Lead Number column from original_leads dataframe\noriginal_leads=pd.read_csv('..\/input\/lead-scoring-dataset\/Lead Scoring.csv')\noriginal_leads= original_leads[['Lead Number']]\noriginal_leads.head()","1c1e1a22":"# Concatenating the 2 dataframes based on index and displaying the top 10 rows\n# This is done son that Lead Score is associated to the Lead Number of each Lead. This will help in quick identification of the lead.\nleads_with_score = pd.concat([original_leads, lead_full_pred], axis=1)\nleads_with_score.head(10)","a9c0fbcd":"pd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","5d7813fa":"#feature_importance = abs(new_params)\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nfeature_importance","a6e31bb5":"sorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\nsorted_idx","508e83ef":"pos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure(figsize=(10,6))\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center', color = 'tab:blue',alpha=0.8)\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X_train[col8].columns)[sorted_idx], fontsize=12)\nfeatax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","2ca1f327":"pd.DataFrame(feature_importance).reset_index().sort_values(by=0,ascending=False).head(3)","360ac738":"#### Sensitivity","9ac03e4d":"### <font color=purple> Suppress Warnings","fd439d56":"- #### <font color=green> The size of the dataframe remains same even after removing duplicates,Hence we can conclude that there are no duplicate values in dataset.","bb873806":"- #### <font color=green>Most of the lead have their Email opened as their last activity.\n- #### <font color=green>Conversion rate for leads with last activity as SMS Sent is almost 60%.","d3c96f6d":"- ### <font color=darkgreen>Based on the  analysis above we have seen that many columns are not adding any information to the model, hence we can drop them for further analysis","e20c19cb":"#### Specificity","35490014":"##  <font color=purple> 4. Data Visualization","ece2f103":"### <font color=purple> Concatenating the 2 dataframes based on index.","c197c80e":"#### The variable  <font color=blue>'Lead Source_Facebook'<\/font> is having very high p value,  Lets drop it","21a75973":"- ### <font color=blue> What matters most in choosing course","e0ebef13":" ### <font color=blue>Search","b2ce4509":"- #### <font color=green>The dataset contains 9240 rows and 37 columns","7073b0f9":"###  <font color=blue>Specialization","563ba5a9":"### <font color=purple>1.1 Importing relevant libraries","d2bfc18b":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","a60f62dc":"### After trying several models, we finally chose a model with the following characteristics:\n- ###  <font color = blue>All variables have p-value < 0.05. <\/font>\n- ### <font color = blue>All the features have very low VIF values, meaning, there is hardly any muliticollinearity among the features. This is also evident from the heat map.<\/font>\n- ###  <font color = blue>The overall accuracy of 0.80 at a probability threshold of 0.34 on the test dataset is also very acceptable.<\/font>","8d4f8329":"####  Lets check VIF","b3b8cf51":"### <font color=purple>3.1. Handling 'Select' in categorical columns:","63efe0ab":"- #### <font color=green>The source of the Leads were more from Google, Direct traffic and Olark Chat\n- #### <font color=green> Most of the Leads were converted from Welingak Website, Reference and Google too\n    \n- #### <font color=darkblue> To improve overall lead conversion rate, focus should be on improving lead converion of olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.","b1aeb424":"- #### <font color=green>Most entries are 'Better Career Prospects'.","fede2c95":"#### Whenever sales teams approach potential lead via email or call . The potential lead has high percentage of becoming hot lead","f52e27be":"### <font color=purple>3.4. Remove columns with too much variation in the parameters(Highly Skewed)","863418bb":"### Area Under the Curve","e69608ea":"###  <font color=blue>Converted","7fcff58f":"##  <font color=purple> 3. Data Cleaning","fbd24861":"#### The variables have VIF < 2,which is good for a good model.","d8d424b1":"###  <font color=purple> ROC for Test Dataset","d73d25e9":"#### <font color=blue>As a rule of thumb, an AUC can be classed as follows,\n\n- 0.90 - 1.00 = excellent\n- 0.80 - 0.90 = good\n- 0.70 - 0.80 = fair\n- 0.60 - 0.70 = poor\n- 0.50 - 0.60 = fail\n#### <font color=blue>Since we got a value of 0.89, our model seems to be doing well on the test dataset.","2646ee8d":"#### Positive Predictive Value","99b2dc7b":"**Running our First Training Model**","4854e12f":" ###  <font color=purple>5.2. Test-Train Split","d642c02e":" ### <font color=purple>5.1. Creating dummy variables for all categorical columns","c2b7fed7":"##  <font color=purple>6. Model Building","61b2391d":"- #### <font color=green>There are many customers who are Unemployed\n- #### <font color=green>Housewife and Working Professionals going for the course have high chances of joining it. ","ddfe5763":"#### Using the probability threshold value 0f 0.34 on the test dataset to predict if a lead will convert","e092db64":"### <font color=purple>6.6. Precision and Recall","374c6d6e":"### This is done so that Lead Score is associated to the Lead Number of each Lead. This will help in quick identification of the lead.","333385d0":"#### The variable  <font color=blue>'Lead Source_Referral Sites'<\/font> is having very high p value,  Lets drop it","97877f60":"###  <font color=blue> Email and call","ed812040":"- #### <font color=green>Most entries are 'No'","1a447e57":"###  <font color=purple>3.3. Number of unique categories in the columns:","0d9604a7":"### <font color=purple>6.5. Finding Optimal Cutoff Point","9f9cbaa6":"#### The variable  <font color=blue>'What is your current occupation_Housewife'<\/font> has highest p-value.Lets drop it.","92ea6e29":"### <font color=blue>City","c34ab474":"#### <font color=blue>Looking at the confusion matrix again","1428f009":"#### The variable  <font color=blue>'Lead Source_Organic Search'<\/font> is having very high p value,  Lets drop it","948e96ec":"![image.png](attachment:image.png)","c4d3d21c":"## Calculating the F1 score","abee6d41":"- #### <font color=green> We can see 7 columns have been removed from the data","cd6c58d2":"#### <font color=green>This confirms that all 'Select' values are converted to NAN values","30369c7a":"- ### <font color=green>Now the Data is clean with 98.5% of the rows retained after data cleaning.Now we can start with the analysis part","2548dc26":"### <font color=blue>Last Notable Activity","3a3f4298":"### Selecting Top 3 features which contribute most towards the probability of a lead getting converted","3bd6b448":" ### <font color=purple>6.3. Metrics beyond simply accuracy","a4539d2d":"## <font color=purple>8. Calculating Lead score for the entire dataset\n    \n### Lead Score = 100 * Conversion Probability\n#### This needs to be calculated for all the leads from the original dataset (train + test)","9160584f":"#### Specificity","722dd573":"### <font color=purple>Creating a dataframe with the actual churn flag and the predicted probabilities","31f1002c":"### <font color=blue>Current Occupation","a5a1bac6":"#### <font color=purple>Creating new column 'predicted' with 1 if Conversion_Prob > 0.5 else 0","ffe5eb9d":"#### Sorting the feature variables based on their relative coefficient values","1b8a10c9":"## <font color=purple>10. Conclusion","2ba055da":"- #### <font color=green>There are missing values in the data. Lets drop the columns having more than 45% NA values.","85012f6a":"### <font color=green>After Data cleaning and considering only useful variables we are able to get conversion rate of 38%","9c4ff4d9":"#### Positive Predictive Value","b315588a":"As we can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\n# <font color=purple>Objective:\n- Select the most promising leads, i.e. the leads that are most likely to convert into paying customers. \n- The company requires to build a model wherein we need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n\n ","ffbdf6a5":"###  <font color=blue>Total vists & Total time spent on website","75ea0bf4":"### <font color=purple>3.5. For the columns with higher number of categories with less % impute them with <font color=brown>'Others'","d66e4857":" ### <font color=purple>Area under the curve","f04f8698":" ### <font color=purple>6.1. Feature Selection Using RFE","db913c8f":" ###  <font color=purple>5.3. Rescaling the Features","7c6cd25c":"###  <font color=blue>Lead Source","c6aaf590":"### <font color=purple>3.2. Checking and handling missing values:","d72463ed":"#### We decided not to drop first level of K categories and manually drop the least significant other category variables hence we have used drop_first=False.","4c036bce":"- #### <font color=green>53.7% of the Leads have their origin from Landing page submission and 39% of Leads from API\n- #### <font color=green>Lead Add Form has more than 90% conversion rate but count of lead are not very high.\n- #### <font color=green>Lead Import are very less in count.\n    \n- #### <font color=darkblue>To improve overall lead conversion rate, we need to focus more on improving lead converion of API and Landing Page Submission origin and generate more leads from Lead Add Form.","6a955e15":"#### Sensitivity","b540655f":"## <font color=purple>7. Making predictions on the test set","1b11e859":"### <font color=purple>6.4. Plotting ROC","b280d315":"#### <font color=blue>As we can observe that there are select values for many columns.This is because customer did not select any option from the list, hence it shows select.Select values are as good as NULL.","f76d4e12":"### Let's plot accuracy sensitivity and specificity for various probabilities","b8628dd2":"#### The variable  <font color=blue>'What matters most to you in choosing a course_Flexibility & Convenience'<\/font> is having very high p value,  Lets drop it","fc04f55e":"- #### <font color=green>Most of the customers are from India","566b86d7":"##  <font color=purple>5. Data Preparation","b6fe7b64":"\n#### <font color=green>The columns 'Tags' is the lead-score variable generated by sales team after a potential lead is convereted to hotlead.Hence not required for analysis","6e645f2b":"#### Concatenating the train and the test dataset with the Conversion Probabilities","5a83117b":"### <font color=purple>  2.2 Data Inspection","baa051b2":"#### False Positive Rate","2120f58a":"#### Our latest model have the following features:\n#### <font color = blue>All variables have p-value < 0.05. <\/font>\n#### <font color = blue>All the features have very low VIF values, meaning, there is hardly any muliticollinearity among the features. This is also evident from the heat map.<\/font>\n#### <font color = blue>Got The overall accuracy of 0.81 at a probability threshold of 0.5 .<\/font>\n    \n###  So we need not drop any more variables and we can proceed with making predictions using this model only","e51b2174":"#### Negative Predictive value","de40f0b3":"#### Selecting the coefficients of the selected features from our final model excluding the intercept","ff0160f7":"### <font color=blue>From the precision-recall graph above, we get the optical threshold value as close to .41. However our business requirement here is to have Lead Conversion Rate around 80%.\n### <font color=purple>This is already achieved with our earlier threshold value of 0.34. So we will stick to this value.","0175ab37":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","40999655":"#### The variable  <font color=blue>'Lead Origin_API'<\/font> is having very high p value,  Lets drop it","4a433707":"###  <font color=purple> 2.1 Reading the input files","439d1283":"- #### <font color=green>Median for converted and not converted leads are the same.\n- #### <font color=green> Leads spending more time on the weblise are more likely to be converted.\n- #### <font color=darkblue> Website should be made more engaging to make leads spend more time.\n- #### <font color=darkblue> Nothing conclusive can be said on the basis of Total Visits.","5ab2f574":"#### Plot showing the feature variables based on their relative coefficient values","51161805":"###  <font color=blue>Last Activity","a33cf66b":"- #### <font color=green>35% of the total customers belong to Mumbai\n- #### <font color=green>Most leads are from mumbai, Thane & Outskirts, and other cities of Maharashtra with high conversion rate.    ","1949f3ac":"### <font color=purple>1.3 Adjusting Jupyter View\n","cc14fc71":"- #### <font color=green>Looking at the above information we see that there are some columns which have null data.\n","f4e8412f":"#### False Postive Rate","3ab7842f":"#### Getting a relative coeffient value for all the features wrt the feature with the highest coefficient","40bf150b":"### <font color=blue> Precision\nTP \/ TP + FP","8c5b1c78":"# <font color=purple>Problem Statement\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\n \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:","347397c1":"## <font color=purple>9. Determining Feature Importance","960a561f":"### <font color=purple>6.2. Assessing the model with StatsModels","d759bd23":"auc = auc_val(fpr,tpr)\nauc\n","9525dcf5":"### <font color=purple> 3.6. Percentage of retained rows after data cleaning:","a62cd103":"- #### <font color=green>Customers who were in Travel and Tourism domain has less Lead Conversion rate compared to other domains\n- #### <font color=green>Focus should be more on the Specialization with high conversion rate.   ","1c7f9e35":"#### The variable  <font color=blue>'Lead Origin_Landing Page Submission'<\/font> is having high vif value,   Lets drop it","2fc2f167":"#### The variable  <font color=blue>'What matters most to you in choosing a course_Better Career Prospects'<\/font> is having high VIF value,   It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex.","2f5ca19d":"### <font color=purple>6.7. Precision and recall tradeoff","14511e4b":"####  <font color=purple>'TotalVisits columns has certain outliers, Lets treat them\n####  <font color=purple>We will cap the outliers to 95% value for analysis.","9cf95b58":"### <font color=blue> Recall\nTP \/ TP + FN","56651270":"#### <font color=purple> Using sklearn utilities for the same","89b51f2b":"###  <font color=blue>Country","1a6fbbfe":"#### Negative Predictive Value","4331fbf2":"##  <font color=purple> 2. Reading and understanding the Data","f3964d9e":"### <font color=purple>Checking VIFs","e4699f4c":"###  <font color=blue>Lead Origin","14e4396f":"- #### <font color=green> We can see from the plot that not many Leads were 'Converted'. Out of 9103 Leads, only around 3500 (38%) of the Leads were converted"}}