{"cell_type":{"f8ecc172":"code","b28bf296":"code","d6462f7e":"code","010ad07a":"code","8dff06f8":"code","9dd61059":"code","1bf7dde4":"code","18cd077e":"code","ebf16cad":"code","e05603d7":"code","f407a285":"code","f5d635cb":"code","81466c25":"code","57341853":"code","36313b76":"code","acfa383e":"code","37de6279":"code","4bbeea00":"code","7f6ae022":"code","773eb589":"code","cb2fc370":"code","369e472f":"code","b9053dc3":"code","16d2b30f":"code","093fcd24":"code","e5d3af7e":"code","60097acc":"code","c7f69c5f":"code","d81918bb":"code","8b3769e7":"code","bbfa86d6":"code","215ca1bc":"code","283ba6c1":"code","e375536c":"code","1edd064c":"code","c1e8b27f":"code","f79d3ec4":"code","1f5da243":"code","c6df1725":"code","f2c0d0fd":"code","a44bf291":"code","715137a2":"code","b1d2cdba":"markdown","2683e00d":"markdown","7d8c97e2":"markdown","13bd3a02":"markdown","734938bc":"markdown","f61b0a16":"markdown","faf8f034":"markdown","ee36f37e":"markdown","f231f1fd":"markdown","19712d2a":"markdown","889f1d89":"markdown","94b62b74":"markdown","6c4b42c9":"markdown","3c92f186":"markdown","ae1a9971":"markdown","8b77d870":"markdown","bcb4a231":"markdown","addc7db6":"markdown","d29f7d4f":"markdown","2e39a08b":"markdown","dcf66f7a":"markdown","63260b05":"markdown","972c2efc":"markdown","3c5a3d0a":"markdown","d42ec5a5":"markdown","569ce7d9":"markdown","8ef4102a":"markdown","04ff139b":"markdown","9379881d":"markdown","2faccddd":"markdown","626d645b":"markdown"},"source":{"f8ecc172":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 20)\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","b28bf296":"data = pd.read_csv(\"\/kaggle\/input\/daily-temperature-of-major-cities\/city_temperature.csv\")\ndata.head()","d6462f7e":"data['City'].value_counts()","010ad07a":"chennai = data[data[\"City\"] == \"Chennai (Madras)\"]\nchennai.head()","8dff06f8":"chennai[\"Year\"].value_counts()","9dd61059":"\"\"\"-99 is put in place of missing values. \nWe will have to forward fill with the last non missing value before -99\n\"\"\"\nchennai[\"AvgTemperature\"] = np.where(chennai[\"AvgTemperature\"] == -99, np.nan, chennai[\"AvgTemperature\"])\nchennai.isnull().sum()","1bf7dde4":"chennai[\"AvgTemperature\"] = chennai[\"AvgTemperature\"].ffill()\nchennai.isnull().sum()","18cd077e":"chennai.dtypes\nchennai[\"Time_steps\"] = pd.to_datetime((chennai.Year*10000 + chennai.Month*100 + chennai.Day).apply(str),format='%Y%m%d')\nchennai.head()","ebf16cad":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    \"\"\"to plot the series\"\"\"\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Temprature\")\n    plt.grid(True)","e05603d7":"time_step = chennai[\"Time_steps\"].tolist()\ntemprature = chennai[\"AvgTemperature\"].tolist()\n\nseries = np.array(temprature)\ntime = np.array(time_step)\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","f407a285":"plt.figure(figsize=(10, 6))\nplot_series(time[-365:], series[-365:])","f5d635cb":"split_time = 8000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","81466c25":"naive_forecast = series[split_time - 1:-1]","57341853":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, naive_forecast)","36313b76":"#Zoom in and see only few points\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid, start=0, end=150)\nplot_series(time_valid, naive_forecast, start=1, end=151)","acfa383e":"print(tf.keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())","37de6279":"def moving_average_forecast(series, window_size):\n    \"\"\"Forecasts the mean of the last few values.\n     If window_size=1, then this is equivalent to naive forecast\"\"\"\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)","4bbeea00":"moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, moving_avg)","7f6ae022":"print(tf.keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","773eb589":"diff_series = (series[365:] - series[:-365])\ndiff_time = time[365:]\n\nplt.figure(figsize=(10, 6))\nplot_series(diff_time, diff_series)\nplt.show()","cb2fc370":"diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, diff_series[split_time - 365:])\nplot_series(time_valid, diff_moving_avg)\nplt.show()","369e472f":"diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_past)\nplt.show()","b9053dc3":"print(tf.keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())","16d2b30f":"diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_smooth_past)\nplt.show()","093fcd24":"print(tf.keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())","e5d3af7e":"series1 = tf.expand_dims(series, axis=-1)\nds = tf.data.Dataset.from_tensor_slices(series1[:20])\nfor val in ds:\n    print(val.numpy())\n","60097acc":"\ndataset = ds.window(5, shift=1)\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=\" \")\n    print()","c7f69c5f":"dataset = ds.window(5, shift=1, drop_remainder=True)\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=\" \")\n    print()","d81918bb":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\nfor window in dataset:\n    print(window.numpy())","8b3769e7":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\nfor x,y in dataset:\n    print(x.numpy(), y.numpy())","bbfa86d6":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\nfor x,y in dataset:\n    print(x.numpy(), y.numpy())","215ca1bc":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\ndataset = dataset.batch(2).prefetch(1)\nfor x,y in dataset:\n    print(\"x = \", x.numpy())\n    print(\"y = \", y.numpy())\n    print(\"*\"*25)","283ba6c1":"window_size = 60\nbatch_size = 32\nshuffle_buffer_size = 1000","e375536c":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"\n    To create a window dataset given a numpy as input\n    \n    Returns: A prefetched tensorflow dataset\n    \"\"\"\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","1edd064c":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","c1e8b27f":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 60])","f79d3ec4":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set,epochs=500)","1f5da243":"def model_forecast(model, series, window_size):\n    \"\"\"\n    Given a model object and a series for it to predict, this function will return the prediction\n    \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","c6df1725":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","f2c0d0fd":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","a44bf291":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","715137a2":"loss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","b1d2cdba":"Now using ffill() method to fill the np.nan that we created","2683e00d":"# Naive forecast\n\nIn naive forecast, we will take the record in month - 1 (the month previously) and assume that it will be carried forward for the next observation also.","7d8c97e2":"# Moving average forecast\n\nIn moving average forecast, we will take the value of average for the previous window period and take it as the prediction for the next period.","13bd3a02":"I always like to import the libraries in the alphabetical order so that is it easy to review when needed","734938bc":"Since the plot above is so crowded, we will take for a small section of the dataset and visualize it.","f61b0a16":"### Step 2: tf window option groups 5 (window size) into a single line\n\nBut for the last observations for which there are no observations to group will be kept as remaining as in the outupt of this cell","faf8f034":"I wanted to develop a timeseries model for a single city. For this purpouse, I am taking the city Chennai (previously known as Madras), from Tamil Nadu, India. The city where I reside.\n\nChennai generally has only two season. It is hot for almost throughout the year, and rains in November\/December months.","ee36f37e":"## Finding the correct learning rate\n\nUsing a call back for LearningRateScheduler(). For every epoch this just changes the learning rate a little so that the learning rate varies from 1e-8 to 1e-6\n\nAlso a new loss function Huber() is introduced which is less sensitive to outliers.","f231f1fd":"Checking if all the year has complete records","19712d2a":"## Differencing\n\nWe will use a technique called differencing to remove the trend and seasonality from the data. \nHere we difference the data between what the value was 365 days (1 year back). The differencing should always follow the seasonal pattern. ","889f1d89":"### Step 3: Drop reminder set to True will drop the variables which are not having the grouping","94b62b74":"# How to prepare a window dataset?\n\nA window dataset is used in the dataset prepration of the tensorflow. It yields a prefetched dataset with the x and y variables as tensors. \n\n### Step 1: Converting the numpy array into a tensor using tensor_slices","6c4b42c9":"### Step 4: flat map option will group the 5 observation in a single tensor variable","3c92f186":"Plotting the timeseries for the entire duration","ae1a9971":"### Step 7: Batch option will put the variables into mini-batches suitable for training. It will group both X and y into mini batches","8b77d870":"We plot this on a semilog axis","bcb4a231":"## Smoothing with moving average again\n\nThe above plot has a lot of noise. To smooth it again, we do a moving average on that","addc7db6":"### Step 5: map option will split the variables into X and y variables","d29f7d4f":"Plotting for recent one year only","2e39a08b":"# Conclusion\n\nI have demonstrated in this notebook how to use naive forecast, moving average forecast and build a model in CNN and LSTM using tensorflow dataset prepration.\n\nIf you find this notebook helpful for you, please upvote!","dcf66f7a":"# Introduction\n\nIn this notebook, I am creating a tensorflow based timeseries forecasting model using CNN & LSTM.\n\n## Acknowledgments:\nThis notebook is inspired by the course 4 of TensorFlow in Practice Specialization which is [Sequences, Time Series and Prediction](https:\/\/www.coursera.org\/learn\/tensorflow-sequences-time-series-and-prediction) by Laurence Moroney.\n\nI used this course to prepare for the tensorflow speciality examination, and I am using the methods and codes that Mr. Moroney used in the course. \n\n\n## Sections:\n1. Introduction\n2. Importing and exploring the dataset\n3. Imputting missing values\n4. Naive forecast\n5. Moving average forecast\n6. Preparing a pre-fetched tensorflow dataset\n7. Creating a CNN-LSTM based model\n8. Model metrics\n9. Conclusion\n10. References\n\n\nIf you find this notebook helpful for you, please upvote!","63260b05":"Since there is no single column that contains the date, creating a new column called Time_steps to combine the year month and date fields","972c2efc":"# Importing and exploring the dataset","3c5a3d0a":"### Step 6: shuffle option will shuffle the dataset into random order.\n\nTill the previous step, the observation would have been in the correct order. the shuffle will ensure that the data are randomly mixed up","d42ec5a5":"## Restoring trend and seasonality\nBut these are just the forecast of the differenced timeseries. To get the value for the original timeseries, we have to add back the value of t-365","569ce7d9":"# Imputing missing values\n\nThe dataset has recorded missing values with the number -99. The chennai dataset has missing values close to 29 records. \n\nI will use forward fill method to impute the missing values for the dataset. That is, we will take the previously non missing value and fill it in the place of the missing value.\n\nFirst replacing -99 with np.nan","8ef4102a":"Window size is how many observations in the past do you want to see before making a prediction.\nBatch size is similar to mini-batches set while training the neural network","04ff139b":"# References:\n1. https:\/\/github.com\/lmoroney\/dlaicourse\/blob\/master\/TensorFlow%20In%20Practice\/Course%204%20-%20S%2BP\/S%2BP%20Week%204%20Lesson%205.ipynb\n2. https:\/\/thispointer.com\/python-pandas-how-to-display-full-dataframe-i-e-print-all-rows-columns-without-truncation\/\n3. https:\/\/stackoverflow.com\/questions\/19350806\/how-to-convert-columns-into-one-datetime-column-in-pandas","9379881d":"We take the step where the learning rate drops the steepest to train our neural network.","2faccddd":"There are totally 9,266 records on the dataset. We will keep 8000 records for training (85%) and keep remaining 15% for testing","626d645b":"Checking if all the cities has the data for a full range"}}