{"cell_type":{"92d9fbb4":"code","cf30dba3":"code","d273a8e4":"code","e54a0859":"code","782b30f6":"code","a7314429":"code","69d50c70":"code","59aec314":"code","a1616ede":"code","c3b67913":"code","4d0d36e5":"code","99e0af3a":"code","9cc65b23":"code","27b9eed8":"code","f04684eb":"code","dee2002b":"code","8a47ef91":"code","85cb164c":"code","ec3d039c":"code","77481e73":"markdown","1e95d6d9":"markdown","13dc870d":"markdown","cbe6bfa8":"markdown","7a636e61":"markdown","88e24c80":"markdown","28caafa4":"markdown","7864913f":"markdown","3872820d":"markdown","6ba296f8":"markdown","afdc5613":"markdown","d3c83f3a":"markdown","7a469242":"markdown"},"source":{"92d9fbb4":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom skimage import exposure\nfrom skimage.filters import rank\nfrom skimage.morphology import disk\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom keras.models import load_model\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, AveragePooling2D\n\nfrom tqdm import tqdm_notebook","cf30dba3":"img_size_target = 101\n\n# removed upsample and downsample code, since resizing is not used","d273a8e4":"train_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\n","e54a0859":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","782b30f6":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","a7314429":"# Simple split of images into training and testing sets\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2)","69d50c70":"# Nikhil Tomar -- Unet with layer concatenation in downblock\n# https:\/\/www.kaggle.com\/nikhilroxtomar\/unet-with-layer-concatenation-in-downblock\n\ndef inception(input_layer, base_name, num_filters=16):\n    # Inception module\n    incep_1x1 = Conv2D(num_filters, (1,1), padding='same', activation='relu', name=base_name + 'incep_1x1')(input_layer)\n    incep_3x3_reduce = Conv2D(int(num_filters*1.5), (1,1), padding='same', activation='relu', name=base_name + 'incep_3x3_reduce')(input_layer)\n    incep_3x3 = Conv2D(num_filters, (3,3), padding='same', activation='relu', name=base_name + 'incep_3x3')(incep_3x3_reduce)\n    incep_5x5_reduce = Conv2D(int(num_filters\/4), (1,1), padding='same', activation='relu', name=base_name + 'incep_5x5_reduce')(input_layer)\n    incep_5x5 = Conv2D(int(num_filters\/2), (5,5), padding='same', activation='relu', name=base_name + 'incep_5x5')(incep_5x5_reduce)\n    incep_pool = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same', name=base_name + 'incep_pool')(input_layer)\n    incep_pool_proj = Conv2D(int(num_filters\/2), (1,1), padding='same', activation='relu', name=base_name + 'incep_pool_proj')(incep_pool)\n    incep_output = concatenate([incep_1x1, incep_3x3, incep_5x5, incep_pool_proj], axis = 3, name=base_name + 'incep_output')\n    return incep_output\n\ndef down_block(input_layer, base_name,  num_filters=16, padding='same', dropout=0.25):\n    conv1 = Conv2D(num_filters, (3, 3), activation=\"relu\", padding=padding, name=base_name + 'conv1')(input_layer)\n    conv2 = Conv2D(num_filters, (3, 3), activation=\"relu\", padding=padding, name=base_name + 'conv2')(conv1)\n    pool = MaxPooling2D((2, 2), name=base_name + 'pool')(conv2)\n    output = Dropout(dropout)(pool)\n    return output, conv2\n\ndef down_inception(input_layer, base_name, num_filters=16, padding='same', dropout=0.25):\n    #First Inception module\n    incep_1_output = inception(input_layer, base_name+\"incep1\", num_filters)\n    #Second Inception module\n    incep_2_output = inception(incep_1_output, base_name+\"incep2\", num_filters)\n    pool_incep = MaxPooling2D((2, 2), name=base_name + 'pool')(incep_2_output)\n    output_layer = Dropout(dropout, name=base_name + 'drop')(pool_incep)\n    return output_layer, incep_2_output\n\n\ndef down_block_resnet(x, num_filters=16, kernel_size=(3, 3), padding='same', activation='relu', pool_size=(2, 2), dropout=0.25):\n    conv = resnet_block(x, num_filters=num_filters, kernel_size=kernel_size, padding=padding,\n    activation=activation)\n    pool = conv\n    if pool_size != None:\n        #pool = MaxPooling2D(pool_size) (conv)\n        pool = Conv2D(num_filters, kernel_size, padding='same', strides=pool_size, activation='tanh') (conv)\n        #pool = Conv2D(num_filters, kernel_size, padding='same', activation='tanh') (pool)\n    if dropout != None:\n        pool = Dropout(dropout) (pool)\n    return pool, conv\n\ndef up_block(uconv_input, conv_input, base_name, num_filters=16, padding='same', dropout=0.25):\n    deconv = Conv2DTranspose(num_filters, (3, 3), strides=(2, 2), padding=padding, name=base_name + 'deconv')(uconv_input)\n    uconv1 = concatenate([deconv, conv_input], name=base_name + 'concate')\n    uconv2 = Dropout(dropout, name=base_name + 'drop')(uconv1)\n    uconv3 = Conv2D(num_filters, (3, 3), padding=\"same\", name=base_name + 'conv1')(uconv2)\n    uconv4 = Conv2D(num_filters, (3, 3), padding=\"same\", name=base_name + 'conv2')(uconv3)\n    return uconv4\n\ndef resnet_block(x, num_filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'):\n    conv1 = Conv2D(num_filters, (7, 7), padding=padding) (x)\n    conv1 = _activation(activation, conv1)\n\n    conv2 = Conv2D(num_filters, (5, 5), padding=padding) (x)\n    conv2 = _activation(activation, conv2)\n\n    conv3 = Conv2D(num_filters, (3, 3), padding=padding) (x)\n    conv3 = _activation(activation, conv3)\n\n    return concatenate([conv1, conv2, conv3, x])\n\ndef Unet_standard(num_filters=16,):\n    input_img = Input((img_size_target, img_size_target, 1), name='img')\n    input_features = Input((1, ), name='feat')\n\n    pool1, conv1 = down_block(input_img, \"down1\", num_filters * 1, dropout=0.3)\n    pool2, conv2 = down_block(pool1, \"down2\", num_filters * 2, dropout=0.5)\n    pool3, conv3 = down_block(pool2, \"down3\", num_filters * 4, dropout=0.5)\n    pool4, conv4 = down_block(pool3, \"down4\", num_filters * 8, dropout=0.5)\n\n    # Middle\n    middle1 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    middle2 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(middle1)\n\n    deconv4 = up_block(middle2, conv4, \"up4\", num_filters * 8, dropout=0.5)\n    deconv3 = up_block(deconv4, conv3, \"up3\", num_filters * 4, padding='valid', dropout=0.5)\n    deconv2 = up_block(deconv3, conv2, \"up2\", num_filters * 2, dropout=0.5)\n    deconv1 = up_block(deconv2, conv1, \"up1\", num_filters * 1, padding='valid', dropout=0.5)\n    deconv1 = Dropout(0.5) (deconv1)\n    output_layer = Conv2D(1, (1, 1), padding='same', activation='sigmoid') (deconv1)\n\n    return Model(inputs=[input_img], outputs=[output_layer])\n\ndef Unet_incept(num_filters=16,):\n    input_img = Input((img_size_target, img_size_target, 1), name='img')\n    input_features = Input((1, ), name='feat')\n\n    pool1, conv1 = down_inception(input_img, \"down1\", num_filters * 1, dropout=0.3)\n    pool2, conv2 = down_inception(pool1, \"down2\", num_filters * 2, dropout=0.5)\n    pool3, conv3 = down_inception(pool2, \"down3\", num_filters * 4, dropout=0.5)\n    pool4, conv4 = down_inception(pool3, \"down4\", num_filters * 8, dropout=0.5)\n\n    # Middle\n    middle1 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    middle2 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(middle1)\n\n    deconv4 = up_block(middle2, conv4, \"up4\", num_filters * 8, dropout=0.5)\n    deconv3 = up_block(deconv4, conv3, \"up3\", num_filters * 4, padding='valid', dropout=0.5)\n    deconv2 = up_block(deconv3, conv2, \"up2\", num_filters * 2, dropout=0.5)\n    deconv1 = up_block(deconv2, conv1, \"up1\", num_filters * 1, padding='valid', dropout=0.5)\n    deconv1 = Dropout(0.5) (deconv1)\n    output_layer = Conv2D(1, (1, 1), padding='same', activation='sigmoid') (deconv1)\n\n    return Model(inputs=[input_img], outputs=[output_layer])\n\n","59aec314":"model = Unet_standard()\n#model = Unet_incept()","a1616ede":"optimizer_Adam = Adam(lr=0.0001)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nmodel.summary()\nmodel.save_weights('imageWeights.h5')","c3b67913":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=0.000001)\nepochs = 5          # REDUCE FROM 100 FOR KAGGLE RUNTIME LIMIT\nbatch_size = 16     # REDUCED TO 16 FOR MEMORY USE\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])","4d0d36e5":"def plot_training_results(history):\n    fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n    ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","99e0af3a":"plot_training_results(history)","9cc65b23":"\n\ndef contrast_stretch(img):\n    # Contrast stretching\n    p2, p98 = np.percentile(img, (2, 98))\n    if (p2==p98):\n        return img      # some images are just one color, so they gerenate an divide by zero error, so return original image\n    img_contrast_stretch = exposure.rescale_intensity(img, in_range=(p2, p98))\n    return img_contrast_stretch\n\ndef equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Equalization\n    img_equalized = exposure.equalize_hist(img)\n    return img_equalized\n\ndef adaptive_equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Adaptive Equalization\n    img_adaptive_equalized = exposure.equalize_adapthist(img, clip_limit=0.03)\n    return img_adaptive_equalized\n\ndef local_equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Local Equalization--for details see http:\/\/scikit-image.org\/docs\/dev\/auto_examples\/color_exposure\/plot_local_equalize.html\n    selem = disk(30)\n    img_local_equal = rank.equalize(img, selem=selem)\n    return img_local_equal\n","27b9eed8":"def display_equalizations(img):\n    fix, axs = plt.subplots(1, 5, figsize=(15,5))\n    axs[0].imshow(img, cmap=\"Greys\")\n    axs[0].set_title(\"Original image\")\n\n    axs[1].imshow(contrast_stretch(img), cmap=\"Greys\")\n    axs[1].set_title(\"Contrast stretching\")\n\n    axs[2].imshow(equalization(img), cmap=\"Greys\")\n    axs[2].set_title(\"Equalized image\")\n\n    axs[3].imshow(adaptive_equalization(img), cmap=\"Greys\")\n    axs[3].set_title(\"Adaptive Equalization image\")\n\n    axs[4].imshow(local_equalization(img), cmap=\"Greys\")\n    axs[4].set_title(\"Local Equalization image\")\n\nimg = train_df.images.loc[ids_train[11]]\ndisplay_equalizations(img)\nimg = train_df.images.loc[ids_train[14]]\ndisplay_equalizations(img)\nimg = train_df.images.loc[ids_train[27]]\ndisplay_equalizations(img)","f04684eb":"# Redo the train\/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train\/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(contrast_stretch).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_contrast_stretch = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_contrast_stretch)","dee2002b":"# Redo the train\/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train\/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_equalization)","8a47ef91":"# Redo the train\/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train\/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(adaptive_equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_adaptive_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_adaptive_equalization)","85cb164c":"# Redo the train\/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train\/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(local_equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\n\nhistory_local_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_local_equalization)","ec3d039c":"\n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1, figsize=(15,15))\n#ax[0].title = \"Loss\"\nax[0].plot(history.history['loss'], color='b', label=\"Stock image\")\nax[0].plot(history_contrast_stretch.history['loss'], color='g', label=\"contrast_stretch \")\nax[0].plot(history_equalization.history['loss'], color='y', label=\"equalization \")\nax[0].plot(history_adaptive_equalization.history['loss'], color='r', label=\"adaptive_equalization \")\nax[0].plot(history_local_equalization.history['loss'], color='c', label=\"local_equalization\")\n\nax[0].plot(history.history['val_loss'], color='b', linestyle=':')\nax[0].plot(history_contrast_stretch.history['val_loss'], color='g', linestyle=':')\nax[0].plot(history_equalization.history['val_loss'], color='y', linestyle=':')\nax[0].plot(history_adaptive_equalization.history['val_loss'], color='r', linestyle=':')\nax[0].plot(history_local_equalization.history['val_loss'], color='c', linestyle=':')\nlinestyle=':'\nlegend = ax[0].legend(loc='best', shadow=True)\n#plt.ylim(0,1)\n\n#ax[0].title = \"Accuracy\"\nax[1].plot(history.history['acc'], color='b', label=\"Stock Image\")\nax[1].plot(history_contrast_stretch.history['acc'], color='g', label=\"contrast_stretch\")\nax[1].plot(history_equalization.history['acc'], color='y', label=\"equalization\")\nax[1].plot(history_adaptive_equalization.history['acc'], color='r', label=\"adaptive_equalization\")\nax[1].plot(history_local_equalization.history['acc'], color='c', label=\"local_equalization\")\n\nax[1].plot(history.history['val_acc'], color='b', linestyle=':')\nax[1].plot(history_contrast_stretch.history['val_acc'], color='g', linestyle=':')\nax[1].plot(history_equalization.history['val_acc'], color='y', linestyle=':')\nax[1].plot(history_adaptive_equalization.history['val_acc'], color='r', linestyle=':')\nax[1].plot(history_local_equalization.history['val_acc'], color='c', linestyle=':')\nlegend = ax[1].legend(loc='best', shadow=True)","77481e73":"#  adaptive_equalized","1e95d6d9":"# Try out differen versions of image equalization\n\nFor details see http:\/\/scikit-image.org\/docs\/dev\/auto_examples\/color_exposure\/plot_equalize.html ","13dc870d":"# Results Graph\nStock images perform better than any equalization technique.","cbe6bfa8":"# Training with basic images","7a636e61":"\n# Testing image equalization options\n\nTry out different ways to do image equalization. See scikit-image's Histogram Equalization documentatin at http:\/\/scikit-image.org\/docs\/dev\/auto_examples\/color_exposure\/plot_equalize.html\n<BR>\nTesting out:\n- Contrast Stretching\n- Equalization\n- Adaptive Equalization\n- Local Equalization\n\n### TBDR - None of these options improved results\n\n### This was origianlly based on \"U-net, dropout, augmentation, stratification\" by Peter H\u00f6nigschmid \nhttps:\/\/www.kaggle.com\/phoenigs\/u-net-dropout-augmentation-stratification\n<br>\nRather than resizing the images from 101x101 to 128x128, this kernel adjusts the padding on U-net \n","88e24c80":"# Contrast stretching","28caafa4":"# Params and helpers","7864913f":"## Issues and suggestions\nI am relatively new to python and these techniques, so suggestions or error corrections are welcome. Some issues I am still aware of:\n<br>\n- Some of the equalization methods generate this error, \"Possible precision loss when converting from float64 to uint16\" but I don't think this affects the results.\n- There should be some easy way to apply the equalizaiton functions to the training\/testing arrays, but I was lazy and just re-ran the train_test_split function each time.\n- I kept the network simple with no dropout or batch normalization layers. While these would improve the results, I don't think they will change the relative performance of the methods.\n- Are there some other image adjustement techniques I should be testing?","3872820d":"# Equalization","6ba296f8":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","afdc5613":"# Display some sample images","d3c83f3a":"# Build model","7a469242":"# Local Equalization"}}