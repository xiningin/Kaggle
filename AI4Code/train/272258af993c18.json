{"cell_type":{"ad256b5c":"code","b17f331d":"code","936136ed":"code","5259803d":"code","5812d5b5":"code","5f843411":"code","2dc3f0eb":"code","6335ca01":"code","a10c73f0":"code","67a941a4":"code","a1964cca":"code","621c5474":"code","7271bb2b":"code","54fd4847":"code","df9ffb46":"code","028280a4":"code","ce49b6e2":"code","e0f8bd91":"code","8867e4e2":"code","da8d4248":"code","f0619222":"code","bbfd4d9e":"code","f162209d":"code","9c033647":"markdown","b20df377":"markdown","d1981bfc":"markdown"},"source":{"ad256b5c":"import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras import optimizers\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n\nimport time\nt_start = time.time()","b17f331d":"cv_total = 5\n#cv_index = 1 -5\n\n\nversion = 1\nbasic_name_ori = f'Unet_resnet_v{version}'\nsave_model_name = basic_name_ori + '.model'\nsubmission_file = basic_name_ori + '.csv'\n\nprint(save_model_name)\nprint(submission_file)","936136ed":"img_size_ori = 101\nimg_size_target = 101\n\ndef upsample(img):# not used\n    return img\n    \ndef downsample(img):# not used\n    return img","5259803d":"# Loading of training\/testing ids and depths\ntrain_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\nlen(train_df)","5812d5b5":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","5f843411":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","2dc3f0eb":"#### Reference  from Heng's discussion\n# https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/63984#382657\ndef get_mask_type(mask):\n    border = 10\n    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n\n    cover = (mask>0.5).sum()\n    if cover < 8:\n        return 0 # empty\n    if cover == ((mask*outer) > 0.5).sum():\n        return 1 #border\n    if np.all(mask==mask[0]):\n        return 2 #vertical\n\n    percentage = cover\/(101*101)\n    if percentage < 0.15:\n        return 3\n    elif percentage < 0.25:\n        return 4\n    elif percentage < 0.50:\n        return 5\n    elif percentage < 0.75:\n        return 6\n    else:\n        return 7\n\ndef histcoverage(coverage):\n    histall = np.zeros((1,8))\n    for c in coverage:\n        histall[0,c] += 1\n    return histall\n\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_target, 2)\n\ntrain_df[\"coverage_class\"] = train_df.masks.map(get_mask_type)\n","6335ca01":"train_all = []\nevaluate_all = []\nskf = StratifiedKFold(n_splits=cv_total, random_state=1234, shuffle=True)\nfor train_index, evaluate_index in skf.split(train_df.index.values, train_df.coverage_class):\n    train_all.append(train_index)\n    evaluate_all.append(evaluate_index)\n    print(train_index.shape,evaluate_index.shape) # the shape is slightly different in different cv, it's OK","a10c73f0":"def get_cv_data(cv_index):\n    train_index = train_all[cv_index-1]\n    evaluate_index = evaluate_all[cv_index-1]\n    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    return x_train,y_train,x_valid,y_valid","67a941a4":"cv_index = 1\ntrain_index = train_all[cv_index-1]\nevaluate_index = evaluate_all[cv_index-1]\n\nprint(train_index.shape,evaluate_index.shape)\nhistall = histcoverage(train_df.coverage_class[train_index].values)\nprint(f'train cv{cv_index}, number of each mask class = \\n \\t{histall}')\nhistall_test = histcoverage(train_df.coverage_class[evaluate_index].values)\nprint(f'evaluate cv{cv_index}, number of each mask class = \\n \\t {histall_test}')\n\nfig, axes = plt.subplots(nrows=2, ncols=8, figsize=(24, 6), sharex=True, sharey=True)\n\n# show mask class example\nfor c in range(8):\n    j= 0\n    for i in train_index:\n        if train_df.coverage_class[i] == c:\n            axes[j,c].imshow(np.array(train_df.masks[i])  )\n            axes[j,c].set_axis_off()\n            axes[j,c].set_title(f'class {c}')\n            j += 1\n            if(j>=2):\n                break","a1964cca":"def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n","621c5474":"# Build model\ndef build_model(input_layer, start_neurons, DropoutRatio = 0.5):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio\/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    #uconv1 = Dropout(DropoutRatio\/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n    output_layer =  Activation('sigmoid')(output_layer_noActi)\n    \n    return output_layer","7271bb2b":"def get_iou_vector(A, B):\n    A = np.squeeze(A) # new added \n    B = np.squeeze(B) # new added\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n            metric.append(1)\n            continue\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0)  )\/ (np.sum(union > 0) )\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)","54fd4847":"def build_complie_model(lr = 0.01):\n    input_layer = Input((img_size_target, img_size_target, 1))\n    output_layer = build_model(input_layer, 16,0.5)\n\n    model1 = Model(input_layer, output_layer)\n\n    c = optimizers.adam(lr = lr)\n    model1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n    return model1","df9ffb46":"def plot_history(history,metric_name):\n    fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\n    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax_loss.legend()\n    ax_score.plot(history.epoch, history.history[metric_name], label=\"Train score\")\n    ax_score.plot(history.epoch, history.history[\"val_\" + metric_name], label=\"Validation score\")\n    ax_score.legend()\n\ndef predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test = model.predict(x_test).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict(x_test_reflect).reshape(-1, img_size_target, img_size_target)\n    preds_test += np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    return preds_test\/2","028280a4":"# training\nious = [0] * cv_total\nfor cv_index in range(cv_total):\n    basic_name = f'Unet_resnet_v{version}_cv{cv_index+1}'\n    print('############################################\\n', basic_name)\n    save_model_name = basic_name + '.model'\n    \n    x_train, y_train, x_valid, y_valid =  get_cv_data(cv_index+1)\n    \n    #Data augmentation\n    x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n    y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n\n    model = build_complie_model(lr = 0.01)\n\n    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',\n                                  factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n    epochs = 20 #small number for demonstration \n    batch_size = 32\n    history = model.fit(x_train, y_train,\n                        validation_data=[x_valid, y_valid], \n                        epochs=epochs,\n                        batch_size=batch_size,\n                        callbacks=[ model_checkpoint,reduce_lr], \n                        verbose=2)\n    plot_history(history,'my_iou_metric')\n    \n    model.load_weights(save_model_name)\n    \n    preds_valid = predict_result(model,x_valid,img_size_target)\n    ious[cv_index] = get_iou_vector(y_valid, (preds_valid > 0.5))\n    \n#model1.summary()","ce49b6e2":"for cv_index in range(cv_total):\n    print(f\"cv {cv_index} ious = {ious[cv_index]}\")","e0f8bd91":"\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","8867e4e2":"x_test = np.array([(np.array(load_img(\"..\/input\/test\/images\/{}.png\".format(idx), grayscale = True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n","da8d4248":"# average the predictions from different folds\nt1 = time.time()\npreds_test = np.zeros(np.squeeze(x_test).shape)\nfor cv_index in range(cv_total):\n    basic_name = f'Unet_resnet_v{version}_cv{cv_index+1}'\n    model.load_weights(basic_name + '.model')\n    preds_test += predict_result(model,x_test,img_size_target) \/cv_total\n    \nt2 = time.time()\nprint(f\"Usedtime = {t2-t1} s\")","f0619222":"\nt1 = time.time()\nthreshold  = 0.5 # some value in range 0.4- 0.5 may be better \npred_dict = {idx: rle_encode(np.round(preds_test[i]) > threshold) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\nt2 = time.time()\n\nprint(f\"Usedtime = {t2-t1} s\")","bbfd4d9e":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv(submission_file)","f162209d":"t_finish = time.time()\nprint(f\"Kernel run time = {(t_finish-t_start)\/3600} hours\")","9c033647":"#### calculate mask type for stratify, the difficuly of training different mask type is different. \n* Reference  from Heng's discussion, search \"error analysis\" in the following link\n\nhttps:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/63984#382657****","b20df377":"### U-net with simple Resnet Blocks v3, StratifiedKFold \n* Make the validation data distribution more close to train and possibly final test data\n* For demonstration only, can use with previous  version or other models to get higher score: \n\n  https:\/\/www.kaggle.com\/shaojiaxin\/u-net-with-simple-resnet-blocks\n  \n  https:\/\/www.kaggle.com\/shaojiaxin\/u-net-with-simple-resnet-blocks-v2-new-loss     \n        \n","d1981bfc":"#### Show  some examples of different mask"}}