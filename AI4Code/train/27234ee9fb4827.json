{"cell_type":{"c923adf4":"code","c9babe02":"code","754fad91":"code","cd311ac8":"code","60abca5a":"code","280e9339":"code","aa50534e":"code","a13c0676":"code","d65fc41c":"code","8c95c0ae":"code","7ca28c38":"code","0b01fc18":"code","a2d2eeab":"markdown","8821e107":"markdown","22371da7":"markdown","ccbdc2f9":"markdown"},"source":{"c923adf4":"from tqdm import tqdm\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import roc_auc_score","c9babe02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","754fad91":"random_state = 42\nnp.random.seed(random_state)\n\ntrain = pd.read_csv(r'..\/input\/train.csv')\ntest = pd.read_csv(r'..\/input\/test.csv')","cd311ac8":"cols = [t for t in train.columns if 'var' in t]","60abca5a":"def little_minimizer(series_pos, series_neg):\n    max_pos = series_pos.max()\n    min_pos = series_pos.min()\n    \n    max_neg = series_neg.max()\n    min_neg = series_neg.min()\n    \n    return [[min_pos, max_pos], [min_neg, max_neg]]\n\ndef create_minimizer(df, teach_cols, target_column = 'target'):\n    dicty = {}\n    for c in teach_cols:\n        dicty[c] = little_minimizer(df[df[target_column] == 1][c], df[df[target_column] == 0][c])\n    return dicty\n\ndef minimize_features(array, min_dict, cols, target):\n    counter = 0\n    array = array.tolist()\n    for i, a in enumerate(array):\n        if a < min_dict[cols[i]][1 - target][0] or a > min_dict[cols[i]][1 - target][1]:\n            counter += 1\n    return counter","280e9339":"minimizer = create_minimizer(train, cols, target_column = 'target')\ntrain['var_positive'] = train[cols].apply(lambda x: minimize_features(x, minimizer, cols, 1), axis = 1)","aa50534e":"train.groupby('var_positive')['target'].agg(['mean', 'count'])","a13c0676":"train = train.sort_values(by = 'target', ascending = False)\nfor c in tqdm(range(len(cols))):\n    train[cols[c]] = pd.concat([\n                            train[train['target'] == 1][cols[c]].sort_values(ascending = False),\n                            train[train['target'] == 0][cols[c]].sort_values(ascending = False)\n                         ]).values","d65fc41c":"m1 = [81, 139, 12, 146, 76, 174, 21, 80, 166, 165, 13, 148, 198, 34, 115, 109, 44, 169, 149, 92, 108, 154, 33, 9, 192, 122, 121, 86, 123, 107, 127, 36, 172, 75, 177, 197, 87, 56, 93, 188, 131, 186, 141, 43, 104, 150, 31, 132, 23, 114, 58, 28, 116, 85, 194, 83]\nm2 = [6, 110, 53, 26, 22, 99, 190, 2, 133, 0, 179, 1, 40, 184, 170, 78, 191, 94, 67, 18, 173, 118, 164, 89, 91, 147, 95, 35, 155, 106, 71, 157, 48, 162, 180, 163, 5, 145, 119, 32, 130, 49, 167, 90, 24, 195, 135, 151, 125, 128, 111, 52, 137, 70, 105, 51, 112, 199, 66, 82, 196, 175, 11, 74, 144, 8]\ns = [26, 81, 139, 110, 12, 2, 22, 80, 53, 146, 179, 198, 99, 44, 0, 174, 76, 6, 166, 148, 133, 191, 40, 109, 190, 13, 123, 170, 165, 86, 108, 94, 21, 78, 1, 154, 184, 163, 91, 95, 75, 18, 93, 157, 89, 34, 119, 180, 115, 164, 92, 155, 9, 147, 56, 188, 122, 33, 130, 169, 5, 135, 51, 125, 141, 106, 151, 197, 162, 195, 172, 127, 121, 67, 111, 177, 173, 145, 132, 32, 43, 114, 131, 49, 36, 167, 88, 35, 107, 87, 175, 83, 149, 118, 196, 168, 150]","8c95c0ae":"x_train = scale(train.iloc[:, 2:])","7ca28c38":"train[\"prediction\"] = np.std(x_train[:, s], axis=1) + np.mean(x_train[:, m2], axis=1) - np.mean(x_train[:, m1], axis=1)","0b01fc18":"roc_auc_score(train['target'], train['prediction'])","a2d2eeab":"I spent 2 days for this competition trying to find the magic =). Still in the process but may be it will be interesting and useful for someone","8821e107":"You can see that for 1% of data we can find target = 0 with 100% accuracy. That comes from observation that [min, max] range for 0's and 1's is different","22371da7":"**Interesting observation 1.** Seems like by this condition we can fix target = 0 with 100% accuracy","ccbdc2f9":"![](http:\/\/)**Interesting observation 2.** Seems like by this we can easily classify train with 100% accuracy. Not sure how to use for test though =)"}}