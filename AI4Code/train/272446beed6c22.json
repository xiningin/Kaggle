{"cell_type":{"df58a12c":"code","cce8edd8":"code","52c1379f":"code","75288d3e":"code","336e021b":"code","aacb67cd":"code","18eb558b":"code","b4a9932c":"code","e7d5c1b9":"code","34da4480":"code","4cd6a937":"code","b501f024":"code","c7274830":"code","4ae032dc":"code","cf371587":"code","b85fcdbe":"code","286a7ece":"code","b70b800a":"code","8ed4112e":"code","32603b06":"code","c24ee2b3":"code","51705434":"code","6415aedf":"code","7d958079":"code","bd30dc8a":"markdown","0cdaaff5":"markdown","63684e4b":"markdown","96dca858":"markdown","8896ef59":"markdown","dd611546":"markdown","1fd575b0":"markdown","0a6a0f5f":"markdown","3fc63604":"markdown"},"source":{"df58a12c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","cce8edd8":"df = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")","52c1379f":"df.head()","75288d3e":"df.info()","336e021b":"df.drop(\"id\",axis = 1,inplace=True)","aacb67cd":"df.isnull().sum()","18eb558b":"#distributions\ndf_Stat=df[['label','tweet']].groupby('label').count().reset_index()\ndf_Stat.columns=['label','count']\ndf_Stat['percentage']=(df_Stat['count']\/df_Stat['count'].sum())*100\ndf_Stat","b4a9932c":"df['length'] = df['tweet'].apply(len)\ndf.head(10)","e7d5c1b9":"#Exploratory Data Analysis\n\nsns.barplot('label','length',data = df,palette='PRGn')\nplt.title('Average Word Length vs label')\nplt.show()","34da4480":"sns.countplot(x= 'label',data = df,palette=\"PRGn\")\nplt.title('Label Counts')\nplt.show()","4cd6a937":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\nsns.histplot(df[df[\"label\"] == 1][\"length\"],bins = 30,ax = ax1, kde=True).set(title = \"normsl tweets\")\nsns.histplot(df[df[\"label\"] == 0][\"length\"],bins = 30,ax = ax2, kde = True).set(title = \"Hate tweets\")\nplt.show()","b501f024":"nltk.download('stopwords')","c7274830":"from nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\ndef process_tweet(tweet):\n    tweet =  \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())\n    tweet = nltk.word_tokenize(tweet)\n    stemmer = PorterStemmer()\n    stem = [stemmer.stem(word) for word in tweet]\n    words = [word for word in stem if word not in stopwords.words('english')]\n    tweet = \" \".join(words)\n    \n    return tweet\n\ndf[\"clean_tweet\"] = df[\"tweet\"].apply(process_tweet)\ndf.head()","4ae032dc":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ntext = \" \".join(review for review in df.clean_tweet)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(text)\nfig = plt.figure(figsize = (10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"To Create Cloud of words for all words\")\nplt.show()","cf371587":"from sklearn.feature_extraction.text import TfidfVectorizer","b85fcdbe":"vectorizer = TfidfVectorizer(use_idf=True)\nX = vectorizer.fit_transform(df[\"clean_tweet\"])","286a7ece":"# df1 = pd.DataFrame(X)\n# df1.columns = vectorizer.get_feature_names()\n# df1.head()","b70b800a":"from sklearn.model_selection import train_test_split","8ed4112e":"df[\"label\"].unique()","32603b06":"X_train,X_test,y_train,y_test = train_test_split(X,df[\"label\"],test_size = 0.2,random_state = 42)","c24ee2b3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n\nclf_A = LogisticRegression()\nclf_B = AdaBoostClassifier()\nclf_C = DecisionTreeClassifier()\nclf_D = SVC()\nclf_E = RandomForestClassifier()\nclf_F = MultinomialNB()\nclfs = [clf_A,clf_B,clf_C,clf_D,clf_E,clf_F]","51705434":"df_score = pd.DataFrame(index=None, columns=['model','recall_score','precision_score','f1_score','accuracy-score'])\nfor clf in clfs:\n    clf.fit(X_train,y_train)\n    pred = clf.predict(X_test)\n    score1 = recall_score(y_test,pred)\n    score2 = precision_score(y_test,pred)\n    score3 = f1_score(y_test,pred)\n    acuracy_score = accuracy_score(y_test,pred)\n  \n    df_score = df_score.append(pd.Series({\n                \"model\" : clf.__class__.__name__,\n                \"recall_score\" : score1,\n                \"precision_score\" : score2,\n                \"f1_score\" : score3,\n                \"accuracy-score\" : acuracy_score}),ignore_index = True)\n\ndf_score","6415aedf":"from sklearn.model_selection import GridSearchCV\nparameters = {\n    'max_depth': [1,2,5,10,15,50],\n    'max_features': ['auto','sqrt','log2'],\n    'criterion' : ['gini', 'entropy'],\n    'splitter' : ['best', 'random'],\n    'max_leaf_nodes': [1,2,5,10],\n}\n\nfrom sklearn.metrics import make_scorer\nscorer = make_scorer(recall_score)\n\ngrid_obj = GridSearchCV(clf_C, parameters, scoring=scorer, cv=5,verbose = 1,n_jobs = -1)\n# Fit the data\ngrid_fit = grid_obj.fit(X_train, y_train)\nbest_clf = grid_fit.best_estimator_","7d958079":"best_clf.get_params()","bd30dc8a":"## Project Overview\n\nUse of Social media has been increasing day by day and also hate Speech is also increasing along with number of users. So it is tough challenge for companies to monitor each and every tweet of users, so we are developing a machine learning model to identify the hate speech tweets automatically which saves lot of resources for companies","0cdaaff5":"**Most frequent Word in tweet**","63684e4b":"<i>Please upvote  and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself.<\/i>","96dca858":"**Observations:**\n* There are `0` Null value in data","8896ef59":"**Character count**","dd611546":"**The distribution of both seems to be almost same. 90 to 120 characters in a tweet are the most common among both.**","1fd575b0":"### Preprocessing the tweet column","0a6a0f5f":"----","3fc63604":"**Here, Accurate Model is DecisionTreeClassifier.**"}}