{"cell_type":{"30f8b1cc":"code","60fc9b89":"code","53e997e3":"code","11ae4bc7":"code","c006e8e7":"code","cd082ad7":"code","17a237ce":"code","b91d7a7e":"code","4ff940f2":"code","17c1746b":"code","c0d47118":"code","a877230b":"code","14172a19":"markdown","e502e6ce":"markdown","a8f6b330":"markdown","e07b5f5d":"markdown","6961a632":"markdown","86bff916":"markdown","fcf1a5f5":"markdown","7a24c4e7":"markdown","47aef8cc":"markdown","a5d7961c":"markdown","2240ee55":"markdown"},"source":{"30f8b1cc":"! ls ..\/input\/severstal-unet-se-resnext50","60fc9b89":"! python ..\/input\/mlcomp\/mlcomp\/setup.py","53e997e3":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.jit import load\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap","11ae4bc7":"eff_net = load('..\/input\/severstaleffnet\/traced_effnetb7_mixup_retrain.pth').cuda()\ncls = load('..\/input\/severstall-effnetb0-fimal-stage\/traced_effnetb0_averaged.pth').cuda()","c006e8e7":"import functools\nimport math\nimport re\nfrom collections import OrderedDict\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\nfrom torchvision.models.densenet import DenseNet\nfrom torchvision.models.resnet import BasicBlock, ResNet\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\nclass SENetEncoder(SENet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n\n        del self.last_linear\n        del self.avg_pool\n\n    def forward(self, x):\n        for module in self.layer0[:-1]:\n            x = module(x)\n\n        x0 = x\n        x = self.layer0[-1](x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nsenet_encoders = {\n    'senet154': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['senet154'],\n        'out_shapes': (2048, 1024, 512, 256, 128),\n        'params': {\n            'block': SEBottleneck,\n            'dropout_p': 0.2,\n            'groups': 64,\n            'layers': [3, 8, 36, 3],\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet50': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['se_resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet101': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['se_resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet152': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['se_resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 8, 36, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext50_32x4d': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['se_resnext50_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext101_32x4d': {\n        'encoder': SENetEncoder,\n        #         'pretrained_settings': pretrained_settings['se_resnext101_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n}\n\n\nclass ResNetEncoder(ResNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.fc\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu(x0)\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)\n\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    'resnet18': {\n        'encoder': ResNetEncoder,\n        #         'pretrained_settings': pretrained_settings['resnet18'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [2, 2, 2, 2],\n        },\n    },\n\n    'resnet34': {\n        'encoder': ResNetEncoder,\n        #         'pretrained_settings': pretrained_settings['resnet34'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet50': {\n        'encoder': ResNetEncoder,\n        #         'pretrained_settings': pretrained_settings['resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet101': {\n        'encoder': ResNetEncoder,\n        #         'pretrained_settings': pretrained_settings['resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n        },\n    },\n\n    'resnet152': {\n        'encoder': ResNetEncoder,\n        #         'pretrained_settings': pretrained_settings['resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 8, 36, 3],\n        },\n    },\n}\n\n\nclass DenseNetEncoder(DenseNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.classifier\n        self.initialize()\n\n    @staticmethod\n    def _transition(x, transition_block):\n        for module in transition_block:\n            x = module(x)\n            if isinstance(module, nn.ReLU):\n                skip = x\n        return x, skip\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n\n        x = self.features.conv0(x)\n        x = self.features.norm0(x)\n        x = self.features.relu0(x)\n        x0 = x\n\n        x = self.features.pool0(x)\n        x = self.features.denseblock1(x)\n        x, x1 = self._transition(x, self.features.transition1)\n\n        x = self.features.denseblock2(x)\n        x, x2 = self._transition(x, self.features.transition2)\n\n        x = self.features.denseblock3(x)\n        x, x3 = self._transition(x, self.features.transition3)\n\n        x = self.features.denseblock4(x)\n        x4 = self.features.norm5(x)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict):\n        pattern = re.compile(\n            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        # remove linear\n        state_dict.pop('classifier.bias')\n        state_dict.pop('classifier.weight')\n\n        super().load_state_dict(state_dict)\n\n\ndensenet_encoders = {\n    'densenet121': {\n        'encoder': DenseNetEncoder,\n        #         'pretrained_settings': pretrained_settings['densenet121'],\n        'out_shapes': (1024, 1024, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 24, 16),\n        }\n    },\n\n    'densenet169': {\n        'encoder': DenseNetEncoder,\n        #         'pretrained_settings': pretrained_settings['densenet169'],\n        'out_shapes': (1664, 1280, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 32, 32),\n        }\n    },\n\n    'densenet201': {\n        'encoder': DenseNetEncoder,\n        #         'pretrained_settings': pretrained_settings['densenet201'],\n        'out_shapes': (1920, 1792, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 48, 32),\n        }\n    },\n\n    'densenet161': {\n        'encoder': DenseNetEncoder,\n        #         'pretrained_settings': pretrained_settings['densenet161'],\n        'out_shapes': (2208, 2112, 768, 384, 96),\n        'params': {\n            'num_init_features': 96,\n            'growth_rate': 48,\n            'block_config': (6, 12, 36, 24),\n        }\n    },\n\n}\n\nencoders = {}\nencoders.update(resnet_encoders)\n# encoders.update(dpn_encoders)\n# encoders.update(vgg_encoders)\nencoders.update(senet_encoders)\nencoders.update(densenet_encoders)\n\n\ndef get_encoder(name, encoder_weights=None):\n    Encoder = encoders[name]['encoder']\n    encoder = Encoder(**encoders[name]['params'])\n    encoder.out_shapes = encoders[name]['out_shapes']\n\n    if encoder_weights is not None:\n        settings = encoders[name]['pretrained_settings'][encoder_weights]\n        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_params(encoder_name, pretrained='imagenet'):\n    settings = encoders[encoder_name]['pretrained_settings']\n\n    if pretrained not in settings.keys():\n        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n\n    formatted_settings = {}\n    formatted_settings['input_space'] = settings[pretrained].get('input_space')\n    formatted_settings['input_range'] = settings[pretrained].get('input_range')\n    formatted_settings['mean'] = settings[pretrained].get('mean')\n    formatted_settings['std'] = settings[pretrained].get('std')\n    return formatted_settings\n\n\ndef get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n    return functools.partial(preprocess_input, **params)\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\nclass EncoderDecoder(Model):\n\n    def __init__(self, encoder, decoder, activation):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == 'softmax':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError('Activation should be \"sigmoid\"\/\"softmax\"\/callable\/None')\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s `encoder` and `decoder` (return logits!)\"\"\"\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)`\n        and apply activation function (if activation is not `None`) with `torch.no_grad()`\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n            if self.activation:\n                x = self.activation(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True, attention_type=None):\n        super().__init__()\n        if attention_type is None:\n            self.attention1 = nn.Identity()\n            self.attention2 = nn.Identity()\n        elif attention_type == 'scse':\n            self.attention1 = SCSEModule(in_channels)\n            self.attention2 = SCSEModule(out_channels)\n\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.block(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(DecoderBlock):\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UnetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels=(256, 128, 64, 32, 16),\n            final_channels=1,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None\n    ):\n        super().__init__()\n\n        if center:\n            channels = encoder_channels[0]\n            self.center = CenterBlock(channels, channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = None\n\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.final_conv = nn.Conv2d(out_channels[4], final_channels, kernel_size=(1, 1))\n\n        self.initialize()\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        if self.center:\n            encoder_head = self.center(encoder_head)\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n\n\nclass Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                      stride=stride, padding=padding, bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass Unet(EncoderDecoder):\n    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        center: if ``True`` add ``Conv2dReLU`` block on encoder head (useful for VGG models)\n        attention_type: attention module used in decoder of the model\n            One of [``None``, ``scse``]\n    Returns:\n        ``torch.nn.Module``: **Unet**\n    .. _Unet:\n        https:\/\/arxiv.org\/pdf\/1505.04597\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            decoder_use_batchnorm=True,\n            decoder_channels=(256, 128, 64, 32, 16),\n            classes=1,\n            activation='sigmoid',\n            center=False,  # usefull for VGG models\n            attention_type=None\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = UnetDecoder(\n            encoder_channels=encoder.out_shapes,\n            decoder_channels=decoder_channels,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n            center=center,\n            attention_type=attention_type\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = 'u-{}'.format(encoder_name)","cd082ad7":"# Loading model plain torch. You need the source code that defines them in order to load them\nunet2 = Unet('se_resnext50_32x4d', classes=4, encoder_weights=None, activation='softmax')\nckpt_path = '..\/input\/weight-segmentation\/se_resnext50_32x4d_Unet_checkpoint_185.pth'\ndevice = torch.device('cuda')\nunet2 = unet2.to(device)\nunet2.eval()\nprint() # So we don't see long info\n# state = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n# unet2.load_state_dict(state[\"state_dict\"])","17a237ce":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            p, label = self.models[0](x)\n            res.append(p)\n            for m in self.models[1:]:\n                res.append(m(x)[0])\n        res = torch.stack(res)\n        return torch.mean(res, dim=0), label\n\nmodel = eff_net","b91d7a7e":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n        ),\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '\/kaggle\/input\/severstal-steel-defect-detection\/test_images'\nbatch_size = 1\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [A.HorizontalFlip(p=1)],\n    [A.VerticalFlip(p=1)],\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","4ff940f2":"def tta_mean(preds):\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    return preds.detach().cpu().numpy()\n\nthresholds = [0.6, 0.99, 0.6, 0.6] # [0.5, 0.5, 0.5, 0.5] | [0.55, 0.55, 0.55, 0.55] | [0.6, 0.99, 0.6, 0.6]\ncls_tresholds = [0.2, 0.25, 0.2, 0.1]\nmin_area = [600, 600, 1000, 2000] # instead of 900 -> 1000 in old version\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])\/\/batch_size\nwith torch.no_grad():\n    for loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n        preds = []\n        preds_aux = []\n        image_file = []\n        labels = []\n        for i, batch in enumerate(loaders_batch):\n            features = batch['features'].cuda()\n            pred_aux, label = cls(features)\n            pred_raw, _ = model(features)\n            labels.append(label)\n            p = torch.sigmoid(pred_raw)\n            p_aux = torch.sigmoid(pred_aux)\n            image_file = batch['image_file']\n\n            # inverse operations for TTA\n            p = datasets[i].inverse(p)\n            p_aux = datasets[i].inverse(p_aux)\n            preds.append(p)\n            preds_aux.append(p_aux)\n    \n        # TTA mean\n        preds = tta_mean(preds)\n        preds_aux = tta_mean(preds_aux)\n        labels = tta_mean(labels)\n        labels = labels[0] \n    \n        # Batch post processing\n        for p, p_aux, file in zip(preds, preds_aux, image_file):\n            file = os.path.basename(file)\n            # Image postprocessing\n            for i in range(4):\n                p_channel = np.zeros((256, 1600), dtype=np.uint8)\n                imageid_classid = file+'_'+str(i+1)\n                if labels[i] > cls_tresholds[i]:\n                    p_channel = p[i]\n                    p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n                    if p_channel.sum() < min_area[i]:\n                        p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n                    else:\n                        p_channel = (p[i] + p_aux[i]) \/ 2  # Take mean\n                        p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n                        if p_channel.sum() < min_area[i]:\n                            p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n                res.append({\n                    'ImageId_ClassId': imageid_classid,\n                    'EncodedPixels': mask2rle(p_channel)\n                })\n        \ndf = pd.DataFrame(res)\ndf.to_csv('submission.csv', index=False)","17c1746b":"df = pd.DataFrame(res)\ndf = df.fillna('')\ndf.to_csv('submission.csv', index=False)","c0d47118":"df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\ndf['empty'] = df['EncodedPixels'].map(lambda x: not x)\ndf[df['empty'] == False]['Class'].value_counts()","a877230b":"# %matplotlib inline\n\n# df = pd.read_csv('submission.csv')[:40]\n# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\n# for i, row in enumerate(df.itertuples()):\n# #     img_path = '\/kaggle\/input\/tmk-pics\/IMG_99642' + str(i \/\/ 4) + '_crop.jpg'\n#     print(row)\n#     img_name = row.ImageId_ClassId\n#     img_name = img_name.rsplit('_', 1)[0]\n#     img_path = '\/kaggle\/input\/tmk-pics\/' + img_name\n#     img = cv2.imread(img_path)\n#     mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n#         if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n# #     if mask.sum() == 0:\n# #         continue\n    \n#     fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n    \n#     axes[0].imshow(img\/255)\n#     cv2.imwrite('.\/mask_' + img_name, mask*60)\n#     axes[1].imshow(mask*60)\n#     axes[0].set_title(row.Image)\n#     axes[1].set_title(row.Class)\n#     plt.show()","14172a19":"Histogram of predictions","e502e6ce":"Save predictions","a8f6b330":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation","e07b5f5d":"### Import required libraries","6961a632":"### Create TTA transforms, datasets, loaders","86bff916":"### Load models","fcf1a5f5":"### Loaders' mean aggregator","7a24c4e7":"### Install MLComp library(offline version):","47aef8cc":"### Visualization","a5d7961c":"Catalyst allows to trace models. That is an extremely useful features in Pytorch since 1.0 version: \n\nhttps:\/\/pytorch.org\/docs\/stable\/jit.html\n\nNow we can load models without re-defining them","2240ee55":"### Models' mean aggregator"}}