{"cell_type":{"f8fb6acd":"code","f4033868":"code","b8ef311b":"code","9cc76c9c":"code","cf1dea44":"code","9799035c":"code","7cbb5aee":"code","c5921762":"code","c2a2a551":"code","4e7cfddb":"code","0e1b589f":"code","5bfa94a4":"code","8ee5f2ac":"code","962ed727":"code","5e672741":"code","dae59170":"code","700844de":"code","f5d024dd":"code","5b517816":"code","414e9f98":"code","12046067":"code","0e9315e1":"code","3b08cec4":"code","965f5ee1":"code","8383140a":"code","238050d5":"code","33fb48cf":"code","a053775f":"code","322ab9b2":"code","401b07df":"code","12f4b45d":"code","f778c1ef":"code","3a41cca8":"code","5af8312d":"code","9fe506f1":"code","579dd23a":"code","c90e5111":"code","344288a4":"code","b1d8c8a9":"code","3c7be345":"code","7a012dd2":"code","01937d09":"code","da9cb9bd":"markdown","c66b7bb2":"markdown","f50ffa81":"markdown","af2218e9":"markdown","47e264b9":"markdown","428e7da0":"markdown","d46e2d49":"markdown","ae825ba4":"markdown","068df46a":"markdown"},"source":{"f8fb6acd":"# This Python 3 environment comes with many helpful analytics libraries installed\n\n## =====================\n## IMPORT LIBRARIES\n## =====================\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt     # for visualization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model # for model 1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor # for model 2\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f4033868":"## =====================\n## IMPORT DATA\n## =====================\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","b8ef311b":"# Inspect size of data\nprint(\"Train data shape:\", train.shape)\nprint(\"Test data shape:\", test.shape)\n\n# look at a few rows using the DataFrame.head() method\n# train.head()\nprint(train.head())","9cc76c9c":"missing = train.isnull().sum()\nmissing = missing[missing>0]\nmissing.sort_values(inplace=True)\nprint(missing)","cf1dea44":"# Basic information like count, mean, std, min, max etc\n# train.SalePrice.describe()\nprint (train.SalePrice.describe())","9799035c":"# Plot settings\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Plot histogram of SalePrice\nprint (\"Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()","7cbb5aee":"# Use np.log() to transform train.SalePric and calculate the skewness a second time\n# Re-plot histogram\ntarget = np.log1p(train.SalePrice)\nprint (\"\\n Skew is:\", target.skew())\nplt.hist(target, color='blue')\nplt.show()","c5921762":"# consider the non-numeric features and display details of columns\ncategoricals = train.select_dtypes(exclude=[np.number])\n#categoricals.describe()\nprint(categoricals.describe())\nprint(categoricals.dtypes)","c2a2a551":"# return a subset of columns matching the specified data types\nnumeric_features = train.select_dtypes(include=[np.number])\n# numeric_features.dtypes\nprint(numeric_features.dtypes)","4e7cfddb":"# displays the correlation between the columns and examine the correlations between the features and the target.\ncorr = numeric_features.corr()\n\n# The first five features are the most positively correlated with SalePrice, while the next five are the most negatively correlated.\nprint (corr['SalePrice'].sort_values(ascending=False)[:6], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","0e1b589f":"#to get the unique values that a particular column has.\n#train.OverallQual.unique()\nprint(train.OverallQual.unique())","5bfa94a4":"#investigate the relationship between OverallQual and SalePrice.\n#We set index='OverallQual' and values='SalePrice'. We chose to look at the median here.\nquality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\nprint(quality_pivot)","8ee5f2ac":"#visualize this pivot table more easily, we can create a bar plot\n#Notice that the median sales price strictly increases as Overall Quality increases.\nquality_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","962ed727":"#to generate some scatter plots and visualize the relationship between the Ground Living Area(GrLivArea) and SalePrice\nplt.scatter(x=train['GrLivArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Above grade (ground) living area square feet')\nplt.show()\n","5e672741":"# do the same for GarageArea.\nplt.scatter(x=train['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()\n","dae59170":"# train = train[train['GrLivArea'] < 4000]\n\n# create a new dataframe with some outliers removed\ntrain = train[train['GarageArea'] < 1200]\n\n# display the previous graph again without outliers\nplt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))\nplt.xlim(-200,1400)     # This forces the same scale as before\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","700844de":"# create a DataFrame to view the top null columns and return the counts of the null values in each column\nnulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])\nnulls.columns = ['Null Count']\nnulls.index.name = 'Feature'\n#nulls\nprint(nulls)","f5d024dd":"#to return a list of the unique values\nprint (\"Unique values are:\", train.MiscFeature.unique())","5b517816":"# consider the non-numeric features and display details of columns\ncategoricals = train.select_dtypes(exclude=[np.number])\n#categoricals.describe()\nprint(categoricals.describe())","414e9f98":"# When transforming features, it's important to remember that any transformations that you've applied to the training data before\n# fitting the model must be applied to the test data.\n\n#Eg:\nprint (\"Original: \\n\")\nprint (train.Street.value_counts(), \"\\n\")\n","12046067":"# our model needs numerical data, so we will use one-hot encoding to transform the data into a Boolean column.\n# create a new column called enc_street. The pd.get_dummies() method will handle this for us\ntrain['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\ntest['enc_street'] = pd.get_dummies(test.Street, drop_first=True)\n\nprint ('Encoded: \\n')\nprint (train.enc_street.value_counts())  # Pave and Grvl values converted into 1 and 0","0e9315e1":"# look at SaleCondition by constructing and plotting a pivot table, as we did above for OverallQual\ncondition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","3b08cec4":"# Dealing with missing values\ndata = train.select_dtypes(include=[np.number]).interpolate().dropna()\n\n# Check if the all of the columns have 0 null values.\n# sum(data.isnull().sum() != 0)\nprint(sum(data.isnull().sum() != 0))","965f5ee1":"# separate the features and the target variable for modeling.\n# We will assign the features to X and the target variable(Sales Price)to y.\n\ny = np.log(train.SalePrice)\nX = data.drop(['SalePrice', 'Id'], axis=1)\n# exclude ID from features since Id is just an index with no relationship to SalePrice.\n\n#======= partition the data ===================================================================================================#\n#   Partitioning the data in this way allows us to evaluate how our model might perform on data that it has never seen before.\n#   If we train the model on all of the test data, it will be difficult to tell if overfitting has taken place.\n#==============================================================================================================================#\n# also state how many percentage from train data set, we want to take as test data set\n# In this example, about 33% of the data is devoted to the hold-out set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n\n#========= Begin modelling =========================#\n#    Linear Regression Model                        #\n#===================================================#\n\n# ---- first create a Linear Regression model.\n# First, we instantiate the model.\nlr = linear_model.LinearRegression()\n\n# ---- fit the model \/ Model fitting\n# lr.fit() method will fit the linear regression on the features and target variable that we pass.\nmodel_1 = lr.fit(X_train, y_train)\n\n# ---- Evaluate the performance and visualize results\n# r-squared value is a measure of how close the data are to the fitted regression line\n# a higher r-squared value means a better fit(very close to value 1)\nprint(\"R^2 is: \\n\", model_1.score(X_test, y_test))","8383140a":"# use the model we have built to make predictions on the test data set.\npredictions = model_1.predict(X_test)\n\n# calculates the rmse\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions))","238050d5":"# view this relationship between predictions and actual_values graphically with a scatter plot.\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","33fb48cf":"# create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.\nsubmission = pd.DataFrame()\n# The first column must the contain the ID from the test data.\nsubmission['Id'] = test.Id\n\n# select the features from the test data for the model as we did above.\nfeats = test.select_dtypes(\n    include=[np.number]).drop(['Id'], axis=1).interpolate()\n\n# generate predictions\npredictions = model_1.predict(feats)\n\n# transform the predictions to the correct form\n# apply np.exp() to our predictions becasuse we have taken the logarithm(np.log()) previously.\nfinal_predictions = np.exp(predictions)\n\n# check the difference\nprint(\"Original predictions are: \\n\", predictions[:10], \"\\n\")\nprint(\"Final predictions are: \\n\", final_predictions[:10])","a053775f":"# assign these predictions and check\nsubmission['SalePrice'] = final_predictions\n# submission.head()\nprint(submission.head())","322ab9b2":"# export to a .csv file as Kaggle expects.\n# pass index=False because Pandas otherwise would create a new index for us.\nsubmission.to_csv('submission1.csv', index=False)","401b07df":"# create a new dataframe with some outliers removed\n# train = train[train['GrLivArea'] < 4000]\n\n# display the previous graph again without outliers\nplt.scatter(x=train['GrLivArea'], y=np.log(train.SalePrice))\nplt.xlim(-200,4200)     # This forces the same scale as before\nplt.ylabel('Sale Price')\nplt.xlabel('GrLivArea')\nplt.show()","12f4b45d":"# separate the features and the target variable for modeling.\n# We will assign the features to X and the target variable(Sales Price)to y.\n\ny = np.log(train.SalePrice)\nX = data.drop(['SalePrice', 'Id'], axis=1)\n# exclude ID from features since Id is just an index with no relationship to SalePrice.\n\n#======= partition the data ===================================================================================================#\n#   Partitioning the data in this way allows us to evaluate how our model might perform on data that it has never seen before.\n#   If we train the model on all of the test data, it will be difficult to tell if overfitting has taken place.\n#==============================================================================================================================#\n# also state how many percentage from train data set, we want to take as test data set\n# In this example, about 33% of the data is devoted to the hold-out set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n\n#========= Begin modelling =========================#\n#    Linear Regression Model                        #\n#===================================================#\n\n# ---- first create a Linear Regression model.\n# First, we instantiate the model.\nlr = linear_model.LinearRegression()\n\n# ---- fit the model \/ Model fitting\n# lr.fit() method will fit the linear regression on the features and target variable that we pass.\nmodel_1 = lr.fit(X_train, y_train)\n\n# ---- Evaluate the performance and visualize results\n# r-squared value is a measure of how close the data are to the fitted regression line\n# a higher r-squared value means a better fit(very close to value 1)\nprint(\"R^2 is: \\n\", model_1.score(X_test, y_test))","f778c1ef":"# use the model we have built to make predictions on the test data set.\npredictions = model_1.predict(X_test)\n\n# calculates the rmse\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions))","3a41cca8":"# view this relationship between predictions and actual_values graphically with a scatter plot.\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model - amended 1-1')\nplt.show()","5af8312d":"# create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.\nsubmission = pd.DataFrame()\n# The first column must the contain the ID from the test data.\nsubmission['Id'] = test.Id\n\n# select the features from the test data for the model as we did above.\nfeats = test.select_dtypes(\n    include=[np.number]).drop(['Id'], axis=1).interpolate()\n\n# generate predictions\npredictions = model_1.predict(feats)\n\n# transform the predictions to the correct form\n# apply np.exp() to our predictions becasuse we have taken the logarithm(np.log()) previously.\nfinal_predictions = np.exp(predictions)\n\n# check the difference\nprint(\"Original predictions are: \\n\", predictions[:10], \"\\n\")\nprint(\"Final predictions are: \\n\", final_predictions[:10])","9fe506f1":"# assign these predictions and check\nsubmission['SalePrice'] = final_predictions\n# submission.head()\nprint(submission.head())","579dd23a":"# export to a .csv file as Kaggle expects.\n# pass index=False because Pandas otherwise would create a new index for us.\nsubmission.to_csv('submission1-1.csv', index=False)","c90e5111":"regressor = RandomForestRegressor(n_estimators=10)\n\nmodel_2 = regressor.fit(X_train, y_train)\n\nprint(\"R^2 is: \\n\", model_2.score(X_test, y_test))","344288a4":"# use the model we have built to make predictions on the test data set.\npredictions = model_2.predict(X_test)\n\n# calculates the rmse\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions))","b1d8c8a9":"# view this relationship between predictions and actual_values graphically with a scatter plot.\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Random Forest Regression Model')\nplt.show()","3c7be345":"# create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.\nsubmission = pd.DataFrame()\n# The first column must the contain the ID from the test data.\nsubmission['Id'] = test.Id\n\n# select the features from the test data for the model as we did above.\nfeats = test.select_dtypes(\n    include=[np.number]).drop(['Id'], axis=1).interpolate()\n\n# generate predictions\npredictions = model_2.predict(feats)\n\n# transform the predictions to the correct form\n# apply np.exp() to our predictions becasuse we have taken the logarithm(np.log()) previously.\nfinal_predictions = np.exp(predictions)\n\n# check the difference\nprint(\"Original predictions are: \\n\", predictions[:10], \"\\n\")\nprint(\"Final predictions are: \\n\", final_predictions[:10])","7a012dd2":"# assign these predictions and check\nsubmission['SalePrice'] = final_predictions\n# submission.head()\nprint(submission.head())","01937d09":"# export to a .csv file as Kaggle expects.\n# pass index=False because Pandas otherwise would create a new index for us.\nsubmission.to_csv('submission2.csv', index=False)","da9cb9bd":"# IMPORTS","c66b7bb2":"# Outliers \/ Anomalies","f50ffa81":"# Model 2: Random Forest","af2218e9":"# Model 1: Multiple Linear Regression","47e264b9":"# Feature Engineering","428e7da0":"# NUMERIC DATA","d46e2d49":"# INSPECTION","ae825ba4":"# Model 1.1: Multiple Linear Regression, restricted features","068df46a":"# CATEGORICAL DATA"}}