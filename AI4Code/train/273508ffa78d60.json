{"cell_type":{"f9232785":"code","07238218":"code","d70a8c7e":"code","d612e1f2":"code","3e9ff6b9":"code","ce488d3d":"code","51e1821b":"code","77ed7fe0":"code","53b45c66":"code","31c04612":"code","f54c4c13":"code","26b2cd52":"code","f9ec65e3":"code","874b7172":"code","15d72247":"code","219a8337":"code","4bcbf4a6":"code","8b975181":"code","6bd622a1":"code","6e0af44d":"code","0426edd0":"code","11d36b4b":"code","75e83c7c":"code","6dfec81c":"code","0f587b31":"code","eb80024d":"code","6b65bbe5":"code","882842a0":"code","4b4cbc3b":"code","0144e56f":"code","06156a77":"code","e951f4a5":"code","1ffa2b52":"code","78a1234c":"code","8c118b15":"code","1d38590d":"code","be41b120":"code","ee53cb12":"code","c968bd2d":"code","338ec925":"code","51ca565d":"code","8626c068":"code","0ae7b4bd":"code","60354f28":"code","1a72b859":"code","cb0e9115":"code","9bf612c5":"markdown","27cc6aad":"markdown","c68e594c":"markdown","b3a8ad36":"markdown","a3839bc7":"markdown","0b1a84df":"markdown","0ad0b95b":"markdown","0571191d":"markdown","ffbb1635":"markdown","d80564ac":"markdown","34673ef1":"markdown","f13f3f72":"markdown","1a5638fe":"markdown","3949eb86":"markdown","4feac797":"markdown","57a4f69b":"markdown","fd0e5cc6":"markdown","685de235":"markdown","b0705e72":"markdown","1aa7e1d2":"markdown","fb658305":"markdown","d845613b":"markdown","a55c31ac":"markdown","726536b9":"markdown","4b8a99ea":"markdown","0b477695":"markdown","c6929733":"markdown","c8437e23":"markdown","3c2a7364":"markdown","d035e8ff":"markdown","53d3d2b7":"markdown","8a27be7a":"markdown","16613b5f":"markdown","0705553e":"markdown","ddba9ae6":"markdown","29ad8bea":"markdown","f7364874":"markdown","d900ce6e":"markdown","f8417768":"markdown","d9a16d11":"markdown","5458bb52":"markdown","e216416c":"markdown","598b8282":"markdown","525d4986":"markdown"},"source":{"f9232785":"# python utilities\nimport random\nimport os\n\n# general data science\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ---------- scikit-learn ------------\n# preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# models\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator # for custom estimators\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# model evaluation\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.metrics import fbeta_score, make_scorer\n# -------------------------------------\n\n# imbalanced-learn\nfrom imblearn.pipeline import Pipeline # if using imblearn's sampling we must use this over sklearn's Pipeline \nfrom imblearn.over_sampling import SMOTE, RandomOverSampler # oversampling\nfrom imblearn.under_sampling import EditedNearestNeighbours, TomekLinks, NearMiss, RandomUnderSampler # undersampling\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything()","07238218":"# load the data\ndata = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.shape","d70a8c7e":"data.head()","d612e1f2":"data.info()","3e9ff6b9":"X = data.drop(\"Churn\", axis=1)\ny = data[\"Churn\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)","ce488d3d":"df_train = pd.concat([X_train, y_train], axis=1)","51e1821b":"df_train.shape","77ed7fe0":"df_train.head()","53b45c66":"df_train.isna().mean()","31c04612":"df_train[\"TotalCharges\"][df_train[\"customerID\"] == \"2775-SEFEE\"].values[0]","f54c4c13":"df_train.apply(lambda x: x==' ', axis=1).mean()","26b2cd52":"df_train = df_train[df_train[\"TotalCharges\"] != ' ']\ndf_train[\"TotalCharges\"] = df_train[\"TotalCharges\"].astype('float64')","f9ec65e3":"df_train[[\"tenure\", \"TotalCharges\", \"MonthlyCharges\"]].describe()","874b7172":"df_train.nunique().sort_values(ascending=False)","15d72247":"df_train = df_train.drop(\"customerID\", axis=1)","219a8337":"low_unq_feats = df_train.columns[df_train.nunique()<10]\nfor feat in low_unq_feats:\n    print(feat, df_train[feat].unique())","4bcbf4a6":"df_train[\"SeniorCitizen\"] = df_train[\"SeniorCitizen\"].replace({0:\"No\",1:\"Yes\"})","8b975181":"df_train.dtypes","6bd622a1":"sns.countplot(\"Churn\", data=df_train)","6e0af44d":"(df_train[\"Churn\"]==\"No\").sum(), (df_train[\"Churn\"]==\"Yes\").sum()","0426edd0":"cat_feats = df_train.columns[df_train.dtypes == 'object'][:-1] # :-1 to remove churn\ncat_feats","11d36b4b":"fig, ax = plt.subplots(4,4,figsize=(14,14))\nax = ax.flatten()\nfor i,feat in enumerate(cat_feats):\n    plt.sca(ax[i])\n    df_unq = df_train[feat].value_counts().sort_values(ascending=False)\n\n    sns.barplot(df_unq.index, df_unq.values, order=df_unq.index)\n    \n    plt.xlabel(str(feat), color='red', fontsize=14)\n    plt.xticks(rotation=45, ha='right')\n    \nplt.tight_layout(h_pad=2)\nplt.show()","75e83c7c":"df_train[\"Churn\"] = df_train[\"Churn\"].replace({\"No\":0, \"Yes\":1})","6dfec81c":"fig, ax = plt.subplots(4,4,figsize=(14,14))\nax = ax.flatten()\nfor i,feat in enumerate(cat_feats):\n    plt.sca(ax[i])\n    sns.barplot(x=feat, y=\"Churn\", data=df_train)\n                \n    plt.xlabel(str(feat), color='red', fontsize=14)\n    plt.xticks(rotation=45, ha='right')\n    \nplt.tight_layout(h_pad=2)\nplt.show()","0f587b31":"import plotly.graph_objects as go\n\nfor feat in cat_feats:\n    df_train[feat] = df_train[feat].replace({\"No\":0,\"Yes\":1,\"No internet service\":0,\"No phone service\":0})\ndf_train[\"gender\"] = df_train[\"gender\"].replace({\"Female\":0,\"Male\":1})\ndf_train[\"InternetService\"] = df_train[\"InternetService\"].replace({\"DSL\":1,\"Fiber optic\":1})\ndf_train[\"Contract\"] = df_train[\"Contract\"].replace({\"One year\":1, \"Two year\":1, \"Month-to-month\":0})\ndf_train[\"PaymentMethod\"] = df_train[\"PaymentMethod\"].replace({\"Mailed check\":0, \"Bank transfer (automatic)\":1, \n                                                               \"Electronic check\":1, \"Credit card (automatic)\":1})\n\n\n# based on the plot at https:\/\/plotly.com\/python\/radar-chart\/\nfig = go.Figure()\n\nfig.add_trace(go.Scatterpolar(\n      r=df_train.loc[df_train[\"Churn\"]==0,cat_feats].mean().tolist(),\n      theta=cat_feats.tolist(),\n      fill='toself',\n      name='No churn'\n))\n\nfig.add_trace(go.Scatterpolar(\n      r=df_train.loc[df_train[\"Churn\"]==1,cat_feats].mean().tolist(),\n      theta=cat_feats.tolist(),\n      fill='toself',\n      name='Churn'\n))\n\nfig.update_layout(\n  polar=dict(\n    radialaxis=dict(\n      visible=True,\n      range=[0, 1]\n    )),\n  showlegend=False\n)\n\nfig.show()","eb80024d":"num_feats = [\"tenure\", \"TotalCharges\", \"MonthlyCharges\"]\ndf_train[num_feats].describe()","6b65bbe5":"fig, ax = plt.subplots(1,3,figsize=(14,6))\nfor i,feat in enumerate(num_feats):\n    plt.sca(ax[i])\n    sns.boxplot(x=\"Churn\", y=feat, data=df_train, ax=ax[i])\n    plt.ylabel(\"\")\n    plt.title(feat, color=\"red\", fontsize=14)\n    \nplt.tight_layout()\nplt.show()","882842a0":"df_train[num_feats].corr()","4b4cbc3b":"def tidy_up(df):\n    df = df[df[\"TotalCharges\"] != ' ']\n    df[\"TotalCharges\"] = df[\"TotalCharges\"].astype('float64')\n    \n    df[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].replace({0:\"No\",1:\"Yes\"})\n    \n    df[\"Churn\"] = df[\"Churn\"].replace({\"No\":0, \"Yes\":1})\n    \n    df.drop(\"customerID\", axis=1, inplace=True)\n    \n    X = df.drop(\"Churn\", axis=1)\n    y = df[\"Churn\"]\n    \n    return X,y\n\nX_train, y_train = tidy_up(pd.concat([X_train, y_train], axis=1))\nX_test, y_test = tidy_up(pd.concat([X_test, y_test], axis=1))","0144e56f":"# define categorical features & initialize encoder\ncat_feats = X_train.columns[X_train.dtypes == 'object']\nonehot_encoder = OneHotEncoder() ","06156a77":"# define numeric features & initialize scaler\nnum_feats = [\"tenure\", \"TotalCharges\", \"MonthlyCharges\"]\nscaler = StandardScaler()","e951f4a5":"# fit CategoricalNB to categorical features\nord_encoder = OrdinalEncoder()\nX_train_c = X_train[cat_feats]\nX_train_c = ord_encoder.fit_transform(X_train_c)\nnb_c = CategoricalNB()\nnb_c.fit(X_train_c, y_train)\n\n# fit GaussianNB to numeric features\nX_train_n = X_train[num_feats]\nnb_n = GaussianNB()\nnb_n.fit(X_train_n, y_train)\n\n# get predicted class probabilities, P(Y=1|X),from each model. Then stack predictions and train another GaussianNB. \ntrain_preds_c = nb_c.predict_proba(X_train_c)[:,1]\ntrain_preds_n = nb_n.predict_proba(X_train_n)[:,1]\ntrain_preds_cn = np.vstack((train_preds_c, train_preds_n)).T\nnb_cn = GaussianNB()\nnb_cn.fit(train_preds_cn, y_train)\n\n# test set predictions\nX_test_c = X_test[cat_feats]\nX_test_c = ord_encoder.transform(X_test_c)\ntest_preds_c = nb_c.predict(X_test_c)\nX_test_n = X_test[num_feats]\ntest_preds_n = nb_n.predict(X_test_n)\ntest_preds_cn = np.vstack((test_preds_c, test_preds_n)).T\ntest_preds = nb_cn.predict(test_preds_cn)\n\n# evaluate model\nprint(\"Train accuracy...\")\nprint(classification_report(y_train, nb_cn.predict(train_preds_cn)))\nprint(\"Test accuracy...\")\nprint(classification_report(y_test, test_preds))","1ffa2b52":"ct = ColumnTransformer([('cat_feats', onehot_encoder, cat_feats),\n                        ('num_feats', scaler, num_feats)])\n\nmodel = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n\n# no undersampling\/oversampling\npipe = Pipeline([(\"preprocessing\", ct),\n                 (\"logreg\", model)])\n\nkf = StratifiedKFold(n_splits=5)\n\ngrid = GridSearchCV(pipe, param_grid={'logreg__C': [0.01, 0.1, 1, 10]}, cv=kf, scoring='f1')\ngrid.fit(X_train, y_train)","78a1234c":"grid.best_estimator_[\"logreg\"]","8c118b15":"predict = grid.best_estimator_.predict(X_test)\nprint(classification_report(y_test, predict))","1d38590d":"def brief_classification_report(y_test, predict):\n    rep = np.array(precision_recall_fscore_support(y_test, predict))\n#     print(\"       precision       recall            f1\")\n#     print(\"0\\t\", \"\\t\\t\".join([\"%0.02f\" % x for x in rep[:-1,0]]) )\n    print(\"1\\t\", \"\\t\\t\".join([\"%0.02f\" % x for x in rep[:-1,1]]))\n\nsm_names = [\"Edited NN\", \"Tomek Links\", \"Random Undersampling\", \"Near-Miss\", \"SMOTE\", \"Random Oversampling\"] \nsms = [EditedNearestNeighbours(), TomekLinks(), RandomUnderSampler(), NearMiss(), SMOTE(), RandomOverSampler()]\n\nct = ColumnTransformer([('cat_feats', onehot_encoder, cat_feats),\n                        ('num_feats', scaler, num_feats)])\n\nmodel = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n\nprint(\"       precision       recall            f1\")\nfor sm_name, sm in zip(sm_names, sms):\n    \n    pipe = Pipeline([(\"preprocessing\", ct),\n                     (\"sampling\", sm),\n                    (\"logreg\", model)])\n\n    kf = StratifiedKFold(n_splits=5)\n\n    grid = GridSearchCV(pipe, param_grid={'logreg__C': [0.01, 0.1, 1, 10]}, cv=kf, scoring='f1')\n    grid.fit(X_train, y_train)\n    \n    predict = grid.best_estimator_.predict(X_test)\n    print(sm_name)\n    brief_classification_report(y_test, predict)","be41b120":"# This is a custom estimator. Code courtesty of:  https:\/\/stackoverflow.com\/a\/53926097\/7638741 .\nclass ClfSwitcher(BaseEstimator):\n    def __init__(\n        self, \n        estimator = SVC(),\n    ):\n        \"\"\"\n        A Custom BaseEstimator that can switch between classifiers.\n        :param estimator: sklearn object - The classifier\n        \"\"\" \n\n        self.estimator = estimator\n\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","ee53cb12":"ct = ColumnTransformer([('cat_feats', onehot_encoder, cat_feats),\n                    ('num_feats', scaler, num_feats)])\n\npipe = Pipeline([(\"preprocessing\", ct),\n                 (\"sampling\", RandomOverSampler()),\n                 (\"clf\", ClfSwitcher())])\n\nparameters = [\n    {\n        'clf__estimator': [SVC()],\n        'clf__estimator__C': [0.1, 1, 10, 20],\n        'clf__estimator__kernel': ['rbf', 'poly']\n    },\n    {\n        'clf__estimator': [KNeighborsClassifier()],\n        'clf__estimator__n_neighbors':[3,5,10,20]\n    },\n    {\n        'clf__estimator': [RandomForestClassifier()],\n        'clf__estimator__n_estimators': [100,200], \n        'clf__estimator__max_depth': [15,30], \n        'clf__estimator__max_features': [5,10],\n        'clf__estimator__min_samples_leaf': [4,8]\n    },\n]\n\nkf = StratifiedKFold(n_splits=5)\n\ngrid = GridSearchCV(estimator = pipe, param_grid=parameters, cv=kf, scoring='f1')\n\ngrid.fit(X_train, y_train)","c968bd2d":"def format_cv_results(search):\n    df = pd.concat([pd.DataFrame(grid.cv_results_[\"params\"]),pd.DataFrame(grid.cv_results_[\"mean_test_score\"], columns=[\"Score\"])],axis=1)\n    df = df.sort_values(\"Score\", ascending=False)\n    return df.fillna(value=\"\")\ndf_res = format_cv_results(grid)\ndf_res","338ec925":"predict = grid.best_estimator_.predict(X_test)\nprint(classification_report(y_test, predict))","51ca565d":"fpr, tpr, thresholds = roc_curve(y_test, grid.predict_proba(X_test)[:,1])\nroc_auc = roc_auc_score(y_test, predict)\n\nplt.plot(fpr, tpr, lw=1, label='AUC = %0.2f'%(roc_auc))\nplt.plot([0, 1], [0, 1], '--k', lw=1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve')\nplt.legend(loc=\"lower right\", frameon = True).get_frame().set_edgecolor('black')","8626c068":"ct = ColumnTransformer([('cat_feats', onehot_encoder, cat_feats),\n                        ('num_feats', scaler, num_feats)])\n\nmodel = XGBClassifier(learning_rate=0.02, \n                    n_estimators=200,\n                    booster = 'gbtree',\n                    objective='binary:logistic')\n\npipe = Pipeline([(\"preprocessing\", ct),\n                (\"sampling\", RandomOverSampler()),\n                (\"xgb\", model)])\n\ntuned_parameters = {\n        'xgb__min_child_weight': [1, 5, 10],\n        'xgb__gamma': [0.5, 1, 1.5, 2, 5, 10],\n        'xgb__subsample': [0.6, 0.8, 1.0],\n        'xgb__colsample_bytree': [0.6, 0.8, 1.0],\n        'xgb__max_depth': [3, 5, 8]\n        }\n\nkf = StratifiedKFold(n_splits=5)\n\ngrid = RandomizedSearchCV(estimator = pipe, \n                                   param_distributions=tuned_parameters, \n                                   cv=kf,\n                                   n_iter=20, \n                                   scoring='f1', \n                                   n_jobs=-1, \n                                   verbose=3)\n\ngrid.fit(X_train, y_train)","0ae7b4bd":"preds = grid.best_estimator_.predict(X_test)\nprint(classification_report(y_test, preds))","60354f28":"fpr, tpr, thresholds = roc_curve(y_test, grid.best_estimator_.predict_proba(X_test)[:,1])\nroc_auc = roc_auc_score(y_test, preds)\n\nplt.plot(fpr, tpr, lw=1, label='AUC = %0.2f'%(roc_auc))\nplt.plot([0, 1], [0, 1], '--k', lw=1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve')\nplt.legend(loc=\"lower right\", frameon = True).get_frame().set_edgecolor('black')","1a72b859":"feat_names = []\nfor feat in cat_feats:\n    for level in X_train[feat].unique():\n        feat_names.append(\"%s_%s\" % (feat,level))\nfeat_names.extend(num_feats)\n\nimportances = grid.best_estimator_[\"xgb\"].feature_importances_\nimportances_dict = {f:i for f,i in zip(feat_names, importances)}\n\nn = 20 # only plot a few \nimportances = pd.DataFrame.from_dict(importances_dict, orient='index').rename(columns={0: 'Gini-importance'}).head(n)\n\nimportances.sort_values(by='Gini-importance', ascending=False).plot(kind='bar', rot=45, figsize=(14,6), fontsize=14)\nplt.xticks(ha='right')\nplt.show()","cb0e9115":"import pickle\nwith open('customer-churn_XGBoost','wb') as f:\n    pickle.dump(grid.best_estimator_, f)","9bf612c5":"There don't appear to be other \"NA\" strings. We see that the categorical variables are generally either \"Yes\", \"No\", or \"No phone\/internet service.\" Let's recode SeniorCitizen as a \"Yes\" or \"No\" also.","27cc6aad":"# Import libraries","c68e594c":"OnlineSecurity and InternetService appear at the top of the feature importances. Perhaps more resources should be invested into improving the company's internet and online services.","b3a8ad36":"## Naive Bayes\nAs a baseline model we will use Naive Bayes. Unfortunately, Naive Bayes is not straightforward to implement when there are a combination of categorical and numeric features. We solve this by fitting a categorical Naive Bayes model to the categorical features, a Gaussian Naive Bayes model to the numeric features, and aggregating them with -- you guessed it -- a third, Gaussian Naive Bayes model.\n\nTwo notes:\n- We have not used any cross-validation yet.\n- Naive Bayes does not suffer as much from imbalanced data, so we did not employ undersampling\/oversampling. Naive Bayes does suffer from small datasets, though. This is because it's more likely that an arbitrary train\/test split will have differently distributed features, and thus have incorrectly specified priors. To compound that, the impact of these incorrect priors is larger for small datasets.","a3839bc7":"### Relation to target variable\nFor plotting purposes, it will help to convert customer churn to a numeric 0\/1 variable.","0b1a84df":"The ranges look reasonable. Lets check to make sure there aren't any different \"NA\" strings in the categorical variables.","0ad0b95b":"Let's do a couple sanity checks. First, let's make sure none of the numeric features are negative.","0571191d":"# Modeling","ffbb1635":"## Rescaling numerical features\nHere we rescale numeric features. This helps several classifiers -- in particular, SVM and KNN -- perform better.","d80564ac":"There are a number of insights to take away here. Churn is relatively independent of gender and PhoneService. InternetService, PaperlessBilling, and SeniorCitizen are all associated with churn. OnlineSecurity, OnlineBackup, DeviceProtection, and TechSupport tend to retain customers. Although correlated, these factors may not be causative -- longer kept customers may simply be more likely to use these services. Finally, customers with dependents or partners are more likely to stay, possibly indicating the company's family plans are well-received.","34673ef1":"The XGBoost model performs about the same as the random forest model (for the hyperparameters we searched). The recall of the XGBoost model is slightly higher but the f1 score is slightly lower.","f13f3f72":"We see customer churn tends to be associated with lower tenure and TotalCharges. This makes sense, the longer a customer stays and the more they pay, the more likely they will stay. Interestingly these two distributions are especially right skewed for the churn class -- and with a median tenure around only 10 months. This indicates most people who leave have low tenure and TotalCharges, but there are some customers with very high tenure and TotalCharges that end up leaving.\n\nWe see that monthly charges are higher for the churn class. This may be due to deals that these customers are not getting because they don't stay long enough. It could also be because they didn't initially research different plans, and so they wound up paying higher and then ultimately found a better deal elsewhere.","1a5638fe":"We see with an appropriate classification threshold we can achieve around a 85% recall with only a 20% false positive rate.","3949eb86":"# Exploratory data analysis\nHere we take a look at the training data. We will examine the distributions of our predictor variables and target variable. We will look to see how each predictor variable relates with other predictors and with the target variable.","4feac797":"We look at three other algorithms: SVM, KNN, and random forests. After searching a few parameters, we will assess which model seems to be performing the best.","57a4f69b":"## XGBoost\nWe now try to improve on the previous random forest model by applying XGBoost, which is generally considered a more powerful version of random forest. We use a larger hyper-parameter space, but only perform a randomized search.","fd0e5cc6":"# Telco customer churn prediction\nThe objective of this project is to predict whether a customer will leave a fictional telecommunications company. We perform an exploratory data analysis (EDA), preprocess the data, compare several models and sampling strategies for imbalanced data.\n\nThe data set description on Kaggle is given below:\n> **Context**\n>\n> \"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]\n>\n> **Content**\n>\n> Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n> \n> The data set includes information about:\n> \n> - Customers who left within the last month \u2013 the column is called Churn\n> - Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device > > protection, tech support, and streaming TV and movies\n> - Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n> - Demographic info about customers \u2013 gender, age range, and if they have partners and dependents\n>","685de235":"The plot below provides a more succint view of the factors contributing to churn\/no churn. To create it, we recoded \"no\" as 0 and \"yes\" as 1. For the few categories that have other levels, we recoded them (somewhat arbitraily) according to whether we thought they would lead to customer churn.","b0705e72":"We see there are a mix of datatypes, but most of them are categorical variables (in particular, binary yes\/no). According to above, there are three numeric variables: SeniorCitizen, tenure, and MonthlyCharges; however, it looks like SeniorCitizen is a binary 0,1 variable. Also, looking at the dataframe, TotalCharges looks numeric, but is listed as \"object\" above. We suspect these are \"NA\" strings.\n\nWe will deal with these later.","1aa7e1d2":"In this section we will:\n- Try several methods for correcting imbalanced data (when applicable) such as: Synthetic Minority Oversampling Technique (SMOTE), Near-Miss undersampling, Tomek Links undersampling.\n- Try several models such as: Naive Bayes, Logistic Regression, SVM, KNN, Random Forest. If time permits we will look at more advanced models like Gradient Boosted Trees (XGBoost) and Neural Networks.\n- Perform cross-validation to optimize hyperparameters and prevent overfitting.\n- Our model evaluation on the test set will be guided by *recall* and the *f1 score*. Recall is an appropriate metric for customer churn prediction. We want to be able to identify customers that are going to leave so that we can talk to them (i.e. send them email offers). If we accidently predict a user will leave when they don't plan on it -- well -- talking to them won't incur a great cost.","fb658305":"Let's drop customerID since it should not contain any relevant information.","d845613b":"## Logistic Regression\nBelow we train a logistic regression model and optimize the regularization parameter using cross-validation. We chose to use $\\ell_1$ (LASSO) regularization over $\\ell_2$ because the $\\ell_2$ solution was not converging. \n\nWe illustrate the issue with imbalanced data.","a55c31ac":"There are only a few missing values here. Lets remove these while we're at it.","726536b9":"## Missing values","4b8a99ea":"As we would expect, total charges are highly correlated with tenure and monthly charges. It may be wise to remove one of these features from our model eventually. Monthly charges are not very correlated with tenure.","0b477695":"During our EDA we performed a couple transformations. We'll want to apply these (and any subsequent transformations) to X_train, y_train, X_test, y_test.","c6929733":"# A quick look at the data","c8437e23":"## Churn frequency","3c2a7364":"## Conclusion & future work\nWe were able to achieve above 80% recall, 50% precision, and 0.65 f1 by XGBoost. This equates to correctly identifying 80% of customer churn cases, while unnecessarily targeting loyal customers 50% of the time. Assuming reasonable actions are taken, i.e. emailing the customer an offer, this model could be leveraged to improve customer satisfcation. \n\nFor other models, logistic regression and random forest also performed well. Naive Bayes, SVM, and KNN did not perform well, possibly in part due to the large number of categorical variables. Our EDA indicated that factors like gender and phone service do not impact customer churn.  Internet service, paperless billing, and whether the customer was a senior citizen or not all were related to churn. The feature importances from our model further indicate that internet service (or not) is a dominating factor in why customers stay or leave.\n\nFuture work should explore multivariate relationships relevant to churn (are senior citizens and no-internet separate causes of churn or do senior citizens not use internet as often?). More advanced models could be built, such as neural network models. Furthermore, advanced hyperparameter optimization methods (see hypopt package) should be preferred over random\/grid-search. \n\nPlease let me know if you have any suggestions! \ud83d\ude0a","d035e8ff":"### Barplots for distributions","53d3d2b7":"# Preprocessing","8a27be7a":"The random forest model slightly outperforms the previous logistic regression model in f1 score. Another model diagnostic we can use is the ROC curve and the AUC score, which we plot below.","16613b5f":"## Encoding categorical features\nA common way to encode categorical features is one-hot encoding. One-hot encoding is good because it preserves all information about the categories, the downside is it increases the dimension and sparsity of the data, especially for high cardinality features. Since our feature space is relatively small (16 categorical features with only 2-4 unique values each) we will just try one-hot-encoding our categorical features. ","0705553e":"In our training data, there are 1496 cases of customer churn. The other 4128 customers were retained. There is an **imbalanced data** issue here. To better predict customers that will leave, we will need to use models that allow for class weights, or we will need to use undersampling and\/or oversampling.","ddba9ae6":"## SVM, KNN, Random Forests","29ad8bea":"The sampling methods worked -- we've sacrificed precision for recall!\n\nThe f1 scores of the three methods are relatively close except Near-miss which has a good recall score but very poor precision, ultimately lowering the f1 score. Tomek links provides a good balance between precision and recall. Edited NN, random undersampling, SMOTE, and random oversampling all perform approximately the same (precision:0.51-0.53, recall:0.82-0.84).\n\nFrom what I've read, there is no principled way to choose a sampling method, and the \"best\" sampling method may itself be algorithm dependent. We think the performance of SMOTE and other complex sampling algorithms may be limited due to the fact most of our data is categorical. **Based on our observations we will use random oversampling.** ","f7364874":"## Feature Importances","d900ce6e":"## Save the best model (optional)","f8417768":"## Numeric features","d9a16d11":"## Categorical features","5458bb52":"# Creating a train-test split","e216416c":"With no sampling, the recall score is only 0.58. This means we are only correctly identifying 58% of all customer churn. The accuracy score of 0.81 is misleading because there are many more \"no churn\" examples that \"churn\" examples. \n\nBelow we employ several different sampling methods and compare their performance.","598b8282":"We can see that random forest scored the best, SVM scored second best, and KNN scored last. Essentially all random forest models scored above SVM and KNN. Note that we could just not be exploring the parameter space for each model enough. In particularly, KNN's performance appears to continually increase with the `n_neighbors` parameter. The best model selected during validation was a random forest model with `max_depth=15, max_features=5, min_samples_leaf=8, n_estimators=100`. We could likely improve this model further by exploring more parameters, but we will stop here.\n\nBelow is the testing report for the best model (out of RF, SVM, KNN)","525d4986":"Based on the above, it looks like there are no missing values. **Hold up** -- the missing values are not encoded as typical NA values. We see below that TotalCharges uses a \"space\" string for NA values."}}