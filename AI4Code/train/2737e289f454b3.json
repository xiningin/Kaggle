{"cell_type":{"23e49356":"code","50a23cf9":"code","3192d13c":"code","d6e803b9":"code","cf34835b":"code","1af1a971":"code","6de523d0":"code","283c2730":"code","15fa2463":"code","35819305":"code","b7c0aa27":"code","e3ccb44c":"code","8f3f656a":"code","1187489d":"code","67ccea1e":"code","e8b5ed23":"code","c1fe0686":"code","67cb26d9":"code","aa826e5a":"code","c7e87d48":"code","6696af48":"code","d4affe2d":"code","a4f27749":"code","f362732e":"code","4c251165":"code","2c48259c":"code","9d222373":"code","2bf0899b":"code","9bfb85df":"code","4b6388b0":"code","b91dcb6f":"code","bbd71f2a":"code","12c0d87d":"code","00950ad4":"markdown","5ba35258":"markdown","078f9b28":"markdown","7db11b01":"markdown","87c75314":"markdown","fccaca22":"markdown","e557414f":"markdown","076e019e":"markdown","3c4a8e6b":"markdown","75a43a75":"markdown","68e14ac0":"markdown"},"source":{"23e49356":"!pip install torchsummary","50a23cf9":"import os\nimport random\nfrom collections import defaultdict\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nfrom fastprogress import master_bar, progress_bar\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.utils import make_grid\nfrom torchsummary import summary\n\n\n%matplotlib inline","3192d13c":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n    \ndef get_logger(\n    filename='log',\n    disable_stream_handler=False,\n    disable_file_handler=False\n):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    \n    if not disable_stream_handler:\n        handler1 = StreamHandler()\n        handler1.setFormatter(Formatter(\"%(message)s\"))\n        logger.addHandler(handler1)\n    \n    if not disable_file_handler:\n        handler2 = FileHandler(filename=f\"{filename}.log\")\n        handler2.setFormatter(Formatter(\"%(message)s\"))\n        logger.addHandler(handler2)\n    \n    return logger","d6e803b9":"def get_image_status(train_path, test_path, use_tqdm=False):\n    if use_tqdm:\n        train_path = tqdm(train_path)\n        test_path = tqdm(test_path)\n\n    rgb_status = {'R': defaultdict(list), 'G': defaultdict(list), 'B': defaultdict(list)}\n    hsv_status = {'H': defaultdict(list), 'S': defaultdict(list), 'V': defaultdict(list)}\n    size_status = {'height': defaultdict(list), 'width': defaultdict(list), 'aspect': defaultdict(list)}\n    \n    def get_stats_dict(path):\n        rgb_status = {'R': defaultdict(list), 'G': defaultdict(list), 'B': defaultdict(list)}\n        hsv_status = {'H': defaultdict(list), 'S': defaultdict(list), 'V': defaultdict(list)}\n        size_status = defaultdict(list)\n        \n        for p in path:\n            img = cv2.imread(p)\n            for i, c in enumerate(['B', 'G', 'R']):\n                rgb_status[c]['mean'].append(img[:,:,i].mean())\n                rgb_status[c]['std'].append(img[:,:,i].std())\n                rgb_status[c]['max'].append(img[:,:,i].max())\n                rgb_status[c]['min'].append(img[:,:,i].min())\n\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n            for i, c in enumerate(['H', 'S', 'V']):\n                hsv_status[c]['mean'].append(img[:,:,i].mean())\n                hsv_status[c]['std'].append(img[:,:,i].std())\n                hsv_status[c]['max'].append(img[:,:,i].max())\n                hsv_status[c]['min'].append(img[:,:,i].min())\n        \n            size_status['height'].append(img.shape[0])\n            size_status['width'].append(img.shape[1])\n            size_status['aspect'].append(img.shape[0] \/ img.shape[1])\n        \n        return rgb_status, hsv_status, size_status\n    \n    print('loading train images...')\n    tr_rgb_status, tr_hsv_status, tr_size_status = get_stats_dict(train_path)\n    \n    print('loading test images...')\n    te_rgb_status, te_hsv_status, te_size_status = get_stats_dict(test_path)\n    \n    return [tr_rgb_status, tr_hsv_status, tr_size_status], [te_rgb_status, te_hsv_status, te_size_status]\n\n\ndef show_image_status(tr_rgb_status, tr_hsv_status, tr_size_status, \n                      te_rgb_status, te_hsv_status, te_size_status,\n                      fig_height=3, fig_width=5):\n\n    print('** RGB status histogram (R\/G\/B)-(mean\/std\/max\/min) **')\n    fig, axes = plt.subplots(3, 4)\n    fig.set_size_inches(4 * fig_width, 3 * fig_height)\n\n    sns.distplot(tr_rgb_status['R']['mean'], ax=axes[0, 0], kde=False)\n    sns.distplot(te_rgb_status['R']['mean'], ax=axes[0, 0], kde=False)\n    sns.distplot(tr_rgb_status['R']['std'], ax=axes[0, 1], kde=False)\n    sns.distplot(te_rgb_status['R']['std'], ax=axes[0, 1], kde=False)\n    sns.distplot(tr_rgb_status['R']['max'], ax=axes[0, 2], kde=False)\n    sns.distplot(te_rgb_status['R']['max'], ax=axes[0, 2], kde=False)\n    sns.distplot(tr_rgb_status['R']['min'], ax=axes[0, 3], kde=False)\n    sns.distplot(te_rgb_status['R']['min'], ax=axes[0, 3], kde=False)\n\n    sns.distplot(tr_rgb_status['G']['mean'], ax=axes[1, 0], kde=False)\n    sns.distplot(te_rgb_status['G']['mean'], ax=axes[1, 0], kde=False)\n    sns.distplot(tr_rgb_status['G']['std'], ax=axes[1, 1], kde=False)\n    sns.distplot(te_rgb_status['G']['std'], ax=axes[1, 1], kde=False)\n    sns.distplot(tr_rgb_status['G']['max'], ax=axes[1, 2], kde=False)\n    sns.distplot(te_rgb_status['G']['max'], ax=axes[1, 2], kde=False)\n    sns.distplot(tr_rgb_status['G']['min'], ax=axes[1, 3], kde=False)\n    sns.distplot(te_rgb_status['G']['min'], ax=axes[1, 3], kde=False)\n\n    sns.distplot(tr_rgb_status['B']['mean'], ax=axes[2, 0], kde=False)\n    sns.distplot(te_rgb_status['B']['mean'], ax=axes[2, 0], kde=False)\n    sns.distplot(tr_rgb_status['B']['std'], ax=axes[2, 1], kde=False)\n    sns.distplot(te_rgb_status['B']['std'], ax=axes[2, 1], kde=False)\n    sns.distplot(tr_rgb_status['B']['max'], ax=axes[2, 2], kde=False)\n    sns.distplot(te_rgb_status['B']['max'], ax=axes[2, 2], kde=False)\n    sns.distplot(tr_rgb_status['B']['min'], ax=axes[2, 3], kde=False)\n    sns.distplot(te_rgb_status['B']['min'], ax=axes[2, 3], kde=False)\n    \n    plt.show()\n    \n    print('** HSV status histogram (H\/S\/V)-(mean\/std\/max\/min) **')\n    fig, axes = plt.subplots(3, 4)\n    fig.set_size_inches(4 * fig_width, 3 * fig_height)\n\n    sns.distplot(tr_hsv_status['H']['mean'], ax=axes[0, 0], kde=False)\n    sns.distplot(te_hsv_status['H']['mean'], ax=axes[0, 0], kde=False)\n    sns.distplot(tr_hsv_status['H']['std'], ax=axes[0, 1], kde=False)\n    sns.distplot(te_hsv_status['H']['std'], ax=axes[0, 1], kde=False)\n    sns.distplot(tr_hsv_status['H']['max'], ax=axes[0, 2], kde=False)\n    sns.distplot(te_hsv_status['H']['max'], ax=axes[0, 2], kde=False)\n    sns.distplot(tr_hsv_status['H']['min'], ax=axes[0, 3], kde=False)\n    sns.distplot(te_hsv_status['H']['min'], ax=axes[0, 3], kde=False)\n\n    sns.distplot(tr_hsv_status['S']['mean'], ax=axes[1, 0], kde=False)\n    sns.distplot(te_hsv_status['S']['mean'], ax=axes[1, 0], kde=False)\n    sns.distplot(tr_hsv_status['S']['std'], ax=axes[1, 1], kde=False)\n    sns.distplot(te_hsv_status['S']['std'], ax=axes[1, 1], kde=False)\n    sns.distplot(tr_hsv_status['S']['max'], ax=axes[1, 2], kde=False)\n    sns.distplot(te_hsv_status['S']['max'], ax=axes[1, 2], kde=False)\n    sns.distplot(tr_hsv_status['S']['min'], ax=axes[1, 3], kde=False)\n    sns.distplot(te_hsv_status['S']['min'], ax=axes[1, 3], kde=False)\n\n    sns.distplot(tr_hsv_status['V']['mean'], ax=axes[2, 0], kde=False)\n    sns.distplot(te_hsv_status['V']['mean'], ax=axes[2, 0], kde=False)\n    sns.distplot(tr_hsv_status['V']['std'], ax=axes[2, 1], kde=False)\n    sns.distplot(te_hsv_status['V']['std'], ax=axes[2, 1], kde=False)\n    sns.distplot(tr_hsv_status['V']['max'], ax=axes[2, 2], kde=False)\n    sns.distplot(te_hsv_status['V']['max'], ax=axes[2, 2], kde=False)\n    sns.distplot(tr_hsv_status['V']['min'], ax=axes[2, 3], kde=False)\n    sns.distplot(te_hsv_status['V']['min'], ax=axes[2, 3], kde=False)\n    \n    plt.show()\n    \n    print('** Height\/Width\/Aspect status histogram **')\n    fig, axes = plt.subplots(1, 3)\n    fig.set_size_inches(3 * fig_width, fig_height)\n\n    sns.distplot(tr_size_status['height'], ax=axes[0], kde=False)\n    sns.distplot(te_size_status['height'], ax=axes[0], kde=False)\n    \n    sns.distplot(tr_size_status['width'], ax=axes[1], kde=False)\n    sns.distplot(te_size_status['width'], ax=axes[1], kde=False)\n    \n    sns.distplot(tr_size_status['aspect'], ax=axes[2], kde=False)\n    sns.distplot(te_size_status['aspect'], ax=axes[2], kde=False)\n    \n    plt.show()","cf34835b":"logger = get_logger(\n    filename='running',\n    disable_stream_handler=False,\n    disable_file_handler=False,\n)","1af1a971":"INPUT_DIR = '..\/input\/ailab-ml-training-1\/'\nARTIFACT_DIR = '..\/input\/v1-ailab1-cv\/'\n\nPATH = {\n    'train': os.path.join(INPUT_DIR, 'train.csv'),\n    'submission': os.path.join(INPUT_DIR, 'sample_submission.csv'),\n    'train_image_dir': os.path.join(INPUT_DIR, 'train_images\/train_images'),\n    'test_image_dir': os.path.join(INPUT_DIR, 'test_images\/test_images'),\n    'state_dict': os.path.join(ARTIFACT_DIR, 'best_state_dicts.pth'),\n    'oof': os.path.join(ARTIFACT_DIR, 'oof.npy'),\n    'predictions': os.path.join(ARTIFACT_DIR, 'predictions.npy'),\n}\n\nID = 'fname'\nTARGET = 'label'\n\nSEED = 42\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nseed_everything(SEED)","6de523d0":"train_df = pd.read_csv(PATH['train'])\nsubmission_df = pd.read_csv(PATH['submission'])","283c2730":"train_df[ID] = train_df[ID].apply(lambda x: os.path.join(PATH['train_image_dir'], x))","15fa2463":"def softmax(logits):\n    return np.exp(logits) \/ np.sum(np.exp(logits), axis=1, keepdims=True)","35819305":"predictions = np.load(PATH['predictions'])\npredictions = softmax(predictions)\npredictions_label, predictions_proba = np.argmax(predictions, axis=1), np.max(predictions, axis=1)\n\npseudo_df = submission_df.copy()\npseudo_df[TARGET] = predictions_label\npseudo_df['proba'] = predictions_proba\n\npseudo_df = pseudo_df.loc[pseudo_df['proba'] > 0.999, :]\npseudo_df = pseudo_df.drop('proba', axis=1)\npseudo_df[ID] = pseudo_df[ID].apply(lambda x: os.path.join(PATH['test_image_dir'], x))","b7c0aa27":"train_df.shape[0], pseudo_df.shape[0], submission_df.shape[0]","e3ccb44c":"# print(f'number of train data: {len(train_df)}')\n# print(f'number of test data: {len(submission_df)}')","8f3f656a":"# print(f'number of unique label: {train_df[TARGET].nunique()}')","1187489d":"# sns.countplot(train_df[TARGET])\n# plt.title('train label distribution')\n# plt.show()","67ccea1e":"# train_path = [os.path.join(PATH['train_image_dir'], fn) for fn in train_df[ID]]\n# test_path = [os.path.join(PATH['test_image_dir'], fn) for fn in submission_df[ID]]\n# [train_rgb_status, _, _], [test_rgb_status, _, _] = get_image_status(train_path, test_path, use_tqdm=False)","e8b5ed23":"# print('Mean\/std of color mean')\n# print('train: {:.5f} (+-) {:.3f}'.format(\n#     np.mean(train_rgb_status['R']['mean']),\n#     np.mean(train_rgb_status['R']['std'])))\n# print('test : {:.5f} (+-) {:.3f}'.format(\n#     np.mean(test_rgb_status['R']['mean']),\n#     np.mean(test_rgb_status['R']['std'])))","c1fe0686":"class KmnistDataset(Dataset):\n    def __init__(\n        self,\n        paths,\n        labels,\n        transform=None,\n        with_memory_cache=False,\n    ):\n        super().__init__()\n\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.with_memory_cache = with_memory_cache\n\n        if with_memory_cache:\n            self.images = [None,] * len(paths)\n    \n    def load_image(self, path):\n#         image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n#         image = image.reshape(image.shape[0], image.shape[1], 1)\n        image = cv2.imread(path)\n        \n        return image\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        if self.with_memory_cache:\n            image = self.images[idx]\n            if image is None:\n                path = self.paths[idx]\n                image = self.load_image(path)\n                self.images[idx] = image\n        else:\n            path = self.paths[idx]\n            image = self.load_image(path)\n            \n        label = self.labels[idx]\n        \n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        return image, label","67cb26d9":"def get_dataloader(\n    X,\n    Y,\n    transform=None,\n    with_memory_cache=False,\n    batch_size=32,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True,\n):\n    dataset = KmnistDataset(\n        X,\n        Y,\n        transform=transform,\n        with_memory_cache=with_memory_cache,\n    )\n    \n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    \n    return loader","aa826e5a":"num_samples = 20\nsample = train_df.groupby(TARGET).apply(lambda df: df.sample(num_samples))\nfnames = sample[ID].to_list()\nlabels = sample[TARGET].to_list()","c7e87d48":"# transform = A.Compose([\n#     A.Normalize((0,), (1,)),\n#     ToTensorV2(),\n# ])\n\n# loader = get_dataloader(\n#     fnames, labels, PATH['train_image_dir'], \n#     transform=transform, batch_size=len(fnames))\n# loader = iter(loader)\n# x, y = next(loader)\n\n# imgtile = make_grid(x, nrow=num_samples)\n# imgtile = imgtile.permute(1, 2, 0).cpu()\n# plt.figure(figsize=(2 * 10, 2 * num_samples))\n# plt.imshow(imgtile)\n# plt.show()","6696af48":"IMSIZE = 128\n\ntransform = A.Compose([\n    A.Rotate(limit=5, p=1.0),\n    A.RandomResizedCrop(IMSIZE, IMSIZE, scale=(0.9, 1.0), ratio=(0.9, 1.1), p=1.0),\n    A.Cutout(num_holes=1, max_h_size=IMSIZE\/\/2, max_w_size=IMSIZE\/\/2, fill_value=48.89935, p=1.0),\n    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nloader = get_dataloader(fnames, labels, transform=transform, batch_size=len(fnames))\nloader = iter(loader)\nx, y = next(loader)\n\nimgtile = make_grid(x, nrow=num_samples, normalize=True)\nimgtile = imgtile.permute(1, 2, 0).cpu()\nplt.figure(figsize=(2 * 10, 2 * num_samples))\nplt.imshow(imgtile)\nplt.show()","d4affe2d":"# https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/resnet.htm\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n                     stride=stride, padding=padding, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n    \nclass SSELayer(nn.Module):\n    def __init__(self, channel):\n        super().__init__()\n        self.conv = nn.Sequential(\n            conv1x1(channel, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.conv(x)\n        return x * y.expand_as(x)\n\n\nclass CSELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel \/\/ reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel \/\/ reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n    \nclass SCSELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.sse = SSELayer(channel)\n        self.cse = CSELayer(channel, reduction)\n    \n    def forward(self, x):\n        return self.sse(x) + self.cse(x)\n    \n\ndef se_layer(channel, se_type=None, reduction=16):\n    if se_type is None:\n        return nn.Identity()\n    elif se_type == 'cse':\n        return CSELayer(channel, reduction)\n    elif se_type == 'sse':\n        return SSELayer(channel)\n    elif se_type == 'scse':\n        return SCSELayer(channel, reduction)\n    else:\n        raise NotImplementedError\n    \n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 norm_layer=None, se_type=None, reduction=16):\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        \n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.se = se_layer(planes, se_type=se_type, reduction=reduction)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10, width_per_group=64,\n                 se_type=None, reduction=16):\n        super(ResNet, self).__init__()\n        norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.base_width = width_per_group\n        self.se_type = se_type\n        self.reduction = reduction\n        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        norm_layer = self._norm_layer\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample, norm_layer=norm_layer,\n                            se_type=self.se_type, reduction=self.reduction))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, norm_layer=norm_layer,\n                                se_type=self.se_type, reduction=self.reduction))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n    \n    \ndef small_resnet18(se_type='scse'):\n    return ResNet(BasicBlock, [2, 2, 2, 2], se_type=se_type)\n\n\ndef small_resnet34(se_type='scse'):\n    return ResNet(BasicBlock, [3, 4, 6, 3], se_type=se_type)","a4f27749":"# class Model(nn.Module):\n#     def __init__(self, depth=18, se_type='scse'):\n#         super().__init__()\n#         if depth == 18:\n#             self.model = small_resnet18(se_type=se_type)\n#         elif depth == 34:\n#             self.model = small_resnet34(se_type=se_type)\n#         else:\n#             raise NotImplementedError\n    \n#     def forward(self, x):\n#         return self.model(x)\n\n\n# class Model(nn.Module):\n#     def __init__(self, depth=34):\n#         super().__init__()\n#         if depth == 18:\n#             self.model = models.resnet18(pretrained=True)\n#             self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n#         elif depth == 34:\n#             self.model = models.resnet34(pretrained=True)\n#             self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n#         else:\n#             raise NotImplementedError\n    \n#     def forward(self, x):\n#         return self.model(x)\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = models.resnext50_32x4d(pretrained=True)\n        self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n    \n    def forward(self, x):\n        return self.model(x)","f362732e":"model = Model()\nsummary(model, (3, IMSIZE, IMSIZE), device='cpu')","4c251165":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef accuracy_score_torch(y_pred, y):\n    y_pred = torch.argmax(y_pred, axis=1).cpu().numpy()\n    y = y.cpu().numpy()\n\n    return accuracy_score(y, y_pred)\n\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2","2c48259c":"def train(\n    params,\n    model,\n    optimizer,\n    criterion,\n    dataloader,\n    parent_bar=None,\n):\n    model.train()\n    losses = AverageMeter()\n    metrics = AverageMeter()\n    \n    for x, y in progress_bar(dataloader, parent=parent_bar):\n        x = x.to(dtype=torch.float32, device=DEVICE)\n        y = y.to(dtype=torch.long, device=DEVICE)\n        \n        optimizer.zero_grad()\n        y_pred = model(x)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        losses.update(loss.item())\n        metrics.update(accuracy_score_torch(y_pred, y))\n    \n    return losses.avg, metrics.avg\n\n\ndef valid(\n    params,\n    model,\n    criterion,\n    dataloader,\n):\n    model.eval()\n    losses = AverageMeter()\n    metrics = AverageMeter()\n\n    for x, y in dataloader:\n        x = x.to(dtype=torch.float32, device=DEVICE)\n        y = y.to(dtype=torch.long, device=DEVICE)\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n        \n        losses.update(loss.item())\n        metrics.update(accuracy_score_torch(y_pred, y))\n    \n    return losses.avg, metrics.avg","9d222373":"params = {\n    'n_splits': 5,\n    'batch_size': 64,\n    'test_batch_size': 128,\n    'lr': 1e-3,\n    'weight_decay': 1e-5,\n    'epochs': 25,\n    'restart': 25,\n}","2bf0899b":"kf = StratifiedKFold(n_splits=params['n_splits'], random_state=SEED, shuffle=True)\noof = np.zeros((len(train_df), 10))\nbest_state_dicts = []\n\nlogger.info('** start kfold training **')\nfor i, (dev_idx, val_idx) in enumerate(kf.split(train_df[ID], train_df[TARGET])):\n    logger.info(f'[fold: {i}]')\n    \n    # ------------------------------\n    # dev\/val dataloader\n    # ------------------------------\n    \n    dev_df = train_df.iloc[dev_idx, :].reset_index(drop=True)\n    val_df = train_df.iloc[val_idx, :].reset_index(drop=True)\n    \n    # pseudo labeling\n    dev_df = pd.concat([dev_df, pseudo_df], axis=0).reset_index(drop=True)\n    \n    dev_transform = A.Compose([\n        A.Rotate(limit=5, p=1.0),\n        A.RandomResizedCrop(IMSIZE, IMSIZE, scale=(0.9, 1.0), ratio=(0.9, 1.1), p=1.0),\n        A.Cutout(num_holes=1, max_h_size=IMSIZE\/\/2, max_w_size=IMSIZE\/\/2, fill_value=48.89935, p=1.0),\n        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n    val_transform = A.Compose([\n        A.Resize(IMSIZE, IMSIZE, p=1),\n        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n    \n    dev_dataloader = get_dataloader(\n        dev_df[ID],\n        dev_df[TARGET],\n        transform=dev_transform,\n        with_memory_cache=True,\n        batch_size=params['batch_size'],\n        shuffle=True,\n        pin_memory=True,\n    )\n    val_dataloader = get_dataloader(\n        val_df[ID],\n        val_df[TARGET],\n        transform=val_transform,\n        with_memory_cache=True,\n        batch_size=params['test_batch_size'],\n        shuffle=False,\n        pin_memory=True,\n    )\n    \n    # ----------------------------------------\n    # model, loss, optimizer, scheduler\n    # ----------------------------------------\n\n    model = Model().to(DEVICE)\n    optimizer = Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n    scheduler = CosineAnnealingWarmRestarts(optimizer, params['restart'])\n    criterion = nn.CrossEntropyLoss()\n    \n    # ------------------------------\n    # training loop\n    # ------------------------------\n    \n    best_loss = np.inf\n    best_state_dict = model.state_dict()\n    \n    mb = master_bar(range(params['epochs']))\n    for epoch in mb:\n        # train\n        dev_loss, dev_metric = train(\n            params, model, optimizer, criterion, dev_dataloader, parent_bar=mb)\n        \n        # valid\n        val_loss, val_metric = valid(\n            params, model, criterion, val_dataloader)\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state_dict = model.state_dict()\n\n        msg = (\n            'epoch: {}\/{}'\n            ' - lr: {:.6f}'\n            ' - loss: {:.5f}'\n            ' - acc: {:.5f}'\n            ' - val_loss: {:.5f}'\n            ' - val_acc: {:.5f}'\n        ).format(\n            epoch + 1,\n            params['epochs'],\n            optimizer.state_dict()['param_groups'][0]['lr'],\n            dev_loss,\n            dev_metric,\n            val_loss,\n            val_metric,\n        )\n        logger.info(msg)\n\n        if scheduler is not None:\n            scheduler.step()\n    \n    # ------------------------------\n    # end of training loop\n    # ------------------------------\n    \n    best_state_dicts.append(best_state_dict)\n\n    model.load_state_dict(best_state_dict)\n    model.eval()\n    _oof = []\n\n    for x, _ in val_dataloader:\n        x = x.to(dtype=torch.float32, device=DEVICE)\n\n        with torch.no_grad():\n            y_pred = model(x)\n            _oof.append(y_pred.cpu().numpy())\n    \n    oof[val_idx] = np.concatenate(_oof)","9bfb85df":"cv_score = accuracy_score(train_df[TARGET], np.argmax(oof, axis=1))\nlogger.info(f'CV: {cv_score:.5f}')","4b6388b0":"test_transform = A.Compose([\n    A.Resize(IMSIZE, IMSIZE, p=1),\n    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\ntest_dataloader = get_dataloader(\n    submission_df[ID].apply(lambda x: os.path.join(PATH['test_image_dir'], x)),\n    submission_df[TARGET],\n    transform=test_transform,\n    with_memory_cache=True,\n    batch_size=params['test_batch_size'],\n    shuffle=False,\n    pin_memory=True,\n)","b91dcb6f":"predictions = np.zeros((len(submission_df), 10))\n\nfor state_dict in best_state_dicts:\n    model = Model().to(DEVICE)\n    model.load_state_dict(state_dict)\n    model.eval()\n    _predictions = []\n    \n    for x, _ in test_dataloader:\n        x = x.to(dtype=torch.float32, device=DEVICE)\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            _predictions.append(y_pred.cpu().numpy())\n    \n    _predictions = np.concatenate(_predictions)\n    predictions += _predictions \/ len(best_state_dicts)","bbd71f2a":"np.save('oof', oof)\nnp.save('predictions', predictions)\n\nsaved_best_state_dicts = {}\nfor i, bsd in enumerate(best_state_dicts):\n    saved_best_state_dicts[f'f{i}'] = bsd\ntorch.save(saved_best_state_dicts, 'best_state_dicts.pth')\n\nsubmission_df[TARGET] = np.argmax(predictions, axis=1).tolist()","12c0d87d":"submission_df.to_csv('submission.csv', index=False)\nfrom IPython.display import FileLink\nFileLink('submission.csv')","00950ad4":"## utils","5ba35258":"# Pseudo Labeling","078f9b28":"# EDA","7db11b01":"# Training","87c75314":"# Loading","fccaca22":"## dataset, dataloader","e557414f":"# Prediction","076e019e":"# Save outputs","3c4a8e6b":"## model","75a43a75":"```\nMean\/std of color mean\ntrain: 48.89935 (+-) 86.269\ntest : 47.06204 (+-) 84.471\n```","68e14ac0":"# Libraries"}}