{"cell_type":{"87c5f68a":"code","1a783d73":"code","d3051320":"code","d8474e69":"code","cecf6d5e":"code","3016b119":"code","aeb8e686":"code","c4e0f643":"code","9625f71c":"code","0023005c":"code","8e9bc60f":"code","853faffa":"code","55f00613":"code","c19be967":"code","5b221c20":"code","39c2098c":"code","aa2f3ed0":"code","c257b778":"code","071094eb":"markdown","1b10aeb6":"markdown","9b629185":"markdown","727b5242":"markdown","42f3fe28":"markdown","77110676":"markdown","856c431d":"markdown","6bfe172c":"markdown","5dab04f6":"markdown","4da97306":"markdown"},"source":{"87c5f68a":"!nvidia-smi","1a783d73":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')","d3051320":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport random\nimport cv2\npd.set_option('display.max_columns', None)\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Machine Learning\nfrom xgboost import XGBRegressor\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","d8474e69":"csv_dir = '..\/input\/petfinder-pawpularity-score'\ntest_dir = '..\/input\/petfinder-pawpularity-score\/test'\nmodels_dir = '..\/input\/pawpularity-contest-models\/Swin_large_EfficientnetB4'\n\ntest_file_path = os.path.join(csv_dir, 'test.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\nprint(f'Test file: {test_file_path}')\nprint(f'Models path: {models_dir}')","cecf6d5e":"test_df = pd.read_csv(test_file_path)\nsample_df = pd.read_csv(sample_sub_file_path)","3016b119":"def return_filpath(name, folder):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path","aeb8e686":"test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))","c4e0f643":"test_df.head()","9625f71c":"target = ['Pawpularity']\nnot_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\ncols = list(test_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","0023005c":"params = {\n    'model_1': 'swin_large_patch4_window12_384',\n    'model_2': 'efficientnet_b4',\n    'dense_features': features,\n    'pretrained': False,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'batch_size': 8,\n    'num_workers' : 2,\n    'out_features': 1,\n    'debug': False\n}","8e9bc60f":"if params['debug']:\n    test_df = test_df.sample(frac=0.1)","853faffa":"def get_test_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )","55f00613":"class CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label","c19be967":"class PetNet(nn.Module):\n    def __init__(self, model_1_name=params['model_1'], model_2_name=params['model_2'],\n                 out_features=params['out_features'], inp_channels=params['inp_channels'],\n                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n        super().__init__()\n        \n        # Transformer\n        self.model_1 = timm.create_model(model_1_name, pretrained=pretrained,\n                                         in_chans=inp_channels)\n        n_features_1 = self.model_1.head.in_features\n        self.model_1.head = nn.Linear(n_features_1, 128)\n        \n        # Conventional CNN\n        self.model_2 = timm.create_model(model_2_name, pretrained=pretrained,\n                                         in_chans=inp_channels)\n        out_channels = self.model_2.conv_stem.out_channels\n        kernel_size = self.model_2.conv_stem.kernel_size\n        stride = self.model_2.conv_stem.stride\n        padding = self.model_2.conv_stem.padding\n        bias = self.model_2.conv_stem.bias\n        self.model_2.conv_stem = nn.Conv2d(inp_channels, out_channels,\n                                           kernel_size=kernel_size,\n                                           stride=stride, padding=padding,\n                                           bias=bias)\n        n_features_2 = self.model_2.classifier.in_features\n        self.model_2.classifier = nn.Linear(n_features_2, 128)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128 + 128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, image, dense):\n        transformer_embeddings = self.model_1(image)\n        conv_embeddings = self.model_2(image)\n        features = torch.cat([transformer_embeddings, conv_embeddings, dense],\n                             dim=1)\n        x = self.dropout(features)\n        output = self.fc(x)\n        return output","5b221c20":"predictions_nn = None\nfor model_name in glob.glob(models_dir + '\/*.pth'):\n    model = PetNet()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = CuteDataset(\n        images_filepaths = test_df['image_path'].values,\n        dense_features = test_df[params['dense_features']].values,\n        targets = sample_df['Pawpularity'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n            images = images.to(params['device'], non_blocking=True)\n            dense = dense.to(params['device'], non_blocking=True)\n            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n                \n    test_df[model_name.split('\/')[-1].split('_')[-3]] = temp_preds\n\n    if predictions_nn is None:\n        predictions_nn = temp_preds\n    else:\n        predictions_nn += temp_preds\n        \npredictions_nn \/= (len(glob.glob(models_dir + '\/*.pth')))","39c2098c":"sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['Pawpularity'] = predictions_nn","aa2f3ed0":"sub_df.head()","c257b778":"sub_df.to_csv('submission.csv', index=False)","071094eb":"# CFG","1b10aeb6":"# CNN Model","9b629185":"# Augmentations","727b5242":"# Dataset","42f3fe28":"# Import","77110676":"**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. \ud83d\ude0a**","856c431d":"# About This Notebook\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'> This implementation is based on a vanilla <b>swin_large_patch4_window12_384<\/b> and <b>efficientnet_b4<\/b> in Pytorch for the Pawpularity Competition.<br>\nThis model uses <b>both images and dense features<\/b> for score prediction.<br>\n<b>This scores around 18.1 LB.<\/b><\/p>\n\n<p style='color: #fc0362; font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 20px'>Here I have attempted to use a Vision transformer model and a CNN model in conjunction. The embeddings from both the Swin and EfficientNet model are concatenated along with the dense features before passing through a 2 layer fully connected network.<\/p>\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 18px'>\nTraining Params: -\n<ol style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li> <b>Dataset<\/b>: - 3-channel RGB Images (384x384) with separate dense features <\/li>\n<li> <b>Augmentations<\/b>: - Resize, Normalize, HorizontalFlip, VerticalFlip, RandomBrightness, RandomResizedCrop, HueSaturationValue, RandomBrightnessContrast <\/li>\n<li> <b>Optimizer<\/b>: - AdamW <\/li>\n<li> <b>Scheduler<\/b>: - CosineAnnealingLR <\/li>\n<li> <b>Model<\/b>: - swin_large_patch4_window12_384, efficientnet_b4 <\/li>\n<li> <b>Initial Weights<\/b>: - Imagenet <\/li>\n<li> <b>Max Epochs<\/b>: - 8 (~29 min per epoch on P100 PCIE GPU) <\/li>\n<li> <b>Saved Weights<\/b>: - 10-fold ensemble. Weights having highest OOF score on RMSE metric were saved. <\/li>\n<\/ol>\n<\/p>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\nThis notebook only contains the inference for the model as described above.<br><br>\nIf you are looking for a starter training notebook please follow the link below: -<br><\/p>\n<ul style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li>Baseline Model Notebook:- <a href=https:\/\/www.kaggle.com\/manabendrarout\/pawpularity-score-starter-image-dense-train>Training Notebook<\/a><\/li>\n<\/ul>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<b>NB:-<\/b> This training notebook uses a different NN architecture. Not the exact architecture used for this notebook. But apart from the architecture, everything else (training parameters, optimizers, schedulers, etc) is same. I had to use a different architecture for demonstration because Kaggle has a timeout limit which is not possible to adhere with the transformer model.\n<\/p>\n\n![SETI](https:\/\/www.petfinder.my\/images\/cuteness_meter.jpg)  \n\n**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. \ud83d\ude0a**","6bfe172c":"# Get GPU Info","5dab04f6":"# Prediction","4da97306":"# Submission"}}