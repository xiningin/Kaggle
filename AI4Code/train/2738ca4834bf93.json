{"cell_type":{"d91e8e01":"code","4e9a44d3":"code","0922d6a8":"code","d70e31c9":"code","7cfd7a9f":"code","8d0aaacf":"code","f40b2de4":"code","e841f310":"code","fa4abde4":"code","d8cf5328":"code","5a28caff":"code","63f1e8e7":"code","2caee38c":"code","10bf08ff":"code","6b4c299e":"code","9c8c4289":"code","d1ba9a1c":"code","870c6c1d":"code","a7e4cc95":"code","d6a7cf45":"code","2ab9f765":"code","830533ef":"code","ef4bf2c4":"code","13392545":"code","83b2e101":"code","203f8ca5":"code","16f3b28c":"code","0d8305fd":"code","0ac0442f":"code","7640370b":"markdown","ac9055c8":"markdown","3c59f06b":"markdown","096f3e51":"markdown","d1b45518":"markdown","5f115dfa":"markdown","0922a248":"markdown","b73e8bf8":"markdown","857a9324":"markdown","00363141":"markdown","16d1ef1f":"markdown","23f2f9e2":"markdown","9e9f3ec9":"markdown","0af5a981":"markdown","334b16c2":"markdown","c4641e73":"markdown","886fb7c9":"markdown","3beebefa":"markdown","d7407be5":"markdown","63692a2d":"markdown","e0e61443":"markdown","9ebb885f":"markdown","679d5c42":"markdown","d250c9d2":"markdown","e0f667fb":"markdown","4dde9e38":"markdown","df84127f":"markdown","a6facf8c":"markdown","218244a9":"markdown"},"source":{"d91e8e01":"# Stsandard libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime\nimport time\n\n# Utilities\nfrom viz_utils import *\nfrom ml_utils import *\nfrom custom_transformers import *\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_val_predict, \\\n                                    learning_curve\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, \\\n    accuracy_score, precision_score, recall_score, f1_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE","4e9a44d3":"# Data path\ndf_ori = import_data('..\/input\/creditcardfraud\/creditcard.csv', optimized=True)\ndf_ori.columns = [col.lower() for col in df_ori.columns]\n\n# Results\nprint(f'Data dimension: {df_ori.shape}')\ndf_ori.head()","0922d6a8":"# Target class balance\nfig, ax = plt.subplots(figsize=(7, 7))\nlabel_names = ['Non-Fraud', 'Fraud']\ncolor_list = ['darkslateblue', 'crimson']\ntext = f'Total\\n{len(df_ori)}'\ntitle = 'Target Class Balance'\n\n# Visualizing it through a donut chart\ndonut_plot(df_ori, col='class', ax=ax, label_names=label_names, colors=color_list, title=title, text=text)","d70e31c9":"df_overview = data_overview(df_ori, corr=True, label_name='class')\ndf_overview.head(20)","7cfd7a9f":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 6))\ntarget_correlation_matrix(data=df_ori, label_name='class', corr='positive', ax=axs[0])\ntarget_correlation_matrix(data=df_ori, label_name='class', corr='negative', ax=axs[1])\n\nplt.tight_layout()\nplt.show()","8d0aaacf":"numplot_analysis(df_ori.drop('class', axis=1), fig_cols=3)","f40b2de4":"# Features to be used on dataprep pipeline\nfeatures_ori = list(df_ori.drop('time', axis=1).columns)\n\n# Construction a pre-processing pipeline from columns_transformers.py\npre_processing_pipe = Pipeline([\n    ('selector', FeatureSelection(features=features_ori)),\n    ('dup_dropper', DropDuplicates()),\n    ('splitter', SplitData(target='class'))\n])\n\n# Executing the pipeline\nX_train, X_test, y_train, y_test = pre_processing_pipe.fit_transform(df_ori)\nmodel_features = list(X_train.columns)\n\n# Looking at the results\nprint(f'Dimens\u00f5es de X_train: {X_train.shape}')\nprint(f'Dimens\u00f5es de y_train: {y_train.shape}')\nprint(f'\\nDimens\u00f5es de X_test: {X_test.shape}')\nprint(f'Dimens\u00f5es de y_test: {y_test.shape}')","e841f310":"# Splitting the data by the dtype\nnum_attribs, cat_attribs = split_cat_num_data(X_train)\nprint(f'Total of numerical features: {len(num_attribs)}')\nprint(f'Total of categorical features: {len(cat_attribs)}')","fa4abde4":"# Preparing a dictionary to feed the ClassifiersAnalysis class\nset_prep = {\n    'X_train_prep': X_train.values,\n    'X_test_prep': X_test.values,\n    'y_train': y_train,\n    'y_test': y_test\n}","d8cf5328":"# Logistic Regression hyperparameters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Decision Trees hyperparameters\ntree_param_grid = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': [3, 5, 10, 20],\n    'max_features': np.arange(1, X_train.shape[1]),\n    'class_weight': ['balanced', None],\n    'random_state': [42]\n}\n\n# Random Forest hyperparameters\nforest_param_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [3, 5, 10, 20, 50],\n    'n_estimators': [50, 100, 200, 500],\n    'random_state': [42],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': ['balanced', None]\n}\n\n# LightGBM hyperparameters\nlgbm_param_grid = {\n    'num_leaves': list(range(8, 92, 4)),\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'max_depth': [3, 4, 5, 6, 8, 12, 16],\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'bagging_freq': [3, 4, 5, 6, 7],\n    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n    'reg_alpha': np.linspace(0.1, 0.95, 10),\n    'reg_lambda': np.linspace(0.1, 0.95, 10),\n}\n\nlgbm_fixed_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}","5a28caff":"# Preparando set de classificadores\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    },\n    'DecisionTrees': {\n        'model': DecisionTreeClassifier(),\n        'params': tree_param_grid\n    },\n    'RandomForest': {\n        'model': RandomForestClassifier(),\n        'params': forest_param_grid\n    },\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(**lgbm_fixed_params),\n        'params': lgbm_param_grid\n    }\n}","63f1e8e7":"# Instanciando classe e treinando set de classificadores\nclf_tool = BinaryClassifiersAnalysis()\nclf_tool.fit(set_classifiers, X_train, y_train, random_search=True, cv=3, verbose=5)","2caee38c":"df_performances = clf_tool.evaluate_performance(X_train, y_train, X_test, y_test, cv=3)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\ndf_performances.reset_index(drop='True').style.background_gradient(cmap=cm)","10bf08ff":"fig, ax = plt.subplots(figsize=(13, 12))\nlgbm_feature_importance = clf_tool.feature_importance_analysis(model_features, specific_model='LightGBM', ax=ax)\nplt.show()","6b4c299e":"clf_tool.plot_roc_curve()","9c8c4289":"clf_tool.plot_score_distribution('LightGBM')","d1ba9a1c":"# Separa\u00e7\u00e3o por faixa\nclf_tool.plot_score_bins('LightGBM', bin_range=0.1)","870c6c1d":"# Applying undersampling\nrus = RandomUnderSampler()\nX_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n\n# Training new classifiers using undersampling\nundersamp_approach= '_undersamp'\nclf_tool.fit(set_classifiers, X_train_under, y_train_under, random_search=True, cv=3, approach=undersamp_approach)","a7e4cc95":"df_performances = clf_tool.evaluate_performance(X_train_under, y_train_under, X_test, y_test, cv=3)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\ndf_performances.reset_index(drop='True').style.background_gradient(cmap=cm)","d6a7cf45":"clf_tool.plot_roc_curve()","2ab9f765":"clf_tool.plot_score_distribution('LightGBM_undersamp')","830533ef":"clf_tool.plot_score_bins('LightGBM_undersamp', bin_range=0.1)","ef4bf2c4":"lgbm_set_classifier = {}\nlgbm_set_classifier['LightGBM'] = set_classifiers['LightGBM']","13392545":"# Applying oversampling\nsm = SMOTE(sampling_strategy='minority', random_state=42)\nX_train_over, y_train_over = sm.fit_sample(X_train, y_train)\n\n# Treinando novo modelo ap\u00f3s undersampling\noversamp_approach= '_oversamp'\nclf_tool.fit(lgbm_set_classifier, X_train_over, y_train_over, random_search=True, cv=3, approach=oversamp_approach)","83b2e101":"df_performances = clf_tool.evaluate_performance(X_train_over, y_train_over, X_test, y_test, cv=3)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\ndf_performances.reset_index(drop='True').style.background_gradient(cmap=cm)","203f8ca5":"clf_tool.plot_roc_curve()","16f3b28c":"clf_tool.plot_score_distribution('LightGBM_oversamp')","0d8305fd":"clf_tool.plot_score_bins('LightGBM_oversamp', bin_range=0.1)","0ac0442f":"df_performances.query('approach == \"Teste\"').sort_values(by='auc', ascending=False)","7640370b":"___\n* _Training each model selected with RandomizedSearchCV_\n___","ac9055c8":"___\n* _Splitting the probabilities on bins_\n___","3c59f06b":"___\n* _Target correlation (positive and negative)_\n___","096f3e51":"___\n* _ROC Curve for training and testing data_\n___","d1b45518":"## Oversampling","5f115dfa":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Libraries<\/a><\/span><\/li><li><span><a href=\"#Reading-the-data\" data-toc-modified-id=\"Reading-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Reading the data<\/a><\/span><\/li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Modeling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Undersampling\" data-toc-modified-id=\"Undersampling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Undersampling<\/a><\/span><\/li><li><span><a href=\"#Oversampling\" data-toc-modified-id=\"Oversampling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Oversampling<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","0922a248":"___\n* _Splitting the probabilities on bins_\n___","b73e8bf8":"___\n* _Numerical features distribution_\n___","857a9324":"___\n* _Evaluating on training and test data_\n___","00363141":"___\n* _Evaluating on training and test data_\n___","16d1ef1f":"# Libraries","23f2f9e2":"Filtering the model\/approach with highest AUC on test data:","9e9f3ec9":"___\n* _Pr\u00e9-processing Pipeline_\n___","0af5a981":"___\n* _Score distributions on training and testing data_\n___","334b16c2":"___\n* _Splitting the probabilities on bins_\n___","c4641e73":"___\n* _Models: LogisticRegression, DecisionTrees, RandomForest and LightGBM_\n___","886fb7c9":"___\n* _Score distributions on training and testing data_\n___","3beebefa":"___\n* _Complete overview of each feature_\n___","d7407be5":"___\n* _Score distributions on training and testing data_\n___","63692a2d":"# Modeling","e0e61443":"___\n* _Looking at the feature importance of a specific model_\n___","9ebb885f":"Let's first read the data with an optimized approach (low memory usage)","679d5c42":"# Reading the data","d250c9d2":"___\n* _ROC Curve for training and testing data_\n___","e0f667fb":"On the problem context of creating a model to predict fraudulent transactions base on the use of credit card, we have a dataset showing us details of these transactions realized during two days of September 2013 in Europe. The data collected has 492 frauds itentified among 284,807 transactions, so this is a really unbalanced data as long as only 0.172% of all the transactions available are really frauds.\n\nBeyond this, we have 30 features where 28 of them are generated by some dimensionality reduction step (Principal Componente Analysis - PCA) and by security, the provider didn't specified the meaning of those features. So the other 2 can be defined as:\n\n* **Time:** Seconds between each transaction and the first one;\n* **Amount:** Transaction's value.\n\nFor the evaluation metric it's recommended to use AUC (Are Under the Curve ROC), as long as we have a really unbalanced dataset and on those cases it's not good to use standard metrics like accuracy or the confusion matrix itself. In this case we will go deep into models probabilities.\n\nBy the end, if you like this kernel, please **upvote!**","4dde9e38":"___\n* _ROC Curve for training and testing data_\n___","df84127f":"Let's change the set_classifiers dictionary to optimize time","a6facf8c":"## Undersampling","218244a9":"___\n* _Evaluating on training and test data_\n___"}}