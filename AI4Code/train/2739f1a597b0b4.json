{"cell_type":{"66ae8dff":"code","849731c8":"code","9ed9c268":"code","5e42ec84":"code","d383940b":"code","d900f6d4":"code","21910216":"code","85643e88":"code","fe387a70":"code","7eceab49":"code","cab8ac0f":"code","3e961bef":"code","7372f6fb":"code","30a0ea5c":"code","4e51332f":"code","b35ce6e1":"code","ae1f409d":"code","5a66ffbf":"code","f3dd0014":"code","96f03c2e":"code","5ada1297":"code","024f7d74":"code","e09b7fce":"code","da325b87":"code","72d09328":"markdown","4274f9bc":"markdown","704c8a1d":"markdown","2c8cc670":"markdown","10c45430":"markdown","9754e577":"markdown","25eb2ae1":"markdown","3925afce":"markdown","a7d44d2a":"markdown","b5cb212b":"markdown","edf45b3b":"markdown","2bf044e1":"markdown","cc86568a":"markdown","d950859e":"markdown","5548cff0":"markdown","34de7e4d":"markdown","782bd206":"markdown","cfa4fc3d":"markdown","3a7a03de":"markdown","0a79c44e":"markdown","597649f3":"markdown","4c39024a":"markdown","cbd4bda6":"markdown","78735ad1":"markdown","0240920a":"markdown","4ab0c14e":"markdown"},"source":{"66ae8dff":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.applications.vgg19 import VGG19\nfrom keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense,Flatten,Dropout\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix","849731c8":"(x_train,y_train),(x_test,y_test) = cifar10.load_data()","9ed9c268":"print(\"x train shape:\", x_train.shape)\nprint(\"y train shape:\",y_train.shape)\nprint(\"x test shape:\",x_test.shape)\nprint(\"y test shape:\",y_test.shape)","5e42ec84":"number_of_class = 10\ny_train = to_categorical(y_train,num_classes = number_of_class)\ny_test = to_categorical(y_test,num_classes = number_of_class)","d383940b":"plt.imshow(x_train[10000])\nplt.axis(\"off\")\nplt.title(\"sample automobile image\")\nplt.show()\n\nplt.figure()\n\nplt.imshow(x_train[20010])\nplt.title(\"sample horse image\")\nplt.axis(\"off\")\nplt.show()","d900f6d4":"fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n\naxs[0, 0].imshow(x_train[1000])\naxs[0, 0].axis(\"off\")\n\naxs[1, 0].imshow(x_train[2003])\naxs[1, 0].axis(\"off\")\n\n\naxs[0, 1].imshow(x_train[30])\naxs[0, 1].axis(\"off\")\n\n\naxs[1, 1].imshow(x_train[4000])\naxs[1, 1].axis(\"off\")\n\n\naxs[0, 2].imshow(x_train[10000])\naxs[0, 2].axis(\"off\")\n\n\naxs[1, 2].imshow(x_train[2000])\naxs[1, 2].axis(\"off\")\n\n\naxs[2, 2].imshow(x_train[3001])\naxs[2, 2].axis(\"off\")\n\n\naxs[2, 0].imshow(x_train[4001])\naxs[2, 0].axis(\"off\")\n\n\naxs[2, 1].imshow(x_train[4008])\naxs[2, 1].axis(\"off\")\n\nplt.show()","21910216":"def resize(img):\n    numberofImage = img.shape[0]\n    new_array = np.zeros((numberofImage,48,48,3))\n    for i in range(numberofImage):\n        new_array[i] = tf.image.resize(img[i],(48,48))\n    return new_array","85643e88":"x_train_i = resize(x_train)","fe387a70":"print(\"x train_i shape:\",x_train_i.shape)","7eceab49":"x_test_i = resize(x_test)","cab8ac0f":"print(\"x test_i shape:\",x_test_i.shape)","3e961bef":"plt.imshow(x_train[10000])\nplt.axis(\"off\")\nplt.title(\"32x32 automobile image\")\nplt.show()\n\nplt.figure()\n\nplt.imshow(x_train_i[10000].astype(np.uint8))\nplt.axis(\"off\")\nplt.title(\"48x48 automobile image\")\nplt.show()","7372f6fb":"vgg19 = VGG19(include_top = False, weights = \"imagenet\", input_shape = (48,48,3))","30a0ea5c":"vgg19.summary()","4e51332f":"vgg19.layers","b35ce6e1":"model = Sequential()\nfor layer in vgg19.layers:\n    model.add(layer)","ae1f409d":"for layer in model.layers:\n    layer.trainable = False","5a66ffbf":"model.add(Flatten())\nmodel.add(Dense(units = 256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 128, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = number_of_class, activation = \"softmax\"))","f3dd0014":"model.summary()","96f03c2e":"model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","5ada1297":"epochs = 20\nbatch_size = 500\nsteps_per_epoch = (x_train_i[0] \/\/ batch_size)","024f7d74":"hist = model.fit(x_train_i, y_train, validation_split = 0.1,\n                epochs = epochs, batch_size = batch_size)\n                ","e09b7fce":"plt.figure(figsize=(6,6))\nplt.plot(hist.history['loss'], color='b', label=\"Training loss\")\nplt.plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.figure(figsize=(6,6))\nplt.plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend(loc = \"lower right\")\nplt.show()","da325b87":"y_predict=model.predict(x_test_i)\ny_predict_classes=np.argmax(y_predict,axis=1)\ny_true=np.argmax(y_test,axis=1)\nconfusion_mtx=confusion_matrix(y_true,y_predict_classes)\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(confusion_mtx,annot=True,linewidths=0.01,cmap=\"Blues\",linecolor=\"white\",fmt=\".1f\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","72d09328":"#### VGG19 Structere:","4274f9bc":"## Evaluate the Model","704c8a1d":"## Import Libraries","2c8cc670":"![image.png](attachment:image.png)","10c45430":"### Visualizing","9754e577":"In this tutorial, Objects in the CIFAR-10 dataset will be tried to be detected by using the VGG-19 model, which is a transfer learning method created by using a Convolutional Neural Network structure.","25eb2ae1":"![image.png](attachment:image.png)","3925afce":"<i> Let's look at all the layers of our model. <\/i>","a7d44d2a":"# CIFAR-10 Object Detection with VGG-19","b5cb212b":"<b> Since VGG19 works with images with at least 48x48 pixels and the images in our dataset are 32x32 pixels, we need to increase the dimension of the images in our dataset. <\/b>","edf45b3b":"## Increase Dimension","2bf044e1":"### Confusion Matrix","cc86568a":"<i> Let's look at the before and after pictures that we reshaped into 48x48 pixels. <\/i>","d950859e":"<i> Let's add the last layers of our model. <\/i>","5548cff0":"### About CIFAR-10","34de7e4d":"VGG-19 is a trained Convolutional Neural Network, from Visual Geometry Group, Department of Engineering Science, University of Oxford. The number 19 stands for the number of layers with trainable weights. 16 Convolutional layers and 3 Fully Connected layers. The VGG-19 was trained on the ImageNet challenge (ILSVRC) 1000-class classification task. The network takes a (224, 224, 3) RBG image as the input.","782bd206":"### What is VGG19","cfa4fc3d":"### Label Encoding","3a7a03de":"## Compining the Model","0a79c44e":"The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.","597649f3":"## VGG19","4c39024a":"## Fitting the Model","cbd4bda6":"## Loading Data & Data Preprocessing","78735ad1":"<i> We don't want our previously trained layers to be retrained. <\/i>","0240920a":"### Titles in the Study:","4ab0c14e":"- Import Libraries\n- Loading Data & Data Preprocessing\n- VGG19\n- Compining the Model\n- Fitting the Model\n- Evaluate the Model"}}