{"cell_type":{"f17642a0":"code","b9d6cb2d":"code","dd6acc0c":"code","6daf9753":"code","ba71ca2b":"code","947f312f":"code","964c0076":"code","47c86a5e":"code","b4c062f9":"code","cd71830a":"code","21cc00c0":"code","16882723":"code","88e96185":"code","2ca9bbaf":"code","ea7a1e9b":"code","24f3d941":"code","82a769ed":"code","696b4377":"code","d836482f":"code","a37a2f4d":"code","68700402":"code","af1d4d36":"code","2d078d9c":"code","02a61726":"code","834ef650":"code","4e33339c":"code","ca3d6758":"code","7997978e":"code","dfd3857d":"code","2052e429":"code","405da93d":"code","573c1aa5":"code","af096c18":"code","c1186945":"code","2613bf6d":"code","782e637a":"code","186cd242":"markdown","9f240296":"markdown","9b864baf":"markdown","b95f79cc":"markdown","9584bd73":"markdown","e48407c7":"markdown","41963fa9":"markdown","d679f683":"markdown","dec94491":"markdown","9d7100b1":"markdown","effde5ac":"markdown","d5e8dbbc":"markdown","bdf59b73":"markdown","81c63c59":"markdown","2b0337f0":"markdown","9ca2b283":"markdown","3ba7c245":"markdown","a57fd1a9":"markdown","186f5dc6":"markdown"},"source":{"f17642a0":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","b9d6cb2d":"from fastai import *\nfrom fastai.vision import *\nimport pandas as pd\nimport matplotlib.pyplot as plt","dd6acc0c":"# Making pretrained weights work without needing to find the default filename\nif not os.path.exists('\/tmp\/.cache\/torch\/checkpoints\/'):\n        os.makedirs('\/tmp\/.cache\/torch\/checkpoints\/')\n!cp '..\/input\/resnet50\/resnet50.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet50-19c8e357.pth'","6daf9753":"import os\nos.listdir('..\/input')","ba71ca2b":"print('Make sure cudnn is enabled:', torch.backends.cudnn.enabled)","947f312f":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 999\nseed_everything(SEED)","964c0076":"base_image_dir = os.path.join('..', 'input\/aptos2019-blindness-detection\/')\ntrain_dir = os.path.join(base_image_dir,'train_images\/')\ndf = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\ndf['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\ndf = df.drop(columns=['id_code'])\ndf = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ndf.head(10)","47c86a5e":"len_df = len(df)\nprint(f\"There are {len_df} images\")","b4c062f9":"df['diagnosis'].hist(figsize = (10, 5))","cd71830a":"bs = 64 #smaller batch size is better for training, but may take longer\nsz=224","21cc00c0":"tfms = get_transforms(do_flip=True,flip_vert=True,max_rotate=360,max_warp=0,max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)\nsrc = (ImageList.from_df(df=df,path='.\/',cols='path') #get dataset from dataset\n        .split_by_rand_pct(0.2) #Splitting the dataset\n        .label_from_df(cols='diagnosis') #obtain labels from the level column\n      )\ndata= (src.transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') #Data augmentation\n        .databunch(bs=bs,num_workers=4) #DataBunch\n        .normalize(imagenet_stats) #Normalize     \n       )","16882723":"data.show_batch(rows=3, figsize=(7,6))","88e96185":"from sklearn.metrics import cohen_kappa_score\ndef quadratic_kappa(y_hat, y):\n    return torch.tensor(cohen_kappa_score(y_hat.argmax(dim=-1), y, weights='quadratic'),device='cuda:0')","2ca9bbaf":"learn = cnn_learner(data, base_arch=models.resnet50, metrics = [quadratic_kappa])","ea7a1e9b":"learn.fit_one_cycle(4,max_lr = 1e-2)","24f3d941":"learn.recorder.plot_losses()\nlearn.recorder.plot_metrics()","82a769ed":"learn.unfreeze()\nlearn.fit_one_cycle(6, max_lr=slice(1e-6,1e-3))","696b4377":"learn.recorder.plot_losses()\nlearn.recorder.plot_metrics()","d836482f":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","a37a2f4d":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","68700402":"idx = 1\nim,cl = learn.data.dl(DatasetType.Valid).dataset[idx]\ncl = int(cl)\nim.show(title=f\"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}\")","af1d4d36":"xb,_ = data.one_item(im) #put into a minibatch of batch size = 1\nxb_im = Image(data.denorm(xb)[0])\nxb = xb.cuda()","2d078d9c":"m = learn.model.eval()","02a61726":"type(m)","834ef650":"len(m)","4e33339c":"from fastai.callbacks.hooks import *","ca3d6758":"def hooked_backward(cat=cl):\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(xb)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g\nhook_a,hook_g = hooked_backward()","7997978e":"acts  = hook_a.stored[0].cpu() #activation maps\nacts.shape","dfd3857d":"grad = hook_g.stored[0][0].cpu() #gradients\ngrad.shape","2052e429":"grad_chan = grad.mean(1).mean(1) # importance weights\ngrad_chan.shape","405da93d":"mult = F.relu(((acts*grad_chan[...,None,None])).sum(0)) # GradCAM map\nmult.shape","573c1aa5":"#Utility function to display heatmap:\ndef show_heatmap(hm):\n    _,ax = plt.subplots()\n    sz = list(xb_im.shape[-2:])\n    xb_im.show(ax,title=f\"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}\")\n    ax.imshow(hm, alpha=0.6, extent=(0,*sz[::-1],0),\n              interpolation='bilinear', cmap='magma')\n    return _,ax","af096c18":"show_heatmap(mult)","c1186945":"def GradCAM(idx:int,interp:ClassificationInterpretation, image = True):\n    m = interp.learn.model.eval()\n    im,cl = interp.learn.data.dl(DatasetType.Valid).dataset[idx]\n    cl = int(cl)\n    xb,_ = interp.data.one_item(im) #put into a minibatch of batch size = 1\n    xb_im = Image(interp.data.denorm(xb)[0])\n    xb = xb.cuda()\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(xb)\n            preds[0,int(cl)].backward() \n    acts  = hook_a.stored[0].cpu() #activation maps\n    grad = hook_g.stored[0][0].cpu()\n    grad_chan = grad.mean(1).mean(1)\n    mult = ((acts*grad_chan[...,None,None])).sum(0) #F.relu(((acts*grad_chan[...,None,None])).sum(0))\n    if image:\n        _,ax = plt.subplots()\n        sz = list(xb_im.shape[-2:])\n        xb_im.show(ax,title=f\"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}\")\n        ax.imshow(mult, alpha=0.4, extent=(0,*sz[::-1],0),\n              interpolation='bilinear', cmap='magma')\n    return mult","2613bf6d":"_ = GradCAM(np.random.randint(len(learn.data.valid_ds)),interp)","782e637a":"return_fig = interp.plot_top_losses(6,heatmap=True,return_fig = True)","186cd242":"**Training:**\n\nWe use transfer learning, where we retrain the last layers of a pretrained neural network. I use the ResNet50 architecture trained on the ImageNet dataset, which has been commonly used for pre-training applications in computer vision. Fastai makes it quite simple to create a model and train:","9f240296":"Now, whenever this function above is called, it will return `hook_a`, which stores the output of the convolutional layers, and `hook_g` which stores the gradients with respect to the predicted class (`grad=True` and `preds[0,int(cat)].backward()`). Here is how we can view these values:","9b864baf":"## GradCAM","b95f79cc":"# What is your model looking at?\n## Model Analysis - GradCAM with fastai (APTOS)\n\n![image.png](attachment:image.png)\n\nIn this kernel, we will use a technique known as Gradient-weighted Class Activation Mapping, or GradCAM, to analyze the predictions of our model, and understand what parts of the image our CNNs are looking at. GradCAM uses the activations of the network to produce localization heatmaps.\n\nI will use fastai, because GradCAM is already implemented (with a caveat), and since my previous kernels were also using fastai.","9584bd73":"**BEFORE YOU FORK, PLEASE SUPPORT AND UPVOTE**","e48407c7":"Let's evaluate our model:","41963fa9":"PyTorch and fastai have nicely separated the convolutional parts of the model from the fully connected layers. There are two elements in this \"Sequential container\". The first element are the convolutional layers, while the second element are the fully connected layers. We want the output of the convolutional layers, i.e. the output of `m[0]`. ","d679f683":"Let's put this into one large function that will take in an index for an image in the dataset, and return the GradCAM map:","dec94491":"Now we average pool the gradients, which gives us \"importance weights\" for each of the 2048 activation maps. We then multiply the feature maps by their importance weights and sum and finally pass into ReLU. This is our GradCAM map.","9d7100b1":"Now let's access the model of the `Learner` object.","effde5ac":"I hope this kernel helps you utilize GradCAM in your own models and help you train better models. If you found this kernel useful, please be sure to upvote!\n\n**Future work:**\n* More conclusions regarding GradCAM heatmaps\n* Guided backprop\n* Analyze kernels in middle layers","d5e8dbbc":"The dataset is highly imbalanced, with many samples for level 0, and very little for the rest of the levels. When we use GradCAM, let's see what the heatmaps are for the classes with lower counts. ","bdf59b73":"## Reading data and Basic EDA\n\nHere I am going to open the dataset with pandas, check distribution of labels.","81c63c59":"2048 are the number of channels, and 7 is the height and width of the feature maps. So we have 2048 feature maps with sizes 7x7.","2b0337f0":"Let's first get a single image and pu[](http:\/\/)t it into a minibatch. We put it into a minibatch as that is what our model expects.","9ca2b283":"** Theory and objective of GradCAM **\n\nThe idea behind GradCAM is:\n* The activations, which are outputted at the end of the convolutional part of the model (before being passed to global pooling and the class scores are outputted), will contain useful information regarding the importance of features in the image for classification.\n* While there may be many different features important in the model, the gradient of the predicted class with respect to these activations will contain information about what features lead to the final prediction of the class.\n\nWe multiply the activation map with the gradient map to get the final GradCAM localization map.","3ba7c245":"Now we will have to access the output and gradients of `m[0]`. To do so, we use [hooks](https:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/nnft_tutorial.html#forward-and-backward-function-hooks) in PyTorch. \n* These hooks are somewhat abstracted by the fastai library. Instead, the fastai library has a function `hook_output` that will create a hook and store the activations when a backpropagation of the model is done.","a57fd1a9":"What are potential uses for GradCAM? GradCAM is meant to provide some form of interpretability to these neural network models. In doing so, we can also better evaluate failure modes and identify biases. Which such information, we can develop better models.\n\nAlong those lines, `fastai` already has GradCAM implemented, but it is only implemented when displaying images that the model was most wrong about. This will help us better understand where our model is going wrong. To do this we use the `interp.plot_top_losses()` function. The implementation is identical to the my code above (the above code was written based on the fastai source code and fastai course).","186f5dc6":"## Training (Transfer learning)"}}