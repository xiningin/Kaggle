{"cell_type":{"baf943f3":"code","9078e582":"code","9c70a345":"code","65d96a4e":"code","dda8ee73":"code","c85f4a2e":"code","e77b59af":"code","92bf4605":"code","96fa77b9":"code","b09ea528":"code","a4ec5952":"code","1e5fae4b":"code","48313dcf":"code","78284705":"code","3b791a81":"code","3998a0ee":"code","bc389287":"code","5ee8127e":"code","fc402de3":"code","9ac2db95":"code","197734ab":"markdown","79b8a76e":"markdown","28178be1":"markdown"},"source":{"baf943f3":"pip install -q faker","9078e582":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport re, random\n\nfrom faker import Faker\nfrom babel.dates import format_date\n\npd.options.display.max_colwidth = None\nsns.set_style('darkgrid')","9c70a345":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers, losses, callbacks, utils, models, Input","65d96a4e":"class config():  \n    SAMPLE_SIZE = 10_00_000\n    \n    X_LEN = 30\n    LATENT_DIM = 8\n    Y_LEN = 13\n    \n    NUM_ENCODER_TOKENS = 35\n    NUM_DECODER_TOKENS = 13\n    \n    VALIDATION_SIZE = 0.1\n    BATCH_SIZE = 32\n    MAX_EPOCHS = 25\n        \n    DATE_FORMATS = [\n        'short', 'medium', 'long', 'full',\n        'd MMM YYY', 'd MMMM YYY', 'dd\/MM\/YYY',\n        'EE d, MMM YYY', 'EEEE d, MMMM YYY', 'd of MMMM YYY',\n    ]","dda8ee73":"faker = Faker()\nprint('Sample dates for each format\\n')\n\nfor fmt in set(config.DATE_FORMATS):\n    print(f'{fmt:20} =>  {format_date(faker.date_object(), format=fmt, locale=\"en\")}')","c85f4a2e":"def clean_date(raw_date):\n    return raw_date.lower().replace(',', '')\n\n\ndef create_dataset(num_rows):\n    dataset = []\n    \n    for i in tqdm(range(num_rows)):\n        dt = faker.date_object()\n        for fmt in config.DATE_FORMATS:\n            try:\n                date = format_date(dt, format=fmt, locale='en')\n                human_readable = clean_date(date)\n                machine_readable = f\"@{dt.isoformat()}\"\n            except AttributeError as e:\n                date = None\n                human_readable = None\n                machine_readable = None\n            if human_readable is not None and machine_readable is not None:\n                dataset.append((human_readable, machine_readable))\n \n    return pd.DataFrame(dataset, columns=['human_readable', 'machine_readable'])","e77b59af":"dataset = create_dataset(config.SAMPLE_SIZE)\ndataset  = dataset.drop_duplicates(subset=['human_readable']).sample(frac=1.0).reset_index(drop=True)\nprint(dataset.shape)\ndataset.head()","92bf4605":"human_tokenizer = Tokenizer(char_level=True)\nmachine_tokenizer = Tokenizer(char_level=True)\n\nhuman_tokenizer.fit_on_texts(dataset['human_readable'].values)\nmachine_tokenizer.fit_on_texts(dataset['machine_readable'].values)\n\nprint(human_tokenizer.word_index)\nprint(machine_tokenizer.word_index)","96fa77b9":"def preprocess_input(date, tokenizer, max_len):\n    seq = [i[0] for i in tokenizer.texts_to_sequences(date.lower().replace(',', ''))]\n    seq = pad_sequences([seq], padding='post', maxlen=max_len)[0]\n    return seq","b09ea528":"%%time\n\nX = np.array(list(map(lambda x: preprocess_input(x, human_tokenizer, config.X_LEN), dataset['human_readable'])))\ny = np.array(list(map(lambda x: preprocess_input(x, machine_tokenizer, config.Y_LEN), dataset['machine_readable'])))\n\nX.shape, y.shape","a4ec5952":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=config.VALIDATION_SIZE, random_state=19)","1e5fae4b":"def generate_batch(X, y, batch_size=config.BATCH_SIZE):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = X[j:j+batch_size]\n            decoder_input_data = y[j:j+batch_size]\n            output = y[j:j+batch_size]\n            decoder_output_data = np.zeros_like(output)\n            decoder_output_data[:,:-1] = output[:, 1:]\n            decoder_target_data = utils.to_categorical(decoder_output_data, num_classes=config.NUM_DECODER_TOKENS)\n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","48313dcf":"# Encoder\nencoder_inputs = Input(shape=(None,))\nenc_emb =  layers.Embedding(config.NUM_ENCODER_TOKENS, config.LATENT_DIM)(encoder_inputs)\nencoder_lstm = layers.LSTM(config.LATENT_DIM, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = layers.Embedding(config.NUM_DECODER_TOKENS, config.LATENT_DIM)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = layers.LSTM(config.LATENT_DIM, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\ndecoder_dense = layers.Dense(config.NUM_DECODER_TOKENS, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","78284705":"model.summary()\nutils.plot_model(model, show_shapes=True, expand_nested=True)","3b791a81":"es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True)\nrlp = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1)\n\n\nhistory = model.fit_generator(\n    generator=generate_batch(X_train, y_train), steps_per_epoch = np.ceil(len(X_train)\/config.BATCH_SIZE),\n    validation_data=generate_batch(X_valid, y_valid), validation_steps=np.ceil(len(X_valid)\/config.BATCH_SIZE),\n    epochs=config.MAX_EPOCHS, callbacks=[es, rlp], \n)","3998a0ee":"fig, ax = plt.subplots(figsize=(20, 6))\npd.DataFrame(history.history).plot(ax=ax)\ndel history","bc389287":"# Encode the input sequence to get the \"thought vectors\"\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(config.LATENT_DIM,))\ndecoder_state_input_c = Input(shape=(config.LATENT_DIM,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)","5ee8127e":"def word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\ndef predict_sequence(source):\n    states_value = encoder_model.predict(source)\n    target_seq = np.ones((1,1))*machine_tokenizer.word_index[\"@\"]\n    prediction = []\n    for i in range(13):\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n        token_index = np.argmax(output_tokens[0, -1, :])\n#         print(token_index)\n        prediction.append(token_index)\n        target_seq[0, 0] = token_index\n        states_value = [h, c]\n        \n    return decode_sequence(machine_tokenizer, prediction)\n\ndef decode_sequence(tokenizer, source):\n    target = list()\n    for i in source:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ''.join(target)","fc402de3":"query_idx = 19\n\nprint('SOURCE     :', decode_sequence(human_tokenizer, np.squeeze(X[query_idx:query_idx+1])))\nprint('TARGET     :', decode_sequence(machine_tokenizer, np.squeeze(y[query_idx:query_idx+1])))\nprint('PREDICTION :', predict_sequence(X[query_idx:query_idx+1]))","9ac2db95":"query_text = 'saturday 19 september 1998'\nquery = np.array(list(map(lambda x: preprocess_input(x, human_tokenizer, config.X_LEN), [query_text])))\n\nprint('SOURCE     :', query_text)\nprint('PREDICTION :', predict_sequence(query))","197734ab":"# Evaluation","79b8a76e":"# Data Generation","28178be1":"# Modelling"}}