{"cell_type":{"47271972":"code","db395aaa":"code","08744e8c":"code","0b72482f":"code","d5c41d11":"code","0f51c39f":"code","86f3c01b":"code","2e510371":"code","35b2acdb":"code","c1a67e30":"code","971a692e":"code","1f7ea9d4":"code","fd521a6a":"code","e49ec5a0":"code","70e0928b":"code","e24298af":"code","f77289aa":"code","d76b9788":"code","7001702f":"code","1b403ff5":"code","3cf38e3f":"code","77b9e898":"code","4345368b":"code","e5a8e608":"code","4b759e07":"code","5074ad48":"code","3b472420":"code","eaa66bad":"code","20bcd0a3":"code","067dda7c":"code","208a4ac6":"code","8b9393f0":"code","a5d28e8a":"code","4b0ade84":"code","b57b7146":"code","f19008a7":"code","a59be569":"code","aedff36f":"code","886b0b3a":"code","798fef14":"code","43eeb62f":"code","e78bd29c":"code","c404cc08":"code","b82fd16a":"code","c55422f6":"code","b4ef8ad6":"code","24b075a9":"code","faacc612":"code","686ba3bf":"code","208a1785":"code","36961fbc":"code","13b777d6":"code","d4a03e6d":"markdown","7da357d4":"markdown","e3a3a26b":"markdown","3c7a6c63":"markdown","bdfce856":"markdown","04d30782":"markdown","31af402a":"markdown","5a1a603b":"markdown","e4c6cbb0":"markdown","b6e982a5":"markdown","1797cdef":"markdown","10416c5a":"markdown","93f714ed":"markdown","d6ac813e":"markdown","b9ed272e":"markdown","5e42c3f9":"markdown","7c2748e8":"markdown","effbef13":"markdown","28fe2213":"markdown","2f1ce0ae":"markdown","94108be0":"markdown","9825ae99":"markdown","8fc79e09":"markdown","bf409ca0":"markdown","4d34ab7a":"markdown","5fe2cc15":"markdown","0b14fde9":"markdown","a019c308":"markdown"},"source":{"47271972":"import seaborn as sns\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\npd.set_option('display.max_rows', 80)\npd.set_option('display.max_columns', 50)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom tqdm import tqdm_notebook\nfrom catboost import CatBoostRegressor\n\nfrom itertools import product\nimport sklearn\nimport scipy.sparse \nimport lightgbm \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","db395aaa":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_cats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv', index_col=['ID'])\ntest['date_block_num'] = 34\nall_data = pd.concat([sales, test], axis=0)","08744e8c":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales.item_price.min(), sales.item_price.max()*1.1)\nsns.boxplot(x=sales.item_price);","0b72482f":"sales = sales[sales.item_cnt_day < 2000]\nsales = sales[sales.item_price < 300000]","d5c41d11":"sales[sales.item_cnt_day < 0]","0f51c39f":"sales[sales.item_price < 0]","86f3c01b":"price_correction = all_data[(all_data['shop_id'] == 32) & (all_data['item_id'] == 2973) & (all_data['date_block_num'] == 4) & (all_data['item_price'] > 0)].item_price.median()\nall_data.loc[all_data['item_price'] < 0, 'item_price'] = price_correction","2e510371":"shops","35b2acdb":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nall_data.loc[all_data.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nall_data.loc[all_data.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nall_data.loc[all_data.shop_id == 10, 'shop_id'] = 11","c1a67e30":"def make_lag(sales, items):\n    # Create \"grid\" with columns\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n    # For every month we create a grid from all shops\/items combinations from that month\n    grid = [] \n    for block_num in sales['date_block_num'].unique():\n        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n    # Groupby data to get shop-item-month aggregates\n    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n    # Fix column names\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n    # Join it to the grid\n    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n    # Same as above but with shop-month aggregates\n    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n    # Same as above but with item-month aggregates\n    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n    gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n    # Downcast dtypes from 64 to 32 bit to save memory\n    all_data = downcast_dtypes(all_data)\n    del grid, gb \n    gc.collect();\n    \n    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n    # List of columns that we will use to create lags\n    cols_to_rename = list(all_data.columns.difference(index_cols)) \n\n    shift_range = [1, 2, 3, 4, 5, 12]\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n\n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n\n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n        del train_shift\n\n    # Don't use old data from year 2013\n    all_data = all_data[all_data['date_block_num'] >= 12] \n\n    # List of all lagged features\n    fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n    # We will drop these at fitting stage\n    to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n    # Category for each item\n    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\n    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n    all_data = downcast_dtypes(all_data)\n    gc.collect();\n    return all_data\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","971a692e":"X = make_lag(all_data, items)","1f7ea9d4":"X.tail()","fd521a6a":"X['neversold'] = X.groupby(['item_id', 'shop_id'])['target'].transform('size').map(lambda x: 1 if x<=1 else 0)","e49ec5a0":"shops['city'] = shops.shop_name.map(lambda x: x.split()[0])\nshops['city'] = shops['city'].replace('!\u042f\u043a\u0443\u0442\u0441\u043a', '\u042f\u043a\u0443\u0442\u0441\u043a')\nshops['Moscow'] = shops.city.map(lambda x: 1 if x == '\u041c\u043e\u0441\u043a\u0432\u0430' else 0)\nshops['MO'] = shops.city.map(lambda x: 1 if x in ['\u0427\u0435\u0445\u043e\u0432', '\u0425\u0438\u043c\u043a\u0438', '\u0421\u0435\u0440\u0433\u0438\u0435\u0432', '\u041c\u044b\u0442\u0438\u0449\u0438', '\u041a\u043e\u043b\u043e\u043c\u043d\u0430', '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439', '\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430'] else 0)\nshops['distshop'] = shops.city.map(lambda x: 1 if x in ['\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f', '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d'] else 0)\nshops['city'] = shops.city.map(lambda x: 'unknown' if x in ['\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f', '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d'] else x)\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops.drop(['shop_name', 'city'], axis=1)","70e0928b":"item_cats['subcat1'] = item_cats['item_category_name'].map(lambda x: x.split(' - ')[0])\nitem_cats['subcat1'] = LabelEncoder().fit_transform(item_cats['subcat1'])\nitem_cats['subcat2'] = item_cats['item_category_name'].map(lambda x: x.split(' - ')[-1])\nitem_cats['subcat2'] = LabelEncoder().fit_transform(item_cats['subcat2'])\nitem_cats = item_cats.drop('item_category_name', axis = 1)","e24298af":"X = pd.merge(X, item_cats, on=['item_category_id'], how='left')\nX = pd.merge(X, shops, on=['shop_id'], how='left')","f77289aa":"cumsum1 = X.groupby('item_id')['target'].cumsum() - X['target']\ncumcnt1 = X.groupby('item_id')['target'].cumcount()\nencoded_feature = cumsum1\/cumcnt1\nencoded_feature.fillna(0.3343, inplace=True) \nX['mean_enc_item'] = encoded_feature\n\ndel cumsum1, cumcnt1\ngc;","d76b9788":"X['month'] = X['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nX['days'] = X['month'].map(days).astype(np.int8)\n","7001702f":"item_price = all_data[all_data.date_block_num < 34].groupby('item_id')['item_price'].median()\nX = pd.merge(X, item_price, how='left', on='item_id')\nX['item_price'] = X.groupby(\"item_category_id\")['item_price'].transform(lambda x: x.fillna(x.mean()))","1b403ff5":"X[X.item_price.isnull()].item_category_id.value_counts()","3cf38e3f":"item_cats[item_cats.item_category_id == 0]","77b9e898":"nau_id = items[items.item_name.map(str.lower).str.contains('\u043d\u0430\u0443\u0448\u043d\u0438\u043a\u0438')]['item_id']\n\nX = X.fillna(X[(X.item_price.notnull())&(X.item_id.isin(nau_id))]['item_price'].median())","4345368b":"X['low_low_price'] = (X['item_price'] < 100).astype('int8')\nX['low_price'] = ((X['item_price'] >= 100)& (X['item_price'] < 300)).astype('int8')\nX['medium_price'] = ((X['item_price'] >= 300)& (X['item_price'] < 500)).astype('int8')\nX['high_price'] = ((X['item_price'] >= 500)& (X['item_price'] < 850)).astype('int8')\nX['very_high_price'] = (X['item_price'] >= 850).astype('int8')","e5a8e608":"X.head()","4b759e07":"def train_test_modern(all_data):\n    y_train = all_data[(all_data.date_block_num >= 12)&(all_data.date_block_num < 33)]\\\n        .set_index(['shop_id', 'item_id', 'date_block_num'])['target'].clip(0,20)\n    y_valid = all_data[all_data.date_block_num == 33]\\\n        .set_index(['shop_id', 'item_id', 'date_block_num'])['target'].clip(0,20)\n    all_data = all_data.drop(['target','target_shop','target_item'], axis=1)\n    X_train = all_data[(all_data.date_block_num >= 12)&(all_data.date_block_num < 33)]\\\n        .set_index(['shop_id', 'item_id', 'date_block_num']).copy()\n    X_valid = all_data[all_data.date_block_num == 33]\\\n        .set_index(['shop_id', 'item_id', 'date_block_num']).copy()\n    X_test = all_data[all_data.date_block_num == 34]\\\n        .set_index(['shop_id', 'item_id', 'date_block_num']).copy()\n    return X_train, X_valid, y_train, y_valid, X_test","5074ad48":"def lgb_train(X_train, X_test, y_train, y_test, categorical_features, params):\n    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n    lgvalid = lightgbm.Dataset(X_test, y_test,categorical_feature=categorical_features)\n    model = lightgbm.train(params, lgtrain, 4000, valid_sets=[lgvalid], early_stopping_rounds=400, verbose_eval=200)\n    pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n    return model, pred_test","3b472420":"categorical_features = ['item_category_id', 'month', 'subcat1', 'subcat2', 'city_code']","eaa66bad":"X_train, X_valid, y_train, y_valid, X_test = train_test_modern(X)","20bcd0a3":"params = {\n    \"objective\" : \"regression\",\n    'max_depth' : 8,\n    \"metric\" : \"rmse\", \n    \"num_threads\" : -1,\n    \"learning_rate\" : 0.05,\n    'bagging_fraction': 0.8,\n    \"verbosity\" : -1\n    }","067dda7c":"model, pred_valid_lgb = lgb_train(X_train, X_valid, y_train, y_valid, categorical_features, params)","208a4ac6":"np.sqrt(mean_squared_error(y_valid, pred_valid_lgb))","8b9393f0":"feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance':model.feature_importance()}).sort_values('importance', ascending=False)[:100]\n\nplt.figure(figsize=(10,12))\nsns.barplot(x=feature_importance.importance, y=feature_importance.feature);","a5d28e8a":"pred_test_lgb = model.predict(X_test, num_iteration=model.best_iteration )","4b0ade84":"def cat_train(X_train, X_test, y_train, y_test, categorical_features, params):\n    model = CatBoostRegressor(**(params))\n    model.fit(X_train, \n        y_train.astype(int), \n        eval_set=(X_test, y_test.astype(int)), \n        cat_features=categorical_features, \n        use_best_model=True, \n        verbose=200)\n\n    pred = model.predict(X_test)\n    return model, pred","b57b7146":"params_cat =  {'iterations': 1000,\n    'random_seed': 63,\n    'learning_rate': 0.05,\n    'eval_metric': 'RMSE',\n    'bagging_temperature': 0.2,\n    'early_stopping_rounds': 200,\n    'leaf_estimation_method': 'Newton'}","f19008a7":"model_cat, pred_valid_cat = cat_train(X_train, X_valid, y_train, y_valid, categorical_features, params_cat)","a59be569":"np.sqrt(mean_squared_error(y_valid, pred_valid_cat))","aedff36f":"pred_test_cat = model_cat.predict(X_test)","886b0b3a":"columns_to_scale = ['target_lag_1', 'target_item_lag_1', 'target_shop_lag_1',\n       'target_lag_2', 'target_item_lag_2', 'target_shop_lag_2',\n       'target_lag_3', 'target_item_lag_3', 'target_shop_lag_3',\n       'target_lag_4', 'target_item_lag_4', 'target_shop_lag_4',\n       'target_lag_5', 'target_item_lag_5', 'target_shop_lag_5',\n       'target_lag_12', 'target_item_lag_12', 'target_shop_lag_12', 'mean_enc_item', 'item_price']","798fef14":"scaler = StandardScaler()\nX_train_lr = scaler.fit_transform(X_train[columns_to_scale])\nX_valid_lr = scaler.transform(X_valid[columns_to_scale])\nX_test_lr = scaler.transform(X_test[columns_to_scale])","43eeb62f":"lr = LinearRegression()\nlr.fit(X_train_lr, y_train)\npred_valid_lr = lr.predict(X_valid_lr)","e78bd29c":"np.sqrt(mean_squared_error(y_valid, pred_valid_lr))","c404cc08":"pred_test_lr = lr.predict(X_test_lr)","b82fd16a":"knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\nknn_model.fit(X_train_lr[-30000:], y_train[-30000:])\npred_valid_knn = knn_model.predict(X_valid_lr)","c55422f6":"np.sqrt(mean_squared_error(y_valid, pred_valid_knn))","b4ef8ad6":"pred_test_knn = knn_model.predict(X_test_lr)","24b075a9":"train_new = pd.DataFrame(index=X_valid.index)\ntrain_new['lgb'] = pred_valid_lgb\ntrain_new['catboost'] = pred_valid_cat\ntrain_new['lr'] = pred_valid_lr\ntrain_new['knn'] = pred_valid_knn","faacc612":"lr_meta = LinearRegression()\nlr_meta.fit(train_new, y_valid)","686ba3bf":"test_new = pd.DataFrame(index=X_test.index)\ntest_new['lgb'] = pred_test_lgb\ntest_new['catboost'] = pred_test_cat\ntest_new['lr'] = pred_test_lr\ntest_new['knn'] = pred_test_knn","208a1785":"result = lr_meta.predict(test_new)","36961fbc":"def submis_write(res, filename='sub.csv'):\n    submis = pd.DataFrame({'ID': test.index, 'item_cnt_month': res})\n    submis.to_csv(filename, index=False)","13b777d6":"submis_write(result)","d4a03e6d":"Then we will create features on category information: subcategory and subcategory 2. There are many common words in this categories.","7da357d4":"Now we should create table from the predictions of all models","e3a3a26b":"We should remove outliners from train data","3c7a6c63":"Binary features for prices","bdfce856":"Add a flag to check, that this item has never been sold in this shop","04d30782":"## Model 2 - Catboost","31af402a":"# EDA","5a1a603b":"But there is one category, about which we know nothing. This is category id - 0. Let's discover it","e4c6cbb0":"## Model 1 - lightgbm","b6e982a5":"Add several simple features","1797cdef":"We don't know prices for items in test data, so lets use median for all items. For items which wasn't reperesented in train data we will use mean price for category","10416c5a":"Create submission","93f714ed":"Make time lags for shops and items","d6ac813e":"But the price less then zero isn't normal. We will change it with median.","b9ed272e":"## Model3 - KNN Regressor","5e42c3f9":"Now I want to add different features from shop names: city, feature that indicates, that this city is Moscow district, feature to check that this shop is distance shop.","7c2748e8":"Import data","effbef13":"We have many position, where amount of sold items less then zero. This is the return of the items. We ill cast such positions to zero.","28fe2213":"Add mead encoder features","2f1ce0ae":"Fit meta model","94108be0":"## Model 2 - Liner Regression","9825ae99":"## Ensembling","8fc79e09":"Make final prediction","bf409ca0":"Check shop names. There are several shops with different id's, but the same name","4d34ab7a":"# Models","5fe2cc15":"We have no common category, but we can find all items headphones and take their price","0b14fde9":"# Data prepare","a019c308":"Let's check the target for outliners"}}