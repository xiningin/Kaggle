{"cell_type":{"24e9c393":"code","6e9e2f82":"code","74062945":"code","9b47d1ef":"code","eb90af27":"code","a5bfc362":"code","c1707899":"code","40c69849":"code","6ade985d":"code","8a94cd5c":"code","61efe8f8":"code","8a79db92":"code","d8b3706b":"code","df53cea9":"code","b1313e93":"code","cf94b3d0":"code","9e22099f":"code","aaeb5f83":"code","d3fadc60":"code","789936ae":"code","7d87db50":"code","a667b49e":"code","0e3a2af5":"code","a6e6623f":"code","9b420465":"code","6293668e":"code","d4de2fed":"code","94dcf513":"code","7bd36908":"code","fd152e50":"code","c6df5040":"code","fefbcf04":"code","c162fa91":"code","b9e5ce47":"code","4945c2d7":"code","3ab065e4":"code","f0fd673f":"code","c4bfa07d":"code","c761f89a":"markdown","f829d718":"markdown","560b1bf1":"markdown","f468f303":"markdown","525bab15":"markdown","0144c8c8":"markdown","94d6145d":"markdown","67938091":"markdown","b238f06a":"markdown","69a7211d":"markdown","77ca24aa":"markdown","49fb46f7":"markdown","a8d716b8":"markdown","99448c82":"markdown","464811a1":"markdown","dbc49209":"markdown","d6aedf6c":"markdown","2eff407d":"markdown","cf481c44":"markdown","0f2dace5":"markdown","c6f047e3":"markdown","c832f923":"markdown","20b9d45b":"markdown"},"source":{"24e9c393":"# import required libraries for data analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","6e9e2f82":"# open csv file\ndf = pd.read_csv('\/kaggle\/input\/indian-liver-patient-records\/indian_liver_patient.csv')\ndf.columns = df.columns.map(str.lower)                              # column names to lowercase\ndf.head()","74062945":"# check data types, it provides also details on null values, so next checking of null values may not be required\ndf.info()","9b47d1ef":"df.describe(include='all').T","eb90af27":"# replacing missing values with mean\ndf.albumin_and_globulin_ratio.fillna(df.albumin_and_globulin_ratio.mean(), inplace=True)","a5bfc362":"# are there still any missing values?\ndf.info()","c1707899":"# correlation between variables\nsns.set()\nsns.pairplot(df, kind='reg')","40c69849":"# correlation between variables\nsns.set()\nsns.pairplot(df, hue='dataset', kind='reg')","6ade985d":"# correlation with dataset - target value\ndf.corr()['dataset']","8a94cd5c":"# full correlation table\ndf.corr().style.background_gradient(cmap='viridis')","61efe8f8":"# this is simply my selection (from highly correlated features, you could also use different from pairs)\ndf.drop(['direct_bilirubin', 'aspartate_aminotransferase', 'total_protiens', 'albumin'], axis=1, inplace=True)","8a79db92":"# outlier check\nplt.figure(figsize=(15, 20))\n\nfor i, c in enumerate(df.drop('dataset', axis=1).select_dtypes(include='number').columns):\n    plt.subplot(10,2,i*2+1)\n    sns.boxplot(df[c], color='blue')\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\n    \n    plt.subplot(10,2,i*2+2)\n    sns.boxplot(df[c].apply('log1p'), color='red')\n    plt.title('Log1p distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","d8b3706b":"plt.figure(figsize=(15, 12))\n\nfor i, c in enumerate(df.select_dtypes(include='number').columns):\n    plt.subplot(5,2,i+1)\n    sns.distplot(df[c])\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","df53cea9":"# save skewed features\nskewed_cols = ['albumin_and_globulin_ratio','total_bilirubin', 'alkaline_phosphotase', 'alamine_aminotransferase']","b1313e93":"# Apply log1p transformation on dataframe - just selected values\nfor c in skewed_cols:\n    df[c] = df[c].apply('log1p')","cf94b3d0":"# Next check & fix strongly skewed features\n# apply log1p transform\nplt.figure(figsize=(15, 12))\n\nfor i, c in enumerate(skewed_cols):\n    plt.subplot(5,2,i+1)\n    sns.distplot(df[c].apply(np.log1p))\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","9e22099f":"from sklearn.preprocessing import LabelEncoder, RobustScaler","aaeb5f83":"# gender contains string values Male, Female; these will be converted into 0, 1, as ML algorithms like just numerical values\nle = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf.gender.head()","d3fadc60":"rs = RobustScaler()\nfor c in df[['age', 'gender', 'total_bilirubin', 'alkaline_phosphotase', 'alamine_aminotransferase', 'albumin_and_globulin_ratio']].columns:\n    df[c] = rs.fit_transform(df[c].values.reshape(-1, 1))\ndf.head()","789936ae":"from sklearn.utils import resample\ndf.dataset.value_counts()","7d87db50":"# Split data on majority and minority.. minority is dataset == 2\nminority = df[df.dataset==2]\nmajority = df[df.dataset==1]\n\nprint('Minority size:', minority.shape)\nprint('Majority size:', majority.shape)","a667b49e":"# choosing upsample as even now we do not have too much data\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\nprint('Minority upsampled size:', minority_upsample.shape)","0e3a2af5":"# merge majority with upsampled minority\ndf = pd.concat([minority_upsample, majority], axis=0)","a6e6623f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('dataset', axis=1), df['dataset'], test_size=0.25, random_state=123)\n\nprint('Train values shape:', X_train.shape)\nprint('Test values shape:', X_test.shape)\nprint('Train target shape:', y_train.shape)\nprint('Test target shape:', y_test.shape)","9b420465":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score","6293668e":"# Logistic Regression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","d4de2fed":"# Support Vector Machines\nmodel = SVC()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","94dcf513":"# Random Forest\nmodel = RandomForestClassifier(n_jobs=-1,random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","7bd36908":"# Neural nets\nmodel = MLPClassifier()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","fd152e50":"# K-Neighbors\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","c6df5040":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","fefbcf04":"# Extra Trees\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","c162fa91":"from sklearn.model_selection import GridSearchCV, KFold","b9e5ce47":"# Random forest\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs1 = GridSearchCV(RandomForestClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs1.fit(X_train, y_train)\n\nprint('Best score:', gs1.best_score_)\nprint('Best score:', gs1.best_params_)\n","4945c2d7":"# XGBoost\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'learning_rate': [0.01,0.05,0.1],\n    'booster': ['gbtree', 'gblinear'],\n    'gamma': [0, 0.5, 1],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [0.5, 1, 5],\n    'base_score': [0.2, 0.5, 1]\n}\n\ngs2 = GridSearchCV(XGBClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs2.fit(X_train, y_train)\n\nprint('Best score:', gs2.best_score_)\nprint('Best score:', gs2.best_params_)","3ab065e4":"# Extra Tree\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs3 = GridSearchCV(ExtraTreesClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs3.fit(X_train, y_train)\n\nprint('Best score:', gs3.best_score_)\nprint('Best score:', gs3.best_params_)","f0fd673f":"from sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import VotingClassifier\n\n# votes = [\n#     ('rf', RandomForestClassifier(n_jobs=-1, criterion='gini', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100)),\n#     ('xgb', XGBClassifier(n_jobs=-1, base_score=0.2, booster='gbtree', gamma=0, learning_rate=0.1, n_estimators=500, reg_alpha=0, reg_lambda=1)),\n#     ('xt', ExtraTreesClassifier(n_jobs=-1, criterion='gini', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500))\n# ]\n\nvotes = [\n    ('rf', gs1.best_estimator_),\n    ('xgb', gs2.best_estimator_),\n    ('xt', gs3.best_estimator_)\n]\n\n# soft voting based on weights\nvotesClass = VotingClassifier(estimators=votes, voting='soft', n_jobs=-1)\nvotesClass_cv = cross_validate(votesClass, X_train, y_train, cv=KFold(3, random_state=123))\nvotesClass.fit(X_train, y_train)\n\nvotesClass_cv","c4bfa07d":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = votesClass\n#model.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","c761f89a":"# Classification with XGBoost and hyperparameter optimization","f829d718":"## Encode & Scale\nI will use sklearn library to encode gender and scale numerical variables","560b1bf1":"OK, f1 increased to 0.64, model is not overtrained, even roc_auc is higher so our model is able to do classification better, SVM works fine.... but it's pretty much same as logistic regression, isn't it!? Let's try famous random forest classifier.","f468f303":"### Handle missing values\nWe are going to fix missing values on albumin_and_globulin_ratio field, easiest way is often best way, so just replace it with mean or median","525bab15":"## Voting - we will use these 3 models in final, merge them\nObjective here is not to make more accurate model (but it can be), however we want to more stable model","0144c8c8":"## Model selection\nWe will use several popular models and see how they perform on our dataset.\nAt the end we choose 3 best performing models and will merge them together.","94d6145d":"Ok, Logistic Regression worked fine, nothing special, but still 0.62 f1 score on test dataset is pretty well, model also does not overfit. But we want better score, let's try now SVM that should work well on low dimension data","67938091":"Also not so good","b238f06a":"## Conclusion\nIn this kernel I've tried to present basic work of data scientist. As I am just starting, excuse possible mistakes and propose better solutions in comment section.\n\nThis is my **very first kernel on kaggle**, give me thumb up if you enjoyed at least a bit ;)","69a7211d":"Hell yeah! That's why it's so famous, got 0.84 f1 on test dataset, roc_auc reaching 0.85, all looks great just on fact model is overfitting training data... however, still generalize fine on test data.","77ca24aa":"### What can we see from describe & info ?\n- for gender we have 2 unique values - sounds reasonable\n- there seems to be extremes for several fields (compare 75% with max) - we will have to fix it\n- dataset, our target variable, has 2 values\n- albumin_and_globulin_ratio have some missing values those must be handled","49fb46f7":"## Split data for modeling\nThis is very needed in order to be able to compare performance of model on unseen data. I will choose test size to be 0.25. Dataset is also split to X (features) and y (target) variables.","a8d716b8":"**Outcome?** I've seen in many datasets there is none. Data scientist just simply show correlation and skip any actions to be done... not in this case! \nWe have strong correlation between some variables:\n- direct_bilirubin & total_bilirubin\n- aspartate_aminotransferase & alamine_aminotransferase\n- total_protiens & albumin\n- albumin_and_globulin_ratio & albumin\n\nWe will drop some of them as features should be independent","99448c82":"Incredible! Extra Trees has definitely best performance from all models we've tried:\n- 86% precission\n- 86% recall\n- 0.86 f1\n- 0.85 roc_auc","464811a1":"Not so good as random forest","dbc49209":"## Balance data\nAs our dataset is imbalanced, will use sklearn's sample to have same ratio of target variables (we could use also SMOTE from imblearn).\nThis may not be needed each time as some algorithms already have techniques for imbalanced datasets","d6aedf6c":"#### What can we get out of dist plot and box plot?\n- dataset is slightly imbalanced\n- slightly skewed features: albumin_and_globulin_ration\n- strongly skewed features: total_bilirubin, direct_bilirubin, alkaline_phosphotase, alamine_aminotransferase, aspartate_aminotransferase\n\nWe will fix these values using log1p transformation and then scale variables using RobustScaler as this one is good for data with outliers. If you think this is not good procedure, let me know!","2eff407d":"## Model evaluation & optimization\nAs our dataset is not too big, we will use GridSearchCV for parameter tuning, in case of large datasets RandomizedSearchCV could be better. Our main objective will be to improve roc_auc score and avoid overfitting","cf481c44":"### Outliers & transformation\nNow check data using boxplot and distplot to see what features are skewed, what is ratio of outliers, if log1p helps etc","0f2dace5":"**Great**, we've got f1 score 0.84, it's not better that we've had, but even not worse.\nOur model is still overfitting data, it could be much better having more data but model still generalize well on test set.\nIf you have any suggestions how to reduce overfitting, place your comment here ;)","c6f047e3":"## Let's start exploratory data analysis (EDA)\nMost datascientist will tell you that EDA is most funny part of datascience work. You are exploring different relations between data and how they interact... so do it!","c832f923":"... which one is most favorite machine learning model on Kaggle? XGBoost! ... let's try it, unfortunately it's not part of sklearn so must be installed separately (or is, but in slightly modified version)","20b9d45b":"Nice one, XGBoost is catching random forest in performance!"}}