{"cell_type":{"8f1f47c6":"code","026b7571":"code","86a40579":"code","dd043562":"code","f55a7fd4":"code","afe5c2b8":"code","e0ce3916":"code","1efda5b9":"code","4397748e":"code","96bdaeec":"code","4e9a8a17":"code","28054447":"code","741c6882":"code","faf0a805":"code","ce521fb3":"code","32a22fb9":"code","cd0c4f1e":"markdown","dae5365d":"markdown","dcebfdf9":"markdown","84819ade":"markdown","c18560b1":"markdown","42bca343":"markdown","11eedd1a":"markdown","2b92578f":"markdown","3535b0f5":"markdown","51e8b215":"markdown"},"source":{"8f1f47c6":"import numpy as np \nimport pandas as pd \nimport matplotlib as mp\nimport os\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation,TruncatedSVD\nimport matplotlib.pyplot as plt","026b7571":"#uploading data in dataframe\ntrain=pd.read_csv(\"..\/input\/train.csv\",sep=',')","86a40579":"#displaying exemple data\ntrain.head(5)","dd043562":"#displaying exemple of insincere data \ntrain[train.target==1].head(5)","f55a7fd4":"#displayin dataframe info\ntrain.info()","afe5c2b8":"#counting target values\ntrain.target.value_counts()","e0ce3916":"train['word_count'] = train['question_text'].apply(lambda x: len(str(x).split(\" \")))\n","1efda5b9":"#basic statistic about word_count\ntrain.word_count.describe()","4397748e":"#lower case\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#Removing Punctuation\ntrain['question_text'] = train['question_text'].str.replace('[^\\w\\s]','')\n#Removing numbers\ntrain['question_text'] = train['question_text'].str.replace('[0-9]','')\n#Remooving stop words and words with length <=2\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop and len(x)>2))\n#Stemming\n#from nltk.stem import SnowballStemmer\n#ss=SnowballStemmer('english')\n#train['question_text'] = train['question_text'].apply(lambda x: \" \".join(ss.stem(x) for x in x.split()))\nfrom nltk.stem import WordNetLemmatizer\nwl = WordNetLemmatizer()\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(wl.lemmatize(x,'v') for x in x.split()))","96bdaeec":"from nltk.stem import SnowballStemmer,WordNetLemmatizer,PorterStemmer,LancasterStemmer\nwl = WordNetLemmatizer()\nss=SnowballStemmer('english')\nps=PorterStemmer()\nls=LancasterStemmer()\ntest_list=['does','peaople','writing','beards','enjoyment','bought','leaves','gave','given','generaly','would']\nfor item in test_list :\n    print('lemmatizer : %s'%wl.lemmatize(item,'v'))\n    print('SS stemmer : %s'%ss.stem(item))\n    print('PS stemmer : %s'%ps.stem(item))\n    print('LS stemmer : %s'%ls.stem(item))\n","4e9a8a17":"train.head(5)","28054447":"tfidf_v = TfidfVectorizer(min_df=20,max_df=0.8,sublinear_tf=True,ngram_range={1,2})\n#matrixTFIDF= tfidf_v.fit_transform(train.question_text)\nmatrixTFIDF= tfidf_v.fit_transform(train[train.target==1].question_text)","741c6882":"print(matrixTFIDF.shape)","faf0a805":"svd=TruncatedSVD(n_components=15, n_iter=10,random_state=42)\nX=svd.fit_transform(matrixTFIDF)             ","ce521fb3":"def get_topics(components, feature_names, n=15):\n    for idx, topic in enumerate(components):\n        print(\"Topic %d:\" % (idx))\n        print([(feature_names[i], topic[i])\n                        for i in topic.argsort()[:-n - 1:-1]])","32a22fb9":"get_topics(svd.components_,tfidf_v.get_feature_names())","cd0c4f1e":"### Topic modeling using LSA","dae5365d":"We have to deal with unbalanced target Feature...","dcebfdf9":"# Topic Modelling using Latent Semantic Analysis","84819ade":"## Text transformation","c18560b1":"## Import of libraries","42bca343":"## Data upload","11eedd1a":"## Feature extraction","2b92578f":"get_topics give the n most contributif words in a topic","3535b0f5":"## Topic Modeling insincere questions\nFor topic modeling we are going to use a TFIDF matrix transformation.","51e8b215":"There is no missing values "}}