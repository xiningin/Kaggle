{"cell_type":{"7fc560af":"code","aaddcd50":"code","b3daf8e6":"code","adc0f3f9":"code","9ff8036c":"code","ee7cc01c":"code","0d5fbdc6":"code","fb3c6fd6":"code","cfe6cc83":"code","51324ecc":"code","1e38ce9a":"code","d028c903":"code","38d5cb50":"code","017a8c1c":"code","c656ae0f":"code","0621ed33":"code","c393e4f0":"code","4aafb1a3":"code","6568f726":"code","51c340d5":"code","71a1a23e":"code","ddbd4a91":"code","4b89525d":"code","5d78ee8e":"code","43c70d92":"markdown","18d7926b":"markdown","7f4ea49a":"markdown","3bcb12f3":"markdown","4211b1f7":"markdown","4a9917dd":"markdown","2d8d0a26":"markdown","8ebeb52f":"markdown","cf06eb3f":"markdown","f8b79a42":"markdown","501e8e30":"markdown","520590b9":"markdown","3c4b785b":"markdown","51388121":"markdown","3d46a1bb":"markdown","4159910d":"markdown","c3aee91f":"markdown","f74f0112":"markdown","e226d8d6":"markdown","82526f12":"markdown","8c46df4f":"markdown","d0a3ccc0":"markdown","d925ccb9":"markdown","5312b58e":"markdown","f4733744":"markdown","dabad4d9":"markdown","bf83e25e":"markdown","12789897":"markdown","03d26c5b":"markdown","7c423fd2":"markdown","9003a971":"markdown","9b2ef41a":"markdown","75c795e1":"markdown","6aa885a4":"markdown","787ce26a":"markdown","783c7c36":"markdown","a8cb82fd":"markdown","ffa1673f":"markdown","4e2259b2":"markdown","9bad3e71":"markdown","2c949e1a":"markdown","6369d874":"markdown","15c9103b":"markdown","79f47fcf":"markdown","88d46a08":"markdown","40b3f92f":"markdown","93195f64":"markdown","74a5170b":"markdown","95fa025e":"markdown"},"source":{"7fc560af":"! pip install transformers[sentencepiece] > \/dev\/null","aaddcd50":"from transformers import pipeline","b3daf8e6":"classifier_sentiment = pipeline(\"sentiment-analysis\")","adc0f3f9":"classifier_sentiment(\"I fully understand what you are saying.\")","9ff8036c":"classifier_sentiment(\"I absolutely hate pineapple in pizzas. It's an abomination.\")","ee7cc01c":"classifier_zero_shot = pipeline(\"zero-shot-classification\")","0d5fbdc6":"classifier_zero_shot(\n    \"You can grow as a Programmer if you learn about the functional paradigm\",\n    candidate_labels=[\"profession\", \"politics\", \"education\"])","fb3c6fd6":"generator = pipeline(\"text-generation\")","cfe6cc83":"generator(\"We all like to spend time with dogs\")","51324ecc":"generator(\"The stars in the night sky are beautiful\",\n          max_length=30,\n          max_return_sequences=3)","1e38ce9a":"unmasker = pipeline(\"fill-mask\")","d028c903":"unmasker(\n    \"It is so hot in here, that you can barely go <mask>.\",\n    top_k=3)","38d5cb50":"unmasker(\n    \"I am quite <mask> and so I decided not to go.\",\n    top_k=2)","017a8c1c":"ner = pipeline(\"ner\", grouped_entities=True)","c656ae0f":"ner(\"My name is Rito, and I am a Computer Vision Consultant based in Kolkata.\")","0621ed33":"question_answerer = pipeline(\"question-answering\")","c393e4f0":"question_answerer(\n    question=\"Who became a desciple of the Buddha?\",\n    context=\"Empress Khema, the wife of Emperor Bimbisara, became a desciple of the Buddha.\"\n)","4aafb1a3":"question_answerer(\n    question=\"Who was the husband of Empress Khema?\",\n    context=\"Empress Khema, the wife of Emperor Bimbisara, became a desciple of the Buddha.\"\n)","6568f726":"summarizer = pipeline(\"summarization\")","51c340d5":"summarizer(\n    \"\"\"\n    \u201cI really connected with your writing. You are such a talented writer. I wish I could write this well.\u201d\n    Ah, the praises of the internet comments can really make your day. \u201cI laughed out loud while reading this,\u201d\n    said another. Yeah me too, except I was the one writing those sentences.\n\n    I am sure my high school English teacher would have something entirely different to say about that.\n    \u201cMaybe writing is not for you,\u201d would probably be her words. Maybe not in a such direct down-putting tone,\n    since teachers are in general not supposed to discourage people from trying, but you get the idea.\n\n    Which got me thinking, why do we refer to someone as talented when we know that they weren\u2019t born with this\n    skill? You weren\u2019t born with the ability to speak, write stories or make computer programs. It\u2019s a skill\n    that you have have learned over time, mostly due to spending a lot of hours in that particular field. Sure,\n    natural capabilities matter when you are competing with the best in the world, but for most people\n    competing with the best is not a part of their daily job.\n\n    I started learning English in primary school. I had an English teacher that tried her best to teach English,\n    except her best way of teaching was often quite the opposite of what is best for learning a foreign language.\n    She followed the iron practices that were recommended by the board of education, but after years of following\n    those practices, my ability to write in English came down to jotting down a few lousy sentences and eventually\n    I ran out steam. The only thing that I can remember from those practices, was filling in thick workbooks full\n    of exercises. And papers. So many exercise papers still warm from the printing.\n    \"\"\"\n)","71a1a23e":"translator_de = pipeline(\"translation\", model=\"Helsinki-NLP\/opus-mt-en-de\")","ddbd4a91":"translator_de(\"I am Rito, and I am looking for a taxi to reach the symposium.\")","4b89525d":"translator_bn = pipeline(\"translation\", model=\"Helsinki-NLP\/opus-mt-bn-en\")","5d78ee8e":"translator_bn(\"\u0997\u09a4\u0995\u09be\u09b2 \u09b0\u09be\u09a4 \u09a5\u09c7\u0995\u09c7 \u0996\u09c1\u09ac \u09ac\u09c3\u09b7\u09cd\u099f\u09bf \u09b9\u09df\u09c7\u099b\u09c7, \u099a\u09be\u09b0\u09bf\u09a6\u09bf\u0995 \u099c\u09b2\u09c7 \u09ad\u09c7\u09b8\u09c7 \u0997\u09c7\u099b\u09c7\u0964\")","43c70d92":"I have shown you several pipeline APIs and what they do and how they work.\n\nHere are the suggested next steps-\n\n* **Experiment.** Play with the `pipeline`s by yourself, try different things with different parameters, run inference on examples of your own, test edge cases, run wild with it. I believe that is the best way to learn.\n* **Do Projects.** Use the knowledge you gained here in simple projects. We only learn by doing.\n* **Hugging Face Course.** Hugging Face has recently released a [free course](https:\/\/huggingface.co\/course\/chapter1) where they teach you about Transformers and their library ecosystem. It is a good next step.\n* **Explore.** Explore the Hugging Face [website](https:\/\/huggingface.co\/), learn more about their APIs, Services, pre-trained models, and many more things it has to offer.\n\n___","18d7926b":"> *Whoa, it's poetic!*","7f4ea49a":"# Text Generation","3bcb12f3":"You pass the sentence with the masked word, and it will predict it. You can choose the number of predictions you want to see. You just have to pass a value to the `top_k` parameter.","4211b1f7":"Note that you are not explicitly telling the model about the language of the input language or the output language. A language model is only trained for translating from one language to another. It cannot translate other languages. If you want to translate from German to English, you'd have to use another model from the model hub. You can find all the translation models [here](https:\/\/huggingface.co\/models?pipeline_tag=translation).","4a9917dd":"# A Gentle Introduction to the Hugging Face API","2d8d0a26":"![hf-api-pipeline.jpg](attachment:1c1340a1-50e6-4aa1-a9ee-aa2b3424a7f8.jpg)","8ebeb52f":"This is one of the most practical tasks that has existed for a long time. Deep Learning really changes the landscape of the task.\n\nLet's see it in action.","cf06eb3f":"Let me show you how this works. First, you instantiate an object and assign it a name just like you did in the case of the Sentiment Analysis pipeline.","f8b79a42":"# Sentiment Analysis","501e8e30":"Natural Language Processing is a fast-advancing field. And it is also one of the fields that require a huge amount of computational resources to make important progress. And although breakthroughs are openly announced, and papers are released in free-to-access repositories such as [ar$\\chi$iv](https:\/\/arxiv.org\/), Open Review, Papers with Code, etc., and despite (sometimes) having the code freely available on GitHub, using those language models is not something widely accessible and easy.\n\nLet me provide more context. BERT is a state-of-the-art encoder language model. It takes days to train the model from the ground up even when using very powerful GPUs that only a few entities have access to. In 2019, NVIDIA [used](https:\/\/nvidianews.nvidia.com\/news\/nvidia-achieves-breakthroughs-in-language-understandingto-enable-real-time-conversational-ai) $ 1472 $ NVIDIA V100 GPUs to train BERT from scratch in $ 53 $ minutes. Yes, $ 1,472 $!\n\nOne estimate [puts](https:\/\/twitter.com\/eturner303\/status\/1266264358771757057?s=20) the cost of training GPT-3, a $ 175 $ billion parameter model, for a single training run at \\$12 Million USD.\n\nAnd such language models are released every now and then. How do you use these powerful language models for your task?\n\nHere Hugging Face comes to the scene. They aim to solve this problem by providing pre-trained models, and simple API so that you can use them, fine-tune them, and use the API in your applications.\n\nIn this Notebook, my goal is to introduce the Hugging Face `pipeline` API to accomplish very interesting tasks by utilizing powerful pre-trained models present in the models hub of Hugging Face.\n\n> What this Notebook is **NOT**:\n> * An exhaustive list of all `pipeline` items or a list of all possible parameters in `pipeline` objects\n> * An in-depth dive into the inner-workings of the API\n> * An introduction to the models and the NLP behind the tasks accomplished by the `pipeline`s\n\nTo follow through this Notebook, you need not have any prior knowledge of Natural Language Processing. I, however, assume minor prior experience in writing Python code.\n___","520590b9":"See, the given sentence is about a profession, and the model tells you that. And it knows that it is much more related to education than politics.\n\nEssentially, when you are doing Zero-Shot Classification, you are supplying a string to the `pipeline`, and also labels. The pipeline returns how accurate **those labels are**.\n\n___","3c4b785b":"# Zero Shot Classification","51388121":"___","3d46a1bb":"Now, that's something! The efficacy of the summarizer really shocked me. It does this task so well.\n\n___","4159910d":"# Mask Filling","c3aee91f":"It can answer multiple questions from the same prompt.","f74f0112":"See, it returns a dictionary contained in a list that has two items, `label` and `score`. The `label` part tells us its prediction, and the `score` tells us its confidence score.\n\n\n> *As an aside, I think they are structured this way because this structure is easily compatible with `.json` and similar filetypes which are very common in APIs.*\n\n\nLet's see another example.","e226d8d6":"We can accomplish more interesting things with transfer learning. But that is a story for another day.\n\n___","82526f12":"Then you pass a string, along with the labels of your choice to test how well they correspond to your sentence.","8c46df4f":"See, how the classifier is aware of my emotion about pineapple pizzas? \ud83d\ude43","d0a3ccc0":"# Introduction","d925ccb9":"Before I heard about Deep Learning, I heard about Natural Language Processing, because I heard that you can generate text with it! This is the most exciting part of NLP to me, personally.\n\nAs the name suggests, this `pipeline` lets you generate text. It just needs you to supply a prompt, a text that will determine what the generated text will be. Let me show you how.","5312b58e":"When you want to classify something using Deep Learning, in many cases you need to train it with labeled examples. This approach is known as Supervised Learning. Even when leveraging transfer learning, you need to train your model with quite a few labeled examples in the domain of your choice. Zero-shot classification is different than that. In this, you use a pre-trained model to classify a given string and some labels of your choice. The model returns you the confidence score for each model.","f4733744":"# Named Entity Recognition","dabad4d9":"This pipeline provides you with an answer to a question you ask from the context that you provide.","bf83e25e":"That's it. You call the `pipeline()` method with the task you want to accomplish as an argument. And you assign a name to it. You are done now. You can now begin to use the object as a function to achieve what you want. Let's see an example-","12789897":"Before I begin going through the specific `pipeline`s, let me tell you something beforehand that you will find yourself. Hugging Face API is very intuitive. When you want to use a `pipeline`, you have to instantiate an object, then you pass data to that object to get result. Very simple! You are soon to see what I mean.","03d26c5b":"# Question Answering","7c423fd2":"# Translation","9003a971":"The cool thing is that Hugging Face is not limited to Romance languages or European languages in general. Let me show you by translating a piece of text from Bengali.","9b2ef41a":"___","75c795e1":"If you have not seen a `pip` install with a square bracket in it before, don't worry. It means that you are installing the `transformers` package with extra support added for the `sentencepiece` package. You can find out more about square parentheses installs in `pip` [here](https:\/\/stackoverflow.com\/questions\/46775346\/what-do-square-brackets-mean-in-pip-install).","6aa885a4":"#### *by [Ritobrata Ghosh](https:\/\/ghosh-r.github.io)*","787ce26a":"# Installing and Importing","783c7c36":"In this Notebook, I will go over, describe, and provide examples for the following tasks using Hugging Face `pipeline`-\n\n* Sentiment Analysis\n* Zero-Shot Classification\n* Text Generation\n* Mask-Filling\n* Named Entity Recognition\n* Question Answering\n* Summarization\n* Translation\n\n___","a8cb82fd":"The `grouped_entities` parameter just ensures that the `pipeline` can recognize the names that have more than one word, like, you know, Chocolate Factory.","ffa1673f":"___","4e2259b2":"If you notice any error, mistake, or typo, please feel free to let me know. If something is not clear, please let me know that as well. I will do my best to correct it and will credit you here.\n\nLet me know if you have any questions at all.\n\n___\n\nYou can find me on Twitter (@AllesistKode) and connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ritobrata-ghosh\/). I blog on my [personal website](https:\/\/ghosh-r.github.io) as well.","9bad3e71":"Let's summarize a piece of text from a [blog post](https:\/\/blog.royalsloth.eu\/posts\/i-wish-i-could-write-this-well\/) I recently read.","2c949e1a":"# Conclusion","6369d874":"This `pipeline` recognizes the proper nouns in your sentence. And it also classifies them. Let's see an example.","15c9103b":"It can predict the words quite well. Just remember to mask the word using `<mask>`.\n___","79f47fcf":"In this pipeline, if there is a word that you hide and supply the string with the hidden word, the `pipeline` will predict that word. This is like magic!","88d46a08":"We will use the utilities available in the `pipeline` namespace in the `transformers` library.\n\n___","40b3f92f":"# Summarization","93195f64":"See how it recognizes a person, his profession, and location on its own, and also labels them as \"PER\", \"ORG\", and \"LOC\" (meaning location), respectively.\n\n___","74a5170b":"See what I mean! Even with zero training, it can create logically coherent text that is even interesting to read. Robots will take over writers soon \ud83e\udd16!\n\n\nYou can control the maximum length of the output of the `pipeline`, and even ask for multiple possible outputs, like so-","95fa025e":"The translation `pipeline` translates a piece of text from one language to another. It also lets you choose the language model of your choice.\n\nFirst, let's translate a text from English to German. I will use the [`Helsinki-NLP\/opus-mt-en-de`](https:\/\/huggingface.co\/Helsinki-NLP\/opus-mt-en-de) model for this task."}}