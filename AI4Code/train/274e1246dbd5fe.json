{"cell_type":{"5cd8abf0":"code","1a80f88f":"code","fddcda9f":"code","f6c9fdf7":"code","fe15319b":"code","5d248285":"code","d375bcc2":"code","7456bbca":"code","5e979929":"code","5cee7427":"code","d9aa254a":"code","7e66654f":"code","05ea6275":"code","b36cfa4d":"code","ae25b0d8":"code","6b66e3fb":"code","a3de2d4c":"code","c92402b1":"code","51787d0e":"code","fd15f973":"code","3a0591c3":"code","b2b34d12":"code","53f48bc9":"code","837acd16":"code","27a4c5e2":"code","7151b3ba":"code","593468d6":"code","20a33d68":"code","4fecdca7":"code","54c37730":"code","9c2f0392":"code","6d9a3e0a":"code","02e503dc":"code","50807537":"code","b6dc51f2":"code","8d0bc35a":"code","23000bab":"code","2de57f55":"code","5815ed34":"code","9e929575":"code","0becd679":"code","53f930e0":"code","9e9f9c52":"code","8147bddd":"code","de0de8a7":"code","a3341d61":"code","b0853d08":"code","54cb4aae":"code","30e8fbcc":"code","94a0bbf6":"code","b71f5034":"code","1c21abbf":"code","0dce71d0":"code","c7ece8a5":"code","2bb308a2":"code","1442f768":"code","b95f3263":"code","203f193f":"code","bff8b36b":"code","98d96340":"code","e565d3ed":"code","caba8f59":"code","8cb7b92c":"code","240854b1":"code","27fce741":"code","5228fcd4":"code","264d39bd":"code","52236170":"code","35b29a91":"code","dbd58fd9":"code","ad04a2ee":"code","806a1254":"code","8b6714e8":"code","1e40f883":"code","2fe6814b":"code","60a52886":"code","d80523c7":"code","e640ef05":"code","eacdb748":"code","f47ade5f":"markdown","828e90af":"markdown","2ef78440":"markdown","a0baed22":"markdown","82c6ee58":"markdown","8aef8c27":"markdown","35b1cc92":"markdown","2062a792":"markdown","df81aebf":"markdown","67f0f07a":"markdown","4f1f0c1d":"markdown","de4a0b1e":"markdown","e7fda66c":"markdown","ff61a901":"markdown","25bbeaec":"markdown","6739fc92":"markdown","9551a2a1":"markdown","ff0ba6db":"markdown","cf253cd5":"markdown","af48e88a":"markdown","11ab0864":"markdown","ac3c6550":"markdown","c7c6535a":"markdown","c4d7a899":"markdown","0275c224":"markdown","86031b0d":"markdown","59dfbd51":"markdown","06aa8bca":"markdown","3dd353d3":"markdown","83699982":"markdown","1a8ba002":"markdown","1f9e08a7":"markdown","faf014c7":"markdown"},"source":{"5cd8abf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a80f88f":"# %load ..\/input\/ames-iowa-house-prices-handmade-data\/Ames Iowa Housing Kaggle Edition\/standard_pipeline.py\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nscope_names = dir()\n\ndef make_name_from_estimator(obj_estimator):\n    \n    name = str(obj_estimator).split('(')\n    name = name[0].strip('(')\n    return name\n\ndef return_repeatedstratifiedkfold(splits = 5, repeats = 5, rand_state = 88):        \n    from sklearn.model_selection import RepeatedStratifiedKFold\n    \n    cv_splitter = RepeatedStratifiedKFold(\n        n_splits = splits,\n        n_repeats = repeats,\n        random_state = rand_state\n        )\n    \n    return cv_splitter\n\ndef run_rfecv(df, lst_X, y, estimator, scoring_method, cv_splitter = None):\n    \n    if \"RFECV\" not in scope_names:\n        from sklearn.feature_selection import RFECV    \n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    rfecv = RFECV(\n        estimator = estimator,\n        cv = cv_splitter,\n        scoring = scoring_method,\n        n_jobs = -1\n        )\n    \n    rfecv.fit(X = df[lst_X], y = df[y])\n    \n    results = {\n        \"features\": df[lst_X].columns[rfecv.support_],\n        \"split_scores\": rfecv.grid_scores_,\n        \"fit_estimator\": rfecv.estimator_,\n        \"rankings\": rfecv.ranking_\n        }\n    \n    return results\n\ndef run_kbest(feature_data, target_values, n_passers = \"all\", score_func = None):\n#     if (score_func == None) and (\"mutual_info_regression\" not in scope_names):\n#         from skelarn.feature_selection import mutual_info_regression\n#         score_func = mutual_info_regression\n    \n#     if \"SelectKBest\" not in scope_names:\n#         from sklearn.feature_selection import SelectKBest\n\n    from sklearn.feature_selection import SelectKBest\n    \n    kbest = SelectKBest(score_func = score_func, k = n_passers)\n    kbest.fit(X = feature_data, y = target_values)\n        \n    results = {\n        \"kbest_scores\": kbest.scores_,\n        \"kbest_pvalues\": kbest.pvalues_,\n        \"kbest_params\": kbest.get_params(),\n        #\"passing_features\": kbest.get_support()\n        }\n    \n    return results\n\ndef manyset_manymethod_kbest(\n    dict_datasets,\n    dict_targets,\n    lst_score_funcs,\n    y = \"is_benign\",    \n    n_passers = \"all\"    \n    ):\n    \n    result_dicts = {}\n    \n    for alias, dataset in dict_datasets.items():\n        set_results = {}\n        \n        for method in lst_score_funcs:\n            try:\n                rslt = run_kbest(\n                    feature_data = dataset,\n                    target_values = dict_targets[alias],                    \n                    n_passers = n_passers,\n                    score_func = method\n                )\n                \n                set_results[ str(method).split(' ')[1] ] = rslt\n            except Exception as e:\n                set_results[ str(method).split(' ')[1] ] = {\"Error\":str(e)}\n        \n        result_dicts[alias] = set_results\n    \n    return result_dicts\n\ndef run_randsearch(estimator, X_data, y_data, dict_params,\n                   scoring_method = None,\n                   n_combinations = 128,\n                   cv_splitter = None,\n                   get_trainset_scores = False\n                   ):\n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    randomized_search = RandomizedSearchCV(\n        estimator = estimator,\n        param_distributions = dict_params,\n        n_iter = n_combinations,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )        \n    \n    randomized_search.fit(X = X_data, y = y_data)\n    dict_cv_results = randomized_search.cv_results_\n    \n    results = {\n        \"cv_results\": dict_cv_results,\n        \"cv_results_best\": dict_cv_results['params'][randomized_search.best_index_],\n        \"best_params\": randomized_search.best_params_,\n        \"best_score\": randomized_search.best_score_,\n        \"best_estimator\": randomized_search.best_estimator_,\n        \"refit_time\": randomized_search.refit_time_\n        }\n    \n    return results\n\ndef manymodel_manyfeatureset_randsearch(\n        dict_estimators_params,\n        dict_feature_sets,\n        dict_target_features,\n        scoring_method = None,\n        n_combinations = 128,\n        cv_splitter = None,\n        get_trainset_scores = False        \n        ):\n    \n    model_results = {}\n    \n    for estimator, param_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(estimator)        \n        featureset_results = {}\n        \n        for alias, feature_data in dict_feature_sets.items():\n            featureset_results[alias] = run_randsearch(\n                estimator = estimator,\n                X_data = feature_data,\n                y_data = dict_target_features[alias],\n                dict_params = param_grid)\n        \n        model_results[name] = featureset_results\n    \n    return model_results\n\ndef run_gridsearch(\n        estimator, X_data, y_data, dict_params,\n        scoring_method = None,\n        cv_splitter = None,\n        get_trainset_scores = False\n        ):\n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    grid_search = GridSearchCV(\n        estimator = estimator,\n        param_grid = dict_params,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )\n    \n    grid_search.fit(X = X_data, y = y_data)\n    cv_results = grid_search.cv_results_\n    \n    results = {\n        \"best_params\": grid_search.best_params_,\n        \"best_score\": grid_search.best_score_,\n        \"best_estimator\": grid_search.best_estimator_,\n        \"cv_results\": cv_results,\n        \"best_of_cv_results\": cv_results['params'][grid_search.best_index_]\n        }    \n    \n    return results\n\ndef manymodel_manyfeatureset_hparam_gridsearch(\n    dict_estimators_params,\n    dict_X_combinations,\n    dict_y_data,\n    scoring_method,\n    cv_splitter = return_repeatedstratifiedkfold(),\n    get_trainset_scores = False):\n    \n    dict_manymodel_gridsearch = {}\n    \n    for model, hparam_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(model)\n        rslt = {}\n        \n        for alias, feature_combination in dict_X_combinations.items():\n            rslt[alias] = run_gridsearch(\n                estimator = model,\n                dict_params = hparam_grid,                \n                y_data = dict_y_data[alias],\n                X_data = feature_combination,\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter,\n                get_trainset_scores = get_trainset_scores\n            )            \n            \n        dict_manymodel_gridsearch[name] = rslt\n        \n    return dict_manymodel_gridsearch\n    \ndef run_crossvalscore(\n        estimator, train_data, target_feature,\n        scoring_method = None,\n        cv_splitter = None\n        ):\n    \"\"\"    \n\n    Parameters\n    ----------\n    estimator : MODEL OR ESTIMATOR\n        DESCRIPTION.\n    train_data : TYPE\n        DESCRIPTION.\n    target_feature : TYPE\n        DESCRIPTION.\n    scoring_method : TYPE, optional\n        DESCRIPTION. The default is None.\n    cv_splitter : TYPE, optional\n        DESCRIPTION. The default is None.\n\n    Returns\n    -------\n    results : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    \n    if (cv_splitter == None) and (\"RepeatedStratifiedKFold\" not in scope_names):\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    cvscores = cross_val_score(\n        estimator = estimator,\n        X = train_data,\n        y = target_feature,\n        scoring = scoring_method,\n        cv = cv_splitter,\n        n_jobs = -1\n        )    \n    \n    mean_of_scores_withnan = np.mean(cvscores)\n    nanmean_of_scores = np.nanmean(cvscores)\n    standard_deviation = np.std(cvscores)\n    variance = np.var(cvscores)    \n    \n    results = {\n        \"mean_score\": mean_of_scores_withnan,\n        \"nanmean_score\": nanmean_of_scores,\n        \"std\": standard_deviation,\n        \"var\": variance\n        }    \n    \n    return results\n\ndef manymodel_manyfeatureset_cvs(\n        lst_estimators,\n        dict_featuresets,        \n        scoring_method,\n        cv_splitter = return_repeatedstratifiedkfold(),\n        ):\n    \"\"\"\n    Parameters\n    ----------\n    lst_stimators: List-like of Estimator Objects\/Models\n        A list with properly instantiated estimators or algorithm models, ex:\n            MLPRegressor(hidden_layer_sizes = (64, 64)).\n    \n    dict_featuresets: Dictionary\n        A dictionary. The keys are arbitrary names\/aliases used to identify the\n        different sets of training data. The values are the corresponding list with\n        feature data to be passed \"as-is\" to an estimator placed on index 0. The\n        target data will be placed on index 1. THe input may be dataframes,\n        series, or NumPy n-dimensional arrays. Format:\n            { \"dataset_name\": [features, target_values] }\n    \n    scoring_method: String\n        A string giving the scoring metric or criteria, such as root mean squared error\n        or accuracy score.\n    \n    cv_splitter: CV Splitter Object, default RepeatedStratifiedKFold(n_repeats = 5, n_splits = 5)\n        An instance of an SKLearn cross-validation splitter.\n    \n    Returns\n    -------\n    dict_results: Dictionary\n        A dictionary whose keys are the estimator names with parenthesis and parameters removed.\n        The values are also dictionaries.\n        The \"second layer\" dictionaries corresponding to the estimator names\n        have the feature set aliases as keys and\n        dictionaries with summary statistics about the cross-validation scores.\n        Sample Structure:\n        {\n            \"LinearRegression\":\n                {\n                    \"df_raw\":\n                        {\"mean_score\":NaN, \"nanmean_score\":0.53, \"std\":3, \"var\":9},\n                    \"df_processed\":\n                        {\"mean_score\": 0.75, \"nanmean_score\":0.75, \"std\":0.5, \"var\":0.70710}\n             },\n            \"MLPRegression\":\n                {\n                    \"df_raw\":\n                        { \"mean_score\":NaN, \"nanmean_score\":0.83, \"std\":3, \"var\":9 },\n                    \"df_processed\":\n                        {\"mean_score\": 0.85, \"nanmean_score\":0.95, \"std\":0.1, \"var\":0.31622}\n                }    \n        }\n\n    \"\"\"\n    \n    dict_results = dict()\n    \n    for estimator in lst_estimators:\n        name = make_name_from_estimator(estimator)        \n        featureset_rslt = dict()\n        \n        for alias, dataset in dict_featuresets.items():\n            featureset_rslt[alias] = run_crossvalscore(\n                estimator = estimator,\n                train_data = dataset[0],\n                target_feature = dataset[1],\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter\n                )\n        \n        dict_results[name] = featureset_rslt\n    \n    return dict_results\n\n\ndef make_traintest(df, train_fraction = 0.7, random_state_val = 88):\n    df = df.copy()\n    df_train = df.sample(frac = train_fraction, random_state = random_state_val)    \n    bmask_istrain = df.index.isin(df_train.index.values)\n    df_test = df.loc[ ~bmask_istrain ]\n    \n    #return (df_train, df_test)\n    \n    return {\n        \"train\":df_train,\n        \"test\":df_test\n        }\n\ndef train_models(\n        dict_model_and_dataset\n        ):\n    \"\"\"\n\n    Parameters\n    ----------\n    dict_model_and_dataset : TYPE\n        DESCRIPTION: A dictionary with an arbitrary alias\/name for the model and data set\n        as its keys and the corresponding estimator's data set as a list. The list\n        must have the estimator object at index 0,\n        features' data at index 1 and the target variables' data at index 2.\n        Ex:\n            {\"MLPRegressor_baseline\": [regressor_object,\n                                       train[list_of_features],\n                                       train[\"target_feature\"]\n                                       ]\n             }\n\n    Returns\n    -------\n    fit_models : DICTIONARY\n        DESCRIPTION: A dictionary with the estimator objects after their `.fit()`\n        method has been called with the proper corresponding data as `fit` method\n        parameters.\n        In case of failure to train an estimator, the estimator object is set to\n        `None`.\n        The aliases supplied to this function instance are used as keys.\n\n    \"\"\"\n    \n    fit_models = {}\n    \n    for alias, model_and_data in dict_model_and_dataset.items():\n        name = alias\n        estimator = model_and_data[0]\n        try:\n            estimator.fit(X = model_and_data[1], y = model_and_data[2])\n        except:\n            estimator = None\n        finally:\n            fit_models[name] = estimator\n        \n    return fit_models","fddcda9f":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold","f6c9fdf7":"#!pip install pip --upgrade\n!python -m pip install --upgrade pip","fe15319b":"!pip install -U tensorflow\nimport tensorflow as tf\ntf.__version__","5d248285":"import pandas as pd, numpy as np, datetime as dt\nimport seaborn as sbrn, matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n\npath_to_train = \"https:\/\/raw.githubusercontent.com\/SS-Runen\/Kaggle-Competition-Data\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/train.csv\"\ndf_train = pd.read_csv(filepath_or_buffer = path_to_train, index_col = \"Id\")\n\npath_to_holdout = \"https:\/\/github.com\/SS-Runen\/Kaggle-Competition-Data\/raw\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/test.csv\"\ndf_holdout = pd.read_csv(filepath_or_buffer = path_to_holdout, index_col = \"Id\")\n\n## ** Remove Data Leakers\n\nleakers = [\n    \"MoSold\",\n    \"YrSold\",\n    \"SaleType\",\n    \"SaleCondition\",\n    \"SaleType\",\n    \"SaleCondition\"\n]\ndf_train.drop(columns = leakers, inplace = True)\n\nfor item in leakers:\n    try:\n        df_holdout.drop(columns = item, inplace = True)\n    except Exception as e:\n        print(\"Error occured processing\", item, ':')\n        print('\\n', e)\n        \n## ** Change numeric features to their proper data types.\nlst_true_integers = [    \n    \"YearBuilt\",\n    \"YearRemodAdd\",    \n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"BedroomAbvGr\",\n    \"KitchenAbvGr\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"GarageYrBlt\",\n    \"GarageCars\",    \n]\nlst_true_integers.sort()\n\nlst_true_floats = [\n    \"LotFrontage\",\n    \"LotArea\",\n    \"MasVnrArea\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"LowQualFinSF\",\n    \"GrLivArea\",\n    \"GarageArea\",\n    \"WoodDeckSF\",\n    \"OpenPorchSF\",\n    \"PoolArea\",\n    \"EnclosedPorch\",\n    \"3SsnPorch\",\n    \"ScreenPorch\",\n    \"MiscVal\",    \n]\nlst_true_floats.sort()\n\n### *** Make list of features and their data types.\n\nlst_numerics = [n for n in lst_true_integers]\n\nfor item in lst_true_floats:\n    lst_numerics.append(item)\n    #lst_numeric_dtypes.append(item)\n\nfor feature in lst_true_integers:\n    if feature not in lst_numerics:\n        print(\"Integer Feature missing from Numerics List:\", feature)\nelse:\n    print(\"All integer features added.\")\n        \nfor feature in lst_true_floats:\n    if feature not in lst_numerics:\n        print(\"Float Feature missing from Numerics List:\", feature)\nelse:\n    print(\"All floating numeral features added.\")\n\n## ** Situation-specific Cleaning\ndef make_age_from_year(integer_year):\n    return int(dt.datetime.now().year) - integer_year\n\nlst_year_columns = [\"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\"]\nfor feature in lst_year_columns:\n    df_train[feature] = df_train[feature].apply(func = make_age_from_year)\n    df_holdout[feature] = df_holdout[feature].apply(func = make_age_from_year)\n    \nlst_categoricals = []\nfor feature in df_train.columns.values:\n    if (feature not in lst_numerics) and (str(feature) != \"SalePrice\"):\n        lst_categoricals.append(feature)        \n\ndf_train[\"SalePrice\"] = df_train[\"SalePrice\"].astype(dtype = np.float64)\nlst_categoricals.sort()\n\n## ** Data Visualization\n### *** Rough View: Distributions\n\nlst_floats_and_target = [ n for n in lst_true_floats ]\nlst_floats_and_target.append(\"SalePrice\")\nlst_categoricals_and_target = [n for n in lst_categoricals]\nlst_categoricals_and_target.append(\"SalePrice\")\n\ndf_corr_floats = df_train[ lst_floats_and_target ].corr()","d375bcc2":"df_train[lst_floats_and_target].hist(bins = 5, xrot = 90, figsize = (12,12), layout = (4,5))\n#df_train[lst_true_integers].hist(bins = 8, xrot = 45)\n#df_train[lst_categoricals].hist(xrot = 45)","7456bbca":"df_train[lst_floats_and_target].plot(kind='density', subplots=True, layout=(4,5), sharex=False, figsize = (12,12) )","5e979929":"df_train[lst_floats_and_target].plot(kind='box', subplots=True, layout=(4,5), sharex=False, sharey=False, figsize = (12,12))","5cee7427":"#PyPlot correlation matrix\nfig_corrmat = plt.figure()\nax = fig_corrmat.add_subplot(1, 1, 1)\nmatrix_axis = ax.matshow(df_train[lst_floats_and_target].corr(), vmin=-1, vmax=1)\nfig_corrmat.colorbar(matrix_axis)\nticks = np.arange(0, len(df_train[lst_floats_and_target].corr().columns.values), 1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(df_train[lst_floats_and_target].corr().columns.values)\nax.set_yticklabels(df_train[lst_floats_and_target].corr().index.values)\nplt.xticks(rotation = 90)\nplt.show()","d9aa254a":"#Seaborn bersion of correlation matrix\nsbrn.heatmap( df_corr_floats )","7e66654f":"help(scatter_matrix)","05ea6275":"#Scatter plot with PyPlot\nscatter_matrix( df_train[lst_floats_and_target], figsize = (24,24) )\nplt.show()","b36cfa4d":"hist_axis = plt.axes()\nhist_axis.plot(\"X\", \"Y\", df_train[[\"SalePrice\", \"LotArea\"]])","ae25b0d8":"def get_incomplete_features(df, threshold = 0.05):\n    row_count = df.shape[0]\n    sr_null_counts = df.isnull().sum()\n    lst_drop = []\n    lst_have_nulls = sr_null_counts.index.values.tolist()\n    end_decorator = \"\\n******\"\n    for feature in lst_have_nulls:\n        threshold = 0.05\n        null_percent = (sr_null_counts[feature] \/ row_count)\n\n        if null_percent > threshold:\n            print(\"To Drop: \", feature, type(df_train[feature]), type(df_train[feature].iloc[0]) )\n            lst_drop.append(feature)            \n                \n    return lst_drop\n\nlst_missing_value_failers = get_incomplete_features(df = df_train, threshold = 0.05)\n\ndf_train.drop(columns = lst_missing_value_failers, inplace = True)\n\n# dict_drop_errors = {}\n# for feature in lst_missing_value_failers:\n#     try:\n#         df_holdout.drop(inplace = True, columns = feature)\n#     except Exception as e:\n#         dict_drop_errors[\"holdout\"] = {feature:e}\n    \n#     try:\n#         df_test.drop(inplace = True, columns = feature)\n#     except Exception as e:\n#         dict_drop_errors[\"test\"] = {feature:e}\n\nlst_features = df_train.columns.tolist()\nlst_features.remove(\"SalePrice\")\ndf_holdout = df_holdout[ lst_features ]\n\nfor feature in lst_missing_value_failers:\n    if feature in lst_numerics:\n        lst_numerics.remove(feature)\n    if feature in lst_categoricals:\n        lst_categoricals.remove(feature)","6b66e3fb":"temp = pd.Series(data = df_train.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","a3de2d4c":"df_holdout.shape","c92402b1":"df_train.shape","51787d0e":"def var_threshold(df, minimum_variance = 0.20):\n    dict_variances = {}\n    lst_drop = []\n    df = df.copy()\n    \n    for feature in df.columns.values:\n        name = feature\n        var = df[name].var()        \n        dict_variances[name] = var\n        \n        if var < minimum_variance:\n            lst_drop.append(name)\n    \n    return {\"variances\":dict_variances, \"failers\":lst_drop}","fd15f973":"def make_traintest(df, train_fraction = 0.7, random_state_val = 88):\n    df = df.copy()\n    df_train = df.sample(frac = train_fraction, random_state = random_state_val)    \n    bmask_istrain = df.index.isin(df_train.index.values)\n    df_test = df.loc[ ~bmask_istrain ]\n    \n    #return (df_train, df_test)\n    \n    return {\n        \"train\":df_train,\n        \"test\":df_test\n        }","3a0591c3":"split = make_traintest(df_train)\ndf_housing_clean = df_train.copy()\n\ndf_train = split[\"train\"]\ndf_test = split[\"test\"]\n\ndf_train_unimputed, df_test_unimputed, df_holdout_unimputed = df_train.copy(), df_test.copy(), df_holdout.copy()\n\ndf_train_modes = df_train.mode()\nfor feature in df_train_modes.columns.values:\n    df_train[feature] = df_train[feature].fillna(value = df_train_modes[feature].iloc[0])\n#Note: When the following line was used to impute, it triggered confusing error messages.\n#df_train[feature].fillna(inplace = True, value = df_train_modes[feature].iloc[0])\n\ndf_test_modes = df_test.mode()\nfor feature in df_test_modes.columns.values:\n    df_test[feature] = df_test[feature].fillna(value = df_test_modes[feature].iloc[0])\n    \ndf_holdout_modes = df_holdout.mode()\nfor feature in df_holdout_modes.columns.values:\n    df_holdout[feature] = df_holdout[feature].fillna(value = df_holdout_modes[feature].iloc[0])","b2b34d12":"from sklearn.impute import SimpleImputer\n\nimputer_mean = SimpleImputer(\n    strategy = \"mean\",\n    verbose = True,\n    add_indicator = False,\n    missing_values = np.nan\n)\n\nlst_current_numeric_columns = df_train.select_dtypes(include = \"number\").columns.tolist()\ndf_train[lst_numerics] = imputer_mean.fit_transform(X = df_train[lst_numerics])\ndf_train[\"SalePrice\"] = imputer_mean.fit_transform(X = df_train[[\"SalePrice\"]])\n\nlst_current_numeric_columns.remove(\"SalePrice\")\ndf_test[lst_numerics] = imputer_mean.fit_transform(X = df_test[lst_numerics])\ndf_test[\"SalePrice\"] = imputer_mean.fit_transform(X = df_test[[\"SalePrice\"]])\n\ndf_holdout[lst_numerics] = imputer_mean.fit_transform(X = df_holdout[lst_numerics])","53f48bc9":"temp = pd.Series(data = df_train.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","837acd16":"temp = pd.Series(data = df_test.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","27a4c5e2":"temp = pd.Series(data = df_holdout.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","7151b3ba":"#Old process was too complicated. These functions are here for reference but are not used.\n##=========================================================================================\n\n#     label_encoder = LabelEncoder()    \n    \n#     encoded_data = label_encoder.fit_transform( sr.dropna().astype(dtype = str) )    \n    \n#     value_counts = sr.astype(dtype = str).value_counts().sort_values(ascending = False)    \n#     mode = value_counts.iloc[0]\n    \n#     bmask_isnull = sr.isnull()\n#     sr_string_version = sr.astype(dtype = str)\n#     sr_string_version[bmask_isnull] = mode\n    \n#     ndarr_clean = label_encoder.transform(sr_string_version)\n#     ndarr_clean = label_encoder.inverse_transform(encoded_data) \n    \n#     return {\"data\":ndarr_clean, \"impute_value\":mode}\n\n# #Calling the hand-made functions.\n# for feature in lst_categoricals:\n#     try:\n#         impute_results = impute_categorical(df_train[feature])\n#         df_train[feature] = impute_results[\"data\"]\n        \n# #         bmask_isnull_test = df_test[feature].isnull()\n# #         df_test.loc[bmask_isnull_test, feature] = impute_results[\"impute_value\"]\n        \n    \n#         impute_results = impute_categorical(df_test[feature])\n#         df_test[feature] = impute_results[\"data\"]\n        \n#     except Exception as e:\n#         print(\"Exception in imputing\", feature, \":\")\n#         print(e)\n# dict_classes = {}\n# for feature in lst_categoricals:\n#     impute_results = impute_categorical(df_train[feature])\n#     df_train[feature] = impute_results[\"data\"]\n    \n#     encoding_result = label_encode(df_train[feature])\n#     try:\n#         dict_classes[feature] = encoding_result[\"encoder_object\"].classes_\n#     except:\n#         pass\n    \n# #       bmask_isnull_test = df_test[feature].isnull()\n# #       df_test.loc[bmask_isnull_test, feature] = impute_results[\"impute_value\"]\n        \n    \n#     impute_results = impute_categorical(df_test[feature])\n#     df_test[feature] = impute_results[\"data\"]","593468d6":"df_train_modes = df_train.mode()\nfor feature in df_train_modes.columns.values:\n    df_train[feature] = df_train[feature].fillna(value = df_train_modes[feature].iloc[0])\n    #df_train[feature].fillna(inplace = True, value = df_train_modes[feature].iloc[0])\n\ndf_test_modes = df_test.mode()\nfor feature in df_test_modes.columns.values:\n    df_test[feature] = df_test[feature].fillna(value = df_test_modes[feature].iloc[0])\n    \ndf_holdout_modes = df_holdout.mode()\nfor feature in df_holdout_modes.columns.values:\n    df_holdout[feature] = df_holdout[feature].fillna(value = df_holdout_modes[feature].iloc[0])","20a33d68":"#Label-encoding categorical features.\nlabel_encoder = LabelEncoder()\ndf_labeled_categoricals = df_train[lst_categoricals]\n\ndict_classes = {}\n\nfor feature in df_labeled_categoricals.columns.values:\n    df_labeled_categoricals[feature] = label_encoder.fit_transform(df_labeled_categoricals[feature].dropna())    \n    dict_classes[feature] = label_encoder.classes_    \n\n#Scaling features.\nminmax_scaler = MinMaxScaler()\ndf_scaled_numerics = df_train[lst_numerics].copy()\ndf_scaled_categoricals = df_labeled_categoricals[lst_categoricals].copy()\n\n#Min-max-scaled data frame with all true numeric data-types.\ndf_scaled_numerics[lst_numerics] = minmax_scaler.fit_transform(X = df_train[lst_numerics])\n\n#Separate data frame for categorical features.\ndf_scaled_categoricals[lst_categoricals] = minmax_scaler.fit_transform(X = df_labeled_categoricals[lst_categoricals])\n\n#Get list of features that lack variance.\nresults = var_threshold(df = df_scaled_numerics, minimum_variance = 0.03)\ndict_variances, lst_variance_failers = results[\"variances\"], results[\"failers\"]\n\ncategorical_results = var_threshold(df = df_scaled_categoricals, minimum_variance = 0.03)\ndict_categoricals_variances, lst_variance_failers_categorical = categorical_results[\"variances\"], categorical_results[\"failers\"]\n\nfor failer in lst_variance_failers_categorical:\n    lst_variance_failers.append(failer)\n    \n#Drop features from lists of features.\nfor feature in lst_variance_failers:\n    try:\n        lst_numerics.remove(feature)       \n    except Exception as e:\n        print(\"Error removing\", feature, \"from list of true numerics\/numeric data types.\", '\\n', e)\n\nfor feature in lst_variance_failers:\n    try:\n        lst_categoricals.remove(feature)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from lst_categoricals.\", '\\n', e)\n\n#Drop features from data frames.\nfor feature in lst_variance_failers:\n    try:\n        df_train.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from training data.\", '\\n', e)\nelse:\n    print(\"=====\\nAll variance failers removed from train data.\\n=====\")\n        \nfor feature in lst_variance_failers:\n    try:\n        df_holdout.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from holdout data.\", '\\n', e)\nelse:\n    print(\"=====\\nAll variance failers removed from holdout data.\\n=====\")\n\nfor feature in lst_variance_failers:\n    try:\n        df_test.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from test data.\", '\\n', e)\nelse:\n    print(\"=====\\nAll variance failers removed from test data.\\n=====\")","4fecdca7":"print(\"Nulls in train set:\", max(df_train.isnull().sum()) )\nprint(\"Nulls in test set:\", max(df_test.isnull().sum()) )\nprint(\"Nulls in holdout set:\", max(df_holdout.isnull().sum()) )","54c37730":"df_train.shape","9c2f0392":"df_train.head(2)","6d9a3e0a":"df_test.shape","02e503dc":"df_test.head(2)","50807537":"df_holdout.shape","b6dc51f2":"df_holdout.head(2)","8d0bc35a":"#Get dummies. Done to solve error from TPOT complaining about receiving classification data.\ndf_train_onehot = df_train.copy()\ndf_train_onehot = df_train_onehot.append(df_test)\ndf_train_onehot = df_train_onehot.append(df_holdout)\n\ndf_train_onehot = pd.get_dummies(df_train_onehot)\nprint(df_train_onehot.index)\n\ndf_holdout_onehot = df_train_onehot[1461: ]\ndf_train_onehot.drop(inplace = True, index = df_holdout_onehot.index)\ndf_holdout_onehot.drop(columns = \"SalePrice\", inplace = True)\n\nsplit_onehot = make_traintest(df_train_onehot)\ndf_train_onehot = split_onehot[\"train\"]\ndf_test_onehot = split_onehot[\"test\"]","23000bab":"df_train_onehot.shape","2de57f55":"df_train_onehot.head(2)","5815ed34":"df_test_onehot.shape","9e929575":"df_test_onehot.head(2)","0becd679":"df_holdout_onehot.shape","53f930e0":"df_holdout_onehot.head(2)","9e9f9c52":"for column in df_train_onehot.columns.values:\n    if column not in df_holdout_onehot.columns.values:\n        print(column)","8147bddd":"def save_submission_file(dict_data, filename):\n    df = pd.DataFrame(data = dict_data)\n    timestamp = str( dt.datetime.now() )\n    name = filename + '-' + timestamp + \".csv\"\n    \n    df.to_csv(\n        path_or_buf = name,\n        index = False\n    )\n    \n    return None","de0de8a7":"url_to_raw = \"https:\/\/github.com\/SS-Runen\/Kaggle-Competition-Data\/raw\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/df_train_statclean.csv\"\ndf_train_statclean = pd.read_csv(\n    filepath_or_buffer = url_to_raw,\n    index_col = \"Id\"\n)\nmax(df_train_statclean.isnull().sum())","a3341d61":"df_train_statclean.head()","b0853d08":"url_to_raw = \"https:\/\/github.com\/SS-Runen\/Kaggle-Competition-Data\/raw\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/df_test_statclean.csv\"\ndf_test_statclean = pd.read_csv(\n    filepath_or_buffer = url_to_raw,\n    index_col = \"Id\"\n)\n\nmax(df_test_statclean.isnull().sum())","54cb4aae":"df_test_statclean.head()","30e8fbcc":"url_to_raw = \"https:\/\/github.com\/SS-Runen\/Kaggle-Competition-Data\/raw\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/df_holdout_statclean.csv\"\ndf_holdout_statclean = pd.read_csv(\n    filepath_or_buffer = url_to_raw,\n    index_col = \"Id\"\n)\n\nmax(df_holdout_statclean.isnull().sum())","94a0bbf6":"df_holdout_statclean.head()","b71f5034":"#!pip install -U keras-tuner\n!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git@1.0.2rc2\n!pip install autokeras","1c21abbf":"from autokeras import StructuredDataRegressor\n\nprint(\"Search Started.\")\n# define the search\nsearch = StructuredDataRegressor(max_trials = 4, overwrite = True, loss = 'mean_squared_error')\n\n# perform the search\n# search.fit(\n#     x = df_train_onehot.drop(columns = \"SalePrice\"),\n#     y = df_train_onehot[\"SalePrice\"],\n#     verbose=0)\nsearch.fit(\n    x = df_train_statclean.drop(columns = \"SalePrice\"),\n    y = df_train_statclean[\"SalePrice\"],\n    epochs = 4)\nprint(\"Search Finished.\")","0dce71d0":"print(\"Started AutoKeras evaluation.\")\ntry:    \n    mse = search.evaluate(\n        x = df_test_statclean.drop(columns = \"SalePrice\"),\n        y = df_test_statclean[\"SalePrice\"],\n        )\n    print('RMSE:{}'.format(np.sqrt(mse)))\nexcept Exception as e:\n        print(\"Error thrown attempting to evaluate AutoKeras model:\\n\", e)","c7ece8a5":"# use the model to make a prediction\nyhat = search.predict(df_holdout_statclean)\nprint('First Prediction: %.3f' % yhat[0] )\n\n# get the best performing model\nmodel = search.export_model()\n\n# summarize the loaded model\nprint(\"Model Summary\")\nmodel.summary()\n\n# save the best performing model to file\nmodel.save('mdl_autokeras_statclean')","2bb308a2":"url_to_raw_holdout = \"https:\/\/raw.githubusercontent.com\/SS-Runen\/Kaggle-Competition-Data\/master\/Ames%20Iowa%20Housing%20Kaggle%20Edition\/df_holdout_statclean.csv\"\nyhat = model.predict(url_to_raw_holdout)\n\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout_statclean.index.tolist(), \"SalePrice\":yhat.reshape(yhat.size, )},\n    filename = \"AutoKeras on Stat-cleaned from Traditional Notebook\"\n)","1442f768":"# !pip install auto-sklearn\n# import autosklearn\n# autosklearn.__version__","b95f3263":"# #Install autosklearn on Kaggle\n# #URL: https:\/\/www.kaggle.com\/general\/146842\n# !apt-get remove swig\n# !apt-get install swig3.0 build-essential -y\n# !ln -s \/usr\/bin\/swig3.0 \/usr\/bin\/swig\n# !apt-get install build-essential\n# !pip install --upgrade setuptools\n# !pip install auto-sklearn","203f193f":"!pip install tpot","bff8b36b":"import tpot\nfrom tpot import TPOTRegressor\ntpot.__version__","98d96340":"print(df_train_onehot.shape)","e565d3ed":"# #Reduced to 3 because error: pipeline failed to complete.\n# #Failed because n_splits being greater than the amount of values\/samples in a class.\n# cv_rskfold = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 5, random_state = 88)\n\n# mdl_tpot_regressor = TPOTRegressor(\n#     n_jobs = -1,\n#     generations = 8,\n#     population_size = 32,\n#     cv = cv_rskfold\n# )\n\n# mdl_tpot_regressor.fit(\n#     features = df_train_onehot.drop(columns = \"SalePrice\"),\n#     target = df_train_onehot[\"SalePrice\"])","caba8f59":"from tpot import TPOTRegressor\n# define evaluation procedure\ncv_rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=88)\n# define search\nmodel = TPOTRegressor(\n    generations=4,\n    population_size=8,\n    scoring='neg_mean_absolute_error',\n    cv=cv_rskf,\n    verbosity=2,\n    random_state=1,\n    n_jobs=-1\n)","8cb7b92c":"print(df_train_onehot.isnull().sum().sort_values())","240854b1":"# perform the search\nmodel.fit(\n    df_train_onehot.drop(columns = \"SalePrice\"),\n    imputer_mean.fit_transform(df_train_onehot[[\"SalePrice\"]]).reshape(\n        len(df_train_onehot[\"SalePrice\"]), )\n)","27fce741":"# export the best model\nmodel.export('TPOT Baseline Model, Onehot.py')\nresults_tpot_clean = model.predict(features = df_holdout_onehot)\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":results_tpot_clean},\n    filename = \"TPOTRegressor Lifted from Docs, Stat-clean\"\n)","5228fcd4":"mdl_tpot_regressor.fit(\n    features = df_train_unimputed.drop(columns = \"SalePrice\"),\n    target = df_train_unimputed[\"SalePrice\"])\n\nresults_tpot_unclean = mdl_tpot_regressor.predict(features = df_holdout_unimputed)","264d39bd":"mdl_tpot_regressor.score(df_test.drop(columns = \"SalePrice\"), df_test[\"SalePrice\"])\nmdl_tpot_regressor.export('tpot_pipeline_unclean.py')","52236170":"save_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":results_tpot_unclean},\n    filename = \"Baseline TPOT on Unclean Data\"\n)","35b29a91":"!pip install hyperopt","dbd58fd9":"!git clone https:\/\/github.com\/hyperopt\/hyperopt-sklearn.git\n#!cd hyperopt\n!pip install -e .\n!pip show ","ad04a2ee":"!pip show hpsklearn","806a1254":"from sklearn.svm import SVC, LinearSVC, LinearSVR\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom hpsklearn import HyperoptEstimator\nimport xgboost\n\nmdl_svc, mdl_linSVC = SVC(), LinearSVC()\nmdl_randforest_class = RandomForestClassifier(\n    n_jobs = -1,\n    random_state=88,\n    max_samples = 0.70,\n    min_samples_leaf=0.05\n    )\nlst_classifiers = [mdl_svc, mdl_linSVC, mdl_randforest_class]\n\nmdl_svr, mdl_xgb_regress = LinearSVR(), xgboost.XGBRegressor()\nmdl_randforest_regress = RandomForestClassifier(\n    n_jobs = -1,\n    random_state=88,\n    max_samples=0.70,\n    min_samples_leaf=0.05\n    )\n#lst_regressors = [mdl_xgb_regress, mdl_svr, mdl_randforest_regress]\nlst_regressors = [mdl_xgb_regress, mdl_svr]\n\nprep_pca, prep_onehot = PCA(), OneHotEncoder()\nimputer_mean = SimpleImputer()\nlst_preprocessors = [prep_pca, prep_onehot, imputer_mean]\n\nestimator_hyperopt = HyperoptEstimator(\n    regressor=lst_regressors,\n    preprocessing=lst_preprocessors,\n    max_evals=32,\n    trial_timeout=300)","8b6714e8":"#Used to be:\n#imputer_mean.fit_transform(df_train_onehot[[\"SalePrice\"]]).reshape(len(df_train_onehot[\"SalePrice\"]), )\ndf_train_onehot_saleprice = pd.DataFrame(data = imputer_mean.fit_transform(df_train_onehot[[\"SalePrice\"]]), index = df_train_onehot.index)","1e40f883":"# estimator_hyperopt.fit(\n#     X = df_train_onehot.drop(columns = \"SalePrice\"),\n#     y = df_train_onehot_saleprice.values\n# )\n\n# test_score = estimator_hyperopt.score(\n#     X = df_test.drop(columns = \"SalePrice\").values,\n#     y = df_test[\"SalePrice\"].values\n#     )\n\n# mdl_best_hyperopt = estimator_hyperopt.best_model()\n# print(\"Best HyperOpt model:\\n\", mdl_best_hyperopt)\n\nestimator_hyperopt.fit(\n    X = df_train_statclean.drop(columns = \"SalePrice\").values,\n    y = df_train_statclean[\"SalePrice\"]\n)\n\ntest_score = estimator_hyperopt.score(\n    X = df_test_statclean.drop(columns = \"SalePrice\").values,\n    y = df_test_statclean[\"SalePrice\"].values\n    )\n\nmdl_best_hyperopt = estimator_hyperopt.best_model()\nprint(\"Best HyperOpt model:\\n\", mdl_best_hyperopt)","2fe6814b":"import xgboost as xgb\nxgb.__version__","60a52886":"DMatrix_train = xgb.DMatrix(\n    df_train_statclean.drop(columns = \"SalePrice\"),\n    label = df_train_statclean[\"SalePrice\"].values\n)\n\nDMatrix_test = xgb.DMatrix(\n    df_test_statclean.drop(columns = [\"SalePrice\", \"MSSubClass\"]),\n    label = df_test_statclean[\"SalePrice\"].values\n)\n\nparameters = {\n    \"verbosity\": 3,\n    \"max_depth\": 8,\n    \"subsample\": 0.7,\n    \"seed\": 105,\n    \"booster\": \"gbl\",\n    \"eta\": 0.1\n}\n\nrounds = 2048\n\neval_sets = [(DMatrix_train, \"train-on-train\"), (DMatrix_test, \"test_set\")]\n\ndict_evaluation_results = {}\n\n\nmdl_xgb = xgb.train(\n    params = parameters,\n    dtrain = DMatrix_train,\n    num_boost_round = rounds,\n    evals = eval_sets,\n    evals_result = dict_evaluation_results\n)","d80523c7":"dict_evaluation_results","e640ef05":"DMatrix_holdout = xgb.DMatrix(df_holdout_statclean)\nxgb_predictions = mdl_xgb.predict(DMatrix_holdout)\n\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout_statclean.index.tolist(), \"SalePrice\":xgb_predictions},\n    filename = \"Baseline XGBoost on Stat-cleaned Data v1.5 GBL\"\n)","eacdb748":"print(\"Done\")","f47ade5f":"### *** Error during Auto-Keras Search\n\n```\nTypeError\n<ipython-input-100-056999a55c5a> in <module>\n      9     x = df_train_onehot.drop(columns = \"SalePrice\"),\n     10     y = df_train_onehot[\"SalePrice\"],\n---> 11     verbose=0)\n     12 print(\"Search Finished.\")\n\nTraceback...\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```\n\nSame error occurs on AutoKeras attempt for uncleaned data.\n\n```\nTypeError\n<ipython-input-102-c26467ea69a1> in <module>\n      6 search_unimputed.fit(\n      7     x = df_train_unimputed.drop(columns = \"SalePrice\"),\n----> 8     y = df_train_unimputed[\"SalePrice\"]\n      9 )\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```\n","828e90af":"## ** Description of the Notebook","2ef78440":"### *** Simplified Imputation of Categoricals","a0baed22":"# * Manual XGBoost as Baseline","82c6ee58":"The code ind the next cell returns an error message.\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.7 \/opt\/conda\/lib\/python3.7\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-8xqtft7d\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- setuptools wheel numpy Cython Check the logs for full command output.","8aef8c27":"## ** Splitting Data and Imputing Categorical Features","35b1cc92":"### *** Old Process","2062a792":"## ** Initial Cleaning\n\nThis step was added after AutoKeras failed to create a usable model 5 times.","df81aebf":"## ** TPOT","67f0f07a":"## ** AutoKeras","4f1f0c1d":"This notebook is an ongoing improvement over the previous notebook create by this same user: [*Ames Iowa Housing Prices Prediction*, by SS_Runen](https:\/\/www.kaggle.com\/joachimrives\/ames-iowa-housing-price-prediction). An important goal of the project is to integrate automated machine learning solutions.\n\nThe secondary purposes of the project are:\n- To compare the best possible hand-made ML models with equally well-done Auto-ML implementations.\n- Making the code less \"messy\" and easier to read.\n- To correct any other errors in the previous notebook that are discovered thru review.\n\nCreated on October 08, 2020.","de4a0b1e":"### *** TPOT Predictions on Imputed Data","e7fda66c":"Code below also returns an error:\n\nAttributeError: type object 'Callable' has no attribute '_abc_registry'\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.7 \/opt\/conda\/lib\/python3.7\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-0k9xfbte\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- setuptools wheel numpy Cython Check the logs for full command output.","ff61a901":"## ** Imputation of Numeric Features","25bbeaec":"### *** Error During TPOT Search\n\nStrangely, even though all previous tests show no null-equivalent values in all the data sets, an error that could be caused\nby unimputed data is thrown.\n\n```\nYou might need a way of handling missing values, such as pandas.DataFrame.fillna or sklearn.preprocessing.Imputer. See our Missing Values tutorial for more details.\n\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\n```\n\n```\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-200-11d97e9d55bf> in <module>\n     12 mdl_tpot_regressor.fit(\n     13     features = df_train_onehot.drop(columns = \"SalePrice\"),\n---> 14     target = df_train_onehot[\"SalePrice\"])\n\n\nValueError: Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array.\n```","6739fc92":"### *** Removing Features with 5% or More Missing Values","9551a2a1":"## ** AutoSKLearn","ff0ba6db":"# Ames Iowa Housing Prices Prediction with AutoML","cf253cd5":"### *** TPOT Predictions on Unimputed\/Unclean Data","af48e88a":"Code below throws error.\n\nAttributeError: type object 'Callable' has no attribute '_abc_registry'\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.7 \/opt\/conda\/lib\/python3.7\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-as0te6id\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- setuptools wheel numpy Cython Check the logs for full command output.","11ab0864":"The data will be split into train and test sets to simulate real-world data gathering. Also, changes have been made to the variance filtering procedure for categoricals. The original procedure was:\n1. Make a separate dataframe with all categorical features. Remove all rows with missing values.\n2. Make the columns numeric using a LabelEncoder.\n3. Measure the variance of the features in the encoded dataframe. Based on the results, a list of \"failing features\" will be created.\n\nThe problem with this approach is that many features' variance will be randomly changed by missing values in other features. The features will be imputed first to avoid randomly removing data points.","ac3c6550":"Known issue:\n\n```\n\/opt\/conda\/lib\/python3.7\/site-packages\/kerastuner\/engine\/metrics_tracking.py:92: RuntimeWarning: All-NaN axis encountered\n  return np.nanmin(values)\n```\n\nPossible cause is Tensorflow version:\n\n```\n\/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow_addons\/utils\/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.3.1 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps:\/\/github.com\/tensorflow\/addons\n  UserWarning\n```\n\nStill results in error as of version 1.1 of this notebook:\n```\nTypeError                                 Traceback (most recent call last)\n<ipython-input-40-056999a55c5a> in <module>\n      9     x = df_train_onehot.drop(columns = \"SalePrice\"),\n     10     y = df_train_onehot[\"SalePrice\"],\n---> 11     verbose=0)\n     12 print(\"Search Finished.\")\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```","c7c6535a":"## ** Custom Functions","c4d7a899":"Error on line 12.\n\nRuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation:\n\nhttps:\/\/epistasislab.github.io\/tpot\/using\/","0275c224":"## ** Hyperopt SKLearn","86031b0d":"## ** Issues","59dfbd51":"### *** Filtering Based on Variance Threshold","06aa8bca":"## ** Imputation of Categorical Features","3dd353d3":"## ** Importing Stat-cleaned Data Sets of Traditional ML Handbook","83699982":"## ** Beginning of Data Processing","1a8ba002":"### *** Baseline AutoKeras Using Cleaned Data","1f9e08a7":"# * Using Auto-ML Libraries","faf014c7":"Code below complains of Nan or missing value. Manual check for nan values shows 1."}}