{"cell_type":{"9e9dd355":"code","41a70240":"code","22ebb891":"code","01038f33":"code","343e8f26":"code","a359f512":"code","a725a48b":"code","89ee8cc0":"code","06b8e092":"code","f72cc35a":"code","ba0ae515":"code","e1d141b7":"code","014d4c4e":"code","9165d35d":"code","9b799773":"code","56d95078":"code","c59ace9f":"code","96e2b9bd":"code","71355135":"code","ac6885b9":"code","ca2e700d":"code","a4baf6c9":"code","af164112":"code","39d0279a":"code","3b0847ac":"code","4e0c2b40":"code","893d3b8f":"markdown","728d98fe":"markdown","db317c45":"markdown","999b6434":"markdown","cc59287b":"markdown","62afa50b":"markdown","ffe1c525":"markdown","0ac96b4a":"markdown","3fb99883":"markdown","57114475":"markdown","b085c0c4":"markdown","1ec023c1":"markdown","f54234eb":"markdown","08f82bf9":"markdown","dc9a3c6d":"markdown","6456bbfe":"markdown","ab613e05":"markdown","e6e06e8d":"markdown","0d14f579":"markdown","f2b91a6f":"markdown","dfa55463":"markdown","a8ec5819":"markdown","1151d711":"markdown","83aae63d":"markdown","8009e969":"markdown"},"source":{"9e9dd355":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41a70240":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport timeit, time\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.experimental import WideDeepModel as WDM\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,f1_score,precision_score,balanced_accuracy_score\n\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \n\n\nfrom plotly.offline import iplot, init_notebook_mode, plot, download_plotlyjs\ninit_notebook_mode()\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.subplots import make_subplots","22ebb891":"url = 'https:\/\/i.pinimg.com\/originals\/e3\/f4\/f1\/e3f4f140bbd3716c76d18a00ea11f22e.jpg'\n  \nImage(url= url, width=600, height=600, unconfined=True)\n","01038f33":"df = pd.read_csv('..\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv')\ndf.drop(columns= ['img_name'], axis=1, inplace= True)\n\ndf.head()","343e8f26":"# Copied from starter code. \ndef plot(X,y):\n    for i in range(3):\n        plt.title(y[i],)\n        plt.imshow(X[i].reshape(48,48))\n        plt.show()\n        \nx_dis = df['pixels'][0:3].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\ny_dis= df['ethnicity'][0:3]\n\nplot(x_dis, y_dis)","a359f512":"sns.barplot(y=list(df['ethnicity'].value_counts().values),x= list(df['ethnicity'].value_counts().index))","a725a48b":"sns.barplot(y=list(df['gender'].value_counts().values),x= list(df['gender'].value_counts().index))","89ee8cc0":"sns.distplot(df['age'])","06b8e092":"# Find the average of all emotion counts\nm = df.groupby('ethnicity').count().mean().values[0]\n#print(\"Mean of all ethnicity counts: \" + str(m))\n\nethnicity = list(df.ethnicity.unique())\n\noversampled = pd.DataFrame()\nfor n in ethnicity:\n    #print('\\n' + n)\n    l = len(df[df.ethnicity==n])\n    print('Before sampling: ' + str(l))\n    \n    if (l>=m):\n        dft = df[df.ethnicity==n].sample(int(m))\n        oversampled = oversampled.append(dft)\n        #print('Ater sampling: ' + str(len(dft)))\n    else:\n        frac = int(m\/l)\n        dft = pd.DataFrame()\n        for i in range(frac+1):\n            dft = dft.append(df[df.ethnicity==n])\n            \n        dft = dft[dft.ethnicity==n].sample(int(m))\n        oversampled = oversampled.append(dft)\n        #print('Ater sampling: ' + str(len(dft)))\n        \noversampled = oversampled.sample(frac=1).reset_index().drop(columns=['index'])\n\nsns.barplot(y=list(oversampled['ethnicity'].value_counts().values),x= list(oversampled['ethnicity'].value_counts().index))","f72cc35a":"\n# split the data into train and test sets. \nX = oversampled.drop(['ethnicity'], axis=1)\ny= np.array(oversampled['ethnicity'].values)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True )\n\n# spliting the image data and tabular data as input a and b. \n#Converting the pixel data into array. \n\nX_test_A = X_test['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\")) #converting data to numpy array\nX_test_A = np.array(X_test_A)\/255.0 #normalization\n\nX_t = []\nfor i in range(X_test_A.shape[0]):\n    X_t.append(X_test_A[i].reshape(48,48,1)) #reshaping the data to (n,48,48)\n    \nX_test_A_shaped = np.array(X_t)\nprint(len(X_test_A_shaped))\n\nX_train_A = X_train['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\")) #converting data to numpy array\nX_train_A = np.array(X_train_A)\/255.0 #normalization\n\nX_t_a = []\nfor i in range(X_train_A.shape[0]):\n    X_t_a.append(X_train_A[i].reshape(48,48,1)) #reshaping the data to (n,48,48)\n    \nX_train_A_shaped = np.array(X_t_a)\nprint(len(X_train_A_shaped))\nprint(X_train_A_shaped.shape)\n\n# remove the pixels from the x input data. \nX_test_B = X_test.drop(['pixels'], axis=1)\nX_train_B = X_train.drop(['pixels'], axis=1)\n\n\ntrain = pd.DataFrame(data=y_train, columns=['ethnicity'])\ntest = pd.DataFrame(data=y_test, columns=['ethnicity'])\n\nf, axes = plt.subplots(1, 2, figsize=(12, 5), sharex= True)\naxes[0].set_title('Test Data split')\naxes[1].set_title('Train Data split')\nsns.barplot(y=list(train['ethnicity'].value_counts().values),x= list(train['ethnicity'].value_counts().index),ax=axes[0])\nsns.barplot(y=list(test['ethnicity'].value_counts().values),x= list(test['ethnicity'].value_counts().index),ax=axes[1])\n\n","ba0ae515":"def scaler_std(series):\n    '''\n    input= df['series']\n    output= scaled series\n   \n    '''\n    mean = series.values.mean()\n    std = series.values.std()\n    return series.apply(lambda x: (x-mean)\/std)\n\nX_test_B['age'] = scaler_std(X_test_B['age'])\nX_train_B['age'] = scaler_std(X_train_B['age'])\n\n# the gender column to be encoded. \nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\nX_test_B = ct.fit_transform(X_test_B)\nX_train_B = ct.fit_transform(X_train_B)","e1d141b7":"url = 'https:\/\/www.researchgate.net\/profile\/Kaveh_Bastani\/publication\/328161216\/figure\/fig3\/AS:679665219928064@1539056224036\/Illustration-of-the-wide-and-deep-model-which-is-an-integration-of-wide-component-and.ppm'\n    \nImage(url= url, width=600, height=600, unconfined=True)","014d4c4e":"#Wide and Deep model with SGD optimizer\/ activation = selu \n\nstart_time = time.time()\n# model using image information and batch normalization\nmodel_image_selu = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(100, activation= 'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(50, activation=  'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(5,activation= 'softmax')\n   \n])\n\n#model using tabular data\nmodel_data_selu = keras.models.Sequential(\n[\n    keras.layers.Input(shape=[3]),\n    keras.layers.Dense(5,activation= 'softmax')\n])\n\n# compile the NN\ncombined_model = WDM(model_data_selu,model_image_selu)\ncombined_model.compile(loss= 'sparse_categorical_crossentropy', optimizer='sgd',metrics= ['SparseCategoricalAccuracy'])\n\n\n\n# fit the data\ninput_x = [X_train_B, X_train_A_shaped]\ninput_y = y_train\n\nval_data = [X_test_B, X_test_A_shaped], y_test\n\ncombined_model.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nselu_data_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","9165d35d":"# data and image info using optimizer 'adam'\nmodel_image = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'relu'),\n    keras.layers.Dense(100, activation= 'relu'),\n    keras.layers.Dense(50, activation= 'relu'),\n    keras.layers.Dense(5,activation= 'softmax')\n   \n])\n\n#tried with activation = 'selu' didnt increase the accuracy. The value remained around 0.790 for test data and train data was about 0.9. However 'relu' yeilds better. \n\n\nmodel_data = keras.models.Sequential(\n[\n    keras.layers.Input(shape=[3]),\n    keras.layers.Dense(3, activation= 'relu'),\n    keras.layers.Dense(5, activation= 'softmax')\n])\n\nmodel = WDM(model_data,model_image)\n\n\n#data and image with adam optimizer\nstart_time = time.time()\n\ninput_x = [X_train_B, X_train_A_shaped]\ninput_y = y_train\n\nval_data = [X_test_B, X_test_A_shaped], y_test\n\n\nmodel.compile(loss= 'sparse_categorical_crossentropy', optimizer='adam',metrics= ['SparseCategoricalAccuracy'])\n\nmodel.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nadam_time_data_image = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","9b799773":"#data and image with sgd optimizer\n\nstart_time = time.time()\n\ninput_x = [X_train_B, X_train_A_shaped]\ninput_y = y_train\n\nval_data = [X_test_B, X_test_A_shaped], y_test\n\n\nmodel_image_sgd = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'relu'),\n    keras.layers.Dense(100, activation= 'relu'),\n    keras.layers.Dense(50, activation= 'relu'),\n    keras.layers.Dense(5,activation= 'softmax')\n   \n])\n\nmodel_data_sgd = keras.models.Sequential(\n[\n    keras.layers.Input(shape=[3]),\n    keras.layers.Dense(3, activation= 'relu'),\n    keras.layers.Dense(5, activation= 'softmax')\n])\n\nmodel_sgd = WDM(model_data_sgd,model_image_sgd)\n\nmodel_sgd.compile(loss= 'sparse_categorical_crossentropy', optimizer='sgd',metrics= ['SparseCategoricalAccuracy'])\n\nmodel_sgd.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nsgd_data_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","56d95078":"#Model with image data using sgd optimizer \/ activation = selu\n\nstart_time = time.time()\n# model using image information and batch normalization\nselu_image = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(100, activation= 'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(50, activation=  'selu', kernel_initializer= 'lecun_normal'),\n    keras.layers.Dense(5,activation= 'softmax')\n   \n])\n\n# compile the NN\nselu_image.compile(loss= 'sparse_categorical_crossentropy', optimizer='sgd',metrics= ['SparseCategoricalAccuracy'])\n\n# fit the data\ninput_x = X_train_A_shaped\ninput_y = y_train\n\nval_data = X_test_A_shaped, y_test\n\nselu_image.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nselu_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","c59ace9f":"#image only with adam optimizer\nstart_time = time.time()\n\nadam_image = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'relu'),\n    keras.layers.Dense(100, activation= 'relu'),\n    keras.layers.Dense(50, activation= 'relu'),\n    keras.layers.Dense(5,activation= 'softmax')\n   \n])\n\ninput_x = X_train_A_shaped\ninput_y = y_train\n\nval_data = X_test_A_shaped, y_test\n\n\nadam_image.compile(loss= 'sparse_categorical_crossentropy', optimizer='adam',metrics= ['SparseCategoricalAccuracy'])\n\nadam_image.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nadam_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n","96e2b9bd":"#image only with sgd optimizer\n\nstart_time = time.time()\n\nsgd_image = keras.models.Sequential(\n[\n    keras.layers.Flatten(input_shape=[48,48,1]),\n    keras.layers.Dense(300, activation= 'relu'),\n    keras.layers.Dense(100, activation= 'relu'),\n    keras.layers.Dense(50, activation= 'relu'),\n    keras.layers.Dense(5,activation= 'softmax')\n  \n])\n\nsgd_image.compile(loss= 'sparse_categorical_crossentropy', optimizer='sgd',metrics= ['SparseCategoricalAccuracy'])\n\ninput_x = X_train_A_shaped\ninput_y = y_train\n\nval_data = X_test_A_shaped, y_test\n\nsgd_image.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nsgd_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","71355135":"# function to capture the diagonal values of the matrix \n\ndef true_pred(model, confusion_matrix, y_test):\n    '''\n    Input:- \n    confusion_matrix = is a np.ndarray\n    model = string name of the model. \n    y_test = classification label series\n    \n    Output:- \n    diagonal values of confusion matrix (true predictions) in dataframe \n    \n    '''\n    test = np.matrix(confusion_matrix)\n    n,m = test.shape\n    \n    # get label names from y_test column\n    labels = sorted(np.unique(y_test))\n    #print(labels)\n    # list of values \n    values = []\n    \n    if n == len(labels):\n        for i in range(m):\n            values.append(test[i,i])\n    else :\n        print('The lengths of y_test does not match with confusion matrix shape')\n    \n    #print(values)\n    data = { model: values}\n    #print(data)\n    df= pd.DataFrame(data=data, index= labels)\n    \n    return df\n","ac6885b9":"## Image with CNN \/ sgd optimizer\nstart_time = time.time()\n\nCNN_image_sgd = keras.models.Sequential(\n[\n    keras.layers.Conv2D(64,7, activation= 'relu', padding= 'same', input_shape= [48,48,1]), \n    keras.layers.MaxPooling2D(2), \n    keras.layers.Conv2D(128,3, activation= 'relu', padding= 'same'),\n    keras.layers.MaxPooling2D(2), \n    keras.layers.Conv2D(256,3, activation= 'relu', padding= 'same'),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Flatten(input_shape= [48,48,1]),\n    keras.layers.Dense(128, activation= 'relu'),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(64, activation= 'relu'),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(32, activation= 'relu'),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(5,activation= 'softmax')\n    \n])\n\nCNN_image_sgd.compile(loss= 'sparse_categorical_crossentropy', optimizer='sgd',metrics= ['SparseCategoricalAccuracy'])\n\ninput_x = X_train_A_shaped\ninput_y = y_train\n\nval_data = X_test_A_shaped, y_test\n\nCNN_image_sgd.fit(input_x, input_y, validation_data=val_data, shuffle= True, epochs=250,verbose=0, batch_size= 50)\n\nCNN_image_time = (time.time() - start_time)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","ca2e700d":"pd.DataFrame(CNN_image_sgd.history.history).plot(figsize=(8,5))\nplt.grid(True)","a4baf6c9":"#image with CNN\/ sgd optimizer\ny_pred_test_7 = np.argmax(CNN_image_sgd.predict(X_test_A_shaped), axis=1)\ny_pred_train_7 = np.argmax(CNN_image_sgd.predict(X_train_A_shaped), axis=1)\n\n#Gathering true predictions image CNN --> Test\ndf13 = true_pred('CNN_test_image_sgd', confusion_matrix(y_pred_test_7, y_test), y_test)\n\n# Gathering true prediction image models --> Train\ndf14 = true_pred('CNN_train_image_sgd', confusion_matrix(y_pred_train_7, y_train), y_train)\n\nprint('CNN Train Classification report')\nprint(classification_report(y_pred_train_7, y_train))\n\nprint('CNN Test Classification report')\nprint(classification_report(y_pred_test_7, y_test))\n","af164112":"# plot history for each model. Compare with data\/image vs image only model performance.\ndf1_hist = pd.DataFrame(combined_model.history.history) #selu_data_image\ndf2_hist = pd.DataFrame(model.history.history) #adam_data_image\ndf3_hist = pd.DataFrame(model_sgd.history.history) #sgd_data_image\ndf4_hist = pd.DataFrame(selu_image.history.history)\ndf5_hist = pd.DataFrame(adam_image.history.history)\ndf6_hist = pd.DataFrame(sgd_image.history.history)\ndf7_hist = pd.DataFrame(CNN_image_sgd.history.history)\n","39d0279a":"\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(25, 14), sharex= True, sharey=True, constrained_layout= False )\nplt.ylim(0,1.75)\naxs[0,0].plot(df1_hist.index, df1_hist[list(df1_hist.columns)])\naxs[0,1].plot(df2_hist.index, df2_hist[list(df2_hist.columns)])\naxs[0,2].plot(df3_hist.index, df3_hist[list(df3_hist.columns)])\naxs[1,0].plot(df4_hist.index, df4_hist[list(df4_hist.columns)])\naxs[1,1].plot(df5_hist.index, df5_hist[list(df5_hist.columns)])\naxs[1,2].plot(df6_hist.index, df6_hist[list(df6_hist.columns)])\n#axs[2,0].plot(df7_hist.index, df7_hist[list(df7_hist.columns)])\n\n\naxs[0, 0].set_title(\"Selu_data_image\")\naxs[0, 1].set_title(\"adam_data_image\")\naxs[0, 2].set_title(\"sgd_data_image\")\n\naxs[1, 0].set_title(\"selu_image\")\naxs[1, 1].set_title(\"adam_image\")\naxs[1, 2].set_title(\"sgd_image\")\n\n# axs[2,0].set_title('CNN_image')\n# fig.delaxes(axs[2,1]) #The indexing is zero-based here\n# fig.delaxes(axs[2,2]) #The indexing is zero-based here\n\naxs[0, 0].set_xlabel(\"Epochs\")\naxs[0, 0].set_ylabel(\"loss\/accuracy\")\naxs[0, 1].set_xlabel(\"Epochs\")\naxs[0, 2].set_xlabel('Epochs')\n#axs[0, 1].set_ylabel(\"loss\/accuracy\")\naxs[1, 0].set_xlabel(\"Epochs\")\naxs[1, 0].set_ylabel(\"loss\/accuracy\")\naxs[1, 1].set_xlabel(\"Epochs\")\naxs[1, 2].set_xlabel('Epochs')\n\n# axs[2, 0].set_ylabel(\"loss\/accuracy\")\n# axs[2, 0].set_xlabel('Epochs')\n\naxs[0,0].grid(axis='both')\naxs[0,1].grid(axis='both')\naxs[0,2].grid(axis='both')\naxs[1,0].grid(axis='both')\naxs[1,1].grid(axis='both')\naxs[1,2].grid(axis='both')\n#axs[0,0].legend(df1['loss'], [\"loss\"], loc=1)\naxs[0,0].legend(labels= ['loss', 'val_loss','SparseCategoricalAccuracy', 'val_SparseCategoricalAccuracy' ])","3b0847ac":"# data and image with SGD optimizer \/ activation= selu\ny_pred_test_1 = np.argmax(combined_model.predict([X_test_B, X_test_A_shaped]), axis=1)\ny_pred_train_1 = np.argmax(combined_model.predict([X_train_B, X_train_A_shaped]), axis=1)\n\n# data and image with adam optimizer \/ activation = relu\ny_pred_test_2 = np.argmax(model.predict([X_test_B, X_test_A_shaped]), axis=1)\ny_pred_train_2 = np.argmax(model.predict([X_train_B, X_train_A_shaped]), axis=1)\n\n# data and image with SGD optimizer \/ activation = relu\ny_pred_test_3 = np.argmax(model_sgd.predict([X_test_B, X_test_A_shaped]), axis=1)\ny_pred_train_3 = np.argmax(model_sgd.predict([X_train_B, X_train_A_shaped]), axis=1)\n\n\n#image with SGD optimizer\/ activation = selu\ny_pred_test_4 = np.argmax(selu_image.predict(X_test_A_shaped), axis=1)\ny_pred_train_4 = np.argmax(selu_image.predict(X_train_A_shaped), axis=1)\n\n# image with adam optimizer\ny_pred_test_5 = np.argmax(adam_image.predict(X_test_A_shaped), axis=1)\ny_pred_train_5 = np.argmax(adam_image.predict(X_train_A_shaped), axis=1)\n\n# image with sgd optimizer\ny_pred_test_6 = np.argmax(sgd_image.predict(X_test_A_shaped), axis=1)\ny_pred_train_6 = np.argmax(sgd_image.predict(X_train_A_shaped), axis=1)\n\n# Getting raw input data for comparision. \ninput_train = pd.DataFrame(train['ethnicity'].value_counts())\ninput_train.rename(columns={\"ethnicity\": \"Input_train\"},inplace= True)\n\ninput_test = pd.DataFrame(test['ethnicity'].value_counts())\ninput_test.rename(columns={\"ethnicity\": \"Input_test\"},inplace= True)\n\n# Gathering true prediction data\/image models -->Test. \ndf1 = true_pred('sgd_test_data_image_selu', confusion_matrix(y_pred_test_1, y_test), y_test)\ndf2 = true_pred('adam_test_data_image_relu',confusion_matrix(y_pred_test_2, y_test), y_test)\ndf3 = true_pred('sgd_test_data_image_relu' ,confusion_matrix(y_pred_test_3, y_test), y_test)\n\n# Gathering true prediction data\/image models -->Train. \ndf4 = true_pred('sgd_train_data_image_selu', confusion_matrix(y_pred_train_1, y_train), y_train)\ndf5 = true_pred('adam_train_data_image_relu',confusion_matrix(y_pred_train_2, y_train), y_train)\ndf6 = true_pred('sgd_train_data_image_relu' ,confusion_matrix(y_pred_train_3, y_train), y_train)\n\n\n# Gathering true prediction image models --> Test\ndf7 = true_pred('sgd_test_image_selu', confusion_matrix(y_pred_test_4, y_test), y_test)\ndf8 = true_pred('adam_test_image_relu', confusion_matrix(y_pred_test_5, y_test), y_test)\ndf9 = true_pred('sgd_test_image_relu', confusion_matrix(y_pred_test_6, y_test), y_test)\n\n# Gathering true prediction image models --> Train\ndf10 = true_pred('sgd_train_image_selu', confusion_matrix(y_pred_train_4, y_train), y_train)\ndf11 = true_pred('adam_train_image_relu', confusion_matrix(y_pred_train_5, y_train), y_train)\ndf12 = true_pred('sgd_train_image_relu', confusion_matrix(y_pred_train_6, y_train), y_train)\n\n\n# test data efficiency\ndf_confusion_matrix= pd.concat([df1,df2,df3,df7,df8,df9,df13, input_test], axis=1, sort=True)\n#print(df_confusion_matrix)\n\n# train data efficiency\ndf_confusion_matrix_train= pd.concat([df4,df5,df6,df10,df11,df12,df14, input_train], axis=1, sort=True)\n#print(df_confusion_matrix_train)\n\n\n# Model performance vs test data input for each label. \nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['sgd_test_data_image_selu'].values),\n    name='sgd_dat_img_selu',\n    marker_color='indianred'))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['adam_test_data_image_relu'].values),\n    name='adam_dat_img_relu',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['sgd_test_data_image_relu'].values),\n    name='sgd_dat_img_relu',\n    marker_color='DarkSlateGrey'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['sgd_test_image_selu'].values),\n    name='sgd_image_selu',\n    marker_color='MediumPurple'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['adam_test_image_relu'].values),\n    name='adam_image_relu',\n    marker_color='turquoise'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['sgd_test_image_relu'].values),\n    name='sgd_image_relu',\n    marker_color='lightpink'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['CNN_test_image_sgd'].values),\n    name='CNN_image',\n    marker_color='silver'\n))\nfig.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix['Input_test'].values),\n    name='input_test',\n    marker_color='LightSkyBlue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title=\"Model performance vs input test data, label wise\")\nfig.update_yaxes(title= \"True Predictions\")\nfig.update_xaxes(title= 'labels')\nfig.show()","4e0c2b40":"\n# Model performance vs train data input for each label. \nfig1 = go.Figure()\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['sgd_train_data_image_selu'].values),\n    name='sgd_dat_img_selu',\n    marker_color='indianred'))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['adam_train_data_image_relu'].values),\n    name='adam_dat_img_relu',\n    marker_color='lightsalmon'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['sgd_train_data_image_relu'].values),\n    name='sgd_dat_img_relu',\n    marker_color='DarkSlateGrey'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['sgd_train_image_selu'].values),\n    name='sgd_image_selu',\n    marker_color='MediumPurple'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['adam_train_image_relu'].values),\n    name='adam_image_relu',\n    marker_color='turquoise'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['sgd_train_image_relu'].values),\n    name='sgd_image_relu',\n    marker_color='lightpink'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['CNN_train_image_sgd'].values),\n    name='CNN_image',\n    marker_color='silver'\n))\nfig1.add_trace(go.Bar(\n    x=df_confusion_matrix.index,\n    y=list(df_confusion_matrix_train['Input_train'].values),\n    name='input_test',\n    marker_color='LightSkyBlue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig1.update_layout(barmode='group', xaxis_tickangle=-45, title=\"Model performance vs input train data, label wise\")\nfig1.update_yaxes(title= \"True Predictions\")\nfig1.update_xaxes(title= 'labels')\nfig1.show()","893d3b8f":"# References\n1. https:\/\/www.kaggle.com\/debanga\/beginner-s-eda-ferc by Zenify. (Inspired for balanced dataset approach)\n2. https:\/\/www.kaggle.com\/nipunarora8\/starter-age-gender-and-ethnicity-a06b3d59-e by Nipun Arora (converting the pixel data into array)\n3. Wide & Deep Learning for Recommender Systems - Paper by Google Inc team.\n4. https:\/\/www.kaggle.com\/dansbecker\/running-kaggle-kernels-with-a-gpu by DanB (referred for, GPU setting on Kaggle)","728d98fe":"## Image data adam optimizer\/ relu","db317c45":"## CNN model performance","999b6434":"# Model y_true prediction vs input","cc59287b":"## Model Evaluation","62afa50b":"## Image data sgd optimizer\/ selu","ffe1c525":"## Model with image only","0ac96b4a":"Image source Google AI ","3fb99883":"Inportant Findings:- \n1. The ethnicity label '4' has better performance since the data has more familiar data in comparision to other labels. (Under represented)\n2. The ethnicity label '0' has low performance since the data has less familiar data and has high variation (Over representation)\n3. Image only models have better performance incomparision to Wide_Deep models\n4. CNN has high accuracy and performance","57114475":"# Wide and Deep model with SGD optimizer\/ activation = relu","b085c0c4":"## Tabular and Image data","1ec023c1":"The dataset is evaluated using \n1. Tabular Data and Image data\n2. Image only data \n3. CNN model","f54234eb":"# Introduction\n\nThe dataset is analyised for predicting the 'ethnicity' using tabular data (age & gender) along with images. Each model is evaluated for accuracy of prediction and recall, this notebook was prepared  after spending nearly 40 hours. Several of optimisation and hyperparameter tuning codes are not covered in this notebook, while it was performed seperately. \n\n**This Notebook is run on GPU for faster result. 'Please upvote if you like this Notebook'**","08f82bf9":"## Data cleaning and rearranging","dc9a3c6d":"The train data seems to have overfitting issue using CNN and hence the accuracy performance on test set is bit low. (Incomparision to the training set)","6456bbfe":"## Model Selection and Evaulation","ab613e05":"## Image data sgd optimizer\/ relu","e6e06e8d":"## CNN","0d14f579":"The dataset is unbalanced, there is over representation of ethnicity '0' while under representing '4'. This leads to uncertainity in prediction, inorder to minimise the uncertainity an effort of balancing the dataset is done. The labels with larger representations have been sampled to mean size and the under represented ethnicity is duplicated to meet the mean size of the dataset. ","f2b91a6f":"Inportant Findings:- \n1. Overfitting of training data is seen in several models. CNN and Sgd image with relu activation. ","dfa55463":"image_source = 'fi.pinterest.com'","a8ec5819":"# Wide and Deep model with adam optimizer\/ activation = relu","1151d711":"# Wide and Deep model with SGD optimizer\/ activation = selu","83aae63d":"# Classification report for CNN","8009e969":"# Data Preprocessing"}}