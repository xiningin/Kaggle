{"cell_type":{"4d68c9a7":"code","54e0ec12":"code","586e3cb1":"code","83d6b194":"code","73aa25af":"code","1da698a3":"code","928611f2":"code","9310123f":"code","2980bb55":"code","aaee4585":"code","6a1c162c":"code","e100811d":"code","eebea576":"code","fb55050f":"code","d0c8dcf2":"code","d4b7746b":"code","02baa7d3":"code","9aff0bad":"code","1fe1e2c5":"code","f7373951":"code","35e23049":"code","754e5c2b":"code","e2e2ef18":"code","ccae19c8":"code","c91d72bd":"code","a593f603":"code","169009e1":"code","31a5dcae":"code","ce522d0b":"code","3298dd8d":"code","dee837e3":"code","1529a8f8":"code","1de54c3a":"code","88da212f":"code","a35b01fc":"code","181ba17f":"code","662124ed":"code","01edb144":"code","1f665379":"code","e5faad32":"code","a742f610":"code","415ae808":"code","578b9725":"code","5c1b1465":"code","7a7cbd56":"code","2d272f7a":"code","d0f60ffd":"code","f2229fb3":"code","efb96b81":"code","e9d71567":"code","cee02fa1":"code","4afe0cdc":"markdown","634f467b":"markdown","f957b547":"markdown","1d701ca6":"markdown","0936d09a":"markdown","f925b871":"markdown","abb684ee":"markdown","60bf03bf":"markdown","973e8860":"markdown","5bdfe6f4":"markdown","8bba088c":"markdown","dde9532f":"markdown","41e49b8d":"markdown","1e44879e":"markdown","930ad4f9":"markdown","d695cab0":"markdown","d4e4828b":"markdown","a3f6eb5f":"markdown","5b21ea14":"markdown","37edb201":"markdown"},"source":{"4d68c9a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54e0ec12":"df = pd.read_csv('\/kaggle\/input\/joe-biden-tweets\/JoeBidenTweets.csv')\ndf.head()","586e3cb1":"import nltk\nimport re\nimport string\nfrom string import punctuation\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    return text\n\n\ndef punctuation_stopwords_removal(git_text):\n    remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_git_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_git_text","83d6b194":"df['tweet'] = df['tweet'].apply(lambda x: clean_text(x))\ndf['tweet'] = df['tweet'].apply(punctuation_stopwords_removal)\ndf.head()","73aa25af":"# removing optional columns\ndf.drop(['id', 'url'], axis=1, inplace=True)","1da698a3":"import plotly.express as px\nfrom collections import Counter\n\ndef plot_most_common_terms(df):\n    word_list = []\n    \n    for i, j in df.iterrows():\n        for word in j['tweet']:\n            word_list.append(word)\n        \n    count_dict = Counter(word_list)\n    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n    \n    fig = px.histogram(most_common_words_df,\n                       x='word', \n                       y='count',\n                       title='Most common terms used in Joe Biden\\'s tweets.',\n                       color_discrete_sequence=['#D8E46B'] )\n    fig.show()","928611f2":"plot_most_common_terms(df)","9310123f":"import cv2\nimport urllib\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\n\ndef load_mask(mask_url):\n    with urllib.request.urlopen(mask_url) as resp:\n        mask = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        mask = cv2.imdecode(mask, cv2.IMREAD_COLOR)\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        \n    return mask","2980bb55":"new_mask = load_mask('https:\/\/www.creativefreedom.co.uk\/wp-content\/uploads\/2017\/06\/Twitter-featured.png')\n\nwordcloud = WordCloud(\n    background_color=\"white\",\n    mask=new_mask,\n    random_state=42,\n    max_font_size=50,\n    max_words=1000,\n)\n\ndf_tweet = pd.read_csv('\/kaggle\/input\/joe-biden-tweets\/JoeBidenTweets.csv')\ntext = ''.join(df_tweet['tweet'].values)\n\n\nwordcloud.generate(text) \n\nimage_colors = ImageColorGenerator(new_mask)\n\nplt.figure(figsize=(16, 8))\nplt.imshow(wordcloud.recolor(color_func=image_colors))\nplt.axis(\"off\")\nplt.show()","aaee4585":"import datetime\n\ndf['timestamp'] = df['timestamp'].apply(lambda x : datetime.datetime.strptime(x.split(\" \")[0], \"%Y-%m-%d\"))\ndf.head()","6a1c162c":"df_count = pd.DataFrame()\n\ndf_count['replies'] = df.groupby(['timestamp'])['replies'].sum()\ndf_count['retweets'] = df.groupby(['timestamp'])['retweets'].sum()\ndf_count['quotes'] = df.groupby(['timestamp'])['quotes'].sum()\ndf_count['likes'] = df.groupby(['timestamp'])['likes'].sum()\ndf_final = df_count.reset_index()\ndf_count.head()","e100811d":"import plotly.graph_objects as go\n\ndef plot_frequency_of_tweet_properties(df):\n    # Create traces\n    fig = go.Figure()\n    \n    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['replies'],\n                        mode='lines',\n                        name='Replies'))\n    \n    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['retweets'],\n                        mode='lines',\n                        name='Re-tweets'))\n    \n    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['quotes'],\n                        mode='lines',\n                        name='Quotes'))\n    \n    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['likes'],\n                        mode='lines',\n                        name='Likes'))\n\n    fig.show()","eebea576":"plot_frequency_of_tweet_properties(df_final)","fb55050f":"df_debate_tweets = pd.read_csv('\/kaggle\/input\/first-gop-debate-twitter-sentiment\/Sentiment.csv')\ndf_debate = pd.DataFrame()\ndf_debate['tweet'] = df_debate_tweets['text']\ndf_debate['sentiment'] = df_debate_tweets['sentiment']\n\ndf_debate['tweet'] = df_debate['tweet'].apply(lambda x: clean_text(x))\ndf_debate['tweet'] = df_debate['tweet'].apply(lambda x: x.replace(\"rt\", \"\"))\n\ndf_debate.head()","d0c8dcf2":"def encode_labels(df):\n    for i, j in df_debate.iterrows():\n        if j['sentiment']=='Negative':\n            j['sentiment']=0\n        elif j['sentiment']=='Positive':\n            j['sentiment']=1\n        elif j['sentiment']=='Neutral':\n            j['sentiment']=2\n            \n    return df_debate","d4b7746b":"df_debate = encode_labels(df_debate)","02baa7d3":"df_debate.head()","9aff0bad":"import torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","1fe1e2c5":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","f7373951":"#loading our BERT model\nBERT_UNCASED = '\/kaggle\/input\/bert-base-uncased'","35e23049":"#loading the pre-trained BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(BERT_UNCASED)","754e5c2b":"# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\nsample_body = 'danscavino gopdebate w realdonaldtrump delivered the highest ratings in the history of presidential debates'\ntokens = tokenizer.tokenize(sample_body)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_body}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","e2e2ef18":"# using encode_plus to add special tokens : [CLS]:101, [SEP]:102, [PAD]:0\nencodings = tokenizer.encode_plus(\n            sample_body,\n            max_length=32,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n)\n\nencodings.keys()","ccae19c8":"print('Input IDs : {}'.format(encodings['input_ids'][0]))\nprint('\\nAttention Mask : {}'.format(encodings['attention_mask'][0]))","c91d72bd":"# setting maximum length of tweets\nMAX_LENGTH = 150","a593f603":"class Tweets(Dataset):\n    \n    def __init__(self, tweet, label, tokenizer, max_len):\n        self.tweet = tweet\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        tweet = str(self.tweet[item])\n        label = self.label[item]\n        \n        encoding = self.tokenizer.encode_plus(\n        tweet,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'tweet': tweet,\n         'input_ids': encoding['input_ids'],\n         'attention_mask': encoding['attention_mask'],\n         'label': torch.tensor(label, dtype=torch.long)\n          }","169009e1":"from sklearn.model_selection import train_test_split\n\ntraining_data, testing_data = train_test_split(\n    df_debate,\n    test_size=0.1,\n    random_state=RANDOM_SEED\n)\n\ntesting_data, validation_data = train_test_split(\n    testing_data,\n    test_size=0.5,\n    random_state=RANDOM_SEED\n)","31a5dcae":"training_data.head()","ce522d0b":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = Tweets(tweet=data.tweet.to_numpy(),\n    label=data.sentiment.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    \n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\n\n\nBATCH_SIZE = 64\ntrain_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\ntesting_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\nval_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)","3298dd8d":"df = next(iter(train_data_loader))\ndf.keys()","dee837e3":"df['input_ids'].squeeze().shape, df['attention_mask'].squeeze().shape, df['label'].shape","1529a8f8":"print('tweet  : ', df['tweet'][0])\nprint('input_ids : ', df['input_ids'].squeeze()[0])\nprint('attention_mask : ', df['attention_mask'].squeeze()[0])\nprint('label : ', df['label'][0])","1de54c3a":"bert_model = BertModel.from_pretrained(BERT_UNCASED)","88da212f":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encodings['input_ids'],\n  attention_mask=encodings['attention_mask']\n)","a35b01fc":"last_hidden_state.shape, pooled_output.shape","181ba17f":"class SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert_model = BertModel.from_pretrained(BERT_UNCASED)\n        self.dropout = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert_model(\n        input_ids=input_ids,\n        attention_mask = attention_mask\n        )\n        output = self.dropout(pooled_output)\n        return self.out(output)","662124ed":"\"\"\"\nlabel 0: Negative\nlabel 1: Positive\nlabel 2: Neutral\n\"\"\"\nclass_names = [0, 1, 2]\nsentiment_classifier = SentimentClassifier(len(class_names))\nsentiment_classifier = sentiment_classifier.to(device)","01edb144":"EPOCHS = 10\n\noptimizer = AdamW(sentiment_classifier.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","1f665379":"def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].squeeze().to(device)\n        attention_mask = d['attention_mask'].squeeze().to(device)\n        targets = d['label'].to(device)\n\n        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","e5faad32":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].squeeze().to(device)\n            attention_mask = d['attention_mask'].squeeze().to(device)\n            targets = d['label'].to(device)\n\n            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","a742f610":"%%time\nfrom collections import defaultdict\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}\/{}'.format(epoch+1,EPOCHS))\n    print('-' * 10)\n    \n    train_acc, train_loss = train_model(sentiment_classifier, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n    \n    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n    \n    val_acc, val_loss = eval_model(sentiment_classifier, val_data_loader, loss_fn, device, len(validation_data))\n    \n    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        print('Saving the best model ...')\n        torch.save(sentiment_classifier.state_dict(), 'best_model.bin')\n        best_accuracy = val_acc\n    ","415ae808":"testing_data.shape","578b9725":"def predict_sentiment_category(tweet, model):\n    class_names = ['negative', 'positive', 'neutral']\n    encoded_message = tokenizer.encode_plus(tweet, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n    input_ids = encoded_message['input_ids'].to(device)\n    attention_mask = encoded_message['attention_mask'].to(device)\n    \n    output = model(input_ids=input_ids, attention_mask=attention_mask)\n    _, prediction_idx = torch.max(output, dim=1)\n        \n    return (prediction_idx, class_names[prediction_idx])","5c1b1465":"tweet = testing_data['tweet'][3448]\nprint('Sample tweet from Joe Biden : ', tweet)\nprint('Predicted sentiment category : ', predict_sentiment_category(tweet, sentiment_classifier)[1])","7a7cbd56":"df = pd.read_csv('\/kaggle\/input\/joe-biden-tweets\/JoeBidenTweets.csv')\ndf.head()","2d272f7a":"df['tweet'] = df['tweet'].apply(lambda x: clean_text(x))","d0f60ffd":"joe_sentiment = []\nfor i, j in df.iterrows():\n    joe_sentiment.append(predict_sentiment_category(j['tweet'], sentiment_classifier)[0].cpu().numpy().data[0])","f2229fb3":"df['sentiment']= joe_sentiment\ndf.head()","efb96b81":"df_joe_negative = df[df['sentiment']==0]\ndf_joe_positive = df[df['sentiment']==1]\ndf_joe_neutral = df[df['sentiment']==2]","e9d71567":"def plot_word_cloud(df_tweet, colormap, category):\n    new_mask = load_mask('https:\/\/www.creativefreedom.co.uk\/wp-content\/uploads\/2017\/06\/Twitter-featured.png')\n\n    wordcloud = WordCloud(\n        background_color=\"white\",\n        colormap=colormap,\n        mask=new_mask,\n        random_state=42,\n        max_font_size=50,\n        max_words=1000,\n    )\n\n    text = ''.join(df_tweet['tweet'].values)\n\n\n    wordcloud.generate(text) \n\n    image_colors = ImageColorGenerator(new_mask)\n\n    plt.figure(figsize=(16, 8))\n    plt.title('Word cloud for {} tweets'.format(category))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","cee02fa1":"from matplotlib.colors import LinearSegmentedColormap\ncolors_pos = [\"#88D969\", \"#1D800E\"]\ncmap_pos = LinearSegmentedColormap.from_list(\"mycmapos\", colors_pos)\n\ncolors_neg = [\"#ffbaba\", \"#ff0000\"]\ncmap_neg = LinearSegmentedColormap.from_list(\"mycmapos\", colors_neg)\n\ncolors_neutral = [\"#ffecb3\", \"#ffca28\"]\ncmap_neutral = LinearSegmentedColormap.from_list(\"mycmapos\", colors_neutral)\n\n\n\n\nplot_word_cloud(df_joe_positive, cmap_pos, 'positive')\nplot_word_cloud(df_joe_negative, cmap_neg, 'negative')\nplot_word_cloud(df_joe_neutral, cmap_neutral, 'neutral')","4afe0cdc":"<h2 style=\"text-align:center; background-color:#D8E46B\">Creating data loaders<\/h2>","634f467b":"<h2 style=\"text-align:center; background-color:#D8E46B\">Importing Libraries<\/h2>","f957b547":"1. https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/\n2. https:\/\/huggingface.co\/transformers\/\n3. http:\/\/jalammar.github.io\n4. Word cloud reference : https:\/\/www.kaggle.com\/ihelon\/ukrainian-descriptions-word-cloud-tutorial","1d701ca6":"In order to classify tweets from Joe Biden, I would be leveraging a BERT based model. In order to train the model, I will be using [First GOP Debate Twitter Sentiment](https:\/\/www.kaggle.com\/crowdflower\/first-gop-debate-twitter-sentiment) which contains sentiment value (positive, negative and neutral) as well s the tweet.","0936d09a":"<h1 style=\"text-align:center; background-color:#D8E46B\">Word cloud of positive, negative and neutral Joe Biden's tweets<\/h1>","f925b871":"<h2 style=\"text-align:center; background-color:#D8E46B\">Results and Evaluation<\/h2>","abb684ee":"<h1 style=\"text-align:center; background-color:#D8E46B\">Exploratory Data Analysis<\/h1>","60bf03bf":"In this notebook, I have tried to perform sentiment analysis on Joe Biden's tweets using a BERT-based model. This model is trained on [\"First GOP Debate Twitter Sentiment\"](https:\/\/www.kaggle.com\/crowdflower\/first-gop-debate-twitter-sentiment). After which, the trained model is used to classify Joe Biden's tweets into three categories : *positive, negative and neutral*. The rest of the notebook is organized as follows :\n\n1. Exploratory Data Analysis\n    * Most common words employed in Joe Biden's tweets\n    * Twitter word cloud of most commons terms\n    * Analyze how the frequency of *likes, retweets, replies and quotes* vary with time.\n\n2. Sentiment Analysis on Tweets\n    * Importing libraries\n    * Loading BERT model and BERT tokenizer\n    * Sample tweets\n    * Creating data loaders\n    * Sentiment classifier model\n    * Mode training\n    * Results and evaluations\n  \n3. Final dataset that contains sentiment classification of Joe Biden's tweets.\n4. Word clouds for positive, negative and neutral tweets from Joe Biden.\n5. Future work\n6. References","973e8860":"<h2 style=\"text-align:center; background-color:#D8E46B\">Model training<\/h2>","5bdfe6f4":"<h1 style=\"text-align:center; background-color:#D8E46B\">Final dataset containing Joe Biden's Tweets and the sentiment classification<\/h1>","8bba088c":"<h1 style=\"text-align:center; background-color:#D8E46B\">Future Work<\/h1>","dde9532f":"<h2 style=\"text-align:center; background-color:#D8E46B\">Class for tweets dataset<\/h2>","41e49b8d":"So our BERT model has achieved 93.7% accuracy on training dataset and 74.0% accuracy on validation dataset. This was achieved after training our BERT model on \"GOP debate\" dataset. Now, we will use this trained model to classify tweets from Joe Biden into 3 categories : *positive, negative and neutral*.","1e44879e":"<h1 style=\"text-align:center; background-color:#C8D741\">Deep Learning Model for Performing Sentiment Analysis of Tweets from Joe Biden<\/h1>","930ad4f9":"<h2 style=\"text-align:center; background-color:#D8E46B\">Loading BERT model and BERT tokenizer<\/h2>","d695cab0":"<h1 style=\"text-align:center; background-color:#D8E46B\">References<\/h1>","d4e4828b":"<h2 style=\"text-align:center; background-color:#D8E46B\">Sentiment Classifier Model<\/h2>","a3f6eb5f":"<h2 style=\"text-align:center; background-color:#D8E46B\">Sample Tweet<\/h2>","5b21ea14":"1. I still need to employ BERT large model to test its accuracy on GitHub message classification use-case.\n2. The dataset provided has rich textual information. This can be leveraged for other NLP tasks such as natural language generation.","37edb201":"<h1 style=\"text-align:center; background-color:#D8E46B\">Sentiment Analysis on Tweets<\/h1>"}}