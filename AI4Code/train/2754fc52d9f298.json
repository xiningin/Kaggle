{"cell_type":{"7e1d46b9":"code","5c4529fe":"code","b66d7ffe":"code","7b97c621":"code","ff725994":"code","a2989604":"code","f66b5f88":"code","c3f429b9":"code","df8edf1f":"code","d1925fd1":"code","1419ba1e":"code","227004b6":"code","fb0d6205":"code","245ea3f9":"code","a641b88d":"code","6f0d47f2":"markdown","13599318":"markdown","ba1c0f36":"markdown","b97439a7":"markdown","dcba5a62":"markdown","37f10011":"markdown"},"source":{"7e1d46b9":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import naive_bayes, feature_extraction, metrics, model_selection\nimport pandas as pd\nfrom collections import Counter\n\n%matplotlib inline","5c4529fe":"data = pd.read_csv('..\/input\/spam.csv', encoding='latin-1')\ndata.head(n=10)","b66d7ffe":"count_c = pd.value_counts(data[\"v1\"], sort = True)\ncount_c.plot(kind = 'bar', color = ['blue', 'orange'])\nplt.show()","7b97c621":"count_c.plot(kind = 'pie', autopct = '%1.0f%%')\nplt.show()","ff725994":"count1 = Counter(\" \".join(data[data['v1']=='ham']['v2']).split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0:'words in non-spam', 1:'count'})\ncount2 = Counter(\" \".join(data[data['v1']=='spam']['v2']).split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0:'words in spam',1: 'count'})","a2989604":"df1.plot.bar(legend = False)\ny_pos = np.arange(len(df1[\"words in non-spam\"]))\nplt.xticks(y_pos, df1[\"words in non-spam\"])\nplt.title('More frequent words in non-spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","f66b5f88":"df2.plot.bar(legend = False, color= 'orange')\ny_pos = np.arange(len(df2[\"words in spam\"]))\nplt.xticks(y_pos, df2[\"words in spam\"])\nplt.title('More frequent words in spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","c3f429b9":"f = feature_extraction.text.CountVectorizer(stop_words = 'english')\nX = f.fit_transform(data[\"v2\"])\nnp.shape(X)","df8edf1f":"data['v1'] = data['v1'].map({'ham':0,'spam':1})\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['v1'], test_size = 0.33, random_state = 42)\nprint([np.shape(X_train),np.shape(X_test)])","d1925fd1":"list_alpha = np.arange(1\/100000, 20, 0.11)\nscore_train = np.zeros(len(list_alpha))\nscore_test = np.zeros(len(list_alpha))\nrecall_test = np.zeros(len(list_alpha))\nprecision_test= np.zeros(len(list_alpha))\ncount = 0\nfor alpha in list_alpha:\n    bayes = naive_bayes.MultinomialNB(alpha=alpha)\n    bayes.fit(X_train, y_train)\n    score_train[count] = bayes.score(X_train, y_train)\n    score_test[count]= bayes.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n    count = count + 1 ","1419ba1e":"mat = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\ndata1 = pd.DataFrame(data = mat, columns = ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\ndata1.head(10)","227004b6":"best_index = data1['Test Precision'].idxmax()\ndata1.iloc[best_index, :]","fb0d6205":"data1[data1['Test Precision']==1].head(n=5)","245ea3f9":"best_index = data1[data1['Test Precision']==1]['Test Accuracy'].idxmax()\nbayes = naive_bayes.MultinomialNB(alpha=list_alpha[best_index])\nbayes.fit(X_train, y_train)\ndata1.iloc[best_index, :]","a641b88d":"m_confusion_test = metrics.confusion_matrix(y_test, bayes.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])","6f0d47f2":"It has been observed that multinomial naive bayes seems to work more effectively than bernoulli naive bayes.","13599318":"After the words in each of the respective classes has been found, the words are to be loaded in as features for the model that we build.","ba1c0f36":"Loading the dataset and observing its content","b97439a7":" Importing Libraries","dcba5a62":"Here the dataset is visualized using the plot function as a bar as well as a pie diagram","37f10011":"The analysed dataset is then has to be further analysed for its content. The words that most commonly occur in non-spam messages and spam messages. These words are extracted and put into a Data Frame format for easy viewing. The most_common function is used to remove the stop words that what so evr has no significance in telling the class to which the message belong to."}}