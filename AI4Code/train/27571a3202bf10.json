{"cell_type":{"d4696d18":"code","d3187b06":"code","b392c635":"code","90cdc44e":"code","45c0fe71":"code","828cd8c1":"code","e10cb538":"code","c6b56cff":"code","db05a0af":"code","4922701b":"code","dc99e14e":"code","b16614d3":"code","ea6f02ea":"code","6b35a929":"code","ccf02a4a":"code","80ba074c":"code","055c7b7c":"code","182980a3":"code","4bc85b0f":"code","3fb6a211":"code","da4601b0":"code","d17bd8e1":"code","9ee1b368":"code","78f0fa44":"markdown","76cb8bc4":"markdown","9051ecb8":"markdown","92519b18":"markdown","43ea6c39":"markdown","8cb2029f":"markdown","580290c7":"markdown","649402df":"markdown","630797c3":"markdown","11bdb4cf":"markdown","7a5e06c4":"markdown","9b3a8f4f":"markdown","7c1bd4a3":"markdown","6bcf6cdf":"markdown","7aadac75":"markdown","5ae55627":"markdown","2563d8c8":"markdown","f2a132fa":"markdown","ca10bddb":"markdown","005c0a94":"markdown","3a03a46b":"markdown","36bfce64":"markdown","7b3dd86c":"markdown","dfb6a82b":"markdown","63e7bf1a":"markdown","0385fe6c":"markdown"},"source":{"d4696d18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder # scaling data for pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer # apply scaling to dataset\nfrom sklearn.pipeline import make_pipeline # create pipeline for data processing\nfrom sklearn.model_selection import train_test_split # train\/test split data\nfrom sklearn.linear_model import LogisticRegression # classificatino model\nfrom sklearn.ensemble import RandomForestClassifier # classification model\nfrom sklearn.cluster import KMeans # calculate Kmeans clustering w\/num_clusters determined by elbow method\nfrom sklearn.metrics import confusion_matrix # verify model accuracy metrics\nimport sklearn.metrics as metrics # accuracy\/precision\/recall\/f1\nimport matplotlib # plotting\nfrom sklearn.metrics import roc_curve, auc # roc\/auc curve visualization\nfrom matplotlib import pyplot # plotting\nfrom sklearn.model_selection import cross_val_score # cross validation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d3187b06":"dataset = pd.read_csv('\/kaggle\/input\/video-game-sales-with-ratings\/Video_Games_Sales_as_at_22_Dec_2016.csv')\ndataset.head()","b392c635":"# Bin NA_Sales into Blockbuster or NB based on units sold\nBlockBuster = dataset.loc[:, \"Global_Sales\"] > 1.0\n#dataset.loc[:,\"Global_Sales\"] = dataset.loc[:,\"Global_Sales\"].astype(int)\ndataset.loc[BlockBuster, \"Global_Sales\"] = 1\ndataset.loc[~BlockBuster, \"Global_Sales\"] = 0\n# Drop nulls\nnullRemoved = dataset.dropna(axis=0) #6825 observations with no nulls\n# Convert to int\nnullRemoved = nullRemoved.astype({'User_Score':float})","90cdc44e":"from tabulate import tabulate\ntabulate(nullRemoved.info(), headers='keys', tablefmt='psql')","45c0fe71":"nullRemoved.loc[nullRemoved.loc[:, \"Rating\"] == \"AO\", \"Rating\"] = \"M\"\nnullRemoved.loc[nullRemoved.loc[:, \"Rating\"] == \"K-A\", \"Rating\"] = \"E\"\nnullRemoved.loc[nullRemoved.loc[:, \"Rating\"] == \"RP\", \"Rating\"] = \"T\"\nnullRemoved.loc[nullRemoved.loc[:, \"Rating\"] == \"E10+\", \"Rating\"] = \"E\"\n","828cd8c1":"target =  nullRemoved[['Global_Sales']].values #y\nfeatures = nullRemoved[['Platform','Genre','Critic_Score','Critic_Count','User_Score','User_Count','Rating']] #X\n","e10cb538":"preprocess = make_column_transformer(\n    (StandardScaler(), ['Critic_Score', 'Critic_Count','User_Score','User_Count' ]), #features scale\n    (OneHotEncoder(), ['Platform', 'Genre', 'Rating']) #OHE categorical features\n)","c6b56cff":"kmeans_features = preprocess.fit_transform(features).toarray()","db05a0af":"wcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(kmeans_features)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()  #4 is ideal number of clusters\n","4922701b":"X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)\n","dc99e14e":"kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 69)\ny_kmeans = kmeans.fit_predict(kmeans_features)\n","b16614d3":"features = features.assign(Kmeans_labels=pd.Series(y_kmeans, index=features.index))\n","ea6f02ea":"X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)\n","6b35a929":"model = make_pipeline(\n    preprocess,\n    LogisticRegression(solver='lbfgs',penalty='l2'))\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_test)\nprint(\"Logistic Regression Score: %f\" % model.score(X_test, y_test))","ccf02a4a":"BothProbabilities = model.predict_proba(features)\nprobabilities = BothProbabilities[:,1]\n","80ba074c":"model2 = make_pipeline(\n        preprocess,\n        RandomForestClassifier(n_estimators=100))\nmodel2.fit(X_train, y_train.ravel())\ny_pred2 = model2.predict(X_test)\nprint(\"\\nRandom Forest Score: %f\" % model2.score(X_test, y_test)) ","055c7b7c":"BothProbabilities2 = model2.predict_proba(features)\nprobabilities2 = BothProbabilities2[:,1]\n","182980a3":"cm = confusion_matrix(y_test, y_pred)\nprint ('\\nLogistic Regression Confusion Matrix and Metrics')\nThreshold = 0.3 # Some number between 0 and 1\nprint (\"\\nProbability Threshold is chosen to be:\", Threshold)\npredictions = (probabilities > Threshold).astype(int)\ntn, fp, fn, tp = cm.ravel()\nprint (\"\\nTP, TN, FP, FN:\", tp, \",\", tn, \",\", fp, \",\", fn)\nprint(\"\\nAccuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"\\nPrecision:\",metrics.precision_score(y_test, y_pred))\nprint(\"\\nRecall:\",metrics.recall_score(y_test, y_pred))\nprint(\"\\nF1:\",metrics.f1_score(y_test, y_pred))\nprint(\"\\nAverage precision-recall score:\",metrics.average_precision_score(y_test, y_pred))\n","4bc85b0f":"cm2 = confusion_matrix(y_test, y_pred2)\nprint ('\\nRandom Forest Confusion Matrix and Metrics')\nThreshold = 0.01 # Some number between 0 and 1\nprint (\"Probability Threshold is chosen to be:\", Threshold)\npredictions = (probabilities2 > Threshold).astype(int)\ntn, fp, fn, tp = cm2.ravel()\nprint (\"TP, TN, FP, FN:\", tp, \",\", tn, \",\", fp, \",\", fn)\nprint(\"\\nAccuracy:\",metrics.accuracy_score(y_test, y_pred2))\nprint(\"\\nPrecision:\",metrics.precision_score(y_test, y_pred2))\nprint(\"\\nRecall:\",metrics.recall_score(y_test, y_pred2))\nprint(\"\\nF1:\",metrics.f1_score(y_test, y_pred2))\nprint(\"\\nAverage precision-recall score:\",metrics.average_precision_score(y_test, y_pred2))\n","3fb6a211":"fpr, tpr, th = roc_curve(target, probabilities)\nAUC = auc(fpr, tpr)\n\nplt.rcParams[\"figure.figsize\"] = [8, 8] # Square\nfont = {'weight' : 'bold', 'size' : 18}\nmatplotlib.rc('font', **font)\nplt.figure()\nplt.title('Logistic Regression ROC Curve')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.plot(fpr, tpr, LW=3, label='ROC curve (AUC = %0.2f)' % AUC)\nplt.plot([0, 1], [0, 1], color='navy', LW=3, linestyle='--') # reference line for random classifier\nplt.legend(loc=\"lower right\")\nplt.show() #ideal threshold is around 0.3\n","da4601b0":"fpr, tpr, th = roc_curve(target, probabilities2)\nAUC = auc(fpr, tpr)\n\nplt.rcParams[\"figure.figsize\"] = [8, 8] # Square\nfont = {'weight' : 'bold', 'size' : 18}\nmatplotlib.rc('font', **font)\nplt.figure()\nplt.title('Random Forest ROC Curve')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.plot(fpr, tpr, LW=3, label='ROC curve (AUC = %0.2f)' % AUC)\nplt.plot([0, 1], [0, 1], color='navy', LW=3, linestyle='--') # reference line for random classifier\nplt.legend(loc=\"lower right\")\nplt.show() #ideal threshold is around 0.1","d17bd8e1":"X_train2  = preprocess.fit_transform(X_train).toarray()\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='lbfgs')))\nmodels.append(('RC', RidgeClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RF', RandomForestClassifier(n_estimators=100)))\n# evaluate each model in turn\nresults = []\nnames = []\nout = []\n# need to use preprocess to scale and encode X_train and y_train\nfor name, model in models:\n    #model = preprocess.fit_transform(features).toarray()\n    kfold = StratifiedKFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_train2, y_train.ravel(), cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n","9ee1b368":"plt.figure() #created empty frame for plt\npyplot.boxplot(results, labels=names)\n#pyplot.subplot()\npyplot.title('Algorithm Comparison')\npyplot.xlabel('Models')\npyplot.ylabel('Accuracy')\n#plt.xlim(right=20)\n#plt.subplot(888)\npyplot.show()","78f0fa44":"Check data shape and dtype, especially of converted User_Score attribute","76cb8bc4":"Train\/test split","9051ecb8":"The target is highly non-linear so I made this a classification problem: \ncan I predict if a game will be a 'blockbuster', defined as over 1 million units \nsold globlally?","92519b18":"Add Kmeans labels to features df","43ea6c39":"The dataset is the Video Game Sales with Ratings dataset from Kaggle. Initially\nthere are 16 attributes and 16719 observations, but after data preparation and \nEDA there are 6825 observations. I use 9 attributes in my model as features: \n'Global_Sales' is the target and Kmeans is imputed and the label is added as a \nfeature, with all features in the Kmeans model undergoing normalization and \nencoding because of the different scales and data types. I use the elbow method\nto determine the ideal number of clusters for Kmeans. ","8cb2029f":"Group like categories: AO -> M, RP -> T, K-A\/E10+ -> E","580290c7":"Make Random Forest Classification model","649402df":"Define target and featuress for model using domain knowledge of likely significant attributes related to sales","630797c3":"RF ROC curve","11bdb4cf":"Train\/test split with KMEANS attribute added","7a5e06c4":"Make pipeline that scales and encodes features","9b3a8f4f":"Compare Algorithms","7c1bd4a3":"Using domain knowledge, I choose the features, train\/test split the data and created\na pipeline to scale and encode them before going into the models. Kmeans is used \non my features (3 categorical, 4 numeric) to identify clustering within the data. \nThis label is added as a imputed feature to my model features. I use Logistic \nRegression and Random Forrest Classification for my Supervised models. I print \nthe confusion matrices, ROC curves, and metrics for both models.","6bcf6cdf":"Based upon the ROC curves for each model I tuned the Threshold value to the point\non the curve closest to the upper-left corner. The Random Forest Classifier is more\naccurate than the Logistic Regression model when it comes to identifying blockbuster\nvideogames. The ideal number of clusters for Kmeans was 4. Both models are highly\naccurate, but the recall and F1 rates are much much better for my Random Forest\nmodel. I conclude that I can can classify games based on whether they will sell \none million global units with decent accuracy. My model of models shows that I have\nused two of the highest accuracy classifiers.","7aadac75":"ProbaProbabiliities for thresholdbiliities for threshold","5ae55627":"Make Logistic Regression Classification model","2563d8c8":"Use elbow method to determine ideal number of clusters\n","f2a132fa":"Attributes\n\nName               6825 non-null str,\nPlatform           6825 non-null str     Feature OHE Kmeans,\nYear_of_Release    6825 non-null float64,\nGenre              6825 non-null str     Feature OHE Kmeans,\nPublisher          6825 non-null str,\nNA_Sales           6825 non-null float64,\nEU_Sales           6825 non-null float64,\nJP_Sales           6825 non-null float64,\nOther_Sales        6825 non-null float64,\nGlobal_Sales       6825 non-null float64 Target  OHE Kmeans \"Binned into 2 classes, non-linear distribution\",\nCritic_Score       6825 non-null float64 Feature StandardScaled Kmeans,\nCritic_Count       6825 non-null float64 Feature StandardScaled Kmeans,\nUser_Score         6825 non-null float64 Feature StandardScaled Kmeans,\nUser_Count         6825 non-null float64 Feature StandardScaled Kmeans,\nDeveloper          6825 non-null str,\nRating             6825 non-null str     Feature OHE Kmeans \"Binned into 3 classes, non-uniform distribution\",\nKmeans_labels      6825 non-null int32   Feature n_clusers=4,\n","ca10bddb":"Fitting K-Means to the dataset","005c0a94":"Spot check algorithm performance against other models.","3a03a46b":"Define dataset and check that it is well imported","36bfce64":"Make confusion matrices","7b3dd86c":"Scale data for use in Kmeans","dfb6a82b":"Bin Global_Sales into Blockbuster based on selling over 1 million units --> drop all rows with NAs (6825 remain)","63e7bf1a":"Probabiliities for threshold","0385fe6c":"LR ROC curve"}}