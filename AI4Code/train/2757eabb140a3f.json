{"cell_type":{"8be2a167":"code","418a72e7":"code","a7a50118":"code","976a6e93":"code","7030dba9":"code","ebb7154d":"code","f4c70fb1":"code","1be0cbf3":"code","a5c71168":"code","c6b81bfa":"code","d75e3772":"code","5b276e2e":"code","842eb9a7":"code","9d85ce46":"code","c9dedaec":"code","591c6c4a":"markdown","972ebb29":"markdown","3b466165":"markdown","ceef7c64":"markdown","1d9d8dd9":"markdown","25a2fe2d":"markdown"},"source":{"8be2a167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# we need to fit model with sequence of tokens with specific length\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\n# normal LSTM\/GRU and the Version with Cuda\nfrom keras.layers import Dense, Embedding, GRU, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam, rmsprop\n\n# keras wrapper for k-fold cross-validation\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# normsl cross validation\nfrom sklearn.model_selection import cross_val_score, train_test_split\n# cross validation for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","418a72e7":"x_raw = []\ny_raw = []\n\nwith open(\"..\/input\/spam.csv\", encoding = \"ISO-8859-1\") as f:\n    for line in f:\n        y_raw.append(line.split()[0])\n        x_raw.append(' '.join(i for i in line.split()[1:]))","a7a50118":"y = [1 if i=='ham' else 0 for i in y_raw]\n\nprint(max(len(s) for s in x_raw))\nprint(min(len(s)for s in x_raw))\nsorted_X = sorted(len(s) for s in x_raw)\nprint(sorted_X[len(sorted_X) \/\/ 2])","976a6e93":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_raw)\nsequences = tokenizer.texts_to_sequences(x_raw)\n\nvocab_size = len(tokenizer.word_index)+1\nprint(vocab_size)","7030dba9":"# divide sum of length of all sequences by number of all sequences to find averge length of each sequence\nsum([len(x) for x in sequences]) \/\/ len(sequences)","ebb7154d":"pad = 'post' \nmax_len = 25\nembedding_size = 100\nbatch_size = 20\nsequences = pad_sequences(sequences, maxlen=max_len, padding=pad, truncating=pad)\nsequences.shape\n\nX_train, X_test, y_train, y_test = train_test_split(sequences, y, test_size = 0.2, random_state= 0)","f4c70fb1":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nmodel.add(Dropout(0.8))\nmodel.add(LSTM(140, return_sequences=False))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(1, activation='sigmoid', name='Classification'))\nmodel.summary()","1be0cbf3":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#save_best = ModelCheckpoint('SpamDetection.hdf', save_best_only=True, monitor='val_acc', mode='max')\n# callback_early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)","a5c71168":"# Uses Automatic Verification Datasets (fastest option)\n# model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.1, callbacks=[callback_early_stopping])\nn_epochs = 10\nresults = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n","c6b81bfa":"# model.load_weights(filepath='SpamDetection.hdf')\neval_ = model.evaluate(X_test, y_test)\nprint(eval_[0], eval_[1]) # loss \/ accuracy","d75e3772":"def plot_model(result):\n    acc = result.history['acc']\n    val_acc = result.history['val_acc']\n    loss = result.history['loss']\n    val_loss = result.history['val_loss']\n    x = range(1, len(acc)+1)\n    \n    plt.figure(figsize=(12, 5))\n    plt.subplot(1,2,1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label= 'Validation acc')\n    plt.legend()\n    \n    plt.subplot(1,2,2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='validation loss')\n    plt.legend()\n    \nplot_model(results)","5b276e2e":"model1 = Sequential()\nmodel1.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nmodel1.add(Dropout(0.8))\nmodel1.add(GRU(140, return_sequences=False))\nmodel1.add(Dropout(0.86))\nmodel1.add(Dense(1, activation='sigmoid', name='Classification'))\nmodel1.summary()\n\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nresults1 = model1.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2)\n\neval_ = model1.evaluate(X_test, y_test)\nprint(eval_[0], eval_[1]) # loss \/ accuracy\n\nplot_model(results1)","842eb9a7":"model2 = Sequential()\nmodel2.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nmodel2.add(Dropout(0.8))\nmodel2.add(Bidirectional(LSTM(140, return_sequences=False)))\nmodel2.add(Dropout(0.8))\nmodel2.add(Dense(1, activation='sigmoid', name='Classification'))\nmodel2.summary()\n\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nresults2 = model2.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2)\n\neval_ = model2.evaluate(X_test, y_test)\nprint(eval_[0], eval_[1]) # loss \/ accuracy\n\nplot_model(results2)","9d85ce46":"from keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n\nn_epochs = 10\nmodel3 = Sequential()\nmodel3.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nmodel3.add(Conv1D(128, 3, activation='relu'))\nmodel3.add(MaxPool1D(3))\nmodel3.add(Dropout(0.2))\nmodel3.add(Conv1D(128, 3, activation='relu'))\nmodel3.add(GlobalMaxPooling1D())\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(32, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.summary()\nmodel3.add(Dense(1, activation='sigmoid'))\n\n\nmodel3.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nresult3 = model3.fit(X_train, y_train, batch_size = batch_size, epochs=n_epochs, validation_split=0.2, verbose=1)\neval_ = model3.evaluate(X_test, y_test)\nprint(eval_[0], eval_[1]) # loss \/ accuracy\n\nplot_model(result3)","c9dedaec":"# word2vec = {}\n# f = open('glove.6B\/glove.6B.100d.txt', encoding=\"utf-8\")\n# for line in f:\n#     values = line.split()\n#     word = values[0]\n#     vec = np.asarray(values[1:], dtype='float32')\n#     word2vec[word] = vec\n    \n# print(len(word2vec))\n\n# embedding_matrix = np.zeros((vocab_size, embedding_size))\n\n# for word,i in  tokenizer.word_index.items():\n#     if i < vocab_size:\n#         embedding_vector = word2vec.get(word)\n#         if embedding_vector is not None:\n#          # words not in the glove will be set to zero   \n#             embedding_matrix[i] = embedding_vector\n            \n\n# n_epochs = 20\n# model4 = Sequential()\n# model4.add(Embedding(input_dim=vocab_size, output_dim=embedding_size,weights = [embedding_matrix], input_length=max_len, trainable=True))\n# model4.add(Conv1D(128, 3, activation='relu'))\n# model4.add(MaxPool1D(3))\n# model4.add(Dropout(0.2))\n# model4.add(Conv1D(128, 3, activation='relu'))\n# model4.add(GlobalMaxPooling1D())\n# model4.add(Dropout(0.2))\n# model4.add(Dense(64, activation='relu'))\n# model4.add(Dropout(0.2))\n# model4.add(Dense(32, activation='relu'))\n# model4.add(Dropout(0.2))\n# model4.add(Dense(16, activation='relu'))\n# model4.add(Dropout(0.2))\n# model4.summary()\n# model4.add(Dense(1, activation='sigmoid'))\n\n\n# model4.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n# save_best = ModelCheckpoint('SMS.hdf', save_best_only=True, monitor='val_acc', mode='max')\n# result4 = model4.fit(X_train, y_train, batch_size = batch_size, epochs=n_epochs, validation_split=0.2, verbose=1,  callbacks=[save_best])\n# eval_ = model4.evaluate(X_test, y_test)\n# print(eval_[0], eval_[1]) # loss \/ accuracy\n\n# plot_model(result4)","591c6c4a":"* **CNN with Glove Model**","972ebb29":"* **LSTM MODEL**","3b466165":"# Results\n\ncorpus : 9000 words\n\nResults\n\nModel                Accuracy\n \n*  LSTM  :              100%\n*  Bidirectional LSTM : 100%\n*  GRU :                100%\n*  CNN :                100%\n*  Glove-CNN :          ","ceef7c64":"* **Bidirectional LSTM MODEL**","1d9d8dd9":"* **CNN MODEL**","25a2fe2d":"* **GRU MODEL**"}}