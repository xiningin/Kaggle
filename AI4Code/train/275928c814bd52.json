{"cell_type":{"735e4998":"code","ac611b2e":"code","bdfa6763":"code","cd4b804f":"code","e743eb54":"code","185000df":"code","e63c9dc3":"code","9644402a":"code","39bac668":"code","fe62ce2b":"code","0a4538c3":"code","e4d100b8":"code","386ad114":"code","7a217078":"markdown","d41712bf":"markdown","e728b5e3":"markdown","ea6941c5":"markdown","c5f36cf9":"markdown"},"source":{"735e4998":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# Any results you write to the current directory are saved as output.","ac611b2e":"import os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline","bdfa6763":"def get_images(directory):\n        normal_images=[]\n        pneumonia_images=[]\n        for f in os.listdir(directory):\n            if f == 'NORMAL':\n                for image in os.listdir(directory+r'\/'+f):\n                    images=cv2.imread(directory+r'\/'+f+r'\/'+image)\n                    try:\n                        images=cv2.resize(images,(500,500))\n                        normal_images.append(images)\n                    except:\n                        continue\n\n            elif f == 'PNEUMONIA':\n                for image in os.listdir(directory+r'\/'+f):\n                    images=cv2.imread(directory+r'\/'+f+r'\/'+image)\n                    try:\n                        images=cv2.resize(images,(500,500))\n                        pneumonia_images.append(images)\n                    except:\n                        continue\n            else:\n                continue\n\n        return np.array(normal_images),np.array(pneumonia_images)\n        ","cd4b804f":"n_train_images,p_train_images=get_images('..\/input\/chest_xray\/chest_xray\/train')\nn_test_images,p_test_images=get_images('..\/input\/chest_xray\/chest_xray\/test')","e743eb54":"print(n_train_images.shape,p_train_images.shape)","185000df":"n_train_images=n_train_images.reshape((1341,-1))\np_train_images=p_train_images.reshape((3875,-1))","e63c9dc3":"### Data Augmentation\ntrain_images_list= [i for i in n_train_images]\ndef data_augmentation(image):\n    \n    image_1= image + 40\n    image_2= image * 2\n    \n    train_images_list.append(image_1)\n    train_images_list.append(image_2)\n    \nfor i in range(n_train_images.shape[0]):\n    data_augmentation(n_train_images[i])","9644402a":"train_images=np.array(train_images_list)\ntrain_images = train_images[:3875]","39bac668":"train_images.shape","fe62ce2b":"X_train= np.vstack((train_images,p_train_images))","0a4538c3":"y=np.zeros((3875,1))\ny_1=np.ones((3875,1))\ny=np.append(y,y_1)\ny=y.reshape((7750,1))","e4d100b8":"n_test_images=n_test_images.reshape((-1,750000))\np_test_images=p_test_images.reshape((-1,750000))","386ad114":"X_test= np.vstack((n_test_images,p_test_images))","7a217078":"> **2. Data PreProcessing**","d41712bf":"> **1. Installing Dependencies**","e728b5e3":"**Now we have prepared the data. We have X_train as input and y as output of this Supervised Learning problem.I have used Naive Data Augmentation Techniques just to give you a hint that how it is done. This will also work properly**\n\n","ea6941c5":"1. **Data Preparation for CNN**\n\n\n**Step1**: \n            This is going to be a simplest CNN using few conv layers and pooling layers to classify X-ray as \n            1)Normal \n            2)Pneumonia. \n            ![Normal and Pnuemonia X-Rays](https:\/\/www.dropbox.com\/s\/v8r0svsrjaxntsy\/jZqpV51.png?dl=0)\n            The reason for keeping it simple is to explore parameter tunning and regularization  as well.         \n            We will first develop model using tensorflow and then for step 2 we shall use Keras. Here in step 1, we             shall start with loading data normally, the preprocessing (if required) and then finding paramters\n            using smaller datasets. Along with that regularization techniques such as Dropout, Data Augmentation               would also be used.            \n            \n **Step2**: At the end, transfer learning concept will be used to check if the accuracy can be improved. We are                using transfer learning because the data available is not enough. The basic feature extraction of                  already trained models can be used in this project and improve the accuaracy. ","c5f36cf9":"As we can see we have got an imbalanced trainings set. An imabalanced traing set causes following problems:\n\n1. We don\u2019t get optimized results for the class which is unbalanced in real time as the model\/algorithm never gets sufficient look at the underlying class\n\n2. It creates a problem of making a validation or test sample as its difficult to have representation across classes in case number of observation for few classes is extremely less\n\nIt can be solved by three ways:\n\n1. Oversampling the underrepresented class ( increase its instances by copying )\n\n2. Undersampling the over-represented class(by deleting instances)\n\n3. Data Augmentation ( This means oversampling by using few tricks on images data such as rotation and scaling of existing images to increase their numbers)"}}