{"cell_type":{"b294d1ef":"code","e46fd80c":"code","a94196bb":"code","257511ef":"code","af3e5f09":"code","7ecc27af":"code","f19ce7c4":"code","abda0921":"code","b587dbc8":"code","0ce4eeda":"code","fdf1a717":"code","c62b833b":"code","1829dbed":"code","a8f6f019":"code","f0ac21cc":"code","f2728769":"code","c5db3240":"code","99b33afb":"code","19211114":"code","bf6147f9":"code","f060a797":"code","91d42f58":"code","6e2b956d":"code","78ce0b8d":"code","b0422031":"code","4b3e9728":"code","fd0e288f":"code","6c3aecaa":"code","885bdbea":"code","f707d442":"code","a81092e0":"code","88e81e8d":"markdown","80fa7352":"markdown","c214e55b":"markdown","156ae4b4":"markdown","5914d050":"markdown","d7bf9da6":"markdown","8cbfc81e":"markdown","2618fc5c":"markdown","466dfcd0":"markdown","b3197b79":"markdown","84c1d90f":"markdown","67c576e2":"markdown"},"source":{"b294d1ef":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_columns', 100)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","e46fd80c":"# Check columns\/features you have \ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(df_train.shape)\nprint(df_train.columns)","a94196bb":"# Have a look over null values, so preparing for the \"missing values process\" step\ndf_train.info()","257511ef":"# Check sample values for features\ndf_train.head()","af3e5f09":"# Check STAT details (pd.set_option('display.max_columns', 100) will help display all columns)\ndf_train.describe()","7ecc27af":"# Split dataset into feature matrix X and target value y\nX_train = df_train.drop('SalePrice', axis=1)\ny_train = df_train['SalePrice']","f19ce7c4":"# Display KDE(Kernel Density Estimate) graph \nsns.displot(y_train, kde=True);","abda0921":"# Check these attributes for target's distribution\nprint(\"Skewness: %f\" % y_train.skew())\nprint(\"Kurtosis: %f\" % y_train.kurt())","b587dbc8":"# Correlation Matrix on Heatmap\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True, cmap='RdBu_r'); ","0ce4eeda":"# Rank & Plot Top 9 Features Correlated with Target\nk = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf, ax = plt.subplots(figsize=(12, 9))\nsns.set(font_scale=1.2)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values, cmap='RdBu_r')\nplt.show()","fdf1a717":"# Further, Top Correlated Features Can be Picked for Scatterplot to See if Some Linear Relations We Could Find  \ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], height=2)\nplt.show();","c62b833b":"# First, Find & Sort NULL Values Existed in Features (Numerical)\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending=False) # df_train.count() will count non-NA by default\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","1829dbed":"# Here, We Drop All Features with > 1 NULL Values and Drop 1 NULL Item for Electrical\ndf_train = df_train.drop(missing_data[missing_data['Total'] > 1].index, 1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max()","a8f6f019":"# We could find 2 outliers (right-bottom) are obviously out of this near-linear relation\nfig, ax = plt.subplots()\nax.scatter(X_train['GrLivArea'], y_train)\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","f0ac21cc":"# Delete Outliers (crietrias could be obvsered and choosen, here > 4000 and < 300000)\ndf_train = df_train.drop(df_train[(df_train['GrLivArea'] > 4000) & (df_train['SalePrice'] < 300000)].index)\n\n# Plot De-Outlier Graph\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","f2728769":"# After Plotting Distribution & Probability, skewness and peakness could be observered\nsns.distplot(df_train['SalePrice'] , fit = norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot = plt)","c5db3240":"# After Applying Log Transformation, Much More Fit on Normal Distribution\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","99b33afb":"df_train = pd.get_dummies(df_train)\nprint(df_train.shape)","19211114":"# Lasso Regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","bf6147f9":"# Elastic Net Regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","f060a797":"# Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","91d42f58":"# Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","6e2b956d":"# XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","78ce0b8d":"# LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","b0422031":"# Define CV Score Function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, df_train['SalePrice'].values, scoring=\"neg_mean_squared_error\", cv=kf))\n    return(rmse)","4b3e9728":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fd0e288f":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6c3aecaa":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","885bdbea":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f707d442":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a81092e0":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","88e81e8d":"**Clean Missing Values**","80fa7352":"After importing needed libs, the 1st thing is to get to know with your dataset. Following the approach offered by [[1]](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#2.-First-things-first:-analysing-'SalePrice'), a walk-thru over all features included in the dataset will help you build a good understanding of them, like what is, perhaps why it will or not matter for saleprices...\n\nA trivial step seems like, but try not skipping it. Here is [my version](https:\/\/docs.google.com\/spreadsheets\/d\/1ST913g5RF-pc8TDRvx8CvlrscsSaLkEf4t1_1F7k45c\/edit?usp=sharing).","c214e55b":"**Check Target (SalePrice)**","156ae4b4":"**Remove Outliers**","5914d050":"# Data Preprocessing","d7bf9da6":"**Check Features**","8cbfc81e":"# Model Tuning","2618fc5c":"**Convert Categorical Features**","466dfcd0":"**Check Correlation between Features and Target**","b3197b79":"**Log Transformation**","84c1d90f":"This is my first notebook completed in kaggle. The main analysis flows I learned from these references [1] [2] Appreciate these work!\nI reproduced the flow, did updates for deprecated functions and for better displays, and added comments hoping could help you have a \"comfortable\" journey with this dataset. \n\nThank you all for you reading. See u soon in other datasets. \n\n[1] COMPREHENSIVE DATA EXPLORATION WITH PYTHON (by Pedro Marcelino)    \n[2] Stacked Regressions to predict House Price (by Serigne)","67c576e2":"# EDA"}}