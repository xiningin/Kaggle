{"cell_type":{"da8619c8":"code","600a4858":"code","49a3e101":"code","d6b01129":"code","bcdeee2d":"code","833483c5":"code","5f895715":"code","5a80ff08":"code","b83335db":"code","b759ee20":"code","a17fd180":"code","1492d48b":"code","3a75542c":"code","a6a48741":"code","ad235aa2":"code","faa40125":"code","f9a5911f":"code","47dfb20a":"code","fbbf8a3c":"code","efe007f1":"code","5624a3e2":"code","0bdd72b0":"code","7dd253f5":"code","10a1f9c2":"code","ca43ccd0":"code","84bcea92":"code","d16ce5df":"code","63283f3b":"code","b71141c0":"code","9f6993c1":"code","4e386449":"code","9fd2ee5e":"code","71050f5c":"code","9aa64fc0":"code","a85bdfa7":"code","1dddf484":"code","a7525698":"code","a6d34997":"code","99ee219c":"code","457e1b7b":"code","01e4fee8":"code","9ff3bdd5":"code","65c36bf5":"code","c70f8bca":"code","02aff406":"code","46b7cd20":"code","bbed6658":"code","6678af97":"code","3981d913":"code","1ec23c7b":"markdown","1a394bb8":"markdown","aaa3dc60":"markdown","033e3898":"markdown","5a338a24":"markdown","d113c2ef":"markdown","a3ca0ae1":"markdown","01528c3c":"markdown","7d113122":"markdown"},"source":{"da8619c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","600a4858":"import requests # library to handle requests\nimport pandas as pd # library for data analsysis\nimport numpy as np # library to handle data in a vectorized manner\nimport random # library for random number generation\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nfrom numpy.random import seed\nfrom numpy.random import randint\nfrom random import randrange\n!conda install -c conda-forge matplotlib=3.1.2 --yes\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n# tranforming json file into a pandas dataframe library\nfrom pandas.io.json import json_normalize\n# libraries for displaying images\nfrom IPython.display import Image \nfrom IPython.core.display import HTML \nimport folium\n!conda install -c conda-forge geopy --yes \nfrom geopy.geocoders import Nominatim\nimport urllib.request\n!conda install beautifulsoup4 --yes\n!conda install lxml --yes\nprint('lxml installed')\nfrom bs4 import BeautifulSoup\nprint(\"Libraries imported\")","49a3e101":"#This is our first dataset which we have collected.\ndf1_hosp=pd.read_csv('\/kaggle\/input\/hospitals_district.csv')\n\n#Removing the unnecessary column\ndf1_hosp = df1_hosp.drop(columns='Unnamed: 0',axis=1)\ndf1_hosp.info()","d6b01129":"#make groups by Districts\ndf1_hosp.groupby(by='Districts', axis=0).first()","bcdeee2d":"#check for the null values\ndf1_hosp.isnull().sum()","833483c5":"#define our latitude and longitude of New Delhi\nlatitude = 28.6141793\nlongitude = 77.2022662\nprint(latitude, longitude)","5f895715":"#create a folium map for above data\ndistricts_map = folium.Map(location=[latitude, longitude], zoom_start=11) \n\n# add the Hospitals as blue circle markers\nfor lat, lng, label in zip(df1_hosp['Latidude'], df1_hosp['Longitude'],df1_hosp['Districts']):\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(districts_map)\n\n# display map\ndistricts_map","5a80ff08":"#This is our second data which contains population,Areas, density etc.\n# Get the delhi data with districts\ndelhi_dis='https:\/\/www.indianmirror.com\/india-post\/indianpincode.html'\n\n#Using web scraping we fetched the table\ndf2_pop=pd.read_html(delhi_dis, header=0)[6]\n\ndf2_pop.head()","b83335db":"#lets remove the columns which are not required\ndf2_pop = df2_pop.drop(columns=['Code','Area code','Latitude & Longitude'])\n\n#Make the column name of Districts uniform for all the tables\ndf2_pop = df2_pop.rename(columns={'District':'Districts'})\ndf2_pop","b759ee20":"#Now we will merge above 2 dataframes to get populations of the districts\n#merge on the basis of Districts\ndf_comb = pd.merge(df1_hosp,df2_pop,on=\"Districts\")\ndf_comb.head()","a17fd180":"#Below is our third data source which contains the active cases of COVID-19\n#Now we will work with our last dataset\ncases_data='https:\/\/api.covid19india.org\/csv\/latest\/district_wise.csv'\n\n#This will give us the list of Districts from all India. We need to filter them in next steps\ndf_cases=pd.read_csv(cases_data)\n\ndf_cases.head(5)","1492d48b":"#Filtering the dataset from all India Districts to the districts available in State Delhi\ndf_cases = df_cases[df_cases['State']== 'Delhi']\ndf_cases.head()","3a75542c":"#As we can see that not all the columns are of much use for us. For example Recovered,Deceased are not of much use\n#here for our analysis\n#So we will take only the important columns as below\ndf_cases = df_cases[['State','District','Confirmed']].reset_index(drop=True)\n\n#Similarly we are making name of column District uniform for all the tables\ndf_cases = df_cases.rename(columns={'District':'Districts'})\ndf_cases.head()","a6a48741":"#Now we will merge the above dataset with the dataset which contains population and Hospitals Address \ndf_final = pd.merge(df_comb,df_cases,on='Districts')\n\n#Here we are dropping the duplicate columns as Pincode is available in multiple tables\ndf_final=df_final.drop(columns=['Headquarters','Pincode','State_y'])\ndf_final.head()","ad235aa2":"# We will check the correlation between the columns\nfig = plt.figure(1, figsize=(8, 6))\ndf_corr = df_final.corr()\nmatrix = np.triu(df_corr)\n\nax=sns.heatmap(df_corr, annot = True, vmin=-1, vmax=1, center= 0, mask=matrix, cmap= 'coolwarm')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n","faa40125":"#Our first step to consume our FourSquare API\nCLIENT_ID = 'PHTW51CXQQ4E41FAF3AJZMW2MSVCCL1GN2FWIL2OV43OAVC4' # your Foursquare ID\nCLIENT_SECRET = '32EO1EG3OPPHROKBAVXVPNYLBOD5VD5GZEWX2MBTFC1FQ1RU' # your Foursquare Secret\nVERSION = '20180604'\nLIMIT = 50","f9a5911f":"from geopy.geocoders import Nominatim\n\n#Providing the location as New Delhi\naddress = 'New Delhi'\n\ngeolocator = Nominatim(user_agent=\"foursquare_agent\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint(\"The latitude and longitude coordinates are:\")\nprint(latitude, longitude)","47dfb20a":"#We are providing search query as \"Hospital\" as we want information about the Hospitals in Delhi\n#Providing radius as 100 Kms or 100000 metres\nsearch_query = 'Hospital'\nradius = 100000\nprint(search_query + ' .... OK!')","fbbf8a3c":"url = 'https:\/\/api.foursquare.com\/v2\/venues\/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nurl","efe007f1":"results = requests.get(url).json()\nresults","5624a3e2":"# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_v2 = json_normalize(venues)\ndf_v2.head()","0bdd72b0":"# keep only columns that include venue name, and anything that is associated with location\nfiltered_columns = ['name', 'categories'] + [col for col in df_v2.columns if col.startswith('location.')] + ['id']\ndataframe_filtered = df_v2.loc[:, filtered_columns]\n\n# function that extracts the category of the venue\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']\n\n# filter the category for each row\ndataframe_filtered['categories'] = dataframe_filtered.apply(get_category_type, axis=1)\n\n# clean column names by keeping only last term\ndataframe_filtered.columns = [column.split('.')[-1] for column in dataframe_filtered.columns]\n\ndataframe_filtered.head()\n","7dd253f5":"venues_map = folium.Map(location=[latitude, longitude], zoom_start=11) \n\n# add a red circle marker to represent the New Delhi\nfolium.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Hospitals',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map)\n\n# add the Hospitals as blue circle markers\nfor lat, lng, label in zip(dataframe_filtered.lat, dataframe_filtered.lng, dataframe_filtered.categories):\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(venues_map)\n\n# display map\nvenues_map","10a1f9c2":"#Now we are trying to fetch the data of a particular hospital using ID\nvenue_id='4c0b6d8f6071a5930644e132'\nurl = 'https:\/\/api.foursquare.com\/v2\/venues\/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)\nurl","ca43ccd0":"result = requests.get(url).json()\n#print(result['response']['venue'].keys())\n#result['response']['venue']","84bcea92":"# Using below code to get the rating of the hospital\ntry:\n    print(result['response']['venue']['rating'])\nexcept:\n    print('This venue has not been rated yet.')","d16ce5df":"#venue_id = dataframe_filtered['id'][1]\nvenue_id = '4fc07ce6e4b0c9a9f10172cb' # ID of Hospital\n#venue_id = dataframe_filtered['id'][6]\nurl = 'https:\/\/api.foursquare.com\/v2\/venues\/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)\n\nresult = requests.get(url).json()\ntry:\n    print(result['response']['venue'].keys())\n    print(result['response']['venue']['name'])\n    print(result['response']['venue']['rating'])\nexcept:\n    print('This venue has not been rated yet.')","63283f3b":"#venue_id = dataframe_filtered['id'][1]\nvenue_id = '507a79cbe4b01b417aabcd9d' # ID of Hospital\n#venue_id = dataframe_filtered['id'][6]\nurl = 'https:\/\/api.foursquare.com\/v2\/venues\/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)\n\nresult = requests.get(url).json()\ntry:\n    print(result['response']['venue'].keys())\n    print(result['response']['venue']['id'])\n    print(result['response']['venue']['rating'])\nexcept:\n    print('This venue has not been rated yet.')","b71141c0":"#now we will try to find the ratings of the hospitals in Delhi and if rating is not there, we need to handle \n#using missing values logic\nfor names, ids in zip(dataframe_filtered['name'],dataframe_filtered['id']):\n    venue_ids = ids\n    url = 'https:\/\/api.foursquare.com\/v2\/venues\/{}?client_id={}&client_secret={}&v={}'.format(venue_ids, CLIENT_ID, CLIENT_SECRET, VERSION)      \n    result = requests.get(url).json()\n    try:\n        dataframe_filtered['ratings'] = result['response']['venue']['rating']\n    except:\n        dataframe_filtered['ratings'] = float('NaN')","9f6993c1":"#now we have our ratings for the hospitals\n#next step will be to fill the values which are null for the ratings\ndataframe_filtered['ratings'] = dataframe_filtered['ratings'].fillna(dataframe_filtered['ratings'].mean())","4e386449":"#Also changing the name of the hospital column name to Names\ndataframe_filtered = dataframe_filtered.rename(columns={'name':'Names'})","9fd2ee5e":"#Now we have all our ratings\n#Next step will be to combine the df_final dataframe and dataframe_filtered dataframe\ndf_total = pd.merge(df_final,dataframe_filtered, on='Names')","71050f5c":"df_total.head()","9aa64fc0":"df_total.shape","a85bdfa7":"df_total.columns","1dddf484":"label = df_total['Districts']","a7525698":"#now we will try to remove duplicate columns which are not of much use\ndf_total = df_total.drop(columns=['cc','city','state','country','formattedAddress','crossStreet',\n                                  'postalCode','neighborhood','id','categories','address','lat','lng',\n                                  'labeledLatLngs','distance'])\ndf_total.head()","a6d34997":"#now we will remove the object columns for applying K Means Clustering(making data suitable for fitting the model)\ndf_nonObj = df_total.drop(columns=['Names','Sub Districts','Districts','Latidude','Longitude','State_x','Postal Code'])\ndf_nonObj.head()","99ee219c":"# As we can see, we have few null values in ratings, just to provide variations, let's assign random number to them\nrandomlist = []\ntotal_len = len(df_nonObj['ratings'])\nfor i in range(0,total_len):\n    n = random.randint(3,10)\n    randomlist.append(n)\ndf_nonObj['ratings'] = randomlist\n#print(df_nonObj['ratings'])","457e1b7b":"#check the optimal k value\nks = range(1,10)\ninertias=[]\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(df_nonObj)\n    inertias.append(model.inertia_)\nplt.figure(figsize=(10,8)) \nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","01e4fee8":"def k_means(n_clust, data_frame, true_labels):\n   \n    k_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\n    k_means.fit(data_frame)\n    c_labels = k_means.labels_\n    df = pd.DataFrame({'clust_label': c_labels, 'orig_label': true_labels.tolist()})\n    ct = pd.crosstab(df['clust_label'], df['orig_label'])\n    y_clust = k_means.predict(data_frame)\n    display(ct)\n    print('% 9s' % 'inertia  homo    compl   v-meas   ARI     AMI     silhouette')\n    print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n      %(k_means.inertia_,\n      homogeneity_score(true_labels, y_clust),\n      completeness_score(true_labels, y_clust),\n      v_measure_score(true_labels, y_clust),\n      adjusted_rand_score(true_labels, y_clust),\n      adjusted_mutual_info_score(true_labels, y_clust),\n      silhouette_score(data_frame, y_clust, metric='euclidean')))","9ff3bdd5":"#provinding one of the optimal values of K\nk_means(n_clust=5, data_frame=df_nonObj, true_labels=label)","65c36bf5":"#Let's predict the values using our model\nkmeans5 = KMeans(n_clusters=5)\ny_kmeans5 = kmeans5.fit_predict(df_nonObj)\nprint(y_kmeans5)","c70f8bca":"#Lets add the clusters in our dataframe\ndf_nonObj['cluster'] = y_kmeans5\ndf_nonObj.tail()","02aff406":"#get the values of our centroids of clusters\nkmeans5.cluster_centers_","46b7cd20":"#confirmed cases vs ratings of hospitals\nplt.title('Confirmed cases vs ratings of hospitals')\nplt.xlabel('Confirmed Cases')\nplt.ylabel('Ratings')\nplt.scatter(df_nonObj['Confirmed'],df_nonObj['ratings'], c=y_kmeans5, cmap = 'rainbow')","bbed6658":"#confirmed cases vs Population\nplt.title('Confirmed cases vs Population')\nplt.xlabel('Confirmed Cases')\nplt.ylabel('Population')\nscatter= plt.scatter(df_nonObj['Confirmed'],df_nonObj['Population (2001)'], c=y_kmeans5, cmap = 'rainbow')\nplt.colorbar(scatter)","6678af97":"#confirmed cases vs Density\nplt.title('Confirmed cases vs Density (\/km\u00b2)')\nplt.xlabel('Confirmed Cases')\nplt.ylabel('Density (\/km\u00b2)')\nplt.scatter(df_nonObj['Confirmed'],df_nonObj['Density (\/km\u00b2)'],c=y_kmeans5,  cmap = 'rainbow')\n","3981d913":"df_nonObj['cluster'] = df_nonObj['cluster'].astype(str)\nfinal_map = folium.Map(location=[latitude, longitude], zoom_start=11) \n\n# add a red circle marker to represent the New Delhi\nfolium.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Hospitals',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map)\n# add the Hospitals as blue circle markers\nfor lat, lng, label,dense,confm in zip(df_total.Latidude, df_total.Longitude, df_nonObj.cluster,df_total['Density (\/km\u00b2)'],df_total.Confirmed):\n    colors = ['blue', 'green', 'yellow', 'red', 'voilet']\n    labels = '{}, {}, {}'.format(label, dense,confm)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color=colors[int(label)],\n        popup=labels,\n        clustered_marker = True,\n        fill = True,\n        fill_color=colors[int(label)],\n        fill_opacity=0.6\n    ).add_to(final_map)\n\n# display map\nfinal_map","1ec23c7b":"#Analysis of the above graph:\n\n1. Confirmed cases and Density have a high relation with value of 0.95\n2. Area and Confirmed cases have a negative relationship. Which means inversely proportional to each other.","1a394bb8":"Now we will observe the clusters\n\n* Blue cluster which are near New Delhi have low cases(at around 42 per district) and also having low density of 4430 per sqaure kms. Also this cluster have higher number of hospitals.\n* Green cluster have highest cases of 184 and also high density of around 25759 per sqaure kms. This cluster have comparatively medium number of hospitals.\n* Yellow cluster have moderate cases of 70 and density of 9033 per sqaure kms. This cluster have low number of hospitals.\n* Now red cluster have low cases of 37 per district and density of 13477 per square kms. This cluster have low number of hospitals.\n* Now Violet cluster have moderate cases of around 60 per district and density of 13000 square kms. Also having moderate number of hospitals.","aaa3dc60":"Result Section:\n\n* Number of cases have a linear relationship with Population and Density.\n* Number of cases do not have linear relationship with Ratings of the hospitals.","033e3898":"Recommendations we can make based on the results and we can also provide it to DELHI Government as a prototype:\n\n* Green cluster(mostly areas nearby Karol Bagh) needs to get more medical supplies in comparison to other clusters.\n* After green cluster, yellow cluster needs to be taken care because of low hospitals and high cases.\n* Then violet cluster(areas in north Delhi) needs supply of medicines and other amenities based on all the parameters.","5a338a24":"2. Data where you describe the data that will be used to solve the problem and the source of the data.\n\nApproach to solve this problem: \n1. Call the FourSquare API to get the details of Hospitals in Delhi\n2. Collect the information about the hospitals with proper address,Pincode, sub-districts etc\n3. Collect the information about each districts such as population, area, density etc.\n4. Lastly collect the information about the current COVID-19 cases in each districts.\n\nNow we have data such as positions of hospital names, areas, population, density, active cases, hospitals rating etc. Now we will combine all the data and will try to create clusters and find relations. \n\nSources of data:\n1. FourSquare API\n2. https:\/\/www.indianmirror.com\/india-post\/indianpincode.html to get area, density, population etc\n3. https:\/\/api.covid19india.org\/csv\/latest\/district_wise.csv to get latest COVID-19 cases\n4. Wikipedia and other sites to get hospitals districts.\n\n","d113c2ef":"This is just a small observation for demo purpose :)","a3ca0ae1":"Problem Statement: Provide suggestion for medical check-up booths amid COVID-19 based on the areas, population,\n                   total number of cases in that area, hospitals in that area(to check the current medical \n                   facility status etc)\n        \nTarget Audience: Government, medical agencies, NGOs and all related to COVID-19\n    \nObserved Problem: During the prime time of COVID-19, there was a requirement to identify the places where the                         medical and food supply is less and cases are more. Due to panic situation, a lot number of small                   industries are shut down and due to which there is lack of medicine and food. \n                  So I planned to identify the areas district wise in Delhi where proper supply of medicine and                       food is not there.\n        ","01528c3c":"Cluster colors: 0 - Blue 1 - Green 2 - Yellow 3 - Red 4 - Violet","7d113122":"Above is our final data which is available for analysis. \nNow we will start fetching the details of the Delhi from FourSquare API which is our main task."}}