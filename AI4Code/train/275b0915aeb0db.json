{"cell_type":{"ef700fba":"code","3d68ce23":"code","397caacb":"code","796b8a24":"code","d20abb2f":"code","a3fd202a":"code","036c7760":"code","fe052921":"code","d947a975":"code","d12b3eb5":"code","0f17d0cb":"code","469896e4":"code","dff5ebbd":"code","a98b8a81":"code","2ff710d3":"code","ff322153":"code","a540aaa8":"code","29371794":"code","8bd9c3a9":"code","a07f3c6e":"code","ac05ffa0":"code","7e421570":"code","0f792c8b":"code","b1c7a54c":"code","f354d510":"code","979e0352":"code","ad0e39de":"code","586bd2ea":"code","93328b29":"code","ef1508a9":"code","882783d3":"code","782bf04a":"code","1dcd2946":"code","2c3e5a26":"code","e15252e4":"code","e823512d":"code","4554322d":"code","814c4959":"code","3d7f1f94":"markdown","492d04de":"markdown","1b156f1c":"markdown","fbda5969":"markdown","3c70ce37":"markdown","40109eea":"markdown","938950fc":"markdown","f4c0f645":"markdown"},"source":{"ef700fba":"import re\nimport string\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport collections \n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport nltk\nimport gensim\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n######## VADER #####\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nvader = SentimentIntensityAnalyzer()","3d68ce23":"df = pd.read_csv('..\/input\/covishield-vaccine-tweets\/Covishield_tweets.csv')\ndf","397caacb":"df = df.drop(['Time','From-User','From-User-Id','Unnamed: 0'], axis=1)","796b8a24":"df.head(5)","d20abb2f":"df.isnull().sum()","a3fd202a":"df.describe()","036c7760":"df.info()","fe052921":"import string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","d947a975":"df['Text'] = df['Text'].apply(lambda x:clean_text(x))","d12b3eb5":"df.Text.head(10)","0f17d0cb":"sentiments = []\nfor count,tweet in enumerate(df.Text):\n#     print(tweet)\n    sentiments.append(vader.polarity_scores(tweet))\n    print(count,vader.polarity_scores(tweet))\n","469896e4":"len(sentiments)","dff5ebbd":"target = []\nfor count,sentiment in enumerate(sentiments):\n    print(count)\n    if sentiment['compound'] >= 0.05: \n        print(\"Positive\")\n        target.append('POSITIVE')\n    elif sentiment['compound'] <= -0.05 : \n        print(\"Negative\")\n        target.append('NEGATIVE')\n    else: \n        print(\"Neutral\")\n        target.append('NEUTRAL')","a98b8a81":"print(target)","2ff710d3":"target = pd.DataFrame(target, columns=['target'])\ntarget","ff322153":"type(target)","a540aaa8":"df = pd.concat([df, target], axis = 1)","29371794":"df","8bd9c3a9":"df.head(5)","a07f3c6e":"temp = df.groupby('target').count()['Text'].reset_index().sort_values(by='Text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","ac05ffa0":"plt.figure(figsize=(12,6))\nsns.countplot(x='target',data=df)","7e421570":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize","0f792c8b":"fig = go.Figure(go.Funnelarea(\n    text = temp.target,\n    values = temp.Text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","b1c7a54c":"stop_words = set(stopwords.words(\"english\"))","f354d510":"split_words = []\ntop = []\nfor tweet in df.Text:\n    top.append(tweet)","979e0352":"top = str(top)\ntweet = str(df.Text)","ad0e39de":"wordcount = {}\nfor word in tweet.lower().split():\n    word = word.replace(\".\",\"\")\n    word = word.replace(\",\",\"\")\n    word = word.replace(\":\",\"\")\n    word = word.replace(\"\\\"\",\"\")\n    word = word.replace(\"!\",\"\")\n    word = word.replace(\"\u00e2\u20ac\u0153\",\"\")\n    word = word.replace(\"\u00e2\u20ac\u02dc\",\"\")\n    word = word.replace(\"*\",\"\")\n    word = word.replace(\"rt\",\"\")\n    word = word.replace(\"\",\"\")\n    if word not in stop_words:\n        if word not in wordcount:\n            wordcount[word] = 1\n        else:\n            wordcount[word] += 1","586bd2ea":"word_counter = collections.Counter(wordcount)\nlst = word_counter.most_common(20)\ntemp = pd.DataFrame(lst, columns = ['Word', 'Count'])\ntemp.plot.bar(x='Word',y='Count')\ntemp.columns = ['Word','Count']\ntemp.style.background_gradient(cmap='Purples')","93328b29":"my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'a',' a ',' a','a ''below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]","ef1508a9":"df['Text_new'] = df['Text'].apply(lambda x:' '.join([word for word in x.split() if word not in  (my_stopwords)]))","882783d3":"df.Text.count()","782bf04a":"df.Text_new.count()","1dcd2946":"df.head(10)","2c3e5a26":"fig = px.bar(temp, x=\"Word\", y=\"Count\", title='Commmon Words in Selected Text', orientation='v', \n             width=700, height=700,color='Count')\nfig.show()","e15252e4":"w2v_model = gensim.models.word2vec.Word2Vec(size=300, \n                                            window=7, \n                                            min_count=10, \n                                            workers=8)","e823512d":"%%time\ndocuments = [_text.split() for _text in df.Text_new] ","4554322d":"w2v_model.build_vocab(documents)","814c4959":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","3d7f1f94":"# Creating targets( +ve, - ve, neutral)","492d04de":"# Text cleaning","1b156f1c":"# Imports","fbda5969":"# Current knowings :\u00b6\nData is very positive that means people's mood and actions are good towards the vaccination. But some of them could be wrong too.","3c70ce37":"## Hope you like it and help me to learn more about nltk and Sentiment Analysis\ud83d\ude04\n# Please leave your valuable comment below.","40109eea":"# Covishield Tweets Analysis using nltk and VADER\n### This is a beginner notebook to the dataset of twitter Bharat-Bio Tech Tweets.\n### Let me know about the best practises and help me to learn more.\n### Feel free to correct me and give suggestions by commenting below.\ud83d\ude04\n# Notebook under completion!","938950fc":"# Reading csv file","f4c0f645":"# Data Vizualizations"}}