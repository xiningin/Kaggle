{"cell_type":{"76aa93dc":"code","1a784579":"code","61ad918c":"code","8b853199":"code","c1868a15":"code","473d2c1b":"code","59e0b97c":"code","2abe4a8c":"code","84143edc":"code","6f8451ff":"code","58b3eb19":"code","dd82ef84":"code","6a2d3d7f":"code","e68c45e0":"markdown","3c77e98a":"markdown","d53f0874":"markdown","adbf523e":"markdown","e77631c0":"markdown","89b3c99f":"markdown","7f7de64a":"markdown","d89e9b08":"markdown","8b5d08ac":"markdown"},"source":{"76aa93dc":"!pip install kaggle-environments --upgrade ","1a784579":"%%writefile submission.py\nimport random\nimport numpy as np\n\nclass UCBAgent:\n    # Optimal Settings:\n    # exploration=12,  opp_reward=0.6, warmup=1, choose='max'\n    # exploration=0.1, opp_reward=0.1, warmup=3, choose='random'\n    def __init__(self, exploration=0.2,  opp_reward=0.2, warmup=2, winrate=0.8, choose='max', verbose=True):\n        self.exploration = exploration\n        self.choose      = choose\n        self.opp_reward  = opp_reward\n        self.warmup      = warmup\n        self.winrate     = winrate\n        self.verbose     = verbose\n        self.history     = None\n        self.state       = None\n\n        \n    def init_state(self, observation, configuration, force=False):\n        if self.state is None or force:\n            self.history = {\n                \"actions\":  [],\n                \"opponent\": [],\n                \"reward\":   [],\n            }\n            self.state = {\n                \"our_rewards\":  np.zeros(configuration.banditCount, dtype=np.float),\n                \"opp_rewards\":  np.zeros(configuration.banditCount, dtype=np.float),\n                \"our_visits\":   np.zeros(configuration.banditCount, dtype=np.float),\n                \"opp_visits\":   np.zeros(configuration.banditCount, dtype=np.float),                \n                \"total_visits\": np.zeros(configuration.banditCount, dtype=np.float),                \n            }        \n        \n        \n    def update_state(self, observation, configuration):\n        if self.state is None:\n            self.init_state(observation, configuration)\n        \n        self.history['reward'].append( observation.reward )\n        if len(self.history['actions']):\n            # observation.reward is cumulative reward\n            our_reward      = int(self.history['reward'][-1] > self.history['reward'][-2])\n            our_last_action = self.history['actions'][-1]\n            if len( set(observation.lastActions) ) == 1:\n                opp_last_action = our_last_action\n            else:\n                opp_last_action = list( set(observation.lastActions) - {our_last_action} )[0]\n            self.history['opponent'].append(opp_last_action)\n\n            self.state['our_rewards'][  our_last_action ] += our_reward\n            self.state['opp_rewards'][  opp_last_action ] += self.opp_reward\n            self.state['our_visits'][   our_last_action ] += 1\n            self.state['opp_visits'][   opp_last_action ] += 1\n            self.state['total_visits'][ our_last_action ] += 1\n            self.state['total_visits'][ opp_last_action ] += 1\n            \n    \n        \n    def scores(self, observation, configuration):\n        total_visits = np.sum(self.state['our_visits']) + 1\n        our_visits   = np.max([ self.state['our_visits'], np.ones(len(self.state['our_visits'])) ])\n        scores = (\n            (self.state['our_rewards'] + self.state['opp_rewards']) \/ our_visits \n            + np.sqrt( self.exploration * np.log(total_visits) \/ our_visits )\n        )\n        scores *= configuration.decayRate ** self.state['total_visits']\n        return scores\n\n        \n    # observation   {'remainingOverageTime': 60, 'step': 1, 'reward': 1, 'lastActions': [54, 94]}\n    # configuration {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n    def agent(self, observation, configuration):\n\n        self.update_state(observation, configuration)\n\n        scores = self.scores(observation, configuration)\n\n        winners  = np.argwhere( (self.state['our_visits'] != 0) \n                              & ( \n                                    (self.state['our_visits'] <= self.state['our_rewards'] + (self.warmup - 1)) \n                                  | (\n                                      np.nan_to_num(self.state['our_rewards'] \/ self.state['our_visits']) \n                                      >= self.winrate * configuration.decayRate ** (observation.step\/configuration.banditCount)\n                                    )\n                                )\n                              ).flatten()\n        untried  = np.argwhere( self.state['our_visits'] == 0).flatten()\n        \n        if self.warmup and len(winners):\n            action = np.random.choice(winners)  # keep trying winners until we lose\n        elif self.warmup and len(untried):\n            action = np.random.choice(untried)\n        else:\n            if self.choose == 'random':\n                action = random.choices( population=np.arange(len(scores)), weights=scores, k=1 )[0]        \n            elif self.choose == 'max':\n                action = np.argmax(scores)\n            else:\n                assert False, self.choose \n                \n        if self.verbose:\n            if True or observation.step < configuration.banditCount:\n                print()\n                print('observation = ', observation)\n                print(f'scores = {list(scores.round(2))}')\n                for key, values in self.state.items():\n                    print(f'self.state[\"{key}\"] = {list(values)}')\n                print(f'action = {action}')\n\n        self.history['actions'].append(action)\n        return int(action)\n\n    \n    def __call__(self, observation, configuration):\n        return self.agent(observation, configuration)\n    \nucb_instance = UCBAgent() \ndef ucb_agent(observation, configuration):\n    return ucb_instance.agent(observation, configuration)","61ad918c":"%run submission.py","8b853199":"from kaggle_environments import evaluate, make, utils\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np","c1868a15":"env = make(\"mab\", debug=False)\nenv.reset()\nenv.run([\"submission.py\", \"submission.py\"])\nevaluate(\"mab\", [\"submission.py\", \"..\/input\/candy-cane-random-agent\/submission.py\"])","473d2c1b":"%%time\n\nenv = make(\"mab\", debug=False)\nenv.reset()\nresults = np.array(Parallel(-1)([\n    delayed(evaluate)(\"mab\", [UCBAgent().agent, \"..\/input\/candy-cane-random-agent\/submission.py\"])    \n    for n in range(10)\n])).reshape(-1,2)\nprint('results:\\n', results)\nprint('mean: ', np.mean(results, axis=0).round(1))\nprint('std:  ', np.std(results,  axis=0).round(1))","59e0b97c":"%%time\n# DOCS: https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-agents-comparison\/\nfor choose in [ 'max', 'random' ]:\n    warmups = [ 0, 1, 2, 3 ] if choose == 'max' else [ 0, 1, 2, 3, 4, 5, 6 ]\n    scores  = np.zeros(( len(warmups), len(warmups) ), dtype=np.int)\n\n    def evaluate_warmups(i1, e1, i2, e2):\n        exploration = 8 if choose == 'max' else 0.1\n        result = evaluate(\"mab\", [\n            UCBAgent(exploration=exploration, opp_reward=0, choose=choose, warmup=e1), \n            UCBAgent(exploration=exploration, opp_reward=0, choose=choose, warmup=e2)\n        ])\n        return (i1, e1, i2, e2, result)\n\n    results = Parallel(-1)( \n        delayed(evaluate_warmups)(i1, e1, i2, e2)\n        for i1, e1 in enumerate(warmups)\n        for i2, e2 in enumerate(warmups)\n        # for n in range(2)\n    )\n    for (i1, e1, i2, e2, result) in results:\n        scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n        scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n\n    df_scores = pd.DataFrame(\n        scores, \n        index   = list(map(lambda n: f'{n:.0f}', warmups)), \n        columns = list(map(lambda n: f'{n:.0f}', warmups)),\n    )\n    plt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\n    plt.title(f'warmup | choose={choose}')\n    sns.heatmap(\n        df_scores, annot=True, cbar=False, \n        cmap='coolwarm', linewidths=1, \n        linecolor='black', fmt=\"d\",\n    )\n    plt.tick_params(labeltop=True, labelright=True)\n    plt.xticks(rotation=90, fontsize=11)\n    plt.yticks(rotation=0,  fontsize=11)\n    print(f'warmup | choose={choose}')\n    print(df_scores.mean(axis=1))","2abe4a8c":"%%time\n\nenv = make(\"mab\", debug=False)\nenv.reset()\nresults = np.array(Parallel(-1)([\n    delayed(evaluate)(\"mab\", [\n        UCBAgent(choose='max',    exploration=12), \n        UCBAgent(choose='random', exploration=0.1),\n    ])    \n    for n in range(10)\n])).reshape(-1,2)\nprint('results:\\n', results)\nprint('mean: ', np.mean(results, axis=0).round(1))\nprint('std:  ', np.std(results,  axis=0).round(1))","84143edc":"%%time\n# DOCS: https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-agents-comparison\/\n\nexplorations = [ 0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 32 ]\n# explorations = list(np.arange(0.0, 1.0, 0.1)) + [ 1, 1.41, 2 ]\nscores       = np.zeros(( len(explorations), len(explorations) ), dtype=np.int)\n\n\ndef evaluate_explorations(i1, e1, i2, e2):\n    result = evaluate(\"mab\", [\n        UCBAgent(exploration=e1, opp_reward=0, choose='max', warmup=True), \n        UCBAgent(exploration=e2, opp_reward=0, choose='max', warmup=True)\n    ])\n    return (i1, e1, i2, e2, result)\n\nresults = Parallel(-1)( \n    delayed(evaluate_explorations)(i1, e1, i2, e2)\n    for i1, e1 in enumerate(explorations)\n    for i2, e2 in enumerate(explorations)\n    # for n in range(2)\n)\nfor (i1, e1, i2, e2, result) in results:\n    scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n    scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n    \ndf_scores = pd.DataFrame(\n    scores, \n    index   = list(map(lambda n: f'{n:.1f}', explorations)), \n    columns = list(map(lambda n: f'{n:.1f}', explorations)),\n)\nplt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\nplt.title('exploration | choose=max')\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap='coolwarm', linewidths=1, \n    linecolor='black', fmt=\"d\",\n)\nplt.tick_params(labeltop=True, labelright=True)\nplt.xticks(rotation=90, fontsize=11)\nplt.yticks(rotation=0,  fontsize=11)\nprint(df_scores.mean(axis=1))","6f8451ff":"%%time\n# DOCS: https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-agents-comparison\/\n\n# explorations = [ 0.1, 0.25, 0.5, 1, 1.4, 2, 4, 8, 16, 32 ]\nexplorations = list(np.arange(0.0, 1.0, 0.05))\nscores       = np.zeros(( len(explorations), len(explorations) ), dtype=np.int)\n\n\ndef evaluate_explorations(i1, e1, i2, e2):\n    result = evaluate(\"mab\", [\n        UCBAgent(exploration=e1, opp_reward=0, choose='random', warmup=3), \n        UCBAgent(exploration=e2, opp_reward=0, choose='random', warmup=3)\n    ])\n    return (i1, e1, i2, e2, result)\n\nresults = Parallel(-1)( \n    delayed(evaluate_explorations)(i1, e1, i2, e2)\n    for i1, e1 in enumerate(explorations)\n    for i2, e2 in enumerate(explorations)\n    # for n in range(2)\n)\nfor (i1, e1, i2, e2, result) in results:\n    scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n    scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n    \ndf_scores = pd.DataFrame(\n    scores, \n    index   = list(map(lambda n: f'{n:.2f}', explorations)), \n    columns = list(map(lambda n: f'{n:.2f}', explorations)),\n)\nplt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\nplt.title('exploration | choose=random')\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap='coolwarm', linewidths=1, \n    linecolor='black', fmt=\"d\",\n)\nplt.tick_params(labeltop=True, labelright=True)\nplt.xticks(rotation=90, fontsize=11)\nplt.yticks(rotation=0,  fontsize=11)\nprint(df_scores.mean(axis=1))","58b3eb19":"%%time\n\nopp_rewards = list(np.arange(0.0, 1.0, 0.1))\nscores      = np.zeros(( len(opp_rewards), len(opp_rewards) ), dtype=np.int)\n\ndef evaluate_opp_rewards(i1, e1, i2, e2):\n    result = evaluate(\"mab\", [\n        UCBAgent(opp_reward=e1, choose='max', exploration=8, warmup=1), \n        UCBAgent(opp_reward=e2, choose='max', exploration=8, warmup=1),\n    ])\n    return (i1, e1, i2, e2, result)\n\nresults = Parallel(-1)( \n    delayed(evaluate_opp_rewards)(i1, e1, i2, e2)\n    for i1, e1 in enumerate(opp_rewards)\n    for i2, e2 in enumerate(opp_rewards)\n    # for n in range(2)\n)\nfor (i1, e1, i2, e2, result) in results:\n    scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n    scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n    \ndf_scores = pd.DataFrame(\n    scores, \n    index   = list(map(lambda n: f'{n:.1f}', opp_rewards)), \n    columns = list(map(lambda n: f'{n:.1f}', opp_rewards)),\n)\nplt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\nplt.title('opp_reward | choose=max')\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap='coolwarm', linewidths=1, \n    linecolor='black', fmt=\"d\",\n)\nplt.tick_params(labeltop=True, labelright=True)\nplt.xticks(rotation=90, fontsize=11)\nplt.yticks(rotation=0,  fontsize=11)\nprint(df_scores.mean(axis=1))","dd82ef84":"%%time\n\nopp_rewards = list(np.arange(0.0, 1.0, 0.1))\nscores      = np.zeros(( len(opp_rewards), len(opp_rewards) ), dtype=np.int)\n\ndef evaluate_opp_rewards(i1, e1, i2, e2):\n    result = evaluate(\"mab\", [\n        UCBAgent(opp_reward=e1, choose='random', exploration=0.1, warmup=3), \n        UCBAgent(opp_reward=e2, choose='random', exploration=0.1, warmup=3),\n    ])\n    return (i1, e1, i2, e2, result)\n\nresults = Parallel(-1)( \n    delayed(evaluate_opp_rewards)(i1, e1, i2, e2)\n    for i1, e1 in enumerate(opp_rewards)\n    for i2, e2 in enumerate(opp_rewards)\n    # for n in range(2)\n)\nfor (i1, e1, i2, e2, result) in results:\n    scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n    scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n    \ndf_scores = pd.DataFrame(\n    scores, \n    index   = list(map(lambda n: f'{n:.1f}', opp_rewards)), \n    columns = list(map(lambda n: f'{n:.1f}', opp_rewards)),\n)\nplt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\nplt.title('opp_reward | choose=random')\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap='coolwarm', linewidths=1, \n    linecolor='black', fmt=\"d\",\n)\nplt.tick_params(labeltop=True, labelright=True)\nplt.xticks(rotation=90, fontsize=11)\nplt.yticks(rotation=0,  fontsize=11)\nprint(df_scores.mean(axis=1))","6a2d3d7f":"%%time\n# DOCS: https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-agents-comparison\/\nfor choose in [ 'max', 'random' ]:\n    winrates = [ 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.6, 0.5 ]\n    scores   = np.zeros(( len(winrates), len(winrates) ), dtype=np.int)\n\n    def evaluate_warmups(i1, e1, i2, e2):\n        exploration = 12 if choose == 'max' else 0.2\n        warmup      =  1 if choose == 'max' else 2 \n        result = evaluate(\"mab\", [\n            UCBAgent(exploration=exploration, opp_reward=0, choose=choose, warmup=warmup, winrate=e1), \n            UCBAgent(exploration=exploration, opp_reward=0, choose=choose, warmup=warmup, winrate=e2)\n        ])\n        return (i1, e1, i2, e2, result)\n\n    results = Parallel(-1)( \n        delayed(evaluate_warmups)(i1, e1, i2, e2)\n        for i1, e1 in enumerate(winrates)\n        for i2, e2 in enumerate(winrates)\n        # for n in range(2)\n    )\n    for (i1, e1, i2, e2, result) in results:\n        scores[i1, i2] += (result[0][0] or 0) - (result[0][1] or 0)\n        scores[i2, i1] += (result[0][1] or 0) - (result[0][0] or 0)\n\n    df_scores = pd.DataFrame(\n        scores, \n        index   = list(map(lambda n: f'{n:.2f}', winrates)), \n        columns = list(map(lambda n: f'{n:.2f}', winrates)),\n    )\n    plt.figure(figsize=(scores.shape[0]\/\/1.5, scores.shape[1]\/\/1.5))\n    plt.title(f'winrates | choose={choose}')\n    sns.heatmap(\n        df_scores, annot=True, cbar=False, \n        cmap='coolwarm', linewidths=1, \n        linecolor='black', fmt=\"d\",\n    )\n    plt.tick_params(labeltop=True, labelright=True)\n    plt.xticks(rotation=90, fontsize=11)\n    plt.yticks(rotation=0,  fontsize=11)\n    print(f'winrates | choose={choose}')\n    print(df_scores.mean(axis=1))","e68c45e0":"# winrate\n","3c77e98a":"High variance result\n- choose=max | winrate=0.85\n- choose=random | winrate=0.8","d53f0874":"# opt_reward\n- choose=max    | opt_reward prefers 0.6+\n- choose=random | opt_reward prefers 0.1 if opponent is not copying, but 0.3 or 0.8 if they are","adbf523e":"# Choose\n\nchoose='random' is better than choose='max'","e77631c0":"Agent successfully plays against itself","89b3c99f":"# Exploration\n- choose=random + warmup=True | Smaller exploration is better, 0.2 is optimal\n- choose=max    + warmup=True | Higher  exploration is better, 12 is optimal","7f7de64a":"# Santa 2020 Candy Cane - Optimized UCB\n\nThe Multi-Armed Bandit Problem is described here: https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit\n\nIn this notebook we will be taking a random distribution based on the UCB Upper Confidence Bound Score:\n```\nscore = (our_reward \/ our_visits) + sqrt( exploration * log(total_visits) \/ our_visits ) \n```\n\nWe also implement a greedy warmup phase, repeatedly pulling each bandit until we reach N losses, as well as rewarding opponent moves on the assumption they are also looking for high payout bandits.","d89e9b08":"# Warmup\n\n- choose=max | warmup=1 is best\n- choose=random | warmup=3 is best  ","8b5d08ac":"UCB Agent can consistently beat random agent by a small margin "}}