{"cell_type":{"5eca2e98":"code","4b2d1f1d":"code","fd7f965c":"code","6db034a2":"code","cdba0cc4":"code","597b9492":"code","cda16a75":"code","18762f2a":"code","16a89975":"code","a0a949d2":"code","1f690546":"code","7022a2b8":"code","9a1e8093":"code","150fbc6e":"code","58866e8c":"code","ce3f278b":"code","b0596d50":"code","9cdeb61a":"code","cd1e915c":"code","6dd7142f":"markdown","c6fffc6e":"markdown","e9a98f5f":"markdown","b2bfe2e0":"markdown","fa109d38":"markdown"},"source":{"5eca2e98":"# Since I am quite new to ML, I was inspired by the heart failure prediction dataset and the code by emrearslan123 found\n# here on kaggle for the stroke dataset https:\/\/www.kaggle.com\/emrearslan123\/machine-learning-on-stroke-prediction-dataset \n# and the awesome ML introduction book 'Machine Learning with Python for Everyone' by Mark. E. Fenner\n# to try a simple ML project by myself.\n# For this dataset, Logistic Regression was found to be one of the best models in terms of accuracy, however, the other\n# four models tested were also very close or equal regarding accuracy to Logistic Regression. ","4b2d1f1d":"import sklearn\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","fd7f965c":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","6db034a2":"df.head(5)\n# HeartDisease: 0 = no, 1 = yes","cdba0cc4":"df.shape","597b9492":"df.info()","cda16a75":"# Check the data visually\nsns.set(style=\"ticks\", color_codes=True)\n\nsns.pairplot(df, hue='Sex', vars=['FastingBS', 'RestingBP', 'Cholesterol', 'MaxHR', 'Age'],\n             height = 2, plot_kws={'alpha':.2})","18762f2a":"df.describe().T","16a89975":"plt.figure(figsize=(10,6))\nsns.displot(df[\"MaxHR\"])\nplt.title(\"MaxHR\", size=15)\nplt.show()","a0a949d2":"plt.figure(figsize=(10,6))\nsns.barplot(x=df[\"HeartDisease\"], y=df[\"Sex\"])\nplt.title(\"Heart Disease Numbers vs Sex\", size=15)\nplt.show()","1f690546":"plt.figure(figsize=(10,6))\nsns.barplot(x=df[\"ChestPainType\"], y=df[\"HeartDisease\"])\nplt.title(\"ChestPainType vs HeartDisease\", size=15)\nplt.show()","7022a2b8":"X = df.drop([\"HeartDisease\"], axis=1)\ny = df[\"HeartDisease\"]\ny = pd.DataFrame(y, columns=[\"HeartDisease\"])\ndisplay(X.head()) # these are our features\ndisplay(y.head()) # this is our target","9a1e8093":"numerical_cols = X.select_dtypes([\"float64\",\"int64\"])\nscaler = sklearn.preprocessing.StandardScaler()\nX[numerical_cols.columns] = scaler.fit_transform(X[numerical_cols.columns])\nX.head()","150fbc6e":"# One hot encoding should be done on this dataset to convert all non-numeric \n# into numeric features.\ncategorical_cols = X.select_dtypes(\"object\")\nX = pd.get_dummies(X, columns=categorical_cols.columns)\nX.head()","58866e8c":"# Split the heart failure dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Prepare DataFrame for the Accuracy results\nmodels = pd.DataFrame(columns=[\"Model\",\"Accuracy Score\"])\n\n# Define the models we want to train and test\nclassifiers = {'logReg' : LogisticRegression(),\n               'GradBC' : GradientBoostingClassifier(),\n               'randomforest' : RandomForestClassifier(n_estimators=1000, random_state=42),\n               'DT' : DecisionTreeClassifier(max_depth=3),\n               'AdaBC' : AdaBoostClassifier()}\n\n# fit and predict each model \nfor model_name, model in classifiers.items():\n    model.fit(X_train, y_train.values.ravel())\n    predictions = model.predict(X_test)\n    score = accuracy_score(predictions, y_test)\n    print(model_name , {score})\n    new_row={\"Model\": model_name, \"Accuracy Score\": score}\n    models = models.append(new_row, ignore_index=True)\n\n# sort the results\nmodels.sort_values(by=\"Accuracy Score\", ascending=False)","ce3f278b":"# Take a deeper look into one of the models with highest accuracy,\n# meaning for the sake of simplicity only for Logistic Regression and its metrics.\n\n# First, take a look at the non-optimized logReg model:\nlogReg = LogisticRegression()\n\nscores = []\nfor r in range(20):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n    fit = logReg.fit(X_train, y_train.values.ravel())\n    pred = fit.predict(X_test)\n    score = sklearn.metrics.mean_squared_error(y_test, pred)\n    scores.append(score)\n    \nscores = pd.Series(np.sqrt(sorted(scores)))\ndf = pd.DataFrame({'RMSE': scores})\ndf.index.name = 'Repeat'\ndisplay(df.T)","b0596d50":"# Visualize the RMSE values and describe the result\nax = plt.figure(figsize=(4,3)).gca()\nsns.swarmplot(y = 'RMSE', data = df, ax = ax)\nax.set_xlabel('Over Repeated Train-Test Splits')\ndisplay(df.describe().T)\n\n# The RMSE (error) value of our non-optimized logReg model is quite low, that's good!","9cdeb61a":"# The Confusion Matrix cm of correct and incorrect predictions\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\ncm = sklearn.metrics.confusion_matrix(y_test, pred)\nprint(cm)\nax = sns.heatmap(cm, annot=True, square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual');\n\n# So far, the model predicts very well True-positive cases and True-negative cases.","cd1e915c":"# This is to find out what hyperparameters are available for LogisticRegression\nprint(sklearn.linear_model.LogisticRegression.get_params(logReg).keys())\n\n# Try to improve the hyperparameters of the logReg model with GridSearchCV\nparam_grid = [    \n    {'penalty' : ['l1', 'l2'],\n    'solver' : ['liblinear','saga']}\n]\n\n# Now create the GridSearch object\ngrid_model = sklearn.model_selection.GridSearchCV(logReg,\n                               return_train_score = True,\n                               param_grid = param_grid,\n                               cv = 20,\n                               verbose = True,\n                               n_jobs = -1)\n\n# Fit the grid_model on the heart failure data\nbest_model = grid_model.fit(X_train, y_train.values.ravel())\n\nparam_cols = ['param_penalty']\nscore_cols = ['mean_train_score', 'std_train_score',\n              'mean_test_score', 'std_test_score']\n\n# Look at first 5 params with head\ndf = pd.DataFrame(grid_model.cv_results_).head(10)\n\ndisplay(df[param_cols + score_cols])\n\nparam_cols = ['param_solver']\nscore_cols = ['mean_train_score', 'std_train_score',\n              'mean_test_score', 'std_test_score']\n\n# Look at first 5 params with head\ndf = pd.DataFrame(grid_model.cv_results_).head(10)\n\ndisplay(df[param_cols + score_cols])\n\n# As a result, we see that the model for this dataset cannot be optimized\n# for the two hyperparameters penalty and solver each.\n# However, this might be different for another similar dataset.","6dd7142f":"## Standardize the dataset","c6fffc6e":"## Split heart failure data, fit, predict, evaluate accuracy of several models","e9a98f5f":"## Evaluate Heart Failure Dataset ","b2bfe2e0":"# Machine Learning with Python on Heart Failure Dataset","fa109d38":"## Evaluate and optimize selected model 'Logistic Regression'"}}