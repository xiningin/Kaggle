{"cell_type":{"a2662499":"code","a65104d3":"code","2b2a92fe":"code","ae87d2b5":"code","9b689724":"code","3402bf91":"code","f046a29c":"code","e35e137c":"code","b906805d":"code","09e11595":"code","432772ae":"code","8deb51ff":"code","3a1290eb":"code","0fb60088":"code","1acb3966":"code","f21d3ec6":"code","10e202ea":"markdown","f2720cff":"markdown","57c08a83":"markdown","844256ca":"markdown","340c7d08":"markdown"},"source":{"a2662499":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn #API for building neural networks\nfrom torch.utils.data import Dataset, DataLoader #Imports the Dataset and Dataloader classes\nimport os","a65104d3":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device)) #Use GPU if available","2b2a92fe":"import torchaudio\n\nclass AudioDataset(Dataset):\n    def __init__(self, annotations_file):\n        self.annotations = pd.read_csv(annotations_file, header=None, \n                               names=['Path', 'Label'], delimiter=',')\n        \n    def __len__(self):\n        return(len(self.annotations))\n    \n    def __getitem__(self, index):\n        audio = torchaudio.load(self.annotations['Path'][index])\n        label = self.annotations['Label'][index]\n        return(audio[0][0], label)","ae87d2b5":"audioMNIST_train = AudioDataset('..\/input\/audiomnistannot\/train_audioMNIST.csv')\naudioMNIST_test = AudioDataset('..\/input\/audiomnistannot\/test_audioMNIT.csv')\naudio, label = next(iter(audioMNIST_train))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(label)\nplt.plot(audio.numpy())\nplt.show()","9b689724":"for i in range(20):\n    print(len(audioMNIST_train[i][0]))","3402bf91":"class AudioDatasetSpectrogram(Dataset):\n    def __init__(self, annotations_file, ToSpectrogram, ToDB):\n        self.annotations = pd.read_csv(annotations_file, header=None, \n                               names=['Path', 'Label'], delimiter=',')\n        self.ToSpectrogram = ToSpectrogram\n        self.ToDB = ToDB\n        \n    def __len__(self):\n        return(len(self.annotations))\n    \n    def __getitem__(self, index):\n        audio_padded = torch.zeros((1,48000))\n        audio = torchaudio.load(self.annotations.iloc[index, 0])\n        audio_padded[0,:len(audio[0][0])] = audio[0][0]\n        label = self.annotations.iloc[index, 1]\n        spectrogram = self.ToSpectrogram(audio_padded)\n        spectrogram = self.ToDB(spectrogram)\n        return(spectrogram, label)","f046a29c":"ToSpectrogram = torchaudio.transforms.MelSpectrogram()\nToDB = torchaudio.transforms.AmplitudeToDB()\ntrain_dataset = AudioDatasetSpectrogram('..\/input\/audiomnistannot\/train_audioMNIST.csv', ToSpectrogram, ToDB)\ntest_dataset = AudioDatasetSpectrogram('..\/input\/audiomnistannot\/test_audioMNIT.csv', ToSpectrogram, ToDB)","e35e137c":"spectrogram, label = next(iter(train_dataset))\nprint(spectrogram.shape)\nplt.imshow(spectrogram[0], cmap='inferno')","b906805d":"train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True, num_workers=10)\ntest_loader = DataLoader(test_dataset, batch_size=500, shuffle=True, num_workers=10)","09e11595":"temp_img = 0\nfor img, label in train_loader:\n    temp_img += img.mean(dim=0)\ndiv = len(train_dataset)\/train_loader.batch_size\nmean_img = (temp_img\/div).float().to(device)","432772ae":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.ConvNet = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=7, stride=3),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(38*19*10, 10)\n        )\n        \n    def forward(self, x):\n        logits = self.ConvNet(x)\n        return(logits)","8deb51ff":"model = CNN().to(device)\nprint(model)","3a1290eb":"model_loss = nn.CrossEntropyLoss()\nmodel_optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","0fb60088":"def train_model(model, dataloader, model_loss, optimizer, device, mean=0):\n    total_loss = 0\n    total_correct = 0\n    model.train() # Specifies that the model should compute gradients\n    for X, y in dataloader:\n        #Zero mean and transfer data to device\n        X = X.to(device)\n        X = X - mean\n        y = y.to(device)\n        # Forward pass\n        prediction = model(X)\n        loss = model_loss(prediction, y.long())\n        # Update loss and score\n        total_loss += loss.item()\n        total_correct += (prediction.argmax(1)==y).sum().item()\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    size = len(dataloader.dataset)\n    avg_loss = loss\/size\n    score = total_correct\/size\n    return(avg_loss, score)\n\ndef test_model(model, dataloader, model_loss, device, mean=0):\n    total_loss = 0\n    total_correct = 0\n    model.eval() #Specifies that the model does not need to keep track of gradients\n    for X, y in dataloader:\n        X = X.to(device)\n        X = X - mean\n        y = y.to(device)\n        prediction = model(X)\n        loss = model_loss(prediction, y.long())\n        total_loss += loss.item()\n        total_correct += (prediction.argmax(1) == y).sum().item()\n        \n    size = len(dataloader.dataset)\n    avg_loss = total_loss\/size\n    score = total_correct\/size\n    return(avg_loss, score)","1acb3966":"train_losses, train_scores, test_losses, test_scores = [], [], [], []\nfor epoch in range(30):\n    print('Epoch:', epoch)\n    train_loss, train_score = train_model(model, train_loader, model_loss, model_optimizer, device, mean_img)\n    test_loss, test_score = test_model(model, test_loader, model_loss, device, mean_img)\n    train_losses.append(train_loss)\n    train_scores.append(train_score)\n    test_losses.append(test_loss)\n    test_scores.append(test_score)","f21d3ec6":"#Plot the train and test errors and losses\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,1, sharex=True)\naxs[0].plot(train_scores, label='Train')\naxs[0].plot(test_scores, label='Test')\naxs[0].legend(loc='lower right')\naxs[0].set_ylabel('Score')\naxs[1].plot(train_losses)\naxs[1].plot(test_losses)\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('Loss')\nfig.suptitle('Train x Test')\nfig.subplots_adjust(hspace = .001)\naxs[0].set_xticklabels(())\naxs[0].title.set_visible(False)\nplt.plot()\nprint('Final test score: ', test_scores[-1])","10e202ea":"Now I must create a dataset with the data.  \nPyTorch provides an API for converting audio files to Tensors, torchaudio.","f2720cff":"Before building the model I will calculate the mean, so we can zero mean the data.","57c08a83":"This will return the raw audio data as a tensor. Although it is possible to use the raw data to train a neural network, a more common approach is to apply convolutional neural networks to the spectogram of the audio, so we will return the spectogram instead.  \nAlso, we need to have all the audios to have the same length to make the spectrograms all the same size, so we will zero pad all the audios until they reach the desired length. Since the audios have less than 1s in length, we will zero pad them so they all have 1s second.","844256ca":"### Audio data\nI will use the audio MNIST dataset to demonstrate how to use audio data with PyTorch.  \nThe first thing I must do is create the annotations files for training and testing.","340c7d08":"    # With this code I have built a list with all the annotations.\n\n    import os\n\n    path = '..\/input\/audio-mnist\/data\/'\n    annot_list = []\n    for folder in os.listdir(path):\n        if os.path.isdir(path + '\/' + folder):\n            for file in os.listdir(path + '\/' + folder):  \n                file_path = path + '\/' + folder + '\/' + file\n                label = file[0]\n                annot_list.append((file_path, label))\n    # Although the os.listdir does not follow any apparent order, I will shuffle it just to be safe.\n\n    import random\n    random.shuffle(annot_list)\n\n    # Split in train and test\n\n    train_size = int(0.75*len(annot_list))\n    train_list = annot_list[:train_size]\n    test_list = annot_list[train_size:]\n\n    # Finally created the annnotations file\n\n    import csv\n\n    with open('train_audioMNIST.csv', mode='w') as csv_file:  \n        csv_writer = csv.writer(csv_file)\n        for item in train_list:\n            csv_writer.writerow([item[0], item[1]])  \n\n    with open('test_audioMNIT.csv', mode='w') as csv_file:  \n        csv_writer = csv.writer(csv_file)\n        for item in test_list:\n            csv_writer.writerow([item[0], item[1]]) "}}