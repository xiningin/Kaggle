{"cell_type":{"6d2666cd":"code","9b66484a":"code","02c85c98":"code","0f3619ac":"code","7e302eec":"code","c432d577":"code","d69a91e6":"code","418b22fd":"code","127e07f9":"code","6f25d511":"code","045ac750":"code","3b7a8670":"code","0785ec8b":"code","8bf8748a":"code","65e4f0e7":"code","25dba059":"code","1f5d0fca":"code","bb6e211c":"code","4fa942ac":"code","6a2a9689":"code","b2b4c0cf":"code","d8426d52":"code","b8c7b8d7":"code","711b6023":"code","bf0a320a":"code","eb27e34c":"code","d99898c0":"code","e10296db":"code","e610fed9":"code","a1de9840":"code","27bf2a9d":"code","fff986bf":"code","1b4d01b6":"code","87133004":"code","08be984e":"code","31ab9d13":"code","509a8762":"code","cc288a29":"code","2e5058df":"code","e7ae74d8":"markdown","8dda5859":"markdown","984e4a2d":"markdown","64d826d8":"markdown","60fd65c2":"markdown","7b4fc85b":"markdown","4ff58b83":"markdown","28f40001":"markdown","eca7a0f5":"markdown","af5d0727":"markdown","6352cf3c":"markdown","74c91d44":"markdown","234238b7":"markdown","0ec2f81c":"markdown","16090640":"markdown","6d702f7b":"markdown","dfd51b62":"markdown","56cc34e7":"markdown","61c40a3e":"markdown","c85e6ca9":"markdown","6dbd1d9d":"markdown","b7f47138":"markdown"},"source":{"6d2666cd":"from sklearn import preprocessing\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, HashingEncoder\nfrom category_encoders import HelmertEncoder, SumEncoder, BackwardDifferenceEncoder, PolynomialEncoder\nfrom category_encoders import TargetEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, LeaveOneOutEncoder, CatBoostEncoder, GLMMEncoder\nfrom keras import utils\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings, gc, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\n# We shall be compiling a summary table as we go along.\nsummary = pd.DataFrame({'inp2out_map': pd.Series(dtype=object),   # input-to-output map\n                        'nunique'    : pd.Series(dtype=int),      # number of unique (or distinct) values in output\n                        'unique'     : pd.Series(dtype='object'), # unique values in output\n                        'shape'      : pd.Series(dtype=int),      # rows-by-columns of output array\n                        'tictoc'     : pd.Series(dtype=int)})     # computation time i seconds\nsummary.index.name = 'encoder'\n# The grand summary is printed at the end of this notebook.","9b66484a":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col='id') # [['cat10', 'cat5', 'target']]\ntrain.sample(5)","02c85c98":"%%time\n# Would the output differ whether or not we supply the target as input?\n# Let's run a test with 10 encoders which optionally accept the target as input:\npick = train.columns[train.columns.str.startswith('cat')]\nfor ncoda in [OrdinalEncoder, HelmertEncoder, SumEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, BackwardDifferenceEncoder]:\n    tis = ncoda().fit_transform(train[pick])\n    tat = ncoda().fit_transform(train[pick], train['target']) \n#   Print 'True' if same; print 'False' otherwise\n    print((tis==tat).all().all(), ncoda)","0f3619ac":"# Some encoders use the target for computing the output; they can't run without being given the target. These are the target encoders.\nfor ncoda in [TargetEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, LeaveOneOutEncoder, CatBoostEncoder, GLMMEncoder]:\n    try:\n#       Run without train['target']:\n        tis = ncoda().fit_transform(train[pick])\n        print('Passed:', ncoda)    \n    except Exception as complaint:\n        print(complaint)\n        print('See, told ya it was going to break:', ncoda)    \n    gc.collect()","7e302eec":"# Let's do a scan for encoders which output a column named, 'intercept', which suggests contrast encoding, which we will see in the last section.\nfor ncoda in [OrdinalEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, HashingEncoder,\n              HelmertEncoder, SumEncoder, BackwardDifferenceEncoder, PolynomialEncoder]:\n    out = ncoda().fit_transform(train[pick])\n    if 'intercept' in out.columns:\n        print(str(ncoda))","c432d577":"# Let's zoom into a single column.\ntrain['cat10'].nunique(), train['cat10'].unique()\n# cat10 alone has 299 unique values altogether. This value in termed 'cardinality'.\n# This is an extreme case. Cardinalities are usually lower e.g. exam grades = A, B, C, D, E would have cardinality=5.","d69a91e6":"%%time\nfor which in [preprocessing.LabelEncoder, preprocessing.OrdinalEncoder, OrdinalEncoder,  # Section 1\n              preprocessing.OneHotEncoder, OneHotEncoder,                                # Section 2\n              preprocessing.LabelBinarizer, BinaryEncoder, BaseNEncoder,                 # Section 3\n              CountEncoder,                                                              # Section 4\n              HelmertEncoder, SumEncoder, BackwardDifferenceEncoder]:                    # Section 5\n    if which==preprocessing.OrdinalEncoder or which==preprocessing.OneHotEncoder: \n        inp = train['cat10'].values.reshape(-1, 1)\n    else:\n        inp = train['cat10']\n\n    tic = time.time()\n    if which==preprocessing.OneHotEncoder: \n        out = which(sparse=False).fit_transform(inp)\n    else:\n        out = which().fit_transform(inp)\n    tictoc = time.time() - tic\n\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n#   Grab the label, apply some minor hiding cosmetics:\n    label = str(which).replace(\"<class '\", \"\").replace(\"'>\", \"\")\n    if inp2out_map.isnull().any().any():\n        print(label, \"doesn't map one-to-one\")\n    summary.loc[label] = inp2out_map, len(unik), unik, inp2out_map.shape, tictoc\ncolumns_show = ['nunique', 'unique', 'shape', 'tictoc']\nsummary[columns_show]","418b22fd":"try:\n    out = LabelEncoder().fit_transform(train[['cat10', 'cat5']])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')    ","127e07f9":"out = OrdinalEncoder().fit_transform(train[['cat10', 'cat5']])\n# no complains","6f25d511":"def redressOutput(out):\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    return inp2out_map, len(unik), unik, inp2out_map.shape","045ac750":"tic = time.time()\nout = pd.factorize(train['cat10'])[0]\ntictoc = time.time() - tic\nsummary.loc['pd.factorize'] = redressOutput(out) + (tictoc, )\n\nlabelordinal_encoders = ['sklearn.preprocessing._label.LabelEncoder',\n                         'sklearn.preprocessing._encoders.OrdinalEncoder',\n                         'category_encoders.ordinal.OrdinalEncoder',\n                         'pd.factorize']\nsummary.loc[labelordinal_encoders, columns_show ]","3b7a8670":"# like scikit's LabelEncoder, pd.factorize can only handle one column at a time\ntry:\n    out = pd.factorize(train[['cat10', 'cat5']])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')    ","0785ec8b":"summary.loc[ summary.index.str.contains('OneHot') , columns_show ]\n# We've got two one-hot encoders so far. One from sklearn.preprocessing; another by category_encoders. Both work in a similar way. We can use either.","8bf8748a":"inp2out_map = summary.loc['category_encoders.one_hot.OneHotEncoder', 'inp2out_map']\ninp2out_map","65e4f0e7":"for row_idx, row_data in inp2out_map.iterrows():\n    vcount = row_data.value_counts().sort_index()\n    if not (vcount==pd.Series({0: 298, 1: 1})).all():\n        print('oopsy')\n# Loop passes without any oopsy, confirming that each row had strictly 1 one and 298 zeros.","25dba059":"# Let's take the chance to visualise the input-to-output mapping.\nplt.imshow(inp2out_map, cmap='gray'); plt.axis('equal'); _ = plt.axis('off')\n# black = zero; white = one. We find strictly 1 one on each row, zero everywhere else.","1f5d0fca":"tic = time.time()\nout = pd.get_dummies(train['cat10'])\ntictoc = time.time() - tic\nsummary.loc['pd.get_dummies'] = redressOutput(out) + (tictoc, )\n\nonehot_encoders = ['sklearn.preprocessing._encoders.OneHotEncoder',\n                   'category_encoders.one_hot.OneHotEncoder',\n                   'pd.get_dummies']\nsummary.loc[onehot_encoders, columns_show ]","bb6e211c":"try:\n    utils.to_categorical(train['cat10'])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')","4fa942ac":"tic = time.time()\nborrow = preprocessing.LabelEncoder().fit_transform(train['cat10'])\nout = utils.to_categorical(borrow)\ntictoc = time.time() - tic\nsummary.loc['utils.to_categorical'] = redressOutput(out) + (tictoc, )\n\nonehot_encoders = ['sklearn.preprocessing._encoders.OneHotEncoder',\n                   'category_encoders.one_hot.OneHotEncoder',\n                   'pd.get_dummies',\n                   'utils.to_categorical']\nsummary.loc[onehot_encoders, columns_show ]\n# We have at our disposal 4 one-hot encoders by different libraries.","6a2a9689":"def redressOutput(out):\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    return inp2out_map, len(unik), unik, inp2out_map.shape","b2b4c0cf":"inp = [0, 1, 2, 3, 4]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# All good: 5 unique values in, 5 unique values out.","d8426d52":"inp = [-1, 0, 1, 2, 3]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# 5 unique values in but just 4 out. What's happening here?","b8c7b8d7":"inp = [-2, -1, 0, 1, 2]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# Now it's even worse: 5 unique values in, just 3 unique values out.","711b6023":"for before, after in zip (inp, out):\n    print(before, after)\n# This explains why. Negative values weren't mapped the way we thought. \n# -2 was mapped to the same outcome as 1. \n# -1 got mapped to the same outcome as 2.","bf0a320a":"summary.loc[ ['category_encoders.binary.BinaryEncoder', 'category_encoders.basen.BaseNEncoder'] ][ columns_show ]","eb27e34c":"tis = summary.loc['category_encoders.binary.BinaryEncoder', 'inp2out_map']\ntat = summary.loc['category_encoders.basen.BaseNEncoder', 'inp2out_map']\n(tis==tat).all().all()","d99898c0":"tis.apply(lambda x: np.unique(x))\n# Column cat10_0 is all zeros and is therefore redundant.","e10296db":"# We can pass the option drop_invariant=True to avoid that redundancy.\nBaseNEncoder(drop_invariant=True).fit_transform(train['cat10'])\n# Now the redundant column disappears; we get 9 columns instead of 10.","e610fed9":"summary.loc['category_encoders.count.CountEncoder', 'inp2out_map']\n# The count encoder seems to output all sorts of integers.","a1de9840":"# Let's take a look where those values come from. For sampling sake we take the last 3 values and try to derive them.\nout = CountEncoder().fit_transform(train['cat10'], train['target'])\ninp, out.tail(3)","27bf2a9d":"# Where did 3011 come from?\n(train['cat10']=='HC').sum()","fff986bf":"# Where did 565 come from?\n(train['cat10']=='BF').sum()","1b4d01b6":"# Where did 5917 come from?\n(train['cat10']=='LM').sum()","87133004":"summary.loc[ 'category_encoders.helmert.HelmertEncoder', 'inp2out_map' ]","08be984e":"inp2out_map = summary.loc[ 'category_encoders.sum_coding.SumEncoder', 'inp2out_map' ]\ninp2out_map","31ab9d13":"column_sum = inp2out_map.sum()\ncolumn_sum\n# This is the signature of sum encoding: except the ```intercept``` column all columns sum to zero.","509a8762":"column_sum[ column_sum!= 0 ]","cc288a29":"summary.loc[ 'category_encoders.backward_difference.BackwardDifferenceEncoder', 'inp2out_map' ]","2e5058df":"summary[columns_show]","e7ae74d8":"# One-to-one simple, target-independent encoders","8dda5859":"## 1.2 pandas does label encoding too","984e4a2d":"Compared to label and ordinal encoders, we find that with one-hot encoders:\n* ```nunique``` dropped from 299 to 2;\n* the number of columns increased from 1 to 299.\n\nLet's see how a one-hot encoder maps input to output:","64d826d8":"### Warning\n```keras.utils.to_categorical``` doesn't work with negative input.","60fd65c2":"## 5.3 Backward-Difference Encoder","7b4fc85b":"# 2. One-hot encoders\n## 2.1 by scikit-learn and catagory-encoders","4ff58b83":"## 2.3 by keras\nBut with numeric input only. ```cat10``` is string, not numeric. We would need to first convert from string to numeric.","28f40001":"# Encoder types: a broad-stroke scan\nWe've got 2 dozen encoders here. Let's take an overview by trying to group them into families according to observable behaviors.","eca7a0f5":"We shall explore 24 encoders from 4 libraries:\n\n| library | one-hot encoders | other simple encoders | contrast encoders | target\/Bayesian encoders |\n| --- | --- | --- | --- | --- |\n| [sklearn.preprocessing](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing) | OneHotEncoder | LabelEncoder <br> OrdinalEncoder <br> LabelBinarizer | |\n| [category_encoders](https:\/\/contrib.scikit-learn.org\/category_encoders) | OneHotEncoder | OrdinalEncoder <br> BinaryEncoder <br> BaseNEncoder <br> CountEncoder <br> HashingEncoder| HelmertEncoder <br> SumEncoder <br> BackwardDifferenceEncoder <br> PolynomialEncoder | TargetEncoder <br> MEstimateEncoder <br> WOEEncoder <br> JamesSteinEncoder <br> LeaveOneOutEncoder <br> CatBoostEncoder <br> GLMMEncoder |\n| [pandas](https:\/\/pandas.pydata.org) | get_dummies | factorize | | |\n| [keras.utils](https:\/\/keras.io\/api\/utils) | to_categorical | | | |\n\n<br>\nEncoders map the original categories (often dtype=string) to a set of representing values (often dtype=int for simple encoders; dtype=float for target encoders). This notebook walks through a tour of the encoders listed in the table, exploring each non-target encoder one by one, producing a comparison table at the end. Target encoders shall be explored in detail in a separate notebook.\n<br><br>\nWhen to use which encoder to solve what problems? There is a good guide here: [Encode Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning Pipeline](https:\/\/innovation.alteryx.com\/encode-smarter).","af5d0727":"## 5.2 Sum Encoder","6352cf3c":"## 1.1 LabelEncoder vs OrdinalEncoder\n* LabelEncoder encodes one variable at a time; meant for encoding target labels (as in classification problems). \n* OrdinalEncoder encodes multiple variables\/columns at a time; meant for encoding features (plural).\n\nLet's see that in action:","74c91d44":"# 5. Contrast Encoders\nThese are [contrast encoders](https:\/\/stats.idre.ucla.edu\/r\/library\/r-library-contrast-coding-systems-for-categorical-variables) characterised by the presence of an ```intercept``` in the output.","234238b7":"One-hot encoding is thus name because for each row there is strictly one 1; all other columns must be zero. Let's do a quick check:","0ec2f81c":"This notebook is getting a little long. We've covered simple, one-to-one mapping encoders. Let's do target encoders in another notebook!","16090640":"# Is encoding optional?\nNot always. Some packages can't digest string-type data without encoding. 'Donkey', 'horse' and 'mule', for instance, would not work whereas 0, 1 and 2 would.\n\nEven when the package can digest data without encoding, they sometimes learn encoded data better.","6d702f7b":"## 5.1 Helmert Encoder","dfd51b62":"# 4. Count Encoder","56cc34e7":"# 3. Binary & Base-N Encoders\nBase-N encoding is the superset of \n* binary encoding (N=2);\n* one-hot encoding (N=1).\nBy default ```category_encoders.BaseNEncoder``` takes N=2; the output is there for identical to ```category_encoders.BinaryEncoder```:","61c40a3e":"#### But do we really need 10 columns?\n2^8, 2^9 = 256, 512 so we should only need 9 columns to binary-encode 299 categories.","c85e6ca9":"# Grand summary","6dbd1d9d":"# 1. Label & Ordinal encoders\nFrom the table we find the first 3 rows:\n* sklearn.preprocessing._label.LabelEncoder\n* sklearn.preprocessing._encoders.OrdinalEncoder\n* category_encoders.ordinal.OrdinalEncoder\n\nrather similar to each other:\n* they all output 299 unique numbers, where 299 is the cardinality of the original input;\n* they all output a single column;\n* they basically do one-to-one mapping of the original input;\n* they run quickly compared to the rest.","b7f47138":"## 2.2 by pandas"}}