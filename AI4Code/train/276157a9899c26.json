{"cell_type":{"95456171":"code","9f6131b8":"code","e1fc909a":"code","0450b561":"code","686ce893":"code","0b85250a":"code","53da6439":"code","3a77bd3a":"markdown"},"source":{"95456171":"import ast\nimport os\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()","9f6131b8":"TRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\ndf_train = pd.read_csv(os.path.join(TRAIN_PATH,'train.csv'))","e1fc909a":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row","0450b561":"def draw_yolox_predictions(img, bboxes, color=(45,45,252)):\n    for i in range(len(bboxes)):\n            box = bboxes[i]\n            x0 = int(box[0])\n            y0 = int(box[1])\n            x1 = int(box[2])\n            y1 = int(box[3])\n\n            cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n    return img\n\ndef xywh2xyxy(bboxes):\n    \n    output = []\n    \n    for box in bboxes:   \n        box[0] = box[0] #x0\n        box[1] = box[1] #y0\n        box[2] = box[0] + box[2] #x1\n        box[3] = box[1] + box[3] #y1  \n        output.append(box)\n    \n    return output","686ce893":"# Taken only annotated photos\ndf_train[\"num_bbox\"] = df_train['annotations'].apply(lambda x: str.count(x, 'x'))\n\n#Annotations \ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\n#Path of images\ndf_train = df_train.progress_apply(get_path, axis=1)","0b85250a":"videos_df = []\nfor video_id in range(3):\n    video_id_df = df_train.query(\"video_id==\" + str(video_id))\n    print(\"Bboxes on video \" + str(video_id) + \" per frame: \" + str(video_id_df.count()[0]))\n    print(\"Qty of Bboxes on video \" + str(video_id) + \": \" + str(video_id_df['num_bbox'].sum()))\n    videos_df.append(video_id_df)","53da6439":"IMAGES_PATHS = \"\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/\"\n%cd \/kaggle\/working\n\nvideos_qty = df_train['video_id'].unique().tolist()\n\nfor video_id in videos_qty:\n    #Choose a video and get its df\n    video_df = df_train[df_train.video_id==video_id]\n    \n    print(\"Exporting video \" + str(video_id) + \"...\")\n    out = cv2.VideoWriter('video_' + str(video_id) + '.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, (1280,720))\n    \n    #Get all the sequences of that video\n    video_sequences = video_df['sequence'].unique().tolist()\n    \n    for video_sequence in video_sequences:\n        #Choose a sequence and go thru each of the video frames\n        sequence_frames = df_train[df_train.sequence==video_sequence]['video_frame'].tolist()\n        \n        print(\"Writing sequence: \" + str(video_sequence) + \" to video \" + str(video_id))\n        for video_frame in tqdm(sequence_frames):\n            #use that video frame to load the image\n            filename = IMAGES_PATHS + 'video_' + str(video_id) + '\/' + str(video_frame) +'.jpg'\n            img = cv2.imread(filename)\n            \n            #Draw annotations to img\n            img_row = df_train[df_train.image_path==filename]\n            bboxes = img_row['bboxes'].values[0]\n            bboxes = xywh2xyxy(bboxes)\n\n            img = draw_yolox_predictions(img, bboxes)\n\n            height, width, layers = img.shape\n            size = (width,height)\n            out.write(img)\n            \n    out.release()","3a77bd3a":"# Generate videos from COTS dataset frames\nI hope you find this notebook useful!\n\nSpecial thanks to **CASFRANCO**, much of this code is from his notebook:\n* https:\/\/www.kaggle.com\/casfranco\/eda-let-s-understand-the-data-protect-the-reef"}}