{"cell_type":{"657d7acf":"code","7cc8c6b2":"code","ac2cc9f1":"code","64905827":"code","8e18f528":"code","a2ecc781":"code","64e3cc0e":"code","4e3b729a":"code","6dee17cf":"code","4d2368fe":"code","b7c6858b":"code","2acb66fd":"code","77eb523d":"markdown","2c2b07e3":"markdown","9c5eef35":"markdown","6d71dd65":"markdown","80ae094c":"markdown"},"source":{"657d7acf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n","7cc8c6b2":"data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\nprint(data.shape)\ndata.head()","ac2cc9f1":"test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nX_test = test.drop(['id'], axis=1)","64905827":"X = data.drop(['id', 'song_popularity'], axis=1)\ny = data['song_popularity']","8e18f528":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)","a2ecc781":"class TabularNetworkConfig():\n    def __init__(\n        self, \n        target_feature_name, \n        target_feature_labels, \n        numeric_feature_names, \n        categorical_features_with_vocabulary,\n        num_outputs,\n        out_activation,\n        num_transformer_blocks, \n        num_heads,\n        embedding_dim,\n        mlp_hidden_units_factors,\n        dropout_rate,\n        use_column_embedding,\n    ):\n        self.TARGET_FEATURE_NAME = target_feature_name\n        self.TARGET_FEATURE_LABELS = target_feature_labels\n        self.NUMERIC_FEATURE_NAMES = numeric_feature_names\n        self.CATEGORICAL_FEATURES_WITH_VOCABULARY = categorical_features_with_vocabulary\n        self.CATEGORICAL_FEATURE_NAMES = list(self.CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n        self.FEATURE_NAMES = self.NUMERIC_FEATURE_NAMES + self.CATEGORICAL_FEATURE_NAMES\n        self.NUM_OUT = num_outputs\n        self.OUT_ACTIVATION = out_activation\n        self.NUM_TRANSFORMER_BLOCKS = num_transformer_blocks\n        self.NUM_HEADS = num_heads\n        self.EMBEDDING_DIM = embedding_dim\n        self.MLP_HIDDEN_UNITS_FACTORS = mlp_hidden_units_factors\n        self.DROPOUT_RATE = dropout_rate\n        self.USE_COLUMN_EMBEDDING = use_column_embedding\n        \n\nclass BaseTabularNetwork():\n    @staticmethod\n    def get_inputs(config):\n        return {\n        feature_name: L.Input(\n            name=feature_name,\n            shape=(),\n            dtype=(tf.float32 if feature_name in config.NUMERIC_FEATURE_NAMES else tf.string),\n        )\n        for feature_name in config.FEATURE_NAMES\n    }\n    \n    @staticmethod\n    def encode_inputs(inputs, config, prefix=''):\n        cat_features = []\n        num_features = []\n        for feature_name in inputs:\n            if feature_name in config.CATEGORICAL_FEATURE_NAMES:\n                vocabulary = config.CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n                lookup = L.StringLookup(\n                    vocabulary=vocabulary,\n                    mask_token=None,\n                    num_oov_indices=0,\n                    output_mode=\"int\",\n                    name=f\"{prefix}{feature_name}_lookup\"\n                )\n                encoded_feature = lookup(inputs[feature_name])\n                embedding = L.Embedding(\n                    input_dim=len(vocabulary), output_dim=config.EMBEDDING_DIM,\n                    name=f\"{prefix}{feature_name}_embeddings\"\n                )\n                encoded_feature = embedding(encoded_feature)\n                cat_features.append(encoded_feature)\n            else:\n                encoded_feature = L.Reshape((1, ), name=f\"{prefix}{feature_name}_reshape\")(inputs[feature_name])\n                num_features.append(encoded_feature)\n\n        return cat_features, num_features","64e3cc0e":"def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n    mlp_layers = []\n    for units in hidden_units:\n        mlp_layers.append(normalization_layer),\n        mlp_layers.append(L.Dense(units, activation=activation))\n        mlp_layers.append(L.Dropout(dropout_rate))\n\n    return keras.Sequential(mlp_layers, name=name)","4e3b729a":"class TabTransformer(BaseTabularNetwork):\n    @classmethod\n    def from_config(cls, name, config):\n        inputs = cls.get_inputs(config)\n        cat_features, num_features = cls.encode_inputs(inputs, config)\n        \n        cat_features = tf.stack(cat_features, axis=1)\n        num_features = L.concatenate(num_features)\n        \n        if config.USE_COLUMN_EMBEDDING:\n            num_columns = cat_features.shape[1]\n            column_embedding = L.Embedding(\n                input_dim=num_columns, output_dim=config.EMBEDDING_DIM\n            )\n            column_indices = tf.range(start=0, limit=num_columns, delta=1)\n            cat_features = cat_features + column_embedding(column_indices)\n        \n        for block_idx in range(config.NUM_TRANSFORMER_BLOCKS):\n            attention_output = L.MultiHeadAttention(\n                num_heads=config.NUM_HEADS,\n                key_dim=config.EMBEDDING_DIM,\n                dropout=config.DROPOUT_RATE,\n                name=f\"multihead_attention_{block_idx}\",\n            )(cat_features, cat_features)\n            x = L.Add(name=f\"skip_connection1_{block_idx}\")([attention_output, cat_features])\n            x = L.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n            feedforward_output = create_mlp(\n                hidden_units=[config.EMBEDDING_DIM],\n                dropout_rate=config.DROPOUT_RATE,\n                activation=keras.activations.gelu,\n                normalization_layer=L.LayerNormalization(epsilon=1e-6),\n                name=f\"feedforward_{block_idx}\",\n            )(x)\n            x = L.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n            cat_features = L.LayerNormalization(name=f\"layer_norm2_{block_idx}\", epsilon=1e-6)(x)\n        \n        cat_features = L.Flatten()(cat_features)\n        num_features = L.LayerNormalization(epsilon=1e-6)(num_features)\n        features = L.concatenate([cat_features, num_features])\n        mlp_hidden_units = [factor * features.shape[-1] for factor in config.MLP_HIDDEN_UNITS_FACTORS]\n        features = create_mlp(\n            hidden_units=mlp_hidden_units,\n            dropout_rate=config.DROPOUT_RATE,\n            activation=keras.activations.selu,\n            normalization_layer=L.BatchNormalization(),\n            name=\"MLP\",\n        )(features)\n        \n        outputs = L.Dense(units=config.NUM_OUT, activation=config.OUT_ACTIVATION, name=\"outputs\")(features)\n        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n        return model","6dee17cf":"model_config = TabularNetworkConfig(\n    target_feature_name=\"song_popularity\", \n    target_feature_labels=[\"0\", \"1\"], \n    numeric_feature_names=[\n        'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n        'speechiness', 'tempo', 'audio_valence'\n    ], \n    categorical_features_with_vocabulary={\n        'key': list(map(str, range(12))),\n        'audio_mode': [\"0\", \"1\"],\n        'time_signature': [\"2\", \"3\", \"4\", \"5\"]   \n    },\n    num_outputs=1,\n    out_activation=\"sigmoid\",\n    num_transformer_blocks=3, \n    num_heads=4,\n    embedding_dim=32,\n    mlp_hidden_units_factors=[2, 1],\n    dropout_rate=0.2,\n    use_column_embedding=True,\n)\n\nMAX_EPOCHS  = 250\n\nget_callbacks = lambda : [\n    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=10, verbose=1, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n]","4d2368fe":"keras.utils.plot_model(\n    TabTransformer.from_config(\"tab_transformer\", model_config),\n    show_shapes=True, rankdir=\"LR\", to_file=\"tab_transformer.png\"\n)","b7c6858b":"preds = []\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    mean_imputer = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')), \n        ('scaler', StandardScaler())\n    ]).fit(X_train[model_config.NUMERIC_FEATURE_NAMES])\n    mode_imputer = SimpleImputer(strategy='most_frequent').fit(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    \n    X_train = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_train[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_train[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n    X_valid = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_valid[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_valid[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n    X_test_ = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_test[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_test[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n        \n    data_train = tf.data.Dataset.from_tensor_slices((\n    {col: X_train[col].values.tolist() for col in model_config.FEATURE_NAMES}, \n        y_train.values.tolist()\n    )).batch(1024)\n    data_valid = tf.data.Dataset.from_tensor_slices((\n        {col: X_valid[col].values.tolist() for col in model_config.FEATURE_NAMES}, \n        y_valid.values.tolist()\n    )).batch(1024)\n    data_test = tf.data.Dataset.from_tensor_slices((\n        {col: X_test_[col].values.tolist() for col in model_config.FEATURE_NAMES}\n    )).batch(1024)\n    \n    model = TabTransformer.from_config(\"deep_network\", model_config)\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n    model.fit(data_train, validation_data=data_valid, callbacks=get_callbacks(), epochs=MAX_EPOCHS)  \n    preds.append(model.predict(data_test))","2acb66fd":"submissions = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nsubmissions['song_popularity'] = np.array(preds).mean(axis=0)\nsubmissions.to_csv('preds.csv', index=False)","77eb523d":"# Model","2c2b07e3":"# Tab Transformer\n\nThe TabTransformer (introduced [TabTransformer: Tabular Data Modeling Using Contextual Embeddings](https:\/\/arxiv.org\/pdf\/2012.06678.pdf)) is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher predictive accuracy.\n\nThe TabTransformer architecture works as follows:\n* All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n* A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n* The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n* The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n* A softmax classifer is applied at the end of the model.","9c5eef35":"# Submissions","6d71dd65":"# Training","80ae094c":"# Data"}}