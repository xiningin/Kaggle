{"cell_type":{"e89fc0b3":"code","31b08578":"code","60bbe67d":"code","3fb4afdf":"code","8779b4ef":"code","b4ebdebc":"code","66ee84b6":"code","79aaba2b":"code","bfa46e75":"code","aadbe5a2":"code","29894021":"code","ce282dc1":"code","c9e16f22":"code","ffe678f4":"code","498d5967":"code","21cebe6d":"code","e57e9b3e":"code","a06806b3":"code","386af08e":"code","30fcf4d8":"code","abd0e067":"code","db651501":"code","4f6b24b2":"code","9cd1828e":"code","5de2ca5f":"code","bf8c9700":"code","61bbcf5c":"code","41f6b740":"code","269788e4":"code","9bc430a8":"code","facaa6b3":"code","d1708d28":"code","5336a6ee":"code","808f83f5":"code","64cbaaa7":"code","3b5ca783":"code","bc9de5ec":"code","3f7d93f8":"code","89a3027f":"code","d350fc25":"code","7da89aec":"code","16a33bdd":"code","8a4ffc41":"code","ba59ff24":"code","78cd8f77":"code","9a739ff4":"code","02aebcc6":"code","49176a72":"code","240afa9f":"code","7a6c6fb5":"code","42ca7679":"markdown","2e37f004":"markdown","d5c880ed":"markdown","0fa0b314":"markdown","1786ba11":"markdown","e0726fbd":"markdown","3db83877":"markdown","039cfbd7":"markdown","5dad1cf1":"markdown","ad7c0cf4":"markdown","5fd08247":"markdown","83e03fa6":"markdown","d1451ff4":"markdown"},"source":{"e89fc0b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","31b08578":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","60bbe67d":"fake_data = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue_data = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")","3fb4afdf":"fake_data.head()","8779b4ef":"true_data.head()","b4ebdebc":"true_data['class'] = 1\nfake_data['class'] = 0","66ee84b6":"# Merging both the datasets\ndata = pd.concat([true_data,fake_data])","79aaba2b":"data.head()","bfa46e75":"data['class'].value_counts().plot(kind = 'bar', color = ['g','b'])\nplt.xlabel('Class')\nplt.ylabel('No. of particulars')","aadbe5a2":"# Checking for missing values\ndata.isnull().sum()","29894021":"data['subject'].value_counts()","ce282dc1":"# Merging the title and text column\ndata['text'] = data['title'] + \" \" + data['text']\n\n# Dropping the title, subject and date columns\ndata.drop(columns = ['title','date','subject'], inplace = True)","c9e16f22":"data.head()","ffe678f4":"split = np.random.rand(len(data)) < 0.7\ntrain = data[split]\ntest = data[~split]","498d5967":"# Checking whether target classes are balanced\n# If they are balanced in the train set, then we do not need to check the test set as well\ntrain['class'].value_counts()","21cebe6d":"train['class'].value_counts().plot(kind = 'bar', color = ['r','b'])","e57e9b3e":"train.head()","a06806b3":"import warnings\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)","386af08e":"import re\nimport string","30fcf4d8":"# Removing whitespaces and single word characters from the text\ntrain['text'] = train.text.apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\ntest['text'] = test.text.apply(lambda x: re.sub(r'[^\\w\\s]', '',x))","abd0e067":"train['text'] = [re.sub(r'\\b\\w{1,3}\\b', '', c) for c in train['text']]\ntest['text'] = [re.sub(r'\\b\\w{1,3}\\b', '', c) for c in test['text']]","db651501":"# Removing numeric characters\ntrain['text'] = [re.sub('\\d','',n) for n in train['text']]\ntest['text'] = [re.sub('\\d','',n) for n in test['text']]","4f6b24b2":"# Converting all text to lower case characters\ntrain['text'] = [t.lower() for t in train['text']]\ntest['text'] = [t.lower() for t in test['text']]","9cd1828e":"train['text'].dropna(inplace = True)\ntest['text'].dropna(inplace = True)","5de2ca5f":"#import nltk\n#regex_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n#train['text'] = train['text'].map(regex_tokenizer.tokenize())\n#test['text'] = test['text'].map(regex_tokenizer.tokenize())","bf8c9700":"import nltk\nfrom nltk.tokenize import word_tokenize\n\n# Word Tokenization\ntrain['text'] = [word_tokenize(i) for i in train['text']]\ntest['text'] = [word_tokenize(i) for i in test['text']]","61bbcf5c":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\ntrain['text'] = [[i for i in j if not i in stop_words] for j in train['text']]\ntest['text'] = [[i for i in j if not i in stop_words] for j in test['text']]","41f6b740":"#train.drop(columns = ['text'], inplace = True)\n#test.drop(columns = ['text'], inplace = True)","269788e4":"from tensorflow.keras.preprocessing.text import Tokenizer","9bc430a8":"train['text'] = train['text'].apply(lambda x : ' '.join(x))\ntest['text'] = test['text'].apply(lambda x : ' '.join(x))","facaa6b3":"X = train.text\ntest_X = test.text\ntrain_labels = train['class'].values\ntest_labels = test['class'].values","d1708d28":"from collections import Counter\n\n# Finding the number of unique word in the corpus\ndef word_counter(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","5336a6ee":"counter = word_counter(X)\nwords = len(counter)","808f83f5":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train['text'])","64cbaaa7":"train_X = tokenizer.texts_to_sequences(train['text'])","3b5ca783":"# Numeric tokens for the first 25 words of the the 5th news entry in our train set\ntrain_X[5][:25]","bc9de5ec":"test_X = tokenizer.texts_to_sequences(test['text'])","3f7d93f8":"test_labels = test['class'].values","89a3027f":"word_index = tokenizer.word_index\nfor word, num in word_index.items():\n    print(f\"{word} -> {num}\")\n    if num == 10:\n        break        ","d350fc25":"nos = np.array([len(x) for x in train_X])\nlen(nos[nos  < 500])","7da89aec":"len(counter)","16a33bdd":"from keras.preprocessing.sequence import pad_sequences\n\n#Lets keep all news with upto 500 words, add padding to news with less than 500 words\nmaxlen = 500 \n\n#Making all news of size max length defined above\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)","8a4ffc41":"# All sequences have been made of length 500\n# if a news had more than 500 words they have been truncated to 500\n# if a news had less than 500 that sequence has been padded with 0\nlen(train_X[20])","ba59ff24":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nimport tensorflow as tf\n\ndef leaky_relu(z, name = None):\n    return tf.maximum(0.01*z,z, name = name)\n\nmodel = Sequential()\n\nmodel.add(Embedding(words+1,32,input_length = 500)) # embedding layer\nmodel.add(LSTM(64)) # RNN layer\n#model.add(Dense(units = 32 , activation = leaky_relu)) # Dense layer with leaky_relu activation\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate = 3e-4)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])","78cd8f77":"model.summary()","9a739ff4":"#Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_X,train_labels) ","02aebcc6":"model.fit(train_X, train_labels, validation_split=0.2, epochs=10)","49176a72":"predictions = model.predict_classes(test_X)","240afa9f":"from sklearn.metrics import accuracy_score, classification_report\naccuracy_score(test_labels,predictions)","7a6c6fb5":"print(classification_report(test_labels,predictions))","42ca7679":"### Tokenization,Punctuations, Stopwords and special characters removal ","2e37f004":"### Text Data Cleaning","d5c880ed":"First, using simple tokenization i.e. assigning a number to every single word in our text.","0fa0b314":"Let us see how balanced our dataset is.","1786ba11":"### Vectorization (Converting word to numbers)","e0726fbd":"The 'News' subject contains all kinds of news in general and is not categorized, also since we have merged two datasets the subject column contains different names for all topics. So, we would be better off dropping the subject  column.","3db83877":"### Initial Data Split (Train and Test sets)","039cfbd7":"After merging dataframes, we'll shuffle the entries randomly because we will be splitting the data into train and test sets and we need both the splits to be balanced.","5dad1cf1":"## Data Cleaning and Exploratory Analysis","ad7c0cf4":"### Sequence Padding","5fd08247":"The target classes are balanced","83e03fa6":"Our dataset is quite balanced","d1451ff4":"Some of the options at our disposal:\n#### Bag of words\n#### TF-IDF vectorization\n#### Tokenizer (Tensorflow(keras) tokenizer)\n#### word2vec\n#### GloVe vectors"}}