{"cell_type":{"793d51cb":"code","a6d127da":"code","2b6a0422":"code","d24c2e4a":"code","26992984":"code","7619b70b":"code","59f8826f":"code","25043dd9":"code","1485bb21":"code","8598c3ce":"code","d9e3db7d":"code","2c7a514c":"code","a9e3074f":"code","acc3e79b":"code","e2e08237":"code","c1235aae":"code","fc6096dc":"code","e4311501":"code","a8638e52":"code","80f70a6b":"code","ca7412b7":"code","26916eaa":"code","d0ee89ff":"code","14505be3":"code","6ce6aa91":"code","e47dcd5f":"code","1c5ea3a9":"code","46ff02c9":"code","1d045191":"code","5ab4dfed":"code","6756f909":"code","14a2dff1":"code","790f2565":"code","147a810f":"code","63f8196c":"code","de9fb8ea":"code","3d116294":"code","d905dd39":"code","b06f184e":"code","fac7c5f2":"code","a2ada75a":"code","52c43c91":"code","9ff60e6b":"code","1a4ffa82":"code","59000d57":"code","db356cfc":"code","9f2c90e1":"code","8558ccec":"code","e6397f23":"code","e01e8dac":"code","27e638d3":"code","d5f2489a":"code","c8d84a78":"code","486c1ee1":"code","f9df0822":"code","c46d2135":"code","802d8065":"code","9fbd8b0d":"code","00b9375d":"code","115600b8":"code","4da90c2e":"code","4289da97":"code","7d4d29fc":"code","f507bad0":"code","47bca201":"code","a852a515":"code","f330663c":"code","851956c9":"code","7b14bc56":"code","070ea3c8":"code","0137d43e":"code","89d93a43":"code","224802af":"code","08739d81":"code","0988f7ee":"code","0fb51ef8":"code","36fea663":"code","44ee2e39":"code","43b92af5":"code","3dc9cb0a":"markdown","a04fc4fa":"markdown","8e84db55":"markdown","5b0fbdb4":"markdown","96d7d0a9":"markdown","5c0eac42":"markdown","2bcfb8aa":"markdown","3e5c75a1":"markdown","a1b52b59":"markdown","1d738505":"markdown","8a040bbc":"markdown","3a94c522":"markdown","f29b5453":"markdown","cc1ae019":"markdown","66f703bd":"markdown","961b7e58":"markdown","ab807b9a":"markdown","3a302a75":"markdown","440f5bd3":"markdown","5dc70633":"markdown","49e3f0a9":"markdown"},"source":{"793d51cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6d127da":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PREPROCESSING\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('seaborn-whitegrid')","2b6a0422":"sales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsubmission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","d24c2e4a":"print(f'sales_train.csv : {sales.shape}')\nsales.head(3)","26992984":"sales.info()","7619b70b":"sales[['date_block_num','item_price','item_cnt_day']].describe().T","59f8826f":"print('There are {} records in our train dataframe.'.format(sales.shape[0]))\nprint('there are {} unique Item in sales dataset'.format(len(sales['item_id'].unique())))","25043dd9":"pd.DataFrame(sales.nunique(),columns={'count'}).sort_values('count', ascending = False)","1485bb21":"pd.DataFrame(sales.isnull().sum(),columns={'count'})","8598c3ce":"# inspiration code: https:\/\/www.kaggle.com\/gaetanlopez\/how-to-make-clean-visualizations\n\nrecords_num = str(int(sales.shape[0]\/1000))\ncolumns_num = sales.shape[1]\nmean_priced = int(sales['item_price'].describe().loc['mean'])\nunique_items = len(sales['item_id'].unique())\n\nfig=plt.figure(figsize=(5,2),facecolor='white')\n\n\nax0=fig.add_subplot(1,1,1)\n# Hide grid lines and take care of the color\nax0.grid(False)\nax0.set_facecolor('white')\nax0.text(1.1,1,\"Key Figures\",color='black',fontsize=28, fontweight='bold', fontfamily='monospace',\n         ha='center')\n\n\nax0.text(0,0.4, records_num+'k' ,color='blue',fontsize=25, fontweight='bold', fontfamily='monospace',ha='center')\nax0.text(0,0.001,\"Number of items \\nin the train dataset\",color='dimgrey',fontsize=17,\n         fontweight='light', fontfamily='monospace',ha='center')\n\nax0.text(0.75,0.4,columns_num ,color='blue',fontsize=25, fontweight='bold', fontfamily='monospace',ha='center')\nax0.text(0.75,0.001,\"Number of features \\nin the dataset\",color='dimgrey',fontsize=17, \n         fontweight='light', fontfamily='monospace',ha='center')\n\nax0.text(1.5,0.4, mean_priced ,color='blue',fontsize=25, fontweight='bold', fontfamily='monospace',ha='center')\nax0.text(1.5,0.001,\"Mean price for \\neach item\",color='dimgrey',fontsize=17, fontweight='light', \n         fontfamily='monospace',ha='center')\n\nax0.text(2.25,0.4,unique_items ,color='blue',fontsize=25, fontweight='bold', fontfamily='monospace',ha='center')\nax0.text(2.25,0.001,\"Number of unique \\nitems\",color='dimgrey',fontsize=17, fontweight='light', \n         fontfamily='monospace',ha='center')\n\nax0.set_yticklabels('')\nax0.tick_params(axis='y',length=0)\nax0.tick_params(axis='x',length=0)\nax0.set_xticklabels('')\n\nfor direction in ['top','right','left','bottom']:\n    ax0.spines[direction].set_visible(False)","d9e3db7d":"print(f'test.csv : {test.shape}')\ntest.head(3)","2c7a514c":"test.info()","a9e3074f":"print('There are {} records in our test dataframe.'.format(test.shape[0]))","acc3e79b":"pd.DataFrame(test.isnull().sum(),columns={'count'})","e2e08237":"print(f'shops.csv : {shops.shape}')\nshops.head(3)","c1235aae":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['city'].unique()","fc6096dc":"shops.loc[shops['city']=='!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\n\n# encoding\nshops['city_code'] = LabelEncoder().fit_transform(shops['city']).astype(np.int8)\nshops.head(3)","e4311501":"print(f'items.csv : {items.shape}')\nitems.head(3)","a8638e52":"# Create the date the product was first sold as a feature\nitems['first_sale_date'] = sales.groupby('item_id').agg({'date_block_num': 'min'})['date_block_num']\n\nitems.head(3)","80f70a6b":"items['first_sale_date'].isna().sum()","ca7412b7":"# Replace NaN of first_sale_date with 34\nitems['first_sale_date'] = items['first_sale_date'].fillna(34)","26916eaa":"print(f'item_categories.csv : {item_categories.shape}')\nitem_categories.head(3)","d0ee89ff":"item_categories['item_category_name'][:10]","14505be3":"item_categories['item_maincategory_name'] = item_categories['item_category_name'].str.split(' - ').map(lambda x: x[0])\nprint('We have {} unique main categories.'.format(len(item_categories['item_maincategory_name'].unique())))\nitem_categories['item_maincategory_name'].unique()","6ce6aa91":"item_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b Android', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b MAC', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b PC', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\n\nitem_categories.loc[item_categories['item_maincategory_name']=='\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b (\u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b)', 'item_maincategory_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b'\n\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)', 'item_maincategory_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)', 'item_maincategory_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438'\n","e47dcd5f":"item_categories['item_maincategory_id'] = LabelEncoder().fit_transform(item_categories['item_maincategory_name']).astype(np.int8)\nitem_categories.head(3)","1c5ea3a9":"item_categories.item_maincategory_name.value_counts()","46ff02c9":"def make_etc(x):\n    if len(item_categories[item_categories['item_maincategory_name']==x]) >= 5:\n        return x\n    else:\n        return 'etc'\n\n# Replace with 'etc' if category count is less than 5\nitem_categories['item_maincategory_name'] = item_categories['item_maincategory_name'].apply(make_etc)","1d045191":"item_categories['item_subcategory_name'] = item_categories['item_category_name'].str.split('-')\\\n.map(lambda x: '-'.join(x[1:]).strip() if len(x) > 1 else x[0].strip())\nprint('We have {} unique sub categories.'.format(len(item_categories['item_subcategory_name'].unique())))\nitem_categories['item_subcategory_name'].unique()","5ab4dfed":"item_categories.item_subcategory_name.value_counts()","6756f909":"item_categories['item_subcategory_id'] = LabelEncoder().fit_transform(item_categories['item_subcategory_name']).astype(np.int8)\nitem_categories.head(3)","14a2dff1":"item_categories.item_subcategory_id.value_counts()","790f2565":"print(f'sample_submission.csv : {submission.shape}')\nsubmission.head(3)","147a810f":"# Merge Item and Item Categories dataframes on 'item_category_id'\nitem_info = pd.merge(items, item_categories, on='item_category_id', how='inner')\nitem_info.head(2)","63f8196c":"# Merge sales and item_info dataframes on 'item_id'\ntrain_tmp = pd.merge(sales,item_info, on='item_id', how='inner')\ntrain_tmp.head(2)","de9fb8ea":"# Merge train_tmp and sales dataframes on 'shop_id'\ntrain = pd.merge(train_tmp, shops, on='shop_id', how='inner')\ntrain.head(3)","3d116294":"train['total_sales'] = train['item_price'] * train['item_cnt_day']\ntrain.head(3)","d905dd39":"test_tmp = pd.merge(test,item_info, on='item_id', how='inner')\ntest = pd.merge(test_tmp, shops, on='shop_id', how='inner')\ntest.head(3)","b06f184e":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n","fac7c5f2":"train = downcast_dtypes(train)\nprint(train.info())","a2ada75a":"test = downcast_dtypes(test)\nprint(test.info())","52c43c91":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nflierprops = dict(marker='o', markerfacecolor='purple', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=train.item_cnt_day, flierprops=flierprops)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price, flierprops=flierprops)","9ff60e6b":"print('There are {} records with item price higher than 50000'.format(len(train[train['item_price']>50000])))\ntrain[train['item_price']>50000]","1a4ffa82":"print('There are {} records with item count day higher than 0'.format(len(train[train['item_cnt_day']<0])))\ntrain[train['item_cnt_day']<0][0:3]","59000d57":"print('There are {} records with item count day higher than 1000'.format(len(train[train['item_cnt_day']>1000])))\ntrain[train['item_cnt_day'] > 1000]","db356cfc":"# Extract data with a item_price greater than 0\nsales_train = train[train['item_price'] > 0]\n# Extract data with a item_priceof less than 50,000\nsales_train = sales_train[sales_train['item_price'] < 50000]\n# Extract data with item_cnt_day greater than 0\nsales_train = sales_train[sales_train['item_cnt_day'] > 0]\n# Extract data with item_cnt_day less than 1,000\nsales_train = sales_train[sales_train['item_cnt_day'] < 1000]","9f2c90e1":"print('Before removing outliers: ', train.shape)\nprint('After removing outliers: ', sales_train.shape)","8558ccec":"# Leaking to imporve performance\n# unique_test_shop_id = test['shop_id'].unique()\n# sales_train = sales_train[sales_train['shop_id'].isin(unique_test_shop_id)]","e6397f23":"import pandas_profiling as pp\npp.ProfileReport(sales_train)","e01e8dac":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import resample\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor  \n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom scipy import optimize, stats \nfrom keras.utils import np_utils","27e638d3":"sales_train.head(3)","d5f2489a":"train = sales_train.copy()","c8d84a78":"train.drop(['date_block_num','item_price','item_name','item_category_name','item_maincategory_name',\n            'item_subcategory_name','shop_name','city'], axis=1, inplace=True)\ntrain.head()","486c1ee1":"train['date'] = pd.to_datetime(train['date'], dayfirst=True)\ntrain['date'] = train['date'].apply(lambda x: x.strftime('%Y-%m'))\ntrain.head()","f9df0822":"df = train.groupby(['date','shop_id','item_id','item_category_id','first_sale_date',\n                       'item_maincategory_id','item_subcategory_id','city_code']).sum()\ndf = df.pivot_table(index=['shop_id','item_id','item_category_id','first_sale_date',\n                       'item_maincategory_id','item_subcategory_id','city_code'],\n                    columns='date', values='item_cnt_day', fill_value=0)\ndf.reset_index(inplace=True)\ndf.head().T","c46d2135":"test.head()","802d8065":"test.drop(['item_name','item_category_name','item_maincategory_name',\n            'item_subcategory_name','shop_name','city'], axis=1, inplace=True)","9fbd8b0d":"df_test = pd.merge(test, df, on=['shop_id','item_id','item_category_id','first_sale_date',\n                       'item_maincategory_id','item_subcategory_id','city_code'], how='left')\ndf_test.drop(['ID', '2013-01'], axis=1, inplace=True)\ndf_test = df_test.fillna(0)\ndf_test.head().T","00b9375d":"# split into train and test sets\nY_train = df['2015-10'].values\nX_train = df.drop(['2015-10'], axis = 1)\nX_test = df_test\n\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape)","115600b8":"x_train, x_test, y_train, y_test  = train_test_split( X_train, Y_train, test_size=0.20, random_state=1)","4da90c2e":"print ('Train set:', x_train.shape,  y_train.shape)\nprint ('Test set:', x_test.shape,  y_test.shape)","4289da97":"%time\nETR = ExtraTreesRegressor(n_estimators=100, random_state=0)\nETR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, ETR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, ETR.predict(x_test)))\nprint('Test set score:', ETR.score(x_train,y_train))","7d4d29fc":"%time\nADB= AdaBoostRegressor(random_state=0, n_estimators=100)\nADB.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, ADB.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, ADB.predict(x_test)))\nprint('Test set score:', ADB.score(x_train,y_train))","f507bad0":"%time\nBYNR = linear_model.BayesianRidge()\nBYNR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, BYNR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, BYNR.predict(x_test)))\nprint('Test set score:', BYNR.score(x_train,y_train))","47bca201":"%time\nLR = LinearRegression()\nLR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, LR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, LR.predict(x_test)))\nprint('Test set score:', LR.score(x_train,y_train))","a852a515":"%time\nRFR = RandomForestRegressor(n_estimators = 100)\nRFR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, RFR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, RFR.predict(x_test)))\nprint('Test set score:', RFR.score(x_train,y_train))","f330663c":"%time\nXGB = XGBRegressor(max_depth=16,n_estimators=200,seed=1)\nXGB.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, XGB.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, XGB.predict(x_test)))\nprint('Test set score:', XGB.score(x_train,y_train))","851956c9":"%time\nLGBM = LGBMRegressor(max_depth=16,n_estimators=200,seed=1)\nLGBM.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, LGBM.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, LGBM.predict(x_test)))\nprint('Test set score:', LGBM.score(x_train,y_train))","7b14bc56":"extraTreesRegressor_score = ETR.score(x_train,y_train)\nadaBoostRegressor_score = ADB.score(x_train,y_train)\nbayesianRidge_score = BYNR.score(x_train,y_train)\nlinearRegression_score = LR.score(x_train,y_train)\nrandomForestRegressor_score = RFR.score(x_train,y_train)\nXGBRegressor_score = XGB.score(x_train,y_train)\nLGBMRegressor_score = LGBM.score(x_train,y_train) \n\nresults = pd.DataFrame([[\"ExtraTreesRegressor\",extraTreesRegressor_score],[\"AdaBoostRegressor\",adaBoostRegressor_score],\n                        [\" BayesianRidge\",bayesianRidge_score],[\"LinearRegression\",linearRegression_score],\n                        [\"RandomForestRegressor\",randomForestRegressor_score],[\"XGBRegressor\",XGBRegressor_score],\n                        [\"RLGBMRegressor\",LGBMRegressor_score]],\n                        columns = [\"Models\",\"Score\"]).sort_values(by='Score',ascending=False)\n\n\nresults.style.background_gradient(cmap='Blues')","070ea3c8":"prediction = ETR.predict(X_test)","0137d43e":"df_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nprint(df_submission.shape)\ndf_submission.head()","89d93a43":"df_submission['item_cnt_month'] = prediction\ndf_submission.to_csv('prediction.csv', index=False)\ndf_submission.head()","224802af":"prediction = XGB.predict(X_test)","08739d81":"prediction = list(map(round, prediction))","0988f7ee":"df_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nprint(df_submission.shape)\ndf_submission.head()","0fb51ef8":"df_submission['item_cnt_month'] = prediction\ndf_submission.to_csv('prediction.csv', index=False)\ndf_submission.head()","36fea663":"prediction = RFR.predict(X_test)","44ee2e39":"df_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nprint(df_submission.shape)\ndf_submission.head()","43b92af5":"df_submission['item_cnt_month'] = prediction\ndf_submission.to_csv('prediction.csv', index=False)\ndf_submission.head()","3dc9cb0a":"# Item Categories","a04fc4fa":"## Data Description\n### File descriptions\n- **sales_train.csv** - the training set. Daily historical data from January 2013 to October 2015.\n- **test.csv** - the test set. You need to forecast the sales for these shops and products for November 2015.\n- **sample_submission.csv** - a sample submission file in the correct format.\n- **items.csv** - supplemental information about the items\/products.\n- **item_categories.csv** - supplemental information about the items categories.\n- **shops.csv**- supplemental information about the shops.","8e84db55":"### Extract the city from shop_name column","5b0fbdb4":"# Submition Sample","96d7d0a9":"We have some identical cities with typo. For example we have `!\u042f\u043a\u0443\u0442\u0441\u043a` and `\u042f\u043a\u0443\u0442\u0441\u043a`.Lets take care of this in next block.\nWe also encode the cities name using the [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n","5c0eac42":"## Summary of sales","2bcfb8aa":"## Visualization & Profiling","3e5c75a1":"It seems we have some douplicate values here. For example, we have `\u0418\u0433\u0440\u044b` and `\u0418\u0433\u0440\u044b Android`.Lets take care of these douplicates first then we encode the categories using the LabelEncoder.(\u0418\u0433\u0440\u044b means games in Russian!)","a1b52b59":"### Data fields\n- **ID** - an Id that represents a (Shop, Item) tuple within the test set\n- **shop_id** - unique identifier of a shop\n- **item_id** - unique identifier of a product\n- **item_category_id** - unique identifier of item category\n- **item_cnt_day** - number of products sold. You are predicting a monthly amount of this measure\n- **item_price** - current price of an item\n- **date** - date in format dd\/mm\/yyyy\n- **date_block_num** - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n- **item_name** - name of item\n- **shop_name** - name of shop\n- **item_category_name** - name of item category","1d738505":"## Modeling","8a040bbc":"# Shops","3a94c522":"# Merge and make the test Dataframe","f29b5453":"# Merge and make the train dataframes","cc1ae019":"# Items","66f703bd":"# Handeling the outliers","961b7e58":"# Sales","ab807b9a":"It seems that we have main category and subcategory with a dash between.\nwe can extract name of the main category and subcategory from item_category_name and put them in separate columns.","3a302a75":"We will remove the obvious outliers in the dataset - the items that sold more than 1000 in one day and the item with price greater than 50,000.","440f5bd3":"There are no missing values in sales dataframe.","5dc70633":"# Test","49e3f0a9":"# Down Cast the Datasets"}}