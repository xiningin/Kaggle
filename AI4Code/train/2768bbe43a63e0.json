{"cell_type":{"faef8819":"code","f926dd6a":"code","cec693c8":"code","93d47423":"code","0387e04a":"code","c4b8a4a5":"code","405300b3":"code","8e7554c6":"code","39b73e40":"code","31772c0b":"code","53b55ca5":"code","b8c31ae0":"code","f729e58b":"code","3a9c8a10":"code","390e8e1a":"code","c0a92ccd":"code","247bfe71":"code","6a5ddcba":"code","e2f2c705":"code","11a5b2b8":"markdown","aff5af9d":"markdown","6ee2b270":"markdown","46354699":"markdown","54951eb7":"markdown","a12d2d60":"markdown","561a0aa3":"markdown","fd8e8c5e":"markdown","1ce63138":"markdown","5eb0bc61":"markdown","c9041ff1":"markdown","eab8ed26":"markdown","4c09c555":"markdown","00c92911":"markdown","8ddd83dd":"markdown","f25ba216":"markdown","075e8ee1":"markdown","e2e55f81":"markdown","81837d46":"markdown","e984391c":"markdown","8404e19e":"markdown","966e3dfe":"markdown","82e5584f":"markdown","15394ba1":"markdown","128e5649":"markdown","d0d6d4b7":"markdown"},"source":{"faef8819":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\ndf = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf = df.loc[:, ~df.columns.isin(['CLIENTNUM','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'count'])]\ndf.head()","f926dd6a":"df.info()\n","cec693c8":"df.isnull().sum()\n","93d47423":"num_df = df.select_dtypes(exclude=['object'])\nnum_df.columns = num_df.columns.str.replace(\"_\", \" \")\nnum_df.describe().T","0387e04a":"num_cols = num_df.columns\nplt.figure(figsize=(10,90))\ncolors = ['red', 'green', 'orange', 'blue', 'brown', 'purple', 'skyblue', 'grey', 'black', 'red', 'green', 'orange', 'blue', 'brown', 'purple', 'skyblue', 'grey', 'black',]\n#num_df['Attrition_Flag'] = df['Attrition_Flag']\nfor i, col in enumerate(num_cols):\n    plt.subplot(17,1, i+1)\n    ax = sns.kdeplot(num_df[col], fill=True, color = colors[i])\n    #ax.set_xlabel(col, size=15)\n    #ax.set(yticklabels=[])\n    ax.set(xlabel=None)\n    plt.title(\"Distribution of {}\".format(col), size=20, pad=10)\n    ax.xaxis.set_tick_params(labelsize=10)\n    \nplt.show()   ","c4b8a4a5":"df.columns = df.columns.str.replace(\"_\", \" \")\n\n#Subsetting churners and non-churners\nnon_churners = df[df['Attrition Flag'] == 'Existing Customer']\nchurners = df[df['Attrition Flag'] == 'Attrited Customer']\n\n\n#Existing customers heatmap\nnon_churners_corr =  non_churners.corr()\nnon_churners_mask = np.triu(np.ones_like(non_churners_corr, dtype=bool))\nplt.figure(figsize=(10,10))\nsns.heatmap(non_churners_corr, mask=non_churners_mask, linewidths = 0.5, cmap= 'RdBu', annot= True, cbar=False, fmt = '.1f', vmin= -1, vmax=1)\nplt.title(\"Correlation between features from existing customers\", size=20, pad=10)\nplt.show()\n\n\n#Attrited customers heatmap\nchurners_corr =  churners.corr()\nchurners_mask = np.triu(np.ones_like(churners_corr, dtype=bool))\nplt.figure(figsize=(10,10))\nsns.heatmap(churners_corr, mask = churners_mask, linewidths = 0.5, cmap= 'RdBu', annot= True,  cbar=False, fmt = '.1f',  vmin= -1, vmax=1)\nplt.title(\"Correlation between features from churners\", size=20, pad=10)\n\nplt.show()","405300b3":"Churner = pd.DataFrame(round(df.value_counts('Attrition Flag', normalize=True)*100,0)).reset_index()\nChurner = Churner.rename(columns={'Attrition Flag': 'Customer Status', 0: 'Class Incidence'}) \nChurner.style.format({'Class Incidence': \"{:20,.0f}%\"}).background_gradient(cmap='Blues')\n","8e7554c6":"df['count'] = 1\ncols = list(df.select_dtypes('object'))\ncols = cols[1:]\n#cols = [x.replace('_', ' ') for x in cols]\n\nplt.figure(figsize=(10,25))\n\nfor i, col in enumerate(cols):\n    plt.subplot(5,1,i+1)\n    \n    total = df.groupby(col)['count'].sum().reset_index()\n    churner = df[df[\"Attrition Flag\"] =='Attrited Customer'].groupby(col)['count'].sum().reset_index()\n\n\n    churner['count'] = [k for k, j in zip(churner['count'], total['count'])]\n    total['count'] = [k for k, j in zip(total['count'], total['count'])]\n\n    \n    ax = sns.barplot(y='count', x = col, data = total, color='darkblue')\n    ax = sns.barplot(y='count', x = col, data = churner, color = 'lightblue')\n    plt.ylabel('N\u00ba of Customers', size=10)\n    plt.title(\"Distribution of churners by {}\".format(col), size=10, pad=20)\n   \n   \n    top_bar = mpatches.Patch(color='darkblue', label= 'Customer')\n    bottom_bar =  mpatches.Patch(color='lightblue', label= 'Churner' )\n    plt.xticks(size=10)\n    plt.legend(handles=[top_bar, bottom_bar])  \n    ax.set(xlabel=None)\n    plt.tight_layout() \n    \n    \nplt.show()\n\n","39b73e40":"#creating Dataframe of number of customers grouped by Income Category\nIncome_Category = pd.DataFrame(df.value_counts('Income Category').reset_index())\nIncome_Category = Income_Category.rename(columns={0:'N\u00ba of Customers'})\n\n\n#dict of number of churners by income category to map\nincome_churner = df[df[\"Attrition Flag\"] =='Attrited Customer'].groupby('Income Category')['count'].sum()\ndict(income_churner)\n\n\n#adding columns of n\u00ba of churners and % of Churners\nIncome_Category[\"N\u00ba of Churners\"] = Income_Category[\"Income Category\"].map(income_churner)\nIncome_Category['% of churn'] = round((Income_Category[\"N\u00ba of Churners\"] \/ Income_Category['N\u00ba of Customers'])*100,0)\n\n\nIncome_Category","31772c0b":"#creating blue customers df and grouping by income category\n\nBlue_Customers =  df[df[\"Card Category\"] == 'Blue'].groupby('Income Category')['count'].sum().reset_index()\nBlue_Customers = Blue_Customers.rename(columns={'count':'N\u00ba of Blue Cards'})\nBlue_Customers.style.background_gradient(cmap='Blues', subset='N\u00ba of Blue Cards')\nBlue_Customers","53b55ca5":"Blue_Customers =  df[df[\"Card Category\"] == 'Blue'].groupby(['Income Category','Months Inactive 12 mon'])['count'].sum().reset_index()\nBlue_Customers = Blue_Customers.rename(columns={'count':'N\u00ba of Blue Cards'})\n\nBlue_Customers.style.background_gradient(cmap='Blues', subset='N\u00ba of Blue Cards')\n","b8c31ae0":"\ndf_origin = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\n\ndf_origin['Attrition_Flag'] = df_origin['Attrition_Flag'] = df_origin['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})\ndf_origin = df_origin.loc[:, ~df_origin.columns.isin(['CLIENTNUM','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'count'])]\n\ndf_origin = pd.get_dummies(df_origin)\ndf_origin.head()\n","f729e58b":"#Importing sklearn attributes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\n\n#isolating target variable\nX = df_origin.drop('Attrition_Flag', axis=1).values\ny = df_origin['Attrition_Flag'].values\n\n#splitting the data and fitting to the model and predicting\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)\n\n#applying SMOTE, scaling the data and PCA\n\nsm = SMOTE(random_state=42, sampling_strategy= 1.0)\nscaler = StandardScaler()\npca = PCA(n_components=19)\nrfc = RandomForestClassifier()\n\n#fitting and training\nX_train, y_train = sm.fit_resample(X_train, y_train)\nscaler.fit_transform(X_train, y_train)\npca.fit_transform(X_train, y_train)\nrfc.fit(X_train, y_train)\n\n#predicting\ny_pred = rfc.predict(X_test)\n\n","3a9c8a10":"#classification_report\n\nreport = classification_report(y_test, y_pred, output_dict=True)\nreport = pd.DataFrame(report).reset_index()\nreport = report.rename(columns={\"index\": \"Metric\"})\nreport = report.round(2)\nreport","390e8e1a":"plot_confusion_matrix(rfc,\n                      X_test, y_test,\n                      cmap=plt.cm.Blues,\n                      display_labels = ['Existing','Attrited'])\n\nplt.show()","c0a92ccd":"from sklearn.model_selection import GridSearchCV\n\nparams = {'criterion': ['gini', 'entropy'],\n            'max_features': ['sqrt', 'log2'],\n               'max_depth': [3,5,7,10],\n               'min_samples_split': range(2,5,1),\n               'min_samples_leaf': range(2,5,1)\n               }\nrs = GridSearchCV(rfc, \n                        params,\n                        cv = 3, \n                        verbose = 1,\n                        n_jobs=-1 \n                        )\nrs.fit(X_train, y_train)\nrs.best_params_","247bfe71":"rf = RandomForestClassifier(criterion = 'entropy', max_depth= 10, max_features = 'sqrt', min_samples_leaf = 3, min_samples_split = 4)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\n","6a5ddcba":"report = classification_report(y_test, y_pred, output_dict=True)\nreport = pd.DataFrame(report).reset_index()\nreport = report.rename(columns={\"index\": \"Metric\"})\nreport = report.round(2)\nreport","e2f2c705":"plot_confusion_matrix(rf,\n                      X_test, y_test,\n                      cmap=plt.cm.Blues,\n                      display_labels = ['Existing','Attrited'])\n\nplt.show()","11a5b2b8":"# Classification Report and Confusion Matrix","aff5af9d":"# Classification Report and Confusion Matrix after Tuning","6ee2b270":"# Conclusions\n\nThe model after hyper parameter tunning returned a 80% of recall, which means it predicted correct the churners from the test set 81% of the times.\n\nIt is very important to predict potential attrited customes, but even more important is to know what to do when we know a customer is about to churn.\n\n\nThe model helps to reduce the churn rate when the business understands why a customer is about to become a attrited customer and how they ended up in that situation, so the team could offer customized services and solutions for each customer (or group of customers).\n\nThis references the importance to gather data about the reasons of churning, and what customers think about the services offered.\n\nSo this is how i think a ML model can help business managers. \n\n\n## If you have come this far in my notebook, thank you! If you liked the work or would want to support the job please give me some feedbacks in the comments and upvote if it was helpful.\n","46354699":"### Going deeper in the analysis (2)\nAnother thing to notice is the blue card category. \n\nWe might have people with high income that may have blue card, this is something the team would need to check. \n\nPeople with high income that uses just the blue card may be customers of another players as well and they may see the service as not attractive which make them as potential churners.","54951eb7":"# Conclusions of the EDA\n\n* The Avg Ratio Utilization needs a better description. We do not know if the closer to 1 the better.\n\n* Also, the income category has a high incidence of unknowns\n\n* There are a lot of customers with blue cards that could be in a higher card category. Customers with blue cards and an income higher than 80K might be potential churners.\n\n* We can add more metrics to know the customers and the churn better: [Net Promoter Score (NPS)](https:\/\/www.netpromoter.com\/know\/) and for the churners, the reasons they are unsigned the service. \n\n\n\n","a12d2d60":"# Applying the Machine Learning Model - First thoughts\n\nThe data and the task tells us we have a supervised learning of binary classification (Attrited Customer or Existing Customer)\n\nFor the ML model, we have to pay attention in two main topics:\n\n1) As the purpose of a ML model is to generalize well unseen data, we have to avoid overfitting, which is when the model fits the data too well to be true and it does not generalize unseen data well.\n\n2) As we are talking about a binary classification with a class imbalance, the evaluation of the model will be focused on predicting well the Attrited Customer. \n\nWith this two things in mind, i will work with a RandomForest Classifier, because it works well with unbalanced target.\n\n# Preprocessing data\n\nFirst we will encode features as a usual step.\n\nFor the 1st topic, we will first split the data in train and test set and apply SMOTE, an oversampling technique that inserts sinthetic data points using Centroids. [To know more about SMOTE](https:\/\/towardsdatascience.com\/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5) \n\nAlso, we will add a [Principal Component Analysis (PCA)](https:\/\/towardsdatascience.com\/machine-learning-step-by-step-6fbde95c455a). I will work with PCA even using random forest because of dimensionality reduction, which reduces the number of features for the RF to process. \n\n\n\n\n# Evaluating the Model\n\nFor the 2nd topic, as the goal is to predict Attrited Customers, we will use the confusion matrix to evaluate the model, as we have the Recall Metric.\n\nThe Confusion Matrix shows the target values against those values predicted by the RF. It tells us what kind of errors the model does as True Positives, True Negatives, False Positives and False Negatives..\n\nRecall is important because it does not matter here if we raise a false alarm for churners, but potential churn should not go undetected.\n\n[If you wish to learn more about the Confusion Matrix and Its metrics](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/)\n\n\nNow let's head to the model.\n\n\n\n","561a0aa3":"# EDA\n###  Checking num values and data types\n","fd8e8c5e":"# Data Description from the [data source](https:\/\/leaps.analyttica.com\/sample_cases\/11)\n### Categorical Variables\n* **Clientnum:** Unique identifier for the customer holding the account\n* **Attrition_Flag**: flag to indicate if the customer is an existing customer or has attrited\n* **Customer_Age:** Customer's Age in Years\n* **Gender:** M=Male, F=Female\n* **Dependent_count:** Number of dependents\n* **Education_Level:** Educational Qualification of the account holder (example: high school, college graduate, etc.)\n* **Marital_Status:** Demographic variable - Married, Single, Unknown\n* **Income_Category:** Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n\n\n### Numerical Variables\n* **Card_Category:** Type of Card (Blue, Silver, Gold, Platinum)\n* **Months_on_book:** Months on book (Time of Relationship)\n* **Total_Relationship_Count:** Total no. of products held by the customer\n* **Months_Inactive_12_mon:** No. of months inactive in the last 12 months\n* **Contacts_Count_12_mon:** Num\tNo. of Contacts in the last 12 months\n* **Credit_Limit:** Credit Limit on the Credit Card\n* **Total_Revolving_Bal:** Total Revolving Balance on the Credit Card\n* **Avg_Open_To_Buy:** Open to Buy Credit Line (Average of last 12 months)\n* **Total_Amt_Chng_Q4_Q1:** Change in Transaction Amount (Q4 over Q1) \n* **Total_Trans_Amt:** Total Transaction Amount (Last 12 months)\n* **Total_Trans_Ct:** Total Transaction Count (Last 12 months)\n* **Total_Ct_Chng_Q4_Q1:** Change in Transaction Count (Q4 over Q1) \n* **Avg_Utilization_Ratio:** Average Card Utilization Ratio","1ce63138":"# Categorical Data - Know your customer better\n\nThe categorical analysis may show us more insights about the customers as well as a macro view about customers attributes. We'll start checking the balance of the target variable followed by some stacked bar charts for each categorical value separated between Attrited and Existing Customers. This will let us know where most of the customers are and also where to pay attention for potential Attrited Customers.\n\n\n# Incidence of churners in the number of customers\n\nWe can start checking the incidence of Existing and Attrited Customers. If it shows a class imbalance we should deal with it before applying the model. [More about class imbalance.](http:\/\/https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/)\n","5eb0bc61":"We can go deeper and find out the number of months inactive in the last 12 months for Blue Card holders in each Income Category","c9041ff1":"### Going deeper in the analysis (1)\n\nOne thing to notice from the stacked bar charts above is that the relative % of churners in the Income Category looks the same for all categories.\n\nThis is important to know because the team will need to think about solutions to prevent churn for every income category as there is no category that stands out with a high incidence of churn. Although we have almost 50% of churn incidence in three categories.\n\nLet's check it","eab8ed26":"# Numerical Data - First view of the customers\n\nThe data analysis will start with basic statistics followed by a Kernel Density distribution (KDE) of numerical features and the correlation between them. \n\nThe KDE plot is a smoothed version of the histogram and has the same concept. [To check more about Kde ](https:\/\/www.data-to-viz.com\/graph\/density.html)\n\nThe KDE plot tells what characteristics we have the most and if the data is skewed or not.\n\nThe Skewness in resume is the degree of asymmetry observed in a distribution, it is a mewasure of how distorted a sample is from a bell-shaped curve. [To know kmore about skewness]( https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/what-is-skewness-statistics\/)\n","4c09c555":"Encoding Variables","00c92911":"# Preprocessing Steps","8ddd83dd":"# Basic Statistics","f25ba216":"\n# Know your credit card customers to know your potential churners\n\n  ![](https:\/\/image.cnbcfm.com\/api\/v1\/image\/106187392-1571322289627gettyimages-1044704448.jpg?v=1571322316&w=740&h=416)\n\n\n#                 Introduction\n### This notebook has two main purposes:\n\n1. To explore what are the attributes of customer Churn from the given dataset \n2. To create a ML model that is able to predict potential churners to help to reduce churn rate.\n\n\nThe two objectives of this notebook will help the manager to understand the anatomy of the churners from the business, so the team can tackle what is causing churn and think about customized solutions or services for potential churners. \n\n\nThe work starts with an Exploratory Data Analysis (EDA) from both categorical and numerical data from the dataset to explore the customers before the ML model ","075e8ee1":"# Correlation between numeric features\n\nThe correlation is shown as a heatmap. \n\nThe heatmap shows the pearson correlation coefficient between two variables. \n\nThe correlation shows how strong the relationship between two variables is.\nIt varies between -1 and 1, where 1 is a strong positive relationship and -1 is a strong negative relationship. 0 indicates no relationship.\n\nA correlation coefficient of 1 means that a positive increase in one variable is a positive increase of a fixed proportion in the other.\nA correlation coefficient of -1 means that a positive increase in one variable is a negative decrease of a fixed proportion in the other.\n \n[To know more about heatmaps](https:\/\/www.python-graph-gallery.com\/heatmap\/) \n\n[To know more about pearson correlation coefficient](http:\/\/https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/) \n\nAlso for the correlation the heatmap is splitted from attrited customers and existing customers so we can compare if there is any significant different between them.","e2e55f81":"# Fitting and Training with best hyperparameters","81837d46":"# Hyperparameter Tunning","e984391c":"#### It does not look the data has issues with data types or missing values","8404e19e":"# References:\n# \n[SKlearn Random Forest Classifier documentation:](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest%20classifier#sklearn.ensemble.RandomForestClassifier) \n[Machine Learning Step By Step:](https:\/\/towardsdatascience.com\/machine-learning-step-by-step-6fbde95c455a)  \n[Skewness:](https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/what-is-skewness-statistics\/) \n[KDE Viz:](https:\/\/www.data-to-viz.com\/graph\/density.html) \n[Heatmap: ](https:\/\/www.python-graph-gallery.com\/heatmap\/)\n[NPS: ](https:\/\/www.netpromoter.com\/know\/)\n[Confusion Matrix:](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/) \n[SMOTE:](https:\/\/towardsdatascience.com\/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5) ","966e3dfe":"# Distribution of Churners by categorical variables.\n\n**Attributes of all customers:**\n\n* 1) most of them are graduated\n* 2) The income category 'Less than $40k' has the most number of customers\n* 3) By far the Blue Card category is the service that customers have.","82e5584f":"# Applying the model with SMOTE, PCA and Scaling","15394ba1":"# Conclusions\n\nWe have some comb graphics which tell us the data may have been rounded. But checking the description of the data the comb distribution seems fine.\n\nAlso, we have some skewness in a few numeric variables. \n\nWe will have to deal this transforming the skewed data to a normally distributed data as a preprocessing step.\n\nNow we go ahead to the heatmap correlation\n\n\n","128e5649":"The table shows we have a class imbalance in the data, as more than 80% of customers are labeled as Existing Customer","d0d6d4b7":"# Distribution of numeric features"}}