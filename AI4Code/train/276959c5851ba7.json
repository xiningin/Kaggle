{"cell_type":{"1b7d424a":"code","6c5063e9":"code","575de861":"code","ef84bbcc":"code","e20ae7cf":"code","bfe356cb":"code","6e0f1c87":"code","4861a9d6":"code","fbc9ad55":"code","d8fe8c89":"code","ab3bb45b":"code","f47acfd3":"code","eff1c737":"code","447d11d0":"code","fa12b953":"code","6cb88c14":"markdown"},"source":{"1b7d424a":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\n\nimport random\nrandom.seed(42) # The answer\n \nimport os\nimport sys\nimport gc\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedKFold\nfrom sklearn import metrics","6c5063e9":"meta_train = pd.read_csv('..\/input\/metadata_train.csv')\nlen(meta_train)","575de861":"meta_train.head()","ef84bbcc":"%%time\nsubset_train = pq.read_pandas('..\/input\/train.parquet', columns=[str(i) for i in range(len(meta_train))]).to_pandas()","e20ae7cf":"%%time\ntrain_length = 8712 \npositive_length = len(meta_train[meta_train['target']==1])\ntrain_df = pd.DataFrame()\nrow_index = 0\n\nfor i in range(train_length):\n    # downsampling\n    if meta_train.loc[i,'target'] == 1 or random.random() < positive_length \/ train_length:\n        subset_train_row = subset_train[str(i)]\n        train_df.loc[row_index, 'signal_min'] = np.min(subset_train_row)\n        train_df.loc[row_index, 'signal_max'] = np.max(subset_train_row)\n        train_df.loc[row_index, 'signal_mean'] = np.mean(subset_train_row)\n        train_df.loc[row_index, 'signal_mean_sq'] = np.mean(subset_train_row)**2\n        train_df.loc[row_index, 'signal_max_min_diff'] = np.subtract(np.max(subset_train_row),np.min(subset_train_row))\n#         train_df.loc[row_index, 'signal_median'] = np.median(subset_train_row)\n#         train_df.loc[row_index, 'signal_ptp'] = np.ptp(subset_train_row)\n        \n        train_df.loc[row_index, 'signal_id'] = i\n        row_index += 1\n        \nprint(\"positive length: \" + str(positive_length))\n\nprint(\"train length: \" + str(len(train_df)))","bfe356cb":"train_df = pd.merge(train_df, meta_train, on='signal_id')\ntrain_df.to_csv(\"train.csv\", index=False)\ntrain_df.head()","6e0f1c87":"train_df.drop(['id_measurement'],axis=1,inplace=True)","4861a9d6":"x_train = train_df\ntarget = x_train['target']\ninput_target = x_train['target']\nx_train.drop('target', axis=1, inplace=True)\nx_train.drop('signal_id', axis=1, inplace=True)\nfeatures = x_train.columns\nparam = {'num_leaves': 80,\n         'min_data_in_leaf': 60, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.05,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 42,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\nmax_iter=5","fbc9ad55":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\noof = np.zeros(len(x_train))\nfeature_importance_df = pd.DataFrame()\nscore = [0 for _ in range(folds.n_splits)]\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train.values, target.values)):\n    print(\"Fold No.{}\".format(fold_+1))\n    trn_data = lgb.Dataset(x_train.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(x_train.iloc[val_idx][features],\n                           label=target.iloc[val_idx])\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(x_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n    if fold_ == max_iter - 1: break\nif (folds.n_splits == max_iter):\n    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(target, oof)))\nelse:\n     print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))","d8fe8c89":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\n","ab3bb45b":" gc.collect()","f47acfd3":"%%time\nmeta_test = pd.read_csv('..\/input\/metadata_test.csv')","eff1c737":"%%time\ntest_df = pd.DataFrame()\nrow_index = 0\nfor i in range(10):\n    subset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i*2000 + j + 8712) for j in range(2000)]).to_pandas()\n    for j in range(2000):\n        subset_test_row = subset_test[str(i*2000 + j + 8712)]\n        test_df.loc[row_index, 'signal_min'] = np.mean(subset_test_row)\n        test_df.loc[row_index, 'signal_max'] = np.max(subset_test_row)\n        test_df.loc[row_index, 'signal_mean'] = np.mean(subset_test_row)\n        test_df.loc[row_index, 'signal_mean_sq'] = np.mean(subset_test_row)**2\n        test_df.loc[row_index, 'signal_max_min_diff'] = np.subtract(np.max(subset_test_row),np.min(subset_test_row))\n#         test_df.loc[row_index, 'signal_median'] = np.median(subset_test_row)\n#         test_df.loc[row_index, 'signal_ptp'] = np.ptp(subset_test_row)\n        test_df.loc[row_index, 'signal_id'] = i*2000 + j + 8712\n        row_index += 1\nsubset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i + 28712) for i in range(337)]).to_pandas()\nfor i in tqdm(range(337)):\n    subset_test_row = subset_test[str(i + 28712)]\n    test_df.loc[row_index, 'signal_min'] = np.min(subset_test_row)\n    test_df.loc[row_index, 'signal_max'] = np.max(subset_test_row)\n    test_df.loc[row_index, 'signal_mean'] = np.mean(subset_test_row)\n    test_df.loc[row_index, 'signal_mean_sq'] = np.mean(subset_test_row)**2\n    test_df.loc[row_index, 'signal_max_min_diff'] = np.subtract(np.max(subset_test_row),np.min(subset_test_row))\n#     test_df.loc[row_index, 'signal_median'] = np.median(subset_test_row)\n#     test_df.loc[row_index, 'signal_ptp'] = np.ptp(subset_test_row)\n    test_df.loc[row_index, 'signal_id'] = i + 28712\n    row_index += 1\ntest_df = pd.merge(test_df, meta_test, on='signal_id')\ntest_df.to_csv(\"test.csv\", index=False)\ntest_df.head()","447d11d0":"test_df.drop(['id_measurement'],axis=1,inplace=True)","fa12b953":"x_test = test_df\nx_filename = x_test['signal_id']\nx_test = x_test.drop('signal_id', axis=1)\n\npredictions = clf.predict(x_test, num_iteration=clf.best_iteration)\n\nsub_df = pd.DataFrame({\"signal_id\":x_filename.values})\nsub_df[\"target\"] = pd.Series(predictions).round()\nsub_df['signal_id'] = sub_df['signal_id'].astype(np.int64)\nsub_df['target'] = sub_df['target'].astype(np.int64)\nsub_df.to_csv(\"submission.csv\", index=False)","6cb88c14":"### Inspired by my own kernel (https:\/\/www.kaggle.com\/delayedkarma\/lightgbm-cv) and leo's https:\/\/www.kaggle.com\/bluexleoxgreen\/simple-feature-lightgbm-baseline"}}