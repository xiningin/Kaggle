{"cell_type":{"ffe1e72b":"code","c0092f89":"code","18eed86b":"code","e1531386":"code","32bd7ea4":"code","85939170":"code","f5b7a5e7":"code","834989fe":"code","4759df9f":"code","a72cd647":"code","2ae66ca4":"code","b6afc6ea":"code","b91b20bc":"code","5987eb3a":"code","9981d0eb":"markdown","252fcad2":"markdown","4dac8b5a":"markdown","196e9691":"markdown","7f1fe273":"markdown","d3ecbaf3":"markdown","a3766b11":"markdown","abff63c7":"markdown","b3374362":"markdown","f02579c4":"markdown","213d3d47":"markdown","2fc99b3b":"markdown","ac2aab20":"markdown","44f3f72c":"markdown","d8f9352a":"markdown","00c5fee0":"markdown","9ad554a0":"markdown","71f97edc":"markdown","551364ea":"markdown","0e030502":"markdown","c5c5dfdc":"markdown","81a00cb8":"markdown","efd15dde":"markdown","b9c9e66c":"markdown","f37372db":"markdown","87494be2":"markdown","f3b8e0b4":"markdown"},"source":{"ffe1e72b":"!wget https:\/\/labfile.oss.aliyuncs.com\/courses\/2534\/cifar-10-python.tar.gz","c0092f89":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\n# \u5b9a\u4e49\u9884\u5904\u7406\u5217\u8868\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# CIFAR10: 60000 \u5f20 32x32 \u5927\u5c0f\u7684\u5f69\u8272\u56fe\u7247\uff0c\u8fd9\u4e9b\u56fe\u7247\u5171\u5206 10 \u7c7b,\u6bcf\u7c7b\u6709 6000 \u5f20\u56fe\u50cf\n# root:\u6307\u5b9a\u6570\u636e\u96c6\u6240\u5728\u4f4d\u7f6e\n# train=True\uff1a\u8868\u793a\u82e5\u672c\u5730\u5df2\u7ecf\u5b58\u5728\uff0c\u65e0\u9700\u4e0b\u8f7d\u3002\u82e5\u4e0d\u5b58\u5728\uff0c\u5219\u4e0b\u8f7d\n# transform\uff1a\u9884\u5904\u7406\u5217\u8868\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u8fd4\u56de\u9884\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\u5408\ntrain_dataset = torchvision.datasets.CIFAR10(root='.\/', train=True,\n                                             download=True, transform=transform)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='.\/', train=False,\n                                            download=True, transform=transform)\nprint(\"\u8bad\u7ec3\u96c6\u7684\u56fe\u50cf\u6570\u91cf\u4e3a\uff1a\", len(train_dataset))\nprint(\"\u6d4b\u8bd5\u96c6\u7684\u56fe\u50cf\u6570\u91cf\u4e3a\", len(test_dataset))","18eed86b":"batch_size = 256      # \u8bbe\u7f6e\u6279\u6b21\u4e2a\u6570\n# shuffle=True:\u8868\u793a\u52a0\u8f7d\u6570\u636e\u524d\uff0c\u4f1a\u5148\u6253\u4e71\u6570\u636e\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                          shuffle=False)\ntest_loader, test_loader","e1531386":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\ndef imshow(img):\n    # \u7531\u4e8e\u52a0\u8f7d\u5668\u4ea7\u751f\u7684\u56fe\u7247\u662f\u5f52\u4e00\u5316\u540e\u7684\u56fe\u7247\uff0c\u56e0\u6b64\u8fd9\u91cc\u9700\u8981\u5c06\u56fe\u7247\u53cd\u5f52\u4e00\u5316\n    # \u53d8\u6210\u5f52\u4e00\u5316\u524d\u7684\u56fe\u50cf\n    img = img \/ 2 + 0.5\n    # \u5c06\u56fe\u50cf\u4ece Tensor \u8f6c\u4e3a Numpy\n    npimg = img.numpy()\n    #\u4ea7\u751f\u7684\u6570\u636e\u4e3a C\u00d7W\u00d7H \u800c plt \u5c55\u793a\u7684\u56fe\u50cf\u4e00\u822c\u90fd\u662f W\u00d7H\u00d7C\n    #\u56e0\u6b64\uff0c\u8fd9\u91cc\u4f1a\u6709\u4e00\u4e2a\u7ef4\u5ea6\u7684\u53d8\u6362\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# \u968f\u673a\u83b7\u5f97\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\n# \u5c06\u8fd9\u4e9b\u56fe\u50cf\u8fdb\u884c\u5c55\u793a\nimshow(torchvision.utils.make_grid(images))","32bd7ea4":"import torch.nn.functional as F\nimport torch.nn as nn\n\n#\u7f51\u7edc\u6a21\u578b\u7684\u5efa\u7acb\n'''\u5b9a\u4e49\u7f51\u7edc\u6a21\u578b'''\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=10):\n        super(VGG16, self).__init__()\n        self.features = nn.Sequential(\n            #1\n            nn.Conv2d(3,64,kernel_size=3,padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            #2\n            nn.Conv2d(64,64,kernel_size=3,padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2,stride=2),\n            #3\n            nn.Conv2d(64,128,kernel_size=3,padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            #4\n            nn.Conv2d(128,128,kernel_size=3,padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2,stride=2),\n            #5\n            nn.Conv2d(128,256,kernel_size=3,padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            #6\n            nn.Conv2d(256,256,kernel_size=3,padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            #7\n            nn.Conv2d(256,256,kernel_size=3,padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2,stride=2),\n            #8\n            nn.Conv2d(256,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            #9\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            #10\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2,stride=2),\n            #11\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            #12\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            #13\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2,stride=2),\n            nn.AvgPool2d(kernel_size=1,stride=1),\n            )\n        self.classifier = nn.Sequential(\n            #14\n            nn.Linear(512,4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            #15\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            #16\n            nn.Linear(4096,num_classes),\n            )\n        #self.classifier = nn.Linear(512, 10)\n \n    def forward(self, x):\n        out = self.features(x) \n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n# \u5b9a\u4e49\u5f53\u524d\u8bbe\u5907\u662f\u5426\u652f\u6301 GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VGG16().to(device)\nmodel","85939170":"learning_rate = 0.01\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ncriterion,optimizer","f5b7a5e7":"import datetime\n\nstarttime = datetime.datetime.now()\nnum_epochs = 20\n# \u5b9a\u4e49\u6570\u636e\u957f\u5ea6\nn_total_steps = len(train_loader)\nprint(\"Start training....\")\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # \u539f\u59cb\u6570\u636e\u96c6\u7684\u5927\u5c0f\uff0c\u6bcf\u4e2a\u6279\u6b21\u7684\u5927\u5c0f\u4e3a: [4, 3, 32, 32] \n        # \u5c06\u6570\u636e\u8f6c\u4e3a\u6a21\u578b\u652f\u6301\u7684\u73af\u5883\u7c7b\u578b\u3002\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # \u6a21\u578b\u7684\u6b63\u5411\u4f20\u64ad\uff0c\u5f97\u5230\u6570\u636e\u6570\u636e\u7684\u9884\u6d4b\u503c\n        outputs = model(images)\n        # \u6839\u636e\u9884\u6d4b\u503c\u8ba1\u7b97\u635f\u5931\n        loss = criterion(outputs, labels)\n\n        # \u56fa\u5b9a\u6b65\u9aa4\uff1a\u68af\u5ea6\u6e05\u7a7a\u3001\u53cd\u5411\u4f20\u64ad\u3001\u53c2\u6570\u66f4\u65b0\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 50 == 0:\n            print (f'Epoch [{epoch+1}\/{num_epochs}], Step [{i+1}\/{n_total_steps}], Loss: {loss.item():.4f}')\n        \nendtime = datetime.datetime.now()\nprint ((endtime - starttime).seconds)\nprint('Finished Training')","834989fe":"PATH = '.\/cnn.pth'\ntorch.save(model.state_dict(), PATH)\nprint(\"The model have been saved\uff01\")","4759df9f":"new_model = VGG16().to(device)\nnew_model.load_state_dict(torch.load(PATH))","a72cd647":"with torch.no_grad():\n    # \u7edf\u8ba1\u9884\u6d4b\u6b63\u786e\u7684\u56fe\u50cf\u6570\u91cf\u548c\u8fdb\u884c\u4e86\u9884\u6d4b\u7684\u56fe\u50cf\u6570\u91cf\n    n_correct = 0\n    n_samples = 0\n    # \u7edf\u8ba1\u6bcf\u7c7b\u56fe\u50cf\u4e2d\uff0c\u9884\u6d4b\u6b63\u786e\u7684\u56fe\u50cf\u6570\u91cf\u548c\u8be5\u7c7b\u56fe\u50cf\u7684\u5b9e\u9645\u6570\u91cf\n    n_class_correct = [0 for i in range(10)]\n    n_class_samples = [0 for i in range(10)]\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = new_model(images)\n        # \u5229\u7528 max \u51fd\u6570\u8fd4\u56de 10 \u4e2a\u7c7b\u522b\u4e2d\u6982\u7387\u6700\u5927\u7684\u4e0b\u6807\uff0c\u5373\u9884\u6d4b\u7684\u7c7b\u522b\n        _, predicted = torch.max(outputs, 1)\n        n_samples += labels.size(0)\n        # \u901a\u8fc7\u5224\u65ad\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u6807\u7b7e\u662f\u5426\u76f8\u540c\uff0c\u6765\u7edf\u8ba1\u9884\u6d4b\u6b63\u786e\u7684\u6837\u672c\u6570\n        n_correct += (predicted == labels).sum().item()\n        # \u8ba1\u7b97\u6bcf\u79cd\u79cd\u7c7b\u7684\u9884\u6d4b\u6b63\u786e\u6570\n        if len(labels)<batch_size:\n            batch_size=len(labels)\n        for i in range(batch_size):\n            label = labels[i]\n            pred = predicted[i]\n            if (label == pred):\n                n_class_correct[label] += 1\n            n_class_samples[label] += 1\n    # \u8f93\u51fa\u603b\u7684\u6a21\u578b\u51c6\u786e\u7387\n    acc = 100.0 * n_correct \/ n_samples\n    print(f'Accuracy of the network: {acc} %')\n\n    # \u8f93\u51fa\u6bcf\u4e2a\u7c7b\u522b\u7684\u6a21\u578b\u51c6\u786e\u7387\n    for i in range(10):\n        acc = 100.0 * n_class_correct[i] \/ n_class_samples[i]\n        print(f'Accuracy of {classes[i]}: {acc} %')","2ae66ca4":"!wget https:\/\/labfile.oss.aliyuncs.com\/courses\/2534\/cat2.jpg","b6afc6ea":"## \u56fe\u7247\u7684\u52a0\u8f7d\u51fd\u6570\nfrom PIL import Image\n\ninfer_path='cat2.jpg'\nimg = Image.open(infer_path)\nplt.imshow(img)   \nplt.show()  ","b91b20bc":"def load_image(file):\n    im = Image.open(file)\n    # \u5c06\u5927\u5c0f\u4fee\u6539\u4e3a 32*32 \u7b26\u5408\u6a21\u578b\u8f93\u51fa\n    im = im.resize((32,32),Image.ANTIALIAS)\n    # \u5efa\u7acb\u56fe\u7247\u77e9\u9635\n    im = np.array(im).astype(np.float32)\n    ## WHC -> CHW\n    im = im.transpose((2,0,1))\n    im = im \/ 255.0\n    # \u8f6c\u4e3a batch,c,w,h\n    im = np.expand_dims(im,axis=0)\n\n    print(\"im_shape \u7684\u7ef4\u5ea6\",im.shape)\n    return im\n#\u52a0\u8f7d\u5982\u4f55\u6a21\u578b\u8f93\u5165\u7684\u56fe\u50cf\nimg = load_image(infer_path)\n","5987eb3a":"\nimg = load_image(infer_path)\nimg = torch.from_numpy(img)\nprediction = model(img.to(device))\nprint(\"The picture is a \", classes[np.argmax(prediction.cpu().detach().numpy()[0])])","9981d0eb":"<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> \u672c\u8bfe\u7a0b\u5185\u5bb9\u7248\u6743\u5f52\u5b9e\u9a8c\u697c\u6240\u6709\uff0c\u7981\u6b62\u8f6c\u8f7d\u3001\u4e0b\u8f7d\u53ca\u975e\u6cd5\u4f20\u64ad\u3002<\/i><\/div>","252fcad2":"\u6700\u540e\uff0c\u8ba9\u6211\u4eec\u5c06\u56fe\u50cf\u653e\u5165\u6a21\u578b\uff0c\u5e76\u6839\u636e\u6a21\u578b\u7684\u7ed3\u679c\u9884\u6d4b\u8be5\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u7c7b\u3002\u7531\u4e8e\u6a21\u578b\u8fd4\u56de\u7684\u53ea\u662f\u4e00\u4e2a\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7c7b\u578b\u540d\u79f0\u6570\u7ec4 label_list \uff0c\u6839\u636e\u4e0b\u6807\uff0c\u627e\u5230\u7c7b\u522b\u7684\u5177\u4f53\u540d\u79f0\u3002","4dac8b5a":"\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 `model.load_state_dict(torch.load(PATH))`  \u6765\u52a0\u8f7d\u672c\u5730\u7684\u6a21\u578b\uff1a","196e9691":"\u5f97\u5230\u6570\u636e\u96c6\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5229\u7528 ` torch.utils.data.DataLoader` \u5c06\u6570\u636e\u96c6\u5305\u88c5\u6210\u4e00\u4e2a\u6570\u636e\u751f\u6210\u5668\uff1a","7f1fe273":"#### \u6a21\u578b\u7684\u6d4b\u8bd5","d3ecbaf3":"\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5229\u7528\u5b9a\u4e49\u7684\u52a0\u8f7d\u5668\uff0c\u52a0\u8f7d\u51e0\u5f20\u56fe\u7247\uff0c\u89c2\u5bdf\u4e00\u4e0b\u56fe\u7247\u7684\u5177\u4f53\u6548\u679c\uff1a","a3766b11":"\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u5229\u7528\u6d4b\u8bd5\u96c6\u6765\u8ba1\u7b97\u6a21\u578b\u7684\u603b\u8bc6\u522b\u51c6\u786e\u7387\u4ee5\u53ca\u5206\u522b\u5bf9\u6bcf\u4e00\u7c7b\u56fe\u50cf\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff1a","abff63c7":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u6765\u8be5\u6570\u636e\u96c6\u5408\u3002\u7531\u4e8e\u8be5\u6570\u636e\u96c6\u8fc7\u5927\uff0c\u5982\u679c\u7ebf\u4e0a\u76f4\u63a5\u4e0b\u8f7d\u8be5\u6570\u636e\u96c6\u7684\u8bdd\uff0c\u901f\u5ea6\u4f1a\u5f88\u6162\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5148\u5c06\u6570\u636e\u96c6\u4e0a\u4f20\u5230\u4e86\u5b9e\u9a8c\u697c\u7684\u4e91\u670d\u52a1\u5668\u4e2d\uff0c\u6211\u4eec\u76f4\u63a5\u4ece\u8fd9\u4e0a\u9762\u4e0b\u8f7d Cifar-10 \u6570\u636e\u96c6\u3002","b3374362":"\u4e0a\u9762\u4ee3\u7801\u7684\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0c\u6211\u4eec\u6240\u7f16\u5199\u7684\u6a21\u578b\u7ed3\u6784\u548c\u4e0a\u9762\u7684\u56fe\u50cf\u4e00\u81f4\u3002","f02579c4":"\u4e0a\u9762\u5f97\u5230\u7684\u6570\u636e\u96c6\u5c31\u662f\u6807\u51c6\u5316\u540e\u7684 Tensor \u6570\u636e\u3002`torchvision.datasets.CIFAR10` \u5728\u8fd0\u884c\u65f6\uff0c\u4f1a\u67e5\u627e `root` \u76ee\u5f55\u4e0b\u662f\u5426\u5b58\u5728\u6240\u9700\u8981\u7684\u6570\u636e\u96c6\u5408\u3002\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u52a0\u8f7d\u3002\u5982\u679c\u4e0d\u5b58\u5728\uff0c PyTorch \u4f1a\u4ece\u5b98\u7f51\u4e0a\u4e0b\u8f7d\u8be5\u6570\u636e\u96c6\u5408\u3002\u7531\u4e8e\u5b98\u7f51\u5c5e\u4e8e\u5916\u7f51\uff0c\u56e0\u6b64\u76f4\u63a5\u4ece\u5b98\u7f51\u4e0b\u8f7d\u6570\u636e\u96c6\u5408\u7684\u901f\u5ea6\u662f\u975e\u5e38\u6162\u7684\u3002\u8fd9\u4e5f\u5c31\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u9009\u62e9\u4ece\u5b9e\u9a8c\u697c\u7684\u4e91\u670d\u52a1\u5668\u4e2d\u4e0b\u8f7d\u6570\u636e\u7684\u539f\u56e0\u3002","213d3d47":"#### \u6a21\u578b\u7684\u5efa\u7acb","2fc99b3b":"\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 PIL \u5bf9\u56fe\u7247\u8fdb\u884c\u52a0\u8f7d\u4e0e\u5c55\u793a\uff1a","ac2aab20":"\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u52a0\u8f7d\u672c\u5730\u5df2\u7ecf\u4fdd\u5b58\u597d\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u6a21\u578b\u7684\u6d4b\u8bd5\u3002","44f3f72c":"\u5728\u5c06\u56fe\u7247\u653e\u5165\u6a21\u578b\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u56fe\u7247\u8fdb\u884c\u5904\u7406\uff0c\u4f7f\u5176\u53d8\u6210\u7b26\u5408\u6a21\u578b\u8f93\u5165\u7684\u56fe\u50cf\uff1a","d8f9352a":"\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u76f4\u63a5\u4ece\u767e\u5ea6\u56fe\u7247\u4e2d\u4e0b\u8f7d\u4e00\u5f20\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5\u3002\u8fd9\u91cc\uff0c\u6211\u5df2\u7ecf\u4e0b\u8f7d\u4e86\u4e00\u5f20\u732b\u7684\u56fe\u7247\u5e76\u4e0a\u4f20\u5230\u4e86\u5b9e\u9a8c\u697c\u4e2d:","00c5fee0":"### \u5b9e\u9a8c\u603b\u7ed3","9ad554a0":"\u7531\u4e8e\u6211\u4eec\u8fd9\u91cc\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u662f\u4e00\u6c14\u5475\u6210\u7684\uff0c\u56e0\u6b64\u5185\u5b58\u4e2d\u662f\u5b58\u5728\u5df2\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u4f46\u662f\u4e3a\u4e86\u8bb2\u89e3\u6a21\u578b\u52a0\u8f7d\u7684\u77e5\u8bc6\u70b9\uff0c\u8fd9\u91cc\u6211\u4eec\u8fd8\u662f\u4ece\u672c\u5730\u8bfb\u53d6\u521a\u624d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002","71f97edc":"\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 `model.state_dict(modl,PATH)` \u6765\u5c06\u6307\u5b9a\u6a21\u578b model \u4fdd\u5b58\u5230 PATH \u4e2d\uff1a","551364ea":"#### \u6a21\u578b\u7684\u5e94\u7528","0e030502":"\u5728\u5bf9\u6570\u636e\u8fdb\u884c\u8bfb\u53d6\u4e4b\u524d\uff0c\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u4e00\u4e9b\u9884\u5904\u7406\u64cd\u4f5c\u3002\u5728 \u300a\u6570\u636e\u7684\u9884\u5904\u7406\u300b\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u8be6\u7ec6\u7684\u9610\u8ff0\u4e86 `torchvision.transforms.Compose()` \u7684\u4f7f\u7528\u65b9\u6cd5\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8be5\u51fd\u6570\u5b9a\u4e49\u4e00\u4e2a\u6570\u636e\u5904\u7406\u7684\u96c6\u5408\uff0c\u4e13\u95e8\u7528\u4e8e\u6570\u636e\u7684\u5904\u7406\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u5bf9\u4e0b\u8f7d\u7684\u56fe\u50cf\u8fdb\u884c\u7684\u6570\u636e\u64cd\u4f5c\u6709\uff1a Tensor \u7c7b\u578b\u7684\u8f6c\u6362 \u548c \u6570\u636e\u7684\u6807\u51c6\u5316\u3002","c5c5dfdc":"\u5efa\u7acb\u5b8c\u6a21\u578b\u540e\uff0c\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u5bf9\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\u8fdb\u884c\u5b9a\u4e49\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u4f5c\u4e3a\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528 SGD \u7b97\u6cd5\u4f5c\u4e3a\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a","81a00cb8":"## \u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684 CIFAR10 \u8bc6\u522b","efd15dde":"#### \u6570\u636e\u7684\u52a0\u8f7d","b9c9e66c":"\u4e3a\u4e86\u4fdd\u8bc1\u8bad\u7ec3\u6a21\u578b\u5728\u540e\u9762\u7684\u4f7f\u7528\uff0c\u5728\u5b8c\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u540e\uff0c\u6211\u4eec\u90fd\u4f1a\u5c06\u6a21\u578b\u6301\u4e45\u5316\uff0c\u5373\u4fdd\u5b58\u65e9\u6307\u5b9a\u76ee\u5f55\u4e0b\uff0c\u65b9\u4fbf\u4e0b\u6b21\u76f4\u63a5\u52a0\u8f7d\u3002","f37372db":"\u8ba9\u6211\u4eec\u5229\u7528 PyTorch \u4ee3\u7801\u6765\u5efa\u7acb\u4e00\u4e2a VGG16 \u7f51\u8def\u7528\u4e8e CIFAR \u7684\u8bc6\u522b\uff1a","87494be2":"\u672c\u5b9e\u9a8c\u9996\u5148\u4ecb\u7ecd\u4e86 CIFAR10 \u6570\u636e\u96c6\uff0c\u7136\u540e\u5229\u7528\u76f8\u5173\u51fd\u6570\u5bf9\u5176\u8fdb\u884c\u4e86\u9884\u5904\u7406\u64cd\u4f5c\u3002\u63a5\u4e0b\u6765\uff0c\u6839\u636e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u76f8\u5173\u77e5\u8bc6\uff0c\u5229\u7528 PyTorch \u5efa\u7acb\u4e86\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u7136\u540e\uff0c\u5b9a\u4e49\u635f\u5931\u548c\u4f18\u5316\u5668\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002\u6700\u540e\u5c06\u63d0\u524d\u5206\u5272\u51fa\u6765\u7684\u6d4b\u8bd5\u96c6\u653e\u5165\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5f97\u5230\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002","f3b8e0b4":"#### \u6a21\u578b\u7684\u8bad\u7ec3"}}