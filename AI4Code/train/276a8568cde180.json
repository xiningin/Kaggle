{"cell_type":{"e1ceb2ed":"code","e35dd98b":"code","4f8466a5":"code","0f1d76f5":"code","c1e42629":"code","f99e548a":"code","7f0be059":"code","1f7c38eb":"code","03962fe9":"code","af13b74b":"code","514fd290":"code","1129b3f5":"code","abbdf70a":"code","a30c1f73":"code","ca1c471e":"markdown","916b9a7d":"markdown","437662af":"markdown","d9ed6c96":"markdown","39dceaa9":"markdown","f3662fd9":"markdown","3c1e5a7b":"markdown","5632ed23":"markdown","9d96daaf":"markdown","30621295":"markdown","17443615":"markdown","60eca6ca":"markdown","88b8cbc0":"markdown","35956406":"markdown","b5ce0f57":"markdown","01a91f33":"markdown","6b4e683b":"markdown","5c32d87b":"markdown","5b5edb74":"markdown","51b5dabd":"markdown","c0ffe78f":"markdown"},"source":{"e1ceb2ed":"from nltk.tokenize import sent_tokenize\ntext = \"Awesome! I am learning NLP.\"\nfor sent in sent_tokenize(text):\n    print(sent)","e35dd98b":"from spacy.lang.en import English\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\ndoc = nlp('Hello, world. Here are two sentences.')\nfor sent in doc.sents:\n    print(sent)","4f8466a5":"import spacy\nfrom spacy import displacy\n\nnlp = spacy.load('en_core_web_sm')\n\ndoc = nlp('My name is Michael, I live in Bangalore.')\ndisplacy.render(doc,style='dep',jupyter=True,options={'distance':140})","0f1d76f5":"from nltk.tokenize import word_tokenize\ntext = \"God is Great! I won a lottery.\"\nfor word in word_tokenize(text):\n    print(word)","c1e42629":"from spacy.lang.en import English\nnlp = English()\n# Created by processing a string of text with the nlp object\ndoc = nlp(\"God is Great! I won a lottery.\")\n\n# Iterate over tokens in a Doc\nfor token in doc:\n    print(token.text)","f99e548a":"## Download Wordnet through NLTK in python console:\nimport nltk\nnltk.download('wordnet')\n\nfrom nltk.stem import WordNetLemmatizer \n\n# Init the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Lemmatize Single Word\nprint(lemmatizer.lemmatize(\"bats\"))\nprint(lemmatizer.lemmatize(\"are\"))\nprint(lemmatizer.lemmatize(\"feet\"))\n\n# Define the sentence to be lemmatized\nsentence = \"The striped bats are hanging on their feet for best support\"\n\n# Tokenize: Split the sentence into words\nword_list = nltk.word_tokenize(sentence)\nprint(word_list)\n\n# Lemmatize list of words and join\nlemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\nprint(lemmatized_output)","7f0be059":"print(lemmatizer.lemmatize(\"stripes\", 'v'))\nprint(lemmatizer.lemmatize(\"stripes\", 'n'))  ","1f7c38eb":"import spacy\n\n# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nsentence = \"The striped bats are hanging on their feet for best support\"\n\n# Parse the sentence using the loaded 'en' model object `nlp`\ndoc = nlp(sentence)\n\n# Extract the lemma for each token and join\n\" \".join([token.lemma_ for token in doc])","03962fe9":"from textblob import TextBlob, Word\n\n# Lemmatize a word\nword = 'stripes'\nw = Word(word)\nw.lemmatize()","af13b74b":"# Lemmatize a sentence\nsentence = \"The striped bats are hanging on their feet for best\"\nsent = TextBlob(sentence)\n\" \". join([w.lemmatize() for w in sent.words])","514fd290":"import pattern\nfrom pattern.en import lemma, lexeme\n\nsentence = \"The striped bats were hanging on their feet and ate best fishes\"\n\" \".join([lemma(wd) for wd in sentence.split()])","1129b3f5":"# Lexeme's for each word \n[lexeme(wd) for wd in sentence.split()]","abbdf70a":"from pattern.en import parse\nprint(parse('The striped bats were hanging on their feet and ate best fishes',lemmata=True, tags=False, chunks=False))","a30c1f73":"from gensim.utils import lemmatize\nsentence = \"The striped bats were hanging on their feet and ate best fishes\"\n[wd.decode('utf-8').split('\/')[0] for wd in lemmatize(sentence)]","ca1c471e":"#### Pattern Lemmatizer","916b9a7d":"## Morphological Analysis and Part-of-Speech Tagging\n\nHaving texts separated in tokens, the next step is usually morphosyntactic analysis, in order to identify characteristics as word lemma and parts of speech.\n\nIt is important to distinguish two concepts: lexeme and word form.\n1. the words \u201cbook\u201d and \u201cbooks\u201d refer to the same concept and thus have the same lexeme and have different word forms.\n2. the words \u201cbook\u201d and \u201cbookshelf\u201d have different word forms and different lexemes as they refer to two different concepts.\n\nFinding word lemmata brings the advantage of having a single form for all words that have similar meanings. For example, the words \u201cconnect\u201d, \u201cconnected\u201d, \u201cconnecting\u201d, \u201cconnection\u201d, and \u201cconnections\u201d roughly refer to the same concept and have the same lemma. Also, this process reduces the total number of terms to handle, which is advantageous from a computer processing point of view, as it reduces the size and complexity of data in the system.\n\nThe process of determining the word lemma is called `lemmatization`.\n\nAnother method called `word stemming` is common due to its simplicity. Word stemming reduces words to their base form by removing suffi xes. The remaining form is not necessarily a valid root but it is usually suffi cient that related words map to the same stem or to a reduced set of stems if words are irregular. For example the words \u201cmice\u201d and \u201cmouse\u201d have the lemma \u201cmouse\u201d but some stemmers produce \u201cmic\u201d and \u201cmous\u201d, respectively.\n\nWe will see how to perform lemmatisation using:\n1. Wordnet Lemmatizer\n2. Spacy Lemmatizer\n3. TextBlob\n4. CLiPS Pattern\n5. Stanford CoreNLP\n6. Gensim Lemmatizer\n7. TreeTagger","437662af":"### Word Tokenisation","d9ed6c96":"## General Architecture for Information Extraction","39dceaa9":"#### Gensim Lemmatize\n\nGensim provide lemmatization facilities based on the pattern package. It can be implemented using the lemmatize() method in the utils module. By default lemmatize() allows only the \u2018JJ\u2019, \u2018VB\u2019, \u2018NN\u2019 and \u2018RB\u2019 tags.","f3662fd9":"Sometimes, the same word can have a multiple lemmas based on the meaning \/ context","3c1e5a7b":"# 1: INTRODUCTION TO NATURAL LANGUAGE ANALYTICS","5632ed23":"## Tokenization and Sentence Boundary Detection\n\nDocument processing usually starts by separating document\u2019s texts in its atomic units. Breaking a stream of text into tokens (words, numbers, and symbols) is known as `tokenization`\n\nIt is a quite straightforward process for languages that use spaces between words, such as most languages using the Latin alphabet. Tokenizers often rely on simple heuristics as \n\n1. All contiguous strings of alphabetic characters are part of one token, the same applies to numbers,\n2. Tokens are separated by whitespace characters\u2014space and line break\u2014or by punctuation characters that are not included in abbreviations\n\n`Sentence boundary` detection, as its name suggests, addresses the problem of finding sentence boundaries. The concept of sentence is central in several natural language processing tasks, since sentences are standard textual units which confine a variety of linguistic phenomena such as collocations and variable binding.","9d96daaf":"#### spaCy Lemmatization\nspaCy determines the part-of-speech tag by default and assigns the corresponding lemma. It comes with a bunch of prebuilt models where the \u2018en\u2019 we just downloaded above is one of the standard ones for english.","30621295":"## NLP Approaches","17443615":"Although IE approaches differ significantly, the core process is usually organized as a processing pipeline that include domain independent components \n\n    \u2014 tokenization, morphological analysis, part-of-speech tagging, syntactic parsing\n\nand domain specific IE components\n\n    \u2014 named entity recognition and co-reference resolution, relation identification, information fusion, among others.\n\n![NLP Overview](https:\/\/static.wixstatic.com\/media\/7ebe53_b04d3b23cbbe4cbbb5f5867d2fc89670~mv2.png\/v1\/fill\/w_800,h_477\/NLPPipeline.png)","60eca6ca":"### Sentence Tokenisation","88b8cbc0":"!sudo apt-get install default-libmysqlclient-dev\n!pip install pattern","35956406":"### Chunking\nThis method is generally an optional method in the preprocessing of text. Chunking tries to split the sentences further into more meaningful pieces. For example the statement\n    `My name is Michael, I live in Bangalore.`\nWe can split this sentence into 2 parts one that provides information on the identity and the other on habitat. This is generally achieved through Semantic Segmentation using POS tagging and building syntactic trees that can then be split on the bases of need.","b5ce0f57":"## Ambiguity in NLP\n\n`Linguistic ambiguity` is sometimes a source of humor, but many common words and sentences have multiple interpretations that pass unnoticed. For example, the noun \u2018bank\u2019 has many meanings. It can refer to a financial institution, or a river margin, or to the attitude of betting or relying upon something. Humans rarely confuse these meanings, because of the different contexts in which tokens of this word occur, and because of real world knowledge. Everyone who reads the newspapers knows that \u2018the West Bank of Jordan\u2019 does not refer to a financial institution. \n\n\u2018Bank\u2019 is an instance of lexical ambiguity. But whole sentences can be ambiguous with respect to their structure and hence their meaning. Sentences that no human would deem ambiguous can cause problems to computer programs e.g.\n\n        \u2018She boarded the airplane with two engines.\u2019\n\nIt\u2019s obvious to you and I that the suitcases belong to the woman, and the engines belong to the airplane, but how is a computer supposed to know this? This is known as `syntactic ambiguity`\n\n\n","01a91f33":"## NLP and linguistics\nIt is very important for a good NLP engineer to understand a certain level of linguistic principles. We most often find that we need to understand the `syntactic structure` along with the `semantic understanding` of a sentence.\n`Pragmatics and context` is another aspect we need to take into account when working with NLP. For e.g.\n\n        \u2018You owe me five dollars\u2019\n\nCould mean a request for payment rather than an stating of a fact. Hence intention is very important during understanding of a sentence. \n\n        \u2018natural language processing\u2019\nIf i type the above sentence in the query box of a search engine, what am I really looking for? A definition?\nReferences to the literature? Experts in NLP? Courses in NLP? An \u2018intelligent\u2019 search engine might be able to figure this out, by looking at my previous queries. Each of the candidate preceding queries listed below might point the search engine in a different direction. Hence `use and context` are very important in umderstanding any sentence.","6b4e683b":"## Process Overview\n\nThe process is composed of successive NLP steps starting by making contents uniform, and ending with the identification of the roles of the words and how they are arranged. \n\nThe first steps are usually tokenization and sentence boundary detection. Its purpose is to break contents into sentences and define the limit of each token: word, punctuation mark, or other character clusters such as currencies. \n\nAfterwards, all processing is usually conducted in a per-sentence fashion and tokens are considered atomic.\n\nThen, morphological analysis makes tokens uniform by determining word lemmata, and part-of-speech tagging assigns a part-of-speech to each token. The final step is usually syntactic parsing. These NLP steps prepare the textual contents for subsequent identifi cation and extraction of relevant information.\n\n![NLP Tasks](https:\/\/static.wixstatic.com\/media\/7ebe53_d3caf2dbaad84b43a5a0d43b2bad5769~mv2.png\/v1\/fill\/w_542,h_369\/nlp-tasks.png)","5c32d87b":"Relative to the technology used, earlier AI systems were essentially rule based approaches, also called `knowledge engineered` approaches. This type of technology is still used in modern approaches, at least partially. It uses hard coded rules created by human experts that encode linguistic knowledge by matching patterns over a variety of structures: text strings, part-of-speech tags, dictionary entries. The rules are usually targeted for specific languages and domains and this type of systems are generically very accurate and ready to use out of the box. Motivated by the success of theses approaches the scope was broadened to include more unstructured and noisy sources and, as result, `statistical learning` algorithms were introduced.\nCurrently NLP adopts `Hybrid approaches`, which use a mix of the previous two, combine the best features of each kind of approach viz. the accuracy of rule based approaches with the coverage and adaptability of machine learning approaches.","5b5edb74":"#### NLTK\nWordnet is an large, freely and publicly available lexical database for the English language aiming to establish structured semantic relationships between words. It offers lemmatization capabilities as well and is one of the earliest and most commonly used lemmatizers.\n\nNLTK offers an interface to it, but you have to download it first in order to use it. Follow the below instructions to install nltk and download wordnet.","51b5dabd":"#### TextBlob Lemmatizer\nTexxtBlob is a powerful, fast and convenient NLP package as well. Using the Word and TextBlob objects, its quite straighforward to parse and lemmatize words and sentences respectively.","c0ffe78f":"#### Stanford CoreNLP Lemmatization"}}