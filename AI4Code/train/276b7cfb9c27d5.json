{"cell_type":{"67f80c9b":"code","2a897c42":"code","74953937":"code","e1026e62":"code","35028662":"code","f5fd6624":"code","974eea2e":"code","1562581c":"code","68624c15":"code","589782c5":"code","3760d197":"code","b4a795e5":"code","8223d43d":"code","d65e523a":"code","a683750c":"code","5ae75693":"code","79d74aea":"code","812e611d":"code","3c9abdf0":"code","6f855cc4":"code","1576a7ab":"code","e3c94204":"code","7dfe4df1":"code","a9260190":"code","9ef529ea":"code","c3af7e87":"code","fa13bfb7":"code","6171d58b":"code","377d6937":"code","71e7d92f":"code","e8b03ddc":"code","63e7fbd5":"code","55275600":"code","1dcc5baf":"code","c2b57125":"code","1c029fa2":"code","b204583c":"code","0d3c28de":"code","4fe10edb":"code","ab293692":"code","9a8323df":"code","ab93a134":"code","2fee186c":"code","aedf80b6":"code","3c47d437":"code","f233b799":"code","aa8c59bd":"code","0d7425f4":"code","8d86c634":"code","b58a5381":"markdown","9bd6561a":"markdown","d62788c1":"markdown","66ba7670":"markdown","9407b84a":"markdown","30ed2beb":"markdown","34640e4e":"markdown","67f09d30":"markdown","d0999fc7":"markdown","329d2d35":"markdown","5d284bf2":"markdown","06c17b40":"markdown","3c8711da":"markdown","4d148ce7":"markdown","957f15fd":"markdown","b21d5399":"markdown","ef7811d7":"markdown","c722ab6c":"markdown","e4926f3a":"markdown","1cc81f89":"markdown","121e2a68":"markdown","c0663e73":"markdown","0139d52d":"markdown","56739e6d":"markdown","7b4d7192":"markdown","9638b3b6":"markdown","6cf3a4ea":"markdown","10180204":"markdown","5a297209":"markdown","fbd962d4":"markdown","ea6ef877":"markdown","67310170":"markdown","0260f197":"markdown","14f80ecc":"markdown","269c2ec3":"markdown","8602d0b5":"markdown","9d1a499d":"markdown","441e0e39":"markdown","e73ddd44":"markdown","9674403a":"markdown","b4f68b21":"markdown","93b65006":"markdown","72ca8e2c":"markdown","5b0b7eb9":"markdown","d51ae502":"markdown","c465f66c":"markdown","b7bbee67":"markdown","96ddddea":"markdown","6af14e1d":"markdown","adc1271b":"markdown","a5edb827":"markdown","c108ba8c":"markdown","3d339447":"markdown","aa57043f":"markdown","08417de7":"markdown","31747e93":"markdown","1a0d8aa8":"markdown","4f287adb":"markdown"},"source":{"67f80c9b":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom string import digits\n\n# data normalization dependences\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\n# models\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.pipeline import Pipeline\n\n# model evaluation metrics\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# visualization dependences\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n","2a897c42":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nraw_data = train.posts","74953937":"train.head()","e1026e62":"test.head()","35028662":"def word_count(df,posts,new_col):\n    \"\"\" \n    This is a function is to create a new column pf word count per row. \n  \n    Parameters: \n    df (obj): Data frame.\n    posts (obj): Posts per user.\n    new_col (obj): New column of word counts per row.\n  \n    Returns: \n    None.\n  \n    \"\"\"\n    df[new_col] = df[posts].apply(lambda x: len(str(x).split(' ')))","f5fd6624":"word_count(train,'posts','word_count')","974eea2e":"def plot_type_dist(df):\n    \"\"\" \n    This is a function is to plot the distribution of the personality types. \n  \n    Parameters: \n    df (obj): Data frame. \n  \n    Returns: \n    obj: Bar plot of the personality types.\n  \n    \"\"\"\n    fig = plt.figure(figsize=(12,6))\n    plt.title('Distribution of MBTI personality types')\n    plt.ylabel('frequency')\n    df.groupby('type')['word_count'].sum().sort_values(ascending=False).plot(kind='bar')","1562581c":"plot_type_dist(train)","68624c15":"def new_labels(df,typ):\n    \"\"\" \n    This is a function to generate binary label resposes for personality types. \n\n    Parameters: \n    df (obj) : DataFrame.\n    typ (string): A personality type. \n\n    Returns: \n    Adds mind, energy, nature, tactics columns to the a DataFrame.\n\n    \"\"\"\n    \n    labels=['mind','energy','nature','tactics']\n    label_classes={'I': 0, 'E': 1,'S': 0, 'N': 1,'F': 0, 'T': 1,'P': 0, 'J': 1}\n    \n    def gen_response(typ):\n        \"\"\" \n        This is a function to generate binary label resposes for personality types. \n\n        Parameters: \n        typ (string): A personality type. \n\n        Returns: \n        obj: Returns a list of binary values for each label.\n\n        \"\"\"\n        labs = lambda typ:list(typ)\n        return [label_classes.get(item) for item in labs(typ)]\n    \n    label_values=df[typ].apply(gen_response)\n\n    for index,label in enumerate(labels):\n        df[label]= np.array(list(label_values))[:,index]","589782c5":"new_labels(train,'type')","3760d197":"def plot_type(df,clas,label1,label2,subplot_pos):\n    \"\"\" \n    This is a function to plot personality labels. \n\n    Parameters: \n    df (obj): DataFrame.\n    clas (obj): Column personality class.\n    label1 (obj): Class label.\n    label2 (obj): Class label.\n    subplot_pos (obj): Position of plot.\n\n    Returns: \n    obj: Returns a subplot.\n\n    \"\"\"\n        \n    data=df[clas]\n    plt.figure(figsize=(15,5))\n    plt.subplot(subplot_pos)\n    plt.title('Destribution of '+ label1 + ' Vs ' + label2)\n    data.value_counts().plot(kind='bar')\n    plt.xticks(ticks=[0,1],labels=[label1,label2],rotation=0)","b4a795e5":"plot_type(train,'mind', 'Introvert', 'Extrovert',221)\nplot_type(train,'energy', 'Intuition','Sensing',222) \nplot_type(train,'nature', 'Thinking','Feeling',223) \nplot_type(train,'tactics', 'Judging','Perceiving',224) \n\nplt.subplots_adjust(left = 0.125,right = 0.9 ,bottom = 0.5 ,top = 2.5  )\nplt.show()","8223d43d":"from wordcloud import WordCloud, STOPWORDS\ndef word_cloud(df,clas,label1,label2):\n    \"\"\" \n    This is a function to display word cloud from a personality category. \n\n    Parameters: \n    df (obj): DataFrame.\n    clas (obj): Column personality class.\n    label1 (obj): Class label.\n    label2 (obj): Class label.\n\n    Returns: \n    obj: Returns two word clouds.\n\n    \"\"\"\n    \n    fig, ax = plt.subplots(len(df[clas].unique()), sharex=True, figsize=(15,10*len(df[clas].unique())))\n    k = 0\n    for i in df[clas].unique():\n        df_4 = df[df[clas] == i]\n        if i == 0:\n            tit = label1\n        else:\n            tit = label2\n        wordcloud = WordCloud(background_color=\"white\").generate(df_4['posts'].to_string())\n        ax[k].imshow(wordcloud)\n        ax[k].set_title(tit)\n        ax[k].axis(\"off\")\n        k+=1\n        \nword_cloud(train,'mind','Introverts','Extroverts')","d65e523a":"def word_count(df):\n    \"\"\" \n    This is a function to display a pie charts of the number of words used by different classes. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: Returns pe charts of classes.\n\n    \"\"\"\n        \n    classes = [['mind','energy'],['nature', 'tactics']]\n    label=[['Introvert','Extrovert'],['Sensing','Intuition'],['Feeling','Thinking'],['Judging','Percieving']]\n    \n    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12,6))  # creates 2x2 grid of subplots \n    fig.suptitle('Word Count Percentage Of Labels In Each Class') # Header title\n    z=0\n    for i, row in enumerate(ax): # loop through the matplotlib subplots grid\n        for j,obj in enumerate(row):\n            x1=pd.DataFrame(data=df.groupby(classes[i][j]).word_count.sum()) # groupby classes\n            obj.pie(x1.iloc[:,0],labels=label[z],autopct='%1.1f%%', shadow=True,explode=(0, 0.1)) # plot pie chart\n            z+=1\n            obj.set_title(classes[i][j],fontsize=16) # set titles","a683750c":"word_count(train)","5ae75693":"#standard text encoding\ntrain.posts = train.posts.apply(lambda x: x.encode('ascii','ignore').decode(\"utf-8\"))\ntest.posts = test.posts.apply(lambda x: x.encode('ascii','ignore').decode('utf-8'))","79d74aea":"#dictionary of words and their contractions\ncontractions = { \n\"ain't\": \"am not\",\"aren't\": \"are not\",\n\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n\"could've\": \"could have\",\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\"don't\": \"do not\",\n\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\"haven't\": \"have not\",\n\"he'd\": \"he would\",\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\"how'd\": \"how did\",\n\"how're\":\"how are\",\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\"how's\": \"how is\",\n\"i'd\": \"i would\",\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\"i've\": \"i have\",\n\"isn't\": \"is not\",\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\"it's\": \"it is\",\n\"let's\": \"let us\",\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\"might've\": \"might have\",\n\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\"she's\": \"she is\",\n\"should've\": \"should have\",\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n\"so's\": \"so is\",\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\"that's\": \"that is\",\n\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\"they're\": \"they are\",\n\"they've\": \"they have\",\"to've\": \"to have\",\n\"wasn't\": \"was not\",\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\"we're\": \"we are\",\n\"we've\": \"we have\",\"weren't\": \"were not\",\n\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\"what's\": \"what is\",\n\"what've\": \"what have\",\"when's\": \"when is\",\n\"when've\": \"when have\",\"where'd\": \"where did\",\n\"where's\": \"where is\",\"where've\": \"where have\",\n\"who'll\": \"who will\",\"who'll've\": \"who will have\",\n\"who's\": \"who has\",\"who've\": \"who have\",\n\"why's\": \"why is\",\"why've\": \"why have\",\n\"will've\": \"will have\",\"won't\": \"will not\",\n\"won't've\": \"will not have\",\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","812e611d":"def expand_text(df,contr_dict):\n    \"\"\" \n    This is a function to expand contractions of words by applying another function. \n  \n    Parameters: \n    df (object): Data frame.\n  \n    Returns: \n    obj: Returns a dataframe.\n  \n    \"\"\"\n    df=df.copy()\n    def convert(post):\n        \"\"\" \n        This is a function to convert strings to lower case and convert contractions as well. \n  \n        Parameters: \n        post (string): Post from every user.\n  \n        Returns: \n        object: Returns a post.\n  \n        \"\"\"\n        post=post.lower()\n        for key,val in contr_dict.items():\n            if key in post:\n                pospostt = post.replace(key,val)\n        return post\n    df['posts']=df['posts'].apply(convert)\n    return df\n\ntrain = expand_text(train,contractions)\ntest=expand_text(test,contractions)\n","3c9abdf0":"def remove_url(df):\n    \"\"\" \n    This is a function to remove web urls. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame without web urls.\n\n    \"\"\"\n    #create space between posts\n    df.posts = df.posts.apply(lambda x: x.replace('|||',' ||| '))\n    #Regular expression matching web urls in the dataset\n    pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    subs_url = r'url-web'\n    #replace the links with text\n    df['posts'] = df['posts'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n\nremove_url(train)\nremove_url(test)","6f855cc4":"def remove_punc(df):\n    \"\"\" \n    This is a function to remove punctuation. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame without punctuation.\n\n    \"\"\"\n    #maps 31 special characters to space.\n    table = str.maketrans(\"~)\\\/><`^%$#@+=&*:;_'[{]|}(\",26*\" \")\n    df['posts'] = train['posts'].apply(lambda x: x.translate(table))\n\nremove_punc(train)\nremove_punc(test)","1576a7ab":"def remove_digts(df):\n    \"\"\" \n    This is a function to remove digits. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame without digits.\n\n    \"\"\"\n    #maps digits to space.\n    remove_digits = str.maketrans('', '', digits)\n    df.posts = df.posts.apply(lambda x: x.translate(remove_digits))\n    \nremove_digts(train)\nremove_digts(test)","e3c94204":"def remove_dup_space(df):\n    \"\"\" \n    This is a function to remove duplicate white space. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame.\n\n    \"\"\"\n    df['posts'] = df['posts'].apply(lambda x: \" \".join(x.split()))\n    \nremove_dup_space(train)\nremove_dup_space(test)","7dfe4df1":"import itertools\ndef std_words(df):\n    \"\"\" \n    This is a function to standardise words. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame.\n\n    \"\"\"\n    df['posts'] = df['posts'].apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n    \nstd_words(train)\nstd_words(test)","a9260190":"def remove_stop_words(df):\n    \"\"\" \n    This is a function to remove stop words as well as personality types text. \n\n    Parameters: \n    df (obj): DataFrame.\n\n    Returns: \n    obj: DataFrame.\n\n    \"\"\"\n    stop = stopwords.words('english')\n    df['posts'] = df['posts'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    \nremove_stop_words(train)\nremove_stop_words(test)","9ef529ea":"freq_train = pd.Series(' '.join(train['posts']).split()).value_counts()[:10]\nprint('train data: \\n',freq_train)\n\nfreq_test = pd.Series(' '.join(test['posts']).split()).value_counts()[:10]\nprint('test data: \\n',freq_test)","c3af7e87":"freq_train = list(freq_train.index)\nfreq_test = list(freq_test.index)\n\ndef remv_fq(df,freq_list):\n    \"\"\" \n    This is a function to remove frequently used words. \n\n    Parameters: \n    df (obj): DataFrame.\n    freq_list (obj): List of frequently used words.\n\n    Returns: \n    obj: DataFrame\n\n    \"\"\"\n    df['posts'] = df['posts'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_list))\n    \nremv_fq(train,freq_train)\nremv_fq(test,freq_test)","fa13bfb7":"wordnet_lemmatizer = WordNetLemmatizer()\ndef my_tokenizer(post):\n    \"\"\" \n    This is a function to lemmatize and tokenize posts. \n  \n    Parameters: \n    post (string): Post from user.\n  \n    Returns: \n    string: Returns lemmatized post.\n  \n    \"\"\"\n    tokens = nltk.tokenize.word_tokenize(post)\n    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens if len(token)>2]\n    new_post = \" \".join(tokens)\n    return new_post\ntrain.posts = train.posts.apply(lambda x: my_tokenizer(x))\ntest.posts = test.posts.apply(lambda x: my_tokenizer(x))\n\n","6171d58b":"# #Saving clean dataframes to csv file\ntrain.to_csv('cleaned_train.csv',sep=',',index=False)\ntest.to_csv('cleaned_test.csv',sep=',',index=False)\n","377d6937":"train = pd.read_csv('cleaned_train.csv')\ntest = pd.read_csv('cleaned_test.csv')","71e7d92f":"labels = ['mind','energy','nature','tactics']\nX_train = train.posts.apply(lambda x: np.str_(x)) # convert data to strings\nX_test = test.posts.apply(lambda x: np.str_(x))\ny_train = train[labels] ","e8b03ddc":"# initialize pipe\n# use crss_val_score function on uncleaned data\n# scoring method neg_log_loss \n# output mean score\n\npipe_1 = Pipeline([('vec', CountVectorizer(decode_error='ignore',)),\\\n                   ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=200,n_jobs=-1)))])\n\ncv_pipe_1_score=cross_val_score(pipe_1,raw_data,y_train,scoring='neg_log_loss',cv=3)\nnp.mean(cv_pipe_1_score)","63e7fbd5":"# initialize pipe\n# use crss_val_score function cleaned data\n# scoring method neg_log_loss \n# output average score\n\npipe_1 = Pipeline([('vec', CountVectorizer()),\\\n                   ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=200,n_jobs=-1)))])\n\ncv_pipe_1_score=cross_val_score(pipe_1,X_train,y_train,scoring='neg_log_loss',n_jobs=-1,cv=3)\nnp.mean(cv_pipe_1_score)","55275600":"# fit and make predictions on train data \n# use y_train predictions to create confusion matrix\n\npipe_1 = Pipeline([('vec', CountVectorizer()),\\\n                   ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=1000)))])\n\npipe_1.fit(X_train,y_train)\npred_pipe_1 = pipe_1.predict(X_train)","1dcc5baf":"def plot_cnf_matrix(pred):\n    fig, ax = plt.subplots(ncols=2,nrows=2,sharex=True,sharey=True,figsize=(10,5))\n    sns.heatmap(confusion_matrix(y_train.mind,pred[:,0]),annot=True,ax=ax[0,0],cbar=False)\n    sns.heatmap(confusion_matrix(y_train.energy,pred[:,1]),annot=True,ax=ax[0,1],cbar=False)\n    sns.heatmap(confusion_matrix(y_train.nature,pred[:,2]),annot=True,ax=ax[1,0],cbar=False)\n    sns.heatmap(confusion_matrix(y_train.tactics,pred[:,3]),annot=True,ax=ax[1,1],cbar=False) \n    ax[0,0].set_title('mind')\n    ax[0,1].set_title('energy')\n    ax[1,0].set_title('nature')\n    ax[1,1].set_title('tactics')\n    plt.suptitle('Confusion metrix for predictions on train data')\nplot_cnf_matrix(pred_pipe_1)","c2b57125":"#This function create kaggle submission file.\n\ndef predictions(model,test_data,filename):\n    \"\"\" \n    This is a function to predict testing data and create a submission file. \n  \n    Parameters: \n    model (obj): Model used for prediction.\n    test_data (obj) : Test dataset.\n    filename (obj) : The name of the file to be created.\n  \n    Returns: \n    obj: Returns the first 5 entries in the submission file.\n  \n    \"\"\"\n    mp = model.predict_proba(test_data)\n    submission=pd.DataFrame({'id': test.id,labels[0]:np.where(mp[:,0]>0.5,1,0),labels[0]:np.where(mp[:,0]>0.50,1,0),\\\n                             labels[1]:np.where(mp[:,1]>0.50,1,0),labels[2]:np.where(mp[:,2]>0.50,1,0),labels[3]:np.where(mp[:,3]>0.50,1,0)})\n    submission.to_csv(filename,encoding='UTF-8', index=False,sep=',')\n    return submission.head(5)","1c029fa2":"pipe_1 = Pipeline([('vec', CountVectorizer()),\\ \n                   ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=200,n_jobs=-1)))])\n\npipe_1.fit(X_train,y_train) # fit ligistic regression \npredictions(pipe_1,X_test,'log_count_vec.csv') # call functions **predictions** to create submission file","b204583c":"pipe_tfidf = Pipeline([('vec', TfidfVectorizer()),\\\n                       ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=200,n_jobs=-1)))])\npipe_tfidf.fit(X_train,y_train) # fit ligistic regression \npred_idf = pipe_tfidf.predict(X_train)\nplot_cnf_matrix(pred_idf)","0d3c28de":"predictions(pipe_tfidf,X_test,'log_count_tfidf.csv') # call functions **predictions** to create submission file","4fe10edb":"pipe_lg = Pipeline([('tfidf',TfidfVectorizer()),\\\n        ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=200,class_weight='balanced',n_jobs=-1),n_jobs=-1))])\nparam_grid = {'tfidf__ngram_range':[(1,2),(1,3)]}#,'clf__estimator__C':[20,30]}\npipe_lg_gs = GridSearchCV(pipe_lg,param_grid,scoring='neg_log_loss',cv=3,n_jobs=-1) # takes time to run\n%time pipe_lg_gs.fit(X_train,y_train)\nprint(pipe_lg_gs.best_score_)","ab293692":"pipe_lg_gs.best_params_","9a8323df":"best_est_1= Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1, 2))),\\\n        ('clf',OneVsRestClassifier(LogisticRegression(solver='lbfgs',C=30,max_iter=200,class_weight='balanced')))])\nbest_est_1.fit(X_train,y_train)\npred_best_est_1 = best_est_1.predict(X_train)\nplot_cnf_matrix(pred_best_est_1)","ab93a134":"#### kaggle submission \npredictions(best_est_1,X_test,'gs_val_best_est.csv')","2fee186c":"# # submit to kaggle\npredictions(best_est_1,X_test,'gs_val_best_est_proba.csv')","aedf80b6":"best_est_2= Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1, 2))),\\\n        ('clf',OneVsRestClassifier(MultinomialNB(),n_jobs=-1))])\n# fit Xtrain\nbest_est_2.fit(X_train,y_train)\n# predict X_train\npred_best_est_2 = best_est_2.predict(X_train)\nplot_cnf_matrix(pred_best_est_2)","3c47d437":"predictions(best_est_2,X_test,'gs_val_best_est_proba.csv')","f233b799":"best_est_3= Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1, 2))),\\\n        ('clf',OneVsRestClassifier(LinearSVC(),n_jobs=1))])\nbest_est_3.fit(X_train,y_train)\npred_best_est_3 = best_est_3.predict(X_train)\nplot_cnf_matrix(pred_best_est_3)","aa8c59bd":"cv_pipe_1_score=cross_val_score(best_est_3,X_train,y_train,scoring='f1_samples',n_jobs=-1,cv=3)\nnp.mean(cv_pipe_1_score) # value must be closer to +\/- zero","0d7425f4":"best_est_3= Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1, 2))),\\\n        ('clf',OneVsRestClassifier(LinearSVC(),n_jobs=-1))])\nparam_grid = {'clf__estimator__C':[20,30]}\npipe_lg_gs = GridSearchCV(best_est_3,param_grid,scoring='f1_samples',n_jobs=-1)\npipe_lg_gs.fit(X_train,y_train)\nprint(pipe_lg_gs.best_score_)","8d86c634":"# This makes prediction on test data for submissoin\nlogreg = Pipeline([('tfidf',TfidfVectorizer(max_features=5000,ngram_range=(1, 3))),\\\n        ('clf',OneVsRestClassifier(LogisticRegressionCV(n_jobs=-1,max_iter=200,cv=10)))])\nlogreg.fit(X_train,y_train)\npredictions(logreg,X_test,'LogisticRegressionCV.csv')","b58a5381":"### Naive Bayes Multinomial Classifier\nNaives Bayes Classifier optimizes a posteror function, it uses naive assumptions that individual class are conditionally independent. Likelihood and prior are very good estimates for this algorithm. We are using this algorithm because it's fast. Estimator pipelining is used here and the tfidf vectorizer is used. We're going to use the same parameters for TF-IDF to make things easy for model selection.\n\n**Training MultinomialNB estimator**","9bd6561a":"**Common words**<br>\n\nHere we check frequently occurring words so we can remove them because they do not add much value. We first look at them.","d62788c1":"## Word Cloud\nWe then create two word clouds for introverts and extroverts before we clean words to see the frequency of words used.<br>\n- From the word cloud, we see that introverts tend to watch youtube (word 'watch' and 'youtube' is bigger) more than extroverts who are probably outdoors experiencing life.\n- Extroverts use the word 'know' and 'thank' more than introverts, they also seem to use the word 'think' more which is also interesting to observe.\n- We also see that introverts mention the word 'people' more than extroverts, a little nosy perhaps?","66ba7670":"So by using the predict_proba function to make predictions it improved the score with a large amount, that means sometimes converting probabilites to binary numbers is miss leading. Now kaggle score:\n\n      - from 6.78674 \n      - to 0.42160\n  \nThis means we must find models that make probability estimations.\n\n- Classifiers -\n\n    - MultinomialNB\n    - LinearSCV\n    - LogisticRegressionCV","9407b84a":"This matrix indicate that the model overfits on training data so we need to evaluate these results from kaggle.","30ed2beb":"## Word Count for Each Class\nWe use word count column to plot out how much words a personality profile label uses for posts.","34640e4e":"### Import Packages","67f09d30":"# Exploratory Data Analysis (EDA)\nWe now go into the first important process of performing initial investigations on the data so that we can discover patterns and spot anomalies by summarising the data and using graphical representations.","d0999fc7":"**Expanding Contractions** <br>\nWe convert each contraction to its expanded, original form. The function will also convert text to lowercase. This helps with text standardization.","329d2d35":"## Models\n#### Steps to follow:\n\n- Split data to **X_train** and **y_train**\n- Fit model\n    - Use scikit learn Pipelines\n        + Find best parameters(GridSearch)\n    - Measure model performance","5d284bf2":"## Visualise data\nVisualising our data is an important to spot patterns and the distribution of the data. We will count the number of words per user and visualise the distribution of the data.","06c17b40":"The is overfitting on training data becuase it did not do well on kaggle test, but it lightly imporved the score from:\n\n    -  from 6.96197\n    -  to  6.78674\nHere are the scores but we have been using the predict function to make predictions, this limits you from choosing the optimal boundry threshold. So now we're going to predict using predict_proba function to allow for more options.","3c8711da":"## Data Preview\nWe explore the train and test data for preliminary data exploration.","4d148ce7":"This sugggest our confidence parameter range is not that good because we can not tell if these are the best parameters Gridearch can select. This suppose to choose values in between but this task we can not repeat because we compiutational paoer in efficience. Do it another time. Now let's actully use this estimators.","957f15fd":"**Removing web urls**<br>\nLet's replace website urls with string represention, this helps to avoid loosing information and makes analysis easy when working with plain text than web urls.","b21d5399":"Split X_train, y_train and X_test getting ready for modeling.","ef7811d7":"**Removing Noise**<br>\nWe will perform the following steps to remove noise in our dataset:\n- removing the web-urls\n- removing punctuations\n- removing digits","c722ab6c":"This is the cv score of the best estimator selected by GridSearch. So we will use the best params seleted to fit the model.","e4926f3a":"## Individual Words Analysis\n- Expanding Contractions\n- Removing Noise\n- Standardizing words\n- Stop Words\n- Tokenising","1cc81f89":"- The bar chart below shows that the dataset is heavily skewed toward Introversion over Extroversion and Intuition over Sensitivity.<br>\n- The dataset is also moderately skewed towards Judging over Perceiving and thinking over Feeling.","121e2a68":"Read seralized clean data from disk.","c0663e73":"## Load Datasets\nWe load the train dataset that will be used to train our models as well as the test dataset to be classified.","0139d52d":"**Standardizing words**\n\nSometimes words are not in a proper formats. For example: \u201cI looooveee you\u201d should be \u201cI love you\u201d. So we use itertools to fix these words.","56739e6d":"**Preliminary testing**<br>\n**Step1:**  training logistic regression on **Raw** data. This allows to campare model performance after applying pre-processing steps and also gives direction on how to improve the model. Applying CountVectorizer to convert raw text to a vector matrix. And the cross validation score method was used to measure perfomance.","7b4d7192":"So with the default parameters the linearSVC models fit well in the train data, this may indicate that the models is overfit. To try and test if it does not overfit we use cross_val_score for evaluation.","9638b3b6":"# Recommendations\nOur dataset had web URLs and we did not extract useful information like titles from them which could have improved our model performance by providing us with more input. Data can also be sourced from different social media sites to make sure we get a variety of personality types since it is possible that introverts were more likely to participate in the PersonalityCafe forum which is werre our dataset comes from. Using a site like facebook might be more beneficial to solving the problem of an unbalanced dataset. Since facebook has a broad community, surveys can be recommended to users with their permission to participate in research.","6cf3a4ea":"So before seeing kaggle score looking at the confussion matrix shows that this model does not overfit much on the train data. So The number of **True Negative**s and **False Positives** increased a bit so we don't know if the model is good. So let's check the kaggle score.","10180204":"# Conclusion\n\nOne of the reasons behind choosing a logistic regression is that in the process of getting accuracy we found that we had better results with not as high a hit in terms of computational performance . This is important as we are trying to find a model that fits well and quicker with the tools available to us. This is also crucial in that we are able to give a high level of confidence in the results and also similarly provide classification with relatively good speed in getting there.\nThe data was imbalanced and this caused a degree of complication for making this work. This imbalance required of us to find creative ways to manage this. We managed to find a good work around by using the class weight variable within the different classes. This is one of the ways in which we had available to work around this problem.<br>\n\nWe used a couple of vectorizing techniques, namely, count-vectorizer and TF-IDF. Count vectorizer was great in that it allowed us to build a dictionary of words that appear within certain classes. This is a wonderful pre-processing technique which allows for some brilliant encoding for the length of the dictionary and the occurrences words in the posts. TF-IDF is another great way of vectorizing in that it allows us to in essence eliminate words that will appear \"too often\" within the collection of posts. This is great as it then can add weight to data from certain posts and in essence extract meaning from the remaining posts.<br>\n\nKey Findings<br>\nLinear Support Vector Vs Logistic RegressionCV\nIf you use logistic regression, it's shape is convex and there will be a single minimum. But during optimization, you may find weights that are near to optimal points and not exactly on the optimal point. This means that you can have multiple classifies that reduce the error and maybe set it to zero for the training data but with different weights which are slightly different. This can lead to different decision boundaries. Our Score for Linear RegressionCV was 5.29716\n\n","5a297209":"# Content\n<ul class=\"toc-item\"><li><span><a href=\"#Preparing-Environment-and-Uploading-Data\" data-toc-modified-id=\"Preparing-Environment-and-Uploading-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Preparing environment and uploading data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Import Packages<\/a><\/span><\/li><li><span><a href=\"#Load-Datasets\" data-toc-modified-id=\"Load-Datasets-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Load Datasets<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"Exploratory-Data-Analysis-(EDA)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Exploratory Data Analysis (EDA)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preview\" data-toc-modified-id=\"Data-Preview-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Data Preview<\/a><\/span><\/li><li><span><a href=\"#Visualise-data\" data-toc-modified-id=\"Visualise-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Visualise data<\/a><\/span><\/li><li><span><a href=\"#Distributions-of-Personality-Types\" data-toc-modified-id=\"Distributions-of-Personality Types-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Distributions of Personality Types<\/a><\/span><\/li><li><span><a href=\"#Word-Cloud\" data-toc-modified-id=\"Word-Cloud-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Word Cloud<\/a><\/span><\/li><li><span><a href=\"#Word-Count-for-Each-Class\" data-toc-modified-id=\"Word-Count-for-Each-Class-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>Word Count for Each Class<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Text-Encoding\" data-toc-modified-id=\"Text-Encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Text Encoding<\/a><\/span><\/li><li><span><a href=\"#Individual-Words-Analysis\" data-toc-modified-id=\"Individual-Words-Analysis-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Individual Words Analysis<\/a><\/span><\/li><li><span><a href=\"#Serializing-Train-and-Test-Data\" data-toc-modified-id=\"Serializing-Train-and-Test-Data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Serializing Train and Test Data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Building-Machine-Learning-Models\" data-toc-modified-id=\"Building-Machine-Learning-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>4.Building Machine Learning Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Models<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Recommendations<\/a><\/span><\/li>","fbd962d4":"# Data Preprocessing\nNow that we are done exploring our data we can move on to processing the data. Before we make predictions, we first need to clean our data and put it into the right format that will be ready for training our models.","ea6ef877":"The cv score above shows a small improvement in the model performance when trained on cleaneddata, Maybe the raw data it's self can be used to train models with minimal data pre-processing. So this Lead to seek best model optimization strategies and to use other matrix vectorizers.\n\nA look at confusion matrix visualization.","67310170":"Using cleaned data","0260f197":"So here the scoring function is not neg_log_loss we are using f1_samples because LinearSVC does not predict probabilities. This f1_samples or just F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. So the confussion matrix and cross validation score tell us that the model is not overfitting too bad. So let's try to use gridsearch.","14f80ecc":"### Logistic Regression CV","269c2ec3":"**Remove Duplicated White Spaces**","8602d0b5":"Let us now look at the distribution of the MBTI personality types on the train dataset.\n-  From the bar plot below we see that we have very few samples of 'ES' types and most of the dataset comprise of introverts, this is a class inbalance.","9d1a499d":"![title](https:\/\/codesachin.files.wordpress.com\/2015\/08\/linearly_separable_4.png) \nSource of the picture: https:\/\/codesachin.files.wordpress.com\/2015\/08\/linearly_separable_4.png.<br> The central premise of Logistic Regression is the assumption that your input space can be separated into two nice regions, one for each class, by a linear boundary  between the blue dots and the red dots.This dividing plane is called a linear discriminant.","441e0e39":"Kaggle Score shows that TF-IDF performs better than CountVectorizer so Counter Vectorizer overfitted on the train data\n\nKaggle Score:\n\n    - TF-IDF 6.96197\n    - CountVec 7.81432\nNow TF-IDF is used as vectorizer for the project. Find best parameters for TF-IDF and also optimize the Logistic Regression models. The goal is to minimize the cost function while optimazing the objective function. So to do this we choose best models and best parameters for estimators. Let's use GridSearch To find the best parameters.","e73ddd44":"#### **Highlights on steps performed above**<br>\n\nSo far two tasks were performed , **Exploratory Data Analysis** which essentially helps to understand the data. From the EDA processes, It was found that data was not equally distributed amongst the classes, and class labels their distribution is not balanced, that was handled in the modeling section. Second task was **Data Pre-processing**, the goal was to process data based on the intuition gained from EDA activities. Most importantly there is no formula on how to clean data preparing it for Machine learning. This step was revised during modeling, it's because it sets baseline for model performance.\n\nIn the following section various Machine learning algorithms were implimented on the data to predict personality types. Types like \"INTJ\" were converted to binary classes **mind**|**energy**|**nature**|**tactics** as individual columns. This made the problem multi-label classification task. Classifying each user post as either `introvert` or `extrovert` for Mind class and `sensing` or `intuition` for Energy etc... was not easy with classical modeling algorithms.\n\nCleaned text data was to build models. But this data was in text format so it needed transformation. Suggested by scikit learn pipelines were used to transform data at the same time fitting models to avoid data leakage.","9674403a":"## Distributions of Personality Types\nWe look at the distributions of the 4 different categorical labels (mind, energy, nature and tactics).","b4f68b21":"Here we train LinearSVC classifier without changing default parameters. We plot the confussion matrix to check precision accuracy.","93b65006":"We now add the 4 new columns and get a preview of how our DataFrame looks like.","72ca8e2c":"## Serializing Train and Test Data\nAt this point we can now save or serialise our dataset by saving the DataFrame at its current state. We create cleaned files ready to be used for modelling in future.","5b0b7eb9":"**Removing Punctuations**<br>\nThe next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text data. Removing all instances of it will help us reduce the size of the training data. All the punctuation, including \u2018#\u2019 and \u2018@\u2019, will be removed from the training and testing data.","d51ae502":"### Logistic Regression\n\n**How it works**<br>\nThis algorithm is used for classification tasks to predict categorical data using probabilities, it takes values between 0 and 1, unlike Linear Regression where you predict continuous real values that take any values between -inf and +inf. Logistic Regression performs better with binary classes.\n\nSince Logistic Regression predicts one binary class, for this task there are multiple classes. Hence used One-Vs-Rest Classifiers, the strategy consists of fitting one classifier per class. This strategy is efficient and interpretable. It is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.\nSource: [Scikit Learn](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html)","c465f66c":"## Text Encoding\nAll python code is in UTI-8. It is ideal that data is in this format as well so that we can do a thorough cleaning of the data.","b7bbee67":"# Introduction\n\nDoes what you say on social platforms define who you are? Machine learning techniques can be used to classify personalities based on what people post. This notebook seeks to achieve this classification using  the MBTI data provided on kaggle.\n\nThe Myers\u2013Briggs Type Indicator (MBTI) is a psychological classification about humans experiences using four principal psychological functions, sensation, intuition, feeling, and thinking, constructed by Katharine Cook Briggs and Isabel Briggs Myers.\n\n## Problem statement\nThe objective of this project is to build and train a model or find the best model capable of predicting a person's MBTI label using what they post in online platforms such as twitter, youtube and tumblr. Natural Language Processing is used to transform the data into a machine learning format. Exploratory Data Analysis,encoding, cleaning noise in the data, stemming, lemmatisation, removing stop words and standardizing the data forms part of the preprocessing. This data will then be used to train a classifier capable of assigning 16 distinct MBTI labels to a person's online forum post.\n\nEach MBTI personality type consists of four binary variables, they are: \n\n- Mind: Introverted (I) or Extraverted (E):<br>\n    Describes diffrent attitudes people use to direct their energy.\n- Energy: Sensing (S) or Intuitive (N):<br>\n    Describes poeple's method of processing information.\n- Nature: Feeling (F) or Thinking (T):<br>\n    Describes people's method for making decisions.\n- Tactics: Perceiving (P) or Judging (J):<br>\n    Describes poeple's method orientation to the outside world and the behaviours one exhibits.\n## Data source\n- Train: The dataset was sourced from Kaggle (https:\/\/www.kaggle.com\/c\/edsa-mbti\/data)\nThe training data consists of 6505 rows, with two columns (type and posts). The type column represents the user's personality type and the posts columns represents a user's posts from social media platforms and includes URLs, emojis and pictures. Each row represents one user from the Personality Cafe website forums (http:\/\/personalitycafe.com\/forum\/).\nThe training data for this competition consists of 6505 rows, with two columns.\n\n- Test: There are 2169 testing observations and the testing data only consists of the id column and the posts column.\n","96ddddea":"**Removal of Stop Words**<br>\nWe use Corpus class stopwords that has a list of frequently occurring words including unnecesary words like personality types to help us remove those words from the posts.","6af14e1d":"# Preparing Environment and Uploading Data\nWe will begin by importing python libraries that will enable us to load our data, make predictions and make statistical calculations which will best describe the performance of our models on the training data.","adc1271b":"Analysis from the pie charts below:<br>\n- Mind - Introverts type more words than extroverts \n- Energy - More people tend to be More intuitive than sensitive\n- Nature - More people tend to write more about how they feel than how they think,the proportion is not as big.\n- Tactics - More people tend to be more judging than perceiving\n\nThe Introversion-Extroversion and particularly the Intuition-Sensing proportions are quite imbalanced. The Thinkin-Feeling and to a lesser extent the Judging-Perceiving are relatively more balanced.","a5edb827":"All these pre-processing steps helps us in reducing our vocabulary so that the features produced in the end are more effective.<br> **Tokenize**<br>\nThe last step we need to make is to tokenise our data by splitting text into a list of words.","c108ba8c":"### LinearSVC Classifier","3d339447":"The model had a higher accuracy on the train data miss classifying few user posts, this shows the model overfits train dataset.\n\nTest model on kaggle  and check performance\n","aa57043f":"**Removing Digits**<br>\nDigits will not add any value to our analysis so we remove them as well.","08417de7":"# Building Machine Learning Models\nWe are now done with pre-processing the data and ready to train models for prediction.","31747e93":"Use TF-IDF vectorizer to transform the data and evaluate the models performance. This type of Word vectorize is better than Count Vector in a sense that it tries to determine words that are important and that explains a particular document. Word that appear in all document are given low importance so they are not significant and are remove from the document terms.\n\nSo below fit a logistic regression with Tfidf vectorizer, then compare kaggle score with Count Vectorizer. Assuming it should perform better than Count Vectorizer.","1a0d8aa8":"We now look at the distributions of the 4 personality categories.","4f287adb":"* On the confussion matrix shows that this classifier for three classes could not predict correct any of the labels.<br>\nThis classifier commits **type |** and **type || errors**"}}