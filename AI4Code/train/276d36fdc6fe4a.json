{"cell_type":{"e03b0311":"code","712ebf85":"code","ebb03332":"code","4e3b503f":"code","1ca73aa6":"code","2d3e29ee":"code","8b4aec4c":"code","f02b3915":"code","a7fb829c":"code","b250ebd4":"code","f9943538":"code","329dbc6b":"code","1dc63dd5":"code","d13c9bd7":"code","7fda97db":"code","628d0231":"code","a1963deb":"code","bb25e816":"code","4ceabd84":"code","5e2dbadd":"code","1f2d17df":"code","d3700b21":"code","f7c908a8":"code","ad66335f":"code","2258c7bc":"code","d89c0fd7":"code","82b42d68":"code","77f6ed6b":"code","37157c43":"code","f562c4aa":"code","f1752cbd":"code","c87a7f72":"code","85020487":"code","da8669af":"code","26e47c3b":"code","b4b48faa":"code","7172d6d2":"code","aa1a0284":"code","689d2c97":"code","9740796a":"code","4ea7273a":"code","c0767ddb":"code","b5f483f1":"code","c5a31a31":"code","3bb42ac7":"code","63bbf889":"code","9dac3db5":"code","c622c8ec":"code","61e9c731":"code","89445f5d":"code","afa53e04":"code","e8a28caf":"code","bef61573":"code","30a4f9c7":"code","961e83b6":"code","2d8bf0d0":"code","79e18829":"code","b0f1af18":"code","fd1024ff":"code","bf77c375":"code","2c16466d":"code","43e5145d":"code","e50f64b3":"code","bb6245e6":"code","229edd86":"code","ac55707d":"code","aec3f34c":"code","268d0fe8":"code","d505faf0":"code","ae6d2363":"code","ae64368e":"code","99ccd62c":"code","e105b257":"code","112f2ba7":"code","7db14ebe":"code","1782a521":"code","7fff335f":"code","575a1cb8":"code","3268b000":"code","4941d9e2":"code","6e051ba6":"code","cad986e6":"code","d8236e15":"code","df29426e":"code","6bcd928d":"code","539ce39f":"code","d72e4a6d":"code","5d41593a":"code","3ca4b30e":"code","cfab76a8":"code","67b000ef":"code","d4bfe686":"code","4fc8e6c9":"code","c24fdd48":"code","c09bb9bb":"code","56b0852a":"code","80a4d668":"code","7ac59693":"code","27949fe2":"code","6415f346":"code","fbed77b6":"code","b544b2ad":"code","053c6b65":"code","3378f2b2":"code","d747dc19":"code","6a5c22f8":"code","717a879e":"code","330f30fe":"code","3c94fadc":"code","b06bcf8d":"code","cf5aa216":"code","a4de9b6f":"code","efcbab2e":"code","2bf76432":"code","6c36f102":"code","f35e099c":"code","84af475f":"code","0aa94394":"code","6d46e91c":"code","9808bc0c":"code","9083347a":"code","bad98e8a":"code","6bc5acc2":"code","1354f9fd":"code","5bc1e38f":"code","5baea699":"code","2e1747ee":"code","ca4ec3a7":"code","58ab2133":"code","e3456b65":"code","79bdef2d":"code","4284bea1":"code","d3cbb559":"code","489d3b42":"code","08a4b985":"code","d638761b":"code","038298ce":"code","4fabf027":"code","22e9ffcd":"code","e60df2f2":"code","4e929454":"code","b1e58263":"code","8ba1ecbf":"code","a525f818":"code","6a7e07a9":"code","e660fa59":"code","ac825d71":"code","5f59daf3":"code","64861899":"code","e89a4dec":"code","ab6a24f9":"code","5ca924cc":"code","1a21032e":"code","5de835c0":"code","632cdb9d":"markdown","b0351c28":"markdown","7484ee28":"markdown","5569d4e5":"markdown","259c73dc":"markdown","9ee1a861":"markdown","5d0760cf":"markdown","be13e82d":"markdown","354313be":"markdown","0dc1cdbf":"markdown","d2a97531":"markdown","6bbda47e":"markdown","e0e2b8cd":"markdown","8ba8b787":"markdown","16e56859":"markdown","d44de627":"markdown","640c1b82":"markdown","6a26d26b":"markdown","a7a78d6e":"markdown","034019a0":"markdown","86a8030c":"markdown","e8aab656":"markdown","f3e2984b":"markdown","cc5dd8c6":"markdown","8b31510d":"markdown","84f589a5":"markdown","725908fa":"markdown","257cf310":"markdown","fcd76392":"markdown","4cc01629":"markdown","f8303731":"markdown","7721b3b8":"markdown","3f0fddde":"markdown","9ed49f63":"markdown","18266d05":"markdown","08f4f973":"markdown","7f21427f":"markdown","b1893948":"markdown","2e15fe90":"markdown","10d32f85":"markdown","f4be866e":"markdown","e330b0b7":"markdown","9a24e44b":"markdown","bb4a1f1f":"markdown","692800ff":"markdown","961f4bf3":"markdown","1c3d906b":"markdown","24bb7a35":"markdown","5d568fec":"markdown","16249acd":"markdown","b1d3db74":"markdown","f7f74e53":"markdown","1a57ea78":"markdown","c57c0b76":"markdown","fd1771e4":"markdown","148a52ee":"markdown","f3247a4d":"markdown","b4da62d0":"markdown","4f52d039":"markdown","ae4a9190":"markdown","0ab7655f":"markdown","16b2a5ba":"markdown","9163e881":"markdown","2289653b":"markdown","f39406c2":"markdown","692c310e":"markdown","6f034a0d":"markdown","cb77c5fa":"markdown","e3d695fc":"markdown","f48cd9c7":"markdown","315b874a":"markdown","bcc8b477":"markdown","a25da13c":"markdown","76fe9462":"markdown","c61ca5e6":"markdown","a468ffe3":"markdown","a6ca59b4":"markdown","3927412c":"markdown","e8706f31":"markdown","c07012f3":"markdown","d47b4a3a":"markdown","3e674389":"markdown","52bf82ae":"markdown","dd6a5bdc":"markdown","89823b8c":"markdown","19277c08":"markdown","a4f90ee4":"markdown","4b052ef2":"markdown","1edfc1cd":"markdown","668c2748":"markdown","2b4b69d0":"markdown","210dcf24":"markdown","85cd4711":"markdown","d065c8f3":"markdown","d19b3493":"markdown","fd0a9c16":"markdown","06e99993":"markdown","f0e3b2c0":"markdown","bacf60f5":"markdown","8716edea":"markdown","4f28df43":"markdown","9ef5f051":"markdown","a7159988":"markdown","1a8d56d7":"markdown","dba689a3":"markdown","ecef3e0c":"markdown","abee67e3":"markdown","8a163e22":"markdown","0507212a":"markdown","8518c1b5":"markdown","c4e30766":"markdown","6a2e436e":"markdown","348b5aa9":"markdown","21534c94":"markdown","b1645107":"markdown","8c3ab7c8":"markdown","e2854ada":"markdown","2f4e9945":"markdown","3917cd35":"markdown","5b7f9d35":"markdown","bf644513":"markdown","fe119a52":"markdown","359f91b8":"markdown","a4e6faae":"markdown","ea54d81c":"markdown","80671888":"markdown","e4c930af":"markdown","8f9ef740":"markdown","8a9adb9e":"markdown","478a5d1a":"markdown","438956f1":"markdown","4cb4c40d":"markdown","56dae70d":"markdown","8cf11323":"markdown","62d0ff1c":"markdown","2510e5e9":"markdown","e337ee42":"markdown","c6e5fb50":"markdown","2705b350":"markdown","326a3b08":"markdown","c6350f7c":"markdown","4eeb48d8":"markdown","8432e4ef":"markdown","32b3efdb":"markdown","d345e77f":"markdown","f3caef64":"markdown","d55689b6":"markdown","2fdf4a9a":"markdown","4592c31f":"markdown","d8105b4c":"markdown","9956d125":"markdown","3ffaf81d":"markdown","584b0853":"markdown","ce8e8e47":"markdown","a3ba7940":"markdown","d43b98a4":"markdown","c98c00cf":"markdown","c4085a10":"markdown","e6f38c0a":"markdown","3d851908":"markdown","8b4cf410":"markdown","2f9de033":"markdown","da10d0aa":"markdown","0988a5e0":"markdown","daadb78a":"markdown","5604a3c9":"markdown","46e20cce":"markdown","d48d2104":"markdown","07e9cef6":"markdown","eb653892":"markdown","336e1347":"markdown","edaefa31":"markdown","fcdf4262":"markdown","29896283":"markdown","85fdead9":"markdown","111512ba":"markdown","b5173d12":"markdown","7daec601":"markdown","e0880fd2":"markdown","6d169051":"markdown","7d4cf87a":"markdown","d78634fc":"markdown","4caac9db":"markdown","526fb8b1":"markdown","036a6ed2":"markdown","010c5dbc":"markdown","7104aeb8":"markdown","e1a1d339":"markdown","8fce3c5e":"markdown","5b289ab7":"markdown","980b264c":"markdown","9b1272fe":"markdown","2cb3e5ea":"markdown","b956f105":"markdown","861d92ed":"markdown","af196c43":"markdown","08ce9908":"markdown","d0de4dea":"markdown","2ff11bb9":"markdown","cff80d17":"markdown","f412536f":"markdown","9a149cb9":"markdown","40d5922f":"markdown","1cb92593":"markdown","4a04b407":"markdown","1056eee3":"markdown","36f3308c":"markdown","f5808937":"markdown","3e55c29e":"markdown","df8c360d":"markdown","c7354546":"markdown","d764d5d2":"markdown","d366f3a3":"markdown","8b706d32":"markdown","59514df7":"markdown","5b8e6c6e":"markdown","3c2439bf":"markdown","9865192d":"markdown","8a4a9d9e":"markdown","fcdb2bfe":"markdown"},"source":{"e03b0311":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","712ebf85":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","ebb03332":"train_original = train.copy()\ntest_original = test.copy()","4e3b503f":"train.columns","1ca73aa6":"test.columns","2d3e29ee":"train.dtypes","8b4aec4c":"print('Training data shape: ', train.shape)\ntrain.head()","f02b3915":"print('Test data shape: ', test.shape)\ntest.head()","a7fb829c":"#train[\"Loan_Status\"].size\ntrain[\"Loan_Status\"].count()","b250ebd4":"train[\"Loan_Status\"].value_counts()","f9943538":"# Normalize can be set to True to print proportions instead of number \ntrain[\"Loan_Status\"].value_counts(normalize=True)*100","329dbc6b":"train[\"Loan_Status\"].value_counts(normalize=True).plot.bar(title = 'Loan_Status')","1dc63dd5":"train[\"Gender\"].count()","d13c9bd7":"train[\"Gender\"].value_counts()","7fda97db":"train['Gender'].value_counts(normalize=True)*100","628d0231":"train['Gender'].value_counts(normalize=True).plot.bar(title= 'Gender')","a1963deb":"train[\"Married\"].count()","bb25e816":"train[\"Married\"].value_counts()","4ceabd84":"train['Married'].value_counts(normalize=True)*100","5e2dbadd":"train['Married'].value_counts(normalize=True).plot.bar(title= 'Married')","1f2d17df":"train[\"Self_Employed\"].count()","d3700b21":"train[\"Self_Employed\"].value_counts()","f7c908a8":"train['Self_Employed'].value_counts(normalize=True)*100","ad66335f":"train['Self_Employed'].value_counts(normalize=True).plot.bar(title='Self_Employed')","2258c7bc":"train[\"Credit_History\"].count()","d89c0fd7":"train[\"Credit_History\"].value_counts()","82b42d68":"train['Credit_History'].value_counts(normalize=True)*100","77f6ed6b":"train['Credit_History'].value_counts(normalize=True).plot.bar(title='Credit_History')\n","37157c43":"train['Dependents'].count()","f562c4aa":"train[\"Dependents\"].value_counts()","f1752cbd":"train['Dependents'].value_counts(normalize=True)*100","c87a7f72":"train['Dependents'].value_counts(normalize=True).plot.bar(title=\"Dependents\")","85020487":"train[\"Education\"].count()","da8669af":"train[\"Education\"].value_counts()","26e47c3b":"train[\"Education\"].value_counts(normalize=True)*100","b4b48faa":"train[\"Education\"].value_counts(normalize=True).plot.bar(title = \"Education\")","7172d6d2":"train[\"Property_Area\"].count()","aa1a0284":"train[\"Property_Area\"].value_counts()","689d2c97":"train[\"Property_Area\"].value_counts(normalize=True)*100","9740796a":"train[\"Property_Area\"].value_counts(normalize=True).plot.bar(title=\"Property_Area\")","4ea7273a":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train[\"ApplicantIncome\"]);\n\nplt.subplot(122)\ntrain[\"ApplicantIncome\"].plot.box(figsize=(16,5))\nplt.show()","c0767ddb":"train.boxplot(column='ApplicantIncome',by=\"Education\" )\nplt.suptitle(\" \")\nplt.show()","b5f483f1":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train[\"CoapplicantIncome\"]);\n\nplt.subplot(122)\ntrain[\"CoapplicantIncome\"].plot.box(figsize=(16,5))\nplt.show()","c5a31a31":"plt.figure(1)\nplt.subplot(121)\ndf=train.dropna()\nsns.distplot(df['LoanAmount']);\n\nplt.subplot(122)\ntrain['LoanAmount'].plot.box(figsize=(16,5))\n\nplt.show()","3bb42ac7":"plt.figure(1)\nplt.subplot(121)\ndf = train.dropna()\nsns.distplot(df[\"Loan_Amount_Term\"]);\n\nplt.subplot(122)\ndf[\"Loan_Amount_Term\"].plot.box(figsize=(16,5))\nplt.show()","63bbf889":"print(pd.crosstab(train[\"Gender\"],train[\"Loan_Status\"]))\nGender = pd.crosstab(train[\"Gender\"],train[\"Loan_Status\"])\nGender.div(Gender.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Percentage\")\nplt.show()","9dac3db5":"print(pd.crosstab(train[\"Married\"],train[\"Loan_Status\"]))\nMarried=pd.crosstab(train[\"Married\"],train[\"Loan_Status\"])\nMarried.div(Married.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Married\")\nplt.ylabel(\"Percentage\")\nplt.show()","c622c8ec":"print(pd.crosstab(train['Dependents'],train[\"Loan_Status\"]))\nDependents = pd.crosstab(train['Dependents'],train[\"Loan_Status\"])\nDependents.div(Dependents.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Dependents\")\nplt.ylabel(\"Percentage\")\nplt.show()","61e9c731":"print(pd.crosstab(train[\"Education\"],train[\"Loan_Status\"]))\nEducation = pd.crosstab(train[\"Education\"],train[\"Loan_Status\"])\nEducation.div(Education.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Education\")\nplt.ylabel(\"Percentage\")\nplt.show()","89445f5d":"print(pd.crosstab(train[\"Self_Employed\"],train[\"Loan_Status\"]))\nSelfEmployed = pd.crosstab(train[\"Self_Employed\"],train[\"Loan_Status\"])\nSelfEmployed.div(SelfEmployed.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Self_Employed\")\nplt.ylabel(\"Percentage\")\nplt.show()","afa53e04":"print(pd.crosstab(train[\"Credit_History\"],train[\"Loan_Status\"]))\nCreditHistory = pd.crosstab(train[\"Credit_History\"],train[\"Loan_Status\"])\nCreditHistory.div(CreditHistory.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Credit_History\")\nplt.ylabel(\"Percentage\")\nplt.show()","e8a28caf":"print(pd.crosstab(train[\"Property_Area\"],train[\"Loan_Status\"]))\nPropertyArea = pd.crosstab(train[\"Property_Area\"],train[\"Loan_Status\"])\nPropertyArea.div(PropertyArea.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"Property_Area\")\nplt.ylabel(\"Loan_Status\")\nplt.show()","bef61573":"train.groupby(\"Loan_Status\")['ApplicantIncome'].mean().plot.bar()","30a4f9c7":"bins=[0,2500,4000,6000,81000]\ngroup=['Low','Average','High', 'Very high']\ntrain['Income_bin']=pd.cut(df['ApplicantIncome'],bins,labels=group)\n","961e83b6":"print(pd.crosstab(train[\"Income_bin\"],train[\"Loan_Status\"]))\nIncome_bin = pd.crosstab(train[\"Income_bin\"],train[\"Loan_Status\"])\nIncome_bin.div(Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\nplt.xlabel(\"ApplicantIncome\")\nplt.ylabel(\"Percentage\")\nplt.show()","2d8bf0d0":"bins=[0,1000,3000,42000]\ngroup =['Low','Average','High']\ntrain['CoapplicantIncome_bin']=pd.cut(df[\"CoapplicantIncome\"],bins,labels=group)","79e18829":"print(pd.crosstab(train[\"CoapplicantIncome_bin\"],train[\"Loan_Status\"]))\nCoapplicantIncome_Bin = pd.crosstab(train[\"CoapplicantIncome_bin\"],train[\"Loan_Status\"])\nCoapplicantIncome_Bin.div(CoapplicantIncome_Bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))\nplt.xlabel(\"CoapplicantIncome\")\nplt.ylabel(\"Percentage\")\nplt.show()","b0f1af18":"train[\"TotalIncome\"]=train[\"ApplicantIncome\"]+train[\"CoapplicantIncome\"]","fd1024ff":"bins =[0,2500,4000,6000,81000]\ngroup=['Low','Average','High','Very High']\ntrain[\"TotalIncome_bin\"]=pd.cut(train[\"TotalIncome\"],bins,labels=group)","bf77c375":"print(pd.crosstab(train[\"TotalIncome_bin\"],train[\"Loan_Status\"]))\nTotalIncome = pd.crosstab(train[\"TotalIncome_bin\"],train[\"Loan_Status\"])\nTotalIncome.div(TotalIncome.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(2,2))\nplt.xlabel(\"TotalIncome\")\nplt.ylabel(\"Percentage\")\nplt.show()","2c16466d":"bins = [0,100,200,700]\ngroup=['Low','Average','High']\ntrain[\"LoanAmount_bin\"]=pd.cut(df[\"LoanAmount\"],bins,labels=group)","43e5145d":"print(pd.crosstab(train[\"LoanAmount_bin\"],train[\"Loan_Status\"]))\nLoanAmount=pd.crosstab(train[\"LoanAmount_bin\"],train[\"Loan_Status\"])\nLoanAmount.div(LoanAmount.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))\nplt.xlabel(\"LoanAmount\")\nplt.ylabel(\"Percentage\")\nplt.show()","e50f64b3":"train=train.drop([\"Income_bin\",\"CoapplicantIncome_bin\",\"LoanAmount_bin\",\"TotalIncome\",\"TotalIncome_bin\"],axis=1)","bb6245e6":"#train['Dependents'].replace(('0', '1', '2', '3+'), (0, 1, 2, 3),inplace=True)\n#test['Dependents'].replace(('0', '1', '2', '3+'), (0, 1, 2, 3),inplace=True)\ntrain['Dependents'].replace('3+',3,inplace=True)\ntest['Dependents'].replace('3+',3,inplace=True)\ntrain['Loan_Status'].replace('N', 0,inplace=True)\ntrain['Loan_Status'].replace('Y', 1,inplace=True)","229edd86":"matrix = train.corr()\nf, ax = plt.subplots(figsize=(10, 12))\nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\",annot=True);","ac55707d":"train.isnull().sum()","aec3f34c":"train[\"Gender\"].fillna(train[\"Gender\"].mode()[0],inplace=True)\ntrain[\"Married\"].fillna(train[\"Married\"].mode()[0],inplace=True)\ntrain['Dependents'].fillna(train[\"Dependents\"].mode()[0],inplace=True)\ntrain[\"Self_Employed\"].fillna(train[\"Self_Employed\"].mode()[0],inplace=True)\ntrain[\"Credit_History\"].fillna(train[\"Credit_History\"].mode()[0],inplace=True)","268d0fe8":"train[\"Loan_Amount_Term\"].value_counts()","d505faf0":"train[\"Loan_Amount_Term\"].fillna(train[\"Loan_Amount_Term\"].mode()[0],inplace=True)","ae6d2363":"train[\"Loan_Amount_Term\"].value_counts()","ae64368e":"train[\"LoanAmount\"].fillna(train[\"LoanAmount\"].median(),inplace=True)","99ccd62c":"train.isnull().sum()","e105b257":"test.isnull().sum()","112f2ba7":"test[\"Gender\"].fillna(test[\"Gender\"].mode()[0],inplace=True)\ntest['Dependents'].fillna(test[\"Dependents\"].mode()[0],inplace=True)\ntest[\"Self_Employed\"].fillna(test[\"Self_Employed\"].mode()[0],inplace=True)\ntest[\"Loan_Amount_Term\"].fillna(test[\"Loan_Amount_Term\"].mode()[0],inplace=True)\ntest[\"Credit_History\"].fillna(test[\"Credit_History\"].mode()[0],inplace=True)\ntest[\"LoanAmount\"].fillna(test[\"LoanAmount\"].median(),inplace=True)","7db14ebe":"test.isnull().sum()","1782a521":"sns.distplot(train[\"LoanAmount\"]);","7fff335f":"train['LoanAmount'].hist(bins=20)","575a1cb8":"train['LoanAmount_log'] = np.log(train['LoanAmount'])\ntrain['LoanAmount_log'].hist(bins=20)","3268b000":"sns.distplot(train[\"LoanAmount_log\"])","4941d9e2":"test[\"LoanAmount_log\"]=np.log(train[\"LoanAmount\"])\ntest['LoanAmount_log'].hist(bins=20)","6e051ba6":"sns.distplot(test[\"LoanAmount_log\"])","cad986e6":"train[\"TotalIncome\"]=train[\"ApplicantIncome\"]+train[\"CoapplicantIncome\"]","d8236e15":"train[[\"TotalIncome\"]].head()","df29426e":"test[\"TotalIncome\"]=test[\"ApplicantIncome\"]+test[\"CoapplicantIncome\"]","6bcd928d":"test[[\"TotalIncome\"]].head()","539ce39f":"sns.distplot(train[\"TotalIncome\"])","d72e4a6d":"train[\"TotalIncome_log\"]=np.log(train[\"TotalIncome\"])\nsns.distplot(train[\"TotalIncome_log\"])","5d41593a":"sns.distplot(test[\"TotalIncome\"])","3ca4b30e":"test[\"TotalIncome_log\"] = np.log(train[\"TotalIncome\"])\nsns.distplot(test[\"TotalIncome_log\"])","cfab76a8":"train[\"EMI\"]=train[\"LoanAmount\"]\/train[\"Loan_Amount_Term\"]\ntest[\"EMI\"]=test[\"LoanAmount\"]\/test[\"Loan_Amount_Term\"]","67b000ef":"train[[\"EMI\"]].head()","d4bfe686":"test[[\"EMI\"]].head()","4fc8e6c9":"sns.distplot(train[\"EMI\"])","c24fdd48":"sns.distplot(test[\"EMI\"])","c09bb9bb":"train[\"Balance_Income\"] = train[\"TotalIncome\"]-train[\"EMI\"]*1000 # To make the units equal we multiply with 1000\ntest[\"Balance_Income\"] = test[\"TotalIncome\"]-test[\"EMI\"]","56b0852a":"train[[\"Balance_Income\"]].head()","80a4d668":"test[[\"Balance_Income\"]].head()","7ac59693":"train=train.drop([\"ApplicantIncome\",\"CoapplicantIncome\",\"LoanAmount\",\"Loan_Amount_Term\"],axis=1)","27949fe2":"train.head()","6415f346":"test = test.drop([\"ApplicantIncome\",\"CoapplicantIncome\",\"LoanAmount\",\"Loan_Amount_Term\"],axis=1)","fbed77b6":"test.head()","b544b2ad":"train=train.drop(\"Loan_ID\",axis=1)\ntest=test.drop(\"Loan_ID\",axis=1)","053c6b65":"train.head(3)","3378f2b2":"test.head(3)","d747dc19":"X=train.drop(\"Loan_Status\",1)","6a5c22f8":"X.head(2)","717a879e":"y=train[[\"Loan_Status\"]]","330f30fe":"y.head(2)","3c94fadc":"X = pd.get_dummies(X)","b06bcf8d":"X.head(3)","cf5aa216":"train=pd.get_dummies(train)\ntest=pd.get_dummies(test)","a4de9b6f":"train.head(3)","efcbab2e":"test.head(3)","2bf76432":"from sklearn.model_selection import train_test_split","6c36f102":"x_train,x_cv,y_train,y_cv=train_test_split(X,y,test_size=0.3,random_state=1)","f35e099c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","84af475f":"logistic_model = LogisticRegression(random_state=1)","0aa94394":"logistic_model.fit(x_train,y_train)","6d46e91c":"pred_cv_logistic=logistic_model.predict(x_cv)","9808bc0c":"score_logistic =accuracy_score(pred_cv_logistic,y_cv)*100 ","9083347a":"score_logistic","bad98e8a":"pred_test_logistic = logistic_model.predict(test)","6bc5acc2":"from sklearn.tree import DecisionTreeClassifier","1354f9fd":"tree_model = DecisionTreeClassifier(random_state=1)\n","5bc1e38f":"tree_model.fit(x_train,y_train)","5baea699":"pred_cv_tree=tree_model.predict(x_cv)","2e1747ee":"score_tree =accuracy_score(pred_cv_tree,y_cv)*100 ","ca4ec3a7":"score_tree","58ab2133":"pred_test_tree = tree_model.predict(test)","e3456b65":"from sklearn.ensemble import RandomForestClassifier","79bdef2d":"forest_model = RandomForestClassifier(random_state=1,max_depth=10,n_estimators=50)","4284bea1":"forest_model.fit(x_train,y_train)","d3cbb559":"pred_cv_forest=forest_model.predict(x_cv)","489d3b42":"score_forest = accuracy_score(pred_cv_forest,y_cv)*100","08a4b985":"score_forest","d638761b":"pred_test_forest=forest_model.predict(test)","038298ce":"from sklearn.model_selection import GridSearchCV","4fabf027":"paramgrid = {'max_depth': list(range(1,20,2)),'n_estimators':list(range(1,200,20))}","22e9ffcd":"grid_search = GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)","e60df2f2":"grid_search.fit(x_train,y_train)","4e929454":"grid_search.best_estimator_","b1e58263":"grid_forest_model = RandomForestClassifier(random_state=1,max_depth=3,n_estimators=101)","8ba1ecbf":"grid_forest_model.fit(x_train,y_train)","a525f818":"pred_grid_forest = grid_forest_model.predict(x_cv)","6a7e07a9":"score_grid_forest = accuracy_score(pred_grid_forest,y_cv)*100","e660fa59":"score_grid_forest","ac825d71":"pred_grid_forest_test = grid_forest_model.predict(test)","5f59daf3":"from xgboost import XGBClassifier","64861899":"xgb_model = XGBClassifier(n_estimators=50,max_depth=4)","e89a4dec":"xgb_model.fit(x_train,y_train)","ab6a24f9":"pred_xgb=xgb_model.predict(x_cv)","5ca924cc":"score_xgb = accuracy_score(pred_xgb,y_cv)*100","1a21032e":"score_xgb","5de835c0":"importances = pd.Series(forest_model.feature_importances_,index=X.columns)\nimportances.plot(kind='barh', figsize=(12,8))","632cdb9d":"Let us now drop the variables which we used to create these new features. Reason for doing this is, the correlation between those old features and these new features will be very high and logistic regression assumes that the variables are not highly correlated. We also wants to remove the noise from the dataset, so removing correlated features will help in reducing the noise too.","b0351c28":"# Feature Engineering","7484ee28":"# Relation between \"Loan_Status\" and \"Property_Area\"","5569d4e5":"# Logistic Regression","259c73dc":"Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan. Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.","9ee1a861":"i)Logistic Regression\n\nii)Decision Tree\n\niii)Random Forest\n\niv)Random Forest with Grid Search\n\nv)XGBClassifier","5d0760cf":"People who are Graduated : 480\n\nPeople who are not Graduated : 134","be13e82d":"# Analysis on \"Gender\" variable :","354313be":"Whose Loan Amount was low and Loan was approved : 86\n\nWhose Loan Amount was low and Loan was not approved : 38\n\nWhose Loan Amount was Average and Loan was approved : 207\n\nWhose Loan Amount was Average and Loan was not approved : 83\n\nWhose Loan Amount was High and Loan was approved : 39\n\nWhose Loan Amount was High and Loan was not approved : 27\n\nIt can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less.","0dc1cdbf":"Have a look of train dataset \"EMI\"","d2a97531":"Have a look of test dataset \"Balance Income\"","6bbda47e":"Let\u2019s make predictions for the test dataset.","e0e2b8cd":"We will treat the missing values in all the features one by one.\n\nWe can consider these methods to fill the missing values:\n\nFor numerical variables: imputation using mean or median\n    \nFor categorical variables: imputation using mode","8ba8b787":"We see a lot of outliers in this variable and the distribution is fairly normal. We will treat the outliers in later sections.","16e56859":"After drop train dataset will look :","d44de627":"Let us find the feature importance now, i.e. which features are most important for this problem. We will use feature_importances_attribute of sklearn to do it.","640c1b82":"There are missing values in Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term and Credit_History features.","6a26d26b":"# Decision Tree\n\n","a7a78d6e":"We have 12 independent variables and 1 target variable, i.e. Loan_Status in the train dataset. Let\u2019s also have a look at the columns of test dataset.","034019a0":"# Categorical Independent Variable vs Target Variable","86a8030c":"our predictions are almost 78% accurate, i.e. we have identified 78% of the loan status correctly for our Random Forest model.","e8aab656":"Find out the optimized value","f3e2984b":"It can be inferred that Applicant income does not affect the chances of loan approval which contradicts our hypothesis in which we assumed that if the applicant income is high the chances of loan approval will also be high.","cc5dd8c6":"Among 614 Loan_Status :\nAccepted : 422 \nRejected : 192","8b31510d":"Let\u2019s list out feature-wise count of missing values.","84f589a5":"In this section, we will look at the structure of the train and test datasets. Firstly, we will check the features present in our data and then we will look at their data types.","725908fa":"# Objective of the Notebook","257cf310":"Let\u2019s prepare the data for feeding into the models.","fcd76392":"# About The Data:","4cc01629":"Let\u2019s make predictions for the test dataset.","f8303731":"# Let\u2019s look at the distribution of \"LoanAmount\" variable :","7721b3b8":"Due to these outliers bulk of the data in the loan amount is at the left and the right tail is longer. This is called right skewness.\n\nOne way to remove the skewness is by doing the log transformation. As we take the log transformation, it does not affect the smaller values much, but reduces the larger values. \n\nSo, we get a distribution similar to normal distribution.","3f0fddde":"Let's create Balance Income feature now and check its distribution.","9ed49f63":"# Analysis on \"Education\" variable :","18266d05":"We can see that there are a higher number of graduates with very high incomes, which are appearing to be the outliers.","08f4f973":"Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.","7f21427f":"We can find out that 'Credit_History','Balance Income' feature are most important. So, feature engineering helped us in predicting our target variable.","b1893948":"Let\u2019s look at the steps that we will follow in this notebook.\n\n1.Problem Statement\n\n2.Hypothesis Generation\n\n3.Loading the data\n\n4.Understanding the data\n\n5.Exploratory Data Analysis (EDA)\n\ni)Univariate Analysis\n\nii)Bivariate Analysis\n\n6.Missing value and outlier treatment\n\n7.Feature Engineering\n\n8.Model Building :\n    \ni)Logistic Regression\n\nii) Decision Tree\n\niii) Random Forest\n\niv) Random Forest with Grid Search\n\nv)XGBClassifier","2e15fe90":"# Missing value imputation","10d32f85":"Let\u2019s have a look at the parameters that we are going to use in our model","f4be866e":"It can be seen that in loan amount term variable, the value of 360 is repeating the most.\nSo we will replace the missing values in this variable using the mode of this variable.","e330b0b7":"Size of our \"Self_Employed\" variable is : 582","9a24e44b":"Now let\u2019s try to find a way to fill the missing values in Loan_Amount_Term. \nWe will look at the value count of the Loan amount term variable.","bb4a1f1f":"58% people have 0 dependent\n\n17% people have 1 dependent\n\n17% people have 2 dependent\n\n8% people have 3+ dependent","692800ff":"Sklearn requires the target variable in a separate dataset. So, we will drop our target variable from the train dataset and save it in another dataset.","961f4bf3":"# \"ApplicantIncome\" distribution :","1c3d906b":"Frequency table of a variable will give us the count of each category in that variable.","24bb7a35":"Let's import LogisticRegression and accuracy_score from sklearn and fit the logistic regression model.","5d568fec":"This is a standard supervised classification task.A classification problem where we have to predict whether a loan would be approved or not. In a classification problem, we have to predict discrete values based on a given set of independent variable(s).Classification can be of two types:","16249acd":"# Find the important feature","b1d3db74":"Whose TotalIncome was Low and loan was approved : 10\n\nWhose TotalIncome was Low and loan was not approved : 14\n\nWhose TotalIncome was Aerage and loan was apprvoed : 87\n\nWhose TotalIncome was Average and loan was not approved : 32\n\nWhose TotalIncome was High and loan was approved : 159\n\nWhose TotalIncome was High and loan was not approved : 65\n\nWhose TotalIncome was Very High and loan was approved : 166\n\nWhose TotalIncome was Very High and loan was not approed : 81\n\nWe can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income.","f7f74e53":"This notebook is divided into the below sections:\n\n1.Introduction to the problem.\n\n2.Exploratory Data Analysis (EDA) and PreProcessing.\n\n3.Feature engineering and Model building.","1a57ea78":"Total number of People : 614\n\nPeople from Semiurban area : 233\n\nPeople from Urban area : 202\n\nPeople from Rural area : 179\n    ","c57c0b76":"# Analysis on \"Dependents\" variable :","fd1771e4":"Train file will be used for training the model, i.e. our model will learn from this file. It contains all the independent variables and the target variable.\n\nTest file contains all the independent variables, but not the target variable. We will apply the model to predict the target variable for the test data.","148a52ee":"iii)Final prediction can be a function of all the predictions made by the individual learners.","f3247a4d":"# Introduction to the Notebook","b4da62d0":"Size of our \"Gender\" variable is : 614","4f52d039":"We will try to improve the accuracy by tuning the hyperparameters for this model. We will use grid search to get the optimized values of hyper parameters. Grid-search is a way to select the best of a family of hyper parameters, parametrized by a grid of parameters.","ae4a9190":"Have a look of train dataset \"Balance Income\"","0ab7655f":"# Relation between \"Loan_Status\" and \"Gender\"","16b2a5ba":"We will tune the max_depth and n_estimators parameters. max_depth decides the maximum depth of the tree and n_estimators decides the number of trees that will be used in random forest model.","9163e881":"i)Applicants with high income should have more chances of loan approval.\n\nii)Applicants who have repaid their previous debts should have higher chances of loan approval.\n\niii)Loan approval should also depend on the loan amount. If the loan amount is less, chances of loan approval should be high.\n\niv)Lesser the amount to be paid monthly to repay the loan, higher the chances of loan approval.","2289653b":"Logistic Regression model gives : 79% prediction accuracy\n\nDecision Tree model gives :   71%  prediction accuracy\n\nRandom Forest model gives : 78%  prediction accuracy\n\nRandom Forest with Grid Search model gives :  77%   prediction accuracy\n\nXGBClassifier model gives : 78%     prediction accuracy ","f39406c2":"Welcome to this notebook on Loan Prediction Practice Problem. Below is a brief introduction of this notebook to get you acquainted with what you will be learning.","692c310e":"Total number of people : 582\n\nSelf_Employed : 82\n\nNot_Self_Employed : 500","6f034a0d":"We have similar features in the test dataset as the train dataset except the Loan_Status. We will predict the Loan_Status using the model that we will build using the train data.","cb77c5fa":"Let's import XGBClassifier and fit the model.","e3d695fc":"Let\u2019s make a copy of train and test data so that even if we have to make any changes in these datasets we would not lose the original datasets.","f48cd9c7":"# Read Train and Test Data","315b874a":"The loan of 422(around 69%) people out of 614 was approved.","bcc8b477":"This is a very important stage in any data science\/machine learning pipeline. It involves understanding the problem in detail by brainstorming as many factors as possible which can impact the outcome. It is done by understanding the problem statement thoroughly and before looking at the data.","a25da13c":"# Random Forest","76fe9462":"# Relation between \"Loan_Status\" and \"Dependents\"","c61ca5e6":"Around 84% applicants have repaid their debts.","a468ffe3":"Fit the grid search model","a6ca59b4":"Let's import Random Forest Classifier","3927412c":"After exploring all the variables in our data, we can now impute the missing values and treat the outliers because missing data and outliers can have adverse effect on the model performance.","e8706f31":"Number of married people whose Loan was approed : 285\n\nNumber of married people whose Loan was not approed : 113\n\nNumber of unmarried people whose Loan was approed : 134\n\nNumber of unmarried people whose Loan was not approed : 79\n\nProportion of Married applicants is higher for the approved loans.","c07012f3":"our predictions are almost 77% accurate, i.e. we have identified 77% of the loan status correctly for our Random Forest model with grid search.","d47b4a3a":"Let\u2019s check the distribution of EMI variable.","3e674389":"# Target Variable","52bf82ae":"save the target variable \"Loan_Status\" in another dataset","dd6a5bdc":"Size of \"Property_Area\" variable : 614","89823b8c":"# Import required packages","19277c08":"# Let\u2019s drop the bins which we created for the exploration part. We will change the 3+ in dependents variable to 3 to make it a numerical variable","a4f90ee4":"As 'LogisticRegression' object has no attribute 'feature_importances_' so we choose next high accuracy predictive model.\nRandom Forest model is 2nd highest model.\n\nUsing Random Forest model we can find out most important feature among the features.","4b052ef2":"i)RandomForest is a tree based bootstrapping algorithm wherein a certain no. of weak learners (decision trees) are combined to make a powerful prediction model.","1edfc1cd":"The optimized value for the max_depth variable is 3 and for n_estimator is 101,random_state = 1. Now let\u2019s build the model using these optimized values.","668c2748":"This notebook is designed for people who want to solve binary classification problems using Python. By the end of this notebook, you will have the necessary skills and techniques required to solve such problems.","2b4b69d0":"# Model Building :","210dcf24":"# Analysis on \"Married\" variable :","85cd4711":"So our predictions are almost 79% accurate, i.e. we have identified 79% of the loan status correctly for our logistic regression model.","d065c8f3":"# Analysis on \"Self_Employed\" variable : ","d19b3493":"# XGBoost","fd0a9c16":"\n\nWe will use the train_test_split function from sklearn to divide our train dataset. So, first let us import train_test_split.","06e99993":"Number of dependents on the loan applicant : 0  and Loan was approed : 238\n\nNumber of dependents on the loan applicant : 0  and Loan was not approed : 107\n\nNumber of dependents on the loan applicant : 1  and Loan was approed : 66\n\nNumber of dependents on the loan applicant : 1  and Loan was not approed : 36\n\nNumber of dependents on the loan applicant : 2  and Loan was approed : 76\n\nNumber of dependents on the loan applicant : 2  and Loan was not approed : 25\n\nNumber of dependents on the loan applicant : 3+  and Loan was approed : 33\n\nNumber of dependents on the loan applicant : 3+  and Loan was not approed : 18\n\nDistribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.","f0e3b2c0":"Lets recall some of the hypotheses that we generated earlier:","bacf60f5":"# Analysis on \"Credit_History\" variable :","8716edea":"We can see there are three format of data types:\n","4f28df43":"Let's have a look in test set [LoanAmount]","9ef5f051":"Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features.\n\nBinary Classification : In this classification we have to predict either of the two given classes. For example: classifying the gender as male or female, predicting the result as win or loss, etc.\n\nMulticlass Classification : Here we have to classify the data into three or more classes. For example: classifying a movie's genre as comedy, action or romantic, classify fruits as oranges, apples, or pears, etc.","a7159988":"# Bivariate Analysis","1a8d56d7":"We will use scikit-learn (sklearn) for making different models which is an open source library for Python. It is one of the most efficient tool which contains many inbuilt functions that can be used for modeling in Python.","dba689a3":"# Expectation from the Notebook","ecef3e0c":"# Table of Contents","abee67e3":"# Relation between \"Loan_Status\" and \"Education\"","8a163e22":"Number of Female whose Loan was approed : 75\n\nNumber of Male whose Loan was approed : 339\n\nNumber of Female whose Loan was not approed : 37\n\nNumber of Male whose Loan was not approed : 150\n\nProportion of Male applicants is higher for the approved loans.","0507212a":"First of all we will find the relation between target variable and categorical independent variables. Let us look at the stacked bar plot now which will give us the proportion of approved and unapproved loans.","8518c1b5":"We will first look at the target variable, i.e., Loan_Status.\n\nAs it is a categorical variable, let us look at its frequency table, percentage distribution and bar plot.","c4e30766":"Total number of People : 614\n\n78% are Graduated and 22% are not Graduated","6a2e436e":"# Relation between \"Loan_Status\" and \"Credit_History\"","348b5aa9":"After drop test dataset will look :","21534c94":"Categorical features: These features have categories (Gender, Married, Self_Employed, Credit_History, Loan_Status)","b1645107":"float64: It represents the variable which have some decimal values involved. They are also numerical variables. Numerical variables in our dataset are: CoapplicantIncome, LoanAmount, Loan_Amount_Term, and Credit_History.","8c3ab7c8":"Size of our \"Dependents\" variable is : 599","e2854ada":"ii)For every individual learner, a random sample of rows and a few randomly chosen variables are used to build a decision tree model.","2f4e9945":"Among 601 person: \nMale : 489 \nFemale : 112","3917cd35":"# Relation between \"Loan_Status\" and \"Married\"","5b7f9d35":"After looking at the problem statement, we will now move into hypothesis generation. It is the process of listing out all the possible factors that can affect the outcome.","bf644513":"People who are from Rural area and loan was approved : 110\n\nPeople who are from Rural area and loan was not approved : 69\n\nPeople who are from Semiurban area and loan was approved : 179\n\nPeople who are from Semiurban area and loan was not approved : 54\n\nPeople who are from Urban area and loan was approved : 133\n\nPeople who are from Semiurban area and loan was not approved : 69\n\nProportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.","fe119a52":"our predictions are almost 78% accurate, i.e. we have identified 78% of the loan status correctly for our XGBClassifier model.","359f91b8":"We can see it is shifted towards left, i.e., the distribution is right skewed. So, let\u2019s take the log transformation to make the distribution normal.","a4e6faae":"Salary: Applicants with high income should have more chances of loan approval.\n\nPrevious history: Applicants who have repayed their previous debts should have higher chances of loan approval.\n\nLoan amount: Loan approval should also depend on the loan amount. If the loan amount is less, chances of loan approval should be high.\n\nLoan term: Loan for less time period and less amount should have higher chances of approval.\n\nEMI: Lesser the amount to be paid monthly to repay the loan, higher the chances of loan approval.\n\nThese are some of the factors which i think can affect the target variable, you can come up with many more factors","ea54d81c":"Provide range for max_depth from 1 to 20 with an interval of 2 and from 1 to 200 with an interval of 20 for n_estimators.","80671888":"Now we will train the model on training dataset and make predictions for the test dataset. But can we validate these predictions?\nOne way of doing this is we can divide our train dataset into two parts:train and validation.\nWe can train the model on this train part and using that make predictions for the validation part.\nIn this way we can validate our predictions as we have the true predictions for the validation part (which we do not have for the test dataset).","e4c930af":"Fit the model :","8f9ef740":"We see a similar distribution as that of the applicant income. Majority of coapplicant\u2019s income ranges from 0 to 5000. We also see a lot of outliers in the coapplicant income and it is not normally distributed.","8a9adb9e":"XGBoost is a fast and efficient algorithm.XGBoost works only with numeric variables and we have already replaced the categorical variables with numeric variables.","478a5d1a":"From the Grapch we see that :\n\nNumber of married people : 65%\n\nNumber of unmarried people : 35%    ","438956f1":"droping the target variable \"Loan_Status\"","4cb4c40d":"# Hypothesis Generation","56dae70d":"# Problem Statement","8cf11323":"Now we will see the LoanAmount variable. As it is a numerical variable, we can use mean or median to impute the missing values.\n\nWe will use median to fill the null values as earlier we saw that loan amount have outliers so the mean will not be the proper approach as it is highly affected by the presence of outliers.","62d0ff1c":"n_estimator: This specifies the number of trees for the model.\n    \nmax_depth: We can specify maximum depth of a tree using this parameter.","2510e5e9":"# Copy of original data","e337ee42":"In our train dataset the \"Gender\" variable contain\nMale : 81%\nFemale: 19%     ","c6e5fb50":"Size of our target variable is : 614","2705b350":"We will try to find the mean income of people for which the loan has been approved vs the mean income of people for which the loan has not been approved.","326a3b08":"Now lets visualize each variable separately. Different types of variables are Categorical, ordinal and numerical.","c6350f7c":"Numerical features: These features have numerical values (ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term)","4eeb48d8":"our predictions are almost 71% accurate, i.e. we have identified 71% of the loan status correctly for our Decision tree model.","8432e4ef":"After looking at every variable individually in univariate analysis, we will now explore them again with respect to the target variable.","32b3efdb":"Total Income - As discussed during bivariate analysis we will combine the Applicant Income and Coapplicant Income. If the total income is high, chances of loan approval might also be high.","d345e77f":"# Understanding the Data","f3caef64":"Just have a look of test dataset \"TotalIncome\"","d55689b6":"Understanding the problem statement is the first and foremost step. This would help you give an intuition of what you will face ahead of time. Let us see the problem statement -","2fdf4a9a":"Let\u2019s check the distribution of test dataset Total Income.","4592c31f":"\n\n\n\nGiven below is the description for each variable.\n\n\n\n\nVariable ----- Description\n\nLoan_ID -->> Unique Loan ID\n\nGender -->> Male\/ Female\n\nMarried -->> Applicant married (Y\/N)\n\nDependents -->> Number of dependents\n\nEducation -->> Applicant Education (Graduate\/ Under Graduate)\n\nSelf_Employed -->> Self employed (Y\/N)\n\nApplicantIncome -->> Applicant income\n\nCoapplicantIncome -->> Coapplicant income\n\nLoanAmount -->>  Loan amount in thousands\n\nLoan_Amount_Term -->> Term of loan in months\n\nCredit_History -->> credit history meets guidelines\n\nProperty_Area -->> Urban\/ Semi Urban\/ Rural\n\nLoan_Status -->> Loan approved (Y\/N)","d8105b4c":"# Relation between \"Loan_Status\" and \"Income\"","9956d125":"Now we will make dummy variables for the categorical variables. Dummy variable turns categorical variables into a series of 0 and 1, making them lot easier to quantify and compare. \n\nLet us understand the process of dummies first:","3ffaf81d":"Based on the domain knowledge, we can come up with new features that might affect the target variable. We will create the following three new features:","584b0853":"We can see it is shifted towards left, i.e., the distribution is right skewed. So, let\u2019s take the log transformation to make the distribution normal.","ce8e8e47":"Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:","a3ba7940":"# Missing Value and Outlier Treatment","d43b98a4":"Size of Education variable : 614","c98c00cf":"Drop \"Loan_ID\"","c4085a10":"# distribution of \"LoanAmountTerm\" variable :","e6f38c0a":"Here the y-axis represents the mean applicant income. We don\u2019t see any change in the mean income.\nSo, let\u2019s make bins for the applicant income variable based on the values in it and analyze the corresponding loan status for each bin.","3d851908":"# Independent Variable (Ordinal)","8b4cf410":"38% people from Semiurban area\n\n33% people from Urban area\n\n29% people from Rural area","2f9de033":"Lets try to test the above mentioned hypotheses using bivariate analysis.","da10d0aa":"object: Object format means variables are categorical. Categorical variables in our dataset are: Loan_ID, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Loan_Status.","0988a5e0":" For this practice problem, we have been given two CSV files: train and test.","daadb78a":"Below are some of the factors which I think can affect the Loan Approval (dependent variable for this loan prediction problem):","5604a3c9":"Now lets look at the correlation between all the numerical variables. We will use the heat map to visualize the correlation. Heatmaps visualize data through variations in coloring.\nThe variables with darker color means their correlation is more.","46e20cce":"We will build the following models in this section.","d48d2104":"Let\u2019s predict the Loan_Status for validation set","07e9cef6":"Just have a look of train dataset  \"TotalIncome\"","eb653892":"It shows that if coapplicant\u2019s income is less the chances of loan approval are high. But this does not look right. The possible reason behind this may be that most of the applicants don\u2019t have any coapplicant so the coapplicant income for such applicants is 0 and hence the loan approval is not dependent on it. So we can make a new variable in which we will combine the applicant\u2019s and coapplicant\u2019s income to visualize the combined effect of income on loan approval.","336e1347":"# Relation between \"Loan_Status\" and \"Loan Amount\"","edaefa31":"Let\u2019s make predictions for the test dataset.","fcdf4262":"# Data","29896283":"After creating new features, we can continue the model building process. So we will start with logistic regression model and then move over to more complex models like RandomForest and XGBoost","85fdead9":"Balance Income - This is the income left after the EMI has been paid. Idea behind creating this variable is that if this value is high, the chances are high that a person will repay the loan and hence increasing the chances of loan approval.","111512ba":"# Let\u2019s look at the \"CoapplicantIncome\" distribution:\n","b5173d12":"People who are Self_Employed and Loan was approed : 56\n\nPeople who are Self_Employed and Loan was not approed : 26\n\nPeople who are not Self_Employed and Loan was approed : 343\n\nPeople who are not Self_Employed and Loan was not approed : 157\n\nThere is nothing significant we can infer from Self_Employed vs Loan_Status plot.","7daec601":"We will analyze the coapplicant income and loan amount variable in similar way.","e0880fd2":"Number of people who are Graduate and Loan was approed : 340\n\nNumber of people who are Graduate and Loan was no approed : 140\n\nNumber of people who are Not Graduate and Loan was approed : 82\n\nNumber of people who are Not Graduate and Loan was not approed : 52\n\nProportion of Graduate applicants is higher for the approved loans.\n","6d169051":"Total number of people : 611\n\nMarried : 398\n\nUnmarried : 213    ","7d4cf87a":"Among 582 people only 14% are Self_Employed and rest of the 86% are Not_Self_Employed","d78634fc":"# Print the data types","4caac9db":"Total number of debts : 564\n\nRepaid Debts : 475\n\nNot Repaid Debts : 89    ","526fb8b1":"It can be inferred that most of the data in the distribution of applicant income is towards left which means it is not normally distributed. We will try to make it normal in later sections as algorithms works better if the data is normally distributed.\n\nThe boxplot confirms the presence of a lot of outliers\/extreme values. This can be attributed to the income disparity in the society.","036a6ed2":"We see that the most correlated variables are (ApplicantIncome - LoanAmount) and (Credit_History - Loan_Status).","010c5dbc":"In this section, we will do univariate analysis. It is the simplest form of analyzing data where we examine each variable individually. \n\nFor categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable.\n\nFor numerical features, probability density plots can be used to look at the distribution of the variable.","7104aeb8":"People with credit history as 1 and loan was approved : 378\n\nPeople with credit history as 1 and loan was not approved : 97\n\nPeople with credit history as 0 and loan was approved : 7\n\nPeople with credit history as 0 and loan was not approved : 82\n\nIt seems people with credit history as 1 are more likely to get their loans approved.","e1a1d339":"Now create the EMI feature.","8fce3c5e":"Let's import Decison Tree Classifier","5b289ab7":"Consider the \u201cGender\u201d variable. It has two classes, Male and Female.\n\nAs logistic regression takes only the numerical values as input, we have to change male and female into numerical value.\n\nOnce we apply dummies to this variable, it will convert the \u201cGender\u201d variable into two variables(Gender_Male and Gender_Female),\none for each class, i.e. Male and Female.\n\n\nGender_Male will have a value of 0 if the gender is Female and a value of 1 if the gender is Male.","980b264c":"# What is hypothesis generation?","9b1272fe":"Ordinal features: Variables in categorical features having some order involved (Dependents, Education, Property_Area)","2cb3e5ea":"Again have a look of test dataset \"EMI\"","b956f105":"We have 614 rows and 13 columns in the train dataset and 367 rows and 12 columns in test dataset.","861d92ed":"Number of 0 Dependent : 345\n\nNumber of 1 Dependent : 102\n\nNumber of 2 Dependesnt : 101\n\nNumber of 3+ Dependent : 51","af196c43":"Ordinal features: Variables in categorical features having some order involved (Dependents, Education, Property_Area)","08ce9908":"# Univariate Analysis","d0de4dea":"Size of our \"Married\" variable is : 611","2ff11bb9":"Let\u2019s check the distribution of train dataset Total Income.","cff80d17":"We see a lot of outliers in this variable and the distribution is fairly normal. We will treat the outliers in later sections.","f412536f":"# Shape of the dataset","9a149cb9":"int64: It represents the integer variables. ApplicantIncome is of this format.","40d5922f":"Loan prediction is a very common real-life problem that each retail bank faces atleast once in its lifetime. If done correctly, it can save a lot of man hours at the end of a retail bank.\n\n\nAlthough this notebook is specifically built to give you a walkthrough of Loan Prediction problem, you can always refer the content to get a comprehensive overview to solve a classification problem.","1cb92593":"# Numerical Independent Variable vs Target Variable","4a04b407":"As we can see that all the missing values have been filled in the train dataset.\n\nLet\u2019s fill all the missing values in the test dataset too with the same approach","1056eee3":"Let's import GridSearchCV","36f3308c":"There are very less missing values in Gender, Married, Dependents, Credit_History and Self_Employed features so we can fill them using the mode of the features","f5808937":"Now lets check whether all the missing values are filled in the dataset.","3e55c29e":"# Relation between \"Loan_Status\" and \"Self_Employed\"","df8c360d":"# Analysis on \"Property_Area\" variable :","c7354546":"Size of our \"Credit_History\" variable is : 564","d764d5d2":"Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.","d366f3a3":"# Random Forest with Grid-search","8b706d32":"Let\u2019s visualize the effect of log transformation. \n\nWe will do the similar changes to the test file simultaneously.","59514df7":"Let's drop the \"Loan_ID\" variable as it do not have any effect on the loan status. We will do the same changes to the test dataset which we did for the training dataset.","5b8e6c6e":"# Independent Variable (Numerical)","3c2439bf":"The dataset has been divided into training and validation part.\n\n70% data will use for train the model and rest of the 30% data will use for checking validation of the model. ","9865192d":"# Independent Variable (Categorical)","8a4a9d9e":"EMI - EMI is the monthly amount to be paid by the applicant to repay the loan. Idea behind making this variable is that people who have high EMI\u2019s might find it difficult to pay back the loan. We can calculate the EMI by taking the ratio of loan amount with respect to loan amount term.","fcdb2bfe":"Now calculate how accurate our predictions are by calculating the accuracy."}}