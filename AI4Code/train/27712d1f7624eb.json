{"cell_type":{"a35b072d":"code","f051c43f":"code","5c1d0f62":"code","5a326a34":"code","1e96760e":"code","ffb4f238":"code","2f4078bc":"code","f8b47b0b":"code","44327253":"code","3b7a92e4":"code","9b3dee38":"code","0833509d":"code","60344bd8":"code","3b285c21":"code","42d40124":"markdown","b6ba4adb":"markdown","a53d340d":"markdown","b320fe6c":"markdown","750e7bc1":"markdown","02871d0d":"markdown","a0ad93a6":"markdown","0b7b0e1f":"markdown","5a6a5242":"markdown","217aaf39":"markdown"},"source":{"a35b072d":"!pip install sklearn","f051c43f":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# generate training data\n# generate 2000 training samples from gaussian distribution as a 2000x1 vector\nX_train = np.random.randn(2000, 1)\n# generate gaussian noise for each samples (noise same shape with input)\nnoise = np.random.randn(2000, 1)\n# create y = 4 + 3X and add noise\nY_train = 4+3*X_train+noise\n\n# generate test data\n# generate 10 test samples from gaussian distribution as a 10x1 vector\nX_test = np.random.randn(10, 1)\n# generate gaussian noise for each samples (noise same shape with input)\nnoise = np.random.randn(10, 1)\n# create y = 4 + 3X and add noise\nY_test = 4+3*X_test+noise","5c1d0f62":"# visualize data\n# plot training samples as points (\"o\"), index 0 to get scalar values, transparency 0.2\nplt.plot(X_train[:, 0], Y_train[:, 0], \"o\", alpha=0.2, label=\"train\")\n# plot test samples as points (\"o\")\nplt.plot(X_test[:, 0], Y_test[:, 0], \"o\", label=\"test\")\n# set plot title \nplt.title(\"Data points\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top right\nplt.legend()\n# show plot\nplt.show()","5a326a34":"from sklearn import linear_model\n\n# create LinearRegression model\nregressor = linear_model.LinearRegression()\n\n# train the regression model using fit()\nregressor.fit(X_train, Y_train)","1e96760e":"# visualize the trained results\n# learned regressor line will have W as .coef_ and b as .intercept_ of the regressor\nprint(f\"Learned parameters- W: {regressor.coef_} - b:{regressor.intercept_}\")\n# plot training samples as points (\"o\"), index 0 to get scalar values, transparency 0.2\nplt.plot(X_train[:, 0], Y_train[:, 0], \"o\", alpha=0.2, label=\"train\")\n# plot test samples as points (\"o\")\nplt.plot(X_test[:, 0], Y_test[:, 0], \"o\", label=\"test\")\n# plot regression model as a line (X as x axis and projection on regressor line as y axis)\nplt.plot(X_train, regressor.coef_ * X_train + regressor.intercept_, label=\"regression model\")\n# set plot title\nplt.title(\"Data points\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","ffb4f238":"# use the function predict() of regressor to predict with X_test\nY_preds = regressor.predict(X_test)\n# sklearn LinearRegressor() provide score() function to estimate loss of learned regression model\nprint(f\"Test score: {regressor.score(X_test, Y_test)}\")","2f4078bc":"# visualize the trained results\n# plot predicted samples as points (\"o\")\nplt.plot(X_test[:, 0], Y_preds[:, 0], \"o\", label=\"prediction\")\n# plot generated test samples as points (\"o\")\nplt.plot(X_test[:, 0], Y_test[:, 0], \"o\", label=\"test\")\n# plot regression model as a line (X as x axis and projection on regressor line as y axis), transparency 0.2\nplt.plot(X_train, regressor.coef_ * X_train + regressor.intercept_, label=\"regression model\", alpha=0.2)\n# set plot title\nplt.title(\"Data points\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","f8b47b0b":"cov = [[1, 0], [0, 1]]\nmean0 = [2, 2]\nmean1 = [5, 7]\nN = 1000\n\n# generate train set\n# class 0\n# generate class 0 samples x axis and y axis values from gaussian distribution\n# numpy provides multivariate_normal() function to generate multiple values at once\nX0_train = np.random.multivariate_normal(mean0, cov, N)\n# class 0 have labels 0\nY0_train = np.zeros(N)\n# class 1\n# generate class 1 samples x axis and y axis values from gaussian distribution\nX1_train = np.random.multivariate_normal(mean1, cov, N)\n# class 1 have labels 1\nY1_train = np.ones(N)\n# concatenate the training data for each class to create training set with both class.\nX_train = np.concatenate([X0_train, X1_train], axis=0)\nY_train = np.concatenate([Y0_train, Y1_train], axis=0)","44327253":"# generate test set\nM = 10\n\nX0_test = np.random.multivariate_normal(mean0, cov, M)\nY0_test = np.zeros(M)\n# class 1\nX1_test = np.random.multivariate_normal(mean1, cov, M)\nY1_test = np.ones(M)\n# concatenate the training data for each class to create test set with both class.\nX_test = np.concatenate([X0_test, X1_test], axis=0)\nY_test = np.concatenate([Y0_test, Y1_test], axis=0)","3b7a92e4":"# visualize generated numbers\n# plot class 0 samples as red points (\"ro\"), transparency = 0.2\nplt.plot(X0_train[:, 0], X0_train[:, 1], \"ro\", alpha=0.2, label=\"train 0\")\n# plot class 0 samples as blue points (\"bo\"), transparency = 0.2\nplt.plot(X1_train[:, 0], X1_train[:, 1], \"bo\", alpha=0.2, label=\"train 1\")\n# plot test samples as black point (\"o\", color=\"black\")\nplt.plot(X_test[:, 0], X_test[:, 1], \"o\", color=\"black\", label=\"test\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","9b3dee38":"from sklearn import linear_model\n\n# create LogisticRegression model\nclassifier = linear_model.LogisticRegression()\n\n# train the regression model with fit() function\nclassifier.fit(X_train, Y_train)","0833509d":"# visualize the trained results\n# learned regressor line will have W as .coef_ and b as .intercept_ of the regressor\nprint(f\"Learned parameters- W: {classifier.coef_} - b:{classifier.intercept_}\")\n# plot class 0 samples as red points (\"ro\"), transparency = 0.2\nplt.plot(X0_train[:, 0], X0_train[:, 1], \"ro\", alpha=0.2, label=\"train 0\")\n# plot class 0 samples as blue points (\"bo\"), transparency = 0.2\nplt.plot(X1_train[:, 0], X1_train[:, 1], \"bo\", alpha=0.2, label=\"train 1\")\n# plot test samples as black point (\"o\", color=\"black\")\nplt.plot(X_test[:, 0], X_test[:, 1], \"o\", color=\"black\", label=\"test\")\n# plot classification boundary line\nplt.plot(X_train[:, 0], (0-classifier.intercept_ - classifier.coef_[0, 0]*X_train[:, 0])\/classifier.coef_[0, 1], label=\"classification boundary\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","60344bd8":"# create LogisticRegression model\nY_preds = classifier.predict(X_test)\nprint(f\"Test score: {classifier.score(X_test, Y_test)}\")","3b285c21":"# visualize the trained results\n# get test samples from each class\nX_test_class0 = X_test[Y_preds == 0]\nX_test_class1 = X_test[Y_preds == 1]\n# plot class 0 test samples as red points (\"ro\")\nplt.plot(X_test_class0[:, 0], X_test_class0[:, 1], \"ro\", label=\"test 0\")\n# plot class 1 test samples as blue points (\"ro\")\nplt.plot(X_test_class1[:, 0], X_test_class1[:, 1], \"bo\", label=\"test 1\")\n# plot decision boundary\nplt.plot(X_train[:, 0], (0-classifier.intercept_ - classifier.coef_[0, 0]*X_train[:, 0])\/classifier.coef_[0, 1], label=\"classification boundary\")\n# set axis name\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","42d40124":"Given the generated data (both training and testing), we create a `LinearRegression` object (from `sklearn.linear_model` package) and train it with the training data","b6ba4adb":"Next, we should dive into three basic algorithms for classification, regression, and clustering problems\n\n## Hello world - linear regression with `sklearn`\nWe will begin first with linear regression, the simplest and foremost machine learning algorithm.\n\nFirst, we need to generate random data. Please refer to [the NumPy lecture](https:\/\/drive.google.com\/file\/d\/1gLaygnaxiPE5KssySFjvFLUuy8OwGhKm\/view) of COTAI for random number generation.","a53d340d":"Create test set with similar statistic as the training set.\n\nTest samples = 10.","b320fe6c":"We learned logistic classifier: $\\begin{bmatrix} w_0&w_1&b \\end{bmatrix}$\n\nWe one sample point: $\\begin{bmatrix} x&y&1 \\end{bmatrix}$\n\nThe classifier boundary will be:\n\\begin{equation}\n\\begin{bmatrix} w_0&w_1&b \\end{bmatrix} \\cdotp \\begin{bmatrix} x&y&1 \\end{bmatrix}^T = 0\n\\end{equation}\n\nIn simple form:\n\\begin{equation}\nx.w_0 + y.w_1 + b = 0\n\\end{equation}\n\nSo we can plot a line with y as a function of x:\n\\begin{equation}\ny = \\frac{0 - w_0.x - b}{w_1}\n\\end{equation}","750e7bc1":"# Introduction to sklearn\n**Introduction to `sklearn`**\n\n`sklearn` (stands for Sci-kit learn) is a well-known open-source Python package which provides various machine learning algorithms and techniques for the three classes of problems above\n* Classification\n* Regression\n* Clustering\n\n`sklearn` also provides a wide range of utilities for machine learning projects, including Dimensionality reduction (e.g. PCA), Model selection (e.g. grid search, cross valation), and Data preprocessing (e.g. feature extraction).\n\n**Limitation of `sklearn`**\n\nDespite of its easy-to-use and famous, `sklearn` still cannot be able to be applied in large-scale machine learning project due to a crucial fact: the algorithms and techniques implemented in `sklearn` are all using CPUs for computation. However, `sklearn` still plays an important role in machine learning projects, especially in phases where runtime is not a key problem.\n\n**Install `sklearn` for Python**","02871d0d":"Given the generated data (both training and testing), we create a `LogisticRegression` object (from `sklearn.linear_model` package) and train it with the training data","a0ad93a6":"## Logistic regression - a simple linear classification algorithm\nIn this section, we examine logistic regression, the simplest classification algorithm for two-class classification problems (i.e. binary classification)\n\nFirst, let's generate the training data for each class and the testing data\n\nWe want the samples to be independant on each axis so we set the covariance matrix to $\\begin{bmatrix} 1&0 \\\\ 0&1 \\end{bmatrix}$.\n\nWe want the samples to be easily separatable (for illustration) so mean values of x axis and y axis of class 0 and class 1 should be far from each other. So we choose means $\\begin{bmatrix} 2&2 \\end{bmatrix}$ for class 0 and means $\\begin{bmatrix} 5&7 \\end{bmatrix}$ for class 1.\n\nEach class will have 1000 samples.\n\nClass 0 will have labels = 0 and class 1 will have labels = 1.","0b7b0e1f":"Next, let's use the linear regression model to predict `Y_test` and tell how good our model is","5a6a5242":"Next, let's use the logistic regression model to predict `Y_test` and tell how good our model is","217aaf39":"# Major classes of machine learning problems\nAssume that we have a training dataset ${\\cal{D}}_\\text{train} = \\{(x_i, y_i)\\}_{i=1}^N$ for learning, we also have some new data ${\\cal{D}}_\\text{test} = \\{x_i\\}_{i=1}^M$ and we have to predict the corresponding $y_i$ value of each $x_i \\in {\\cal{D}}_\\text{test}$. In summary, we have\n* Training set: ${\\cal{D}}_\\text{train} = \\{(x_i, y_i)\\}_{i=1}^N$\n* Test set: ${\\cal{D}}_\\text{test} = \\{(x_i, y_i)\\}_{i=1}^M$\n\nThere are three main classes of problems following this form\n\n\n* Regression problem: when $\\{y_i\\}_{i=1}^N$ are continuous\n<image src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/3a\/Linear_regression.svg\/1200px-Linear_regression.svg.png\" width=\"400px\">\n\n    * Example 1: \n        * $x_i$ is the information about some person's health\n        * $y_i$ is his height \n    * Example 2: \n        * $x_i$ is the information about some person's highshcool marks\n        * $y_i$ is the probability that he will be able to get into FTU\n    * Example 3:\n        * $x_i$ is the information about a house (e.g. area, location, etc.)\n        * $y_i$  is the price of the house\n* Classification problem: when $\\{y_i\\}_{i=1}^N$ are discrete and the number of possible values of them are finite\n<image src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcR6wQfnkIma64VyKE8m0HJxbUYj-zyvJpfm_kBvIKEQehpvtZiX&s\" width=\"400px\">\n\n    * Example 1:\n        * $x_i$ is an image\n        * $y_i$ is whether the image contains a car or not\n    * Example 2:\n        * $x_i$ is the information about a car\n        * $y_i$ is the type of the car (e.g. sport car, etc.)\n\n    >**NOTE**: when the number of possible values of $y_i$ can be infinite (e.g. face recognition), it's not a **classification** problem, it's a **recognition** problem\n\n* Clustering problem: when $\\{y_i\\}_{i=1}^N$ aren't given in ${\\cal{D}}_\\text{train}$\n<image src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/k-means-copy.jpg\" width=\"400px\">\n\n    * Example:\n        * $x_i$ is the information about a customer\n        * $y_i$ (not given in the training set but must be predicted) is his class of customer"}}