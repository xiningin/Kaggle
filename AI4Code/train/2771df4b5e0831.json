{"cell_type":{"5318a416":"code","e47c65f6":"code","7ecbf20e":"code","dc56766a":"code","bd814ac1":"code","7f0b9a1c":"code","45e0fe54":"code","d48ef576":"code","b20eb4b4":"code","e725fd76":"code","cee13eb0":"code","d12ccac2":"code","481e5319":"code","189f6458":"code","ff3c414c":"code","9d39111c":"code","2a099e2b":"code","adc04008":"code","bfea0019":"code","919de0f7":"code","b8060c9e":"code","9962d25b":"code","ad05998b":"code","147fb5a5":"code","f9b692e2":"code","7f5b340e":"code","0e71c0c2":"code","9d082e9a":"code","41f44f0d":"code","e7bd6693":"code","ed908075":"code","c70afc7a":"code","1e3d6f33":"code","7adbb15f":"code","002d55e0":"code","1f6289c2":"code","ef78cb3c":"code","7e284249":"code","0733659e":"code","b5bc7147":"code","eb335fbe":"code","ad5f4def":"code","e564876e":"code","27e60f96":"code","25c760b0":"code","53c0f0f4":"code","b10309f6":"code","7c6695aa":"code","aab68f7a":"code","6618625f":"code","1646d63a":"code","1346bdce":"markdown","46e9dc7f":"markdown","52d6f9fc":"markdown","1970eef7":"markdown","df160eb0":"markdown","6fdac3be":"markdown","6f11b9d3":"markdown","40e15a21":"markdown","a6435013":"markdown","542eb16f":"markdown","e9a6a748":"markdown"},"source":{"5318a416":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')","e47c65f6":"PATH=\"..\/input\/\"\nos.listdir(PATH)","7ecbf20e":"print(\"There are {} files in test folder\".format(len(os.listdir(os.path.join(PATH, 'test' )))))","dc56766a":"%%time\ntrain_df = pd.read_csv(os.path.join(PATH,'train.csv'), dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n","bd814ac1":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))","7f0b9a1c":"rows = 150000\nsegments = int(np.floor(train_df.shape[0] \/ rows))\nprint(\"Number of segments: \", segments)","45e0fe54":"train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\ntrain_X .shape","d48ef576":"def create_features(seg_id, seg, X):\n    xc = pd.Series(seg['acoustic_data'].values)   \n    zc = np.fft.fft(xc)\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    X.loc[seg_id, 'min'] = xc.min()\n    X.loc[seg_id, 'sum'] = xc.sum()\n    X.loc[seg_id, 'mad'] = xc.mad()\n    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n    X.loc[seg_id, 'skew'] = xc.skew()\n    X.loc[seg_id, 'med'] = xc.median()\n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n    X.loc[seg_id, 'Istd'] = imagFFT.std()\n    X.loc[seg_id, 'Imax'] = imagFFT.max()\n    X.loc[seg_id, 'Imin'] = imagFFT.min()\n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    X.loc[seg_id, 'std_first_25000'] = xc[:25000].std()\n    X.loc[seg_id, 'std_last_25000'] = xc[-25000:].std()\n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n","b20eb4b4":"def create_features_test(seg):\n    xc = pd.Series(seg['acoustic_data'].values)   \n    zc = np.fft.fft(xc)\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    return np.c_[ xc.mean(),\n           xc.std(),\n           xc.max(),\n           xc.min(),\n           xc.sum(),\n           xc.mad(),\n           xc.kurtosis(),\n           xc.skew(),\n           xc.median(),\n           np.abs(xc).mean(),\n           np.quantile(xc, 0.95),\n           np.quantile(xc, 0.99),\n           np.quantile(xc, 0.05),\n           np.quantile(xc, 0.01),\n           realFFT.mean(),\n           realFFT.std(),\n           realFFT.max(),\n           realFFT.min(),\n           imagFFT.mean(),\n           imagFFT.std(),\n           imagFFT.max(),\n           imagFFT.min(),\n           xc[:50000].std(),\n           xc[-50000:].std(),\n           xc[:25000].std(),\n           xc[-25000:].std(),\n           xc[:10000].std(),\n           xc[-10000:].std()]","e725fd76":"def create_X(x):\n    test= create_features_test(x)\n    test=test.reshape(14,2) \n    return \n","cee13eb0":"# iterate over all segments\nfor seg_id in tqdm_notebook(range(segments)):\n    seg = train_df.iloc[seg_id*rows:seg_id*rows+rows]\n    create_features(seg_id, seg, train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","d12ccac2":" train_X","481e5319":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\ntest_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)","189f6458":"for seg_id in tqdm_notebook(test_X.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    create_features(seg_id, seg, test_X)","ff3c414c":"print(\"Train X: {} y: {} Test X: {}\".format(train_X.shape, train_y.shape, test_X.shape))","9d39111c":"train_X.head()","2a099e2b":"test_X.head()","adc04008":"def plot_distplot(feature):\n    plt.figure(figsize=(16,6))\n    plt.title(\"Distribution of {} values in the train and test set\".format(feature))\n    sns.distplot(train_X[feature],color=\"green\", kde=True,bins=120, label='train')\n    sns.distplot(test_X[feature],color=\"blue\", kde=True,bins=120, label='test')\n    plt.legend()\n    plt.show()","bfea0019":"def plot_distplot_features(features, nlines=4, colors=['green', 'blue'], df1=train_X, df2=test_X):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(nlines,2,figsize=(16,4*nlines))\n    for feature in features:\n        i += 1\n        plt.subplot(nlines,2,i)\n        sns.distplot(df1[feature],color=colors[0], kde=True,bins=40, label='train')\n        sns.distplot(df2[feature],color=colors[1], kde=True,bins=40, label='test')\n    plt.show()","919de0f7":"features = ['mean', 'std', 'max', 'min', 'sum', 'mad', 'kurt', 'skew']\n\nplot_distplot_features(features)","b8060c9e":"features = ['med','abs_mean', 'q95', 'q99', 'q05', 'q01']\nplot_distplot_features(features,3)","9962d25b":"features = ['Rmean', 'Rstd', 'Rmax','Rmin', 'Imean', 'Istd', 'Imax', 'Imin']\nplot_distplot_features(features)","ad05998b":"features = ['std_first_50000', 'std_last_50000', 'std_first_25000','std_last_25000', 'std_first_10000','std_last_10000']\nplot_distplot_features(features,3)","147fb5a5":"scaler = StandardScaler()\nscaler.fit(pd.concat([train_X, test_X]))\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\nscaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)","f9b692e2":"features = ['mean', 'std', 'max', 'min', 'sum', 'mad', 'kurt', 'skew']\nplot_distplot_features(features, nlines=4, colors=['red', 'magenta'], df1=scaled_train_X, df2=scaled_test_X)","7f5b340e":"features = ['med','abs_mean', 'q95', 'q99', 'q05', 'q01']\nplot_distplot_features(features, nlines=3, colors=['red', 'magenta'], df1=scaled_train_X, df2=scaled_test_X)","0e71c0c2":"features = ['Rmean', 'Rstd', 'Rmax','Rmin', 'Imean', 'Istd', 'Imax', 'Imin']\nplot_distplot_features(features, nlines=4, colors=['red', 'magenta'], df1=scaled_train_X, df2=scaled_test_X)","9d082e9a":"features = ['std_first_50000', 'std_last_50000', 'std_first_25000','std_last_25000', 'std_first_10000','std_last_10000']\nplot_distplot_features(features, nlines=3, colors=['red', 'magenta'], df1=scaled_train_X, df2=scaled_test_X)","41f44f0d":"def plot_acc_agg_ttf_data(feature, title=\"Averaged accoustic data and ttf\"):\n    fig, ax1 = plt.subplots(figsize=(16, 8))\n    plt.title('Averaged accoustic data ({}) and time to failure'.format(feature))\n    plt.plot(train_X[feature], color='r')\n    ax1.set_xlabel('training samples')\n    ax1.set_ylabel('acoustic data ({})'.format(feature), color='r')\n    plt.legend(['acoustic data ({})'.format(feature)], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_y, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)","e7bd6693":"plot_acc_agg_ttf_data('mean')","ed908075":"plot_acc_agg_ttf_data('std')","c70afc7a":"plot_acc_agg_ttf_data('max')","1e3d6f33":"plot_acc_agg_ttf_data('min')","7adbb15f":"plot_acc_agg_ttf_data('sum')","002d55e0":"plot_acc_agg_ttf_data('mad')","1f6289c2":"plot_acc_agg_ttf_data('kurt')","ef78cb3c":"plot_acc_agg_ttf_data('skew')","7e284249":"plot_acc_agg_ttf_data('std_first_50000')","0733659e":"plot_acc_agg_ttf_data('std_last_50000')","b5bc7147":"plot_acc_agg_ttf_data('std_first_25000')","eb335fbe":"plot_acc_agg_ttf_data('std_last_25000')","ad5f4def":"plot_acc_agg_ttf_data('std_first_10000')","e564876e":"plot_acc_agg_ttf_data('std_last_10000')","27e60f96":"from keras.models import Sequential\nfrom keras.layers import Dense, CuDNNGRU, SimpleRNN, LSTM ,  Dropout, Activation, Flatten, Input, Conv1D, MaxPooling1D\nfrom keras.optimizers import adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import RMSprop","25c760b0":"validation_point=351\nendpoint=train_X.shape[0]-1\ntttt=validation_point-endpoint\n\nX_train=scaled_train_X.values[validation_point:endpoint,].reshape(tttt,14,2)\nprint('X_train.shape',X_train.shape)\ny_train=train_y.values[validation_point:endpoint,]\nprint('y_train.shape',y_train.shape)\n\nX_validation=scaled_train_X.values[0:validation_point-1,].reshape(validation_point-1,14,2)\nprint('X_validation.shape',X_validation.shape)\ny_validation=train_y.values[0:validation_point-1,]\nprint('y_validation.shape',y_validation.shape)\n\nX_train.shape[1]","53c0f0f4":"i = (X_train.shape[1],X_train.shape[2])\nmodel = Sequential ()\nmodel.add(Conv1D(5, 3, activation='relu', input_shape= i))\nmodel.add(MaxPooling1D(2))\nmodel.add(LSTM(50,  return_sequences=True))\nmodel.add(LSTM(10))\nmodel.add(Dense(240))\nmodel.add(Dense(120))\nmodel.add(Dense(60))\nmodel.add(Dense(30))\nmodel.add(Dense(1))\n\n\nmodel.summary()","b10309f6":"import keras\nfrom keras.optimizers import RMSprop\nopt = keras.optimizers.adam(lr=.005)\n\nmodel.compile(loss=\"mae\",\n              optimizer=opt, metrics=['mean_absolute_error'])\n             # metrics=['accuracy'])\n\n\nbatch_size = 128 # mini-batch with 32 examples\nepochs = 50\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=1,\n    validation_data=(X_validation ,y_validation ))","7c6695aa":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n","aab68f7a":"X_test=scaled_test_X.values.reshape(test_X.shape[0],14,2)\nprint(X_test.shape)\n\n#submission.head()\n\n\nfor i, seg_id in enumerate(tqdm(submission.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    x = seg['acoustic_data'].values\n    submission.time_to_failure[i]= model.predict(np.expand_dims(X_test[i], 0))\n    \n    \n","6618625f":"submission_newfeatures=submission\nsubmission_newfeatures.head()\n","1646d63a":"submission_newfeatures.to_csv('submission_newfeatures.csv')","1346bdce":"# <a id='4'>New features exploration<\/a> \n\n\n## Aggregated features\n\nLet's visualize the new features distributions. The graphs below shows the distplot (histograms and density plots) for all the new features, for train (<font color=\"green\">green<\/font>) and test (<font color=\"blue\">blue<\/font>) data.","46e9dc7f":"## Scaled features\n\nLet's scale now the aggregated features and show again the resulting graphs.   \nWe are fiting the scaler with both train and test data.\nWe use <font color=\"red\">red<\/font> from train and <font color=\"magenta\">magenta<\/font> for test data.","52d6f9fc":"# <a id='5'>Defining Model <\/a> ","1970eef7":"# <a id='3'>Calculate aggregated features<\/a>  ","df160eb0":"# <a id='2'>Prepare the data analysis<\/a>  \n\n## Load packages","6fdac3be":"<h1><center><font size=\"6\">Feature analysis LANL Earthquake New Approach EDA  <\/font><\/center><\/h1>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n- <a href='#3'>Calculate aggregated features<\/a>\n- <a href='#4'>New features exploration<\/a>  \n- <a href='#5'>Conclusions<\/a>  \n- <a href='#6'>References<\/a>  \n","6f11b9d3":"## Aggregated features and time to failure\n\nLet's also show aggregated features and time to failure on the same graph. ","40e15a21":"# <a id='1'>Introduction<\/a>  \n\n## Simulated earthquake experiment\nThe data are from an experiment conducted on rock in a double direct shear geometry subjected to bi-axial loading, a classic laboratory earthquake model.\n\nTwo fault gouge layers are sheared simultaneously while subjected to a constant normal load and a prescribed shear velocity. The laboratory faults fail in repetitive cycles of stick and slip that is meant to mimic the cycle of loading and failure on tectonic faults. While the experiment is considerably simpler than a fault in Earth, it shares many physical characteristics.\n\nLos Alamos' initial work showed that the prediction of laboratory earthquakes from continuous seismic data is possible in the case of quasi-periodic laboratory seismic cycles.\n\n## Competition\nIn this competition, the team has provided a much more challenging dataset with considerably more aperiodic earthquake failures.\nObjective of the competition is to predict the failures for each test set.","a6435013":"## Load data","542eb16f":"![](http:\/\/)# <a id='6'>References<\/a>  \n\n[1] LANL Earthquake Prediction, https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction  \n[2] Shaking Earth, https:\/\/www.kaggle.com\/allunia\/shaking-earth  \n[3] Earthquake FE - more features and samles, https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples  \n[4] Laboratory observations of slow earthquakes and the spectrum of tectonic fault slip modes, https:\/\/www.nature.com\/articles\/ncomms11104   \n[5] Machine Learning Predicts Laboratory Earthquakes, https:\/\/agupubs.onlinelibrary.wiley.com\/doi\/full\/10.1002\/2017GL074677  \n","e9a6a748":"# <a id='5'>Conclusions<\/a>  \n\nWe analyzed the distribution of the aggregated features and also the time to failure and the aggregated features on the same graph.  \n"}}