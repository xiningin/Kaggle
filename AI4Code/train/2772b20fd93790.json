{"cell_type":{"804a2ef9":"code","d90587ae":"code","62a6627a":"code","fcf18194":"code","c2dc879c":"code","af8c814e":"code","98dfea54":"code","052f9826":"code","f8e0a97e":"code","25ddc3fb":"code","b827a8da":"code","a9178bee":"code","c000c2cc":"code","7bc61ebe":"code","4ec73076":"code","33f77d20":"code","c13abcb2":"code","f4bed093":"code","81222bfe":"code","9b813153":"code","8a75662c":"code","6c034aee":"code","a822a656":"code","c62725ba":"code","c195165f":"code","25066194":"markdown","5098746e":"markdown","c6dde34d":"markdown","479f0312":"markdown","2425c340":"markdown","24336555":"markdown","97034a1d":"markdown","048a9793":"markdown","2be765b2":"markdown","af09b0eb":"markdown","9d07effd":"markdown","ba55be31":"markdown","8b66f3cc":"markdown","6efa9e67":"markdown","28804127":"markdown","a37f1e39":"markdown","e81a6ceb":"markdown","10a3bb7b":"markdown","e85decb9":"markdown","e0dab913":"markdown","2ed41d03":"markdown","f36c16cf":"markdown","5b9420fa":"markdown","ace4ca69":"markdown"},"source":{"804a2ef9":"#%% Imports\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Metrics \nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# ML Models\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor \nimport xgboost as xg \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\n\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap\n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","d90587ae":"#%% Read train.csv\ntrain_csv = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\n\n# Initial glance at train.csv\nprint(train_csv.info(verbose = True,show_counts=True))","62a6627a":"#%% Read train.csv\ntest_csv = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\n\n# Initial glance at train.csv\nprint(test_csv.info(verbose = True,show_counts=True))","fcf18194":"#%% PlotMultiplePie \n# Input: df = Pandas dataframe, categorical_features = list of features , dropna = boolean variable to use NaN or not\n# Output: prints multiple px.pie() \n\ndef PlotMultiplePie(df,categorical_features = None,dropna = False):\n    # set a threshold of 30 unique variables, more than 50 can lead to ugly pie charts \n    threshold = 30\n    \n    # if user did not set categorical_features \n    if categorical_features == None: \n        categorical_features = df.select_dtypes(['object','category']).columns.to_list()\n        \n    print(\"The Categorical Features are:\",categorical_features)\n    \n    # loop through the list of categorical_features \n    for cat_feature in categorical_features: \n        num_unique = df[cat_feature].nunique(dropna = dropna)\n        num_missing = df[cat_feature].isna().sum()\n        # prints pie chart and info if unique values below threshold \n        if num_unique <= threshold:\n            print('Pie Chart for: ', cat_feature)\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            fig = px.pie(df[cat_feature].value_counts(dropna = dropna), values=cat_feature, \n                 names = df[cat_feature].value_counts(dropna = dropna).index,title = cat_feature,template='ggplot2')\n            fig.show()\n        else: \n            print('Pie Chart for ',cat_feature,' is unavailable due high number of Unique Values ')\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            print('\\n')","c2dc879c":"#%% Use PlotMultiplePie to see the distribution of the categorical variables \nPlotMultiplePie(train_csv.drop(\"id\",axis = \"columns\"))","af8c814e":"#%% Print the continous features in the dataset \ncontinous_features = train_csv.drop([\"id\",\"target\"],axis = \"columns\").select_dtypes(['float64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.histplot(train_csv[cont_feature])","98dfea54":"plt.figure()\nplt.title(\"target\")\nax = sns.histplot(train_csv[\"target\"])","052f9826":"# save the 'id' for Train and Test \ntrain_csv_id = train_csv['id'].to_list()\ntest_csv_id = test_csv['id'].to_list()\n\n# Seperate train_clean into target and features \ny = train_csv['target']\nX_train_clean = train_csv.drop('target',axis = 'columns')\n\n# Save the index for X_train_clean \nX_train_clean_index = X_train_clean.index.to_list()\n\n# Row bind train.csv features with test.csv features \n# this makes it easier to apply label encoding onto the entire dataset \nX_total = X_train_clean.append(test_csv,ignore_index = True)\n\n# save the index for test.csv \nX_test_clean_index = np.setdiff1d(X_total.index.to_list() ,X_train_clean_index) \n\n# drop id from X_total\nX_total = X_total.drop('id',axis = 'columns')\n\nX_total.info()","f8e0a97e":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_total.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_total_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_total)","25ddc3fb":"##% Split X_total_encoded \nX_train_clean_encoded = X_total_encoded.iloc[X_train_clean_index, :]\nX_test_clean_encoded = X_total_encoded.iloc[X_test_clean_index, :].reset_index(drop = True) \nX_train_clean_encoded.info()","b827a8da":"##% Before and After LabelEncoding for train.csv \ndisplay(X_train_clean.head().drop(\"id\",axis = 'columns'))\ndisplay(X_train_clean_encoded.head())","a9178bee":"##% Before and After LabelEncoding for test.csv \ndisplay(test_csv.head().drop(\"id\",axis = 'columns'))\ndisplay(X_test_clean_encoded.head())","c000c2cc":"# Create test and train set 80-20\n#%%  train-test stratified split using a 80-20 split\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_train_clean_encoded, y, shuffle = True, test_size=0.2, random_state=0)\n\ntrain_X.info()","7bc61ebe":"##% evaluateRegressor\n# from sklearn.metrics import mean_squared_error, mean_absolute_error\ndef evaluateRegressor(true,predicted,message):\n    MSE = mean_squared_error(true,predicted,squared = True)\n    MAE = mean_absolute_error(true,predicted)\n    RMSE = mean_squared_error(true,predicted,squared = False)\n    print(message)\n    print(\"MSE:\", MSE)\n    print(\"MAE:\", MAE)\n    print(\"RMSE:\", RMSE)","4ec73076":"def PlotPrediction(true,predicted, title = \"Dataset: \"):\n    fig = plt.figure(figsize=(20,20))\n    ax1 = fig.add_subplot(111)\n    ax1.set_title(title + 'True vs Predicted')\n    ax1.scatter(list(range(0,len(true))),true, s=10, c='r', marker=\"o\", label='True')\n    ax1.scatter(list(range(0,len(predicted))), predicted, s=10, c='b', marker=\"o\", label='Predicted')\n    plt.legend(loc='upper right');\n    plt.show()","33f77d20":"#% Initial Models\n# from sklearn.ensemble import RandomForestRegressor\n# import lightgbm as lgb\n\nRFReg = RandomForestRegressor(n_estimators = 10, criterion = \"mse\", max_depth = 10, n_jobs = -1,random_state = 0).fit(train_X, train_y)\nLGBMReg = lgb.LGBMRegressor(max_depth = 10,random_state=0).fit(train_X,train_y)","c13abcb2":"# % Model Metrics\n\nprint(\"Random Forest Regressor\") \npredicted_train_y = RFReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = RFReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\nprint(\"LightGBM Regressor\") \npredicted_train_y = LGBMReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = LGBMReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")","f4bed093":"# Cross validation using Random Forest\nfrom sklearn.model_selection import KFold\n\nparam_rf = {'n_estimators': 20, \n            'criterion': 'mse', \n            'max_depth':100,\n            'n_jobs' : -1,\n            'random_state' : 0}\n\ndef K_Fold_RandomForest(X_train,y_train, params_set, num_folds = 5):\n    models = []\n    folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n\n    y_preds = np.zeros_like(y_train, dtype=np.float64)\n    feature_importance_df = pd.DataFrame()\n\n        # 5 times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n\n        CV_RF = RandomForestRegressor(**params_set).fit(train_X, train_y)\n        models.append(CV_RF)\n        \n    for num in range(0,len(models)): \n        print(f\"     model {num}\")\n        print(\"Train set RMSE:\", mean_squared_error(train_y,models[num].predict(train_X),squared = False))\n        print(\" Test set RMSE:\", mean_squared_error(valid_y,models[num].predict(valid_X),squared = False))\n        print(\"\\n\")\n    return models\n\nrf_models = K_Fold_RandomForest(X_train_clean_encoded,y,param_rf,5)","81222bfe":"# Predict y_prds using models from crossvalidation \ndef predict_cv(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict(X)\n        \n    return y_preds\/len(models_cv)\npredicted_y = predict_cv(rf_models,X_train_clean_encoded)\n# evalute model using the entire dataset from Train.csv\nevaluateRegressor(y,predicted_y,\"Train.csv\")\nPlotPrediction(y[0:5000],predicted_y[0:5000], title = \"Train.csv: \")","9b813153":"##% Parameter Tuning for LightGBM\n# store the catagorical features names as a list      \ncat_features = X_train_clean_encoded.select_dtypes(['object']).columns.to_list()\n\n# https:\/\/medium.com\/analytics-vidhya\/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\n# from lightgbm import LGBMRegressor \n# from bayes_opt import BayesianOptimization\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=False, verbose_eval =False, metrics=['rmse'])\n\n        return -np.min(score['rmse-mean']) # min or max can change best_param\n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    # https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/bayes_opt\/bayesian_optimization.pyta\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                      {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 1000),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = -1\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points=5, n_iter=25) # 20 combinations \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'rmse'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'regression'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set\n\nbest_params = search_best_param(X_train_clean_encoded,y,cat_features)","8a75662c":"#Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","6c034aee":"def K_Fold_LightGBM(X_train, y_train , cat_features, params_set, num_folds = 5):\n    num = 0\n    models = []\n    folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n\n        # 5 times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        CV_LGBM = lgb.train(params_set,\n                 train_data,\n                 num_boost_round = 2500,\n                 valid_sets = valid_data,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 50\n                 )\n        # increase early_stopping_rounds can lead to overfitting \n        models.append(CV_LGBM)\n        \n        print(\"Train set RMSE:\", mean_squared_error(train_y,models[num].predict(train_X),squared = False))\n        print(\" Test set RMSE:\", mean_squared_error(valid_y,models[num].predict(valid_X),squared = False))\n        print(\"\\n\")\n        num = num + 1\n        \n    return models\n\nlgbm_models = K_Fold_LightGBM(X_train_clean_encoded,y,cat_features,best_params,5)","a822a656":"# evalute model using the entire dataset from Train.csv\npredicted_y = predict_cv(lgbm_models,X_train_clean_encoded)\nevaluateRegressor(y,predicted_y,\"Train.csv\")\nPlotPrediction(y[0:5000],predicted_y[0:5000], title = \"Train.csv: \")","c62725ba":"# Prediction for Test.csv\npredictLGBM = predict_cv(rf_models,X_test_clean_encoded) \npredictRF = predict_cv(lgbm_models,X_test_clean_encoded)\n\nsubmissionLGBM = pd.DataFrame({'id':test_csv_id,'target':predictLGBM})\nsubmissionRF = pd.DataFrame({'id':test_csv_id,'target':predictRF})\n\ndisplay(submissionLGBM.head())\ndisplay(submissionRF.head())","c195165f":"#% Submit Predictions \nsubmissionLGBM.to_csv('submissionCV_LGBM4.csv',index=False)\nsubmissionRF.to_csv('submissionCV_RF4.csv',index=False)","25066194":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Introduction](#Introduction)\n* [Importing Libraries](#Importing-Libraries)\n* [Task Details](#Task-Details)\n* [Read in Data](#Read-in-Data)\n    - [Train.csv](#Train.csv)\n    - [Test.csv](#Test.csv)\n    - [Notes](#Notes)\n* [Data Visualization](#Data-Visualization)\n    - [Categorical Features](#Categorical-Features)\n    - [Continuous Features](#Continuous-Features)\n    - [Target](#Target)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Label Encoding](#Label-Encoding)\n    - [Train-Test Split](#Train-Test-Split)\n* [Initial Models](#Initial-Models)\n* [Random Firest  Regressor](#Random-Forest-Regressor)\n    - [RF Cross Validation](#RF-Cross-Validation)\n* [LightGBM Regressor](#LightGBM-Regressor)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n* [Prediction for Test Data](#Prediction-for-Test-Data)\n* [Conclusion](#Conclusion)","5098746e":"<a id=\"Label-Encoding.csv\"><\/a>\n## Label Encoding","c6dde34d":"<a id=\"Preprocessing-Data\"><\/a>\n# Preprocessing Data\nBecause Train.csv and Test.csv have no missing data imputation is not needed.  \nLabel encoding is still require as this dataset has categorical features ","479f0312":"<a id=\"Bayesian-Optimization\"><\/a>\n## Bayesian Optimization","2425c340":"<a id=\"Initial Models\"><\/a>\n# Initial Models\nI applied different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n \n1. RandomForest\n2. LightGBM","24336555":"<a id=\"Task-Details\"><\/a>\n# Task Detail \n\n## Goal\nFor this competition, you will be predicting a continuous **target** based on a number of feature columns given in the data. All of the feature columns, **cat0** - **cat9** are **categorical**, and the feature columns **cont0** - **cont13** are **continuous**.\n\n## Metric\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n\nwhere  is the predicted value,  is the original value, and  is the number of rows in the test data.","97034a1d":"<a id=\"Data-Visualization\"><\/a>\n# Data Visualization ","048a9793":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data","2be765b2":"<a id=\"#Train-Test-Split\"><\/a>\n# Train-Test Split","af09b0eb":"<a id=\"Target\"><\/a>\n## Target","9d07effd":"<a id=\"LightGBM-Regressor\"><\/a>\n# LightGBM Regressor\nthe LightGBM model can be imporved by applying cross validation as well. I also used parameter tuning to obtain a better model. ","ba55be31":"<a id=\"Notes\"><\/a>\n## Notes\n\nTrain.csv and Test.csv have no missing values so imputation is not needed. Since there aren't many features in this dataset I can do a quick explanatory data analysis on the features and target.","8b66f3cc":"<a id=\"Prediction-for-Test.csv\"><\/a>\n# Prediction for Test.csv","6efa9e67":"<a id=\"Random-Forest-Regressor\"><\/a>\n# Random Forest Regressor\nThe model can be improved by using cross validation. I used 5-fold cross validation. ","28804127":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","a37f1e39":"<a id=\"Train.csv\"><\/a>\n## Train.csv","e81a6ceb":"<a id=\"LightGBM-Cross-Validation\"><\/a>\n## LightGBM Cross Validation","10a3bb7b":"<a id=\"RF-Cross-Validation\"><\/a>\n## RF Cross Validation","e85decb9":"<a id=\"Categorical-Features\"><\/a>\n## Categorical Features","e0dab913":"# <center>TabularPlaygroundRegressor Feb2021<\/center>\n<img src= \"https:\/\/assets.thesmartcube.com\/smartcube\/app\/uploads\/2020\/10\/shutterstock_image-12.jpg\" height=\"200\" align=\"center\"\/>","2ed41d03":"<a id=\"Introduction\"><\/a>\n# Introduction\nThis is my second competition notebook on Kaggle. I hope to learn more about working with tabular data and I hope anyone who reads this learns more as well! I will be using various machine learning techniques such as LightGBM, Random Forest, Support Vector Machine, and XGBoost. If you have any questions or comments please leave below! As always leave a upvote as well!  ","f36c16cf":"<a id=\"Continuous-Features\"><\/a>\n## Continuous Features","5b9420fa":"<a id=\"Test.csv\"><\/a>\n## Test.csv","ace4ca69":"<a id=\"Conclusion\"><\/a>\n<a id=\"Conclusion\"><\/a>\n# Conclusion\n\n**Conclusion**\n* Cross validation is useful in getting a better model and to lessen overfitting.   \n\n**Challenges**\n* The dataset was rather large which led to long computation time.\n* Although I used various machine learning algorithims, I wasn't able to improve on score much.    \n\n**Closing Remarks**  \n* Please comment and like the notebook if it of use to you! Have a wonderful year! \n\n**Other Notebooks** \n* [https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80](https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80)\n* [https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95](https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95)\n\n2-7-2020\nJoseph Chan "}}