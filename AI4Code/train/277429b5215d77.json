{"cell_type":{"a33e136a":"code","a808c12f":"code","1e5e5a1f":"code","532636f4":"code","cbbc6c45":"code","8b0274db":"code","d6ec90cf":"code","b553b830":"code","f01cf877":"code","35e1f82d":"code","eb755c5e":"code","90bb1e13":"code","bd2c44a2":"code","529897c7":"code","6c41f2c3":"code","61963143":"code","e42d9124":"code","48f7ebed":"code","3a06bd9a":"code","de01e766":"code","915ef546":"code","23e12251":"markdown","aa6671dd":"markdown","4043c2cd":"markdown","7f0d348f":"markdown","93b3c523":"markdown","957ac2d7":"markdown","f1f01ac7":"markdown","0b34e5f4":"markdown","27f50e79":"markdown"},"source":{"a33e136a":"!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","a808c12f":"from sklearnex import patch_sklearn\npatch_sklearn()","1e5e5a1f":"import os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n","532636f4":"RANDOM_SEED = 2021\nPROBAS = True\nFOLDS = 5\nN_ESTIMATORS = 1000\n\nTARGET = 'Survived'","cbbc6c45":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv', index_col='PassengerId')\n# Pseudo labels taken from great BIZEN notebook: https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-pseudo-labeling-voting-ensemble\npseudo_labels = pd.read_csv(\"..\/input\/tps-apr-2021-label\/pseudo_label.csv\")\ntest[TARGET] = pseudo_labels[TARGET]\nall_df = pd.concat([train, test]).reset_index(drop=True)\n\ntarget = train.pop('Survived')","8b0274db":"# Age fillna with mean age for each class\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n# Cabin, fillna with 'X' and take first letter\nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n# Embarked, fillna with 'X' value\nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n# Name, take only surnames\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])","d6ec90cf":"all_df.head(5)","b553b830":"label_cols = ['Name', 'Ticket', 'Sex']\nonehot_cols = ['Cabin', 'Embarked']\nnumerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","f01cf877":"def label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\nall_df = pd.concat([numerical_df, label_encoded_df, onehot_encoded_df, target_df], axis=1)","35e1f82d":"all_df.head(5)","eb755c5e":"all_df_scaled = all_df.drop([TARGET], axis = 1).copy()\n\nscaler = StandardScaler()\nscaler.fit(all_df.drop([TARGET], axis = 1))\nall_df_scaled = scaler.transform(all_df_scaled)\n\nall_df_scaled = pd.DataFrame(all_df_scaled, columns=all_df.drop([TARGET], axis = 1).columns)","90bb1e13":"all_df_scaled.head(5)","bd2c44a2":"X = all_df_scaled\ny = all_df[TARGET]\n\nprint (f'X:{X.shape} y: {y.shape} \\n')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = RANDOM_SEED)\nprint (f'X_train:{X_train.shape} y_train: {y_train.shape}')\nprint (f'X_test:{X_test.shape} y_test: {y_test.shape}')\n\ntest = all_df_scaled[len(train):]\nprint (f'test:{test.shape}')","529897c7":"%%time\nsvc_kernel_rbf = SVC(kernel='rbf', random_state=0, C=0.779481782160288, gamma=0.10264575666119422 )\nsvc_kernel_rbf.fit(X_train, y_train)\ny_pred = svc_kernel_rbf.predict(X_test)\naccuracy_score(y_pred, y_test)","6c41f2c3":"%%time\nfinal_pred = svc_kernel_rbf.predict(test)","61963143":"submission['Survived'] = np.round(final_pred).astype(int)\nsubmission.to_csv('svc_kernel_rbf.csv')","e42d9124":"def objective(trial):\n    from sklearn.svm import SVC\n    params = {\n        'C': trial.suggest_loguniform('C', 0.01, 0.1),\n        'gamma': trial.suggest_categorical('gamma', [\"auto\"]),\n        'kernel': trial.suggest_categorical(\"kernel\", [\"rbf\"])\n    }\n\n    svc = SVC(**params)\n    svc.fit(X_train, y_train)\n    return svc.score(X_test, y_test)","48f7ebed":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"maximize\",\n                            pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=5, show_progress_bar=True)","3a06bd9a":"print(f\"Best Value from optune: {study.best_trial.value}\")\nprint(f\"Best Params from optune: {study.best_params}\")\n\nif study.best_trial.value >= 0.88515:\n    best_value = study.best_params\nelse:\n    best_value = {'C': 0.779481782160288, 'gamma': 0.10264575666119422, 'kernel': 'rbf'}\n    print(f\"Using precalculated best params instead: {best_value}\")\n    ","de01e766":"%%time\nn_folds = 20\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(X, y)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(X.iloc[train_index]), pd.DataFrame(X.iloc[valid_index])\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    svc_kernel_rbf = SVC(**best_value)\n    svc_kernel_rbf.fit(X_train, y_train)\n    print(\"  Accuracy: {}\".format(accuracy_score(y_valid, svc_kernel_rbf.predict(X_valid))))\n    y_pred += svc_kernel_rbf.predict(test)\n\ny_pred \/= n_folds\n\nprint(\"\")\nprint(\"Done!\")","915ef546":"submission['Survived'] = np.round(y_pred).astype(int)\nsubmission.to_csv('svc_kernel_rbf_10_folds_optune.csv')","23e12251":"# Feature engeenring","aa6671dd":"# Installing scikit-learn-intelex\n\nPackage also avaialble in conda  - please refer to details https:\/\/github.com\/intel\/scikit-learn-intelex","4043c2cd":"# Loading data","7f0d348f":"# Single SVM run\nRunning single SVM prediction","93b3c523":"# \ud83d\ude80 Optimizing Kaggle kernels using Intel(R) Extension for Scikit-learn\n\nFor classical machine learning algorithms, we often use the most popular Python library, scikit-learn. We use it to fit models and search for optimal parameters, but\u202fscikit-learn\u202fsometimes works for hours, if not days. Speeding up this process is something anyone who uses scikit-learn would be interested in.\n\nI want to show you how to get results faster without changing the code. To do this, we will use another Python library,\u202f[scikit-learn-intelex](https:\/\/github.com\/intel\/scikit-learn-intelex). It accelerates scikit-learn and does not require you changing the code written for scikit-learn.\n\nWhile SVM is pretty slow in stock scikit-learn, with Intel Extension it can be now used for regular iterative work\n\nThis kernel is based on [[TPS 2021-04] Support Vector Machines](https:\/\/www.kaggle.com\/ekozyreff\/tps-2021-04-support-vector-machines) and feature preprocessing from [Ensemble-learning meta-classifier for stacking](https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking) as best result public kernel at the moment","957ac2d7":"# Original code below\nKeep code mix from different kernels","f1f01ac7":"# Enable Intel(R) Extension for Scikit-learn\nDo magic here - patching scikit-learn ","0b34e5f4":"# \ud83d\udcdc Conclusions\n\nWith scikit-learn-intelex patching you can:\n\n*     Use your scikit-learn code for training and inference without modification.\n*     Train and predict scikit-learn models and get more time for experiments\n*     Get the same quality of predictions\n\n*Please, upvote if you like.*","27f50e79":"# Hyperparams selection and Kfolds\nAs we have pretty fast SVM now - we can try running optune for params search and KFolds for final submission\nI using pretty small range for optune serach and less iterations as i've already validate it throught many other runs"}}