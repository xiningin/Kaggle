{"cell_type":{"929ff400":"code","1f43f763":"code","0f9311c0":"code","34dee699":"code","8b4f70a9":"code","29110e69":"code","f54c09b3":"code","a7f51111":"code","cc361281":"code","47d223f8":"code","9693b97b":"code","9758673b":"code","f2ad94de":"code","214dccbe":"code","84152cd3":"code","f2fbf2ac":"code","78c36280":"code","9e01b71a":"code","a9a802f5":"code","36c2d3bf":"code","cadc65b7":"markdown","3402896b":"markdown","5a64bee0":"markdown","bd829866":"markdown","a4c60734":"markdown","acb91f69":"markdown","41e5441f":"markdown","c703f676":"markdown","3fe03678":"markdown","908f42d7":"markdown","c804061f":"markdown","9a67d8ab":"markdown","643887bb":"markdown","4f7b06e8":"markdown","50fcbc21":"markdown"},"source":{"929ff400":"# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n# Outputs taken as inputs and their individual scores\n\npred_lgb = pd.read_csv('..\/input\/tps-10-lgbm-onemodel-threeseeds-blend\/random_seeds_blending_submission.csv') # 0.85644\npred_xgb = pd.read_csv('..\/input\/tsp-single-xgboost-model\/xgb.csv') # 0.85649\npred_hgb = pd.read_csv('..\/input\/tps-21-oct-single-histgbm-0-85651\/HistGBM.csv') # 0.85651\npred_stack = pd.read_csv('..\/input\/noob-stacking-0-85654\/LGBM_overfit.csv') # 0.85654\n\nsubmission = pd.read_csv('..\/input\/tps-oct-2021-single-lightgbm\/submission.csv')","1f43f763":"predictions = [pred_lgb, pred_xgb, pred_hgb, pred_stack]\n\nresults = pd.DataFrame()\nfor i, ds in enumerate(predictions):\n    results[f'target_{i+1}'] = ds['target']","0f9311c0":"results.head()","34dee699":"# Correlation Matrix - all are very very highly correlated with each other, as expected\n\nresults.corr()","8b4f70a9":"# Plotting these 4 model predictions\n\nhist_data = [pred_lgb.target, pred_xgb.target, pred_hgb.target, pred_stack.target]\ngroup_labels = ['lgb', 'xgb', 'hgb', 'stack']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\nfig.show()\n\n# stack is quite different compared to others. ","29110e69":"# I will assign weights to the submissions, as per their scores\n\nsubmission['target'] = (results['target_1']*1 +results['target_2']*2 +results['target_3']*3 +\nresults['target_4']*4)\/10","f54c09b3":"submission.head()","a7f51111":"submission['target'] = (submission['target']-submission['target'].min())\/(submission['target'].max()-submission['target'].min())","cc361281":"submission.head()","47d223f8":"submission.to_csv('submission_weighted_average.csv',index=False) # gave a score of 0.85655","9693b97b":"results['mean']=results.mean(axis=1)\nresults['min']=results.min(axis=1)\nresults['max']=results.max(axis=1)\n\nfor i in np.arange(0,len(results)):\n    if results.loc[i,'mean']>0.5:\n        results.loc[i,'final']=results.loc[i,'max']\n    else:\n        results.loc[i,'final']=results.loc[i,'min']\n        \nresults.head()","9758673b":"submission['target'] = results['final']\nsubmission.to_csv('submission_minmax.csv',index=False) # gave a score of 0.85639","f2ad94de":"# It works best on highly correlated models\n\n#power = 3 # gave a score of 0.85651\npower = 4 # gave a score of 0.85651\n\nsubmission['target'] = (results['target_1']**power +results['target_2']**power\n                        +results['target_3']**power + results['target_4']**power)\/4","214dccbe":"submission.head()","84152cd3":"submission.to_csv('submission_power_average.csv',index=False)","f2fbf2ac":"# Normalising the predictions\n\npredictions = [pred_lgb, pred_xgb, pred_hgb, pred_stack]\n\nresults = pd.DataFrame()\nfor i, ds in enumerate(predictions):\n    ds['target'] = (ds['target'] - ds['target'].min())\/(ds['target'].max() - ds['target'].min())\n    results[f'target_{i+1}'] = ds['target']","78c36280":"results.head()","9e01b71a":"submission['target'] = (results['target_1']*results['target_2']*\n                        results['target_3']*results['target_4'])**(1.0\/4)","a9a802f5":"submission.head()","36c2d3bf":"submission.to_csv('submission_geometric_mean.csv',index=False) # gave a score of 0.85650 (unscaled targets)\n# gave a score of 0.85650 after normalising the predictions","cadc65b7":"I first came up with this custom, yet questionable method of aggregating predictions in previous month's TPS- https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/272825).","3402896b":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Normalising The Predictions<\/center><\/h1>\n<\/div>","5a64bee0":"kailai brought up this method in the following discussion - https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/276069","bd829866":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Weighted Averaging<\/center><\/h1>\n<\/div>","a4c60734":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Geometric Mean<\/center><\/h1>\n<\/div>","acb91f69":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Min Max of Predictions<\/center><\/h1>\n<\/div>","41e5441f":"Hi and welcome to a tutorial on how to make the best of the best in any competition.\nSince the last couple of months, I have been merely looking at how competitors would blend best model submissions and push their scores up the leaderboard.\n\nThough the TPS is strictly for learning, I am making this notebook to introduce myself and everyone to the interesting world of blending.\n\nI have taken the best submissions across the following models and publinc notebooks-\n1. Single HGBM - https:\/\/www.kaggle.com\/ankitkalauni\/tps-21-oct-single-histgbm-0-85651\n2. Single XGBoost - https:\/\/www.kaggle.com\/mohammadkashifunique\/tsp-single-xgboost-model\n4. LightGBM - https:\/\/www.kaggle.com\/mlanhenke\/tps-10-lgbm-onemodel-threeseeds-blend\n5. Stacking XGB, CB and LGB - https:\/\/www.kaggle.com\/ankitkalauni\/simple-overfitted-stacking-lgbm-xgb-cb\n\n**Please do upvote their beautiful work and follow them for more :)**","c703f676":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Make a DataFrame of all submissions<\/center><\/h1>\n<\/div>","3fe03678":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Blending for Noobs<\/center><\/h1>\n<\/div>","908f42d7":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Summary and Take-aways<\/center><\/h1>\n<\/div>","c804061f":"I would also thank VASILEIOS KONSTANTAKOS for making an illuminating notebook on the same topic - https:\/\/www.kaggle.com\/vkonstantakos\/blending-weighted-average","9a67d8ab":"After this experiment where I tried different ways of aggregating the predictions, I am mentioning the summary and my next action steps-\n\n1. Weighted Average - 0.85652 - BEST\n2. Power Averaging - 0.85651\n3. Geometric Mean - 0.85650\n4. MinMax aggregation - 0.85639 - WORST\n\nNext step would be to re-tune some of the models I have mentioned (by changing ranges in Optuna study) or drop 1-2 of the worst ones, keeping only the best ones for the next experiment.\n\n**Do share with me any other methods I can try and upvote if you found this useful :)**","643887bb":"My immense gratitude to Edrick Kesuma for creating this notebook on the topic of Power Averaging, in the previous month's TPS - https:\/\/www.kaggle.com\/edrickkesuma\/power-averaging-is-your-friend","4f7b06e8":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Power Averaging<\/center><\/h1>\n<\/div>","50fcbc21":"<div style=\"background-color:rgba(5, 29, 31, 0.5);\">\n    <h1><center>Importing Packages and Submissions<\/center><\/h1>\n<\/div>"}}