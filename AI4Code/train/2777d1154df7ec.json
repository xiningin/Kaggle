{"cell_type":{"8671578f":"code","6d151af2":"code","af057c16":"code","283ec667":"code","dba76730":"code","bcb1bd82":"code","0fce36e4":"code","e2451261":"code","6fbcdd3d":"code","9e665497":"code","afe587df":"code","89fd3bc6":"code","83745ecc":"code","fd01b636":"code","6a3dc911":"code","d2a432dd":"code","9fd1db9c":"code","b807d0e1":"code","69294286":"code","00037e4f":"code","602f5539":"code","4c92bcaf":"code","e71a90b3":"code","0ad49c50":"code","276729dc":"code","72e6e493":"code","7bec2b5b":"code","cf422462":"code","aa236f36":"code","d95796f6":"code","75cb84c5":"code","a79ddf4f":"code","2cfda6b3":"code","28fca8a7":"code","2c8c8541":"code","4681796b":"code","f04e4a3f":"code","b1083856":"code","441f37f6":"code","325d06c8":"code","c1df31fb":"code","f762d940":"code","ace132a6":"code","8dbde3e5":"code","c93c4a6f":"code","4a18b749":"code","6790e31a":"code","350793f3":"code","7898d178":"code","004eb9ae":"code","46cdb544":"code","58825a9a":"code","1964afea":"code","791cdb1a":"code","8bf09386":"code","114e8ddb":"code","16b44642":"code","5b359d0e":"code","c10dc9e8":"code","3bbcba85":"code","061e8370":"code","b74dc25f":"code","842ad559":"code","e9291cf9":"code","48152d28":"code","27f0ad1b":"code","d71e4f03":"code","e53db397":"code","54cebb52":"code","31feb5e9":"markdown","e4ca32a5":"markdown","6f0dc5ee":"markdown","d1d895b0":"markdown","a15e10f3":"markdown","5b244241":"markdown","b290a8f6":"markdown","e6074976":"markdown","b65ac59c":"markdown","7d254785":"markdown","2503da35":"markdown","e3887bb9":"markdown","1b17c642":"markdown","1b82f5e6":"markdown","263dfc94":"markdown","4b3c02b5":"markdown","45d70e45":"markdown","2a87f4bb":"markdown","926c8ac0":"markdown","9fc7157f":"markdown","832f51a9":"markdown","258371d7":"markdown","311597c3":"markdown","b6641625":"markdown","788c1532":"markdown","15df93eb":"markdown","f9bbef05":"markdown","3746c506":"markdown","7a350727":"markdown","5ec9184f":"markdown","6c6b9c57":"markdown","8413a4b0":"markdown","9dfabe9d":"markdown","890eebc5":"markdown","1f79473a":"markdown","a1846c12":"markdown","027c13a3":"markdown","a84ff026":"markdown","ae785a28":"markdown","d2622dfa":"markdown","aba371d3":"markdown","570922c6":"markdown","d23e1d3e":"markdown","dddb85a3":"markdown","a7fad5fd":"markdown","3971674b":"markdown","a1ae7d24":"markdown","3768ed22":"markdown"},"source":{"8671578f":"!pip install seaborn --upgrade\nimport pandas as pd # data manipulation\nimport numpy as np # linear algebra\n\n# data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import qqplot # normality check\nimport plotly.express as px\nfrom sklearn.tree import plot_tree # decision tree \n\n# data preprocessing\nfrom imblearn.over_sampling import SMOTE # deal with imbalance data\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer # scale data\n\n# classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression # linear classification\nfrom sklearn.svm import LinearSVC, SVC # support vector machines\nfrom sklearn.tree import DecisionTreeClassifier # tree based\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,\\\nAdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\n\n# model evaluation and selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import classification_report, plot_roc_curve\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, accuracy_score, roc_auc_score","6d151af2":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","af057c16":"df.shape","283ec667":"df.info()","dba76730":"numeric = ['age', 'creatinine_phosphokinase', \n           'ejection_fraction', 'platelets', \n           'serum_creatinine', 'time']\ncategorical = ['anaemia', 'diabetes', 'high_blood_pressure', \n               'sex', 'smoking']","bcb1bd82":"df.age = df.age.astype('int64')","0fce36e4":"df.isnull().sum()","e2451261":"target_count = df.DEATH_EVENT.value_counts()\ndeath_color = ['navy', 'crimson']\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(6, 5))\n    sns.countplot(data=df, x='DEATH_EVENT', palette=death_color)\n    for name , val in zip(target_count.index, target_count.values):\n        plt.text(name, val\/2, f'{round(val\/sum(target_count)*100, 2)}%\\n({val})', ha='center',\n                color='white', fontdict={'fontsize':13})\n    plt.xticks(ticks=target_count.index, labels=['No', 'True'])\n    plt.yticks(np.arange(0, 230, 25))\n    plt.show()","6fbcdd3d":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, numeric)):\n        plt.subplot(3, 3, i+1)\n        sns.histplot(data=df, x=name, color=col)\n    plt.suptitle('Histograms of Numeric features', fontsize=15)","9e665497":"fig, axes = plt.subplots(6, 2, figsize=(10, 20))\nplt.subplots_adjust(hspace=0.4)\naxes = axes.ravel()\nfor i, name, col in zip(np.arange(0, 14, 2), numeric, colors):\n    sns.boxplot(data=df, x=name, ax=axes[i], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=True)\n    sns.boxplot(data=df, x=name, ax=axes[i+1], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=False)\nplt.suptitle('Boxplot of Numeric features with repect to the target class\\n(with and without outliers)', \n             fontsize=15)\nplt.show()","afe587df":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(12, 15))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, categorical)):\n        plt.subplot(3, 2, i+1)\n        sns.countplot(data=df, x=name, hue='DEATH_EVENT')","89fd3bc6":"X = df.iloc[:, :-1]\ny = df['DEATH_EVENT']","83745ecc":"print(X.shape)\nprint(y.shape)","fd01b636":"smote = SMOTE(random_state=2021, n_jobs=-1, k_neighbors=5)\nsmote.fit(X, y)\nX_smote, y_smote = smote.fit_resample(X, y)","6a3dc911":"print(X_smote.shape)\nprint(y_smote.shape)","d2a432dd":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nqqplot(df.creatinine_phosphokinase, fit=True, line='45', ax=ax[0])\nax[0].set_title('before transformation')\nqqplot(np.log10(df.creatinine_phosphokinase), fit=True, line='45', ax=ax[1])\nax[1].set_title('after transformation')\nplt.suptitle('q-q plot for creatinine_phosphokinase', fontweight='bold')\nplt.show()","9fd1db9c":"p = -1\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nqqplot(df.serum_creatinine, fit=True, line='45', ax=ax[0])\nax[0].set_title('before transformation')\n\nqqplot(df.serum_creatinine**p, fit=True, line='45', ax=ax[1])\nax[1].set_title('after transformation')\n\nplt.suptitle('q-q plot for serum_creatinine', fontweight='bold')\nplt.show()","b807d0e1":"pt = PowerTransformer(method='yeo-johnson')\nX_pt = pt.fit_transform(X_smote)","69294286":"mm = MinMaxScaler()\nX_scaled = mm.fit_transform(X_pt)","00037e4f":"pd.DataFrame(X_scaled, columns=X.columns).hist(figsize=(10, 10))\nplt.show()","602f5539":"rf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, \n                            class_weight='balanced', random_state=2021)\nrf.fit(X_scaled, y_smote)","4c92bcaf":"feature_imp = pd.DataFrame(np.round(rf.feature_importances_*100, 2), index=X.columns, columns=['importance%'])\nfeature_imp = feature_imp.sort_values(by='importance%', ascending=False)\nfeature_imp.plot(kind='barh', figsize=(8, 5))\nplt.xlabel('percentage')\nplt.show()","e71a90b3":"imp_features = feature_imp.index[:3]\nimp_features","0ad49c50":"X_selected = pd.DataFrame(X_scaled, columns=X.columns)[imp_features]","276729dc":"X_selected","72e6e493":"model_data = X_selected\nmodel_data['target'] = y_smote","7bec2b5b":"model_data","cf422462":"px.scatter_3d(model_data, x='time', y='serum_creatinine', z='ejection_fraction', color='target')","aa236f36":"X_train, X_test, y_train, y_test = train_test_split(model_data.drop(['target'], axis=1), \n                                                    model_data['target'], \n                                                    test_size=0.25, \n                                                    random_state=2021, \n                                                    stratify=model_data['target']\n                                                   )\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","d95796f6":"knn_clf = KNeighborsClassifier(n_jobs=-1)","75cb84c5":"knn_params = {\n    'n_neighbors': np.arange(2, 12)\n}\nknn_cv = GridSearchCV(knn_clf, knn_params, scoring='f1', n_jobs=-1, cv=10)\nknn_cv.fit(X_train, y_train)","a79ddf4f":"knn_cv.best_params_","2cfda6b3":"knn_train_pred = cross_val_predict(knn_cv, X_train, y_train, cv=10, n_jobs=-1)","28fca8a7":"print(classification_report(y_train, knn_train_pred, digits=4, target_names=['not gonna die', 'will die']))","2c8c8541":"lr_clf = LogisticRegression(class_weight='balanced', random_state=2021, n_jobs=-1)","4681796b":"lr_params = {\n    'penalty': ['l1', 'l2', 'elasticnet'], \n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], \n    #'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\nlr_cv = GridSearchCV(lr_clf, lr_params, scoring='f1', cv=10, n_jobs=-1)\nlr_cv.fit(X_train, y_train)","f04e4a3f":"lr_cv.best_params_","b1083856":"lr_train_pred = cross_val_predict(lr_cv, X_train, y_train, cv=10, n_jobs=-1)","441f37f6":"print(classification_report(y_train, lr_train_pred, digits=4, target_names=['not gonna die', 'will die']))","325d06c8":"lin_svm_clf = SVC(kernel='linear', class_weight='balanced', random_state=2021)","c1df31fb":"params = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nlin_svm_cv = GridSearchCV(lin_svm_clf, params, scoring='f1', n_jobs=-1, cv=10)\nlin_svm_cv.fit(X_train, y_train)","f762d940":"lin_svm_cv.best_params_","ace132a6":"lin_svm_train_pred = cross_val_predict(lin_svm_cv, X_train, y_train, cv=10, n_jobs=-1)","8dbde3e5":"print(classification_report(y_train, lin_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","c93c4a6f":"rbf_svm = SVC(kernel='rbf', class_weight='balanced')","4a18b749":"params = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nrbf_svm_cv = GridSearchCV(rbf_svm, params, scoring='f1', n_jobs=-1, cv=10)\nrbf_svm_cv.fit(X_train, y_train)","6790e31a":"rbf_svm_cv.best_params_","350793f3":"rbf_svm_train_pred = cross_val_predict(rbf_svm_cv, X_train, y_train, cv=10, n_jobs=-1)","7898d178":"print(classification_report(y_train, rbf_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","004eb9ae":"dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021)","46cdb544":"params = {\n    'criterion': ['gini', 'entropy'], \n    'max_depth': np.arange(2, 22, 2), # depth of tree\n    'min_samples_split': [2, 3, 4], # min. no. of samples a node must have before it splits \n    'min_samples_leaf': [1, 2, 3, 4] # min. non of samples a leaf node must have\n}\ndt_cv = GridSearchCV(dt_clf, params, scoring='f1', n_jobs=-1, cv=10)\ndt_cv.fit(X_train, y_train)","58825a9a":"dt_cv.best_params_","1964afea":"dt_train_pred = cross_val_predict(dt_cv, X_train, y_train, cv=10, n_jobs=-1)","791cdb1a":"best_dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021, \n                                    max_depth=4, criterion='entropy', min_samples_split=2, \n                                     min_samples_leaf= 1)","8bf09386":"best_dt_clf.fit(X_train, y_train)","114e8ddb":"plt.figure(figsize=(15, 6))\nplot_tree(best_dt_clf, filled=True, \n          #feature_names=['time', 'serum_creatinine', 'ejection_fraction']\n         )\nplt.show()","16b44642":"rf = RandomForestClassifier(n_jobs=-1, random_state=2021, class_weight='balanced')","5b359d0e":"params = {\n    #'n_estimators': [100, 200, 300], \n    'max_depth': np.arange(2, 22, 1), \n    #'min_samples_split': [2, 3, 4], \n    #'min_samples_leaf': [1, 2, 3, 4], \n    'criterion': ['gini', 'entropy']\n}\nrf_cv = RandomizedSearchCV(rf, params, scoring='f1', n_jobs=-1, cv=10, random_state=2021, n_iter=20)\nrf_cv.fit(X_train, y_train)","c10dc9e8":"rf_cv.best_params_","3bbcba85":"rf_train_pred = cross_val_predict(rf_cv, X_train, y_train, cv=10, n_jobs=-1, verbose=1)","061e8370":"print(classification_report(y_train, rf_train_pred, digits=4, target_names=['not gonna die', 'will die']))","b74dc25f":"models = ['kNN', 'Logistic Regression', 'Linear SVM', 'Non-Linear SVM', \n          'Decision Tree', 'Random Forest']\nmodel_colors = sns.color_palette(\"Dark2\")\naccuracy = []\nrecall = []\nprecision = []\nf1 = []\nauc = []\npredictions = [knn_train_pred, lr_train_pred, lin_svm_train_pred, \n               rbf_svm_train_pred, dt_train_pred, rf_train_pred]\n\nfor model_pred in predictions:\n    accuracy.append(accuracy_score(y_train, model_pred))\n    precision.append(precision_score(y_train, model_pred))\n    recall.append(recall_score(y_train, model_pred))\n    f1.append(f1_score(y_train, model_pred))\n    auc.append(roc_auc_score(y_train, model_pred))","842ad559":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, accuracy, color=model_colors)\n    for m, a in zip(models, accuracy):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Accuracy percentage (%)')\n    plt.title('Model comparison on training data using Accuracy')\n    plt.show()","e9291cf9":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, recall, color=model_colors)\n    for m, a in zip(models, recall):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Recall percentage (%)')\n    plt.title('Model comparison on training data using Recall')\n    plt.show()","48152d28":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, precision, color=model_colors)\n    for m, a in zip(models, precision):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Precision percentage (%)')\n    plt.title('Model comparison on training data using Precision')\n    plt.show()","27f0ad1b":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, f1, color=model_colors)\n    for m, a in zip(models, f1):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('F1 percentage (%)')\n    plt.title('Model comparison on training data using F1 score')\n    plt.show()","d71e4f03":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, auc, color=model_colors)\n    for m, a in zip(models, auc):\n        plt.text(m, a+0.01 , round(a, 3), ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('AUC')\n    plt.title('Model comparison on training data using AUC')\n    plt.show()","e53db397":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nfor name, model in zip(models, best_models):\n    best_predictions = model.predict(X_test)\n    print(name.upper())\n    print(classification_report(y_test, best_predictions))\n    print(\"-------------------------------------------------------------\")","54cebb52":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    for name, model in zip(models, best_models):\n        best_predictions = model.predict(X_test)\n        fpr, tpr, thresholds = roc_curve(y_test, best_predictions)\n        plt.plot(fpr, tpr, linewidth=2, label=name)\n        plt.plot([0, 1], [0, 1], 'k--') \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Reccll)')\n    plt.legend()\n    plt.show()","31feb5e9":"* The qq plot below shows the effect of log trasformation on creatinine_phosphokinase. QQ plot (or quantile-quantile plot) is a plot where the axes are purposely transformed in order to make a normal (or Gaussian) distribution appear in a straight line. In other words, a perfectly normal distribution would exactly follow a line with slope = 1 and intercept = 0.","e4ca32a5":"## 3.4 Decision Tree\n**Pros**\n- requires very little data preparation and doesn't require feature scaling or centering.\n- simple to understand and iterpret.\n\n**Cons**\n- Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. It is very likely that the model will not generalize well. One way to limit this problem is to use Principal Component Analysis which often results in a better orientation of the training data.\n- the main issue with Decision Trees is that they are very sensitive to small variations in the training data. Random Forests can limit this instability by averaging predictions over many trees.","6f0dc5ee":"## Feature Selection using Random Forest","d1d895b0":"![](https:\/\/www.lifespan.io\/wp-content\/uploads\/2017\/10\/shutterstock_488843971.jpg)","a15e10f3":"### Linear SVM classification (Hard Margin)\nAs seen from the 3D plot of the data. The problem doesnt not look like it can be separated using a hard margin svm since there is some noise in both the classes.","5b244241":"**1. Age:** age of patient (in years)\n\n**2. Anaemia:** Decrease of red blood cells or hemoglobin\n\n**3. High blood pressure:** If a patient has hypertension\n\n**4. Creatinine phosphokinase:** Level of the CPK enzyme in the blood (mcg\/L)\n\n**5. Diabetes:** If the patient has diabetes\n\n**6. Ejection fraction:** Percentage of blood leaving the heart at each contraction\n\n**7. Sex:** Woman or man\n\n**8. Platelets:** Platelets in the blood (kiloplatelets\/mL)\n\n**9. Serum creatinine:** Level of creatinine in the blood (mg\/dL)\n\n**10. Serum sodium:** Level of sodium in the blood (mEq\/L)\n\n**11. Smoking:** If the patient smokes\n\n**12. Time:** Follow-up period (in days)\n\n**13. (target) death event:** If the patient died during the follow-up period","b290a8f6":"# Heart <font color=\"red\">Failure<\/font> Clinical Records\nreached 99% precision! What did it cost? Recall","e6074976":"## Fix age data type","b65ac59c":"# <a id=predict>5. Predictions on Test Data<\/a>\n[Back to index](#toc)","7d254785":"### DT Visualized","2503da35":"### Predictions on training data using cross val predict\ncross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (\u201cclean\u201d meaning that the prediction is made by a model that never saw the data during training).","e3887bb9":"## Model comparison on test data using ROC Curve\n- the ROC curve, plots the true positive rate (another name for recall) against the false positive rate (FPR). \n- Once again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible\n- One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n- Random forest has the highest auc.","1b17c642":"## Missing Values\nThere are no missing values","1b82f5e6":"`NOTE` similar approach is used for all classifiers below","263dfc94":"# <a id=toc>Table of contents<\/a>\n1. [Data Exploration](#eda)\n2. [Data Prepartion](#data_prep)\n3. [Data Modelling and Hyperparameter Tuning](#model)\n4. [Model Evaluation](#eval)\n5. [Prediction on Test Data](#predict)\n6. [Conclusion](#conclude)","4b3c02b5":"## Distribution of features after transformation and scaling","45d70e45":"# <a id=data_prep>2. Data Preparation<\/a>\n[Back to index](#toc)","2a87f4bb":"### Distribution of Numeric Features\n- features `creatinine_phosphokinase` and `serum_creatinine` are extremely positive or right skewed","926c8ac0":"### Hyperparameter tuning","9fc7157f":"## 3.3 Support Vector Machine (SVM)","832f51a9":"**2. serum_creatinine** using reciprocal transform (p = -1). This transformation has a radical effect as it reverses the order among the values of same sign, therefore, larger values become smaller and visa verse","258371d7":"### Distribution Categorical Features w.r.t target class","311597c3":"## Data types","b6641625":"## Data Transformation\nDuring EDA for the numeric features, the histograms of few features indicated skewness. Some of the features like `creatinine_phosphokinase` and `serum_creatinine` were extremely skewed. Skewed features like these can be made more Gaussian-like using power transforms or log transforms. For example: \n\n**1. creatinine_phosphokinase** using the log transformation can make data conform to normality. In this case log-transform does remove or reduce the skewness since the original data follows a log-normal distribution or approximately so. ","788c1532":"### Target\n- The target class or label is imbalanced","15df93eb":"## Dataset and feature description","f9bbef05":"## Fix Class Imbalance using SMOTE\nSMOTE is an oversampling technique where the synthetic samples are generated for the minority class, in our case, 1's ","3746c506":"###  Classification report","7a350727":"## 3.1 k-Nearest Neighbour Classifier\nLet start with  simple and a lazy learner where an object is classified by a plurality vote of its neighbors.","5ec9184f":"### Classification Report of training data","6c6b9c57":"### Best parameter value to achieve the highest F1 score","8413a4b0":"### Optimum value to maximize performance\nTherefore, highest f1 score is achieved by knn using 9 neighbours. ","9dfabe9d":"## 3.2 Logistic Regression\nLogistic regression uses logit function to compute the probability of the outcomes, in our case, the target class `Death Event`","890eebc5":"# <a id=eda>1. Data Exploration<\/a>\n[Back to index](#toc)","1f79473a":"# Problem Description\n- To create a model in order to predict the likelihood of a patient dying due to heart failure.\n- This a binary clasification problem since the target class (Death Event) consists of two classes True or False","a1846c12":"## 3.5 Random Forest\n- It is an ensemble(group of predictors) of decision trees.\n- It has all the hyperparameters of the decision tree","027c13a3":"The power transformer of sklearn learn provides two methods to make the distribution gaussian-like\n- Boxcox\n- Yeo-johnson\n\nBoth these methods searches for the right value of p (just like in the above example) in order to make the distribution normal. Yeo-johnson is a upgraded version of Boxcox as it deals with the data with negative values","a84ff026":"### Hyper-parameter tuning using Grid Search","ae785a28":"# <a id=conclude>Conclusion<\/a>\n[Back to index](#toc)","d2622dfa":"## Data shape","aba371d3":"- Instead of all the 13 features, only 3 features `time`, `serum_creatinine` and `ejection_fraction` are sufficient to model the data. Using top 7 or all the features resulted in overfitting. \n- Random Forest has the best performance as compared to other models on both train and test data","570922c6":"## Normalise Data\nFinally, normalise the data using the min max scaler which scales the data to the 0-1 range. Scaling is required for ML algo like SVM, Logistic regression, knn which are sensitive to scaling and outliers (applicable for both classification and regression problems).","d23e1d3e":"### Predictions on training data using cross val predict","dddb85a3":"# <a id=model>3. Data Modelling and Hyperparameter Tuning<\/a>\n[Back to index](#toc)\n\nTherefore, the goal is now to separate both the classes as shown the figure below\n\n`Note` All the classifiers have been tuned to maximize the f1 score instead of accuracy. F1 score is the harmonic mean of recall and precision. This score will favor classifiers with a similar precision and recall. I could have achieved high recall or precision but unfortunately, we cannot have it both ways as increasing precision reduces recall, and visa verse","a7fad5fd":"### Non-Linear Classification (Soft Margin)\nUsing rbf kernal","3971674b":"## Separate features and target class","a1ae7d24":"## EDA","3768ed22":"# <a id=eval>4. Model Evaluation<\/a>\n[Back to index](#toc)\n\nRandom Forest has outperformed all the other classifers in accuracy, precision, recall, f1 score and auc score. "}}