{"cell_type":{"8ddd2b93":"code","95f9e3f1":"code","a373cfd1":"code","85198714":"code","c38099db":"code","70d0232e":"code","262ce01d":"code","a81e5822":"code","4001ecb9":"code","c1f8473f":"code","1f9ec13b":"code","3dff9c49":"code","d9b5d97b":"code","38100357":"code","f22437b6":"code","dcb06499":"code","9cb28274":"code","061a7292":"code","60abe3d2":"code","4216c4e1":"code","a9247d2f":"markdown","17a89556":"markdown","10c74c1c":"markdown","87f89d4e":"markdown","a8c0519b":"markdown","d71a5f72":"markdown","f336bf8d":"markdown","e29450cb":"markdown","1421b54c":"markdown","d35faf42":"markdown","5488324c":"markdown","6e0fbfa8":"markdown","77d0b16a":"markdown","d0f3953e":"markdown","df6ec9db":"markdown","18775081":"markdown"},"source":{"8ddd2b93":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport random\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom glob import glob\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom skimage.feature import hog\n\nimport PIL\nimport cv2\nimport pickle\n\n","95f9e3f1":"car_paths = glob(\"..\/input\/the-car-connection-picture-dataset\"+\"\/*\")[:5000]\nneg_paths = []\n\nfor class_path in glob(\"..\/input\/natural-images\/natural_images\"+\"\/*\"):\n    if class_path != \"..\/input\/natural-images\/natural_images\/car\":\n        paths = random.choices(glob(class_path+\"\/*\"),k=700)\n        neg_paths = paths + neg_paths\n        \nprint(\"There are {} car images in the dataset\".format(len(car_paths)))\nprint(\"There are {} negative images in the dataset\".format(len(neg_paths)))\n","a373cfd1":"car_paths[:5]","85198714":"neg_paths[:5]","c38099db":"example_image = np.asarray(PIL.Image.open(car_paths[0]))\nhog_features,visualized = hog(example_image,orientations=9,pixels_per_cell=(16,16),\n                              cells_per_block=(2,2),\n                              visualize=True,\n                              multichannel=True\n                             )\n\nfig = plt.figure(figsize=(12,6))\nfig.add_subplot(1,2,1)\nplt.imshow(example_image)\nplt.axis(\"off\")\nfig.add_subplot(1,2,2)\nplt.imshow(visualized,cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()","70d0232e":"hog_features.shape","262ce01d":"pos_images = []\nneg_images = []\n\npos_labels = np.ones(len(car_paths))\nneg_labels = np.zeros(len(neg_paths))\n\nstart = time.time()\n\nfor car_path in car_paths:    \n    img = np.asarray(PIL.Image.open(car_path))\n    # We don't have to use RGB channels to extract features, Grayscale is enough.\n    img = cv2.cvtColor(cv2.resize(img,(96,64)),cv2.COLOR_RGB2GRAY)\n    img = hog(img,orientations=9,pixels_per_cell=(16,16),\n              cells_per_block=(2,2)\n             )\n    \n    pos_images.append(img)\n\nfor neg_path in neg_paths:\n    img = np.asarray(PIL.Image.open(neg_path))\n    img = cv2.cvtColor(cv2.resize(img,(96,64)),cv2.COLOR_RGB2GRAY)\n    img = hog(img,orientations=9,pixels_per_cell=(16,16),\n              cells_per_block=(2,2)\n             )\n    \n    neg_images.append(img)\n    \nx = np.asarray(pos_images + neg_images)\ny = np.asarray(list(pos_labels) + list(neg_labels))\n    \nprocessTime = round(time.time()-start,2)\nprint(\"Reading images and extracting features has taken {} seconds\".format(processTime))\n\nprint(\"Shape of image set\",x.shape)\nprint(\"Shape of labels\",y.shape)","a81e5822":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","4001ecb9":"# Creating a SVC object\nsvc = SVC()\n\n# We'll use Cross Validation Grid Search to find best parameters.\n# Classifier will be trained using each parameter \nsvc.fit(x_train,y_train)\n\ny_pred = svc.predict(x_test)\nprint(\"Accuracy score of model is \",accuracy_score(y_pred=y_pred,y_true=y_test)*100)\n","c1f8473f":"def slideExtract(image,windowSize=(96,64),channel=\"RGB\",step=12):\n    \n    # Converting to grayscale\n    if channel == \"RGB\":\n        img = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n    elif channel == \"BGR\":\n        img = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    elif channel.lower()!=\"grayscale\" or channel.lower()!=\"gray\":\n        raise Exception(\"Invalid channel type\")\n    \n    # We'll store coords and features in these lists\n    coords = []\n    features = []\n    \n    hIm,wIm = image.shape[:2] \n\n    \n    # W1 will start from 0 to end of image - window size\n    # W2 will start from window size to end of image\n    # We'll use step (stride) like convolution kernels.\n    for w1,w2 in zip(range(0,wIm-windowSize[0],step),range(windowSize[0],wIm,step)):\n       \n        for h1,h2 in zip(range(0,hIm-windowSize[1],step),range(windowSize[1],hIm,step)):\n            window = img[h1:h2,w1:w2]\n            features_of_window = hog(window,orientations=9,pixels_per_cell=(16,16),\n                                     cells_per_block=(2,2)\n                                    )\n            \n            coords.append((w1,w2,h1,h2))\n            features.append(features_of_window)\n    \n    return (coords,np.asarray(features))","1f9ec13b":"example_image = np.asarray(PIL.Image.open(\"..\/input\/the-car-connection-picture-dataset\/Acura_ILX_2013_28_16_110_15_4_70_55_179_39_FWD_5_4_4dr_Mro.jpg\"))\ncoords,features = slideExtract(example_image,channel=\"RGB\")\n\ncoords[:5]","3dff9c49":"features.shape","d9b5d97b":"from sklearn.preprocessing import MinMaxScaler\n\nclass Heatmap():\n    \n    def __init__(self,original_image):\n        \n        # Mask attribute is the heatmap initialized with zeros\n        self.mask = np.zeros(original_image.shape[:2])\n    \n    # Increase value of region function will add some heat to heatmap\n    def incValOfReg(self,coords):\n        w1,w2,h1,h2 = coords\n        self.mask[h1:h2,w1:w2] = self.mask[h1:h2,w1:w2] + 30\n    \n    # Decrease value of region function will remove some heat from heatmap\n    # We'll use this function if a region considered negative\n    def decValOfReg(self,coords):\n        w1,w2,h1,h2 = coords\n        self.mask[h1:h2,w1:w2] = self.mask[h1:h2,w1:w2] - 30\n    \n    def compileHeatmap(self):\n        \n        # As you know,pixel values must be between 0 and 255 (uint8)\n        # Now we'll scale our values between 0 and 255 and convert it to uint8\n        \n        # Scaling between 0 and 1 \n        scaler = MinMaxScaler()\n        \n        self.mask = scaler.fit_transform(self.mask)\n        \n        \n        # Scaling between 0 and 255\n        self.mask = np.asarray(self.mask * 255).astype(np.uint8)\n        \n        # Now we'll threshold our mask, if a value is higher than 170, it will be white else\n        # it will be black\n        self.mask = cv2.inRange(self.mask,170,255)\n        \n        return self.mask","38100357":"example_mask = np.zeros((200,200))\nexample_mask[70:100,60:120] = 255\n\nplt.imshow(example_mask,cmap=\"gray\")\nplt.show()","f22437b6":"# Find contours function of openCV will help us the find white regions.\ncontours, hierarchy = cv2.findContours(example_mask.astype(np.uint8),1,2)[-2:]\nfor c in contours:\n    if cv2.contourArea(c) < 10*10:\n        continue\n    (x,y,w,h) = cv2.boundingRect(c)\n    \n    rgb_ver = cv2.cvtColor(example_mask.astype(np.uint8),cv2.COLOR_GRAY2RGB)\n    im = cv2.rectangle(rgb_ver,(x,y),(x+w,y+h),(255,0,0),3)\n\nplt.imshow(im)","dcb06499":"def detect(image):\n    \n    # Extracting features and initalizing heatmap\n    coords,features = slideExtract(image)\n    htmp = Heatmap(image)\n    \n    \n    for i in range(len(features)):\n        # If region is positive then add some heat\n        decision = svc.predict([features[i]])\n        if decision[0] == 1:\n            htmp.incValOfReg(coords[i])\n            # Else remove some heat\n        else:\n            htmp.decValOfReg(coords[i])\n    \n    # Compiling heatmap\n    mask = htmp.compileHeatmap()\n    \n    cont,_ = cv2.findContours(mask,1,2)[:2]\n    for c in cont:\n        # If a contour is small don't consider it\n        if cv2.contourArea(c) < 70*70:\n            continue\n        \n        (x,y,w,h) = cv2.boundingRect(c)\n        image = cv2.rectangle(image,(x,y),(x+w,y+h),(255),2)\n    \n    return image","9cb28274":"detected = detect(np.asarray(PIL.Image.open(\"..\/input\/the-car-connection-picture-dataset\/Acura_ILX_2013_28_16_110_15_4_70_55_179_39_FWD_5_4_4dr_ylA.jpg\")\n                            ))\nplt.imshow(detected)","061a7292":"import io\nimport requests\n\nbyte_img = requests.get(\"https:\/\/images2.minutemediacdn.com\/image\/upload\/c_crop,h_843,w_1500,x_0,y_69\/f_auto,q_auto,w_1100\/v1554995310\/shape\/mentalfloss\/istock-472964014.jpg\").content\nbyte_img = io.BytesIO(byte_img)\n\nimg = np.asarray(PIL.Image.open(byte_img))\n\ndetected = detect(img)\nplt.imshow(detected)\nplt.axis(\"off\")\nplt.show()","60abe3d2":"byte_img = requests.get(\"https:\/\/i.vimeocdn.com\/video\/551464436_640x360.jpg\").content\nbyte_img = io.BytesIO(byte_img)\n\nimg = np.asarray(PIL.Image.open(byte_img))\n\ndetected = detect(img)\nplt.imshow(detected)\nplt.axis(\"off\")\nplt.show()","4216c4e1":"byte_img = requests.get(\"https:\/\/icdn2.digitaltrends.com\/image\/digitaltrends\/2020-volvo-xc90-r-banff-500x500.jpg\").content\nbyte_img = io.BytesIO(byte_img)\n\nimg = np.asarray(PIL.Image.open(byte_img))\n\ndetected = detect(img)\nplt.imshow(detected)\nplt.axis(\"off\")\nplt.show()","a9247d2f":"* Let's try our sliding window function in an image.","17a89556":"# Introduction\nHello people, welcome to this kernel. In this kernel I am going to build a **car** object detector using **support vector machine classifier** and **sliding window localization concept**. Before starting, let's take a look at the table of content\n\n# Table of Content\n1. Importing Libraries and Data Preprocessing\n1. Step 1: Building Classifier\n1. Step 2: Sliding Window and Heatmap\n    1. What are they?\n        * Sliding Window \n        * Heatmap\n    1. Building Sliding Window and Heatmap\n1. Combining Everything\n1. Results","10c74c1c":"# Combining Everything\nIn this section we'll combine everything in a function. This function will take an image and return the detected representation of image.\nBut before starting, I want to show you how we detect whites in a mask. ","87f89d4e":"# Step 2: Sliding Window and Heatmap\n\n### What are they:\n\n**Sliding Window** \n\nIn object detection tasks, hardest thing is localizing objects. There are several ways to do it, but in this kernel we'll use the primitive one: Sliding window.\n\nIn sliding window, we'll split the images by 96x64 boxes. As you can remind, we've used 96x64 images while training classifier as well. And after that, we'll classify all the boxes. If a box is positive (includes car), it will be a potential region. Let's take a look at the images below:\n\n![slidingwindow.jpg](attachment:slidingwindow.jpg)\n![](https:\/\/coe.northeastern.edu\/Research\/rcl\/projects\/SWO\/swo.gif)\n\n* First image is a static representation of sliding window, but we will use a sliding window like the sliding \nwindow in second image.\n* Don't worry if you don't understand completely, you will definetely understand in coding section.\n\n**Heatmap**\n\nBut if we accept that all the regions that classified positive (there is a car) are true, there would be a lot of false detections. Such as:\n\n![falsepositive.jpg](attachment:falsepositive.jpg)\n\nIn order to solve this problem we'll use heatmaps. In heatmap, we'll create a black image (all pixels will be zero) with shape of original image. And if detect a car in a region, we'll add some value to regions's pixel in the black image. Let's take a look at the images below:\n\n![heatmap1.jpg](attachment:heatmap1.jpg)\n![heatmap2.png](attachment:heatmap2.png)\n","a8c0519b":"# Importing Libraries and Data Preprocessing\nIn this section we'll import libraries and prepare data for building classifier. ","d71a5f72":"* As you can see, we've resized all images to 96x64 because bigger or smaller than 96x64 would be bad for detecting cars.\n\n\n**And now we came to the final step, splitting our dataset into train and test.**","f336bf8d":"* And everything is ready.","e29450cb":"# Step 1: Building Classifier\nOur data is ready, now we'll build a Support Vector Machine classifier using sklearn. ","1421b54c":"Now we can start to define our function.","d35faf42":"# Result\nIn this kernel I've built an object detector using support vector machine classification and sliding windows. This system is not bad, but working with high resolution images takes so many time, so its not as useful as haar cascades.\n\nThanks for your attention.","5488324c":"### Determining paths of images:","6e0fbfa8":"* As we can see, region 1 is just predicted as positive for one time, but region 2 is predicted a lot of times positive. So we won't consider region 1 as a car. We'll just consider region 2.","77d0b16a":"### Reading images and extracting features:\nIn image classification tasks, we don't use images directly, because some features of images such as colors are generally redundant and expensive to consider. So we use feature extraction techniques. In deep learning based approachs we use convolution kernels (learnt filters) to extract features and in machine learning based approachs we use statistics based approachs such as HOG (Histogram of Oriented Gradients) Let me show you how hog feature extracted image look like.","d0f3953e":"* As we can see there are 285 windows in a 320x240 image. That's why sliding window concept is slow.\n\nOur sliding window is ready, now we'll define our heatmap. It will be a class.","df6ec9db":"## Building sliding window and heatmap:\n\n* First, we'll define a function that slides over an image and returns coords of the cells and feature extracted cells \n\n(such as [coords=(w1=60,w2=154,h1=90,h2=154),features=(hog features with sahpe 540)]))","18775081":"* And if we take a look at the hog_features, we can see its a vector. You should learn details of histogram of oriented gradients, I recommend analytics vidhya, they're explaining well."}}