{"cell_type":{"72c5a2a3":"code","7e355cfa":"code","927c9787":"code","ff4a873e":"code","d1c09292":"code","422ea908":"code","a01001f2":"code","84f306f7":"code","faf5e5ba":"code","71f49e1a":"code","c9e99a0a":"code","4f9e3d51":"code","752314da":"code","a8cc314b":"code","106e7db5":"code","57de7dbd":"code","d6be95bf":"code","0fc9fb88":"code","8586e84b":"code","f6d39a47":"code","35377b62":"code","b55d6f1a":"code","a75b31ed":"code","acfae2db":"code","a21cb7b6":"code","6448acb0":"code","79478588":"code","d9f6b56b":"code","bd30b73b":"code","93373139":"code","24c4b042":"code","6ea3b88a":"markdown","46cb5da2":"markdown","aeabe3ed":"markdown","ebb1f4cb":"markdown","e498a0ff":"markdown","b9719018":"markdown","266cdc99":"markdown","feb90789":"markdown","6db07fac":"markdown","00822a42":"markdown","9d88c053":"markdown","13f78a17":"markdown","5b430e43":"markdown","86669f25":"markdown"},"source":{"72c5a2a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7e355cfa":"import spacy\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.corpus  import stopwords\nimport re\nfrom gensim.utils import lemmatize\nimport matplotlib.pyplot as plt\n","927c9787":"import keras\nfrom keras.models import load_model\nfrom keras.layers import Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import text\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical","ff4a873e":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",'ner'])","d1c09292":"df_train=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_train.csv',encoding='utf-8-sig')","422ea908":"df_val=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_val.csv',encoding='utf-8-sig')","a01001f2":"df_test=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_test.csv',encoding='utf-8-sig')","84f306f7":"import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import text\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense,LSTM,Dropout,Conv1D,MaxPooling1D","faf5e5ba":"classes=dict(list(zip(set(df_train.l2.unique()),range(0,70))))","71f49e1a":"df_train['l2']=df_train['l2'].map(classes)\ndf_val['l2']=df_val['l2'].map(classes)\ndf_test['l2']=df_test['l2'].map(classes)\n","c9e99a0a":"glove_dir=\"\/kaggle\/input\/glove6b100dtxt\"\n\nembedding_index={}\nf=open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\nfor line in f:\n    values=line.split()\n    word=values[0]\n    coefs=np.asarray(values[1:],dtype='float32')\n    embedding_index[word]=coefs\nf.close()\nprint('Found %s word vectors ' % len(embedding_index))","4f9e3d51":"stop = stopwords.words('english')","752314da":"def cleaning(df):\n    df.loc[:,'text']=pd.DataFrame(df.loc[:,'text'].str.lower())\n    #df.loc[:,'text'] = [re.sub(r'\\d+','', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(r'[^a-zA-Z]',' ', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(r\"\\b[a-zA-Z]\\b\", ' ', i) for i in df.loc[:,'text']]\n    \n    #df.loc[:,'text'] = [re.sub(r\"[#|\\.|_|\\^|\\$|\\&|=|;|,|\u2010|-|\u2013|(|)|\/\/|\\\\+|\\|*|\\']+\",'', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(' +',' ', i) for i in df.loc[:,'text']]\n    return(df)\n    ","a8cc314b":"def lemmatization(df, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in df.loc[:,'text']:\n        #print(len(sent))\n        doc = nlp(sent)\n        texts_out.append([token.lemma_ for token in doc])\n    return(texts_out)","106e7db5":"def textprocessing(df,Is_Test=1):\n    df=cleaning(df)\n    df['lemmatized_text_token']=lemmatization(df)\n    df['lemmatized_text_token']=df['lemmatized_text_token'].apply(lambda x:[i for i in x if i not in (stop) ])\n    #df_train_trunc=df_train[df_train['lemmatized_text_token'].apply(lambda c: len(c))>15]\n    if Is_Test:\n        with open('tokenizer.pickle', 'rb') as handle:\n            tokenizer = pickle.load(handle)\n    else:\n        tokenizer=Tokenizer(oov_token='<unknown>')\n        tokenizer.fit_on_texts(df['lemmatized_text_token'])\n        with open('tokenizer.pickle', 'wb') as handle:\n            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    word2id = tokenizer.word_index\n    id2word = {v:k for k, v in word2id.items()}\n    vocab_size = len(word2id) + 1 \n    sequence=tokenizer.texts_to_sequences(df['lemmatized_text_token'])\n    sequence=pad_sequences(sequence,maxlen=200)\n    if Is_Test:\n        return(sequence,word2id)\n    else:\n        return(sequence,vocab_size,word2id)","57de7dbd":"train_X,vocab_size,word2id=textprocessing(df_train,0)","d6be95bf":"val_X,word2id=textprocessing(df_val,1)","0fc9fb88":"test_X,word2id=textprocessing(df_test,1)","8586e84b":"maxlen=200\nmax_words=vocab_size\n\nembedding_dim=100\nembedding_matrix=np.zeros((max_words,embedding_dim))\n\nfor word, i in word2id.items():\n    if i <max_words:\n        embedding_vector=embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i]=embedding_vector","f6d39a47":"model_UniDirectional=Sequential()\nmodel_UniDirectional.add(Embedding(max_words,embedding_dim,input_length=maxlen))\nmodel_UniDirectional.add(Dropout(0.2))\nmodel_UniDirectional.add(Conv1D(64, 5, activation='elu'))\nmodel_UniDirectional.add(MaxPooling1D(pool_size=2))\nmodel_UniDirectional.add(LSTM(196,return_sequences=True))\n#model_UniDirectional.add(Dropout(0.2))\n#model_UniDirectional.add(LSTM(132,return_sequences=True))\n#model_UniDirectional.add(Dropout(0.2))\n#model_UniDirectional.add(LSTM(64,return_sequences=True))\n#model_UniDirectional.add(Dropout(0.2))\nmodel_UniDirectional.add(LSTM(32))\nmodel_UniDirectional.add(Dense(70,activation='softmax'))\nmodel_UniDirectional.summary()","35377b62":"def model_training(model,train_X,train_Y,val_X,val_Y,model_name):\n    model.layers[0].set_weights([embedding_matrix])\n    model.layers[0].trainable=False\n    model.compile(optimizer='adam',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n    model.fit(train_X,train_Y,\n                    batch_size=1024,\n                    epochs=10,\n                    validation_data=(val_X,val_Y)\n                    )\n    modelname_to_save=model_name+'.h5'\n    model.save(modelname_to_save)\n    return(model)","b55d6f1a":"model_unidirectional=model_training(model_UniDirectional,train_X,to_categorical(df_train['l2']),val_X,to_categorical(df_val['l2']),'model_BiLSTM')","a75b31ed":"def plot_model(model):\n    plt.plot(model.history.history['accuracy'])\n    plt.plot(model.history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    plt.plot(model.history.history['loss'])\n    plt.plot(model.history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","acfae2db":"\nplot_model(model_unidirectional)","a21cb7b6":"m=model_unidirectional.evaluate(test_X,to_categorical(df_test['l2']))","6448acb0":"print('Test Loss and Accuracy for unidirectional LSTM layer model is :',m)","79478588":"model_BiDirectional=Sequential()\nmodel_BiDirectional.add(Embedding(max_words,embedding_dim,input_length=maxlen))\nmodel_BiDirectional.add(Dropout(0.2))\nmodel_BiDirectional.add(Conv1D(64, 5, activation='elu'))\nmodel_BiDirectional.add(MaxPooling1D(pool_size=2))\nmodel_BiDirectional.add(Bidirectional(LSTM(196,return_sequences=True)))\nmodel_BiDirectional.add(Bidirectional(LSTM(32)))\nmodel_BiDirectional.add(Dense(70,activation='softmax'))\nmodel_BiDirectional.summary()","d9f6b56b":"model_BiDirectional_LSTM=model_training(model_BiDirectional,train_X,to_categorical(df_train['l2']),val_X,to_categorical(df_val['l2']),'model_BiLSTM')","bd30b73b":"plot_model(model_BiDirectional)","93373139":"m1=model_BiDirectional.evaluate(test_X,to_categorical(df_test['l2']))","24c4b042":"print('Test Loss and Accuracy for Bi-idirectional LSTM layer model is :',m1)","6ea3b88a":"This function encapsulates everything that is required for text processing before feeding it to model for training. The input is text and output is uniform length indexed tokenized sentences.\nThis function also distinguishes between train\/test data and process it accordingly","46cb5da2":"Set the embedding layer with weights defined by Glovec Word embedding and make it non-trainable, so that model execution doesn't changes the meaning of weights in Word embedding based on training data","aeabe3ed":"I will be using spacy for lemmetization, so following code will load spacy model for english language","ebb1f4cb":"While cleaning, my focus is to remove everything except english alphabet. This shall take care of digits, soecial symbols etc","e498a0ff":"Let's now try Bidirectional LSTM","b9719018":"<font size=\"5\">**Load required Libraries**<\/font>","266cdc99":"nlp(sent) in following code tokenizes the data and the following statement takes care of lemma. In almost every implementation, these tokens are further concatenated to form complete sentence, If we try doing that with this volume of data, it is going to take a lot of time. So I kept the data in tokenize format only(for any model, data should be in token, so no point in concatenating tokens here)","feb90789":"Ok..so keeping architecture similar in both kind of LSTM models(Uni vs Bi), we find a 1% improvement of Test accuracy","6db07fac":"Following code just loads english language stop words from NLTK. Later I will also add few more stop words based on their frequency from data itself","00822a42":"<font size=\"5\">**Objective:**<\/font>\n\nWikipedia Data is a great source of labelled and unlabelled data, a lot of models are built and trained on it. \nIt really gives a chance to explore NLP world with this data. A ot of sentiment\/text classification models have been built in past on tweet data, IMDB data, and yes on Wikipedia data also. But the extent of classes was limited to fewer(may be 10-12)\n\nThis kernel is an attempt to classify wikipedia articles, in 70 illustrious classes. ","9d88c053":"since data is in abundace, I could have chose to build my own work embedding, but still I am using Glovec word embeddings(100 dimesions)","13f78a17":"<font size=\"5\">**DataSet:**<\/font>\n\nData utilised for this kernel has been taken from kaggle dataset. Everytime you google any resource that has a wikipedia page, you see a small description of the same from wikipedia on search page itself. This is usually the first paragrapgh of respective page.\nThe Articles\/Text present in this dataset is this first paragraph of Wikipedia data set itself. Along with this text data, the Dataset classify this data as in 3, 70 and 216 categories. Each categorical column adds more granularity to categorisation of these articles","5b430e43":"Time to load word embedding in an embedding matrix that can utilised in Embedding layer of model","86669f25":"For this problem, I am using a LSTM layer with usual drop outs and pooling"}}