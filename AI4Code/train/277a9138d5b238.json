{"cell_type":{"6d609f4b":"code","2962cda8":"code","7cf28889":"code","a57ef6ed":"code","8ee07113":"code","ec94cd11":"code","07b2e468":"code","e92f1e99":"code","9161499a":"code","734da864":"code","ab3e6983":"code","6a2e7cae":"code","ee6be686":"code","7632ed90":"code","b7b492c6":"code","727437b7":"code","dc87a9bd":"code","991f3b96":"code","397c53ad":"code","3a07db1b":"code","52afb61d":"code","3b93f8cf":"code","aff41d47":"code","d99bb2d3":"code","11c5bfe8":"code","7471baba":"code","fc3e5576":"code","5c7eb002":"code","4dec8719":"code","2b889edd":"code","99c533bb":"code","06b2da73":"code","e8446f2c":"code","fa57d346":"code","99f6e4dd":"code","941e3d63":"code","b950aa08":"code","05401ddb":"code","68bfef2d":"code","696d32e0":"code","0048e6fa":"code","6544fac0":"code","9a896d0f":"code","a2630bd6":"code","ba9a9d0b":"code","178e9a0a":"code","49acf1da":"code","554bcfd8":"code","69694f6a":"code","c548246c":"code","abe10f90":"code","3f2b8b76":"code","81598735":"code","65007d15":"code","4def4464":"code","47a6585f":"code","6701f83a":"code","def78ae0":"code","e5d99a45":"code","35daa569":"code","ad85e283":"code","df1d976e":"code","09e2995e":"code","3bf9f95d":"code","f2ccec76":"code","d6a80a24":"code","41e5c71e":"code","cebb0a46":"code","d133e23f":"code","caf85576":"code","38540271":"code","d1e2ff69":"code","8bc564e4":"code","ca568851":"code","24e6e2fb":"code","f63c885c":"code","ebd51651":"code","e63d80e1":"code","a752eb34":"code","7534396c":"code","fa163800":"code","1a6b104e":"code","df6b5d30":"code","8974929c":"code","e1260fdb":"code","33300957":"code","ebe46888":"code","97569416":"code","2244dce8":"code","22706951":"code","2b85000c":"code","6b72bfdd":"code","6bc19273":"code","58f4d8a0":"code","e64c2dc4":"code","9d806326":"code","4282dc98":"code","63b86cda":"code","3a22be0f":"code","e20bb053":"code","3499e12a":"code","4a70e27e":"code","006e09c9":"code","919cc475":"code","caf34072":"code","54357ac3":"code","a065441b":"code","9c9ad885":"code","ff2785c8":"code","547d737d":"code","8343842d":"code","e9754ec3":"code","2302d265":"code","c896989c":"code","7985abb6":"code","02679f57":"code","f780c345":"code","4143ec6e":"code","aca3fa7b":"code","7703a85e":"code","2a58d734":"code","ac090f4a":"code","19655f95":"code","2f080880":"code","51b94675":"code","88c9c266":"code","72c0b476":"code","be98d123":"code","7a96b87f":"code","b5405255":"code","48ec2a49":"code","e3c285bc":"code","e141fd06":"code","7ae1a31a":"code","281373e4":"code","6fae8c40":"code","ccae8057":"code","6b00acab":"code","cc3bc4e1":"code","6f274c28":"code","5d486b4c":"code","fb85708f":"code","14fdfd40":"markdown","d70a48bf":"markdown","f204e36e":"markdown","c70716dd":"markdown","358a16c2":"markdown","841e5971":"markdown","121b34ef":"markdown","d7af9f1e":"markdown","268ce38d":"markdown","a4ff30ca":"markdown","79c65405":"markdown","bd208494":"markdown","ed7a84a8":"markdown","f12a79c4":"markdown","41b31b67":"markdown","9e28f9d5":"markdown","f910dbca":"markdown","3285283d":"markdown","1607749a":"markdown","51f5b6f7":"markdown"},"source":{"6d609f4b":"#main libraries\nimport os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\n\n#machine learning libraries:\nfrom sklearn.model_selection import KFold,StratifiedKFold, cross_validate, cross_val_score, train_test_split\nfrom sklearn.preprocessing  import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport re\n%matplotlib inline\nsns.set_theme(style=\"whitegrid\")\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\")","2962cda8":"'''Data Preprocessing'''","7cf28889":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#joining all the data together\nfull_data = pd.concat([train_data, test_data])\n\n#making a copy of the original data\ntrain_data_orig = train_data.copy()\ntest_data_orig = test_data.copy()\n\ntrain_data.head()","a57ef6ed":"train_data.shape","8ee07113":"#printing the shape of the data\nprint('This data contrains {} rows and {} columns splitted into train and test datasets with ratio {}'.format(full_data.shape[0], full_data.shape[1], round((test_data.shape[0]\/train_data.shape[0])*100,2)))","ec94cd11":"cols = train_data.columns\nprint(f\"We have {len(cols)} columns : \\n{cols}\")","07b2e468":"#Seeing the information about the data\ntrain_data.info()\nprint(\"_\"*40)\ntest_data.info()\n","e92f1e99":"#To show the types of columns\ntrain_data.dtypes.to_frame().rename(columns={0:'Column type'})\n","9161499a":"#to find unique values in each column\nfor col in train_data.columns:\n    print('We have {} unique values in {} column'.format(\n        len(train_data[col].unique()),col))\n    print('-'*35)","734da864":"train_data['SibSp'].unique()","ab3e6983":"train_data['Parch'].unique()","6a2e7cae":"train_data['Embarked'].unique()","ee6be686":"train_data['Pclass'].unique()","7632ed90":"train_data['Sex'].unique()","b7b492c6":"print('Age columns vary from {} to {}'.format(train_data['Age'].min(), train_data['Age'].max()))","727437b7":"#describing our data\ntrain_data[train_data.select_dtypes(exclude='object').columns].drop('PassengerId', axis = 1).describe().\\\nstyle.background_gradient(axis=1, cmap = sns.light_palette('skyblue', as_cmap = True))\n","dc87a9bd":"#finding the null values in each column\ntrain_data.isnull().sum().to_frame().rename(columns={0:\"Null values\"})","991f3b96":"#Visualizing the null values in each column\nplt.figure(figsize= (20,6));\nsns.heatmap(train_data.isnull(), cmap = 'viridis');","397c53ad":"#To see the correlation between columns and target column\ncorr = train_data.corr()\ncorr['Survived'].sort_values(ascending = False)[1:].to_frame().\\\nstyle.background_gradient(axis = 1, cmap = sns.light_palette('green', as_cmap=True))","3a07db1b":"#lets take a look to the shape of columns\ntrain_data.skew().to_frame().rename(columns = {0: 'Skewness'}).sort_values('Skewness')\n","52afb61d":"#Visualizing columns haveing highest Skewness\nfig, axes = plt.subplots(1,3, figsize=(20,8));\nfig.suptitle('Highest Skewness', fontsize = 25)\n\nfor i, col in zip(range(3), ['Fare', 'SibSp', 'Parch']):\n    sns.kdeplot(train_data[col], ax = axes[i], hue=train_data['Survived'])\n    axes[i].set_title(col+'Distribution')\n    ","3b93f8cf":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index= False).mean().sort_values(by='Survived', ascending=False)","aff41d47":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index= False).mean().sort_values(by='Survived', ascending=False)","d99bb2d3":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index= False).mean().sort_values(by='Survived', ascending=False)","11c5bfe8":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index= False).mean().sort_values(by='Survived', ascending=False)","7471baba":"train_data.hist(figsize=(15,12),bins = 20, color=\"#107009AA\")\nplt.title(\"Features Distribution\")\nplt.show()","fc3e5576":"sns.displot(train_data, x=\"Pclass\", discrete=True)","5c7eb002":"sns.kdeplot(train_data['Pclass'], shade=True, color=\"r\")","4dec8719":"sns.kdeplot(data=train_data, x=\"Pclass\", hue=\"Survived\")","2b889edd":"sns.displot(train_data, x=\"Pclass\", hue=\"Survived\", multiple=\"dodge\")","99c533bb":"#for Pclass \ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","06b2da73":"sns.displot(train_data, x=\"Age\", discrete=True)","e8446f2c":"sns.kdeplot(train_data['Age'], shade=True, color=\"r\")","fa57d346":"sns.kdeplot(data=train_data, x=\"Age\", hue=\"Survived\")","99f6e4dd":"sns.displot(train_data, x=\"Age\", hue=\"Survived\", multiple=\"dodge\")","941e3d63":"sns.displot(train_data, x=\"SibSp\", discrete=True)","b950aa08":"sns.kdeplot(train_data['SibSp'], shade=True, color=\"r\")","05401ddb":"sns.kdeplot(data=train_data, x=\"SibSp\", hue=\"Survived\")","68bfef2d":"sns.displot(train_data, x=\"SibSp\", hue=\"Survived\", multiple=\"dodge\")","696d32e0":"sns.countplot(x ='Embarked', data = train_data)","0048e6fa":"cp = sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train_data)","6544fac0":"\n\nsns.boxplot(x='Embarked',y='Survived',data=train_data,palette='rainbow')","9a896d0f":"sns.countplot(x ='Sex', data = train_data)","a2630bd6":"cp = sns.countplot(x=\"Sex\", hue=\"Survived\", data=train_data)","ba9a9d0b":"sns.boxplot(x='Sex',y='Survived',data=train_data,palette='rainbow')","178e9a0a":"sns.displot(train_data, x=\"Fare\", discrete=True)","49acf1da":"sns.kdeplot(train_data['Fare'], shade=True, color=\"r\")","554bcfd8":"sns.kdeplot(data=train_data, x=\"Fare\", hue=\"Survived\")","69694f6a":"sns.displot(train_data, x=\"Fare\", hue=\"Survived\", multiple=\"dodge\")","c548246c":"sns.displot(train_data, x=\"Parch\", discrete=True)","abe10f90":"sns.kdeplot(train_data['Parch'], shade=True, color=\"r\")","3f2b8b76":"sns.kdeplot(data=train_data, x=\"Parch\", hue=\"Survived\")","81598735":"sns.displot(train_data, x=\"Parch\", hue=\"Survived\", multiple=\"dodge\")","65007d15":"sns.heatmap(train_data.corr())","4def4464":"train_data['Survived'].value_counts().to_frame().rename(columns = {'Survived': 'Total Count'})\n","47a6585f":"grid = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","6701f83a":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","def78ae0":"sns.set(style='whitegrid', context='talk', palette='viridis');\nsns.pairplot(data=train_data,hue='Survived');","e5d99a45":"sns.boxplot('Survived','Age', data=train_data, orient='v')\nsns.boxplot('Survived','Fare', data=train_data, orient='v')\n","35daa569":"sns.scatterplot(data=train_data, x=\"Age\", y=\"Fare\")","ad85e283":"test_df = test_data.drop(columns=['Name','PassengerId','Cabin','Ticket'])\nnunique = test_df.nunique(dropna=False)\nnunique","df1d976e":"plt.figure(figsize=(14,6))\n_ = plt.hist(nunique.astype(float)\/test_df.shape[0], bins=100)","09e2995e":"#FEATURES WITH HUGE NUMBER OF unique values\nmask = (nunique.astype(float)\/test_df.shape[0] > 0.8)\ntest_df.loc[:, mask]","3bf9f95d":"mask = (nunique.astype(float)\/test_df.shape[0] < 0.8)\ntest_df.loc[:25, mask]","f2ccec76":"test_data['Pclass'].value_counts()","d6a80a24":"train_df = pd.read_csv('..\/input\/titanic\/train.csv').drop(['Ticket','Cabin'], axis =1)\ny_train = train_df[['Survived']]\ntrain_df = train_df.drop(['Survived'], axis = 1)\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv').drop(['Ticket','Cabin'], axis=1)\ntest_PassengerId = test_df['PassengerId'].values #for submission","41e5c71e":"combine_data = pd.concat([train_df, test_df]).reset_index()\ncombine_data.tail()","cebb0a46":"# `dropna = False` makes nunique treat NaNs as a distinct value\nfeats_counts = combine_data.nunique(dropna = False)","d133e23f":"feats_counts.sort_values()","caf85576":"# constant_features = feats_counts.loc[feats_counts==2].index.tolist()","38540271":"# print(constant_features)","d1e2ff69":"# constant_features = feats_counts.loc[feats_counts==7].index.tolist()\n","8bc564e4":"# print(constant_features)","ca568851":"combine_data['Age'].fillna(int(combine_data['Age'].dropna().mean()), inplace=True)\ncombine_data['Embarked'].fillna(combine_data['Embarked'].dropna().mode(), inplace=True)","24e6e2fb":"combine_data['Fare'].fillna(int(combine_data['Fare'].dropna().mean()), inplace=True)\ncombine_data['Fare'].isnull().sum()","f63c885c":"combine_data.isnull().sum()","ebd51651":"combine_data['Embarked'].fillna(combine_data['Embarked'].dropna().mode().iloc[0], inplace=True)","e63d80e1":"combine_data['Embarked'].isnull().sum()","a752eb34":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n\ncombine_data.loc[:, 'Name_length'] = combine_data.Name.apply(len)\ncombine_data.loc[:, 'Word_count'] = combine_data.Name.apply(lambda x: len(x.split()))\n\ncombine_data.loc[:, 'Title'] = combine_data['Name'].apply(get_title)\nrare = ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n\ncombine_data['Title'] = combine_data['Title'].replace(rare, 'Rare')\ncombine_data['Title'] = combine_data['Title'].replace('Mlle', 'Miss')\ncombine_data['Title'] = combine_data['Title'].replace('Ms', 'Miss')\ncombine_data['Title'] = combine_data['Title'].replace('Mme', 'Mrs')\n\n# Remove Name column\ncombine_data.drop('Name', axis=1, inplace=True)","7534396c":"train_data['family'] = train_data['SibSp'] + train_data['Parch'] + 1","fa163800":"cp = sns.countplot(x=\"family\", hue=\"Survived\", data=train_data)","1a6b104e":"sns.kdeplot(train_data['family'], shade=True, color=\"r\")","df6b5d30":"combine_data = combine_data.drop(columns=['PassengerId', 'Name_length', 'Word_count'], axis=1)\ncat_cols = list(combine_data.select_dtypes(include=['object']).columns)\nnum_cols = list(combine_data.select_dtypes(exclude=['object']).columns)\ncombine_data.head(3)","8974929c":"# data_enc = pd.DataFrame(index = combine_data.index)\n\nfor col in tqdm_notebook(combine_data[cat_cols]):\n    combine_data[col+\"2\"] = combine_data[col].factorize()[0]","e1260fdb":"combine_data.head()","33300957":"# #appending the duplicate columns in dup_cols\n# dup_cols = []\n\n# for i, c1 in enumerate(tqdm_notebook(data_enc.columns)):\n#     for c2 in data_enc.columns[i+1:]:\n#         if c2 not in dup_cols and np.all(data_enc[c1] == data_enc[c2]):\n#             dup_cols[c2] = c1","ebe46888":"# dup_cols","97569416":"# #As we can see,there is no duplicate column. So we dont need to dump the duplicate columns\n# import cPickle as pickle\n# pickle.dump(dup_cols, open('dup_cols.p', 'w'), protocol=pickle.HIGHEST_PROTOCOL)","2244dce8":"#Drop from the dataset\n##combine_data.drop(dup_cols.keys(), axis=1, inplace=True)","22706951":"scaler = StandardScaler()\nscaler.fit(combine_data[num_cols])","2b85000c":"print(scaler.mean_)","6b72bfdd":"print(scaler.transform(combine_data[num_cols]))","6bc19273":"scaled_data = pd.DataFrame(scaler.transform(combine_data[num_cols]))\nscaled_data.head()","58f4d8a0":"combine_data[num_cols]","e64c2dc4":"scaled_data = scaled_data.iloc[:,1:]\nscaled_data.head()","9d806326":"scaled_data = scaled_data.rename(columns={1:'Pclass2',2:'Age2',3:'SibSp2',4:'Parch2',5:'Fare2'})","4282dc98":"scaled_data.head()","63b86cda":"combine_data = pd.concat([combine_data, scaled_data], axis=1)\ncombine_data.head()","3a22be0f":"scatter_data = pd.concat([combine_data[:891], y_train], axis=1)","e20bb053":"X_train, X_test, y_train, y_test = train_test_split(combine_data[:891],y_train, test_size=0.25, random_state=42)","3499e12a":"X_train.columns","4a70e27e":"scaled_ = ['Sex2', 'Embarked2', 'Title2', 'Pclass2', 'Age2', 'SibSp2',\n       'Parch2', 'Fare2']\nclf = LogisticRegression()\nclf.fit(X_train[scaled_],y_train)\nprob = clf.predict_proba(X_test[scaled_])","006e09c9":"prob","919cc475":"print(clf)\nprint('classes:', clf.classes_)\nprint('coefficients', clf.coef_)\nprint('intercept:', clf.intercept_)","caf34072":"from sklearn.metrics import classification_report","54357ac3":"print(classification_report(y_test, clf.predict(X_test[scaled_])))","a065441b":"ytrain_hat = clf.predict(X_train[scaled_])\nytest_hat = clf.predict(X_test[scaled_])","9c9ad885":"from sklearn.metrics import mean_squared_error","ff2785c8":"print(\" Train MSE: \",mean_squared_error(y_train, ytrain_hat))\nprint(\" Test MSE: \",mean_squared_error(y_test, ytest_hat))","547d737d":"from sklearn.metrics import confusion_matrix","8343842d":"#for test data\nconfusion_matrix(y_test, ytest_hat)","e9754ec3":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(solver='svd')\nlda.fit(X_train[scaled_], y_train)\nytrain_hat = lda.predict(X_train[scaled_])\nytest_hat = lda.predict(X_test[scaled_])\n","2302d265":"print(\" USING LINEAR DISCRIMINANT ANALYSIS\")\nprint(\"TRAIN MSE: \", mean_squared_error(y_train, ytrain_hat))\nprint(\"TEST MSE: \", mean_squared_error(y_test, ytest_hat))","c896989c":"confusion_matrix(y_test, ytest_hat)","7985abb6":"print(classification_report(y_test, ytest_hat))","02679f57":"from sklearn.metrics import plot_confusion_matrix\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(lda, X_test[scaled_], y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","f780c345":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train[scaled_], y_train)\nytrain_hat = qda.predict(X_train[scaled_])\nytest_hat = qda.predict(X_test[scaled_])","4143ec6e":"print(\" USING Quadratic DISCRIMINANT ANALYSIS\")\nprint(\"TRAIN MSE: \", mean_squared_error(y_train, ytrain_hat))\nprint(\"TEST MSE: \", mean_squared_error(y_test, ytest_hat))","aca3fa7b":"from sklearn.metrics import plot_confusion_matrix\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(qda, X_test[scaled_], y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","7703a85e":"print(classification_report(y_test, ytest_hat))","2a58d734":"qda.priors_","ac090f4a":"qda.means_","19655f95":"knn = KNeighborsClassifier(n_neighbors = 10)\npred = knn.fit(X_train[scaled_], y_train)\nknn.fit(X_train[scaled_], y_train)\nytrain_hat = knn.predict(X_train[scaled_])\nytest_hat = knn.predict(X_test[scaled_])","2f080880":"print(\" USING KNearestNeighbor Classifier\")\nprint(\"TRAIN MSE: \", mean_squared_error(y_train, ytrain_hat))\nprint(\"TEST MSE: \", mean_squared_error(y_test, ytest_hat))","51b94675":"from sklearn.metrics import plot_confusion_matrix\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(qda, X_test[scaled_], y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","88c9c266":"knn = KNeighborsClassifier(n_neighbors = 9)\npred = knn.fit(X_train[scaled_], y_train)\nknn.fit(X_train[scaled_], y_train)\nytrain_hat = knn.predict(X_train[scaled_])\nytest_hat = knn.predict(X_test[scaled_])","72c0b476":"print(\" USING KNearestNeighbor Classifier\")\nprint(\"TRAIN MSE: \", mean_squared_error(y_train, ytrain_hat))\nprint(\"TEST MSE: \", mean_squared_error(y_test, ytest_hat))","be98d123":"from sklearn.metrics import plot_confusion_matrix\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(qda, X_test[scaled_], y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","7a96b87f":"combine_data.columns","b5405255":"nscaled_ = ['Pclass', 'Sex2', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked2',\n       'Title2']\nrf = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\nrf.fit(X_train[nscaled_], y_train)\nprint(\"%.4f\" % rf.oob_score_)","48ec2a49":"ytrain_hat = rf.predict(X_train[nscaled_])\nytest_hat = rf.predict(X_test[nscaled_])","e3c285bc":"print(\" USING Random Forest Classifier\")\nprint(\"TRAIN MSE: \", mean_squared_error(y_train, ytrain_hat))\nprint(\"TEST MSE: \", mean_squared_error(y_test, ytest_hat))","e141fd06":"print(classification_report(y_test, ytest_hat))","7ae1a31a":"from sklearn.metrics import plot_confusion_matrix\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(rf, X_test[nscaled_], y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","281373e4":"#I our case Random FOrest works better than other machine learning models.\n#So, I am going to apply Random Forest","6fae8c40":"test_pred = rf.predict(combine_data[891:][nscaled_])\ntest_pred\n","ccae8057":"test_pred = pd.DataFrame(test_pred,columns=['Survived',] )\ntest_pred","6b00acab":"check = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ncheck","cc3bc4e1":"check = check.drop(['Survived'], axis =1)","6f274c28":"submission = pd.concat([check,test_pred], axis = 1)\nsubmission","5d486b4c":"submission.dtypes","fb85708f":"submission.to_csv('..\/submission.csv', index = False)","14fdfd40":"# Data Preprocessing","d70a48bf":"For Embarked","f204e36e":"    Pclass\n    Sex\n    SibSp and Parch","c70716dd":"# Data Exploration and Analysis","358a16c2":"Visualizing Parch","841e5971":"Visualizing Fare","121b34ef":"Visualizing Sex","d7af9f1e":"# Analyze by pivoting features","268ce38d":"Using Quadratic Discriminant Analysis","a4ff30ca":"Frequency Distribution for Age","79c65405":"VISUALIZING LOGISTIC REGRESSION AND LINEAR REGRESSION","bd208494":"# BASIC EDA\n    For Training Data","ed7a84a8":"Frequency Distribution for Sibsp","f12a79c4":"Using K-NEAREST NEIGHBORS","41b31b67":"Using RANDOM FOREST CLASSIFIER","9e28f9d5":"Frequency Distribution of Pclass","f910dbca":"Using Linear Discriminant Analysis","3285283d":"\n    WE have a lot of null values in cabin 687 and age 177 columns\n\n    Survived column have a higher correlation with:\n        Pclass -0.338481\n        Fare 0.257307\n        Parch 0.081629    \n\n    We have some columns with a high skewness\n        Fare 4.787317\n        SibSp 3.695352\n","1607749a":"Removing duplicated features","51f5b6f7":"For test data"}}