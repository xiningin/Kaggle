{"cell_type":{"b3408b82":"code","aa785550":"code","a8973cc8":"code","56a8f955":"code","089fface":"code","df1e6568":"code","2057be98":"code","d39a8f88":"code","f7a7b801":"code","d7a828e5":"code","9b0183bb":"code","827ca117":"code","bb66108d":"code","c9d2d980":"code","50ab9a31":"code","ace14aee":"code","e396f6ea":"code","c945ac3f":"code","89538af6":"code","01be7deb":"code","15ceca3e":"code","e81e562e":"code","4888cc6e":"code","c5e5c1bf":"code","e5633a56":"code","5976f02a":"code","3bdbb701":"markdown","7b428064":"markdown","3e593af8":"markdown","c1ac0f46":"markdown","9526d326":"markdown"},"source":{"b3408b82":"!pip install allennlp","aa785550":"!pip show torch torchtext allennlp\n!echo '\\n'\n!ls ..\/input","a8973cc8":"!ls ..\/input\/movie-review-sentiment-analysis-kernels-only","56a8f955":"import pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm_notebook\nimport allennlp.common.tqdm as tqdm\ntqdm._tqdm = tqdm_notebook","089fface":"train = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\", dtype={\"phrase\": np.str})\ntest = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep=\"\\t\", dtype={\"phrase\": np.str})","df1e6568":"len(train)","2057be98":"train.where(train['Phrase'] == \" \").dropna()","d39a8f88":"train.drop(2005, inplace=True)","f7a7b801":"train_len = train.shape[0]\nprint(\"len of train =\", train_len)","d7a828e5":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))\nprint('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","9b0183bb":"#idx_separator = 124805\nidx_separator = int(len(train) * 0.8)\nidxs = np.random.permutation(train.shape[0])\ntrain_idxs = idxs[:idx_separator]\nval_idxs = idxs[idx_separator:]\ntrain_df = train.iloc[train_idxs,:]\nval_df = train.iloc[val_idxs,:]\n\nprint(\"train_df len =\", len(train_df))\nprint(\"val_df len =\", len(val_df))","827ca117":"train_df.to_csv(\"..\/input\/train.csv\")\nval_df.to_csv(\"..\/input\/val.csv\")","bb66108d":"!ls ..\/input","c9d2d980":"from typing import Dict\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer, ELMoTokenCharactersIndexer, TokenCharactersIndexer\nfrom allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, CharacterTokenizer\nfrom allennlp.data.fields import *\nfrom allennlp.data import Vocabulary","50ab9a31":"class MRDatasetReader(DatasetReader):\n    def __init__(self,\n             tokenizer: Tokenizer = None,\n             token_indexers: Dict[str, TokenIndexer] = None,\n             lazy: bool = False) -> None:\n        super().__init__(lazy)\n\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n    \n    def _read(self, file_path):\n        with open(cached_path(file_path), \"r\") as data_file:\n            df = pd.read_csv(file_path, dtype={\"PhraseId\": np.int, \"Phrase\": np.str, \"Sentiment\": np.int8, \"SentenceId\": np.int})\n            #print(\"count = \", len(train.where(train['Sentiment'] == 0).dropna()))\n            for i, item in df.iterrows():\n                phrase_id = item[\"PhraseId\"]\n                sentence_id = item[\"SentenceId\"]\n                phrase = item[\"Phrase\"]\n                sentiment = item[\"Sentiment\"]\n                yield self.text_to_instance(phrase_id, sentence_id, phrase, sentiment)\n            \n    def text_to_instance(self, phrase_id, sentence_id, phrase, sentiment) -> Instance:\n        tokenized_phrase = self._tokenizer.tokenize(phrase)\n        \n        phrase_field = TextField(tokenized_phrase, self._token_indexers)\n        #phrase_id_field = MetadataField(phrase_id)\n        #sentence_id_field = MetadataField(sentence_id)\n        fields = {\n            \"phrase\": phrase_field\n        }\n        \n        #print(f\"sentiment = {sentiment} | sentiment-1 = {sentiment-1}\")\n        fields[\"labels\"] = LabelField(str(int(sentiment)))\n        #fields[\"labels\"] = LabelField(sentiment, skip_indexing=True)\n        \n        return Instance(fields)","ace14aee":"glove_pretrained_embedding = \"https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/datasets\/glove\/glove.6B.100d.txt.gz\"\noptions_file = \"https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/models\/elmo\/2x4096_512_2048cnn_2xhighway\/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\nweight_file = \"https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/models\/elmo\/2x4096_512_2048cnn_2xhighway\/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"","e396f6ea":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\n\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder, ElmoTokenEmbedder\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\nfrom allennlp.modules import Highway\nfrom allennlp.modules.elmo import Elmo\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import PytorchSeq2SeqWrapper\nfrom allennlp.modules.seq2vec_encoders import *\nfrom allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\nfrom allennlp.data.iterators import *\nfrom allennlp.training.trainer import Trainer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom allennlp.training.learning_rate_schedulers import LearningRateWithMetricsWrapper\nfrom allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file","c945ac3f":"class MRModel(Model):\n    def __init__(self, vocab, text_field_embedder, input_size, hidden_size, dropout = 0.0):\n        super().__init__(vocab)\n        \n        self.text_field_embedder = text_field_embedder\n        \n        lstm1 = nn.LSTM(input_size=input_size, hidden_size=HIDDEN_DIM, num_layers=1, bidirectional=True, batch_first=True)\n        self.lstm1 = PytorchSeq2VecWrapper(lstm1)\n        \n        #self.lin1 = nn.Linear(in_features=self.lstm1.get_output_dim(), out_features=50)        \n        self.lin1 = nn.Linear(in_features=self.lstm1.get_output_dim(), out_features=5)\n        \n        self.loss = nn.CrossEntropyLoss()\n        self.accuracy = CategoricalAccuracy()\n        \n        if dropout > 0:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = lambda x: x\n        \n    def forward(self, \n                phrase: Dict[str, torch.LongTensor],\n                labels: torch.LongTensor = None):\n        mask = get_text_field_mask(phrase)\n        embedded_phrase = self.text_field_embedder(phrase)\n        encoded_phrase = self.lstm1(embedded_phrase, mask)\n    \n        x = self.dropout(F.relu(self.lin1(encoded_phrase)))\n        \n        tag_logits = F.softmax(x, dim=1)\n\n        output = {\"tag_logits\": tag_logits}\n\n        if labels is not None:\n            self.accuracy(tag_logits, labels)\n            output[\"loss\"] = self.loss(tag_logits, labels)\n\n        return output\n    \n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}","89538af6":"token_indexers = { \n    \"tokens\": SingleIdTokenIndexer(lowercase_tokens=True),\n    \"elmo\": ELMoTokenCharactersIndexer(namespace=\"elmo\"),\n    \"token_characters\": TokenCharactersIndexer(character_tokenizer=CharacterTokenizer(byte_encoding=\"utf-8\", start_tokens=[259], end_tokens=[260]))\n}\n\nreader = MRDatasetReader(token_indexers=token_indexers)\n\ntrain_dataset = reader.read(\"..\/input\/train.csv\")\nval_dataset = reader.read(\"..\/input\/val.csv\")","01be7deb":"vocab = Vocabulary.from_instances(train_dataset + val_dataset)\ntokens_token_embedder_weight = _read_pretrained_embeddings_file(file_uri=glove_pretrained_embedding, embedding_dim=100, vocab=vocab)","15ceca3e":"EMBEDDING_DIM = 100\nHIDDEN_DIM = 100","e81e562e":"tokens_token_embedder = Embedding(embedding_dim=100, trainable=False, weight=tokens_token_embedder_weight, num_embeddings=vocab.get_vocab_size('tokens'))\nelmo_token_embedder = ElmoTokenEmbedder(options_file=options_file, weight_file=weight_file, do_layer_norm=False)","4888cc6e":"token_characters = TokenCharactersEncoder(embedding=Embedding(embedding_dim=EMBEDDING_DIM, num_embeddings=vocab.get_vocab_size('tokens')), \n                                          encoder=CnnEncoder(embedding_dim=EMBEDDING_DIM, num_filters=50, ngram_filter_sizes=[4,5]), \n                                          dropout=0.2)\n\ntoken_embedders_config = {\n    \"tokens\": tokens_token_embedder,\n    \"elmo\": elmo_token_embedder,\n    \"token_characters\": token_characters\n}\n\ntf_embedder = BasicTextFieldEmbedder(token_embedders=token_embedders_config)","c5e5c1bf":"model = MRModel(vocab, tf_embedder, input_size=1224, hidden_size=HIDDEN_DIM, dropout=0.2)","e5633a56":"lr = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\niterator = BucketIterator(batch_size=256, sorting_keys=[(\"phrase\", \"num_tokens\")])\niterator.index_with(vocab)\n\nlearning_rate_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n\ntrainer = Trainer(model=model,\n                  optimizer=optimizer,\n                  iterator=iterator,\n                  train_dataset=train_dataset,\n                  validation_dataset=val_dataset,\n                  patience=3,\n                  shuffle=True,\n                  num_epochs=5,\n                  cuda_device=0,\n                  learning_rate_scheduler=LearningRateWithMetricsWrapper(learning_rate_scheduler))","5976f02a":"trainer.train()","3bdbb701":"### Movie review sentiment with PyTorch\nIn this kernel I want to create a rather simple LSTM bidirectional model while using ElMo, Glove and optionally some FastText.","7b428064":"#### Model\nCreating the model.","3e593af8":"We use glove and Elmo as our embeddings for now.","c1ac0f46":"#### Dataset\nPreparing the dataset for AllenNLP.","9526d326":"## Notes\n* LR: higher than 1e-3 is bad\n* Dropout: 0.2 seems alright, has strong effect on train accuracy, while val accuracy seems to stay around the same (compared to 0.0 dropout)\n* Batch size?\n* Embedding: increasing i.e. from 16 to 100 results in a much worse val accuracy in epoch 1, from 0.6 to 0.5, when not adjusting CnnEncoder\n* CnnEncoder: has positive effect as filter size\/layers are increased, but only when the Embedding is big enough (I set it to 100 I think)\n* Hidden size?"}}