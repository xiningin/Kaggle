{"cell_type":{"a997e53f":"code","039089c0":"code","6eb1a224":"code","40f6873b":"code","80e3f90d":"code","009a7f01":"code","8af282f3":"code","a65db8e5":"code","77998037":"code","390e3ca5":"code","7c2d9f5f":"code","b34591bf":"code","8fca01e0":"code","2d734cfe":"code","cb24c3ea":"code","aecaa797":"markdown","6f70aeb7":"markdown","3e8057e7":"markdown","70cb93b6":"markdown","951d8f01":"markdown","9c551649":"markdown","1051583a":"markdown","3a23e03d":"markdown","7f9127b1":"markdown","673d93ef":"markdown","66848acf":"markdown","314aa86b":"markdown"},"source":{"a997e53f":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport gc\nimport time\nimport random\nfrom sklearn.model_selection import train_test_split\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\nDIR_INPUT = '\/kaggle\/input\/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}\/train'\nDIR_TEST = f'{DIR_INPUT}\/test'\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nrandom.seed(seed)","039089c0":"#Install + import Weights and Biases\n!pip install --upgrade wandb\nimport wandb","6eb1a224":"train_df = pd.read_csv(f'{DIR_INPUT}\/train.csv')\ntrain_df.shape\ntrain_df.head()","40f6873b":"#Extract the bbox column and explode it into x,y,w,h\ntrain_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","80e3f90d":"image_ids = train_df['image_id'].unique()\n\n#Shuffled\n#train_ids, valid_ids = train_test_split(image_ids, test_size=0.2, random_state=seed)\n\n#Non-shuffled (default)\n#First 75% of the data is training, last 25% is validation\nvalid_ids = image_ids[-674:]\ntrain_ids = image_ids[:-674]","009a7f01":"print(image_ids.shape)\nprint(valid_ids.shape)\nprint(train_ids.shape)","8af282f3":"#take the bounding boxes and put them into their respective category (train, valid) by id \nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]","a65db8e5":"class WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique() #all images\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        #load image and normalize image pixels to 0-1\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        #turn each bounding box into format [x_start, y_start, x_end, y_end]\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        #calculate areas of each bounding box\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels #all boxes are labelled as 1 (wheat head)\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        #apply transformations to this image\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","77998037":"# Albumentations (image data augmentation)\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n#Collate function\ndef collate_fn(batch):\n    return tuple(zip(*batch))","390e3ca5":"#IOU + MAP Metric Calculation\nfrom collections import namedtuple\nfrom typing import List, Union\n\nBox = namedtuple('Box', 'xmin ymin xmax ymax')\n\n\ndef calculate_iou(gt: List[Union[int, float]],\n                  pred: List[Union[int, float]],\n                  form: str = 'pascal_voc') -> float:\n    \"\"\"Calculates the IoU.\n    \n    Args:\n        gt: List[Union[int, float]] coordinates of the ground-truth box\n        pred: List[Union[int, float]] coordinates of the prdected box\n        form: str gt\/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        IoU: float Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        bgt = Box(gt[0], gt[1], gt[0] + gt[2], gt[1] + gt[3])\n        bpr = Box(pred[0], pred[1], pred[0] + pred[2], pred[1] + pred[3])\n    else:\n        bgt = Box(gt[0], gt[1], gt[2], gt[3])\n        bpr = Box(pred[0], pred[1], pred[2], pred[3])\n        \n\n    overlap_area = 0.0\n    union_area = 0.0\n\n    # Calculate overlap area\n    dx = min(bgt.xmax, bpr.xmax) - max(bgt.xmin, bpr.xmin)\n    dy = min(bgt.ymax, bpr.ymax) - max(bgt.ymin, bpr.ymin)\n\n    if (dx > 0) and (dy > 0):\n        overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (bgt.xmax - bgt.xmin) * (bgt.ymax - bgt.ymin) +\n            (bpr.xmax - bpr.xmin) * (bpr.ymax - bpr.ymin) -\n            overlap_area\n    )\n\n    return overlap_area \/ union_area\n\n#MAP Calculation\ndef find_best_match(gts, predd, threshold=0.5, form='pascal_voc'):\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n    \n    Args:\n        gts: Coordinates of the available ground-truth boxes\n        pred: Coordinates of the predicted box\n        threshold: Threshold\n        form: Format of the coordinates\n        \n    Return:\n        Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n    \n    for gt_idx, ggt in enumerate(gts):\n        iou = calculate_iou(ggt, predd, form=form)\n        \n        if iou < threshold:\n            continue\n        \n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\n\ndef calculate_precision(preds_sorted, gt_boxes, threshold=0.5, form='coco'):\n    \"\"\"Calculates precision per at one threshold.\n    \n    Args:\n        preds_sorted: \n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n\n    fn_boxes = []\n\n    for pred_idx, pred in enumerate(preds_sorted):\n        best_match_gt_idx = find_best_match(gt_boxes, pred, threshold=threshold, form=form)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n\n            # Remove the matched GT box\n            gt_boxes = np.delete(gt_boxes, best_match_gt_idx, axis=0)\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fn += 1\n            fn_boxes.append(pred)\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fp = len(gt_boxes)\n    precision = tp \/ (tp + fp + fn)\n    return precision, fn_boxes, gt_boxes\n\n\ndef calculate_image_precision(preds_sorted, gt_boxes, thresholds=(0.5), form='coco', debug=False):\n    \n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    for threshold in thresholds:\n        precision_at_threshold, _, _ = calculate_precision(preds_sorted,\n                                                           gt_boxes,\n                                                           threshold=threshold,\n                                                           form=form\n                                                          )\n        if debug:\n            print(\"@{0:.2f} = {1:.4f}\".format(threshold, precision_at_threshold))\n\n        image_precision += precision_at_threshold \/ n_threshold\n    \n    return image_precision","7c2d9f5f":"\n\ndef train(args, model, device, train_data_loader, optimizer, epoch, iteration):\n    print(\"Epoch: \", epoch)\n    \n    #Train Loop\n    for batch_idx, (images, targets) in enumerate(train_data_loader):\n        \n        #Train and calculate loss\n        model.train()\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        \n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            #Log the train loss and iteration\n            wandb.log({\"train_loss\": loss_value, \"iteration_train\": iteration})\n        \n        iteration += 1 #iteration increases for every batch\n        \n    \n    return iteration #returns the updated new iteration\n            ","b34591bf":"#[xmin, ymin, xmax, ymax] => [x, y, width, height]\ndef convert_boxes_format(boxes):\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    return boxes\n\nclass_id_to_label = {\n    0: \"target\",\n    1: \"wheat\"\n}\n\n#Test will run as validation and log bounding boxes with confidence\/prediction scores\ndef test(args, model, device, test_data_loader, epoch, iteration):\n    \n    \n    epoch_precision_score = []\n    for batch_idx, (images, targets) in enumerate(test_data_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        model.eval()\n        outputs = model(images)\n        \n        #Calculate Precision Score Per Batch\n        batch_precision_score = []\n        for i, image in enumerate(images):\n            \n            #predicted bounding boxes\n            boxes = outputs[i]['boxes'].data.cpu().numpy()\n            boxes = convert_boxes_format(boxes)\n            \n            scores = outputs[i]['scores'].data.cpu().numpy()\n            \n            #ground truth bounding boxes\n            gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n            gt_boxes = convert_boxes_format(gt_boxes)\n\n            # Sort highest confidence -> lowest confidence\n            preds_sorted_idx = np.argsort(scores)[::-1]\n            preds_sorted = boxes[preds_sorted_idx]\n\n            iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n            image_precision = calculate_image_precision(preds_sorted, gt_boxes,\n                                            thresholds=iou_thresholds,\n                                            form='coco', debug=False)\n            \n            #print(\"Average Precision of image: {0:.4f}\".format(image_precision))\n            batch_precision_score.append(image_precision)\n        \n        batch_precision_score = np.mean(batch_precision_score) #average precision score for the batch\n        epoch_precision_score.append(batch_precision_score)\n        \n        if batch_idx % args.log_interval == 0:\n            #logs the precision score per batch and also the iteration\n            wandb.log({\"batch_score_validation\": batch_precision_score, \"iteration_validation\": iteration})\n        \n        #Log bounding boxes\n        if batch_idx % args.image_log_interval == 0:\n            \n            #Log 1 image with bounding boxes for this batch\n            for i, image in enumerate(images[:1]): \n\n\n                scores = outputs[i]['scores'].data.cpu().numpy().astype(np.float64)\n\n                #predicted bounding boxes\n                boxes = outputs[i]['boxes'].data.cpu().numpy().astype(np.float64)\n                predicted_boxes = []\n                for b_i, box in enumerate(boxes):\n    \n                    box_data = {\"position\" : {\n                          \"minX\" : box[0],\n                          \"maxX\" : box[2],\n                          \"minY\" : box[1],\n                          \"maxY\" : box[3] \n                        },\n                      \"class_id\" : 1,\n                      \"box_caption\" : \"wheat: (%.3f)\" % (scores[b_i]),\n                      \"domain\": \"pixel\",\n                      \"scores\" : { \"score\" : scores[b_i] }\n                    }\n                    predicted_boxes.append(box_data)\n                    \n\n                #ground truth bounding boxes\n                gt_boxes = targets[i][\"boxes\"].cpu().numpy().astype(np.float64)\n                target_boxes = [] \n                for b_i, box in enumerate(gt_boxes):\n  \n                    box_data = {\"position\" : {\n                          \"minX\" : box[0],\n                          \"maxX\" : box[2],\n                          \"minY\" : box[1],\n                          \"maxY\" : box[3] \n                        },\n                      \"class_id\" : 0,\n                      \"domain\": \"pixel\",\n                      \"box_caption\" : \"ground_truth\"\n                    }\n                    target_boxes.append(box_data)\n                \n                \n                image = image.permute(1,2,0).cpu().numpy().astype(np.float64)\n                \n                #create image object and log\n                img = wandb.Image(image, boxes = \n                                  {\"predictions\": \n                                   {\"box_data\": predicted_boxes, \n                                    \"class_labels\" : class_id_to_label},\"ground_truth\": {\"box_data\": target_boxes}})\n                \n                wandb.log({\"bounding_boxes\": img})\n                \n                \n                if(batch_precision_score > 0.3):\n                    #Log an image the model performs well on\n                    wandb.log({\"bounding_boxes\": img})\n                else:\n                    #Log an image the model performs poorly on\n                    wandb.log({\"bad_bounding_boxes\": img})\n            \n        iteration += 1\n    \n    score = np.mean(epoch_precision_score)\n    wandb.log({\"epoch_score_validation\": score, \"epoch\": epoch})\n    return iteration","8fca01e0":"#Load dataset\ntrain_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\nvalid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())","2d734cfe":"def run():\n    \n    #Default Hyperparameter values if no sweep is defined\n    config_default = {         \n    \"batch_size\": 16,          # input batch size for training (default: 64)\n    \"test_batch_size\": 8,    # input batch size for testing (default: 1000)\n    \"epochs\": 30,             # number of epochs to train (default: 10)\n    \"lr\": 0.005,               # learning rate (default: 0.01)\n    \"momentum\": 0.9,          # SGD momentum (default: 0.5) \n    \"no_cuda\": False,         # whether to disable CUDA training\n    \"seed\": 42,               # random seed (default: 42)\n    \"log_interval\": 1,      #how many batches to wait before logging in train\/test loops\n    \"image_log_interval\": 10,\n    \"decay\": 0.0005\n    }\n    os.environ[\"WANDB_API_KEY\"] = \"1988d5611a46703042decc8a5dfafd20a130796f\"\n    wandb.init(config=config_default, entity=\"authors\", project=\"GWD-fasterRCNN\")\n    config = wandb.config\n    \n    # load a model; pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    num_classes = 2  # wheat (1)....or not wheat (0)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    use_cuda = not config[\"no_cuda\"] and torch.cuda.is_available()\n    device = torch.device('cuda') if use_cuda else torch.device('cpu')\n\n    # Set random seeds and deterministic pytorch for reproducibility\n    # random.seed(config.seed)       # python random seed\n    torch.manual_seed(config[\"seed\"]) # pytorch random seed\n    np.random.seed(config[\"seed\"]) # numpy random seed\n    torch.backends.cudnn.deterministic = True\n\n\n    #Create dataloaders\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=config[\"batch_size\"],\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n\n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=config[\"test_batch_size\"],\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n\n\n    model.to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"decay\"])\n    \n    ####\n    #wandb.watch(model, log=\"all\") #this line only needs to be run once to hook W&B to your model (comment out for future runs)\n    ####\n    \n    #Keep track of train and test's iterations seperatedly (this allows for easier plotting because you can use iteration_train or iteration_test for your X axis)\n    iteration_train = 0\n    iteration_test = 0\n    for epoch in range(1, config[\"epochs\"] + 1):\n        #Runs training and returns the new training iteration counter\n        iteration_train = train(config, model, device, train_data_loader, optimizer, epoch, iteration_train)\n        #Runs testing\/valiadtion and returns the new test iteration counter\n        iteration_test = test(config, model, device, valid_data_loader, epoch, iteration_test)\n        print(\"Iteration Train: \", iteration_train)\n        print(\"Iteration Test: \", iteration_test)\n\n    torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')\n    wandb.save('fasterrcnn_resnet50_fpn.pth')\n    \n    #Clear memory\n    del model\n    del train_data_loader\n    del valid_data_loader\n    gc.collect()\n    torch.cuda.empty_cache()","cb24c3ea":"#Run sweep\n#wandb.agent(sweep_id, run)\nrun()","aecaa797":"# Analyzing Bad Batches - What Wheat Images does FasterRCNN Struggle With ?\n\nIn a previous notebook, we gave a hyperparameter sweep template using the Weights and Biases library: [Notebook](http:\/\/https:\/\/www.kaggle.com\/kshen3778\/hyperparameter-search-for-fasterrcnn?scriptVersionId=36233123)\n\nThis notebook will look at how a model can perform differently on different batches of images and go more into depth on the W&B bounding box logging feature.\nBecause our wheat dataset is collected from different geographical regions that vary visually, this may result in differing performance on different images. This notebook uses [Weights and Biases library](https:\/\/docs.wandb.com\/)'s logging functionality with FasterRCNN in order to see what data the model is performing poorly on. We also look at how shuffling can create batches that are representative of the data.\n\nWe run our model for 30 epochs. We reserve approximately 75% of images as training data and 25% as validation for logging. An explanation of the results is available at the end of the notebook.\n\n\n## [Explore Results in a Live Dashboard \u2192](https:\/\/app.wandb.ai\/authors\/GWD-fasterRCNN?workspace=user-kshen)\n![](https:\/\/i.imgur.com\/5YqjGr7.png)\n\n\n- [W&B for Kaggle](https:\/\/www.wandb.com\/kaggle)\n\n\n- Original FasterRCNN notebooks (@pestipeti):\n    - [Kernel 1](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)\n\n    - [Kernel 2](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-inference)","6f70aeb7":"In the test function we will also log the bounding boxes so we can see visualizations.","3e8057e7":"\n![Screenshot_252.png](attachment:Screenshot_252.png)","70cb93b6":"From the images above, we can see that good images are mostly on a darker surface with clearly defined wheat heads. The background is not cluttered with other vegetation. On the other hand, bad images where the model does poorly are often cluttered with a lot of vegetation intertwined with the wheat heads. We can see that it is often challenging to visually distinguish wheat heads, which may cause errors in labeling.","951d8f01":"### Benefits of Shuffling Data\n\nBy comparing the \"non-shuffled\" with the shuffled runs, we see the benefit of shuffling data. By mixing good images with bad images, it averages out the model's performance. This prevents the model from learning from a specific set of images and then being tested\/validated on images from another region or with different features. At most, it may result in a few points higher in the score, and at the very least, shuffling the data gives a more representative view of the model's true capabilities during validation\/testing.\n","9c551649":"By looking at the charts produced, we can see that the \"non-shuffled\" run performs very differently and is much lower on the average validation score logged per epoch. This difference is even more staggering when the per batch validation score is logged. We see periodic dips to the 0.2 level then rising back up. The main reason behind this is the ordering of the data.\n\nBecause \"non-shuffled\" simply takes the last 674 images as validation data, there could be specific images that cause inferior performance. These could be images from a specific geographical region or due to some noise. We can find out what images the model performs poorly on by explicitly logging bad images when the calculated batch validation score is lower than 0.3 (this is done at the end of the test() function above).\n\nBelow we show some examples of bad images and good images in the \"non-shuffled\" run along with the model's predictions.","1051583a":"# Setup","3a23e03d":"The run function is our main function that puts everything together.\n\n**Note:** We are keeping track of the iterations separately for train and test. \n\nThis will allow for easier plotting of values in your W&B dashboard.\n\nYou can set the X axis for charts to use iteration_train for displaying training loss\/accuracy, or iteration_test for displaying values such as validation\/test loss\/accuracy.\n\nThe wandb.watch() function only needs to be called once, so in subsequent runs we can comment it out.","7f9127b1":"### Blue boxes = ground truth labels, Red boxes = model predictions\n\n### Bounding Boxes of Good Images\n![](https:\/\/i.imgur.com\/h4jKYA0.png)\n\n### Bounding Boxes of Bad Images\n![](https:\/\/i.imgur.com\/5Wmoi29.png)","673d93ef":"## Train-Validation Split","66848acf":"# Results and Analysis\nWe log three runs in total each going for 30 epochs:\n\n**1. Non-shuffled: We don't shuffle the dataset before doing a train-validation split:** \n\nhttps:\/\/app.wandb.ai\/authors\/GWD-fasterRCNN\/runs\/3h9d743s\n\n**2. Shuffled-dataloader: We only shuffle the data in the Dataloader (shuffle=True):**\n\nhttps:\/\/app.wandb.ai\/authors\/GWD-fasterRCNN\/runs\/2akilbxr\n\n**3. Shuffled-both: We shuffle the data before splitting (using train_test_split) and also shuffle the data in the Dataloader (shuffle=True):** \n\nhttps:\/\/app.wandb.ai\/authors\/GWD-fasterRCNN\/runs\/2jjqtlf1\n\n","314aa86b":"# Logging Metrics and Predicted Bounding Boxes\nHere we define train and test functions that will use the W&B logging features."}}