{"cell_type":{"4abf03c7":"code","61d459c5":"code","99763949":"code","358c94cc":"code","81488439":"code","2a947d8d":"code","4d526513":"code","87337c90":"code","bbecf712":"code","839e4f0b":"code","5b008b25":"code","ecbd8766":"code","18e1a841":"code","ed85bdaf":"markdown","91305fb5":"markdown","3b00e4f8":"markdown","e1fea4ec":"markdown","70669b2e":"markdown","7b691b99":"markdown","934f7676":"markdown","99c288f0":"markdown","3a7f8bb5":"markdown","0bc38466":"markdown","1a584f2b":"markdown","4d73fe28":"markdown","efdd05c1":"markdown","8d955863":"markdown","b19facda":"markdown","7b8ef9cf":"markdown","e0aa71d0":"markdown","7ae6d73e":"markdown","2ce3e245":"markdown","5fe1366f":"markdown"},"source":{"4abf03c7":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom keras.layers.normalization import BatchNormalization\nimport os","61d459c5":"X = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ny = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nprint(X.shape)\nprint(y.shape)\nX = X.reshape(-1, X.shape[1], X.shape[2], 1)","99763949":"print(min(X.flatten()))\nprint(max(X.flatten()))","358c94cc":"y_labels = np.where(y == 1)[1]\nbars = np.asarray(range(0,10,1))\nn, bins, patches = plt.hist(x=y_labels, bins=len(bars), color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nbins += 0.5\nplt.xticks(bins,bars)\nplt.xlabel('Digit', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)","81488439":"class LeNet:\n\n    def build(width, height, depth, classes):\n    \n        model = Sequential()\n        inputShape = (height, width, depth)\n\n        model.add(Conv2D(20, 5, padding=\"same\",\n                         input_shape=inputShape))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Conv2D(50, 5, padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Flatten())\n        model.add(Dense(500))\n        model.add(Activation(\"relu\"))\n\n        model.add(Dense(classes))\n        model.add(Activation(\"softmax\"))\n\n        return model\n","2a947d8d":"class MiniVGGNet:\n    \n    def build(width, height, depth, classes):\n        model = Sequential()\n        inputShape = (height, width, depth)\n        \n        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        model.add(Flatten())\n        model.add(Dense(500))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(classes))\n        model.add(Activation(\"softmax\"))\n        \n        return model","4d526513":"(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.05, random_state=42)\nprint(\"[INFO] compiling model...\")\nopt = SGD(lr=0.01)\nmodel1 = LeNet.build(width=64, height=64, depth=1, classes=10)\nmodel1.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","87337c90":"print(\"[INFO] training network...\")\nH = model1.fit(trainX, trainY, validation_split=0.15, batch_size=128, epochs=50, verbose=1)","bbecf712":"print(\"[INFO] evaluating network...\")\npredictions = model1.predict(testX, batch_size=128)\nprint(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=[str(x) for x in np.unique(y_labels)]))","839e4f0b":"plt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, 50), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, 50), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, 50), H.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, 50), H.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\nplt.show()","5b008b25":"model2 = MiniVGGNet.build(width=64, height=64, depth=1, classes=10)\nmodel2.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\nH1 = model2.fit(trainX, trainY, validation_split=0.15, batch_size=128, epochs=50, verbose=1)","ecbd8766":"print(\"[INFO] evaluating network...\")\npredictions = model2.predict(testX, batch_size=128)\nprint(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=[str(x) for x in np.unique(y_labels)]))","18e1a841":"plt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, 50), H1.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, 50), H1.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, 50), H1.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, 50), H1.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\nplt.show()","ed85bdaf":"### EDA\nLet's start with some basic EDA before we dive into the network architectures. The directory has two files in the npy format - the input image array and the target array.\n","91305fb5":"We can also see that the pixel values have been normalized between 0 and 1. Neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such it is good practice to normalize the pixel values so that each pixel value has a value between 0 and 1.","3b00e4f8":"### Training LeNet\n\nWe now split our training and testing data and then instantiate a LeNet object to train the data on.","e1fea4ec":"### Overview\nThis post is about classifying sign language digits using two popular CNN architectures that are often used for digits classification - LeNet and VGGNet.\n\nFor the classificaion problem, I have implemented both the architectures as classes and built models for each of the architectures to evaluate the accuracy. ","70669b2e":"We now plot a histogram to get an idea about the number of images in each class. A disproportional amount of images in any one class could end up biasing the model towards that class.","7b691b99":"### Training mini-VGGNet\n\nWe now train the smaller version of VGGNet.","934f7676":"As we can see, our input consists of around 2000 training images with 64x64 dimensions. The target array is a 10d-vector containing a 1 in the respective index of the class it represents and a 0 otherwise. For example, the target for an image which belongs to class 4 is [0,0,0,0,1,0,0,0,0,0].\n\nWe also reshape our input array so that the number of channels is included in the shape. Since all the images are grayscale, we reshape the input array such that the number of channels is set to 1. ","99c288f0":"Plotting for training and validation loss and accuracy.","3a7f8bb5":"Below is a table highlighting the specifics of each layer in the network. \n\n\n| Layer               |    Output    | Filter Size\/N |\n|---------------------|:------------:|:-------------:|\n| Input image         |  64 x 64 x 1 |               |\n| Conv layer          | 64 x 64 x 32 |    3 x 3\/32   |\n| Activation          | 64 x 64 x 32 |               |\n| Batch normalization | 64 x 64 x 32 |               |\n| Conv layer          | 64 x 64 x 32 |    3 x 3\/32   |\n| Activation          | 64 x 64 x 32 |               |\n| Batch normalization | 64 x 64 x 32 |               |\n| Maxpool             | 32 x 32 x 32 |     2 x 2     |\n| Dropout             | 32 x 32 x 32 |               |\n| Conv layer          | 32 x 32 x 64 |    3 x 3\/64   |\n| Activation          | 32 x 32 x 64 |               |\n| Batch normalization | 32 x 32 x 64 |               |\n| Conv layer          | 32 x 32 x 64 |    3 x 3\/64   |\n| Activation          | 32 x 32 x 64 |               |\n| Batch normalization | 32 x 32 x 64 |               |\n| Maxpool             | 16 x 16 x 64 |     2 x 2     |\n| Dropout             | 16 x 16 x 64 |               |\n| Fully connected     |      500     |               |\n| Activation          |      500     |               |\n| Batch normalization |      500     |               |\n| Dropout             |      500     |               |\n| Fully connected     |      10      |               |\n| Softmax             |      10      |               |\n\n\n<br><p>\nThe advantage of combining convolutional layers together is that the kernels are able to learn a richer set of features before pooling is done. Batch normalization and dropout are \"utility layers\" which help prevent overfitting on the data.<\/p>","0bc38466":"Looking at the graph of the training loss, we can see that the model converges more smoothly in case of the VGGNet. We can also see that the model achieves high training accuracy with possible overfit, but the results from the testing set are satisfying enough to validate the model as good enough.","1a584f2b":"For our implementation of the architecture, a single input to the model is a grayscale image of dimensions 64x64x1. We start with a Conv2D layer having 20 convolutional filters of receptive field F = 5, i.e, 5x5 filters. This is activated using ReLu and fed into a Maxpooling layer of 2x2 with the same stride that reduces the input dimensions by half while keeping the depth the same. \n\nThe above combination of layers is fed into another similar set where we have 50 convolutional filters of F = 5, followed by ReLu activation and Maxpooling. The output of these two sets is fed into a Fully Connected Layer of 500 nodes that is then directed towards a Softmax classifier which outputs the 10d vector that indicates the class of the image.","4d73fe28":"![LeNet Architecture](http:\/\/blog.dataiku.com\/hs-fs\/hubfs\/Dataiku%20Dec%202016\/Image\/le_net.png?width=620&name=le_net.png)\n\n[(Image source)](https:\/\/blog.dataiku.com\/deep-learning-with-dss)","efdd05c1":"### LeNet\n\nThe LeNet architecture is an excellent grassroots architecture that is both small enough to run on your system and large enough to provide interesting and robust results, especially for digit classification problems. The network was conceptualized by Yann LeCunn in his 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf).","8d955863":"Each number class has more or less the same number of training images so biasing the model is not a major problem at this point. ","b19facda":"### VGGNet\n\nThe VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, [Very Deep Convolutional Networks for Large Scale Image Recognition](https:\/\/arxiv.org\/abs\/1409.1556). The VGGNet architecture is generally characterized by the 3x3 convolutional layers across the entire network. The network is also known for stacking multiple Convolutional layers before applying Maxpooling.\n\nHowever, the implementation below is a much shallower version of the original VGGNet that I found in Adrian Rosebrock's book, [Deep Learning for Computer Vision](https:\/\/www.pyimagesearch.com\/deep-learning-computer-vision-python-book\/). Because we have a relatively small amount of training data, we can demonstrate that a much lighter network still produces an output that is good enough. Nevertheless, the below implementation still adheres to the core principles of VGGNet and also includes batch normalization and dropout layers.","7b8ef9cf":"Plotting for training and validation loss and accuracy.","e0aa71d0":"As we can see, the decrease in the training loss does not happen immediately. Rather, it gradually reduces and converges at a slower rate, which gives us the impression that running the model for about 50 epochs must have probably overfit the model. This also shows in the comparitively lower precision obtained from the testing set.","7ae6d73e":"We train the model by calling the fit() function of the LeNet model.","2ce3e245":"As we can see, the architecture consists of 2 sets of Convolutional Layer => Max Pool Layer, followed by a Hidden Layer that is finally fed into a Softmax classifier,","5fe1366f":"We now test the model on the test set."}}