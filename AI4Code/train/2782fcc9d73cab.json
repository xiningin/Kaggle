{"cell_type":{"de6fc65f":"code","1c00747d":"code","29c991be":"code","7371e92e":"code","777820dd":"code","c51c81fa":"code","e3d98bb7":"code","8f0bb7bc":"code","dda06328":"code","db29c2b3":"markdown","95ea3be0":"markdown","3106a42f":"markdown","b78dfed4":"markdown","cee637f3":"markdown","b1bb8d9c":"markdown","3228c8f0":"markdown","a95ba9a4":"markdown","93598daa":"markdown","d8fbb24c":"markdown","00d92ade":"markdown","2ce3fcba":"markdown","b79a2b2c":"markdown","5068afc6":"markdown","274256b7":"markdown"},"source":{"de6fc65f":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport re\nfrom nltk.corpus import stopwords","1c00747d":"df= pd.read_csv(\"..\/input\/datatwitter\/tweets.csv\")\ndf.head()","29c991be":"print(\"Shape of the dataframe is\",df.shape)\nprint(\"The number of nulls in each column are \\n\", df.isna().sum())","7371e92e":"def cleaning_text(text) :\n    text = re.sub(r'[^0-9A-Za-z \\t]',r'' ,text)\n    text = re.sub(r'\\b\\w(1,2)\\b',r'',text)\n    text = re.sub(r'[@]',r' ',text)\n    text = re.sub(r'\\s\\s+',r'',text)\n    return text\n\ndf['CleanTweet'] = df['Tweet'].apply(lambda x: cleaning_text(x))\ndf.head()","777820dd":"def tokennization(text):\n    text = re.split ('\\W+', text)\n    return text\n\ndf['Tokennization'] = df['CleanTweet'].apply(lambda x: tokennization(x.lower()))\ndf.head()","c51c81fa":"from nltk.corpus import stopwords","e3d98bb7":"nltk.download(\"stopwords\")","8f0bb7bc":"stopword = nltk.corpus.stopwords.words('indonesian')\n\ndef remove_stopwords(text):\n  text = [word for word in text if word not in stopword]\n  return text\n\ndf['Stop_removal'] = df['Tokennization'].apply(lambda x: remove_stopwords(x))\ndf.head()","dda06328":"import pandas as pd\ndf = pd.DataFrame(df)\ndf.to_csv('output.csv', header='Lokasi', index=False)","db29c2b3":"# Tokenisasi","95ea3be0":"Pada proses ini saya menggunakan pandas, re dan nltk, pandas digunakan untuk bekerja pada proses dataset\/dataframe sedangkan nltk dan re untuk memproses data teks.","3106a42f":"> Pada dataset ini didapatkan 65 data kosong pada kolom Lokasi sedangkan pada kolom Tweet tidak ada data yang kosong","b78dfed4":"Pada proses ini saya melakukan proses text cleaning pada data text yang saya dapatkan dari crawling tweet dengan kata kunci \"indihome\" adapun proses pembersihan teks yang dilakukan antara lain :\n* meghapus data selain data teks (seperti emotikon ataupun angka)\n* tokenisasi, dan\n* menghapus kata-kata yang tidak dibutuhkan\/stopwords\n\nnamun selanjutnya menyimpan datanya dalam file csv\n","cee637f3":"Pada proses ini menghapuskan angka, whitecpace, emoticon, @, dan sebagainya selanjutnya disimpan dalam kolom CleanTweet","b1bb8d9c":"Pada proses ini menggunakan library nltk dengan modul stopwords yang berisikan kata-kata yang umum digunakan dan dianggap tidak memiliki nilai untuk masuk dalam proses analisa sentimen teks, selanjutnya dengan library nltk dan modul stopwords ini digunakan untuk menyisakan kata yang ada dalam teks yang tidak ada dalam list stopwords dan disimpan pada kolom Stop_removal","3228c8f0":"Pada proses ini membagi kalimat perkata selanjutnya disimpan dalam kolom Tokennization","a95ba9a4":"> Pada dataset ini terdapat 2 kolom yaitu lokasi dan isi tweeet","93598daa":"Describe dataset","d8fbb24c":"# Menghapus kata-kata yang tidak dibutuhkan","00d92ade":"# Simpan dalam csv","2ce3fcba":"Baca data dari hasil crawling twitter sebelumnya","b79a2b2c":"Pada proses ini menggunakan pandas untuk eksport hasil proses cleaning teks dengan ekstensi csv","5068afc6":"# Import Library package yang dibutuhkan","274256b7":"# Merapihkan data text "}}