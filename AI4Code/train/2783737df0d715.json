{"cell_type":{"4beefdd4":"code","78bb2dac":"code","5d96235d":"code","c7cab79e":"code","de855749":"code","a546ef0e":"code","1c527210":"code","1883630a":"code","f33c0763":"code","7e9871f5":"code","f077da0e":"code","7ce7e830":"code","a5db4736":"code","ce660ab6":"code","62e957f6":"code","c99d64d1":"markdown","1b80059b":"markdown","9317e8f9":"markdown","f9d99720":"markdown","3a1388c6":"markdown","5e2c3435":"markdown","8c83d45b":"markdown","ce198a47":"markdown","3694831f":"markdown","c74044a3":"markdown","cf9139e1":"markdown"},"source":{"4beefdd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78bb2dac":"data = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndata.info()\nqualities = data.quality","5d96235d":"#If the quality value is above 5, we accept the wine as quality\ndata[\"quality\"] = [1 if i>data.quality.mean()+1 else 0 for i in data.quality]\ndata","c7cab79e":"x_data = data.drop([\"quality\"],axis=1)\ny = data.quality.values","de855749":"#Normalization\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n#Create a dictionary to see all values at the end\nscores = {}","a546ef0e":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state = 2)","1c527210":"from sklearn.linear_model import LogisticRegression\n\nlg = LogisticRegression()\nlg.fit(x_train,y_train)\nscores[\"Linear_Regression\"] = lg.score(x_test,y_test)\n\nprint(\"Logistic Regression Accuracy = \",lg.score(x_test,y_test))","1883630a":"from sklearn.neighbors import KNeighborsClassifier\n\nscore = 0\nbest_n = 0\nscore_list = []\n\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    score_temp = knn.score(x_test,y_test)\n    score_list.append(score_temp)\n    if score_temp > score:\n        score = score_temp\n        best_n = i\n        \nscores[\"KNN\"] = score\n\nprint(\"KNN Accuracy : \",score)\nplt.plot(range(1,100),score_list)","f33c0763":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\nscores[\"SVM\"] = svm.score(x_test,y_test)\n\nprint(\"Accuracy of SVM:\",svm.score(x_test,y_test))","7e9871f5":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nscores[\"NB\"] = nb.score(x_test,y_test)\n\nprint(\"Accuracy of Navie Byes = \",nb.score(x_test,y_test))","f077da0e":"from sklearn.tree import DecisionTreeClassifier\n\ntr = DecisionTreeClassifier()\ntr.fit(x_train,y_train)\n\nscores[\"DecisionTree\"] = nb.score(x_test,y_test)\n\nprint(\"Accuracy of Decision Tree = \",nb.score(x_test,y_test))","7ce7e830":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\n\nscores[\"Random Forest\"] = rf.score(x_test,y_test)\n\nprint(\"Accuracy of Random Forest:\",rf.score(x_test,y_test))\n","a5db4736":"from sklearn.cluster import KMeans\n\nwcss = []\nfor k in range(1,20):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,20),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")","ce660ab6":"kmeans2 = KMeans(n_clusters=2)\nclusters = kmeans2.fit_predict(data)\n\ndata[\"label\"] = clusters","62e957f6":"scores","c99d64d1":"* The best cluster is 3","1b80059b":"# K-MEANS","9317e8f9":"# K-Nearest Neighbour Classification","f9d99720":"# NA\u0130VE BYES","3a1388c6":"# LOGISTIC REGRESSON","5e2c3435":"* At least we can see which has most accuracy","8c83d45b":"TRA\u0130N TEST SPL\u0130T","ce198a47":"NORMAL\u0130ZAT\u0130ON","3694831f":"# RANDOM FOREST CLASSIFIER","c74044a3":"# SVM","cf9139e1":"# DECISION TREE CLASSIFIER"}}