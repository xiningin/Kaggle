{"cell_type":{"05cb8b2f":"code","9776e75b":"code","96bf0f0c":"code","bd5fd27d":"code","f8317968":"code","f1350193":"code","82d13d47":"code","cb913048":"code","7049de12":"code","f2083e2a":"code","e5d9da08":"code","21831a87":"code","eb2ae030":"code","ad4ae340":"code","1bae88bc":"code","f72b94e5":"code","c42a2056":"code","38a36742":"code","82cdc054":"code","5ecc7b32":"code","d137250c":"code","b7fd5ea7":"code","0d341bcd":"code","f5d2773c":"code","f6f03c99":"code","e96cff44":"code","433025df":"code","4643f180":"code","bb60db31":"code","4f9e7f56":"code","536f7a5f":"code","e5b957c6":"code","d1b88bc9":"markdown","f1c85389":"markdown","ac12acd2":"markdown","6d4ec510":"markdown","dac6fcb9":"markdown","d0630947":"markdown"},"source":{"05cb8b2f":"!git clone https:\/\/github.com\/huggingface\/transformers\n","9776e75b":"cd transformers\n","96bf0f0c":"!pip install .\n!pip install -r .\/examples\/requirements.txt","bd5fd27d":"cd .. ","f8317968":"!dir transformers\/examples\/","f1350193":"!ls ..\/input\/kerastransformer\/example\/wikitext-2-raw-v1\/wikitext-2-raw\/","82d13d47":"pwd","cb913048":"#!rm -r \/kaggle\/working\/outputs\n!mkdir \/kaggle\/working\/outputs\n!ls \/kaggle\/working\/\n","7049de12":"!mkdir \/kaggle\/working\/input_data\n!dir \/kaggle\/working\/","f2083e2a":"!cp \/kaggle\/input\/kerastransformer\/example\/wikitext-2-raw-v1\/wikitext-2-raw\/wiki.train.raw \/kaggle\/working\/input_data\n!cp \/kaggle\/input\/kerastransformer\/example\/wikitext-2-raw-v1\/wikitext-2-raw\/wiki.test.raw \/kaggle\/working\/input_data\n!du -s -h \/kaggle\/working\/input_data\/*","e5d9da08":"!head -n 4 \/kaggle\/working\/input_data\/wiki.test.raw \n","21831a87":"# from transformers import BertTokenizer, BertConfig, BertForMaskedLM\n# #help(BertModel)\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForMaskedLM.from_pretrained('bert-base-uncased')","eb2ae030":"# #!rm -r \/kaggle\/working\/output_pretrained\n# !mkdir \/kaggle\/working\/output_pretrained\n# model.save_pretrained(\"\/kaggle\/working\/output_pretrained\")\n# tokenizer.save_pretrained(\"\/kaggle\/working\/output_pretrained\")\n\n# !du -s -h \/kaggle\/working\/output_pretrained\/*","ad4ae340":"# !python transformers\/examples\/run_language_modeling.py \\\n#     --output_dir=\/kaggle\/working\/output_pretrained \\\n#     --model_type=bert \\\n#     --model_name_or_path=\/kaggle\/working\/output_pretrained \\\n#     --train_data_file=\/kaggle\/working\/input_data\/wiki.train.raw \\\n#     --do_eval \\\n#     --eval_data_file=\/kaggle\/working\/input_data\/wiki.test.raw \\\n#     --mlm","1bae88bc":"# Baseline perplexity score for pretrained model\n\n#!head \/kaggle\/working\/output_pretrained\/eval_results.txt","f72b94e5":"# !rm -r \/kaggle\/working\/output_pretrained\n# !rm \/kaggle\/working\/input_data\/bert_cached_lm_510_wiki.test.raw","c42a2056":"!python transformers\/examples\/run_language_modeling.py \\\n    --output_dir=\/kaggle\/working\/outputs \\\n    --model_type=bert \\\n    --model_name_or_path=bert-base-uncased \\\n    --do_train \\\n    --train_data_file=\/kaggle\/working\/input_data\/wiki.train.raw \\\n    --do_eval \\\n    --eval_data_file=\/kaggle\/working\/input_data\/wiki.test.raw \\\n    --num_train_epochs=1 \\\n    --save_steps=400 \\\n    --eval_all_checkpoints \\\n    --mlm","38a36742":"!du -s -h \/kaggle\/working\/outputs\/*","82cdc054":"import glob\nimport os\nfrom transformers import WEIGHTS_NAME\nprint(WEIGHTS_NAME)\ndirectory_cpt = list(os.path.dirname(c) for c in sorted(glob.glob(\"\/kaggle\/working\/outputs\" + \"\/**\/\" + WEIGHTS_NAME, recursive=True)))\nprint(directory_cpt)","5ecc7b32":"for directory_cpt_name in directory_cpt:\n    tmp_dir = directory_cpt_name+\"\/eval_results.txt\"\n    print(\"\\n\"+tmp_dir)\n    !head {tmp_dir}","d137250c":"!head -n 10 \/kaggle\/working\/outputs\/vocab.txt","b7fd5ea7":"!head -n 10 \/kaggle\/working\/outputs\/special_tokens_map.json","0d341bcd":"!head \/kaggle\/working\/outputs\/config.json","f5d2773c":"# Load cached data and see\nimport pickle\nwith open(\"\/kaggle\/working\/input_data\/bert_cached_lm_510_wiki.train.raw\", \"rb\") as handle:\n    tmp_data = pickle.load(handle)\n","f6f03c99":"print(type(tmp_data))\nprint(len(tmp_data))\nprint(tmp_data[0])","e96cff44":"# Length of each sentence\nprint(len(tmp_data[0]))\nprint(len(tmp_data[1]))\n","433025df":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"\/kaggle\/working\/outputs\")\nprint(tokenizer.decode(tmp_data[0]))\n","4643f180":"print(tokenizer.max_len)\nprint(tokenizer.max_len_single_sentence)","bb60db31":"!head -n 15 \/kaggle\/working\/input_data\/wiki.test.raw","4f9e7f56":"from transformers import pipeline\nhelp(pipeline)","536f7a5f":"\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"\/kaggle\/working\/outputs\",\n    tokenizer=\"\/kaggle\/working\/outputs\"\n)","e5b957c6":"result = fill_mask( \"Robert Boulter is an [MASK] film actor\")\nresult","d1b88bc9":"RUN FINE TUNING","f1c85389":"**EVALUATE**","ac12acd2":"Pretrained Masked LM bert model gives baseline perplexity of **9.2404** on test wiki data","6d4ec510":"1. Clone Transformers library\n2. Install requirements\n3. Add data - Used Keras_transformers dataset which has WikiText dataset added\n4. Move data from input_data folder to working directory where Tokenized and transformed data will be stored\n5. Use pretrained LM model to get baseline score\n6. Run Fine tune LM code (Storing various checkpoints)\n7. It shows perplexity as evaluation metric ( Perplxity here is Masked LM loss )\n8. Also, inspect the cached dataset used by the model\n","dac6fcb9":"**Get pretrained model and baseline perplexity score on test dataset**","d0630947":"# Try language modeling fine tuning on WikiText data"}}