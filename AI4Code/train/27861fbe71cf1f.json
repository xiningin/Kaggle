{"cell_type":{"31989a1c":"code","f7bcf3fb":"code","0b26be47":"code","5a160670":"code","383afc9f":"code","cfac6f5e":"code","7c1af7e8":"code","200fa51d":"code","ed8b7523":"code","50706481":"code","8f053022":"code","d4336750":"code","56f61e1b":"code","67de64b9":"code","5e58f555":"code","6ff0fa35":"code","a22f6689":"code","e994a7fe":"code","ffd7d51c":"code","78ae869d":"code","ba531bee":"code","aac888fd":"code","fa58e87a":"code","3dd81ae1":"markdown","31e21a94":"markdown","dfeefb1f":"markdown","9800fd91":"markdown","6a381eca":"markdown","c6ba5882":"markdown","22462e2a":"markdown","e52e92f0":"markdown","457e442d":"markdown","ed37e500":"markdown","85e575b9":"markdown","ce5c95c6":"markdown","6b1c4b85":"markdown","2973f4ae":"markdown"},"source":{"31989a1c":"from collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.metrics import geometric_mean_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nfrom scipy.stats import kurtosis, skew\nfrom scipy import stats","f7bcf3fb":"FILE_PATH = '..\/input\/oil-spill\/oil-spill.csv'\n\noil_df = pd.read_csv(FILE_PATH, header=None)\n\n# Dropping unused columns\noil_df.drop([0,22], axis=1, inplace=True)","0b26be47":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\ndef DataDesc(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef CalOutliers(df_num): \n    '''\n    \n    Leonardo Ferreira 20\/10\/2018\n    Set a numerical value and it will calculate the upper, lower and total number of outliers\n    It will print a lot of statistics of the numerical feature that you set on input\n    \n    '''\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print(color.BOLD+f'Lower outliers: {len(outliers_lower)}'+ color.END) # printing total number of values in lower cut of outliers\n    print(color.BOLD+f'Upper outliers: {len(outliers_higher)}'+ color.END) # printing total number of values in higher cut of outliers\n    print(color.BOLD+f'Total outliers: {len(outliers_total)}'+ color.END) # printing total number of values outliers of both sides\n    print(color.BOLD+f'Non - outliers: {len(outliers_removed)}'+ color.END) # printing total number of non outlier values\n    print(color.BOLD+f'% of Outliers : {round((len(outliers_total) \/ len(outliers_removed) )*100, 4)}'+ color.END ) # Percentual of outliers in points","5a160670":"DataDesc(oil_df)","383afc9f":"fig = plt.figure(figsize=(50,50))\nax = fig.gca()\n\n_ = oil_df.hist(ax=ax)","cfac6f5e":"_ = plt.figure(figsize=(8,6))\n_ = sns.countplot(oil_df.values[:,-1], palette=['#9fb8ad','#475841'])\n_ = plt.xticks([0,1],['Non-Spill','Spill'], fontsize=15)\n\ntarget = oil_df.values[:,-1]\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v \/ len(target) * 100\n    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))","7c1af7e8":"def evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    metric = make_scorer(geometric_mean_score)\n    \n    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n    \n    return scores\n\nX, y = oil_df.values[:,:-1], oil_df.values[:,-1]\n\nprint(X.shape, y.shape, Counter(y))\n\nmodel = DummyClassifier(strategy='uniform')\n\nscores = evaluate_model(X, y, model)\n\nprint('Mean G-Mean: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n","200fa51d":"def get_models():\n    models, names = list(), list()\n    # LR\n    steps = [('t',StandardScaler()),('m',LogisticRegression(solver='liblinear'))] \n    models.append(Pipeline(steps=steps))\n    names.append('LR')\n    \n    # LDA\n    steps = [('t', StandardScaler()),('m',LinearDiscriminantAnalysis())] \n    models.append(Pipeline(steps=steps))\n    names.append('LDA')\n    \n    # NB\n    models.append(GaussianNB())\n    names.append('NB')\n    \n    return models, names\n\n\nmodels, names = get_models()\n\nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    \n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))","ed8b7523":"plt.figure(figsize=(12,6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","50706481":"def get_models():\n    names, models = list(), list()\n    \n    #LR balanced\n    models.append(LogisticRegression(solver='liblinear', class_weight='balanced'))\n    names.append('Balanced')\n        \n    #LR MinMax\n    steps = [('MinMax', MinMaxScaler()),('model',LogisticRegression(solver='liblinear', class_weight='balanced'))]\n    models.append(Pipeline(steps=steps))\n    names.append('Balanced-MinMax')\n    \n    #LR StandardScale\n    steps = [('StandScale', StandardScaler()),('model',LogisticRegression(solver='liblinear', class_weight='balanced'))]\n    models.append(Pipeline(steps=steps))\n    names.append('Balanced-Standard')   \n    \n    #LR Scale + Power Transform\n    steps = [('StandScale', MinMaxScaler()),('pt',PowerTransformer()), ('model',LogisticRegression(solver='liblinear', class_weight='balanced'))]\n    models.append(Pipeline(steps=steps))\n    names.append('Balanced-PowerTransform')\n    \n    return models, names\n\n\nmodels, names = get_models()\n\nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    \n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))","8f053022":"plt.figure(figsize=(12,6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","d4336750":"def get_sampled_data(X, y, name):\n    \n    sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n    \n    if(name == 'LR'):\n        X_s, y_s = sampling.fit_resample(X, y)\n        \n    elif(name == 'MINMAX'):\n        scaler = MinMaxScaler()\n        X_scale = scaler.fit_transform(X)\n        X_s, y_s = sampling.fit_resample(X_scale, y)\n    \n    elif(name == 'STDSCALE'):\n        scaler = StandardScaler()\n        X_scale = scaler.fit_transform(X)\n        X_s, y_s = sampling.fit_resample(X_scale, y)\n    \n    elif(name == 'POWERT'):\n        scaler = MinMaxScaler()\n        powert = PowerTransformer()\n        X_scale = scaler.fit_transform(X)\n        X_t = powert.fit_transform(X_scale)\n        X_s, y_s = sampling.fit_resample(X_t, y)   \n    \n    return X_s, y_s\n","56f61e1b":"def evaluate_model(X, y):\n    \n    model = LogisticRegression(solver='liblinear')\n    \n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    metric = make_scorer(geometric_mean_score)\n    \n    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n    \n    return scores","67de64b9":"results = list()\n\nfor name in ['LR', 'MINMAX', 'STDSCALE', 'POWERT']:\n    X_s, y_s = get_sampled_data(X, y, name)\n    scores = evaluate_model(X_s, y_s)\n    results.append(scores)\n    \n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))","5e58f555":"names = ['LR', 'MINMAX', 'STDSCALE', 'POWERT']\n\nplt.figure(figsize=(12,6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","6ff0fa35":"from sklearn.model_selection import train_test_split\n\nX_s, y_s = get_sampled_data(X, y, 'POWERT')\nX_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.1, stratify=y_s, random_state=1)\n\nmodel = LogisticRegression(solver='lbfgs')\n\nc_values = [10000, 1000, 800, 500, 100, 10, 1.0, 0.1, 0.01, 0.001]\ngrid = dict(C=c_values)\n\nmetric = make_scorer(geometric_mean_score)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring=metric,error_score=0)\n\ngrid_result = grid_search.fit(X_s, y_s)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","a22f6689":"from sklearn.metrics import confusion_matrix, classification_report\n\nbest_model = grid_result.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(classification_report(y_test, y_pred))","e994a7fe":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\ngmeans = np.sqrt(tpr * (1-fpr))\n\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\n\nprint('Best Threshold=%f, G-mean=%.3f' % (thresholds[ix], gmeans[ix]))","ffd7d51c":"plt.figure(figsize=(10,5))\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill') \n\nplt.plot(fpr, tpr, marker='.', label='Logistic') \nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best') \n# axis labels\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend()\n# show the plot\nplt.show()","78ae869d":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\n#Calculate f-Score\nfscore = (2 * precision * recall) \/ (precision + recall)\n\n# locate the index of the largest g-mean\nix = np.argmax(fscore)\n\nprint('Best Threshold=%f, F-score=%.3f' % (thresholds[ix], fscore[ix]))","ba531bee":"no_skill = len(y_test[y_test==1]) \/ len(y_test)\n\nplt.figure(figsize=(10,5))\nplt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill') \nplt.plot(recall, precision, marker='.', label='Logistic') \nplt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best') \n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\n# show the plot\nplt.show()","aac888fd":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","fa58e87a":"cnf_matrix = confusion_matrix(y_test, y_pred)\nclass_names = ['-VE','+VE']\nnp.set_printoptions(precision=2)\n\n\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","3dd81ae1":"<h3><center>4. Evaluate Models <\/center><\/h3>","31e21a94":"<h3>4.3. Evaluate Damp-Sampling with Probablistic models<\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nData sampling provides a way to better prepare the imbalanced training dataset prior to fitting a model. Perhaps the most popular data sampling is the SMOTE oversampling technique for creating new synthetic examples for the minority class. <br><br>This can be paired with the edited nearest neighbor (ENN) algorithm that will locate and remove examples from the dataset that are ambiguous, making it easier for models to learn to discriminate between the two classes. This combination is called SMOTE-ENN and can be implemented using the SMOTEENN class from the imbalanced-learn library\n    <\/div>","dfeefb1f":"<h3>Target Variable Distribution","9800fd91":"<h3><center>6. ROC Curve<\/center><\/h3>","6a381eca":"<h3><center>2. Exploring Data<\/center><\/h3>","c6ba5882":"<h3><center>1. Reading Input data<\/center><\/h3>","22462e2a":"<h2><center>Introduction<\/center><\/h2>\n\n![image.png](attachment:image.png)\n\n<h3>About the Dataset :<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nIn this project, we will use a standard imbalanced machine learning dataset referred to as the oil spill dataset, oil slicks dataset or simply oil. The dataset was introduced in the 1998 paper by Miroslav Kubat, et al. titled Machine Learning for the Detection of Oil Spills in Satellite Radar Images. The dataset is often credited to Robert Holte, a co-author of the paper. The dataset was developed by starting with satellite images of the ocean, some of which contain an oil spill and some that do not. Images were split into sections and processed using computer vision algorithms to provide a vector of features to describe the contents of the image section or patch.\n<br><br>\nIn the task, a model is given a vector that describes the contents of a patch of a satellite image, then predicts whether the patch contains an oil spill or not, e.g. from the illegal or accidental dumping of oil in the ocean. There are 937 cases. Each case is comprised of 48 numerical computer vision derived features, a patch number, and a class label.\n<br><br>\n    A total of nine satellite images were processed into patches. Cases in the dataset are ordered by image and the first column of the dataset represents the patch number for the image. This was provided for the purposes of estimating model performance per-image. In this case, we are not interested in the image or patch number and this first column can be removed. The normal case is no oil spill assigned the class label of 0, whereas an oil spill is indicated by a class label of 1. There are 896 cases for no oil spill and 41 cases of an oil spill.\n<\/div>","e52e92f0":"<h3><center>3. Model Test & Baseline Result<\/center><\/h3>","457e442d":"<h3>4.1. Probabilistic models <\/h3>","ed37e500":"<h3><center>8. Confusion Matrix<\/center><\/h3>","85e575b9":"<blockquote>In this case, we can see that the baseline algorithm achieves a G-mean of about 0.47, close to the theoretical maximum of 0.5. This score provides a lower limit on model skill; any model that achieves an average G-mean above about 0.47 (or really above 0.5) has skill, whereas models that achieve a score below this value do not have skill on this dataset.<\/blockquote>","ce5c95c6":"<h3>4.2. Evaluate balanced Logistic Regression<\/h3>","6b1c4b85":"<h3><center>7. Precision Recall Curve<\/center><\/h3>","2973f4ae":"<h3><center>5. Fitting Model<\/center><\/h3>"}}