{"cell_type":{"5edacb52":"code","c85f8a83":"code","34d271e1":"code","1c4ba7f0":"code","add825e2":"code","e0f0d148":"code","1acba430":"code","e2ad931a":"code","b0a527fc":"code","7ca9c192":"code","d129529f":"code","fdc8b860":"code","e27b9b8e":"code","9bd6eaf1":"code","c6bca036":"code","657a5019":"code","3463c797":"markdown","8338f337":"markdown","ad007159":"markdown","90c91fcb":"markdown","19df88d1":"markdown","92b741f8":"markdown","9f0ec0c6":"markdown","d4df177f":"markdown","216f0083":"markdown","2f4f282d":"markdown","6805b01c":"markdown","608a80b9":"markdown","ed421e17":"markdown","5070bf2a":"markdown","9ac32def":"markdown","2e1bf73c":"markdown","ba3be8e2":"markdown","f08a94fc":"markdown","1956e125":"markdown"},"source":{"5edacb52":"import warnings\nfrom sklearn.exceptions import DataConversionWarning\n\n# Suppress warnings\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)","c85f8a83":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Load train and test data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# All data\ndata  = train.append(test, sort=False)","34d271e1":"# Columns\nprint(len(data.columns))\ndata.columns","1c4ba7f0":"# Example\ndata.sample(n=10)","add825e2":"types = pd.DataFrame(data.dtypes).rename(columns={0: 'type'}).sort_values(by=['type'],ascending=False)\ntypes","e0f0d148":"# Check missing values\ndef check_missing(df):\n    null_val = df.isnull().sum()\n    percent = 100 * df.isnull().sum()\/len(df)\n    missing_table = pd.concat([null_val, percent], axis=1)\n    col = missing_table.rename(columns = {0 : 'Num', 1 : 'Rate'})\n    return col\n\n# Display columns missing values are under 1%.\nprint(\"Data #\"+str(len(data)))\ncols = check_missing(data)\n\ntypes.join(cols).sort_values(by=\"Rate\", ascending=False)","1acba430":"# Drop Cabin\ndata.drop(['Cabin'], axis=1, inplace = True)\n\n# \"Embarked\": Fill NA and map into integer\ndata[\"Embarked\"] = data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0])\ndata[\"Embarked\"] = data[\"Embarked\"].map({\"S\": 0, \"C\" : 1, \"Q\" : 2})\n\n# \"Name\": Map passenger's name with their title\ntitle_mapping    = {\n    '(.+)Mr\\.(.+)': 1, '(.+)Master\\.(.+)': 1,\n    '(.+)Dr\\.(.+)': 2, '(.+)Don\\.(.+)': 2, '(.+)Major\\.(.+)': 2,\n    '(.+)Sir\\.(.+)':2, '(.+)Col\\.(.+)': 2, '(.+)Jonkheer\\.(.+)': 2,\n    '(.+)Capt\\.(.+)': 2,'(.+)Countess\\.(.+)': 2, '(.+)Dona\\.(.+)': 2,\n    '(.+)Rev\\.(.+)': 3,\n    '(.+)Ms\\.(.+)': 4, '(.*)Miss\\.(.+)': 4, '(.+)Mrs\\.(.+)': 4,\n    '(.+)Mme\\.(.+)': 4,'(.+)Lady\\.(.+)': 4, '(.+)Mlle\\.(.+)': 4 \n}\ndata[\"Title\"] = data[\"Name\"].replace(title_mapping, regex=True).astype('int')\n\n# \"Sex\": Map male and female\ndata[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\": 1})","e2ad931a":"# Do nothing.","b0a527fc":"# Estimate missing age from title\nfor i in range(1, 5):\n    age_to_estimate = data.groupby('Title')['Age'].median()[i]\n    data.loc[(data['Age'].isnull()) & (data['Title'] == i), 'Age'] = age_to_estimate\n\n# Estimate missing fare from pclass\nfor i in range(1, 4):\n    fare_to_estimate = data.groupby('Pclass')['Fare'].median()[i]\n    data.loc[(data['Fare'].isnull()) & (data['Pclass'] == i), 'Fare'] = fare_to_estimate\n\n# Standardize numerical values\nss = StandardScaler()\nss.fit_transform(data[['Age', 'Fare']])\n\n# Cut Age into 10 categolies\ndata[\"AgeBin\"]  = pd.qcut(data[\"Age\"], 10, duplicates=\"drop\", labels=False)\n\n# Cut Fare into 10 categolies\ndata[\"FareBin\"] = pd.qcut(data[\"Fare\"], 10, duplicates=\"drop\", labels=False)","7ca9c192":"# Add FamilySize\ndata['FamilySize'] = data[\"Parch\"] + data[\"SibSp\"]\n\n# Add IsFamily\ndata['IsFamily'] = data[\"Parch\"] + data[\"SibSp\"]\ndata.loc[data['IsFamily'] > 1, 'IsFamily']  = 2\ndata.loc[data['IsFamily'] == 1, 'IsFamily'] = 1\ndata.loc[data['IsFamily'] == 0, 'IsFamily'] = 0\n\n# Add FamilySurvival\nDEFAULT_SURVIVAL_VALUE = 0.5\ndata['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\n\n# Get last name\ndata['LastName'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Family has same Lastname and Fare\nfor grp, grp_df in data.groupby(['LastName', 'Fare']):\n    if(len(grp_df) != 1):\n        # A Family group is found\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n            elif (smin==0.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0\n\n# Family(or group) has same Ticket No\nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n                elif (smin==0.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0","d129529f":"# Drop Useless data\ntrain_target = data[:891][\"Survived\"].values\ndata.drop(['Name', 'PassengerId', 'Age', 'Fare', 'Ticket', 'LastName'], axis = 1, inplace = True)\n\n# One-Hot Encoding Categorical variables\ndata = pd.get_dummies(data, columns=[\"Embarked\", \"Title\", \"Sex\", \"IsFamily\", \"FamilySurvival\"], drop_first=True)\n\n# Set data\ntrain = data[:891]\ntest  = data[891:]\n\n# Data types\ndata.dtypes","fdc8b860":"# Plot correlations between variables\nplt.figure(figsize=(20, 10))\nsns.heatmap(train.corr(), annot=True, fmt='.2f')","e27b9b8e":"possible_features = train.columns.copy().drop('Survived')\n\n# Check feature importances\nselector = SelectKBest(f_classif, len(possible_features))\nselector.fit(train[possible_features], train_target)\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Feature importances:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], possible_features[indices[i]]))","9bd6eaf1":"# Display all possible features with Survival\nfig, axs = plt.subplots(8, 2, figsize=(20, 30))\nfor i in range(0, 16):\n    sns.countplot(possible_features[i], data=train, hue=train_target, ax=axs[i%8, i\/\/8])","c6bca036":"# Feature params\nfparams = \\\n    ['Sex_1', 'Title_4', 'FamilySurvival_1.0', 'Pclass', 'FareBin', \n     'FamilySurvival_0.5', 'Embarked_1', 'IsFamily_1', 'IsFamily_2', \n     'Parch', 'Title_3', 'AgeBin', 'SibSp']\n\n# Get params\ntrain_features = train[fparams].values\ntest_features  = test[fparams].values\n\n# Number of Cross Validation Split\nCV_SPLIT_NUM = 6\n\n# Params for RandomForestClassifier\nrfgs_parameters = {\n    'n_estimators': [300],\n    'max_depth'   : [2,3,4],\n    'max_features': [2,3,4],\n    \"min_samples_split\": [2,3,4],\n    \"min_samples_leaf\":  [2,3,4]\n}\n\nrfc_cv = GridSearchCV(RandomForestClassifier(), rfgs_parameters, cv=CV_SPLIT_NUM)\nrfc_cv.fit(train_features, train_target)\nprint(\"RFC GridSearch score: \"+str(rfc_cv.best_score_))\nprint(\"RFC GridSearch params: \")\nprint(rfc_cv.best_params_)","657a5019":"# Predict and output to csv\nsurvived = rfc_cv.best_estimator_.predict(test_features)\npred = pd.DataFrame(pd.read_csv(\"..\/input\/titanic\/test.csv\")['PassengerId'])\npred['Survived'] = survived.astype(int)\npred.to_csv(\"..\/working\/submission.csv\", index = False)","3463c797":"* [Load Data and Libraries](#load)\n* [Check Data](#check)\n* [Data Pre-Processing](#data-process)\n* [Data Visualizations](#visualization)\n* [Training and Estimation](#training)","8338f337":"# Load Data and Libraries <a id='load'><\/a>","ad007159":"# Data Pre-Processing <a id=\"data-process\"><\/a>","90c91fcb":"```\n- Categorical String variables\nCol         Type      Num     Rate\nCabin       object    1014    77.463713\nEmbarked    object    2       0.152788\nName\t\tobject    0       0.000000\nSex         object    0       0.000000\nTicket      object    0       0.000000\n```\n* **Cabin**\n  * Just drop this variable, because 77.4% of values aremissing.\n* **Embarked**\n  * Fill missing values as mode value.(S)\n  * Map strings into integers: S->0, C->1, Q->2\n  * One-Hot Encoding\n* **Name**\n  * Map strings into integers: Mr,Master->1, Dr,Don,Major,etc->2, Rev->3, Ms,Miss,Mrs,Mme->4\n  * One-Hot Encoding\n* **Sex**\n  * Map strings into integers: male->0, female->1\n  * One-Hot Encoding\n* **Ticket**\n  * Leave as it is.","19df88d1":"# Data Visualizations <a id=\"visualization\"><\/a>","92b741f8":"For data pre-processing, categorize variables\n```\n# Categorical String variables\nEmbarked        object\nCabin           object\nTicket          object\nSex             object\nName            object\n\n# Categorical Integer variables (have order)\nParch            int64\nSibSp            int64\nPclass           int64\n\n# Numerical variables\nFare           float64\nAge            float64\n```","9f0ec0c6":"There're 12 variables in dataset","d4df177f":"Drop useless data and keep as Training and Test set.","216f0083":"# Check Data <a id=\"check\"><\/a>","2f4f282d":"```\n- Categorical Integer variables (have order)\nCol     Type    Num   Rate\nPclass  int64   0     0.000000\nSibSp   int64   0     0.000000\n```\n* **Ticket**\n  * Leave as it is.\n* **Ticket**\n  * Leave as it is.","6805b01c":"Pre-process data by their variable types, Categorical, Numerical, Integer, String.  \nAnd we now process training and test set together. (data = training + test)  \n(because we can process values in same scale)","608a80b9":"This time, pick variables by their importances\n```\nFeature importances:\n68.85 Sex_1\n67.32 Title_4\n32.23 FamilySurvival_1.0\n24.60 Pclass\n22.45 FareBin\n7.05 FamilySurvival_0.5\n6.36 Embarked_1\n6.01 IsFamily_1\n2.11 IsFamily_2\n1.83 Parch\n1.28 Title_3\n0.92 AgeBin\n0.53 SibSp\n0.21 FamilySize\n0.18 Title_2\n0.04 Embarked_2\n```","ed421e17":"```\n- Generate new variable information from other variables\nFamirySize\nIsFamily\nFamilySurvival\n```\n* **FamilySize**\n  * Size of Family: Parch + SibSp\n* **IsFamily**\n  * 0 families->0\n  * 1 families->1\n  * 2 or more families->2\n  * One-Hot Encoding\n* **FamilySurvival**\n  * Regard ones have same lastname or same ticketname as family\n  * One of their family survived -> 1\n  * All of their family dead -> 0\n  * No data -> 0.5\n  * One-Hot Encoding","5070bf2a":"Check missing rate","9ac32def":"Display all possible features with Survival","2e1bf73c":"Display feature importances","ba3be8e2":"```\n- Numerical variables\nCol     Type    Num   Rate\nAge     float64 263   20.091673\nFare    float64 1     0.076394\n```\n* **Age**\n  * Fill missing values from median of same name title (Mr,Mrs,etc)\n  * Standardize values\n  * Divide into 10 categories\n* **Fare**\n  * Fill missing values from median of same Pclass\n  * Standardize values\n  * Divide into 10 categories","f08a94fc":"# Training and Estimation <a id=\"training\"><\/a>","1956e125":"Check data example and types"}}