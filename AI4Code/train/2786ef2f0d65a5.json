{"cell_type":{"bfb1db6a":"code","3f2104e2":"code","783c29c8":"code","2ca71786":"code","26f2ae33":"code","b446b991":"code","8566b18e":"code","bd704177":"code","d843ac0e":"code","cce45d5d":"code","4cbc95d3":"code","a472de16":"code","b43e3f09":"code","71149201":"code","6ea2b159":"code","552e8b41":"code","8de21448":"code","e5df98b8":"code","380971c8":"code","47c2a573":"code","8952ca3c":"code","34a6c97d":"code","d4cc337c":"code","58523390":"code","cb831ac1":"code","f3f2b65a":"code","c6bc9515":"code","cafb8d6e":"code","a4040817":"code","3f61ea85":"code","e67f6a67":"code","dff45a9d":"code","acc433e6":"code","f64716c8":"code","d4ef24c6":"code","b01ccf0e":"code","bc13f6aa":"code","e67dbfa6":"code","c56e5611":"code","87fdf399":"code","0f849087":"code","ab5bda45":"code","52391826":"code","e94fe2ba":"code","d94c689d":"code","d873d3e6":"code","0af689c6":"code","0cfc14fe":"code","2f6f3dd5":"code","b35ab897":"markdown","c2d0163d":"markdown","d7f1d063":"markdown","647a45c5":"markdown","43b8c2c9":"markdown","d32f58a5":"markdown","c3525e48":"markdown","cd6ca0a6":"markdown","51a7a041":"markdown","bd09fb0b":"markdown","2668806e":"markdown","edc93228":"markdown","3dc57070":"markdown","9d43b541":"markdown","e1a9262f":"markdown","486afcc5":"markdown"},"source":{"bfb1db6a":"!pip install pydot graphviz","3f2104e2":"import os\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Conv2D, Flatten, ReLU, BatchNormalization, Conv2DTranspose, Dense\n\nfrom IPython.display import clear_output\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.animation as animation\n\nAUTOTUNE = tf.data.AUTOTUNE","783c29c8":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    INPUT_PATH = KaggleDatasets().get_gcs_path()\nelse:\n    strategy = tf.distribute.get_strategy()\n    INPUT_PATH = \"..\/input\/gan-getting-started\"\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint(\"Input Path:\", INPUT_PATH)","2ca71786":"monet_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"\/monet_tfrec\/*.tfrec\")\nphoto_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"\/photo_tfrec\/*.tfrec\")","26f2ae33":"feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n}","b446b991":"def parse_tfrecord(record):\n    features = tf.io.parse_single_example(record, feature_description)\n    \n    image = features['image']\n    image = tf.io.decode_image(image)\n    image = tf.reshape(image, (256, 256, 3))\n    \n    return image","8566b18e":"def normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image \/ 127.5) - 1.0\n    \n    return image","bd704177":"def random_jitter(image):\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.image.random_crop(image, size=[256,256, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.7, 1.2)\n    return image","d843ac0e":"BATCH_SIZE = 32\nBUFFER_SIZE = 1000\nprint(\"Batch Size:\", BATCH_SIZE)\nprint(\"Buffer Size:\", BUFFER_SIZE)","cce45d5d":"!rm -r \/kaggle\/tmp\n!mkdir \/kaggle\/tmp\n!mkdir \/kaggle\/tmp\/monet\n!mkdir \/kaggle\/tmp\/photo\n!ls \/kaggle\/tmp","4cbc95d3":"monet_dataset = tf.data.TFRecordDataset(monet_tfrec_files)\nmonet_dataset = monet_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# monet_dataset = monet_dataset.cache(\"\/kaggle\/tmp\/monet\")\nmonet_dataset = monet_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.repeat()\nmonet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\nmonet_dataset = monet_dataset.batch(BATCH_SIZE)\nmonet_dataset = monet_dataset.prefetch(AUTOTUNE)","a472de16":"plt.figure(figsize=(20,10))\nfor images in monet_dataset.take(1):\n    for i in range(len(images)):\n        plt.subplot(4,8, i+1)\n        plt.imshow((images[i]+1)\/2)\n        plt.xticks([])\n        plt.yticks([])","b43e3f09":"photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\nphoto_dataset = photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# photo_dataset = photo_dataset.cache(\"\/kaggle\/tmp\/photo\")\nphoto_dataset = photo_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.shuffle(BUFFER_SIZE)\nphoto_dataset = photo_dataset.batch(BATCH_SIZE)\nphoto_dataset = photo_dataset.prefetch(AUTOTUNE)","71149201":"plt.figure(figsize=(20,10))\nfor images in photo_dataset.take(1):\n    for i in range(len(images)):\n        plt.subplot(4,8, i+1)\n        plt.imshow((images[i]+1)\/2)\n        plt.xticks([])\n        plt.yticks([])","6ea2b159":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n    return result","552e8b41":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n    \n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n        \n    result.add(tf.keras.layers.ReLU())\n    \n    return result","8de21448":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n    \n    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n    \n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                  kernel_initializer=initializer,\n                                  use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    \n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n    \n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    \n    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                  kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n    \n    return tf.keras.Model(inputs=[inp], outputs=last)","e5df98b8":"def Generator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh') # (bs, 256, 256, 3)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n        \n    x = last(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)","380971c8":"with strategy.scope():\n    # Instantiate generators\n    G_PtoM = Generator()\n    G_MtoP = Generator()\n    # Instantiate discriminators\n    D_P = Discriminator()\n    D_M = Discriminator()","47c2a573":"tf.keras.utils.plot_model(G_PtoM.layers[1], dpi=64, to_file=\"downsample.png\", show_layer_names=False)","8952ca3c":"tf.keras.utils.plot_model(G_PtoM.layers[-3], dpi=64, to_file=\"upsample.png\", show_layer_names=False)","34a6c97d":"tf.keras.utils.plot_model(G_PtoM, show_shapes=True, dpi=64, to_file=\"generator.png\")","d4cc337c":"tf.keras.utils.plot_model(D_P, show_shapes=True, dpi=64, to_file=\"discriminator.png\")","58523390":"LAMBDA = 10","cb831ac1":"with strategy.scope():\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)","f3f2b65a":"def discriminator_loss(real, generated):\n    real_loss = loss_object(tf.ones_like(real), real)\n    generated_loss = loss_object(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    total_disc_loss \/= len(real)\n    \n    return total_disc_loss * 0.5","c6bc9515":"def generator_loss(generated):\n    return loss_object(tf.ones_like(generated), generated)\/len(generated)","cafb8d6e":"def calc_cycle_loss(real_image, cycled_image): \n        return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))","a4040817":"def identity_loss(real_image, same_image):\n    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))","3f61ea85":"with strategy.scope():\n    G_MtoP_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    G_PtoM_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    D_M_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_P_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","e67f6a67":"def generate_images(model, model_r, test_input, figsize=(12,12)):\n    prediction = model(test_input)\n    reconstruction = model_r(prediction)\n    identity = model_r(test_input)\n    \n    display_list = [test_input[0], prediction[0], reconstruction[0], identity[0]]\n    plt.figure(figsize=figsize)\n    title = ['Input', 'Predicted', 'Reconstructed', 'Identity']\n\n    for i in range(4):\n        plt.subplot(1, 4, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n\n    plt.show()\n    \n    return display_list","dff45a9d":"for images in monet_dataset.take(1):\n    generate_images(G_MtoP, G_PtoM, images)","acc433e6":"for images in photo_dataset.take(1):\n    generate_images(G_PtoM, G_MtoP, images)","f64716c8":"EPOCHS = 100","d4ef24c6":"with strategy.scope():\n    G_MtoP_loss = tf.keras.metrics.Mean(name='G_MtoP_loss')\n    G_PtoM_loss = tf.keras.metrics.Mean(name='G_PtoM_loss')\n    D_M_loss = tf.keras.metrics.Mean(name='D_M_loss')\n    D_P_loss = tf.keras.metrics.Mean(name='D_P_loss')","b01ccf0e":"def train_step(real_m, real_p):\n    \n    with tf.GradientTape(persistent=True) as tape:\n        \n        # G_PtoM translates P -> M\n        # G_MtoP translates M -> M\n        \n        fake_m = G_PtoM(real_p, training=True)\n        cycled_p = G_MtoP(fake_m, training=True)\n        \n        fake_p = G_MtoP(real_m, training=True)\n        cycled_m = G_PtoM(fake_p, training=True)\n        \n        # same_m and same_p for identity loss\n        same_m = G_PtoM(real_m, training=True)\n        same_p = G_MtoP(real_p, training=True)\n        \n        # disctiminator outputs\n        disc_real_m = D_M(real_m, training=True)\n        disc_real_p = D_P(real_p, training=True)\n        \n        disc_fake_m = D_M(fake_m, training=True)\n        disc_fake_p = D_P(fake_p, training=True)\n        \n        # Calculate Loss\n        gen_MtoP_loss = generator_loss(disc_fake_p)\n        gen_PtoM_loss = generator_loss(disc_fake_m)\n        \n        total_cycle_loss = calc_cycle_loss(real_m, cycled_m) + calc_cycle_loss(real_p, cycled_p)\n        \n        identity_loss_p = identity_loss(real_p, same_p)\n        identity_loss_m = identity_loss(real_m, same_m)\n        \n        # Total Loss\n        total_gen_MtoP_loss = gen_MtoP_loss + total_cycle_loss + identity_loss_p\n        total_gen_PtoM_loss = gen_PtoM_loss + total_cycle_loss + identity_loss_m\n        \n        disc_p_loss = discriminator_loss(disc_real_p, disc_fake_p)\n        disc_m_loss = discriminator_loss(disc_real_m, disc_fake_m)\n        \n    \n    # Calculate Gradients\n    gen_mtop_gradients = tape.gradient(total_gen_MtoP_loss, G_MtoP.trainable_variables)\n    gen_ptom_gradients = tape.gradient(total_gen_PtoM_loss, G_PtoM.trainable_variables)\n    disc_m_gradients = tape.gradient(disc_m_loss, D_M.trainable_variables)\n    disc_p_gradients= tape.gradient(disc_p_loss, D_P.trainable_variables)\n    \n    # Apply Gradients to optimizers\n    G_MtoP_optimizer.apply_gradients(zip(gen_mtop_gradients, G_MtoP.trainable_variables))\n    G_PtoM_optimizer.apply_gradients(zip(gen_ptom_gradients, G_PtoM.trainable_variables))\n    D_M_optimizer.apply_gradients(zip(disc_m_gradients, D_M.trainable_variables))\n    D_P_optimizer.apply_gradients(zip(disc_p_gradients, D_P.trainable_variables))\n    \n    # Update Running Loss\n    D_M_loss.update_state(disc_m_loss)\n    D_P_loss.update_state(disc_p_loss)\n    G_MtoP_loss.update_state(total_gen_MtoP_loss)\n    G_PtoM_loss.update_state(total_gen_PtoM_loss)","bc13f6aa":"@tf.function\ndef distributed_train_step(real_m, real_p):\n    strategy.run(train_step, args=(real_m, real_p))","e67dbfa6":"# sample_m = next(iter(monet_dataset))\n# sample_p = next(iter(photo_dataset))\n\n# ptom_preds_images = []\n# mtop_reconstructions_images = []\n# ptop_identity_images = []\n\n# mtop_preds_images = []\n# ptom_reconstructions_images = []\n# mtom_identity_images = []\n\nfor epoch in range(EPOCHS):\n    \n#     clear_output()\n#     pmp_display_list = generate_images(G_PtoM, G_MtoP, sample_p)\n#     ptom_preds_images.append(pmp_display_list[1])\n#     mtop_reconstructions_images.append(pmp_display_list[2])\n#     ptop_identity_images.append(pmp_display_list[3])\n#     \n#     mpm_display_list = generate_images(G_MtoP, G_PtoM, sample_m)\n#     mtop_preds_images.append(mpm_display_list[1])\n#     ptom_reconstructions_images.append(mpm_display_list[2])\n#     mtom_identity_images.append(mpm_display_list[3])\n    \n    G_MtoP_loss.reset_states()\n    G_PtoM_loss.reset_states()\n    D_M_loss.reset_states()\n    D_P_loss.reset_states()\n    \n    ds = tf.data.Dataset.zip((photo_dataset, monet_dataset))\n    with tqdm(ds, desc=\"Epoch {}\/{}\".format(epoch+1, EPOCHS)) as t:\n        for image_p, image_m in t:\n            distributed_train_step(image_m, image_p)\n            t.set_description(\n                f\"Epoch: {epoch+1}\/{EPOCHS}, \"\n                f\"G_MtoP: {G_MtoP_loss.result():.4f}, \"\n                f\"G_PtoM: {G_PtoM_loss.result():.4f}, \"\n                f\"D_M: {D_M_loss.result():.4f}, \"\n                f\"D_P: {D_P_loss.result():.4f}\"\n            )","c56e5611":"# def make_animation(images, title):\n#     fig, ax = plt.subplots()\n#     plt.title(title)\n#     ims = []\n#     for image in images:\n#         im = ax.imshow(image, animated=True)\n#         ims.append([im])\n# #     ax.imshow(images[0])\n#     ani = animation.ArtistAnimation(fig, ims, repeat_delay=1000, blit=True)\n#     return ani ","87fdf399":"# plt.rcParams[\"animation.html\"] = \"html5\"\n# make_animation(ptom_preds_images, \"PtoM\")","0f849087":"# make_animation(mtop_reconstructions_images, \"MtoP_Cycle\")","ab5bda45":"# make_animation(ptop_identity_images, \"PtoP_Identity\")","52391826":"# make_animation(mtop_preds_images, \"MtoP\")","e94fe2ba":"# make_animation(ptom_reconstructions_images, \"PtoM_Cycle\")","d94c689d":"# make_animation(mtom_identity_images, \"MtoM_Identity\")","d873d3e6":"test_photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\ntest_photo_dataset = test_photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.batch(BATCH_SIZE)\n# test_photo_dataset = photo_dataset","0af689c6":"plt.figure(figsize=(20,20))\nfor images in test_photo_dataset.take(1):\n    predictions = G_PtoM(images, training=False)\n    for i in range(len(images)):\n        plt.subplot(8,8, 2*i+1)\n        plt.imshow((images[i]+1)\/2)\n        plt.title(\"Photo\")\n        plt.axis('off')\n\n        plt.subplot(8,8, 2*i+2)\n        plt.imshow((predictions[i]+1)\/2)\n        plt.title(\"To Monet\")\n        plt.axis('off')","0cfc14fe":"def get_image(arr):\n    arr = (arr + 1) * 127.5\n    arr = tf.cast(arr, tf.int8)\n    return tf.keras.preprocessing.image.array_to_img(arr)","2f6f3dd5":"from zipfile import ZipFile\n\nfile_index = 1\nwith ZipFile('images.zip', 'w') as submission_zip:\n    for images in tqdm(test_photo_dataset):\n        predictions = G_PtoM.predict(images)\n        for prediction in predictions:\n            filename = f'photo_to_monet_{file_index}.jpg'\n            img = get_image(prediction)\n            img.save(filename)\n            submission_zip.write(filename)\n            os.remove(filename)\n            file_index+=1\n        if file_index>=9950:\n            break\n    submission_zip.close()\n    \nprint(f\"Saved {file_index} files in images.zip\")","b35ab897":"# Models","c2d0163d":"## Photo Dataset","d7f1d063":"## Losses","647a45c5":"### Downsample","43b8c2c9":" ## Monet Dataset","d32f58a5":"## Definitions","c3525e48":"# Model Performance Before Training","cd6ca0a6":"## Optimizers","51a7a041":"### Discriminator","bd09fb0b":"### Training Visualisation","2668806e":"# Training","edc93228":"### Predictions","3dc57070":"# Dataset","9d43b541":"### Generator","e1a9262f":"# Libraries","486afcc5":"### Upsample"}}