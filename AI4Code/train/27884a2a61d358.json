{"cell_type":{"063d42ef":"code","b0f900f8":"code","77772cfe":"code","29dba6b4":"code","6c6f6a69":"code","7b5ddbe4":"code","78266806":"code","5c6f8aaa":"code","6c17167c":"code","5fea8b7b":"code","ec8e281e":"markdown","a33e8cce":"markdown","72289b78":"markdown"},"source":{"063d42ef":"import os\nimport cv2\nimport glob\nimport imageio\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.animation import FuncAnimation\n\nfrom IPython.display import Image, display\n\n%matplotlib inline","b0f900f8":"class InputVideo():\n    def __init__(self, video_file, video_name):\n        self.name_ = video_name\n        self.color_images_ = video_file['colorImages']\n        self.bounding_box_ = video_file['boundingBox']\n        self.landmarks_2D_ = video_file['landmarks2D']\n        self.landmarks_3D_ = video_file['landmarks3D']\n        \n    def name(self):\n        return self.name_\n    \n    def frames(self):\n        return self.color_images_\n    \n    def length(self):\n        return self.color_images_.shape[3]\n    \n    def frame(self, i):\n        return self.color_images_[:, :, :, i]\n    \n    def landmarks_2D(self):\n        return self.landmarks_2D_\n    \n    def landmarks_3D(self):\n        return self.landmarks_3D_\n    \n    def bounding_box(self):\n        return self.bounding_box_","77772cfe":"class HeadPoseDetector():\n    def detect(self, im, landmarks_2d, landmarks_3d):\n        h, w, c = im.shape\n        K = np.array([[w, 0, w\/2],\n                      [0, w, h\/2],\n                      [0, 0, 1]], dtype=np.double)\n        dist_coeffs = np.zeros((4,1)) \n\n        (_, R, t) = cv2.solvePnP(landmarks_3d, landmarks_2d, K, dist_coeffs)\n        return R, t, K, dist_coeffs","29dba6b4":"base_path = '\/kaggle\/input\/youtube-faces-with-facial-keypoints\/'\ndf_videos = pd.read_csv(base_path + 'youtube_faces_with_keypoints_large.csv')\n\n# https:\/\/www.kaggle.com\/selfishgene\/exploring-youtube-faces-with-keypoints-dataset\n# Create a dictionary that maps videoIDs to full file paths\nnpz_file_paths = glob.glob(base_path + 'youtube_faces_*\/*.npz')\nvideo_ids = [x.split('\/')[-1].split('.')[0] for x in npz_file_paths]\n\nfull_video_paths = {}\nfor video_id, full_path in zip(video_ids, npz_file_paths):\n    full_video_paths[video_id] = full_path\n\n# Remove from the large csv file all videos that weren't uploaded yet\ndf_videos = df_videos.loc[df_videos.loc[:,'videoID'].isin(full_video_paths.keys()), :].reset_index(drop=True)","6c6f6a69":"num_samples = 3\nnp.random.seed(11)\nsample_indices = np.random.choice(df_videos.index, size=num_samples, replace=False)\nsample_video_ids = df_videos.loc[sample_indices, 'videoID']\n\nsample_videos = []\nfor i, video_id in enumerate(sample_video_ids):\n    sv = InputVideo(np.load(full_video_paths[video_id]), video_id)\n    sample_videos.append(sv)","7b5ddbe4":"for i, video in enumerate(sample_videos):\n    title = video.name()\n    frames = [video.frame(f) for f in range(video.length())]\n    \n    kwargs_write = {'fps':1.0, 'quantizer':'nq'}\n    imageio.mimsave(f'.\/{title}_orig.gif', frames, fps=24)\n    display(Image(url=f'.\/{title}_orig.gif'))","78266806":"face_landmark_lines = [\n    [1, 5], [5, 7], [7, 9], # right-side jaw\n    [9, 11], [11, 13], [13, 17], # left-side jaw\n    [17, 25], [25, 20], [20, 1], # upper head\n    [28, 31], [28, 25], [28, 20], [31, 32], [31, 36], # nose\n    [32, 52], [36, 52], # nose-mouth\n    [52, 49], [49, 58], [58, 55], [55, 52], # mouth\n    [6, 49], [55 ,12], [58, 9], # mouth-jaw\n    [46, 17], # left eye - jaw\n    [1, 37], # right eye - jaw \n    [37, 38], [38, 39], [39, 40], [40, 41], [41, 42], [42, 37], # right eye\n    [43, 44], [44, 45], [45, 46], [46, 47], [47, 48], [48, 43], # left eye\n    [28, 40], [28, 43]\n]","5c6f8aaa":"def find_bounding_box(landmarks_x, landmarks_y):\n    return min(landmarks_x), min(landmarks_y), max(landmarks_x), max(landmarks_y)","6c17167c":"def process_frame(frame, landmarks_2D, landmarks_3D):\n    processed_img = np.array(frame)\n    overlay = processed_img.copy()\n    \n    # Face grid\n    for mark_id1, mark_id2 in face_landmark_lines:\n        pt1 = ( int(landmarks_2D[mark_id1-1,0]), int(landmarks_2D[mark_id1-1,1]) )\n        pt2 = ( int(landmarks_2D[mark_id2-1,0]), int(landmarks_2D[mark_id2-1,1]) )\n        cv2.line(overlay, pt1, pt2, color=(255,255,255))\n        \n    # Bounding box\n    x0, y0, x1, y1 = map(int, find_bounding_box(landmarks_2D[:, 0], landmarks_2D[:, 1]))\n    cv2.rectangle(overlay, (x0, y0), (x1, y1), color=(255, 255, 255))\n\n    # Transparency\n    alpha = 0.3\n    processed_img = cv2.addWeighted(overlay, alpha, processed_img, 1 - alpha, 0)\n    \n    # Draw pose change\n    image_center = (int(frame.shape[1]\/2), int(frame.shape[0]\/2))\n    nose_tip = landmarks_2D[33]\n#     (R, t, K, dist_coeffs) = HeadPoseDetector().detect(frame, landmarks_2D, landmarks_3D)\n    cv2.line(processed_img, image_center, (int(nose_tip[0]), int(nose_tip[1])), color=(255,0,0))\n    \n    return processed_img","5fea8b7b":"for i, video in enumerate(sample_videos):\n    title = video.name()\n    \n    processed_imgs = [process_frame(video.frame(f),\n                                    video.landmarks_2D()[:, :, f],\n                                    video.landmarks_3D()[:, :, f]) for f in range(video.length())]\n\n    imageio.mimsave(f'.\/{title}_2d.gif', processed_imgs, fps=24)\n    display(Image(url=f'.\/{title}_2d.gif'))","ec8e281e":"### Select sample images (videos) to process","a33e8cce":"### Facial landmarks\nDefine which points need to be connected with a line.\n\n![](https:\/\/www.researchgate.net\/publication\/327500528\/figure\/fig9\/AS:668192443748352@1536320901358\/The-ibug-68-facial-landmark-points-mark-up.ppm)","72289b78":"### Display faces with landmarks and bound. box"}}