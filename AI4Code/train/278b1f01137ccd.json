{"cell_type":{"88ff5710":"code","a467b46e":"code","ef539ce6":"code","36fad6c2":"code","6faa6c0a":"code","6923c75b":"code","5a8a1a82":"code","504a2285":"code","799145b3":"code","994685a9":"code","3bb7f6ad":"code","fcce8dbc":"code","34d58086":"code","984cc6e2":"code","1d7c0add":"code","a4a4423b":"code","95798648":"code","eb8c0e68":"code","6fc2617a":"code","1d2ea76b":"code","b87e03e2":"code","70d80280":"code","1ad2a7de":"code","fed8f2bb":"code","3d51c5c2":"code","9d978f86":"code","f924d513":"code","dc9fa44d":"code","be7af765":"code","63dab8a1":"code","27a6eb18":"code","017176f2":"code","a5e831e2":"code","7c5229a1":"code","6b254a90":"code","8589b7b6":"code","141b5310":"code","a61413bd":"code","2dc4abce":"code","d6a94d0f":"code","54f92637":"code","72c0bac8":"code","f6f0ff83":"code","c2bab3a7":"code","d0817eef":"code","e086f22c":"code","74647516":"code","2debef54":"code","336a5f3f":"code","8dbd3fd4":"markdown","1cae7cc2":"markdown","4b0cfa6a":"markdown","6289b0c9":"markdown","5a1cf1a5":"markdown","40ce5472":"markdown","7d5719f5":"markdown","3402ed90":"markdown","8c3622a7":"markdown"},"source":{"88ff5710":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nimport sklearn.linear_model as sklm\nimport sklearn.neighbors as skn\nimport sklearn.ensemble as ske\nimport catboost as cb\nimport scipy.stats as sstats\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a467b46e":"train = pd.read_csv('..\/input\/analytics-vidhya-jobathon\/AV_train_JobAThon.csv')\ntest = pd.read_csv('..\/input\/analytics-vidhya-jobathon\/AV_test_JobAThon.csv')","ef539ce6":"# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","36fad6c2":"# set target feature\ntargetFeature='Response'","6faa6c0a":"# remove ID from train data\ntrain.drop(['ID'], inplace=True, axis=1)","6923c75b":"# check for duplicates\nprint(train.shape)\ntrain.drop_duplicates(inplace=True)\nprint(train.shape)","5a8a1a82":"train.info()","504a2285":"test.info()","799145b3":"cont_features, cat_features = divideFeatures(train)\ncat_features.head()","994685a9":"# correlation heatmap for all features\ncorr = train.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","3bb7f6ad":"# import random\n# seed = 12\n# np.random.seed(seed)\n\n# # get all not null records for imputing\n# X_impute = train[train['Health Indicator'].isna()==False]\n# y_impute = X_impute.pop('Health Indicator')\n\n# # remove categorical cols and targetFeature from X_impute\n# X_impute = X_impute[cont_features.columns.tolist()]\n# X_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# # impute with CatBoostClassifier\n# imputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\n# imputer_model.fit(X_impute, y_impute)","fcce8dbc":"# X_impute = train[train['Holding_Policy_Duration'].isna()==False]\n# y_impute = X_impute.pop('Holding_Policy_Duration')\n\n# # remove categorical cols and targetFeature from X_impute\n# X_impute = X_impute[cont_features.columns.tolist()]\n# X_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# # impute with RandomForestClassifier\n# imputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\n# imputer_model.fit(X_impute, y_impute)","34d58086":"# get all not null records for imputing\n# X_impute = train[train['Holding_Policy_Type'].isna()==False]\n# y_impute = X_impute.pop('Holding_Policy_Type')\n\n# # remove categorical cols and targetFeature from X_impute\n# cols_impute = cont_features.columns.tolist()\n# cols_impute.remove('Holding_Policy_Type')\n# X_impute = X_impute[cols_impute]\n# X_impute.drop([targetFeature], inplace=True, axis=1)\n\n# # impute with RandomForestClassifier\n# imputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\n# imputer_model.fit(X_impute, y_impute)","984cc6e2":"# print(\"Train Missing:\",train.isna().any().sum())\n#print(\"Test Missing:\",test.isna().any().sum())","1d7c0add":"# X_impute.head()","a4a4423b":"# feature for age difference between Upper_Age and Lower_Age\ntrain['age_diff'] = abs(train['Upper_Age'] - train['Lower_Age'])\n\ntest['age_diff'] = abs(test['Upper_Age'] - test['Lower_Age'])","95798648":"# drop Lower_Age column as it is highly correlated with Upper_age and we also have its info in age_diff\ntrain.drop('Lower_Age', axis=1, inplace=True)\n\ntest.drop('Lower_Age', axis=1, inplace=True)","eb8c0e68":"train['Holding_Policy_Duration'] = pd.to_numeric(train['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))\n\ntest['Holding_Policy_Duration'] = pd.to_numeric(test['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))","6fc2617a":"cont_features, cat_features = divideFeatures(train)\ncat_features","1d2ea76b":"# label encoding on categorical features\ndef mapFeature(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)","b87e03e2":"for col in cat_features.columns:\n    mapFeature(train, col, test)","70d80280":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(train[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(train[feat], drop_first=True, prefix=feat+\"_\")\n        train = pd.concat([train, dummyVars], axis=1)\n        train.drop(feat, axis=1, inplace=True)\n\n\ntrain.head()","1ad2a7de":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(test[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(test[feat], drop_first=True, prefix=feat+\"_\")\n        test = pd.concat([test, dummyVars], axis=1)\n        test.drop(feat, axis=1, inplace=True)\ntest.head()","fed8f2bb":"def fillNan(df, col, value):\n    df[col].fillna(value, inplace=True)\n\n# # setting missing values to most occurring values\n# # try changing with ML algo for missing\nfillNan(train, 'Holding_Policy_Duration', train['Holding_Policy_Duration'].mode()[0])\nfillNan(test, 'Holding_Policy_Duration', test['Holding_Policy_Duration'].mode()[0])\n# df['Holding_Policy_Duration'].isna().any()\n\n# # setting missing values to most occurring values\n# # try changing with ML algo for missing\nfillNan(train, 'Holding_Policy_Type', train['Holding_Policy_Type'].mode()[0])\nfillNan(test, 'Holding_Policy_Type', test['Holding_Policy_Type'].mode()[0])\n# df['Holding_Policy_Type'].isna().any()","3d51c5c2":"import random\nseed = 12\nnp.random.seed(seed)\n\n# shuffle samples\ndf_shuffle = train.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop(targetFeature)\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","9d978f86":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# scale actual test data with transform()\ndf_test = test.copy()\ndf_test1 = df_test.drop('ID',axis=1)\ndf_test1 = pd.DataFrame(scaler.transform(df_test1), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","f924d513":"X_train.info()","dc9fa44d":"knn = skn.KNeighborsClassifier(n_neighbors = 5, n_jobs=-1)\nknn.fit(X_train, y_train)\n\n# predict\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","be7af765":"X_train.columns","63dab8a1":"class_weights = sku.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights\n\n\nsample_weights = sku.class_weight.compute_sample_weight('balanced', y_train)\nsample_weights\n\nlog_model = sklm.LogisticRegression()\nlog_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = log_model.predict(X_train)\ny_test_pred = log_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","27a6eb18":"enet_model = sklm.ElasticNetCV(l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n                    alphas = [1, 0.1, 0.01, 0.001, 0.0005], cv=10)\nenet_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = enet_model.predict(X_train)\ny_test_pred = enet_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","017176f2":"ridge_model = sklm.RidgeCV(scoring = \"neg_mean_squared_error\", \n                    alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1.0, 10], cv=5\n                   )\nridge_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = ridge_model.predict(X_train)\ny_test_pred = ridge_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","a5e831e2":"import catboost as cb\n\ncat_model = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights,\n                           use_best_model=True, iterations=500)\ncat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cat_model.best_score_)\n\ny_train_pred = cat_model.predict(X_train)\ny_test_pred = cat_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","7c5229a1":"import catboost as cb\ncat_model = cb.CatBoostClassifier()\nparams = {'iterations': [501,1001,10001],\n          'depth': [4, 6,8,10],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'leaf_estimation_iterations': [7,10,12],\n          'eval_metric': ['AUC']\n         }\ncat_model_grid = skms.GridSearchCV(estimator=cat_model, param_grid=params, cv=5)\ncat_model_grid.fit(X_train, y_train)","6b254a90":"cat_model_grid.best_params_","8589b7b6":"#Predict the response for test dataset\ny_pred1 = cat_model.predict_proba(df_test1)[:,1]\ntest['Response'] = y_pred1\ntest.head()\ntest[['ID','Response']].to_csv(r'.\/CATBOOST.csv',index=False)","141b5310":"gb_model = ske.GradientBoostingClassifier(loss='deviance', random_state=seed, verbose=0,\n                                    n_estimators=50, max_depth=7,\n                                    min_samples_leaf=1, min_samples_split=8)\ngb_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = gb_model.predict(X_train)\ny_test_pred = gb_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","a61413bd":"#Predict the response for test dataset\ny_pred1 = gb_model.predict_proba(df_test1)[:,1]\ntest['Response'] = y_pred1\ntest.head()\ntest[['ID','Response']].to_csv(r'.\/GB.csv',index=False)","2dc4abce":"extra_model = ske.ExtraTreesClassifier(criterion='gini', random_state=1, verbose=0, n_jobs=-1,\n                              n_estimators=200,max_depth=10,\n                              min_samples_split = 5, min_samples_leaf = 1)\nextra_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = extra_model.predict(X_train)\ny_test_pred = extra_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","d6a94d0f":"rf_model = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=200,max_depth=10, \n                                 min_samples_split = 7, min_samples_leaf = 1\n                                )\nrf_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","54f92637":"import xgboost as xg\n# working without scaling\nxgb_model = xg.XGBClassifier(objective ='binary:logistic', random_state=seed, verbose=0,\n                      n_estimators=500, max_depth = 10)\nxgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb_model.predict(X_train)\ny_test_pred = xgb_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))","72c0bac8":"import lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(objective='binary', class_weight=class_weights, random_state=1, n_jobs=-1,\n                         n_estimators=50)\nlgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\nprint(skm.roc_auc_score(y_train, y_train_pred))\nprint(skm.roc_auc_score(y_test, y_test_pred))\n\n","f6f0ff83":"df_test1.shape","c2bab3a7":"#Predict the response for test dataset\ny_pred1 = lgb_model.predict_proba(df_test1)[:,1]\ntest['Response'] = y_pred1\ntest.head()\ntest[['ID','Response']].to_csv(r'.\/LGBM.csv',index=False)","d0817eef":"import tensorflow as tf\nimport tensorflow_addons as tfa\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","e086f22c":"THRESHOLD = .999\nbestModelPath = '.\/best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') > THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation ACC Score\",min(history.history[\"val_accuracy\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","74647516":"epochs = 40\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n#     k.layers.Dropout(0.3),\n    \n    k.layers.Dense(1024, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(512, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='sigmoid'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n#                   tfa.metrics.F1Score(num_classes=1),\n                  'accuracy'\n              ]\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, \n                      batch_size=2048, \n#                       class_weight=class_weights,\n                      callbacks=[callbacks_list]\n                     )","2debef54":"def getTestResults(m=None):\n    df_final = train.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if targetFeature not in x]\n    df_final_test = df_test1[test_cols]\n    df_y = df_final.pop(targetFeature)\n    df_X = df_final\n\n    scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n    df_final_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n\n    sample_weights = sku.class_weight.compute_sample_weight('balanced', df_y)\n    \n    if m is None:\n\n        lmr = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights)\n        lmr.fit(df_X, df_y)\n\n\n    else:\n        lmr = m\n\n    # predict\n    y_train_pred = lmr.predict(df_X)\n    y_test_pred = lmr.predict_proba(df_final_test)[:,1]\n    if m is not None:\n        y_train_pred = [round(y[0]) for y in y_train_pred]\n        #y_test_pred = [round(y[0]) for y in y_test_pred]\n    print(skm.roc_auc_score(df_y, y_train_pred))\n    return y_test_pred\n\n# ML models\nresults = getTestResults()\n","336a5f3f":"submission = pd.DataFrame({\n    'ID': test['ID'],\n    targetFeature: results,\n})\nprint(submission.Response.value_counts())\n#submission.to_csv(r'.\/DL.csv',index=False)","8dbd3fd4":"KNN","1cae7cc2":"Deep Learning","4b0cfa6a":"LGBM","6289b0c9":"CatBoost\n","5a1cf1a5":"Logistic Regression","40ce5472":"Random Forest","7d5719f5":"XGBoost","3402ed90":"GBM","8c3622a7":"Extra Tree"}}