{"cell_type":{"2a2385a1":"code","b9b0b1bb":"code","32812cde":"code","64a2c19b":"code","7baf0d99":"code","bc2f1528":"code","df17280f":"code","5f9369d5":"code","985bdae3":"code","80fc07dc":"code","1d99d15b":"code","8f4de5b6":"code","3d683a8a":"code","8d031d61":"code","3fbcafa5":"code","d28584f6":"code","c8bd797e":"code","ffd0fd9b":"code","88d0497b":"code","b7289095":"code","0b8a83d3":"code","e7e0cf47":"code","43470457":"code","787776e8":"code","5bbc4fea":"code","d11b8ad1":"code","35f83cd1":"code","f55d4289":"code","404ec465":"code","a9fd33a8":"code","cc105c40":"code","65b470ef":"code","3c72006d":"code","15f68930":"code","8cf3a920":"code","288cf1c2":"code","377322c7":"code","2a93a338":"code","7e47a0bb":"code","dfe20045":"code","fdff2dca":"code","58e9cf14":"markdown","d89b70b3":"markdown","f75ee47a":"markdown","4b38c958":"markdown","8871ce4f":"markdown","58e98525":"markdown","a9362428":"markdown","2b368685":"markdown","382171a6":"markdown","04d0504b":"markdown","8ba1d97e":"markdown","954c876a":"markdown","1e7b7787":"markdown","8b763df3":"markdown","b5265505":"markdown","d6d04dea":"markdown","f8966ba2":"markdown","af562d4a":"markdown"},"source":{"2a2385a1":"from google.cloud import bigquery\nfrom scipy.stats.mstats import zscore\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport matplotlib as mpl\nfrom pathlib import Path\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest \nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime,tzinfo\nimport scipy, json, csv, time, pytz\nfrom pytz import timezone\nimport numpy as np\nimport pandas as pd\nseed = 135\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nimport os\nos.listdir('..\/input\/gcp-bitcoin-project\/')","b9b0b1bb":"#Connecting to Google datastore (use path to ur private key)\nos.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"..\/input\/gcp-bitcoin-project\/Bitcoin Project-615d07137267.json\"\nclient = bigquery.Client()","32812cde":"# The query to get date, number of transactions from Google BigQuery bitcoin blockchain dataset \n# Select records from the last three years and group them with respect to date\nquery_1 = \"\"\"\nSELECT \n   DATE(TIMESTAMP_MILLIS(timestamp)) AS Date,\n   COUNT(transactions) AS Transactions\nFROM `bigquery-public-data.bitcoin_blockchain.blocks`\nGROUP BY date\nHAVING date >= '2016-08-12' AND date <= '2019-08-12'\nORDER BY date\n\"\"\"\nquery_job_1 = client.query(query_1)\n# Waits for the query to finish\niterator_1 = query_job_1.result(timeout=30)\nrows_1 = list(iterator_1)\ndf_1 = pd.DataFrame(data=[list(x.values()) for x in rows_1], columns=list(rows_1[0].keys()))","64a2c19b":"# The query to get sum of all satoshis spent each day and number of blocks\nquery_2 = \"\"\"\nSELECT\n  o.Date,\n  COUNT(o.block) AS Blocks,\n  SUM(o.output_price) AS Output_Satoshis\nFROM (\n  SELECT\n    DATE(TIMESTAMP_MILLIS(timestamp)) AS Date,\n    output.output_satoshis AS output_price,\n    block_id AS block\n  FROM\n    `bigquery-public-data.bitcoin_blockchain.transactions`,\n    UNNEST(outputs) AS output ) AS o\nGROUP BY\n  o.date\nHAVING o.date >= '2016-08-12' AND o.date <= '2019-08-12'\nORDER BY o.date, blocks\n\"\"\"\nquery_job_2 = client.query(query_2)\n# Waits for the query to finish\niterator_2 = query_job_2.result(timeout=30)\nrows_2 = list(iterator_2)\ndf_2 = pd.DataFrame(data=[list(x.values()) for x in rows_2], columns=list(rows_2[0].keys()))\n\ndf_2[\"Output_Satoshis\"]= df_2[\"Output_Satoshis\"].apply(lambda x: float(x\/100000000))","7baf0d99":"df_1.head()","bc2f1528":"df_2.head()","df17280f":"# merge the two dataframes\nresult = pd.merge(df_1,\n                 df_2[['Date', 'Blocks', 'Output_Satoshis']],\n                 on='Date')\nresult.head()","5f9369d5":"# Number of records \nlen(result)","985bdae3":"# get the overview of our data\nresult.describe()","80fc07dc":"sns.kdeplot(result['Blocks'])","1d99d15b":"sns.kdeplot(result['Transactions'])","8f4de5b6":"sns.kdeplot(result['Output_Satoshis'])","3d683a8a":"%matplotlib inline\nplt.style.use('ggplot')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n\ng = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Transactions', data=result, palette='Blues_d')\nplt.title('Transactions per day')","8d031d61":"%matplotlib inline\nplt.style.use('ggplot')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n\ng = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Blocks', data=result, palette='Blues_d')\nplt.title('Blocks per day')","3fbcafa5":"g = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Output_Satoshis', data=result, palette='BuGn_r')\nplt.title('Sum of all satoshis spent each day')","d28584f6":"# check the relation among the features of data\nsns.set(style=\"ticks\")\nsns.pairplot(result)","c8bd797e":"# select the three most important features (Transactions, Blocks, Output Satoshis) from the data\ndata = result[['Output_Satoshis','Blocks','Transactions']]\noutliers_fraction=0.05\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n\n# train isolation forest\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(data) ","ffd0fd9b":"fig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111,projection='3d')\nX = result.iloc[:,1:4].values\ncolors = np.array(['red', 'blue'])\ny_pred = model.fit_predict(data)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], s=25, color=colors[(y_pred + 1) \/\/ 2] )\nax.legend()\n#plt.xlabel('Transactions')\n#plt.ylabel('Blocks')\n#plt.zlabel('Sum of Output Satoshis')\nplt.title('Transactions vs Blocks vs Sum of Output Satoshis: Red represents Anomalies')\nplt.savefig('IsolationForest_anomaly.png', dpi=1000)","88d0497b":"# create a new column for storing the results of Isolation Forest method\nresult['anomaly_IsolationForest'] = pd.Series(model.predict(data))\nresult['anomaly_IsolationForest'] = result['anomaly_IsolationForest'].apply(lambda x: x == -1)\nresult['anomaly_IsolationForest'] = result['anomaly_IsolationForest'].astype(int)\nresult['anomaly_IsolationForest'].value_counts()","b7289095":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Transactions'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Transactions'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Transactions')\nplt.savefig('IsolationForest_anomaly_Transactions.png', dpi=1000)\nplt.show();","0b8a83d3":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Blocks'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Blocks'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Blocks')\nplt.savefig('IsolationForest_anomaly_Blocks.png', dpi=1000)\nplt.show();","e7e0cf47":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Output_Satoshis'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Output_Satoshis'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Sum of Output Satoshis')\nplt.savefig('IsolationForest_anomaly_Output_Satoshis.png', dpi=1000)\nplt.show();","43470457":"# This code has been taken from kernel https:\/\/github.com\/anish-saha\/EventDetection-Paradigm01\/blob\/master\/KMeans.ipynb\n\ndef pca_results(good_data, pca):\n    # Dimension indexing\n    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n    # PCA components\n    components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n    components.index = dimensions\n\n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n\n    # Create a bar plot visualization\n    fig, ax = plt.subplots(figsize = (10,10))\n\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)\n\n\n    # Display the explained variance ratios\n    for i, ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n %.4f\"%(ev))\n\n    # Return a concatenated DataFrame\n    return pd.concat([variance_ratios, components], axis = 1)","787776e8":"data_ = data.copy() # make a copy of data with three already selected features\ndata_ = data_.reset_index(drop=True)\n\ndata_[:] = MinMaxScaler().fit_transform(data_[:])\npca = PCA(n_components=2) # we have selected 2 components in PCA for simplicity\npca.fit(data_)\nreduced_data = pca.transform(data_)\nreduced_data = pd.DataFrame(reduced_data)\n\nnum_clusters = range(1, 20)\n\nkmeans = [KMeans(n_clusters=i, random_state=seed).fit(reduced_data) for i in num_clusters]\nscores = [kmeans[i].score(reduced_data) for i in range(len(kmeans))]\n\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot(num_clusters, scores, linewidth = 4)\nplt.xticks(num_clusters)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show();","5bbc4fea":"correlations = pd.DataFrame(data=data_).corr()\npca_results(correlations, pca)","d11b8ad1":"#Choosing the three clusters based on the elbow curve\nbest_num_cluster__ = 3\nkm__ = KMeans(n_clusters=best_num_cluster__, random_state=seed)\nkm__.fit(reduced_data)\nkm__.predict(reduced_data)\nlabels__1 = km__.labels_\n\n#Choosing the four clusters based on the elbow curve\nbest_num_cluster = 4\nkm = KMeans(n_clusters=best_num_cluster, random_state=seed)\nkm.fit(reduced_data)\nkm.predict(reduced_data)\nlabels = km.labels_\n\n#Choosing the five clusters based on the elbow curve\nbest_num_cluster_ = 5\nkm_ = KMeans(n_clusters=best_num_cluster_, random_state=seed)\nkm_.fit(reduced_data)\nkm_.predict(reduced_data)\nlabels_1 = km_.labels_\n","35f83cd1":"#Plotting based on three cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels__1.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 3 clusters')","f55d4289":"#Plotting based on four cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 4 clusters')","404ec465":"#Plotting based on five cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels_1.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 5 clusters')","a9fd33a8":"reduced_data.loc[0]\nmod = kmeans[best_num_cluster-1]\nmod.cluster_centers_","cc105c40":"reduced_data['Principal Component 1'] = reduced_data[0]\nreduced_data['Principal Component 2'] = reduced_data[1]\nreduced_data.drop(columns = [0, 1], inplace=True)\nreduced_data.head()","65b470ef":"def getDistanceByPoint(data, model):\n    distance = []\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.append(np.linalg.norm(Xa-Xb))\n    return distance\n\noutliers_fraction = 0.05\n# find the distance between each point and its nearest centroid. The largest distances will be consdiered anomalies\ndistance = getDistanceByPoint(reduced_data, kmeans[best_num_cluster-1])\ndistance = pd.Series(distance)\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n\n\n# anomaly_kmeans contain the anomaly result of the above method  (0:normal, 1:anomaly) \nresult['anomaly_kmeans'] = (distance >= threshold).astype(int)\n\n# visualisation of anomaly with cluster view\n#fig, ax = plt.subplots(figsize=(10,6))\ncolors = {0:'blue', 1:'red'}\n#colors = {1:'#f70505', 0:'#0a48f5'}\nplt.figure(figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=result[\"anomaly_kmeans\"].apply(lambda x: colors[x]), s=25)\nplt.xlabel('principal feature 1')\nplt.ylabel('principal feature 2')\nplt.title('Anomaly prediction using KMeans: Red represents Anomaly')\nplt.savefig('KMeans_anomaly.png', dpi=1000)","3c72006d":"result['anomaly_kmeans'].value_counts()","15f68930":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Transactions'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Transactions'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Transactions')\nplt.savefig('KMeans_anomaly_Transactions.png', dpi=1000)\nplt.show();","8cf3a920":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Blocks'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Blocks'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Blocks')\nplt.savefig('KMeans_anomaly_Blocks.png', dpi=1000)\nplt.show();","288cf1c2":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Output_Satoshis'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Output_Satoshis'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Sum of Output Satoshis')\nplt.savefig('KMeans_anomaly_Output_Satoshis.png', dpi=1000)\nplt.show();","377322c7":"# final result dataframe\nresult.head()","2a93a338":"# select the cases for final anomaly in which both the algorithms predicted anomaly\nfinal_anomaly = result.query('anomaly_kmeans == 1 & anomaly_IsolationForest == 1')\nfinal_anomaly.head()","7e47a0bb":"# Select the cases in which either of the two algorithms predicted anomaly\npossible_anomaly = result.query('anomaly_kmeans == 1 | anomaly_IsolationForest == 1')\npossible_anomaly.head()","dfe20045":"# Select the cases where no algorithm predicted anomaly\nno_anomaly = result.query('anomaly_kmeans == 0 & anomaly_IsolationForest == 0')\nno_anomaly.head()","fdff2dca":"total_anomaly = len(final_anomaly)+len(possible_anomaly)\npercent_total_anomaly = total_anomaly*100\/len(result)\nprint('Total records:',len(result))\nprint('Number of final anomaly:', len(final_anomaly))\nprint('Number of possible anomaly:', len(possible_anomaly))\nprint('Total anomaly:', total_anomaly)\nprint('Percentage of total anomaly in the data: %0.2f' % percent_total_anomaly)","58e9cf14":"The best results were drawn with a plot with four clusters. So, we will proceed further by considering four clusters for the problem.","d89b70b3":"### Step 2: Reading and Processing Data","f75ee47a":"So we should now get back to the previous question. Is there some trend here in the graphs? The possible answer can be not exactly. So why did we plot those curves? Often, In anomaly detection, we make use of different algorithms to determine the anomalies in the data. Mostly unsupervised learning-based anomaly detection algorithm uses outliers to detect anomalies. Yes, we will evaluate these outliers employing several anomaly detection algorithms in this notebook and will get back to these curves again at last to determine the anomalies. ","4b38c958":"### Conclusion","8871ce4f":"### Step 3: Model Building & Evaluation","58e98525":"The above elbow-curve helps to determine the number of clusters for the K-means algorithm. The most drastic change recorded in the elbow-curve at 3 or 4 or 5 number of clusters. So let's which one of them is better for our case. ","a9362428":"The above plot cites Transactions, Features, Sum of Output Satoshis features to represents the anomalies in the data using the Isolation Forest method. Now, summarize the predictions just performed by assigning them binary values i.e., 0 or 1 (0 for normal, 1 for anomaly).","2b368685":"There are 38 anomalies predicted by Isolation Forest method in the last three years records. ","382171a6":"We just inherited use of those three previous plots (Transactions, Blocks, Output Satoshis vs. date) for better understanding the Isolation Forest method predicted anomalies by visualizing them with outliers anomaly. ","04d0504b":"#### Method 2: K-Means","8ba1d97e":"#### Method 1: Isolation Forest","954c876a":"> **Google BigQuery Bitcoin Blockchain Dataset** consist of two projects: Transactions and Blocks containing features like block_id, transaction_id, timestamp, input, output, etc., which updates every 10 minutes. [https:\/\/bigquery.cloud.google.com\/dataset\/bigquery-public-data:bitcoin_blockchain](http:\/\/)","1e7b7787":"Hence, we have determined 76 possible anomalies in the last three years (i.e., 10 percent anomaly) of Google BigQuery Bitcoin Blockchain dataset. We have utilized two popular anomaly detection methods: Isolation Forest and K-Means. The notable fact obtained from the results is that both anomaly detection methods predicts 38 anomaly cases, which is reasonably consistent. We can further explore the anomaly detection in blockchain systems by utilizing more data or including other features. Lastly, anomaly detection tools can be effectively used to make the blockchain systems even stronger and safer by automatically recognizing and filtering out anomalous activities. ","8b763df3":"Great, mostly the features of data are following the bell-shaped curve. Now, let's plot the curves of three vital features (Transactions, Blocks, Output Satoshis) of our data with respect to the date. Is there some trend in these plots?","b5265505":"Now, Let's understand how the values of the features in our data are distributed. The bell-shaped curve is the best distribution of the values in the dataset. ","d6d04dea":"# **Anomaly Detection in Blockchain System**\nThis work is aimed to enhance the working of blockchain system by automatically recognizing and filtering out anomalous activities. The targeted components of blockchain architecture are Block and Transaction for the given task. Since they are the most basic and vital aspects in the working of blockchain systems.\n#### What is Anomaly Detection?\nAnomaly detection is a technique used to identify unusual patterns that do not conform to expected behavior, called outliers. It has many applications in business, health monitoring, network traffic systems, etc.\n#### What is the Blockchain System?\n\u201cA blockchain is a time-stamped series of an immutable record of data that is managed by a cluster of computers not owned by any single entity. Each of these blocks of data is secured and bound to each other using cryptographic principles\u201d. In simple terms, blockchain is a chain of blocks; the words \u201cblock\u201d and \u201cchain\u201d are the digital information and public database, respectively. The core components of blockchain architecture:\n1. Node \u2014 user or computer within the blockchain\n2. Transaction \u2014 smallest building block of a blockchain system\n3. Block \u2014 a data structure used for keeping a set of transactions which is distributed to all nodes in the network\n4. Chain \u2014 a sequence of blocks in a specific order\n5. Miners \u2014 specific nodes which perform the block verification process\n6. Consensus\u2014 a set of rules and arrangements to carry out blockchain operations\n\n> More information about blockchain can be found at ([https:\/\/blockgeeks.com\/guides\/what-is-blockchain-technology\/](http:\/\/) [https:\/\/www.investopedia.com\/terms\/b\/blockchain.asp](http:\/\/)).\n\nThis kernel organization can be described as:\n1. Importing Essential Libraries\n2. Reading and Processing data\n3. Model Building & Evaluation\n   *   Isolation Forest Algorithm\n   *   K Means Algorithm\n4. Conclusion","f8966ba2":"There are 38 anomalies predicted by K-Means algorithm in the last three years records. Now, similarly plot the outlier anomaly with respect to Transactions, Blocks, Sum of Output Satoshis. Do not forget to compare the previous model result with the current model results. ","af562d4a":"### Step 1: Import Essential Libraries"}}