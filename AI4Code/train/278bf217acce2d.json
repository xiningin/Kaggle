{"cell_type":{"9fa702f2":"code","3d701475":"code","73b713ba":"code","7fd0ea1c":"code","215975b5":"code","76d15231":"code","88f6351a":"markdown","c47980fb":"markdown","a25f0e26":"markdown","14b09cc1":"markdown","36ec47d2":"markdown","bf073f23":"markdown"},"source":{"9fa702f2":"# This kernel is designed to produce the predicted values for the new dataset\n# Firstly, install some Vietnamese language toolkits and models\n\n# Perform all of this on Ubuntu terminal\n\n# Install 'vncorenlp', a Vietnamese language toolkit\n!pip3 install vncorenlp\n!mkdir -p vncorenlp\/models\/wordsegmenter\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/VnCoreNLP-1.1.1.jar\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/vi-vocab\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp\/ \n!mv vi-vocab vncorenlp\/models\/wordsegmenter\/\n!mv wordsegmenter.rdr vncorenlp\/models\/wordsegmenter\/\n\n# Install the PhoBERT base model\n!wget https:\/\/public.vinai.io\/PhoBERT_base_transformers.tar.gz\n!tar -xzvf PhoBERT_base_transformers.tar.gz\n\n# Install fastBPE, fairseq\n!pip install fastBPE\n!pip uninstall typing -y\n!pip install fairseq\n\n########################################################################################################\n# This kernel is designed to produce the predicted values for the new dataset\n# Import necessary packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nfrom tqdm.notebook import tqdm\n\nfrom vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\".\/vncorenlp\/VnCoreNLP-1.1.1.jar\", annotators = \"wseg\", max_heap_size = '-Xmx500m')\n\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n\nfrom catboost import Pool, CatBoostClassifier\n\nimport xgboost as xgb\n\nfrom hyperopt import hp, fmin, atpe, tpe, Trials\n\nimport argparse\n\nfrom transformers import RobertaConfig\nfrom transformers import RobertaModel\n\n########################################################################################################\n# Now, we prepare the data and embed each sample to the respective matrices\n\n# Load PhoBERT model\nconfig = RobertaConfig.from_pretrained(\n    \"PhoBERT_base_transformers\/config.json\"\n)\nphobert = RobertaModel.from_pretrained(\n    \"PhoBERT_base_transformers\/model.bin\",\n    config = config\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--bpe-codes', \n    default = \".\/PhoBERT_base_transformers\/bpe.codes\",\n    required = False,\n    type = str,\n    help = 'path to fastBPE BPE'\n)\nargs, unknown = parser.parse_known_args()\nbpe = fastBPE(args)\n\n# Load the dictionary\nvocab = Dictionary()\nvocab.add_from_file(\".\/PhoBERT_base_transformers\/dict.txt\")\n\n########################################################################################################\n# CHANGE PATH HERE, \ndata_dir = r'..\/input\/isods-2020\/testing_data_sentiment'   # Where the testing data set is located\ntest_df = pd.read_csv(os.path.join(data_dir, 'testing_data_sentiment.csv'))","3d701475":"# Notice that, the test_df dataset must have columns named 'question' and 'answer'\n\n# Now, prepare data before feeding to the PhoBERT model\n\n# PhoBERT Tokenizer\ndef Pho_tokenizer(text):\n    # Firstly, split the text into sentences and join them by the dot\n    text = text.strip()\n    \n    # Secondly, tokenize\n    text = rdrsegmenter.tokenize(text)\n    text = ' '.join([' '.join(x) for x in text])\n    return text\n\n# Now, apply the method to all samples in the testing dataset\nprint('Tokenize...')\ntest_df['tokenized_question'] = list(map(Pho_tokenizer, test_df.question))\ntest_df['tokenized_answer'] = list(map(Pho_tokenizer, test_df.answer))\nprint('Tokenize: Done')","73b713ba":"# Function to map each sample into matrices\ndef QA2mat(dataset, max_len = 256, masking = False):\n    \n    questions = []\n    answers = []\n    \n    # For each sample in the dataset, we have one tokenized question and one tokenized answer\n    # Then, for each sentence in the question or answer\n    print('Encoding...')\n    for i in tqdm(range(dataset.shape[0])):\n        q = dataset.tokenized_question.values[i].split(' | | | ')\n        a = dataset.tokenized_answer.values[i].split(' | | | ')\n\n        sub_q = []\n        sub_a = []\n\n        for m in q:\n            # Encoding\n            subwords_q = '<s> ' + bpe.encode(m) + ' <\/s>'\n            encoded_sent_q = vocab.encode_line(subwords_q, append_eos = True, add_if_not_exist = False).long().tolist()\n            sub_q.append(encoded_sent_q)\n\n        for n in a:\n            subwords_a = '<s> ' + bpe.encode(n) + ' <\/s>'\n            encoded_sent_a = vocab.encode_line(subwords_a, append_eos = True, add_if_not_exist = False).long().tolist()\n            sub_a.append(encoded_sent_a)\n\n        questions.append(sub_q)\n        answers.append(sub_a)\n    \n    # 'questions' and 'answers' are 2 lists of lists, \n    # each element of 'questions' (or 'answers') contains the encoded sentences of that question (or answer)\n    \n    # Next, padding each sentence within one question (or answer)\n    from tensorflow.keras.preprocessing.sequence import pad_sequences as pad_sequence\n    pad_que = []\n    pad_ans = []\n    \n    print('Padding...')\n    for i in tqdm(range(len(questions))):\n        # Padding\n        pad_que.append(pad_sequence(questions[i], maxlen = max_len, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\"))  # \n        pad_ans.append(pad_sequence(answers[i], maxlen = max_len, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\"))  # \n    \n    if masking == True:\n        mask_que = []\n        for sample in pad_que:\n            mask_sample = []\n            for sent in sample:\n                mask_q = [int(token_id > 0) for token_id in sent]\n                mask_sample.append(mask_q)\n            mask_que.append(mask_sample)\n\n        mask_ans = []\n        for sample in pad_ans:\n            mask_sample = []\n            for sent in sample:\n                mask_a = [int(token_id > 0) for token_id in sent]\n                mask_sample.append(mask_a)\n            mask_ans.append(mask_sample)\n    \n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    phobert.to(device)\n    \n    print('Embedding...')\n    with torch.no_grad():\n        output = []\n        for i in tqdm(range(len(pad_que))):\n            output_ques = phobert(input_ids = torch.tensor(pad_que[i]).to(device))[1]\n            output_ans = phobert(input_ids = torch.tensor(pad_ans[i]).to(device))[1]\n            output.append((output_ques @ output_ans.t()).cpu())\n    \n    return(output)\n\n# Import training embeddings\ntrain_embed = torch.load('..\/input\/embeddings\/train_embed.pt', map_location = torch.device('cpu'))\n\n# Convert to the embeddings for the testing set\ntest_embed = QA2mat(test_df[['tokenized_question', 'tokenized_answer']])\n\n# Load the training labels\ntrain_labels = np.load('..\/input\/embeddings\/train_labels.npy')","7fd0ea1c":"# Transform the matrix embeddings of testing samples into vectors\ndef embedding_transform(embed):\n    flatten_embed = []\n    for text in tqdm(embed):\n        flatten_embed.append(text.flatten())\n    return flatten_embed\n\n# Transform the embeddings and padding (and cutting...)\nmax_len = 512\ntrain_embed = embedding_transform(train_embed)\ntest_embed = embedding_transform(test_embed)\n\ntrain_embed = pad_sequence(train_embed, batch_first = True)[:,:max_len].numpy()\ntest_embed = pad_sequence(test_embed, batch_first = True)[:,:max_len].numpy()","215975b5":"# The hyperparameter space\nparams = {\n    'iter': [1000, 2000, 3000],\n    'l2_leaf': [1e-3, 1e-1, 1, 5],\n    'max_depth': [5, 10, 15],\n    'max_leaves': [5, 10, 15, 20],\n    'border_count': [32, 128, 256]\n}\n\n# Optimal hyperparameters (using Bayesian hyperparameter tuning technique)\nhopt = {'border_count': 2, 'iter': 1, 'l2_leaf': 0, 'max_depth': 0, 'max_leaves': 3}\n\nopt_params = {\n    'iter': params['iter'][hopt['iter']],\n    'l2_leaf': params['l2_leaf'][hopt['l2_leaf']],\n    'max_depth': params['max_depth'][hopt['max_depth']],\n    'max_leaves': params['max_leaves'][hopt['max_leaves']],\n    'border_count': params['border_count'][hopt['border_count']]\n}\n\n###########################################################################\nmode = 'inference'\nmodel_dir = r'..\/input\/isods-2020-pretrained-classifiers'\n###########################################################################\n\nprint(f'{mode.upper()}...')\nif mode == 'train':\n    print('Fitting the model using optimal hyperparameters...')\nelse:\n    print('Inference by the pretrained models...')\nSEEDS = range(7)\ny_pred = np.zeros(test_embed.shape[0])\nfor seed in SEEDS:\n    y_pred_seed = y_pred.copy()\n    for n, (tr, te) in enumerate(KFold(n_splits = 5, \n                                       random_state = seed, \n                                       shuffle = True).split(train_embed, train_labels)):\n        print('Training fold:', n)\n        print('-' * 50)\n        x_tr, x_val = train_embed[tr], train_embed[te]\n        y_tr, y_val = train_labels[tr], train_labels[te]\n\n        _trn = Pool(x_tr, label = y_tr)\n        _val = Pool(x_val, label = y_val)\n        regressor = CatBoostClassifier(loss_function = \"Logloss\",\n                                       eval_metric = \"AUC\",\n                                       task_type = \"GPU\",\n                                       grow_policy = 'Lossguide',\n                                       iterations = opt_params['iter'],\n                                       l2_leaf_reg = opt_params['l2_leaf'],\n                                       max_leaves = opt_params['max_leaves'],\n                                       random_seed = seed,\n                                       od_type = \"Iter\",\n                                       depth = opt_params['max_depth'],\n                                       early_stopping_rounds = 15000,\n                                       border_count = opt_params['border_count'],\n                                       verbose = 500)\n        \n        if mode == 'train':\n            # Fit\n            regressor.fit(train_embed, train_labels)\n            # Store the model\n            regressor.save_model(f'ISODS_II_PhoBERT_catboost_{seed}_{n}')\n        elif mode == 'inference':\n            # Load the model\n            regressor.load_model(os.path.join(model_dir, f'ISODS_II_PhoBERT_catboost_{seed}_{n}'))\n        y_pred_seed += regressor.predict(test_embed) \/ 5\n    y_pred += y_pred_seed \/ len(SEEDS)","76d15231":"# Submission\nids = list(range(1, test_embed.shape[0] + 1))\nlabels = (y_pred > 0.5).astype(int).astype(str)\nlabels[labels == '0'] = 'N'\nlabels[labels == '1'] = 'Y'\nres = {\n    'num': ids,\n    'label': labels\n}\npd.DataFrame.from_dict(res).to_csv('results.csv', index = False)","88f6351a":"* Tokenize the test dataset","c47980fb":"* Using PhoBERT to encode each (question-answer) pair to a pair of vectors","a25f0e26":"* Feature generation","14b09cc1":"# Submission","36ec47d2":"# Training and inference","bf073f23":"# Prepare the dataset"}}