{"cell_type":{"e3bade50":"code","cd806db5":"code","5fa10c7a":"code","d5e9fa89":"code","f7d35ea0":"code","71cb681e":"code","6a962c9d":"code","7151e910":"code","46cf32f4":"code","8a93a315":"code","0eeeb677":"code","fe135069":"code","3ba3c746":"code","1817334c":"code","cf772973":"code","e130e37a":"code","c910b6a7":"code","f082e12a":"code","d8c56a03":"code","2af84718":"code","53a8b547":"code","bb98a0f4":"code","b494ddfa":"code","159ebc1c":"code","4ec0e759":"code","3a124430":"code","6dfe9431":"markdown","52f7387a":"markdown","57cb04ef":"markdown","cab24288":"markdown","452156d9":"markdown","f6de71ad":"markdown","3c02577c":"markdown","863277d9":"markdown","c6088a37":"markdown","b1c9e882":"markdown","486c17f3":"markdown","cb252958":"markdown","c067feb4":"markdown","5c3b2783":"markdown","905a3e56":"markdown","5a82342e":"markdown","df7fcde7":"markdown","96671eaa":"markdown"},"source":{"e3bade50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd806db5":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom statsmodels.stats.diagnostic import het_white","5fa10c7a":"df_test_score  = pd.read_csv('\/kaggle\/input\/predict-test-scores-of-students\/test_scores.csv')","d5e9fa89":"df_test_score.head()","f7d35ea0":"df_test_score.info()","71cb681e":"df_test_score.describe()","6a962c9d":"df_test_score = df_test_score.drop(columns=['student_id'])","7151e910":"for col in ['school' ,'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n    df_test_score[col] = df_test_score[col].astype('category')","46cf32f4":"df_test_score.isnull().sum()","8a93a315":"df_test_score.hist()\nplt.show()","0eeeb677":"attributes = ['n_student', 'pretest', 'posttest']\nscatter_matrix(df_test_score[attributes])","fe135069":"lin_reg =  LinearRegression()\nx = np.array(df_test_score['pretest']).reshape(-1, 1)\ny = np.array(df_test_score['posttest']).reshape(-1, 1)\nlin_reg.fit(x, y)","3ba3c746":"r_sq = lin_reg.score(x, y)\nprint('coefficient of determination:', r_sq)","1817334c":"for col in ['school' , 'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n  sns.catplot(x=col, y=\"posttest\", kind=\"box\", data=df_test_score)","cf772973":"le = preprocessing.LabelEncoder()\nle.fit(df_test_score['gender'])\nle.transform(df_test_score['gender'])\ndf_train = df_test_score\nfor col in ['school' , 'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n  le.fit(df_train[col])\n  df_train[col] = le.transform(df_train[col])","e130e37a":"X = df_train.drop(columns=['posttest'])\ny = df_train['posttest']\ndt_model = RandomForestRegressor()\ndt_model.fit(X, y)","c910b6a7":"feature_list = list(X.columns)\n# Set the style\nimportances = list(dt_model.feature_importances_)\nplt.style.use('fivethirtyeight')\nx_values = list(range(len(importances)))\nplt.bar(x_values, importances, orientation = 'vertical')\nplt.xticks(x_values, feature_list, rotation='vertical')\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","f082e12a":"feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","d8c56a03":"df_sel_var = df_test_score[['pretest', 'teaching_method', 'posttest']]","2af84718":"X = df_sel_var.drop(columns=['posttest'])\ny = df_sel_var['posttest']\nX_training_data, X_testing_data, y_training_data, y_testing_data = train_test_split(X, y, test_size=0.3)","53a8b547":"final_model =  LinearRegression()\nfinal_model.fit(X_training_data, y_training_data)","bb98a0f4":"r_sq = final_model.score(X_training_data, y_training_data)\nprint('Coefficient of determination for training data:', r_sq)","b494ddfa":"X1 = sm.add_constant(X_training_data)\nmodelOLS = sm.OLS(y_training_data, X1)\nmodelOLS_fit = modelOLS.fit()\nmodelOLS_fit.summary()","159ebc1c":"white_test = het_white(modelOLS_fit.resid,  modelOLS_fit.model.exog)","4ec0e759":"#white_test\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nprint(dict(zip(labels, white_test)))","3a124430":"y_pred = final_model.predict(X_testing_data)\nprint('Validation MAE', mean_absolute_error(y_testing_data, y_pred))\nprint('Validation RMSE', sqrt(mean_squared_error(y_testing_data, y_pred)))\nprint('Validation R^2', final_model.score(X_testing_data, y_testing_data))","6dfe9431":"****Model Training","52f7387a":"****EDA","57cb04ef":"We make sure there is no null values.","cab24288":"The Jarque-Bera test ensures the normality of the residuals and all the coeficients are significant and the rest of hipothesis underliying linear model, except ot Heteroskedasticity.","452156d9":"We drop the id since it has no role in the predicting task at all.","f6de71ad":"We start the exploration of the numerical variables.","3c02577c":"The data is transformed into categorical.","863277d9":"Finally, we train a simple linear regression model.","c6088a37":"****TEST SCORE","b1c9e882":"****Variable Selection","486c17f3":"How much variance is explained by pretest? The answer is below","cb252958":"We select only those variables that explain the 95% of the variance. As a result, only two variables are selected.","c067feb4":"We are going to use a base tree model to select the variables, i.e. random forest. We proved two more models (XGboost and CART) with the same variable selection.","5c3b2783":"The predictions are in generral quiet accurate.","905a3e56":"A 90% of the variance is already explained by one variable. So, the strategy consists of including the most relevant variables and eliminating the noising ones. We also explore the categorical-response relationship.","5a82342e":"Heteroskedasticity assumption is also checked.","df7fcde7":"Let's use numerical encoding for the categorical features.","96671eaa":"More interesting than the single univariate plots, we can explore relationship betewwen each pair of variables. immediately it outcomes a linear relathionship between de variables pretest and posttest. By the other hand, it seems to exists a different behaviour in extreme regions of n_students where variable is too high or to low."}}