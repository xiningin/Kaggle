{"cell_type":{"386c592f":"code","2b82d033":"code","719fa223":"code","dfe1f504":"code","31834a16":"code","6b4d043e":"code","be691f74":"code","75c1890c":"code","11cc9b0b":"code","d63a9603":"code","8948d5db":"code","359afb55":"code","6c62356e":"code","a85567f4":"code","02829ee3":"code","bbd12a30":"code","7c57e88d":"code","1cfb7a67":"code","db7dca75":"code","678ca03b":"code","6e6c2496":"code","0ec2def0":"code","ea6c115f":"code","59978d64":"code","3fc522e7":"code","bc7f711c":"code","97799649":"code","57edb705":"code","c08514ca":"code","17469585":"code","18f72ec6":"code","17717353":"code","da2c3237":"code","d0e906d6":"code","a3f5b517":"code","bddf2fec":"code","e3f95892":"code","efe4013b":"code","adf2a113":"markdown","41e23939":"markdown","ff78d1aa":"markdown","8af73cbf":"markdown","4a0bdb32":"markdown","12783c8e":"markdown","7cf5fbef":"markdown","925c839a":"markdown"},"source":{"386c592f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b82d033":"import pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","719fa223":"# Checking for null values\ndf.isnull().sum()","dfe1f504":"# Dropping all the NA values from the dataset\ndf.dropna(inplace = True)","31834a16":"# Checking for empty string\nblank = []\n\nfor index, review, label in df.itertuples():\n    if type(review) == str:\n        if review.isspace() == True:\n            blank.append(index)\n            \nprint(blank)","6b4d043e":"# Checking for duplicates\nprint(df.drop_duplicates(inplace = True))","be691f74":"df.shape","75c1890c":"df['sentiment'].value_counts()","11cc9b0b":"X = df['review']\ny = df['sentiment']","d63a9603":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","8948d5db":"# Building a pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\np_nb = Pipeline([('tfidf', TfidfVectorizer()),\n                 ('clf', MultinomialNB())])\n\np_svc = Pipeline([('tfidf', TfidfVectorizer()),\n                  ('linear', LinearSVC())])","359afb55":"# Using NB\np_nb.fit(X_train, y_train)","6c62356e":"predict = p_nb.predict(X_test)","a85567f4":"from sklearn import metrics\n\nprint(metrics.confusion_matrix(y_test, predict))","02829ee3":"# We can also make confusion metrics using seaborn(heatmap)","bbd12a30":"print(metrics.classification_report(y_test, predict))","7c57e88d":"# Using linearSVC\n\np_svc.fit(X_train, y_train)","1cfb7a67":"predict_svc = p_svc.predict(X_test)","db7dca75":"print(metrics.confusion_matrix(y_test, predict_svc))","678ca03b":"print(metrics.classification_report(y_test, predict_svc))","6e6c2496":"accuracy_svc = metrics.accuracy_score(y_test, predict_svc)","0ec2def0":"# Checking the model\ncustom_review = \"First half was good but after interval it was not good.\"","ea6c115f":"p_svc.predict([custom_review])","59978d64":"custom_review_2 = \"Movie was so so.\"","3fc522e7":"print(p_svc.predict([custom_review_2]))","bc7f711c":"import spacy\nnlp = spacy.load('en_core_web_sm')","97799649":"len(nlp.Defaults.stop_words)","57edb705":"sw = nlp.Defaults.stop_words","c08514ca":"p_svc_2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=sw)),\n                  ('linear', LinearSVC())])","17469585":"p_svc_2.fit(X_train, y_train)","18f72ec6":"prediction_svc_sw = p_svc_2.predict(X_test)","17717353":"print(metrics.confusion_matrix(y_test, prediction_svc_sw))","da2c3237":"print(metrics.classification_report(y_test, prediction_svc_sw))","d0e906d6":"after_sw = metrics.accuracy_score(y_test, prediction_svc_sw)","a3f5b517":"print(f'Before stop words - {accuracy_svc}, After stopwords - {after_sw}')","bddf2fec":"stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']","e3f95892":"p_svc_2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=sw)),\n                  ('linear', LinearSVC())])\n\np_svc_2.fit(X_train, y_train)\nprediction_few_sw = p_svc_2.predict(X_test)\n\nprint(metrics.classification_report(y_test, prediction_svc_sw))","efe4013b":"few_sw = metrics.accuracy_score(y_test, prediction_few_sw)\nfew_sw","adf2a113":"#### Considering neutral as negative only.","41e23939":"### Now lets check if the accuracy is affected by stopwords.","ff78d1aa":"#### Predicting the 'not' also.","8af73cbf":"## Thanking you, don't forget to upvote the notebook.","4a0bdb32":"#### Right now the corpus is not that huge but it might save some time & increase lil accuracy.","12783c8e":"### It menas we do not have empty strings in the dataset.","7cf5fbef":"#### Spcay has 326 default stopwords.","925c839a":"#### This implies that some stop words are retricting the model to predict correctly. Lets try with few stop words only."}}