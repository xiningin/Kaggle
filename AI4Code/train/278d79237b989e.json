{"cell_type":{"fa77886d":"code","c9aef5be":"code","a0135f2f":"code","0e7cbb70":"code","1aa99def":"code","0d84b759":"code","02402ea9":"code","da171d6d":"code","9979462d":"code","d133e826":"code","353587c9":"code","57315c62":"code","5576bddb":"code","ed4104cf":"code","88f3956f":"code","09c03e53":"code","84ed8245":"code","0fa09c56":"code","0b246f9c":"code","f8274760":"code","33ba843f":"code","63187360":"code","4a019c4b":"code","d801f01b":"code","ef75be6c":"code","dcc7072a":"markdown","aef16bba":"markdown","9c713a37":"markdown","517097eb":"markdown","75e12452":"markdown","616ec418":"markdown","ba3931fe":"markdown","5051b358":"markdown","dc3b3c60":"markdown","9c99f1b7":"markdown","4d50a052":"markdown","ebc623d4":"markdown","9e1957a6":"markdown","b90bbed8":"markdown","d3f68358":"markdown","a103a51a":"markdown"},"source":{"fa77886d":"import numpy as np\nfrom numpy import vstack, hstack\n\nimport pandas as pd\nimport cv2\nimport glob\nimport math\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.cuda as tc","c9aef5be":"class FaceMaskDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.transform = transform\n        sub_paths = ['with_mask\/', 'without_mask\/']\n        self.df = self.get_df(path, sub_paths)\n        \n    def get_df(self, path, sub_paths):\n        paths = list()\n        paths.extend(glob.glob(path + sub_paths[0] + \"*.jpg\"))\n        labels = [1] * len(paths)\n        paths.extend(glob.glob(path + sub_paths[1] + \"*.jpg\"))\n        labels.extend([0] * (len(paths) - len(labels)))\n        df = pd.DataFrame({ \"path\" : paths, \"label\" : labels})\n        df = shuffle(df)\n        return df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.df.iloc[idx]['path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.df.iloc[idx]['label']\n        \n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n        \n        return image, label","a0135f2f":"transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RandomCrop(height=128, width=128),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)","0e7cbb70":"path = '..\/input\/face-mask-dataset\/data\/'\n\nfmd = FaceMaskDataset(path, transform=transform)","1aa99def":"plt.imshow(fmd[1][0].permute(1, 2, 0))","0d84b759":"subset = torch.utils.data.Subset(fmd, np.arange(100))","02402ea9":"dataloader = DataLoader(subset, batch_size=8, shuffle=True, num_workers=4)","da171d6d":"class Conv:\n    def __init__(self, batch_size, channel_size, width_size, \n                 height_size, kernel_number, kernel_size=3, \n                 stride_size=1, is_input=False):\n        \n        self.batch_size = batch_size\n        self.channel_size = channel_size\n        self.width_size = width_size\n        self.height_size = height_size\n        self.kernel_number = kernel_number\n        self.kernel_size = kernel_size\n        self.stride_size = stride_size\n        self.is_input = is_input\n        \n        self.padding_size = (kernel_size - 1) \/\/ 2\n        \n        self.layer = nn.Conv2d(channel_size, kernel_number, kernel_size, stride_size,\n                               self.padding_size).cuda().type(torch.float32)\n        \n        nn.init.xavier_uniform_(self.layer.weight)\n\n        self.delta_w = 0\n        self.delta_b = 0\n\n    def forward(self, input=tc.FloatTensor(0)):\n        self.input = input\n        self.act_output = self.relu(self.layer.forward(tc.FloatTensor(self.input)))\n        return self.act_output\n\n    def relu(self, xa, derive=False):\n        # back prop\n        if derive:\n            return torch.ceil(torch.clamp(xa, min=0, max=1)).detach()\n        # forward prop\n        return torch.clamp(xa, min=0).detach()\n\n    def backward(self, next_layer_grad, lr_rate=0.001, momentum_rate=0.9):\n        self.act_output = torch.mul(next_layer_grad, self.relu(self.act_output, derive=True)).detach()\n\n        # this convolution operation for calculating gradient of weights\n        dw_layer = nn.Conv2d(self.batch_size, self.kernel_number, self.act_output.shape[2], 1,\n                             self.padding_size).cuda().type(torch.float32)\n                             \n        gradx = self.act_output.clone().detach()\n        # flipping for convolution operation\n        gradx = gradx.cpu().detach().numpy()[:, :, ::-1, ::-1]                 \n        # set weight and bias with 0  with activation gradient\n        dw_layer.weight.data = tc.FloatTensor(gradx.copy()).transpose(0, 1).detach()\n        del gradx\n        dw_layer.bias.data = tc.FloatTensor(np.zeros(self.kernel_number)).detach()\n                             \n        # momentum update\n        self.delta_w = momentum_rate * self.delta_w + (\n                dw_layer.forward(self.input.transpose(0, 1)).transpose(0, 1).detach() \/ (self.batch_size)).detach()\n        self.delta_b = momentum_rate * self.delta_b + (torch.sum(self.act_output, dim=[0, 2, 3]).detach() \/ (\n                self.act_output.shape[0] * self.batch_size * self.act_output.shape[2] * self.act_output.shape[\n            3])).detach()\n                             \n        # weight and bias update\n        self.layer.weight.data -= lr_rate * self.delta_w.detach()\n        self.layer.bias.data -= lr_rate * self.delta_b.detach()\n\n        # if it is not input that initially given model then calculate input gradient as well\n        if (self.is_input == False):\n            # convolution operation for calculating input gradient\n            dx_layer = nn.Conv2d(self.kernel_number, self.channel_size, self.kernel_size, self.stride_size,\n                                 self.padding_size).cuda().type(torch.float32)\n            # get weight from conv layer\n            temp_weight = self.layer.weight.data.clone().cpu().numpy()\n            # flip kernel for convolution operation\n            temp_weightx = temp_weight[:, :, ::-1, ::-1]\n            #set weight with flipped kernel\n            dx_layer.weight.data = tc.FloatTensor(temp_weightx.copy()).transpose(0, 1).detach()\n            #set bias with zero\n            dx_layer.bias.data = tc.FloatTensor(np.zeros(self.channel_size)).detach()\n            #input gradient\n            out = dx_layer.forward(self.act_output)\n\n            del temp_weight\n            del temp_weightx\n\n            return out\n        else:\n            #return input itself\n            # del self.act_output\n            return self.input","9979462d":"class MaxPooling:\n    def __init__(self, kernel_size=2, stride_size=2):\n        self.kernel_size = kernel_size\n        self.stride_size = stride_size\n        self.layer = nn.MaxPool2d(self.kernel_size, self.stride_size, return_indices=True).cuda().type(torch.float32)\n\n    def forward(self, input=tc.FloatTensor(0)):\n        self.input = input.detach()\n        self.output, self.indices = self.layer.forward(self.input)\n        return self.output\n\n    def backward(self, next_layer_grad):\n        #unpool operation for back propagation\n        back_layer = nn.MaxUnpool2d(self.kernel_size, self.stride_size).cuda().type(torch.float32)\n        #output of backpropagation\n        out = back_layer.forward(next_layer_grad, self.indices, output_size=self.input.shape).type(\n            torch.float32).detach()\n        del back_layer\n\n        return out","d133e826":"class Dense:\n    def __init__(self, batch_size, input_size, dense_number, is_input=False):\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.dense_number = dense_number\n        self.is_input = is_input\n        #initialize weights and biases\n        self.weight = tc.FloatTensor(\n            np.random.normal(size=(self.input_size, self.dense_number), loc=0, scale=0.01).astype(np.float32)).type(\n            torch.float32).detach()\n        self.bias = tc.FloatTensor(np.zeros((1, dense_number))).type(torch.float32).detach()\n        #momentum terms\n        self.delta_w = 0\n        self.delta_b = 0\n\n    def forward(self, input=tc.FloatTensor(0).detach()):\n        self.input = input.detach()\n        self.act_output = self.relu(torch.mm(self.input, self.weight) + self.bias).detach()\n        return self.act_output\n\n    def relu(self, xa, derive=False):\n        # back prop\n        if derive:\n            return torch.ceil(torch.clamp(xa, min=0, max=1)).detach()\n        # forward prop\n        return torch.clamp(xa, min=0).detach()\n\n    def backward(self, next_layer_grad, lr_rate=0.001, momentum_rate=0.9):\n        #find gradient activation functions\n        self.act_output = torch.mul(next_layer_grad, self.relu(self.act_output, derive=True)).type(\n            torch.float32).detach()\n        #find momentum terms\n        self.delta_w = momentum_rate * self.delta_w + (\n                torch.mm(self.input.transpose(0, 1), self.act_output).detach() \/ self.batch_size).detach()\n        self.delta_b = momentum_rate * self.delta_b + (\n                torch.sum(self.act_output, dim=0, keepdim=True).detach() \/ self.batch_size).detach()\n        #update weight and bias\n        self.weight -= lr_rate * self.delta_w.detach()\n        self.bias -= lr_rate * self.delta_b.detach()\n        #check whether input is data, if it is not, then find input gradient\n        if self.is_input == False:\n            #gradient operation for input\n            out = torch.mm(self.act_output, self.weight.transpose(0, 1)).type(torch.float32).detach()\n            return out\n        else:\n            #return input itself\n            return self.input","353587c9":"class Flatten:\n    def forward(self, input=tc.FloatTensor(0).detach()):\n        self.input = input.detach()\n        self.batch_size = self.input.shape[0]\n        self.channel_size = self.input.shape[1]\n        self.width = self.input.shape[2]\n        self.height = self.input.shape[3]\n\n        self.output = self.input.view(self.batch_size, -1).detach()\n        return self.output\n\n    def backward(self, next_layer_grad):\n        return next_layer_grad.view(self.batch_size, self.channel_size, self.width, self.height).detach()","57315c62":"class Dropout:\n    def __init__(self, dropout):\n        self.dropout = dropout\n\n    def forward(self, input):\n        self.input = input.detach()\n        self.dropout_matrix = tc.FloatTensor(np.zeros(self.input.shape)).detach()\n        #create mask\n        self.dropout_matrix = tc.FloatTensor(\n            np.random.binomial(1, 1 - self.dropout, size=self.input.shape)).expand_as(\n            self.input).detach()\n        #apply mask\n        self.output = torch.mul(self.input, self.dropout_matrix).detach() \/ (1 - self.dropout + np.exp(-32))\n        return self.output\n\n    def backward(self, next_layer_grad):\n        return torch.mul(next_layer_grad, self.dropout_matrix).detach() \/ (1 - self.dropout + np.exp(-32))","5576bddb":"class Output:\n    def __init__(self, class_number):\n        self.class_number = class_number\n        self.y = None\n\n    def forward(self, input=tc.FloatTensor(0).detach(), target=tc.FloatTensor(0).detach()):\n        self.input = input.detach()\n        self.target = target\n        self.output, self.loss = self.softmax_with_cross_entropy(self.input, self.target)\n        return self.loss\n\n    def softmax_with_cross_entropy(self, x, target, derive=False):\n        #softmax\n        self.y = torch.eye(self.class_number)[tc.FloatTensor(target).type(torch.long).view(-1).detach(), :].cuda().type(\n            torch.float32)\n        \n        if derive == True:\n            return x - self.y\n\n        maximum = torch.max(x, dim=1, keepdim=True)[0].detach()\n        self.pred = torch.exp(x - maximum).detach()\n        self.pred = self.pred.detach() \/ torch.sum(self.pred, 1, keepdim=True).detach()\n        #categorical cross entropy\n        self.loss = -torch.sum(self.y * torch.log(self.pred + np.exp(-32))).type(torch.float32).detach() \/ \\\n                    self.pred.shape[0]\n\n        return self.pred, self.loss\n\n    def backward(self, output, y):\n        return self.softmax_with_cross_entropy(output, y, derive=True)","ed4104cf":"class Sigmoid():\n    def forward(self, input):\n        return torch.sigmoid(input)\n    \n    def backward(self, output):\n        sigmoid = self.forward(output)\n        return sigmoid * (1 - sigmoid)","88f3956f":"class ReLU():\n    def forward(self, input):\n        return torch.clamp(input, min=0).detach()\n    \n    def backward(self, output):\n        return torch.ceil(torch.clamp(output, min=0, max=1)).detach()","09c03e53":"images, labels = next(iter(dataloader))\nimages = images.cuda()\nlabels = labels.cuda().type(torch.cuda.FloatTensor)\nprint(images.shape, labels.shape)","84ed8245":"def init_model():\n    model = []\n    model.append(Conv(batch_size=32, channel_size=3, width_size=128, \n                      height_size=128, kernel_number=64, is_input=True))\n    model.append(MaxPooling())\n    model.append(Conv(batch_size=32, channel_size=64, width_size=64, \n                      height_size=64, kernel_number=128))\n    model.append(MaxPooling())\n    model.append(Flatten())\n    model.append(Dropout(dropout=0.5))\n    model.append(Dense(batch_size=32, input_size=131072, dense_number=64))\n    model.append(ReLU())\n    model.append(Dense(batch_size=32, input_size=64, dense_number=2))\n    return model","0fa09c56":"def forward(images, model):\n    out = images\n    for m in model:\n        out = m.forward(out)\n    return out","0b246f9c":"epochs = 101\noutput = Output(class_number=2)\n#losses = np.zeros((epochs, 1))\n\nk_fold = 3\nnr = math.ceil(len(fmd) \/ 10)\nindices = np.arange(len(fmd))\n\nepoch_loss = np.zeros((k_fold, epochs))\nepoch_accuracy = np.zeros((k_fold, epochs))\n\nmodels = [init_model(), init_model(), init_model()]\n\nfor fold in range(k_fold):\n\n    fmd.df = shuffle(fmd.df)\n    subset = torch.utils.data.Subset(fmd, indices[fold * nr: nr * (fold + 1)])\n    dataloader = DataLoader(subset, batch_size=32, shuffle=True, num_workers=4)\n\n    valid_idx = int(len(dataloader) * 0.8)\n\n    for epoch in range(epochs):\n        \n        valid_out = []\n        valid_target = []\n        \n        losses = []\n        \n        for i, (images, labels) in enumerate(dataloader):\n            images = images.cuda()\n            labels = labels.cuda().type(torch.cuda.FloatTensor)\n\n            # forward\n            out = forward(images, models[fold])\n\n            # training\n            if(i <= valid_idx):\n\n                loss = output.forward(out, labels.cuda())\n                losses.append(loss)\n\n                # backward\n                deriv = output.backward(out, labels)\n                for m in reversed(models[fold]):\n                    deriv = m.backward(deriv)\n\n            # validation\n            else:\n                valid_out.append(out)\n                valid_target.append(labels)\n\n                if((i + 1) == len(dataloader)):\n                    total_acc = 0\n                    for i, valid in enumerate(valid_out):\n                        valids = torch.argmax(valid, dim=1)\n                        total_acc += np.sum(valid_target[i].cpu().numpy() ==\n                                            valids.cpu().numpy())\n\n                    epoch_loss[fold][epoch] = sum(losses)\/len(losses)\n                    epoch_accuracy[fold][epoch] = total_acc \/ (32 * len(valid_target))\n\n                    if(epoch % 25 == 0):\n                        print(\"Fold:{:4d}, Epoch:{:4d}, Loss:{:1.3f}, Acc:{:1.3f}\"\n                              .format(fold, epoch, epoch_loss[fold][epoch], \n                                      epoch_accuracy[fold][epoch]))","f8274760":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\naxes[0].plot(np.arange(len(epoch_accuracy[0])), epoch_accuracy[0])\naxes[1].plot(np.arange(len(epoch_accuracy[1])), epoch_accuracy[1])\naxes[2].plot(np.arange(len(epoch_accuracy[2])), epoch_accuracy[2])\nfig.tight_layout()","33ba843f":"dataloader = DataLoader(fmd, batch_size=len(fmd), shuffle=True, num_workers=4)\nimages, labels = next(iter(dataloader))","63187360":"X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3)","4a019c4b":"def get_ensemble(x, models):\n    meta_X = list()\n    for i, X in tqdm(enumerate(x)):\n        outs = list()\n        for m in models:\n            out = forward(X.unsqueeze(0).cuda(), m)\n            outs.extend(out.cpu())\n        meta_X.append(hstack(outs))\n    return meta_X","d801f01b":"meta_train = get_ensemble(X_train, models)\nmeta_test = get_ensemble(X_test, models)","ef75be6c":"# Train ensemble\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(meta_train, y_train)\n\n# Get Test accuracy\nyhat = model.predict(meta_test)\nacc = accuracy_score(y_test, yhat)\nprint(\"Final Accuracy:{:1.3f}\".format(acc))","dcc7072a":"# \u2591\u2591\u2591\u2591\u2591Ensemble\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591","aef16bba":"# \u2591\u2591\u2591\u2591\u2591Dataset\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591","9c713a37":"## Flatten","517097eb":"## Dropout","75e12452":"## Sigmoid","616ec418":"## Dense","ba3931fe":"## MaxPooling","5051b358":"## ReLU    ","dc3b3c60":"# \u2591\u2591\u2591\u2591\u2591GOAL\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\nThis notebook is pure Pytorch implementation for learning purpose.<br>\nThe scope is not to get high accuracy but to get an ensemble of all models<br>\nand create a slightly stronger model.","9c99f1b7":"## Output","4d50a052":"<pre>\n \u2591\u2591                                        \n  \u2593\u2593\u2591\u2591        \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591          \u2592\u2592\u2592\u2592  \n  \u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u2592\u2592\u2593\u2593\u2593\u2593\u2592\u2592\u2593\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  \u2591\u2591\u2588\u2588\u2593\u2593  \n  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2588\u2588\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \n \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \n \u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\n \u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\n \u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2591\u2591\u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2588\u2588\u2591\u2591\n \u2591\u2591\u2592\u2592\u2588\u2588\u2593\u2593\u2591\u2591  \u2591\u2591\u2593\u2593\u2588\u2588\u2593\u2593\u2591\u2591\u2591\u2591\u2593\u2593\u2588\u2588\u2593\u2593\u2591\u2591\u2591\u2591\u2592\u2592\u2593\u2593\u2588\u2588\u2591\u2591\u2591\u2591\n \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591      \u2593\u2593\u2593\u2593\u2591\u2591\u2592\u2592\u2593\u2593\u2592\u2592      \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588  \n \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2593\u2593\u2588\u2588\u2588\u2588\u2591\u2591\u2592\u2592\u2588\u2588\u2593\u2593\u2592\u2592\u2591\u2591\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\n \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2591\u2591\n \u2591\u2591\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\n      \u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591        \u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592    \n\n<b>Face Mask Detection<\/b> - Pytorch CNN\nby Alin Cijov\n<\/pre>","ebc623d4":"# \u2591\u2591\u2591\u2591\u2591Analysis\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591","9e1957a6":"## Convolutional","b90bbed8":"# \u2591\u2591\u2591\u2591\u2591Model\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591","d3f68358":"# \u2591\u2591\u2591\u2591\u2591Implementation\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591","a103a51a":"# \u2591\u2591\u2591\u2591\u2591Training\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591"}}