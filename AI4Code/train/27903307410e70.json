{"cell_type":{"21c3c163":"code","d9e9d94f":"code","33f12123":"code","51cd9177":"code","cb5d5a01":"code","beacc5aa":"code","c89e63c2":"code","128561f1":"code","dfaa04ae":"code","53b4e5c2":"code","bfa8fb76":"code","375ec1e4":"code","f60058a2":"code","ef14de2e":"code","e801e115":"code","7c642103":"code","ece77f8b":"code","1935fe60":"code","18d4dac2":"code","2be533bd":"code","aa901bc5":"code","21745483":"code","46dada34":"code","39bc3cb3":"code","3451b6c4":"code","1ed5a8bb":"code","cd76495b":"code","30536bb4":"code","e5a5aa93":"code","e293b1fd":"code","3fa950d1":"code","aaa93684":"code","ad7d9f85":"code","05c97778":"code","c81ff691":"markdown","44d8e97c":"markdown","16e68a81":"markdown","f6054529":"markdown","b5cf8c23":"markdown","fb0c5ef2":"markdown","46e23742":"markdown","356605c6":"markdown","da10800c":"markdown","c2bdf6e3":"markdown","2b0823f9":"markdown","595b00f6":"markdown","136f5496":"markdown","c33d3d4f":"markdown","9e4ff8c2":"markdown","04469e08":"markdown","cb506fd1":"markdown","c584d039":"markdown","5568d364":"markdown","f99ca2cf":"markdown"},"source":{"21c3c163":"#Import all the libraries that we might need","d9e9d94f":"import pandas as pd\nimport numpy as np\nimport keras\nnp.random.seed(2)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\nimport itertools\n\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","33f12123":"#Data\n\ndata=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\ndata.head()","51cd9177":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","cb5d5a01":"def model_func(X,Y,a,b):\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=a, random_state=b)\n\n    X_train = np.array(X_train)\n    X_test=np.array(X_test)\n    y_train=np.array(y_train)\n    y_test=np.array(y_test)\n\n    model = Sequential([\n         #First Layer\n        Dense(units=16, input_dim=29, activation='relu'),\n          #Second Layer\n        Dense(units=24,activation='relu'),\n        Dropout(0.3),\n        #Third Layer\n        Dense(20,activation='relu'),\n         #Fourth Layer\n        Dense(24,activation='relu'),\n           #Fifth Layer\n        Dense(1,activation='sigmoid')  \n           ])\n    print(\"Yay\")\n    return X_train, X_test, y_train, y_test, model","beacc5aa":"\ndata.isnull().sum()\nsns.heatmap(data.isnull(), cbar = False).set_title(\"Missing values heatmap\")","c89e63c2":"data.Class.value_counts()","128561f1":"data=data.drop(['Time'],axis=1)","dfaa04ae":"data['normalized_amount']=StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n# Dropping the actual Amount column from the dataset.\ndata=data.drop(['Amount'],axis=1)","53b4e5c2":"X=data.iloc[:,data.columns!='Class']\ny=data.iloc[:,data.columns=='Class']\n\n#SPLIT AND MODEL CREATION. 0.3=FRACTION OF DATA AND 0 RANDOM STATE\nX_train, X_test, y_train, y_test,model=model_func(X,y,0.3,0)\nmodel.summary()","bfa8fb76":"#We choose an optimizer adam. \nmodel.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train, batch_size=15, epochs=5)  #REMEMBER TO CHANGE THE EPOCHS TO 4 OR 5 FOR ACCURACY TO RAISE!!!!","375ec1e4":"score=model.evaluate(X_test,y_test)\nprint(score)","f60058a2":"y_pred=model.predict(X_test)\ny_test=pd.DataFrame(y_test)\n\ncnf_matrix=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","ef14de2e":"print(roc_auc_score(y_test, y_pred))\n\nfpr, tpr, thresholds = roc_curve(y_test,y_pred)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()\n","e801e115":"y_pred_modelT=model.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred_modelT.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","7c642103":"print(roc_auc_score(y, y_pred_modelT))\n\nfpr, tpr, thresholds = roc_curve(y,y_pred_modelT)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()\n","ece77f8b":"dataNoClass = data.drop(columns = ['Class'])\ndataNoClass.corrwith(data.Class).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Class\", fontsize = 20,\n        rot = 45, grid = True)","1935fe60":"# To see the actual distribution of data \nsns.pairplot(data, hue = 'Class', vars = ['V2', 'V4', 'V11'] )","18d4dac2":"X_resample,y_resample=SMOTE().fit_sample(X,y.values.ravel())\n\n#DF\ny_resample=pd.DataFrame(y_resample)\nX_resample=pd.DataFrame(X_resample)","2be533bd":"X_train_Over, X_test_Over, y_train_Over, y_test_Over, modelOver =model_func(X_resample,y_resample,0.3,0)\nmodelOver.summary()","aa901bc5":"modelOver.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodelOver.fit(X_train_Over,y_train_Over, batch_size=15, epochs=5)","21745483":"y_pred_Over=modelOver.predict(X_test_Over)\ny_test_Over=pd.DataFrame(y_test_Over)\n\ncnf_matrix_Over=confusion_matrix(y_test_Over,y_pred_Over.round())\nplot_confusion_matrix(cnf_matrix_Over,classes=[0,1])\nplt.show()","46dada34":"print(roc_auc_score(y_test_Over, y_pred_Over))\nfpr, tpr, thresholds = roc_curve(y_test_Over,y_pred_Over)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()\n","39bc3cb3":"y_pred_OverT=modelOver.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred_OverT.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","3451b6c4":"print(roc_auc_score(y, y_pred_OverT))\nfpr, tpr, thresholds = roc_curve(y,y_pred_OverT)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()","1ed5a8bb":"fraud_indices=np.array(data[data.Class==1].index)\nno_records_fraud=len(fraud_indices)\nprint(no_records_fraud)\n\nnormal_indices=data[data.Class==0].index\n\nrandom_normal_indices=np.random.choice(normal_indices,no_records_fraud,replace=False)\nrandom_normal_indices=np.array(random_normal_indices)\nprint(len(random_normal_indices))\n\nunder_sample_indices=np.concatenate([fraud_indices,random_normal_indices])\nprint(len(under_sample_indices))","cd76495b":"under_sample_data=data.iloc[under_sample_indices,:]","30536bb4":"X_undersample=under_sample_data.iloc[:,under_sample_data.columns!='Class']\ny_undersample=under_sample_data.iloc[:,under_sample_data.columns=='Class']","e5a5aa93":"X_train_Under, X_test_Under, y_train_Under, y_test_Under, modelUnder =model_func(X_undersample,y_undersample,0.3,0)\nmodelUnder.summary()","e293b1fd":"\nmodelUnder.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodelUnder.fit(X_train_Under,y_train_Under, batch_size=15, epochs=150) #A lot of epochs due to little data\n\n#modelUnder.compile(optimizer='adagrad', loss='binary_crossentropy',metrics=['accuracy'])\n#modelUnder.fit(X_train_Under,y_train_Under, batch_size=15, epochs=100) #A lot of epochs due to little data","3fa950d1":"y_pred_Under=modelUnder.predict(X_test_Under)\ny_test_Under=pd.DataFrame(y_test_Under)\n\ncnf_matrix_Under=confusion_matrix(y_test_Under,y_pred_Under.round())\nplot_confusion_matrix(cnf_matrix_Under,classes=[0,1])\nplt.show()","aaa93684":"print(roc_auc_score(y_test_Under, y_pred_Under))\nfpr, tpr, thresholds = roc_curve(y_test_Under,y_pred_Under)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()","ad7d9f85":"y_pred_UnderT=modelUnder.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred_UnderT.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","05c97778":"print(roc_auc_score(y, y_pred_UnderT))\nfpr, tpr, thresholds = roc_curve(y,y_pred_UnderT)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.show()","c81ff691":"They are highly clustered so we can use SMOTE technique in order to increase the accuracy","44d8e97c":"For the entire dataset","16e68a81":"**Confusion matrix def**","f6054529":"For the entire dataset","b5cf8c23":"Prediction and ConfusionMatrix","fb0c5ef2":"None","46e23742":"COMPILER ADAM","356605c6":"# SMOTE","da10800c":"**NOW WE OBTAIN DATA, SPLIT IT AND CREATE A MODEL**","c2bdf6e3":"We want to see correlation in order to see which variables tend to be more related to the class","2b0823f9":"# UNDERSAMPLING","595b00f6":"For the entire dataset","136f5496":"**Split test and train**","c33d3d4f":"**OPTIMIZER ADAM**","9e4ff8c2":"**OTHER OPTIMIZER BECAUSE WE HAVE LITTLE DATA**\n\nWe chose adam to converge close to the minimum and adagrad to obtein the most suitable option, loss is low. Hope not to overfit. \nIve finally defidec to add more opochs as no statistical difference has been observed.","04469e08":"We see how V2, V4 and V11 are the most correlated one. We can se a graph in order to see how the spaces do. ","cb506fd1":"**Model and spliting data def**","c584d039":"We normalize the amount ","5568d364":"We drop time as we think it's insignificant","f99ca2cf":"We resample using smote function"}}