{"cell_type":{"5e2f622c":"code","31da68da":"code","7c3343e3":"code","c774e323":"code","7e6d705b":"code","df1e130d":"code","55788a4d":"code","08279922":"code","186d35a7":"code","621458dc":"code","1cc5eb69":"code","49291bbe":"markdown","5971aa9e":"markdown","aeea8ca6":"markdown","ef0a5b8c":"markdown","baef0ab1":"markdown","7c1feb6d":"markdown"},"source":{"5e2f622c":"!pip uninstall -y pyarrow\n!pip install -U datasets pyarrow transformers wandb","31da68da":"import gc\nimport os\nimport shutil\nimport warnings\n\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import trange\nfrom transformers import (\n    RobertaPreTrainedModel,\n    RobertaTokenizerFast,\n    RobertaModel,\n    Trainer, \n    TrainingArguments,\n    EarlyStoppingCallback,\n    set_seed\n)\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport yaml\n\nwarnings.filterwarnings(\"ignore\")","7c3343e3":"def create_df(path):\n    df = pd.read_csv(path)\n    if \"target\" in df.columns:\n        df = df.rename(columns={\"target\": \"labels\"})\n    return df\n\ndef create_dataset(df, tokenizer, tokenizer_kwargs=None):\n    source_col = \"excerpt\"\n    \n    def tokenize_function(example, tokenizer, tokenizer_kwargs=None):\n        return tokenizer(example[source_col], **tokenizer_kwargs)\n    \n    if \"labels\" in df.columns:\n        df = df[[source_col, \"labels\"]]\n    else:\n        df = df[[source_col]]\n        \n    dataset = datasets.Dataset.from_pandas(df)\n    if tokenizer_kwargs is None:\n        tokenizer_kwargs = {}\n    dataset = dataset.map(\n        tokenize_function,\n        batched=True, \n        remove_columns=[source_col], \n        fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_kwargs\": tokenizer_kwargs}\n    )\n    return dataset","c774e323":"class RobertaForSequenceRegression(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        config.hidden_dropout_prob = 0.0\n        config.layer_norm_eps = 1e-7\n        config.num_labels = 1\n        config.problem_type = \"regression\"\n        self.num_labels = config.num_labels\n        self.config = config\n        \n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.regressor = RobertaRegressionHead(config)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None\n    ):\n        assert self.config.problem_type == \"regression\" and self.num_labels == 1\n        \n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        last_hidden_states = outputs[0]\n        logits = self.regressor(last_hidden_states, attention_mask)\n        \n        loss = None\n        if labels is not None:\n            loss = torch.sqrt(F.mse_loss(logits.view(-1), labels.view(-1)))\n        \n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\nclass RobertaRegressionHead(nn.Module):\n    \"\"\"Head for sentence-level regression tasks.\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, last_hidden_states, attention_mask):\n        x = mean_pooling(last_hidden_states, attention_mask)\n        x = self.dense(x)\n        x = self.activation(x)\n        x = self.layer_norm(x)\n        x = self.out_proj(x)\n        return x    \n\n@torch.jit.script\ndef mean_pooling(last_hidden_state, attention_mask):\n    attention_mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n    sum_hidden_state = torch.sum(last_hidden_state * attention_mask, 1)\n    sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n    embeddings = sum_hidden_state \/ sum_mask\n    return embeddings","7e6d705b":"class ContinuousStratifiedKFold(StratifiedKFold):\n    def split(self, X, y, groups=None):\n        num_bins = int(np.floor(1 + np.log2(len(y))))\n        bins = pd.cut(y, bins=num_bins, labels=False)\n        return super().split(X, bins, groups)\n\ndef root_mean_squared_error(eval_pred):\n    logits, labels = eval_pred\n    logits = logits.squeeze()\n    return {\"rmse\": mean_squared_error(labels, logits, squared=False)}","df1e130d":"config_str = \"\"\"\npath:\n    train_path: \"..\/input\/commonlitreadabilityprize\/train.csv\"\n    test_path: \"..\/input\/commonlitreadabilityprize\/test.csv\"\n    sample_submission_path: \"..\/input\/commonlitreadabilityprize\/sample_submission.csv\"\n\nmodel_name: \"roberta-base\"\n\ntokenizer:\n    padding: \"do_not_pad\"\n    truncation: True\n    max_length: 256\n\ntrainer:\n    evaluation_strategy: \"steps\"\n    per_device_train_batch_size: 16\n    per_device_eval_batch_size: 16\n    learning_rate: 2.0e-5\n    weight_decay: 0.01\n    num_train_epochs: 5\n    lr_scheduler_type: \"linear\"\n    warmup_steps: 10\n    log_level: \"warning\"\n    logging_strategy: \"steps\"\n    logging_steps: 10\n    save_strategy: \"steps\"\n    save_steps: 10\n    save_total_limit: 1\n    fp16: False\n    eval_steps: 10\n    dataloader_num_workers: 2\n    load_best_model_at_end: True\n    metric_for_best_model: \"eval_loss\"\n    greater_is_better: False\n\nwandb:\n    api_key: \"your_api_key\"\n    project: \"Kaggle_CommonLit\"\n\nearly_stopping_patience: 30\n\nseed: 42\n\"\"\"\n\nconfig = yaml.safe_load(config_str)","55788a4d":"torch.cuda.empty_cache()\ngc.collect()\n\nos.environ[\"WANDB_API_KEY\"] = config[\"wandb\"][\"api_key\"]\nos.environ[\"WANDB_PROJECT\"] = config[\"wandb\"][\"project\"]\n\nresults = []\nskf = ContinuousStratifiedKFold(n_splits=5, shuffle=True, random_state=config[\"seed\"])\ntrain_valid_df = create_df(config[\"path\"][\"train_path\"])\nfor fold, (train_index, valid_index) in enumerate(skf.split(train_valid_df, train_valid_df[\"labels\"])):\n    print(f\"= fold {fold}\", \"=\"*80)\n    set_seed(config[\"seed\"]+fold)\n    \n    tokenizer = RobertaTokenizerFast.from_pretrained(config[\"model_name\"])\n    \n    train_df = train_valid_df.loc[train_index].reset_index(drop=True)\n    valid_df = train_valid_df.loc[valid_index].reset_index(drop=True)\n\n    train_dataset = create_dataset(train_df, tokenizer, tokenizer_kwargs=config[\"tokenizer\"])\n    valid_dataset = create_dataset(valid_df, tokenizer, tokenizer_kwargs=config[\"tokenizer\"])\n\n    model = RobertaForSequenceRegression.from_pretrained(config[\"model_name\"])\n    \n    early_stopping_callback = EarlyStoppingCallback(config[\"early_stopping_patience\"])\n    \n    temp_dir = \"temp\"\n    integration = 'wandb' if fold == 0 else 'none'\n    training_args = TrainingArguments(\n        output_dir=temp_dir,\n        seed=config[\"seed\"]+fold,\n        run_name=config[\"model_name\"],\n        report_to=integration,\n        **config[\"trainer\"]\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=root_mean_squared_error,\n        callbacks=[early_stopping_callback]\n    )\n    \n    trainer.train()\n    \n    result = trainer.evaluate(valid_dataset)\n    results.append(result)\n    \n    shutil.rmtree(temp_dir)\n    os.makedirs(f\"fold_{fold}\", exist_ok=True)\n    trainer.save_model(f\"fold_{fold}\")","08279922":"test_df = create_df(config[\"path\"][\"test_path\"])\nsample_submission_df = pd.read_csv(config[\"path\"][\"sample_submissioin_path\"])\n\n\nfor fold in trange(5):\n    model_path = f\"fold_{fold}\"\n    tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n    test_dataset = create_dataset(test_df, tokenizer, config[\"tokenizer\"])\n    model = RobertaForSequenceRegression.from_pretrained(model_path)\n    trainer = Trainer(model=model, tokenizer=tokenizer)\n    prediction_output = trainer.predict(test_dataset)\n    predictions.append(prediction_output.predictions)\npredictions = np.stack(predictions)","186d35a7":"final_prediction = np.mean(predictions, axis=0)\nsample_submission_df[\"target\"] = final_prediction\nsample_submission_df.to_csv(\"submission.csv\", index=False)","621458dc":"sample_submission_df","1cc5eb69":"!head .\/submission.csv","49291bbe":"# \u30e2\u30c7\u30eb","5971aa9e":"# \u5b66\u7fd2","aeea8ca6":"# \u63a8\u8ad6","ef0a5b8c":"# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210","baef0ab1":"# \u305d\u306e\u4ed6\u95a2\u6570","7c1feb6d":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff06\u30a4\u30f3\u30dd\u30fc\u30c8"}}