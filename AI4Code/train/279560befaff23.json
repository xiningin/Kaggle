{"cell_type":{"e39d99ed":"code","5c16fc89":"code","552e3742":"code","f964323b":"code","a03eb3e3":"code","0d9a66dc":"code","ca1da199":"code","f4e341fe":"code","7fc792f3":"code","6c6c7d6f":"code","5a130051":"code","c61c2491":"code","49cc8996":"code","d17c9616":"code","b65ad509":"code","a9d9c4b1":"code","99916b7d":"markdown","5ff3c975":"markdown","eaf1a579":"markdown","f9156cc9":"markdown","9b7f0ab7":"markdown","8c104076":"markdown"},"source":{"e39d99ed":"import pandas as pd\nimport os","5c16fc89":"df = pd.read_csv('..\/input\/BCH-USD.csv')\ndf.head()","552e3742":"main_df = pd.DataFrame()\nratios = ['BTC-USD', 'LTC-USD', 'ETH-USD', 'BCH-USD']\n\nfor ratio in ratios:\n    # note \"f\" is the new replacement for .format\n    dataset = f'..\/input\/{ratio}.csv' # set dataset path\n    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n    df.rename(columns={\"close\":f\"{ratio}_close\", \"volume\":f\"{ratio}_volume\"}, inplace=True)\n    \n    # set time as index column\n    df.set_index(\"time\", inplace=True)\n    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n    \n    # merge all df's \n    if len(main_df) == 0:\n        main_df = df\n    else:\n        main_df = main_df.join(df)\n\nprint(main_df.head())","f964323b":"for c in main_df.columns:\n    print(c)","a03eb3e3":"# define parameters\n\nSEQ_LEN = 60 # number of min. data we use to predict\nFUTURE_PERIOD_PREDICT = 3 # future period to predict\nRATION_TO_PREDICT = \"ETH-USD\" # currency we will predict preice for\n","0d9a66dc":"def classify(current, future):\n    if float(future) > float(current):\n        return 1 # price will rise\n    else:\n        return 0  # price will go down","ca1da199":"main_df['future'] = main_df[f\"{RATION_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\nprint(main_df[[f\"{RATION_TO_PREDICT}_close\", \"future\"]].head())","f4e341fe":"# create trarget column\n \nmain_df['target'] = list(map(classify, main_df[f\"{RATION_TO_PREDICT}_close\"], main_df['future']))\nprint(main_df[[f\"{RATION_TO_PREDICT}_close\", \"future\", 'target']].head())","7fc792f3":"times = sorted(main_df.index.values)\nlast_5pct = times[-int(0.05 * len(times))]\nprint(last_5pct)","6c6c7d6f":"validation_main_df = main_df[(main_df.index >= last_5pct)]\nmain_df = main_df[(main_df.index < last_5pct)]\n","5a130051":"# this fun. for scaling , normaize and balance data\nfrom sklearn import preprocessing\nfrom collections import deque\nimport numpy as np\nimport random\nimport time\ndef preprocess_df(df):\n    df = df.drop('future', 1) # this is only for testing and generate target\n    \n    for col in df.columns:\n        if col != 'target':\n            # pct_change compute precent of change between prev. and immediate value\n            df[col] = df[col].pct_change()\n            df.dropna(inplace=True)\n            df[col] = preprocessing.scale(df[col].values) # scaling values 0-1\n    df.dropna(inplace=True)\n    \n    \n    # this part create a stack or queue of elements and in each iter. remove\n    # one time series and add another one in the end, append everystck when len be 60\n    sequencial_data = []\n    prev_days = deque(maxlen=SEQ_LEN) # make queue or stack of data\n    \n    for i in df.values:\n        prev_days.append([n for n in i[:-1]])\n        if len(prev_days) == SEQ_LEN :\n            sequencial_data.append([np.array(prev_days) , i[-1]])\n    \n    random.shuffle(sequencial_data)\n    \n    buys = []\n    sells = []\n    \n    for seq, target in sequencial_data:\n        if target == 0:\n            sells.append([seq, target])\n        elif target == 1:\n            buys.append([seq, target])\n        \n    random.shuffle(buys)\n    random.shuffle(sells)\n    \n    lower = min(len(buys), len(sells))\n    \n    buys = buys[:lower]\n    sells = sells[:lower]\n    # print(f\"sq data len:{len(sequencial_data)}, balanced len: {len(buys) + len(sells)}\")\n    sequencial_data = buys + sells\n    \n    random.shuffle(sequencial_data)\n    \n    # split data into x, y\n    x = []\n    y = []\n    \n    for seq, target in sequencial_data:\n        x.append(seq)\n        y.append(target)\n        \n    return np.array(x), y","c61c2491":"train_x , train_y = preprocess_df(main_df)\nvalidation_x, validation_y = preprocess_df(validation_main_df)","49cc8996":"print(f\"training data: {len(train_x)} validation: {len(validation_x)}\")\nprint(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\nprint(f\"validation dont buys: {validation_y.count(0)} buys: {validation_y.count(1)}\")","d17c9616":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n\n# define model parameter\nEPOCHS = 10\nBATCH_SIZE = 32\nNAME = f\"{RATION_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"","b65ad509":"model = Sequential()\n# add layers to model\n# if you using cpu version use LSTM not CuDNNLSTM\nmodel.add(CuDNNLSTM(64, input_shape=(train_x.shape[1:]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(CuDNNLSTM(64, input_shape=(train_x.shape[1:]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(CuDNNLSTM(64, input_shape=(train_x.shape[1:])))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(6,activation=\"relu\"))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(2, activation=\"softmax\"))\n\nopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=opt,\n             metrics=['accuracy'])\n\n# tensorbord = TensorBoard(log_dir=f'log\/{NAME}')\n\n# filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\" # uniqe file name that will incluse the epoch and the validation acc for that ecpoch\n# checkpoint = ModelCheckpoint(\"..\/input\/{}.model\".format(filepath, monitor='val_acc', verbos=1, save_best_only=True, mod='max'))\n\nhistory = model.fit(\n        train_x, train_y,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=(validation_x, validation_y))","a9d9c4b1":"# model.save(\"\")","99916b7d":"# load and prepare datasets","5ff3c975":" here we need to add columns names and merge all data frames in one df \n","eaf1a579":"### sperate data into training and validating datasets","f9156cc9":"### Training and Predictions","9b7f0ab7":"### Using 6 minutes cryptocurrancy pricing data to predict the price of the next 6 Minutes","8c104076":"we can't shuffle data and split it, because that is not work in sequencial data."}}