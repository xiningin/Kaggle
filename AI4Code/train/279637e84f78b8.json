{"cell_type":{"d631ee40":"code","0d7a060e":"code","199925a5":"code","6dd1b81c":"code","f0313b93":"code","005f1ae6":"code","2da975fc":"code","45c05a76":"code","c3ac3a5a":"code","30b6085f":"code","026c8a61":"code","373e6d1b":"code","891aedfd":"code","543ffa20":"code","06365274":"code","cf7ff939":"code","463d6959":"code","b6eecc7f":"code","b8c40354":"code","89c0e69b":"code","c454ce87":"code","031743bb":"code","7fabfd50":"code","4ac633b7":"code","b9ff39fa":"code","ce950ad1":"code","82f08847":"code","39f4e5c4":"code","c91835ab":"code","802aa569":"code","d84e734e":"markdown","92d2d1e2":"markdown","ef9e69dc":"markdown","7e858d5c":"markdown"},"source":{"d631ee40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport string\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d7a060e":"!pip install bert-for-tf2\n!pip install sentencepiece","199925a5":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","6dd1b81c":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","f0313b93":"keywords = train[\"keyword\"].value_counts()\nplt.grid()\nsns.barplot(keywords.index, keywords)\nplt.title(\"Keywords\")\nprint(keywords)","005f1ae6":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.title(\"Real or Not\")\nprint(x)","2da975fc":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","45c05a76":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","c3ac3a5a":"import nltk\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')","30b6085f":"print(stop_words)","026c8a61":"import random\nfrom random import shuffle\nrandom.seed(1)\n\n# import these modules \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize, word_tokenize\n#cleaning up text\nimport re\ndef Preprocess_text(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"\u2019\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    \n    #Removing stop words and convert words to base forms\n    clean_line=LemmaSentence(clean_line)\n    return clean_line\n\ndef LemmaSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    New_sentence=[]\n    updated_word_list = list(set([word for word in token_words if word not in stop_words]))\n    for word in token_words:\n        lemmatizer = WordNetLemmatizer()\n        New_sentence.append(lemmatizer.lemmatize(word))\n        New_sentence.append(\" \")\n        \n    return \"\".join(New_sentence)","373e6d1b":"train['text'][0]","891aedfd":"Preprocess_text(train['text'][0])","543ffa20":"train['text']=train.text.apply(lambda x:Preprocess_text(x))","06365274":"test['text']=test.text.apply(lambda x:Preprocess_text(x))","cf7ff939":"import re\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_url(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_html(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuation(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_url(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuation(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_html(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))","463d6959":"x=train[\"text\"]\ny= np.array(list(train[\"target\"]))","b6eecc7f":"def bert_encode(text, tokenizer):\n    \n  num_examples = len(text)\n  \n  sentence = tf.ragged.constant([encode_sentence(s) for s in np.array(text)])\n  \n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n  input_word_ids = tf.concat([cls, sentence], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s = tf.zeros_like(sentence)\n  input_type_ids = tf.concat(\n      [type_cls, type_s], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","b8c40354":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","89c0e69b":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","c454ce87":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntrain_input = bert_encode(x, tokenizer)","031743bb":"max_len = 30\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","7fabfd50":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","4ac633b7":"strategy","b9ff39fa":"from tensorflow.keras.callbacks import EarlyStopping\n\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    early_stopping=EarlyStopping(monitor='val_accuracy',mode='max',patience=5,min_delta=0.01)\n    model.fit(train_input, y, epochs = 10, verbose = 1, batch_size = 128, validation_split = 0.2,callbacks=[early_stopping])","ce950ad1":"test_input=bert_encode(test[\"text\"], tokenizer)","82f08847":"predictions = [np.argmax(i) for i in model.predict(test_input)]","39f4e5c4":"submission = test.id.copy().to_frame()\nsubmission['target'] = predictions","c91835ab":"submission.head()","802aa569":"submission.to_csv('submission.csv', header=True, index=False) ","d84e734e":"The dataset has almost balanced target variable ","92d2d1e2":"Checking an Example text","ef9e69dc":"Checking the text post preprocessing","7e858d5c":"<h1>Data Preprocessing<\/h1>"}}