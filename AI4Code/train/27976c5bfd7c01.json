{"cell_type":{"324dd2fc":"code","81883f35":"code","7c97be5f":"code","b58f9f12":"code","d0f1c8e3":"code","675f49c9":"code","b666b8c6":"code","1be7bfd8":"code","2f55aac6":"code","5d95450d":"code","65e92058":"code","570c2192":"code","38322b5a":"code","f933818c":"code","a9d40a5a":"code","c3d3ee54":"code","e9eb59c2":"code","041bb4ea":"markdown","b8a63967":"markdown","bb9ff453":"markdown","967e8771":"markdown","5b608995":"markdown","f9ad3480":"markdown","07da085a":"markdown","9051b636":"markdown","9b315088":"markdown","0eead14d":"markdown","3976903b":"markdown","1bef5dc9":"markdown","f07b64cd":"markdown","25a4da87":"markdown"},"source":{"324dd2fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nimport nltk\n\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import tree\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.models import Sequential\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","81883f35":"data = pd.read_csv('\/kaggle\/input\/stocknews\/Combined_News_DJIA.csv')\ndata.head()","7c97be5f":"data['headlines'] = data[data.columns[2:]].apply(lambda x: '. '.join(x.dropna().astype(str)),axis=1)","b58f9f12":"data['comment_length'] = data['headlines'].apply(lambda x : len(x))\ndata['comment_length'].hist()","d0f1c8e3":"data['Label'].hist()","675f49c9":"train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']","b666b8c6":"nltk.download('stopwords', quiet=True, raise_on_error=True)\nstop_words_en = set(nltk.corpus.stopwords.words('english'))\nstop_words_en.add(\"b\")\n\nclass CustomTokenizer:\n    \n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n        self.tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n        \n    def _lem(self, token):\n        if (token in stop_words_en):\n            return token  # Solves error \"UserWarning: Your stop_words may be inconsistent with your preprocessing.\"\n        return self.wnl.lemmatize(token)\n    \n    def __call__(self, doc):\n        return [self._lem(t) for t in self.tokenizer.tokenize(doc)]","1be7bfd8":"vectorizer = CountVectorizer(tokenizer=CustomTokenizer(), stop_words=stop_words_en, lowercase=True, min_df=0.0075,  max_df=0.05, ngram_range=(2,2))\n\nfeatures_train = vectorizer.fit_transform(train['headlines'].tolist())\nfeatures_test = vectorizer.transform(test['headlines'].tolist())","2f55aac6":"feature_names = vectorizer.get_feature_names()\nprint(feature_names[50:100])\n\nX_train = pd.DataFrame(features_train.todense(), columns = feature_names)\nX_test = pd.DataFrame(features_test.todense(), columns = feature_names)\n\nX_train.head()","5d95450d":"from collections import defaultdict\n\nup_unigrams = defaultdict(int)\ndown_unigrams = defaultdict(int)\n\nfor word in feature_names:\n    up_unigrams[word] += np.sum(X_train[train['Label']==1][word])\n    down_unigrams[word] += np.sum(X_train[train['Label']==0][word])\n        \ndf_up_unigrams = pd.DataFrame(sorted(up_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_down_unigrams = pd.DataFrame(sorted(down_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_up_unigrams.head()","65e92058":"import seaborn as sns\n\nN=25\n\nfig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_up_unigrams[0].values[:N], x=df_up_unigrams[1].values[:N], ax=axes[0], color='green')\nsns.barplot(y=df_down_unigrams[0].values[:N], x=df_down_unigrams[1].values[:N], ax=axes[1], color='red')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in headlines resulting in stock up', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in headlines resulting in stock down', fontsize=15)\n\nplt.show()","570c2192":"clf = SVC()\nclf = clf.fit(X_train, train[\"Label\"].tolist())\nprint('Accuracy X_train: ' + str(clf.score(X_train, train[\"Label\"].tolist())))\n\npredictions = clf.predict(X_test)\n\npd.crosstab(test[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])","38322b5a":"print (classification_report(test[\"Label\"], predictions))\nprint ('Accuracy X_test: ' + str(accuracy_score(test[\"Label\"], predictions)))","f933818c":"from keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import Adadelta,Adam,RMSprop\nfrom keras.utils import np_utils\nfrom keras import Sequential, optimizers, regularizers\n\nscale = np.max(X_train)\nX_train \/= scale\nX_test \/= scale\n\nmean = np.mean(X_train)\nX_train -= mean\nX_test -= mean\n\nnum_features = X_train.shape[1]\n\nmodel = Sequential()\nmodel.add(Dense(8,input_shape=(num_features,), activation='relu', kernel_regularizer = regularizers.l2(0.1)))\nmodel.add(Dropout(0.5))\n#model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n#model.add(Dropout(0.5))\n#model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.0001)))\n#model.add(Dropout(0.5))\n#model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.1)))\n#model.add(Dropout(0.5))\n#model.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n#model.add(Dropout(0.5))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nY_train = np_utils.to_categorical(train[\"Label\"], 2)\nY_test = np_utils.to_categorical(test[\"Label\"], 2)\n\nhistory = model.fit(X_train, Y_train, batch_size=32, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n    \nscore = model.evaluate(X_test, Y_test)\nprint(score)","a9d40a5a":"class MyTokenizer():\n    def __init__(self):\n        self.tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n        self.stop_words_en = set(nltk.corpus.stopwords.words('english'))\n        self.stop_words_germ = set(nltk.corpus.stopwords.words('german'))\n        self.stop_words = set()\n        self.stop_words.add(\"b\")\n        \n    def tokenize(self, headlines):\n        # Tokenize\n        tokens = [self.tokenizer.tokenize(article) for article in headlines]\n\n        # Lemmatizer\n        clean_tokens = []\n        for words in tokens:\n            clean_tokens.append([self.lemmatizer.lemmatize(word) for word in words])\n\n        # Stop words\n        final_tokens = []\n        for words in clean_tokens:\n            final_tokens.append([word.lower() for word in words if word.lower() not in self.stop_words_en and word.lower() not in self.stop_words_germ and word.lower() not in self.stop_words])\n            \n        return final_tokens","c3d3ee54":"from gensim.models import Word2Vec\n\ntokenizer = MyTokenizer()\n\nheadlines_train = train[\"headlines\"]\nheadlines_test= test[\"headlines\"]\n\ntokens_train = tokenizer.tokenize(headlines_train)\ntokens_test = tokenizer.tokenize(headlines_test)\n\nmodel = Word2Vec(tokens_train, min_count=1,size= 50,workers=3, window =3, sg = 1)\n\nword_vectors = model.wv\nprint(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n\nprint(model.wv.most_similar('husband'))","e9eb59c2":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom keras.layers import Flatten, Dense, LSTM, GRU, SpatialDropout1D, Bidirectional, concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import SGD\n\nmy_tokenizer = MyTokenizer()\n\nheadlines_train = train[\"headlines\"]\nheadlines_test= test[\"headlines\"]\ntokens_train = my_tokenizer.tokenize(headlines_train)\ntokens_test = my_tokenizer.tokenize(headlines_test)\n\ntokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(train[\"headlines\"])\n        \nvocab_size = 20000 #len(tokenizer.word_index) + 1\nprint('Number of words: ' + str(vocab_size))\n\nX_train_tokens = tokenizer.texts_to_sequences(tokens_train)\nX_test_tokens = tokenizer.texts_to_sequences(tokens_test)\n\nmax_length = 0\nfor words in X_train_tokens:\n    if len(words)>max_length:\n        max_length = len(words)\nmax_length = 200\nprint('max_length: ' + str(max_length))\n\nX_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')\n\ny_train = train['Label']\ny_test = test['Label']\nY_train = np_utils.to_categorical(y_train, 2)\nY_test = np_utils.to_categorical(y_test, 2)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 16, input_length=max_length))\n#model.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2)) \n#model.add(Bidirectional(LSTM(units=64, recurrent_dropout=0.5)))\n#model.add(Flatten())\n#model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu', kernel_regularizer = regularizers.l2(0.01)))\n#model.add(Dropout(0.5))\n#model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n#model.add(Dropout(0.5))\n#model.add(GRU(units=32, dropout=0.5, recurrent_dropout=0.5))\n#model.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nprint(model.summary())\n\nhistory = model.fit(X_train_pad, Y_train, batch_size=32, epochs=50, verbose=0, validation_data=(X_test_pad, Y_test))\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nloss, accuracy = model.evaluate(X_test_pad,Y_test)\nprint('Testing Accuracy is {} '.format(accuracy*100))","041bb4ea":"# Stock market prediction from daily news\n","b8a63967":"# First attempt with Tf-idf and simple Classifier\n\nStarting from the daily headlines, the sentences were tokenized into words by using the RegexpTokenizer of nltk. Lemmatisation is also applied. The following CustomTokenizer takes over this task.","bb9ff453":"Split the data into train and test set. As proposed, the data until 31.12.2014 is used as training set and the following two years as test set. This is roughly a 80%\/20% split.","967e8771":"Using the CustomTokenizer, the TfidfVectorizer convert the collection of raw headlines to a matrix of TF-IDF features.\n- English stopwords are removed, all words are converted to lowercase\n- With ngram_range=(2,2) only word-tuples are considered\n- min_df and max_df ignore terms that have a document frequency strictly lower\/higher than the given threshold (in percent)\n\nFor fitting the vectorizer, only the training set is used. ","5b608995":"Use SVC Classifier to predict testdata","f9ad3480":"# Inspect and prepairing Dataset\n\nLoad the dataset and have a first impression of the data.\n\nThe file consists of the Top25 news headline per day and a label with only two values: \n- \"1\" when DJIA Adj Close value rose or stayed the same\n- \"0\" when DJIA Adj Close value decreased","07da085a":"Inspect the feature names and the TF-IDF features","9051b636":"# Keras Sequential model","9b315088":"First, all headlines are merged into one column","0eead14d":"# Use Word2Vec\n\nUse Word2Vec to find most similar words","3976903b":"# Word embedding","1bef5dc9":"The accuracy is not so good. Only slightly better than guessing.","f07b64cd":"Histogram of the length of the headlines","25a4da87":"Distribution of labels"}}