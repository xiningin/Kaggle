{"cell_type":{"a2491ee3":"code","63060c4a":"code","ad65efd8":"code","d3420c1a":"code","8900b3e2":"code","b0d9e011":"code","f639ac6e":"code","4c1f9492":"code","23504c06":"code","203c8299":"code","c91da722":"code","497d9f43":"code","1b8d36f5":"code","a03e8e15":"code","529d170f":"code","1ed6843f":"code","693a2471":"code","d9579700":"code","38f13214":"code","3ff81a42":"code","7e374e9f":"code","4265c632":"markdown","0f775987":"markdown","55e5e76a":"markdown","4ea5a073":"markdown","dfb0a40a":"markdown"},"source":{"a2491ee3":"import numpy as np\nimport os\nfrom torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A","63060c4a":"import torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm","ad65efd8":"class LyftUdacity(Dataset):\n    def __init__(self,img_dir,transform = None):\n        self.transforms = transform\n        image_paths = [i+'\/CameraRGB' for i in img_dir]\n        seg_paths = [i+'\/CameraSeg' for i in img_dir]\n        self.images,self.masks = [],[]\n        for i in image_paths:\n            imgs = os.listdir(i)\n            self.images.extend([i+'\/'+img for img in imgs])\n        for i in seg_paths:\n            masks = os.listdir(i)\n            self.masks.extend([i+'\/'+mask for mask in masks])\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self,index):\n        img = np.array(Image.open(self.images[index]))\n        mask = np.array(Image.open(self.masks[index]))\n        if self.transforms is not None:\n            aug = self.transforms(image=img,mask=mask)\n            img = aug['image']\n            mask = aug['mask']\n            mask = torch.max(mask,dim=2)[0]\n        return img,mask","d3420c1a":"data_dir = ['..\/input\/lyft-udacity-challenge\/data'+i+'\/data'+i for i in ['A','B','C','D','E']]","8900b3e2":"def get_images(image_dir,transform = None,batch_size=1,shuffle=True,pin_memory=True):\n    data = LyftUdacity(image_dir,transform = t1)\n    train_size = int(0.8 * data.__len__())\n    test_size = data.__len__() - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n    train_batch = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n    test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n    return train_batch,test_batch\n","b0d9e011":"t1 = A.Compose([\n    A.Resize(160,240),\n    A.augmentations.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])","f639ac6e":"train_batch,test_batch = get_images(data_dir,transform =t1,batch_size=8)","4c1f9492":"for img,mask in train_batch:\n    img1 = np.transpose(img[0,:,:,:],(1,2,0))\n    mask1 = np.array(mask[0,:,:])\n    img2 = np.transpose(img[1,:,:,:],(1,2,0))\n    mask2 = np.array(mask[1,:,:])\n    img3 = np.transpose(img[2,:,:,:],(1,2,0))\n    mask3 = np.array(mask[2,:,:])\n    fig , ax =  plt.subplots(3, 2, figsize=(18, 18))\n    ax[0][0].imshow(img1)\n    ax[0][1].imshow(mask1)\n    ax[1][0].imshow(img2)\n    ax[1][1].imshow(mask2)\n    ax[2][0].imshow(img3)\n    ax[2][1].imshow(mask3)\n    break","23504c06":"pip install torchsummary ","203c8299":"class encoding_block(nn.Module):\n    def __init__(self,in_channels, out_channels):\n        super(encoding_block,self).__init__()\n        model = []\n        model.append(nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False))\n        model.append(nn.BatchNorm2d(out_channels))\n        model.append(nn.ReLU(inplace=True))\n        model.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False))\n        model.append(nn.BatchNorm2d(out_channels))\n        model.append(nn.ReLU(inplace=True))\n        self.conv = nn.Sequential(*model)\n    def forward(self, x):\n        return self.conv(x)    ","c91da722":"class unet_model(nn.Module):\n    def __init__(self,out_channels=23,features=[64, 128, 256, 512]):\n        super(unet_model,self).__init__()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n        self.conv1 = encoding_block(3,features[0])\n        self.conv2 = encoding_block(features[0],features[1])\n        self.conv3 = encoding_block(features[1],features[2])\n        self.conv4 = encoding_block(features[2],features[3])\n        self.conv5 = encoding_block(features[3]*2,features[3])\n        self.conv6 = encoding_block(features[3],features[2])\n        self.conv7 = encoding_block(features[2],features[1])\n        self.conv8 = encoding_block(features[1],features[0])        \n        self.tconv1 = nn.ConvTranspose2d(features[-1]*2, features[-1], kernel_size=2, stride=2)\n        self.tconv2 = nn.ConvTranspose2d(features[-1], features[-2], kernel_size=2, stride=2)\n        self.tconv3 = nn.ConvTranspose2d(features[-2], features[-3], kernel_size=2, stride=2)\n        self.tconv4 = nn.ConvTranspose2d(features[-3], features[-4], kernel_size=2, stride=2)        \n        self.bottleneck = encoding_block(features[3],features[3]*2)\n        self.final_layer = nn.Conv2d(features[0],out_channels,kernel_size=1)\n    def forward(self,x):\n        skip_connections = []\n        x = self.conv1(x)\n        skip_connections.append(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        skip_connections.append(x)\n        x = self.pool(x)\n        x = self.conv3(x)\n        skip_connections.append(x)\n        x = self.pool(x)\n        x = self.conv4(x)\n        skip_connections.append(x)\n        x = self.pool(x)\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n        x = self.tconv1(x)\n        x = torch.cat((skip_connections[0], x), dim=1)\n        x = self.conv5(x)\n        x = self.tconv2(x)\n        x = torch.cat((skip_connections[1], x), dim=1)\n        x = self.conv6(x)\n        x = self.tconv3(x)\n        x = torch.cat((skip_connections[2], x), dim=1)\n        x = self.conv7(x)        \n        x = self.tconv4(x)\n        x = torch.cat((skip_connections[3], x), dim=1)\n        x = self.conv8(x)\n        x = self.final_layer(x)\n        return x","497d9f43":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","1b8d36f5":"model = unet_model().to(DEVICE)","a03e8e15":"from torchsummary import summary\nsummary(model, (3, 256, 256))","529d170f":"LEARNING_RATE = 1e-4\nnum_epochs = 10","1ed6843f":"loss_fn = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.cuda.amp.GradScaler()","693a2471":"for epoch in range(num_epochs):\n    loop = tqdm(enumerate(train_batch),total=len(train_batch))\n    for batch_idx, (data, targets) in loop:\n        data = data.to(DEVICE)\n        targets = targets.to(DEVICE)\n        targets = targets.type(torch.long)\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())","d9579700":"def check_accuracy(loader, model):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(DEVICE)\n            y = y.to(DEVICE)\n            softmax = nn.Softmax(dim=1)\n            preds = torch.argmax(softmax(model(x)),axis=1)\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) \/ ((preds + y).sum() + 1e-8)\n\n    print(f\"Got {num_correct}\/{num_pixels} with acc {num_correct\/num_pixels*100:.2f}\")\n    print(f\"Dice score: {dice_score\/len(loader)}\")\n    model.train()","38f13214":"check_accuracy(train_batch, model)","3ff81a42":"check_accuracy(test_batch, model)","7e374e9f":"for x,y in test_batch:\n    x = x.to(DEVICE)\n    fig , ax =  plt.subplots(3, 3, figsize=(18, 18))\n    softmax = nn.Softmax(dim=1)\n    preds = torch.argmax(softmax(model(x)),axis=1).to('cpu')\n    img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n    preds1 = np.array(preds[0,:,:])\n    mask1 = np.array(y[0,:,:])\n    img2 = np.transpose(np.array(x[1,:,:,:].to('cpu')),(1,2,0))\n    preds2 = np.array(preds[1,:,:])\n    mask2 = np.array(y[1,:,:])\n    img3 = np.transpose(np.array(x[2,:,:,:].to('cpu')),(1,2,0))\n    preds3 = np.array(preds[2,:,:])\n    mask3 = np.array(y[2,:,:])\n    ax[0,0].set_title('Image')\n    ax[0,1].set_title('Prediction')\n    ax[0,2].set_title('Mask')\n    ax[1,0].set_title('Image')\n    ax[1,1].set_title('Prediction')\n    ax[1,2].set_title('Mask')\n    ax[2,0].set_title('Image')\n    ax[2,1].set_title('Prediction')\n    ax[2,2].set_title('Mask')\n    ax[0][0].axis(\"off\")\n    ax[1][0].axis(\"off\")\n    ax[2][0].axis(\"off\")\n    ax[0][1].axis(\"off\")\n    ax[1][1].axis(\"off\")\n    ax[2][1].axis(\"off\")\n    ax[0][2].axis(\"off\")\n    ax[1][2].axis(\"off\")\n    ax[2][2].axis(\"off\")\n    ax[0][0].imshow(img1)\n    ax[0][1].imshow(preds1)\n    ax[0][2].imshow(mask1)\n    ax[1][0].imshow(img2)\n    ax[1][1].imshow(preds2)\n    ax[1][2].imshow(mask2)\n    ax[2][0].imshow(img3)\n    ax[2][1].imshow(preds3)\n    ax[2][2].imshow(mask3)   \n    break","4265c632":"# Architecture","0f775987":"# Training","55e5e76a":"# Dataset","4ea5a073":"## Transforms","dfb0a40a":"# Metrics"}}