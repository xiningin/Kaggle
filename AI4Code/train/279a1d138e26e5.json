{"cell_type":{"1aea68c8":"code","d19b373f":"code","ef2bc372":"code","c1d0c6a7":"code","8764f068":"code","ab721be3":"code","7009abdd":"code","8a8d65ed":"code","1060957f":"code","3444a4ac":"code","eb1b7106":"code","5b1b7ea8":"code","cde9b812":"code","f43d2fe2":"code","f89790a2":"markdown","4ee15342":"markdown","1f6de53b":"markdown","24e2d2e2":"markdown","07818b85":"markdown","63b623ae":"markdown","6b50ac1f":"markdown","be00627c":"markdown","5d376edf":"markdown"},"source":{"1aea68c8":"import numpy as np \nimport pandas as pd \nimport os\nfrom glob import glob\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm import tqdm\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.tree import *\nfrom sklearn.ensemble import *","d19b373f":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest  = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\nsample_sub = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')\ndisplay(train)\ndisplay(test)\ndisplay(sample_sub)","ef2bc372":"print('Different Unique Stocks IDS in training ',len(glob('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')),\n      'Different Unique Stocks IDS in test ',len(glob('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*'))\n     )\nprint('Uniques in \\n',train.nunique())\npx.histogram(train.time_id)","c1d0c6a7":"def file_path_to_volatility(path,info=False):\n    \"\"\"We are losing much important when we try to find past record data as we only get 3 column stock_id time_id vol \"\"\"\n    part_data = []\n    stock_id = path.split('\/')[4].split('=')[1]\n    sample_book = pd.read_parquet(path)\n    sample_book['wap'] = (sample_book['bid_price1'] * sample_book['ask_size1'] + sample_book['ask_price1'] * sample_book['bid_size1']) \/ (sample_book['bid_size1']+ sample_book['ask_size1'])\n    sample_book.dropna(inplace=True)\n    for gid0,gid in tqdm(sample_book.groupby('time_id')):\n        gid['log_return'] = gid['wap'].apply(lambda x:np.log(x)).diff()\n        if info :\n            print(f'Realized Volatiliy for time id {gid.time_id.iloc[0]} is ' ,np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2))))\n        info_1 = np.array([stock_id,gid.time_id.iloc[0],np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2)))])\n        info_2 = group_to_feature(gid)\n#         print(describe_info.shape)\n        info_1 = np.append(info_1,info_2) \n        part_data.append(info_1)\n    return part_data\nbook_dir = glob('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*\/*')\n# PRE LOADING THIS FILE (ALREADY RAN AND SAVED IN INPUTS)\n# train_data_vol = []\n# for i in tqdm(book_dir):\n#     train_data_vol.append(file_path_to_volatility(i))\n# col_1 = ['stock_id','time_id','vol']\n# col_2 = [f\"desc_{i}\" for i in range(63)]\n# past_data = pd.concat([pd.DataFrame(i,columns=col_1+col_2) for i in train_data_vol])\n# past_data.stock_id = past_data.stock_id.astype('int64')\npast_data = pd.read_csv('..\/input\/starter\/train_data.csv')\nprint(past_data.dtypes)\nprint(train.dtypes)\ndisplay(past_data)","8764f068":"def group_to_feature(df):\n    \"\"\"Capture Features \"\"\"\n    feature_sum = ['seconds_in_bucket','bid_price1', 'ask_price1','bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2','ask_size2']\n    index = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n    val1 = df[feature_sum].describe().loc[index].values.reshape(-1)\n#     val2 = np.sum(df[feature_sum]).values\n    return val1","ab721be3":"# METRICS\ndef rmspe(y_pred,y_true):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n# print('root mean square percentage error'.upper(),rmspe(train_pred['target'],train_pred['vol']),\n#      'root mean square error'.upper(),mean_squared_error(train_pred['target'],train_pred['vol']))\nprint(train.shape,past_data.shape)\ndisplay(pd.merge(train,past_data,on=['stock_id','time_id']))","7009abdd":"def path_to_data(path):\n    \"\"\" This return a merged dataframe of trades where the trades actually took place \"\"\"\n#     print(path)\n    stock_id = path.split('\/')[-1].split('=')[1]\n    curr_book = pd.read_parquet(path)\n    curr_trade = pd.read_parquet(path.replace('book','trade'))\n    merged_data = pd.merge(curr_book,curr_trade,on=['time_id','seconds_in_bucket'])\n    merged_data['stock_id'] = stock_id\n#     print(curr_book.shape,curr_trade.shape,len(merged_data))\n    if len(merged_data) ==0 :\n        merged_data = curr_trade.merge(curr_book, how='cross',suffixes=['','_y'])\n        merged_data['diff'] = abs(merged_data.seconds_in_bucket-merged_data.seconds_in_bucket_y)\n        merged_data = pd.merge(merged_data.groupby(['time_id','seconds_in_bucket'])['diff'].min().reset_index(),merged_data,how=\"left\")\n        merged_data.drop(columns=['time_id_y','seconds_in_bucket_y','diff'],inplace=True)\n        merged_data['stock_id'] = stock_id\n    merged_data.dropna(inplace=True)\n    merged_data.reset_index(drop=True)\n    return merged_data\n\ndef read_all_files(path):\n    \"\"\" Reads All file in the sub Folder (path \/ *) and read all parquets (trade\/book) and picks only the first occurence based on Stock + Time\n        Returns a list of all dataframs use concat to join them back .\"\"\"\n    demo_all = []\n    for i in tqdm(glob(os.path.join(path,'*'))):\n        demo_merged = path_to_data(i)\n        demo = demo_merged.groupby(['stock_id','time_id']).first().reset_index()\n        demo.stock_id = demo.stock_id.astype('int64')\n        demo_all.append(demo)\n    return demo_all\n\ndef files_to_numbers(demo_all,vol_calculated,csv_path = '..\/input\/optiver-realized-volatility-prediction\/train.csv'):\n    \"\"\" Takes in a List of DataFrame and Merges them with a CSV File and then with preprocessed data that we have where we calculate the Volatility\n        at end of 10 min or bucket mark \"\"\"\n    csv_file = pd.read_csv(csv_path)\n    demo = pd.concat(demo_all).reset_index(drop=True)\n    demo_vol = pd.merge(csv_file,demo,on=['stock_id','time_id'])\n    demo_vol_all_data = pd.merge(demo_vol,vol_calculated)\n    return demo_vol_all_data\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","8a8d65ed":"demo_all = read_all_files('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet')\ndata = files_to_numbers(demo_all,past_data,'..\/input\/optiver-realized-volatility-prediction\/train.csv')\ndisplay(data)\n# print(data.stock_id.value_counts())\n# print(data.time_id.nunique())","1060957f":"feature = list(data.columns)\nfeature.remove('stock_id')\nfeature.remove('time_id')\nfeature.remove('target')\ndata_df = data.copy()\ndata_df.drop(columns=['stock_id','time_id'],inplace=True)\ntrain_data,train_label = data_df[feature],data_df['target']\nstd_scaler = StandardScaler()\ntrain_data_transform = std_scaler.fit_transform(train_data)\n\n\nprint('LINEAR REGRESSION')\nlr = LinearRegression()\nlr.fit(train_data_transform,train_label)\nprint(lr.score(train_data_transform,train_label),mean_squared_error(lr.predict(train_data_transform),train_label),\n      rmspe(lr.predict(train_data_transform),train_label),rmspe(train_label,lr.predict(train_data_transform)))\nprint('Scoring \\t Mean Squared Error \\t RMSPE METRIC(COMP)')\n# print(mean_squared_error(lr.predict(train_data_transform),train_label),rmspe(lr.predict(train_data_transform),train_label))\n# print(lr.predict(train_data_transform[:5]),train_label[:5].values)\n\nprint('LIGHT GBM')\nlgbm = lgb.LGBMRegressor()\nlgbm.fit(train_data_transform,train_label)\n# print('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(lgbm.score(train_data_transform,train_label),mean_squared_error(lgbm.predict(train_data_transform),train_label)\n      ,rmspe(lgbm.predict(train_data_transform),train_label),rmspe(train_label,lgbm.predict(train_data_transform)))\n\nprint('XGBOOST')\nxgbreg =xgb.XGBRegressor()\nxgbrreg = xgb.XGBRFRegressor()\nxgbreg.fit(train_data_transform,train_label)\nxgbrreg.fit(train_data_transform,train_label)\n# print('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(xgbreg.score(train_data_transform,train_label),mean_squared_error(xgbreg.predict(train_data_transform),train_label),\n      rmspe(xgbreg.predict(train_data_transform),train_label),rmspe(train_label,xgbreg.predict(train_data_transform)))\nprint(xgbrreg.score(train_data_transform,train_label),mean_squared_error(xgbrreg.predict(train_data_transform),train_label),\n      rmspe(xgbrreg.predict(train_data_transform),train_label),rmspe(train_label,xgbrreg.predict(train_data_transform)))","3444a4ac":"train_x , val_x , train_y ,val_y = train_test_split(train_data,train_label,test_size=.1)\nstd_scaler = StandardScaler()\ntrain_x_transform = std_scaler.fit_transform(train_x)\nval_x_transform = std_scaler.transform(val_x)","eb1b7106":"import plotly.graph_objects as go\nprint('Decision Tree')\nres_dtr = []\nfor i in range(15):\n    dtr = DecisionTreeRegressor(max_depth=i+1)\n    dtr.fit(train_x_transform,train_y)\n    print(i+1,dtr.score(train_x_transform,train_y),rmspe(dtr.predict(train_x_transform),train_y),dtr.score(val_x_transform,val_y),rmspe(dtr.predict(val_x_transform),val_y))\n    res_dtr.append([dtr.score(train_x_transform,train_y),rmspe(dtr.predict(train_x_transform),train_y),dtr.score(val_x_transform,val_y),rmspe(dtr.predict(val_x_transform),val_y)])\nres_dtr = np.array(res_dtr)\ngraph_labels = ['Train Score','Train RMPSE','TEST SCORE','TEST RMPSE']\nfig = go.Figure()\nfor i in range(res_dtr.shape[1]):\n    fig.add_trace(go.Line(name=f\"{graph_labels[i]}\",y=res_dtr[:,i]))\nfig.show()","5b1b7ea8":"dtr_params ={\n#     'criterion':['gini','entropy'],\n    'max_depth':[3,5,8,11],\n    'min_samples_split':[5,10],\n#     'min_samples_leaf':[20,40,60,80],\n    'max_features':['auto','log2'],\n#     'max_leaf_nodes':[50,100]\n}\ndtr = DecisionTreeRegressor()\ndtr_random = RandomizedSearchCV(dtr, dtr_params,verbose=2,n_jobs=-1)\nbest_param = dtr_random.fit(train_x_transform,train_y)\nprint(best_param.best_params_)\nprint(best_param.score(train_x_transform,train_y),rmspe(best_param.predict(train_x_transform),train_y),best_param.score(val_x_transform,val_y),rmspe(best_param.predict(val_x_transform),val_y))","cde9b812":"print('Random Forest Regressor')\nres_forest = []\nfor i in range(10):\n    rfr = DecisionTreeRegressor(max_depth=i+1)\n    rfr.fit(train_x_transform,train_y)\n    print(i+1,rfr.score(train_x_transform,train_y),rmspe(rfr.predict(train_x_transform),train_y),rfr.score(val_x_transform,val_y),rmspe(rfr.predict(val_x_transform),val_y))\n    res_forest.append([rfr.score(train_x_transform,train_y),rmspe(rfr.predict(train_x_transform),train_y),rfr.score(val_x_transform,val_y),rmspe(rfr.predict(val_x_transform),val_y)])\nres_forest = np.array(res_forest)\ngraph_labels = ['Train Score','Train RMPSE','TEST SCORE','TEST RMPSE']\nfig = go.Figure()\nfor i in range(res_forest.shape[1]):\n    fig.add_trace(go.Line(name=f\"{graph_labels[i]}\",y=res_forest[:,i]))\nfig.show()\nrfr = DecisionTreeRegressor(max_depth=5)\nrfr.fit(train_x_transform,train_y)\nprint(rfr.score(train_x_transform,train_y),rmspe(rfr.predict(train_x_transform),train_y),rfr.score(val_x_transform,val_y),rmspe(rfr.predict(val_x_transform),val_y),np.sqrt(mean_squared_error(rfr.predict(train_x_transform),train_y)),np.sqrt(mean_squared_error(rfr.predict(val_x_transform),val_y)))","f43d2fe2":"demo_all_pred = read_all_files('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet')\n\nbook_test_dir = glob('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*\/*')\ntrain_data_vol_pred = []\nfor i in tqdm(book_test_dir):\n    train_data_vol_pred.append(file_path_to_volatility(i))\npast_data_pred = pd.concat([pd.DataFrame(i,columns=past_data.columns) for i in train_data_vol_pred])\npast_data_pred.stock_id = past_data_pred.stock_id.astype('int64')\npast_data_pred.time_id = past_data_pred.time_id.astype('int64')\n\ndata_pred = files_to_numbers(demo_all_pred,past_data_pred,'..\/input\/optiver-realized-volatility-prediction\/test.csv')\ndata_pred.dropna(inplace=True)\ndata_pred.reset_index(drop=True)\nrow_orderings = data_pred.row_id\ndata_pred.drop(columns='row_id',inplace=True)\ndata_pred = data_pred[feature]\npred = rfr.predict(std_scaler.transform(data_pred))\nsub = pd.merge(sample_sub,pd.DataFrame({'row_id':row_orderings,'target':pred}),on='row_id',how='left',suffixes=['_old',''])[sample_sub.columns]\nsub.target.fillna(0.003048022,inplace=True)\ndisplay(sub)\nsub.to_csv('submission.csv',index=False)","f89790a2":"STEPS TO START TRAINING\n1. Read All Files , Pick First Occurence by Grouping on stockid time_id\n2. Merge With Training csv to give these labels\/target and then with preprocessed data that we have where we calculate the Volatility at end of 10 min or bucket mark.\n3. This gives a size of 428913 and train csv is of size 428932 . 19 Points have been discarded by us .","4ee15342":"To make Prediction on any folder we need 3 things ,\n1. Read All files in it \n2. Merge with the Correct CSV File it is related to \n3. Merge with Volatily result ie past data","1f6de53b":"Stock ID Act as a kind of unique token for different stock.\n\nTime ID not exact in sequence but kind of specifying a token for time bucket .\/\/A bit Vague","24e2d2e2":"1. LINEAR REGRESSION SCORES .305 - .315           ||          .40340\n2. Lightgbm SCORES .2867 || .308\n3. XGBoost .27 .29  || .303\n4. Decision Tree GRID .295\n5. Random Forest .292","07818b85":"Keep More features like mean count etc in the rows","63b623ae":"**Currently Features such as Stock ID ,time_id ,Stock in bucket are not at all being used.**","6b50ac1f":"Analysis on Train csv First Row \n\n0\t5\t0.004136","be00627c":"While using file_path_to_volatility i am losing on information like the mean,count,std on these values as they are not merged on later so we will be making some calculations and aggregations to find some values to have their infulence and give some information.","5d376edf":"**Try to combine the 2 trade list and make the data suitable for Model **\n\n..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\n\n..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0\n\nAfter Merging Trade Book and Trade Train , i will add the train.csv file will give some rows the orignal true volatility and some of them NANS.\nFirst Just considering the orginals one ."}}