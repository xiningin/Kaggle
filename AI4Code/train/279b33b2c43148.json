{"cell_type":{"d967453c":"code","ed857b1e":"code","db2f2a70":"code","93b37ad6":"code","21f88824":"code","dd2413a4":"code","6d32544e":"code","78d94b8b":"code","871187b1":"code","567bd343":"code","c821fa2d":"code","6a28ca12":"code","65b654dc":"code","42037ea9":"code","4ff7321a":"code","3da99329":"code","f34c9602":"code","aa111ae2":"markdown","cd332093":"markdown","3f908313":"markdown","66fe5de6":"markdown","073e98a1":"markdown"},"source":{"d967453c":"# ! pip install -q pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n! pip install -q torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https:\/\/download.pytorch.org\/whl\/cu111\/torch_stable.html\n! pip install -qU torchtext \n! pip install -q pytorch-lightning==1.4.9\n! pip install -q \"git+https:\/\/github.com\/PyTorchLightning\/lightning-flash.git#egg=lightning-flash[text]\"\n! pip install -qU torchmetrics>=0.5.0 rich wandb","ed857b1e":"! mkdir \/kaggle\/temp\n! pip list | grep torch\n! pip list | grep tokenizers\n! pip list | grep transformers\n! pip list | grep datasets\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","db2f2a70":"! nvidia-smi\nENABLE_ORT = False\nif ENABLE_ORT:\n    import torch\n    if torch.cuda.is_available():\n        ! pip install -q torch-ort -f https:\/\/download.onnxruntime.ai\/onnxruntime_stable_cu111.html\n        ! python -m torch_ort.configure","93b37ad6":"import gc\ngc.enable()\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport pytorch_lightning as pl\n\nimport wandb\n\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Type, Union\nfrom torch.optim.lr_scheduler import (\n    _LRScheduler, StepLR, MultiStepLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n)\n\nfrom transformers import (\n    AdamW,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers.trainer_pt_utils import get_parameter_names\n\nimport flash\nfrom flash import Trainer\nfrom flash.core.optimizers import LinearWarmupCosineAnnealingLR\nfrom flash.core.finetuning import NoFreeze\nfrom flash.text import QuestionAnsweringData, QuestionAnsweringTask\nfrom flash.text.question_answering.finetuning import QuestionAnsweringFreezeEmbeddings","21f88824":"# Setup and login into wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n! wandb login $WANDB_API_KEY","dd2413a4":"INPUT_DIR = \"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\"\nTEMP_PATH = \".\/\"\nINPUT_DATA_PATH = os.path.join(INPUT_DIR, \"train.csv\")\nTRAIN_DATA_PATH = os.path.join(TEMP_PATH, \"_train.csv\")\nVAL_DATA_PATH = os.path.join(TEMP_PATH, \"_val.csv\")\nPREDICT_DATA_PATH = os.path.join(INPUT_DIR, \"test.csv\")\n\n@dataclass\nclass HyperParams:\n    seed: int = 42\n    \n    # dataset specific\n    train_val_split: float = 0.1\n    batch_size: int = 4\n    \n    # model specific\n    backbone: str = \"xlm-roberta-base\"\n    pretrained: bool = True\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    ## Optimizer and Scheduler specific\n    # lr=2e-7,\n    # lr=5.531681197617226e-05, ## Adam\n    # lr=0.0001096478196143185, ## AdamW\n    optimizer = 'AdamW'\n    learning_rate = 3e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n    lr_scheduler: str = \"cosine_schedule_with_warmup\"\n    num_warmup_steps: float = 0.1\n    lr_scheduler_config = {\n        \"interval\": \"step\",\n        \"frequency\": 1,\n    }\n    \n    # Training\/Finetuning args\n    debug: bool = False\n    num_gpus: int = torch.cuda.device_count()\n    accumulate_grad_batches: int = 2\n    enable_ort: bool = ENABLE_ORT\n    max_epochs: int = 10\n    finetuning_strategy: str= \"no_freeze\"\n    stochastic_weight_avg: bool = False\n\nHYPER_PARAMS = HyperParams()\npl.seed_everything(HYPER_PARAMS.seed)","6d32544e":"# Display a small portion of the dataset\ndf = pd.read_csv(INPUT_DATA_PATH)\ndisplay(df.head())","78d94b8b":"fraction = 1 - HYPER_PARAMS.train_val_split\n\n# Splitting data into train and val beforehand since preprocessing will be different for datasets.\ntamil_examples = df[df[\"language\"] == \"tamil\"]\ntrain_split_tamil = tamil_examples.sample(frac=fraction,random_state=200)\nval_split_tamil = tamil_examples.drop(train_split_tamil.index)\n\nhindi_examples = df[df[\"language\"] == \"hindi\"]\ntrain_split_hindi = hindi_examples.sample(frac=fraction,random_state=200)\nval_split_hindi = hindi_examples.drop(train_split_hindi.index)\n\ntrain_split = pd.concat([train_split_tamil, train_split_hindi]).reset_index(drop=True)\nval_split = pd.concat([val_split_tamil, val_split_hindi]).reset_index(drop=True)\n\ntrain_split.to_csv(TRAIN_DATA_PATH, index=False)\nval_split.to_csv(VAL_DATA_PATH, index=False)","871187b1":"# 1. Create the DataModule\ndatamodule = QuestionAnsweringData.from_csv(\n    train_file=TRAIN_DATA_PATH,\n    val_file=VAL_DATA_PATH,\n    batch_size=HYPER_PARAMS.batch_size,\n    backbone=HYPER_PARAMS.backbone\n)","567bd343":"class ChaiiQuestionAnswering(QuestionAnsweringTask):\n    def __init__(\n       self,\n        backbone: str = \"distilbert-base-uncased\",\n        optimizer: Type[torch.optim.Optimizer] = torch.optim.Adam,\n        lr_scheduler: Optional[Union[Type[_LRScheduler], str, _LRScheduler]] = None,\n        metrics: Union[Callable, Mapping, Sequence, None] = None,\n        learning_rate: float = 5e-5,\n        enable_ort: bool = False,\n    ):\n        super().__init__(\n            backbone=backbone,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            metrics=metrics,\n            learning_rate=learning_rate,\n            enable_ort=enable_ort,\n        )\n\n    @staticmethod\n    def jaccard(str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n        \n    def compute_metrics(self, generated_tokens, batch):\n        scores = []\n        for example in batch:\n            predicted_answer = generated_tokens[example[\"example_id\"]]\n            target_answer = example[\"answer\"][\"text\"][0] if len(example[\"answer\"][\"text\"]) > 0 else \"\"\n            scores.append(ChaiiQuestionAnswering.jaccard(predicted_answer, target_answer))\n\n        result = {\"jaccard_score\": torch.mean(torch.tensor(scores))}\n        return result\n    \n#     def configure_optimizers(self):\n#         decay_parameters = get_parameter_names(self.model, [torch.nn.LayerNorm])\n#         decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n#         optimizer_grouped_parameters = [\n#             {\n#                 \"params\": [p for n, p in self.model.named_parameters() if n in decay_parameters],\n#                 \"weight_decay\": HYPER_PARAMS.weight_decay,\n#             },\n#             {\n#                 \"params\": [p for n, p in self.model.named_parameters() if n not in decay_parameters],\n#                 \"weight_decay\": 0.0,\n#             },\n#         ]\n#         optimizer = AdamW(optimizer_grouped_parameters,lr=self.learning_rate,correct_bias=True)\n\n#         if self.lr_scheduler is not None:\n#             return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n#         return optimizer\n    \n#     def configure_optimizers(self):\n#         opt_parameters = (\n#             []\n#         )  # To be passed to the optimizer (only parameters of the layers you want to update).\n#         named_parameters = list(self.model.named_parameters())\n\n#         # According to AAAMLP book by A. Thakur, we generally do not use any decay\n#         # for bias and LayerNorm.weight layers.\n#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n#         set_2 = [\"layer.4\", \"layer.5\", \"layer.6\", \"layer.7\"]\n#         set_3 = [\"layer.8\", \"layer.9\", \"layer.10\", \"layer.11\"]\n#         init_lr = 1e-6\n\n#         for i, (name, params) in enumerate(named_parameters):\n\n#             weight_decay = 0.0 if any(p in name for p in no_decay) else 0.01\n\n#             if name.startswith(\"roberta.embeddings\") or name.startswith(\"roberta.encoder\"):\n#                 # For first set, set lr to 1e-6 (i.e. 0.000001)\n#                 lr = init_lr\n\n#                 # For set_2, increase lr to 0.00000175\n#                 lr = init_lr * 1.75 if any(p in name for p in set_2) else lr\n\n#                 # For set_3, increase lr to 0.0000035\n#                 lr = init_lr * 3.5 if any(p in name for p in set_3) else lr\n\n#                 opt_parameters.append(\n#                     {\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr}\n#                 )\n\n#             # For regressor and pooler, set lr to 0.0000036 (slightly higher than the top layer).\n#             if name.startswith(\"qa_outputs\"):\n#                 lr = init_lr * 3.6\n\n#                 opt_parameters.append(\n#                     {\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr}\n#                 )\n\n#         optimizer = AdamW(opt_parameters, lr=init_lr, correct_bias=True)\n#         if self.lr_scheduler is not None:\n#             return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n#         return optimizer\n\n    def configure_optimizers(self):\n        # To be passed to the optimizer (only parameters of the layers you want to update).\n        opt_parameters = []\n        weight_decay = HYPER_PARAMS.weight_decay    \n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        named_parameters = list(self.model.named_parameters())\n    \n        def get_lrs(start_lr, last_lr):\n            step = np.abs(np.log(start_lr) - np.log(last_lr)) \/ 3\n            return [start_lr,  np.exp(np.log(start_lr) + step), np.exp(np.log(start_lr)+2*step), last_lr]\n        \n        def is_decay_param(name: str):\n            return not any(p in name for p in no_decay)\n        \n        ranges = [(0,69), (69, 133), (133, 197), (197, 199)]\n        lrs = get_lrs(1e-8, self.learning_rate)\n        \n        for _range, lr in zip(ranges, lrs):\n            params = named_parameters[_range[0]:_range[1]]\n            decay_parameters = [p for n, p in params if is_decay_param(n)]\n            no_decay_parameters = [p for n, p in params if not is_decay_param(n)]\n\n            opt_parameters.append(\n                {\"params\": decay_parameters, \"weight_decay\": weight_decay, \"lr\": lr}\n            )\n\n            # According to AAAMLP book by A. Thakur, we generally do not use any decay\n            # for bias and LayerNorm.weight layers.\n            opt_parameters.append(\n                {\"params\": no_decay_parameters, \"weight_decay\": 0.0, \"lr\": lr}\n            )\n        \n\n        optimizer = AdamW(opt_parameters, lr=self.learning_rate, correct_bias=True)\n        if self.lr_scheduler is not None:\n            return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n        return optimizer\n    \n    def configure_optimizers(self):\n            # To be passed to the optimizer (only parameters of the layers you want to update).\n            opt_parameters = []\n            weight_decay = HYPER_PARAMS.weight_decay    \n            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n            for n, p in self.model.named_parameters():\n                if \"qa_outputs\" in n:\n                    if \"bias\" in n:\n                        opt_parameters.append(\n                            {\"params\": p, \"weight_decay\": 0.0, \"lr\": self.learning_rate}\n                        )\n                    else:\n                        opt_parameters.append(\n                            {\"params\": p, \"weight_decay\": weight_decay, \"lr\": self.learning_rate}\n                        )\n            optimizer = AdamW(opt_parameters, lr=self.learning_rate, correct_bias=True)\n            if self.lr_scheduler is not None:\n                return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n            return optimizer","c821fa2d":"model = ChaiiQuestionAnswering(\n    backbone=HYPER_PARAMS.backbone,\n    learning_rate=HYPER_PARAMS.learning_rate,\n    lr_scheduler=(HYPER_PARAMS.lr_scheduler, {\"num_warmup_steps\": HYPER_PARAMS.num_warmup_steps}, HYPER_PARAMS.lr_scheduler_config),\n    enable_ort=HYPER_PARAMS.enable_ort,\n)","6a28ca12":"lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    save_top_k=1,\n    filename='checkpoint\/{epoch:02d}-{val_loss:.4f}',\n    mode='max',\n)\n\nwandb_logger = pl.loggers.WandbLogger(\n    project='chaii-competition',\n    group=f\"{HYPER_PARAMS.backbone}\",\n    job_type=f\"{HYPER_PARAMS.finetuning_strategy}\",\n    name=\"ORT=False_LR=1e-5_NoFreeze_CW=0.1_AG=2_WRate=0.5_4grpLRD\",\n    log_model=True,\n    config=asdict(HYPER_PARAMS),\n)\n\nearlystopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')\n\nswa = pl.callbacks.StochasticWeightAveraging()","65b654dc":"FIND_LR = False\ncallbacks = [lr_monitor, earlystopping, checkpoint_callback]\ncallbacks_with_swa = callbacks + [swa]\n\nif FIND_LR:\n    callbacks.append(QuestionAnsweringFreezeEmbeddings(model_type=model.model.config.model_type))\n\ntrainer = Trainer(\n    fast_dev_run=HYPER_PARAMS.debug,\n    logger=wandb_logger if not HYPER_PARAMS.debug else True,\n    callbacks=callbacks,\n    gpus=HYPER_PARAMS.num_gpus,\n    log_every_n_steps=10,\n    weights_summary='top',\n    auto_lr_find=FIND_LR,\n    max_epochs=HYPER_PARAMS.max_epochs,\n    accumulate_grad_batches=HYPER_PARAMS.accumulate_grad_batches,\n    num_sanity_val_steps=0,\n    stochastic_weight_avg=HYPER_PARAMS.stochastic_weight_avg,\n)","42037ea9":"def get_num_training_steps(datamodule, trainer) -> int:\n    \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n    dataset_size = len(datamodule.train_dataloader())\n    num_devices = max(1, trainer.num_gpus, trainer.num_processes)\n    effective_batch_size = trainer.accumulate_grad_batches * num_devices\n    max_estimated_steps = (dataset_size \/\/ effective_batch_size) * trainer.max_epochs\n    return max_estimated_steps\n\nnum_training_steps_for_lr_find = get_num_training_steps(datamodule, trainer)\nprint(f\"Num of training steps: {num_training_steps_for_lr_find}\")","4ff7321a":"if FIND_LR:\n    ## Find LR ##\n    # Run learning rate finder\n    lr_finder = trainer.tuner.lr_find(\n        model=model,\n        datamodule=datamodule,\n        min_lr=1e-8, \n        max_lr=1,\n        num_training=num_training_steps_for_lr_find,\n        mode=\"linear\",\n        early_stop_threshold=None,\n        update_attr=False,\n    )\n    fig = lr_finder.plot(suggest=True)\n    fig.show()\n    new_lr = lr_finder.suggestion()\n    print(f\"Suggested Learning Rate: {new_lr}\")\nelse:\n    ## Finetune the model ##\n    if not HYPER_PARAMS.debug:\n        wandb_logger.watch(model)\n    \n    if HYPER_PARAMS.finetuning_strategy == \"freeze\":\n        print(\"Frozen Model\")\n        trainer.finetune(model, datamodule=datamodule)\n    else:\n        print(\"Unfrozen Model\")\n        trainer.fit(model, datamodule=datamodule)\n    \n    if not HYPER_PARAMS.debug:\n        wandb.finish()","3da99329":"torch.cuda.empty_cache()\ngc.collect()","f34c9602":"# Convert the prediction queries to dictionary format.\npredict_data = pd.read_csv(PREDICT_DATA_PATH)\npredict_data = predict_data[predict_data.columns[:3]].to_dict(orient=\"list\")\n\n# Answer some Questions!\npredictions = model.predict(predict_data)\nprint(predictions)\n\n# Create submission.\nsubmission = {\"id\": [], \"PredictionString\": []}\nfor prediction in predictions:\n    submission[\"id\"].extend(prediction.keys())\n    submission[\"PredictionString\"].extend(prediction.values())\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv(\".\/submission.csv\", index=False)","aa111ae2":"# Hindi and Tamil Question Answering with Lightning Flash","cd332093":"## 1. Create the DataModule","3f908313":"## 3. Create the trainer and finetune the model","66fe5de6":"## Prediction","073e98a1":"## 2. Build the task"}}