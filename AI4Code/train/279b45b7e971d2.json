{"cell_type":{"d27dbbea":"code","191c5ec0":"code","1eff381d":"code","e7b6f25c":"code","ae752631":"code","910fbf3f":"code","f1178650":"code","4b6011bf":"code","9308d444":"code","30c84ab1":"code","21968a86":"code","62466b98":"code","95938316":"code","5f79b164":"code","ec89ad13":"code","4db90536":"code","5e563fe0":"code","bf00848b":"code","c3978d26":"code","1eb1e0c7":"code","f2fbe9bf":"code","3f3b5491":"code","eaa9a040":"code","8bfc3614":"code","0a6246b3":"code","f41eec4f":"code","1b52b241":"code","ecb5988a":"code","53e8f507":"code","25fa5246":"code","6c7740b0":"code","0e8494c6":"code","a711dfdc":"code","9dce594b":"code","71564c81":"code","50a9a920":"code","6d596266":"code","240a4a94":"code","247638d2":"code","460d4237":"code","bfef7923":"code","e8359795":"code","bfc7748e":"code","f15e2201":"markdown","26f4fd98":"markdown","670e251c":"markdown","ffa3071a":"markdown","5bc46e85":"markdown","d19e78bc":"markdown","428f994a":"markdown","03954006":"markdown","f05eb074":"markdown","8bde6d82":"markdown","58fe8fd4":"markdown","99ed2718":"markdown","e0b6d158":"markdown","327982c8":"markdown","a90de644":"markdown","1807fc41":"markdown","ebf8510b":"markdown","4ea54581":"markdown","fa8bf6b3":"markdown","2b915b09":"markdown","bd66376a":"markdown","f1b131d5":"markdown","88a9e14a":"markdown","11c9cb43":"markdown","907ff4d3":"markdown"},"source":{"d27dbbea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport sklearn as sk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","191c5ec0":"#lets import the train dataset to the variable train\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(train.shape)\ntrain.head()","1eff381d":"#lets import test dataset to variable test\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(test.shape)\ntest.head()","e7b6f25c":"print(test.info())\nprint(train.info())","ae752631":"women = train.loc[train.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","910fbf3f":"men = train.loc[train.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","f1178650":"#printing the dimesnion of the data sets\nprint(train.shape)\nprint(test.shape)","4b6011bf":"#missing value treatment\nprint(train.isnull().sum())","9308d444":"#plotting missing values \nimport missingno as ms\nms.matrix(train)","30c84ab1":"test.isnull().sum()","21968a86":"train.isnull().sum()","62466b98":"train['Age'].fillna(train['Age'].mean(), inplace = True)\ntest['Age'].fillna(test['Age'].mean(), inplace = True)\ntrain['Fare'].fillna(train['Fare'].mean(), inplace = True)\ntrain['Embarked'].fillna('S', inplace = True)","95938316":"train.isnull().sum()","5f79b164":"test.isnull().sum()","ec89ad13":"#Dropping Columns that may not be required\ntrain.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\ntest.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)","4db90536":"train.head()","5e563fe0":"test.head()","bf00848b":"# we are setting the passangerID as index in train data set.\ntrain.set_index('PassengerId',inplace=True)","c3978d26":"train.info()","1eb1e0c7":"test.info()","f2fbe9bf":"train = pd.get_dummies(train, columns=[\"Sex\"], drop_first=True)\ntrain = pd.get_dummies(train, columns=[\"Embarked\"],drop_first=True)\n","3f3b5491":"test = pd.get_dummies(test, columns=[\"Sex\"], drop_first=True)\ntest = pd.get_dummies(test, columns=[\"Embarked\"],drop_first=True)","eaa9a040":"#Mapping the data.\ntrain['Fare'] = train['Fare'].astype(int)\ntrain.loc[train.Fare<=7.91,'Fare']=1\ntrain.loc[(train.Fare>7.91) &(train.Fare<=14.454),'Fare']=2\ntrain.loc[(train.Fare>14.454)&(train.Fare<=31),'Fare']=3\ntrain.loc[(train.Fare>31),'Fare']=4","8bfc3614":"#Mapping the data.\ntrain['Age']=train['Age'].astype(int)\ntrain.loc[ train['Age'] <= 18, 'Age']= 1\ntrain.loc[(train['Age'] > 18) & (train['Age'] <= 36), 'Age'] = 2\ntrain.loc[(train['Age'] > 36) & (train['Age'] <= 54), 'Age'] = 3\ntrain.loc[(train['Age'] > 54) & (train['Age'] <= 72), 'Age'] = 4\ntrain.loc[train['Age'] > 72, 'Age'] = 5","0a6246b3":"train.corr()","f41eec4f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.subplots(figsize = (10,10))\nsns.heatmap(train.corr(), annot=True,cmap=\"PiYG\")\nplt.title(\"Correlations\");","1b52b241":"plt.subplots(figsize = (10,10))\nax=sns.countplot(x='Pclass',hue='Survived',data=train)\nplt.title(\"Survival Rate Based on Class\", fontsize = 25)\nleg=ax.get_legend()\nleg.set_title('')\nlegs=leg.texts\nlegs[0].set_text('Survived')\nlegs[1].set_text(\"Did Not Survive\")","ecb5988a":"plt.subplots(figsize = (10,10))\nax=sns.countplot(x='Age',hue='Survived',data=train)\nplt.title(\"Survival rate based on Age\", fontsize = 25)\nleg=ax.get_legend()\nleg.set_title('')\nlegs=leg.texts\nlegs[0].set_text('Survived')\nlegs[1].set_text(\"Did Not Survive\")","53e8f507":"plt.subplots(figsize = (10,10))\nax=sns.countplot(x='Fare',hue='Survived',data=train)\nplt.title(\"Survival based on Fare\", fontsize = 25)\nleg=ax.get_legend()\nleg.set_title('')\nlegs=leg.texts\nlegs[0].set_text('Survived')\nlegs[1].set_text(\"Did Not Survive\")","25fa5246":"plt.subplots(figsize = (10,10))\nax=sns.countplot(x='Sex_male',data=train,hue='Survived')\ntrain.shape\nplt.title('Passenger distribution of survived vs not-survived')\nplt.xlabel('Gender')\nplt.ylabel(\"Number of Passenger Survived\")\nlabels = ['Female', 'Male']\n#Fixing xticks.\nplt.xticks(sorted(train.Survived.unique()),labels)\nleg = ax.get_legend()\nleg.set_title('')\nlegs=leg.texts\nlegs[0].set_text('Survived')\nlegs[1].set_text(\"Did Not Survive\")","6c7740b0":"print(train.head())\nprint(test.head())","0e8494c6":"from sklearn.ensemble import RandomForestClassifier\n\ny = train[\"Survived\"]\n\nfeatures = [\"Pclass\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\nprint(model.score(X, y))","a711dfdc":"import warnings\nwarnings.filterwarnings('ignore')","9dce594b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom xgboost import XGBClassifier","71564c81":"X_train,X_test,y_train,y_test = train_test_split(train.drop('Survived',axis=1),train['Survived'], test_size=30,random_state=15)\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","50a9a920":"y_predict = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_predict)\nprint(cm)\naccuracy_score(y_test, y_predict)","6d596266":"print(train.head())\nprint(test.head())","240a4a94":"features = ['Pclass', 'Sex_male', 'Embarked_Q',\n           'Embarked_S', 'Parch', 'SibSp', 'Fare']\n\ntrain_label = train[['Survived']]\ntrain_features = train[features]\ntrain_features.head()","247638d2":"train.shape","460d4237":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# max_depth is 5 , we will plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt_default.fit(train_features, train_label)","bfef7923":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\ny_pred_default = dt_default.predict(train_features)\nprint(classification_report(train_label, y_pred_default))","e8359795":"# Printing confusion Matrix and accuracy\nprint(confusion_matrix(train_label, y_pred_default))\nprint(accuracy_score(train_label, y_pred_default))","bfc7748e":"#Final Submission\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived':predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f15e2201":"# XGBOOST","26f4fd98":"**Info() - gives the Categorial information about the data sets.**","670e251c":"**It is important to carry out missing value treatment because :**\n* The performance of our predictive model or data insights will be affected if the missing values are not duly handled.\n* It Can reduce the power and can lead to a biased model as we have not analysed the relationship with other variables of the data sets correctly.","ffa3071a":"XGBoost is a decision-tree-based ML algorithm that uses a gradient boosting framework.","5bc46e85":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/media0.giphy.com\/media\/g2e22SIHPcQw0\/giphy.gif\" alt=\"Heat beating\" style=\"height:300px;margin-top:3rem;\"> <\/div>","d19e78bc":"**Mapping the data**'\n* Fare and Age is mapped based on  range to a numeric value.","428f994a":"#  <H>Term Project Tutorial<\/H>\n# NAME : KUSHAAL KASHYAP NARASIMHAN\n# UTA ID : 1001979047","03954006":"# Lets plot graph for different features based on survival.","f05eb074":"**With XGBOOST we have attained an accuracy of 79.5% , XGBClassifier attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models**.\n\nLets further try to increase the accuracy .","8bde6d82":"**Creating Dummies**\n* get_dummies() - It is used for data manipulation. It converts categorical data into dummy or indicator variables of the columns whose data has to be manipulated.\n* Here we are using it on columns Sex and Embarked for both Train and Test data sets.","58fe8fd4":"With the ramdom forest classifier we got a accuracy score of 72% .\nLets try to improve it","99ed2718":"**Continuing missing value treatment**\n* Dropping the columns Name ,Ticket and Cabin as this data does not affect the prediction model and also as cabin has more than 80% of null values.\n","e0b6d158":"**A decision tree is a tree-like structure where an internal node represents attricutes and the branch represents a decision rule and each leaf node represents the outcome. The topmost node is called root node it partitions the tree.Tree like structure help in decision making.**","327982c8":"**head() - Is used to get 1st n rows.**","a90de644":"# Decision Tree Classifier","1807fc41":"# NORMALIZING THE DATA","ebf8510b":"# Now lets understand all the columns :\n\n* Survival = 0 - No , 1 - Yes\n* Pclass(Ticket Class) = 1st , 2nd and 3rd\n* SibSp = number of siblings + number of his\/her spouse\n* Parch = number of parents travelled + number of children\n* Embarked Port of Embarkation - C = Cherbourg, Q = Queenstown and S = Southampton","4ea54581":"# **Random Forest Classifier**\n\n* First lets use the random forest model which is constructed of many \"trees\" ,that will individually consider each passenger's data and vote on whether the individual survived. Then,the random forest model makes a democratic decision: the outcome with the most votes wins!\n\n\n --From alexisbcook","fa8bf6b3":"**Above shows the percentage of Women and Men who survived the titanic crash.**\n\n-- The above result is from training dataset.","2b915b09":"***In the above graph the white lines indicates the missing values.***","bd66376a":"**Starting with the missing value treatment**\n* We calculate the mean for  columns Age and Fare and fill those inplace of Na.\n* The na position in Embarked is filled with 'S'.","f1b131d5":"**Heat Map**\n\n* It contains values representing different shades of the same colour.The brighter shades of the chart represent higher values than the lighter shade represents lower values.","88a9e14a":"# Titanic_1912_Survival_Prediction \ud83d\udea2","11c9cb43":"In this workbook ,I will be using different machine learning algorithms to get maximum accuracy. They have provided with train and test datasets .\n\n**Train Dataset**: This dataset is used to train ML models\n**Test Dataset**: This dataset is used to evaluate ML models","907ff4d3":"**We have got a accuracy of 81.7% using DecisionTreeClassifier**"}}