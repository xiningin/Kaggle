{"cell_type":{"f829ab8a":"code","58e39d69":"code","b5377bb5":"code","ff5aa56a":"code","043f4b5a":"code","5042670c":"code","96024c72":"code","8211f455":"code","bb5d4edf":"code","adc4c88b":"code","7e008c37":"code","2d731863":"code","0a543fcf":"code","dc443830":"code","5123bb03":"code","883fd32d":"markdown","9a60bc17":"markdown","54e8accb":"markdown","7c59af52":"markdown","37f6f430":"markdown","9367b539":"markdown","56ff110d":"markdown","c5339875":"markdown","e88ce84b":"markdown","e0565156":"markdown","04ec1c4f":"markdown","e06a79e5":"markdown","214c48f7":"markdown","4d6c1d2f":"markdown","8df5e24b":"markdown","da06a1a3":"markdown","453bcf12":"markdown"},"source":{"f829ab8a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, preprocessing\nfrom xgboost.sklearn import XGBClassifier","58e39d69":"# load data\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.info()\ndf.describe(include='all')","b5377bb5":"# Convert dtype\ndf['stroke'] = df['stroke'].astype(object)\ndf['hypertension'] = df['hypertension'].astype(object)\ndf['heart_disease'] = df['heart_disease'].astype(object)\n\n# BMI missing value\ndf[\"bmi\"] = df[\"bmi\"].fillna(df[\"bmi\"].mean())\n\n# rename columns\ndf.rename(columns = {'Residence_type':'residence_type'}, inplace = True)\n\n# Drop id columns\ndf = df.drop(columns=[\"id\"])","ff5aa56a":"cat_data = [x for x in df.columns if df[x].dtype == \"object\"]\nnum_data = [y for y in df.columns if df[y].dtype != \"object\"]\n\nfor col in cat_data:\n    plt.title(col)\n    sns.countplot(df[col])\n    plt.show()\n\nfor col in num_data:\n    plt.title(col)\n    sns.histplot(df[col],kde=True)\n    plt.show()","043f4b5a":"df['stroke'] = df['stroke'].astype(int)\nall_col = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']\n\nY = 'stroke ~ gender'\nfor i in all_col[1:]:\n    Y = Y + '+' + i \nresults = smf.ols(Y, data=df).fit()\nprint(results.summary())","5042670c":"gender = pd.get_dummies(df[['gender']])\nwork_type = pd.get_dummies(df[['work_type']])\n\n# ever_married\nmapping = {'Yes':1, 'No':0}\never_married = df['ever_married'].map(mapping)\n\n# residence_type\nmapping = {'Urban':1, 'Rural':0}\nresidence_type = df['residence_type'].map(mapping)\n\n# smoking_status\nmapping = {'smokes':3, 'formerly smoked':2, 'never smoked':1, 'Unknown':0}\nsmoking_status = df['smoking_status'].map(mapping)\n\ndf_combine = pd.concat([gender,\n                        df['age'],\n                        df['hypertension'],\n                        df['heart_disease'],\n                        ever_married,\n                        work_type,\n                        residence_type,\n                        df['avg_glucose_level'],\n                        df['bmi'],\n                        smoking_status, \n                        df['stroke']], axis=1)\n\ndf_select = pd.concat([df['age'],\n                       df['hypertension'],\n                       df['heart_disease'],\n                       ever_married,\n                       work_type,\n                       df['avg_glucose_level'], \n                       df['stroke']], axis=1)","96024c72":"stroke_cnt = df_combine['stroke'].loc[df_combine['stroke']==1].count()\n\ndf_combine_equal = df_combine.loc[df_combine['stroke']==0].sample(n=stroke_cnt*2, random_state=1)\ndf_combine_equal = pd.concat([df_combine_equal, df_combine.loc[df_combine['stroke']==1]], axis=0)\n\ndf_select_equal = df_select.loc[df_select['stroke']==0].sample(n=stroke_cnt*2, random_state=1)\ndf_select_equal = pd.concat([df_select_equal, df_select.loc[df_select['stroke']==1]], axis=0)","8211f455":"plt.figure(figsize=(15,15))\nsns.heatmap(df_combine.corr(),annot=True,cmap=\"rainbow\")\nplt.title(\"Correleation Heatmap\",fontsize=20,color=\"c\")\nplt.show()","bb5d4edf":"def plot_cm(model, Y_test, Y_pred):\n    cm = metrics.confusion_matrix(Y_test, Y_pred)\n    \n    f, axs = plt.subplots(2,1,figsize=(5,10))\n    ax= plt.subplot(211)\n    sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap=\"YlGnBu\");\n    ax.set_title(model);\n    ax.set_xlabel('Predicted labels');\n    ax.set_ylabel('True labels');\n    \n    ax1= plt.subplot(212)\n    ax1.set_position([0.1, 0.13, 0.7, 0.6])\n    data=[['Accuracy:', round(metrics.accuracy_score(Y_test, Y_pred), 4)],\n          ['Precision:',round(metrics.precision_score(Y_test, Y_pred), 4)],\n          ['Recall:',round(metrics.recall_score(Y_test, Y_pred), 4)],\n          ['F1 Score:',round(metrics.f1_score(Y_test, Y_pred, average='weighted', labels=np.unique(Y_pred)), 4)]]\n    ax1.axis('tight')\n    ax1.axis('off')\n    ax1.table(cellText=data,loc=\"center\").scale(1, 1.5)\n\n    plt.show()","adc4c88b":"from keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\n\ndef compile_nn_model(df_for_shape):\n    model = Sequential()\n    model.add(Dense(16, input_dim=df_for_shape.shape[1], activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","7e008c37":"def combined_model(df_input):\n    \n    # Test train split\n    X = df_input.iloc[:, :-1].values\n    Y = df_input.iloc[:, -1].values\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n\n    # XGBClassifier\n    model = 'XGBClassifier'\n    xgb = XGBClassifier(\n        learning_rate= 0.01,\n        n_estimators=1000, \n        use_label_encoder =False)\n    xgb.fit(X_train,Y_train,eval_metric='auc')\n    Y_pred = xgb.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n\n    # DecisionTreeClassifier\n    model = 'DecisionTreeClassifier'\n    clf = DecisionTreeClassifier(criterion=\"entropy\",\n                                 max_depth=7)\n    clf.fit(X_train,Y_train)\n    Y_pred = clf.predict(X_test)    \n    plot_cm(model, Y_test, Y_pred)\n\n    # RandomForestClassifier\n    model = 'RandomForestClassifier'\n    forest = ensemble.RandomForestClassifier(n_estimators = 1000)\n    forest.fit(X_train,Y_train)\n    Y_pred = forest.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n\n    # SVM\n    model = 'SupportVectorMachine'\n    svc = SVC()\n    svc.fit(X_train,Y_train)\n    Y_pred = svc.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n    \n    # SimpleNeuralNetwork\n    df_input = df_input.astype('float32')\n\n    X = df_input.iloc[:, 1:-1].values\n    Y = df_input.iloc[:, -1].values\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n    Y_train = np_utils.to_categorical(Y_train)\n\n    model = compile_nn_model(X_test)\n    callback = EarlyStopping(monitor='f1', patience=3)\n    history = model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0, callbacks=[callback])\n    plt.plot(history.history['loss'])\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.show()\n\n    Y_pred = model.predict(X_test)\n    Y_pred = np.argmax(Y_pred, axis=1)\n\n    model = 'SimpleNeuralNetwork'\n    plot_cm(model, Y_test, Y_pred)","2d731863":"combined_model(df_combine)","0a543fcf":"combined_model(df_select)","dc443830":"combined_model(df_combine_equal)","5123bb03":"combined_model(df_select_equal)","883fd32d":"## Logistic Regression\nPerform logistic regression analysis on all fields.<br>\nSelect fields with significant differences for future analysis.","9a60bc17":"## Import Packages\nImport all necessary packages","54e8accb":"## Stroke Prediction - NYCU Midterm Project","7c59af52":"## Models\n1. XGBoost Classifier\n1. Decision Tree Classifier\n1. Support Vector Machine\n1. Simple Neural Network","37f6f430":"### Results for`df_combine_equal` dataset","9367b539":"## Correlation","56ff110d":"## Evaluation Function\nCustomized function for result evaluation and result visualization.","c5339875":"## Sampling\nIn order to avoid the huge difference data amounts between `stroke = 0` and `stroke = 1`,<br>\nwe select all datas from `stroke = 1` and randomly sample twice the amount of `stroke = 1`'s data from `stroke = 0`.","e88ce84b":"### Results for`df_combine` dataset","e0565156":"Convert categorical datas to objects.<br>\nFill missing values in `bmi`.<br>\nRename `Residence_type` to `residence_type`.","04ec1c4f":"## Descriptive Statistics\nUse seaborn package to plot all categorical and numerical columns.","e06a79e5":"### Results for`df_combine_equal` dataset","214c48f7":"## Categorical Feature","4d6c1d2f":"All model's test-train split ratio were set to 0.2","8df5e24b":"## Results","da06a1a3":"## Data Preprocess\nRead datasets and take a brief look.","453bcf12":"### Results for`df_select` dataset"}}