{"cell_type":{"173c2894":"code","a3d34727":"code","1fb95de7":"code","416b0fcd":"code","c24e1351":"code","1f7259a7":"code","435f5331":"code","09107c53":"code","53b7cece":"code","1e78bd7e":"code","d8b66f97":"code","9b4f0023":"code","91e8a84b":"markdown","ba6872f6":"markdown","22ba6238":"markdown","7713be01":"markdown","2e86721e":"markdown","b253b845":"markdown","47f9ee90":"markdown","1501de28":"markdown","77026ef7":"markdown"},"source":{"173c2894":"# Loading Necessary Python Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sbn\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom matplotlib import pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom collections import Counter\n \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3d34727":"train_data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest_data = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\n\n","1fb95de7":"#Since the first column in the dataset is 'id', lets make it our index\ntrain_data.set_index('id', inplace = True)\ntrain_data\n#row0 = train_data.iloc[0]\n#row0","416b0fcd":"data_variation = pd.DataFrame(train_data.std())\ndata_variation","c24e1351":"percentage_missing_values = pd.DataFrame(train_data.isna().sum() \/ train_data.isna().count()*100)\npercentage_missing_values","1f7259a7":"\"\"\"\nOption 1: We can drop the rows or columns that contain a missing value. i.e train_data.dropna(axis=1) for dropping columns with NaN, dropna(axis=0) for dropping rows with NaN\nOption two: We can opt to fill the missing values with the preceding value train_ydata.fillna(method='ffill') or succeeding value train_data.fillna(method='backfill')\nOption three: Imputation which may use mean, median & mode strategies. With an exception of mode, other imputation strategies only works with numerical datasets\nOption three: Interpolation-Linear\nOption four: Imputation by K-NN\nin our case we shall use multivariate feature imputation by using 'IterativeImputer' class\n\"\"\"\n\nmissing_columns = ['song_duration_ms','acousticness','danceability','energy','instrumentalness','key','liveness','loudness']\n\nfor i in missing_columns:\n    train_data.loc[train_data.loc[:,i].isnull(),i]=train_data.loc[:,i].mean()\n    \ntrain_data.isnull().sum() #Now all our columns have no missing values","435f5331":"   # Our subplots to fit in 2 rows and 5 columns\n    \nfig1, ax1 = plt.subplots(2, 5, figsize=(10, 5), gridspec_kw={\n        'top' : 1.5,\n        'hspace' : .6,\n        'right' : 2.4\n    })\n    \n    # Continuous numerical features\nax1[0, 0].set_title('song_duration_ms')\nsbn.kdeplot(data=train_data, x=\"song_duration_ms\", ax=ax1[0, 0], color='red', fill=True, alpha=.32)\n    \nax1[1, 0].set_title('acousticness')\nsbn.kdeplot(data=train_data, x=\"acousticness\", ax=ax1[1, 0], color=\"indigo\", fill=True, alpha=.32)\n    \nax1[0, 1].set_title('danceability')\nsbn.kdeplot(data=train_data, x=\"danceability\", ax=ax1[0, 1], color='blue', fill=True, alpha=.32)\n    \nax1[1, 1].set_title('energy')\nsbn.kdeplot(data=train_data, x=\"energy\", ax=ax1[1, 1], color=\"green\", fill=True, alpha=.32)\n    \nax1[0, 2].set_title('instrumentalness')\nsbn.kdeplot(data=train_data, x=\"instrumentalness\", ax=ax1[0, 2], color=\"aqua\", fill=True, alpha=.32)\n    \nax1[1, 2].set_title('liveness')\nsbn.kdeplot(data=train_data, x=\"liveness\", ax=ax1[1, 2], color=\"darkred\", fill=True, alpha=.32)\n    \nax1[0, 3].set_title('loudness')\nsbn.kdeplot(data=train_data, x=\"loudness\", ax=ax1[0, 3], color=\"aqua\", fill=True, alpha=.32)\n    \nax1[1, 3].set_title('speechiness')\nsbn.kdeplot(data=train_data, x=\"speechiness\", ax=ax1[1, 3], color=\"aquamarine\", fill=True, alpha=.32)\n    \nax1[0, 4].set_title('tempo')\nsbn.kdeplot(data=train_data, x=\"tempo\", ax=ax1[0, 4], color=\"black\", fill=True, alpha=.32)\n    \nax1[1, 4].set_title('audio_valence')\nsbn.kdeplot(data=train_data, x=\"audio_valence\", ax=ax1[1, 4], color=\"magenta\", fill=True, alpha=.32)\n    \nplt.show()","09107c53":"# Checking Data Correlations\n\ntrain_data.corr()","53b7cece":"# Data Features Correlation Map\nplt.figure(figsize=(None))\nplt.title('Train Data Features Correlation')\nsbn.heatmap(train_data.corr(), vmin=-2, vmax=2, cmap='viridis',mask=np.triu(train_data.corr()))\nplt.show()","1e78bd7e":"# Checking Features Interactions\n\nfig, ax = plt.subplots(1, 4, figsize=(None))\n\nplt.subplots_adjust(top=2, right=3, wspace=.2)\n\nax[0].set_title('Energy vs Acousticness', fontstyle='italic', fontsize=22)\nsbn.scatterplot(x='energy', y='acousticness', data=train_data, hue='song_popularity', ax=ax[0], alpha=.70)\n\nax[1].set_title('Energy vs Loudness', fontstyle='italic', fontsize=22)\nsbn.scatterplot(x='energy', y='loudness', data=train_data, hue='song_popularity', ax=ax[1], alpha=.70)\n\n\nax[2].set_title('Acousticness vs Loudness', fontstyle='italic', fontsize=22)\nsbn.scatterplot(x='acousticness', y='loudness', data=train_data, hue='song_popularity', ax=ax[2], alpha=.70)\n\nax[3].set_title('Energy vs liveness', fontstyle='italic', fontsize=22)\nsbn.scatterplot(x='energy', y='liveness', data=train_data, hue='song_popularity', ax=ax[3], alpha=.70)\n\nplt.show()","d8b66f97":"target = 'song_popularity'\nfeatures = [i for i in train_data.columns if i not in [target]]\n\nprint(features)\ntarget","9b4f0023":"# Song Popularity Distribution - Our dependent Variable in this case\n\nplt.figure(figsize=[8,6])\nsbn.distplot(train_data[target], color='b',hist_kws=dict(edgecolor=\"gold\", linewidth=3), bins=30)\nplt.title('Distribution of Dependent Variable')\nplt.show()","91e8a84b":"**Checking Missing Values** ","ba6872f6":"**Our Training Data Looks Good So Far**\n**Lets now Choose Features (X) and Dependent Variable (y) also known as Target**","22ba6238":"**Data Loading**","7713be01":"***Handling Missing Values in the data***","2e86721e":"**Investigating Patterns in the Features**\n\n***Continuous Numerical Features Visualization***","b253b845":"***To avoid default indexing, we need to set our first column as our index***","47f9ee90":"*** Scatter Plots of Feature-Feature Fusion ***","1501de28":"****Checking Standard Deviation to understand the extend of our data variation****","77026ef7":"**Exploratory Data Analysis**"}}