{"cell_type":{"e35ffbd8":"code","8a0b3f5a":"code","7182dac5":"code","99901fa0":"code","0567c7c7":"code","1d7dcdf8":"code","7948e5ef":"code","2e0f7ea6":"code","6bc0dfd1":"code","3787ccb8":"code","fff6b457":"code","ead26fa6":"code","75427c94":"code","0c7485d0":"code","4fabdf21":"code","9332d5a9":"code","8e1d2168":"code","a8d3e48c":"code","b660b002":"code","70bd8e79":"code","ac4d2b35":"code","1a8aefca":"code","0daa521f":"code","d20f4790":"code","30c5a95a":"code","915ba4d4":"code","7fe7fc17":"code","d0d8b337":"code","ec47ebe1":"code","747152e4":"code","02c5fd30":"code","9bbfd9e8":"code","dc7ebe1d":"code","e21ce787":"code","377b675b":"code","9856942d":"code","ad474555":"code","fdf59268":"code","0fbd191d":"code","dea4cca2":"code","350e2819":"code","01cf2368":"code","6665dc80":"code","ef50eba6":"code","7b3c98c0":"code","53103064":"code","f4759276":"code","d055f1c1":"code","f9757182":"code","44bfbc62":"code","821fcd41":"code","2f12e28f":"code","fe87e9f8":"code","1e62678d":"code","03a29557":"code","a2b65e43":"code","c7d9a020":"code","060c48a8":"code","cfb9c785":"code","79b041c1":"code","002dc55e":"code","7e5bcc53":"code","02ca032b":"code","b815b4f6":"code","b87dc2c0":"code","b9e68885":"code","a3ec949d":"code","42681896":"code","7aaac2cd":"code","726d31fe":"code","72a453dc":"code","e09ee29c":"code","3e6b9588":"code","a5da3ab5":"code","a9271f02":"code","2da3f8f1":"code","16c2c1d5":"code","388fecd1":"code","a6932e3a":"code","679eb6f0":"code","f18f15f0":"code","c70759eb":"code","858878ce":"code","d14e4643":"code","3080a8e3":"code","b8da02cf":"code","fe2e28d5":"code","cc7b2893":"code","3f1f5624":"code","cf945410":"code","0414c8f1":"code","780d0a56":"code","6fb55efc":"code","ce448665":"code","11d4029a":"code","ec935140":"code","a24187be":"code","3e846542":"code","4386fdb4":"code","59200090":"code","9bc69569":"markdown","c33f10a3":"markdown","f54b02d3":"markdown","9ae66e96":"markdown","5a8c060f":"markdown","b6071516":"markdown","c528781d":"markdown","14bd3ff8":"markdown","bcf65950":"markdown","eee90fa2":"markdown","e514942e":"markdown","a826e683":"markdown","29dd6c32":"markdown","483dfb48":"markdown","49ec89bd":"markdown","3a37a362":"markdown","301946b9":"markdown","7a127b63":"markdown","c2364d7a":"markdown","8c304015":"markdown","d89178c4":"markdown","dfa95f70":"markdown","adadb8a9":"markdown","3857150c":"markdown","c98d82c2":"markdown","6150f1b5":"markdown","169a5b77":"markdown","82db6110":"markdown","e4e4f30c":"markdown","62c9835a":"markdown","70cb9b8c":"markdown","76ff21ba":"markdown","0466940b":"markdown","0ae548a8":"markdown","f527393f":"markdown","dc98f2b1":"markdown","3aadcc94":"markdown","264e763b":"markdown","3639d171":"markdown"},"source":{"e35ffbd8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stats\nimport plotly.express as exp\nimport matplotlib.pyplot as plt\nimport pickle\nimport warnings\nimport pickle\nwarnings.filterwarnings(\"ignore\")","8a0b3f5a":"data = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')","7182dac5":"data.head()","99901fa0":"data.info()","0567c7c7":"data.describe()","1d7dcdf8":"data.shape","7948e5ef":"data.Seats.replace(to_replace=0,value=data.Seats.median(),inplace=True)","2e0f7ea6":"data.drop(\"Unnamed: 0\",axis=1,inplace=True)","6bc0dfd1":"data.Year = data.Year.astype(\"object\")","3787ccb8":"data.head()","fff6b457":"data.select_dtypes(exclude=np.number).nunique()","ead26fa6":"data[\"Brand\"] = data.Name.apply(lambda x:x.split()[0])","75427c94":"data.head()","0c7485d0":"data.drop(\"Name\",axis=1,inplace=True)","4fabdf21":"data.Brand.replace({\"Isuzu\":\"ISUZU\",\"Land\":\"Land Rover\",\"Mini\":\"Mini Cooper\"},inplace=True)","9332d5a9":"sorted(data.Brand.unique())","8e1d2168":"data[\"Model_Age\"] = 2021 - data.Year.apply(lambda x:int(x))","a8d3e48c":"data.drop(\"Year\",axis=1,inplace=True)","b660b002":"(data.isnull().sum() \/ len(data)) * 100","70bd8e79":"data.drop(\"New_Price\",axis=1,inplace=True)","ac4d2b35":"for i in [\"Mileage\",\"Engine\",\"Power\"]:\n    data[i].replace({np.nan:\"nan nan\"},inplace=True)","1a8aefca":"(data.isnull().sum() \/ len(data)) * 100","0daa521f":"for i in [\"Mileage\",\"Engine\",\"Power\"]:\n    data[i] = data[i].apply(lambda x:x.split()[0])","d20f4790":"for i in [\"Mileage\",\"Engine\",\"Power\"]:\n    data[i].replace({\"nan\":\"0\"},inplace=True)\n    data[i].replace({\"null\":\"0\"},inplace=True)","30c5a95a":"for i in [\"Mileage\",\"Engine\",\"Power\"]:\n    data[i] = data[i].astype(\"float\")","915ba4d4":"for i in [\"Mileage\",\"Engine\",\"Power\"]:\n    data[i].replace({0:np.nan},inplace=True)","7fe7fc17":"data.isnull().sum() \/ len(data)*100","d0d8b337":"for i in [\"Mileage\",\"Engine\",\"Power\",\"Seats\"]:\n    data[i].fillna(data[i].median(),inplace=True)","ec47ebe1":"data.isnull().sum() \/ len(data)*100","747152e4":"num_cols = [\"Kilometers_Driven\",\"Mileage\",\"Engine\",\"Power\",\"Model_Age\",\"Seats\",\"Price\"]","02c5fd30":"plt.figure(figsize=(15,10))\nfor index,i in enumerate(num_cols):\n    plt.tight_layout(pad=2,h_pad=2)\n    plt.subplot(4,2,index+1)\n    sns.histplot(data[i])","9bbfd9e8":"plt.figure(figsize=(15,10))\nfor index,i in enumerate(num_cols):\n    plt.tight_layout(pad=2,h_pad=2)\n    plt.subplot(4,2,index+1)\n    sns.boxplot(data[i])","dc7ebe1d":"sns.histplot(data.Price)","e21ce787":"sns.boxplot(data.Price)","377b675b":"stats.skew(data.Price)","9856942d":"data.select_dtypes(exclude=np.number).nunique()","ad474555":"for i in data.select_dtypes(exclude=np.number).columns[1:-1]:\n    plt.figure()\n    sns.countplot(data[i])","fdf59268":"for i in data.select_dtypes(exclude=np.number).columns[1:-1]:\n    print(i,\"- % of data\")\n    print(data[i].value_counts(normalize=True))\n    print()","0fbd191d":"bar = exp.bar(data_frame=data,y=\"Location\",title=\"Distribution of Cars Across Locations\")\nbar.update_layout({\"title_x\":0.49})\nbar.update_xaxes({\"title_text\":\"Frequency of records\"})","dea4cca2":"bar = exp.bar(data_frame=data,y=\"Brand\",title=\"Distribution of Cars Across Brands\",height=1000)\nbar.update_layout({\"title_x\":0.49})\nbar.update_xaxes({\"title_text\":\"Frequency of records\"})","350e2819":"for i in [\"Seats\",\"Model_Age\"]:\n    print(i,\"- Count of unique levels\")\n    print(data[i].value_counts(normalize=True)*100)\n    print()","01cf2368":"data.skew()","6665dc80":"data.kurt()","ef50eba6":"for i in data.select_dtypes(include=np.number).columns:\n    print(\"Normality Probability of\",i)\n    print(\"p:\",stats.shapiro(data[i])[1])","7b3c98c0":"data.groupby(\"Model_Age\")[\"Price\"].mean().sort_values(ascending=False)","53103064":"data.groupby(\"Seats\")[\"Price\"].mean().sort_values(ascending=False)","f4759276":"for i in data.select_dtypes(exclude=np.number).columns[:-1]:\n    plt.figure()\n    data.groupby(i)[\"Price\"].mean().sort_values(ascending = True).plot(kind=\"barh\")\n    plt.xlabel(\"Mean Price in lakhs\")","d055f1c1":"plt.figure(figsize=(10,10))\ndata.groupby(\"Brand\")[\"Price\"].median().sort_values(ascending = True).plot(kind=\"barh\")\nplt.xlabel(\"Median Price in lakhs \")","f9757182":"plt.figure(figsize=(10,5))\ndata.groupby(\"Location\")[\"Price\"].median().sort_values(ascending = True).plot(kind=\"barh\")\nplt.xlabel(\"Median Price in lakhs \")","44bfbc62":"sns.boxplot(data.Owner_Type,y=data.Price)","821fcd41":"sns.boxplot(data.Transmission,y=data.Price)","2f12e28f":"sns.boxplot(data.Fuel_Type,y=data.Price)","fe87e9f8":"sns.boxplot(x=data.Seats,y=data.Price)","1e62678d":"data.groupby(\"Seats\")[\"Price\"].median()","03a29557":"columns = ['Kilometers_Driven', 'Mileage', 'Engine', 'Power',\n       'Model_Age']","a2b65e43":"for i in columns:\n    plt.figure()\n    sns.regplot(data[i],data.Price)\n    plt.xlabel(i)\n    plt.ylabel(\"Price\")","c7d9a020":"plt.figure(figsize=(10,7))\nsns.heatmap(data.corr(),annot=True,vmin = -1 , vmax=1,mask=np.triu(np.ones(data.corr().shape)))","060c48a8":"sns.pairplot(data)","cfb9c785":"data_copy_1 = data.copy()\ndata.drop(\"Kilometers_Driven\",axis=1,inplace=True)","79b041c1":"data1 = data.copy()","002dc55e":"data.head()","7e5bcc53":"data = pd.get_dummies(data,columns=[\"Fuel_Type\",\"Transmission\"])","02ca032b":"data.Owner_Type.unique()","b815b4f6":"for i in data.index:\n    if data.loc[i,\"Owner_Type\"] == \"First\":\n        data.loc[i,\"Owner_Type\"] = 1\n    if data.loc[i,\"Owner_Type\"] == \"Second\":\n        data.loc[i,\"Owner_Type\"] = 2\n    if data.loc[i,\"Owner_Type\"] == \"Third\":\n        data.loc[i,\"Owner_Type\"] = 3\n    if data.loc[i,\"Owner_Type\"] == \"Fourth & Above\":\n        data.loc[i,\"Owner_Type\"] = 4\n        ","b87dc2c0":"data.Owner_Type = data.Owner_Type.astype(\"int64\")","b9e68885":"data.Brand = data.Brand.map(data.Brand.value_counts(normalize=True))","a3ec949d":"data.Location = data.Location.map(data.groupby(\"Location\")[\"Price\"].median())","42681896":"data.head()","7aaac2cd":"data2 = data.copy()","726d31fe":"num_cols = [\"Location\",\"Owner_Type\",\"Mileage\",\"Engine\",\"Power\",\"Seats\",\"Brand\",\"Model_Age\"]","72a453dc":"X = data.drop(\"Price\",axis=1).copy()\nX_sc = data.drop(\"Price\",axis=1).copy()\nY = data.Price","e09ee29c":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nSC = StandardScaler()\nSC = StandardScaler().fit(X_sc[list(num_cols)])\nX_sc[list(num_cols)] = SC.transform(X_sc[list(num_cols)])","3e6b9588":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.30,random_state=20)\nx_train_sc,x_test_sc,y_train,y_test = train_test_split(X_sc,Y,test_size=0.30,random_state=20)","a5da3ab5":"from mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import  mean_squared_error , r2_score\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","a9271f02":"Results = pd.DataFrame(columns=[\"Model\",\"Train R2\",\"Test R2\",\"Test RMSE\",\"Variance\"])","2da3f8f1":"knn = KNeighborsRegressor().fit(x_train_sc,y_train)\nknn_train_pred = knn.predict(x_train_sc)\nknn_test_pred = knn.predict(x_test_sc)\nr2 = r2_score(y_test,knn_test_pred)\nr2_train = r2_score(y_train,knn_train_pred)\nrmse = np.sqrt(mean_squared_error(y_test,knn_test_pred))\nvariance = r2_train - r2\nResults = Results.append({\"Model\":\"K-Nearest Neighbors\",\"Train R2\":r2_train,\"Test R2\":r2,\"Test RMSE\":rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2:\",r2)\nprint(\"RMSE:\",rmse)","16c2c1d5":"Results","388fecd1":"svr = SVR().fit(x_train_sc,y_train)\nsvr_train_pred = svr.predict(x_train_sc)\nsvr_test_pred = svr.predict(x_test_sc)\nr2 = r2_score(y_test,svr_test_pred)\nr2_train = r2_score(y_train,svr_train_pred)\nrmse = np.sqrt(mean_squared_error(y_test,svr_test_pred))\nvariance = r2_train - r2\nResults = Results.append({\"Model\":\"Support Vector Machine\",\"Train R2\":r2_train,\"Test R2\":r2,\"Test RMSE\":rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2:\",r2)\nprint(\"RMSE:\",rmse)","a6932e3a":"Results","679eb6f0":"dt = DecisionTreeRegressor().fit(x_train,y_train)\ndt_train_pred = dt.predict(x_train)\ndt_test_pred = dt.predict(x_test)\nr2 = r2_score(y_test,dt_test_pred)\nr2_train = r2_score(y_train,dt_train_pred)\nrmse = np.sqrt(mean_squared_error(y_test,dt_test_pred))\nvariance = r2_train - r2\nResults = Results.append({\"Model\":\"Decision Tree\",\"Train R2\":r2_train,\"Test R2\":r2,\"Test RMSE\":rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2:\",r2)\nprint(\"RMSE:\",rmse)","f18f15f0":"Results","c70759eb":"rf = RandomForestRegressor().fit(x_train,y_train)\nrf_train_pred = rf.predict(x_train)\nrf_test_pred = rf.predict(x_test)\nr2 = r2_score(y_test,rf_test_pred)\nr2_train = r2_score(y_train,rf_train_pred)\nrmse = np.sqrt(mean_squared_error(y_test,rf_test_pred))\nvariance = r2_train - r2\nResults = Results.append({\"Model\":\"Random Forest\",\"Train R2\":r2_train,\"Test R2\":r2,\"Test RMSE\":rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2:\",r2)\nprint(\"RMSE:\",rmse)\n","858878ce":"Results","d14e4643":"knn_model = KNeighborsRegressor()\nsfs_knn = SequentialFeatureSelector(knn_model,k_features=\"best\",cv=5,scoring=\"neg_mean_squared_error\").fit(x_train_sc,y_train)\nknn_model = KNeighborsRegressor().fit(x_train_sc[list(sfs_knn.k_feature_names_)],y_train)\ntrain_prediction = knn_model.predict(x_train_sc[list(sfs_knn.k_feature_names_)])\ntest_prediction = knn_model.predict(x_test_sc[list(sfs_knn.k_feature_names_)])\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"knn using Forward Feature Elimination\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\nprint(\"RMSE on test:\",test_rmse)","3080a8e3":"Results","b8da02cf":"SVR_model = SVR()\nsfs_svr = SequentialFeatureSelector(SVR_model,k_features=\"best\",cv=3,scoring=\"neg_mean_squared_error\").fit(x_train_sc,y_train)\nSVR_model = SVR().fit(x_train_sc[list(sfs_svr.k_feature_names_)],y_train)\ntrain_prediction = SVR_model.predict(x_train_sc[list(sfs_svr.k_feature_names_)])\ntest_prediction = SVR_model.predict(x_test_sc[list(sfs_svr.k_feature_names_)])\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"Support Vector Machine using Forward Feature Elimination\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\n","fe2e28d5":"Results","cc7b2893":"DT_model = DecisionTreeRegressor()\nsfs_dt = SequentialFeatureSelector(DT_model,k_features=\"best\",cv=5,scoring=\"neg_mean_squared_error\").fit(x_train,y_train)\nDT_model = DecisionTreeRegressor().fit(x_train[list(sfs_dt.k_feature_names_)],y_train)\ntrain_prediction = DT_model.predict(x_train[list(sfs_dt.k_feature_names_)])\ntest_prediction = DT_model.predict(x_test[list(sfs_dt.k_feature_names_)])\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"Decision Tree using Forward Feature Elimination\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\nprint(\"RMSE on test:\",test_rmse)\n","3f1f5624":"Results","cf945410":"RF_model = RandomForestRegressor()\nsfs_rf = SequentialFeatureSelector(RF_model,k_features=\"best\",cv=5,scoring=\"neg_mean_squared_error\").fit(x_train,y_train)\nRF_model = RandomForestRegressor().fit(x_train[list(sfs_rf.k_feature_names_)],y_train)\ntrain_prediction = RF_model.predict(x_train[list(sfs_rf.k_feature_names_)])\ntest_prediction = RF_model.predict(x_test[list(sfs_rf.k_feature_names_)])\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"Random Forest Regressor using Forward Feature Elimination\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\nprint(\"RMSE on test:\",test_rmse)\n","0414c8f1":"Results","780d0a56":"parameters={\"n_neighbors\":[3,4,5,6,7,8,9,10],\"metric\":[\"minkowski\",\"euclidean\",\"manhattan\",\"chebyshev\"],\n            \"weights\":[\"uniform\",\"distance\"]}","6fb55efc":"Knn_model_grid = KNeighborsRegressor()\ngrid_knn = GridSearchCV(Knn_model_grid,param_grid=parameters,scoring=\"neg_mean_squared_error\",cv=5).fit(x_train_sc[list(sfs_knn.k_feature_names_)],y_train)","ce448665":"grid_knn.best_params_","11d4029a":"Knn_model_grid = KNeighborsRegressor(n_neighbors=10,weights=\"distance\",metric=\"manhattan\").fit(x_train_sc[list(sfs_knn.k_feature_names_)],y_train)\ntrain_prediction = Knn_model_grid.predict(x_train_sc[list(sfs_knn.k_feature_names_)])\ntest_prediction = Knn_model_grid.predict(x_test_sc[list(sfs_knn.k_feature_names_)])\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"Knn using parameters from Grid Search\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\nprint(\"RMSE on test:\",test_rmse)\n","ec935140":"Results","a24187be":"from sklearn.ensemble import BaggingRegressor","3e846542":"Bag_Knn_model = KNeighborsRegressor(n_neighbors=10,weights=\"distance\",metric=\"manhattan\")\nX = x_train_sc[list(sfs_knn.k_feature_names_)]\nX_test = x_test_sc[list(sfs_knn.k_feature_names_)]\ny = y_train\nBagged_Knn = BaggingRegressor(base_estimator=Bag_Knn_model,random_state=0,max_features=11,bootstrap_features=True).fit(X,y)\ntrain_prediction = Bagged_Knn.predict(X)\ntest_prediction = Bagged_Knn.predict(X_test)\nr2_train = r2_score(y_train,train_prediction)\nr2_test = r2_score(y_test,test_prediction)\ntest_rmse = np.sqrt(mean_squared_error(y_test,test_prediction))\nvariance = r2_train - r2_test\nResults = Results.append({\"Model\":\"Bagged_Knn with tuned knn\",\"Train R2\":r2_train,\"Test R2\":r2_test,\"Test RMSE\":test_rmse,\"Variance\":variance},ignore_index=True)\nprint(\"R2 on test:\",r2_test)\nprint(\"RMSE on test:\",test_rmse)\n","4386fdb4":"Results","59200090":"Bag_Knn_model = KNeighborsRegressor(n_neighbors=10,weights=\"distance\",metric=\"manhattan\")\nfinal_X = x_train_sc[list(sfs_knn.k_feature_names_)]\nfinal_y = y_train\nfinal_model = BaggingRegressor(base_estimator=Bag_Knn_model,random_state=0,max_features=11,bootstrap_features=True).fit(final_X,final_y)","9bc69569":"# EDA - Preprocessing\n### Encoding","c33f10a3":"#### NOTE - \n- For non-linear regresion models like knn, svm, we need to have all the variables in a numerical nature. Hence, we will convert the categorical variables in numerical through encoding\n- During EDA, we observed that for transmission and fuel type, we had 2 and 5 categories respectively. Hence they can be one-hot-encoded.\n- For owner type, we can convert it into numerical.\n- For location and brand, we cannot afford to do one hot encoding due to more than 10 categories for each variable.\n- For location, we will use target encoding using median as there are many cities with similar prices and this can help our models generalise better.\n- For brands, we will use frequency encoding using median as we saw that as the proportion of brands in the dataset are inversely related to the prices of cars. The less common the availability of brand, the more its price ( We are using median due to presence of extremely high outliers in price )","f54b02d3":"##### Inferences - \n- Majority of vehicles are from Mumbai followed by Hyderabad.\n- Cars from Ahmedabad and Bangalore have the lowest representation","9ae66e96":"##### Infereneces -\n- Understandably, premium and luxury cars have a higher average price.\n- We can see that common consumer brands like Honda, Maruti, Hyundai, Ford , Chevrolet have median prices of around 5 lakhs or less , which is affordable for medium income class people.\n- As we move towards premium or high end brands like BMW, Audi, Volvo, Benz, etc, the median prices increases to close to 20-25 lakhs.\n- Luxury car brands like Bentley, Porsche and Lamborghini have medians prices > 50 lakhs.","5a8c060f":"# MODEL BUILDING AND SAVING","b6071516":"# EDA - Preprocessing \n### Scaling","c528781d":"#### BI-VARIATE ANALYSIS \nWe are going to check the relationship of our target variable with other factors","14bd3ff8":"#### Infereneces - \n- After using forward feature selection to get the best set of features for our model, we observed that for Decision tree and random forest, our models have produced near 1 r2 scores on train data. However, their is test r2 scores are significantly lower and the drastic difference in performances can also be seen in high rmse on test data and high variance. These are indication that the decision tree and random forest models are overfitting the data. Hence they won't be useful.\n- Support Vector Machine is producing a better performance on test i.e 0.83 compared to train 0.77. However, it produces very high root mean squared error compared to other model.\n- Compared to SVM, K nearest neighbors model has produced the lowest variance in train and test scores i.e 0.019, indicating its not overfitting and generalizing very well.\n- Moreoever, it is also performing very well on both train (0.90) and test (0.88). \n\nHence, we can continue with knn using the best features but also attempt to do hyperparameter tuning","bcf65950":"# MODEL SELECTION \/ FEATURE SELECTION","eee90fa2":"# HYPER-PARAMETER TUNING and VALIDATION \/ CHECKING FOR OVERFIT","e514942e":"##### Inferences - \n- Price of the car has a very positive and strong relationship with engine and power of the car. Higher the power \/ engine, higher the price of cars.\n- There appears to be a  weak but visible negative relationship between age of model and price. Understandably, if the model has become old, the price of the car would drop. \n- There also appears to be a weak but visible negative relationship between mileage and price. If the car provides more mileage its price drop. However, it needs to be checked.\n- We can see there is no apparent relationship between km driven price of cars.","a826e683":"#### Infereneces - \nBased on the 4 models created, we can observe - \n- Support Vector Machine has the lowest performance on the training dataset i.e 0.72 in R2 whereas the Decision tree has the lowest performance in test dataset 0.79.\n- The Decision Tree has reported the highest level of overfitting across all models, with a variance of 0.22 in train and test scores.\n- Random Forest produced a near perfect performance on train data and significantly lower but good performance on test. However, a variance of 10% in results suggest that Random Forest model has also overfitted the model. \n- K nearest neighbors has performed well on both dataset i.e 0.87 and a neglible variance, indicating the model is generalising well. However, it produces a high test mape compared to other models.\n\nWe will use sequential feature selector and check if choosing the best features will have an impact on the performances.","29dd6c32":"##### Inferences - \n- Primarily we have 84% of 5 seater cars in the dataset, followed by 11% of 7 seater vehicles.\n- In general, we have many vehicles whose models are between 4-10 years old. ","483dfb48":"##### Inferences - \nBased on a couple of statistical tests, we can conclude:\n- None of the numerical variables i.e km, mileage, engine, power, age and price, are normally distributed.\n- Except Mileage, all other numerical variables have very high skewness.\n- Km has the highest amount of skewness due to a couple of very extreme values.\n","49ec89bd":"# Understanding the data structure","3a37a362":"##### Inferences - \n- Except mileage, all our numerical variables right positively skewed, indicating presence of large values.\n- We can observe that we have outliers in almost all numerical variables.\n- We wont consider seats as it is a discrete variable and based on general knowledge, most cars have 5 seats, with others having less than or more than 5 could be SUVs or mini cars.\n- The variable price, is highly skewed and has many outliers.","301946b9":"##### Inferences - \n- First hand cars have a higher median price than others.\n- Automatic vehicles have a median price of around 16-17 lakhs.\n- Cars with 2 seats tend to have a very high median price of close to 60 lakhs, way higher than other other. \n- Diesel cars have the highest median price of close to 10 lakhs.\n- Though 7 seats cars have a median price close to 12 lakhs, we can see many 7 seaters also ranging from 40 to 80 lakhs.","7a127b63":"#### UNIVARIATE ANALYSIS - \nWe are going to check the distribution and representation of different variables individually.","c2364d7a":"# Problem Statement","8c304015":"##### Infereneces - \n- Electric cars, since they are the latest and environment friendly vehicles, have a high average price, similar to diesel variant.\n- Cars from Coimbatore, Bangalore and Kochi have the highest average price. \n- Average price of automatic cars is close to 20 lakhs compared to the 5 lakh for manual.\n- Understantably, First hand cars fetch an average price of close to 10 lakhs, more than 8 lakhs for 2nd hand bearing in mind the model.","d89178c4":"# Data Description","dfa95f70":"##### Inferences -\n- Almost 86% of values in the new price column is missing. Replacing missing values here wont be useful. TO BE DROPPED\n- In other variables, we have less than 1% of values missing. We can perform imputations over here.","adadb8a9":"##### Inferences - \n- Latest model variant of cars, released less than 6 years ago, have a higher average price above 10 lakhs.\n- Apart from usual 4 seaters, Cars with very less seats (presumably supercars) and many seats (presumably SUVs) have high average prices.","3857150c":"# EDA - \n###  EXPLORATION :","c98d82c2":"# EDA\n### PREPROCESSING : Missing Value Treatment","6150f1b5":"##### Inferences - \n- Financially feasible \/ Consumer centric brands like Maruti and Hyundai have a large representation in the data (more than 1000 records each ). \n- Other financially feasible brands like Honda, Ford, Skoda and Mahindra also have sufficient representation.\n- Amongst luxury \/ premium brands, Mercedez , BMI and Audi have the highest representation.\n- This could indicate not only representation of cars in dataset but also the how many people actually own cars from these brands on the whole. We could use this proportion later on.","169a5b77":"# Data Cleaning","82db6110":"##### Inferences - \n- Petrol (53%) and Diesel(45%) cars are primarily represented in the dataset.\n- Other fuel type present but with very neglibile representation are lpg, cng and electric.\n- Predominant share of cars are manual ( close to 4200 in count )\n- Almost 80% ( close to 5000 ) cars in the dataset are first hand, with around 15-16% representation of second hand vehicles.","e4e4f30c":"##### Inferences - \n- Cars in Coimbatore have the highest median price i.e 8 lakhs.\n- We can see many cluster of cities where median prices of cars are similar.\n- Price of vehicles in Bangalore and Kochi are similar.\n- Mumbai, Hyderabad and Ahmedabad have similar prices close to 6 lakhs.\n- Pune and Jaipur have median prices around 4.2-4.3 lakhs\n- We can use this information to make process of predicting prices more easy for models as many cities have similar car price.","62c9835a":"##### Inferences - \n- On Using grid search, a set of best parameters were produced. \n- Using the parameters, we created the knn model along with features produced from foward feature selection.\n- We gained a r2 of 0.99 on train and 0.90 on test, indicating the model is performing better with new parameters found.\n- However, a variance is close to 9% in variance could indicate some level of overfitting.\n- But, our test rmse has reduced from 0.363 to 0.333 and mean absolute percentage error has reduced from 0.20 to 0.189, indicating that even though the model might overfit a little, it is still able to make produce lesser error on test data compared to previous model.\n\nWe can also try bagging multiple Knn models and see if it is able to reduce variance and improve accuracy","70cb9b8c":"##### Inferences - \n- Our Bagged knn model with tuned parameters and best features is now giving a train r2 of 0.9577 and a test r2 of 0.905.\n- A reduction in variance from almost 9% to 5% indicates that overfitting has been reduced well.\n- The test rsme has also reduced from 3.33 to 3.24.\nThe bagged knn looks like the best performing model based on the performance and comparison of metrics across multiple models.","76ff21ba":"Our goal is to make a machine learning model that can predict the price of the user car. ","0466940b":"#### Inference - \n- Most of the cars in the dataset have a resale price around 3-15 lakhs\n- There are a group of cars that have been listed on the website whose prices are very high.\n- These could be premium brand vehicles.\n- These prices have been identified as outliers.\n- Also, the skewness value of the prices of vehicles is 3.33, indicating very high level of positive skewness.\n- The presence of these kind of prices could affect a linear regression model performance. ","0ae548a8":"### Final Insights from EDA - \n\nSome very important insights about the distributions of the data revealed the following - \n- Most of the numericl independent features like Engine and Power, have very high skewness due to presence of many outliers. \n- However, these outliers aren't any error or unnatural phenomenas. These might belong to other premium brand or sport cars which have very special features. We do not want to remove these features as they will reduce the predicting power of our model. \n- The target column price, has extremely high amount of skewness due to the presence of many outliers. However, we cannot afford  to alter and drop these values as these belong to premium brand or other high end vehicles. \n- Keeping such values however, could lead to high amount of skewness in residuals and a lot of non-linearity issues. \n- For such reasons, we will not use linear models. \n- We will try non-linear models.\n\nBased on the other analysis done to find relationship between price and other features, we derived the following significant insights :\n- Amongst numerical variables, Engine, Power, Mileage and Model Age could be useful in predicting price as they have shown relationship with price.\n- We can also observe that the seats of the car don't have any relation with its price. However, 2 seater cars do have a significantly higher median price than other cars. These kind of cars could be the sports or premier cars.\n- Also, a 7 seater car tends to have a high median price compared to other vehicles.\n- Hence, we can create new feature for seats based on these insights.\n- We noticed from correlations and scatterplots that kilometers covered doesn't have any effect on vehicle price. Hence, we can drop it.\n- We noticed that automatic cars are more highly priced compared to vehicles with manual transmission.\n- Diesel and Electric are again much higher priced in terms of median price compared to other vehicles.\n- As we move from consumer \/ common car brands to premium \/ high end brands, the median prices of cars increases.\n- We can see the price of cars are similar for many cluster of cities.\n- There is also a relationship between no. of owners and car price i.e if the car has had many owners (including present and past), it price could actually be lower.  ","f527393f":"# Importing packages and dataset","dc98f2b1":"- Name : The brand and model of the car.\n- Location : The location in which the car is being sold or is available for purchase.\n- Year : The year or edition of the model.\n- Kilometers Driven : The total kilometres driven in the car by the previous owner(s) in KM.\n- Fuel Type : The type of fuel used by the car. (Petrol, Diesel, Electric, CNG, LPG)\n- Transmission : The type of transmission used by the car. (Automatic \/ Manual)\n- Owner Type : Whether the ownership is Firsthand, Second hand or other.\n- Mileage : The standard mileage offered by the car company in kmpl or km\/kg\n- Engine : The displacement volume of the engine in CC.\n- Power : The maximum power of the engine in bhp\n- Seats : The number of seats in the car.\n- Price : The price of the used car in INR Lakhs.","3aadcc94":"#### MULTIVARIATE ANALYSIS","264e763b":"##### Inferences - \n- Majority of our vehicles have a mileage of 15-23.\n- Most of the engines in vehciles are around 1000-1500cc\n- Majority of the vehicles have been used by the present owner for mainly around 5-10 years.\n- Predominant share of cars have around 5 seats.\n- Most of the cars of a power of around 800-1000bhp\n- In terms of Price, our target, we observed that most of the cars in the dataset are price around 0-20 lakhs.","3639d171":"##### Inferences - \n- In respect to price, we can power and engine are highly correlated with it. \n- We can also see mileage having a below average but visible negative relationship with price\n- On understanding more on mileage, we can see its not correlated with any other.\n- However, when considering engine and power, they have a very strong positive correlation with each other, indicating possible signs of multicollinearity, which might arise later."}}