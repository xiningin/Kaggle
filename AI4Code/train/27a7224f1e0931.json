{"cell_type":{"f292f86f":"code","d356254f":"code","7b9173d2":"code","f985235c":"code","8ff30960":"code","409f09c6":"code","6d95ecc0":"code","26338633":"code","4b212943":"code","d22d8c4e":"markdown","9e911e58":"markdown","820e593e":"markdown","f1906022":"markdown","3c9a7ad7":"markdown","88445022":"markdown","71213038":"markdown","1f8e65e0":"markdown","93b12de1":"markdown"},"source":{"f292f86f":"import pandas\nimport numpy as np\n\ntrain_set = pandas.read_csv(\"..\/input\/train.csv\")\ntest_set = pandas.read_csv(\"..\/input\/test.csv\")\ntrain_set = train_set.drop('id',axis=1)\nprint(train_set.describe())","d356254f":"train_set['type'], categories = train_set['type'].factorize()\nprint(train_set.describe())","7b9173d2":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,10))\naux_plot = fig.add_subplot(111)\nfig.colorbar(aux_plot.matshow(train_set.corr()))\n\naux_plot.set_xticklabels(train_set.columns)\naux_plot.set_yticklabels(train_set.columns)\n\nplt.show()","f985235c":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CreateExtraFeatures(BaseEstimator,TransformerMixin):\n    def __init__(self):pass\n\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        X['hair_soul'] = X['hair_length'] * X['has_soul']\n        X['flesh_soul'] = X['rotting_flesh'] * X['has_soul']\n        return np.c_[X]","8ff30960":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\npipeline_num = Pipeline([\n    (\"extra_feat\",CreateExtraFeatures())\n])\n\npipeline_cat = Pipeline([\n    (\"categorical_encoder\", OneHotEncoder(sparse=False))\n])\n\nfrom sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion([\n    (\"pip,num\",pipeline_num),\n    (\"pip_cat\",pipeline_cat)\n])","409f09c6":"X_train = train_set.drop('type',axis=1)\ny_train = train_set.get('type')\nX_train= X_train.append(test_set)\n\nnum_attributes = [\"bone_length\",\"rotting_flesh\",\"hair_length\",\"has_soul\"]\ncat_attributes = [\"color\"]\nX_train= full_pipeline.fit_transform(X_train[num_attributes],X_train[cat_attributes].values)\n\nX_test = X_train[371:]\nX_train = X_train[:371]","6d95ecc0":"from sklearn.neural_network import MLPClassifier\n\nnn_clf = MLPClassifier(max_iter=3000)\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = [{\"hidden_layer_sizes\":range(3,20), \"activation\":['identity', 'logistic', 'tanh', 'relu'], \"solver\":[\"lbfgs\",\"sgd\",\"adam\"],\"learning_rate\":[\"adaptive\"]}]\ngrid_search = GridSearchCV(nn_clf,param_grid=grid_params,cv=3,verbose=0)\n","26338633":"grid_search.fit(X_train,y_train)\n\nprint(grid_search.best_estimator_)\nprint(grid_search.best_score_)","4b212943":"y_pred = grid_search.predict(X_test)\n\nsubmissions = pandas.DataFrame(y_pred, index=test_set.id,columns=[\"type\"])\nsubmissions[\"type\"] = categories[submissions[\"type\"]]\nsubmissions.to_csv('.\/submission.csv', index=True)","d22d8c4e":"Finally, having the dataset prepared we can now start thinking about our neural network. \nScikit-Learn as an easy to use implementation of a neural network, so it will be our choice. \nTo define a most capable combination of Hyperparameters it is a good practice to use something like GridSearch with a cross-validation evaluation to ensure that the best model is chosen.\n","9e911e58":"Hi, this is my first notebook. I'm trying to help newbies like myself to get started with machine learning and simple classifications problems. \n\nLet's start by importing the dataset, and verify if there is any column with missing values. ","820e593e":"As you can see the algorithm has around 74.9% of accuracy.\n \nLast step is to create the submission file.","f1906022":"This image is really useful and will help us to analyze features and their impact on the 'type' column. \nAs you can see the 'type' column has a high value of negative correlation with columns 'color', 'has_soul' and 'rotting_flesh'. Although the correlation with the 'hair_length' is not very big. \n\nAnalyzing the correlation between other features it's possible to see that 'has_soul' and 'hair_length' have an interesting value as well as the column 'has_soul' with 'rotting_flesh'. \nLets then try to extract some new features from here. We can, for example, multiply 'has_soul' times 'hair_length' to obtain a new feature much more correlated to the 'type' column.\n\nLets then create a class responsible for this process.","3c9a7ad7":"Before calling the fit function of the pipeline we should notice that we want this to apply to the test set too. So we are going to join them, knowing that the first 371 set of elements are from the training set.","88445022":"As you can see the 'type' column now appears as a numerical set of values. \n\nYou can also see that a second variable called categories was used. This variable contains the information that will allow us to revert this factorization process. \n\nTo gain a little more insights about the data we can make a simple plot to help us.","71213038":"We can then fit it to our dataset and find the best combinations of parameters and the best score. \nLet's see!","1f8e65e0":"As you can see the count is the same for every numerical column in the dataset, and the range of values is between 0 and 1. \nThe dataset given has the label column called 'type' which has values who belong to a category. Our next step is to convert string categories in integer values.","93b12de1":"In this class we are creating 2 new features, combining 3 existing ones. \n\nAs we want to be able to easily append more transformers from Scikit-Learn it is a good approach to create a pipeline. In this case, 3 pipelines, one for numerical features, other for categorical features and a third to join both of this pipelines. \n\nFor numerical features, it is enough to use only our custom transformer, since there aren't missing values and the data is already between 0 and 1. \n\nFor categorical features we will use OneHotEncoder from Scikit-Learn, to factorize string values and to create a dense matrix with values assigned to 1 being Hot. "}}