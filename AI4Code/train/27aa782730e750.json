{"cell_type":{"7fea284a":"code","4fb4bd6f":"code","b4726e89":"code","f50a6c2d":"code","aa1f36dd":"code","b98daa00":"code","14ad07fe":"code","69c5ad6f":"code","4cfbfc12":"code","2bd63fa6":"code","c55874ed":"code","877f8b43":"code","3c940005":"code","5eb26f48":"code","512206e5":"code","6d41290f":"code","e69be3b3":"code","297b87fa":"code","a155e8c0":"code","f2645303":"code","fea35e94":"code","cd5c9823":"code","5e6df92c":"code","93cf92c5":"code","c1b9bf03":"code","5f385758":"code","78e1ed3a":"markdown","e1063ff1":"markdown","736c9593":"markdown","af3c33d6":"markdown","06c5388e":"markdown","3455cd20":"markdown","1dd12a5f":"markdown","2791fa06":"markdown","570f5f2d":"markdown","516c480f":"markdown","e1c69d78":"markdown","b782933e":"markdown","e684a120":"markdown","b155d2e0":"markdown"},"source":{"7fea284a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\n\n\n","4fb4bd6f":"multiplec = pd.read_csv('..\/input\/multipleChoiceResponses.csv')\n","b4726e89":"mkeys=multiplec.keys()\n#excludes the columns corresponding to text explaining\nmultiplec=multiplec.drop([ mkeys[2], mkeys[8], mkeys[10],mkeys[23], mkeys[24], mkeys[25],mkeys[26], mkeys[44], mkeys[56], mkeys[64], mkeys[83], mkeys[85], \n                       mkeys[87], mkeys[107], mkeys[109], mkeys[123],mkeys[125], mkeys[150], \n                          mkeys[194], mkeys[223], mkeys[249],mkeys[262], mkeys[264],mkeys[276],\n                      mkeys[304],mkeys[306], mkeys[329], mkeys[341], mkeys[371], mkeys[385], mkeys[394],mkeys[283]],axis=1)\nmultiplec.keys()","f50a6c2d":"multi2=multiplec.iloc[1:,:]\n","aa1f36dd":"#since the first column is actually composed of integers, we do not include it in the encoding\nobj_df = multi2.select_dtypes(include=['object']).copy()\nobj_df = obj_df.drop(mkeys[0],axis=1)","b98daa00":"#One-Hot encoding using pandas\nobj_df=pd.get_dummies(obj_df)\nnp.shape(obj_df)","14ad07fe":"#scaling the time column\nx = multi2.iloc[:,0].values.astype(float) \nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x.reshape(-1,1))\ndf_scaled = pd.DataFrame(x_scaled,columns=['Time from Start to Finish (seconds)'])\n#adjusting the indices\ndf_scaled.index+=1","69c5ad6f":"#Creating our encoded data frame\nencoded_data = pd.concat([df_scaled,obj_df,multi2.select_dtypes(exclude=['object']).copy()],axis=1)\nprint(np.shape(encoded_data))\nencoded_data.head()","4cfbfc12":"#changes nan values two -1\nencoded_data = encoded_data.fillna(value=-1)\nenc_keys=encoded_data.keys()","2bd63fa6":"#pca to reduce dimentionality\npca = PCA(n_components=10)\npca.fit(encoded_data)\nvar=pca.explained_variance_\nn_com=np.linspace(1,10,10)\nplt.scatter(n_com,var\/np.sum(var),marker='x')\nplt.xlabel('Eigenvectors')\nplt.ylabel('Variance')\nprint('First eigen-vector var fraction:', var[0]\/np.sum(var), ' second var', var[1]\/np.sum(var))\ndata_proj= pca.transform(encoded_data)","c55874ed":"#mean inertia for the clusters formed\ninert = []\n#elbow method\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=10)\n    kmeans.fit(data_proj)\n    inert.append(kmeans.inertia_)\nplt.plot(range(1,11),inert)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()","877f8b43":"#looking at the 2 cluster case\nn_c=2\nkmeans_pca = KMeans(n_clusters=n_c,init='k-means++',max_iter=500,n_init=10,random_state=0) \nclus_index = kmeans_pca.fit_predict(data_proj) #saves the cluster labels in the positions of each observation\n\n#Scatter plots of the projections over the first two principal components\nfor i in range(0,n_c):\n    plt.scatter(np.asarray(data_proj)[clus_index == i, 0], np.asarray(data_proj)[clus_index == i,1], alpha=0.7)\n\nplt.scatter(kmeans_pca.cluster_centers_[:,0], kmeans_pca.cluster_centers_[:,1],marker='x',s=100,c='black')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\n\nplt.title('Clusters')\n","3c940005":"\ndef get_feat(cl_num,cl_index):\n    #Creates a bar plot that show the coefficients associated with the 40 most relevant features that the\n    #SVM learns \n    #cl_num: the current cluster label\n    #cl_index: an array containing the cluster labels obtained from k-means\n    print(1)\n    labels=np.zeros(len(cl_index))\n#gets positions of the zero cluster members\n    ii=np.where(cl_index==cl_num)\n    labels[ii]=1\n    labels=pd.DataFrame(labels)\n    supp = SVC(kernel='linear')\n    supp.fit(encoded_data,np.ravel(labels))\n    print(2)\n    \n    coef=(abs(supp.coef_.ravel()))\n#The coefficients returned correspond to the features, thus the largest coeficients \n#show the most important features. We can take a look at the first 20 coeficients\n    top_coef= np.argsort(coef)[-40:]\n    feature_names=encoded_data.keys()[top_coef]\n    #creates plots\n    plt.figure(figsize=(13,13))\n    plt.barh(range(len(feature_names)), coef[top_coef], align='center')\n    plt.yticks(range(len(feature_names)), feature_names)\n    \n    plt.savefig(str(cl_num)+'.pdf')\n    \n    print('The most relevant feature is: ', encoded_data.keys()[top_coef][-1])\n    \n    return supp, labels, feature_names\n\n\ndef K_fold(clf,K,X,Y):\n    #in case we need to test our model accuracy\n    #clf: the trained model\n    #K: subdivisions of the training data\n    #X: features\n\n    print('ya')\n    kf = KFold(n_splits=K)\n    lin_model = clf\n\n    #using the confusion matrix\n    lin_ERR = []\n    l_pre=[]\n    l_rec=[]\n    print('ya2')\n\n    for train_index, test_index in kf.split(X):\n        print(1)\n        xk_train, xk_test = X.iloc[train_index], X.iloc[test_index]\n        print(2)\n        yk_train, yk_test = Y.iloc[train_index], Y.iloc[test_index]\n        print(3)\n        lin_model.fit(xk_train, yk_train)\n        print(4)\n        lin_pred=lin_model.predict(xk_test)\n        print(5)\n        lin_conf=confusion_matrix(yk_test,lin_pred)\n        \n        #precision recal and f1 values\n        l_pre.append(lin_conf[1,1]\/(lin_conf[1,1]+lin_conf[0,1]))\n        l_rec.append(lin_conf[1,1]\/(lin_conf[1,1]+lin_conf[1,0]))\n\n\n        lin_ERR.append((lin_conf[0,1]+lin_conf[1,0])\/np.sum(lin_conf))\n        \n    return np.mean(lin_ERR), np.mean(l_pre), np.mean(l_rec)","5eb26f48":"#bar plot for cluster 0\nmodel1, lab1, feat1 =get_feat(0,clus_index)","512206e5":"#error, prec, rec = K_fold(model_used,3, encoded_data, labs)","6d41290f":"cluster0=encoded_data.iloc[clus_index==0][feat1]\ncluster0.head()","e69be3b3":"#then, we can make a new data ser with these features\ncluster1=encoded_data.iloc[clus_index==1][feat1]\ncluster1.head()","297b87fa":"cluster0['Q41_Part_3_Very important'].value_counts()","a155e8c0":"cluster1['Q41_Part_3_Very important'].value_counts()","f2645303":"hist0=np.sum(cluster0, axis=0)\nhist1=np.sum(cluster1, axis=0)\nfig, ax = plt.subplots(1,2, figsize=(13,5))\nax[0].bar(np.arange(1, len(hist0)+1),hist0)\nax[0].set_title('Cluster 0')\nax[1].bar(np.arange(1, len(hist1)+1),hist1)\nax[1].set_title('Cluster 1')\n\nprint('.')","fea35e94":"max0=np.where(max(hist0)==hist0)\nmin0=np.where(min(hist0)==hist0)\n\nmax1=np.where(max(hist1)==hist1)\nmin1=np.where(min(hist1)==hist1)\n\nprint('Max values in cluster 0')\nprint(cluster0.keys()[max0])\n\nprint('Max values in cluster 1')\nprint(cluster1.keys()[max1])\n","cd5c9823":"print('Do you use python on a regular basis? -Cluster 0')\nprint(cluster0['Q16_Part_1_Python'].value_counts())\n\nprint('Do you use python on a regular basis? -Cluster 1')\nprint(cluster1['Q16_Part_1_Python'].value_counts())\n\nprint('Would you recomend Python to an  aspiring data scientist? -Cluster0')\nprint(cluster0['Q18_Python'].value_counts())\n\nprint('Would you recomend Python to an  aspiring data scientist? -Cluster1')\nprint(cluster1['Q18_Python'].value_counts())\n","5e6df92c":"#gets tsome relevant questions\nq_keys=[mkeys[372],mkeys[355], mkeys[342], mkeys[263],mkeys[108],mkeys[124],mkeys[129], mkeys[128], \n        mkeys[86],mkeys[84],]\nquestions=multiplec.head()[q_keys].iloc[0,:]\ndata0=multi2.iloc[clus_index==0][q_keys]\ndata0 = data0.fillna(value=-1)\n\ndata1=multi2.iloc[clus_index==1][q_keys]\ndata1 = data1.fillna(value=-1)","93cf92c5":"print(questions)","c1b9bf03":"print('Opininons about data and programing languages- Cluster 0')\nprint('.')\ni=0\nfor k in q_keys:\n    col=data0[k]\n    print(questions[i])\n    print('_____ ', col.value_counts().keys()[0])\n    print('_____ ',col.value_counts().keys()[1])\n    print('.')\n    i+=1","5f385758":"print('Opininons about data and programing languages- Cluster 1')\nprint('.')\ni=0\nfor k in q_keys:\n    col=data1[k]\n    print(questions[i])\n    print('_____ ', col.value_counts().keys()[0])\n    print('_____ ',col.value_counts().keys()[1])\n    print('.')\n    i+=1","78e1ed3a":"Now, using the projected data set data_proj we are going to attempt and create some clusters. Using \nthe elbow method, we can determine that 2 clusters are a good starting point. These two clusters are then plotted, with their respective centroids. ","e1063ff1":"This results imply that the cluster 1 is composed of people with more\nexperience\/ training that those in cluster 0. In addition, it would seem that, those in cluster 0 where not presented with a lot of quetions about types of data and programing languages (this is infered from the large number of -1 answers). It is worth noting that python is popular with experienced and inexperienced programers. Another interesting insight is that most people devote little time in interpreting their models, meaning that black box threatment of ML algorithms is very prominent in the medium.\n\nIt would be expected that a 3-means clustering would build on these results to further separate kagglers, who responded the 2018 ML survey\n","736c9593":" A quick value count on the most important feature, could shine some light on the statistical differences between both clusters.","af3c33d6":"## Cluster Characterization \n\nIn order to find the relevant features for each cluster, we are going to use a SVM with linear kernel. This model\nuses the cluster labels to find the probability of an observation belonging to either class. Since, the kernel is linear, the absolute values of the coeficients can be threated as meassure of feature relevancy.","06c5388e":"## Preprocessing the data\n\nLet's import the data set corresponding to multiple choice questions:","3455cd20":"\n\nOn this small notebook, I going to attempt to analyse the kaggle 2018 survey data without presuppositions. In order to do this I will attempt to create my own \"dummy labels\" through k-means clustering, then apply some suppervised learning algorithms. I refer to these labels as \"dummy labels\" because in this case, the classification capabilities of the models will not be as relevant as the statistical structures found after training them in the data. The labels exists solely as a token ofr the model to latch on to during training. ","1dd12a5f":"We can then analyse the results quickly by counting","2791fa06":"Now, because many of the features are cathegorical, it is necesary to encode them before sending them to an algorithm. One-hot enconding is a feasible choice, since it does not create unwanted order in the data. It is also important to scale the column \"Time from start to finish (seconds)\", to match the 0-1 interval of the rest of the data. To deal with missing values, they will be replaced by -1.","570f5f2d":"## Identifying relevant features\n\nThe bar plot shown above show the top 40 features that an SVM model learns, in order to classify the data in either \nof the two clusters. The most relevant feature relates to the question \"Is reproducibility important in ML\". More specifically, the coice \"It is very important\". Using this top 40 features we are going to go back to the un-encoded set and create a reduced data set for each cluster.\n","516c480f":"There is a clear tendency in cluster 0. The majority of the answers in this clusters is 0, which means that the option was not selected. On the other hand, most cluster 1 members, chose 1 (equivalent to saying that it is very important), but a significant portion of them also chose 0. This ambiguity is solved thorugh the use of the other features. By summing over the columns of the reduced data set, we can estimate the amount of people that answered each question positively.\n","e1c69d78":"So it seems that cluster 1 is composed mainly of people with a mostly good opinion about python as a programing language, while those in cluster 0 do not. Taking another look at the important features used in the SVM model classification of both clusters, we find that most of the important questions in differentiating both clusters have to do with types of data, how they area manipulated and interpreted and programing languages. In other words, ML related questions. We can  then use this knowledge to go back to the original data set and look at some of these questions for each cluster","b782933e":"## Unsupervised Learning\n\nOne-hot encoding has created a data matrix with 4405 columns. In order to reduce computing time in k-menas, PCA is used to find a low dimentional representation of the data.","e684a120":"Question labels are as follows:","b155d2e0":"So in general cluster zero has many more persons who either didn't choose an answer or were not show a question (given that missing elemnts were given a -1 value) \n\nWe can then take a look at the maximum values in both clusters to further understand the difference between\nthe two groups"}}