{"cell_type":{"8f96d9d5":"code","8cce7c78":"code","51f1a9b1":"code","1db0247c":"code","9f125103":"code","fc48ad25":"code","6679b5f8":"code","612bd42a":"code","725288cc":"code","853c01db":"code","101836e2":"code","0797dc2a":"code","3806da45":"code","6709e293":"code","76c15164":"code","c39afbac":"code","b8e59e92":"code","4afa767f":"code","bc2b4cc4":"code","740d4f46":"code","a2cfc096":"code","3f375104":"code","d42424e7":"code","f03a8719":"code","11f7d109":"code","59d292eb":"code","ebd377ad":"code","c75a80f3":"code","ebee9523":"code","ac92c852":"code","6714909a":"code","a60b4370":"code","6f52ae88":"code","875436eb":"code","cee94ceb":"code","728c3271":"code","9253c92e":"code","699903f7":"markdown","79ccdf19":"markdown","7338968d":"markdown","c8c934f8":"markdown","fe361cb4":"markdown","9a73f742":"markdown","ddd7a70c":"markdown","a3941eb6":"markdown","6a5b0179":"markdown","e4e121bf":"markdown","cc0c0b8f":"markdown","5f8e0033":"markdown","a6baee34":"markdown","89b34f0e":"markdown","5d63e911":"markdown","2e6d776c":"markdown","474f0365":"markdown","1a322fed":"markdown","ce80bddf":"markdown","eccc7413":"markdown","dd758141":"markdown","f174a890":"markdown","66b898e4":"markdown","d17483ba":"markdown","0be12b51":"markdown"},"source":{"8f96d9d5":"from fastai.vision.all import *","8cce7c78":"set_seed(16)","51f1a9b1":"path = Path(\"..\/input\")","1db0247c":"path.ls()","9f125103":"data_path = path\/'cassava-leaf-disease-classification'\ndata_path.ls()","fc48ad25":"df = pd.read_csv(data_path\/'train.csv')","6679b5f8":"df.head()","612bd42a":"def get_x(row): return data_path\/'train_images'\/row['image_id']","725288cc":"PILImage.create(get_x(df.iloc[0]))","853c01db":"df.iloc[0]","101836e2":"df.head()","0797dc2a":"df['image_id'] = df['image_id'].apply(lambda x: f'train_images\/{x}')","3806da45":"df.head()","6709e293":"idx2lbl = {0:\"Cassava Bacterial Blight (CBB)\",\n          1:\"Cassava Brown Streak Disease (CBSD)\",\n          2:\"Cassava Green Mottle (CGM)\",\n          3:\"Cassava Mosaic Disease (CMD)\",\n          4:\"Healthy\"}\n\ndf['label'].replace(idx2lbl, inplace=True)","76c15164":"df.head()","c39afbac":"blocks = (ImageBlock, CategoryBlock)","b8e59e92":"splitter = RandomSplitter(valid_pct=0.2)","4afa767f":"def get_x(row): return data_path\/row['image_id']\n\ndef get_y(row): return row['label']","bc2b4cc4":"item_tfms = [Resize(448)]\nbatch_tfms = [RandomResizedCropGPU(224), *aug_transforms(), Normalize.from_stats(*imagenet_stats)]","740d4f46":"block = DataBlock(blocks = blocks,\n                 get_x = get_x,\n                 get_y = get_y,\n                 splitter = splitter,\n                 item_tfms = item_tfms,\n                 batch_tfms = batch_tfms)","a2cfc096":"dls = block.dataloaders(df, bs=64)","3f375104":"dls.show_batch(figsize=(12,12))","d42424e7":"# Making pretrained weights work without needing to find the default filename\nif not os.path.exists('\/root\/.cache\/torch\/hub\/checkpoints\/'):\n        os.makedirs('\/root\/.cache\/torch\/hub\/checkpoints\/')\n!cp '..\/input\/resnet50\/resnet50.pth' '\/root\/.cache\/torch\/hub\/checkpoints\/resnet50-19c8e357.pth'","f03a8719":"learn = cnn_learner(dls, resnet50, opt_func=ranger, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)","11f7d109":"def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr \/= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr\/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)","59d292eb":"@patch\ndef fine_tune_flat(self:Learner, epochs, base_lr=4e-3, freeze_epochs=1, lr_mult=100, pct_start=0.75, \n                   first_callbacks = [], second_callbacks = [], **kwargs):\n    \"Fine-tune applied to `fit_flat_cos`\"\n    self.freeze()\n    self.fit_flat_cos(freeze_epochs, slice(base_lr), pct_start=0.99, cbs=first_callbacks, **kwargs)\n    base_lr \/= 2\n    self.unfreeze()\n    self.fit_flat_cos(epochs, slice(base_lr\/lr_mult, base_lr), pct_start=pct_start, cbs=second_callbacks)","ebd377ad":"learn.lr_find()","c75a80f3":"cbs1 = [MixUp(alpha = 0.7)]\ncbs2 = [MixUp(alpha = 0.3)]","ebee9523":"learn.fine_tune_flat(5, base_lr=1e-3, pct_start=0.72, first_callbacks=cbs1, second_callbacks=cbs2)","ac92c852":"sample_df = pd.read_csv(data_path\/'sample_submission.csv')\nsample_df.head()","6714909a":"sample_copy = sample_df.copy()","a60b4370":"sample_copy['image_id'] = sample_copy['image_id'].apply(lambda x: f'test_images\/{x}')","6f52ae88":"test_dl = learn.dls.test_dl(sample_copy)","875436eb":"test_dl.show_batch()","cee94ceb":"preds, _ = learn.tta(dl=test_dl)","728c3271":"sample_df['label'] = preds.argmax(dim=-1).numpy()","9253c92e":"sample_df.to_csv('submission.csv',index=False)","699903f7":"We can see that when we write custom `get_` functions, it will accept one *row* of our `DataFrame` to look at, and so we can filter as a result.\n\nNext we'll come up with some basic data augmentations. \n\nOur `item_tfms` should ensure everything is ready to go into a batch, so we will use `Resize`.\n\nOur `batch_tfms` should apply any extra augmentations we may want. We'll use `RandomResizedCropGPU`, `aug_transforms`, and apply our `Normalize`:\n> We will normalize our data based on ImageNet, since that is what our pretrained model was trained with","79ccdf19":"We have an `image_id` and a `label`. We're going to modify our values in `image_id` to make our lives easier when it comes to running inference. \n\nWhy? \n\n\nIn fastai we have a `get_x` and a `get_y` and this will dictate how it will *always* look for our data, regardless of how it is stored. If we built a `get_y` based on the current `DataFrame`, it would look something like so:","7338968d":"#### loading train.csv with the help of pandas","c8c934f8":"Next we'll want to split our data somehow. We'll use a `RandomSplitter` and split our data 80\/20","fe361cb4":"set the random, torch, and numpy seeds with the set_seed function","9a73f742":"Now we won't run into an issue when we're testing. ","ddd7a70c":"We'll look at a batch of data to make sure it all looks okay:","a3941eb6":"#### ls fxn is used to see all files and directories inhere","6a5b0179":"## Submitting some results\n","e4e121bf":"## Importing the Library","cc0c0b8f":"Next we'll make an inference dataloader through the `test_dl` method:","5f8e0033":"Let's look at the sample submission dataframe first:","a6baee34":"And now we can submit them:","89b34f0e":" ***But*** there is a very large issue here. We always have our `get_x` tied to the training directory which makes it more complicated for us to work with our `test_images` directory.\n\nWhat's the solution? \n\nAdd `train_images` into the dataframe through a `lambda` function:","5d63e911":"### Adjusting our label\n\nWhat else can we do?\n\nLet's change our lables into something more readable through a dictionary (these come from the `json` file):","2e6d776c":"## Setting up our data\n","474f0365":"#### Sample data","1a322fed":"## Building the `DataBlock`\n\u200b\nLet's think about how our problem looks. `fastai` provides blocks to center around *most* situations, and this is no exception.\n\u200b\nWe know our input is an image and our output is a category, so let's use `ImageBlock` and `CategoryBlock`:","ce80bddf":"## Training Model","eccc7413":"Next we'll grab some predictions. We will use the `.tta` method to run test-time-augmentation which can help boost our accuracy some:","dd758141":"And now we can turn this into some `DataLoaders`. We're going to pass in some items (which in our case is our `DataFrame`) and a batch size to use. We will use 64:","f174a890":"Our `DataBlock` is also going to want to know how to get our data. Since our data all stems from a `csv`, we will make a `get_x` and `get_y` function:\n(we already made our `get_x`)","66b898e4":"Let's look at the sample submission dataframe first:","d17483ba":"Lets Build our first DataBlock","0be12b51":"Now that our weights are setup, let's look at how to use `cnn_learner`. We're going to use a few tricks during our training that fastai can help us out with. \n\nSpecifically we will be using the `ranger` optimizer function and `LabelSmoothingCrossEntropy` as our loss function.\n\nAlong with these we'll be using the `accuracy` metric as this is how this competition will grade our results with:"}}