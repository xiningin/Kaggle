{"cell_type":{"31895f48":"code","e619e249":"code","bf1fc079":"code","daf5c636":"code","1c067146":"code","565038a7":"code","9f62c1bd":"code","0bce49a5":"code","7b805330":"code","e4f01d7f":"code","0af47a08":"code","2833a389":"code","4482f118":"code","ab271b49":"code","72a9bd43":"code","945b039d":"code","87453b5c":"code","fd488bf0":"code","b597bbb6":"code","c1f9bf85":"code","fde4e1a6":"code","ae9a0367":"code","a31fbfe0":"code","0300fe63":"code","07f78f47":"code","cf3f5c92":"code","992b6eed":"code","46ca89d5":"code","b9d0891a":"code","8e3fce18":"code","04ba62be":"code","0e6cafe4":"code","92bdd25b":"code","f327e69b":"code","ac9787a3":"code","115fe4a5":"code","411d3bc7":"code","93705320":"code","7d057a07":"code","47a1e5c1":"code","b49b13c5":"code","e6885ab8":"code","41c02430":"code","6bba410c":"code","f6d13caa":"code","34f2a3d6":"code","de920ff6":"code","908e8ac3":"code","85c932fe":"code","191a7a3e":"code","9a1f110d":"code","1ca38eee":"code","aee44d3a":"code","8711f72b":"code","ab58481a":"code","0729d362":"code","b8fc8067":"code","a588f9a7":"code","4ac982a8":"code","e79a2582":"code","ac0c3788":"code","39f13be8":"code","66f78111":"markdown","eeea1aac":"markdown","e09c8ab9":"markdown","f559a176":"markdown","64346f00":"markdown","af85e76b":"markdown","01532bf4":"markdown","4ee516c2":"markdown","ada96fd9":"markdown","61475d60":"markdown","67565a76":"markdown","1acc7900":"markdown","cd123496":"markdown","20f80f6f":"markdown","60ac8ae2":"markdown","d8c993f5":"markdown","9f2076ab":"markdown","98d778c4":"markdown","f789eae4":"markdown","08e841aa":"markdown","a1ba43fa":"markdown","244022c3":"markdown","49a9e3c5":"markdown","89a15e93":"markdown","5f6f6671":"markdown","f44bc6ba":"markdown","2a47752a":"markdown","695c5c8f":"markdown","cd1b01b4":"markdown","6afe34ba":"markdown","34953636":"markdown","5092fb7c":"markdown","11be2bbd":"markdown","0c2e395b":"markdown","a53cb437":"markdown","4f3863a1":"markdown","4c4ff7d9":"markdown","5d2e7a07":"markdown","aa7032b0":"markdown","6659c0e8":"markdown","902982d5":"markdown","69c2ecb8":"markdown","e73475c3":"markdown","be218dcc":"markdown","86f69447":"markdown","f4cd2767":"markdown","28e6797c":"markdown","3a95c30c":"markdown","56cbfe5b":"markdown","a8b993bb":"markdown","0829d07a":"markdown","78ee1f28":"markdown","b7950a9d":"markdown","b4e64027":"markdown","16bcfe63":"markdown","df348fab":"markdown","ed7450e4":"markdown","c68bac97":"markdown","76c64ace":"markdown","3ffb17ee":"markdown","48a717e2":"markdown","08422cdb":"markdown","33813f9d":"markdown","355b05da":"markdown","fa4d1f74":"markdown","5da158ee":"markdown"},"source":{"31895f48":"#let us start by importing the relevant libraries\n\n%matplotlib inline\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n#import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report,roc_auc_score\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\n","e619e249":"vehdf= pd.read_csv(\"..\/input\/vehicle-2.csv\")\n#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n#print(vehdf)\nvehdf.head(200)\n","bf1fc079":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder \nle = LabelEncoder() \ncolumns = vehdf.columns\n#Let's Label Encode our class variable: \nprint(columns)\nvehdf['class'] = le.fit_transform(vehdf['class'])\nvehdf.shape","daf5c636":"vehdf.info()","1c067146":"from sklearn.impute import SimpleImputer\n\nnewdf = vehdf.copy()\n\nX = newdf.iloc[:,0:19] #separting all numercial independent attribute\n#y = vehdf.iloc[:,18] #seprarting class attribute. \n#imputer = SimpleImputer()\nimputer = SimpleImputer(missing_values=np.nan, strategy='median', verbose=1)\n#fill missing values with mean column values\ntransformed_values = imputer.fit_transform(X)\ncolumn = X.columns\nprint(column)\nnewdf = pd.DataFrame(transformed_values, columns = column )\nnewdf.describe()\n","565038a7":"\n\nprint(\"Original null value count:\", vehdf.isnull().sum())\nprint(\"\\n\\nCount after we impiuted the NaN value: \", newdf.isnull().sum())","9f62c1bd":"newdf.describe().T","0bce49a5":"newdf.shape","7b805330":"plt.style.use('seaborn-whitegrid')\n\nnewdf.hist(bins=20, figsize=(60,40), color='lightblue', edgecolor = 'red')\nplt.show()","e4f01d7f":"\n\n#Let us use seaborn distplot to analyze the distribution of our columns and see the skewness in attributes\nf, ax = plt.subplots(1, 6, figsize=(30,5))\nvis1 = sns.distplot(newdf[\"scaled_variance.1\"],bins=10, ax= ax[0])\nvis2 = sns.distplot(newdf[\"scaled_variance\"],bins=10, ax=ax[1])\nvis3 = sns.distplot(newdf[\"skewness_about.1\"],bins=10, ax= ax[2])\nvis4 = sns.distplot(newdf[\"skewness_about\"],bins=10, ax=ax[3])\nvis6 = sns.distplot(newdf[\"scatter_ratio\"],bins=10, ax=ax[5])\n\nf.savefig('subplot.png')\n\n  ","0af47a08":"skewValue = newdf.skew()\nprint(\"skewValue of dataframe attributes: \", skewValue)","2833a389":"#Summary View of all attribute , The we will look into all the boxplot individually to trace out outliers\n\nax = sns.boxplot(data=newdf, orient=\"h\")\n","4482f118":"plt.figure(figsize= (20,15))\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf['pr.axis_aspect_ratio'], color='orange')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf.skewness_about, color='purple')\n\nplt.subplot(3,3,3)\nsns.boxplot(x= newdf.scaled_variance, color='brown')\n\nplt.show()\n\n","ab271b49":"plt.figure(figsize= (20,15))\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf['radius_ratio'], color='red')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf['scaled_radius_of_gyration.1'], color='lightblue')\n\nplt.subplot(3,3,3)\nsns.boxplot(x= newdf['scaled_variance.1'], color='yellow')\n\nplt.show()","72a9bd43":"plt.figure(figsize= (20,15))\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf['max.length_aspect_ratio'], color='green')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf['skewness_about.1'], color='grey')\n\n\nplt.show()","945b039d":"newdf.shape\n","87453b5c":"from scipy.stats import iqr\n\nQ1 = newdf.quantile(0.25)\nQ3 = newdf.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n\n","fd488bf0":"cleandf = newdf[~((newdf < (Q1 - 1.5 * IQR)) |(newdf > (Q3 + 1.5 * IQR))).any(axis=1)]\ncleandf.shape","b597bbb6":"plt.figure(figsize= (20,15))\nplt.subplot(8,8,1)\nsns.boxplot(x= cleandf['pr.axis_aspect_ratio'], color='orange')\n\nplt.subplot(8,8,2)\nsns.boxplot(x= cleandf.skewness_about, color='purple')\n\nplt.subplot(8,8,3)\nsns.boxplot(x= cleandf.scaled_variance, color='brown')\nplt.subplot(8,8,4)\nsns.boxplot(x= cleandf['radius_ratio'], color='red')\n\nplt.subplot(8,8,5)\nsns.boxplot(x= cleandf['scaled_radius_of_gyration.1'], color='lightblue')\n\nplt.subplot(8,8,6)\nsns.boxplot(x= cleandf['scaled_variance.1'], color='yellow')\n\nplt.subplot(8,8,7)\nsns.boxplot(x= cleandf['max.length_aspect_ratio'], color='lightblue')\n\nplt.subplot(8,8,8)\nsns.boxplot(x= cleandf['skewness_about.1'], color='pink')\n\nplt.show()\n","c1f9bf85":"def correlation_heatmap(dataframe,l,w):\n    #correlations = dataframe.corr()\n    correlation = dataframe.corr()\n    plt.figure(figsize=(l,w))\n    sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\n    plt.title('Correlation between different fearures')\n    plt.show();\n    \n# Let's Drop Class column and see the correlation Matrix & Pairplot Before using this dataframe for PCA as PCA should only be perfromed on independent attribute\ncleandf= newdf.drop('class', axis=1)\n#print(\"After Dropping: \", cleandf)\ncorrelation_heatmap(cleandf, 30,15)","fde4e1a6":"sns.pairplot(cleandf, diag_kind=\"kde\")","ae9a0367":"#display how many are car,bus,van. \nnewdf['class'].value_counts()\n\nsplitscaledf = newdf.copy()\nsns.countplot(newdf['class'])\nplt.show()","a31fbfe0":"#now separate the dataframe into dependent and independent variables\n#X1= newdf.drop('class',axis=1)\n#y1 = newdf['class']\n#print(\"shape of new_vehicle_df_independent_attr::\",X.shape)\n#print(\"shape of new_vehicle_df_dependent_attr::\",y.shape)\n\nX = newdf.iloc[:,0:18].values\ny = newdf.iloc[:,18].values\n\nX","0300fe63":"from sklearn.preprocessing import StandardScaler\n#We transform (centralize) the entire X (independent variable data) to normalize it using standardscalar through transformation. We will create the PCA dimensions\n# on this distribution. \nsc = StandardScaler()\nX_std =  sc.fit_transform(X)          \n","07f78f47":"\ncov_matrix = np.cov(X_std.T)\nprint(\"cov_matrix shape:\",cov_matrix.shape)\nprint(\"Covariance_matrix\",cov_matrix)","cf3f5c92":"eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eigenvectors)\nprint('\\n Eigen Values \\n%s', eigenvalues)","992b6eed":"\n# Make a set of (eigenvalue, eigenvector) pairs:\n\neig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n\n# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\neig_pairs.sort()\n\neig_pairs.reverse()\nprint(eig_pairs)\n\n# Extract the descending ordered eigenvalues and eigenvectors\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\n# Let's confirm our sorting worked, print out eigenvalues\nprint('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)","46ca89d5":"tot = sum(eigenvalues)\nvar_explained = [(i \/ tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n# eigen vector... there will be 18 entries as there are 18 eigen vectors)\ncum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 18 entries with 18 th entry \n# cumulative reaching almost 100%","b9d0891a":"\nplt.bar(range(1,19), var_explained, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,19),cum_var_exp, where= 'mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.show()","8e3fce18":"# P_reduce represents reduced mathematical space....\n\nP_reduce = np.array(eigvectors_sorted[0:8])   # Reducing from 8 to 4 dimension space\n\nX_std_8D = np.dot(X_std,P_reduce.T)   # projecting original data into principal component dimensions\n\nreduced_pca = pd.DataFrame(X_std_8D)  # converting array to dataframe for pairplot\n\nreduced_pca","04ba62be":"\nsns.pairplot(reduced_pca, diag_kind='kde') \n#sns.pairplot(reduced_pca1, diag_kind='kde') ","0e6cafe4":"#now split the data into 70:30 ratio\n\n#orginal Data\nOrig_X_train,Orig_X_test,Orig_y_train,Orig_y_test = train_test_split(X_std,y,test_size=0.30,random_state=1)\n\n#PCA Data\npca_X_train,pca_X_test,pca_y_train,pca_y_test = train_test_split(reduced_pca,y,test_size=0.30,random_state=1)\n#pca_X_train,pca_X_test,pca_y_train,pca_y_test = train_test_split(reduced_pca1,y,test_size=0.30,random_state=1)","92bdd25b":"\nsvc = SVC() #instantiate the object\n","f327e69b":"#fit the model on orighinal raw data\nsvc.fit(Orig_X_train,Orig_y_train)","ac9787a3":"#predict the y value\nOrig_y_predict = svc.predict(Orig_X_test)\n\n","115fe4a5":"#now fit the model on pca data with new dimension\nsvc1 = SVC() #instantiate the object\nsvc1.fit(pca_X_train,pca_y_train)\n\n#predict the y value\npca_y_predict = svc1.predict(pca_X_test)","411d3bc7":"#display accuracy score of both models\n\nprint(\"Model Score On Original Data \",svc.score(Orig_X_test, Orig_y_test))\nprint(\"Model Score On Reduced PCA Dimension \",svc1.score(pca_X_test, pca_y_test))\n\nprint(\"Before PCA On Original 18 Dimension\",accuracy_score(Orig_y_test,Orig_y_predict))\nprint(\"After PCA(On 8 dimension)\",accuracy_score(pca_y_test,pca_y_predict))","93705320":"# Calculate Confusion Matrix & PLot To Visualize it\n\ndef draw_confmatrix(y_test, yhat, str1, str2, str3, datatype ):\n    #Make predictions and evalute\n    #model_pred = fit_test_model(model,X_train, y_train, X_test)\n    cm = confusion_matrix( y_test, yhat, [0,1,2] )\n    print(\"Confusion Matrix For :\", \"\\n\",datatype,cm )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [str1, str2,str3] , yticklabels = [str1, str2,str3] )\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n\ndraw_confmatrix(Orig_y_test, Orig_y_predict,\"Van \", \"Car \", \"Bus\", \"Original Data Set\" )\n\ndraw_confmatrix(pca_y_test, pca_y_predict,\"Van \", \"Car \", \"Bus\", \"For Reduced Dimensions Using PCA \")\n\n#Classification Report Of Model built on Raw Data\nprint(\"Classification Report For Raw Data:\", \"\\n\", classification_report(Orig_y_test,Orig_y_predict))\n\n#Classification Report Of Model built on Principal Components:\n\nprint(\"Classification Report For PCA:\",\"\\n\", classification_report(pca_y_test,pca_y_predict))","7d057a07":"splitscaledf.head(850)","47a1e5c1":"splitscale_X = splitscaledf.iloc[:,0:18].values\nsplitscale_y = splitscaledf.iloc[:,18].values\n\nprint(\"Indpendent Variable X\",splitscale_X )\nprint(\"Class Variable y\",splitscale_y )","b49b13c5":"#splitting the data in train and test sets into 70:30 Ratio\n\nSplitScale_X_train, SplitScale_X_test, SplitScale_y_train, SplitScale_y_test = train_test_split(splitscale_X,splitscale_y, test_size = 0.3, random_state = 10)","e6885ab8":"ssx_train_sd = StandardScaler().fit_transform(SplitScale_X_train)\nssx_test_sd = StandardScaler().fit_transform(SplitScale_X_test)\n\nprint(len(ssx_train_sd))\nprint(len(ssx_test_sd))\n","41c02430":"# generating the covariance matrix and the eigen values for the PCA analysis\ncov_matrix_1 = np.cov(ssx_train_sd.T) # the relevanat covariance matrix\nprint('Covariance Matrix \\n%s', (cov_matrix_1))\n\n#generating the eigen values and the eigen vectors\ne_vals, e_vecs = np.linalg.eig(cov_matrix_1)\nprint('Eigenvectors \\n%s' %(e_vecs))\nprint('\\nEigenvalues \\n%s' %e_vals)","6bba410c":"# Step 3 (continued): Sort eigenvalues in descending order\n\n# Make a set of (eigenvalue, eigenvector) pairs\neig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n\n# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\neig_pairs.sort()\n\neig_pairs.reverse()\nprint(eig_pairs)\n\n# Extract the descending ordered eigenvalues and eigenvectors\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\n# Let's confirm our sorting worked, print out eigenvalues\nprint('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)","f6d13caa":"tot = sum(eigenvalues)\nvar_explained = [(i \/ tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n# eigen vector... there will be 8 entries as there are 8 eigen vectors)\ncum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 8 entries with 8 th entry \n# cumulative reaching almost 100%","34f2a3d6":"plt.bar(range(1,19), var_explained, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,19),cum_var_exp, where= 'mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.show()","de920ff6":"# P_reduce represents reduced mathematical space....\n\nP_reduce_1 = np.array(eigvectors_sorted[0:8])   # Reducing from 8 to 4 dimension space\n\nX_train_std_pca = np.dot(ssx_train_sd,P_reduce_1.T)   # projecting original data into principal component dimensions\n\nX_test_std_pca = np.dot(ssx_test_sd,P_reduce_1.T) \n#Proj_data_df_new = pd.DataFrame(X_std_8D_1) \n\nprint(X_train_std_pca)\nprint(X_test_std_pca)\n\nProjected_df_train = pd.DataFrame(X_train_std_pca)\nProjected_df_test = pd.DataFrame(X_test_std_pca)","908e8ac3":"sns.pairplot(Projected_df_train, diag_kind='kde')\n","85c932fe":"### Pairplot Analysis : On Test PCA Data Set","191a7a3e":"sns.pairplot(Projected_df_test, diag_kind='kde')","9a1f110d":"ssx_train_sd.shape, P_reduce_1.T.shape, X_train_std_pca.shape, X_test_std_pca.shape","1ca38eee":"clf1 = SVC()\nclf1.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', clf1.score(ssx_test_sd, SplitScale_y_test))\n\nclf2 = SVC()\nclf2.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', clf2.score(X_test_std_pca, SplitScale_y_test))\n\n\n#print(\"Before PCA On Original 18 Dimension\",accuracy_score(Orig_y_test,Orig_y_predict))\n#print(\"After PCA(On 8 dimension)\",accuracy_score(pca_y_test,pca_y_predict))\n\n#predict the y value\npca_yhat_predict= clf2.predict(X_test_std_pca)\n\n#orginal data yhat value\norig_yhat_predict = clf1.predict(ssx_test_sd)\n\nprint(\"Before PCA On Original 18 Dimension\",accuracy_score(SplitScale_y_test,orig_yhat_predict))\nprint(\"After PCA(On 8 dimension)\",accuracy_score(SplitScale_y_test,pca_yhat_predict))","aee44d3a":"\ndraw_confmatrix(SplitScale_y_test, orig_yhat_predict,\"Van \", \"Car \", \"Bus\", \"Original Data Set\" )\n\ndraw_confmatrix(SplitScale_y_test, pca_yhat_predict,\"Van \", \"Car \", \"Bus\", \"For Reduced Dimensions Using PCA \")\n\n#Classification Report Of Model built on Raw Data\nprint(\"Classification Report For Raw Data:\", \"\\n\", classification_report(SplitScale_y_test,orig_yhat_predict))\n\n#Classification Report Of Model built on Principal Components:\n\nprint(\"Classification Report For PCA:\",\"\\n\", classification_report(SplitScale_y_test,pca_yhat_predict))","8711f72b":"import itertools\n\ndef classifiers_hypertune(name,rf,param_grid,x_train_scaled,y_train,x_test_scaled,y_test,CV):\n    CV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=CV, verbose= 1, n_jobs =-1 )\n    CV_rf.fit(x_train_scaled, y_train)\n    \n    y_pred_train = CV_rf.predict(x_train_scaled)\n    y_pred_test = CV_rf.predict(x_test_scaled)\n    \n    print('Best Score: ', CV_rf.best_score_)\n    print('Best Params: ', CV_rf.best_params_)\n    \n    \n    \n    #Classification Report\n    print(name+\" Classification Report: \")\n    print(classification_report(y_test, y_pred_test))\n    \n   \n    #Confusion Matrix for test data\n    draw_confmatrix(y_test, y_pred_test,\"Van\", \"Car\", \"Bus\", \"Original Data Set\" )\n    print(\"SVM Accuracy Score:\",round(accuracy_score(y_test, y_pred_test),2)*100)\n    ","ab58481a":"\n#Training on SVM Classifier\nfrom sklearn.model_selection import GridSearchCV\nsvmc = SVC()\n\n#Let's See What all parameters one can tweak \nprint(\"SVM Parameters:\", svmc.get_params())\n\n# Create the parameter grid based on the results of random search \nparam_grid = [\n  {'C': [0.01, 0.05, 0.5, 1], 'kernel': ['linear']},\n  {'C': [0.01, 0.05, 0.5, 1],  'kernel': ['rbf']},\n ]\n\nparam_grid_1 = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]","0729d362":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid,X_train_std_pca, SplitScale_y_train, X_test_std_pca, SplitScale_y_test,10)","b8fc8067":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid,ssx_train_sd, SplitScale_y_train, ssx_test_sd, SplitScale_y_test,10)","a588f9a7":"classifiers_hypertune(\"Support Vector Classifier_iterarion2\", svmc, param_grid_1,X_train_std_pca, SplitScale_y_train, X_test_std_pca, SplitScale_y_test,10)","4ac982a8":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid_1,ssx_train_sd, SplitScale_y_train, ssx_test_sd, SplitScale_y_test,10)","e79a2582":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nmodel.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', model.score(ssx_test_sd, SplitScale_y_test))\n\nmodel.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', model.score(X_test_std_pca, SplitScale_y_test))\n\n","ac0c3788":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', nb.score(ssx_test_sd, SplitScale_y_test))\n\nnb.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', nb.score(X_test_std_pca, SplitScale_y_test))\n","39f13be8":"from sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(criterion = 'entropy' )\n\ndt_model.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', dt_model.score(ssx_test_sd, SplitScale_y_test))\n\ndt_model.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', dt_model.score(X_test_std_pca, SplitScale_y_test))\n","66f78111":" \nWe found from our pairplot analysis that, Scaled Variance & Scaled Variance.1 and elongatedness and pr.axis_rectangularity to be strongly correlated , so they need to dropped of treated carefully before we go for model building. \n\n## Choosing the right attributes which can be the right choice for model building\n\nsince our objective is to reocgnize whether an object is a van or bus or car based on some input features. so our main assumption is there is little or no multicollinearity between the features. if our dataset has perfectly positive or negative attributes as can be obseverd fro our correlation analysis, there is a high chance that the performance of the model will be impacted by a problem called \u2014 \u201cMulticollinearity\u201d. Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results.\n\nif two features is highly correlated then there is no point using both features.in that case, we can drop one feature. SNS heatmap gives us the correlation matrix where we can see which features are highly correlated. \n\nFrom above correlation matrix we can see that there are many features which are highly correlated. if we carefully analyse, we will find that many  features are there which having more than 0.9 correlation. so we can decide to get rid of those columns whose correlation is +-0.9 or above.There are 8 such columns:\n\n- max.length_rectangularity\n- scaled_radius_of_gyration\n- skewness_about.2\n- scatter_ratio\n- elongatedness\n- pr.axis_rectangularity\n- scaled_variance\n- scaled_variance.1\n\n### Outcome: \nAlso we observed that more than 50 % of our attributes ar highly correlated , so  what we can we do best to deal with this kind problem of Multicollinearity\n\n### Well, There are multiple ways to deal with this problem. The easiest way is to delete or eliminate one of the perfectly correlated features.  We can pick one of the tiwo highly correalated variables and drop another one. like in our case Scaled Variance & Scaled Variance.1 are having strong positive correlation , so we can pick one and drop one as they will only make our dimension redundant.\n\n### Similarly between elongatedness and pr.axis_rectangularity we can pick one as they have very strong negative correlation. This approach can be used to select the feature we want to carry forward for model analysis. But there is another better approach called PCA\n\n### Another method, is to use a dimension reduction algorithm such as Principle Component Analysis (PCA). We will go for PCA and analyse the same going forward: \n","eeea1aac":"# Approach 2: ","e09c8ab9":"## Let's Plot The Box Plot Once Agaian To See if outliers are removed.","f559a176":"## Quick Insights On Confusion Matrix: \n         \n     - In our approach 1: we saw that our model actual instances of Van : 59, car : 133 and bus : 62   \n     - But in our second approach where we split the data set and then scaled , we saw the actual categorization instances as Van : 71, Car : 125, Bus: 58 \n     \n## Confusion Matrix For : \n\n### Original Data Set:\n \n [[ 68   1   2]\n [  1 118   6]\n [  0   1  57]]\n   \n     and \n     \n For Reduced Dimensions Using PCA \n [[ 68   3   0]\n [  2 118   5]\n [  1   1  56]]\n \n \n- Clearly both the model has almost same level of precitiability when it comes to correctly classifying van , car and bus. Which was not the case with approach 1. \n\n- IN approach one our confuson matrix showed that model on original data out of 133 actual car instance, correctly predicted 129 instances to be cars. while in appraich to out of actual 125 cars , it has correctly predicted 118 of instances to be a car . \n","64346f00":"## Hypertuning SVM using hyper Parameters: \n\n### Iteration 1: \n\n### In Case Of PCA: \n","af85e76b":"## Calculating covariance matrix:\n\nCovariance matrix should be 18*18 matrix\n","01532bf4":"### Naive Bayes","4ee516c2":"### Visualizing The plot : Principal Componenet Vs Explained Variance Ratio","ada96fd9":"\n> We will undestand how to deal with the problem of curse of dimesnioanlity using PCA and will also see how to implement principal component analysis using python. \n\n\n**Data Set**: We will make use of vehicle-2.csv data set\u00a0\nThe data contains features extracted from the silhouette of vehicles in\ndifferent angles. Four Corgie & model vehicles were used for the\nexperiment: a double decker bus, Cheverolet van, Saab 9000 and an\nOpel Manta 400 cars. This particular combination of vehicles was\nchosen with the expectation that the bus, van and either one of the cars\nwould be readily distinguishable, but it would be more difficult to\ndistinguish between the cars.\n\n**Objective:**\n\nThe purpose is to classify a given silhouette as one of three types of\nvehicle, using a set of features extracted from the silhouette. The vehicle\nmay be viewed from one of many different angles.\nHow We Will Meet Our Objective?\nWe will Apply dimensionality reduction technique\u200a-\u200aPCA and train a model using the reduced set of principal components (Attributes\/dimension). Then we will build Support Vector Classifier on raw data and also on PCA components to see how the model perform on the reduced set of dimension. We will also print the confusion matrix for both the scenario and see how our model has performed in classifying various vehicle types based on given silhouette of vehicles.\n\n**Let's Go Hands-On:**","61475d60":"Quick Observation : \n   - Most of the data attributes seems to be normally distributed \n   - scaled valriance 1 and skewness about 1 and 2, scatter_ratio, seems to be right skwed .\n   - pr.axis_rectangularity seems to be haing outliers as there are some gaps found in the bar plot. ","67565a76":"### Fit SVC Model ON Train-test Data: \n\nLet's build two Support Vector Classifier Model one with 18 original independent variables and the second one with only the 8 new reduced variables constructed using PCA.","1acc7900":"# Let's further tweak the parameters to see if we can improve our model accuracy :","cd123496":"## Let's Apply Grid Search & Cross-Validation:To Tune Our Model and Validate The Model's Accuracy Score","20f80f6f":"# Understanding the relationship between all independent attribute:\n\nWe will be using data correlation: \n\nData Correlation: Is a way to understand the relationship between multiple variables and attributes in your dataset. Using Correlation, you can get some insights such as:\n\n- One or multiple attributes depend on another attribute or a cause for another attribute.\n- One or multiple attributes are associated with other attributes.\n\nSpearman and Pearson are two statistical methods to calculate the strength of correlation between two variables or attributes. Pearson Correlation Coefficient can be used with continuous variables that have a linear relationship.\n\n## Pearson Correlation Coefficient: \n\nWe will use Pearson Correlation Coefficient to see what all attributes are linearly related and also visualize the same in the seaborns scatter plot. \n","60ac8ae2":"# Univariate Analysis Using Boxplot:\n\nIn descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\n","d8c993f5":"### Scaling The Independent Data Set: ","9f2076ab":"### Quick Comments: \n\n### Confusion Metric Analysis ON Original  Data: \n\nConfusion Matrix For : \n Original Data Set \n [[ 58   0   1]\n [  1 129   3]\n [  6   1  55]]\n     \n       - Our model on original data set has correctly classified 58 van out of 59 actuals vans and has errored only in one case where it has wrongly predicted van to be a bus.  \n       - IN case of 133 actual cars our svm model has correcly classified 129 cars. it has wrongly classified 3 cars to be a bus and also 1 car to be a van\n       - In case of 62 instances of actual bus , our model has correctly classified 55 buses , It has faltered in classifying wrongly 6 buses to be a van and one bus to be a car. \n       \n       \n### Confusion Metric Analysis ON Reduced Dimesnion After PCA : \n\nFor Reduced Dimensions Using PCA:\n [[ 57   2   0]\n [  2 126   5]\n [  1   7  54]]\n    \n    - Out of 59 actual instances of vans our model has correctly predicted 57 vans and errored in 2 instances where it wrongly classified vans to be a car. \n    - Out of 133 actuals cars , our mdoel has correclty classified 126 of them to be a  car and faltered in 7 cases where it wrongly classified 5 cars to a bus and 2 cars to be a van. \n    \n    - Out of 62 actual bus , our model has correclty classified 54 of them to be a bus. It has faltered in 8 cases where it wrongly classified 7 bus to be a car and 1 bus to be a van.\n    \n# Let's See The Classification Report Metrics:\n\nClassification Report For Raw Data: \n               precision    recall  f1-score   support\n\n         0.0       0.89      0.98      0.94        59\n         1.0       0.99      0.97      0.98       133\n         2.0       0.93      0.89      0.91        62\n\n   micro avg       0.95      0.95      0.95       254\n   macro avg       0.94      0.95      0.94       254\nweighted avg       0.95      0.95      0.95       254\n\nClassification Report For PCA: \n               precision    recall  f1-score   support\n\n         0.0       0.95      0.97      0.96        59\n         1.0       0.93      0.95      0.94       133\n         2.0       0.92      0.87      0.89        62\n\n   micro avg       0.93      0.93      0.93       254\n   macro avg       0.93      0.93      0.93       254\nweighted avg       0.93      0.93      0.93       254\n\n\n### Insights On Classification Reports: \n\n### On original data: \n   \n     - our model has 99 % precison  score when it comes to classify car from the given set of silhoutte parameters. It has 89 % precision when it comes to classifying the input as van, while it has 93 % precison when it come to predict data as bus. \n     \n     - In term of recall score our model has recal score of 98 % for van classification, 97 % for car and 89 % for bus. \n     - OUr model has an weighted average of 95 % for all classification metrics. \n   \n### On Reduced Dimensions After PCA: \n\n     - Our model has highest precision score of 95 % when it comes to predict van type, which is better as compared to predcition done on original data set, which came out with the precision score of 89 % for van. \n     - Recall score is almost neck to neck with what our model scored on original data set. It showed highest recall score of 97 % in classifying data as car. ","98d778c4":"# Approach 1: ","f789eae4":"### Separate The Data Into Independent & Dependent attribute\n","08e841aa":"### We can see a slight improvement in best model which was picked by our gridsearchcv method: 96.79 % and also we saw a slight increase in model accuracy score : 97 % ","a1ba43fa":"### Decisiontree Classifier: ","244022c3":"### Fitting SVC ON PCA Data: ","49a9e3c5":"### Iteration 2 : On original Data Set","89a15e93":"### Quick Insights: \n\n    - We can see that after we performed PCA our new dataframe with reduced dimesnions has no to zero linear relationship among themseleves, which is the main objective of using PCA tool. Almost all attribures have cloud of data in the mathematical space with no clear positive or negative correlation.\n    ","5f6f6671":"### Quick Observation: \n   - On training data set we saw that our support vector classifier without performing PCA has an accuracy score of 95 % \n   - But when we applied the SVC model on PCA componenets(reduced dimensions) our model scored 93 %. \n   - Considering that original dataframe had 18 dimensions and After PCA dimension reduced to 8, our model has fared well in terms of accuracy score. ","f44bc6ba":"### It is clealry visible from the pairplot above that:\nAfter dimensionality reduction using PCA our attributes have become independent with no correlation among themselves. As most of them have cloud of data points with no lienaer kind of relationship.","2a47752a":"## Loading Data Set","695c5c8f":"# Calculate Confusion Matrix & Plot To Visualize it","cd1b01b4":"## Treating Outliers Using IQR: Upper whisker\n\nThe interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1.\n  ","6afe34ba":"    - So we can see that how Principal component analysis can help us cherry pick only the relevant features by analysing the relationship between independent attributes to see which one will be more helpful in building our model without introducing any impurity in it. \n    \n    - We can increase the models performance both in terms of processing speed and the ease of implementing it . \n    \n    - Also we learned that one should always make sure data is normalized\/scaled before perfroming PCA, else the result will not be as per expectations\n    - Also it is recommended though it's not a rule , to split your data set into test-train before you apply data normalization\/scaling. This will help our model to perfrom more realistically in production environment. \n    - If one scale the model before splitting it into train-test , there is a high chance of data leakage . By data leakage i mean that our model will be alaready influenced by test data , as it has tasted a bit of test data even before it will fit and tested on it. \n    \n    - Also once one have tried every bit of techniques of removing outliers, treating the missing value, normalizing  the data whenever required, applied the diffrent models to come up with better model based on metrics which suits more the problem statement. It is advisable to fine tune it using hyperparameter tuning techniques which tunes the model performances and also employs Cross-fold validation internally to make sure our model is ready to face production environment. \n    \n- It would be good if we also measure how other classifier model like logistic regression, decisiontree , emsemble model like randomforrestclassifier will perfrom both on original data & PCA data. It would be interesting to see the confusion matrix and various others metrics like recall score, precision score, f1-score etc. \n\n- But Since our objective was to see how PCA data set impacts the model perfromance specially on training data set , we have here achived our objective and saw that PCA is great tool to make our model perfrom better . ","34953636":"### Quick Insight: \n\n### Hyper tuning of Model on PCA Data Set: \n\n   - We generally tune some of the important hyperparameters of the model which are not the model parameter. like here we played with C value : penalty and the type of kernel : rbf\/liner  \n   \n   - GridSearchCV get's the best paramter from the array of parameters and find the best model and score for us. \n   - Here on perfroming gridsearch hyper tuning of SVM model we got the best parameter to  be \n    : Best Score:  0.9256756756756757\n    : Best Params:  {'C': 1, 'kernel': 'rbf'}\n    : Accruacy Score : 95 % which seems to be similar to than what we measured earlier in our appraoch 2\n    \n    - We can further play with hyper parameters and see if it helps to score our model better. ","5092fb7c":"## Sort eigenvalues in descending order","11be2bbd":"## It Seems that Support Vectore Classifier is a better model to classifiy the given silhoutte info as van, bus, car . ","0c2e395b":"## Quick Insights: \n We can see that :\n    - circularity, class, hollow_ratio,max.length_rectangularity, , max.length_aspect_ratio, compactness has no missing values rest all features are having some kind of missing values \n    - All attributes are of numerical type\n # Finding The Missing Value: \n   Let's find the count of each attribite & treat the missing values   ","a53cb437":"### Quick Observation: \n    - From above we plot we can clealry observer that 8 dimension() are able to explain 95 %variance of data. \n    - so we will use first 8 principal components going forward and calulate the reduced dimensions. ","4f3863a1":"\n### On Orginal Data Set: \n\n   - When we apploed GridSearchCV on Our model which is using orginal data set, we saw\n      - Best Score:  0.956081081081081\n      - Best Params:  {'C': 1, 'kernel': 'rbf'}\n      - Accuracy Score : We saw an improvement in accuracy score to  96 % which is better than what we observed in approach 2 and approach 1\n","4c4ff7d9":"# Observation on boxplots: \n   - pr.axis_aspect_ratio, skewness_about, max_length_aspect_ratio, skewness_about_1, \n   - scaled_radius_of_gyration.1, scaled_variance.1, radius_ratio, skewness_about, scaled_variance.1 \n are some of the attributes with outliers. which is visible with all dotted points   ","5d2e7a07":"### Obseravtion: \n\nIf you carefully observe above, our orginal dataframe vehdf and new dataframe df , we will find that , After we imputed the datfarme series , using simpleimputer, we can see that the missing NaN values from our orginal vehdf datframe columns are treated and replaced using mode strategy. ","aa7032b0":"## Descriptive statistical summary \n\ndescribe() Function gives the mean, std and IQR values. It excludes character column and calculate summary statistics only for numeric columns.","6659c0e8":"# Let's Perfrom The PCA and See How Our Model Perform:\n\n### When we split our data set into test train and then apply scaling. \n   \nIn previous  process of PCA and model performance comparison, we scaled our data before splitting them into train and test, which may have lead to some kind of data leakages. Let's see how our PCA process and model accuracy behaves when we scale our data after splitting the into train-test set \n   ","902982d5":"# Understanding each attributes : \n\nUnivariate \n\n  - Quick descriptive statistics to make some meaningful sense of data \n  - Plotting univariate distribution\n  - Finding outliers & skewness in data series.  \n  - Treating outliers\n   ","69c2ecb8":"## Quick Insights : From Correlation Hetamap: \n\n### Strong\/fare Correlation: \n\n          - Scaled Variance & Scaled Variance.1 seems to be strongly correlated with value of 0.98\n          - skewness_about_2 and hollow_ratio seems to be strongly correlated, corr coeff: 0.89\n          - ditance_circularity and radius_ratio seems to have high positive correlation with corr coeff: 0.81\n          - compactness & circularity , radius_ratio & pr.axis_aspect_ratio also seems ver averagely correlated with coeff: 0.67.\n          - scaled _variance and scaled_radius_of_gyration, circularity & distance_circularity also seems to be highly correlated with corr coeff: 0.79\n          - pr.axis_recatngularity and max.length_recatngularity also seems to be strongly correlated with coeff: 0.81 \n          - scatter_ratio and elongatedness seems to be have strong negative correlation val : 0.97\n          - elongatedness and pr.axis_rectangularity seems to have strong negative correlation, val:  0.95\n       \n          \n   \n### Little To No Correlation: \n          -  \n          -max_length_aspect_ratio & radius_ratio have average correlation with coeff: 0.5\n          - pr.axis_aspect_ratio & max_length_aspect_ratio seems to have very little correlation\n          - scaled_radius_gyration & scaled_radisu_gyration.1 seems to be very little correlated\n          - scaled_radius_gyration.1 & skewness_about seems to be very little correlated\n          - skewness_about & skewness_about.1 not be correlated\n          - skewness_about.1 and skewness_about.2 are not correlated.\n        \nlet's visualize the same with pairplot , to see how it looks visually . \n\n        \n## Pairplot Analysis:           ","e73475c3":"## Calculating covariance matrix:\n\nCovariance matrix should be 18*18 matrix","be218dcc":"## Let's Apply StandardScaler to normalize our data set","86f69447":"### Quick Insights On descriptive stats: \n   -  Compactness has mean and median values almost similar , it signifies that it is normally distribited and has no skewness\/outlier \n   - circularity : it also seems to be normally distribted as mean amd median has similar values\n   - scatter_ratio feature seems to be having some kind of skewness and outlier\n   - Scaled variance 1 & 2 ","f4cd2767":"## Let's Look How Some Other Classifier Models Perform Bith On Original Data & PCA treated data sets\n\n### Logistic Regression","28e6797c":"### Confusion Matrix: \n","3a95c30c":"## Closing Comments: ","56cbfe5b":"\n# Principal Component Analysis(PCA):\n\n     - Basically PCA is a dimension redcuction methodology which aims to reduce a large set of (often correlated) variables into a smaller set of (uncorrelated) variables, called principal components, which holds sufficient information without loosing the the relevant info much.\n     \n     - Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components.\n     \n## We will perform PCA in following steps:\n   - Split our data into train and test data set\n   - normalize the tarining set using standard scalar\n   - Calculate the covariance matrix.\n   - Calculate the eigenvectors and their eigenvalues.\n   - Sort the eigenvectors according to their eigenvalues in descending order.\n   - Choose the first K eigenvectors (where k is the dimension we'd like to end up with).\n   - Build new dataset with reduced dimensionality.\n   ","a8b993bb":"### For Original Data Set: ","0829d07a":"## Fitting Model and measuring score simply on Original Data : ","78ee1f28":"## Let's Split Our Data Into Test & Train Data Set","b7950a9d":"# Performing EDA: \n\n  - Finding Any Missing Value\n  - Finding Outliers\n  - Understanding attributes using descriptive statistics\n  - visualizing attribute distribution using univariate and multivariate analysis\n  - Finding attribute correlation and analysing which attribute is more important","b4e64027":"### Quick Comment: \n\nWe can see that all out boxplot for all the attributes which had outlier have been treate and removed. Since no. of outliers were less we opted to remove it. Generally we avoid this as it can lead to info loss in case of large data sets with large no of outliers","16bcfe63":"### As we now have the IQR scores, it\u2019s time to get hold on outliers. The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas True indicates presence of an outlier.","df348fab":"## Calculating Eigen Vectors & Eigen Values: Using numpy linear algebra function\n","ed7450e4":"## Dimensionality Reduction\n\nNow 8 dimensions seems very reasonable. With 8 variables we can explain over 95% of the variation in the original data!","c68bac97":"# Let's train the model with both original data and pca data with new dimension\n\n### Fitting SVC model On Original Data","76c64ace":"### Quick Observation: After we split the data set into test & train set before applying standardscaler():\n\n   - On training data set we saw that our support vector classifier without performing PCA has an accuracy score of 95 % \n   - But when we applied the SVC model on PCA componenets(reduced dimensions) our model still performed better with 95 % with only miniscule fall in score. \n   - So earlier when we scaled the data first and then use it to build our model we saw a 1-2 % difference in accuracy score of svc model build on orignal & pca data set respectively. But when we changed our strategy by splitting the data set into train-test first and then using it scale the train & test data set we avoided the possible data leakage, which resulted in very minor change in accuracy score, when model was built both on original data set & pca data set. \n   \n   - Considering that original dataframe had 18 dimensions and After PCA dimension reduced to 8, our model has fared well in terms of accuracy score. ","3ffb17ee":"## Quick Insights:\nAs observed in our correlation heatmap our pairplot seems to validate the same. Scaled Variance & Scaled Variance.1 seems to be have very strong positive correlation with value of 0.98. skewness_about_2 and hollow_ratio also seems to have strong positive correation with coeff: 0.89\n\nscatter_ratio and elongatedness seems to be have very strong negative correlation. elongatedness and pr.axis_rectangularity seems to have strong negative correlation with val of ","48a717e2":"### Pairplot Analysis : On Training PCA Data Set","08422cdb":"## Plotting The Explained Variance and Princiapl Components:  ","33813f9d":"### Let us check The Pairplot Of Reduced Dimension After PCA: ","355b05da":"## Quick Eyeballing To See if there is any missing values","fa4d1f74":"### Sort eigenvalues in descending order","5da158ee":"# Dimensionality Reduction:  "}}