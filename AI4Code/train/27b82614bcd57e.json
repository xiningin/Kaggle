{"cell_type":{"a4ab1693":"code","faf1f249":"code","a23de074":"code","e1782d8a":"code","0c7f50ec":"code","1d7d4220":"code","11513a42":"code","9663c918":"code","955f222b":"code","4d344a6c":"code","c1cb0b77":"code","f9782f9a":"code","0005859c":"code","37d0da3e":"code","f07ebe9f":"code","e5a0a52e":"code","80f067cc":"code","28bf3326":"code","e6648ea2":"code","9ec04ac4":"code","9810b12d":"code","250026d5":"code","60d829ba":"code","af08f15b":"code","49005898":"code","d4e1ca5f":"code","7f28d323":"code","ffca835a":"code","3e34561c":"code","b345bb79":"code","e1ed5a99":"code","c716fd66":"code","b5d5e68e":"code","5ff53e05":"code","8880c568":"code","85c9594f":"code","8577fd08":"code","a65b70b6":"code","e2f5c3fc":"code","f0ab8e7b":"code","60fda439":"code","30515cb7":"code","1dbf972a":"code","1efe9a1e":"code","4a409420":"code","57c26d53":"code","24463292":"code","940b2885":"code","0cdb80c8":"code","abee557d":"code","802dcb54":"code","c73afe81":"code","4b05ee16":"code","ec657839":"code","32c1bef0":"code","3f0851b8":"code","6c1046f7":"code","60c75135":"code","e6c390e6":"code","986e655f":"code","31f043e7":"code","b0c3d32e":"code","8d3ebd15":"code","1beeb173":"code","4b0b51c3":"code","f1b7ef02":"code","f17ea737":"code","d7eed061":"code","5c1365e4":"code","8bf9fd4d":"markdown","c45a31de":"markdown","9bac0c94":"markdown","b2378c13":"markdown","fbdd1ed1":"markdown","ea4428c6":"markdown","bdf0facb":"markdown","026b6dfa":"markdown","da51ecd9":"markdown","09cbe06b":"markdown","5601085d":"markdown","e1a95fd3":"markdown","81fce066":"markdown","fd7080e7":"markdown","b71f4dc2":"markdown","b03a29b9":"markdown","6da3bab1":"markdown","784a893f":"markdown","c2ad0f41":"markdown","3b79d2c3":"markdown","57214d17":"markdown","e74f2ce9":"markdown","01e10a2f":"markdown","6c49981c":"markdown","020d066e":"markdown","ad08debd":"markdown","e1898c2d":"markdown","869e1851":"markdown","41b0c59f":"markdown","78fdf44a":"markdown","fce0cb59":"markdown","0c541650":"markdown","ab016548":"markdown","4d23f811":"markdown","9f058235":"markdown","4a0014d2":"markdown","f64b261c":"markdown","93c0a6e2":"markdown","8fc5b56e":"markdown","ec086439":"markdown","a14e7228":"markdown","f1c3a1a8":"markdown","e20b80ce":"markdown","27941dcf":"markdown","5e9d6a35":"markdown","20d4a3d0":"markdown","6c90269f":"markdown","a8e2828c":"markdown","060c7222":"markdown","0e4aba26":"markdown","ce78ed33":"markdown","20ca95a5":"markdown","13508db4":"markdown","737d88ff":"markdown","ca91c79b":"markdown","b7ba00c0":"markdown","fede806d":"markdown","d73f4a0a":"markdown","218dd04e":"markdown","26eeb6c8":"markdown","d57892f4":"markdown","f89679d6":"markdown","8568d99d":"markdown"},"source":{"a4ab1693":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","faf1f249":"df = pd.read_csv(\"..\/input\/adult.csv\")\ndf.head()","a23de074":"# replace \"?\" to NaN\ndf.replace(\"?\", np.nan, inplace = True)\ndf.head(5)","e1782d8a":"missing_data = df.isnull()\nmissing_data.sum()","0c7f50ec":"missing_col = []\nfor column in missing_data.columns.values.tolist():\n    if(missing_data[column].sum() > 0):\n        print(\"Column: \",column)\n        print(\"Missing Data: {} ({:.2f}%)\".format(missing_data[column].sum(), (missing_data[column].sum() * 100\/ len(df))))\n        print(\"Data Type: \",df[column].dtypes)\n        print(\"\")\n        missing_col.append(column)","1d7d4220":"import matplotlib.pyplot as plt\n%matplotlib inline","11513a42":"fig1 = plt.figure(figsize=(18,5))\ni = 0\nfor column in missing_col:\n    bad = missing_data[column].sum()\n    good = len(df) - missing_data[column].sum()\n    x = [bad, good]\n    labels = [\"Missing Data\", \"Good Data\"]\n    explode = (0.1, 0)\n    i = i+1\n    ax = fig1.add_subplot(1,3,i)\n    ax.pie(x,explode = explode, labels = labels, shadow = True,autopct='%1.1f%%', colors = ['#ff6666', '#99ff99'],rotatelabels = True, textprops={'fontsize': 18})\n    centre_circle = plt.Circle((0,0),0.4,color='black', fc='white',linewidth=0)\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    ax.axis('equal')\n    ax.set_title(column, fontsize = 25)\nplt.tight_layout()\nplt.show()\n","9663c918":"# Calculate Mode\nworkclass_mode = df['workclass'].value_counts().idxmax()\noccupation_mode = df['occupation'].value_counts().idxmax()\nnative_country_mode = df['native.country'].value_counts().idxmax()","955f222b":"print(\"Mode of workclass: \",workclass_mode)\nprint(\"Mode of Occupation: \",occupation_mode)\nprint(\"Mode of natice.country: \",native_country_mode)","4d344a6c":"df_manual = df","c1cb0b77":"#replace the missing categorical values by the most frequent value\ndf_manual[\"workclass\"].replace(np.nan, workclass_mode, inplace = True)\ndf_manual[\"occupation\"].replace(np.nan, occupation_mode, inplace = True)\ndf_manual[\"native.country\"].replace(np.nan, native_country_mode, inplace = True)","f9782f9a":"df_manual.isnull().sum()","0005859c":"count = 0\nfor column in df_manual.columns.values.tolist():\n    if df_manual[column].dtype == 'object':\n        print(\"Column Name: \",column)\n        print(\"Data Type: \", df_manual[column].dtypes)\n        print(\"\")\n        count = count + 1\nprint(\"Count : \",count)\n","37d0da3e":"dummy = pd.get_dummies(df_manual[\"workclass\"])\ndummy.head()","f07ebe9f":"#Rename column names\ndummy.rename(columns={'Federal-gov':'work-Federal-gov', \n                      'Local-gov':'work-Local-gov',\n                      'Private': 'work-Private',\n                      'Self-emp-inc': 'work-Self-emp-inc',\n                      'Self-emp-not-inc': 'Self-emp-not-inc',\n                      'State-gov': 'work-State-gov',\n                      'Without-pay' : 'work-Without-pay'}, inplace=True)\ndummy.head()","e5a0a52e":"dummy.drop(\"Never-worked\", axis = 1, inplace=True)\ndummy.head()","80f067cc":"# merge data frame \"df\" and \"dummy\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"workplace\" from \"df\"\ndf_manual.drop(\"workclass\", axis = 1, inplace=True)\ndf_manual.head()","28bf3326":"dummy = pd.get_dummies(df_manual[\"education\"])\ndummy.head()","e6648ea2":"dummy.drop(\"Some-college\", axis = 1, inplace=True)\ndummy.head()","9ec04ac4":"# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"education\", axis = 1, inplace=True)\ndf_manual.head()","9810b12d":"dummy = pd.get_dummies(df_manual[\"marital.status\"])\ndummy.head()","250026d5":"dummy.drop(\"Never-married\", axis = 1, inplace=True)\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"marital.status\", axis = 1, inplace=True)\ndf_manual.head()","60d829ba":"dummy = pd.get_dummies(df_manual[\"occupation\"])\ndummy.head()","af08f15b":"dummy.drop(\"Other-service\", axis = 1, inplace=True)\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"occupation\", axis = 1, inplace=True)\ndf_manual.head()","49005898":"dummy = pd.get_dummies(df_manual[\"relationship\"])\ndummy.head()","d4e1ca5f":"dummy.drop(\"Other-relative\", axis = 1, inplace=True)\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"relationship\", axis = 1, inplace=True)\ndf_manual.head()","7f28d323":"dummy = pd.get_dummies(df_manual[\"race\"])\ndummy.head()","ffca835a":"dummy.drop(\"Other\", axis = 1, inplace=True)\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"race\", axis = 1, inplace=True)\ndf_manual.head()","3e34561c":"dummy = pd.get_dummies(df_manual[\"sex\"])\ndummy.head()","b345bb79":"dummy.drop(\"Male\", axis = 1, inplace=True)\ndummy.rename(columns={ 'Female' : 'Female\/Male'}, inplace = True)\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"sex\", axis = 1, inplace=True)\ndf_manual.head()","e1ed5a99":"dummy = pd.get_dummies(df_manual[\"native.country\"])\ndummy.head()","c716fd66":"# merge data frame \"df\" and \"dummy_variable_1\" \ndf_manual = pd.concat([df_manual, dummy], axis=1)\n# drop original column \"fuel-type\" from \"df\"\ndf_manual.drop(\"native.country\", axis = 1, inplace=True)\ndf_manual.head()","b5d5e68e":"dummy = pd.get_dummies(df_manual[\"income\"])\ndummy.head()","5ff53e05":"dummy.rename(columns={ '>50K' : 'Income > 50K'}, inplace = True)\ndummy.drop('<=50K', axis = 1, inplace = True)","8880c568":"df_manual = pd.concat([df_manual, dummy], axis=1)\ndf_manual.drop(\"income\", axis = 1, inplace=True)\ndf_manual.head()","85c9594f":"df_manual.dtypes","8577fd08":"X = df_manual.iloc[:,:-1].values\ny = df_manual[\"Income > 50K\"].iloc[:].values","a65b70b6":"df_manual.describe(include = 'all')","e2f5c3fc":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","f0ab8e7b":"import seaborn as sns\nsns.countplot(y)","60fda439":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","30515cb7":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(learning_rate = 0.1, n_estimators = 100)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Model Accuracy = {:.2f}%\".format(accuracies.mean()* 100))","1dbf972a":"from mlxtend.plotting import plot_confusion_matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True,\n                               cmap = 'Dark2')\nplt.show()","1efe9a1e":"# import lightgbm as lgb\n# d_train = lgb.Dataset(X_train, label = y_train)\n# params = {}\n# params['learning_rate'] = 0.01\n# params['boosting_type'] = 'gbdt'\n# params['objective'] = 'binary'\n# params['metric'] = 'binary_logloss'\n# params['sub_feature'] = 0.5\n# params['num_leaves'] = 10\n# params['min_data'] = 50\n# params['max_depth'] = 10\n# clf = lgb.train({},train_set = d_train)","4a409420":"# #Prediction\n# y_pred=clf.predict(X_test)","57c26d53":"# for i in range(0,y_pred.shape[0]):\n#     if y_pred[i]>=0.5:       # setting threshold to .5\n#        y_pred[i]=1\n#     else:  \n#        y_pred[i]=0","24463292":"missing_col = []\nfor column in missing_data.columns.values.tolist():\n    if(missing_data[column].sum() > 0):\n        print(\"Column: \",column)\n        print(\"Missing Data: {} ({:.2f}%)\".format(missing_data[column].sum(), (missing_data[column].sum() * 100\/ len(df))))\n        print(\"Data Type: \",df[column].dtypes)\n        print(\"\")\n        missing_col.append(column)","940b2885":"df_dl_method = pd.read_csv(\"..\/input\/adult.csv\")\n# replace \"?\" to NaN\ndf_dl_method.replace(\"?\", np.nan, inplace = True)\ndf_dl_method.head(5)","0cdb80c8":"df_without_null = df_dl_method.dropna()","abee557d":"# reset index, because we droped two rows\ndf_without_null.reset_index(drop = True, inplace = True)\ndf_without_null.drop([\"occupation\", \"native.country\"], axis = 1, inplace = True)\ndf_without_null.head()","802dcb54":"def encoder(dataframe, col, drop_dummy_trap = \"\"):\n    dummy = pd.get_dummies(dataframe[col])\n    dataframe = pd.concat([dataframe, dummy], axis=1)\n    if(len(drop_dummy_trap) != 0):\n        # drop original column \"fuel-type\" from \"df\"\n        dataframe.drop(drop_dummy_trap, axis = 1, inplace=True)\n    dataframe.drop(col, axis = 1, inplace = True)\n    return dataframe","c73afe81":"df_without_null = encoder(dataframe = df_without_null, col = \"education\", drop_dummy_trap = \"Some-college\")\n#df_without_null = encoder(df = df_without_null, col = \"occupation\", drop_dummy_trap = \"Other-service\")\ndf_without_null = encoder(dataframe = df_without_null, col = \"marital.status\", drop_dummy_trap = \"Never-married\")\ndf_without_null = encoder(dataframe = df_without_null, col = \"relationship\", drop_dummy_trap = \"Other-relative\")\ndf_without_null = encoder(dataframe = df_without_null, col = \"race\", drop_dummy_trap = \"Other\")\ndf_without_null = encoder(dataframe = df_without_null, col = \"sex\", drop_dummy_trap = \"Male\")\ndf_without_null = encoder(dataframe = df_without_null, col = \"income\", drop_dummy_trap = \"<=50K\")","4b05ee16":"X_train = df_without_null.drop([\"workclass\"], axis = 1).iloc[:].values\ny_train = df_without_null.iloc[:,1].values","ec657839":"df_test=df_dl_method.loc[pd.isnull(df_dl_method[\"workclass\"])]\ndf_test.head()","32c1bef0":"df_test.drop([\"workclass\", \"occupation\", \"native.country\"], axis = 1, inplace = True)\ndf_test.head()","3f0851b8":"df_test = encoder(df_test, col = \"education\", drop_dummy_trap = \"Some-college\")\n#df_test = encoder(df_test, col = \"occupation\", drop_dummy_trap = \"Other-service\")\ndf_test = encoder(df_test, col = \"marital.status\", drop_dummy_trap = \"Never-married\")\ndf_test = encoder(df_test, col = \"relationship\", drop_dummy_trap = \"Other-relative\")\ndf_test = encoder(df_test, col = \"race\", drop_dummy_trap = \"Other\")\ndf_test = encoder(df_test, col = \"sex\", drop_dummy_trap = \"Male\")\n#df_test = encoder(df_test, col = \"native.country\", drop_dummy_trap = \"\")\ndf_test = encoder(df_test, col = \"income\", drop_dummy_trap = \"<=50K\")","6c1046f7":"X_test = df_test.iloc[:].values","60c75135":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()","e6c390e6":"y_train[:] = labelencoder.fit_transform(y_train[:])\ny_train","986e655f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","31f043e7":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(learning_rate = 0.1, n_estimators = 100)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)","b0c3d32e":"decode = dict(zip(labelencoder.transform(labelencoder.classes_), labelencoder.classes_))","8d3ebd15":"for i in range(0,y_pred.shape[0]):\n    y_pred[i] = decode[y_pred[i]]\ny_pred    ","1beeb173":"df_dl_method.head()","4b0b51c3":"fill = pd.DataFrame(y_pred, columns = [\"workclass\"])","f1b7ef02":"j = 0\nfor i in range(0, df_dl_method.shape[0]):\n    if(pd.isnull(df_dl_method.workclass[i])):\n        df_dl_method.workclass[i] = y_pred[j]\n        j = j+1","f17ea737":"df_dl_method.workclass.isnull().sum()","d7eed061":"df_viz = pd.read_csv(\"..\/input\/adult.csv\")\n# replace \"?\" to NaN\ndf_viz.replace(\"?\", np.nan, inplace = True)","5c1365e4":"fig1 = plt.figure(figsize=(20,5))\ni = 1\ncolumn = \"workclass\"\n\nbad = df_viz[column].isnull().sum()\ngood = len(df_viz) - df_viz[column].isnull().sum()\nx = [bad, good]\nlabels = [\"Missing Data\", \"Good Data\"]\nexplode = (0.1, 0)\nax = fig1.add_subplot(1,2,i)\nax.pie(x,explode = explode, labels = labels, shadow = True,autopct='%1.1f%%', colors = ['#ff6666', '#99ff99'],rotatelabels = True, textprops={'fontsize': 18})\ncentre_circle = plt.Circle((0,0),0.4,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nax.axis('equal')\nax.set_title(column + \"(before)\", fontsize = 25) \ni = i+1\n\nbad = df_dl_method[column].isnull().sum()\ngood = len(df) - df_dl_method[column].isnull().sum()\nx = [bad, good]\nlabels = [\"Missing Data\", \"Good Data\"]\nexplode = (0.1, 0)\nax = fig1.add_subplot(1,2,i)\nax.pie(x,explode = explode, labels = labels, shadow = True,autopct='%1.1f%%', colors = ['#ff6666', '#99ff99'],rotatelabels = True, textprops={'fontsize': 18})\ncentre_circle = plt.Circle((0,0),0.4,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nax.axis('equal')\nax.set_title(column + \"(after)\", fontsize = 25)\n\n\nplt.tight_layout()\nplt.show()","8bf9fd4d":"### Drop one column to avoid Dummy Variable Trap. Here we dropped \"Never-worked\" column and took it as base value.  \nHere we reduced the multi-colinearity.","c45a31de":"## Encode \"relationship\"","9bac0c94":"###  Check for missing Data","b2378c13":"**Encode all the categorical values **","fbdd1ed1":"## Fix 'workclass'","ea4428c6":"Since the symbol is now replaced with NaN which is null value, these are easily recognised and computed to check for the summation of the missing data available in the dataset.","bdf0facb":"## 1. Import Data Set","026b6dfa":"# Thank You!","da51ecd9":"### Add the dummy variables to Main Data Frame  \nWe append the dummy variable with the main dataframe and drop the original column workplace.","09cbe06b":"### Impact of Missing Data on Data set","5601085d":"## Let's make a dataframe without any Null Values","e1a95fd3":"## 2. Data Preprocessing","81fce066":"### Encoding \"workclass\"","fd7080e7":"### As we can see from above statistics, we have 3 missing columns,  \n1. workclass : 1836 missing data  \n2. occupation: 1843 missing data  \n3. native.country: 583 missing data  \n  \n### Note:  All the missing data are categorical data  ","b71f4dc2":"## Predict Missing Values using XGBoost","b03a29b9":"**Copy our original data frame to a dummy data frame**","6da3bab1":"After being replaced by the mode values effectively the algorithm doesnt find any missing values. The null value is computed zero for each attribute item.","784a893f":"## Encode \"education\"","c2ad0f41":"### Here the observation is highly imbalanced because there are more observations where Income <= 50K than >50K","3b79d2c3":"**Without any numerical precedence**","57214d17":"Here we used pd.get_dummies function to convert categorical values into label encoded binary values\n**1** indicates the positive verification while, **0** indicates negative verification.\n\nThese are Dummy variables alternatively called as indicator variables that take discrete values such as 1 or 0 marking the presence or absence of a particular category.","e74f2ce9":"<a id=\"ref3\"><\/a>\n## Deal with missing data\n**How to deal with missing data:**\n\n    \n    1. Drop data \n        a. drop the whole row\n        b. drop the whole column\n    2. Replace data\n        a. replace it by mean\n        b. replace it by frequency\n        c. replace it based on other functions","01e10a2f":"## Implement XGBoost (Gradient Boosting) Classifier Model","6c49981c":"Import Pandas and Numpy Library","020d066e":"## Fill the Missing Values of \"workclass\" using our Predicted Values","ad08debd":"### 1. Take all the rows from the dataframe without any missing value and make a training dataset  \n### 2. Build and Train a Model (Classifier \/ Regressor) with the Training Data  \n### 3. Take all rows with missing values and predict for missing value using the trained model  \n### 4. Fill the Null Values with predicted values \n### 5. Repeat above steps for all other values  ","e1898c2d":"The dataset contains featured attributes like age, workclass, education, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week & native-country which has characterisation of categorical & integer values. The missing data is represented using '?' symbol.","869e1851":"# Convert Categorical Variables to Continuous Variable (Label Encoding with Binarization)","41b0c59f":"## Encode \"marital.status\"","78fdf44a":"Now here the missing data is dealt-with accordingly based on certain rules.","fce0cb59":"# 3. Build Model","0c541650":"From this statistics we get a total no of 1836 workclass data, 1843 occupation data, and 583 native country data missing from the existing dataset.","ab016548":"## Fix Missing Data","4d23f811":"## Fixing Missing Data with ML Models","9f058235":"## Encode \"race\"","4a0014d2":"### Replace \" ? \" by NaN (Handling ' ? ' symbols)","f64b261c":"Visualization of impact of data.","93c0a6e2":"**Print all the categorical variables**","8fc5b56e":"## Encode \"occupation\"","ec086439":"### **Abstract**: Predict whether income exceeds  50K \/year based on census data.  Also known as \"Census Income\" dataset.  \nData Set: https:\/\/archive.ics.uci.edu\/ml\/datasets\/adult","a14e7228":"# Adult Census Income Data Analysis","f1c3a1a8":"The above table shows the label encoded category.","e20b80ce":"We renamed the attribute for our convinience","27941dcf":"## Encode \"Income\"  (Target Variable)\n**Income > 50K = 1, otherwise 0**","5e9d6a35":"## Replace \"?\" to NaN","20d4a3d0":"### Collect more statistics about Missing data","6c90269f":"# Lets divide out dataframe into feature variable(X) and target variable(Y)","a8e2828c":"## Encode \"sex\"","060c7222":"## We have above 9 Categorical Variables","0e4aba26":"## After Encoding Categorical Data to Numeric Value","ce78ed33":"## Available Missing Values","20ca95a5":"## Data Standardization  \n**Data standardization is the process of rescaling one or more attributes so that they have a mean value of 0 and a standard deviation of 1.**","13508db4":"During data filtering the '?' symbol is replaced with \"NaN\" to get a definite information from the dataset.","737d88ff":"The mode of each category is computed to fix the missing data based on frequency. The null values present in any column of dataset is replaced by the mode of the same column data.","ca91c79b":"1. **Check for any Null Values**","b7ba00c0":"## Check for any \"object\" datatype  \n**object = string + numerical data**\n\nHere we check for any remaining categorical variables.","fede806d":"### Split Train Test Split","d73f4a0a":"## Encode \"native.country\"","218dd04e":"# What is Dummy Variable Trap?  \nThe Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.   \n<img src = \"https:\/\/qph.fs.quoracdn.net\/main-qimg-4445db89d9218ba35ceedcf8d1e73d35.webp\"><\/img>   \nLets encode the categorical column called \u201cCountry\u201d and its values are -** [India, Germany, France]**  \nIn ML regression models, predictions will do the good job if categorical values are converted into numerical (binary vectors ) values. Encoding categorical data technique to apply for the above categorical set and the values a.k.a dummy variables will become  \n<img src = \"https:\/\/qph.fs.quoracdn.net\/main-qimg-c9ee21fe8c9294294f81ee7d39dddedb.webp\"><\/img>  \nWhich dummy variable column do we need drop?   \nThe answer is - we can drop any of one dummy variables column. It can predict the dropped column\u2019s value based on other two columns. **Let\u2019s take the record no 3 in the above table, both dummy variable values are \u20180\u2019. So obviously another dummy variable column value is \u20181\u2019 and categorical value is \u2018Germany\u2019**","26eeb6c8":"## See Target Variable Distribution","d57892f4":"### As we have only categorical missing data we will use \" Replace by Frequency or Mode\" Method","f89679d6":"### Replace the missing categorical values by the most frequent value","8568d99d":"**NO Null Values are present**"}}