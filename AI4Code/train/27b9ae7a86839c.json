{"cell_type":{"6eb12e7d":"code","fed1175a":"code","2c5860b1":"code","e72cd2b0":"code","04b12406":"code","20bf61ce":"code","5fe7ceb4":"code","100ce4d4":"code","7a7be3ce":"code","eae6b55e":"code","9d1f657e":"code","f2dfc5ff":"code","532af038":"code","b99ccfae":"code","0bbaa5c5":"code","4de18b95":"code","f020b993":"code","c19cd2fd":"code","73531ea9":"code","d9930eed":"code","72d00596":"code","c6ee5f54":"code","5995f80c":"code","802244c0":"code","4d2bb2bd":"code","68bf192f":"code","1c8b3fd1":"code","c199cc20":"code","edbc89d2":"code","388e84c9":"code","feeac341":"code","7211d16a":"code","f84345be":"code","d5ae218d":"markdown","141f39aa":"markdown","3e145b7a":"markdown","55d8077d":"markdown","d4c2443b":"markdown","3c117675":"markdown","a9f1efda":"markdown"},"source":{"6eb12e7d":"%matplotlib inline\nimport os\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom sklearn.neural_network import MLPClassifier\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torchvision import models\n\nimport time\nimport copy\n\n#from sklearn.datasets import fetch_openml\n\n#from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","fed1175a":"WD = os.path.join(Path.home(), \"data\", \"pystatml\", \"dl_mnist_pytorch\")\nos.makedirs(WD, exist_ok=True)\nos.chdir(WD)\nprint(\"Working dir is:\", os.getcwd())\nos.makedirs(\"data\", exist_ok=True)\nos.makedirs(\"models\", exist_ok=True)","2c5860b1":"def load_mnist(batch_size_train, batch_size_test):\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('data', train=True, download=True,\n                       transform=transforms.Compose([transforms.ToTensor(),\n                                                     transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std of the MNIST dataset\n                                                    ])),\n        batch_size=batch_size_train, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('data', train=False, transform=transforms.Compose([transforms.ToTensor(),\n                                                                          transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std of the MNIST dataset\n                                                                         ])),\n        batch_size=batch_size_test, shuffle=True)\n    return train_loader, val_loader","e72cd2b0":"train_loader, val_loader = load_mnist(64, 10000)\ndataloaders = dict(train=train_loader, val=val_loader)","04b12406":"# Info about the dataset\nD_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\nD_out = len(dataloaders[\"train\"].dataset.targets.unique())\nprint(\"Datasets shapes:\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\nprint(\"N input features:\", D_in, \"Output classes:\", D_out)","20bf61ce":"batch_idx, (example_data, example_targets) = next(enumerate(train_loader))\nprint(\"Train batch:\", example_data.shape, example_targets.shape)\nbatch_idx, (example_data, example_targets) = next(enumerate(val_loader))\nprint(\"Val batch:\", example_data.shape, example_targets.shape)","5fe7ceb4":"def show_data_label_prediction(data, y_true, y_pred=None, shape=(2, 3)):\n    y_pred = [None] * len(y_true) if y_pred is None else y_pred\n    fig = plt.figure()\n    for i in range(np.prod(shape)):\n        plt.subplot(*shape, i+1)\n        plt.tight_layout()\n        plt.imshow(data[i][0], cmap='gray', interpolation='none')\n        plt.title(\"True: {} Pred: {}\".format(y_true[i], y_pred[i]))\n        plt.xticks([])\n        plt.yticks([])","100ce4d4":"show_data_label_prediction(data=example_data, y_true=example_targets, y_pred=None,shape=(2, 3))","7a7be3ce":"X_train = train_loader.dataset.data.numpy()\n#print(X_train.shape)\nX_train = X_train.reshape((X_train.shape[0], -1))\ny_train = train_loader.dataset.targets.numpy()\nX_test = val_loader.dataset.data.numpy()\nX_test = X_test.reshape((X_test.shape[0], -1))\ny_test = val_loader.dataset.targets.numpy()\nprint(X_train.shape, y_train.shape)","eae6b55e":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n# Turn up tolerance for faster convergence\nclf = LogisticRegression(C=50., multi_class='multinomial', solver='sag', tol=0.1)\nclf.fit(X_train, y_train)\n#sparsity = np.mean(clf.coef_ == 0) * 100\nscore = clf.score(X_test, y_test)\nprint(\"Test score with penalty: %.4f\" % score)","9d1f657e":"coef = clf.coef_.copy()\nplt.figure(figsize=(10, 5))\nscale = np.abs(coef).max()\nfor i in range(10):\n    l1_plot = plt.subplot(2, 5, i + 1)\n    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n    cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l1_plot.set_xlabel('Class %i' % i)\n    \nplt.suptitle('Classification vector for...')\nplt.show()","f2dfc5ff":"mlp = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=5, alpha=1e-4,\nsolver='sgd', verbose=10, tol=1e-4, random_state=1,\nlearning_rate_init=0.01, batch_size=64)\nmlp.fit(X_train, y_train)","532af038":"print(\"Training set score: %f\" % mlp.score(X_train, y_train))\nprint(\"Test set score: %f\" % mlp.score(X_test, y_test))\nprint(\"Coef shape=\", len(mlp.coefs_))","b99ccfae":"fig, axes = plt.subplots(4, 4)\n# use global min \/ max to ensure all weights are shown on the same scale\nvmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\nfor coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n    vmax=.5 * vmax)\n    ax.set_xticks(())\n    ax.set_yticks(())\nplt.show()","0bbaa5c5":"class TwoLayerMLP(nn.Module):\n    def __init__(self, d_in, d_hidden, d_out):\n        super(TwoLayerMLP, self).__init__()\n        self.d_in = d_in\n        self.linear1 = nn.Linear(d_in, d_hidden)\n        self.linear2 = nn.Linear(d_hidden, d_out)\n    def forward(self, X):\n        X = X.view(-1, self.d_in)\n        X = self.linear1(X)\n        return F.log_softmax(self.linear2(X), dim=1)","4de18b95":"def train_val_model(model, criterion, optimizer, dataloaders, num_epochs=25,\n    scheduler=None, log_interval=None):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    # Store losses and accuracies accross epochs\n    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n    for epoch in range(num_epochs):\n        if log_interval is not None and epoch % log_interval == 0:\n            print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n            print('-' * 10)\n            \n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train() # Set model to training mode\n        else:\n            model.eval() # Set model to evaluate mode\n        running_loss = 0.0\n        running_corrects = 0\n        # Iterate over data.\n        nsamples = 0\n        for inputs, labels in dataloaders[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            nsamples += inputs.shape[0]\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # forward\n            # track history if only in train\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                    # statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        if scheduler is not None and phase == 'train':\n            scheduler.step()\n        #nsamples = dataloaders[phase].dataset.data.shape[0]\n        epoch_loss = running_loss \/ nsamples\n        epoch_acc = running_corrects.double() \/ nsamples\n        losses[phase].append(epoch_loss)\n        accuracies[phase].append(epoch_acc)\n        if log_interval is not None and epoch % log_interval == 0:\n            print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n                phase, epoch_loss, 100 * epoch_acc))\n        # deep copy the model\n        if phase == 'val' and epoch_acc > best_acc:\n            best_acc = epoch_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            \n    if log_interval is not None and epoch % log_interval == 0:\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, losses, accuracies","f020b993":"model = TwoLayerMLP(D_in, 50, D_out).to(device)\nprint(next(model.parameters()).is_cuda)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\ncriterion = nn.NLLLoss()","c19cd2fd":"# Explore the model\nfor parameter in model.parameters():\n    print(parameter.shape)\n    \nprint(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))\nmodel, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\nnum_epochs=1, log_interval=1)\nprint(next(model.parameters()).is_cuda)\ntorch.save(model.state_dict(), 'models\/mod-%s.pth' % model.__class__.__name__)","73531ea9":"batch_idx, (example_data, example_targets) = next(enumerate(val_loader))\nexample_data = example_data.to(device)\nwith torch.no_grad():\n    output = model(example_data).cpu()\nexample_data = example_data.cpu()","d9930eed":"# print(output.is_cuda)\n# Softmax predictions\npreds = output.argmax(dim=1)\nprint(\"Output shape=\", output.shape, \"label shape=\", preds.shape)\nprint(\"Accuracy = {:.2f}%\".format((example_targets == preds).sum().item() * 100. \/ len(example_targets)))\nshow_data_label_prediction(data=example_data, y_true=example_targets, y_pred=preds,shape=(3, 4))","72d00596":"errors = example_targets != preds\n#print(errors, np.where(errors))\nprint(\"Nb errors = {}, (Error rate = {:.2f}%)\".format(errors.sum(), 100 * errors.sum().item() \/ len(errors)))\nerr_idx = np.where(errors)[0]\nshow_data_label_prediction(data=example_data[err_idx], y_true=example_targets[err_idx],\ny_pred=preds[err_idx], shape=(3, 4))","c6ee5f54":"model = TwoLayerMLP(D_in, 50, D_out)\nmodel.load_state_dict(torch.load('models\/mod-%s.pth' % model.__class__.__name__))\nmodel.to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\ncriterion = nn.NLLLoss()\nmodel, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\nnum_epochs=10, log_interval=2)\n_ = plt.plot(losses['train'], '-b', losses['val'], '--r')","5995f80c":"class MLP(nn.Module):\n    def __init__(self, d_layer):\n        super(MLP, self).__init__()\n        self.d_layer = d_layer\n        layer_list = [nn.Linear(d_layer[l], d_layer[l+1]) for l in range(len(d_layer) - 1)]\n        self.linears = nn.ModuleList(layer_list)\n    def forward(self, X):\n        X = X.view(-1, self.d_layer[0])\n        # relu(Wl x) for all hidden layer\n        for layer in self.linears[:-1]:\n            X = F.relu(layer(X))\n            # softmax(Wl x) for output layer\n        return F.log_softmax(self.linears[-1](X), dim=1)","802244c0":"model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\ncriterion = nn.NLLLoss()\nmodel, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\nnum_epochs=10, log_interval=2)\n_ = plt.plot(losses['train'], '-b', losses['val'], '--r')","4d2bb2bd":"train_loader, val_loader = load_mnist(16, 1000)\ntrain_size = 10 * 16\n# Stratified sub-sampling\ntargets = train_loader.dataset.targets.numpy()\nnclasses = len(set(targets))","68bf192f":"indices = np.concatenate([np.random.choice(np.where(targets == lab)[0], int(train_size \/ nclasses),replace=False)\n                          for lab in set(targets)])","1c8b3fd1":"np.random.shuffle(indices)\ntrain_loader = torch.utils.data.DataLoader(train_loader.dataset, batch_size=16, \n                                           sampler=torch.utils.data.SubsetRandomSampler(indices))","c199cc20":"# Check train subsampling\ntrain_labels = np.concatenate([labels.numpy() for inputs, labels in train_loader])\nprint(\"Train size=\", len(train_labels), \" Train label count=\", {lab:np.sum(train_labels == lab) for lab in set(train_labels)})\nprint(\"Batch sizes=\", [inputs.size(0) for inputs, labels in train_loader])","edbc89d2":"# Put together train and val\ndataloaders = dict(train=train_loader, val=val_loader)","388e84c9":"# Info about the dataset\nD_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\nD_out = len(dataloaders[\"train\"].dataset.targets.unique())\nprint(\"Datasets shape\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\nprint(\"N input features\", D_in, \"N output\", D_out)","feeac341":"model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\ncriterion = nn.NLLLoss()","7211d16a":"model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\nnum_epochs=100, log_interval=20)\n_ = plt.plot(losses['train'], '-b', losses['val'], '--r')","f84345be":"model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\nmodel, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\nnum_epochs=100, log_interval=20)\n_ = plt.plot(losses['train'], '-b', losses['val'], '--r')","d5ae218d":"# Reduce the size of training dataset","141f39aa":"# Dataset: MNIST Handwritten Digit Recognition","3e145b7a":"# Model: Two Layer MLP","55d8077d":"# MLP with pytorch","d4c2443b":"1. Recall of linear classifier\n2. MLP with scikit-learn\n3. MLP with pytorch\n4. Test several MLP architectures\n5. Limits of MLP","3c117675":"# Recall of linear classi\u001cer\nBinary logistic regression\n1. neuron as output layer\n        \ud835\udc53(\ud835\udc65) = \ud835\udf0e(\ud835\udc65\ud835\udc47\ud835\udc64)\n# Softmax Classifier (Multinomial Logistic Regression)\n   * Input \ud835\udc65: a vector of dimension (0) (layer 0).\n   * Ouput \ud835\udc53(\ud835\udc65) a vector of (1) (layer 1) possible labels\n* The model as (1) neurons as output layer\n        \ud835\udc53(\ud835\udc65) = softmax(\ud835\udc65\ud835\udc47\ud835\udc4a + \ud835\udc4f)","a9f1efda":"# Test several MLP architectures"}}