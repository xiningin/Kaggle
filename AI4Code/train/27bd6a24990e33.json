{"cell_type":{"aeb94658":"code","5654c671":"code","4d90d7c7":"code","ed946e72":"code","56dda566":"code","2a4fd9c8":"code","460de562":"code","dfd0addf":"code","add78dd6":"code","b312869f":"code","14efc101":"code","58dbc11e":"code","ac6d754b":"code","53a82af2":"code","8c2b6bd3":"code","fda656fa":"code","4b6428bf":"code","045a3315":"markdown","b55bd17a":"markdown","5dfaafec":"markdown","398570ce":"markdown","d55b456a":"markdown","aeb94bc5":"markdown","91a8df2c":"markdown","aa80fd41":"markdown","40a0d376":"markdown"},"source":{"aeb94658":"import torch \nfrom torch import nn \nimport torch.nn.functional as F\nimport numpy as np \nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl \nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup, AdamW","5654c671":"!pip3 install deberta\nfrom DeBERTa import deberta\n\nvocab_path, vocab_type = deberta.load_vocab(pretrained_id='base')\ntokenizer = deberta.tokenizers[vocab_type](vocab_path)","4d90d7c7":"#taking only the id,excerpt,target\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\",usecols=[\"id\",\"excerpt\",\"target\"])\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)\ndf.head()","ed946e72":"test_df.head()","56dda566":"#any null rows\nprint(\"TRAIN NULLS: \\n\",df.isnull().sum())\nprint(\"TEST NULLS: \\n\",df.isnull().sum()) ","2a4fd9c8":"#\\n should be removed\ndf.loc[0,\"excerpt\"]","460de562":"# \"\\s\" should be replaced\ndf.loc[1000,\"excerpt\"]","dfd0addf":"#remove \\n and replace \\'s with 'sfrom the text\ndef prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","add78dd6":"#after preprocessing\ndf.loc[0,\"excerpt\"],df.loc[1000,\"excerpt\"]","b312869f":"max_words = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\nprint(\"maximum words in instance:\",max_words)","14efc101":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n# read training data\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\n# create folds\ndf = create_folds(df, num_splits=5)","58dbc11e":"df.head()","ac6d754b":"BATCH_SIZE = 4\nEPOCHS = 2\nLEARNING_RATE = 2e-5\nNUM_TRAIN_STEPS = int((df.shape[0]\/BATCH_SIZE)*EPOCHS)\nNUM_WARMUP_STEPS = 0\nFOLDS = df.kfold.unique()\nNUM_FOLDS = df.kfold.nunique() ","53a82af2":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1e-8\n        \n    def forward(self,output,target):\n        return torch.sqrt(F.mse_loss(output,target)+self.eps)","8c2b6bd3":"class deBertaModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = transformers.AutoModel.from_pretrained(\"microsoft\/deberta-base\")\n        #self.model = transformers.AutoModel.from_pretrained(\"..\/input\/deberta\/base\")\n        self.drop = nn.Dropout(0.5)\n        self.fc = nn.Linear(768,1)\n    \n    def forward(self,inputs):\n        out = self.model(**inputs)\n        last_hiddens = out[0]\n        out = self.drop(last_hiddens[:,0,:].squeeze(1))\n        return self.fc(out)\n    \n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)\n        return [optimizer],[scheduler] \n    \n    def loss_fn(self,output,target):\n        return RMSELoss()(output.view(-1),target.view(-1))\n    \n    def training_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        return loss\n    \n    def validation_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        self.log(\"val_loss\",loss,prog_bar=True)\n","fda656fa":"class deBertaDataset(Dataset):\n    def __init__(self,texts,labels,max_len):\n        super().__init__()\n        self.texts = texts\n        self.max_len = max_len\n        self.labels = labels\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/deberta\/base\")\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self,idx):\n        text = \" \".join(self.texts[idx].split())\n        label = self.labels[idx]\n        inputs = self.tokenizer(text,return_tensors=\"pt\",max_length = self.max_len, padding=\"max_length\",truncation=True)\n        return {\n            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n                      \"attention_mask\":inputs[\"attention_mask\"][0],},\n            \"label\":torch.tensor(label,dtype=torch.float)\n        }\n    \n","4b6428bf":"for fold in FOLDS:\n    print(\"Fold :\",fold)\n    train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n    train_dataset = deBertaDataset(train_df.excerpt.values,train_df.target.values,max_len=max_words)\n    valid_dataset = deBertaDataset(valid_df.excerpt.values,valid_df.target.values,max_len=max_words)\n    train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n    valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    bert_model = deBertaModel() \n    trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\")],checkpoint_callback=False)\n    trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n    trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","045a3315":"RMSE as Criterion","b55bd17a":"# Tokenize Dataset and Dataloader","5dfaafec":"# Create Folds ","398570ce":"Directly Copied from [Abhishek Notebook](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds)","d55b456a":"# Trainer","aeb94bc5":"# Data and Preprocess","91a8df2c":"Thanks to this [notebook](https:\/\/www.kaggle.com\/shivanandmn\/bert-pytorch-commonlit-readability-simple) for a starter.\n\nI had read some papers on SOTA and found that DeBERTa did amazingly well. I wanted to give a vanilla version of it a try. [There is an inference notebook](https:\/\/www.kaggle.com\/crained\/deberta-pytorch-inference) as well since you have to run with no internet. ","aa80fd41":"# Imports","40a0d376":"# DeBerta Model and Training Module"}}