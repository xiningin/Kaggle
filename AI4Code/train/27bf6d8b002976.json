{"cell_type":{"fa12b7d5":"code","ed1e9904":"code","69bff25f":"code","252efabe":"code","b56f6f6a":"code","61765425":"code","7a3da493":"code","3ad38708":"code","e9f8e951":"code","c5594994":"code","69faef5f":"code","5a10d0cf":"code","d0b1591a":"code","7e62ddcb":"code","b53a5dd1":"code","8974dcfe":"code","b85efaf4":"code","9f7244fb":"code","d2716b7f":"code","571f96e4":"code","efa24c9b":"code","e55cea66":"code","77e1efc3":"code","bfa9fe73":"code","cc87289a":"code","d8e594ae":"code","e1dac8e4":"code","27d11943":"markdown","5dedd8b2":"markdown","adeaaa62":"markdown","4982bed4":"markdown","0303307c":"markdown","bc820d2f":"markdown","bbfe14c7":"markdown","c1efd8e5":"markdown","f3463515":"markdown","bf5794c7":"markdown","9d59fd67":"markdown","74916a7d":"markdown","22e063e3":"markdown","779040d3":"markdown"},"source":{"fa12b7d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport joblib","ed1e9904":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.set_index(['PassengerId'], inplace=True)\ntrain.shape","69bff25f":"train.head()","252efabe":"train.info()","b56f6f6a":"total = train.isnull().sum().sort_values(ascending=False)\n\nmissing = pd.concat([total], axis=1)\nmissing.head(11)","61765425":"train.describe()","7a3da493":"# Display a DataFrame with proportion of Survived\ns = train.Survived\ncts = s.value_counts()\npct = s.value_counts(normalize = True).mul(100).round(1)\npd.DataFrame({'counts': cts, 'percent': pct})\n","3ad38708":"#proportion of Survived by Gender\ndf1 = pd.crosstab(train['Sex'], train['Survived'], normalize = 'index').mul(100)\ndf1","e9f8e951":"#proportion of Survived by Pclass\ndf2 = pd.crosstab(train['Pclass'], train['Survived'], normalize = 'index').mul(100)\ndf2","c5594994":"#proportion of Survived by Embarked\ndf3 = pd.crosstab(train['Embarked'], train['Survived'], normalize = 'index').mul(100)\ndf3","69faef5f":"#Create feature of family size\ntrain['FamSize'] = train.apply(lambda row: row.SibSp + row.Parch, axis = 1) \ntrain.head(15)","5a10d0cf":"#create a function to determine deck level\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]","d0b1591a":"#store results in new column named 'Deck'\ntrain['Deck']= train['Cabin'].map(set_deck)\ntrain.head()","7e62ddcb":"#create a list of numerical features\nnum_feat = ['Age', 'FamSize', 'Fare']\n\n#create a list of categorical features \ncat_feat = ['Sex', 'Pclass', 'Deck', 'Embarked']\n\nprint(num_feat)\nprint(cat_feat)","b53a5dd1":"#combine the numeric and categorical lists into a list called features\nfeatures = num_feat + cat_feat\nprint(features)","8974dcfe":"#create a Pipeline object for processing the num_feat with a SimpleImputer and a StandardScaler\nnum_pipe = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())\n    ]\n)\n\n#create a Pipeline object for processing the cat_feat with a SimpleImputer and a OneHotEncoder\ncat_pipe = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n    ('onehot', OneHotEncoder())\n    ]\n)\n\n\n#create a ColumnTransformer object that combines the two pipelines, name it 'preprocessor'\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_pipe, num_feat),\n        ('cat', cat_pipe, cat_feat)\n    ]\n)","b85efaf4":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \npreprocessor.fit(train[features])","9f7244fb":"# Apply the fitted preprocessor to the training data, again selecting only the relevant columns. \n# Store the array created in the previous step into a variable named 'X_train'\n\nX_train = preprocessor.transform(train[features])","d2716b7f":"#create a variable named 'y_train' that contains the training labels\n#print the shapes of X_train and y_train\ny_train = train.Survived\n\nprint('Shape of features: ', X_train.shape)\nprint('Shape of target: ', y_train.shape)","571f96e4":"%%time \n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.3, 0.5, 0.7, 1],\n    'C': [0.01, 0.1, 0.5, 1]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring= 'accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","efa24c9b":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","e55cea66":"%%time \n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [3, 8, 17, 26],\n    'min_samples_leaf': [2, 4, 6]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","77e1efc3":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","bfa9fe73":"%%time \n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=200)\n\nrf_parameters = {\n    'max_depth': [3, 8, 17, 26],\n    'min_samples_leaf': [2, 6, 11]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","cc87289a":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","d8e594ae":"#save your pipeline to a file. \njoblib.dump(preprocessor, 'titanic_preprocessor.joblib')","e1dac8e4":"#determine the best model found above and save that to a file. \nbest_model = RandomForestClassifier(random_state=3, n_estimators=200, max_depth = 8, min_samples_leaf = 2)\nbest_model.fit(X_train, y_train)\n\njoblib.dump(best_model, 'titanic_model.joblib')","27d11943":"Of the 891 observations, \"Cabin\" is missing from 687 rows, \"Age\" is missing from 177 rows, and \"Embarked\" is missing from 2 rows. No other features are missing values.","5dedd8b2":"# Import Statements","adeaaa62":"## Decicion Trees","4982bed4":"# Check for Missing Values","0303307c":"# Preprocessing","bc820d2f":"## Random Forests","bbfe14c7":"# Save Pipeline and Model","c1efd8e5":"## Logistic Regression","f3463515":"The best performing model was the Random Forest with max_depth = 8 and min_samples_leaf = 2 with a cv_score of 0.835","bf5794c7":"# Load Training Data","9d59fd67":"# Model Selection","74916a7d":"# Check Label Distribution","22e063e3":"The dataset has 891 rows of observations and 11 features, plus the target variable \"Survived\".","779040d3":"# Titanic Dataset\n\nMost of the code cells below include comments explaining the task to be performed in those cells. Please delete the comments and add code to perform those tasks. There are a few code cells in which code has already been provided for you. In some cases, you will need to compelte this code. "}}