{"cell_type":{"9163cb24":"code","e48aca41":"code","6940302f":"code","f5240bc0":"code","1c3e304c":"code","9b6a458b":"code","66cd2f91":"code","31400e0d":"code","266a9690":"code","53689086":"code","87892d3e":"code","b83913ee":"code","39733f13":"code","5b74c59a":"code","5a713f8c":"code","b9daf373":"code","e93f018d":"code","774f9eb2":"code","79f15388":"code","6c41e963":"code","4faaeb12":"code","b5e41b16":"code","04a2c9af":"markdown","46c9ff5b":"markdown","63d5ce68":"markdown","8212187f":"markdown","55303ba1":"markdown","a56864c1":"markdown","cdc5f41f":"markdown"},"source":{"9163cb24":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport time","e48aca41":"path = '..\/input\/g-research-crypto-forecasting\/'\n\ndef read_csv_strict(file_name='train.csv'): \n    df = pd.read_csv(path+file_name)\n    for asset in df['Asset_ID'].unique():\n        df.loc[df['Asset_ID']==asset, 'datetime'] = pd.to_datetime(df.loc[df['Asset_ID']==asset,'timestamp'], unit='s')\n    df = df[df['datetime'] < '2021-06-13 00:00:00']\n    return df","6940302f":"# Load train data and asset details\ntrain_data = read_csv_strict()\nasset_details = pd.read_csv(path+'asset_details.csv')","f5240bc0":"train_data.head()","1c3e304c":"asset_details.head()","9b6a458b":"# Join train and asset details to get asset names and weight alongside their time series\ntrain_data = pd.merge(train_data, asset_details, on='Asset_ID')\ntrain_data = train_data[[train_data.columns.tolist()[0], train_data.columns.tolist()[-1], train_data.columns.tolist()[-2]]+train_data.columns.tolist()[2:-2]]","66cd2f91":"# By asset, which range of data do we have in the dataset (in minutes)?\n# How many data points are recorded for each asset (1 data point record = 1 minute)?\ngrouped_stats = train_data.groupby('Asset_Name')['timestamp'].agg([('timestamp', lambda x: (np.min(pd.to_datetime(train_data['timestamp'], unit='s')))), ('timestamp', lambda x: (np.max(pd.to_datetime(train_data['timestamp'], unit='s')))),\n                                                                   ('timestamp', lambda x: int((np.max(pd.to_datetime(train_data['timestamp'], unit='s')) - np.min(pd.to_datetime(train_data['timestamp'], unit='s'))).total_seconds()\/60)),\n                                                                   ('timestamp', lambda x: x.count())])\ngrouped_stats.columns = ['start_date', 'end_date', 'nb_minutes', 'nb_datapoints']\ngrouped_stats['missing_datapoints'] = grouped_stats['nb_minutes'] - grouped_stats['nb_datapoints'] + 1","31400e0d":"grouped_stats","266a9690":"# Data missing for many assets, let's see if it's really true\nfor asset in train_data['Asset_Name'].unique():\n    df_asset = train_data[train_data['Asset_Name']==asset].set_index('timestamp')\n    print(asset, (df_asset.index[1:]-df_asset.index[:-1]).value_counts().head(2))\n    print()","53689086":"# Let's fill missing asset values using forward fill\n# value imputed is the last valid value, this prevents any lookahead (that would have been introduced by backward fill for instance)\ntrain = pd.DataFrame([])\nfor asset in train_data['Asset_Name'].unique():\n    df_asset = train_data[train_data['Asset_Name']==asset].set_index('timestamp')\n    df_asset = df_asset.reindex(range(df_asset.index[0], df_asset.index[-1]+60, 60), method='ffill')\n    print(asset, (df_asset.index[1:]-df_asset.index[:-1]).value_counts().head()) # Are gaps filled correctly?\n    train = pd.concat([train, df_asset])\ntrain.reset_index(level=0, inplace=True)","87892d3e":"# Let's visualise some of the data at hand\n# Let's focus on 2021 Bitcoin and Monero\nbtc = train[train['Asset_Name']=='Bitcoin'].set_index('timestamp')\nmnr = train[train['Asset_Name']=='Monero'].set_index('timestamp')\neth = train[train['Asset_Name']=='Ethereum'].set_index('timestamp')","b83913ee":"print(f'Bitcoin NaN values: {btc.isna().sum()[btc.isna().sum()!=0]}')\nprint(f'Monero NaN values: {mnr.isna().sum()[mnr.isna().sum()!=0]}')\nprint(f'Ethereum NaN values: {eth.isna().sum()[eth.isna().sum()!=0]}')\n\n\nprint('\\nDropping NaN values...')\nfor df in [btc, mnr, eth]:\n    df.dropna(inplace=True)\n    df.reset_index(inplace=True)\n    \nprint('...done.')","39733f13":"# log of Close values is chosen because of the different price scales\nf = plt.figure(figsize=(10,7))\nplt.plot(np.log(btc['Close']), label='BTC');\nplt.plot(np.log(mnr['Close']), label='MNR');\nplt.plot(np.log(eth['Close']), label='ETH');\nplt.title('Closing asset value over time');\nplt.xlabel('Time');\nplt.ylabel('log(close)');\nplt.legend();","5b74c59a":"# Are percentage changes in prices kind of correlated as would Close values be ?\nf = plt.figure(figsize=(10,7))\nplt.plot(btc['Close'].pct_change(), label='BTC');\nplt.plot(mnr['Close'].pct_change(), label='MNR');\nplt.plot(eth['Close'].pct_change(), label='ETH');\nplt.title('Percentage changes in asset closing values over time');\nplt.xlabel('Time');\nplt.ylabel('Close % change');\nplt.legend();","5a713f8c":"# Histogram of Close values for each asset\nf = plt.figure(figsize=(10,7))\nnp.log(btc['Close']).hist(label='BTC');\nnp.log(mnr['Close']).hist(label='MNR');\nnp.log(eth['Close']).hist(label='ETH');\nplt.xlabel('log(close)');\nplt.ylabel('Count');\nplt.title('Distribution of asset closing values');\nplt.legend()\nplt.grid(False);","b9daf373":"# Histogram of Close value changes for each asset\nf = plt.figure(figsize=(10,7))\nbtc['Close'].pct_change().hist(alpha=0.5, label='BTC');\nmnr['Close'].pct_change().hist(alpha=0.3, label='MNR');\neth['Close'].pct_change().hist(alpha=0.3, label='ETH');\nplt.xlabel('Close % change');\nplt.ylabel('Count');\nplt.title('Distribution of percentage changes in closing asset values');\nplt.legend();\nplt.grid(False);","e93f018d":"# So apparently % changes are very very centered around 0...\n# Maybe it's due to the recording resolution, and such changes would appear higher with weekly sampled data","774f9eb2":"# Dataset for correlations between BTC and ETH\nbtc_eth = pd.merge(btc, eth, on='datetime', how='inner')\nbtc_eth.drop('timestamp_y', axis=1, inplace=True)\n\ncols = list()\nfor col in btc_eth.columns:\n    if col == 'timestamp_x':\n        col = col.replace('_x', '')\n    else:\n        col = col.replace('_x', '_BTC').replace('_y', '_ETH')\n    cols.append(col)\n    \nbtc_eth.columns = cols","79f15388":"# Scatter plot of Close values between BTC and ETH\nf = plt.figure(figsize=(10,7))\nplt.scatter(x=btc_eth['Close_BTC'].pct_change(), y=btc_eth['Close_ETH'].pct_change());\nplt.xlabel('BTC close value % change');\nplt.ylabel('ETH close value % change');\nplt.title('ETH close value vs. BTC close value.');","6c41e963":"# ETH and BTC seem to be very positively linearly correlated...\nbtc_eth[['Close_BTC', 'Close_ETH']].corr()","4faaeb12":"# ...but this actually doesn't teach us much.\n# Indeed, positive linear correlation depicts the fact that when BTC close value increases, so does ETH close value, and vice versa and when they decrease...\n# ...but once BTC has increased, ETH has jointly increased, there are no predictive information here","b5e41b16":"# What about the correlation between percentage change in ETH close value and percentage change of the preceding record in BTC close value?\nf = plt.figure(figsize=(10,7))\nplt.scatter(x=btc_eth['Close_BTC'].pct_change(), y=btc_eth['Close_BTC'].pct_change().shift(-1));\nplt.xlabel('Lagged BTC close value % change');\nplt.ylabel('ETH close value % change');\nplt.title('ETH close value vs. BTC close value.');","04a2c9af":"## Data Wrangling","46c9ff5b":"### Data Imputation","63d5ce68":"### Looking for missing data","8212187f":"### Get the Data\n\nAvailable train data inclues test data, this leading to astounding scores on the public leaderboard. In time series, such a phenomenon when info about the future is leaked to our analysis or training process is called a **lookahead**. A lookahead is a way, through data, to find out something about the future earlier thant you ought to know it. Information about what will happen in the future propagates back in time in our modelling and affects how our model behaves earlier in time (generally tends to faking model performance improvement). Here, data posterior to *2021-06-13* is leaky, i.e. it is data bout the future that should not be used for anything else than testing models or assumptions. The `read_csv_strict()` function designed by [dataista0](https:\/\/www.kaggle.com\/julian3833) helps in avoiding the caveat of using future data for training models.","55303ba1":"### Classical Methods\n\nFirst we'll approach this data set with classical, non time series-specific methods.","a56864c1":"## Exploratory Data Analysis","cdc5f41f":"# G-Research Crypto Forecasting Data Wrangling and Exploratory Data Analysis"}}