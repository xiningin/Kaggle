{"cell_type":{"9b1664f9":"code","ab880368":"code","f2f6f31c":"code","f4303ac0":"code","769afc91":"code","667200f3":"code","62cb227f":"code","bf7f5f2f":"code","a30126ad":"code","3a71639e":"code","3d215d10":"code","c9461228":"code","0fa77285":"code","ef1c96aa":"code","6704ff67":"markdown","7d167560":"markdown","10f16f97":"markdown","d1424ea9":"markdown","d024f5b6":"markdown","899d2b16":"markdown","2336c804":"markdown","36a83837":"markdown","7aea33aa":"markdown","bed07c8d":"markdown","45c71353":"markdown","296ee822":"markdown","2c365bbe":"markdown","9d3f9106":"markdown","7cee8500":"markdown","4c2358b8":"markdown"},"source":{"9b1664f9":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding,Bidirectional,Flatten,Dense,GlobalAveragePooling1D,Conv1D,LSTM,GRU\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n%matplotlib inline","ab880368":"for file in os.listdir('..\/input\/sentiment140'):\n    print(file)","f2f6f31c":"columns=['target','id','date','flag','user','text']\ndf=pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',names=columns,encoding='latin-1')\ndf","f4303ac0":"df.isnull().sum()","769afc91":"le=LabelEncoder()\ndf['target']=le.fit_transform(df['target'])\nX=df['text']\nY=df['target']\nY = to_categorical(Y, 3)\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=4)","667200f3":"vocab=100000\nembedding_dim=64\noov_token='<OOV>'\npad_type='post'\ntrunc_type='post'\nmaxlen=64","62cb227f":"tokenizer=Tokenizer(num_words=vocab,oov_token=oov_token)\ntokenizer.fit_on_texts(X_train)\ntrain_sequences=tokenizer.texts_to_sequences(X_train)\ntrain_final=pad_sequences(train_sequences,padding=pad_type,truncating=trunc_type,maxlen=maxlen)\n\ntest_sequences=tokenizer.texts_to_sequences(X_test)\ntest_final=pad_sequences(test_sequences,padding=pad_type,truncating=trunc_type,maxlen=maxlen)\n","bf7f5f2f":"model=tf.keras.Sequential([Embedding(vocab,embedding_dim,input_length=64),\n                          Bidirectional(GRU(64)),\n                          Dense(64,activation='relu'),\n                          Dense(3,activation='softmax')\n                          ]\n                         )","a30126ad":"model.summary()","3a71639e":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy']\n             )","3d215d10":"history=model.fit(train_final,Y_train,epochs=3)","c9461228":"model.save('model.h5')","0fa77285":"loss=history.history['loss']\nacc=history.history['accuracy']\nplt.figure()\nplt.plot(loss,color='g')\nplt.title('Loss Vs Epochs')\nplt.figure()\nplt.plot(acc)\nplt.title('Acuracy Vs Epochs')","ef1c96aa":"model.evaluate(test_final,Y_test)","6704ff67":"**Let's check for null values in dataset**","7d167560":"# Building the Model","10f16f97":"# Details of Dataset\n\n**This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .**\n\n\n1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n\n2. ids: The id of the tweet ( 2087)\n\n3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n\n4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n\n5. user: the user that tweeted (robotickilldozr)\n\n6. text: the text of the tweet (Lyx is cool)","d1424ea9":"<h4><b>Note:<\/b> You can make the model performance better by increasing the number of epochs. Seeing the increasing curve in the graph its possible for the model to reach 90% accuracy within 10 epochs on training data.<\/h4>","d024f5b6":"<h3 style=\"color:red\">Thanks For Reading My Notebook And Don't Forget To Upvote The Notebook :)<\/h3>","899d2b16":"# Data Preprocessing","2336c804":"**Goal**: We have to categorize the text as positive, negative or neutral.\n\nTO execute the task we would be using Natural Language Processing(NLP) to draw sentiment from the texts and categorize them.","36a83837":"<h4><strong>NOTE:<\/strong> As the model trains long time to train I would be training the model for only on 3 epochs. You can train the model further by increasing the epochs and thus increasing the accuracy of the model<\/h4>","7aea33aa":"**Let's save our model so as not to lose the data and also we can use it in future without training the model again**","bed07c8d":"# Let's Import the Necessary Libraries","45c71353":"**In this model we would be using *GRU(Gated Recurrent Unit)* to increase the accuracy of our model.*You can also use LSTM or Conv2D in place of GRU while training the model.***","296ee822":"**Now Let's Evaluate the model on testing set**","2c365bbe":"<center><h2 style='color:blue'>Please Upvote The Notebook If You Find It Useful<\/h2><\/center>","9d3f9106":"**To analyze how are model perform we would plot the loss and accuracy vs the epochs using which we can determine if our model is better or we should make some changes while building it.**","7cee8500":"**We will now split the data into training and testing set**","4c2358b8":"# Model Evaluation"}}