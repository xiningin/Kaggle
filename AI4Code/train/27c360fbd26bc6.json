{"cell_type":{"becfda62":"code","50556723":"code","f12c1a9d":"code","fa9c247b":"code","332d7120":"code","9283311a":"code","f7599109":"code","e1fe6b9c":"code","316980d5":"code","28728ad1":"code","72ce4c3c":"code","3cd0b637":"code","f4990a5a":"code","3c10d0ef":"code","8f3ff2f5":"code","58978005":"code","9786d1c3":"code","04d2a5ad":"code","f9d9ca99":"code","e4bb8d92":"code","45a6539a":"code","c67672a4":"code","dc5eb4ad":"code","be6dcd2e":"code","a741a477":"code","c5d10787":"code","8b0e612c":"code","2c5a7937":"code","35af8ffb":"code","6d23be60":"code","1cf391a7":"code","d0b4f4c6":"code","ae045a4b":"code","c209a55a":"code","3c8c719f":"code","e4de80dc":"code","1016f1c1":"code","ee9cf0e4":"markdown","3dff21a9":"markdown","d5517562":"markdown","5163e9b2":"markdown","fef60f94":"markdown","ce187234":"markdown","12462160":"markdown","c66733be":"markdown","5d77b115":"markdown","368e5cce":"markdown","9bb87f4d":"markdown","02fdb315":"markdown","f8f7319a":"markdown","135d5487":"markdown","c907257f":"markdown","782b84bb":"markdown","e5cef943":"markdown","8a08ac79":"markdown","9cc3c0e3":"markdown","85de08a2":"markdown","393da861":"markdown","7b6e15e5":"markdown","ce8c5391":"markdown","35cb3e71":"markdown","8465880f":"markdown","676e8d04":"markdown"},"source":{"becfda62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#Read data from CSV input\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ncombine = [train_df, test_df]\nprint(train_df.columns.values)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50556723":"train_df.head()","f12c1a9d":"train_df.info()\nprint('_'*40)\ntest_df.info()\n#Missing value analysis\n#This will help identify how many missing values are in each column and take some suitable corrective action\ntrain_df.isnull().sum()","fa9c247b":"train_df.describe()","332d7120":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9283311a":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f7599109":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e1fe6b9c":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","316980d5":"#Visualize the data\ntrain_df.hist(\"Survived\", by=\"Pclass\", grid=\"False\", layout=[1,3],figsize = [10,3])\ntrain_df.hist(\"Survived\", by=\"Sex\",figsize = [10,3])\ntrain_df.hist(\"Survived\", by=\"Embarked\", layout=[1,3],figsize = [10,3])\ntrain_df.hist(\"Age\", bins=10, by = [\"Survived\", \"Sex\"], layout=[1,4],figsize = [20,3])","28728ad1":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\ntrain_df.head()\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","72ce4c3c":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n\nprint(train_df[\"Title\"].unique())    \npd.crosstab(train_df['Title'], train_df['Sex'])","3cd0b637":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","f4990a5a":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()\ntrain_df.tail()","3c10d0ef":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","8f3ff2f5":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntrain_df.head()","58978005":"\n#getting avg age for each tile and assigning avg of that age to same title with missing value\n\ntitles = train_df[\"Title\"].unique()\nfor title in titles:\n    avg_age = train_df[((train_df[\"Title\"]==title) & (train_df[\"Age\"].isnull()==False))][\"Age\"].mean()\n    train_df.loc[((train_df[\"Title\"]==title) & (train_df[\"Age\"].isnull()==True)).tolist(),'Age']=round(avg_age)#replace the missing age values in each group with the corresponding average values\n    test_df.loc[((test_df[\"Title\"]==title) & (test_df[\"Age\"].isnull()==True)).tolist(),'Age']=round(avg_age)#replace the missing age values in each group with the corresponding average values\n    print(\"Average age for title\", title,\"is\", str(round(avg_age)))#printing average values for each title\n\ntrain_df.head()","9786d1c3":" \ntrain_df[\"Age\"].describe()","04d2a5ad":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f9d9ca99":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","e4bb8d92":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","45a6539a":"freq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c67672a4":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","dc5eb4ad":"\"\"\"\nWe can now complete the Fare feature for single missing value in test dataset using mode to\nget the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation \nto guess missing feature as we are replacing\nonly a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.\n\"\"\"\n\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head(20)","be6dcd2e":"#Creating fareband feature and analyzing effect of fare on survivability\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'],4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","a741a477":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","c5d10787":"test_df.head(10)","8b0e612c":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","2c5a7937":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","35af8ffb":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","6d23be60":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","1cf391a7":"# k-NN\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","d0b4f4c6":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","ae045a4b":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","c209a55a":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","3c8c719f":"#adaboost with decision stump\nadaBoost = AdaBoostClassifier(base_estimator=None,\n                              learning_rate=1.0,\n                              n_estimators=100)\n\nadaBoost.fit(X_train, Y_train)\n\nY_pred = adaBoost.predict(X_test)\n\nacc_adaBoost = round(adaBoost.score(X_train, Y_train) * 100, 2)\nacc_adaBoost","e4de80dc":"random_forest = RandomForestClassifier(n_estimators = 80, max_features='auto', criterion='entropy',max_depth=4)\nadaboost_random_forest = AdaBoostClassifier(base_estimator=random_forest,\n                              learning_rate=1.0,\n                              n_estimators=10)\n\nadaboost_random_forest.fit(X_train, Y_train)\n\nY_pred = adaBoost.predict(X_test)\n\nacc_adaboost_random_forest = round(adaBoost.score(X_train, Y_train) * 100, 2)\nacc_adaboost_random_forest","1016f1c1":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","ee9cf0e4":"# Which features contain blank, null or empty values?\n\nThese will require correcting.\n\nCabin > Age > Embarked features contain a number of null values in that order for the training dataset.\nCabin > Age are incomplete in case of test dataset.\nWhat are the data types for various features?\n\n# Helping us during converting goal.\n\nSeven features are integer or floats. Six in case of test dataset.\nFive features are strings (object).","3dff21a9":"Convert the Fare feature to ordinal values based on the FareBand.","d5517562":"Decision tree has the best score","5163e9b2":"# Converting categorical feature to numeric\nConverting categorical feature to numeric","fef60f94":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference Wikipedia.\n\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.","ce187234":"# Which features are categorical?\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\nCategorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n# Which features are numerical?\n\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\nContinous: Age, Fare. Discrete: SibSp, Parch.","12462160":"# Create new feature combining existing features\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","c66733be":"So the unique passenger titles are 'mr.' 'mrs.' 'miss.' 'master.' 'don.' 'rev.' 'dr.' 'mme.' 'ms.' 'major.'\n 'lady.' 'sir.' 'mlle.' 'col.' 'capt.' 'countess.' 'jonkheer.'","5d77b115":"## Assumtions based on data analysis\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n# Correlating.\n\n* We want to know how well does each feature correlate with Survival.\n* We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n# Completing.\n\n* We may want to complete Age feature as it is definitely correlated to survival.\n* We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n# Correcting.\n\n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n# Creating.\n\n* We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n* We may want to engineer the Name feature to extract Title as a new feature.\n* We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n* We may also want to create a Fare range feature if it helps our analysis.\n# Classifying.\n\n* We may also add to our assumptions based on the problem description noted earlier.\n\n* Women (Sex=female) were more likely to have survived.\n* Children (Age<?) were more likely to have survived.\n\n* The upper-class passengers (Pclass=1) were more likely to have survived.","368e5cce":"We can create another feature called IsAlone.","9bb87f4d":"There are lot of missing values in age coloumn. To replace the missing values we can replace them with simple avg of age or do more feature engineering such as give avg age based on their title.**** ","02fdb315":"# Completing a categorical feature\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","f8f7319a":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","135d5487":"Submission","c907257f":"# Wrangle data\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n# Correcting by dropping features\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.","782b84bb":"# Model, predict and solve\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. ","e5cef943":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","8a08ac79":"# What is the distribution of numerical feature values across the samples?\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers (<1%) within age range 65-80.","9cc3c0e3":"We can convert the categorical titles to ordinal.","85de08a2":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference Wikipedia.\n\nThe model confidence score is the highest among models evaluated so far.","393da861":"Let us fine tune random forest with adaboost for better accuracy ","7b6e15e5":"# Analyze by pivoting features\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n* Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n* Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n* SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","ce8c5391":"# Which features are mixed data types?\n\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\n\nTicket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n# Which features may contain errors or typos?\n\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\n\nName feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.","35cb3e71":"# We can replace many titles with a more common name or classify them as Rare.","8465880f":"# What is the distribution of categorical features?\n\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).","676e8d04":"# Converting a categorical feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0."}}