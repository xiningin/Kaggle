{"cell_type":{"654eafa6":"code","44c0eeb0":"code","8500b242":"code","5380b0d1":"code","b3779777":"code","a5b6dd09":"code","da0dff57":"code","1845bbb1":"code","84d6dd0a":"code","c98e0ab7":"code","f254e7c9":"code","303d2d1d":"code","f53615b9":"code","01dec0fe":"code","f7327ff0":"code","008ff423":"code","0826fbc8":"code","f63287bd":"code","2d7c6f0f":"code","c8708449":"code","9d1e43ff":"code","3768bc28":"code","f9e27ae1":"code","156d64f9":"code","16c70bce":"code","594d7c03":"code","d41617d7":"code","f341b944":"code","cb4d7ede":"markdown","348df1d9":"markdown","45322960":"markdown","640a4db5":"markdown","0637b12e":"markdown","4b7dd507":"markdown","7cca2d77":"markdown","cbcbed7f":"markdown","7d7e8dec":"markdown","565b945d":"markdown","642a7022":"markdown","f7574a7d":"markdown","0526c909":"markdown","2c9dace9":"markdown","415abdc8":"markdown","f7e22f3a":"markdown"},"source":{"654eafa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44c0eeb0":"df = pd.read_csv('\/kaggle\/input\/panama-electricity-load-forecasting\/train.csv')","8500b242":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dropout, Flatten","5380b0d1":"df.head()","b3779777":"print(df.dtypes)","a5b6dd09":"df.isnull().sum()","da0dff57":"df['datetime']=pd.to_datetime(df['datetime'])","1845bbb1":"df.head()","84d6dd0a":"#from datetime import datetime\ndf['week_day']=df['datetime'].dt.dayofweek\ndf['date']=df['datetime'].dt.day\ndf['month']=df['datetime'].dt.month\ndf['hour']=df['datetime'].dt.hour\ndf.head()","c98e0ab7":"#pip install autoviz","f254e7c9":"#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#d = AV.AutoViz(df)","303d2d1d":"from matplotlib.pyplot import figure\nfigure(figsize=(30, 6))\nddd=df[0:15000]\nddd.groupby('datetime')['nat_demand'].median().plot()","f53615b9":"def add_features(df):\n    df['T2M_toc_s']=df['T2M_toc'].shift(-1).fillna(0)\n    df['QV2M_toc_s']=df['QV2M_toc'].shift(-1).fillna(0)\n    df['TQL_toc_s']=df['TQL_toc'].shift(-1).fillna(0)\n    df['W2M_toc_s']=df['W2M_toc'].shift(-1).fillna(0)\n    df['T2M_toc_s']=df['T2M_san'].shift(-1).fillna(0)\n    df['QV2M_san_s']=df['QV2M_san'].shift(-1).fillna(0)\n    df['TQL_san_s']=df['TQL_san'].shift(-1).fillna(0)\n    df['W2M_san_s']=df['W2M_san'].shift(-1).fillna(0)\n    df['T2M_dav_s']=df['T2M_dav'].shift(-1).fillna(0)\n    df['QV2M_dav_s']=df['QV2M_dav'].shift(-1).fillna(0)\n    df['TQL_dav_s']=df['TQL_dav'].shift(-1).fillna(0)\n    df['W2M_dav_s']=df['W2M_dav'].shift(-1).fillna(0)\n    df['Holiday_ID_s']=df['Holiday_ID'].shift(-1).fillna(0)\n    df['holiday_s']=df['holiday'].shift(-1).fillna(0)\n    df['school_s']=df['school'].shift(-1).fillna(0)\n    \n    df['T2M_toc_s1']=df['T2M_toc'].shift(-2).fillna(0)\n    df['QV2M_toc_s1']=df['QV2M_toc'].shift(-2).fillna(0)\n    df['TQL_toc_s1']=df['TQL_toc'].shift(-2).fillna(0)\n    df['W2M_toc_s1']=df['W2M_toc'].shift(-2).fillna(0)\n    df['T2M_toc_s1']=df['T2M_san'].shift(-2).fillna(0)\n    df['QV2M_san_s1']=df['QV2M_san'].shift(-2).fillna(0)\n    df['TQL_san_s1']=df['TQL_san'].shift(-2).fillna(0)\n    df['W2M_san_s1']=df['W2M_san'].shift(-2).fillna(0)\n    df['T2M_dav_s1']=df['T2M_dav'].shift(-2).fillna(0)\n    df['QV2M_dav_s1']=df['QV2M_dav'].shift(-2).fillna(0)\n    df['TQL_dav_s1']=df['TQL_dav'].shift(-2).fillna(0)\n    df['W2M_dav_s1']=df['W2M_dav'].shift(-2).fillna(0)\n    \n    df['nat_demand3']=df['nat_demand'].shift(3).fillna(0)\n    df['nat_demand4']=df['nat_demand'].shift(4).fillna(0)\n    df['nat_demand5']=df['nat_demand'].shift(5).fillna(0)\n    df['nat_demand6']=df['nat_demand'].shift(6).fillna(0)\n    df['nat_demand7']=df['nat_demand'].shift(7).fillna(0)\n    df['nat_demand8']=df['nat_demand'].shift(8).fillna(0)\n    df['nat_demand9']=df['nat_demand'].shift(9).fillna(0)\n    df['nat_demand10']=df['nat_demand'].shift(10).fillna(0)\n    df['nat_demand11']=df['nat_demand'].shift(11).fillna(0)\n    df['nat_demand12']=df['nat_demand'].shift(12).fillna(0)\n    df['nat_demand13']=df['nat_demand'].shift(13).fillna(0)\n    df['nat_demand14']=df['nat_demand'].shift(14).fillna(0)\n    df['nat_demand_n']=df['nat_demand']  \n    #df = pd.get_dummies(df)\n    return df","01dec0fe":"df1 = add_features(df)","f7327ff0":"col=['datetime','nat_demand']\nnew_df= df1.drop(columns=col)\ndf_for_training = new_df.astype(float)\ndf_for_training.head(10)","008ff423":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler = scaler.fit(df_for_training)\ndf_for_training_scaled = scaler.transform(df_for_training)","0826fbc8":"df_for_training_scaled","f63287bd":"X=np.array(df_for_training)\nprint(X)","2d7c6f0f":"trainX = []\ntrainY = []\ntrainY = df_for_training['nat_demand_n'].to_numpy().reshape(-1,1)\ntrainX = df_for_training.to_numpy().reshape(-1, 57,1)\n#trainY = df[['pressure']].to_numpy().reshape(-1, 80)\n#trainX = df.reshape(-1, 80, df.shape[-1])      df_for_training.shape[-1]","c8708449":"#trainX = []\n#trainY = []\n#n_future = 1 \n#n_past = 48\n#for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n#    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n#    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])","9d1e43ff":"trainX, trainY = np.array(trainX), np.array(trainY)\nprint(trainX.shape)\nprint(trainY.shape)","3768bc28":"np.set_printoptions(precision=3)\nnp.set_printoptions(suppress=True)\nnp.set_printoptions(threshold=2000)\n#p = PrintArray(precision=4, linewidth=150, suppress=True)\ntrainX","f9e27ae1":"model = Sequential()\nmodel.add(Bidirectional(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),return_sequences=True)))\n                             #return_sequences=True))\nmodel.add(Bidirectional(LSTM(32, activation='relu', return_sequences=True)))\n#model.add(LSTM(64, activation='relu', return_sequences=True))\n#model.add(Bidirectional(LSTM(24, activation='relu', return_sequences=False)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))#, return_sequences=False))\n#model.add(Dense(32, activation='relu'))#, return_sequences=False))\nmodel.add(Dense(8, activation='relu'))#, return_sequences=False))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='adam', loss='mse')\n#model.summary()","156d64f9":"history = model.fit(trainX, trainY, epochs=5, batch_size=200,verbose=1)\n#validation_split=0.2,\nplt.plot(history.history['loss'], label='Training loss')\n#plt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()","16c70bce":"df_test= pd.read_csv('\/kaggle\/input\/panama-electricity-load-forecasting\/Test_Jan.csv',encoding=\"latin-1\")\ndf_test['datetime']=pd.to_datetime(df_test['datetime'])\ndf_test['week_day']=df_test['datetime'].dt.dayofweek\ndf_test['date']=df_test['datetime'].dt.day\ndf_test['month']=df_test['datetime'].dt.month\ndf_test['hour']=df_test['datetime'].dt.hour\ndf_test = add_features(df_test)\ndf_for_pred=df_test.drop(columns=col)\ndf_for_pred.astype(float)\ndf_pred_scaled=scaler.transform(df_for_pred)\n\nX_pred = []\nfor i in range(n_past, len(df_pred_scaled) - n_future +1):\n    X_pred.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\nX_pred= np.array(X_pred)\n\nprint('X for prediction shape == {}.'.format(X_pred.shape))","594d7c03":"prediction = model.predict(X_pred) \nprediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\ny_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\ny_pred_future\ndf_forecast=[]\ndf_forecast = pd.DataFrame(y_pred_future)\ntemp_df=[]\ntemp_df=pd.DataFrame(np.zeros(n_past))\ndf_forecast =pd.concat([temp_df,df_forecast], ignore_index=True)\ndf_forecast\ndf_actual= pd.read_csv('\/kaggle\/input\/panama-electricity-load-forecasting\/Predict_Jan.csv',encoding=\"latin-1\")\ndf_actual['Pred_nat_demand']=df_forecast\ndf_actual = df_actual.iloc[n_past:]\ndf_actual\na4_dims = (25,8)\nfig, ax = plt.subplots(figsize=a4_dims)\nsns.lineplot(df_actual['datetime'], df_actual['nat_demand'])\nsns.lineplot(df_actual['datetime'], df_actual['Pred_nat_demand'])","d41617d7":"prediction = model.predict(X_pred) \nprediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\ny_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\ny_pred_future\ndf_forecast=[]\ndf_forecast = pd.DataFrame(y_pred_future)\ntemp_df=[]\ntemp_df=pd.DataFrame(np.zeros(n_past))\ndf_forecast =pd.concat([temp_df,df_forecast], ignore_index=True)\ndf_forecast\ndf_actual= pd.read_csv('\/kaggle\/input\/panama-electricity-load-forecasting\/Predict_Jan.csv',encoding=\"latin-1\")\ndf_actual['Pred_nat_demand']=df_forecast\ndf_actual = df_actual.iloc[n_past:]\ndf_actual\na4_dims = (25,8)\nfig, ax = plt.subplots(figsize=a4_dims)\nsns.lineplot(df_actual['datetime'], df_actual['nat_demand'])\nsns.lineplot(df_actual['datetime'], df_actual['Pred_nat_demand'])","f341b944":"df_forecast\n#df_forecast","cb4d7ede":"It seems that there is no NULL values..Great...!!!","348df1d9":"I think we have reasonable insights to create a model. You may further to the EDA to find more insghts if you want..","45322960":"Model has performed well on validation data...\nNow, make ready the Test data set ready for prediction Jan-2020","640a4db5":"Let's look at the 'nat_demand' with the short time frame....","0637b12e":"From above- There are not Categorical dataset only Numerical dataset.\nBut datetime column data type is object. Needs to be change","4b7dd507":"Scaling of data is required....","7cca2d77":"# Data Preprocessing","cbcbed7f":"# Make a Prediction","7d7e8dec":"# The Most Important part...\nNow, for this perticular dataset the past values of the records are import to predict the future demand and the Model has to be dynamic to learn from past data and predict the future demand.\n\n- For this, we have to create parts of data having past 2 days (48 hours)of the data to give it the same to the LSTM model to predict the 49th Hour Load prediction.\n\nI hope I clear, if not please ask me in discussion I will clear your doubt. This is the most import part of the this Notebook","565b945d":"datetime columne is directly can not be used for the Modem building but the we can create a features that repeated in nature any give that to the model so model can identify the pattern related to that features.","642a7022":"# Creating a LSTM Multivariant model \n\nNow, we have to create a multi stage Nuetral network -LSTM (Long Short Term Memory) moedel. \nThe Modem will have the return sequence True to understand the sequncing of the data and identify the pattern. Hope it works the fine.","f7574a7d":"# EDA Observations\nAlmost all variables are following the Gaussian curve - No need to modify the parameters further\nTemp has the Maximum relationship with the required prediction 'nat_demand'\n'Holiday_ID' and 'Holiday' have the negative impact on the predictions values. We may neglect them if our model is not performing well.\nAverage 'nat_demand' on Holidays are less than Normal days.\n","0526c909":"As you can see, values are not in the format in which we required.\nWe have to do the inverse Transform..","2c9dace9":"# Timeseries Multivariant Forecasting with LSTM model\n\nThis notebook is about forecasting of the timeseries Load data avaialable in Train.csv.\nThe change in Load is depend on the multiple variables like Temp, Humidity, Wind and Precipitation.\n\nWe will train a LSTM model, which will take  hourly records available from 2015 to 2019.\nThen, we will predict the future demand for Jan-2020 and Compare with actual data that how model has worked...","415abdc8":"Convert it to Numpy array....Think why >>>","f7e22f3a":"# Train the Model"}}