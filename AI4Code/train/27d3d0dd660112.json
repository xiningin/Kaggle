{"cell_type":{"ff9f1cf1":"code","39af5912":"code","67f4bfb7":"code","f6ece7c4":"code","ed02b0e5":"code","d6e22473":"code","1cb90ee6":"code","102fddd3":"code","aef2fc39":"code","ac6ff62a":"code","6d1bbe16":"code","9fd0639d":"code","1805c9c9":"code","8ac5cbeb":"code","9c5aaba8":"code","3ad38d4e":"code","8cbde1d9":"code","7e34385b":"code","ce22675f":"code","62f38bb9":"code","02934da0":"code","7f68b124":"code","e3b1f640":"code","1dc7aa3e":"code","7d8110f7":"code","e1766e7a":"markdown","de5752c1":"markdown","846c5793":"markdown","b8ab053b":"markdown","156c7730":"markdown","2f68ea42":"markdown","b13dfac2":"markdown","fc57edc5":"markdown","a0fa3f6b":"markdown","f86e5323":"markdown","0d693c92":"markdown","4380f58b":"markdown","4ab2853c":"markdown","fec16b73":"markdown","228721c6":"markdown","8a1cbdee":"markdown"},"source":{"ff9f1cf1":"from IPython.display import display\nfrom PIL import Image\npath=\"..\/input\/breast-cancerjpg\/breast cancer.jpg\"\ndisplay(Image.open(path))","39af5912":"#Basic library \nimport pandas  as pd \nimport numpy as np \nimport math \nimport re \n\n#visualization \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport cufflinks as cf\n%matplotlib inline\ncf.go_offline()\nsns.set_style('whitegrid')\n","67f4bfb7":"#importing Data\nData = '..\/input\/breast-cancer\/Breast_cancer.csv'\ndf = pd.read_csv(Data)\ndf.drop('Unnamed: 0', axis = 1 , inplace = True)\ndf.tail(5)","f6ece7c4":"df.info()","ed02b0e5":"df.describe()","d6e22473":"df['Class'].value_counts()   # Data to predict (1=Posstive Cancer , 0 = Negatice Cancer)","1cb90ee6":"df.corr()","102fddd3":"sns.heatmap(df.isnull() ,  yticklabels='False' , cmap = 'plasma')\nprint(df.isnull().any())","aef2fc39":"# the columns ' ID'  Does not give any important Data or correlation \ndata = df.drop('Id' , axis = 1)\n\n#Find Nan Value Index \nNan_index = data[data.isna().any(axis=1)].index \ndata[data.isna().any(axis=1)][0:5]","ac6ff62a":"# Finding cell size Values\nsize= data[data.isna().any(axis=1)].iloc[:,1].to_list()\n\n# Finding cell Shape Values\nshape = data[data.isna().any(axis=1)].iloc[:,2].to_list()\nprint(\"size index =\",size)\nprint(\"shape index =\",shape)","6d1bbe16":"#crate a empty list to collect the mode of Bare.nucle\nMode_Bare_nuclei = []  #crate a empty list to collect the mode of Bare.nuclei\nfor i, j in zip(size,shape):  # collecting Data \n    Mod= data.loc[(data['Cell.size'] == i) & (data['Cell.shape'] == j)]['Bare.nuclei'].mode()[0]\n    Mode_Bare_nuclei.append(Mod)","9fd0639d":"#replacing Data using index \nfor i, j in zip(Nan_index,Mode_Bare_nuclei):\n    data.loc[i,'Bare.nuclei'] = j\n\ndata.iloc[Nan_index][0:3] # Checking We have no more Nan_values","1805c9c9":"fig, ax1 = plt.subplots (figsize= (20,15))\nax = sns.heatmap(df.corr(),ax=ax1 , linecolor = 'white',linewidths=0.5, \n            annot = True,cmap=sns.diverging_palette(20, 220, n=200))\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","8ac5cbeb":"df_re = df[['Cell.size','Cell.shape','Bare.nuclei','Bl.cromatin']]\ndf_re.iplot(kind = 'box',)\nprint(\"I consider these 4 variable important , therefore I was looking for any clue or trend \")","9c5aaba8":"f, ax = plt.subplots(figsize=(20, 10))\nsns.violinplot(data=df.drop([\"Id\",\"Class\"],axis = 1), width = 0.9, inner = 'quartile')\nprint(\"Here we have a Visual description of mean and the Data Disstribution\")","3ad38d4e":"fig ,(ax1,ax2,ax3) = plt.subplots(1,3,figsize = (20,5))\nsns.countplot(x ='Cl.thickness', data = df , hue='Class',  ax = ax1 )\nsns.countplot(x ='Cell.size', data = df, hue='Class' , ax = ax2)\nsns.countplot(x ='Cell.shape', data = df, hue='Class' , ax = ax3)\nprint('Relation Thickness, size, shape')","8cbde1d9":"fig ,(ax4,ax5,ax6) = plt.subplots(1,3,figsize = (20,5))\nsns.countplot(x ='Marg.adhesion', data = df, hue='Class' , ax = ax4 )\nsns.countplot(x ='Epith.c.size', data = df, hue='Class' , ax = ax5 )\nsns.countplot(x ='Bare.nuclei', data = df, hue='Class' , ax = ax6 )\nprint('Relation Marg.Adhension, Epith C.Size , Bare.nuclei')","7e34385b":"fig ,(ax7,ax8,ax9) = plt.subplots(1,3,figsize = (20,5))\nsns.countplot(x ='Bl.cromatin', data = df, hue='Class' , ax = ax7 )\nsns.countplot(x ='Normal.nucleoli', data = df, hue='Class' , ax = ax8 )\nsns.countplot(x ='Mitoses', data = df, hue='Class' , ax = ax9 )\nprint('Relation Bl.cromatin', 'Normal.nucleoli' , 'Mitoses')","ce22675f":"### applying KNN algorithm\nfrom sklearn.neighbors import KNeighborsClassifier  # Algorithm\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report # metrics\nfrom sklearn.preprocessing import StandardScaler #scaler to 0-1\nfrom sklearn.model_selection import train_test_split #train and split ","62f38bb9":"#StandarScale with new Dataframe\nscaler = StandardScaler()\nscaler_data = scaler.fit_transform(data.drop('Class',axis = 1)) \ndata_p = pd.DataFrame(scaler_data , columns= data.columns[:-1])\ndata_p.head(4) # data_p = Data already Processed","02934da0":"#Splitting Data\nX = data_p\ny = data ['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","7f68b124":"#Finding the n_neighbors that fits the best \nError_Rate = []\nfor i in range (1,50):\n    \n    KNN_Error = KNeighborsClassifier(n_neighbors=i)\n    KNN_Error.fit(X_train,y_train)\n    pred_i = KNN_Error.predict(X_test)\n    Error_Rate.append(np.mean(pred_i != y_test))","e3b1f640":"plt.figure(figsize=(10,6))\nplt.plot(range(1,50), Error_Rate , color = 'blue', linestyle = 'dashed', marker = 'o')\nplt.title('Error rate vs K value')\nprint( \" K=3 is the most accurate rate because the error is closest to 0 \")","1dc7aa3e":"#KNN Algorithm\nKNN = KNeighborsClassifier(n_neighbors=3)\nKNN.fit(X_train,y_train)\n#Predicting \ny_pred_KNN = KNN.predict(X_test)","7d8110f7":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression().fit(X_train,y_train)\ny_predi_LR = LR.predict(X_test)\n\nprint(confusion_matrix(y_test,y_predi_LR))\nprint(f1_score(y_test,y_predi_LR))\nprint(classification_report(y_test,y_predi_LR))","e1766e7a":"> It is evident that most of the mean are between 0 ~ 2 , this might be because it is from microscopic view","de5752c1":"# OSEMN - E - Exploring \/ Visualizing our data\n**Finding any Patterns and Trends**","846c5793":"** Conclusion**\n1. Bl.cromatin : In My understanding it is a mechanical and chemical signalling pathways , It this study (https:\/\/pubmed.ncbi.nlm.nih.gov\/10546890\/) they mentioned that tit is  measured from 1 to 10 (no idea what) the chances of having a maling tumor increase whrn it is above 3\n2. nucleolus :  in this study  mentioned nucleolus as an opponent of cancer cells . In response to stress,some of this anti-cancer body friends participate in preventing impaired or transformed cells from spreading ( the more that you have The less chances of having malignant cancer \" I like this cells)\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6267422\/#:~:text=A%20nucleolus%20as%20an%20opponent%20of%20cancer%20cells%20via%20p53,or%20transformed%20cells%20from%20spreading.\n3. Mitoses :  After reading I understood that mitosis it is a natural process of the cell (reproduce, divide , etc) but sometime there are some mistakes , I guess in this study they were measuring form a sample ( perfect cell Vs mutants) , if it is like that, I would say the More perfec cells you have , it is a good sight the yout body is copying and pasting new cel properly (https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6004076\/","b8ab053b":"### Conclusion \nKNN seems to be a good model to predict the breast Cancer because:\n    1. the acurrance is 96 %\n    2. The best K_neigh number is  3 ","156c7730":"**Applying Maching Learning**\n1. k-nearest neighbors (KNN)\n2. Logistical Regression","2f68ea42":"# OSEMN  M \nModeling our data will give us our predictive power as a wizard","b13dfac2":"****As mentioned Before Cell.size ~ Cell.Shape ~ and Bare.Nuclei have strong relation ","fc57edc5":"**Conclusion**\n1. cl.Thickness:It is intersting to notice that having really thick plasma membrane is not guarantee of  benign or malignant tumor of course no one can guaratee it, but no matter the thickness (1 or 10) cases has been reported ,  and this study suggest  \"The magnitude of the sensed stiffness can either promote or inhibit the migration of cancer cells out of the primary tumor into surrounding tissue\" CHECK THIS :https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S092777651930178X\n\n2. Cl.Size: in this cases the data shows most 0 (benign  tumor) happened to have cell size = 1 , I would say the bigger the cel is the higher chances of having malignant tumor, here we have a possible reason \" Cancer is a complex genetic disease that is caused by specific changes to the genes in one cell or group of cells. These changes disrupt normal cell function \u2013 specifically affecting how a cell grows and divides. CHECK THIS : https:\/\/www.technologynetworks.com\/cancer-research\/articles\/cancer-cells-vs-normal-cells-307366\n\n3. Cl.Shape Same cases of Cl.size\n","a0fa3f6b":"**KNN and Logistica Regression**\n1. In this Kernel I am using a  breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. To predict benign or malignant Cancer\n2. I am using OSEMN methodology (Obtain, Scrub, Explore, Model, iNterpret) to standardized my data analysis process\n3. I am using KNN and Logistical Regression to predict benign or malignant Cancer\n4. Any feedback it is more than welcome","f86e5323":"\n# Data Science OSEMN \n**Obtaining Our Data**","0d693c92":"# OSEMN - Scrubbing \/ Cleaning our data","4380f58b":"**KNN**\n1. The Challenge in KNN it is to find K# that fits the best , in order to do that I First evaluated K from a range (0-50) and stimate the Error ( This is my First KNN)\n2. Then I will used the KNN that has less error (I read on the Web that this method it is called the Elbow Method)\n3. If you Know any other way of Doing that Please let me know ","4ab2853c":"## Logistical Regression ","fec16b73":"**Conclusion**\n1. Marg.Adhension : The greater the Marg.Adhension  values is the higher chances of having malignant tumor also increase , one reason could has been explained in this study \"they affirmed that a group of cell adhesion molecules have been discovered with a wide spectrum of responsibilities, including recruiting, activating, elongating, and maintaining\". CHECK HERE : https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5564663\/\n\n2. Eptih size : The greater the Marge Adhension values is the higher chances of having malignant tumor also increase one of the reason coud be because  according to this study Most cancers arise in epithelia, the tissue type that lines all body cavities. The organization of epithelia enables them to act as a barrier and perform transport of molecules between body compartments. Crucial for their organization and function is a highly specialized network of cell which reveals an important contribution of polarity proteins to the initiation and progression of cancer\" (please cheeck this link for the references  CHECK HERE : https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3927155\/\n\n3. Bare.nuclei : one more time the bigger it is , the higher chances are .. one of the reason could be this \"is one of the most common cancers occurring in the female population world-wide. and it seems that this cells gradually transform to form the cancer cells\"  CHECK HERE : https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3678677\/","228721c6":"# After thinking what would be the best way to handle Missing data\n\n**I concluded that filling the data with mean or mode it wouldbe great , however after taking a look of the Corre (cell-size ~ Cell.shape ~ Bare.nuclei)**\n\n1. I would find the index of the missing Value\n2. I would match other ID (people) Who cell-size ~ Cell.shape are equal to those missing Value\n3. I will find the mode (My assumption is \" if they share same size and shape they just have something in common \")","8a1cbdee":"## KNN seems to have a better performance, however I would consider the fact that we have\n1. 0=458 (benign) and 1=241 (malignant) and this might affect the model (bias) \n2. Cell shape, Cell Size and nucleolus are the most significante varaiable based on the correlation \n3. Between KKN and Logistical Regressiion , KNN predicts better with a f1 score of 96 % vs 95%"}}