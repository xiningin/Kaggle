{"cell_type":{"8281683d":"code","2b9f87ff":"code","8ef6dbe3":"code","ba39adfb":"code","9380b428":"code","1b828db2":"code","62675c3e":"code","3384395c":"code","222d92a6":"code","1bc52489":"code","9d74b731":"code","af52fa27":"code","7ac09b21":"code","9a975fc3":"code","799b2342":"code","97439090":"code","3d73b40f":"code","10ec7e01":"code","de90c157":"code","3434e0f8":"code","96f52ff3":"code","298f8a43":"code","37a21259":"code","fc2455ff":"code","2789b839":"code","3f0b518f":"code","94d34ed1":"code","211d52b6":"code","4a93e2eb":"code","c442b8f3":"code","4d36713e":"code","371e0fe2":"code","37416e72":"code","7c8ab596":"code","c40dc3da":"code","9a1fc51f":"code","807521ba":"code","1b27af6a":"code","c53f45c0":"code","595842de":"code","5675e28f":"code","f9dff2c4":"code","734eefbc":"code","24adcbdf":"code","6aa2f548":"code","3f974fd6":"code","eb85bbe6":"code","6e051d55":"code","e4d79d89":"code","27f1847b":"code","2e1e479d":"code","6ece12a3":"code","259f808b":"code","8431d084":"code","658530e1":"code","2df97a72":"code","d955bef5":"code","d3124e01":"code","7994aa14":"code","20e9c1b9":"code","1081c825":"code","7f5c7cf2":"code","d00a0e1a":"code","d6e9eaff":"code","427025e9":"code","42a73fb2":"code","3f671759":"code","bb0099c7":"code","aa01b97b":"code","78aa697a":"code","252d50b6":"code","311e9d76":"code","7d4decde":"code","ddeb2956":"code","732e9388":"code","23c8fd3a":"code","46d0989b":"code","ea120e67":"code","3dedfb67":"code","449aa1b7":"code","dcb2f786":"code","0ba262bf":"code","561ca8a9":"code","55c747ef":"code","802b22e8":"code","dc6dce76":"code","83a0bd83":"code","c4c09542":"code","3faa26a6":"code","793ce489":"code","b7327a8f":"code","b0ccc2ec":"code","b618c382":"code","bfff88aa":"code","c241bac0":"code","ff8b27a5":"code","0a448e06":"code","d1a8f301":"code","eac528b0":"code","491c920a":"code","af93507e":"code","f2474c1b":"code","fc60412d":"code","c96015c4":"code","01eb9aed":"code","9bc4283a":"code","10cb1de9":"code","b6d2b30c":"code","a98de8ac":"code","89615206":"code","96b2d451":"code","7cda907f":"code","ff35d6af":"code","8905487b":"code","94585bfd":"code","167fa434":"code","f017d832":"code","473dc012":"code","e15f6c99":"code","28882f99":"code","2389c29a":"code","63035603":"code","13404b41":"code","b8bd3f3f":"code","180fe34c":"code","c7c5f6c0":"code","4eb8ac55":"code","7e559d93":"code","69907edf":"code","d2c1e59f":"code","7eb8ceff":"code","7d3802e8":"code","dfde7afc":"code","377513a5":"code","0bf1df8f":"code","2b6bc008":"code","edf41e04":"code","d43379e8":"code","3751b8ca":"code","4a1e90ce":"code","a4cf0c7a":"code","cc0397fd":"code","32b14f49":"code","406820d7":"code","a42b3743":"code","8d299882":"code","c50c1848":"code","3a4afcc6":"markdown","29a3e46e":"markdown","d64df226":"markdown","45a2cd49":"markdown","1866ceb4":"markdown","552e324a":"markdown","1efa1abd":"markdown","02cd35d1":"markdown","aeff3931":"markdown","3ba768eb":"markdown","1654ed75":"markdown","7ee5ea71":"markdown","24bd00a0":"markdown","b341a915":"markdown","ddf98252":"markdown","6ead5f5f":"markdown","25a364bf":"markdown","d43d15c2":"markdown","fedf283c":"markdown","c11e3a93":"markdown","e1096671":"markdown","8b6f7c90":"markdown","fd099116":"markdown","a84b923f":"markdown","9152900e":"markdown","b7e154fa":"markdown","3fb0b3b0":"markdown","7789e9a4":"markdown","9e4723a9":"markdown","faad0475":"markdown","abff98e4":"markdown","c10bbccd":"markdown","79883b39":"markdown","a58d9144":"markdown","43a7d288":"markdown","517a7743":"markdown","687f5466":"markdown","b9185b63":"markdown","b78cd1bf":"markdown","b94cc51a":"markdown","f7f97e58":"markdown","6542c805":"markdown","f716d318":"markdown","1eb75a2c":"markdown","9ab19c1b":"markdown","135f2172":"markdown","7a246625":"markdown","eeace952":"markdown","8836c9fe":"markdown","41bdca57":"markdown","d7eb168a":"markdown","aefe0ea3":"markdown","ecdab457":"markdown","982a935f":"markdown","067b285d":"markdown","406e7baf":"markdown","a62bdc59":"markdown","d07a3a28":"markdown","b1f91008":"markdown","bf1a2135":"markdown","194b2b4c":"markdown","2bb9d924":"markdown","7de1a1ef":"markdown","ffb4f147":"markdown","fd3ada0b":"markdown","6615fa02":"markdown","bc7bc80b":"markdown","b978b106":"markdown","46cb3aae":"markdown","2c7e579a":"markdown","36845a71":"markdown","96f94f15":"markdown","974581ed":"markdown","ef8bb747":"markdown","e35a3984":"markdown","02a0a7aa":"markdown","0f169c2c":"markdown","e7e07ed7":"markdown","55ac44df":"markdown","6425f3d8":"markdown","d1a6ee56":"markdown","0ac44af1":"markdown","6ee8afd7":"markdown","88f32e6c":"markdown","1ea91b25":"markdown","9051b032":"markdown","77e120da":"markdown","3ad62e19":"markdown","573ed963":"markdown","06e364d2":"markdown","5dd7c2a2":"markdown","0f0e2580":"markdown","a23c4822":"markdown","14dd6848":"markdown","9edf9d29":"markdown","0c951fd6":"markdown","556ef4e2":"markdown","e5652b07":"markdown","5d0e9a57":"markdown","f364a97a":"markdown","0903b0e4":"markdown","ff83d659":"markdown","01553761":"markdown","9463f598":"markdown","8ee9c08c":"markdown","59a33cd6":"markdown","cf750118":"markdown","24b2ad44":"markdown","6a2686f2":"markdown"},"source":{"8281683d":"# from google.colab import drive\n# drive.mount('\/content\/drive')","2b9f87ff":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\nfrom sklearn import linear_model\nfrom sklearn.decomposition import TruncatedSVD\n# import os\n# print(os.listdir(\"..\/input\"))\nprint(\"DONE ---------------------------------------\")","8ef6dbe3":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","ba39adfb":"# project_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/train_data.csv') \n# resource_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/resources.csv')\nproject_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/train_data.csv') \nresource_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/resources.csv')\n# project_data = pd.read_csv('..\/train_data.csv') \n# resource_data = pd.read_csv('..\/resources.csv')\n\nprint(\"Done\")","9380b428":"project_data.shape","1b828db2":"project_data.columns.values","62675c3e":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","3384395c":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","222d92a6":"project_data['teacher_prefix'].head(10)","1bc52489":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","9d74b731":"project_data.columns","af52fa27":"project_data.shape","7ac09b21":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nproject_data = pd.merge(project_data, price_data, on='id', how='left')","9a975fc3":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\n\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())\n\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","799b2342":"cat_dict","97439090":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n\n# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())\n    \nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","3d73b40f":"# this code removes \" \" and \"-\". ie Grades 3-5 -> grage3to5\n#  remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\nclean_grades=[]\nfor project_grade in project_data['project_grade_category'].values:\n    project_grade=str(project_grade).lower().strip().replace(' ','').replace('-','to')\n    \n    clean_grades.append(project_grade.strip())\n\nproject_data['clean_project_grade_category']=clean_grades\nproject_data.drop(['project_grade_category'],axis=1,inplace=True)\n\nmy_counter = Counter()\nfor word in project_data['clean_project_grade_category'].values:\n    my_counter.update(word.split())\n    \ngrade_dict = dict(my_counter)\nsorted_project_grade_cat_dict = dict(sorted(grade_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","10ec7e01":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","de90c157":"project_data.head(2)","3434e0f8":"#### Text PreProcessing Functions","96f52ff3":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","298f8a43":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","37a21259":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","fc2455ff":"project_data.shape","2789b839":"## new column added as PreProcessed_Essays and older unProcessed essays column is deleted\nproject_data['preprocessed_essays'] = preprocessed_essays\n# project_data.drop(['essay'], axis=1, inplace=True)","3f0b518f":"project_data.columns","94d34ed1":"# after preprocesing\npreprocessed_essays[20000]","211d52b6":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","4a93e2eb":"#https:\/\/stackoverflow.com\/questions\/26666919\/add-column-in-dataframe-from-list\/3849072\nproject_data['preprocessed_titles'] = preprocessed_titles\n# project_data.drop(['project_title'], axis=1, inplace=True)","c442b8f3":"project_data.columns","4d36713e":"project_data.shape","371e0fe2":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","37416e72":"project_data = pd.merge(project_data, price_data, on='id', how='left')","7c8ab596":"project_data.shape","c40dc3da":"# project_data['']\nfor col_type, new_col in [('project_title', 'title_size'), ('essay', 'essay_size')]:\n    print(\"Now in: \",col_type)\n    col_data = project_data[col_type]\n    print(col_data.head(10))\n    col_size = []\n    for sen in col_data:\n        sen = decontracted(sen)\n        col_size.append(len(sen.split()))\n    project_data[new_col] = col_size\n    col_size.clear()","9a1fc51f":"project_data.shape","807521ba":"project_data.columns","1b27af6a":"project_data['title_size'].head(10)","c53f45c0":"project_data['essay_size'].head(10)","595842de":"project_bkp=project_data.copy()","5675e28f":"project_bkp.shape","f9dff2c4":"## taking random samples of 100k datapoints\nproject_data = project_bkp.sample(n = 100000) \n# resource_data = pd.read_csv('..\/resources.csv')\n\nproject_data.shape\n\n# y_value_counts = row1['project_is_approved'].value_counts()\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding:     \", y_value_counts[1],\" -> \",round(y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects thar are not approved for funding: \", y_value_counts[0],\" -> \",round(y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")","734eefbc":"# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test=train_test_split(\n#     project_data.drop('project_is_approved', axis=1),\n#     project_data['project_is_approved'].values,\n#     test_size=0.3,\n#     random_state=42,\n#     stratify=project_data[['project_is_approved']])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(\n    project_data,\n    project_data['project_is_approved'],\n    test_size=0.2,\n    random_state=42,\n    stratify=project_data[['project_is_approved']])\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","24adcbdf":"# x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)\n\n# print(\"x_train: \",x_train.shape)\n# print(\"y_train: \",y_train.shape)\n# print(\"x_cv   : \",x_cv.shape)\n# print(\"x_cv   : \",y_cv.shape)\n# print(\"x_test : \",x_test.shape)\n# print(\"y_test : \",y_test.shape)","6aa2f548":"# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TRAIN-------------------------\")\nx_train_y_value_counts = x_train['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_train_y_value_counts[1],\" -> \",round(x_train_y_value_counts[1]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_train_y_value_counts[0],\" -> \",round(x_train_y_value_counts[0]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TEST--------------------------\")\nx_test_y_value_counts = x_test['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_test_y_value_counts[1],\" -> \",round(x_test_y_value_counts[1]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_test_y_value_counts[0],\" -> \",round(x_test_y_value_counts[0]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\n# print(\"X_CV----------------------------\")\n# x_cv_y_value_counts = x_cv['project_is_approved'].value_counts()\n# print(\"Number of projects that are approved for funding    \", x_cv_y_value_counts[1],\" -> \",round(x_cv_y_value_counts[1]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"Number of projects that are not approved for funding \",x_cv_y_value_counts[0],\" -> \",round(x_cv_y_value_counts[0]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"\\n\")","3f974fd6":"# from sklearn.utils import resample\n\n# ## splitting x_train in their respective classes\n# x_train_majority=x_train[x_train.project_is_approved==1]\n# x_train_minority=x_train[x_train.project_is_approved==0]\n\n# print(\"No. of points in the Training Dataset : \", x_train.shape)\n# print(\"No. of points in the majority class 1 : \",len(x_train_majority))\n# print(\"No. of points in the minority class 0 : \",len(x_train_minority))\n\n# print(x_train_majority.shape)\n# print(x_train_minority.shape)\n\n# ## Resampling with replacement\n# x_train_minority_upsampled=resample(\n#     x_train_minority,\n#     replace=True,\n\n#     n_samples=len(x_train_majority),\n#     random_state=123)\n\n# print(\"Resampled Minority class details\")\n# print(\"Type:  \",type(x_train_minority_upsampled))\n# print(\"Shape: \",x_train_minority_upsampled.shape)\n# print(\"\\n\")\n# ## Concatinating our Upsampled Minority class with the existing Majority class\n# x_train_upsampled=pd.concat([x_train_majority,x_train_minority_upsampled])\n\n# print(\"Upsampled Training data\")\n# print(\"Total number of Class labels\")\n# print(x_train_upsampled.project_is_approved.value_counts())\n# print(\"\\n\")\n# print(\"Old Training IMBALANCED Dataset Shape         : \", x_train.shape)\n# print(\"New Training BALANCED Upsampled Dataset Shape : \",x_train_upsampled.shape)\n\n# x_train_upsampled.to_csv ('x_train_upsampled_csv.csv',index=False)","eb85bbe6":"x_train.shape","6e051d55":"x_test.shape","e4d79d89":"project_data.columns","27f1847b":"# we use count vectorizer to convert the values into one \nfrom sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_sub = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub.fit(x_train['clean_categories'].values)\n\nx_train_categories_one_hot = vectorizer_sub.transform(x_train['clean_categories'].values)\n# x_cv_categories_one_hot    = vectorizer_sub.transform(x_cv['clean_categories'].values)\nx_test_categories_one_hot  = vectorizer_sub.transform(x_test['clean_categories'].values)\n\n\nprint(vectorizer_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> categories: x_train: \",x_train_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> categories: x_cv   : \",x_cv_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_test : \",x_test_categories_one_hot.shape)","2e1e479d":"# we use count vectorizer to convert the values into one \n# vectorizer_sub_sub = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub_sub.fit(x_train['clean_subcategories'].values)\n\nx_train_sub_categories_one_hot = vectorizer_sub_sub.transform(x_train['clean_subcategories'].values)\n# x_cv_sub_categories_one_hot    = vectorizer_sub_sub.transform(x_cv['clean_subcategories'].values)\nx_test_sub_categories_one_hot  = vectorizer_sub_sub.transform(x_test['clean_subcategories'].values)\n\nprint(vectorizer_sub_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_train: \",x_train_sub_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> sub_categories: x_cv   : \",x_cv_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_test : \",x_test_sub_categories_one_hot.shape)","6ece12a3":"my_counter = Counter()\nfor state in project_data['school_state'].values:\n    my_counter.update(state.split())","259f808b":"school_state_cat_dict = dict(my_counter)\nsorted_school_state_cat_dict = dict(sorted(school_state_cat_dict.items(), key=lambda kv: kv[1]))","8431d084":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\n# vectorizer_state = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\nvectorizer_state = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_state.fit(x_train['school_state'])\n\nx_train_school_state_one_hot = vectorizer_state.transform(x_train['school_state'].values)\n# x_cv_school_state_one_hot    = vectorizer_state.transform(x_cv['school_state'].values)\nx_test_school_state_one_hot  = vectorizer_state.transform(x_test['school_state'].values)\n\nprint(vectorizer_state.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> school_state: x_train: \",x_train_school_state_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> school_state: x_cv   : \",x_cv_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_test : \",x_test_school_state_one_hot.shape)\n# school_one_hot = vectorizer.transform(statelist)\n# print(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\n# print(type(school_one_hot))\n# sparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \n# print(school_one_hot.toarray())","658530e1":"# prefixlist=project_data['teacher_prefix'].values\n# prefixlist=list(prefixlist)\n# cleanedPrefixList = [x for x in prefixlist if x == x] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\n# ## preprocessing the prefix to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\n# prefix_nospace_list = []\n# for i in cleanedPrefixList:\n#     temp = \"\"\n#     i = i.replace('.','') # we are placeing all the '.'(dot) with ''(empty) ex:\"Mr.\"=>\"Mr\"\n#     temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n#     prefix_nospace_list.append(temp.strip())\n\n# cleanedPrefixList=prefix_nospace_list\n\n# vectorizer = CountVectorizer(vocabulary=set(cleanedPrefixList), lowercase=False, binary=True)\n# vectorizer.fit(cleanedPrefixList)\n# print(vectorizer.get_feature_names())\n# prefix_one_hot = vectorizer.transform(cleanedPrefixList)\n# print(\"Shape of matrix after one hot encodig \",prefix_one_hot.shape)\n# prefix_one_hot_ar=prefix_one_hot.todense()\n\n# ##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# # prefixcsv=pd.DataFrame(prefix_one_hot.toarray())\n# # prefixcsv.to_csv('prefix.csv', index=None,header=None)","2df97a72":"my_counter = Counter()\nfor teacher_prefix in project_data['teacher_prefix'].values:\n    teacher_prefix = str(teacher_prefix).lower().replace('.','').strip()\n    \n    my_counter.update(teacher_prefix.split())\nteacher_prefix_cat_dict = dict(my_counter)\nsorted_teacher_prefix_cat_dict = dict(sorted(teacher_prefix_cat_dict.items(), key=lambda kv: kv[1]))","d955bef5":"sorted_teacher_prefix_cat_dict.keys()","d3124e01":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_prefix = CountVectorizer(vocabulary=list(sorted_teacher_prefix_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_prefix = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_prefix.fit(x_train['teacher_prefix'].values)\n\nx_train_prefix_one_hot = vectorizer_prefix.transform(x_train['teacher_prefix'].values)\n# x_cv_prefix_one_hot    = vectorizer_prefix.transform(x_cv['teacher_prefix'].values)\nx_test_prefix_one_hot  = vectorizer_prefix.transform(x_test['teacher_prefix'].values)\n\nprint(vectorizer_prefix.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_test : \",x_test_prefix_one_hot.shape)","7994aa14":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_grade = CountVectorizer(vocabulary=list(sorted_project_grade_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_grade = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_grade.fit(x_train['clean_project_grade_category'].values)\n\nx_train_grade_category_one_hot = vectorizer_grade.transform(x_train['clean_project_grade_category'].values)\n# x_cv_grade_category_one_hot    = vectorizer_grade.transform(x_cv['clean_project_grade_category'].values)\nx_test_grade_category_one_hot  = vectorizer_grade.transform(x_test['clean_project_grade_category'].values)\n\nprint(vectorizer_grade.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_train : \",x_train_grade_category_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> project_grade: x_cv    : \",x_cv_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_test  : \",x_test_grade_category_one_hot.shape)\n","20e9c1b9":"type(x_train_grade_category_one_hot)","1081c825":"x_train_grade_category_one_hot.toarray()","7f5c7cf2":"x_train['price_x'].head(10)","d00a0e1a":"x_train['price_y'].head(10)","d6e9eaff":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['price_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_price_normalized = normalizer.transform(x_train['price_x'].values.reshape(1, -1)).reshape(-1,1)\nx_test_price_normalized  = normalizer.transform(x_test['price_x'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","427025e9":"type(x_train_price_normalized)","42a73fb2":"x_train_price_normalized","3f671759":"# # check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# # standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# # from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import Normalizer\n\n# # price_standardized = standardScalar.fit(project_data['price'].values)\n# # this will rise the error\n# # ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# # Reshape your data either using array.reshape(-1, 1)\n# # transformer = Normalizer().fit(X)\n# normalizer = Normalizer()\n\n# normalizer.fit(x_train_upsampled['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n# # print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# # Now normalize the data\n\n# x_train_price_normalized = normalizer.transform(x_train_upsampled['price'].values.reshape(-1, 1))\n# x_test_price_normalized  = normalizer.transform(x_test['price'].values.reshape(-1, 1))\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(-1, 1))\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","bb0099c7":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_normalizer = Normalizer()\n# normalizer = Normalizer()\n\nteacher_previous_proj_normalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_teacher_previous_proj_normalized = teacher_previous_proj_normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\nx_test_teacher_previous_proj_normalized  = teacher_previous_proj_normalizer.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n# x_cv_teacher_previous_proj_normalized    = teacher_previous_proj_normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_train:  \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_cv   :  \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_test :  \",x_test_prefix_one_hot.shape)\n","aa01b97b":"# x_train_teacher_previous_proj_normalized","78aa697a":"x_train.columns","252d50b6":"project_data['title_size'].head(10)","311e9d76":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['title_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_title_normalized = normalizer.transform(x_train['title_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_title_normalized  = normalizer.transform(x_test['title_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","7d4decde":"type(x_train_title_normalized)","ddeb2956":"# x_train_title_normalized","732e9388":"# project_data['essay_size']","23c8fd3a":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['essay_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_essay_normalized = normalizer.transform(x_train['essay_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_essay_normalized  = normalizer.transform(x_test['essay_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","46d0989b":"# x_train_essay_normalized","ea120e67":"# project_data['quantity_x']","3dedfb67":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['quantity_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_quantity_normalized = normalizer.transform(x_train['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\nx_test_quantity_normalized  = normalizer.transform(x_test['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","449aa1b7":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf_unigram = TfidfVectorizer(min_df=10)\n\nvectorizer_essay_tfidf_unigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf_unigram = vectorizer_essay_tfidf_unigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf_unigram  = vectorizer_essay_tfidf_unigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf_unigram.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf_unigram.shape)","dcb2f786":"type(x_train_essays_tfidf_unigram)","0ba262bf":"len(vectorizer_essay_tfidf_unigram.get_feature_names())","561ca8a9":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf_bigram = TfidfVectorizer(ngram_range=(2,2),min_df=10,max_features=50000)\n# vectorizer_essay_tfidf_bigram = TfidfVectorizer(ngram_range=(2,2),min_df=10,max_features=5000)\n\nvectorizer_essay_tfidf_bigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf_bigram = vectorizer_essay_tfidf_bigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf_bigram  = vectorizer_essay_tfidf_bigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf_bigram.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf_bigram.shape)","55c747ef":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_title_tfidf = TfidfVectorizer(min_df=2)\n\nvectorizer_title_tfidf.fit(x_train['preprocessed_titles'])\n\nx_train_titles_tfidf = vectorizer_title_tfidf.transform(x_train['preprocessed_titles'])\n# x_cv_titles_tfidf    = vectorizer_title_tfidf.transform(x_cv['preprocessed_titles'])\nx_test_titles_tfidf  = vectorizer_title_tfidf.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after TF-IDF -> Title: x_train: \",x_train_titles_tfidf.shape)\n# print(\"Shape of matrix after TF-IDF -> Title: x_cv   : \",x_cv_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_test : \",x_test_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","802b22e8":"# print(STOP)","dc6dce76":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''\n\n\n# words_train_essays = []\n# for i in preprocessed_essays_train :\n#     words_train_essays.extend(i.split(' '))\n\n# ## Find the total number of words in the Train data of Essays.\n# print(\"all the words in the corpus\", len(words_train_essays))\n\n# ## Find the unique words in this set of words\n# words_train_essay = set(words_train_essays)\n# print(\"the unique words in the corpus\", len(words_train_essay))\n\n# ## Find the words present in both Glove Vectors as well as our corpus.\n# inter_words = set(model.keys()).intersection(words_train_essay)\n# print(\"The number of words that are present in both glove vectors and our corpus are {} which is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))\/len(words_train_essay))*100)))\n\n# words_corpus_train_essay = {}\n# words_glove = set(model.keys())\n# for i in words_train_essay:\n#     if i in words_glove:\n#         words_corpus_train_essay[i] = model[i]\n# print(\"word 2 vec length\", len(words_corpus_train_essay))","83a0bd83":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\n# with open('\/content\/drive\/My Drive\/Colab Notebooks\/glove_vectors', 'rb') as f:\nwith open('\/kaggle\/input\/donorschoosedataset\/glove_vectors\/glove_vectors', 'rb') as f:\n# with open('..\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","c4c09542":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_model = TfidfVectorizer()\n# tfidf_model.fit(x_train_upsampled['preprocessed_essays'])\ntfidf_model.fit(x_train['preprocessed_essays'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","3faa26a6":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_train_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors[0]))","793ce489":"# ### NOT DONE HERE\n\n\n# x_cv_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if (word in glove_words) and (word in tfidf_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_weight += tf_idf\n#     if tf_idf_weight != 0:\n#         vector \/= tf_idf_weight\n#     x_cv_essays_tfidf_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors[0]))","b7327a8f":"from tqdm import tqdm_notebook\nx_test_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm_notebook(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_test_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors[0]))","b0ccc2ec":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_title_model = TfidfVectorizer()\n# tfidf_title_model.fit(x_train_upsampled['preprocessed_titles'])\ntfidf_title_model.fit(x_train['preprocessed_titles'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_title_model.get_feature_names(), list(tfidf_title_model.idf_)))\ntfidf_title_words = set(tfidf_title_model.get_feature_names())","b618c382":"# average Word2Vec\n# compute average word2vec for each title.\nx_train_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_train_upsampled['preprocessed_titles']: # for each review\/sentence\nfor sentence in x_train['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_train_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors[0]))","bfff88aa":"# ### NOT DONE HERE\n\n\n# # average Word2Vec\n# # compute average word2vec for each title.\n# x_cv_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_cv['preprocessed_titles']: # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_title_weight =0; # num of words with a valid vector in the title\n#     for word in sentence.split(): # for each word in a title\n#         if (word in glove_words) and (word in tfidf_title_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_title_weight += tf_idf\n#     if tf_idf_title_weight != 0:\n#         vector \/= tf_idf_title_weight\n#     x_cv_tfidf_w2v_title_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors[0]))","c241bac0":"# average Word2Vec\n# compute average word2vec for each title.\nx_test_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_test['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_test_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors[0]))","ff8b27a5":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays = x_train['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_train['essay_sentiment_train'] = essays_sentiments","0a448e06":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays_test = x_test['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays_test):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_test['essay_sentiment_test'] = essays_sentiments","d1a8f301":"sentiment_test=x_test['essay_sentiment_test'].values.reshape(-1,1)\nsentiment_train=x_train['essay_sentiment_train'].values.reshape(-1,1)","eac528b0":"from scipy.sparse import hstack\n\nx_train_onehot = hstack((x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_school_state_one_hot, x_train_prefix_one_hot, x_train_grade_category_one_hot, x_train_price_normalized, x_train_teacher_previous_proj_normalized))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot  = hstack((x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_school_state_one_hot, x_test_prefix_one_hot, x_test_grade_category_one_hot, x_test_price_normalized, x_test_teacher_previous_proj_normalized))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","491c920a":"x_train_onehot.shape","af93507e":"x_train_onehot_tfidf = hstack((x_train_onehot,x_train_titles_tfidf, x_train_essays_tfidf_unigram,sentiment_train)).tocsr()\n# x_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf  = hstack((x_test_onehot,x_test_titles_tfidf, x_test_essays_tfidf_unigram,sentiment_test)).tocsr()\nprint(\"Type -> One Hot TFIDF -> x_train_cv_test: \",type(x_train_onehot_tfidf))\n# print(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> x_test         : \",type(x_test_onehot_tfidf))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF -> x_train_cv_test: \",x_train_onehot_tfidf.shape)\n# print(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> x_test         : \",x_test_onehot_tfidf.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","f2474c1b":"x_train_onehot_tfidf_w2v = hstack((x_train_onehot, x_train_tfidf_w2v_title_vectors, x_train_essays_tfidf_w2v_vectors,sentiment_train)).tocsr()\n# x_cv_onehot_tfidf_w2v    = hstack((x_cv_onehot, x_cv_tfidf_w2v_title_vectors, x_cv_essays_tfidf_w2v_vectors)).tocsr()\nx_test_onehot_tfidf_w2v  = hstack((x_test_onehot, x_test_tfidf_w2v_title_vectors, x_test_essays_tfidf_w2v_vectors,sentiment_test)).tocsr()\nprint(\"Type -> One Hot TFIDF W2V -> x_train_cv_test: \",type(x_train_onehot_tfidf_w2v))\n# print(\"Type -> One Hot TFIDF W2V -> cv             : \",type(x_cv_onehot_tfidf_w2v))\nprint(\"Type -> One Hot TFIDF W2V -> x_test         : \",type(x_test_onehot_tfidf_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF W2V -> x_train_cv_test: \",x_train_onehot_tfidf_w2v.shape)\n# print(\"Shape -> One Hot TFIDF W2V -> cv             : \",x_cv_onehot_tfidf_w2v.shape)\nprint(\"Shape -> One Hot TFIDF W2V -> x_test         : \",x_test_onehot_tfidf_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","fc60412d":"print(\"Done till here\")","c96015c4":"FP_essay_train_set1=[]\nFP_price_train_set1=[]\nFP_previous_posted_train_set1=[]\n\nFP_essay_train_set2=[]\nFP_price_train_set2=[]\nFP_previous_posted_train_set2=[]\n\nFP_essay_train_set3=[]\nFP_price_train_set3=[]\nFP_previous_posted_train_set3=[]\n\nFP_essay_test_set1=[]\nFP_price_test_set1=[]\nFP_previous_posted_test_set1=[]\n\nFP_essay_test_set2=[]\nFP_price_test_set2=[]\nFP_previous_posted_test_set2=[]\n\nFP_essay_test_set3=[]\nFP_price_test_set3=[]\nFP_previous_posted_test_set3=[]\n\n\ndef retrievingFalsePositives(setNumber,part):\n    if(setNumber==1 and part==\"train\"):  \n        FP_train_indexes_set1=[]\n        for i in range(len(y_train)):\n            if((y_train.values[i]==0) and (predictions_train_set1[i]==1) ):\n                FP_train_indexes_set1.append(i)\n\n        for i in FP_train_indexes_set1:\n            FP_essay_train_set1.append(x_train['essay'].values[i])\n            FP_price_train_set1.append(x_train['price_x'].values[i])\n            FP_previous_posted_train_set1.append(x_train['teacher_number_of_previously_posted_projects'].values[i])\n\n    if(setNumber==2 and part==\"train\"):  \n        FP_train_indexes_set2=[]\n        for i in range(len(y_train)):\n            if(y_train.values[i]==0 and predictions_train_set2[i]==1 ):\n                FP_train_indexes_set2.append(i)\n\n        for i in FP_train_indexes_set2:\n            FP_essay_train_set2.append(x_train['essay'].values[i])\n            FP_price_train_set2.append(x_train['price_x'].values[i])\n            FP_previous_posted_train_set2.append(x_train['teacher_number_of_previously_posted_projects'].values[i])\n    \n    if(setNumber==3 and part==\"train\"):  \n        FP_train_indexes_set3=[]\n        for i in range(len(y_train)):\n            if(y_train.values[i]==0 and predictions_train_set3[i]==1 ):\n                FP_train_indexes_set3.append(i)\n\n        for i in FP_train_indexes_set3:\n            FP_essay_train_set3.append(x_train['essay'].values[i])\n            FP_price_train_set3.append(x_train['price_x'].values[i])\n            FP_previous_posted_train_set3.append(x_train['teacher_number_of_previously_posted_projects'].values[i])\n    \n    if(setNumber==1 and part==\"test\"):  \n        FP_test_indexes_set1=[]\n        for i in range(len(y_test)):\n            if(y_test.values[i]==0 and predictions_test_set1[i]==1 ):\n                FP_test_indexes_set1.append(i)\n\n        for i in FP_test_indexes_set1:\n            FP_essay_test_set1.append(x_test['essay'].values[i])\n            FP_price_test_set1.append(x_test['price_x'].values[i])\n            FP_previous_posted_test_set1.append(x_test['teacher_number_of_previously_posted_projects'].values[i])\n\n    if(setNumber==2 and part==\"test\"):  \n        FP_test_indexes_set2=[]\n        for i in range(len(y_test)):\n            if(y_test.values[i]==0 and predictions_test_set2[i]==1 ):\n                FP_test_indexes_set2.append(i)\n        for i in FP_test_indexes_set2:\n            FP_essay_test_set2.append(x_test['essay'].values[i])\n            FP_price_test_set2.append(x_test['price_x'].values[i])\n            FP_previous_posted_test_set2.append(x_test['teacher_number_of_previously_posted_projects'].values[i])\n            \n    if(setNumber==3 and part==\"test\"):  \n        FP_test_indexes_set3=[]\n        for i in range(len(y_test)):\n            if(y_test.values[i]==0 and predictions_test_set3[i]==1 ):\n                FP_test_indexes_set3.append(i)\n        for i in FP_test_indexes_set3:\n            FP_essay_test_set3.append(x_test['essay'].values[i])\n            FP_price_test_set3.append(x_test['price_x'].values[i])\n            FP_previous_posted_test_set3.append(x_test['teacher_number_of_previously_posted_projects'].values[i])","01eb9aed":"# importing all necessary modules\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom PIL import Image # for masking i.e print word in the pattern we want\nimport pandas as pd\n\n# Read 'Youtube04-Eminem.csv' files\n# using encoding = \"latin-1\" to get vertical words arrangement along with horizontal once\n# dataFrame = pd.read_csv(r\"Youtube04-Eminem.csv\", encoding = \"latin-1\")\n# dataFrame.head()\ndef printWordCloud(FP_list):\n    comment_words = ''\n    stopwords = set(STOPWORDS) \n\n    # for val in dataFrame.CONTENT: \n    for val in FP_list: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 500, height = 500, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) ","9bc4283a":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\ndef printBoxPlot(FP_list):\n    plt.boxplot(FP_list)\n    plt.title('Box Plot for PRICE in False Positives')\n    plt.ylabel('Price')\n    plt.grid()\n    plt.show()","10cb1de9":"def printPDF(FP_list):\n    plt.figure(figsize=(10,3))\n    sns.distplot(FP_list)\n    plt.title('PDF for Teacher number who previously posted projects in False Positives')\n    plt.xlabel('Teacher number who previously posted projects')\n    plt.legend()\n    plt.show()","b6d2b30c":"## SVM\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_tfidf = DecisionTreeClassifier(class_weight='balanced')\nparameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\nclf1 = GridSearchCV(dt_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf1.fit(x_train_onehot_tfidf,y_train)\ntrain_auc= clf1.cv_results_['mean_train_score']\ntrain_auc_std= clf1.cv_results_['std_train_score']\ncv_auc = clf1.cv_results_['mean_test_score']\ncv_auc_std= clf1.cv_results_['std_test_score']\nbestMaxDepth_1=clf1.best_params_['max_depth']\nbestMinSampleSplit_1=clf1.best_params_['min_samples_split']\nbestScore_1=clf1.best_score_\nprint(\"BEST MAX DEPTH: \",clf1.best_params_['max_depth'],\" BEST SCORE: \",clf1.best_score_,\"BEST MIN SAMPLE SPLIT: \",clf1.best_params_['min_samples_split']) #clf.best_estimator_.alpha","a98de8ac":"clf1.cv_results_","89615206":"import seaborn as sns; sns.set()\nmax_scores1 = pd.DataFrame(clf1.cv_results_).groupby(['param_min_samples_split', 'param_max_depth']).max().unstack()[['mean_test_score', 'mean_train_score']]\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nsns.heatmap(max_scores1.mean_train_score, annot = True, fmt='.4g', ax=ax[0],annot_kws={\"size\": 26})\nsns.heatmap(max_scores1.mean_test_score, annot = True, fmt='.4g', ax=ax[1],annot_kws={\"size\": 26},cmap=\"YlGnBu\")\nax[0].set_title('Train Set')\nax[1].set_title('CV Set')\nplt.show()","96b2d451":"from sklearn.linear_model import SGDClassifier","7cda907f":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\ndt_tfidf_testModel = DecisionTreeClassifier(class_weight='balanced',min_samples_split=bestMinSampleSplit_1,max_depth=bestMaxDepth_1)\ndt_tfidf_testModel.fit(x_train_onehot_tfidf, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=dt_tfidf_testModel.predict_proba(x_train_onehot_tfidf)[:,1]\npredictions_train_set1=dt_tfidf_testModel.predict(x_train_onehot_tfidf)\n\ny_test_pred=dt_tfidf_testModel.predict_proba(x_test_onehot_tfidf)[:,1]\npredictions_test_set1=dt_tfidf_testModel.predict(x_test_onehot_tfidf)\n\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set1_train=auc(train_fpr, train_tpr)\nauc_set1_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","ff35d6af":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n\n    return predictions","8905487b":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","94585bfd":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","167fa434":"retrievingFalsePositives(1,\"test\")","f017d832":"printWordCloud(FP_essay_test_set1)","473dc012":"printBoxPlot(FP_price_test_set1)","e15f6c99":"printPDF(FP_previous_posted_test_set1)","28882f99":"## SVM\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_tfidf_w2v = DecisionTreeClassifier(class_weight='balanced')\nparameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\nclf2 = GridSearchCV(dt_tfidf_w2v, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf2.fit(x_train_onehot_tfidf_w2v,y_train)\ntrain_auc= clf2.cv_results_['mean_train_score']\ntrain_auc_std= clf2.cv_results_['std_train_score']\ncv_auc = clf2.cv_results_['mean_test_score']\ncv_auc_std= clf2.cv_results_['std_test_score']\nbestMaxDepth_2=clf2.best_params_['max_depth']\nbestMinSampleSplit_2=clf2.best_params_['min_samples_split']\nbestScore_2=clf2.best_score_\nprint(\"BEST MAX DEPTH: \",clf2.best_params_['max_depth'],\" BEST SCORE: \",clf2.best_score_,\"BEST MIN SAMPLE SPLIT: \",clf2.best_params_['min_samples_split']) #clf.best_estimator_.alpha","2389c29a":"import seaborn as sns; sns.set()\nmax_scores2 = pd.DataFrame(clf2.cv_results_).groupby(['param_min_samples_split', 'param_max_depth']).max().unstack()[['mean_test_score', 'mean_train_score']]\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nsns.heatmap(max_scores2.mean_train_score, annot = True, fmt='.4g', ax=ax[0],annot_kws={\"size\": 26})\nsns.heatmap(max_scores2.mean_test_score, annot = True, fmt='.4g', ax=ax[1],annot_kws={\"size\": 26},cmap=\"YlGnBu\")\nax[0].set_title('Train Set')\nax[1].set_title('CV Set')\nplt.show()","63035603":"clf2.cv_results_['mean_fit_time']","13404b41":"clf2.cv_results_\n","b8bd3f3f":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\ndt_tfidf_w2v_testModel = DecisionTreeClassifier(class_weight='balanced',min_samples_split=bestMinSampleSplit_2,max_depth=bestMaxDepth_2)\ndt_tfidf_w2v_testModel.fit(x_train_onehot_tfidf_w2v, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=dt_tfidf_w2v_testModel.predict_proba(x_train_onehot_tfidf_w2v)[:,1]\npredictions_train_set2=dt_tfidf_w2v_testModel.predict(x_train_onehot_tfidf_w2v)\n\ny_test_pred=dt_tfidf_w2v_testModel.predict_proba(x_test_onehot_tfidf_w2v)[:,1]\npredictions_test_set2=dt_tfidf_w2v_testModel.predict(x_test_onehot_tfidf_w2v)\n\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set2_train=auc(train_fpr, train_tpr)\nauc_set2_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","180fe34c":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","c7c5f6c0":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","4eb8ac55":"retrievingFalsePositives(2,\"test\")","7e559d93":"printWordCloud(FP_essay_test_set2)","69907edf":"printBoxPlot(FP_price_test_set2)","d2c1e59f":"printPDF(FP_previous_posted_test_set2)","7eb8ceff":"## #https:\/\/stackoverflow.com\/questions\/47111434\/randomforestregressor-and-feature-importances-error\n\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\ndef selectKImportance(model, X, k):\n    return X[:,model.feature_importances_.argsort()[::-1][:k]]","7d3802e8":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\ndt_tfidf_imp_feature_testModel = DecisionTreeClassifier(class_weight='balanced')\ndt_tfidf_imp_feature_testModel.fit(x_train_onehot_tfidf, y_train)","dfde7afc":"nonZeroFeatures=0\nfor i in range (len(dt_tfidf_imp_feature_testModel.feature_importances_)):\n    if(dt_tfidf_imp_feature_testModel.feature_importances_[i]>0):\n        nonZeroFeatures=nonZeroFeatures+1\n#         print(dt_tfidf_imp_feature_testModel.feature_importances_[i])","377513a5":"nonZeroFeatures","0bf1df8f":"x_train_impFeatureDataset1=selectKImportance(dt_tfidf_imp_feature_testModel,x_train_onehot_tfidf,nonZeroFeatures)\nx_test_impFeatureDataset1=selectKImportance(dt_tfidf_imp_feature_testModel,x_test_onehot_tfidf,nonZeroFeatures)","2b6bc008":"x_train_impFeatureDataset1.shape","edf41e04":"x_test_impFeatureDataset1.shape","d43379e8":"## SVM\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_imp_feature_tfidf = DecisionTreeClassifier(class_weight='balanced')\nparameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\nclf3 = GridSearchCV(dt_imp_feature_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf3.fit(x_train_impFeatureDataset1,y_train)\ntrain_auc= clf3.cv_results_['mean_train_score']\ntrain_auc_std= clf3.cv_results_['std_train_score']\ncv_auc = clf3.cv_results_['mean_test_score']\ncv_auc_std= clf3.cv_results_['std_test_score']\nbestMaxDepth_3=clf3.best_params_['max_depth']\nbestMinSampleSplit_3=clf3.best_params_['min_samples_split']\nbestScore_3=clf3.best_score_\nprint(\"BEST MAX DEPTH: \",clf3.best_params_['max_depth'],\" BEST SCORE: \",clf3.best_score_,\"BEST MIN SAMPLE SPLIT: \",clf3.best_params_['min_samples_split']) #clf.best_estimator_.alpha","3751b8ca":"import seaborn as sns; sns.set()\nmax_scores2 = pd.DataFrame(clf3.cv_results_).groupby(['param_min_samples_split', 'param_max_depth']).max().unstack()[['mean_test_score', 'mean_train_score']]\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nsns.heatmap(max_scores2.mean_train_score, annot = True, fmt='.4g', ax=ax[0],annot_kws={\"size\": 26})\nsns.heatmap(max_scores2.mean_test_score, annot = True, fmt='.4g', ax=ax[1],annot_kws={\"size\": 26},cmap=\"YlGnBu\")\nax[0].set_title('Train Set')\nax[1].set_title('CV Set')\nplt.show()","4a1e90ce":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\ndt_imp_feature_tfidf_testModel = DecisionTreeClassifier(class_weight='balanced',min_samples_split=bestMinSampleSplit_3,max_depth=bestMaxDepth_3)\ndt_imp_feature_tfidf_testModel.fit(x_train_impFeatureDataset1, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=dt_imp_feature_tfidf_testModel.predict_proba(x_train_impFeatureDataset1)[:,1]\npredictions_train_set3=dt_imp_feature_tfidf_testModel.predict(x_train_impFeatureDataset1)\n\ny_test_pred=dt_imp_feature_tfidf_testModel.predict_proba(x_test_impFeatureDataset1)[:,1]\npredictions_test_set3=dt_imp_feature_tfidf_testModel.predict(x_test_impFeatureDataset1)\n\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set3_train=auc(train_fpr, train_tpr)\nauc_set3_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","a4cf0c7a":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","cc0397fd":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","32b14f49":"retrievingFalsePositives(3,\"test\")","406820d7":"printWordCloud(FP_essay_test_set3)","a42b3743":"printBoxPlot(FP_price_test_set3)","8d299882":"printPDF(FP_previous_posted_test_set3)","c50c1848":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Vectorizer\", \"Model\", \"Min. Sample Split\",\"Max Dept\", \"Train AUC\", \"Test AUC\"]\n# auc_set2_train=auc(train_fpr, train_tpr)\n# auc_set2_test=auc(test_fpr, test_tpr)\n\nx.add_row([\"TF-IDF\", \"SVM\", bestMinSampleSplit_1,bestMaxDepth_1,auc_set1_train,auc_set1_test])\nx.add_row([\"Avg W2V\", \"SVM\", bestMinSampleSplit_2,bestMaxDepth_2,auc_set2_train,auc_set2_test])\nx.add_row([\"TF-IDF (Imp. Feature)\", \"SVM\", bestMinSampleSplit_3,bestMaxDepth_3,auc_set3_train,auc_set3_test])\n\n\nprint(x)","3a4afcc6":"## ->->-> 6.3.1.0: TF-IDF (Unigram): Essays (Train, CV, Test)","29a3e46e":"# ->6.3.2: TFIDF - W2V","d64df226":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","45a2cd49":"## Printing Word Cloud FP - Essay for Test : Set 3","1866ceb4":"###   ->->-> 6.3.4.2. A: TF-IDF W2V: Title -> Train","552e324a":"## -> 3.1: Text Preprocessing: Essays","1efa1abd":"# 2: PRE-PROCESSING","02cd35d1":"\n## ->-> 8.3.2: <font color='red'> SET 3<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","aeff3931":"# 8. Decision Tree","3ba768eb":"## Printing Word Cloud FP - Essay for Test : Set 2","1654ed75":"## Retrieving FP for Test : Set 2","7ee5ea71":"## Printing Box Plot FP - Price for Test : Set 2","24bd00a0":"**Conlusion**\n1. **UPSAMPLING** needs to be done on the Minority class to avoid problems related to Imbalanced dataset.\n1. Upsampling will be done by _**\"Resample with replacement strategy\"**_","b341a915":"## -> 1.1: REMOVING NaN:<br>\n**As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis.**","ddf98252":"## Printing Box Plot FP - Price for Test : Set 1","6ead5f5f":"## ->-> 6.2.4: Normalizing Numerical data: Essay Size","25a364bf":"## Heatmap for Tuned Hyper Parameter: Set 2","d43d15c2":"## Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 2","fedf283c":"## Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 1","c11e3a93":"### -> 4.3: Details of our Training, CV and Test datasets.","e1096671":"## -> 3.2: Text Preprocessing: Title","8b6f7c90":"## A. DATA INFORMATION \n### <br>About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","fd099116":"## -> 2.2: Preprocessing: Project Subject Sub Categories","a84b923f":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","9152900e":"# Drawing the WordCloud of the Words in Essay Text","b7e154fa":"## Adding Quantity in the dataset","3fb0b3b0":"# -> 8.3:<font color='red'> SET 3<\/font>  Applying Decision Tree on Important Feature dataset 1","7789e9a4":"# 9. CONCLUSION","9e4723a9":"**Query 1.2: PreProcessing Project Grade Done <br>\nAction Taken: Removed ' ' and '-' from the grades and converted to lower case**","faad0475":"## Printing Word Cloud FP - Essay for Test : Set 1","abff98e4":"# PERFORMING SENTIMENT ANALYSIS","c10bbccd":"**Observation:**\n1. Dataset is highly **IMBALANCED**.\n1. Approved Class (1) is the Majority class. And the Majority class portion in our sampled dataset: ~85%\n1. Unapproved class (0) is the Minority class. And the Minority class portion in our sampled dataset: ~15%","79883b39":"# -> 6.2: VECTORIZING NUMERICAL DATA","a58d9144":"# TASK 2: FINDING FEATURE IMPORTANCE IN SET 1","43a7d288":"## -> 7.2: SET 1:  Merging All ONE HOT with TF-IDF (Title and Essay) features","517a7743":"## Retrieving FP for Test : Set 3","687f5466":"### ->->-> 8.3.3.2: <font color='red'> SET 3<\/font> Confusion Matrix: Test","b9185b63":"## -> 4.2: Splitting the dataset into Train, CV and Test datasets. (60:20:20)","b78cd1bf":"## ->-> 6.3.4.2 TITLE","b94cc51a":"\n## ->-> 8.2.3: <font color='red'> SET 2<\/font> Confusion Matrix","f7f97e58":"### ->->-> 8.1.3.1: <font color='red'> SET 1<\/font> Confusion Matrix: Train","6542c805":"\n## ->-> 8.3.3: <font color='red'> SET 3<\/font> Confusion Matrix","f716d318":"\n## ->-> 8.1.2: <font color='red'> SET 1<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","1eb75a2c":"## ->-> 6.3.2.1: ESSAYS","9ab19c1b":"## Heatmap for Tuned Hyper Parameter","135f2172":"###   ->->-> 6.3.4.1. B: TF-IDF W2V: Essays -> CV","7a246625":"# B. OBJECTIVE\nThe primary objective is to implement the k-Nearest Neighbor Algo on the DonorChoose Dataset and measure the accuracy on the Test dataset.","eeace952":"## -> 7.3: SET 2:  Merging All ONE HOT with TF-IDF W2V (Title and Essay) features","8836c9fe":"## Retrieving False Positives","41bdca57":"**Conclusion:**\n1. Resampling is performed on the Training data.\n1. Training data in now **BALANCED**.","d7eb168a":"# 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY: NOT DONE ","aefe0ea3":"###    -> -> 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)","ecdab457":"### ->->-> 8.1.3.2: <font color='red'> SET 1<\/font> Confusion Matrix: Test","982a935f":"## ->-> 6.2.1: Normalizing Numerical data: Price","067b285d":"## Printing Box Plot FP - Price for Test : Set 3","406e7baf":"## -> 3.3: Calculating the size of Title and Essay","a62bdc59":"## ->-> 6.2.3: Normalizing Numerical data: Title Size","d07a3a28":"## ->-> 6.1.5 Vectorizing Categorical data: Project Grade","b1f91008":"# -> 8.2:<font color='red'> SET 2<\/font>  Applying Decision Tree on TFIDF_W2V.","bf1a2135":"### ->->-> 8.3.3.1: <font color='red'> SET 3<\/font> Confusion Matrix: Train","194b2b4c":"According to Andrew Ng, in the Coursera MOOC on Introduction to Machine Learning, the general rule of thumb is to partition the data set into the ratio of ***3:1:1 (60:20:20)*** for training, validation and testing respectively.","2bb9d924":"## Decision Tree: DonorsChoose Dataset\n\n### A. DATA INFORMATION\n### B. OBJECTIVE\n## 1. READING THE DATASET\n- **1.1 Removing Nan**\n- **1.2 Adding Quantity in Dataset**\n\n## 2. PREPROCESSING \n- **2.1 Preprocessing: Project Subject Categories**\n- **2.2 Preprocessing: Project Subject Sub Categories**\n- **2.3 Preprocessing: Project Grade**\n\n## 3. TEXT PROCESSING\n- **3.1 Text Preprocessing: Essays**\n- **3.2 Text Preprocessing: Title**\n- **3.3 Text Preprocessing: Calculating Size of Title and Essay**\n\n## 4. SAMPLING\n- **4.1 Taking Sample from the complete dataset.**\n- **4.2 Splitting the dataset into Train, CV and Test datasets. (60:20:20)**\n    - 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)\n    - 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets. (Not required  as we are using GridSearch)\n- **4.3 Details of our Training, CV and Test datasets.**\n\n## 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY (Not required  as we are using GridSearch)\n\n## 6. PREPARING DATA FOR MODELS\n- **6.1: VECTORIZING CATEGORICAL DATA**\n    - 6.1.1: Vectorizing Categorical data: Clean Subject Categories.\n    - 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories.\n    - 6.1.3: Vectorizing Categorical data: School State.\n    - 6.1.4: Vectorizing Categorical data: Teacher Prefix.\n    - 6.1.5: Vectorizing Categorical data: Project Grade\n        \n- **6.2: VECTORIZING NUMERICAL DATA**\n    - 6.2.1: Standarizing Numerical data: Price\n    - 6.2.2: Standarizing Numerical data: Teacher's Previous Projects\n    - 6.2.3: Standarizing Numerical data: Title Size\n    - 6.2.4: Standarizing Numerical data: Essay Size\n    - 6.2.5: Standarizing Numerical data: Quantity\n    \n        \n- **6.3: VECTORIZING TEXT DATA**\n    - **6.3.1: TF-IDF**\n        - 6.3.1.0: TF-IDF (Unigram): Essays (Train, CV, Test)\n        - 6.3.1.1: TF-IDF (BiGrams): Essays (Train, CV, Test)\n        - 6.3.1.2: TF-IDF: Title (Train, CV, Test)\n            \n    - **6.3.2: W2V TF-IDF**\n        - 6.3.2.1: TF-IDF Word2Vec: Essays (Train, CV, Test)\n        - 6.3.2.2: TF-IDF Word2Vec: Title (Train, CV, Test)\n\n## PERFORMING SENTIMENT ANALYSIS\n\n## 7. MERGING FEATURES\n- 7.1: Merging all ONE HOT features.\n- 7.2: SET 1: Merging All ONE HOT with TF-IDF (Title and Essay) features.\n- 7.3: SET 2: Merging All ONE HOT with W2V TF-IDF (Title and Essay) features.\n\n## IMPORTANT FUNCTIONS FOR SET 3 (TASK 2)\n- Retrieving False Positives\n- Drawing the WordCloud of the Words in Essay Text\n- Printing BoxPlot\n- Printing PDF\n    \n## 8. Decision Tree\n- **8.1: SET 1 Applying Decision Tree on TF-IDF.**\n    - 8.1.1: SET 1 Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV.\n    - 8.1.2: SET 1 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.1.3: SET 1 Confusion Matrix\n        - 8.1.3.1: SET 1 Confusion Matrix: Train\n        - 8.1.3.2: SET 1 Confusion Matrix: Test\n                - Retrieving FP for Test : Set 1\n                - Printing Word Cloud FP - Essay for Test : Set 1\n                - Printing Box Plot FP - Price for Test : Set 1\n                - Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 1\n                \n- **8.2: SET 2 Applying Decision Tree on W2V TF-IDF.**\n    - 8.2.1: SET 2 Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV.\n    - 8.2.2: SET 2 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.2.3: SET 2 Confusion Matrix\n        - 8.2.3.1: SET 2 Confusion Matrix: Train\n        - 8.2.3.2: SET 2 Confusion Matrix: Test\n                - Retrieving FP for Test : Set 2\n                - Printing Word Cloud FP - Essay for Test : Set 2\n                - Printing Box Plot FP - Price for Test : Set 2\n                - Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 2\n\n## TASK 2: FINDING FEATURE IMPORTANCE IN SET 1\n\n- **8.3: SET 3 Applying Decision Tree on Important Feature dataset 1**\n    - 8.3.1: SET 3 Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV.\n    - 8.3.2: SET 3 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.3.3: SET 3 Confusion Matrix\n        - 8.3.3.1: SET 3 Confusion Matrix: Train\n        - 8.3.3.2: SET 3 Confusion Matrix: Test\n                - Retrieving FP for Test : Set 3\n                - Printing Word Cloud FP - Essay for Test : Set 3\n                - Printing Box Plot FP - Price for Test : Set 3\n                - Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 3\n\n\n## 9. CONCLUSION ","7de1a1ef":"# Printing BoxPlot","ffb4f147":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","fd3ada0b":"###   ->->-> 6.3.4.2. C: TF-IDF W2V: Title -> Test","6615fa02":"# ->-> 6.3.1: TF-IDF","bc7bc80b":"# Printing PDF","b978b106":"# -> 6.3: VECTORIZING TEXT DATA","46cb3aae":"## ->-> 6.2.2: Normalizing Numerical data: Teacher's Previous Projects","2c7e579a":"# 7. MERGING FEATURES","36845a71":"## Retrieving FP for Test : Set 1","96f94f15":"\n## ->-> 8.1.1: <font color='red'> SET 1<\/font> Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV","974581ed":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","ef8bb747":"# 4. SAMPLING\n## -> 4.1: Taking Sample from the complete dataset\n## NOTE: A sample of 10000 Datapoints is taken due to lack computational resource.","e35a3984":"# -> 6.1: VECTORIZING CATEGORICAL DATA","02a0a7aa":"# IMPORTANT FUNCTIONS FOR SET 3 (TASK 2)","0f169c2c":"###   ->->-> 6.3.2.1. A: TF-IDF W2V: Essays -> Train","e7e07ed7":"## ->-> 6.1.3 Vectorizing Categorical data: School State","55ac44df":"### ->->-> 8.2.3.1: <font color='red'> SET 2<\/font> Confusion Matrix: Train","6425f3d8":"**Observation:**\n1. The proportion of Majority class of 85% and Minority class of 15% is maintained in Training, CV and Testing dataset.","d1a6ee56":"## ->-> 6.1.1: Vectorizing Categorical data: Clean Subject Categories","0ac44af1":"###   ->->-> 6.3.4.2. B: TF-IDF W2V: Title -> CV","6ee8afd7":"## Printing PDF FP - Teacher Number previously Posted Projects for Test: Set 3","88f32e6c":"## ->->-> 6.3.1.2: TF-IDF: Title (Train, CV, Test)","1ea91b25":"## ->-> 6.1.4 Vectorizing Categorical data: Teacher Prefix","9051b032":"### -> -> 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.","77e120da":"## -> 7.1: Merging all ONE HOT features","3ad62e19":"###   ->->-> 6.3.4.1. C: TF-IDF W2V: Essays -> Test","573ed963":"**Query 1.1: PreProcessing Teacher Prefix Done <br>\nAction Taken: Removed '.' from the prefixes and converted to lower case**","06e364d2":"#### Merging Project Essays 1 2 3 4 into Essays","5dd7c2a2":"Reference: https:\/\/elitedatascience.com\/imbalanced-classes","0f0e2580":"### ->->-> 8.1.3.2: <font color='red'> SET 2<\/font> Confusion Matrix: Test","a23c4822":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","14dd6848":"#### Checking total number of enteries with NaN values","9edf9d29":"\n## ->-> 8.2.2: <font color='red'> SET 2<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","0c951fd6":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical","556ef4e2":"# 3. TEXT PROCESSING","e5652b07":"\n## ->-> 8.1.3: <font color='red'> SET 1<\/font> Confusion Matrix","5d0e9a57":"# 6. PREPARING DATA FOR MODELS","f364a97a":"#### -> Merging Price with Project Data","0903b0e4":"\n## ->-> 8.3.1: <font color='red'> SET 3<\/font> Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV","ff83d659":"## -> 2.3: Preprocessing: Project Grade Category","01553761":"## -> 2.1: Preprocessing: Project Subject Categories","9463f598":"\n## ->-> 8.2.1: <font color='red'> SET 2<\/font> Hyper parameter tuning to find best 'max_depth' and Best 'min_sample_split' using GRIDSEARCHCV","8ee9c08c":"# -> 8.1:<font color='red'> SET 1<\/font>  Applying Decision Tree on TFIDF.","59a33cd6":"## ->-> 6.2.5: Normalizing Numerical data: Quantity","cf750118":"## 1. READING DATA","24b2ad44":"## ->->-> 6.3.1.1: TF-IDF (BiGrams): Essays (Train, CV, Test)","6a2686f2":"## ->-> 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories"}}