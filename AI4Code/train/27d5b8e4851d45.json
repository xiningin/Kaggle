{"cell_type":{"e93919b0":"code","24d957b4":"code","db56212b":"code","64757eae":"code","9e0d37a8":"code","a9f17382":"code","12460c19":"code","80110d8e":"code","029493fa":"code","cd441277":"markdown","960afa1f":"markdown","ffb85b76":"markdown","1b516d6d":"markdown","e4c09145":"markdown","144a54aa":"markdown","409b6636":"markdown","a2b87955":"markdown"},"source":{"e93919b0":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords # stop words\nfrom sklearn.model_selection import train_test_split # train-test split\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words \nfrom sklearn.feature_extraction.text import TfidfVectorizer # tf-idf\nfrom sklearn.linear_model import LogisticRegression # classifier\n\nimport re\nimport string\n\nsns.set_theme() # setting sns default theme","24d957b4":"# read datasets\ndf_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_train.head()","db56212b":"print(\"Dataframe shape: \", df_train.shape)\nprint(\"Number of rows: \", df_train.shape[0])\nprint(\"Number of cols: \", df_train.shape[1])\nprint(\"Column names: \", list(df_train.columns))\n\nprint(\"Number of unique values in following columns: \")\nfor col_name in list(df_train.columns):\n    print(\" {:>10s} : {:<4d}\".format(col_name, len(df_train[col_name].unique())))\n\nprint(\"Number of NULL values in following columns: \")\nfor col_name in list(df_train.columns):\n    print(\" {:>10s} : {:<4d}\".format(col_name, df_train[col_name].isnull().sum()))\n    \nprint(\"Number of 0 target values: \", df_train['target'].value_counts()[0])\nprint(\"Number of 1 target values: \", df_train['target'].value_counts()[1])\n\nax1 = sns.histplot(df_train['target'])\nax1.set(xlabel='Target')\nax1.set_title('Training dataset')\nplt.show()\n\nlength_true = df_train[df_train['target'] == 1]['text'].apply(lambda x: len(x)).to_numpy()\nlength_false = df_train[df_train['target'] == 0]['text'].apply(lambda x: len(x)).to_numpy()\n\nax2 = sns.histplot({'True':length_true, 'False':length_false}, alpha=0.6)\nax2.set(xlabel='Length of sentences')\nax2.set_title('Training dataset')\nplt.show()","64757eae":"# fucntion to remove URL\ndef removeURL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# fucntion to remove HTML\ndef removeHTML(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# fucntion to remove emoji\ndef removeEMOJI(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# fucntion to remove punctuations\ndef removePUNCT(text):\n    table = text.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n# fucntion to convert to lowercase\ndef removeCASE(text):\n    text = text.lower()\n    return text\n\n# fucntion to remove stop words\nSTOP_WORDS = set(stopwords.words('english')) # stop words\ndef removeSTOP(text):\n    text = ' '.join([i for i in text.split() if i not in STOP_WORDS])\n    return text\n\n\n# apply above functions to training and testing datset\ndf_train['text'] = df_train['text'].apply(lambda x : removeURL(x))\ndf_train['text'] = df_train['text'].apply(lambda x : removeHTML(x))\ndf_train['text'] = df_train['text'].apply(lambda x : removeEMOJI(x))\ndf_train['text'] = df_train['text'].apply(lambda x : removePUNCT(x))\ndf_train['text'] = df_train['text'].apply(lambda x : removeCASE(x))\ndf_train['text'] = df_train['text'].apply(lambda x : removeSTOP(x))\n\n\ndf_test['text'] = df_test['text'].apply(lambda x : removeURL(x))\ndf_test['text'] = df_test['text'].apply(lambda x : removeHTML(x))\ndf_test['text'] = df_test['text'].apply(lambda x : removeEMOJI(x))\ndf_test['text'] = df_test['text'].apply(lambda x : removePUNCT(x))\ndf_test['text'] = df_test['text'].apply(lambda x : removeCASE(x))\ndf_test['text'] = df_test['text'].apply(lambda x : removeSTOP(x))\n\n\n# drop keyword and location columns from training and testing dataset\ndf_train = df_train.drop(labels=['keyword', 'location'], axis=1)\ndf_test = df_test.drop(labels=['keyword', 'location'], axis=1)","9e0d37a8":"df_train.head()","a9f17382":"# training validation split\nx_train, x_val, y_train, y_val = train_test_split(df_train['text'].tolist(), \n                                                    df_train['target'].tolist(), \n                                                    test_size=0.2, \n                                                    random_state=0)","12460c19":"count_vectorizer = CountVectorizer(ngram_range=(1, 3))\nx_train_emb = count_vectorizer.fit_transform(x_train)\nx_val_emb = count_vectorizer.transform(x_val)\nx_test_emb = count_vectorizer.transform(df_test['text'].tolist())\n\n# train model\nmodel = LogisticRegression()\nmodel.fit(x_train_emb, y_train)\n\n# validation\nval_pred = model.predict(x_val_emb)\nval_acc = model.score(x_val_emb, y_val)\nprint(\"Bag of words val: \", round(val_acc*100, 2))","80110d8e":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\nx_train_emb = tfidf_vectorizer.fit_transform(x_train)\nx_val_emb = tfidf_vectorizer.transform(x_val)\nx_test_emb = tfidf_vectorizer.transform(df_test['text'].tolist())\n\n# train model\nmodel = LogisticRegression()\nmodel.fit(x_train_emb, y_train)\n\n# validation\nval_pred = model.predict(x_val_emb)\nval_acc = model.score(x_val_emb, y_val)\nprint(\"TF-IDF val: \", round(val_acc*100, 2))","029493fa":"# submission\ntest_pred = model.predict(x_test_emb) # check the model before submission\n\ndf_sub = pd.DataFrame()\ndf_sub['id'] = df_test['id']\ndf_sub['target'] = test_pred\n\ndf_sub.to_csv('submission.csv',index = False)","cd441277":"## TF-IDF","960afa1f":"## Bag of words","ffb85b76":"## Submission ","1b516d6d":"## Imports","e4c09145":"## Reading training and testing datasets","144a54aa":"## Dataset exploration","409b6636":"## Training - Validatioan split","a2b87955":"## Data preprocessing"}}