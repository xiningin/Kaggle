{"cell_type":{"ad573c76":"code","fd71e3e3":"code","a0923343":"code","bf38303c":"code","299e52b2":"code","b85d4066":"code","5e93fae0":"code","1710e1cb":"code","fe27236a":"code","2db42389":"code","50998053":"code","f146534c":"code","1f349e29":"code","e0fca5d6":"code","da07c4ba":"code","bdc2e8da":"code","ea51ce80":"code","f5779fa0":"code","07557571":"code","60264173":"code","9f2ede24":"code","c47d62b9":"code","1dd8a959":"code","9ab14f6d":"code","7dd13c75":"code","1b82fabb":"code","3aba5f8c":"code","28b319ac":"code","f279cd76":"code","9d6f0453":"code","c75a936f":"code","f183e38f":"code","77402a03":"code","f0bf1189":"code","4f677926":"code","e532d0d5":"code","2799f926":"code","74721ef2":"code","d632f5ac":"code","0fe659f2":"code","58d49c78":"code","cc4d4add":"markdown","740b81ea":"markdown","ec7e00c0":"markdown","dc81edea":"markdown","a98581ce":"markdown","e9c4882c":"markdown","5d6971d0":"markdown","8d4f3b24":"markdown","d7d4f354":"markdown","c5ad8cee":"markdown","7566dcab":"markdown","191cfb43":"markdown","2197b6e2":"markdown","0437688d":"markdown","7dbea1d8":"markdown","ca5fb3c7":"markdown","8c2c697a":"markdown","d36d92bc":"markdown"},"source":{"ad573c76":"# data manipulation \nimport numpy as np\nimport pandas as pd\n\n\n# modeling utilities\nimport scipy.stats as stats\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import  linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\n\n# plotting\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# setting params\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (30, 10),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\n\nsn.set_style('whitegrid')\nsn.set_context('talk')\n\nplt.rcParams.update(params)\npd.options.display.max_colwidth = 600\n\n# pandas display data frames as tables\nfrom IPython.display import display, HTML","fd71e3e3":"## Step 1: loading data\nhour_df = pd.read_csv('..\/input\/hour.csv')\nhour_df.head(2)","a0923343":" print(\"Shape of dataset::{}\".format(hour_df.shape))","bf38303c":"## Step 2: Data types\nhour_df.dtypes","299e52b2":"### Renaming column namees to being more readable\nhour_df.rename(columns={'instant':'rec_id',\n                            'dteday':'datetime',\n                            'holiday':'is_holiday',\n                            'workingday':'is_workingday',\n                            'weathersit':'weather_condition',\n                            'hum':'humidity',\n                            'mnth':'month',\n                            'cnt':'total_count',\n                            'hr':'hour',\n                            'yr':'year'},inplace=True)","b85d4066":"hour_df.columns","5e93fae0":"# dataset summary stats\nhour_df.describe()","1710e1cb":"# date time conversion\nhour_df['datetime'] = pd.to_datetime(hour_df.datetime)\n\n# categorical variables\nhour_df['season'] = hour_df.season.astype('category')\nhour_df['is_holiday'] = hour_df.is_holiday.astype('category')\nhour_df['weekday'] = hour_df.weekday.astype('category')\nhour_df['weather_condition'] = hour_df.weather_condition.astype('category')\nhour_df['is_workingday'] = hour_df.is_workingday.astype('category')\nhour_df['month'] = hour_df.month.astype('category')\nhour_df['year'] = hour_df.year.astype('category')\nhour_df['hour'] = hour_df.hour.astype('category')","fe27236a":"hour_df.dtypes","2db42389":"hour_df.head(1)","50998053":"fig,ax = plt.subplots()\nsn.pointplot(data=hour_df[['hour',\n                           'total_count',\n                           'season']],\n             x='hour',y='total_count',\n             hue='season',ax=ax)\nax.set(title=\"Season wise hourly distribution of counts\")","f146534c":"hour_df[['hour',\n                           'total_count',\n                           'season']].head(10)","1f349e29":"fig,ax = plt.subplots()\nsn.pointplot(data=hour_df[['hour','total_count','weekday']],x='hour',y='total_count',hue='weekday',ax=ax)\nax.set(title=\"Weekday wise hourly distribution of counts\")","e0fca5d6":"fig,ax = plt.subplots()\nsn.boxplot(data=hour_df[['hour','total_count']],x=\"hour\",y=\"total_count\",ax=ax)\nax.set(title=\"Box Plot for hourly distribution of counts\")","da07c4ba":"fig,ax = plt.subplots()\nsn.barplot(data=hour_df[['month',\n                         'total_count']],\n           x=\"month\",y=\"total_count\")\nax.set(title=\"Monthly distribution of counts\")","bdc2e8da":"df_col_list = ['month','weekday','total_count']\nplot_col_list= ['month','total_count']\nspring_df = hour_df[hour_df.season==1][df_col_list]\nsummer_df = hour_df[hour_df.season==2][df_col_list]\nfall_df = hour_df[hour_df.season==3][df_col_list]\nwinter_df = hour_df[hour_df.season==4][df_col_list]\n\nfig,ax= plt.subplots(nrows=2,ncols=2)\nsn.barplot(data=spring_df[plot_col_list],x=\"month\",y=\"total_count\",ax=ax[0][0],)\nax[0][0].set(title=\"Spring\")\n\nsn.barplot(data=summer_df[plot_col_list],x=\"month\",y=\"total_count\",ax=ax[0][1])\nax[0][1].set(title=\"Summer\")\n\nsn.barplot(data=fall_df[plot_col_list],x=\"month\",y=\"total_count\",ax=ax[1][0])\nax[1][0].set(title=\"Fall\")\n\nsn.barplot(data=winter_df[plot_col_list],x=\"month\",y=\"total_count\",ax=ax[1][1])  \nax[1][1].set(title=\"Winter\")","ea51ce80":"sn.violinplot(data=hour_df[['year',\n                            'total_count']],\n              x=\"year\",y=\"total_count\")","f5779fa0":"fig,(ax1,ax2) = plt.subplots(ncols=2)\nsn.barplot(data=hour_df,x='is_holiday',y='total_count',hue='season',ax=ax1)\nsn.barplot(data=hour_df,x='is_workingday',y='total_count',hue='season',ax=ax2)","07557571":"fig,(ax1,ax2)= plt.subplots(ncols=2)\nsn.boxplot(data=hour_df[['total_count',\n                         'casual','registered']],ax=ax1)\nsn.boxplot(data=hour_df[['temp','windspeed']],ax=ax2)","60264173":"hour_df.columns","9f2ede24":"fig,ax1= plt.subplots()\nsn.boxplot(data=hour_df[['total_count',\n                         'hour']],x=\"hour\",y=\"total_count\",ax=ax1)","c47d62b9":"corrMatt = hour_df[[\"temp\",\"atemp\",\n                    \"humidity\",\"windspeed\",\n                    \"casual\",\"registered\",\n                    \"total_count\"]].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nsn.heatmap(corrMatt, mask=mask,\n           vmax=.8, square=True,annot=True)","1dd8a959":"def fit_transform_ohe(df,col_name):\n    \"\"\"This function performs one hot encoding for the specified\n        column.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: label_encoder, one_hot_encoder, transformed column as pandas Series\n\n    \"\"\"\n    # label encode the column\n    le = preprocessing.LabelEncoder()\n    le_labels = le.fit_transform(df[col_name])\n    df[col_name+'_label'] = le_labels\n    \n    # one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    feature_arr = ohe.fit_transform(df[[col_name+'_label']]).toarray()\n    feature_labels = [col_name+'_'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return le,ohe,features_df\n\n# given label encoder and one hot encoder objects, \n# encode attribute to ohe\ndef transform_ohe(df,le,ohe,col_name):\n    \"\"\"This function performs one hot encoding for the specified\n        column using the specified encoder objects.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        le(Label Encoder): the label encoder object used to fit label encoding\n        ohe(One Hot Encoder): the onen hot encoder object used to fit one hot encoding\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: transformed column as pandas Series\n\n    \"\"\"\n    # label encode\n    col_labels = le.transform(df[col_name])\n    df[col_name+'_label'] = col_labels\n    \n    # ohe \n    feature_arr = ohe.fit_transform(df[[col_name+'_label']]).toarray()\n    feature_labels = [col_name+'_'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return features_df","9ab14f6d":"X, X_test, y, y_test = train_test_split(hour_df.iloc[:,0:-3], hour_df.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\n\nX.reset_index(inplace=True)\ny = y.reset_index()\n\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\n\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))","7dd13c75":"stats.probplot(y.total_count.tolist(), dist=\"norm\", plot=plt)\nplt.show()","1b82fabb":"cat_attr_list = ['season','is_holiday',\n                 'weather_condition','is_workingday',\n                 'hour','weekday','month','year']\nnumeric_feature_cols = ['temp','humidity','windspeed','hour','weekday','month','year']\nsubset_cat_features =  ['season','is_holiday','weather_condition','is_workingday']","3aba5f8c":"encoded_attr_list = []\nfor col in cat_attr_list:\n    return_obj = fit_transform_ohe(X,col)\n    encoded_attr_list.append({'label_enc':return_obj[0],\n                              'ohe_enc':return_obj[1],\n                              'feature_df':return_obj[2],\n                              'col_name':col})","28b319ac":"feature_df_list = [X[numeric_feature_cols]]\nfeature_df_list.extend([enc['feature_df'] \\\n                        for enc in encoded_attr_list \\\n                        if enc['col_name'] in subset_cat_features])\n\ntrain_df_new = pd.concat(feature_df_list, axis=1)\nprint(\"Shape::{}\".format(train_df_new.shape))","f279cd76":"X = train_df_new\ny= y.total_count.values.reshape(-1,1)\n\nlin_reg = linear_model.LinearRegression()","9d6f0453":"#Cross Validation\npredicted = cross_val_predict(lin_reg, X, y, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y, y-predicted)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residual')\nplt.show()","c75a936f":"r2_scores = cross_val_score(lin_reg, X, y, cv=10)\nmse_scores = cross_val_score(lin_reg, X, y, cv=10,scoring='neg_mean_squared_error')","f183e38f":"fig, ax = plt.subplots()\nax.plot([i for i in range(len(r2_scores))],r2_scores,lw=2)\nax.set_xlabel('Iteration')\nax.set_ylabel('R-Squared')\nax.title.set_text(\"Cross Validation Scores, Avg:{}\".format(np.average(r2_scores)))\nplt.show()","77402a03":"print(\"R-squared::{}\".format(r2_scores))\nprint(\"MSE::{}\".format(mse_scores))","f0bf1189":"lin_reg.fit(X,y)","4f677926":"test_encoded_attr_list = []\nfor enc in encoded_attr_list:\n    col_name = enc['col_name']\n    le = enc['label_enc']\n    ohe = enc['ohe_enc']\n    test_encoded_attr_list.append({'feature_df':transform_ohe(X_test,\n                                                              le,ohe,\n                                                              col_name),\n                                   'col_name':col_name})\n    \n    \ntest_feature_df_list = [X_test[numeric_feature_cols]]\ntest_feature_df_list.extend([enc['feature_df'] \\\n                             for enc in test_encoded_attr_list \\\n                             if enc['col_name'] in subset_cat_features])\n\ntest_df_new = pd.concat(test_feature_df_list, axis=1) \nprint(\"Shape::{}\".format(test_df_new.shape))","e532d0d5":"test_df_new.head()","2799f926":"X_test = test_df_new\ny_test = y_test.total_count.values.reshape(-1,1)\n\ny_pred = lin_reg.predict(X_test)\n\nresiduals = y_test-y_pred","74721ef2":"r2_score = lin_reg.score(X_test,y_test)\nprint(\"R-squared::{}\".format(r2_score))\nprint(\"MSE: %.2f\"\n      % metrics.mean_squared_error(y_test, y_pred))","d632f5ac":"fig, ax = plt.subplots()\nax.scatter(y_test, residuals)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residuals')\nax.title.set_text(\"Residual Plot with R-Squared={}\".format(np.average(r2_score)))\nplt.show()","0fe659f2":"import statsmodels.api as sm\n\n# Set the independent variable\nX = X.values.tolist()\n\n# This handles the intercept. \n# Statsmodel takes 0 intercept by default\nX = sm.add_constant(X)\n\nX_test = X_test.values.tolist()\nX_test = sm.add_constant(X_test)\n\n\n# Build OLS model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Get the predicted values for dependent variable\npred_y = results.predict(X_test)\n\n# View Model stats\nprint(results.summary())","58d49c78":"plt.scatter(pred_y,y_test)","cc4d4add":"### Working Day Vs Holiday Distribution","740b81ea":"### Monthly distribution of Total Counts","ec7e00c0":"## Visualize Attributes, Trends and Relationships\n\n### Hourly distribution of Total Counts\n+ Seasons are encoded as 1:spring, 2:summer, 3:fall, 4:winter\n+ Exercise: Convert season names to readable strings and visualize data again","dc81edea":"The generated plot is shown in Figure 6-6. We can easily mark out that for the three count related attributes, all of them seem to have a sizable number of outlier values. The casual rider distribution has overall lower numbers though. For weather attributes of temperature and wind speed, we find outliers only in the case of wind speed.","a98581ce":"+ Weekends (0 and 6) and Weekdays (1-5) show different usage trends with weekend's peak usage in during afternoon hours\n+ Weekdays follow the overall trend, similar to one visualized in the previous plot\n+ Weekdays have higher usage as compared to weekends\n+ It would be interesting to see the trends for casual and registered users separately","e9c4882c":"### Year Wise Count Distributions","5d6971d0":"Normality Test","8d4f3b24":"+ Both years have multimodal distributions\n+ 2011 has lower counts overall with a lower median\n+ 2012 has a higher max count though the peaks are around 100 and 300 which is then tapering off","d7d4f354":"### CORRELATIONS\nCorrelation helps us understand relationships between different attributes of the data. Since this chapter focuses on forecasting, correlations can help us understand and exploit relationships to build better models.\nNote\nIt is important to understand that correlation does not imply causation. We strongly encourage you to explore more on the same.\nThe following snippet first prepares a correlational matrix using the pandas utility function corr(). It then uses a heat map to plot the correlation matrix.","c5ad8cee":"### Setting weather to proper datetime and categorical data","7566dcab":"+ Correlation between temp and atemp is very high (as expected)\n+ Same is te case with registered-total_count and casual-total_count\n+ Windspeed to humidity has negative correlation\n+ Overall correlational statistics are not very high.","191cfb43":"### Regression Analysis\n\nRegression analysis is a statistical modeling technique used by statisticians and Data Scientists alike. It is the process of investigating relationships between dependent and independent variables. Regression itself includes a variety of techniques for modeling and analyzing relationships between variables. It is widely used for predictive analysis, forecasting, and time series analysis.\nThe dependent or target variable is estimated as a function of independent or predictor variables. The estimation function is called the regression function .\nNote\nIn a very abstract sense, regression is referred to estimation of continuous response\/target variables as opposed to classification, which estimates discrete targets.\n\n\n### ASSUMPTIONS\nRegression analysis has a few general assumptions while specific analysis techniques have added (or reduced) assumptions as well. The following are important general assumptions for regression analysis:\nThe training dataset needs to be representative of the population being modeled.\nThe independent variables are linearly independent, i.e., one independent variable cannot be explained as a linear combination of others. In other words, there should be no multicollinearity.\nHomoscedasticity of error, i.e. the variance of error, is consistent across the sample .","2197b6e2":"+ Months June-Oct have highest counts. Fall seems to be favorite time of the year to use cycles","0437688d":"+ Early hours (0-4) and late nights (21-23) have low counts but significant outliers\n+ Afternoon hours also have outliers\n+ Peak hours have higher medians and overall counts with virtually no outliers","7dbea1d8":"Analysis From Chart\n+ The above plot shows peaks around 8am and 5pm (office hours)\n+ Overall higher usage in the second half of the day","ca5fb3c7":"## Problem Statement:\n>With environmental issues and health becoming trending topics, usage of bicycles as a mode of transportation has gained traction in recent years. To encourage bike usage, cities across the world have successfully rolled out bike sharing programs. Under such schemes, riders can rent bicycles using manual\/automated kiosks spread across the city for defined periods. In most cases, riders can pick up bikes from one location and return them to any other designated place.\n\n>The bike sharing platforms from across the world are hotspots of all sorts of data, ranging from travel time, start and end location, demographics of riders, and so on. This data along with alternate sources of information such as weather, traffic, terrain, and so on makes it an attractive proposition for different research areas.\n\n>The Capital Bike Sharing dataset contains information related to one such bike sharing program underway in Washington DC. Given this augmented (bike sharing details along with weather information) dataset, can we forecast bike rental demand for this program?","8c2c697a":"### Outliers\n\nWhile exploring and learning about any dataset , it is imperative that we check for extreme and unlikely values. Though we handle missing and incorrect information while preprocessing the dataset, outliers are usually caught during EDA. Outliers can severely and adversely impact the downstream steps like modeling and the results.\nWe usually utilize boxplots to check for outliers in the data. In the following snippet, we analyze outliers for numeric attributes like total_count, temperature, and wind_speed.","d36d92bc":"### PREPROCESSING EDA"}}