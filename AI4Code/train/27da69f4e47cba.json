{"cell_type":{"e1b7b5b2":"code","b0583d41":"code","dcfdd99c":"code","db73638a":"code","d25fa0d6":"code","55d109f0":"code","04faa275":"code","27e6c773":"code","78c1e886":"code","1a82ea0f":"code","12f9b380":"code","6ede4513":"code","fc39a586":"markdown","45adc2d3":"markdown","8269871e":"markdown","09378690":"markdown","34b528e3":"markdown","71154313":"markdown","690130d3":"markdown","56a383eb":"markdown"},"source":{"e1b7b5b2":"import os\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport string\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n\ninput_path = Path('\/kaggle\/input')\nfor dirname, _, filenames in os.walk(input_path):\n    for filename in filenames:\n        print(Path(dirname) \/ filename)","b0583d41":"data_path = input_path \/ 'nlp-getting-started'\ntrain_df = pd.read_csv(data_path \/ 'train.csv')\ntrain_df.head()","dcfdd99c":"test_df = pd.read_csv(data_path \/ 'test.csv')\ntest_df.head()","db73638a":"print(f'Rows missing keyword: {train_df.keyword.isnull().mean() * 100:.2f}%')\nprint(f'Rows missing keyword with target = 1: {train_df.loc[train_df.target==1].keyword.isnull().mean() * 100:.2f}%')\nprint(f'Rows missing keyword with target = 0: {train_df.loc[train_df.target==0].keyword.isnull().mean() * 100:.2f}%')\n\nprint('')\nprint(f'Rows missing location: {train_df.location.isnull().mean() * 100:.2f}%')\nprint(f'Rows missing location with target = 1: {train_df.loc[train_df.target==1].location.isnull().mean() * 100:.2f}%')\nprint(f'Rows missing location with target = 0: {train_df.loc[train_df.target==0].location.isnull().mean() * 100:.2f}%')","d25fa0d6":"def is_number(word): \n        try:\n            float(word.replace(',', ''))\n            return True\n        except ValueError:\n            return False\n\ndef preprocess_text(raw_df):\n    df = raw_df.copy()\n    \n    # Replace mentions & links\n    df['cleaned_text'] = df.text.str.replace('@\\S+', 'mention')\n    df.cleaned_text = df.cleaned_text.str.replace('http\\S+', 'http')\n    \n    # Remove hash from hashtag\n    df.cleaned_text = df.cleaned_text.str.replace('#', '')\n    \n    # Tokenize\n    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n    df['words'] = df.cleaned_text.apply(tokenizer.tokenize)\n\n    # Remove punctuation\n    df.words = df.words.apply(lambda word_list: [w for w in word_list if w not in string.punctuation])\n\n    # Stemming\n    ps = PorterStemmer()\n    df.words = df.words.apply(lambda word_list: [ps.stem(w) for w in word_list])\n\n    # Split stop words and rest\n    df['stop_words'] = df.words.apply(lambda word_list: [w for w in word_list if w in set(stopwords.words('english'))])\n    df.words = df.words.apply(lambda word_list: [w for w in word_list if w not in set(stopwords.words('english'))])\n\n    # Remove numbers\n    df.words = df.words.apply(lambda word_list: [w for w in word_list if not is_number(w)])\n    \n    # Get cleaned text\n    df.cleaned_text = df.words.apply(lambda x: ' '.join(x))\n    \n    return df\n\ntrain_df = preprocess_text(train_df)\ntest_df = preprocess_text(test_df)\ntrain_df.head()","55d109f0":"def stem_keyword(keyword):\n    ps = PorterStemmer()\n    # Handle split keywords\n    words = keyword.split('%20')\n    return '%20'.join([ps.stem(w) for w in words])\n\ndef preprocess_keywords(raw_df):\n    df = raw_df.copy()\n    df['cleaned_keyword'] = df.keyword.fillna('no%20keyword')\n    df.cleaned_keyword = df.cleaned_keyword.apply(stem_keyword)\n    return df\n\ntrain_df = preprocess_keywords(train_df)\ntest_df = preprocess_keywords(test_df)\ntrain_df.head()","04faa275":"# Get columns of training set to form inputs and outputs.\ntrain_text = train_df.cleaned_text.values\ntrain_keywords = train_df.cleaned_keyword.values\ntrain_targets = train_df.target.values\n\n# Get columns of test set to form inputs and outputs.\ntest_text = test_df.cleaned_text.values\ntest_keywords = test_df.cleaned_keyword.values","27e6c773":"# Vectorise text\nword_vec = TfidfVectorizer(\n    ngram_range=(1, 1),\n    max_df=0.99,\n    min_df=2,\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=False,\n    norm='l2'\n)\nword_vec.fit(np.hstack([train_text, test_text]))\ntrain_text_transf = word_vec.transform(train_text).toarray()\ntest_text_transf = word_vec.transform(test_text).toarray()\n\n#\u00a0Vectorise keywords\nkeyword_vec = TfidfVectorizer(\n    ngram_range=(1, 1),\n    max_df=0.99,\n    min_df=2,\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=False,\n    norm='l1'\n)\nkeyword_vec.fit(np.hstack([train_keywords, test_keywords]))\ntrain_keywords_transf = word_vec.transform(train_keywords).toarray()\ntest_keywords_transf = word_vec.transform(test_keywords).toarray()\n\n# Model inputs\ntrain_inputs = np.hstack([train_keywords_transf, train_text_transf])\ntest_inputs = np.hstack([test_keywords_transf, test_text_transf])\n\n# Split training and validataion sets\nX_train, X_val, y_train, y_val = train_test_split(train_inputs, train_targets, test_size=0.2, stratify=train_targets)\nX_test = test_inputs","78c1e886":"# Stop early and save the best model\nmodel_path = f'{datetime.utcnow():%Y%m%d%H%M%S}_best_model.hdf5'\ncheck_point = ModelCheckpoint(model_path, monitor = \"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\nearly_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n\n# Build sequential model\nmodel = Sequential()\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.9))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# Fitting & validation on training set.\nmodel.fit(X_train, y_train,\n          validation_data=(X_val, y_val),\n          batch_size=100,\n          epochs=100,\n          callbacks=[early_stop, check_point])","1a82ea0f":"def predict(X, model):\n    return (model.predict(X) > 0.5).ravel().astype(int)\n\ny_pred = predict(X_test, model)","12f9b380":"sample_sub_df = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\nsub_df = pd.DataFrame({\n    'id': sample_sub_df['id'].values.tolist(),\n    'target': y_pred.ravel()\n})\nsub_df.head()","6ede4513":"sub_df.to_csv('submission.csv', index=False)","fc39a586":"__Notes__:\n* A small fraction of rows are missing a keyword\n* A third of rows are missing a location. Also from above location data is quite dirty.\n* There aren't particularly noticeable differences in either case between target = 0 and 1","45adc2d3":"# Generate predictions\nRetrain model, generate predictions and submit.","8269871e":"Use prepocessed text and keywords to get inputs.","09378690":"Take a quick look at missing values","34b528e3":"# Introduction\nThis notebook proposes a model to predict binary labels for tweets to indicate whether the tweet refers to a disaster or not.","71154313":"# Load data\nRead data from csvs provided into pandas.DataFrame.","690130d3":"# Preprocessing\nClean tweets and transform to sequences of integers for modelling.","56a383eb":"# Training & validation\nTrain and validate a model."}}