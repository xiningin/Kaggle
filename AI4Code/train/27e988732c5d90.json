{"cell_type":{"95f1badb":"code","f10b3135":"code","30047c64":"code","7b73b81b":"code","ef402c4a":"code","00b07447":"code","acc85e66":"code","2dd0a1d2":"code","f26f3ab5":"code","bdd0171d":"code","61093bfa":"markdown","8d6941cb":"markdown","9f2a8801":"markdown","f4396c15":"markdown","92f3bda1":"markdown"},"source":{"95f1badb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f10b3135":"glass_data = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')\n\n# Glass data\nglass_data.info()\nprint(\"--\"*20)\nglass_data.head()","30047c64":"glass_data.values[50:60, :]","7b73b81b":"glass_data['Type'].value_counts()","ef402c4a":"plt.figure(figsize=(10,6))\nglass_data.hist()\nplt.show()","00b07447":"target = glass_data.values[:, -1]\ncounter = Counter(target)\n\nfor k, v in counter.items():\n    per = v \/ len(target) * 100\n    print('Class=%d, Count=%d, Percentage=%.2f%%' % (k,v,per))","acc85e66":"def load_dataset(file_path):\n    df = pd.read_csv(file_path, header=0)\n    \n    data = df.values\n    #Split data into input and output\n    X, y = data[:, :-1], data[:, -1]\n    # Encode the label data\n    y = LabelEncoder().fit_transform(y)\n    \n    return X, y\n    \n# Evaluate the model\ndef evaluate_model(X, y, model):\n    K = 5\n    R = 3\n    # K-Fold on the data\n    cv = RepeatedStratifiedKFold(n_splits=K, n_repeats=R, random_state=1)\n    \n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    \n    return scores\n\ndef get_models():\n    models, names = list(), list()\n    # SVM\n    models.append(SVC(gamma='auto'))\n    names.append('SVC')\n    \n    models.append(KNeighborsClassifier())\n    names.append('KNN')\n    \n    models.append(BaggingClassifier(n_estimators=1000))\n    names.append('BAG')\n    \n    models.append(RandomForestClassifier(n_estimators=1000))\n    names.append('RF')\n    \n    models.append(ExtraTreesClassifier(n_estimators=1000))\n    names.append('ET')\n    \n    return models, names","2dd0a1d2":"file_path = '\/kaggle\/input\/glass\/glass.csv'\n\nX, y = load_dataset(file_path)\n\nmodels, names = get_models()\n\nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))\n\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","f26f3ab5":"class_weights = {0:1.0, 1:1.0, 2:2.0, 3:2.0, 4:2.0, 5:2.0}\n\nrf_model = RandomForestClassifier(n_estimators=1000, class_weight=class_weights)\n\net_model = ExtraTreesClassifier(n_estimators=1000, class_weight=class_weights)\n\n#Evaluate model\nscores = evaluate_model(X, y, rf_model)\net_score = evaluate_model(X, y, et_model)\n\nprint(\"RF Mean Accuracy: %.3f (%.3f)\" % (np.mean(scores), np.std(scores)))\nprint(\"ET Mean Accuracy: %.3f (%.3f)\" % (np.mean(et_score), np.std(et_score)))","bdd0171d":"rf_model.fit(X, y)\n\nrow = [ 1.5232,13.72, 3.72,0.51, 71.75,  0.09 ,10.06 ,  0.0,  0.16  ]\n\nprint('>Predicted=%d (expected 0)' % (rf_model.predict([row])))","61093bfa":"## Making Predictions","8d6941cb":"## Exploring Data","9f2a8801":"## RandomForestClassifier","f4396c15":"# Glass Identification-Multiclass Classification\n\n## Overview\n\nIn this project, we will use a standard imbalanced machine learning dataset referred to as the \u201cGlass Identification\u201d dataset, or simply \u201cglass.\u201d\n\nThe dataset describes the chemical properties of glass and involves classifying samples of glass using their chemical properties as one of six classes. The dataset was credited to Vina Spiehler in 1987.\n\nIgnoring the sample identification number, there are nine input variables that summarize the properties of the glass dataset; they are:\n\n* RI: refractive index\n* Na: Sodium\n* Mg: Magnesium\n* Al: Aluminum\n* Si: Silicon\n* K: Potassium\n* Ca: Calcium\n* Ba: Barium\n* Fe: Iron","92f3bda1":"## Model Building"}}