{"cell_type":{"a2eae546":"code","01f754f6":"code","a0154210":"code","62db8aef":"code","09c6fcda":"code","9664df27":"code","2f481458":"code","3373c02c":"code","09f2ad01":"code","74b97d0c":"code","65c48041":"code","8aadb46b":"code","87990549":"code","6996639c":"code","3f3d3d38":"code","c00a4a0c":"code","bf2fbeb8":"code","c47a16e8":"code","5e4c3ec9":"code","01a26b14":"code","667a051e":"code","2af6e4c8":"code","1b4b9ee0":"code","7b7387c2":"code","1d8eaeb1":"code","102cd6e2":"code","d8a5df02":"code","8fc982d8":"code","e1908543":"code","ae08426b":"code","1386f1ee":"code","f44a6e0d":"code","4a052908":"code","0e4c98ff":"code","8b52a819":"code","5abaee5c":"code","906954d2":"code","1f4457d7":"code","79c94ce0":"markdown","81eb943c":"markdown","5880f17a":"markdown","422815f2":"markdown","b142b287":"markdown","9cbe5cff":"markdown","9a81ff6e":"markdown","5863064b":"markdown","ea799943":"markdown","f2f5631b":"markdown","10d986f8":"markdown","0e6ae14f":"markdown","c669d3fa":"markdown","a5970e98":"markdown","12e75c5c":"markdown","4dabf89d":"markdown","55ef97e4":"markdown","72e960e9":"markdown","abb30bff":"markdown","2bf2417d":"markdown","0e8ce5ee":"markdown","4ac1c669":"markdown","903f5478":"markdown","4dcf722a":"markdown","66ccb8c3":"markdown","c2ccfd20":"markdown","13ba6d90":"markdown","6b3eb118":"markdown","2e7c3fc5":"markdown","7ff37215":"markdown","10eb7a59":"markdown","b1bb1ded":"markdown","b50b0cb6":"markdown","7538ddff":"markdown","06d00948":"markdown","35e41c02":"markdown","c9798410":"markdown"},"source":{"a2eae546":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom numpy.random import randn\nimport random\nfrom IPython.core.display import display,Image\nfrom string import Template\nimport IPython.display\nimport warnings\n\nclass LinearRegression:\n\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            # compute gradients\n            dw = (1 \/ n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 \/ n_samples) * np.sum(y_predicted - y)\n\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n \n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n\ndef mean_squared_error(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)\n\n\n\nX, y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n\nregressor = LinearRegression(learning_rate=0.01, n_iters=1000)\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)\n    \nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\ny_pred_line = regressor.predict(X)\n\n\n\n\ncmap = plt.get_cmap('viridis')\nfig = plt.figure(figsize=(8,6))\nm1 = plt.scatter(X_train, y_train, color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test, y_test, color=cmap(0.5), s=10)\nplt.plot(X, y_pred_line, color='black', linewidth=2, label=\"Prediction\")\nplt.show()","01f754f6":"class logisticRegression:\n\n  def __init__(self,lr=0.001,n_iters=1000):\n    self.lr = lr\n    self.n_iters = n_iters\n    self.weights = None\n    self.bias = None\n\n  def fit(self,X,y):\n    #init parameters\n    n_samples, n_features = X.shape\n    self.weights = np.zeros(n_features)\n    self.bias = 0\n\n    #gradient descent\n    for _ in range(self.n_iters):\n      linear_model = np.dot(X,self.weights) + self.bias\n      y_predicted = self._sigmoid(linear_model)\n\n      dw = (1\/n_samples) * np.dot(X.T,(y_predicted-y))\n      db = (1\/n_samples) * np.sum(y_predicted-y)\n\n      self.weights -= self.lr *dw\n      self.bias -= self.lr * db \n\n  def predict(self,X):\n    linear_model = np.dot(X,self.weights) + self.bias\n    y_predicted = self._sigmoid(linear_model)\n    y_predicted_cls = [1 if i>0.5 else 0 for i in y_predicted]\n    return y_predicted_cls\n  \n  def _sigmoid(self,x):\n    return(1\/(1+np.exp(-x)))\n\n#Test Logistic regression in breast_cancer\nbc = datasets.load_breast_cancer()\nX, y = bc.data, bc.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n\n\ndef accuracy(y_true,y_pred):\n  accuracy = np.sum(y_true == y_pred)\/len(y_true)\n  return accuracy\n\nregressor = logisticRegression(lr=0.0001,n_iters=1000)\nregressor.fit(X_train, y_train)\n\npredictions = regressor.predict(X_test)\n\nprint(\"Accuracy: \",accuracy(y_test, predictions))","a0154210":"from collections import Counter\n\ndef entropy(y):\n  hist = np.bincount(y)\n  ps = hist\/len(y)\n  return(-np.sum([p * np.log2(p) for p in ps if p>0]))\n\n\n\nclass Node:\n  def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n    self.feature = feature\n    self.threshold = threshold\n    self.left = left\n    self.right = right\n    self.value = value\n  \n  def is_leaf_node(self):\n    return(self.value is not None)\n\n\nclass DecisionTree:\n  def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n    self.min_samples_split = min_samples_split\n    self.max_depth = max_depth\n    self.n_feats = n_feats\n    self.root = None\n\n  def fit(self, X, y):\n    self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n    self.root = self._grow_tree(X, y)\n\n  def _grow_tree(self, X, y, depth=0):\n    n_samples, n_features = X.shape\n    n_labels = len(np.unique(y))\n\n    #stopping criteria\n    if(depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n      leaf_value = self._most_common_label(y)\n      return(Node(value=leaf_value))\n    \n    feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n\n    #greedy search\n    best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n\n    left_idxs, right_idxs = self._split(X[:,best_feat],best_thresh)\n\n    left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+1)\n    right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+1)\n    return(Node(best_feat, best_thresh, left, right))\n  \n\n  def _best_criteria(self, X, y, feat_idxs):\n    best_gain = -1\n    split_idx, split_thresh = None, None\n    for feat_idx in feat_idxs:\n      X_column = X[:, feat_idx]\n      thresholds = np.unique(X_column)\n      for threshold in thresholds:\n        gain = self._information_gain(y, X_column, threshold)\n        if(gain>best_gain):\n          best_gain = gain\n          split_idx = feat_idx\n          split_thresh = threshold\n    return(split_idx, split_thresh)\n\n  def _information_gain(self, y, X_column, split_threh):\n    #parent entropy\n    parent_entropy = entropy(y)\n\n    #generate split\n    left_idxs, right_idxs = self._split(X_column, split_threh)\n    if(len(left_idxs == 0) or len(right_idxs)==0):\n      return 0\n\n    #weighted avg vhild entropy\n    n = len(y)\n    n_l, n_r = len(left_idxs), len(right_idxs)\n    e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n    child_entropy = (n_l\/n)*e_l + (n_r\/n)*e_r\n\n    #return ig\n    ig = parent_entropy - child_entropy\n\n    return ig\n  \n  def _split(self, X_column, split_threh):\n    left_idxs = np.argwhere(X_column <= split_threh).flatten()\n    right_idxs = np.argwhere(X_column > split_threh).flatten()\n    return(left_idxs, right_idxs)\n  \n  def predict(self, X):\n    #traverse tree\n    return(np.array([self._traverse_tree(x, self.root) for x in X]))\n\n  def _traverse_tree(self, x, node):\n    if(node.is_leaf_node()):\n      return(node.value)\n\n    if(x[node.feature] <= node.threshold):\n      return(self._traverse_tree(x, node.left))\n    return(self._traverse_tree(x, node.right))\n\n  def _most_common_label(self, y):\n    counter = Counter(y)\n    most_common = counter.most_common(1)[0][0]\n    return(most_common)\n\n#decision tree test\n\n\ndef accuracy(y_true, y_pred):\n  accuracy = np.sum(y_true == y_pred) \/ len(y_true)\n  return(accuracy)\n\n\n\ndata = datasets.load_breast_cancer()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n\n\nclf = DecisionTree(max_depth=20)\nclf.fit(X_train, y_train)\n\n\n\ny_pred = clf.predict(X_test)\nacc = accuracy(y_test, y_pred)\n\nprint(\"Accuracy: \",acc)","62db8aef":"cnf_matrix = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","09c6fcda":"def bootstrap_sample(X, y):\n  n_samples = X.shape[0]\n  idxs = np.random.choice(n_samples, size = n_samples, replace=True)\n  return(X[idxs], y[idxs])\n\ndef most_common_label(y):\n  counter = Counter(y)\n  most_common = counter.most_common(1)[0][0]\n  return(most_common)\n\nclass RandomForest:\n  def __init__(self, n_trees = 100, min_samples_split=2, max_depth=100, n_feats=None):\n    self.n_trees = n_trees\n    self.min_samples_split = min_samples_split\n    self.max_depth = max_depth\n    self.n_feats = n_feats\n    self.trees = []\n\n  def fit(self, X, y):\n    self.trees = []\n    for _ in range(self.n_trees):\n      tree = DecisionTree(min_samples_split = self.min_samples_split, max_depth=self.max_depth, n_feats=self.n_feats)\n      X_sample, y_sample = bootstrap_sample(X, y)\n      tree.fit(X_sample, y_sample)\n      self.trees.append(tree)\n  def predict(self, X):\n    tree_preds = np.array([tree.predict(X) for tree in self.trees])\n    tree_preds = np.swapaxes(tree_preds, 0, 1)\n\n    y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n    return np.array(y_pred)\n\n\nclf_f = RandomForest(n_trees=3)\nclf_f.fit(X_train, y_train)\n\ny_pred = clf_f.predict(X_test)\nacc = accuracy(y_test, y_pred)\n\nprint(\"Accuracy: \",acc)","9664df27":"cnf_matrix = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","2f481458":"class NaiveBayes:\n\n  def fit(self, X, y):\n    n_samples, n_features = X.shape\n    self._classes = np.unique(y)\n    n_classes = len(self._classes)\n\n    #init mean, var, priors\n    self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n    self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n    self._priors = np.zeros(n_classes, dtype=np.float64)\n\n    for c in self._classes:\n      X_c = X[c==y]\n      self._mean[c,:]=X_c.mean(axis=0)\n      self._var[c,:]=X_c.var(axis=0)\n      self._priors[c] = X_c.shape[0] \/ float(n_samples)\n\n  def predict(self,X):\n    y_pred = [self._predict(x) for x in X]\n    return(y_pred)\n    \n  def _predict(self, x):\n    posteriors = []\n    for idx, c in enumerate(self._classes):\n      #print(idx,c)\n      prior = np.log(self._priors[idx])\n      class_conditional = np.sum(np.log(self._pdf(idx,x)))\n      posterior = prior + class_conditional\n      posteriors.append(posterior)\n    return(self._classes[np.argmax(posteriors)])\n\n\n    \n  def _pdf(self, class_idx, x):\n    mean = self._mean[class_idx]\n    var = self._var[class_idx]\n    numerator = np.exp(-(x-mean)**2\/(2 * var))\n    denominator = np.sqrt(2*np.pi*var)\n    return(numerator\/denominator)\n\n\n# Naive Bayes Test\n\n\ndef accuracy(y_true, y_pred):\n  accuracy = np.sum(y_true == y_pred)\/len(y_true)\n  return(accuracy)\n\nX,y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nnb = NaiveBayes()\nnb.fit(X_train, y_train)\npredictions = nb.predict(X_test)\n\nprint(\"Naive Bayes classification accuracy \",accuracy(y_test, predictions))","3373c02c":"cnf_matrix = confusion_matrix(y_test, predictions)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","09f2ad01":"class SVM:\n  def __init__(self, learning_rate = 0.001, lambda_param=0.01, n_iters=1000):\n    self.lr = learning_rate\n    self.lambda_param = lambda_param\n    self.n_iters = n_iters\n    self.w = None\n    self.b = None\n\n  def fit(self, X, y):\n    y_ = np.where(y<=0,-1,1)\n    n_samples, n_features = X.shape\n\n    self.w = np.zeros(n_features)\n    self.b = 0\n\n    for _ in range(self.n_iters):\n      for idx, x_i in enumerate(X):\n        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >=1\n        if condition:\n          self.w -= self.lr * (2*self.lambda_param * self.w)\n        else:\n          self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n          self.b -= self.lr * y_[idx]\n  \n  def predict(self, X):\n    linear_output = np.dot(X, self.w) - self.b\n    return(np.sign(linear_output))\n\nX, y =  datasets.make_blobs(n_samples=50, n_features=3, centers=3, cluster_std=1.05, random_state=40)\ny = np.where(y == 0, -1, 1)\n\nclf = SVM()\nclf.fit(X, y)\npredictions = clf.predict(X)\n \nprint(clf.w, clf.b)\n\ndef visualize_svm():\n     def get_hyperplane_value(x, w, b, offset):\n          return (-w[0] * x + b + offset) \/ w[1]\n\n     fig = plt.figure()\n     ax = fig.add_subplot(1,1,1)\n     plt.scatter(X[:,0], X[:,1], marker='o',c=y)\n\n     x0_1 = np.amin(X[:,0])\n     x0_2 = np.amax(X[:,0])\n\n     x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n     x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n     x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n     x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n     x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n     x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n     ax.plot([x0_1, x0_2],[x1_1, x1_2], 'y--')\n     ax.plot([x0_1, x0_2],[x1_1_m, x1_2_m], 'k')\n     ax.plot([x0_1, x0_2],[x1_1_p, x1_2_p], 'k')\n\n     x1_min = np.amin(X[:,1])\n     x1_max = np.amax(X[:,1])\n     ax.set_ylim([x1_min-3,x1_max+3])\n\n     plt.show()\n\nvisualize_svm()","74b97d0c":"cnf_matrix = confusion_matrix(y, predictions)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","65c48041":"#KNN Algortihm: Iris Dataset\n\nimport csv\nwith open(r'\/kaggle\/input\/ml-scratch-master\/ML_Scratch-master\/knn\/iris.data') as csvfile:\n    lines = csv.reader(csvfile)\n    for row in lines:\n        pass\n        #print (', '.join(row))\n\nimport random\ndef handleDataset(filename, split, trainingSet=[] , testSet=[]):\n    with open(filename, 'r') as csvfile:\n        lines = csv.reader(csvfile)\n        dataset = list(lines)\n        for x in range(len(dataset)-1):\n            for y in range(4):\n                dataset[x][y] = float(dataset[x][y])\n            if random.random() < split:\n                trainingSet.append(dataset[x])\n            else:\n                testSet.append(dataset[x])\n\n#compute euclidean distance\nimport math\ndef euclideanDistance(instance1, instance2, length):\n    distance = 0\n    for x in range(length):\n        distance += pow((instance1[x] - instance2[x]), 2)\n    return math.sqrt(distance)\n\n#Find k-nearest neighbour\nimport operator \ndef getNeighbors(trainingSet, testInstance, k):\n    distances = []\n    length = len(testInstance)-1\n    for x in range(len(trainingSet)):\n        dist = euclideanDistance(testInstance, trainingSet[x], length)\n        distances.append((trainingSet[x], dist))\n    distances.sort(key=operator.itemgetter(1))\n    neighbors = []\n    for x in range(k):\n        neighbors.append(distances[x][0])\n    return neighbors\n\n\n#Predict the class\nimport operator\ndef getResponse(neighbors):\n    classVotes = {}\n    for x in range(len(neighbors)):\n        response = neighbors[x][-1]\n        if response in classVotes:\n            classVotes[response] += 1\n        else:\n            classVotes[response] = 1\n    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n    return sortedVotes[0][0]\n\n\n#Accuracy\ndef getAccuracy(testSet, predictions):\n    correct = 0\n    for x in range(len(testSet)):\n        if testSet[x][-1] == predictions[x]:\n            correct += 1\n    return (correct\/float(len(testSet))) * 100.0\n\n#Main Program\n\n\ndef main():\n    #prepare data\n    trainingset = []\n    testset = []\n    split = 0.67\n    \n    handleDataset('\/kaggle\/input\/ml-scratch-master\/ML_Scratch-master\/knn\/iris.data',split,trainingset,testset)\n    \n    predictions = []\n    k=3\n    for x in range(len(testset)):\n        neighbors = getNeighbors(trainingset,testset[x],k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n        #print('prediction = '+repr(result)+' actual = '+repr(testset[x][-1]))\n    \n    accuracy = getAccuracy(testset,predictions)\n    print('Accuracy: '+repr(accuracy)+'%')\n    \nmain()","8aadb46b":"import numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nX, y = make_blobs(centers=3, n_samples=500, n_features=2, shuffle=True, random_state=40)\n\ndef euclidean_distance(x1,x2):\n  return np.sqrt(np.sum((x1 - x2)**2))\n\nclass KMeans:\n  def __init__(self, K=5, max_iters=100, plot_steps=False):\n    self.K = K\n    self.max_iters = max_iters\n    self.plot_steps = plot_steps\n\n    #list of samples endices for each cluster\n    self.clusters = [[] for _ in range(self.K)]\n\n    #mean feature vector for each cluster\n    self.centroids = []\n\n  def predict(self, X):\n    self.X = X\n    self.n_samples, self.n_features = X.shape\n\n    #initialize centroids\n    random_sample_idxs = np.random.choice(self.n_samples, self.K, replace = False)\n    self.centroids = [self.X[idx] for idx in random_sample_idxs]\n\n    #optimization\n    for _ in range(self.max_iters):\n      #update clusters\n      self.clusters = self._create_clusters(self.centroids)\n      if self.plot_steps:\n        self.plot()\n\n      #update centroids\n      centroids_old = self.centroids\n      self.centroids = self._get_centroids(self.clusters)\n\n\n      #check if converged\n      if self._is_converged(centroids_old, self.centroids):\n        break\n\n      if self.plot_steps:\n        self.plot()\n \n    #return cluster labels\n    return self._get_cluster_labels(self.clusters)\n\n  def _get_cluster_labels(self, clusters):\n    labels = np.empty(self.n_samples)\n    for cluster_idx, cluster in enumerate(clusters):\n      for sample_idx in cluster:\n        labels[sample_idx] = cluster_idx\n    return labels\n\n\n  def _create_clusters(self, centroids):\n    clusters = [[] for _ in range(self.K)]\n    for idx, sample in enumerate(self.X):\n      centroid_idx = self._closest_centroid(sample, centroids)\n      clusters[centroid_idx].append(idx)\n    return clusters\n\n  def _closest_centroid(self, sample, centroids):\n    distances = [euclidean_distance(sample,point) for point in centroids]\n    closest_idx = np.argmin(distances)\n    return closest_idx\n  \n  def _get_centroids(self, clusters):\n    centroids = np.zeros((self.K, self.n_features))\n    for cluster_idx, cluster in enumerate(clusters):\n      cluster_mean = np.mean(self.X[cluster], axis=0)\n      centroids[cluster_idx] = cluster_mean\n    return centroids\n  \n  def _is_converged(self, centroids_old, centroids):\n    distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n    return sum(distances) == 0\n  \n  def plot(self):\n    fig, ax = plt.subplots(figsize=(12,8))\n\n    for i, index in enumerate(self.clusters):\n      point = self.X[index].T\n      ax.scatter(*point)\n    for point in self.centroids:\n      ax.scatter(*point,marker='x',color='black',linewidth=2)\n\n    plt.show()  \n\nclusters = len(np.unique(y))\nprint(clusters)\nkm = KMeans(K= clusters, max_iters=150, plot_steps=False)\ny_pred = km.predict(X)\nkm.plot()","87990549":"class PCA:\n  def __init__(self, n_components):\n    self.n_components = n_components\n    self.components = None\n    self.mean = None\n\n  def fit(self, X):\n    #mean\n    self.mean = np.mean(X, axis=0)\n    #1 row = sample, columns = features\n    X = X - self.mean\n\n    #covariance\n    #but in documentation 1row = features, columns=samples for cov ...so we transpose our data\n    cov = np.cov(X.T)\n\n    #eigenvectors, eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eig(cov)\n\n    #v[:, i]\n    #sort eigenvectors\n    eigenvectors = eigenvectors.T\n    idxs = np.argsort(eigenvalues)[::-1]\n    \n    eigenvalues = eigenvalues[idxs]\n    eigenvectors = eigenvectors[idxs]\n\n    #store first n eigenvectors\n    self.components = eigenvectors[0:self.n_components]\n\n  def transform(self, X):\n    #project data\n    X = X - self.mean\n    return(np.dot(X, self.components.T))\n\n#PCA TEST\ndata = datasets.load_iris()\nX = data.data\ny = data.target\n\n#project data onto the 2 primary principal components\npca = PCA(2)\npca.fit(X)\nX_projected = pca.transform(X)\n\n\n\nx1 = X_projected[:,0]\nx2 = X_projected[:,1]\n\nplt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',3))\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar()\nplt.show()","6996639c":"import itertools\nimport pandas as pd\n\n\ndata = pd.read_csv('..\/input\/apriori-food\/data.csv')\n\n\n\nrowlength = len(data)\nno_features = len(data.values[0])\n\nminimum_support_count = 2\n\nrecords = []\nfor i in range(0, rowlength):\n    records.append([str(data.values[i,j]) for j in range(0, no_features)])\nitems = sorted([item for sublist in records for item in sublist if item != 'Nan'])\n\n#Step-->1\ndef stage_1(items, minimum_support_count):\n    c1 = {i:items.count(i) for i in items}\n    l1= {}\n    \n    for key, value in c1.items():\n        if(value >= minimum_support_count):\n            l1[key] = value\n    return c1, l1\nc1, l1 = stage_1(items, minimum_support_count)\n\n\n\ndef sublist(lst1,lst2):\n    return(set(lst1) <= set(lst2))\n\ndef check_subset_frequency(itemset, l, n):\n    if(n>1):\n        subsets = list(itertools.combinations(itemset,n))\n    else:\n        subsets = itemset\n    for iter1 in subsets:\n        if not iter1 in l:\n            return False\n    return True\n\ndef stage_2(l1, records, minimum_support_count):\n    l1 = sorted(list(l1.keys()))\n    l1 = list(itertools.combinations(l1,2))\n    c2 = {}\n    l2 = {}\n    for iter1 in l1:\n        count = 0\n        for iter2 in records:\n            if(sublist(iter1,iter2)):\n                count += 1\n        c2[iter1] = count\n    for key,value in c2.items():\n        if(value >= minimum_support_count):\n            if(check_subset_frequency(key, l1,2)):\n                l2[key] = value\n    return c2, l2\n\nc2,l2 = stage_2(l1, records, minimum_support_count)\nprint(c2)\nprint(l2)","3f3d3d38":"class NeuralNetwork():\n    \n    def __init__(self):\n        np.random.seed(1)\n        self.synaptic_weights = 2*np.random.random((3,1)) - 1\n    \n    def sigmoid(self,x):\n        return(1\/(1+np.exp(-x)))\n    \n    def sigmoid_derivative(self,x):\n        return(x\/(1+x))\n    \n    def train(self,training_inputs, training_outputs,training_iterations):\n        for itr in range(training_iterations):\n            \n            output = self.think(training_inputs)\n            error = training_outputs - output\n            adjustments = np.dot(training_inputs.T,error*self.sigmoid_derivative(output))\n            self.synaptic_weights += adjustments\n    \n    def think(self,inputs):\n        inputs = inputs.astype(float)\n        output = self.sigmoid(np.dot(inputs,self.synaptic_weights))\n        return output\n    \n    \n\n\n#In short, use this ' if name == \"main\" ' block to prevent (certain) code from being run when the module\n#is imported. Put simply, name is a variable defined for each script that defines whether the script is \n#being run as the main module or it is being run as an imported module.\n\n\n\nif(__name__ == \"__main__\"):\n    neural_network = NeuralNetwork()\n    \n    print(\"Rnadom synaptic weights: \")\n    print(neural_network.synaptic_weights)\n    \n    training_inputs = np.array([[0,0,1],\n                                [1,1,1],\n                                [1,0,1],\n                                [0,1,1]])\n    training_outputs = np.array([[0,1,1,0]]).T\n    \n    neural_network.train(training_inputs,training_outputs,10000)\n    \n    print(\"Synaptic weights after training: \")\n    print(neural_network.synaptic_weights)\n    \n    #A = str(input('Input 1: '))\n    #B = str(input('Input 2: '))\n    #C = str(input('Input 3: '))\n    A = 0\n    B = 1\n    C = 1\n    print(\"New situation: input data = \", A, B, C)\n    print(\"Output data: \")\n    print(neural_network.think(np.array([A,B,C])))","c00a4a0c":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/conv.png'))","bf2fbeb8":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/conv2.png'))","c47a16e8":"class Conv:\n    \n    def __init__(self, num_filters):\n        self.num_filters = num_filters\n        \n        #why divide by 9...Xavier initialization\n        self.filters = np.random.randn(num_filters, 3, 3)\/9\n    \n    def iterate_regions(self, image):\n        #generates all possible 3*3 image regions using valid padding\n        \n        h,w = image.shape\n        \n        for i in range(h-2):\n            for j in range(w-2):\n                im_region = image[i:(i+3), j:(j+3)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        self.last_input = input\n        \n        h,w = input.shape\n        \n        output = np.zeros((h-2, w-2, self.num_filters))\n        \n        for im_regions, i, j in self.iterate_regions(input):\n            output[i, j] = np.sum(im_regions * self.filters, axis=(1,2))\n        return output\n    \n    def backprop(self, d_l_d_out, learn_rate):\n        '''\n        Performs a backward pass of the conv layer.\n        - d_L_d_out is the loss gradient for this layer's outputs.\n        - learn_rate is a float.\n        '''\n        d_l_d_filters = np.zeros(self.filters.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            for f in range(self.num_filters):\n                d_l_d_filters[f] += d_l_d_out[i,j,f] * im_region\n\n        #update filters\n        self.filters -= learn_rate * d_l_d_filters\n\n        return None","5e4c3ec9":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/maxpool.png'))","01a26b14":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/maxpool2.png'))","667a051e":"class MaxPool:\n    def iterate_regions(self, image):\n        h, w, _ = image.shape\n        \n        new_h = h \/\/ 2\n        new_w = w \/\/ 2\n        \n        for i in range(new_h):\n            for j in range(new_w):\n                im_region = image[(i*2):(i*2+2), (j*2):(j*2+2)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        \n        self.last_input = input\n        \n        h, w, num_filters = input.shape\n        output = np.zeros((h\/\/2, w\/\/2, num_filters))\n        \n        for im_region, i, j in self.iterate_regions(input):\n            output[i,j] = np.amax(im_region,axis=(0,1))\n            \n        return output\n    \n    def backprop(self, d_l_d_out):\n        '''\n        Performs a backward pass of the maxpool layer.\n        Returns the loss gradient for this layer's inputs.\n        - d_L_d_out is the loss gradient for this layer's outputs.\n        '''\n        d_l_d_input = np.zeros(self.last_input.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0,1))\n\n            for i2 in range(h):\n                for j2 in range(w):\n                    for f2 in range(f):\n                        #if the pixel was the max value, copy the gradient to it\n                        if(im_region[i2,j2,f2] == amax[f2]):\n                            d_l_d_input[i*2+i2, j*2+j2 ,f2] = d_l_d_out[i, j, f2]\n                            break;\n        return d_l_d_input","2af6e4c8":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/softmax.png'))","1b4b9ee0":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/sogtmax2.png'))","7b7387c2":"class Softmax:\n    def __init__(self, input_len, nodes):\n        # We divide by input_len to reduce the variance of our initial values\n        self.weights = np.random.randn(input_len, nodes)\/input_len\n        self.biases = np.zeros(nodes)\n    \n    def forward(self, input):\n        \n        self.last_input_shape = input.shape\n        \n        input = input.flatten()\n        self.last_input = input\n        \n        input_len, nodes = self.weights.shape\n        \n        totals = np.dot(input, self.weights) + self.biases\n        self.last_totals = totals\n        \n        exp = np.exp(totals)\n        return(exp\/np.sum(exp, axis=0)) \n    \n    def backprop(self, d_l_d_out, learn_rate):\n        \"\"\"  \n        Performs a backward pass of the softmax layer.\n        Returns the loss gradient for this layers inputs.\n        - d_L_d_out is the loss gradient for this layers outputs.\n        \"\"\"\n        \n        #We know only 1 element of d_l_d_out will be nonzero\n        for i, gradient in enumerate(d_l_d_out):\n            if(gradient == 0):\n                continue\n            \n            #e^totals\n            t_exp = np.exp(self.last_totals)\n            \n            #Sum of all e^totals\n            S = np.sum(t_exp)\n            \n            #gradients of out[i] against totals\n            d_out_d_t = -t_exp[i] * t_exp\/ (S**2)\n            d_out_d_t[i] = t_exp[i] * (S-t_exp[i]) \/(S**2)\n            \n            # Gradients of totals against weights\/biases\/input\n            d_t_d_w = self.last_input\n            d_t_d_b = 1\n            d_t_d_inputs = self.weights\n            \n            #Gradients of loss against totals\n            d_l_d_t = gradient * d_out_d_t\n            \n            #Gradients of loss against weights\/biases\/input\n            d_l_d_w = d_t_d_w[np.newaxis].T @ d_l_d_t[np.newaxis]\n            d_l_d_b = d_l_d_t * d_t_d_b  \n            d_l_d_inputs = d_t_d_inputs @ d_l_d_t\n            \n            #update weights\/biases\n            self.weights -= learn_rate * d_l_d_w\n            self.biases -= learn_rate * d_l_d_b\n            return d_l_d_inputs.reshape(self.last_input_shape)","1d8eaeb1":"import mnist\n\ntrain_images = mnist.train_images()[:1000]\ntrain_labels = mnist.train_labels()[:1000]\ntest_images = mnist.test_images()[:1000]\ntest_labels = mnist.test_labels()[:1000]","102cd6e2":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/train.png'))","d8a5df02":"conv = Conv(8)\npool = MaxPool()\nsoftmax = Softmax(13 * 13 * 8, 10)\n\ndef forward(image, label):\n    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n    # to work with. This is standard practice.\n    \n    out = conv.forward((image\/255) - 0.5)\n    out = pool.forward(out)\n    out = softmax.forward(out)\n    \n    #calculate cross-entropy loss and accuracy\n    loss = -np.log(out[label])\n    acc = 1 if(np.argmax(out) == label) else 0\n    \n    return out, loss, acc\n\n\ndef train(im, label, lr=0.005):\n    #forward\n    out,loss,acc = forward(im, label)\n    \n    #calculate initial gradient\n    gradient = np.zeros(10)\n    gradient[label] = -1\/out[label]\n    \n    \n    #Backprop\n    gradient = softmax.backprop(gradient, lr)\n    gradient = pool.backprop(gradient)\n    gradient = conv.backprop(gradient, lr)\n    \n    return loss, acc\n    \n    \nprint('MNIST CNN initialized')\n\nfor epoch in range(3):\n    print('----EPOCH %d ---'%(epoch+1))\n    \n    #shuffle the training data\n    permutation = np.random.permutation(len(train_images))\n    train_images = train_images[permutation]\n    train_labels = train_labels[permutation]\n\n\n    loss = 0\n    num_correct = 0\n\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n\n        #print stats every 100 steps\n        if(i>0 and i %100 == 99):\n            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss \/ 100, num_correct))\n\n            loss = 0\n            num_correct = 0\n        l, acc = train(im, label)\n        loss += l\n        num_correct += acc","8fc982d8":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/1.png'))","e1908543":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/2.png'))","ae08426b":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/3.png'))","1386f1ee":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/4.png'))","f44a6e0d":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/5.png'))","4a052908":"train_data = {\n  'good': True,\n  'bad': False,\n  'happy': True,\n  'sad': False,\n  'not good': False,\n  'not bad': True,\n  'not happy': False,\n  'not sad': True,\n  'very good': True,\n  'very bad': False,\n  'very happy': True,\n  'very sad': False,\n  'i am happy': True,\n  'this is good': True,\n  'i am bad': False,\n  'this is bad': False,\n  'i am sad': False,\n  'this is sad': False,\n  'i am not happy': False,\n  'this is not good': False,\n  'i am not bad': True,\n  'this is not sad': True,\n  'i am very happy': True,\n  'this is very good': True,\n  'i am very bad': False,\n  'this is very sad': False,\n  'this is very happy': True,\n  'i am good not bad': True,\n  'this is good not bad': True,\n  'i am bad not good': False,\n  'i am good and happy': True,\n  'this is not good and not happy': False,\n  'i am not at all good': False,\n  'i am not at all bad': True,\n  'i am not at all happy': False,\n  'this is not at all sad': True,\n  'this is not at all happy': False,\n  'i am good right now': True,\n  'i am bad right now': False,\n  'this is bad right now': False,\n  'i am sad right now': False,\n  'i was good earlier': True,\n  'i was happy earlier': True,\n  'i was bad earlier': False,\n  'i was sad earlier': False,\n  'i am very bad right now': False,\n  'this is very good right now': True,\n  'this is very sad right now': False,\n  'this was bad earlier': False,\n  'this was very good earlier': True,\n  'this was very bad earlier': False,\n  'this was very happy earlier': True,\n  'this was very sad earlier': False,\n  'i was good and not bad earlier': True,\n  'i was not good and not happy earlier': False,\n  'i am not at all bad or sad right now': True,\n  'i am not at all good or happy right now': False,\n  'this was not happy and not good earlier': False,\n}\n\ntest_data = {\n  'this is happy': True,\n  'i am good': True,\n  'this is not happy': False,\n  'i am not good': False,\n  'this is not bad': True,\n  'i am not sad': True,\n  'i am very good': True,\n  'this is very bad': False,\n  'i am very sad': False,\n  'this is bad not good': False,\n  'this is good and happy': True,\n  'i am not good and not happy': False,\n  'i am not at all sad': True,\n  'this is not at all good': False,\n  'this is not at all bad': True,\n  'this is good right now': True,\n  'this is sad right now': False,\n  'this is very bad right now': False,\n  'this was good earlier': True,\n  'i was not happy and not good earlier': False,\n}","0e4c98ff":"class RNN:\n  # A many-to-one Vanilla Recurrent Neural Network.\n\n  def __init__(self, input_size, output_size, hidden_size=64):\n    # Weights\n    self.Whh = randn(hidden_size, hidden_size) \/ 1000\n    self.Wxh = randn(hidden_size, input_size) \/ 1000\n    self.Why = randn(output_size, hidden_size) \/ 1000\n\n    # Biases\n    self.bh = np.zeros((hidden_size, 1))\n    self.by = np.zeros((output_size, 1))\n\n  def forward(self, inputs):\n    '''\n    Perform a forward pass of the RNN using the given inputs.\n    Returns the final output and hidden state.\n    - inputs is an array of one hot vectors with shape (input_size, 1).\n    '''\n    h = np.zeros((self.Whh.shape[0], 1))\n\n    self.last_inputs = inputs\n    self.last_hs = { 0: h }\n\n    # Perform each step of the RNN\n    for i, x in enumerate(inputs):\n      h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n      self.last_hs[i + 1] = h\n\n    # Compute the output\n    y = self.Why @ h + self.by\n\n    return y, h\n\n  def backprop(self, d_y, learn_rate=2e-2):\n    '''\n    Perform a backward pass of the RNN.\n    - d_y (dL\/dy) has shape (output_size, 1).\n    - learn_rate is a float.\n    '''\n    n = len(self.last_inputs)\n\n    # Calculate dL\/dWhy and dL\/dby.\n    d_Why = d_y @ self.last_hs[n].T\n    d_by = d_y\n\n    # Initialize dL\/dWhh, dL\/dWxh, and dL\/dbh to zero.\n    d_Whh = np.zeros(self.Whh.shape)\n    d_Wxh = np.zeros(self.Wxh.shape)\n    d_bh = np.zeros(self.bh.shape)\n\n    # Calculate dL\/dh for the last h.\n    # dL\/dh = dL\/dy * dy\/dh\n    d_h = self.Why.T @ d_y\n\n    # Backpropagate through time.\n    for t in reversed(range(n)):\n      # An intermediate value: dL\/dh * (1 - h^2)\n      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n\n      # dL\/db = dL\/dh * (1 - h^2)\n      d_bh += temp\n\n      # dL\/dWhh = dL\/dh * (1 - h^2) * h_{t-1}\n      d_Whh += temp @ self.last_hs[t].T\n\n      # dL\/dWxh = dL\/dh * (1 - h^2) * x\n      d_Wxh += temp @ self.last_inputs[t].T\n\n      # Next dL\/dh = dL\/dh * (1 - h^2) * Whh\n      d_h = self.Whh @ temp\n\n    # Clip to prevent exploding gradients.\n    for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n      np.clip(d, -1, 1, out=d)\n\n    # Update weights and biases using gradient descent.\n    self.Whh -= learn_rate * d_Whh\n    self.Wxh -= learn_rate * d_Wxh\n    self.Why -= learn_rate * d_Why\n    self.bh -= learn_rate * d_bh\n    self.by -= learn_rate * d_by","8b52a819":"# Create the vocabulary.\nvocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\nvocab_size = len(vocab)\nprint('%d unique words found' % vocab_size)\n\n# Assign indices to each word.\nword_to_idx = { w: i for i, w in enumerate(vocab) }\nidx_to_word = { i: w for i, w in enumerate(vocab) }","5abaee5c":"\ndef createInputs(text):\n  '''\n  Returns an array of one-hot vectors representing the words in the input text string.\n  - text is a string\n  - Each one-hot vector has shape (vocab_size, 1)\n  '''\n  inputs = []\n  for w in text.split(' '):\n    v = np.zeros((vocab_size, 1))\n    v[word_to_idx[w]] = 1\n    inputs.append(v)\n  return inputs\n\ndef softmax(xs):\n  # Applies the Softmax Function to the input array.\n  return np.exp(xs) \/ sum(np.exp(xs))\n\n# Initialize our RNN!\nrnn = RNN(vocab_size, 2)\n\ndef processData(data, backprop=True):\n  '''\n  Returns the RNN's loss and accuracy for the given data.\n  - data is a dictionary mapping text to True or False.\n  - backprop determines if the backward phase should be run.\n  '''\n  items = list(data.items())\n  random.shuffle(items)\n\n  loss = 0\n  num_correct = 0\n\n  for x, y in items:\n    inputs = createInputs(x)\n    target = int(y)\n\n    # Forward\n    out, _ = rnn.forward(inputs)\n    probs = softmax(out)\n\n    # Calculate loss \/ accuracy\n    loss -= np.log(probs[target])\n    num_correct += int(np.argmax(probs) == target)\n\n    if backprop:\n      # Build dL\/dy\n      d_L_d_y = probs\n      d_L_d_y[target] -= 1\n\n      # Backward\n      rnn.backprop(d_L_d_y)\n\n  return loss \/ len(data), num_correct \/ len(data)\n\n# Training loop\nfor epoch in range(1000):\n  train_loss, train_acc = processData(train_data)\n\n  if epoch % 100 == 99:\n    print('--- Epoch %d' % (epoch + 1))\n    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n\n    test_loss, test_acc = processData(test_data, backprop=False)\n    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))","906954d2":"from collections import defaultdict\n\nclass Graph:\n    \n    #Constructor\n    def __init__(self):\n        \n        #deafault dictionary to store graph\n        self.graph = defaultdict(list)\n        \n    #function to add an edge to graph\n    def addEdge(self,u,v):\n        self.graph[u].append(v)\n            \n    #Function to print a BFS of graph\n    def BFS(self,s):\n        #Mark all the vertices as not visited\n        visited = [False]*(len(self.graph))\n            \n        #create a queue for BFS\n        queue = []\n            \n        #Mark the source node as visited and enqueue it\n        queue.append(s)\n        visited[s] = True\n            \n        while queue:\n            #Dequeue a vertex from queue and print it\n            s = queue.pop(0)\n            print(s,end=\" \")\n                \n            #Get all adjacent vertices of the dequeued vertex s\n            #If a adjacent has not been visited, then mark it visited and enqueue it\n            for i in self.graph[s]:\n                if visited[i]==False:\n                    queue.append(i)\n                    visited[i] = True\n\n#Create a graph given in the above diagram\ng = Graph()\ng.addEdge(0,1)\ng.addEdge(0,2)\n\ng.addEdge(1,2)\ng.addEdge(1,4)\n\ng.addEdge(2,3)\ng.addEdge(3,5)\n\ng.addEdge(4,6)\n\ng.addEdge(5,5)\ng.addEdge(6,6)\n\n\n\nprint(\"breadth first search starting from vertex 's'\")\ng.BFS(0)","1f4457d7":"from collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        #default dictionary\n        self.graph = defaultdict(list)\n    #function to add an edge to graph\n    def addEdge(self,u,v):\n        self.graph[u].append(v)\n        \n    #function used by DFS\n    def DFSUtil(self,v,visited):\n        # Mark the current node as visited and print it\n        visited[v] = True\n        print(v,end=' ')\n        \n        # Recur for all the vertices\n        # adjacent to this vertex\n        for i in self.graph[v]:\n            if visited[i] == False:\n                self.DFSUtil(i,visited)\n                \n    # The function to do DFS traversal. It uses recursive DFSUtil()\n    def DFS(self,v):\n        # Mark all the vertices as not visited\n        visited = [False] * (len(self.graph))\n        \n        #call the recursive helper function to print DFS traversal\n        self.DFSUtil(v,visited)\n        \n\n\n# the above diagram \ng = Graph()\ng.addEdge(0,1)\ng.addEdge(0,2)\n\ng.addEdge(1,2)\ng.addEdge(1,4)\n\ng.addEdge(2,3)\ng.addEdge(3,5)\n\ng.addEdge(4,6)\n\ng.addEdge(5,5)\ng.addEdge(6,6)\n  \nprint (\"Following is Depth First Traversal\"\n                  \" (starting from vertex 2)\") \ng.DFS(0)      \n","79c94ce0":"**Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression**","81eb943c":"**Introduction to APRIORI**\n\nApriori is an algorithm used for Association Rule Mining. It searches for a series of frequent sets of items in the datasets. It builds on associations and correlations between the itemsets. It is the algorithm behind \u201cYou may also like\u201d where you commonly saw in recommendation platforms. What is Associate Rule Mining?\n\nARM( Associate Rule Mining) is one of the important techniques in data science. In ARM, the frequency of patterns and associations in the dataset is identified among the item sets then used to predict the next relevant item in the set. This ARM technique is mostly used in business decisions according to customer purchases.\n\nExample: In Walmart, if Ashok buys Milk and Bread, the chances of him buying Butter are predicted by the Associate Rule Mining technique.\n\nSome definitions need to be remembered\n\nBefore we start, go through some terms which are explained below.\n\nSUPPORT_COUNT \u2014 number of transactions in which the itemset appears.\n\nMINIMUM_SUPPORT_COUNT \u2014 the minimum frequency of itemset in the dataset.\n\nCANDIDATE_SET \u2014 C(k) support_count of each item in the dataset.\n\nITEM_SET \u2014 L(k) comparing each item in the candidate_set support count to minimum_support_count and filtering the under frequent itemset.\n\nSUPPORT \u2014 the percentage of transactions in the database follow the rule.\n\nSupport(A->B) = Support_count(A U B)\n\nCONFIDENCE \u2014 the percentage of customers who bought A also bought B.\n\nConfidence(A->B) = [Support_count(AUB)\/Support_count(A)]*100\n\nSteps In Apriori\n\nApriori algorithm is a sequence of steps to be followed to find the most frequent itemset in the given database. This data mining technique follows the join and the prune steps iteratively until the most frequent itemset is achieved. A minimum support threshold is given in the problem or it is assumed by the user.\n1. In the first iteration of the algorithm, each item is taken as a 1-itemsets candidate. The algorithm will count the occurrences of each item.\n\n2. Let there be some minimum support, min_sup ( eg 2). The set of 1 \u2013 itemsets whose occurrence is satisfying the min sup are determined. Only those candidates which count more than or equal to min_sup, are taken ahead for the next iteration and the others are pruned.\n\n3. Next, 2-itemset frequent items with min_sup are discovered. For this in the join step, the 2-itemset is generated by forming a group of 2 by combining items with itself.\n\n4. The 2-itemset candidates are pruned using min-sup threshold value. Now the table will have 2 \u2013itemsets with min-sup only.\n\n5. The next iteration will form 3 \u2013itemsets using join and prune step. This iteration will follow antimonotone property where the subsets of 3-itemsets, that is the 2 \u2013itemset subsets of each group fall in min_sup. If all 2-itemset subsets are frequent then the superset will be frequent otherwise it is pruned.\n\n6. Next step will follow making 4-itemset by joining 3-itemset with itself and pruning if its subset does not meet the min_sup criteria. The algorithm is stopped when the most frequent itemset is achieved.","5880f17a":"# Searching Algorithm\n\n# 1. BFS","422815f2":"**Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. **","b142b287":"**In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall. **","9cbe5cff":"# 5. Naive Bayes","9a81ff6e":"# 3. Recurrent neural network\n\nA **recurrent neural network (RNN)** is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n\n<img src=\"https:\/\/victorzhou.com\/static\/2a37bd4e9b12bcc19e045eaf22fea4e5\/38283\/rnns.webp\"\/>","5863064b":"# 2.Convolutional Neural Network\n\nA Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters\/characteristics.","ea799943":"# 3. Apriori Algorithm","f2f5631b":"# 7. KNN","10d986f8":"# 6. SVM(Linear)","0e6ae14f":"**PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.**","c669d3fa":"# 2. PCA","a5970e98":"# 2. DFS","12e75c5c":"**In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.**","4dabf89d":"# To learn more:\n\n1. CNN from Scratch: https:\/\/www.kaggle.com\/milan400\/cnn-from-scratch-numpy\n2. Neural Network from Scratch : https:\/\/www.kaggle.com\/milan400\/neural-network-from-scratch-numpy-pandas\n3. Converting AutoEncoder to UNET(FER2013) : https:\/\/www.kaggle.com\/milan400\/converting-autoencoder-to-unet-fer2013\n4. Human Emotion Detection by using CNN: https:\/\/www.kaggle.com\/milan400\/human-emotion-detection-by-using-cnn\n5. Plant Pest detection by using SVM: https:\/\/www.kaggle.com\/milan400\/plant-pest-detection-by-using-svm\n6. Plant Pest detection by using CNN: https:\/\/www.kaggle.com\/milan400\/plantpestmodel\n7. FER2013 Denoising using AutoEncoder and UNET: https:\/\/www.kaggle.com\/milan400\/fer2013-denoising-using-autoencoder-and-unet\n8. Employee Salary Analysis and Prediction: https:\/\/www.kaggle.com\/milan400\/employee-salary-analysis-and-prediction","55ef97e4":"# Training","72e960e9":"**BFS is a traversing algorithm where you should start traversing from a selected node (source or starting node) and traverse the graph layerwise thus exploring the neighbour nodes (nodes which are directly connected to source node).**","abb30bff":"**Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.**","2bf2417d":"# Convolution Layer","0e8ce5ee":"# To learn about more complex regression:\n\n1. Regression Neural Network: https:\/\/www.kaggle.com\/milan400\/regression-neural-network\n\n![image.png](attachment:image.png)","4ac1c669":"# Supervised\n\n**Supervised learning is when the model is getting trained on a labelled dataset. Labelled dataset is one which have both input and output parameters. In this type of learning both training and validation datasets are labelled.**\n\n\n# 1. Linear Regression","903f5478":"# 2. Logistic Regression","4dcf722a":"# Visualization of Neural Network using tensorspace.js\n\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*_iuD-XPoKrBKG2TyftR8zA.gif\" width=\"1024\" height=\"1024\">","66ccb8c3":"# Algorithm Included:\n\n**Machine Learning Algorithm:**\n\n* Supervised:\n    1. Linear Regression\n    2. Logistic Regression\n    3. Decision Tree(CART)\n    4. Random Forest\n    4. Naive Bayes\n    5. SVM(Linear)\n    6. KNN\n\n\n* UnSupervrised\n   1. K-Means\n   2. Principal Component Analysis\n   3. Apriori Algorithm\n\n\n* Deep Learning\n  1. Neural Network \n  2. CNN(Convolutional Neural Network)\n  3. RNN(Recurrent Neural Network)\n\n\n* Searching Algorithm\n  1. BFS\n  2. DFS","c2ccfd20":"**In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression: **","13ba6d90":"# Softmax Layer","6b3eb118":"**A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.**","2e7c3fc5":"# 3. Decision Tree(CART)","7ff37215":"# Deep Learning\n\n**Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.**\n\n\n# 1. Neural Network","10eb7a59":"**A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).**","b1bb1ded":"# MaxPooling Layer\n\n\n\nA Max Pooling layer can\u2019t be trained because it doesn\u2019t actually have any weights, but we still need to implement a backprop() method for it to calculate gradients. We\u2019ll start by adding forward phase caching again. All we need to cache this time is the input:\n\nDuring the forward pass, the Max Pooling layer takes an input volume and halves its width and height dimensions by picking the max values over 2x2 blocks. The backward pass does the opposite: we\u2019ll double the width and height of the loss gradient by assigning each gradient value to where the original max value was in its corresponding 2x2 block.","b50b0cb6":"**k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition and observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids. **","7538ddff":"# 4. Random Forest","06d00948":"# Unsupervised\n\n**Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning.**\n\n# 1. K Means","35e41c02":"**In machine learning, na\u00efve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. They are among the simplest Bayesian network models.But they could be coupled with Kernel density estimation and achieve higher accuracy levels.**","c9798410":"# Backpropagation indepth"}}