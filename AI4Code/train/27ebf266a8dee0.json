{"cell_type":{"b7aca871":"code","0d8ca3c6":"code","1bc0a2c4":"code","e6e25e72":"code","e0a9c776":"code","d44ff140":"code","7a62a2a3":"code","45ff36d9":"code","c74c4839":"code","92ea80ed":"code","3a35c5af":"code","168ff0ee":"code","5eeb5799":"markdown","86344824":"markdown","84392071":"markdown","40112955":"markdown","7d972e5a":"markdown","d547726e":"markdown"},"source":{"b7aca871":"import numpy as np\nimport pylab as plt","0d8ca3c6":"# map cell to cell, add circular cell to goal point\npoints_list = [(0,1), (1,5), (5,6), (5,4), (1,2), (2,3), (2,7)]","1bc0a2c4":"goal = 7\n\nimport networkx as nx\nG=nx.Graph()\nG.add_edges_from(points_list)\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G,pos)\nnx.draw_networkx_edges(G,pos)\nnx.draw_networkx_labels(G,pos)\nplt.show()","e6e25e72":"# how many points in graph? x points\nMATRIX_SIZE = 8\n\n# create matrix x*y (reward matrix)\nR = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\nR *= -1\nR","e0a9c776":"# assign zeros to paths and 100 to goal-reaching point\nfor point in points_list:\n    print(point)\n    if point[1] == goal:\n        R[point] = 100\n    else:\n        R[point] = 0\n\n    if point[0] == goal:\n        R[point[::-1]] = 100\n    else:\n        # reverse of point\n        R[point[::-1]]= 0\n\n# add goal point round trip\nR[goal,goal]= 100\n\nR","d44ff140":"Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n\n# learning parameter\ngamma = 0.8\n\ninitial_state = 1\n\ndef available_actions(state):\n    current_state_row = R[state,]\n    av_act = np.where(current_state_row >= 0)[1]\n    return av_act\n\navailable_act = available_actions(initial_state)\n\ndef sample_next_action(available_actions_range):\n    next_action = int(np.random.choice(available_act,1))\n    return next_action\n\naction = sample_next_action(available_act)\n\ndef update(current_state, action, gamma):\n\n  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n\n  if max_index.shape[0] > 1:\n      max_index = int(np.random.choice(max_index, size = 1))\n  else:\n      max_index = int(max_index)\n  max_value = Q[action, max_index]\n\n  Q[current_state, action] = R[current_state, action] + gamma * max_value\n  print('max_value', R[current_state, action] + gamma * max_value)\n\n  if (np.max(Q) > 0):\n    return(np.sum(Q\/np.max(Q)*100))\n  else:\n    return (0)\n\nupdate(initial_state, action, gamma)\n\nQ","7a62a2a3":"# Training\nscores = []\nfor i in range(150):\n    current_state = np.random.randint(0, int(Q.shape[0]))\n    available_act = available_actions(current_state)\n    action = sample_next_action(available_act)\n    score = update(current_state,action,gamma)\n    scores.append(score)\n    print ('Score:', str(score))\n\nprint(\"Trained Q matrix:\")\nprint(Q\/np.max(Q)*100)\n","45ff36d9":"# Testing\ncurrent_state = 0\nsteps = [current_state]\n\nwhile current_state != 7:\n\n    next_step_index = np.where(Q[current_state,]\n        == np.max(Q[current_state,]))[1]\n\n    if next_step_index.shape[0] > 1:\n        next_step_index = int(np.random.choice(next_step_index, size = 1))\n    else:\n        next_step_index = int(next_step_index)\n\n    steps.append(next_step_index)\n    current_state = next_step_index\n\nprint(\"Most efficient path:\")\nprint(steps)\n\nplt.plot(scores)\nplt.show()","c74c4839":"bees = [2]\nsmoke = [4,5,6]\n\nG=nx.Graph()\nG.add_edges_from(points_list)\nmapping={0:'Start', 1:'1', 2:'2 - Bees', 3:'3',\n    4:'4 - Smoke', 5:'5 - Smoke', 6:'6 - Smoke', 7:'7 - Beehive'}\nH=nx.relabel_nodes(G,mapping)\npos = nx.spring_layout(H)\nnx.draw_networkx_nodes(H,pos,\n    node_size=[200,200,200,200,200,200,200,200])\nnx.draw_networkx_edges(H,pos)\nnx.draw_networkx_labels(H,pos)\nplt.show()","92ea80ed":"# re-initialize the matrices for new run\nQ = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n\nenviro_bees = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\nenviro_smoke = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n\ninitial_state = 1\n\ndef available_actions(state):\n    current_state_row = R[state,]\n    av_act = np.where(current_state_row >= 0)[1]\n    return av_act\n\ndef sample_next_action(available_actions_range):\n    next_action = int(np.random.choice(available_act,1))\n    return next_action\n\ndef collect_environmental_data(action):\n    found = []\n    if action in bees:\n        found.append('b')\n\n    if action in smoke:\n        found.append('s')\n    return (found)\n\navailable_act = available_actions(initial_state)\n\naction = sample_next_action(available_act)\n\ndef update(current_state, action, gamma):\n  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n\n  if max_index.shape[0] > 1:\n      max_index = int(np.random.choice(max_index, size = 1))\n  else:\n      max_index = int(max_index)\n  max_value = Q[action, max_index]\n\n  Q[current_state, action] = R[current_state, action] + gamma * max_value\n  print('max_value', R[current_state, action] + gamma * max_value)\n\n  environment = collect_environmental_data(action)\n  if 'b' in environment:\n    enviro_bees[current_state, action] += 1\n\n  if 's' in environment:\n    enviro_smoke[current_state, action] += 1\n\n  if (np.max(Q) > 0):\n    return(np.sum(Q\/np.max(Q)*100))\n  else:\n    return (0)\n\nupdate(initial_state,action,gamma)\n\nscores = []\nfor i in range(700):\n    current_state = np.random.randint(0, int(Q.shape[0]))\n    available_act = available_actions(current_state)\n    action = sample_next_action(available_act)\n    score = update(current_state,action,gamma)\n\n# print environmental matrices\nprint('Bees Found')\nprint(enviro_bees)\nprint('Smoke Found')\nprint(enviro_smoke)","3a35c5af":"Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n\n# subtract bees with smoke, this gives smoke a negative effect\nenviro_matrix = enviro_bees - enviro_smoke\n\n# Get available actions in the current state\navailable_act = available_actions(initial_state)\n\n# Sample next action to be performed\naction = sample_next_action(available_act)\n\n# This function updates the Q matrix according to\n# the path selected and the Q learning algorithm\ndef update(current_state, action, gamma):\n\n    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n\n    if max_index.shape[0] > 1:\n        max_index = int(np.random.choice(max_index, size = 1))\n    else:\n        max_index = int(max_index)\n    max_value = Q[action, max_index]\n\n    Q[current_state, action] = R[current_state, action] + gamma * max_value\n    print('max_value', R[current_state, action] + gamma * max_value)\n\n    environment = collect_environmental_data(action)\n    if 'b' in environment:\n        enviro_matrix[current_state, action] += 1\n    if 's' in environment:\n        enviro_matrix[current_state, action] -= 1\n\n    return(np.sum(Q\/np.max(Q)*100))\n\nupdate(initial_state,action,gamma)\n\nenviro_matrix_snap = enviro_matrix.copy()\n\ndef available_actions_with_enviro_help(state):\n    current_state_row = R[state,]\n    av_act = np.where(current_state_row >= 0)[1]\n    # if there are multiple routes, dis-favor anything negative\n    env_pos_row = enviro_matrix_snap[state,av_act]\n    if (np.sum(env_pos_row < 0)):\n        # can we remove the negative directions from av_act?\n        temp_av_act = av_act[np.array(env_pos_row)[0]>=0]\n        if len(temp_av_act) > 0:\n            print('going from:',av_act)\n            print('to:',temp_av_act)\n            av_act = temp_av_act\n    return av_act\n\n# Training\nscores = []\nfor i in range(300):\n    current_state = np.random.randint(0, int(Q.shape[0]))\n    available_act = available_actions_with_enviro_help(current_state)\n    action = sample_next_action(available_act)\n    score = update(current_state,action,gamma)\n    scores.append(score)\n    print ('Score:', str(score))","168ff0ee":"plt.plot(scores)\nplt.show()","5eeb5799":"Now let\u2019s take this a step further, look at the top image again, notice how the factory is surrounded by smoke and the hive, by bees. Let\u2019s assume that bees don\u2019t like smoke or factories, thus there will never be a hive or bees around smoke. What if our bot could record those environmental factors and turn them into actionable insight? Whenever the bot finds smoke it can turn around immediately instead of continuing to the factory, whenever it finds bees, it can stick around and assume the hive it close.","86344824":"Idea and some code from: http:\/\/firsttimeprogrammer.blogspot.com\/2016\/09\/getting-ai-smarter-with-q-learning.html","84392071":"# ***Reinforcement Learning Simple Example***","40112955":"# First Example","7d972e5a":"What we want is that using reinforcement learning try to find the best path from 0 to 7.","d547726e":"# Second Example"}}