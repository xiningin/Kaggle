{"cell_type":{"af3e90f1":"code","c024daf7":"code","3c6da499":"code","e1d49e27":"code","a2d488b7":"markdown"},"source":{"af3e90f1":"\nimport os\nfrom pathlib import Path\nin_folder_path = Path('..\/input\/k\/leolu1998\/clrp-finetune-roberta-large')\nscripts_dir = Path(in_folder_path \/ 'scripts')","c024daf7":"\nos.chdir(scripts_dir)\nexec(Path(\"imports.py\").read_text())\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\nos.chdir('\/kaggle\/working')\n","3c6da499":"\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\ntokenizer = torch.load('..\/input\/mytokenizers\/roberta-tokenizer.pt')\nmodels_folder_path = Path(in_folder_path \/ 'models')\nmodels_preds = []\nn_models = 10\n\nfor model_num in range(n_models):\n    print(f'Inference#{model_num+1}\/{n_models}')\n    test_ds = CLRPDataset(data=test_df, tokenizer=tokenizer, max_len=Config.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=Config.batch_size)\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to(Config.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    models_preds.append(all_preds)","e1d49e27":"models_preds = np.array(models_preds)\nprint(models_preds.shape)\nprint(models_preds)\nall_preds = models_preds.mean(axis=0)\nprint(all_preds.shape)\nresult_df = pd.DataFrame(\n    {\n        'id': test_df.id,\n        'target': all_preds\n    })\n\n\nresult_df.to_csv('submission.csv', index=False)\nresult_df.head(10)","a2d488b7":"# **Notebooks sequence;)**\n* Train-val split notebook [here](https:\/\/www.kaggle.com\/chamecall\/train-val-split).<br>\n* Pretrain roberta-base on mlm with the competition data notebook [here](https:\/\/www.kaggle.com\/chamecall\/clrp-pretrain).<br>\n* Finetune pretrained roberta-base on readability task notebook [here](https:\/\/www.kaggle.com\/chamecall\/clrp-finetune).<br>\n* Inference model notebook [*CURRENT ONE*].<br>"}}