{"cell_type":{"457c6aab":"code","1c5fb5a5":"code","2a1080a9":"code","5fa547c6":"code","3596d819":"code","ac733f98":"code","8dabecd8":"code","30f0bb27":"code","bbe9f6bc":"code","4554383a":"code","bc4a01bf":"code","1ea57d6f":"code","898d3706":"code","7a05f259":"code","f36355fd":"code","7161009a":"code","6435d7f2":"markdown","84c433ca":"markdown","1317a014":"markdown","761b4170":"markdown","3ccd936c":"markdown","2d9b0ec6":"markdown","f9450000":"markdown","6bf6e2ac":"markdown","32379d06":"markdown","a7d3023b":"markdown","7f49beeb":"markdown","9cdb82fc":"markdown","c5380c05":"markdown","2bd300f4":"markdown","bc0e2509":"markdown"},"source":{"457c6aab":"!pip install opencv-contrib-python","1c5fb5a5":"import os\nfrom os.path import join\nimport shutil\nimport threading\nimport time \nimport random\nimport glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nprint('Tensorflow version: %s' % tf.__version__)","2a1080a9":"DATA_DIR = '..\/input\/udacitytraffic\/object-detection-crowdai'\nANNO_PATH = '..\/input\/udacity\/annotations\/labels_crowdai.csv'","5fa547c6":"ANNO_DF = pd.read_csv(ANNO_PATH)\nIMAGE_IDS = pd.unique(ANNO_DF['Frame']).tolist()\nIMAGE_PATHS = [join(DATA_DIR, img_id) for img_id in IMAGE_IDS]\nNUM_TOTAL_IMAGES = len(IMAGE_PATHS)\n\nANNO_DF","3596d819":"split1 = int(NUM_TOTAL_IMAGES*0.6)\nsplit2 = int(NUM_TOTAL_IMAGES*0.8)\nTRAIN_IMAGE_PATHS = IMAGE_PATHS[:split1]\nVAL_IMAGE_PATHS = IMAGE_PATHS[split1:split2]\nTEST_IMAGE_PATHS = IMAGE_PATHS[split2:]\n\nNUM_TRAIN_IMAGES = len(TRAIN_IMAGE_PATHS)\nNUM_VAL_IMAGES = len(VAL_IMAGE_PATHS)\nNUM_TEST_IMAGES = len(TEST_IMAGE_PATHS)\nprint('Dataset has {} training images, {} validation images and {} unlabled images'.\\\n     format(NUM_TRAIN_IMAGES, NUM_VAL_IMAGES, NUM_TEST_IMAGES))\n\nCLASS_NAMES = pd.unique(ANNO_DF['Label']).tolist() + ['Background']\nN_CLASSES = len(CLASS_NAMES)\n\nprint(f'Classes: {CLASS_NAMES}')","ac733f98":"def visualize_pred(img, annos, ax=None):\n    unique_label = set()\n    for xmin, ymin, xmax, ymax, label in annos:\n        unique_label.add(label)\n        cv2.rectangle(\n                img, \n                pt1=(xmin, ymin), \n                pt2=(xmax, ymax), \n                color=color_map_with_label[label], \n                thickness=3\n            )\n        cv2.putText(\n                img, \n                CLASS_NAMES[label], \n                (xmin, ymin-5), \n                cv2.FONT_HERSHEY_SIMPLEX, \n                2, \n                color_map_with_label[label], \n                2\n            )\n    if ax:\n        ax.imshow(img)\n        ax.set_title(f'{len(annos)} objects detected belonging to {len(unique_label)} classes')\n    else:\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(f'{len(annos)} objects detected belonging to {len(unique_label)} classes')","8dabecd8":"class RegionsMap():\n    def __init__(self, img_path, target_size=(800, 800), resize_before_extract=True):\n        self.img_path = img_path\n        self.img_id = img_path.split('\/')[-1]\n        self.img_array = plt.imread(img_path)\n        self.target_size = target_size\n        self.org_width = self.img_array.shape[1]\n        self.org_height = self.img_array.shape[0]\n        self.get_annos_from_id()\n        self.color_map_with_label = [[255, 12, 12], [12, 255, 12], [12, 12, 255]]\n        \n        if resize_before_extract:\n            self.img_array = cv2.resize(\n                self.img_array, \n                self.target_size, \n                interpolation = cv2.INTER_AREA\n            )\n            self.img_array = self.img_array.astype(np.float32) \/ 255\n            self.scale_annos()\n            \n        self.extract_regions()\n        self.convert_min_max()\n        self.pos_regions = []\n        self.neg_regions = []\n        self.label = []\n        self.colors = ['red', 'green', 'blue', 'black']\n        \n        \n    def __len__(self):\n        return len(self.regions)\n    \n    \n    def get_annos_from_id(self):\n        annos = []\n        element = ANNO_DF[ANNO_DF['Frame']==self.img_id].reset_index()\n        for i, row in element.iterrows():\n            annos.append([row['xmin'], row['ymin'], row['xmax'], row['ymax'], CLASS_NAMES.index(row['Label'])])\n\n        self.annos = np.array(annos)\n    \n    \n    def extract_regions(self):\n        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n        ss.setBaseImage(self.img_array)\n        ss.switchToSelectiveSearchFast()\n        regions = ss.process()\n        self.regions = regions[:2000]\n\n    \n    def convert_min_max(self):\n        # Convert to xmin, ymin, xmax, ymax\n        self.regions[:, 2] += self.regions[:, 0]\n        self.regions[:, 3] += self.regions[:, 1]\n    \n    def scale_annos(self):\n        self.annos[:, 0] = self.annos[:, 0] * self.target_size[0] \/ self.org_width\n        self.annos[:, 2] = self.annos[:, 2] * self.target_size[0] \/ self.org_width\n        self.annos[:, 1] = self.annos[:, 1] * self.target_size[1] \/ self.org_height\n        self.annos[:, 3] = self.annos[:, 3] * self.target_size[1] \/ self.org_height\n        \n        \n    def get_single_iou(self, bb1, bb2):\n        '''\n        Args: \n            `bb1`, `bb2`: contain bounding boxes's info in the following format:\n                            [xmin, ymin, xmax, ymax, (class_id)]\n        '''\n        assert bb1[0] < bb1[2]\n        assert bb1[1] < bb1[3]\n        assert bb2[0] < bb2[2]\n        assert bb2[1] < bb2[3]\n        x_left = max(bb1[0], bb2[0])\n        y_top = max(bb1[1], bb2[1])\n        x_right = min(bb1[2], bb2[2])\n        y_bottom = min(bb1[3], bb2[3])\n        if x_right < x_left or y_bottom < y_top:\n            return 0.0\n        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n        bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n        bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n        iou = intersection_area \/ float(bb1_area + bb2_area - intersection_area)\n        assert iou >= 0.0\n        assert iou <= 1.0\n        \n        return iou\n    \n    \n    def compute_iou_and_map(self):\n        '''\n        Compute IOU for all regions extracted from an image\n        Args:\n            `regions`: List of regions proposed by Selective search\n            `anno`: List of ground-truth boxes\n        '''\n        \n        for i, reg in enumerate(self.regions):\n            ious = [self.get_single_iou(reg, anno) for anno in self.annos]\n            iou = max(ious)\n            if iou > 0.5:\n                # Save positive regions to `self.pos_regions` in format:\n                #    [xmin, ymin, xmax, ymax, label]\n                label_id = self.annos[ious.index(iou)][4]\n                self.label.append(label_id)\n                self.pos_regions.append(self.regions[i].tolist() + [label_id] )\n            else:\n                # Label as background\n                self.label.append(N_CLASSES)\n                self.neg_regions.append(self.regions[i].tolist() + [3])\n                \n                \n    def visualize_regions(self, ax):\n        ax.imshow(self.img_array)\n        \n        for i, (xmin, ymin, xmax, ymax) in enumerate(self.regions):\n            w = xmax - xmin\n            h = ymax - ymin\n            rect = mpatches.Rectangle(\n                (xmin, ymin), w, h, \n                fill=False, \n                edgecolor='black', \n                linewidth=0.5\n            )\n            ax.add_patch(rect)\n            \n        for (xmin, ymin, xmax, ymax, label) in self.pos_regions:\n            w = xmax - xmin\n            h = ymax - ymin\n            rect = mpatches.Rectangle(\n                (xmin, ymin), w, h, \n                fill=False, \n                edgecolor=self.colors[label], \n                linewidth=1.2\n            )\n            ax.add_patch(rect)\n            \n        ax.axis('off')\n        \n    \n    def visualize_annos(self, ax):\n        ax.imshow(self.img_array)\n        \n        for (xmin, ymin, xmax, ymax, label) in self.annos:\n            w = xmax - xmin\n            h = ymax - ymin\n            rect = mpatches.Rectangle(\n                (xmin, ymin), w, h, \n                fill=False, \n                edgecolor=self.colors[label], \n                linewidth=1.5\n            )\n            ax.add_patch(rect)\n        \n            ax.text(\n                xmin, ymin-5, \n                CLASS_NAMES[label], \n                bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 2}, \n                color='black',\n                fontsize=8\n            )\n            \n        ax.axis('off')","30f0bb27":"row = 1\ncol = 2\nindices = random.sample(range(NUM_TOTAL_IMAGES), row*col)\n\nfig = plt.figure(figsize=(15, 15))\nstart = time.time()\nregions_maps = [RegionsMap(IMAGE_PATHS[index]) for index in indices]\nfor i in tqdm(range(row*col)):\n    ax = fig.add_subplot(2*row, col, i+1)\n    regions_maps[i].compute_iou_and_map()\n    regions_maps[i].visualize_regions(ax)\n    print(f'Pos: {len(regions_maps[i].pos_regions)}, Neg: {len(regions_maps[i].neg_regions)}')\n    \nend = time.time()\nprint(f'It takes {(end-start)\/(row*col):.2f}s per image to process with selective search')\n\nfor i in range(row*col, 2*row*col):\n    ax = fig.add_subplot(2*row, col, i+1)\n    regions_maps[i - row*col].visualize_annos(ax)\n    ax.axis('off')\n    ","bbe9f6bc":"from keras.layers import Dense\nfrom keras import Input, Model\nfrom keras import optimizers\nfrom keras.utils import Sequence\nfrom keras.applications.vgg16 import VGG16\nfrom keras.losses import SparseCategoricalCrossentropy\nfrom keras.layers.experimental import preprocessing","4554383a":"INPUT_SIZE = (224, 224)\nINPUT_SHAPE = (224, 224, 3)\nBATCH_SIZE = 128\n\nNUM_POS_PER_BATCH = 10\nNUM_NEG_PER_BATCH = 40\nSTEPS_PER_IMAGE = 7\nN_EPOCHS = 2\nN_ITERATIONS = N_EPOCHS * STEPS_PER_IMAGE * NUM_TRAIN_IMAGES","bc4a01bf":"class DataGenerator(Sequence):\n    def __init__(\n        self, \n        img_paths,\n        dim=INPUT_SHAPE,\n        batch_size=BATCH_SIZE,\n        pos_per_batch=NUM_POS_PER_BATCH,\n        neg_per_batch=NUM_NEG_PER_BATCH,\n        steps_per_img=STEPS_PER_IMAGE,\n        n_classes=N_CLASSES\n    ):\n        self.img_paths = img_paths\n        self.num_imgs = len(self.img_paths)\n        self.dim = dim\n        self.batch_size = batch_size\n        self.pos_per_batch = pos_per_batch\n        self.neg_per_batch = neg_per_batch\n        self.steps_per_img = steps_per_img\n        self.n_classes = n_classes\n        \n        # These variables would change during training\/evaluating time\n        self.curr_img = 0\n        self.regions_map = RegionsMap(img_paths[self.curr_img])\n        self.regions_map.compute_iou_and_map()\n        self.indices = np.arange(len(self.regions_map))\n        \n        self.prefetching = threading.Thread(name='prefetching', target=self.fetch_next_img, args=())\n        self.prefetching.start()\n        \n        \n    def __len__(self):\n        return  self.steps_per_img\n    \n    \n    def __getitem__(self, index):\n        X, y = self.__get_data()\n        \n        return X, y\n    \n        \n    def on_epoch_end(self):\n        self.prefetching.join()\n        self.curr_img = self.next_img\n        self.regions_map = self.next_regions_map\n        self.indices = self.next_indices\n        self.index = self.next_index\n        \n\n    def on_epoch_begin(self):\n        self.prefetching = threading.Thread(name='prefetching', target=self.fetch_next_img, args=())\n        self.prefetching.start()\n\n    def fetch_next_img(self):\n        self.next_img = self.curr_img + 1\n        self.next_regions_map = RegionsMap(self.img_paths[self.curr_img])\n        self.next_regions_map.compute_iou_and_map()\n        self.next_indices = np.arange(len(self.next_regions_map))\n        self.next_index = np.arange(len(self.next_indices))\n        np.random.shuffle(self.next_index)        \n        \n        \n    def __get_data(self):\n        X = np.empty((self.batch_size, *self.dim))\n        Y = np.zeros((self.batch_size,), dtype=np.uint8)\n        \n        if self.pos_per_batch < len(self.regions_map.pos_regions):\n            pos = random.sample(self.regions_map.pos_regions, self.pos_per_batch)\n        else:\n            k = self.pos_per_batch \/\/ len(self.regions_map.pos_regions)\n            r = self.pos_per_batch % len(self.regions_map.pos_regions)\n            pos = k * self.regions_map.pos_regions + random.sample(self.regions_map.pos_regions, r)\n            \n            assert len(pos) == self.pos_per_batch\n        \n        if self.neg_per_batch < len(self.regions_map.neg_regions):\n            neg = random.sample(self.regions_map.neg_regions, self.neg_per_batch)\n        else:\n            k = self.neg_per_batch \/\/ len(self.regions_map.neg_regions)\n            r = self.neg_per_batch % len(self.regions_map.neg_regions)\n            neg = k * self.regions_map.neg_regions + random.sample(self.regions_map.neg_regions, r)\n            \n            assert len(neg) == self.neg_per_batch\n            \n        batch = pos + neg\n        random.shuffle(batch)\n        \n        for i, (xmin, ymin, xmax, ymax, label_id) in enumerate(batch):\n            X[i] = cv2.resize(\n                self.regions_map.img_array[xmin:xmax, ymin:ymax], \n                self.dim[:2], \n                interpolation = cv2.INTER_AREA\n            )\n            Y[i] = label_id\n            \n        return X, Y","1ea57d6f":"train_generator = DataGenerator(TRAIN_IMAGE_PATHS)\nval_generator = DataGenerator(VAL_IMAGE_PATHS)\ntest_generator = DataGenerator(TEST_IMAGE_PATHS)","898d3706":"data_augmentation = tf.keras.Sequential([\n    preprocessing.RandomFlip('horizontal'),\n    preprocessing.RandomRotation(0.2),\n    preprocessing.Rescaling(1.\/127.5, offset= -1)\n])\n\ndef get_model(input_shape=INPUT_SHAPE):\n    vggmodel = VGG16(weights='imagenet', include_top=True, input_shape=input_shape)\n\n    for layers in (vggmodel.layers)[:-3]:\n        layers.trainable = False\n\n    x = vggmodel.layers[-2].output\n    pred = Dense(N_CLASSES, activation=\"softmax\")(x)\n    model = Model(inputs=[vggmodel.input], outputs=[pred])\n    \n    # Compile\n    model.compile(\n        optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=[\"accuracy\"]\n    )\n    return model","7a05f259":"model = get_model()\nmodel.summary()","f36355fd":"history = model.fit(train_generator, \n                    epochs=N_ITERATIONS, \n                    steps_per_epoch=len(train_generator), \n                    validation_data=val_generator)\n\nmodel.evaluate(test_generator)","7161009a":"img = plt.imread('..\/input\/udacitytraffic\/object-detection-crowdai\/1479499931572546561.jpg')\nplt.imshow(img)","6435d7f2":"## Read `.csv` annotation file","84c433ca":"# Procedures for dataset comprehension","1317a014":"## Helper functions","761b4170":"## Construct model","3ccd936c":"# Import required packages","2d9b0ec6":"<h1 style=\"text-align:center; \n           font-weight:bold;\n           font-size:180%\">\n        An Implementation of Region Based Convolutional Neural Networks (R-CNN)<\/h1>\n<h2> <u>Dataset<\/u>: Udacity Self Driving Car <\/h2>\n\nMore details are available at https:\/\/github.com\/udacity\/self-driving-car\/tree\/master\/annotations","f9450000":"> Model configurations","6bf6e2ac":"## Create data generator","32379d06":"## Split into TRAIN-VAL-TEST","a7d3023b":"## Regions labelling (according to IOU)","7f49beeb":"<h1 style=\"font-weight:bold\">Bounding-box regressor<\/h1>","9cdb82fc":"# Training the R-CNN suffers 3 main stage:\n<h3 style=\"color:blue\"> 1. SVM as the classifier<\/h3>\n<h3 style=\"color:blue\"> 2. CNN as the feature extractor<\/h3>\n<h3 style=\"color:blue\"> 3. Bounding-box regressor<\/h3>\n\n### In this experiment, I rather use softmax as the classifier to simplfy the training and testing processes in spite of an lower performance on the model.","c5380c05":"> Start training","2bd300f4":"<h1 style=\"font-weight:bold\">VGG16 fine-tunning<\/h1>","bc0e2509":"> Data generator"}}