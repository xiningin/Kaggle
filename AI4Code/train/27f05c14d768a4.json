{"cell_type":{"ea9f7a0c":"code","cb8dedbb":"code","d7d5178e":"code","ed71003f":"code","536d2485":"code","1536911d":"code","b8255fc3":"code","cc9df6f3":"code","4c3dfa69":"code","d2e23b49":"code","876be19a":"code","d12f0570":"code","b90ec087":"code","b6adf081":"code","b643b051":"markdown","7d0bad41":"markdown","ed8669dd":"markdown","35eaf8d6":"markdown","bf74679c":"markdown","a5779ee6":"markdown","c22be46f":"markdown","cbaddb2c":"markdown","ff8d626b":"markdown","4befbc13":"markdown","80f6dd32":"markdown","9395cde0":"markdown","0cc9f3e6":"markdown","7743222d":"markdown"},"source":{"ea9f7a0c":"from keras.datasets import imdb\nfrom keras import models\nfrom keras import layers\nimport numpy as np\nfrom keras import optimizers","cb8dedbb":"(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)","d7d5178e":"train_data[0]\ntrain_labels[0]","ed71003f":"# word_index is a dictionary mapping words to an integer index \nword_index = imdb.get_word_index()\n# We reverse it, mapping integer indices to words\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n# We decode the review; note that our indices were offset by 3\n# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\ndecoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\nprint(decoded_review)","536d2485":"def vectorize_sequences(sequences, dimension=10000):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):        \n        results[i, sequence] = 1.# set specific indices of results[i] to 1s\n        \n    return results\n    # Our vectorized training data\nx_train = vectorize_sequences(train_data)\n    # Our vectorized test data\nx_test = vectorize_sequences(test_data)\n","1536911d":"print(x_train[0])","b8255fc3":"# Our vectorized labels\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')\nprint(y_train[0])","cc9df6f3":"\nmodel=models.Sequential()\nmodel.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\nmodel.add(layers.Dropout(0.8, noise_shape=None, seed=None))\nmodel.add(layers.Dense(64,activation = 'relu'))\nmodel.add(layers.Dropout(0.2, noise_shape=None, seed=None))\nmodel.add(layers.Dense(1,activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer=optimizers.RMSprop(),\n              metrics=['accuracy'])\n","4c3dfa69":"x_val = x_train[0:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[0:10000]\npartial_y_train=y_train[10000:]\n\n","d2e23b49":"history = model.fit(partial_x_train,\n                   partial_y_train,\n                   epochs=10,\n                   batch_size= 512,\n                   validation_data=(x_val,y_val))","876be19a":"history_dict=history.history\nhistory_dict.keys()","d12f0570":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b90ec087":"plt.clf() \n# clear figure\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b6adf081":"result=model.evaluate(x_test,y_test)\nresult","b643b051":"**A look at the training data and labels**","7d0bad41":"* Loading dataset\n* we will work with 10000 words only\n* The variables  and  are lists of reviews, each review being atrain_data    test_datalist  of  word  indices  (encoding  a  sequence  of  words).    and  train_labels    test_labelsare lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\"","ed8669dd":"**Prepairing the data**\n\nWe  cannot  feed  lists  of  integers  into  a  neural  network.  We  have  to  turn  our  lists  intotensors.\nThere are two ways we could do that:\n1. We could pad our lists so that they all have the same length, and turn them into an integertensor of shape , then use as first layer in our network a(samples, word_indices)layer capable of handling such integer tensors.\n1. We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, thiswould mean for instance turning the sequence  into a 10,000-dimensional vector[3, 5]that would be all-zeros except for indices 3 and 5, which would be ones. Then we coulduse as first layer in our network a  layer, capable of handling floating point vectorDensedata.\n\nWe  will  go  with  the  latter  solution.  Let\u2019s  vectorize  our  data,  which  we  will  domanually for maximum clarity:","35eaf8d6":"**Encoded Samples**","bf74679c":"**The history dictionary**\n\nGetiing keys of history dictionary","a5779ee6":"**Building our network and compiling** \n\nOur input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiestsetup you will ever encounter. A type of network that performs well on such a problemwould  be  a  simple  stack  of  fully-connected *dense* layers  with *relu* activations: \n\nWe will use 3 layer network with 16 hidden units and relu as activation with output layer having sigmoid activation function to get probability.\n\nWe will use binary_crossentropy as our loss function and acuuracy matrix.\nwe will use adam optimizer\n\n","c22be46f":"**We\u2019ll  be  working  with  \"IMDB  dataset\",  a  set  of  50,000  highly-polarized  reviews  fromthe Internet Movie Database. They are split into 25,000 reviews for training and 25,000reviews for testing, each set consisting in 50% negative and 50% positive reviews.**","cbaddb2c":"**Plotting the training and validation losses**","ff8d626b":"**Decoding the integer sequences back into sentences**","4befbc13":"**Plotting the training and validation accuracy**","80f6dd32":" **Importing all libraries**","9395cde0":"**Encoding of lables**","0cc9f3e6":"**Training our model:**\n\nWe will train our model for 20 epochs,\nBatch size of 512\nsame time we will moniter loss and accuracy on 10,000 samples that we have taken apart.","7743222d":"**validating our approch**\n\nSetting  a validation set:\nwe will create a validation set to moniter accuracy during training on data that model has never seen before\n"}}