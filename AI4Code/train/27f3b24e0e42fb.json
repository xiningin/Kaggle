{"cell_type":{"0261909a":"code","bc8a5fa5":"code","0f4aea52":"code","f5f3022e":"code","4c0a11cd":"code","55dd124d":"code","40da29cc":"code","45a402a8":"code","45050b1c":"code","38c92b10":"code","d59ec5e8":"code","151bdc15":"code","a6f69cf3":"code","b92adbbe":"code","b54aa950":"code","000a7274":"code","81536e71":"code","81a7102f":"code","787a0456":"code","ca5f98f5":"code","4c35ac6a":"code","b35b9a35":"code","fc67b9cd":"code","950241fc":"code","ae2c63ae":"code","6365cb3a":"code","b9b30670":"code","85ebd264":"code","715272cd":"code","67fdd191":"code","be638294":"code","c5ecae8b":"code","8fdb4dcc":"code","0536eab0":"code","42e9ee34":"code","9099ed4a":"code","ab697e3e":"code","63f1e292":"code","d2fa85f3":"code","7ed10a20":"code","94f47567":"code","0e7557ca":"code","781e4f25":"code","da1a29ff":"code","305731cf":"code","4a6013a8":"code","5af498ef":"code","32c7fb3c":"code","59da8b5e":"code","0c738560":"code","f55234e1":"code","b04891ae":"code","b19877b9":"code","1626f900":"code","1c0c8f25":"code","1ce33d3d":"code","70604c62":"code","93b2637f":"code","e91b11f4":"code","086889aa":"code","8d83c87d":"code","88803f7e":"code","c7b8f399":"code","b6cae4bd":"code","81593731":"code","6c69987c":"code","5be789bc":"code","dd6cc9ac":"code","20aa3578":"code","1b4cd61c":"code","f55b3f03":"code","6dca2e7f":"code","55123c1d":"code","d0bccfa2":"code","1c2840b4":"code","f85f0d9b":"code","c37d0007":"code","da196f1a":"code","4e3914e0":"code","2a0acfec":"code","928957c3":"code","263558a3":"code","04436cd2":"code","3d3e94ad":"code","d02a0ff4":"markdown","ef42fa1c":"markdown","dbf6f471":"markdown","89e9f9c9":"markdown","7626a084":"markdown","ec0c94bd":"markdown","76d3d185":"markdown","71d0894e":"markdown","d0564ded":"markdown","6220739f":"markdown","321522c9":"markdown","75bda37a":"markdown","92b4d21d":"markdown","417664fb":"markdown","72c244ec":"markdown","731dcb18":"markdown","473286ad":"markdown","76e4715c":"markdown","2e77c52a":"markdown","880668d2":"markdown","2b19965a":"markdown","8f883f2f":"markdown","cb149bb6":"markdown","159ebaa6":"markdown","eb9bdbf0":"markdown","b07fd9e3":"markdown","73baf8cd":"markdown","263d5a9d":"markdown","92cad81e":"markdown","41634c9f":"markdown","89921c48":"markdown","cfe89625":"markdown","2c0cbfd9":"markdown","9618b07f":"markdown","4b425909":"markdown","254831a3":"markdown","8a32bbcf":"markdown","e22fc678":"markdown","0d5c8495":"markdown","1d71e206":"markdown"},"source":{"0261909a":"#Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n#Data Manipulaiton\nimport numpy as np\nimport pandas as pd\n\n#Data Visualisation\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n#Preprocessing\nfrom sklearn.preprocessing import  OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine Learning\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nprint(\"Setup Complete\")\n\n# Ignoring warning \nimport warnings\nwarnings.filterwarnings('ignore')","bc8a5fa5":"#Import train and test data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nprint (\"Import Complete\")","0f4aea52":"#Viewing the first 5 lines for train\ntrain.head()","f5f3022e":"print(\"'train' has {} rows\".format(len(train)))","4c0a11cd":"#Vieweing the first 10 line for test\ntest.head(10)","55dd124d":"print(\"'train' has {} rows\".format(len(test)))","40da29cc":"# Catergories with NaN values\ntrain.isnull().sum()","45a402a8":"# Visual display for missing values across catergories\nmissingno.matrix(train, figsize=(30, 10))","45050b1c":"df_bin = pd.DataFrame() # for discretised continuous variables\ndf_con = pd.DataFrame() # for continuous variables\nprint(\"Variables have been set\")","38c92b10":"# Data types in dataframe\ntrain.dtypes","d59ec5e8":"train.head()","151bdc15":"# How many people survived?\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y=\"Survived\", data = train)\nprint(train.Survived.value_counts()) # counts the number per varible under 'Survived'","a6f69cf3":"# Adding to subset dataframes\ndf_bin[\"Survived\"] = train[\"Survived\"]\ndf_con[\"Survived\"] = train[\"Survived\"]\nprint(\"Addition to subset complete\")","b92adbbe":"# Checking subsets\ndf_bin.head()","b54aa950":"df_con.head()","000a7274":"sns.distplot(train.Pclass)","81536e71":"#Are there any missing variables?\nprint(\"There are {} varibles missing for 'Pclass'\".format(train.Pclass.isnull().sum()))","81a7102f":"# Adding to subset\ndf_bin['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']\nprint(\"Added to subset\")","787a0456":"# How many different names are there?\nprint(\"There are {} unique names\".format(train.Name.value_counts().sum()))","ca5f98f5":"# Are there any missing names?\nprint(\"There are {} missing names\".format(train.Name.isnull().sum()))","4c35ac6a":"plt.figure(figsize=(20,5))\nsns.countplot(y = \"Sex\", data = train)\nplt.title(\"Count for Sex of the passengers\")","b35b9a35":"# Are there any missing values\nprint(\"There are {} missing values for 'Sex'\".format(train.Sex.isnull().sum()))","fc67b9cd":"# Adding to subset dataframes\ndf_bin[\"Sex\"] = train[\"Sex\"]\n# Changing sex to 0 for male and 1 for female\ndf_bin[\"Sex\"] = np.where(df_bin[\"Sex\"] == \"female\", 1, 0) \n\ndf_con [\"Sex\"] = train[\"Sex\"]\nprint(\"Added to subset dataframe\")","950241fc":"# Checking subset\ndf_bin.head()","ae2c63ae":"# How does the sex of the passenger compaare to their survival?\nplt.figure(figsize=(10,10))\nsns.distplot(df_bin.loc[df_bin['Survived'] == 1]['Sex'], label = 'Survived', kde = False );\nsns.distplot(df_bin.loc[df_bin['Survived'] == 0]['Sex'], label = 'Did Not Survive', kde = False );\nplt.title(\"Distribution of Survivalhood based on Sex\")\nplt.legend()","6365cb3a":"# Are there any missing values?\n\nprint(\"There are {} missing values for 'Age'\".format(train.Age.isnull().sum()))","b9b30670":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     label = 'Survived', kde = False );\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     label = 'Did Not Survive', kde = False);\n        plt.legend()\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     label = 'Survived', kde = False );\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     label = 'Did Not Survive', kde = False);\n        plt.legend()","85ebd264":"# Are there any missing values?\nprint(\"There are {} missing values\".format(train.SibSp.isnull().sum()))","715272cd":"# What values are there?\ntrain.SibSp.value_counts()","67fdd191":"df_bin[\"SibSp\"] = train[\"SibSp\"]\ndf_con[\"SibSp\"] = train[\"SibSp\"]\nprint(\"Added to subsets\")","be638294":"plot_count_dist(train, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))","c5ecae8b":"# Are there any missing values?\n\nprint(\"There are {} missing values for 'Parch'\".format(train.Parch.isnull().sum()))","8fdb4dcc":"df_bin[\"Parch\"] = train[\"Parch\"]\ndf_con[\"Parch\"] = train[\"Parch\"]\nprint(\"Added to subsets\")","0536eab0":"# Plotting graphs\nplot_count_dist(train, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='Parch', \n                figsize=(20, 10))","42e9ee34":"# Are there any missing values?\n\nprint(\"There are {} missing values for 'Ticket'\".format(train.Ticket.isnull().sum()))","9099ed4a":"# How many kinds of values are there?\ntrain.Ticket.value_counts()[:20]","ab697e3e":"print(\"There are {} unique ticket IDs\".format(len(train.Ticket.unique())))","63f1e292":"# How many missing values do we have?\n\nprint(\"There are {} missing values for 'Fare'\".format(train.Fare.isnull().sum()))","d2fa85f3":"# How many unique values are there?\nprint(\"There are {} unique Fares\".format(len(train.Fare.unique())))","7ed10a20":"train.Fare.dtype","94f47567":"df_con['Fare'] = train['Fare'] #continuous variable\ndf_bin['Fare'] = pd.cut(train['Fare'], 5) #discretised\nprint(\"Added to subset dataframes\")","0e7557ca":"df_bin.head()","781e4f25":"df_bin.Fare.value_counts()","da1a29ff":"plot_count_dist(data=train,\n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Fare', \n                figsize=(20,10), \n                use_bin_df=True)","305731cf":"# Are there any missing values?\n\nprint(\"There are {} missing values for 'Cabin'\".format(train.Cabin.isnull().sum()))","4a6013a8":"train.Cabin.value_counts()","5af498ef":"# Are there any missing values?\n\nprint(\"There are {} missing values for 'Embarked'\".format(train.Embarked.isnull().sum()))","32c7fb3c":"train.Embarked.value_counts()","59da8b5e":"plt.figure(figsize=(10,5))\nsns.countplot(y = train['Embarked'])\nplt.title(\"Number of people embarked at each port\")\nplt.xlabel(\"Number of people\")","0c738560":"df_bin[\"Embarked\"] = train[\"Embarked\"]\ndf_con[\"Embarked\"] = train[\"Embarked\"]\nprint(\"Added to subset\")","f55234e1":"#Removing rows with missing values\nprint(\"Before dropping df_con has {} rows.\".format(len(df_con)))\ndf_bin = df_bin.dropna(subset=[\"Embarked\"])\ndf_con = df_con.dropna(subset=[\"Embarked\"])\nprint(\"After dropping df_con has {} rows.\".format(len(df_con)))","b04891ae":"#One-Hot Encoding the Discretised Continuous Variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns = one_hot_cols)\n\ndf_bin_enc.head()","b19877b9":"df_con.head(20)","1626f900":"# One-Hot Econding the Catergorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_pclass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')\n","1c0c8f25":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot,\n                        df_pclass_one_hot], axis=1)\n                        \n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Sex', 'Embarked', 'Pclass'], axis=1)","1ce33d3d":"df_con_enc.head(10)","70604c62":"#Selecting the dataframe to use for predictions\nselected_df = df_con_enc\nselected_df.head()","93b2637f":"y = selected_df.Survived\nX = selected_df.drop('Survived', axis = 1)\n\nX_train, val_X, y_train, val_y = train_test_split(X, y, random_state = 1)","e91b11f4":"#Shape of the data without labels\nX_train.shape","086889aa":"X_train.head()","8d83c87d":"# Shape of labels\ny_train.shape","88803f7e":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","c7b8f399":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","b6cae4bd":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","81593731":"#Regular Accuracy\n\nstart_time = time.time()\n# fit model no training data\nmodel = XGBClassifier(verbosity = 0)\nmodel.fit(X_train, y_train, \n         eval_set=[(val_X, val_y)], \n         verbose=False)\n\n# make predictions for test data\ny_pred = model.predict(val_X)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\nacc_xgb = round(accuracy_score(val_y, predictions)*100, 2)\ndt_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_xgb)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))\n","6c69987c":"# Cross Validation 10-Fold Accuracy\n\nstart_time = time.time()\n\n#Defining the model\nXGB_model = xgboost.XGBClassifier(verbosity = 0)\nXGB_model.fit(X_train, y_train, \n             eval_set=[(val_X, val_y)], \n             verbose=False)\n\n# One Pass\n#acc = round(XGB_model.score(X_train, y_train) * 100, 2)\n\n# Cross Validation\nkfold = KFold(n_splits=10, random_state=7)\n# train_pred = model_selection.cross_val_predict(XGB_model, \n#                                                   X_train, \n#                                                   y_train, \n#                                                   cv= kfold, \n#                                                   n_jobs = -1)\n\n# Cross-validation accuracy metric\n\nacc_cv_tomean = cross_val_score(XGB_model, X_train, y_train, cv=kfold)\nacc_cv_xgb = round(acc_cv_tomean.mean()*100, 2)\n# acc_cv = round(np.max(train_pred['test-Accuracy-mean']) * 100, 2)\n\ndt_time = (time.time() - start_time)\n#print(\"Accuracy: %s\" % acc)\nprint(\"Accuracy CV 10-fold: %.2f%%\" % (acc_cv_xgb))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","5be789bc":"# Generating table for Regular ML model scores\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression','Decision Tree', 'XGBoost'],\n    'Score': [        \n        acc_log,   \n        acc_dt,\n        acc_xgb\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","dd6cc9ac":"# Generating table for CV 10-Fold Scores\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression','Decision Tree', 'XGBoost'],\n    'Score': [        \n        acc_log,   \n        acc_dt,\n        acc_xgb\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","20aa3578":"def feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp","1b4cd61c":"# plotting feature importance\nfeature_importance(XGB_model, X_train)","f55b3f03":"X_train.head()","6dca2e7f":"test.head()","55123c1d":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","d0bccfa2":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","1c2840b4":"#Selecting test columns\nSelected_test_cols = X_train.columns\nSelected_test_cols","f85f0d9b":"# Removing duplicated columns\ntest = test.loc[:,~test.columns.duplicated()]","c37d0007":"# Make a prediction using the XGB model on the selected columns\npredictions = XGB_model.predict(test[Selected_test_cols])","da196f1a":"# Predicitions should be 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","4e3914e0":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","2a0acfec":"gender_submission.head()","928957c3":"# Converting submission dataframe 'Survived' column to intergers\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","263558a3":"# Are 'test' and 'sumbission' the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test has ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes are not equal length. Not fit for submission\")","04436cd2":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\n# Already submitted intial EDA. Commented out to avoid the error.\n#submission.to_csv('Titanic ML Walkthrough\/XGBoost_submission.csv', index=False) \nprint('Submission CSV is ready!')","3d3e94ad":"# Check the submission csv to make sure it's in the right format\n# Submission not downloded to Kaggle notebook\n#submissions_check = pd.read_csv(\"Titanic ML Walkthrough\/XGBoost_submission.csv\")\nsubmissions_check.head()","d02a0ff4":"**Checking the distribution**\n\nIt is worth checking the distribution across each feature to ensure there are no outliers, that we may not want to include in this model.","ef42fa1c":"### Target Feature: Parch\n\nDescription: The number of passenger who had parents\/children on board","dbf6f471":"Most passengers paid in the loweest bracket, however, there is a correlation between survivalbility and paying a higher ticket price. Future analysis into correlation between SibSp and Parch with Fares would further justify the reasoning above.","89e9f9c9":"### Target Feature: Pclass\n\nDescription: Class of ticket the passenger possessed \n\nKey:\n1 = 1st, 2 = 2nd, 3 = 3rd","7626a084":"As only two values are missing, these can be dropped as it would not have a great impact on ML prediction. Testing imputation with Embarked in the future is of interest.","ec0c94bd":"# Feature Encoding\n\nPerforming one-hot encoding on df_bin and label encoding on df_con.","76d3d185":"No clear way to catergorise tickers (reduce the number of unique IDs). Will cross-check with other notebooks in the future. Tickets will not be added to the subsets for the time being.","71d0894e":"### Target Feature: Age\n\nDescription: Age of the passenger.","d0564ded":"# Machine Learning Models","6220739f":"### Creating a plot function\n\nPlots will be required for each target feature, thus, creating a funciton will streamline further coding","321522c9":"## Feature Importance","75bda37a":"### Target FeatureL SibSP\n\nDescription: The number of siblings\/parents tha passenger has aboard the Titanic.","92b4d21d":"### Defining a function to fit ML algorithm","417664fb":"### XGBoost Algorithm","72c244ec":"Embarked has catergorical variables as there are 3 ports a passenger could board from.","731dcb18":"# Data Dictionary\n\n**Survival:** 0 = No, 1 = Yes\n\n**pclass (Ticket class):** 1 = 1st, 2 = 2nd, 3 = 3rd\n\n**sex:** Sex\n\n**Age:** Age in years\n\n**sibsp:** number of siblings\/spouses aboard the Titanic\n\n**parch:** number of parents\/children aboard the Titanic\n\n**ticket:** Ticket number\n\n**fare:** Passenger fare\n\n**cabin:** Cabin number\n\n**embarked:** Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","473286ad":"With this feature there are three variables (1,2 and 3) which are also *catergorical* as a person in class 1 and a person in  class 2 != a person in class 3. They are also *ordinal variables* as they have a specific order beyond just being named.","76e4715c":"### Logistic Regression","2e77c52a":"### Target Feature: Fare\n\nDescription: How much did a passenger pay for their ticket?","880668d2":"# Submission","2b19965a":"As there are too many unique items, 'Names' will be not be included in the subsets. Alternatively, grouping the titles (e.g. Mr = 1, Mrs = 2 etc.) may prove effective in the future.","8f883f2f":"### Target Feature: Survived\n\nDescription: Did the passenger survive?\n\nKey: 0 = Did no survive, 1 = Survived","cb149bb6":"Note: Remember 0 = Male and 1 = Female\n\nNot many people survived but for those that did, the majortiy were female. James Cameron at least got this part right in the movie. ","159ebaa6":"Further improvement to be made by imputing or dropping Age. Will not be included for now.","eb9bdbf0":"### Target Feature: Ticket\n\nDescription: The ticket number of the boarding passenger","b07fd9e3":"# Missing Values\n\nIdentifying what rows have NaN values","73baf8cd":"There are 687 missing values out out 891 total values, therefore, Cabin will not be included in sub dataframe. \n\nIf I were to fill in these missing values, i would first reduce the number of unique varibles for the sub dataframe but basing it on the starting letter of each value e.g.(A = 1, B = 2, C = 3 etc.). After this, the missing values can be estimated based on the Fare, SibSp and Parch. If a strong correlation exists with at least two of these factors then ML can be applied to guess the missing value.","263d5a9d":"### Targer Feature: Name\n\nDescription: Name of the passenger","92cad81e":"### Target Feature: Sex\n\nDescription: Sex of the passenger","41634c9f":"After cross validation XGBoost shows the highest accuracy score 80% -  this is a good score considering random guessing (0 or 1) would result in around 50% accuracy. XGBoost will be used for further steps.","89921c48":"# Individual exploration of features","cfe89625":"4 unique items is much easier to work with than 248.","2c0cbfd9":"Most passenger had 0 children and\/or parents and fewer with 1 or 2. The majority of survivors were made op of those who travelled alone, they were more likely to survive if they travled with a child\/parent (1 more likely than 2).","9618b07f":"### Target Feature: Cabin\n\nDescription: The cabin number where the passenger was staying.","4b425909":"### Target Feature: Embarked\n\nDescription: The port the passenger boarded the Titanic","254831a3":"# Preparing two data frames for data analysis\n* Discretised continuous variables (continuous varibles that have been sorted into catergories)\n* Continuous variables","8a32bbcf":"Beacause fair is a float (number) it can be added into the continuous datafram and will be split into bins when it's added into the catergorical dataframe.","e22fc678":"### Decision Tress Classifier","0d5c8495":"Most passengers travelled by themselves and fewer travelled with an increasing number of spouse\/childred. The majority of people who survived were those who travlled alone or had one child\/spouse with them. From this visulisation, those with a child or a spouse were more likely to survive than not.","1d71e206":"**Cabin** may not be worth working through as there are far too many missing values, thus, making predicitons likely to be inaccurate "}}