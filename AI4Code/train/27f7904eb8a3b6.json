{"cell_type":{"c5f939e9":"code","ffe59ec4":"code","9f7e448e":"code","bac26879":"code","93e385e1":"code","5f483779":"code","8a1aed72":"code","83cc9373":"code","b78aa535":"code","bbcc4690":"code","42d5e047":"code","f19e2419":"code","df77c718":"code","af752304":"code","3b07b7d9":"code","4a061ea9":"code","4045aa71":"code","1e677a95":"code","e203ef59":"code","495bb79d":"code","00c11a6e":"markdown","600a1fa8":"markdown","4db2644e":"markdown","d1a02b28":"markdown","38ceeae9":"markdown","c1c20e6c":"markdown","304b7e33":"markdown","b4fde847":"markdown","42d1213c":"markdown","b94b9712":"markdown","6ab808de":"markdown","acdfa943":"markdown"},"source":{"c5f939e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffe59ec4":"#Uncomment below line to install focal_loss library.\n!pip install focal_loss","9f7e448e":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom focal_loss import BinaryFocalLoss\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(mode='min',patience=5)","bac26879":"train_data_path = \"..\/input\/multilabel-classification-dataset\/train.csv\"\n#test_data_path = \"..\/input\/multilabel-classification-dataset\/test.csv\"\ntrain_data = pd.read_csv(train_data_path)\n#test_data = pd.read_csv(test_data_path)","93e385e1":"train_data[\"abstract\"] = train_data['TITLE']+train_data['ABSTRACT']","5f483779":"X = train_data['abstract']\ny = train_data.iloc[:,3:-1]","8a1aed72":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)","83cc9373":"print(\"X_train Size : {} \".format(X_train.shape))\nprint(\"y_train Size : {} \".format(y_train.shape))\nprint(\"X_test Size : {} \".format(X_test.shape))\nprint(\"y_test Size : {} \".format(y_test.shape))","b78aa535":"class Preprocessing:\n    def __init__(self,text):\n        self.stop_words = stopwords.words('english')\n        self.lemma = WordNetLemmatizer()\n        self.token = Tokenizer()\n        self.clean_data = list()\n        for sentence in tqdm(text):\n            sentence = self.remove_punctuations(sentence)\n            sentence = self.stop_word_remover(sentence)\n            sentence = self.lemmatization(sentence)\n            self.clean_data.append(sentence)            \n        self.token_sentence = self.tokenization(self.clean_data)\n        \n    def remove_punctuations(self,text):\n        text = text.lower()\n        text  = re.findall(\"[a-zA-Z]+\",text)\n        return text\n\n    def stop_word_remover(self,text):\n        text = [w for w in text if not w in self.stop_words]\n        return text\n    \n    def lemmatization(self,text):\n        text = \" \".join(self.lemma.lemmatize(w) for w in  text)\n        return text\n    \n    def tokenization(self,text):\n        self.token.fit_on_texts(text)\n        text = self.token.texts_to_sequences(text)\n        return text\n    \npre_train = Preprocessing(X_train)\npre_test = Preprocessing(X_test)","bbcc4690":"#Max Sequence Length\nlen_seq = list()\nfor i in range(0,len(pre_train.token_sentence)):\n    len_seq.append(len(pre_train.token_sentence[i]))\nmax_len_seq = max(len_seq)","42d5e047":"# Converting train & test into sequence\nX_train_seq = pad_sequences(pre_train.token_sentence,maxlen = max_len_seq,padding = 'pre',truncating='post')\nX_test_seq = pad_sequences(pre_test.token_sentence,maxlen = max_len_seq,padding = 'pre',truncating='post')","f19e2419":"vocab_size = len(pre_train.token.word_index) + 1","df77c718":"inputs_layer = tf.keras.Input(shape=(None,), name=\"input\")\ninputs = layers.Embedding(vocab_size, 300, input_length = max_len_seq, trainable = False)(inputs_layer)\ninputs = layers.SimpleRNN(64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\ncs_pred = layers.Dense(1, activation='sigmoid',name='CS',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nph_pred = layers.Dense(1, activation='sigmoid',name='PH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nmath_pred = layers.Dense(1, activation='sigmoid',name='MATH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nstat_pred = layers.Dense(1, activation='sigmoid',name='STAT',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nqb_pred = layers.Dense(1, activation='sigmoid',name='QB',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nqf_pred = layers.Dense(1, activation='sigmoid',name='QF',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\n\nmodel = tf.keras.Model(inputs=[inputs_layer],outputs=[cs_pred,ph_pred,math_pred,stat_pred,qb_pred,qf_pred])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=BinaryFocalLoss(gamma=2),\n    metrics=['accuracy'],)","af752304":"model.summary()","3b07b7d9":"plot_model(model,\"multi_label_classification_model.png\")","4a061ea9":"history = model.fit({\"input\": X_train_seq,}, \n                    {'CS': y_train.iloc[:,0], 'PH':y_train.iloc[:,1], 'MATH': y_train.iloc[:,2], 'STAT': y_train.iloc[:,3], 'QB': y_train.iloc[:,4], 'QF': y_train.iloc[:,5]},\n                    epochs = 25, \n                    validation_data =({\"input\": X_test_seq} ,\n                                     {'CS': y_test.iloc[:,0], 'PH':y_test.iloc[:,1], 'MATH': y_test.iloc[:,2], 'STAT': y_test.iloc[:,3], 'QB': y_test.iloc[:,4], 'QF': y_test.iloc[:,5]}),\n                    callbacks=early_stopping,\n                    verbose=True\n                   )","4045aa71":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper right')\nplt.show()","1e677a95":"inputs_layer = tf.keras.Input(shape=(None,), name=\"input\")\ninputs = layers.Embedding(vocab_size, 300, input_length = max_len_seq, trainable = False)(inputs_layer)\ninputs = layers.LSTM(64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\ncs_pred = layers.Dense(1, activation='sigmoid',name='CS',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nph_pred = layers.Dense(1, activation='sigmoid',name='PH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nmath_pred = layers.Dense(1, activation='sigmoid',name='MATH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nstat_pred = layers.Dense(1, activation='sigmoid',name='STAT',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nqb_pred = layers.Dense(1, activation='sigmoid',name='QB',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\nqf_pred = layers.Dense(1, activation='sigmoid',name='QF',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(inputs)\n\nmodel = tf.keras.Model(inputs=[inputs_layer],outputs=[cs_pred,ph_pred,math_pred,stat_pred,qb_pred,qf_pred])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=BinaryFocalLoss(gamma=2),\n    metrics=['accuracy'],)","e203ef59":"history = model.fit({\"input\": X_train_seq,}, \n                    {'CS': y_train.iloc[:,0], 'PH':y_train.iloc[:,1], 'MATH': y_train.iloc[:,2], 'STAT': y_train.iloc[:,3], 'QB': y_train.iloc[:,4], 'QF': y_train.iloc[:,5]},\n                    epochs = 25, \n                    validation_data =({\"input\": X_test_seq} ,\n                                     {'CS': y_test.iloc[:,0], 'PH':y_test.iloc[:,1], 'MATH': y_test.iloc[:,2], 'STAT': y_test.iloc[:,3], 'QB': y_test.iloc[:,4], 'QF': y_test.iloc[:,5]}),\n                    callbacks=early_stopping,\n                    verbose=True\n                   )","495bb79d":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper right')\nplt.show()","00c11a6e":"# **Introduction**","600a1fa8":"# **Result & Plotting**","4db2644e":"# **Simple RNN**","d1a02b28":"# **Research Paper Labelling Classification : Multi-Label Classification**","38ceeae9":"# **Splitting Dataset**","c1c20e6c":"# **Importing Requiremets**","304b7e33":"1. **Remove Punctuation**\n2. **Stop Words Remover**\n3. **Lemmatize**\n4. **Tokenizer**","b4fde847":"# **Result & Plotting**","42d1213c":"# **Modelling**","b94b9712":"# **Preprocessing**","6ab808de":"# **Load Dataset**","acdfa943":"# **LSTM**"}}