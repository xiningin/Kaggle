{"cell_type":{"d1f1af55":"code","b688253b":"code","dc376e3d":"code","f6d32b42":"code","9eab4b32":"code","f3d32bb0":"code","440d411b":"code","9795c347":"code","b506d8c5":"code","e8310a61":"code","f22dca20":"code","c492a526":"code","bbf4cdd2":"code","7ab99b79":"code","11c43030":"code","50c1ddad":"code","72ede3fa":"code","865bc73c":"code","675d1765":"code","23a5c2fe":"code","36ff4717":"code","d6af47df":"code","f7dafe0d":"code","0f68bc2c":"code","95a5aa5b":"code","d68c1819":"code","88a0e073":"code","b089404d":"code","71614900":"code","4281e36d":"markdown","546835eb":"markdown","7ed797d6":"markdown","136ae6c1":"markdown","157b8b6b":"markdown","a4504725":"markdown","976bb57f":"markdown","c4fe18bd":"markdown","4aea2c13":"markdown","8f739ccf":"markdown","abd76f6a":"markdown","2ef5aa8f":"markdown","7e1fe8b9":"markdown","b83cae20":"markdown","52aee430":"markdown","22264d16":"markdown","f13a504d":"markdown","ff0ceec1":"markdown","f33b1f47":"markdown","fdf21cb8":"markdown","14e416d1":"markdown","f1854547":"markdown","ef8502a4":"markdown","859cbd99":"markdown","4f712bf2":"markdown","910d112b":"markdown","14c562f9":"markdown","bc2dd42b":"markdown","955e7d53":"markdown"},"source":{"d1f1af55":"%%capture\n!pip install nlp\n!pip install fugashi\n!pip install ipadic\n!pip install wandb\n!pip install unidic-lite","b688253b":"from transformers import BertForSequenceClassification, BertJapaneseTokenizer, BertTokenizerFast, Trainer, TrainingArguments\nimport torch\nimport torch.utils.data as torchdata\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom nlp import Dataset\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom typing import List","dc376e3d":"train_df = pd.read_pickle(\"\/kaggle\/input\/rkcup-1\/hands_on_train_df.pickle\")\ntest_df = pd.read_pickle(\"\/kaggle\/input\/rkcup-1\/hands_on_test_df.pickle\")\nsample_submission_df = pd.read_csv(\"\/kaggle\/input\/rkcup-1\/sample_submission.csv\")","f6d32b42":"train_df[train_df.target == 1].head(5)","9eab4b32":"train_df[train_df.target == 0].head(5)","f3d32bb0":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.text, train_df.target, stratify=train_df.target, random_state=0)\n\nprint('X_train', X_train.shape)\nprint('X_valid', X_valid.shape)","440d411b":"train_set = Dataset.from_dict({'text': X_train, 'label': y_train})\nvalid_set = Dataset.from_dict({'text': X_valid, 'label': y_valid})\ntest_set = Dataset.from_dict({'text': test_df.text})\n\ntrain_set[0]","9795c347":"# tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku\/bert-base-japanese-whole-word-masking')\ntokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku\/bert-large-japanese')","b506d8c5":"tokens = tokenizer(\"\u4eca\u65e5\u306f\u6674\u308c\u3067\u3059\")\ntokens","e8310a61":"tokenizer.decode(tokens.input_ids)","f22dca20":"tokenizer.decode(tokens.input_ids, skip_special_tokens=True)","c492a526":"sentences = ['\u4eca\u65e5\u306f\u6674\u308c\u3067\u3059\u3002', '\u4eca\u65e5\u306f\u6674\u308c\u306e\u3061\u66c7\u308a\u3067\u3059']\n\ntokens = tokenizer(sentences, padding=True, truncation=True, max_length=20)\nprint(tokens)\n\n# padding\u3055\u308c\u3066\u540c\u3058length\u306b\u306a\u3063\u3066\u3044\u308b\nassert len(tokens.input_ids[0]) == len(tokens.input_ids[1])","bbf4cdd2":"def preprocess(data):\n    return tokenizer(data['text'], padding=True, truncation=True, max_length=200)\n\n# batch_size\u306edefault value\u306f\u30011000\u306a\u306e\u3067\u5909\u66f4\n# \u3053\u306e\u8a2d\u5b9a\u3092\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u3001dataset\u306epadding\u304c\u6b63\u5e38\u306b\u884c\u308f\u308c\u306a\u3044\uff08\u3059\u3079\u3066\u306elength\u304c\u540c\u3058\u306b\u306a\u3089\u306a\u3044\uff09\ntrain_set_preprocessed = train_set.map(preprocess, batched=True, batch_size=len(train_set))\nvalid_set_preprocessed = valid_set.map(preprocess, batched=True, batch_size=len(valid_set))\ntest_set_preprocessed = test_set.map(preprocess, batched=True, batch_size=len(test_set))","7ab99b79":"train_set_preprocessed[0].keys()","11c43030":"train_set_preprocessed[0]['input_ids'][0:10]","50c1ddad":"train_set_preprocessed.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\nvalid_set_preprocessed.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_set_preprocessed.set_format('torch', columns=['input_ids', 'attention_mask'])","72ede3fa":"train_set_preprocessed[0].keys()","865bc73c":"# model = BertForSequenceClassification.from_pretrained('cl-tohoku\/bert-base-japanese-whole-word-masking')\nmodel = BertForSequenceClassification.from_pretrained('cl-tohoku\/bert-large-japanese')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","675d1765":"batch_size = 8\nepochs = 10\nwarmup_steps = 100\n\nweight_decay = 0.01\n\ntraining_args = TrainingArguments(\n    output_dir='.\/results',\n    num_train_epochs=epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    warmup_steps=warmup_steps,\n    weight_decay=weight_decay,\n    logging_dir='.\/logs',\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_set_preprocessed,\n    eval_dataset=valid_set_preprocessed\n)\n\ntrainer.train()\ntrainer.evaluate()","23a5c2fe":"valid_prob, _, _ = trainer.predict(valid_set_preprocessed)\nauc = roc_auc_score(y_valid, valid_prob[:, 1])\nauc","36ff4717":"valid_pred = np.argmax(valid_prob, axis=1)\nprint(classification_report(y_valid, valid_pred))","d6af47df":"number_of_samples = 10\n\nvalid_prob, _, _ = trainer.predict(valid_set_preprocessed)\nvalid_pred = np.argmax(valid_prob, axis=1)\n\nis_0_label = valid_set_preprocessed['label'] == 0\nis_1_label = valid_set_preprocessed['label'] == 1\n\ndef display_valid_text(indexes: List[int]):\n    \"\"\"\n    Validation Dataset\u306eText\u3092\u51fa\u529b\n    Args:\n        \u51fa\u529b\u5bfe\u8c61\u3068\u306a\u308bValidation Dataset\u306eindex\n    \"\"\"\n    for s in valid_set[indexes]['text']:\n        print(s)\n        print() # \u7a7a\u884c\u8ffd\u52a0","f7dafe0d":"is_true = np.array(valid_pred) == np.array(valid_set_preprocessed['label'])\n\n# label=0, pred=0 \u306eindexes\nis_true_0_label = is_true * is_0_label\n\n# label=1, pred=1 \u306eindexes\nis_true_1_label = is_true * is_1_label","0f68bc2c":"indexes = np.where(is_true_1_label == True)[0][:number_of_samples]        \ndisplay_valid_text(indexes)","95a5aa5b":"indexes = np.where(is_true_0_label == True)[0][:number_of_samples]        \ndisplay_valid_text(indexes)","d68c1819":"is_false = np.array(valid_pred) != np.array(valid_set_preprocessed['label'])\n\n# label=0, pred=1 \u306eindexes\nis_false_0_label = is_false * is_0_label\n\n# label=1, pred=0 \u306eindexes\nis_false_1_label = is_false * is_1_label","88a0e073":"indexes = np.where(is_false_1_label == True)[0][:number_of_samples]        \ndisplay_valid_text(indexes)","b089404d":"indexes = np.where(is_false_0_label == True)[0][:number_of_samples]        \ndisplay_valid_text(indexes)","71614900":"test_prob, _, _ = trainer.predict(test_set_preprocessed)\ntest_df['target'] = test_prob[:, 1]\ntest_df[['id','target']].to_csv(\"baseline.csv\", index=None)","4281e36d":"# Model","546835eb":"input_ids\u306e\u6700\u521d\u306e10\u500b\u306e\u30c8\u30fc\u30af\u30f3\u3092\u8868\u793a:","7ed797d6":"\u30c8\u30fc\u30af\u30f3\u3092\u30c7\u30b3\u30fc\u30c9\u3057\u3066\u623b\u3057\u3066\u307f\u308b:","136ae6c1":"## Negative\nlabel=0, pred=0","157b8b6b":"tokenize\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d:","a4504725":"## \u52d5\u4f5c\u78ba\u8a8d","976bb57f":"# Import Libraries","c4fe18bd":"# Training","4aea2c13":"## Confusion Matrix","8f739ccf":"### Positive\nlabel=1, pred=0","abd76f6a":"## Positive\nlabel=1, pred=1","2ef5aa8f":"# Create Dataset","7e1fe8b9":"## \u6b63\u3057\u304f\u4e88\u6e2c\u3067\u304d\u3066\u3044\u308b\u30b1\u30fc\u30b9","b83cae20":"## Evaluation","52aee430":"# Split training data","22264d16":"# Predict","f13a504d":"CLS\u3084SEP\u306a\u3069\u306e\u30b9\u30da\u30b7\u30e3\u30eb\u30c8\u30fc\u30af\u30f3\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b:","ff0ceec1":"# Tokenize","f33b1f47":"## AUC","fdf21cb8":"\u5b66\u7fd2\u306b\u4e0d\u8981\u306aKey\u304c\u9664\u5916\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u308b:","14e416d1":"\u30c8\u30fc\u30af\u30f3\u5316\u3057\u3066\u307f\u308b","f1854547":"## \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u8a2d\u5b9a\u3059\u308b","ef8502a4":"# Load Data","859cbd99":"## \u4e88\u6e2c\u304c\u5916\u308c\u3066\u3044\u308b\u30b1\u30fc\u30b9","4f712bf2":"\u8907\u6570\u306e\u6587\u7ae0\u3092\u30c8\u30fc\u30af\u30ca\u30a4\u30ba\u3057\u3066\u307f\u308b:","910d112b":"# \u53c2\u8003\n* [Getting Started with Google BERT](https:\/\/www.amazon.com\/Getting-Started-Google-BERT-state\/dp\/1838821597)\n* [Huggingface](https:\/\/huggingface.co\/transformers\/quicktour.html)","14c562f9":"### Negative\nlabel=0, pred=1","bc2dd42b":"# Submission\nScore 0.98886","955e7d53":"## Dataset\u3092Tokenize\u3059\u308b"}}