{"cell_type":{"a929319d":"code","9ed5f15b":"code","944ef537":"code","794cb260":"code","f371ece7":"code","6f524358":"code","cd7b1ef4":"code","cc6d10e4":"code","249ed8b1":"code","222dd870":"code","0ba971d7":"code","642bc583":"code","3b5d2811":"code","80c0d3b8":"code","7abaca45":"code","cafc8aea":"code","e8a8677a":"code","fca89ede":"code","34cdab7f":"code","3d926634":"code","4e2f6f39":"code","06d8c1e2":"code","a0477c39":"code","9b32630c":"code","dee63ff9":"code","432c3b3e":"markdown","2404c1ab":"markdown","d55bd279":"markdown","483e7de2":"markdown","0990a96b":"markdown","729ebca0":"markdown","f19c6473":"markdown","0d8f4264":"markdown","219078e1":"markdown","07cf88f7":"markdown","03e13a50":"markdown","0c82c903":"markdown","77196499":"markdown","c01d8eb3":"markdown","4de5d675":"markdown","ecb9a781":"markdown","4a856964":"markdown","a388cd85":"markdown","0ef2339b":"markdown","47f1c478":"markdown","a0277d8e":"markdown","ed47974d":"markdown","2eb96b3c":"markdown","3aab6f63":"markdown","8aefa9ae":"markdown","401c3464":"markdown","8110d204":"markdown","f94c7a36":"markdown","d7cd2edd":"markdown","734b2cdb":"markdown","41362c15":"markdown","6c9927dd":"markdown","e8ff57c4":"markdown","b392b27a":"markdown","5510a7d1":"markdown"},"source":{"a929319d":"!pip install torchtext==0.6.0 --quiet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.datasets import Multi30k\nfrom torchtext.data import Field, BucketIterator\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport random\nfrom torchtext.data.metrics import bleu_score\nfrom pprint import pprint\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Seeding for reproducible results everytime\n'''\nSEED = 777\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True'''","9ed5f15b":"!python -m spacy download en --quiet\n!python -m spacy download de --quiet","944ef537":"spacy_german = spacy.load(\"de\")\nspacy_english = spacy.load(\"en\")","794cb260":"def tokenize_german(text):\n    return [token.text for token in spacy_german.tokenizer(text)]\n\ndef tokenize_english(text):\n    return [token.text for token in spacy_english.tokenizer(text)]\n\n### Sample Run ###\n\nsample_text = \"I love machine learning\"\nprint(tokenize_english(sample_text))","f371ece7":"german = Field(tokenize=tokenize_german,\n               lower=True,\n               init_token=\"<sos>\",\n               eos_token=\"<eos>\")\n\nenglish = Field(tokenize=tokenize_english,\n               lower=True,\n               init_token=\"<sos>\",\n               eos_token=\"<eos>\")\n\ntrain_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),\n                                                    fields=(german, english))\n\ngerman.build_vocab(train_data, max_size=10000, min_freq=3)\nenglish.build_vocab(train_data, max_size=10000, min_freq=3)","6f524358":"print(f\"Unique tokens in source (de) vocabulary: {len(german.vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")","cd7b1ef4":"# dir(english.vocab)\ne = list(english.vocab.__dict__.values())\n\n'''\nprint(english.vocab.__dict__.keys())\nprint(list(english.vocab.__dict__.values()))\ne = list(english.vocab.__dict__.values())\nfor i in e:\n  print(i)'''","cc6d10e4":"word_2_idx = dict(e[3])\nidx_2_word = {}\nfor k,v in word_2_idx.items():\n      idx_2_word[v] = k","249ed8b1":"print(f\"Number of training examples: {len(train_data.examples)}\")\nprint(f\"Number of validation examples: {len(valid_data.examples)}\")\nprint(f\"Number of testing examples: {len(test_data.examples)}\")\n\nprint(train_data[5].__dict__.keys())\npprint(train_data[5].__dict__.values())","222dd870":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 32\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n                                                                      batch_size = BATCH_SIZE, \n                                                                      sort_within_batch=True,\n                                                                      sort_key=lambda x: len(x.src),\n                                                                      device = device)","0ba971d7":"count = 0\nmax_len_eng = []\nmax_len_ger = []\nfor data in train_data:\n      max_len_ger.append(len(data.src))\n      max_len_eng.append(len(data.trg))\n      if count < 10 :\n        print(\"German - \",*data.src, \" Length - \", len(data.src))\n        print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n        print()\n        count += 1\n\nprint(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\nprint(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))","642bc583":"count = 0\nfor data in train_iterator:\n  if count < 1 :\n    print(\"Shapes\", data.src.shape, data.trg.shape)\n    print()\n    print(\"German - \",*data.src, \" Length - \", len(data.src))\n    print()\n    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n    temp_ger = data.src\n    temp_eng = data.trg\n    count += 1","3b5d2811":"temp_eng_idx = (temp_eng).cpu().detach().numpy()\ntemp_ger_idx = (temp_ger).cpu().detach().numpy()","80c0d3b8":"df_eng_idx = pd.DataFrame(data = temp_eng_idx, columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\ndf_eng_idx.index.name = 'Time Steps'\ndf_eng_idx.index = df_eng_idx.index + 1 \n# df_eng_idx.to_csv('\/content\/idx.csv')\ndf_eng_idx","7abaca45":"df_eng_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\ndf_eng_word = df_eng_idx.replace(idx_2_word)\n# df_eng_word.to_csv('\/content\/Words.csv')\ndf_eng_word","cafc8aea":"class EncoderLSTM(nn.Module):\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n    super(EncoderLSTM, self).__init__()\n\n    # Size of the one hot vectors that will be the input to the encoder\n    #self.input_size = input_size\n\n    # Output size of the word embedding NN\n    #self.embedding_size = embedding_size\n\n    # Dimension of the NN's inside the lstm cell\/ (hs,cs)'s dimension.\n    self.hidden_size = hidden_size\n\n    # Number of layers in the lstm\n    self.num_layers = num_layers\n\n    # Regularization parameter\n    self.dropout = nn.Dropout(p)\n    self.tag = True\n\n    # Shape --------------------> (5376, 300) [input size, embedding dims]\n    self.embedding = nn.Embedding(input_size, embedding_size)\n    \n    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n\n  # Shape of x (26, 32) [Sequence_length, batch_size]\n  def forward(self, x):\n\n    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n    embedding = self.dropout(self.embedding(x))\n    \n    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n\n    return hidden_state, cell_state\n\ninput_size_encoder = len(german.vocab)\nencoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\nencoder_dropout = 0.5\n\nencoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n                           hidden_size, num_layers, encoder_dropout).to(device)\nprint(encoder_lstm)","e8a8677a":"class DecoderLSTM(nn.Module):\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n    super(DecoderLSTM, self).__init__()\n\n    # Size of the one hot vectors that will be the input to the encoder\n    #self.input_size = input_size\n\n    # Output size of the word embedding NN\n    #self.embedding_size = embedding_size\n\n    # Dimension of the NN's inside the lstm cell\/ (hs,cs)'s dimension.\n    self.hidden_size = hidden_size\n\n    # Number of layers in the lstm\n    self.num_layers = num_layers\n\n    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n    self.output_size = output_size\n\n    # Regularization parameter\n    self.dropout = nn.Dropout(p)\n\n    # Shape --------------------> (5376, 300) [input size, embedding dims]\n    self.embedding = nn.Embedding(input_size, embedding_size)\n\n    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n\n    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n    self.fc = nn.Linear(hidden_size, output_size)\n\n  # Shape of x (32) [batch_size]\n  def forward(self, x, hidden_state, cell_state):\n\n    # Shape of x (1, 32) [1, batch_size]\n    x = x.unsqueeze(0)\n\n    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n    embedding = self.dropout(self.embedding(x))\n\n    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n\n    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n    predictions = self.fc(outputs)\n\n    # Shape --> predictions (32, 4556) [batch_size , output_size]\n    predictions = predictions.squeeze(0)\n\n    return predictions, hidden_state, cell_state\n\ninput_size_decoder = len(english.vocab)\ndecoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\ndecoder_dropout = 0.5\noutput_size = len(english.vocab)\n\ndecoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\nprint(decoder_lstm)","fca89ede":"for batch in train_iterator:\n  print(batch.src.shape)\n  print(batch.trg.shape)\n  break\n\nx = batch.trg[1]\nprint(x)","34cdab7f":"class Seq2Seq(nn.Module):\n  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n    super(Seq2Seq, self).__init__()\n    self.Encoder_LSTM = Encoder_LSTM\n    self.Decoder_LSTM = Decoder_LSTM\n\n  def forward(self, source, target, tfr=0.5):\n    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n    batch_size = source.shape[1]\n\n    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n    target_len = target.shape[0]\n    target_vocab_size = len(english.vocab)\n    \n    # Shape --> outputs (14, 32, 5766) \n    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n    hidden_state, cell_state = self.Encoder_LSTM(source)\n\n    # Shape of x (32 elements)\n    x = target[0] # Trigger token <SOS>\n\n    for i in range(1, target_len):\n      # Shape --> output (32, 5766) \n      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n      outputs[i] = output\n      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n\n    # Shape --> outputs (14, 32, 5766) \n    return outputs\n","3d926634":"# Hyperparameters\n\nlearning_rate = 0.001\nwriter = SummaryWriter(f\"runs\/loss_plot\")\nstep = 0\n\nmodel = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\npad_idx = english.vocab.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)","4e2f6f39":"model","06d8c1e2":"def translate_sentence(model, sentence, german, english, device, max_length=50):\n    spacy_ger = spacy.load(\"de\")\n\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n    return translated_sentence[1:]\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\ndef checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n    print('saving')\n    print()\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, '\/kaggle\/working\/checkpoint-NMT')\n    torch.save(model.state_dict(),'\/kaggle\/working\/checkpoint-NMT-SD')","a0477c39":"epoch_loss = 0.0\nnum_epochs = 100\nbest_loss = 999999\nbest_epoch = -1\nsentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\nts1  = []\n\nfor epoch in range(num_epochs):\n  print(\"Epoch - {} \/ {}\".format(epoch+1, num_epochs))\n  model.eval()\n  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n  ts1.append(translated_sentence1)\n\n  model.train(True)\n  for batch_idx, batch in enumerate(train_iterator):\n    input = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Pass the input and target for model's forward method\n    output = model(input, target)\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    # Clear the accumulating gradients\n    optimizer.zero_grad()\n\n    # Calculate the loss value for every epoch\n    loss = criterion(output, target)\n\n    # Calculate the gradients for weights & biases using back-propagation\n    loss.backward()\n\n    # Clip the gradient value is it exceeds > 1\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Update the weights values using the gradients we calculated using bp \n    optimizer.step()\n    step += 1\n    epoch_loss += loss.item()\n    writer.add_scalar(\"Training loss\", loss, global_step=step)\n\n  if epoch_loss < best_loss:\n    best_loss = epoch_loss\n    best_epoch = epoch\n    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n    if ((epoch - best_epoch) >= 10):\n      print(\"no improvement in 10 epochs, break\")\n      break\n  print(\"Epoch_Loss - {}\".format(loss.item()))\n  print()\n  \nprint(epoch_loss \/ len(train_iterator))\n\nscore = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")","9b32630c":"progress  = []\nimport nltk\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfor i, sen in enumerate(ts1):\n  progress.append(TreebankWordDetokenizer().detokenize(sen))\nprint(progress)\n\nprogress_df = pd.DataFrame(data = progress, columns=['Predicted Sentence'])\nprogress_df.index.name = \"Epochs\"\nprogress_df.head()","dee63ff9":"model.eval()\ntest_sentences  = [\"Zwei M\u00e4nner gehen die Stra\u00dfe entlang\", \"Kinder spielen im Park.\", \"Diese Stadt verdient eine bessere Klasse von Verbrechern. Der Spa\u00dfvogel\"]\nactual_sentences  = [\"Two men are walking down the street\", \"Children play in the park\", \"This city deserves a better class of criminals. The joker\"]\npred_sentences = []\n\nfor idx, i in enumerate(test_sentences):\n  model.eval()\n  translated_sentence = translate_sentence(model, i, german, english, device, max_length=50)\n  progress.append(TreebankWordDetokenizer().detokenize(translated_sentence))\n  print(\"German : {}\".format(i))\n  print(\"Actual Sentence in English : {}\".format(actual_sentences[idx]))\n  print(\"Predicted Sentence in English : {}\".format(progress[-1]))\n  print()\n","432c3b3e":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/2560\/1*UPyGSZSuIQ52IjyFdPpm6A.png\">\nThe same concept is extended to a batch size of 5 (experimental), where we consider 5 input encoder's context vectors and the first token <\"sos\"> is sent to the decoder at a time. ","2404c1ab":"## Loading the SpaCy's vocabulary for our desired languages. SpaCy also supports many languages like french, german etc,.\n\n","d55bd279":"# 8. Seq2Seq (Encoder + Decoder) Interface","483e7de2":"## The final seq2seq implementation looks like the figure above.\n\n1. Provide both input (German) and output (English) sentences.\n2. Pass the input sequence to the encoder and extract context vectors.\n3. Pass the output sequence to the decoder, context vecotr from encoder to produce the predicted output sequence.\n","0990a96b":"# 9. Seq2Seq (Encoder + Decoder) Code Implementation","729ebca0":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*d9kP4XoWGnIcmyhX-g4Xvw.png\">","f19c6473":"## For a lighter note, let's explain the process happening in the above image. The Encoder of the Seq2Seq model takes one input at a time. Our input German word sequence is \"ich Liebe Tief Lernen\".\n\n## Also, we append the start of sequence \"SOS\" and the end of sentence \"EOS\" tokens in the starting and in the ending of the input sentence.\n\nTherefore at\u00a0\n1. At time step-0, the \"SOS\" token is sent,\n2. At time step-1 the token \"ich\" is sent,\n3. At time step-2 the token \"Liebe\" is sent,\n4. At time step-3 the token \"Tief\" is sent,\n5. At time step-4 the token \"Lernen\" is sent,\n6. At time step-4 the token \"EOS\" is sent.\n\n## And the first block in the Encoder architecture is the word embedding layer [shown in green block], which converts the input indexed word into a dense vector representation called word embedding (sizes\u200a-\u200a100\/200\/300).\n\n## Then our word embedding vector is sent to the LSTM cell, where it is combined with the hidden state (hs), and the cell state (cs) of the previous time step and the encoder block outputs a new hs and a cs which is passed to the next LSTM  cell. \n\n## It is understood that the hs and cs captured some vector representation of the sentence so far.\n\n## At time step-0, the hidden state and cell state are either initialized fully of zeros or random numbers.\n\n## Then after we sent pass all our input german word sequence, a context vector [shown in yellow block] (hs, cs) is finally obtained, which is a dense representation of the word sequence and can be sent to the decoder's first LSTM (hs, cs) for the corresponding English translation.\n\n## In the above figure, we use 2 layer LSTM  architecture, where we connect the first LSTM to the second LSTM and we then we obtain 2 context vectors stacked on top as the final output.\n\n## It is a must that we design identical encoder and decoder blocks in the seq2seq model.\n\n## The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 5 (Experimental), then we pass 5 sentences at a time to the Encoder, which looks like the below figure.","0d8f4264":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*FtDDCniBMb8HXYEM6PRohQ.png\">\n\n## The decoder also does a single step at a time.\n\n## The Context Vector from the Encoder block is provided as the hidden state (hs) and cell state (cs) for the decoder's first LSTM block.\n\n## The start of the sentence \"SOS\"  token is passed to the embedding NN, then passed to the first LSTM cell of the decoder, and finally, it is passed through a linear layer [Shown in Pink color], which provides an output English token prediction probabilities (4556 Probabilities), hidden state (hs), Cell State (cs).\u00a0\n\n## The output word with the highest probability is chosen, hidden state (hs), Cell State (cs) is passed as the inputs to the next LSTM cell and this process is executed until it reaches the end of sentences \"EOS\".\n## The subsequent layers will use the hidden and cell state from the previous time steps.\n\n## The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 5 (Experimental), then we pass 5 sentences at a time to the Encoder, which provide 5 sets of Context Vectors, and they all are passed into the Decoder, which looks like the below figure.","219078e1":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*sQBwBtwCwqqXY0k5O0ZvMg.png\">","07cf88f7":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*xP8MgIfKwjStFDUo0_W3QA.png\">\n\nThe same concept is extended to a batch size of 5 (experimental), where we consider 5 input sentences and the first token from each sentences is sent to the encoder at a time. Each sequences in the batch is maintained to have the same length using the padding token.","03e13a50":"# 6. Decoder Model Architecture (Seq2Seq)","0c82c903":"# 3. Long Short Term Memory (LSTM)\u200a-\u200aUnder the Hood","77196499":"# Table of Contents:\n\n## 1. Introduction\n## 2. Data Preparation and Pre-processing\n## 3. Long Short Term Memory (LSTM)\u200a-\u200aUnder the Hood\n## 4. Encoder Model Architecture (Seq2Seq)\n## 5. Encoder Code Implementation (Seq2Seq)\n## 6. Decoder Model Architecture (Seq2Seq)\n## 7. Decoder Code Implementation (Seq2Seq)\n## 8. Seq2Seq (Encoder + Decoder) Interface\n## 9. Seq2Seq (Encoder + Decoder) Code Implementation\n## 10. Seq2Seq Model Training\n## 11. Seq2Seq Model Inference","c01d8eb3":"# Teach Force Ratio:\n\n## In addition to other blocks, you will also see the block shown below in the Decoder of the Seq2Seq architecture.\n\n## While model training, we send the inputs (German Sequence) and targets (English Sequence). After the context vector is obtained from the Encoder, we send them Vector and the target to the Decoder for translation.\n## But during model Inference, the target is generated from the decoder based on the generalization of the training data. So the output predicted words are sent as the next input word to the decoder until a \"EOS\" token is obtained.\n\n## So in model training itself, we can use the teach force ratio (tfr), where we can actually control the flow of input words to the decoder.\n\n<img src=\"https:\/\/miro.medium.com\/max\/451\/1*YJpyqouvpmu4_Ej9ockl4A.png\">\n\n## We can send the actual target words to the decoder part while training (Shown in Green Color).\n## We can also send the predicted target word, as the input to the decoder (Shown in Red Color).\n## Sending either of the word (actual target word or predicted target word) can be regulated with a probability of 50%, so at any time step, one of them is passed during the training.\n## This method acts like a Regularization. So that the model trains efficiently and fastly during the process.\n","4de5d675":"# 11. Seq2Seq Model Training","ecb9a781":"# Dataset sneek peek","4a856964":"## Before moving to seq2seq model, we need to create Encoder ,Decoder and create a interface between them in the seq2seq model.\n\n## Let's pass the german input sequence \"Ich liebe tief lernen\" which translates to \"I love deep learning\" in english.\n\n\n","a388cd85":"## The above picture shows the units present under a single LSTM Cell. I will add some references to learn more about LSTM in the last and why it works well for long sequences.\n\n## But to simply put, Vanilla RNN, Gated Recurrent Unit (GRU) is not able to capture the long term dependencies due to its nature of design and suffers heavily by the Vanishing Gradient problem, which makes the rate of change in weights and bias values negligible, resulting in poor generalization.\n\n## But LSTM has some special units called gates (Remember gate, Forget gate, Update gate), which helps to overcome the problems stated before.\n\n## Inside the LSTM cell, we have a bunch of mini neural networks with sigmoid and TanH activations at the final layer and few vector adder, Concat, multiplications operations.\n\n1. Sigmoid NN \u2192 Squishes the values between 0 and 1. Say a value closer to 0 means to forget and a value closer to 1 means to remember.\n2. Embedding NN \u2192 Converts the input word indices into word embedding.\n3. TanH NN \u2192 Squishes the values between -1 and 1. Helps to regulate the vector values from either getting exploded to the maximum or shrank to the minimum.\n\n## The hidden state and the cell state are referred to here as the context vector, which are the outputs from the LSTM cell. The input is the sentence's numerical indexes fed into the embedding NN.","0ef2339b":"## So once we get to understand what can be done in torch text, let's talk about how it can be implemented in the torch text module. Here we are going to make use of 3 classes under torch text.\n\n1. Fields :\n> This is a class under the torch text, where we specify how the preprocessing should be done on our data corpus.\n2. TabularDataset : \n> Using this class, we can actually define the Dataset of columns stored in CSV, TSV, or JSON format and also map them into integers.\n3. BucketIterator :\n> Using this class, we can perform padding our data for approximation and make batches with our data for model training.\n\n## Here our source language (SRC - Input) is German and target language (TRG - Output) is English. We also add 2 extra tokens \"start of sequence\" <sos> and \"end of sequence\" <EOS> for effective model training.","47f1c478":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*7SVU_REkIUALmegTbFI9Fw.png\">","a0277d8e":"# Read the Article in [Medium](https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350)","ed47974d":"## Here I am doing a German to English neural machine translation. But the same concept can be extended to other problems such as Named Entity Recognition (NER), Text Summarization, etc,.\n\n## So the Sequence to Sequence (seq2seq) model in this post uses an encoder-decoder architecture, which uses a type of RNN called LSTM (Long Short Term Memory), where the encoder neural network encodes the input german sequence into a single vector, also called as a Context Vector.\n## This Context Vector is said to contain the abstract representation of the input german sequence.\n## This vector is then passed into the decoder neural network, which is used to output the corresponding English translation sentence, one word at a time.","2eb96b3c":"# 4. Encoder Model Architecture (Seq2Seq)","3aab6f63":"# 7. Decoder Code Implementation (Seq2Seq)","8aefa9ae":"# Necessary Imports ","401c3464":"## I just experimented with a batch size of 32 and a sample target batch is shown below. The sentences are tokenized into list of words and indexed according to the vocabulary. The \"pad\" token gets an index of 1.\n\n## Each column corresponds to a sentence indexed into numbers and we have 32 such sentences in a single target batch and the number of rows corresponds to the maximum length of that sentence. Short sentences are padded with 1 to compensate.\n\n## The table (Idx.csv) contains the numerical indices of the words, which is later fed into the word embedding and converted into dense representation for Seq2Seq processing.","8110d204":"# 5. Encoder Code Implementation (Seq2Seq)","f94c7a36":"# 2. Data Preparation & Pre-processing","d7cd2edd":"# 1. Introduction","734b2cdb":"# 12. Seq2Seq Model Inference","41362c15":"## After setting the language pre-processing criteria, the next step is to create batches of training, testing and validation data using iterators.\n\n## Creating batches is an exhaustive process, luckily we can make use of TorchText's iterator libraries.\n\n## Here we are using BucketIterator for effective padding of source and target sentences. We can access the source (german) batch of data using .src attribute and it's correspondign (english) batch of data using .trg attribute.","6c9927dd":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*aNcybCTdPlrXsCwIo1OfTg.png\">","e8ff57c4":"## Actual text data before tokenized","b392b27a":"## Torch text is a powerful library for making the text data ready for variety of NLP tasks. It has all the tools to perform preprocessing on the textual data.\n\n## Let's see some of the process it can do,\n\n1. Train\/ Valid\/ Test Split: partition your data into a specified train\/ valid\/ test set.\n\n2. File Loading: load the text corpus of various formats (.txt,.json,.csv).\n3. Tokenization: breaking sentences into list of words.\n4. Vocab: Generate a list of vocabulary from the text corpus.\n5. Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.\n6. Word Vector: Convert a word from higher dimension to lower dimension (Word Embedding).\n7. Batching: Generate batches of sample.","5510a7d1":"## Now let's create custom tokenization methods for the languages. Tokenization is a process of breaking the sentence into a list of individual tokens (words).\n\n## We can make use of PyTorch's TorchText library for data pre-processing and SpaCy for vocabulary building (English and German) & tokenization of our data."}}