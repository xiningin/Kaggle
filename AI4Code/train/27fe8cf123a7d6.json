{"cell_type":{"75cb8e78":"code","1aad7641":"code","c8cda0e4":"code","2d018189":"code","98d60110":"code","052ebeaa":"code","fb5f0f97":"code","e5536346":"code","52c20100":"code","111a0bf0":"code","e3870b24":"code","0e917af8":"code","46116eb2":"code","ad1b2372":"code","653feab6":"code","f4c1498d":"code","7d35571d":"code","aa9b94f2":"code","7950d6bf":"code","6cf75cb3":"code","3749068f":"code","4cfd4a67":"code","b286b2fb":"code","cef25c3d":"code","9271594c":"code","306ce099":"code","7c51d469":"code","36507c72":"code","76fbb7cc":"code","0c222c31":"code","7f9d727f":"code","40394446":"code","a0a17888":"code","25f49744":"code","5cde3c29":"code","aba0dd2d":"code","4c5bbc9d":"code","f61b3c70":"code","2c166a86":"code","e71808df":"code","c458ad07":"code","bff5a9b1":"code","cdeda0ba":"code","9349468e":"code","f658d398":"code","20e21f96":"code","d29ff637":"code","cc20f4ac":"code","1a00b2a9":"code","8dd8909b":"code","7903b04d":"code","53a8d2ee":"code","552066db":"code","1deb20ad":"code","0eaef910":"code","be0d5773":"code","bf0198c6":"code","87f8399e":"code","7f4bf00e":"code","95ae5080":"code","46139115":"code","0ed11b57":"code","0cedde90":"code","61574c2f":"code","683fe0d8":"markdown","3cda8234":"markdown","fddc1ee6":"markdown","ab8c7d35":"markdown","3a703944":"markdown","92f27066":"markdown","5ecdc1bc":"markdown","9900ea28":"markdown","6e5c9172":"markdown","f2885e63":"markdown","f096e8b1":"markdown","e5cff67d":"markdown","038e4743":"markdown","7d70be7c":"markdown","8a756d4a":"markdown","7f8c40b1":"markdown","6aba26d3":"markdown","f52383e0":"markdown","f065b97e":"markdown","b4314bed":"markdown","7a9f0684":"markdown","2ea67f03":"markdown","cec8e3fb":"markdown","2d5ad258":"markdown","d271ddef":"markdown","4e60fc62":"markdown"},"source":{"75cb8e78":"!pip install pymannkendall\n!pip install pyhomogeneity\n!pip install pmdarima","1aad7641":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport datetime\nimport math\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\nimport seaborn as sns\n\n# stats and tests\nimport pymannkendall as mk\nimport pyhomogeneity as hg\nfrom scipy import stats\n\n# for imputation of missing values\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# for SARIMAX modeling\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import mse,rmse\nfrom pmdarima.arima import ndiffs\n\n# for LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nimport keras as ks\nimport kerastuner\nfrom kerastuner.tuners import BayesianOptimization\nfrom kerastuner import Objective","c8cda0e4":"auser = pd.read_csv('..\/input\/acea-water-prediction\/Aquifer_Auser.csv',\n                    parse_dates = ['Date'],\n                    low_memory=False,\n                    index_col = ['Date'])\n\nauser = auser.sort_index()\nprint(f'Data loaded for Aquifer Auser\\n start date: {auser.index.min()}\\n end date  : {auser.index.max()}\\n')\nprint('================================================================')\nprint('')\n\nprint('Many missing values at the beginning of series.\\n')\nprint('Will trim original dataset to begin on 1st not null value of the target variable LT2.\\n')\nstart_date = auser[auser['Depth_to_Groundwater_LT2'].notnull()]\\\n                        ['Depth_to_Groundwater_LT2'].index.min()\n\nauser = auser[auser.index>=start_date].copy()\nprint('Aquifer Auser dataset revised start date: ', start_date)\nprint('')\nprint('Auser dataset shape: ', auser.shape)\nprint('================================================================')","2d018189":"# Group variable names into custom lists\ntargets = ['Depth_to_Groundwater_LT2',\n           'Depth_to_Groundwater_SAL',\n           'Depth_to_Groundwater_CoS']\n\nother_wells = ['Depth_to_Groundwater_PAG', \n               'Depth_to_Groundwater_DIEC']\n\n# List of variables - Railnfall\nrainfall = ['Rainfall_Gallicano', \n            'Rainfall_Pontetetto', \n            'Rainfall_Monte_Serra', \n            'Rainfall_Orentano',\n            'Rainfall_Borgo_a_Mozzano', \n            'Rainfall_Piaggione', \n            'Rainfall_Calavorno', \n            'Rainfall_Croce_Arcana', \n            'Rainfall_Tereglio_Coreglia_Antelminelli', \n            'Rainfall_Fabbriche_di_Vallico']\n\n# List of variables - Temperature\ntemp = ['Temperature_Orentano',\n        'Temperature_Monte_Serra', \n        'Temperature_Ponte_a_Moriano', \n        'Temperature_Lucca_Orto_Botanico']\n\n# List of variables - Extaction Volumes\nvol = ['Volume_POL', \n       'Volume_CC1', \n       'Volume_CC2', \n       'Volume_CSA', \n       'Volume_CSAL']\n\n# List if variables - Hydrometry\nhyd = ['Hydrometry_Monte_S_Quirico',\n      'Hydrometry_Piaggione']\n\n#================================================================\n\n# Custom names for plots and sections\n\ntarget1 = 'South sector confined aquifer LT2 well'\ntarget2 = 'North sector unconfined aquifer SAL well'\ntarget3 = 'North sector unconfined aquifer CoS well'\n#================================================================\n\n# Custom Functions\n\n# to construct boxplot of annual values\ndef boxplot_annual(dataset, label):\n    df = dataset[[label]] # label - column name\n    df['Year'] = df.index.year\n    df_piv = df.pivot(columns='Year', values=label)\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    df_piv.plot(ax=ax, kind='box', grid=True)\n    plt.title('Annual distribution of ' + (label))\n    fig.tight_layout()\n    return plt.show();\n\n# to construct boxplot of monthly values\ndef boxplot_monthly_by_year(dataset, label, year):\n    df = dataset[dataset.index.year==year][[label]] # label - column name \n    df['Month'] = df.index.month\n    df_piv = df.pivot(columns='Month', values=label)\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    df_piv.plot(ax=ax, kind='box', grid=True)\n    plt.title('Monthly distribution of ' + (label) + ' for year: '+ str(year))\n    fig.tight_layout()\n    return plt.show();\n\n# correlation heatmap \n# acknowledgment to https:\/\/github.com\/webartifex\/ames-housing\/blob\/main\/2_pairwise_correlations.ipynb\ndef plot_correlation(dataset, title):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_title(title, fontsize=14)\n    # Blank out the upper triangular part of the matrix.\n    mask = np.zeros_like(dataset, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    # Use a diverging color map.\n    cmap = sns.diverging_palette(240, 0, as_cmap=True)\n    # Adjust the labels' font size.\n    labels = dataset.columns\n    ax.set_xticklabels(labels, fontsize=10)\n    ax.set_yticklabels(labels, fontsize=10)\n    # Plot it.\n    sns.heatmap(\n        dataset, vmin=-1, vmax=1, cmap=cmap, center=0, linewidths=.5,\n        cbar_kws={\"shrink\": .5}, square=True, \n        mask=mask, ax=ax)\n\n# collecting values into list\ndef to_list(values):\n    list=[]\n    for val in values:\n        for v in val:\n            list.append(v)\n    return(list)\n\n\n# forecast accuracy and scores\ndef forecast_accuracy(forecast, actual):\n    mape = np.mean(np.abs(list(np.array(forecast) - np.array(actual)))\/np.abs(actual)) # mean absolute percentage error\n    mape = float(\"{0:.4f}\".format(mape))\n    mse = np.mean(np.square(list(np.array(forecast) - np.array(actual)))) # mean squared error\n    mse = float(\"{0:.4f}\".format(mse))\n    mae = np.mean(np.abs(list(np.array(forecast) - np.array(actual)))) # mean absolute error\n    mae = float(\"{0:.4f}\".format(mae))\n    rmse = np.mean(list(np.array(list(np.array(forecast) - np.array(actual)))**2))**.5\n    rmse = float(\"{0:.4f}\".format(rmse))\n    \n    r = stats.linregress(forecast, actual)\n    R2 = float(\"{0:.4f}\".format(r.rvalue ** 2))\n    return({'mape':mape,'mae': mae, 'mse':mse, 'rmse':rmse, 'R2': R2})\n\nprint('================================================================')\nprint('')\ncustom_functions = ['boxplot_annual',\n                    'boxplot_monthly_by_year', \n                    'plot_correlation', 'to_list', \n                    'forecast_accuracy']\nprint('Custom functions:\\n', custom_functions)\nprint('================================================================')\nprint('')","98d60110":"# Statistical summary for Targets:\nprint('Statistical summary for Targets:')\nprint(auser[targets].describe().T, '\\n')\nprint('Zero values will be replaced with nan')\nprint('================================================================')\nprint('')\n\n# Statistical summary for Other Wells:\nprint('Statistical summary for Other wells:')\nprint(auser[other_wells].describe().T, '\\n')\nprint('These features will not be used in modeling')\nprint('================================================================')\nprint('')\n\n# Statistical summary for Rainfall:\nprint('Statistical summary for Rainfall:')\nprint(auser[rainfall].describe().T, '\\n')\nprint(\"\"\"6 missing values Rainfall_Monte_Serra will be filled by interpolation \n365 missing values for Rainfall_Piaggione by advanced sklearn method\"\"\")\nprint('================================================================')\nprint('')\n\n# Statistical summary for Temp:\nprint('Statistical summary for Temperatures:')\nprint(auser[temp].describe().T, '\\n')\nprint(\"\"\"Look at 25% and 75% of data: \nUnlikely distributions of temp ranges, \nwill visualize data\"\"\")\nprint('================================================================')\nprint('')\n\nauser[temp].resample('M').mean().\\\n    plot(figsize = (8, 8), \n         kind='line', \n         sharex=False,\n         subplots=True, \n         xlabel='',\n         title='Monthly averages Temp')\nplt.tight_layout()\nplt.show();\n\nprint(\"\"\"Too many zero temp days for Monte Serra and Monteriano.\nIn some years for all temp Bimodel picks \nindicate equally hot days in winter and summer.\nIn summary, records look too corrupted \nto be usefull in the modeling\"\"\")\nprint('================================================================')\nprint('')\n\n# Statistical summary for Volumes:\nprint('Statistical summary for Volumes:')\nprint(auser[vol].describe().T, '\\n')\nprint('================================================================')\nprint('')\n\n# Statistical summary for Hydrometry:\nprint('Statistical summary for Hydrometry:')\nprint(auser[hyd].describe().T, '\\n')\nprint('Too many missing values for Hydrometry_Piaggione, will not use this feature')\nprint('================================================================')\nprint('')","052ebeaa":"auser['Rainfall_Monte_Serra'].interpolate(inplace=True)\n\n# following takes about 2 mins\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#from sklearn.ensemble import RandomForestRegressor\n\nX1 = auser[targets + rainfall + vol + [hyd[0]]].copy() # will not use other_wells\n\nimp = IterativeImputer(RandomForestRegressor(), max_iter=30, random_state=1)\nresult_X1 = imp.fit_transform(X1)\nresult_X1 = pd.DataFrame(result_X1, columns=X1.columns, index= X1.index)\n\n# cleaned dataset without imputation\nauser = auser[targets + rainfall + vol + [hyd[0]]].copy()\n# without imputed values\nauser_imputed = result_X1.copy()","fb5f0f97":"# visualize imputation\nprint('================================================================')\nprint('')\nprint('Rainfall_Piaggione')\n\nf, (ax1, ax2) = plt.subplots(2,1, figsize=(8, 6), sharex=True)\n\nax1.plot(auser['Rainfall_Piaggione'])\nax1.set_ylabel('mm')\nax1.set_title('Rainfall_Piaggione with nan values')\n\nax2.plot(result_X1['Rainfall_Piaggione'], color='red')\nax2.set_ylabel('mm')\nax2.tick_params(axis = 'x', rotation = 45)\nax2.set_title('Rainfall_Piaggione with imputed values')\nplt.show();","e5536346":"# visualize imputation\nprint('================================================================')\nprint('')\nprint('Hydrometry_Monte_S_Quirico')\nf, ax = plt.subplots(1,1, figsize=(8, 3))\n\nax.plot(auser_imputed[(auser_imputed.index>='2008-01-01') & \n                      (auser_imputed.index<='2009-12-01')]\\\n                        ['Hydrometry_Monte_S_Quirico'],\n        linewidth=0, marker='x', \n        alpha=0.5, label='imputed values', color='red')\n\nax.plot(auser[(auser.index>='2008-01-01') & \n              (auser.index.year<=2010)]\\\n                ['Hydrometry_Monte_S_Quirico'],\n        linewidth=0.5, label='original series')\n\nax.set_ylabel('mtrs')\nax.set_title('Hydrometry_Monte_S_Quirico with imputed values')\nax.tick_params(axis = 'x', rotation = 45)\nax.yaxis.grid()\nplt.legend()\nplt.show();","52c20100":"# visualize imputation\nprint('================================================================')\nprint('')\nprint('Depth_to_Groundwater_LT2')\n\nf, ax3 = plt.subplots(1,1, figsize=(8, 3))\n\nax3.plot(auser[(auser.index>='2011-01-01') & \n                 (auser.index<='2014-01-01')]\\\n        ['Depth_to_Groundwater_LT2'],\n        label='original series')\n\nax3.plot(auser_imputed[(auser_imputed.index>='2011-01-01') & \n                      (auser_imputed.index<='2013-01-01')]\\\n        ['Depth_to_Groundwater_LT2'], \n        'r--', linewidth=0, marker='.', alpha=0.3,\n        label='imputed values')\n\nax3.set_ylabel('m')\nax3.set_title('Depth_to_Groundwater_LT2 with imputed values for 2011-2012')\nax3.tick_params(axis = 'x', rotation = 45)\nax3.yaxis.grid()\n\nplt.legend()\nplt.show();","111a0bf0":"f, ax4 = plt.subplots(1,1, figsize=(8, 3))\n\nax4.plot(auser_imputed[(auser_imputed.index>='2016-01-01') & \n                      (auser_imputed.index<='2016-12-31')]\\\n        ['Depth_to_Groundwater_LT2'], \n        'r--', linewidth=0.5, label='imputed values')\n\nax4.plot(auser[(auser.index>='2015-12-31') &\n              (auser.index<='2017-01-01')]\\\n        ['Depth_to_Groundwater_LT2'], label='original series')\n\nax4.set_ylabel('mtrs')\nax4.set_title('Depth_to_Groundwater_LT2 with imputed values for 2016 --')\nax4.tick_params(axis = 'x', rotation = 45)\nplt.legend()\nplt.show();","e3870b24":"f, (ax5, ax6) = plt.subplots(2,1, figsize=(8, 4), sharex=True)\n\nax5.plot(auser[(auser.index.year==2016)]['Depth_to_Groundwater_LT2'])\nax5.set_ylabel('mtrs')\nax5.set_title('Depth_to_Groundwater_LT2 with nan values, 2016')\n\nax6.plot(auser_imputed[(auser_imputed.index.year==2016)]\\\n         ['Depth_to_Groundwater_LT2'], color='red')\nax6.tick_params(axis = 'x', rotation = 45)\nax6.set_ylabel('mtrs')\nax6.set_title('Depth_to_Groundwater_LT2 with imputed values, 2016')\nplt.show();","0e917af8":"# visualize imputation\nprint('================================================================')\nprint('')\nprint('Depth_to_Groundwater_SAL')\nf, ax7 = plt.subplots(1,1, figsize=(8, 3))\n\nax7.plot(auser_imputed[(auser_imputed.index>='2006-01-01') & \n                      (auser_imputed.index<='2010-12-31')]\\\n        ['Depth_to_Groundwater_SAL'], \n        'r--', linewidth=0.4)\n\nax7.plot(auser[(auser.index>='2006-01-01') & \n              (auser.index<='2011-06-01')]\\\n        ['Depth_to_Groundwater_SAL'])\n\nax7.set_ylabel('mtr')\nax7.set_title('Depth_to_Groundwater_SAL with imputed values for 2006-2010 --')\nax7.tick_params(axis = 'x', rotation = 45)\nax7.yaxis.grid()\nplt.show();","46116eb2":"f, (ax8, ax9) = plt.subplots(2,1, figsize=(8, 4), sharex=True)\n\nax8.plot(auser[(auser.index.year<=2010) &\n               (auser.index.year>=2006)]\\\n         ['Depth_to_Groundwater_SAL'])\nax8.set_ylabel('mtrs')\nax8.set_title('Depth_to_Groundwater_SAL with nan values, 2011-2013')\n\nax9.plot(auser_imputed[(auser_imputed.index.year<=2010) & \n                       (auser_imputed.index.year>=2006)]\\\n         ['Depth_to_Groundwater_SAL'], color='red', \n         linewidth=0.5)\n\nax9.tick_params(axis = 'x', rotation = 45)\nax9.set_title('Depth_to_Groundwater_SAL with imputed values, 2011-2013')\nplt.show();","ad1b2372":"# visualize imputation\nprint('================================================================')\nprint('')\nprint('Depth_to_Groundwater_CoS')\nf, ax10 = plt.subplots(1,1, figsize=(8, 3))\n\nax10.plot(auser_imputed[(auser_imputed.index>='2006-01-01') & \n                      (auser_imputed.index<='2011-12-31')]\\\n        ['Depth_to_Groundwater_CoS'], \n        'r--', linewidth=0.4)\n\nax10.plot(auser[(auser.index>='2006-01-01') & \n              (auser.index<='2012-06-01')]\\\n        ['Depth_to_Groundwater_CoS'])\n\nax10.set_ylabel('mtrs')\nax10.set_title('Depth_to_Groundwater_CoS with imputed values for 2006-2010 --')\nax10.tick_params(axis = 'x', rotation = 45)\nax10.yaxis.grid()\nplt.show();","653feab6":"f, (ax11, ax12) = plt.subplots(2,1, figsize=(8, 4), sharex=True)\n\nax11.plot(auser[(auser.index.year<=2008) &\n               (auser.index.year>=2006)]\\\n         ['Depth_to_Groundwater_CoS'])\nax11.set_ylabel('mtrs')\nax11.set_title('Depth_to_Groundwater_CoS with nan values, 2011-2013')\n\nax12.plot(auser_imputed[(auser_imputed.index.year<=2008) & \n                       (auser_imputed.index.year>=2006)]\\\n         ['Depth_to_Groundwater_CoS'], color='red', linewidth=0.3)\n\nax12.tick_params(axis = 'x', rotation = 45)\nax12.set_title('Depth_to_Groundwater_CoS with imputed values, 2011-2013')\nplt.show();","f4c1498d":"##### DESCRIPTIVE DATA ANALYSIS #####\nprint('Examine Overall Annual Trend for groundwater levels in target wells:')\nprint('================================================================')\n\n# LT2\n#import pymannkendall as mk\n\n# *Credit to Hussain et al., (2019). pyMannKendall: \n# a python package for non parametric Mann Kendall \n# family of trend tests.\n# Journal of Open Source Software, 4(39), 1556, \n# https:\/\/doi.org\/10.21105\/joss.01556*\n\n# Plot annual trend of LT2, resample to Monthly average\n\nfig, ax = plt.subplots(figsize=(8, 3))\n\n# Mann Kendall test to plot trend line\nres = mk.seasonal_test(auser_imputed.resample('M').mean()\\\n                       [[targets[0]]], period=12)\ntrend_line = np.arange(len(\n    auser_imputed.resample('M').mean()\\\n    [[targets[0]]])) \/ 12 * res.slope + res.intercept\n\nax.plot(auser_imputed.resample('M').mean()\\\n        [[targets[0]]])\nax.plot(auser_imputed.resample('M').mean().index, \n        trend_line, color='r')\n\nax.set_title(f'{target1}, annual trend')\nax.set_ylabel('mtrs from the ground floor\\ndetected by the piezometer')\nax.legend(['data', 'trend line']);\nplt.show();\n\nprint('Run homogeneity test to identify if there is shift in distribution:')\nprint('')\n\n#import pyhomogeneity as hg\nh, cp, p, U, avg = hg.pettitt_test(auser_imputed[targets[0]])\nprint('LT2 is non homogeneous: ', h)\nprint('Probable break point occured close to: ', cp)\nprint('Mean at before and after the change point :', avg)\n\nprint('')\n# Comments to observation\nprint(\"\"\"There is increasing trend in groundwater level \nand trend is not constant.\"\"\")","7d35571d":"print('================================================================')\n# SAL\n\n# Plot annual trend of SAL, resample to Monthly average\nfig, ax1 = plt.subplots(figsize=(8, 3))\n\nres = mk.seasonal_test(auser_imputed.resample('M').mean()\\\n                       [[targets[1]]], period=12)\ntrend_line = np.arange(len(\n    auser_imputed.resample('M').mean()\\\n    [[targets[1]]])) \/ 12 * res.slope + res.intercept\n\nax1.plot(auser_imputed.resample('M').mean()\\\n         [[targets[1]]])\nax1.plot(auser_imputed.resample('M').mean().index, \n         trend_line, color='r')\nax1.set_title(f'{target2}, annual trend')\nax1.set_ylabel('mtrs from the ground floor\\ndetected by the piezometer')\nax1.legend(['data', 'trend line'])\nplt.show();\n\n#import pyhomogeneity as hg\nh, cp, p, U, avg = hg.pettitt_test(auser_imputed[targets[1]])\nprint('SAL is non homogeneous: ', h)\nprint('Probable break point occured close to: ', cp)\nprint('Mean at before and after the change point :', avg)","aa9b94f2":"print('================================================================')\n# CoS\n\n\n# Plot annual trend of CoS, resample to Monthly average\nfig, ax2 = plt.subplots(figsize=(8, 3))\nres = mk.seasonal_test(auser_imputed.resample('M').mean()\\\n                       [[targets[2]]], period=12)\ntrend_line = np.arange(len(\n    auser_imputed.resample('M').mean()\\\n    [[targets[2]]])) \/ 12 * res.slope + res.intercept\n\nax2.plot(auser_imputed.resample('M').mean()\\\n         [[targets[2]]])\nax2.plot(auser_imputed.resample('M').mean().index, \n         trend_line, color='r')\nax2.set_title(f'{target3}, annual trend')\nax2.set_ylabel('mtrs from the ground floor\\ndetected by the piezometer')\nax2.legend(['data', 'trend line'])\nplt.show();\n\n#import pyhomogeneity as hg\nh, cp, p, U, avg = hg.pettitt_test(auser_imputed[targets[2]])\nprint('CoS is non homogeneous: ', h)\nprint('Probable break point occured close to: ', cp)\nprint('Mean at before and after the change point :', avg)","7950d6bf":"##### DESCRIPTIVE DATA ANALYSIS #####\nprint('================================================================')\nprint(f'Seasonality for groundwater levels @ {target1}:')","6cf75cb3":"for y in [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]:\n    df2_year = auser_imputed[(auser_imputed.index.year==y)]\n    f, ax = plt.subplots(figsize=(8, 3))\n    sns.boxplot(x=df2_year.index.month, y=df2_year[targets[0]])\n    ax.set_xlabel('')\n    ax.set_ylabel('mtrs')\n    ax.set_title(f'{target1}, Monthly distribution for: {y}')\nplt.show();\n\nyears = [2006, 2007, 2008, \n         2009, 2010, 2011, \n         2012, 2013, 2014, \n         2015, 2016, 2017, \n         2018, 2019]\nhigh_m = [4, 3, 5, 4, 12, 4, 4, 5, 4, 3, 5, 4, 5, 5]\nlow_m = [11, 9, 10, 9, 1, 3, 3, 1, 1, 10, 12, 9, 9, 10]\n\nLT2_hl = pd.DataFrame(list(zip(high_m, low_m)), \n               columns =['High', 'Low'], index= years) \n\nprint(f'Observations of annual Highs and Lows (Month) for {target1}')\n\n\nprint(LT2_hl)\nprint('================================================================')","3749068f":"print('================================================================')\nprint(f'Seasonality for groundwater levels @ {target2}:')","4cfd4a67":"for y in [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]:\n    df2_year = auser_imputed[(auser_imputed.index.year==y)]\n\n    f, ax = plt.subplots(figsize=(8, 3))\n    sns.boxplot(x=df2_year.index.month, y=df2_year[targets[1]])\n    ax.set_xlabel('')\n    ax.set_ylabel('mtrs')\n    ax.set_title(f'{target2}, Monthly distribution for: {y}')\nplt.show();\n\nyears = [2006, 2007, 2008, \n         2009, 2010, 2011, \n         2012, 2013, 2014, \n         2015, 2016, 2017, \n         2018, 2019]\n\nhigh2_m = [4, 2, 12, 4, 11, 3, 12, 3, 2, 2, 2, 12, 3, 12 ]\nlow2_m =  [7, 4, 10, 7, 9, 10, 7, 9, 6, 9, 4, 7, 10, 10 ]\n\nSAL_hl = pd.DataFrame(list(zip(high2_m, low2_m)), \n               columns =['High', 'Low'], index= years)\n\nprint(f'Observations of annual Highs and Lows (Month) for {target2}')\nprint('')\n\nprint(SAL_hl)\nprint('================================================================')","b286b2fb":"print('================================================================')\nprint(f'Seasonality for groundwater levels @ {target3}:')","cef25c3d":"for y in [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]:\n    df2_year = auser_imputed[(auser_imputed.index.year==y)]\n\n    f, ax = plt.subplots(figsize=(8, 3))\n    sns.boxplot(x=df2_year.index.month, y=df2_year[targets[2]])\n    ax.set_xlabel('')\n    ax.set_ylabel('mtrs')\n    ax.set_title(f'{target3}, Monthly distribution for: {y}')\n    plt.show();\n\nyears = [2006, 2007, 2008, \n         2009, 2010, 2011, \n         2012, 2013, 2014, \n         2015, 2016, 2017, \n         2018, 2019]\n\nhigh3_m = [3, 2, 12, 4, 5, 3, 12, 3, 2, 2, 3, 3, 3, 5]\nlow3_m =  [9, 8, 9, 9, 9, 9, 9, 7, 6, 9, 9, 8, 9, 9]\n\nCoS_hl = pd.DataFrame(list(zip(high3_m, low3_m)), \n               columns =['High', 'Low'], index= years)\n\nprint(f'Observations of annual Highs and Lows (Month) for {target3}')\nprint('')\n\nprint(CoS_hl)\nprint('================================================================')","9271594c":"print('============================================================')\nprint(f'Daily\/weekly fluctations for levels @ {target1}:')\n\nprint('Plot for each month of the latest complete year 2019')","306ce099":"#2019 year\nauser_imputed[[targets[0]]].loc['2019-01-01': '2019-01-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-02-01': '2019-02-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-03-01': '2019-03-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-04-01': '2019-04-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-05-01': '2019-05-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-06-01': '2019-06-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-07-01': '2019-07-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-08-01': '2019-08-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-09-01': '2019-09-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-10-01': '2019-10-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-11-01': '2019-11-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[0]]].loc['2019-12-01': '2019-12-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nplt.show();","7c51d469":"print(f'Daily\/weekly fluctations for levels @ {target2}:')\nprint('')\nprint('Plot for each month of the latest complete year 2019')","36507c72":"#2019 year\nauser_imputed[[targets[1]]].loc['2019-01-01': '2019-01-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-02-01': '2019-02-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-03-01': '2019-03-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-04-01': '2019-04-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-05-01': '2019-05-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-06-01': '2019-06-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-07-01': '2019-07-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-08-01': '2019-08-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-09-01': '2019-09-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-10-01': '2019-10-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-11-01': '2019-11-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[1]]].loc['2019-12-01': '2019-12-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nplt.show();","76fbb7cc":"print(f'Daily\/weekly fluctations for levels @ {target3}:')\nprint('')\nprint('Plot for each month of the latest complete year 2019')","0c222c31":"#2019 year\nauser_imputed[[targets[2]]].loc['2019-01-01': '2019-01-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-02-01': '2019-02-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-03-01': '2019-03-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-04-01': '2019-04-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-05-01': '2019-05-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-06-01': '2019-06-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-07-01': '2019-07-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-08-01': '2019-08-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-09-01': '2019-09-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-10-01': '2019-10-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-11-01': '2019-11-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nauser_imputed[[targets[2]]].loc['2019-12-01': '2019-12-28'].plot(marker = 'o', figsize=(8, 2), xlabel='');\nplt.show();","7f9d727f":"print('================================================================')\nprint(f'Examine Impact of rainfalls on groundwater levels @ {target1}:')\nprint('')\nprint('Period of dry (2011-2013) and rainy (2014-2016) years')\n\nauser_imputed[(auser_imputed.index.year>=2011)\n              & (auser_imputed.index.year<=2016)].\\\nresample('M').mean()\\\n[[targets[0]] + rainfall].plot(subplots=True,\n                              linewidth=1,\n                              figsize=(8, 20),\n                              sharex=False, \n                              xlabel='')\nplt.tight_layout()\nplt.show();\nprint('')\nprint('There seems to be about 50 days lag between Rainfall and aquifer recharge.')","40394446":"print('================================================================')\nprint(f'Examine Impact of rainfalls on groundwater levels @ {target2}:')\n\nauser_imputed[(auser_imputed.index.year>=2011)\n              & (auser_imputed.index.year<=2016)].\\\nresample('M').mean()\\\n[[targets[1]] + rainfall].plot(subplots=True,\n                              linewidth=1,\n                              figsize=(8, 20),\n                              sharex=False, \n                              xlabel='')\nplt.tight_layout()\nplt.show();","a0a17888":"print('================================================================')\nprint(f'Examine Impact of rainfalls on groundwater levels @ {target3}:')\n\nauser_imputed[(auser_imputed.index.year>=2011)\n              & (auser_imputed.index.year<=2016)].\\\nresample('M').mean()\\\n[[targets[2]] + rainfall].plot(subplots=True,\n                              linewidth=1,\n                              figsize=(8, 20),\n                              sharex=False, \n                              xlabel='')\nplt.tight_layout()\nplt.show();","25f49744":"print(f'Examine Impact of pumping on groundwater levels @ {target1}:')\n\nfor v in vol:\n    auser_imputed[v] = abs(auser_imputed[v])\n    auser_imputed.resample('M').mean()\\\n    [[targets[0], v]].plot(subplots=True,\n                           grid=True,\n                           figsize=(8, 3),\n                           sharex=False, \n                           xlabel='')\n    plt.tight_layout()\n    plt.show();","5cde3c29":"print(f'Examine Impact of pumping on groundwater levels @ {target2}:')\n\nfor v in vol:\n    auser_imputed[v] = abs(auser_imputed[v])\n    auser_imputed.resample('M').mean()\\\n    [[targets[1], v]].plot(subplots=True,\n                           grid=True,\n                           figsize=(8, 3),\n                           sharex=False, \n                           xlabel='')\n    plt.tight_layout()\n    plt.show();","aba0dd2d":"print(f'Examine Impact of pumping on groundwater levels @ {target3}:')\n\nfor v in vol:\n    auser_imputed[v] = abs(auser_imputed[v])\n    auser_imputed.resample('M').mean()\\\n    [[targets[2], v]].plot(subplots=True,\n                           grid=True,\n                           figsize=(8, 3),\n                           sharex=False, \n                           xlabel='')\n    plt.tight_layout()\n    plt.show();","4c5bbc9d":"print(f'Examine Correlation between all the variables:')\n\npearson = auser_imputed[targets+rainfall+vol].corr(method='pearson')\nplot_correlation(pearson, 'Pearson corr')\nplt.show();","f61b3c70":"# Based on the DA the following features might be used in modeling\n\n# Rainfall 50 days lagged values\nfor r in rainfall:\n    auser_imputed[r+'_50_d_lag'] = auser_imputed[r].shift(50)\n\n# group in the list\nrain_50d_lag = ['Rainfall_Gallicano_50_d_lag',\n                'Rainfall_Pontetetto_50_d_lag',\n                'Rainfall_Monte_Serra_50_d_lag',\n                'Rainfall_Orentano_50_d_lag',\n                'Rainfall_Borgo_a_Mozzano_50_d_lag',\n                'Rainfall_Piaggione_50_d_lag',\n                'Rainfall_Calavorno_50_d_lag',\n                'Rainfall_Croce_Arcana_50_d_lag',\n                'Rainfall_Tereglio_Coreglia_Antelminelli_50_d_lag',\n                'Rainfall_Fabbriche_di_Vallico_50_d_lag']\n\nauser_imputed['total_rain'] = auser_imputed[rainfall].sum(axis=1)\nauser_imputed['total_rain_50_d_lag'] = auser_imputed['total_rain'].shift(50)\n\n# add month and week of the year\nauser_imputed['Month_No'] = auser_imputed.index.month\nauser_imputed['Week_No'] = auser_imputed.index.week\n\n# add column Recharge 'High_LT2' to indicate if month is in 'recharge season'\n# refer to data analysis part\nhigh_month_LT2 = [4,5]\nauser_imputed['High_recharge_LT2'] = np.where(auser_imputed.index.month.isin(high_month_LT2), 1, 0)\n\n# add column Recharge 'High_SAL' to indicate if month is in 'recharge season'\n# refer to data analysis part\nhigh_month_SAL = [2, 3, 11, 12]\nauser_imputed['High_recharge_SAL'] = np.where(auser_imputed.index.month.isin(high_month_SAL), 1, 0)\n\n# add column Recharge 'High_CoS' to indicate if month is in 'recharge season'\n# refer to data analysis part\nhigh_month_CoS = [2, 12, 11, 3]\nauser_imputed['High_recharge_CoS'] = np.where(auser_imputed.index.month.isin(high_month_CoS), 1, 0)\n\n# final dataset with added features\nauser_imputed = auser_imputed.dropna()","2c166a86":"target1 = 'South sector confined aquifer LT2 well'\n\n##### MODELING WITH SARIMAX #####\nprint(f'Modeling GWL @ {target1} with SARIMAX')\nprint('================================================================')\nprint('================================================================')\n\n# split data on training\/test\/future\n# future set will be used to demonstrate forecast with the model\nprint('Split data: ')\n\nLT2 = auser_imputed[auser_imputed.index <= '2019-12-07']\nLT2.dropna(inplace=True)\n\nn = len(LT2)\n\ntrain = LT2.iloc[:int(0.90*n), ]\ntrain = train.asfreq(freq='D')\n\ntest = LT2.iloc[int(0.90*n):, ]\ntest = test.asfreq(freq='D')\n\nfuture = auser_imputed[(auser_imputed.index > '2019-12-07') & \n                       (auser_imputed.index <='2020-01-06')] #data after 2020-01-06 is not continuous\nfuture = future.asfreq(freq='D')\nfuture.dropna(inplace=True)\n\nprint(f'train set: {train.shape}, test set: {test.shape}, forecast shape: {future.shape}')\nprint('================================================================')\nprint('')\n\nprint('Seasonal decomposition')\nprint('')\nplt.rcParams['figure.figsize'] = (8, 8)\ndecomp = seasonal_decompose(train[targets[0]], freq = 365) # data is Daily\ndecomp.plot()\nplt.show();\nprint('================================================================')\n\nprint('Estimating the differencing term to remove trend:\\n')\nprint('Apply ADF\/KPSS test to select optimal d:')\n#from pmdarima.arima import ndiffs\nkpss_diffs = ndiffs(train[targets[0]], alpha=0.05, test='kpss', max_d=6)\nadf_diffs = ndiffs(train[targets[0]], alpha=0.05, test='adf', max_d=6)\nn_diffs = max(adf_diffs, kpss_diffs)\n\nprint(f\"Estimated differencing term: {n_diffs}\")\nprint('================================================================')\n\nprint('Estimating SARIMA(X) hyperparameters')\n# There is a method for automatic grid search for hyperparameters (p\/d\/q)x(P\/D\/Q)s\n# However the method is very computationally expensive\n# and results are not guaranteed to converge.\n# I share in the opinion that better and systematic way to estimate hyperparameters\n# is to examine ACF\/PACF plots for significant lags\nprint('')\nprint('Plot ACF and PACF of differenced series')\nprint('Identify significant lags')\n\n#ACF and PACF with differenced series\ntrain_diff = train[[targets[0]]].diff(1).dropna()\n\nfig, axes = plt.subplots(2, 1, figsize=(8,6))\n\nfig = sm.graphics.tsa.plot_acf(train_diff, lags=100, \n                               ax=axes[0],zero=False)\nfig = sm.graphics.tsa.plot_pacf(train_diff, lags=100, \n                                ax=axes[1], zero=False)\nplt.tight_layout()\nplt.show();\nprint('================================================================')\n\nprint('Both plots roughly repeat significant lags at 30-31 day perediocity;')\nprint('will set s=30')\nprint('')\nprint('ACF: 1st lag is significant q=1, and again at 30, will set Q=1')\nprint('')\nprint('PACF: will try p=2 (on the grounds of simplicity) and P=1')\nprint('')\nprint('d is already estimated d=1, will set D=0.')\nprint('================================================================')","e71808df":"print(f\"Train SARIMA model for {target1} w\/o exogeneous variables\")\n\n# GLOBAL LT2 SARIMA(X) HYPERPARAMETERS\np_LT2 = 2\nd_LT2 = 1\nq_LT2 = 1\nP_LT2 = 1\nD_LT2 = 0\nQ_LT2 = 1\ns_LT2 = 30\n\n# initialize and fit model\nmodel_SARIMA = SARIMAX(train[targets[0]], # targets[0] - GWL LT2\n                       order=(p_LT2, d_LT2, q_LT2),\n                       seasonal_order=(P_LT2, D_LT2, Q_LT2, s_LT2),\n                       enforce_invertibility=False)\n\nmodel_SARIMA_fit = model_SARIMA.fit(disp=0)\nprint(model_SARIMA_fit.summary().tables[1])\nprint('')\n# If the p-value is less than or equal to the significance level, \n# we can conclude that the coefficient is statistically significant.\n# Usually, a significance level of 0.05 works well\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMA_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\n\nprint('')\nprint(\"Diagnostics plots look almost normal\")\nprint(\"Irregularities might be due to the outliers and imputed values\")\nprint(\"Will proceed with this model hyperparameters.\")\n\nprint('================================================================')\nprint('')\n\nprint(f\"Validate SARIMA (({p_LT2},{d_LT2},{q_LT2}),({P_LT2},{D_LT2},{Q_LT2},{s_LT2})) model on the test set\")\nprint('')\n# use fiited model to predict values from the test set\npredictionSAR = model_SARIMA_fit.forecast(len(test))\n\n# prediction confidence intervals\nforecast_SARIMA_conf = model_SARIMA_fit.get_forecast(len(test))\nlower_val = forecast_SARIMA_conf.conf_int()[['lower Depth_to_Groundwater_LT2']].values\nupper_val = forecast_SARIMA_conf.conf_int()[['upper Depth_to_Groundwater_LT2']].values\nlower_series = pd.Series(to_list(lower_val), index=predictionSAR.index) # custom function\nupper_series = pd.Series(to_list(upper_val), index=predictionSAR.index) # custom function\n\n# Plot prediction results\nplt.figure(figsize=(8,4), dpi=80)\n# complete series\nplt.plot(train[train.index.year > 2016][targets[0]],\n         label = 'Train Values')\n# actual values from test set\nplt.plot(test[targets[0]], \n         label = 'Test Values')\n# predicted values\nplt.plot(predictionSAR, color='darkgreen', \n         label = 'Predicted Values')\n# confidence intervals\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"In-sample prediction for GWL @ {target1}\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"SARIMA (({p_LT2},{d_LT2},{q_LT2}),({P_LT2},{D_LT2},{Q_LT2},{s_LT2})) goodness of fit: \")\n\nSarima_LT2_accuracy = forecast_accuracy(predictionSAR.values, test[targets[0]].values)\n\nprint(Sarima_LT2_accuracy)\nprint('================================================================')","c458ad07":"print(f\"Next Train SARIMAX Model for {target1} with exogeneous variables\")\nprint('')\n\n# after dtrying different regressors, the following choice showed\n# best results in terms of AIC criteria and significance of\n# regressors' coeffients\n\np_LT2_ex = 1\nd_LT2_ex = 1\nq_LT2_ex = 1\nP_LT2_ex = 1\nD_LT2_ex = 0\nQ_LT2_ex = 0\ns_LT2_ex = 30\n\nmodel_SARIMAX = SARIMAX(train[targets[0]],\n                        exog=train[['Volume_POL','Volume_CSAL'] +\n                                   ['Depth_to_Groundwater_CoS'] + \n                                   ['total_rain']],\n                        order=(p_LT2_ex, d_LT2_ex, q_LT2_ex),\n                        seasonal_order=(P_LT2_ex, D_LT2_ex, Q_LT2_ex, s_LT2_ex),\n                        enforce_invertibility=False)\n\nmodel_SARIMAX_fit=model_SARIMAX.fit(disp=0)\nprint(model_SARIMAX_fit.summary().tables[1])\nprint('')\n\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMAX_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\n\nprint('================================================================')","bff5a9b1":"print(f\"Validate SARIMAX (({p_LT2_ex},{d_LT2_ex},{q_LT2_ex}),({P_LT2_ex},{D_LT2_ex},{Q_LT2_ex},{s_LT2_ex})) model on test set\")\nprint('')\n# use fiited model to predict values from test set\npredictionSARX = model_SARIMAX_fit.forecast(len(test),\n                                            exog=test[['Volume_POL','Volume_CSAL'] +\n                                                      ['Depth_to_Groundwater_CoS'] + \n                                                      ['total_rain']])\n\n# prediction confidence intervals\nforecast_SARIMAX_conf = model_SARIMAX_fit.get_forecast(len(test),\n                                                       exog=test[['Volume_POL','Volume_CSAL'] +\n                                                                 ['Depth_to_Groundwater_CoS'] + \n                                                                 ['total_rain']])\n\nlower_val = forecast_SARIMAX_conf.conf_int()[['lower Depth_to_Groundwater_LT2']].values\nupper_val = forecast_SARIMAX_conf.conf_int()[['upper Depth_to_Groundwater_LT2']].values\nlower_series = pd.Series(to_list(lower_val), index=predictionSARX.index) # custom function\nupper_series = pd.Series(to_list(upper_val), index=predictionSARX.index) # custom function\n\n# Plot \nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(train[train.index.year > 2016][targets[0]], \n         label = 'Train Values')\nplt.plot(test[targets[0]], \n         label = 'Test Values')\nplt.plot(predictionSARX, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target1} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\n\nprint(f\"SARIMAX (({p_LT2_ex},{d_LT2_ex},{q_LT2_ex}),({P_LT2_ex},{D_LT2_ex},{Q_LT2_ex},{s_LT2_ex})) goodness of fit: \")\nSarimax_LT2_accuracy = forecast_accuracy(predictionSARX, test[targets[0]])\n\nprint(Sarimax_LT2_accuracy)\nprint('')\n\nprint('================================================================')\nprint('')\nprint(\"Closer look at in-sample prediction\")\n\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(test[targets[0]], \n         label = 'Actual Values')\nplt.plot(predictionSARX, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target1} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"Comparison of SARIMA and SARIMAX for modeling GWL @ {target1}:\")\nprint('')\nprint('SARIMA:  ',Sarima_LT2_accuracy)\nprint('SARIMAX: ',Sarimax_LT2_accuracy)\nprint('')\nprint('SARIMA AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMA_fit.aic)))\nprint('SARIMAX AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMAX_fit.aic)))\nprint('================================================================')\nprint('')\nprint(\"\"\"Both models perform almost equally, AIC is lower for SARIMAX,\nwill proceed with SARIMAX for the final training and forecast.\"\"\")\nprint('================================================================')","cdeda0ba":"from numpy.random import seed\nseed(1)\n\nprint(\"Retrain SARIMAX on complete dataset and forecast\")\nprint('')\n\nLT2_model_SARIMAX = SARIMAX(LT2[targets[0]],\n                            exog=LT2[['Volume_POL','Volume_CSAL']+\n                                     ['Depth_to_Groundwater_CoS']+ \n                                     ['total_rain']], \n                            order=(p_LT2_ex, d_LT2_ex, q_LT2_ex),\n                            seasonal_order=(P_LT2_ex, D_LT2_ex, Q_LT2_ex, s_LT2_ex),\n                            enforce_invertibility=False)\n\nLT2_model_SARIMAX_fit = LT2_model_SARIMAX.fit(disp=0)\n\nprint(LT2_model_SARIMAX_fit.summary().tables[1])\nprint('')\n\nprint(\"Demonstrating forecast n-periods ahead\")\n\n# forecast shall start from the last date of LT2 dataset used\n# in training the model\ninput_data = future[['Volume_POL','Volume_CSAL']+\n                    ['Depth_to_Groundwater_CoS']+ \n                    ['total_rain']] # all features used in training\n\nLT2_forecast = LT2_model_SARIMAX_fit.forecast(len(future),\n                                                  exog=input_data)\n\n# prediction confidence intervals\nLT2_forecast_SARIMAX_conf = LT2_model_SARIMAX_fit.get_forecast(len(future),\n                                                      exog=input_data)\n\nlower_val = LT2_forecast_SARIMAX_conf.conf_int()[['lower Depth_to_Groundwater_LT2']].values\nupper_val = LT2_forecast_SARIMAX_conf.conf_int()[['upper Depth_to_Groundwater_LT2']].values\nlower_series = pd.Series(to_list(lower_val), index=LT2_forecast.index) # custom function\nupper_series = pd.Series(to_list(upper_val), index=LT2_forecast.index) # custom function\n\n# output list of forecasted values:\n#LT2_forecast\n\n# Plot\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(LT2[LT2.index >= '2019-10-31'][targets[0]])\nplt.plot(LT2_forecast, color='darkgreen', \n         label='forecast')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\nplt.title(f\"\"\"Out-of-sample Forecast for GWL \n@ {target1} with SARIMAX for {len(future)} days ahead\"\"\")\nplt.show();\n\nprint('')\nprint('===============================================================')\n\nprint(\"\"\"As we have actual values for forecasted (future) period, \nwe can evaluate forecasting accuracy:\"\"\")\nprint('')\n\nforecast_error = future[targets[0]] - LT2_forecast\nforecast_error.plot(kind='bar',figsize=(8,3), \n                    #xticks=(list(range(1,31))),\n                    xlabel='', \n                    ylabel='mtrs', grid=True, \n                    title='Forecast error, SARIMAX model')\nplt.show();\nprint('')\nprint(\"\"\"Forecasting is within +\/- 0.2 mtrs for first 24 days, \nbut worsens thereafter. We can conclude that model performs well\nfor 24 days ahead forecast.\"\"\")\nprint('')\nprint(f'=>END of modeling for {target1}<=')\nprint('===============================================================')","9349468e":"target2 = 'North sector unconfined aquifer SAL well'\n\n##### MODELING WITH SARIMAX #####\nprint(f'Modeling GWL @ {target2} with SARIMAX')\nprint('================================================================')\nprint('================================================================')\n\n# split data on training\/test\/future\n# future set will be used to demonstrate forecast with the model\nprint('Split data: ')\n\nSAL = auser_imputed[auser_imputed.index <= '2019-12-07']\nSAL.dropna(inplace=True)\n\nn = len(SAL)\n\ntrain = SAL.iloc[:int(0.90*n), ]\ntrain = train.asfreq(freq='D')\n\ntest = SAL.iloc[int(0.90*n):, ]\ntest = test.asfreq(freq='D')\n\nfuture = auser_imputed[(auser_imputed.index > '2019-12-07') & \n                       (auser_imputed.index <='2020-01-06')] #data after 2020-01-06 is not continuous\nfuture = future.asfreq(freq='D')\nfuture.dropna(inplace=True)\n\nprint(f'train set: {train.shape}, test set: {test.shape}, forecast shape: {future.shape}')\nprint('================================================================')\nprint('')\n\nprint('Seasonal decomposition')\nprint('')\nplt.rcParams['figure.figsize'] = (8, 8)\ndecomp = seasonal_decompose(train[targets[1]], freq = 365) # data is Daily\ndecomp.plot()\nplt.show();\nprint('================================================================')\nprint('')\n\nprint('Estimating the differencing term to remove trend:\\n')\nprint('Apply ADF\/KPSS test to select optimal d:')\n#from pmdarima.arima import ndiffs\nkpss_diffs = ndiffs(train[targets[1]], alpha=0.05, test='kpss', max_d=6)\nadf_diffs = ndiffs(train[targets[1]], alpha=0.05, test='adf', max_d=6)\nn_diffs = max(adf_diffs, kpss_diffs)\n\nprint(f\"Estimated differencing term: {n_diffs}\")\nprint('================================================================')\nprint('')\n\nprint('Estimating SARIMA(X) hyperparameters')\n# There is a method for automatic grid search for hyperparameters (p\/d\/q)x(P\/D\/Q)s\n# However the method is very computationally expensive\n# and results are not guaranteed to converge.\n# I share in the opinion that better and systematic way to estimate hyperparameters\n# is to examine ACF\/PACF plots for significant lags\nprint('')\nprint('Plot ACF and PACF of differenced series')\nprint('Identify significant lags')\n\n#ACF and PACF with differenced series\ntrain_diff = train[[targets[1]]].diff(1).dropna()\n\nfig, axes = plt.subplots(2, 1, figsize=(8,6))\n\nfig = sm.graphics.tsa.plot_acf(train_diff, lags=100, \n                               ax=axes[0],zero=False)\nfig = sm.graphics.tsa.plot_pacf(train_diff, lags=100, \n                                ax=axes[1], zero=False)\nplt.tight_layout()\nplt.show();\nprint('================================================================')\nprint('')\n\nprint('Both plots roughly repeat significant lags at 30-31 day perediocity;')\nprint('will set s=30')\nprint('')\nprint('ACF: 1st lag is significant q=1, will set Q=0')\nprint('')\nprint('PACF: will try p=2 (on the grounds of simplicity) and P=1')\nprint('')\nprint('d is already estimated d=1, will set D=0.')\nprint('================================================================')","f658d398":"print(f\"Train SARIMA model for {target2} w\/o exogeneous variables\")\n\n# GLOBAL SAL SARIMA(X) HYPERPARAMETERS\np_SAL = 2\nd_SAL = 1\nq_SAL = 1\nP_SAL = 1\nD_SAL = 0\nQ_SAL = 0\ns_SAL = 30\n\n# initialize and fit model\nmodel_SARIMA_2 = SARIMAX(train[targets[1]], # targets[1] - GWL SAL\n                       order=(p_SAL, d_SAL, q_SAL),\n                       seasonal_order=(P_SAL, D_SAL, Q_SAL, s_SAL),\n                       enforce_invertibility=False)\n\nmodel_SARIMA_2_fit = model_SARIMA_2.fit(disp=0)\nprint(model_SARIMA_2_fit.summary().tables[1])\nprint('')\n# If the p-value is less than or equal to the significance level, \n# we can conclude that the coefficient is statistically significant.\n# Usually, a significance level of 0.05 works well\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMA_2_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\n\nprint('')\nprint(\"Diagnostics plots look almost normal\")\nprint(\"Irregularities might be due to the outliers and imputed values\")\nprint(\"Will proceed with this model hyperparameters.\")\n\nprint('================================================================')\nprint('')\n\nprint(f\"Validate SARIMA (({p_SAL},{d_SAL},{q_SAL}),({P_SAL},{D_SAL},{Q_SAL},{s_SAL})) model on the test set\")\nprint('')\n# use fiited model to predict values from the test set\npredictionSAR_2 = model_SARIMA_2_fit.forecast(len(test))\n\n# prediction confidence intervals\nforecast_SARIMA_2_conf = model_SARIMA_2_fit.get_forecast(len(test))\nlower_val2 = forecast_SARIMA_2_conf.conf_int()[['lower Depth_to_Groundwater_SAL']].values\nupper_val2 = forecast_SARIMA_2_conf.conf_int()[['upper Depth_to_Groundwater_SAL']].values\nlower_series2 = pd.Series(to_list(lower_val2), index=predictionSAR_2.index) # custom function\nupper_series2 = pd.Series(to_list(upper_val2), index=predictionSAR_2.index) # custom function\n\n# Plot prediction results\nplt.figure(figsize=(8,4), dpi=80)\n# complete series\nplt.plot(train[train.index.year > 2016][targets[1]],\n         label = 'Train Values')\n# actual values from test set\nplt.plot(test[targets[1]], \n         label = 'Test Values')\n# predicted values\nplt.plot(predictionSAR_2, color='darkgreen', \n         label = 'Predicted Values')\n# confidence intervals\nplt.fill_between(lower_series2.index, \n                 lower_series2, \n                 upper_series2, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"In-sample prediction for GWL @ {target1}\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"SARIMA (({p_SAL},{d_SAL},{q_SAL}),({P_SAL},{D_SAL},{Q_SAL},{s_SAL})) goodness of fit: \")\n\nSarima_SAL_accuracy = forecast_accuracy(predictionSAR_2, test[targets[1]])\n\nprint(Sarima_SAL_accuracy)\nprint('================================================================')","20e21f96":"print(f\"Next Train SARIMAX Model for {target2} with exogeneous variables\")\nprint('')\n\n# after trying different regressors, the following choice showed\n# best results in terms of AIC criteria and significance of\n# regressors' coeffients\n\np_SAL_ex = 1\nd_SAL_ex = 1\nq_SAL_ex = 1\nP_SAL_ex = 2 #\nD_SAL_ex = 0\nQ_SAL_ex = 0\ns_SAL_ex = 30\n\nmodel_SARIMAX_2 = SARIMAX(train[targets[1]],\n                        exog=train[['Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL'] +\n                                   ['Depth_to_Groundwater_CoS'] + \n                                   ['total_rain']],\n                        order=(p_SAL_ex, d_SAL_ex, q_SAL_ex),\n                        seasonal_order=(P_SAL_ex, D_SAL_ex, Q_SAL_ex, s_SAL_ex),\n                        enforce_invertibility=False)\n\nmodel_SARIMAX_2_fit=model_SARIMAX_2.fit(disp=0)\nprint(model_SARIMAX_2_fit.summary().tables[1])\nprint('')\n\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMAX_2_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\n\n\nprint('================================================================')","d29ff637":"print(f\"Validate SARIMAX (({p_SAL_ex},{d_SAL_ex},{q_SAL_ex}),({P_SAL_ex},{D_SAL_ex},{Q_SAL_ex},{s_SAL_ex})) model on test set\")\nprint('')\n# use fiited model to predict values from test set\npredictionSARX_2 = model_SARIMAX_2_fit.forecast(len(test),\n                                            exog=test[['Volume_CC1', \n                                                       'Volume_CC2', \n                                                       'Volume_CSA', \n                                                       'Volume_CSAL'] +\n                                                      ['Depth_to_Groundwater_CoS'] + \n                                                      ['total_rain']])\n\n# prediction confidence intervals\nforecast_SARIMAX_2_conf = model_SARIMAX_2_fit.get_forecast(len(test),\n                                                       exog=test[['Volume_CC1', \n                                                       'Volume_CC2', \n                                                       'Volume_CSA', \n                                                       'Volume_CSAL'] +\n                                                      ['Depth_to_Groundwater_CoS'] + \n                                                      ['total_rain']])\n\nlower_val3 = forecast_SARIMAX_2_conf.conf_int()[['lower Depth_to_Groundwater_SAL']].values\nupper_val3 = forecast_SARIMAX_2_conf.conf_int()[['upper Depth_to_Groundwater_SAL']].values\nlower_series3 = pd.Series(to_list(lower_val3), index=predictionSARX_2.index) # custom function\nupper_series3 = pd.Series(to_list(upper_val3), index=predictionSARX_2.index) # custom function\n\n# Plot \nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(train[train.index.year > 2016][targets[1]], \n         label = 'Train Values')\nplt.plot(test[targets[1]], \n         label = 'Test Values')\nplt.plot(predictionSARX_2, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series3.index, \n                 lower_series3, \n                 upper_series3, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target2} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\n\nprint(f\"SARIMAX (({p_SAL_ex},{d_SAL_ex},{q_SAL_ex}),({P_SAL_ex},{D_SAL_ex},{Q_SAL_ex},{s_SAL_ex})) goodness of fit: \")\nSarimax_SAL_accuracy = forecast_accuracy(predictionSARX_2, test[targets[1]])\n\nprint(Sarimax_SAL_accuracy)\nprint('')\n\nprint('================================================================')\nprint('')\nprint(\"Closer look at in-sample prediction\")\n\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(test[targets[1]], \n         label = 'Actual Values')\nplt.plot(predictionSARX_2, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series3.index, \n                 lower_series3, \n                 upper_series3, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target2} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"Comparison of SARIMA and SARIMAX for modeling GWL @ {target2}:\")\nprint('')\nprint('SARIMA:  ',Sarima_SAL_accuracy)\nprint('SARIMAX: ',Sarimax_SAL_accuracy)\nprint('')\nprint('SARIMA AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMA_2_fit.aic)))\nprint('SARIMAX AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMAX_2_fit.aic)))\nprint('====================================================================')\nprint('')\nprint(\"\"\"SARIMAX model performs better, will proceed with it \nfor the final training and forecast.\"\"\")\nprint('')\nprint('====================================================================')","cc20f4ac":"from numpy.random import seed\nseed(1)\n\nprint(\"Retrain SARIMAX on complete dataset and forecast\")\nprint('')\n\n# while training on all dataset, I dropped 'Volume_CSA'\n# which showed non significant coeff value\n\nSAL_model_SARIMAX = SARIMAX(SAL[targets[1]],\n                            exog=SAL[['Volume_CC1',\n                                      'Volume_CC2',\n                                      #'Volume_CSA',\n                                      'Volume_CSAL'] +\n                                     ['Depth_to_Groundwater_CoS'] + \n                                     ['total_rain']], \n                            order=(p_SAL_ex, d_SAL_ex, q_SAL_ex),\n                            seasonal_order=(P_SAL_ex, D_SAL_ex, Q_SAL_ex, s_SAL_ex),\n                            enforce_invertibility=False)\n\nSAL_model_SARIMAX_fit = SAL_model_SARIMAX.fit(disp=0)\n\nprint(SAL_model_SARIMAX_fit.summary().tables[1])\nprint('')\n\nprint(\"Demonstrating forecast n-periods ahead\")\n\n# forecast shall start from the last date of SAL dataset used\n# in training the model\ninput_data = future[['Volume_CC1', \n                     'Volume_CC2',\n                     #'Volume_CSA',\n                     'Volume_CSAL'] +\n                    ['Depth_to_Groundwater_CoS'] + \n                    ['total_rain']] # all features used in training\n\nSAL_forecast = SAL_model_SARIMAX_fit.forecast(len(future),\n                                                  exog=input_data)\n\n# prediction confidence intervals\nSAL_forecast_SARIMAX_conf = SAL_model_SARIMAX_fit.get_forecast(len(future),\n                                                      exog=input_data)\n\nlower_val4 = SAL_forecast_SARIMAX_conf.conf_int()[['lower Depth_to_Groundwater_SAL']].values\nupper_val4 = SAL_forecast_SARIMAX_conf.conf_int()[['upper Depth_to_Groundwater_SAL']].values\nlower_series4 = pd.Series(to_list(lower_val4), index=SAL_forecast.index) # custom function\nupper_series4 = pd.Series(to_list(upper_val4), index=SAL_forecast.index) # custom function\n\n# output list of forecasted values:\n#SAL_forecast\n\n# Plot\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(SAL[SAL.index >= '2019-10-31'][targets[1]])\nplt.plot(SAL_forecast, color='darkgreen', \n         label='forecast')\nplt.fill_between(lower_series4.index, \n                 lower_series4, \n                 upper_series4, \n                 color='k', alpha=.15)\nplt.title(f\"\"\"Out-of-sample Forecast for GWL \n@ {target2} with SARIMAX for {len(future)} days ahead\"\"\")\nplt.show();\n\nprint('')\nprint('================================================================')\n\nprint(\"\"\"As we have actual values for forecasted (future) period, \nwe can evaluate forecasting accuracy:\"\"\")\nprint('')\n\nforecast_error = future[targets[1]] - SAL_forecast\nforecast_error.plot(kind='bar',\n                    figsize=(8,3), \n                    #xticks=(list(range(1,31))),\n                    xlabel='', \n                    ylabel='mtrs', grid=True, \n                    title='Forecast error, SARIMAX model')\nplt.show();\nprint('')\nprint(\"\"\"Forecasting error is within +\/- 0.4 mtrs, oscilliating\nbetween 'good' and 'bad' outcomes. Periodicity of errors suggests \nthat hyperparameters need to be further optimized.\"\"\")\n\nprint('')\nprint(f'=>END of modeling for {target2}<=')\nprint('================================================================')","1a00b2a9":"target3 = 'North sector unconfined aquifer CoS well'\n\n##### MODELING WITH SARIMAX #####\nprint(f'Modeling GWL @ {target3} with SARIMAX')\nprint('================================================================')\nprint('================================================================')\n\n# split data on training\/test\/future\n# future set will be used to demonstrate forecast with the model\nprint('Split data: ')\n\nCoS = auser_imputed[auser_imputed.index <= '2019-12-07']\nCoS.dropna(inplace=True)\n\nn = len(CoS)\n\ntrain = CoS.iloc[:int(0.90*n), ]\ntrain = train.asfreq(freq='D')\n\ntest = CoS.iloc[int(0.90*n):, ]\ntest = test.asfreq(freq='D')\n\nfuture = auser_imputed[(auser_imputed.index > '2019-12-07') & \n                       (auser_imputed.index <='2020-01-06')] #data after 2020-01-06 is not continuous\nfuture = future.asfreq(freq='D')\nfuture.dropna(inplace=True)\n\nprint(f'train set: {train.shape}, test set: {test.shape}, forecast shape: {future.shape}')\nprint('================================================================')\nprint('')\n\nprint('Seasonal decomposition')\nprint('')\nplt.rcParams['figure.figsize'] = (8, 8)\ndecomp = seasonal_decompose(train[targets[2]], freq = 365) # data is Daily\ndecomp.plot()\nplt.show();\nprint('================================================================')\nprint('')\n\nprint('Estimating the differencing term to remove trend:\\n')\nprint('Apply ADF\/KPSS test to select optimal d:')\n#from pmdarima.arima import ndiffs\nkpss_diffs = ndiffs(train[targets[2]], alpha=0.05, test='kpss', max_d=6)\nadf_diffs = ndiffs(train[targets[2]], alpha=0.05, test='adf', max_d=6)\nn_diffs = max(adf_diffs, kpss_diffs)\n\nprint(f\"Estimated differencing term: {n_diffs}\")\nprint('================================================================')\nprint('')\n\nprint('Estimating SARIMA(X) hyperparameters')\n# There is a method for automatic grid search for hyperparameters (p\/d\/q)x(P\/D\/Q)s\n# However the method is very computationally expensive\n# and results are not guaranteed to converge.\n# I share in the opinion that better and systematic way to estimate hyperparameters\n# is to examine ACF\/PACF plots for significant lags\nprint('')\nprint('Plot ACF and PACF of differenced series')\nprint('Identify significant lags')\n\n#ACF and PACF with differenced series\ntrain_diff = train[[targets[2]]].diff(1).dropna()\n\nfig, axes = plt.subplots(2, 1, figsize=(10,6))\n\nfig = sm.graphics.tsa.plot_acf(train_diff, lags=100, \n                               ax=axes[0],zero=False)\nfig = sm.graphics.tsa.plot_pacf(train_diff, lags=100, \n                                ax=axes[1], zero=False)\nplt.tight_layout()\nplt.show();\nprint('================================================================')\nprint('')\n\nprint('Both plots roughly repeat significant lags at 31 day perediocity;')\nprint('will set s=31')\nprint('')\nprint('ACF: 1st lag is significant q=1, will set Q=1')\nprint('')\nprint('PACF: will try p=1 and P=1')\nprint('')\nprint('d is already estimated d=1, will set D=0.')\nprint('================================================================')","8dd8909b":"print(f\"Train SARIMA model for {target3} w\/o exogeneous variables\")\n\n# GLOBAL CoS SARIMA(X) HYPERPARAMETERS\np_CoS = 1\nd_CoS = 1\nq_CoS = 1\nP_CoS = 1\nD_CoS = 0\nQ_CoS = 1\ns_CoS = 31\n\n# initialize and fit model\nmodel_SARIMA_3 = SARIMAX(train[targets[2]], # targets[2] - GWL CoS\n                       order=(p_CoS, d_CoS, q_CoS),\n                       seasonal_order=(P_CoS, D_CoS, Q_CoS, s_CoS),\n                       enforce_invertibility=False)\n\nmodel_SARIMA_3_fit = model_SARIMA_3.fit(disp=0)\nprint(model_SARIMA_3_fit.summary().tables[1])\nprint('')\n# If the p-value is less than or equal to the significance level, \n# we can conclude that the coefficient is statistically significant.\n# Usually, a significance level of 0.05 works well\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMA_3_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\n\nprint('')\nprint(\"Diagnostics plots look almost normal\")\nprint(\"Irregularities might be due to the outliers and imputed values\")\nprint(\"Will proceed with this model hyperparameters.\")\n\nprint('================================================================')\nprint('')\n\nprint(f\"Validate SARIMA (({p_CoS},{d_CoS},{q_CoS}),({P_CoS},{D_CoS},{Q_CoS},{s_CoS})) model on the test set\")\nprint('')\n# use fiited model to predict values from the test set\npredictionSAR_3 = model_SARIMA_3_fit.forecast(len(test))\n\n# prediction confidence intervals\nforecast_SARIMA_3_conf = model_SARIMA_3_fit.get_forecast(len(test))\nlower_val5 = forecast_SARIMA_3_conf.conf_int()[['lower Depth_to_Groundwater_CoS']].values\nupper_val5 = forecast_SARIMA_3_conf.conf_int()[['upper Depth_to_Groundwater_CoS']].values\nlower_series5 = pd.Series(to_list(lower_val5), index=predictionSAR_3.index) # custom function\nupper_series5 = pd.Series(to_list(upper_val5), index=predictionSAR_3.index) # custom function\n\n# Plot prediction results\nplt.figure(figsize=(8,4), dpi=80)\n# complete series\nplt.plot(train[train.index.year > 2016][targets[2]],\n         label = 'Train Values')\n# actual values from test set\nplt.plot(test[targets[2]], \n         label = 'Test Values')\n# predicted values\nplt.plot(predictionSAR_3, color='darkgreen', \n         label = 'Predicted Values')\n# confidence intervals\nplt.fill_between(lower_series5.index, \n                 lower_series5, \n                 upper_series5, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"In-sample prediction for GWL @ {target3}\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"SARIMA (({p_CoS},{d_CoS},{q_CoS}),({P_CoS},{D_CoS},{Q_CoS},{s_CoS})) goodness of fit: \")\n\nSarima_CoS_accuracy = forecast_accuracy(predictionSAR_3, test[targets[2]])\n\nprint(Sarima_CoS_accuracy)\nprint('================================================================')","7903b04d":"print(f\"Next Train SARIMAX Model for {target3} with exogeneous variables\")\nprint('')\n\n# after trying different regressors, the following choice showed\n# best results in terms of AIC criteria and significance of\n# regressors' coeffients\n\np_CoS_ex = 1\nd_CoS_ex = 1\nq_CoS_ex = 0\nP_CoS_ex = 0 \nD_CoS_ex = 0\nQ_CoS_ex = 1\ns_CoS_ex = 31\n\nmodel_SARIMAX_3 = SARIMAX(train[targets[2]],\n                        exog=train[['Volume_POL','Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL'] +\n                                   ['Depth_to_Groundwater_LT2','Depth_to_Groundwater_SAL'] + \n                                   ['total_rain']],\n                        order=(p_CoS_ex, d_CoS_ex, q_CoS_ex),\n                        seasonal_order=(P_CoS_ex, D_CoS_ex, Q_CoS_ex, s_CoS_ex),\n                        enforce_invertibility=False)\n\nmodel_SARIMAX_3_fit=model_SARIMAX_3.fit(disp=0)\nprint(model_SARIMAX_3_fit.summary().tables[1])\nprint('')\n\nprint('Each term in the model is significant, P>|z| is less than 0.05')\nprint('')\n\nmodel_SARIMAX_3_fit.plot_diagnostics(figsize=(8,9))\nplt.show();\nprint('')\n\nprint('================================================================')","53a8d2ee":"print(f\"Validate SARIMAX (({p_CoS_ex},{d_CoS_ex},{q_CoS_ex}),({P_CoS_ex},{D_CoS_ex},{Q_CoS_ex},{s_CoS_ex})) model on test set\")\nprint('')\n\n# use fiited model to predict values from test set\npredictionSARX_3 = model_SARIMAX_3_fit.forecast(len(test),\n                                            exog=test[['Volume_POL','Volume_CC1', \n                                                       'Volume_CC2', 'Volume_CSA', \n                                                       'Volume_CSAL'] +\n                                                      ['Depth_to_Groundwater_LT2',\n                                                       'Depth_to_Groundwater_SAL'] + \n                                                      ['total_rain']])\n\n# prediction confidence intervals\nforecast_SARIMAX_3_conf = model_SARIMAX_3_fit.get_forecast(len(test),\n                                                       exog=test[['Volume_POL','Volume_CC1', \n                                                       'Volume_CC2', 'Volume_CSA', \n                                                       'Volume_CSAL'] +\n                                                      ['Depth_to_Groundwater_LT2',\n                                                       'Depth_to_Groundwater_SAL'] + \n                                                      ['total_rain']])\n\nlower_val6 = forecast_SARIMAX_3_conf.conf_int()[['lower Depth_to_Groundwater_CoS']].values\nupper_val6 = forecast_SARIMAX_3_conf.conf_int()[['upper Depth_to_Groundwater_CoS']].values\nlower_series6 = pd.Series(to_list(lower_val6), index=predictionSARX_3.index) # custom function\nupper_series6 = pd.Series(to_list(upper_val6), index=predictionSARX_3.index) # custom function\n\n# Plot \nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(train[train.index.year > 2016][targets[2]], \n         label = 'Train Values')\nplt.plot(test[targets[2]], \n         label = 'Test Values')\nplt.plot(predictionSARX_3, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series6.index, \n                 lower_series6, \n                 upper_series6, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target3} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\n\nprint(f\"SARIMAX (({p_CoS_ex},{d_CoS_ex},{q_CoS_ex}),({P_CoS_ex},{D_CoS_ex},{Q_CoS_ex},{s_CoS_ex})) goodness of fit: \")\nSarimax_CoS_accuracy = forecast_accuracy(predictionSARX_3, test[targets[2]])\n\nprint(Sarimax_CoS_accuracy)\nprint('')\n\nprint('================================================================')\nprint('')\nprint(\"Closer look at in-sample prediction\")\n\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(test[targets[2]], \n         label = 'Actual Values')\nplt.plot(predictionSARX_3, color='darkgreen', \n         label = 'Predicted Values')\nplt.fill_between(lower_series6.index, \n                 lower_series6, \n                 upper_series6, \n                 color='k', alpha=.15)\nplt.legend(loc=\"upper left\")\nplt.title(f\"\"\"In-sample prediction for \nGWL @ {target3} with SARIMAX\"\"\")\nplt.show();\n\nprint('================================================================')\nprint('')\nprint(f\"Comparison of SARIMA and SARIMAX for modeling GWL @ {target3}:\")\nprint('')\nprint('SARIMA:  ',Sarima_CoS_accuracy)\nprint('SARIMAX: ',Sarimax_CoS_accuracy)\nprint('')\nprint('SARIMA AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMA_3_fit.aic)))\nprint('SARIMAX AIC criteria: ', float(\"{0:.4f}\".format(model_SARIMAX_3_fit.aic)))\nprint('================================================================')\nprint('')\nprint(\"\"\"SARIMAX model performs better, will proceed with it \nfor the final training and forecast.\"\"\")\nprint('')\nprint('================================================================')","552066db":"from numpy.random import seed\nseed(1)\n\nprint(\"Retrain SARIMAX on complete dataset and forecast\")\nprint('')\n\n# while training on all dataset, I dropped 'Volume_CSAL' and SAL\n# which showed non significant coeff value\n\nCoS_model_SARIMAX = SARIMAX(CoS[targets[2]],\n                            exog=SAL[['Volume_POL',\n                                      'Volume_CC1',\n                                      'Volume_CC2',\n                                      'Volume_CSA'] +\n                                     ['Depth_to_Groundwater_LT2'] +\n                                     ['total_rain']], \n                            order=(p_CoS_ex, d_CoS_ex, q_CoS_ex),\n                            seasonal_order=(P_CoS_ex, D_CoS_ex, Q_CoS_ex, s_CoS_ex),\n                            enforce_invertibility=False)\n\nCoS_model_SARIMAX_fit = CoS_model_SARIMAX.fit(disp=0)\n\nprint(CoS_model_SARIMAX_fit.summary().tables[1])\nprint('')\n\nprint(\"Demonstrating forecast n-periods ahead\")\n\n# forecast shall start from the last date of SAL dataset used\n# in training the model\ninput_data = future[['Volume_POL', \n                     'Volume_CC1',\n                     'Volume_CC2',\n                     'Volume_CSA']+\n                    ['Depth_to_Groundwater_LT2'] +\n                    ['total_rain']] # all features used in training\n\nCoS_forecast = CoS_model_SARIMAX_fit.forecast(len(future),\n                                                  exog=input_data)\n\n# prediction confidence intervals\nCoS_forecast_SARIMAX_conf = CoS_model_SARIMAX_fit.get_forecast(len(future),\n                                                      exog=input_data)\n\nlower_val7 = CoS_forecast_SARIMAX_conf.conf_int()[['lower Depth_to_Groundwater_CoS']].values\nupper_val7 = CoS_forecast_SARIMAX_conf.conf_int()[['upper Depth_to_Groundwater_CoS']].values\nlower_series7 = pd.Series(to_list(lower_val7), index=CoS_forecast.index) # custom function\nupper_series7 = pd.Series(to_list(upper_val7), index=CoS_forecast.index) # custom function\n\n# output list of forecasted values:\n#CoS_forecast\n\n# Plot\nplt.figure(figsize=(8,4), dpi=80)\nplt.plot(CoS[CoS.index >= '2019-10-31'][targets[2]])\nplt.plot(CoS_forecast, color='darkgreen', \n         label='forecast')\nplt.fill_between(lower_series7.index, \n                 lower_series7, \n                 upper_series7, \n                 color='k', alpha=.15)\nplt.title(f\"\"\"Out-of-sample Forecast for GWL \n@ {target3} with SARIMAX for {len(future)} days ahead\"\"\")\nplt.show();\n\nprint('')\nprint('====================================================================')\n\nprint(\"\"\"As we have actual values for forecasted (future) period, \nwe can evaluate forecasting accuracy:\"\"\")\nprint('')\n\nforecast_error = future[targets[2]] - CoS_forecast\nforecast_error.plot(kind='bar',\n                    figsize=(8,3), \n                    #xticks=(list(range(1,31))),\n                    xlabel='', \n                    ylabel='mtrs', grid=True, \n                    title='Forecast error, SARIMAX model')\nplt.show();\nprint('')\nprint(\"\"\"Forecasting error is within +0.8\/- 1.8 mtrs for\nfirst 24 days, thereafter worsens. Error is quite large\nand model does not work well for this series.\"\"\")\n\nprint('')\nprint(f'=>END of modeling for {target3}<=')\nprint('====================================================================')","1deb20ad":"ks.backend.clear_session()","0eaef910":"%%time\n\n##### MODELING WITH LSTM #####\nprint('================================================================')\nprint('================================================================')\nprint(f'Modeling 1 day GWL change in {target1}')\nprint('Takes in 30 days input data and outputs next day change')\nprint('================================================================')\nprint('')\n\n#reproducability\nfrom numpy.random import seed\nseed(1)\nimport tensorflow as tf\ntf.random.set_seed(1)\n\n# liabraries\nfrom sklearn.preprocessing import MinMaxScaler\nimport keras as ks\nimport kerastuner\nfrom kerastuner.tuners import BayesianOptimization\nfrom kerastuner import Objective\n\n# Select input features for model training\ndata_LT2 = auser_imputed[['Depth_to_Groundwater_LT2'] +\n                     ['Month_No'] + \n                     ['Rainfall_Tereglio_Coreglia_Antelminelli_50_d_lag']+\n                     ['Rainfall_Calavorno_50_d_lag']+\n                     ['Rainfall_Fabbriche_di_Vallico_50_d_lag']+\n                     ['Rainfall_Pontetetto_50_d_lag']+\n                     ['Rainfall_Gallicano_50_d_lag'] + \n                     ['Rainfall_Orentano_50_d_lag']+\n                     ['Rainfall_Borgo_a_Mozzano_50_d_lag']+\n                     ['Rainfall_Piaggione_50_d_lag']+\n                     ['Rainfall_Croce_Arcana_50_d_lag']+\n                     ['Rainfall_Monte_Serra_50_d_lag'] +\n                     ['Volume_CC1'] + \n                     ['Volume_CC2'] +\n                     ['Volume_POL']].copy()\n\n# dropping nan if any\ndata_LT2 = data_LT2.dropna()\n\n# **HYPERPARAMETERS**\n# train data: 12 years of data\n# test data: 2 years of data\n# future data to demonstrate forecast: 2019-12-07 to 2020-12-06\n# data after 2020-12-06 is not continuous\n\n# Dictionary of hyperparameters,\n# some of which were optimized with keras BaysianOptimizer (too long to run here)\n\nGLOBAL_SETTINGS = {'batch_size': 92, # optimized with BO\n                   'clip_norm': True,\n                   'clip_value': 1,\n                   'dropout': 0.5, # optimized with BO\n                   'epochs': 200,\n                   'hidden_size': 8, # optimized with BO\n                   'learning_rate': 0.1, # optimized with BO\n                   'seq_length': 30, # days\n                   'output_seq_length': 1, # day\n                   'test_start': pd.to_datetime('06122017', format='%d%m%Y'),\n                   'test_end': pd.to_datetime('06122019', format='%d%m%Y')\n                  }\n\nprint('Model hyperparameters:')\nprint('')\nprint('batch_size:    ',GLOBAL_SETTINGS['batch_size'])\nprint('clip_norm:     ',GLOBAL_SETTINGS['clip_norm'])\nprint('clip_value:    ',GLOBAL_SETTINGS['clip_value'])\nprint('dropout:       ',GLOBAL_SETTINGS['dropout'])\nprint('epochs:        ',GLOBAL_SETTINGS['epochs'])\nprint('hidden_size:   ',GLOBAL_SETTINGS['hidden_size'])\nprint('learning_rate: ',GLOBAL_SETTINGS['learning_rate'])\nprint('================================================================')\nprint('')\n\n# CUSTOM FUNCTIONS to split, scale and window data\n\ndef split_data(data, GLOBAL_SETTINGS):\n    dataset = data[(data.index < GLOBAL_SETTINGS[\"test_start\"])].copy()\n    n = len(dataset)\n    train_df = dataset[0:round(0.9*n)].copy() \n    val_df = dataset[round(0.9*n)+1:].copy()  \n    val_df_ext = dataset[round(0.9*n)+1-GLOBAL_SETTINGS[\"seq_length\"]:].copy()  \n    test_df = data[(data.index >= GLOBAL_SETTINGS[\"test_start\"]) & \n                   (data.index <= GLOBAL_SETTINGS[\"test_end\"])] \n    test_df_ext = pd.concat([dataset.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], test_df], axis=0)                                           \n    return train_df, val_df, val_df_ext, test_df, test_df_ext\n\n# split a multivariate sequence into samples\ndef split_sequences(data, GLOBAL_SETTINGS):\n    X, y = list(), list()\n    for i in range(len(data)):\n        end_ix = i + GLOBAL_SETTINGS[\"seq_length\"]\n        if end_ix >= len(data):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = data.iloc[i:end_ix, 1:].copy(), data.iloc[end_ix, 0].copy() \n        X.append(seq_x)\n        y.append(seq_y)\n    X = np.stack(X, axis=0)\n    y = np.stack(y, axis=0)\n    return X, y\n\ndef preprocess_data(data, label, GLOBAL_SETTINGS):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl.fit(pd.DataFrame(data[label]))\n\n    data_norm = pd.DataFrame(scaler.fit_transform(data),\n                             index=data.index, \n                             columns=data.columns)\n    # split original data\n    train_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data, GLOBAL_SETTINGS)\n    \n    # split normalized data\n    train_df_n, val_df_n, val_df_ext_n, test_df_n, test_df_ext_n= split_data(data_norm, GLOBAL_SETTINGS) \n\n    # split into inputs and label pairs\n    X_train, Y_train = split_sequences(train_df_n, GLOBAL_SETTINGS)\n    X_val, Y_val = split_sequences(val_df_ext_n, GLOBAL_SETTINGS)\n    X_test, Y_test= split_sequences(test_df_ext_n, GLOBAL_SETTINGS)\n    \n    return X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl\n\n\n# CUSTOM FUNCTION to use for forecast with new data\ndef forecast_fluctation_with_new_data(data, model, sequence):\n    \n    import pandas as pd\n    import numpy as np\n    \"\"\" \n    length of data input must be at least equal to sequence\n    \"\"\"\n    forecasted_dates = []\n    X=[]\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        seq_x = data.iloc[i:end_ix,:]\n        X.append(seq_x)\n    X_new = np.stack(X, axis=0)\n    predicted_new = model.model.predict(X_new)\n\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        freq = 'D'                                                    \n        forecast_date = data.index.min() + pd.Timedelta(end_ix, unit=freq)\n        forecasted_dates.append(forecast_date)\n    X_date = np.stack(forecasted_dates, axis=0)\n    forecast = pd.DataFrame({'Date':X_date, 'GWL_1_d_fluctation_forecast':predicted_new.reshape(-1)})\n    forecast.set_index('Date', inplace=True)\n    \n    return forecast\n\n# Data preprocessing, split sets\ntrain_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data_LT2, GLOBAL_SETTINGS)\n\nprint('Dimensions of split datasets:\\n')\nprint('train:        ',train_df.shape)\nprint('val:          ',val_df.shape)\nprint('val extended: ',val_df_ext.shape)\nprint('test_df:      ',test_df.shape)\nprint('test extended:',test_df_ext.shape)\n\n# Split input sequences\nX_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl = preprocess_data(data_LT2, 'Depth_to_Groundwater_LT2', GLOBAL_SETTINGS)\n\nprint('================================================================')\nprint('')\nprint('Dimensions of split input sequences:\\n')\nprint('X_train: ',X_train.shape)\nprint('Y_train: ',Y_train.shape)\nprint('X_val:   ',X_val.shape)\nprint('Y_val:   ',Y_val.shape)\nprint('X_test:  ',X_test.shape)\nprint('Y_test:  ',Y_test.shape)\nprint('================================================================')\nprint('')\n\n\n# **LSTM model**\nmodel = ks.models.Sequential()\nmodel.add(ks.Input(shape=(X_train.shape[1:])))\nmodel.add(ks.layers.LSTM(8, #hidden_size=GLOBAL_SETTINGS['hidden_size'],\n                         unit_forget_bias = True, \n                         dropout = GLOBAL_SETTINGS['dropout']))\nmodel.add(ks.layers.Dense(GLOBAL_SETTINGS['output_seq_length'],\n                          activation='linear'))\n\noptimizer = ks.optimizers.Adam(lr=GLOBAL_SETTINGS['learning_rate'], \n                               epsilon=10E-3, \n                               clipnorm=GLOBAL_SETTINGS[\"clip_norm\"], \n                               clipvalue=GLOBAL_SETTINGS[\"clip_value\"])\n      \nmodel.compile(loss='mse', \n              optimizer=optimizer, \n              metrics=['mae'])\n\nLT2_model = model.fit(X_train, Y_train,\n                      validation_data=(X_val, Y_val),\n                      epochs=GLOBAL_SETTINGS[\"epochs\"], \n                      verbose=2,\n                      batch_size=GLOBAL_SETTINGS[\"batch_size\"],\n                      callbacks=[ks.callbacks.EarlyStopping(monitor='val_loss', \n                                                            mode='min', \n                                                            verbose=0,\n                                                            patience=5)])\n\nprint('================================================================')\nprint('')\npd.DataFrame(LT2_model.history)\\\n[['loss','mae','val_loss']].plot(figsize=(8,3),title='Training Errors')\nplt.show();\n\n#print(LT2_model.model.summary())\n\n#model.save(\"LT2_model.h5\")\n#LT2_model = ks.models.load_model(\"LT2_model.h5\")\n\n# Validate model\nprint('================================================================')\nprint('')\nprint('Evaluate model on test data: ')\n\nloss, mae = LT2_model.model.evaluate(X_test, Y_test)\nmse = float(\"{0:.4f}\".format(loss))\nmae = float(\"{0:.4f}\".format(mae))\nprint(f'Test MSE: {mse}, Test MAE: {mae}')\nprint('')\n\nprint(\"\"\"Note: model outputs MSE and MAE on the scaled test data. \nErrors calculated on original (rescaled) data are more\nrelevant for model evaluation (shown below).\\n\"\"\")\n\nprint('================================================================')\n\nprint('Predict GWL change on new (use Test) data and rescale to original units')\npredicted = scaler_gwl.inverse_transform(LT2_model.model.predict(X_test))\nactual_observations = np.asarray(scaler_gwl.inverse_transform(Y_test.reshape(-1,1)))\n\ndf_ = pd.DataFrame({'actual_observations':actual_observations.reshape(-1), \n                       'predicted':predicted.reshape(-1)}, index=test_df.index)\n# plot\ndf_.plot(figsize=(8, 3)).legend(bbox_to_anchor=(1.01, 1))\nplt.show();\n\nprint('================================================================')\n\nprint('Errors between predicted and actual values(test data):')\nprint('')\n#print(f'R2: {R2}, RMSE: {RMSE}, MAE: {MAE}')\nLT2_lstm_accuracy = forecast_accuracy(predicted.flatten(), \n                                      actual_observations.flatten())\nprint(LT2_lstm_accuracy)\n\nprint('total running time: ')\n\nprint('================================================================')","be0d5773":"##### Demonstrate how to use model to forecast GWL #####\nprint('================================================================')\nprint('Demonstrate how to use model to forecast 1 day Change(mtrs)')\nprint('')\n### Fluctation one day in the future\n\n##### INPUTS #####\n### Model requires AT LEAST 30 days of continuous (sequence) daily records for\n### of rainfalls :\n### Tereglio_Coreglia_Antelminelli\n### Rainfall_Calavorno\n### Fabbriche_di_Vallico\n### Pontetetto\n### Gallicano\n### Orentano\n### Borgo_a_Mozzano\n### Piaggione\n### Croce_Arcana\n### Monte_Serra\n### EACH LAGGED BY 50 DAYS\n### that is actually 80 day of continuous data\n### will result in 30 days after lag applied\n\n### of volumes:\n### Volume_CC1, Volume_CC2, Volume_POL - no lag operator required\n\n### one Engineered feature - Month No\n###################\n\n### Demonstration:\n\n### Predict fluctation of GWL on 6th Jan 2020 \n### Require input data from 5-Jan-2020 back 80 days\ninput_new_data = auser_imputed[(auser_imputed.index >='2019-10-18') & \n                               (auser_imputed.index <='2020-01-05')].copy()\n\n# lag rainfall data 50 days:\nfor r in rainfall:\n    input_new_data[r+'_50_d_lag'] = input_new_data[r].shift(50)\n\n# Add Month No feature    \ninput_new_data['Month_No'] = input_new_data.index.month    \n\n# select only features used in training model:\ninput_new_data = input_new_data[['Month_No',\n                                 'Rainfall_Tereglio_Coreglia_Antelminelli_50_d_lag',\n                                 'Rainfall_Calavorno_50_d_lag',\n                                 'Rainfall_Fabbriche_di_Vallico_50_d_lag',\n                                 'Rainfall_Pontetetto_50_d_lag',\n                                 'Rainfall_Gallicano_50_d_lag',\n                                 'Rainfall_Orentano_50_d_lag',\n                                 'Rainfall_Borgo_a_Mozzano_50_d_lag',\n                                 'Rainfall_Piaggione_50_d_lag',\n                                 'Rainfall_Croce_Arcana_50_d_lag',\n                                 'Rainfall_Monte_Serra_50_d_lag',\n                                 'Volume_CC1',\n                                 'Volume_CC2',\n                                 'Volume_POL']].copy()\n\n# drop nan if any\ninput_new_data = input_new_data.dropna()\n\n# data shall be ordered by date\ninput_new_data = input_new_data.sort_index()\n\nprint('input data shape: ', input_new_data.shape)\nprint('================================================================')\n\nprint('Use custom function: forecast_fluctation_with_new_data')\nforecast_gwl = forecast_fluctation_with_new_data(input_new_data, LT2_model, sequence=30)\nprint('')\nprint(forecast_gwl)\nprint('')\n\nprint(f'=> END of modeling for {target1} <=')\nprint('================================================================')","bf0198c6":"ks.backend.clear_session()","87f8399e":"%%time\n\n##### MODELING WITH LSTM #####\nprint('================================================================')\nprint('================================================================')\nprint(f'Modeling 1 day GWL change in {target2}')\nprint('Takes in 30 days input data and outputs next day change')\nprint('================================================================')\nprint('')\n\n#reproducability\nfrom numpy.random import seed\nseed(1)\nimport tensorflow as tf\ntf.random.set_seed(1)\n\n# liabraries\nfrom sklearn.preprocessing import MinMaxScaler\nimport keras as ks\nimport kerastuner\nfrom kerastuner.tuners import BayesianOptimization\nfrom kerastuner import Objective\n\n# Select input features for model training\ndata_SAL = auser_imputed[['Depth_to_Groundwater_SAL'] +\n                     ['Month_No'] +\n                     ['Rainfall_Tereglio_Coreglia_Antelminelli']+\n                     ['Rainfall_Calavorno']+\n                     ['Rainfall_Fabbriche_di_Vallico']+\n                     ['Rainfall_Pontetetto']+\n                     ['Rainfall_Gallicano'] + \n                     ['Rainfall_Orentano']+\n                     ['Rainfall_Borgo_a_Mozzano']+\n                     ['Rainfall_Piaggione']+\n                     ['Rainfall_Croce_Arcana']+\n                     ['Rainfall_Monte_Serra'] +\n                     ['Volume_CC1'] + \n                     ['Volume_CC2'] +\n                     ['Volume_CSA'] + \n                     ['Volume_CSAL'] +\n                     ['Volume_POL']].copy()\n\n\n# dropping nan if any\ndata_SAL = data_SAL.dropna()\n\n# **HYPERPARAMETERS**\n# train data: 12 years of data\n# test data: 2 years of data\n# future data to demonstrate forecast: 2019-12-07 to 2020-12-06\n# data after 2020-12-06 is not continuous\n\n# Dictionary of hyperparameters,\n# some of which were optimized with keras BaysianOptimizer (too long to run here)\n\nGLOBAL_SETTINGS = {'batch_size': 96, # optimized with BO\n                   'clip_norm': True,\n                   'clip_value': 1,\n                   'dropout': 0.4, # optimized with BO\n                   'epochs': 200,\n                   'hidden_size': 67, # optimized with BO\n                   'learning_rate': 0.07, # optimized with BO\n                   'seq_length': 30, # days\n                   'output_seq_length': 1, # day\n                   'test_start': pd.to_datetime('06122017', format='%d%m%Y'),\n                   'test_end': pd.to_datetime('06122019', format='%d%m%Y')\n                  }\n\nprint('Model hyperparameters:')\nprint('')\nprint('batch_size:    ',GLOBAL_SETTINGS['batch_size'])\nprint('clip_norm:     ',GLOBAL_SETTINGS['clip_norm'])\nprint('clip_value:    ',GLOBAL_SETTINGS['clip_value'])\nprint('dropout:       ',GLOBAL_SETTINGS['dropout'])\nprint('epochs:        ',GLOBAL_SETTINGS['epochs'])\nprint('hidden_size:   ',GLOBAL_SETTINGS['hidden_size'])\nprint('learning_rate: ',GLOBAL_SETTINGS['learning_rate'])\nprint('================================================================')\nprint('')\n\n# CUSTOM FUNCTIONS to split, scale and window data\n\ndef split_data(data, GLOBAL_SETTINGS):\n    dataset = data[(data.index < GLOBAL_SETTINGS[\"test_start\"])].copy()\n    n = len(dataset)\n    train_df = dataset[0:round(0.9*n)].copy() \n    val_df = dataset[round(0.9*n)+1:].copy()  \n    val_df_ext = dataset[round(0.9*n)+1-GLOBAL_SETTINGS[\"seq_length\"]:].copy()  \n    test_df = data[(data.index >= GLOBAL_SETTINGS[\"test_start\"]) & \n                   (data.index <= GLOBAL_SETTINGS[\"test_end\"])] \n    test_df_ext = pd.concat([dataset.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], test_df], axis=0)                                           \n    return train_df, val_df, val_df_ext, test_df, test_df_ext\n\n# split a multivariate sequence into samples\ndef split_sequences(data, GLOBAL_SETTINGS):\n    X, y = list(), list()\n    for i in range(len(data)):\n        end_ix = i + GLOBAL_SETTINGS[\"seq_length\"]\n        if end_ix >= len(data):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = data.iloc[i:end_ix, 1:].copy(), data.iloc[end_ix, 0].copy() \n        X.append(seq_x)\n        y.append(seq_y)\n    X = np.stack(X, axis=0)\n    y = np.stack(y, axis=0)\n    return X, y\n\ndef preprocess_data(data, label, GLOBAL_SETTINGS):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl.fit(pd.DataFrame(data[label]))\n\n    data_norm = pd.DataFrame(scaler.fit_transform(data),\n                             index=data.index, \n                             columns=data.columns)\n    # split original data\n    train_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data, GLOBAL_SETTINGS)\n    \n    # split normalized data\n    train_df_n, val_df_n, val_df_ext_n, test_df_n, test_df_ext_n= split_data(data_norm, GLOBAL_SETTINGS) \n\n    # split into inputs and label pairs\n    X_train, Y_train = split_sequences(train_df_n, GLOBAL_SETTINGS)\n    X_val, Y_val = split_sequences(val_df_ext_n, GLOBAL_SETTINGS)\n    X_test, Y_test= split_sequences(test_df_ext_n, GLOBAL_SETTINGS)\n    \n    return X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl\n\n\n# CUSTOM FUNCTION to use for forecast with new data\ndef forecast_fluctation_with_new_data(data, model, sequence):\n    \n    import pandas as pd\n    import numpy as np\n    \"\"\" \n    length of data input must be at least equal to sequence\n    \"\"\"\n    forecasted_dates = []\n    X=[]\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        seq_x = data.iloc[i:end_ix,:]\n        X.append(seq_x)\n    X_new = np.stack(X, axis=0)\n    predicted_new = model.model.predict(X_new)\n\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        freq = 'D'                                                    \n        forecast_date = data.index.min() + pd.Timedelta(end_ix, unit=freq)\n        forecasted_dates.append(forecast_date)\n    X_date = np.stack(forecasted_dates, axis=0)\n    forecast = pd.DataFrame({'Date':X_date, 'GWL_1_d_fluctation_forecast':predicted_new.reshape(-1)})\n    forecast.set_index('Date', inplace=True)\n    \n    return forecast\n\n# Data preprocessing, split sets\ntrain_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data_SAL, GLOBAL_SETTINGS)\n\nprint('Dimensions of split datasets:\\n')\nprint('train:        ',train_df.shape)\nprint('val:          ',val_df.shape)\nprint('val extended: ',val_df_ext.shape)\nprint('test_df:      ',test_df.shape)\nprint('test extended:',test_df_ext.shape)\n\n# Split input sequences\nX_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl = preprocess_data(data_SAL, 'Depth_to_Groundwater_SAL', GLOBAL_SETTINGS)\n\nprint('================================================================')\nprint('')\nprint('Dimensions of split input sequences:\\n')\nprint('X_train: ',X_train.shape)\nprint('Y_train: ',Y_train.shape)\nprint('X_val:   ',X_val.shape)\nprint('Y_val:   ',Y_val.shape)\nprint('X_test:  ',X_test.shape)\nprint('Y_test:  ',Y_test.shape)\nprint('================================================================')\nprint('')\n\n\n# **LSTM model**\nmodel = ks.models.Sequential()\nmodel.add(ks.Input(shape=(X_train.shape[1:])))\nmodel.add(ks.layers.LSTM(67, #hidden_size=GLOBAL_SETTINGS['hidden_size'],\n                         unit_forget_bias = True, \n                         dropout = GLOBAL_SETTINGS['dropout']))\nmodel.add(ks.layers.Dense(GLOBAL_SETTINGS['output_seq_length'],\n                          activation='linear'))\n\noptimizer = ks.optimizers.Adam(lr=GLOBAL_SETTINGS['learning_rate'], \n                               epsilon=10E-3, \n                               clipnorm=GLOBAL_SETTINGS[\"clip_norm\"], \n                               clipvalue=GLOBAL_SETTINGS[\"clip_value\"])\n      \nmodel.compile(loss='mse', \n              optimizer=optimizer, \n              metrics=['mae'])\n\nSAL_model = model.fit(X_train, Y_train,\n                      validation_data=(X_val, Y_val),\n                      epochs=GLOBAL_SETTINGS[\"epochs\"], \n                      verbose=2,\n                      batch_size=GLOBAL_SETTINGS[\"batch_size\"],\n                      callbacks=[ks.callbacks.EarlyStopping(monitor='val_loss', \n                                                            mode='min', \n                                                            verbose=0,\n                                                            patience=5)])\n\nprint('================================================================')\nprint('')\npd.DataFrame(SAL_model.history)\\\n[['loss','mae','val_loss']].plot(figsize=(8,3),title='Training Errors')\nplt.show();\n\n#print(SAL_model.model.summary())\n\n#model.save(\"SAL_model.h5\")\n#SAL_model = ks.models.load_model(\"SAL_model.h5\")\n\n# Validate model\nprint('================================================================')\nprint('')\nprint('Evaluate model on test data: ')\n\nloss, mae = SAL_model.model.evaluate(X_test, Y_test)\nmse = float(\"{0:.4f}\".format(loss))\nmae = float(\"{0:.4f}\".format(mae))\nprint(f'Test MSE: {mse}, Test MAE: {mae}')\nprint('')\nprint(\"\"\"Note: model outputs MSE and MAE on the scaled test data. \nErrors calculated on original (rescaled) data are more\nrelevant for model evaluation (shown below).\\n\"\"\")\n\nprint('================================================================')\nprint('')\nprint('Make predictions on new (use Test) data and rescale to original units')\npredicted_2 = scaler_gwl.inverse_transform(SAL_model.model.predict(X_test))\nactual_observations_2 = np.asarray(scaler_gwl.inverse_transform(Y_test.reshape(-1,1)))\n\ndf_ = pd.DataFrame({'actual_observations':actual_observations_2.reshape(-1), \n                       'predicted':predicted_2.reshape(-1)}, index=test_df.index)\n# plot\ndf_.plot(figsize=(8, 3)).legend(bbox_to_anchor=(1.01, 1))\nplt.show();\n\nprint('================================================================')\nprint('')\n\nprint('Errors between predicted and actual values(test data):')\nprint('')\n\nSAL_lstm_accuracy = forecast_accuracy(predicted_2.flatten(), \n                                      actual_observations_2.flatten())\nprint(SAL_lstm_accuracy)\n\nprint('')\nprint('total running time: ')\n\nprint('================================================================')","7f4bf00e":"##### Demonstrate how to use model to forecast GWL #####\nprint('================================================================')\nprint('Demonstrate how to use model to forecast 1 day Change(mtrs)')\nprint('')\n### Fluctation one day in the future\n\n##### INPUTS #####\n### Model requires AT LEAST 30 days of continuous (sequence) daily records for\n### of rainfalls :\n### Tereglio_Coreglia_Antelminelli\n### Rainfall_Calavorno\n### Fabbriche_di_Vallico\n### Pontetetto\n### Gallicano\n### Orentano\n### Borgo_a_Mozzano\n### Piaggione\n### Croce_Arcana\n### Monte_Serra\n\n### of volumes:\n### Volume_CC1, Volume_CC2, Volume_POL, Volume_CSA, Volume_CSAL\n\n### one Engineered feature - Month No\n###################\n\n### Demonstration:\n\n### Predict fluctation of GWL on 6th Jan 2020 \n### Require input data from 5-Jan-2020 back 30 days\ninput_new_data = auser_imputed[(auser_imputed.index >='2019-12-07') & \n                               (auser_imputed.index <='2020-01-05')].copy()\n\n# Add Month No feature    \ninput_new_data['Month_No'] = input_new_data.index.month    \n\n# select only features used in training model:\ninput_new_data = input_new_data[['Month_No',\n                                 'Rainfall_Tereglio_Coreglia_Antelminelli',\n                                 'Rainfall_Calavorno',\n                                 'Rainfall_Fabbriche_di_Vallico',\n                                 'Rainfall_Pontetetto',\n                                 'Rainfall_Gallicano',\n                                 'Rainfall_Orentano',\n                                 'Rainfall_Borgo_a_Mozzano',\n                                 'Rainfall_Piaggione',\n                                 'Rainfall_Croce_Arcana',\n                                 'Rainfall_Monte_Serra',\n                                 'Volume_CC1',\n                                 'Volume_CC2', \n                                 'Volume_CSA', \n                                 'Volume_CSAL',\n                                 'Volume_POL']].copy()\n\n# drop nan if any\ninput_new_data = input_new_data.dropna()\n\n# data shall be ordered by date\ninput_new_data = input_new_data.sort_index()\n\nprint('input data shape: ', input_new_data.shape)\nprint('================================================================')\nprint('')\n\nprint('Use custom function: forecast_fluctation_with_new_data')\nforecast_gwl = forecast_fluctation_with_new_data(input_new_data, SAL_model, sequence=30)\nprint('')\nprint(forecast_gwl)\nprint('')\n\nprint(f'=> END of modeling for {target2} <=')\nprint('================================================================')","95ae5080":"ks.backend.clear_session()","46139115":"%%time\n\n##### MODELING WITH LSTM #####\nprint('===============================================================')\nprint('===============================================================')\nprint(f'Modeling 1 day GWL change in {target3}')\nprint('Takes in 30 days input data and outputs next day change')\nprint('===============================================================')\nprint('')\n\n\n#reproducability\nfrom numpy.random import seed\nseed(1)\nimport tensorflow as tf\ntf.random.set_seed(1)\n\n# liabraries\nfrom sklearn.preprocessing import MinMaxScaler\nimport keras as ks\nimport kerastuner\nfrom kerastuner.tuners import BayesianOptimization\nfrom kerastuner import Objective\n\n# Select input features for model training\ndata_CoS = auser_imputed[['Depth_to_Groundwater_CoS'] +\n                     ['Month_No'] +\n                     ['Rainfall_Tereglio_Coreglia_Antelminelli']+\n                     ['Rainfall_Calavorno']+\n                     ['Rainfall_Fabbriche_di_Vallico']+\n                     ['Rainfall_Pontetetto']+\n                     ['Rainfall_Gallicano'] + \n                     ['Rainfall_Orentano']+\n                     ['Rainfall_Borgo_a_Mozzano']+\n                     ['Rainfall_Piaggione']+\n                     ['Rainfall_Croce_Arcana']+\n                     ['Rainfall_Monte_Serra'] +\n                     ['Volume_CC1'] + \n                     ['Volume_CC2'] +\n                     ['Volume_CSA'] + \n                     ['Volume_CSAL'] +\n                     ['Volume_POL']].copy()\n\n\n# dropping nan if any\ndata_CoS = data_CoS.dropna()\n\n# **HYPERPARAMETERS**\n# train data: 12 years of data\n# test data: 2 years of data\n# future data to demonstrate forecast: 2019-12-07 to 2020-12-06\n# data after 2020-12-06 is not continuous\n\n# Dictionary of hyperparameters,\n# some of which were optimized with keras BaysianOptimizer (too long to run here)\n\nGLOBAL_SETTINGS = {'batch_size': 212, # optimized with BO\n                   'clip_norm': True,\n                   'clip_value': 1,\n                   'dropout': 0.0, # optimized with BO\n                   'epochs': 200,\n                   'hidden_size': 215, # optimized with BO\n                   'learning_rate': 0.05, # optimized with BO\n                   'seq_length': 30, # days\n                   'output_seq_length': 1, # day\n                   'test_start': pd.to_datetime('06122017', format='%d%m%Y'),\n                   'test_end': pd.to_datetime('06122019', format='%d%m%Y')\n                  }\n\nprint('Model hyperparameters:')\nprint('')\nprint('batch_size:    ',GLOBAL_SETTINGS['batch_size'])\nprint('clip_norm:     ',GLOBAL_SETTINGS['clip_norm'])\nprint('clip_value:    ',GLOBAL_SETTINGS['clip_value'])\nprint('dropout:       ',GLOBAL_SETTINGS['dropout'])\nprint('epochs:        ',GLOBAL_SETTINGS['epochs'])\nprint('hidden_size:   ',GLOBAL_SETTINGS['hidden_size'])\nprint('learning_rate: ',GLOBAL_SETTINGS['learning_rate'])\nprint('===============================================================')\nprint('')\n\n# CUSTOM FUNCTIONS to split, scale and window data\n\ndef split_data(data, GLOBAL_SETTINGS):\n    dataset = data[(data.index < GLOBAL_SETTINGS[\"test_start\"])].copy()\n    n = len(dataset)\n    train_df = dataset[0:round(0.9*n)].copy() \n    val_df = dataset[round(0.9*n)+1:].copy()  \n    val_df_ext = dataset[round(0.9*n)+1-GLOBAL_SETTINGS[\"seq_length\"]:].copy()  \n    test_df = data[(data.index >= GLOBAL_SETTINGS[\"test_start\"]) & \n                   (data.index <= GLOBAL_SETTINGS[\"test_end\"])] \n    test_df_ext = pd.concat([dataset.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], test_df], axis=0)                                           \n    return train_df, val_df, val_df_ext, test_df, test_df_ext\n\n# split a multivariate sequence into samples\ndef split_sequences(data, GLOBAL_SETTINGS):\n    X, y = list(), list()\n    for i in range(len(data)):\n        end_ix = i + GLOBAL_SETTINGS[\"seq_length\"]\n        if end_ix >= len(data):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = data.iloc[i:end_ix, 1:].copy(), data.iloc[end_ix, 0].copy() \n        X.append(seq_x)\n        y.append(seq_y)\n    X = np.stack(X, axis=0)\n    y = np.stack(y, axis=0)\n    return X, y\n\ndef preprocess_data(data, label, GLOBAL_SETTINGS):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl = MinMaxScaler(feature_range=(-1, 1))\n    scaler_gwl.fit(pd.DataFrame(data[label]))\n\n    data_norm = pd.DataFrame(scaler.fit_transform(data),\n                             index=data.index, \n                             columns=data.columns)\n    # split original data\n    train_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data, GLOBAL_SETTINGS)\n    \n    # split normalized data\n    train_df_n, val_df_n, val_df_ext_n, test_df_n, test_df_ext_n= split_data(data_norm, GLOBAL_SETTINGS) \n\n    # split into inputs and label pairs\n    X_train, Y_train = split_sequences(train_df_n, GLOBAL_SETTINGS)\n    X_val, Y_val = split_sequences(val_df_ext_n, GLOBAL_SETTINGS)\n    X_test, Y_test= split_sequences(test_df_ext_n, GLOBAL_SETTINGS)\n    \n    return X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl\n\n\n# CUSTOM FUNCTION to use for forecast with new data\ndef forecast_fluctation_with_new_data(data, model, sequence):\n    \n    import pandas as pd\n    import numpy as np\n    \"\"\" \n    length of data input must be at least equal to sequence\n    \"\"\"\n    forecasted_dates = []\n    X=[]\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        seq_x = data.iloc[i:end_ix,:]\n        X.append(seq_x)\n    X_new = np.stack(X, axis=0)\n    predicted_new = model.model.predict(X_new)\n\n    for i in range(len(data)):\n        end_ix = i + sequence\n        if end_ix > len(data):\n            break\n        freq = 'D'                                                    \n        forecast_date = data.index.min() + pd.Timedelta(end_ix, unit=freq)\n        forecasted_dates.append(forecast_date)\n    X_date = np.stack(forecasted_dates, axis=0)\n    forecast = pd.DataFrame({'Date':X_date, 'GWL_1_d_fluctation_forecast':predicted_new.reshape(-1)})\n    forecast.set_index('Date', inplace=True)\n    \n    return forecast\n\n# Data preprocessing, split sets\ntrain_df, val_df, val_df_ext, test_df, test_df_ext = split_data(data_CoS, GLOBAL_SETTINGS)\n\nprint('===============================================================')\nprint('')\nprint('Dimensions of split datasets:\\n')\nprint('train:        ',train_df.shape)\nprint('val:          ',val_df.shape)\nprint('val extended: ',val_df_ext.shape)\nprint('test_df:      ',test_df.shape)\nprint('test extended:',test_df_ext.shape)\n\n# Split input sequences\nX_train, Y_train, X_val, Y_val, X_test, Y_test, scaler_gwl = preprocess_data(data_CoS, 'Depth_to_Groundwater_CoS', GLOBAL_SETTINGS)\n\nprint('===============================================================')\nprint('')\nprint('Dimensions of split input sequences:\\n')\nprint('X_train: ',X_train.shape)\nprint('Y_train: ',Y_train.shape)\nprint('X_val:   ',X_val.shape)\nprint('Y_val:   ',Y_val.shape)\nprint('X_test:  ',X_test.shape)\nprint('Y_test:  ',Y_test.shape)\nprint('===============================================================')\n\n\n# **LSTM model**\nmodel = ks.models.Sequential()\nmodel.add(ks.Input(shape=(X_train.shape[1:])))\nmodel.add(ks.layers.LSTM(215, #hidden_size=GLOBAL_SETTINGS['hidden_size'],\n                         unit_forget_bias = True, \n                         dropout = GLOBAL_SETTINGS['dropout']))\nmodel.add(ks.layers.Dense(GLOBAL_SETTINGS['output_seq_length'],\n                          activation='linear'))\n\noptimizer = ks.optimizers.Adam(lr=GLOBAL_SETTINGS['learning_rate'], \n                               epsilon=10E-3, \n                               clipnorm=GLOBAL_SETTINGS[\"clip_norm\"], \n                               clipvalue=GLOBAL_SETTINGS[\"clip_value\"])\n      \nmodel.compile(loss='mse', \n              optimizer=optimizer, \n              metrics=['mae'])\n\nCoS_model = model.fit(X_train, Y_train,\n                      validation_data=(X_val, Y_val),\n                      epochs=GLOBAL_SETTINGS[\"epochs\"], \n                      verbose=2,\n                      batch_size=GLOBAL_SETTINGS[\"batch_size\"],\n                      callbacks=[ks.callbacks.EarlyStopping(monitor='val_loss', \n                                                            mode='min', \n                                                            verbose=0,\n                                                            patience=5)])\n\nprint('===============================================================')\nprint('')\npd.DataFrame(CoS_model.history)\\\n[['loss','mae','val_loss']].plot(figsize=(8,3),title='Training Errors')\nplt.show();\n\n#print(CoS_model.model.summary())\n\n#model.save(\"CoS_model.h5\")\n#CoS_model = ks.models.load_model(\"CoS_model.h5\")\n\n# Validate model\nprint('===============================================================')\nprint('')\nprint('Evaluate model on test data: ')\n\nloss, mae = CoS_model.model.evaluate(X_test, Y_test)\nmse = float(\"{0:.4f}\".format(loss))\nmae = float(\"{0:.4f}\".format(mae))\nprint(f'Test MSE: {mse}, Test MAE: {mae}')\nprint('')\nprint(\"\"\"Note: model outputs MSE and MAE on the scaled test data. \nErrors calculated on original (rescaled) data are more\nrelevant for model evaluation (shown below).\\n\"\"\")\n\nprint('===============================================================')\nprint('')\n\nprint('Make predictions on new (use Test) data and rescale to original units')\npredicted_3 = scaler_gwl.inverse_transform(CoS_model.model.predict(X_test))\nactual_observations_3 = np.asarray(scaler_gwl.inverse_transform(Y_test.reshape(-1,1)))\n\ndf_ = pd.DataFrame({'actual_observations':actual_observations_3.reshape(-1), \n                       'predicted':predicted_3.reshape(-1)}, index=test_df.index)\n# plot\ndf_.plot(figsize=(8, 3)).legend(bbox_to_anchor=(1.01, 1))\nplt.show();\n\n\nprint('===============================================================')\nprint('')\n\nprint('Errors between predicted and actual values(test data):')\nprint('')\n\nCoS_lstm_accuracy = forecast_accuracy(predicted_3.flatten(), \n                                      actual_observations_3.flatten())\nprint(CoS_lstm_accuracy)\n\nprint('')\nprint('total running time: ')\n\nprint('===============================================================')","0ed11b57":"##### Demonstrate how to use model to forecast GWL #####\nprint('===============================================================')\nprint('Demonstrate how to use model to forecast 1 day Change(mtrs)')\nprint('')\n### Fluctation one day in the future\n\n##### INPUTS #####\n### Model requires AT LEAST 30 days of continuous (sequence) daily records for\n### of rainfalls :\n### Tereglio_Coreglia_Antelminelli\n### Rainfall_Calavorno\n### Fabbriche_di_Vallico\n### Pontetetto\n### Gallicano\n### Orentano\n### Borgo_a_Mozzano\n### Piaggione\n### Croce_Arcana\n### Monte_Serra\n\n### of volumes:\n### Volume_CC1, Volume_CC2, Volume_POL, Volume_CSA, Volume_CSAL\n\n### one Engineered feature - Month No\n###################\n\n### Demonstration:\n\n### Predict fluctation of GWL on 6th Jan 2020 \n### Require input data from 5-Jan-2020 back 30 days\ninput_new_data = auser_imputed[(auser_imputed.index >='2019-12-07') & \n                               (auser_imputed.index <='2020-01-05')].copy()\n\n# Add Month No feature    \ninput_new_data['Month_No'] = input_new_data.index.month    \n\n# select only features used in training model:\ninput_new_data = input_new_data[['Month_No',\n                                 'Rainfall_Tereglio_Coreglia_Antelminelli',\n                                 'Rainfall_Calavorno',\n                                 'Rainfall_Fabbriche_di_Vallico',\n                                 'Rainfall_Pontetetto',\n                                 'Rainfall_Gallicano',\n                                 'Rainfall_Orentano',\n                                 'Rainfall_Borgo_a_Mozzano',\n                                 'Rainfall_Piaggione',\n                                 'Rainfall_Croce_Arcana',\n                                 'Rainfall_Monte_Serra',\n                                 'Volume_CC1',\n                                 'Volume_CC2', \n                                 'Volume_CSA', \n                                 'Volume_CSAL',\n                                 'Volume_POL']].copy()\n\n# drop nan if any\ninput_new_data = input_new_data.dropna()\n\n# data shall be ordered by date\ninput_new_data = input_new_data.sort_index()\n\nprint('input data shape: ', input_new_data.shape)\nprint('===============================================================')\nprint('')\n\nprint('Use custom function: forecast_fluctation_with_new_data')\nforecast_gwl = forecast_fluctation_with_new_data(input_new_data, CoS_model, sequence=30)\nprint('')\nprint(forecast_gwl)\n\nprint('')\n\nprint(f'=> END of modeling for {target3} <=')\nprint('===============================================================')","0cedde90":"index=['Sarima LT2', \n       'Sarimax LT2',\n       'LT2 lstm',\n       'Sarima SAL', \n       'Sarimax SAL',\n       'SAL lstm',\n       'Sarima CoS', \n       'Sarimax CoS',   \n       'CoS lstm']\n\naquifer_auser_results = pd.DataFrame.from_dict([Sarima_LT2_accuracy, \n                                                Sarimax_LT2_accuracy,\n                                                LT2_lstm_accuracy, \n                                                Sarima_SAL_accuracy,\n                                                Sarimax_SAL_accuracy,\n                                                SAL_lstm_accuracy,\n                                                Sarima_CoS_accuracy, \n                                                Sarimax_CoS_accuracy, \n                                                CoS_lstm_accuracy])\naquifer_auser_results.index = index\naquifer_auser_results","61574c2f":"print('Baghirova Ellada')\nprint('17th Feb 2021')\nprint('==============The End==============')","683fe0d8":"### Global Imports <a class='anchor' id='gi'><\/a>","3cda8234":"### Features engineering <a class='anchor' id='features'><\/a>","fddc1ee6":"### Comparison of results","ab8c7d35":"### Imputation of missing values <a class='anchor' id='imp'><\/a>\n\nSklearn IterativeImputer is a multivariate imputation method, i.e we use the entire set of features to estimate the missing values. In the IterativeImputer strategy, a machine learning model is built to estimate the missing values. Each feature having missing values is taken as a function of other features. This is done in a round-robin fashion: a feature having missing value is \u201cy\u201d or the dependent variable and other feature columns become \u201cX\u201d or independent variables.","3a703944":"### Methodology <a class='anchor' id='method'><\/a>\n\n**SARIMAX**\n\nSARIMAX is an extention of the stochastic time series ARIMA models, which are often used for medium-range forecasting.\n\nSARIMAX models are denoted SARIMA(p,d,q)(P,D,Q)[S], where S refers to the number of periods in each season, d is the degree of differencing, and the uppercase P, D, and Q refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model. X stands for exogenous variables.\n\nTwo variants are constructed to forecast groundwater level: SARIMA trained on lagged values of target variable and SARIMAX trained on lagged values of target variable and selected exogenous regressors.\n\nWorkflow for both models consists of the following steps:\n\n- Estimate order of differencing 'd' (seasonal decomposition, ADF\/KSS tests) to correct for stationarity\n- Estimate autoregressive and moving averages 'p' and 'q' (ACF\/PACF on differenced series) for smoothing series\n- Estimate seasonality and P, Q terms\n- Train model with estimated parameters\n- Validate model on test data, evaluate diagnostics and tune\n- Add exogenous regressors and repeat the above steps\n- Select model with the best forecasting accuracy\n- Demonstrate forecast of groundwater level 30 days ahead for each observation well.\n\n\n**LSTM**\n\nLong Short-Term Memory (LSTM) network, a special type of recurrent neural network.\n\nLSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series (Source - wikipedia). \n\nAlthough length\/amount of the data available in this study might not be sufficient for deep learning approach, I will assess its potential in forecasting one day ahead change in the groundwater level. In contrast to the SARIMAX models, target variables will be used only for training the model and will not be required as the input for actual forecast. For each observation well LSTM model will output the Change from the previous date.\n\nDiagram shows simple architecture for LT2 LSTM model with input sequence of 30 (days), 16 input features and batch size(None, selected when running the code), and 1 (day) output.","92f27066":"**SAL**\n\nModel predicts **change** in the groundwater level for the next day.","5ecdc1bc":"### SARIMAX <a class='anchor' id='sarimax'><\/a>","9900ea28":"**SAL**","6e5c9172":"**CoS**\n\nModel predicts **change** in the groundwater level for the next day.","f2885e63":"**LT2**\n\nModel predicts **change** in the groundwater level for the next day.","f096e8b1":"### Statistical summary <a class='anchor' id='stat'><\/a>","e5cff67d":"**Recharge season**\n\n*Spring:*\n\nIt is noticable from the seasonal plots that nearly all groundwater rechare in Confined aquifer takes place in spring (months 3-4-5). In the literature this is explained by the fact that there is little evaporation from the soil and little transpiration from plants occuring in the spring, hence ample amounts of rainfall and surface\nwater available for groundwater replenishment. \n\nThe exception is 2010 when highest monthly mean is observed in December (12).\n\n*Summer:*\n\nGroundwater levels are declining in the summer; evapotranspiration is at its peak and most rainfalls do not contribute to groundwater recharge.\n\n*Fall:*\n\nGroundwater levels generally continue to decline or do not increase in the fall; rainfall entering the soil \nmust first recharge unsaturated soil, which was depleted during the summer and little water is left to percolate into groundwater.\n\n*Winter:*\n\nWinter pattern has changed over the years and as 2019 plot shows groundwater level has increased in Dec-Jan, which might be due to the warm winter; ground is not frozen as case could be years ago and rainfalls recharge aquifers.","038e4743":"![model.png](attachment:model.png)","7d70be7c":"## Aquifer Auser","8a756d4a":"### Custom functions and lists <a class='anchor' id='custom'><\/a>","7f8c40b1":"**LT2 - confined zone aquifer**\n\n*LSTM*\n\nAfter training on different combinations of given regressors(rain, vol, hyd, temp), I ended up using total rainfall at 50 days lag, pumping wells POL\/CC1\/CC2 and synthetic feature Month No to compensate for strong seasonality of the target variable; my selection criteria was the minimum out-of-sample forecast error followed by the minimum test MAE. Keras bayesian optimizer was then used to tune hyperparameters (very computationally expensive, will not run in this notebook), that entered final model presented above.  \n\nWhen tried lagged 1-7 days LT2 GWL values as inputs, model performed exceptionally well with R2 almost equal to one and MSE to 0. But closer look at plots showed that LSTM just shifts values by n periods forward and this is not useful result, hence lag input was dropped from the final model.\n\nExcept for R2 test results of LSTM are worse than of SARIMA(X) model. However combination of 'successful' regressors corresponds with insights from data analytics part and suggests that model maybe learned underlying dependency. \n\n*SARIMAX*\n\nBeing more interpretable than LSTM, model allowed iterative selection and statistical assesment of regressors. Regressing LT2 on lags of itself, on level in CoS, pumping from POL\/CSAL and total rain resulted in lowest AIC and best forecasting accuracy. Althought passed significance test at 0.05 alpha, total rain regressor has very low corr coefficient with GWL. This counterintuitive result suggests that there another unidentified source\/mechanism for Auser recharge. Geography points to river Serchio, but I could not find any hydro data to try hypothesis. \n\n**SAL and CoS - unconfined zones aquifer**\n\n*LSTM*\n\nSAL_lstm performed better tha CoS_lstm. For both wells best combination of regressors was found to be total rain (not lagged), extracting\/pumping from POL\/CC1\/CC2\/CSA\/CSAL and synthetic feature Month no.\n\n*SARIMAX*\n\nThere is significant improvement in R2 when transiting from SARIMA to SARIMAX(addition of rain and pumping) for SAL and CoS models. Out-of-sample forecasting errors are much lower than when using LSTM approach. \n\nIn the relevant literature on shallow aquifers Evatranspiration and Air Humidity are two factors often mentioned having impact on GWL fluctations. It is likely that addition of such data can improve model.","6aba26d3":"![Serchio.jpg](attachment:Serchio.jpg)","f52383e0":"**LT2**","f065b97e":"Persistent low on 10\/11th of each month in 2019, and similar \nbehaviour was observed in the earlier years albeit on \ndifferent dates. It is highly unlikely to be natural aquifer fluctation,\nmost probably something to do with measuring instrumentation,\ncalibration and etc. In any case this is unfortunate, as training\nmodels on such data might lower model's forecasting accuracy.","b4314bed":"### [Introduction](#int)\n### [Methodology](#method)\n### [Global Imports](#gi)\n### [Data](#data)\n### [Custom functions and lists](#custom)\n### [Statistical summary](#stat)\n### [Imputation of missing values](#imp)\n### [Descriptive data analysis](#analysis)\n### [Feature engineering](#features)\n### [SARIMAX](#sarimax)\n### [LSTM](#lstm)\n### [Comparison of results](#comparison)\n### [References to works used in this project](#ref)","7a9f0684":"### LSTM <a class='anchor' id='lstm'><\/a>","2ea67f03":"### Descriptive data analysis <a class='anchor' id='analysis'><\/a>\n\n- Examine annual trend\n- Examine seasonal trend\n- Examine weekly\/daily trend\n- Examine impact of rainfall\n- Examine impact of extraction\/volume\n- Summary correlation map","cec8e3fb":"### References <a class='anchor' id='ref'><\/a>\n\n**Acknowledgement for ideas and code used in this study:**\n\n\"Groundwater Level Forecasting with Artificial Neural Networks: A Comparison of LSTM, CNN and NARX\" by Andreas Wunsch, Tanja Liesch, and Stefan Broda, Federal Institute for Geosciences and Natural Resources (BGR), Wilhelmstr. 25-30, 13593 Berlin, Germany Correspondence: Andreas Wunsch (andreas.wunsch@kit.edu).\n\nhttps:\/\/github.com\/faaizuddin\/Time-Series-Forecasting\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/7-scikit-learn-hacks-tips-tricks\/\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html\n\nhttps:\/\/valueml.com\/hyperparameter-tuning-using-tensorflow-in-python\/\n\nhttps:\/\/github.com\/ColdRiver93\/Predictions\/blob\/master\/Predictions\/SARIMAX.py","2d5ad258":"### Data <a class='anchor' id='data'><\/a>","d271ddef":"**CoS**","4e60fc62":"### Introduction <a class='anchor' id='int'><\/a>\n\nThe objective of this study is to analyse impact of meteorological variables and extraction on the fluctation of groundwater level in Auser Aquifer, and derive data-driven model to forecast temporal variation of GWL one day (LSTM) and 30 days (SARIMAX) ahead. \n\nTwo models are developed:\n\n - SARIMAX (seasonal ARIMA with exogenous inputs) and \n - Deep learning RNN LSTM model.\n\nThe project is concluded with comparing forecasting accuracy (in-sample) and  R2 scores for both models.\n\nWe have data on three observation wells for Auser aquifer, one LT2 in confined zone and two SAL & CoS in unconfined zones. Due to the physical differences (outlined below) between types of aquifers, modeling parameters shall be optimized for each well\/target outcome.\n\n**Understanding groundwater level trend in aquifers**\n\nAquifer is a geological formation that is capable of yielding sufficient quantities of water to wells. Ground water occupies the openings in earth materials such as intergranular pores in sands and gravels or cracks or cavities in otherwise solid rock. Porosity and permeability of a given rock or sediment are two primary factors determining quantity of aquifer water yield. (\"A historical summary on ground water levels and trends, division of waters Minnesota DNR, June 1989\").\n\nIt is well established in hydrology science that groundwater levels in aquifers fluctate in the long and short terms, primarily in response to changes in precipitation and extraction\/pumping. Studies also show that modeling of a particular aquifer is not generalizable and subject to local geological and environmental conditions. \n\n**Confined aquifer, observation well LT2**\n\nConfined aquifers are bounded at the top by relatively impermeable formations. The level to which water\nwill rise in a well in a confined aquifer is termed the potentiometric surface. Water level fluctuations in confined observation wells tend to occur more rapidly and changes are much larger. Recharge to the confined aquifer occurs primarily in the upland area where the confining layer is not present and to a lesser extent through slow downward leakage through the confining layer.\n\n\n**Unconfined aquifer, observation well SAL and CoS**\n\nUnconfined aquifers generally are close to the land surface and are exposed to the atmosphere through pores in the overlying formation. The upper surface of the saturated zone in this aquifer is called the water table. The unconfined aquifer, having only permeable unsaturated material above it, is recharged relatively quickly as water infiltrates into the ground."}}