{"cell_type":{"dd1ece04":"code","25628a86":"code","bc819848":"code","b4051996":"code","1468a5ba":"code","b31062d5":"code","27f2f605":"code","9876a4c9":"code","f475de42":"code","fc43ef31":"code","d3e09af7":"code","9c2f86d0":"code","d93ad61b":"code","2dd4d20e":"code","47a7f23c":"code","8ed80fff":"code","de2cc7d2":"code","afc70a24":"code","51de5422":"code","617fa947":"code","1eefd482":"code","faeda5ad":"code","634f6d58":"code","d2724233":"code","935988b2":"code","87fe69d8":"code","2dd20a36":"code","479f3be7":"code","763cf431":"code","6d36cc84":"code","8a649b26":"code","2f3cc0cf":"code","afb0fb49":"code","4d626cac":"code","f2b25f30":"code","0482f36c":"code","b0b19482":"markdown","96d8283d":"markdown","69ea3489":"markdown","00cb38a9":"markdown","cf78f710":"markdown","3c32e626":"markdown","1ce32d6b":"markdown","347bc2c5":"markdown","281e830c":"markdown","d77cde4c":"markdown","ace733b4":"markdown","89ee4626":"markdown","cdc44f2e":"markdown","5c6406dc":"markdown","713c3279":"markdown","e3c2485f":"markdown","88cc1b57":"markdown","166e64e1":"markdown","3d76bb0b":"markdown","1e298c1c":"markdown","95f412a6":"markdown","5412c99d":"markdown","15431846":"markdown","55c3c018":"markdown","96579b18":"markdown","5d60a513":"markdown","f1a18582":"markdown","86bf3434":"markdown","4a17a29f":"markdown","c48a5120":"markdown","c2819c3c":"markdown","e3ed2fa0":"markdown","c0188f08":"markdown","f66d4001":"markdown","ca21782b":"markdown","9aa3682b":"markdown","ac8e645f":"markdown","3e241b60":"markdown","1ab73f38":"markdown","83c4694b":"markdown"},"source":{"dd1ece04":"!pip install pyspark","25628a86":"import os\n\nimport pandas as pd\nimport sklearn as sk\nimport math\nimport psutil\nfrom time import time\nimport calendar\nimport json\n\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\n\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col,unix_timestamp,to_date,min,max,isnull,count,when\nfrom pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType,TimestampType\nimport pyarrow.parquet as pq\n%pylab inline\n\n\n","bc819848":"#Initialise the Spark context\nos.environ[\"PYSPARK_PYTHON\"]=\"python3\"\nos.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n\nNumCores=4 #Kaggle offers 4 CPU cores\/threads.  Change for local machine\n\n\nSpark = SparkSession\\\n.builder\\\n.master(f'local[{int(NumCores)}]')\\\n.appName(\"PBS_Kids_Spark\")\\\n.config(\"spark.executor.memory\", \"4g\") \\\n.config(\"spark.driver.memory\", \"14g\") \\\n.config(\"spark.memory.offHeap.enabled\",True)\\\n.config(\"spark.memory.offHeap.size\",\"10g\")\\\n.config(\"spark.driver.maxResultSize\",0)\\\n.config(\"spark.sql.execution.arrow.enabled\",True)\\\n.getOrCreate()\n","b4051996":"%%time\n#Load data to DataFrames\n\nTrainDf=Spark.read.csv('..\/input\/data-science-bowl-2019\/train.csv',header=True,quote='\"',escape='\"') #quote and escape options required to parse double quotes\nTrainlabelsDf=Spark.read.csv('..\/input\/data-science-bowl-2019\/train_labels.csv',header=True,quote='\"',escape='\"')\nTestDf=Spark.read.csv('..\/input\/data-science-bowl-2019\/test.csv',quote='\"',header=True,escape='\"')\n\n#Load smaller files as panda Dfs\nSpecsDf=pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nsample_submissionDf=pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","1468a5ba":"%%time\n#What is the shape of the data?\nprint(f'rows :{TrainDf.count()}, columns: {len(TrainDf.columns)}')\n#I considered using countApprox to speed up, but the required conversion to rdd slowed things down","b31062d5":"%%time\nTrainDf.createOrReplaceTempView(\"Train\")\nkeepidDf=Spark.sql(f'SELECT installation_id from Train WHERE type=\"Assessment\"').dropDuplicates()\nkeepidDf.createOrReplaceTempView(\"keepid\")\nColumns=','.join(['Train.'+a for a in TrainDf.columns])\nTrainDf=Spark.sql(f'SELECT {Columns} from Train INNER JOIN keepid ON Train.installation_id=keepid.installation_id')\\\n.repartition(NumCores) \n#repartition to ensure data is evenly spread to workers after the filter","27f2f605":"%%time\n#convert timeestamp field to datetime.  \nTrainDf=TrainDf.withColumn('timestamp',unix_timestamp(col('timestamp'), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\").cast(\"timestamp\"))","9876a4c9":"%%time\nNullDf=TrainDf.agg(*[count(when(isnull(c),c)).alias(c) for c in TrainDf.columns])\nNullDf.show()","f475de42":"TrainDf=TrainDf.na.drop()","fc43ef31":"%%time\nprint(f'rows :{TrainDf.count()}, columns: {len(TrainDf.columns)}')","d3e09af7":"print(f'rows :{keepidDf.count()}, columns: {len(keepidDf.columns)}')","9c2f86d0":"%%time\n''''We want to put the data in a pandas dataframe in order to do graphs etc.  \nThe most memory and time efficient method to convert from Spark to Pandas is via a parquet file save\nand read via PyArrow.  But Kaggle machines doen't have sufficient memory for this, so I've commented that code out and used a normal Pandas dataframe load of the .csv source\n'''\n\n# TrainDf.write.mode(\"overwrite\").save('trainDf.parquet')  #Uncomment if using local machine\n# TrainPdDf=pq.read_table('trainDf.parquet').to_pandas()\n\nTrainPdDf=pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv', parse_dates= ['timestamp']) #comment out if using local machine","d93ad61b":"plt.rcParams.update({'font.size': 16})\n\nfig = plt.figure(figsize=(12,10))\nax1 = fig.add_subplot(211)\nax1 = sns.countplot(y=\"type\", data=TrainPdDf, color=\"blue\", order = TrainPdDf.type.value_counts().index)\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(212)\nax2 = sns.countplot(y=\"world\", data=TrainPdDf, color=\"blue\", order = TrainPdDf.world.value_counts().index)\nplt.title(\"number of events by world\")\n\nplt.tight_layout(pad=0)\nplt.show()","2dd4d20e":"plt.rcParams.update({'font.size': 12})\n\nfig = plt.figure(figsize=(12,10))\nse = TrainPdDf.title.value_counts().sort_values(ascending=True)\nse.plot.barh()\nplt.title(\"Event counts by title\")\nplt.xticks(rotation=0)\nplt.show()","47a7f23c":"plt.rcParams.update({'font.size': 12})\n\nfig = plt.figure(figsize=(12,10))\nse = TrainPdDf.installation_id.value_counts().sort_values(ascending=False).head(200)\nse.plot.bar()\nplt.title(\"Event counts by installation id (top 200)\")\nplt.show()","8ed80fff":"%%time\nCounts=TrainDf.groupBy('installation_id').agg(count('installation_id').alias('NumEvents'))\nCounts.select(\"NumEvents\").describe().show()\nprint(f'50% and 90% quartile :{Counts.approxQuantile([\"NumEvents\"],[0.5,0.9],0.05)}')  #Use approxQuantile rather than Quantile for speed","de2cc7d2":"%%time\nCounts.createOrReplaceTempView(\"Counts\")\nwebbotsDf=Spark.sql(f'SELECT * from Counts WHERE NumEvents>15000').select('installation_id')\nprint(f'Number of suspected webbots: {webbotsDf.count()}')\nNotWebbotsDf=Spark.sql(f'SELECT * from Counts WHERE NumEvents<=15000').select('installation_id')\nNotWebbotsDf.registerTempTable(\"NotWebbots\")  \nTrainDf.createOrReplaceTempView(\"Train\")  \n\nColumns=','.join(['Train.'+a for a in TrainDf.columns])\nTrainDf=Spark.sql(f'SELECT {Columns} from Train \\\nINNER JOIN NotWebbots ON Train.installation_id=NotWebbots.installation_id')\\\n.repartition(NumCores) \n#repartition to ensure data is evenly spread to workers after the filter","afc70a24":"def get_time(df):\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\ntrain = get_time(TrainPdDf)","51de5422":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('date')['date'].count()\nse.plot()\nplt.title(\"Event counts by date\")\nplt.xticks(rotation=90)\nplt.show()","617fa947":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('dayofweek')['dayofweek'].count()\nse.index = list(calendar.day_abbr)\nse.plot.bar()\nplt.title(\"Event counts by day of week\")\nplt.xticks(rotation=0)\nplt.show()","1eefd482":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('hour')['hour'].count()\nse.plot.bar()\nplt.title(\"Event counts by hour of day\")\nplt.xticks(rotation=0)\nplt.show()","faeda5ad":"%%time\n#What is the shape of the data?\nprint(f'rows :{TestDf.count()}, columns: {len(TestDf.columns)}')","634f6d58":"TestDf.select('installation_id').dropDuplicates().count()","d2724233":"sample_submissionDf.shape[0]","935988b2":"%%time\nTestDf.createOrReplaceTempView(\"Test\")\nSpark.sql(f'SELECT Train.title from Train INNER JOIN Test ON Train.installation_id=Test.installation_id').count()","87fe69d8":"%%time\n#convert timeestamp field to datetime.  \nTestDf=TestDf.withColumn('timestamp',unix_timestamp(col('timestamp'), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\").\\\n                         cast(TimestampType()))","2dd20a36":"%%time\nTestDates=TestDf.select(to_date(TestDf['timestamp']).alias('date'))\nTrainDates=TrainDf.select(to_date(TrainDf['timestamp']).alias('date'))\nTest_min_date, Test_max_date = TestDates.select(min(\"date\"), max(\"date\")).first()\nTrain_min_date, Train_max_date = TrainDates.select(min(\"date\"), max(\"date\")).first()\nprint(f'The date range in train is: {Train_min_date} to {Train_max_date}')\nprint(f'The date range in test is: {Test_min_date} to {Test_max_date}')","479f3be7":"%%time\n''''We want to put the data in a pandas dataframe in order to do graphs etc.  \nThe most memory and time efficient method to convert from Spark to Pandas is via a parquet file save\n'''\n\n# TrainlabelsDf.write.mode(\"overwrite\").save('TrainlabelsDf.parquet')  #uncomment if using local machine\n# TrainlabelsPdDf=pq.read_table('TrainlabelsDf.parquet').to_pandas()\n\nTrainlabelsPdDf=pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv') #comment out if using local machine","763cf431":"plt.rcParams.update({'font.size': 22})\n\nplt.figure(figsize=(12,6))\nsns.countplot(y=\"title\", data=TrainlabelsPdDf, color=\"blue\", order = TrainlabelsPdDf.title.value_counts().index)\nplt.title(\"Counts of titles\")\nplt.show()","6d36cc84":"plt.rcParams.update({'font.size': 16})\n\nse = TrainlabelsPdDf.groupby(['title', 'accuracy_group'])['accuracy_group'].count().unstack('title')\nse.plot.bar(stacked=True, rot=0, figsize=(12,10))\nplt.title(\"Counts of accuracy group\")\nplt.show()","8a649b26":"TrainlabelsPdDf[TrainlabelsPdDf.installation_id == \"0006a69f\"]","2f3cc0cf":"%%time\nSpark.sql(f'SELECT {Columns} from Train WHERE event_code = 4100 AND installation_id = \"0006a69f\"\\\nAND title == \"Bird Measurer (Assessment)\"').toPandas()\n","afb0fb49":"%%time\nTrainlabelsDf.createOrReplaceTempView(\"Trainlabel\")\nUniqueTrainlabelsDf=Spark.sql(f'SELECT installation_id from Trainlabel').dropDuplicates()\nUniqueTrainlabelsDf.createOrReplaceTempView(\"UnqTrainlabel\")\nkeepidDf.createOrReplaceTempView(\"keepid\")  #we created a list of unique train ids earlier\nSpark.sql(f'SELECT keepid.installation_id as one, UnqTrainlabel.installation_id as two FROM keepid \\\nLEFT JOIN UnqTrainlabel ON keepid.installation_id=UnqTrainlabel.installation_id \\\nWHERE UnqTrainlabel.installation_id IS NULL').count()","4d626cac":"%%time\nColumns=','.join(['Train.'+a for a in TrainDf.columns])\nTrainDf=Spark.sql(f'SELECT {Columns} from Train \\\nINNER JOIN UnqTrainlabel ON Train.installation_id=UnqTrainlabel.installation_id')\\\n.repartition(NumCores) \n#repartition to ensure data is evenly spread to workers after the filter","f2b25f30":"Count1=TrainlabelsDf.count()\nCount2=TrainlabelsDf.select('game_session').dropDuplicates().count()\nprint(f'Number of rows in train_labels: {Count1}')\nprint(f'Number of unique game_sessions in train_labels: {Count2}')\n","0482f36c":"#Uncomment these if running on local machine.  Don't need to save in Kaggle, will load data into subsequent notebooks\n# TrainDf.write.mode(\"overwrite\").save('TrainDf.parquet')\n# TestDf.write.mode(\"overwrite\").save('TestDf.parquet')\n# TrainlabelsDf.write.mode(\"overwrite\").save('TrainlabelsDf.parquet')","b0b19482":"When we exclude the Bird Measurer\/4100 rows we get the correct match with the numbers in train_labels for this installation_id (4 correct, 12 incorrect)","96d8283d":"# Understand the training data","69ea3489":"As we can not train on those installation_id's anyway, I am taking them out of the train set. This reduces our train set further from 8.3 million rows to 7.7 million.","00cb38a9":"[MH]  Check if any installation IDs are overrepresented - likely webbots","cf78f710":"Below, you can see that a lot of Chest Sorter assessments were never solved. Bird Measurer also seems hard with a relatively small amount solved on the first attempt.","3c32e626":"So we have 11 million rows and just 11 columns. However, Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nAs there is no point in keeping training data that cannot be used for training anyway, I am getting rid of the installation_ids that never took an assessment","1ce32d6b":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel):  Spark:  8.6 s; pandas 21.9 microseconds.  ","347bc2c5":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel):  Spark: 10.4s; pandas 310ms.  ","281e830c":"I will first visualize some of the existing columns.  ","d77cde4c":"<table><tr><td><img src=\"https:\/\/spark.apache.org\/images\/spark-logo-trademark.png\"><\/td><td><img src=\"https:\/\/bento.cdn.pbs.org\/hostedbento-prod\/blog\/20170114_200556_794501_pk-channel-16x9.jpeg\"><\/td><\/tr><\/table>","ace733b4":"[MH] And so the exploration is done.  Save the parquet files for use in the next phase:  feature engineering","89ee4626":"## Understanding and visualizing the train labels","cdc44f2e":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel): Spark: 11.6s; pandas 780 ms","5c6406dc":"The date range is more or less the same, so we are talking about a dataset that seems (randomly) split on installation_id. Well actually \"sort of\" as Kaggle seems to have done this on installation_id's with assessments first, and added the \"left-overs\" with no assessments taken to the train set.","713c3279":"So we have 1.1 million rows on a thousand unique installation_ids in the test set. Below, you can see that we have this same amount of rows in the sample submission. This means that there are no installation_ids without assessment in the test set indeed.","e3c2485f":"# Introduction","88cc1b57":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel):  Spark:  17.4s; pandas 5.1s.  ","166e64e1":"Now the question arises: Could there be installation_id's who did assessments (we have already taken out the ones who never took one), but without results in the train_labels? As you can see below, yes there are 628 of those.\n","3d76bb0b":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel): Spark: 22.4s; pandas 288 ms","1e298c1c":"As you can see we have dropped about 3 million rows","95f412a6":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel):  Spark:  2.7s; pandas 46.1s.  Not really apples-to-apples comparison due to lazy evaluation in Spark","5412c99d":"# Understanding the test set","15431846":"[MH] Time to populate a Pandas dataframe:  Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel):  Spark:  52s; pandas 46.1s.  ","55c3c018":"For those like myself looking to familiarise themselves with big data tools, here is my data exploration of the PBS Kids dataset using PySpark.   Using PySpark for exploration on a training dataset of only 4 Gb is a case of overkill:  big data tools are typlically used where the dataset won't fit in RAM,otherwise the parallelisation overhead will generally make things slower than single machine toolsets like scitkitlearn.  And due to Spark's 'lazy evaluation\" method it's certainly the wrong tool for the job for data exploration where lots of different metrics are required of a dataset.   But it was an opportunity to consolidate skills from the edX subject I've just completed \"Big Data Analytics using Spark\"  https:\/\/courses.edx.org\/courses\/course-v1:UCSanDiegoX+DSE230x+3T2019\/course\/.   I don't expect to achieve any great leaderboard result or push the boundaries of machine learning alorithims, rather to practise my PySpark skills.","96579b18":"The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\nI started by visualizing some of these columns","5d60a513":"I will now add some new columns based on the timestamp, and visualize these.","f1a18582":"Another thing that I would like to check is if there is any overlap with regards to installation_id's in the train and test set. As you can see, there are no installation_id's that appear in both train and test.","86bf3434":"The number of unique installations in our \"smaller\" train set is now 4242.","4a17a29f":"When looking at the day of the week, we see no major difference. Of course, we are talking about kids who don't have to go to work ;-)","c48a5120":"When looking at the numbers by hour of the day, I find the distribution a little bit strange. Kids seem up late at night and don't do much early in the morning. Has this something to do with time zones perhaps?  ","c2819c3c":"Check if game_session alone is the unique identifier in train_labels ","e3ed2fa0":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel): Spark: 24.3s; pandas 5.9s","c0188f08":"## Load modules and set up Spark context","f66d4001":"What about the date ranges?","ca21782b":"**You should run this notebook on a local machine** - Kaggle only provides 4.9G of scratch memory, too small for Pyspark as it tends to chew through memory, espcially when doing 'windows' functions.  I've had to use normal Pandas dataframes for the visualisation part of the notebook so it will run on Kaggle.\n\nWorth noting  that the PBS competion doesn't allow kernels with internet access, and hence pyspark.  But you could run Pyspark offline and pickle the resulting models for use in the competition kernels.  \n\nThis notebook covers initial exploration, and I'll post others on feature engineering and on modelling.  I've based the analysis on the popular kernel \"Data Science Bowl 2019 data exploration\"  https:\/\/www.kaggle.com\/erikbruin\/data-science-bowl-2019-data-exploration by Erkin Bruin https:\/\/www.kaggle.com\/erikbruin.  I've run the results on mt laptop in both Erkin's sensible single machine approach and in Pyspark, both to ensure accurancy and compare run-times.  All commentary is Eric's verbatim, unless noted with \"[MH]\".   ","9aa3682b":"As the match between the train dataframe and the train_labels dataframe is not straightforward, it tried to figure out how these dataframes are to be matched by focussing on just one particular installation_id.","ac8e645f":"[MH] Check for null values -there are 1,740 in timestamp.  A tiny fraction for 11m rows - let's drop them","3e241b60":"[MH] Given the median number of events per id is ~1.3k and 90% are less than ~7k events, it seems safe to assume that >15k events are  webbots - or else children with way too much screentime ;-)  Let's remove","1ab73f38":"[MH] Comparison with pandas method (as per \"Data Science Bowl 2019 Data Exploration\" kernel): Spark: 40.6s; pandas 100s","83c4694b":"From Kaggle: The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\nHowever, in the first version I already noticed that I had one attempt too many for this installation_id when mapping the rows with the train_labels for. It turns out that there are in fact also assessment attemps for Bird Measurer with event_code 4100, which should not count (see below). In this case that also makes sense as this installation_id already had a pass on the first attempt"}}