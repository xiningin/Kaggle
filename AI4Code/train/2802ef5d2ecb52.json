{"cell_type":{"57838dbb":"code","2c01ec56":"code","6bc22dd0":"code","e0b6c6ef":"code","d049b999":"code","16f862e7":"code","fafae36a":"code","54aff0de":"code","118448e5":"code","c78350e1":"code","b088e253":"code","1bb6c4c5":"code","f5553859":"code","4ceb095e":"code","e7c86380":"code","a2132f2b":"code","b870ff68":"code","c45ed655":"code","14467075":"code","c95bf412":"code","1312ceb9":"code","2113fef6":"code","53dd67ae":"code","070f388a":"code","d78c5caf":"code","29edafeb":"code","e2415277":"code","da5abdb7":"code","7f7bfeb7":"code","ae5a0b15":"code","e3b839ce":"code","64aabe84":"code","b3bb8023":"code","9ae6debb":"code","83ae9e92":"code","129d0c24":"code","f8484d1c":"code","8cf2ef2b":"code","44d4ed35":"code","f3e282ba":"code","073a35a5":"code","94cd0342":"code","22207940":"code","204d71be":"code","94aadde7":"code","6cea6574":"code","38f8816f":"code","1600cfdd":"code","cc504319":"code","e9bdf53e":"code","d9454111":"code","617bc62e":"markdown","15921763":"markdown","7a4d6c7f":"markdown","89480926":"markdown","2f15f9cd":"markdown","50ebdbee":"markdown","282f06ab":"markdown","713fd404":"markdown","51136eac":"markdown","cba1496d":"markdown","6f1b1ff8":"markdown","c5942fef":"markdown","8b437363":"markdown","b3268cea":"markdown","85d6407e":"markdown"},"source":{"57838dbb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2c01ec56":"#Standard data science libraries.\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.cluster import KMeans #for kmeans algorithm\n\n#For dimensionality reduction.\nfrom sklearn.decomposition import PCA #pca from decomposition module.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import decomposition #decomposition module\n\n#Plotting params.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport seaborn as sb\nrcParams['figure.figsize'] = 12, 4\nsb.set_style('whitegrid')\n\nnp.random.seed(42) # set the seed to make examples repeatable","6bc22dd0":"#Since the files are zipped, they need to be imported with the following approach. \n\nprior = \"order_products__prior.csv\"\norder_train = \"order_products__train.csv\"\norders = \"orders.csv\"\nproducts = \"products.csv\"\naisles = \"aisles.csv\"\ndepartments = \"departments.csv\"","e0b6c6ef":"import zipfile # Unzips the files\nfrom subprocess import check_output    \n\n#Prior Dataset\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+prior+\".zip\",\"r\") as z:\n    z.extractall(\".\")\nprior = pd.read_csv(\"order_products__prior.csv\")\n\n#Order_Train Dataset.\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+order_train+\".zip\",\"r\") as z:\n    z.extractall(\".\")\norder_train = pd.read_csv(\"order_products__train.csv\")\n\n#Orders Dataset.\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+orders+\".zip\",\"r\") as z:\n    z.extractall(\".\")\norders = pd.read_csv(\"orders.csv\")\n\n#Products\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+products+\".zip\",\"r\") as z:\n    z.extractall(\".\")\nproducts = pd.read_csv(\"products.csv\")\n\n#Aisles\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+aisles+\".zip\",\"r\") as z:\n    z.extractall(\".\")\naisles = pd.read_csv(\"aisles.csv\")\n\n#Departments\nwith zipfile.ZipFile(\"\/kaggle\/input\/instacart-market-basket-analysis\/\"+departments+\".zip\",\"r\") as z:\n    z.extractall(\".\")\ndepartments = pd.read_csv(\"departments.csv\")","d049b999":"# Inspect all the dataframes, join them and make a combined df to form clusters. ","16f862e7":"#Put them in a list to print shape.\ncombined_df_list = [products,orders, departments, aisles, prior, order_train]","fafae36a":"#Check the size of the datasets.\nfor i in combined_df_list:\n    print (i.shape)\n#There are two df's which are very large in size, subset to use it on local machine with limited compute power.\ndel combined_df_list","54aff0de":"#Products Dataframe\nproducts.head(2)","118448e5":"#Departments Dataframe\ndepartments.head(2)","c78350e1":"#Aisles Dataframe - Products are kept in aisles.\naisles.head(2)","b088e253":"#Orders Dataframe\norders.head(2)","1bb6c4c5":"#Orders Train Dataframe\norder_train.head(2)","f5553859":"#Products in Orders (Prior) - These files specify which products were purchased in each order. Contains Previous Orders.\nprior.head(2) #notice the reordered feature.","4ceb095e":"#Since the dataframe is too big for in memory computation, reducing prior to only 500k rows. \nprior = prior [:500000]","e7c86380":"#Merge 1 - Prior and Orders DF (Joining Orders to prior df)\n#Combining the Prior and Orders dataframe - shows which user ordered what products and in which order.\ndf1 = pd.merge(prior, orders, on= 'order_id')\ndf1.head(2)","a2132f2b":"#Merge 2\n#Combining the department and aisle df's to product df. \nprod_aisles = pd.merge(products, aisles, on = 'aisle_id')\ndf2 = pd.merge(prod_aisles, departments, on = 'department_id')\ndf2.head(2)","b870ff68":"#Combining df1 anf df2\ncombined_df = pd.merge(df1, df2, on = 'product_id').reset_index(drop=True)\ncombined_df.head(2)","c45ed655":"#Check Nulls\nsb.heatmap(combined_df.isnull(), cbar=True)","14467075":"#These are null values in the feature 'days_since_prior_order'\ncombined_df[combined_df['days_since_prior_order'].isnull()].head(2)\n\n#To be dealt with later, as this does not influence the current scope of work.","c95bf412":"#Most ordering customer. Favourite Customer?\npd.DataFrame(combined_df.groupby('user_id')['product_id'].count()).sort_values('product_id', ascending=False).head(2)\n\n#User_id = 142131","1312ceb9":"#Most ordered items.\npd.DataFrame(combined_df['product_name'].value_counts()).head(5)","2113fef6":"#Most sold items as per aisle.\npd.DataFrame(combined_df['aisle'].value_counts()).head(5)","53dd67ae":"combined_df.shape","070f388a":"#Using aisles and user_id. This shows the users that purchased items from which aisle.\nuser_by_aisle_df = pd.crosstab(combined_df['user_id'], combined_df['aisle'])\nuser_by_aisle_df.head(2)","d78c5caf":"#The final dataframe has about 134 features.\nuser_by_aisle_df.shape","29edafeb":"#Standardization is not needed in this case.\nuser_by_aisle_df.describe() #this confirms that the values dont need to be standardized since they're all 'quantity'.","e2415277":"#Taking array of 'user_by_aisle_df'. To use for elbow method.\nX = user_by_aisle_df.values","da5abdb7":"user_by_aisle_df.head()","7f7bfeb7":"#Implementing the Elbow method to identify the ideal value of 'k'. \n\nks = range(1,10) #hit and trial, let's try it 10 times.\ninertias = []\nfor k in ks:\n    model = KMeans(n_clusters=k)    # Create a KMeans instance with k clusters: model\n    model.fit(X)                    # Fit model to samples\n    inertias.append(model.inertia_) # Append the inertia to the list of inertias\n    \nplt.plot(ks, inertias, '-o', color='black') #Plotting. The plot will give the 'elbow'.\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","ae5a0b15":"#Seeing the above plot, the ideal value for cluster (k) should be between 5 and 6 - since the features beyond these values,\n# do not explain much of the variability in the dataset. \n\n#Decomposing the features into 6 using PCA (seeing the above plot, n_components = 6)\npca = decomposition.PCA(n_components=6)\npca_user_order = pca.fit_transform(X)\n\n#You can do hit and trial here to change the number of components and see how much variation in the data \n#is explained by the chose n_components.","e3b839ce":"#Checking the % variation explained by the 6 pca components.\npca.explained_variance_ratio_.sum()\n#More than half (50%) of the variability in the data can be explained by just 6 components.","64aabe84":"# Plot the explained variances to verify the variation.\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_ratio_, color='black')\nplt.xlabel('PCA features')\nplt.ylabel('variance %')\n\n#A majority of the variance can be explained by just five to six components. Anything beyond that does not capture much of the variation in the dataset.","b3bb8023":"#Chosen components.\nPCA_components = pd.DataFrame(pca_user_order)\nPCA_components.head(5)","9ae6debb":"#Build the model (kmeans using 5 clusters)\nkmeans = KMeans(n_clusters=5)\nX_clustered = kmeans.fit_predict(pca_user_order) #fit_predict on chosen components only.","83ae9e92":"#Visualize it.\n\nlabel_color_mapping = {0:'r', 1: 'g', 2: 'b',3:'c' , 4:'m'}\nlabel_color = [label_color_mapping[l] for l in X_clustered]\n\n#Scatterplot showing the cluster to which each user_id belongs.\nplt.figure(figsize = (15,8))\nplt.scatter(pca_user_order[:,0],pca_user_order[:,2], c= label_color, alpha=0.3) \nplt.xlabel = 'X-Values'\nplt.ylabel = 'Y-Values'\nplt.show()","129d0c24":"#This contains all the clusters which are to be mapped to each user_id in the user_by_aisle_df.\nX_clustered.shape","f8484d1c":"#Mapping clusters to users.\nuser_by_aisle_df['cluster']=X_clustered","8cf2ef2b":"#Checking cluster concentration. \nuser_by_aisle_df['cluster'].value_counts().sort_values(ascending = False)","44d4ed35":"#Check out cluster mapping.\nuser_by_aisle_df.head()","f3e282ba":"# Apriori Algorithm - Association Rules\n\n#Some Theory - just a little.\n\n#Fomatting Data\n#Applying Apriori to get support in order to see what items go well together.\n#Applying Association Rules to get the Confidence and Lift Scores\n#How to come up with up-selling and cross-selling stratgies. The END.","073a35a5":"#Importing Libraries\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","94cd0342":"#Checking with only a few samples. Concept is replicable.\nnp.random.seed(942) # set the seed to make examples repeatable\ndf2 = combined_df.sample(n=1000)[['user_id','product_name']]\nbasket = pd.crosstab(df2['user_id'],df2['product_name']).astype('bool').astype('int')\ndel df2","22207940":"#Checking and removing index.\nbasket=basket.reset_index(drop=True)\nbasket.index","204d71be":"#Lets see if the format is correct.\nbasket.head(2)","94aadde7":"#Calling apriori algorithm on dummified data - basket.\nfrequent_itemsets=apriori(basket, min_support=0.00002, use_colnames=True).sort_values('support', ascending=False) \n\n#These are all the POPULAR (Top 20) items purchased from the store.\nfrequent_itemsets.head(20)","6cea6574":"#Lets check the length of the item sets using a tini lambda function.\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets.head()","38f8816f":"#Putting a new filter to get all items with length 3 or more. (this means items purchased together)\nfrequent_itemsets[frequent_itemsets['length'] >= 3]","1600cfdd":"#FIRST PART - CONFIDENCE\n\n#For association rules, metric can be either confidence or lift. Second argument is minimum threshold (0.5).\n#Trying confidence first.\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\nrules.head()\n\n#The minimum confidence level starts at 0.5 (confidence column). How likely is it for item C to be purchased if A was purchased?\n#Hence if 'Kidz All Natural Baked Chicken Nuggets' was purchased, it is extremely likely that 'Quart Sized Easy Open Freezer Bags' will be purchased in the same transaction. \n#Confidence tells us if item C is purchased, how likely will item A be purchased too.","cc504319":"#SECOND PART - LIFT\n\n#Changing metric to lift. Minimum threshold is 1.\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules.head()\n\n#Lift tells how likely are items bought together as opposed to being bought individually.\n#Row 0: If Kidz All Natural Baked Chicken Nuggets is purchased, then Quart Sized Easy Open Freezer Bags will be purchased too.\n#Row 1: If Quart Sized Easy Open Freezer Bags item is purchased, then Kidz All Natural Baked Chicken Nuggets will be purchased. As there is SLIGHTLY more confidence in row1 (compared to row0).\n","e9bdf53e":"#THIRD PART - CONFIDENCE AND LIFT\n\n#Select life>5 and confidence >.5\nrules[(rules['lift'] >= 5) & (rules['confidence']>= 0.5)] \n\n#Now these items will be mostly be bought together. So you can make Cross-sell\/upsell strategies based on that.","d9454111":"#Next steps - Some tuning to improve performance. ","617bc62e":"### Using Elbow Method \n\nThe bend of the elbow is where the ideal value of k lies.","15921763":"### Using PCA \n\nThe data has been reduced to just 6 components, which explain about 50% variation in the data.","7a4d6c7f":"ASSOCIATION RULES\n\n* First Part - Confidence\n* Second Part - Lift\n* Third Part - Confidence + Lift","89480926":"# Objective\n\nTo analyse customer, products, aisle, prior (purchase history) and order data to create customer segments using k-means algorithm and Apriori Association Rules. Use case - to come up with upselling and cross selling strategies.\n\nPCA will be used to lower the dimensions of the dataset. The data will then be fed into k-means.\nThe ideal value of 'k' will be computed using the elbow method and PCA.\n\n- Customer Segmentation (Part 1)\n- Association Rules (Part 2)\n\nIn the second part, apriori algorithm and association rules have been used to identify the products with the optimum support, lift and confidence metrics. They aid decision making by formulating cross selling and up sell opportunity around products bought together.","2f15f9cd":"# Data Exploration - Mini Version\n\nA lot more will be covered on this in the subsequent commits.","50ebdbee":"# Data Modeling","282f06ab":"### Build The Model - K Means\n\nOnce the dimensionality has been lowered, the model can be built with the most chosen paramters.","713fd404":"Since there are 134 features, they need to be lowered to a lower dimension with only the most important features.\n\nPCA will be implemented using elbow method to compute the ideal value of 'k' clusters.\n\nPCA is most common form of SVD (Singular Value Decomposition), SVD essentially decomposes the matrix into other resultant matrices to reduce information redundancy and noise. In the case above, the idea is to reduce the number of features from 134 to only the most relevant ones that capture the essence of the data.","51136eac":"## Preparing Data ","cba1496d":"# PART 1: Customer Segmentation\n\nThe first part revolves around inspecting the data and segmenting customers into clusters using K-Means.","6f1b1ff8":"## Dimensionality Reduction using Elbow Method and PCA ","c5942fef":"# PART 2: Association Rules","8b437363":"#### Once the df's have been inspected, the next step is to combine them on primary and foreign keys. \n\nMerge 1 - Combining the orders to prior df. This will give the products that were ordered in each order. \n\nMerge 2 - Combining the department and aisle df's to product df. ","b3268cea":"Assumptions\/Caveats\n\nTransaction Data has to be in sparse format apriori algorithm.\n\n* Fast\n* Works well with less data\n* Few (if any) feature engineering requirement \n\nIts a process that deploys pattern recognition to identify and quantify relationships between different yet related items.\nAction 1: Place eggs and bread together so the customer does not need to walk, to get the items.\nAction 2: Advertise eggs to bread buyer so they buy both together. Once you know the products are related.\n\n\nEx: 5000 total transactions, 500 were bread purchases, 350 eggs, and 150 both eggs and bread.\n\nMeasure Association\n\n*  Support: Relative freq of item within a transaction dataset. Support for bread is (500\/5000) = 0.1\n\n*  Confidence: What is the confidence that eggs (item2) will be bought if bread (item1) was purchased. EX: (150\/5000)\/(500\/5000) = 30%. There is a 30% chance that eggs will be bought when bread is bought.\n \n*  Lift: A value that shows the relationship between two items. It is the confidence A->C\/support(C)\n \n    * If lift > 1: A is highly associated with C (Eggs will be bought if Bread, item 'A' was bought)\n    * If lift < 1: If A was purchased, it is unlikely C will be purchased too.\n    * If lift = 1: No association betweem item A and C.\n\nLift = 0.3\/(350\/5000) = 4.28","85d6407e":"#### But how to choose the number of principal components for PCA?\n\nElbow Method or K-Means\n\nImportant Note: The data does not need to be standardized since all the items bought by the user is quantity of units bought. "}}