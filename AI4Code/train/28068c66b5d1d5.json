{"cell_type":{"211226bd":"code","70d69c30":"code","4f1fc797":"code","647dc3e8":"code","c468c0aa":"code","2ba4eba7":"code","a665e344":"code","1cb9c8e1":"code","c24b13ce":"code","55d5f24d":"code","02277710":"code","b027ce96":"code","a94fa134":"code","431f8562":"code","b290716b":"code","4a03214b":"code","3888bf48":"code","aa698d15":"code","232772a0":"code","4a4a3aea":"code","fcd9d712":"code","b4e8b966":"code","41484515":"code","753d9317":"code","2953d77d":"code","d331a504":"code","b4536b41":"code","b27b3279":"code","87f3d4fb":"code","03143a59":"code","f33e4d38":"code","d2a7d487":"code","7cfcdeac":"code","fe8234bc":"code","ee345e35":"code","5fd2c73c":"code","55ac0d15":"code","eba54629":"code","765f7fee":"code","7c6f5936":"code","0e3263e9":"code","7a3aecd6":"code","acb8afda":"code","59b1e842":"code","453a80b3":"code","32a81d83":"code","c57ae120":"code","feb4ace4":"code","155bfd39":"code","81264edd":"code","a6044605":"code","8101c91f":"code","98edaf15":"code","d2460baa":"code","d6d348f4":"code","0a258c30":"code","42bcbfd3":"code","e70aa9e3":"code","1568a486":"code","70a38554":"code","14a98f6b":"code","67a30d95":"code","7d2fe1ef":"code","86821207":"markdown","74f01c99":"markdown","2f1c4270":"markdown","20a90b81":"markdown","f2a31e76":"markdown","97a0155c":"markdown","97368108":"markdown","cccc4067":"markdown","029190ea":"markdown","c2860e28":"markdown","73f18632":"markdown","2d6c372a":"markdown","102537fa":"markdown","b8c3c3c3":"markdown","a9780288":"markdown","7dd8721d":"markdown","abef75f2":"markdown","adbe2727":"markdown","b4b8875f":"markdown","2478fe95":"markdown","0653a6e1":"markdown","fd177160":"markdown","7b5693d1":"markdown","dfd17211":"markdown","ff0d8489":"markdown","12dce38b":"markdown","7a38c37d":"markdown","2070759a":"markdown","8c9dfe0a":"markdown","d8e137dc":"markdown","263724f8":"markdown","b427317f":"markdown"},"source":{"211226bd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70d69c30":"df = pd.read_csv('..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv', parse_dates = ['saledate'])","4f1fc797":"df.head()","647dc3e8":"df.head().T","c468c0aa":"df.info()","2ba4eba7":"df.saledate.head(10)","a665e344":"df.sort_values(['saledate'], ascending=True, inplace=True)\ndf['saledate'].head(10)","1cb9c8e1":"data = df.copy()","c24b13ce":"data.head()","55d5f24d":"data['saleyear'] = data.saledate.dt.year\ndata['salemonth'] = data.saledate.dt.month\ndata['saleday'] = data.saledate.dt.day\ndata['saledayofweek'] = data.saledate.dt.dayofweek\ndata['saledayofyear'] = data.saledate.dt.dayofyear\n","02277710":"data.head().T","b027ce96":"data.drop('saledate', inplace=True, axis=1)","a94fa134":"data.head().T","431f8562":"print(data['state'].value_counts())\n#data['state'].value_counts().plot(kind='bar')","b290716b":"data.head()","4a03214b":"data.info()","3888bf48":"for label, content in data.items():\n    if pd.api.types.is_string_dtype(content):\n        data[label] = content.astype('category').cat.as_ordered()","aa698d15":"data.info()","232772a0":"data['state'].cat.categories","4a4a3aea":"data['state'].cat.codes","fcd9d712":"missing_values=(data.isnull().sum()\/len(data)*100)","b4e8b966":"missing_values","41484515":"#data.to_csv('new_data.csv', index=False)","753d9317":"#data = pd.read_csv('.\/new_data.csv')","2953d77d":"for label, content in data.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n           print(label)","d331a504":"for label, content in data.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Add a binary column which will tell us if the data was missing\n            data[label+'_is_missing'] = pd.isnull(content)\n            #Fill missing numeric values with median.\n            #Reason for choosing median over mean is, median is robust to outliers.\n            data[label] = content.fillna(content.median())","b4536b41":"for label, content in data.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n           print(label)","b27b3279":"for label, content in data.items():\n    if pd.api.types.is_categorical_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)\nlen(label)","87f3d4fb":"for label, content in data.items():\n    if not pd.api.types.is_numeric_dtype(content):\n            #Add a binary column which will tell us if the data was missing\n            data[label+'_is_missing'] = pd.isnull(content)\n            #Turn categories into numbers and then add 1.\n            #Reason for adding 1 is, if there are missing values after converting\n            #categories into numbers, it'll replace missing values(0) by -1.\n            data[label] = pd.Categorical(content).codes+1","03143a59":"data.info()","f33e4d38":"data.head().T","d2a7d487":"data.isnull().sum()","7cfcdeac":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","fe8234bc":"data_train = data[data.saleyear!=2012]\ndata_val = data[data.saleyear==2012]\n\nlen(data_train), len(data_val)","ee345e35":"X_train, y_train = data_train.drop('SalePrice', axis=1), data_train['SalePrice']\nX_valid, y_valid = data_val.drop('SalePrice', axis=1), data_val['SalePrice']","5fd2c73c":"X_train.shape,y_train.shape, X_test.shape, y_test.shape","55ac0d15":"from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\ndef rmsle(y_test, y_pred):\n    \"\"\"\n    Calculates Root mean squared lof error between predictions and true labels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_pred))\n\n# Create function to evaluate model on a few different levels.\n\ndef show_scores(model):\n    train_pred = model.predict(X_train)\n    val_pred = model.predict(X_valid)\n    #If model performs better on validation dataset, it means the model is overfitting.\n    scores = {'Training MAE':mean_absolute_error(y_train, train_pred),\n             \"Validation MAE\": mean_absolute_error(y_valid, val_pred),\n             \"Training RMSLE\": rmsle(y_train, train_pred),\n             \"Valid RMSLE\": rmsle(y_valid, val_pred),\n             \"Training R^2\": r2_score(y_train, train_pred),\n             \"Valid R^2\": r2_score(y_valid, val_pred)}\n    return scores","eba54629":"%%time\nmodel = RandomForestRegressor(n_jobs=-1, random_state=42,\n                             max_samples=10000)\nmodel.fit(X_train, y_train)","765f7fee":"show_scores(model)","7c6f5936":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Different RandomForestRegressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2),\n           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n\n#Instantiating RandomizedSearchCV\nrs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42),\n                              param_distributions=rf_grid,\n                              n_iter=20,\n                              cv=5,\n                              verbose=True)\n#Fitting the RandomizedSearchCV model\nrs_model.fit(X_train, y_train)","0e3263e9":"rs_model.best_params_","7a3aecd6":"show_scores(rs_model)","acb8afda":"%%time\n# Most ideal hyperparameters\nideal_model = RandomForestRegressor(n_estimators=90,\n                                    min_samples_leaf=1,\n                                    min_samples_split=14,\n                                    max_features=0.5,\n                                    n_jobs=-1,\n                                    max_samples=None,\n                                   random_state=42)\nideal_model.fit(X_train, y_train)","59b1e842":"show_scores(ideal_model)","453a80b3":"df_test = pd.read_csv('..\/input\/bluebook-for-bulldozers\/Test.csv',\n                     parse_dates = ['saledate'])","32a81d83":"df_test.shape","c57ae120":"def preprocess_data(df):\n    # Add datetime parameters for saledate\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayofweek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayofyear\"] = df.saledate.dt.dayofyear\n\n    # Drop original saledate\n    df.drop(\"saledate\", axis=1, inplace=True)\n    \n    # Fill numeric rows with the median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                df[label] = content.fillna(content.median())\n                \n        # Turn categorical variables into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add the +1 because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1        \n    \n    return df","feb4ace4":"preprocess_data(df_test)","155bfd39":"df_test.shape, X_train.shape","81264edd":"set(X_train.columns)-set(df_test.columns)","a6044605":"df_test['auctioneerID_is_missing'] = False\ndf_test.head()","8101c91f":"df_test.shape, X_train.shape","98edaf15":"test_pred = ideal_model.predict(df_test)\ntest_pred","d2460baa":"len(test_pred)","d6d348f4":"df_preds = pd.DataFrame()\ndf_preds['SaledID'] = df_test['SalesID']\ndf_preds['SalesPrice'] = test_pred\ndf_preds","0a258c30":"# Find feature importance of our best model\nideal_model.feature_importances_","42bcbfd3":"\n\nimport seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","e70aa9e3":"plot_features(X_train.columns, ideal_model.feature_importances_)","1568a486":"sum(ideal_model.feature_importances_)","70a38554":"df.ProductSize.isna().sum()","14a98f6b":"df.ProductSize.value_counts()","67a30d95":"df.Turbocharged.value_counts()","7d2fe1ef":"df.Thumb.value_counts()","86821207":"Now let's evaluate the RandomizedSearchCV model.","74f01c99":"\n# Predicting the Sale Price of Bulldozers using Machine Learning\n\nIn this notebook, we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.\n\nSince we're trying to predict a number, this kind of problem is known as a regression problem.\n\nThe data and evaluation metric we'll be using (root mean square log error or RMSLE) is from the Kaggle Bluebook for Bulldozers competition.\n\nThe techniques used in here have been inspired and adapted from the fast.ai machine learning course.\nWhat we'll end up with\n\n\nTo work through these topics, we'll use pandas, Matplotlib and NumPy for data anaylsis, as well as, Scikit-Learn for machine learning and modelling tasks.\nTools which can be used for each step of the machine learning modelling process.\n\nWe'll work through each step and by the end of the notebook, we'll have a trained machine learning model which predicts the sale price of a bulldozer given different characteristics about it.\n## 1. Problem Definition\n\nFor this dataset, the problem we're trying to solve, or better, the question we're trying to answer is,\n\n    How well can we predict the future sale price of a bulldozer, given its characteristics previous examples of how much similar bulldozers have been sold for?\n\n## 2. Data\n\nLooking at the dataset from Kaggle, you can you it's a time series problem. This means there's a time attribute to dataset.\n\nIn this case, it's historical sales data of bulldozers. Including things like, model type, size, sale date and more.\n\nThere are 3 datasets:\n\n    Train.csv - Historical bulldozer sales examples up to 2011 (close to 400,000 examples with 50+ different attributes, including SalePrice which is the target variable).\n    Valid.csv - Historical bulldozer sales examples from January 1 2012 to April 30 2012 (close to 12,000 examples with the same attributes as Train.csv).\n    Test.csv - Historical bulldozer sales examples from May 1 2012 to November 2012 (close to 12,000 examples but missing the SalePrice attribute, as this is what we'll be trying to predict).\n\n## 3. Evaluation\n\nFor this problem, Kaggle has set the evaluation metric to being root mean squared log error (RMSLE). As with many regression evaluations, the goal will be to get this value as low as possible.\n\nTo see how well our model is doing, we'll calculate the RMSLE and then compare our results to others on the Kaggle leaderboard.\n\n## 4. Features\n\nFeatures are different parts of the data. During this step, you'll want to start finding out what you can about the data.\n\nOne of the most common ways to do this, is to create a data dictionary.\n\nFor this dataset, Kaggle provide a data dictionary which contains information about what each attribute of the dataset means. You can download this file directly from the Kaggle competition page (account required) or view it on Google Sheets.\n\nWith all of this being known, let's get started!\n\nFirst, we'll import the dataset and start exploring. Since we know the evaluation metric we're trying to minimise, our first goal will be building a baseline model and seeing how it stacks up against the competition.","2f1c4270":"## Predictions on Test Data","20a90b81":"Now let's make a copy of our dataset, so that we have a back-up data incase we mess something up.","f2a31e76":"Nice, missing values in numeric features have been taken care of.","97a0155c":"Loading the data, with **parse_dates** parameter so that the **saledate** column is converted into datetime64[ns] format.","97368108":"Let's checkout our state column","cccc4067":"Looks like the data has been sorted by the ***saledate*** column, let's continue further.","029190ea":"\n## Feature Importance\n\nSince we've built a model which is able to make predictions. The people you share these predictions with (or yourself) might be curious of what parts of the data led to these predictions.\n\nThis is where feature importance comes in. Feature importance seeks to figure out which different attributes of the data were most important when it comes to predicting the target variable.\n\nIn our case, after our model learned the patterns in the data, which bulldozer sale attributes were most important for predicting its overall sale price?\n\nBeware: the default feature importances for random forests can lead to non-ideal results.\n\nTo find which features were most important of a machine learning model, a good idea is to search something like \"[MODEL NAME] feature importance\".\n\nDoing this for our RandomForestRegressor leads us to find the feature_importances_ attribute.\n\nLet's check it out.\n","c2860e28":"Turn categorical features into numbers and fill missing values.","73f18632":"## EDA\n\nLet's take a look at datatypes of all the columns in our data.","2d6c372a":"#### Let's build our own evaluation function","102537fa":"Let's first take care of numerical features.","b8c3c3c3":"If ever in doubt about choosing the right ML algorithm, take a look at [this](https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html).\n\nLet's try with `RandomForestRegressor`","a9780288":"### Let's sort the dataframe by ***saledate*** column.","7dd8721d":"Now let's drop our `saledate` column.","abef75f2":"Manually adjust `df_test` so that number of features match.","adbe2727":"Let's convert all our string's into categories.\n\nOne way to do that is to convert them into pandas categories.\n\nYou can check it out yourself [here](https:\/\/pandas.pydata.org\/docs\/reference\/general_utility_functions.html).","b4b8875f":"## Taking care of the missing values.","2478fe95":"## 6. Modelling","0653a6e1":"## Taking Care of missing values.\n\n#### Let's do that by seperating Numerical & Categorical features.","fd177160":"We've now converted all the features with strings into categories.\nAlthough on the surface we don't see these features\/ their values as numerical, but under the hood these values will be treated as numbers.","7b5693d1":"Now we can't be sure if this is the best model, so I've tried it with different variations in parameters and I find this as an ideal model.","dfd17211":"Let's test our model on a subset. This will help with hyperparameter tuning","ff0d8489":"Let's check for missing values now.","12dce38b":"Let's format the predictions as required.","7a38c37d":"Now let's do the same with our categorical features.","2070759a":"#### Let's use RandomizedSearchCV for hyperparameter tuning.","8c9dfe0a":"### Importing the data and preparing it for modelling**","d8e137dc":"We can see that test data is still not in the right format, let's take care of that now.\n\nLet's find out how the columns differ in test and train datasets using `set` fuction.","263724f8":"Now that our test data is in the right format, let's make the predictions.","b427317f":"Let's now take care of missing values."}}