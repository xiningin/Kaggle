{"cell_type":{"4e71020e":"code","8021866b":"code","7ca6103d":"code","1b58db56":"code","887d5bab":"code","b00016f0":"code","58b69659":"code","2f2caeee":"code","3b588685":"code","a646e0fd":"code","510f29bd":"code","d0613dcc":"code","7a6637a0":"code","c6df45a6":"code","8aa470cf":"code","4791e9fb":"code","9cc4cbf5":"code","88d9bdad":"code","41176215":"code","fb08ddb3":"code","b0f535eb":"code","1e48acfe":"code","ab626ef2":"code","0e28e77f":"code","696d846e":"code","d72e5114":"code","d304c9ef":"code","fdd21461":"code","b9cbfdfa":"code","dda6bdea":"code","8e2f2169":"code","e1716678":"code","59d68fbc":"code","508bd8cd":"code","e5af2ff7":"code","c5fe6087":"code","13bb93af":"code","7a15b909":"code","3a0bd2ba":"code","c86259af":"code","8e0f5407":"code","1c248b97":"markdown","327b5e2d":"markdown","ce38dcdd":"markdown","f713331a":"markdown","e53bd568":"markdown","15350cb7":"markdown","06cc76cf":"markdown","292de852":"markdown","a785d4cd":"markdown","205a651e":"markdown","9d0548ff":"markdown","284e883f":"markdown","a3a3f7fb":"markdown","e9fede0e":"markdown","0f4dd00c":"markdown","9c1caa28":"markdown","9db4690f":"markdown"},"source":{"4e71020e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8021866b":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score","7ca6103d":"data = pd.read_csv(\"\/kaggle\/input\/ibm-watson-marketing-customer-value-data\/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv\")","1b58db56":"data.head()","887d5bab":"data.columns","b00016f0":"data.shape","58b69659":"data.dtypes","2f2caeee":"data.describe()","3b588685":"sns.countplot(\"Response\", hue=\"Gender\", data = data)","a646e0fd":"data.Response.value_counts()","510f29bd":"print(\"Only\",round((len(data[(data.Response == \"Yes\")])\/len(data.Response)*100),2),\"%\",\"of our customer accept an offer made by your Sales Team.\")","d0613dcc":"data.groupby(\"Sales Channel\").agg({\"Response\":\"count\"})","7a6637a0":"channel = list(data[\"Sales Channel\"].unique())\nfor i in channel:\n    output = len(data[(data[\"Sales Channel\"] == i) & \n                      (data[\"Response\"] == \"Yes\")]) \/len(data[(data[\"Sales Channel\"] == i)])\n    print(round((output * 100),2), \"% of offers via the Sales Channel\", i, \"were accepted.\")","c6df45a6":"data.dtypes","8aa470cf":"objects = [\"State\",\"Response\",\"Coverage\",\"Education\",\"EmploymentStatus\",\n           \"Gender\",\"Location Code\",\"Marital Status\",\"Policy Type\",\"Policy\",\"Renew Offer Type\",\"Sales Channel\",\n           \"Vehicle Class\",\"Vehicle Size\"]\n\nfor obj in objects:\n    print(data[obj].value_counts())","4791e9fb":"data = data.drop(columns={\"Customer\",\"Policy\", \"Effective To Date\"})","9cc4cbf5":"# Define a list with all features which are categorial\n\ndata_categorial = data.select_dtypes(include=[\"object\"])\ncategories = list(data_categorial.columns)\ncategories","88d9bdad":"# Encode the categorial Data to numerical\n\nlb = LabelEncoder()\n\nfor i in categories:\n    data[i] = lb.fit_transform(data[i])\n","41176215":"sns.heatmap(data.corr())","fb08ddb3":"y = data[\"Response\"]","b0f535eb":"X = data.drop([\"Response\"], axis=1)","1e48acfe":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=29)","ab626ef2":"lr = LogisticRegression()\n# initialize the model (=lr)\n\nlr.fit(X_train,y_train)\n#fit the model to the train set\n\nacc = lr.score(X_test,y_test)*100\n# comapring the test with the data\n\nprint(\"Logistic Regression Test Accuracy\", round(acc, 2),\"%\")","0e28e77f":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(X_train, y_train)\n# prediction = knn.predict(x_test)\n\ny_pred = knn.predict(X_test)\n\nacc = knn.score(X_test, y_test)*100\nprint(\"2 neighbors KNN Score: \",round(acc,2),\"%\")","696d846e":"from sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\nacc = svm.score(X_test,y_test)*100\nprint(\"SVM Algorithm Test Accuracy\", round(acc, 2),\"%\")","d72e5114":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\nacc = dtc.score(X_test, y_test)*100\nprint(\"Decision Tree Test Accuracy\", round(acc, 2),\"%\")","d304c9ef":"#Downsampling:\n\n#1. Test-Train Split!!\n# concatenate our training data back together\n\nX_down = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\n\nno_effect = X_down[X_down.Response==0]\neffect = X_down[X_down.Response==1]\n\n# downsample majority\n\nno_effect_downsampled = resample(no_effect,\n                               replace = False, # sample without replacement\n                               n_samples = len(effect), # match minority n\n                               random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\n\ndownsampled = pd.concat([no_effect_downsampled, effect])\n\n# checking counts\n\ndownsampled.Response.value_counts()","fdd21461":"downsampled.shape","b9cbfdfa":"y_train_down = downsampled.Response","dda6bdea":"X_train_down = downsampled.drop([\"Response\"], axis = 1)","8e2f2169":"lr = LogisticRegression()\n# initialize the model (=lr)\n\nlr.fit(X_train_down,y_train_down)\n#fit the model to the train set\n\ny_pred = lr.predict(X_test)\n\nacc = lr.score(X_test,y_test)*100\n# comapring the test with the data\n\nprint(\"Prediction\",y_pred[:5])\nprint(\"Logistic Regression Test Accuracy\", round(acc, 2),\"%\")","e1716678":"n_neighbors = 2\nknn = KNeighborsClassifier(n_neighbors = n_neighbors)  # n_neighbors means k\nknn.fit(X_train_down, y_train_down)\n\ny_pred = knn.predict(X_test)\n\nacc = knn.score(X_test, y_test)*100\n\nprint(\"Prediction:\", y_pred[:5])\nprint(n_neighbors,\"neighbors KNN Score: \",round(acc,2),\"%\")","59d68fbc":"acc_train = knn.score(X_train, y_train)*100\nprint(\"The accuracy score for the training data is: \",round(acc_train,2),\"%\")\nacc_test = knn.score(X_test,y_test)*100\nprint(\"The accuracy score for the test data is: \",round(acc_test,2),\"%\")\n\n","508bd8cd":"cv_results = cross_val_score(knn, X_train_down,y_train_down, cv = 5)\ncv_results","e5af2ff7":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_down, y_train_down)\n\ny_pred_dtc = dtc.predict(X_test)\n\nacc_dtc = dtc.score(X_test, y_test)*100\n\nprint(\"Prediction\", y_pred_dtc[:5])\nprint(\"Decision Tree Test Accuracy\", round(acc_dtc, 2),\"%\")","c5fe6087":"acc_train = dtc.score(X_train, y_train)*100\nprint(\"The accuracy score for the training data is: \",round(acc_train,2),\"%\")\nacc_test = dtc.score(X_test,y_test)*100\nprint(\"The accuracy score for the test data is: \",round(acc_test,2),\"%\")","13bb93af":"cv_results = cross_val_score(dtc, X_train_down,y_train_down, cv = 5)\ncv_results","7a15b909":"cnf_matrix = confusion_matrix(y_test, y_pred_dtc)\ncnf_matrix","3a0bd2ba":"dtc_recall = recall_score(y_test, y_pred_dtc)\ndtc_recall","c86259af":"271\/(271+4)","8e0f5407":"dtc_precision = precision_score(y_test,y_pred_dtc)\ndtc_precision","1c248b97":"Your data contains 9134 customers with information about their income, education, gender,residence and so on. Each customer owns a car and you as entrepreneur offers 4 different car insurances to them. The target of this dataset is the Response. The response can be \"Yes\" - the customer accept the offer and \"No\" - the customer didn\u00b4t accept the offer.","327b5e2d":"**5. Supervised Machine Learning with imbalanced data**\n\nLet\u00b4s start now with the prediction of the response future customers.\nFor that we have to find the right model. Since the data has a target which is separated into Yes\/No, we can use the Classification of supervised machine learning. \nFollowing models can be used:\n\n* Logistic Regression\n* KNeighbours Classifier\n* Support Vector Machine\n* Decision Tree","ce38dcdd":"Only 1308 customer accept the offer.","f713331a":"6.1. **LOGISTIC REGRESSION**","e53bd568":"**2. Import Data**\n\nThis is a dataset from IBM Watson Analytics. This dataset gives you information about your customers. You can predict their behavior to retain your customers. You can analyze all relevant customer data and develop focused customer retention programs. And to understand customer demographics and buying behavior. Use predictive analytics to analyze the most profitable customers and how they interact. Take targeted actions to increase profitable customer response, retention, and growth.","15350cb7":"6.2. **K-NEAREST NEIGHBOUR** ","06cc76cf":"The most Offers were made by agents (3477 offers), the least via web.","292de852":"6.3. **DECISION TREE**","a785d4cd":"**4. Data Analysis**","205a651e":"**3. Exploratory Data Analysis (EDA)**","9d0548ff":"Accuracy is better and also the data is continuous.","284e883f":"The Decision Tree has the best accuracy and trainset is continuous. Recall is very high - that\u00b4s good. So the model can predict quite well that a customer won\u00b4t accept the offer. In this case, as an entrepreneur you know now in which customer you shouldn\u00b4t invest. So focus on those customers which will accept an offer.","a3a3f7fb":"**Results**\n\nAll categorial features are well distributet, so I will keep them and encode them to numerical data.\n\nSome columns don\u00b4t make sense or are not so important, e.g. Customer (because it\u00b4s just a unique number), Policy is the same as Policy Type, Effective To Date is also not important, so I will drop them.\n\nThe data is inbalanced regarding the outcome \"Response\"","e9fede0e":"**Results**\n\nThe Models have a really high Accuracy, the model Support Vector Machine seems to be the best decision to use with more than 99% Accuracy. \n\nBUT! this is because the data is imbalanced. The Response with \"No\" has a percentage of 86%, so the models are not useful and don\u00b4t give the accurate view of the data.","0f4dd00c":"**6. Supervised Learning with balanced Data**\n\nTo have a better view on the data, I\u00b4m going to downsample the target. This is better than oversampling in my opinion, so we don\u00b4t give too nuch wight to one certain target.","9c1caa28":"1. **Import Libraries**","9db4690f":"Accuracy is very bad, let\u00b4s try another model"}}