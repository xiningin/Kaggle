{"cell_type":{"85f58ae5":"code","cdac3505":"code","9ddbfd86":"code","47ae28f4":"code","2b3c7b05":"code","dc292f29":"code","1238f086":"code","aa9e4535":"code","e9601bed":"code","ceaa9283":"code","1a68f2a6":"code","5485cdcb":"code","98986fb0":"code","54c7b735":"code","44a83a00":"code","be275f54":"code","cf6b05e9":"code","8117cd5a":"code","ac4240b3":"code","184a3c27":"code","2038fb29":"code","4161e2b8":"code","34c6002e":"code","c644535f":"code","ff719b6f":"code","4ef9f454":"markdown","5a91d7d7":"markdown","0b83b015":"markdown","eae27ee4":"markdown","c22a2d16":"markdown","99e623a1":"markdown","5e0c97ae":"markdown","4d9ec501":"markdown","cb1252cc":"markdown","04102767":"markdown","a5708864":"markdown","ac1fd72a":"markdown","f7461558":"markdown"},"source":{"85f58ae5":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd \nimport numpy as np\nimport re\nimport string\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku","cdac3505":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","9ddbfd86":"test_id = test['id']","47ae28f4":"train.head()","2b3c7b05":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","dc292f29":"train['text']=train['text'].apply(lambda x : remove_URL(x))","1238f086":"train['text']=train['text'].apply(lambda x : remove_html(x))","aa9e4535":"train['text']=train['text'].apply(lambda x: remove_emoji(x))","e9601bed":"train['text']=train['text'].apply(lambda x : remove_punct(x))","ceaa9283":"train = train.drop(columns = {'keyword', 'location', 'id'})","1a68f2a6":"train.head()","5485cdcb":"test = test.drop(columns = {'id', 'location', 'keyword'})","98986fb0":"test.head()","54c7b735":"from transformers import TFBertModel, BertModel\nfrom transformers import BertTokenizer","44a83a00":"bert_base = TFBertModel.from_pretrained('bert-base-uncased')\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")","be275f54":"def bert_encode(data,maximum_len) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data.text)):\n        encoded = TOKENIZER.encode_plus(data.text[i],\n                                        add_special_tokens=True,\n                                        max_length=maximum_len,\n                                        pad_to_max_length=True,\n                                        return_attention_mask=True)\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","cf6b05e9":"BATCH_SIZE = 16\n\n\nEPOCHS = 2\n\n#we will not be using metadata \nUSE_META = False\n\nADD_DENSE = False\nDENSE_DIM = 64\n\nADD_DROPOUT = True\nDROPOUT = .2\n\nTRAIN_BASE = True","8117cd5a":"def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n    \n    \n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n    \n    \n    \n    transformer_layer = model_layer([input_ids,attention_masks])\n    \n    \n    output = transformer_layer[1]\n    \n        \n    if add_dense:\n        print(\"Training with additional dense layer...\")\n        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n    \n    \n    if add_dropout:\n        print(\"Training with dropout...\")\n        output = tf.keras.layers.Dropout(dropout)(output)\n    \n    \n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    \n    print(\"Training without meta-data...\")\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n\n    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","ac4240b3":"print('Encoding Tweets...')\ntrain_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)\nprint('Tweets encoded')\nprint('')\n\n#print('Train length:', len(train_input_ids))\n#print('Test length:', len(test_input_ids))","184a3c27":"BERT_base = build_model(bert_base, learning_rate = 1e-5)\nBERT_base.summary()","2038fb29":"checkpoint = tf.keras.callbacks.ModelCheckpoint('base_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)","4161e2b8":"history = BERT_base.fit([train_input_ids,train_attention_masks], train.target, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)","34c6002e":" preds_base = BERT_base.predict([test_input_ids,test_attention_masks])","c644535f":"submission = pd.DataFrame()\nsubmission['id'] = test_id\nsubmission['prob'] = preds_base\nsubmission['target'] = np.round(submission['prob']).astype(int)\nsubmission.head()","ff719b6f":"submission = submission[['id', 'target']]\nsubmission.to_csv('submission.csv', index = False)","4ef9f454":"Now let's finally import our model and tokenizer","5a91d7d7":"I have used this notebook :https:\/\/www.kaggle.com\/tuckerarrants\/bert-with-huggingface-transformers\/notebook as a refernece for making my model \n","0b83b015":"Lets set the values for our hyperparameters that we will be using ","eae27ee4":"# Hyperparameters","c22a2d16":"Here we do some text preprocessing and drop all the columns that are not required by us ","99e623a1":"# Train the model","5e0c97ae":"# Tokenize","4d9ec501":"Finally let's train our model","cb1252cc":"# Predictions","04102767":"# Data preprocessing","a5708864":"Let's define the function to tokenize our train and test data ","ac1fd72a":"This is how our data looks like after preprocessing","f7461558":"# NLP with Transformers"}}