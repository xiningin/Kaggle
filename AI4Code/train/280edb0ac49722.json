{"cell_type":{"eda5a525":"code","0d22f0d2":"code","fe7134cf":"code","e2fc89e1":"code","903e5990":"code","fd2dbbbd":"code","c892c9ed":"code","cea22ae9":"code","820ddf22":"code","c3875409":"code","b0e65d40":"code","084e00ad":"code","9db686d9":"code","4c526bb9":"code","ccf180f2":"code","89b52b46":"code","d9fc0da1":"code","dd0cf985":"code","2d734393":"code","1742c8a1":"code","d1dbd425":"code","7d97712f":"code","4b6dd85b":"code","af940a60":"code","78b4e0ad":"code","d0594cc0":"code","617f957c":"code","82e8b231":"code","7b80ff77":"code","28709563":"code","e324228d":"code","06685f11":"code","861bb0c6":"code","00478895":"code","32176fb9":"code","3fa61951":"code","bea04209":"code","a2ba2b97":"code","bd54ceb7":"code","4ec984c6":"code","a8f75f28":"code","20cd597f":"code","244bba38":"code","6a411acf":"code","4068c8ee":"markdown","b2847540":"markdown","f8d46d95":"markdown","b64d7b5d":"markdown","4db2aa31":"markdown","73903cc3":"markdown","062c4423":"markdown","8ef03ff7":"markdown","313c234a":"markdown","9cf4d23d":"markdown","30481c51":"markdown","18b888b3":"markdown","1c1c643b":"markdown","c91b0242":"markdown","43054f9a":"markdown","eb78cd94":"markdown","fb6a0457":"markdown","dbcb0f1c":"markdown","748c2be9":"markdown","862bc892":"markdown","99b752ba":"markdown","3ecf58c7":"markdown","557f9df2":"markdown","567d5ab5":"markdown","c2b71403":"markdown"},"source":{"eda5a525":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nimport tensorflow as tf","0d22f0d2":"clr_path = \"..\/input\/landscape-image-colorization\/landscape Images\/color\/\"\ngry_path = \"..\/input\/landscape-image-colorization\/landscape Images\/gray\/\"","fe7134cf":"import os\n\nclr_img_path = []\ngry_img_path = []\n\nfor img_path in os.listdir(clr_path) :\n    clr_img_path.append(os.path.join(clr_path, img_path))\n    \nfor img_path in os.listdir(gry_path) :\n    gry_img_path.append(os.path.join(gry_path, img_path))","e2fc89e1":"clr_img_path.sort()\ngry_img_path.sort()","903e5990":"from PIL import Image\nfrom keras.preprocessing.image import img_to_array\n\nX = []\ny = []\n\nfor i in range(5000) :\n    \n    img1 = cv2.cvtColor(cv2.imread(clr_img_path[i]), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(gry_img_path[i]), cv2.COLOR_BGR2RGB)\n    \n    y.append(img_to_array(Image.fromarray(cv2.resize(img1,(128,128)))))\n    X.append(img_to_array(Image.fromarray(cv2.resize(img2,(128,128)))))","fd2dbbbd":"X = np.array(X)\ny = np.array(y)","c892c9ed":"print(X.min())\nprint(X.max())","cea22ae9":"print(y.min())\nprint(y.max())","820ddf22":"plt.figure(figsize = (10,50))\n\ni = 0\n\nwhile i < 20:\n    \n    x = np.random.randint(0,3000)\n    \n    plt.subplot(10, 2, i+1)\n    plt.imshow(X[x]\/255.0,'gray')\n    plt.axis('off')\n    plt.title('Gray Image')\n    \n    plt.subplot(10, 2, i+2)\n    plt.imshow(y[x]\/ 255.0)\n    plt.axis('off')\n    plt.title('ColorImage')\n    i += 2\n    \nplt.show()","c3875409":"X = (X\/127.5) - 1\ny = (y\/127.5) - 1","b0e65d40":"print(X.shape)\nprint(y.shape)","084e00ad":"print(f'Minimum of X : {X.min()}')\nprint(f'Maximum of X : {X.max()}')","9db686d9":"print(f'Minimum of y : {y.min()}')\nprint(f'Maximum of y : {y.max()}')","4c526bb9":"from sklearn.model_selection import train_test_split","ccf180f2":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle = False)","89b52b46":"print(X_train.shape)\nprint(y_train.shape)","d9fc0da1":"print(X_valid.shape)\nprint(y_valid.shape)","dd0cf985":"img = cv2.imread('..\/input\/modelimage1\/1*ETvcPhYH1lCfXndMiKW-jQ.png')\n\nplt.figure(figsize = (20,5))\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.title('Different types of normalization techniques (C : Channels | H,W : Spatial dimensions | N : sample space)', fontsize = 15)\nplt.axis('off')","2d734393":"from tensorflow_addons.layers import SpectralNormalization\nfrom keras.layers import BatchNormalization\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Concatenate\nfrom keras.layers import Activation\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import ReLU\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Conv2DTranspose\nfrom keras.initializers import RandomNormal\nfrom tensorflow_addons.layers import InstanceNormalization","1742c8a1":"'''\nWeigth initialization is according to [2], which uses the Gaussian\nrandom distribution with mean of 0 and standard deviation of 0.02.\n'''\n\ninit = RandomNormal(mean = 0.0, stddev = 0.02)","d1dbd425":"def d_block (x_input, filters, strides, padding, batch_norm, inst_norm) :\n    \n    x = Conv2D(filters, (4, 4),\n               strides=strides,\n               padding=padding,\n               use_bias= False,\n               kernel_initializer = init)(x_input)\n    \n    '''\n    SpectralNormalization is wrapped around convolution layer.\n    Also, Instance  and  Batch Normalization layers are added.\n    '''\n    \n    if batch_norm == True :\n        x = BatchNormalization   ()(x)\n    if inst_norm  == True :\n        x = InstanceNormalization()(x)\n    x = LeakyReLU(0.2)(x)\n    return x\n\n\ndef u_block (x, skip, filters, strides, padding, batch_norm, inst_norm) :\n    \n    x = Conv2DTranspose(filters, (4, 4),\n                        strides=strides,\n                        padding=padding,\n                        use_bias= False,\n                        kernel_initializer = init)(x)\n    \n    '''\n    All the 3 normalizations applied here as well.\n    '''\n    \n    if batch_norm == True :\n        x = BatchNormalization   ()(x)\n    if inst_norm  == True :\n        x = InstanceNormalization()(x)\n    x = ReLU()(x)\n    conc_x = Concatenate()([x , skip])\n    \n    return conc_x","7d97712f":"def PatchGAN (image_shape) :\n    \n    genI = Input(shape =  image_shape)\n    tarI = Input(shape =  image_shape)\n    conc = Concatenate()([genI, tarI])\n    \n    c064 = d_block(conc, 2**6, 2, 'same', False, False)\n    c128 = d_block(c064, 2**7, 2, 'same', False, True )\n    c256 = d_block(c128, 2**8, 2, 'same', True , False)\n    \n    temp = ZeroPadding2D()(c256)\n    \n    c512 = d_block(temp, 2**9, 1,'valid', True , False)\n    \n    temp = ZeroPadding2D()(c512)\n    \n    c001 = Conv2D(2**0, (4,4), strides=1, padding = 'valid', activation = 'sigmoid', kernel_initializer=init)(temp)\n    \n    model = Model(inputs = [genI, tarI], outputs = c001)\n    return model","4b6dd85b":"d_model = PatchGAN((128,128,3,))\nd_model.summary()","af940a60":"from keras.utils import plot_model","78b4e0ad":"plot_model(d_model, '.\/d_model.png', show_shapes = True)","d0594cc0":"def mod_Unet () :\n    \n    srcI = Input(shape = (128,128,3,))\n    \n    # Contracting path\n    \n    c064 = d_block(srcI, 2**6, 2, 'same', False, False) # _______________________.\n    c128 = d_block(c064, 2**7, 2, 'same', True , False) # ____________________.  .\n    c256 = d_block(c128, 2**8, 2, 'same', True , False) # _________________.  .  .\n    c512 = d_block(c256, 2**9, 2, 'same', True , False) # ______________.  .  .  .\n    d512 = d_block(c512, 2**9, 2, 'same', True , False) # ___________.  .  .  .  .\n    e512 = d_block(d512, 2**9, 2, 'same', True , False) # ________.  .  .  .  .  .\n                                                        #         .  .  .  .  .  .\n    # Bottleneck layer                                            .  .  .  .  .  .\n                                                        #         .  .  .  .  .  .\n    f512 = d_block(e512, 2**9, 2, 'same', True , False) #         .  .  .  .  .  .\n                                                        #         .  .  .  .  .  .\n    # Expanding  path                                             .  .  .  .  .  .\n                                                        #         .  .  .  .  .  .\n    u512 = u_block(f512, e512, 2**9, 2, 'same', True, False)# ____.  .  .  .  .  .\n    u512 = u_block(u512, d512, 2**9, 2, 'same', True, False)# _______.  .  .  .  .\n    u512 = u_block(u512, c512, 2**9, 2, 'same', True, False)# __________.  .  .  .\n    u256 = u_block(u512, c256, 2**8, 2, 'same', True, False)# _____________.  .  .\n    u128 = u_block(u256, c128, 2**7, 2, 'same', True, False)# ________________.  .\n    u064 = u_block(u128, c064, 2**6, 2, 'same', False, True)# ___________________.\n    \n    genI = Conv2DTranspose(3, (4,4), strides = 2, padding = 'same', activation = 'tanh', kernel_initializer = init)(u064)\n    \n    model = Model(inputs = srcI, outputs = genI)\n    return model","617f957c":"g_model = mod_Unet()\ng_model.summary()","82e8b231":"plot_model(g_model, '.\/g_model.png', show_shapes = True)","7b80ff77":"'''\nThe information for batch-size appropriate for BW->COLOR transformation\nwas mentioned as 4 in [1](code source). The optimizer used is Adam with\nlearningRate of 0.0002, beta1 = 0.5 and beta2 = 0.999 according to [2].\n'''\n\nLAMBDA = 100\nBATCH_SIZE = 16\nBUFFER_SIZE  = 400","28709563":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))","e324228d":"train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(batch_size=BATCH_SIZE)\nvalid_dataset = valid_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(batch_size=BATCH_SIZE)","06685f11":"gen0 = mod_Unet()\n\n'''\nUsing the idea of multi-scale discriminator from [4]. We  use three\ndiscriminators downsampling the input volume by a factor of 2 and 4.\n'''\n\ndis0 = PatchGAN((128,128,3,)) # (W\/\/1) x (H\/\/1)\ndis1 = PatchGAN((64, 64, 3,)) # (W\/\/2) x (H\/\/2)\ndis2 = PatchGAN((32, 32, 3,)) # (W\/\/4) x (H\/\/4)","861bb0c6":"bin_entropy = keras.losses.BinaryCrossentropy(from_logits = True)","00478895":"def gen_loss (dis_gen_output, target_image, gen_output) :\n    \n    ad_loss = bin_entropy(tf.ones_like (dis_gen_output) ,  dis_gen_output)\n    l1_loss = tf.reduce_mean(tf.abs(tf.subtract(target_image,gen_output)))\n    \n    '''\n    Total loss = adversarial +  (LAMBDA*L1)\n    '''\n    total_loss = ad_loss + (LAMBDA*l1_loss)\n    \n    return total_loss, ad_loss, l1_loss","32176fb9":"def dis_loss (dis_gen_output, dis_tar_output) :\n    \n    gen_loss = bin_entropy(tf.zeros_like(dis_gen_output), dis_gen_output)\n    tar_loss = bin_entropy(tf.ones_like (dis_tar_output), dis_tar_output)\n    \n    total_dis_loss = gen_loss + tar_loss\n    return total_dis_loss","3fa61951":"img  = cv2.imread('..\/input\/landscape-image-colorization\/landscape Images\/color\/10.jpg')\nimg  = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), (128,128))\na128 = img_to_array(Image.fromarray(img))\n\na128\/= 255.0\n\na064 = cv2.resize(a128, (64,64))\na032 = cv2.resize(a064, (32,32))","bea04209":"plt.figure(figsize = (20,20))\n\nplt.subplot(1,3,1)\nplt.imshow(a128)\nplt.title('128x128', fontsize = 20)\nplt.axis('off')\n\nplt.subplot(1,3,2)\nplt.imshow(a064)\nplt.title('64 x 64', fontsize = 20)\nplt.axis('off')\n\nplt.subplot(1,3,3)\nplt.imshow(a032)\nplt.title('32 x 32', fontsize = 20)\nplt.axis('off')","a2ba2b97":"g_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5, beta_2=0.999)\nd0optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5, beta_2=0.999)\nd1optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5, beta_2=0.999)\nd2optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5, beta_2=0.999)","bd54ceb7":"@tf.function\ndef train_on_batch (b_w_image, tar_image) :\n    \n    with tf.GradientTape(persistent = True) as  g :\n        \n        '''\n        Image Tensors\n        '''\n        gen_image = gen0(b_w_image, training=True)\n        \n        # 128x128\n        dis_tar_output_128 = dis0([b_w_image, tar_image], training = True)\n        dis_gen_output_128 = dis0([b_w_image, gen_image], training = True)\n        \n        \n        tar_image_128 = tar_image\n        gen_image_128 = gen_image\n        \n        tar_image = tf.image.resize(tar_image, [64,64])\n        b_w_image = tf.image.resize(b_w_image, [64,64])\n        gen_image = tf.image.resize(gen_image, [64,64])\n        \n        # 064x064\n        dis_tar_output_064 = dis1([b_w_image, tar_image], training = True)\n        dis_gen_output_064 = dis1([b_w_image, gen_image], training = True)\n        \n        tar_image_064 = tar_image\n        gen_image_064 = gen_image\n        \n        tar_image = tf.image.resize(tar_image, [32,32])\n        b_w_image = tf.image.resize(b_w_image, [32,32])\n        gen_image = tf.image.resize(gen_image, [32,32])\n        \n        # 032x032\n        dis_tar_output_032 = dis2([b_w_image, tar_image], training = True)\n        dis_gen_output_032 = dis2([b_w_image, gen_image], training = True)\n        \n        tar_image_032 = tar_image\n        gen_image_032 = gen_image\n        \n        '''\n        LOSS\n        '''\n        \n        # 128x128\n        g_loss_128, _, _ = gen_loss(dis_gen_output_128, tar_image_128, gen_image_128)\n        d_loss_128 = dis_loss(dis_gen_output_128, dis_tar_output_128)\n        \n        # 064x064\n        g_loss_064, _, _ = gen_loss(dis_gen_output_064, tar_image_064, gen_image_064)\n        d_loss_064 = dis_loss(dis_gen_output_064, dis_tar_output_064)\n        \n        # 032x032\n        g_loss_032, _, _ = gen_loss(dis_gen_output_032, tar_image_032, gen_image_032)\n        d_loss_032 = dis_loss(dis_gen_output_032, dis_tar_output_032)\n        \n        \n        g_total_loss = g_loss_128 + g_loss_064 + g_loss_032\n        d_total_loss = d_loss_128 + d_loss_064 + d_loss_032\n    \n    # compute gradients\n    g_gradients = g.gradient(g_total_loss, gen0.trainable_variables) # generatorLoss\n    \n    d0gradients = g.gradient(d_loss_128, dis0.trainable_variables)   # dis loss 128\n    d1gradients = g.gradient(d_loss_064, dis1.trainable_variables)   # dis loss 064\n    d2gradients = g.gradient(d_loss_032, dis2.trainable_variables)   # dis loss 032\n    \n    \n    # apply gradient descent\n    g_optimizer.apply_gradients(zip(g_gradients, gen0.trainable_variables))\n    \n    d0optimizer.apply_gradients(zip(d0gradients, dis0.trainable_variables))\n    d1optimizer.apply_gradients(zip(d1gradients, dis1.trainable_variables))\n    d2optimizer.apply_gradients(zip(d2gradients, dis2.trainable_variables))","4ec984c6":"for global_b_w_image, global_tar_image in train_dataset.take(1) :\n    pass","a8f75f28":"def fig (b_w_image, gen_image, tar_image) :\n    \n    plt.figure(figsize = (20, 20))\n    \n    plt.subplot(1,3,1)\n    plt.imshow((b_w_image[0] + 1.0) \/ 2.0)\n    plt.title('BandW Image',fontsize = 20)\n    plt.axis('off')\n    \n    plt.subplot(1,3,2)\n    plt.imshow((gen_image[0] + 1.0) \/ 2.0)\n    plt.title('GenerateImg',fontsize = 20)\n    plt.axis('off')\n    \n    plt.subplot(1,3,3)\n    plt.imshow((tar_image[0] + 1.0) \/ 2.0)\n    plt.title('Colored Img',fontsize = 20)\n    plt.axis('off')\n    \n    plt.show()\n\ndef fit (EPOCHS = 200) :\n    \n    for epoch in range(EPOCHS) :\n        \n        print(f'Epoch {epoch} out of {EPOCHS}')\n        \n        for n, (b_w_image, tar_image) in train_dataset.enumerate() :\n            if n ==  265 :\n                print('#....End')\n            if n%20 == 0 :\n                print('#',end='')\n            train_on_batch(b_w_image, tar_image)\n        \n        if epoch%3  == 0 :\n            global_gen_image = gen0(global_b_w_image,training = True)\n            fig(global_b_w_image, global_gen_image, global_tar_image)","20cd597f":"fit(EPOCHS = 100)","244bba38":"for b_w_image,tar_image in valid_dataset.take(20) :\n    gen_image = gen0(b_w_image , training = True)\n    fig(b_w_image, gen_image, tar_image)","6a411acf":"gen0.save('.\/gen0.h5')\ndis0.save('.\/dis0.h5')\ndis1.save('.\/dis1.h5')\ndis2.save('.\/dis2.h5')","4068c8ee":"# Modified U-net (Generator)\n<div style = \"text-align: justify\">The modified U-net is made up of <b>d_block(...) and u_block(...)<\/b> for encoding and decoding, respectively. The basic structure of encoding path is <b>Conv-BatchNorm-LeakyReLU (slope = 0.2) layers<\/b> and that of decoding path is <b>TransposeConv-BatchNorm-ReLU layers<\/b>. In both cases Spectral Normalization <b>[3]<\/b> was applied. The Instance Normalization layer <b>[5]<\/b> was applied for last decoding layer <b>(u064)<\/b>.<\/div>","b2847540":"# Overview of the dataset & Visualizations","f8d46d95":"# fit() method\n\n#### Note\n<div style = \"text-align: justify\">The models have parameter <b>training = True<\/b> so that the batch statistics are calculated (eg. mean and variance) for the test dataset, and not the statistics from training dataset (in case you put <b>training = False<\/b>).<\/div>","b64d7b5d":"# Make tf.datasets","4db2aa31":"# AIM\n<div style = \"text-align: justify\">The aim of this notebook is to <b>generate coloured image, given a grayscale image as input.<\/b> This notebook takes inspiration from <b>End-to-End Conditional GAN based Architectures for Image Colourisation<\/b> by Marc Gorriz Blanch, Marta Mrak, Alan F. Smeaton and Noel E. O'Connor.<\/div>\n\n# Updates\n<div style = \"text-align: justify\">In the version 7 of the notebook, I used the <b>base Pix2Pix GAN model<\/b> for image colourisation. And as stated by the authors of this paper, a fraction of the images generated were desaturated. <b>To mitigate the lack of colourfulness, this model architecture was proposed.<\/b><\/div>\n\n# The dataset can be found [here](https:\/\/www.kaggle.com\/theblackmamba31\/landscape-image-colorization).","73903cc3":"### Images of different resolutions","062c4423":"### Generator loss\n![Gloss](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/images\/gen.png?raw=1)\n\n<div style = \"text-align: justify\">The loss function is the sum of sigmoid cross entropy calculated b\/w output from the discriminator <b>dis_gen_output<\/b> and array of ones, and L1 loss between target image <b>target_image<\/b> and generated image <b>gen_output<\/b>.<\/div>","8ef03ff7":"# Train-test split","313c234a":"# Training for each batch","9cf4d23d":"# Load models","30481c51":"<div style = \"text-align: justify\">The number of images in both <b>color and gray<\/b> folders is 7129. We will be working only on 40% of images in both the folders, <b>due to hardware constrains.<\/b> First step is to sort the images in the same order. Then we can rescale them and pass them as numpy arrays.<\/div>","18b888b3":"### All models are saved","1c1c643b":"### Architecture of the generator model","c91b0242":"# Model Prediction","43054f9a":"### Discriminator loss\n![Dloss](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/images\/dis.png?raw=1)\n\n<div style = \"text-align: justify\">Calculate discriminator loss for real and fake inputs accross all image resolutions, ie. <b>128x128, 64x64 and 32x32.<\/b> Inputs are matrices due to generated colored image and target colored image ie. <b>dis_gen_output and dis_tar_output<\/b>, respectively.<\/div>","eb78cd94":"# Abstract\n<div style = \"text-align: justify\"><b>[1]<\/b> introduced a novel architecture over baseline Pix2Pix GAN model given in <b>[2]<\/b>. The field of image colourisation has seen significant developments in recent years. Most of the computer vision tasks involve grayscale images because they retain the basic structure and to reduce the computation cost. However, adding color to these images <b>helps in distinguishing between real objects and their physical variations, eg. shadows etc.<\/b> State-of-the-art models based on GANs (Generative Adversarial Networks) (Goodfellow et. al) aim to mimic the color distribution by <b>forcing the generated images to be indistinguishable from target images.<\/b> However, these methods suffer from desaturated results (as observed in version 7).<\/div>\n\n# Loss function\n<div style = \"text-align: justify\">The model is trained using an adversarial loss function, where the <b>Discriminator takes in the generated image\/target image and evaluates whether it is a correct transformation of the original input image.<\/b> On the other hand, the generator takes in the original grayscale image and outputs a coloured image. Apart from the adversarial loss, the model weights are also trained with the <b>L1 loss introduced between generated image and ground truth.<\/b><\/div>\n\n# Model Architecture\n<div style = \"text-align: justify\">The model is based on Pix2Pix architecture <b>[2]<\/b> with additions of normalizations <b>[3,4]<\/b> and multiscale discriminators <b>[5]<\/b>. First let's look at the basic architecture.<\/div>\n\n### PatchGAN (Discriminator)\n<div style = \"text-align: justify\">The discriminator is a PatchGAN model which takes in both the grayscale image and coloured image and evaluates whether the coloured image is a valid transformation of the grayscale input. The PatchGAN has an <b>effective receptive field of 70x70<\/b>, ie. every single cell in the output array tells the probability of a 70x70 patch being real or fake.<\/div>\n\n<\/br>\n\n![Dis](https:\/\/brstar96.github.io\/assets\/Images\/MLDLStudy\/What-is-patchGAN-D\/2.jpg)\n\n<\/br>\n\n### Modified U-net Architecture (Generator)\n<div style = \"text-align: justify\"><b>The generator is a modified U-net model.<\/b> It has an encoder block and decoder block. First the image is compressed to the bottleneck layer and then it is upsampled to the output layer. <b>The contracting path tells WHAT ARE THE FEATURES IN THE IMAGE<\/b> and <b>the expanding path tells where these features are located in the image.<\/b> Skip connections are used to transfer image content at different levels from encoding block to decoding block. The main modification made in the original architecture is - <b>Using single convolution layer instead of two.<\/b> Refer <b>[2]<\/b> for more information on these models.<\/div>\n\n<\/br>\n\n![Gen](https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png)","fb6a0457":"### Architecture of the discriminator model","dbcb0f1c":"# Resources\n\n<div style = \"text-align: justify\"><b>Most of the work was done on base Pix2Pix network<\/b> and additional ideas were introduced for better results. Following are the papers I referred to for understanding the different concepts and models to implement,<\/div>\n\n<br>\n\n<div style = \"text-align: justify\"><b>[1]<\/b> M. G. Blanch, M. Mrak, A. F. Smeaton and N. E. O'Connor, \"End-to-End Conditional GAN-based Architectures for Image Colourisation,\" 2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP), 2019, pp. 1-6, doi: 10.1109\/MMSP.2019.8901712.<\/div>\n\n<br>\n\n<div style = \"text-align: justify\"><b>[2]<\/b> Isola, Phillip et al. \u201cImage-to-Image Translation with Conditional Adversarial Networks.\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 5967-5976.<\/div>\n\n<br>\n\n<div style = \"text-align: justify\"><b>[3]<\/b> T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, \u201cSpectral normalization for generative adversarial networks,\u201d arXiv:1802.05957, 2018.<\/div>\n\n<br>\n\n<div style = \"text-align: justify\"><b>[4]<\/b> Wang, T. et al. \u201cHigh-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.\u201d 2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition (2018): 8798-8807.<\/div>\n\n<br>\n\n<div style = \"text-align: justify\"><b>[5]<\/b> D. Ulyanov, A. Vedaldi, and V. Lempitsky, \u201cInstance normalization: The missing ingredient for fast stylization,\u201d arXiv:1607.08022, 2016.<\/div>","748c2be9":"# Add-ons\n<div style = \"text-align: justify\">The modifications over the base Pix2PixGAN found in <b>[1]<\/b> are <b>Batch Normalization, Instance Normalization, Spectral Normalization and Multi-scale discriminators<\/b>. They are mentioned below in brief.<\/div>\n\n### Normalization\n<div style = \"text-align: justify\"><b>BATCH NORMALIZATION<\/b> - was first introduced in Szegedy et. al, 2015. Batch Normalization mainly reduces the internal covariate shift in a deep network. <b>What is internal covariate shift ?<\/b> The hidden layers in the network recieve different batches in each epoch to train on. Also the weights of the previous layers are updated during training. These two elements can <b>cause the change in the distribution of inputs on every epoch when being fed to a hidden layer prompting them to adjust to the new distribution.<\/b> And this takes a lot of time in training the deep network. What Batch Normalization does is that calculates the mean and variance for the entire mini-batch of data accross all spatial dimensions for each channel and <b>normalizes the distribution to have a zero mean and unit variance.<\/b> So now the distribution does not change dramatically speeding up the training process.<\/div>\n\n<\/br>\n\n![BN](https:\/\/miro.medium.com\/max\/728\/1*JqbhYjs4yYieoAG1tjzkkA.png)\n\n<\/br>\n\n<div style = \"text-align: justify\"><b>INSTANCE NORMALIZATION<\/b> - was introduced first in <b>[5]<\/b>. Instance normalization normalizes each sample independently accross all spatial dimensions for each channel. It was found to be useful in style transfer methods. The reasoning was that the contrast of the generated image should only depend on the that of style image. <b>So to discard the contrast information of content image, instance normalization was applied.<\/b><\/div>\n\n<\/br>\n\n![IN](https:\/\/miro.medium.com\/max\/656\/1*H8WrL_Xqxdle8qWgMr82tA.png)\n\n<\/br>\n\n<div style = \"text-align: justify\"><b>SPECTRAL NORMALIZATION<\/b> - was introduced in <b>[3]<\/b>. The SN involves dividing the weights of hidden layers by their spectral norms such that the Lipchitz Constant for each layer and the entire network is 1. <b>First, What is Lipchitz continuity and Lipchitz constant ?<\/b> A function f : R->R is called Lipchitz continueous if the following holds true,<\/div>\n\n<\/br>\n\n![LC](https:\/\/miro.medium.com\/max\/3600\/1*dJv-LGnS0R_CyunOmrQBaA.jpeg)\n\n<\/br>\n\n<div style = \"text-align: justify\">This limits to how fast the gradients can change. <b>The slope of the function at any point is always less than or equal to K.<\/b> K is called the <b>Lipchitz Constant.<\/b> Lipchitz constant can also be calculated as the maximum absolute value to first derivative of f. To make the following constrain hold true, the weight matrix is divided by its spectral norm. <b>The spectral norm is the maximum sigular value of matrix A.<\/b> Using <a href = \"https:\/\/medium.com\/@jonathan_hui\/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491\"><b>SVD<\/b><\/a>, we can compute the maximum singular value of a matrix A. More information can be found <a href = \"https:\/\/jonathan-hui.medium.com\/gan-spectral-normalization-893b6a4e8f53\"><b>here.<\/b><\/a><\/div>\n\n<\/br>\n\n![SN](https:\/\/miro.medium.com\/max\/1050\/1*jHMt-dliKmrPELj6hrRyGg.png)\n\n### Multi-Scale Discriminators\n<div style = \"text-align: justify\">Multi-scale discriminators were introduced in <b>[4].<\/b> When comparing high-resolution images, <b>a larger effective receptive field<\/b> is needed to check for consistency accross the images. This can be done either by increasing the depth of the discriminator or by larger convolutions both of which are memory intensive. So, the solution was <b>to use three discriminators with same structure but feeding them images of different resolutions. So, the discriminator that gets a smaller image checks for consistency with the target image. The discriminator with larger input image can try to mimic finer details.<\/b><\/div>\n\n<\/br>\n\n#### Original code by authors of the paper [1] can be found on [Github](https:\/\/github.com\/bbc\/ColorGAN).","862bc892":"# Loss functions","99b752ba":"# PatchGAN (Discriminator)\n<div style = \"text-align: justify\">The PatchGAN discriminator is made up of down sampling blocks <b>d_block(...)<\/b> where at each step the size of the image is halved and depth is doubled. It comprises of <b>Conv-BatchNorm-LeakyReLU layers<\/b> with added Spectral normalization <b>[3]<\/b> and instance normalization <b>[5]<\/b>. <b>3 such patchGAN models will be used as discriminators <\/b> and they will be fed images of different resolutions <b>[4].<\/b><\/div>","3ecf58c7":"# Import the dataset","557f9df2":"# Custom Training\n<div style = \"text-align: justify\">First we are going to make the appropriate tensorflow datsets, then we declare the optimizer and checkpoint. Then we implement the training function, where we use <b>Gradient Tape<\/b> to retrieve the gradients of trackable objects (weights here) with repect to loss function. <\/div>","567d5ab5":"### Optimizer","c2b71403":"# Model Training"}}