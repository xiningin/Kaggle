{"cell_type":{"b301a06d":"code","5821cece":"code","30b48779":"code","293a36e3":"code","ca8b1c9b":"code","ce5eb200":"code","2bc43560":"code","a96b0529":"code","1fc9f753":"code","1f256544":"code","29b3d37b":"code","83464ca7":"code","b5a3dd46":"code","9d0071e3":"code","8db52873":"code","c6f407d5":"code","8aac81e8":"code","810f440e":"code","daa30d0a":"code","c9a4f7b8":"code","66a19b7d":"code","f13e632f":"code","029c69de":"code","32bb94bf":"code","0cadb95f":"code","15d56a0e":"code","33362855":"code","73b02de4":"code","fc6bee79":"code","e2383121":"code","1f092452":"code","68d1490e":"code","cc394d5b":"code","0ba4f6e8":"code","afb1b95f":"code","b0d3fe5a":"code","88ffcfd9":"code","b9d2613a":"code","b482eea9":"code","fdc4ba80":"code","f1c03783":"code","427bc87f":"code","717358af":"code","5ceb14e6":"code","2803b1c3":"code","32162c07":"code","7816338f":"code","90a2d712":"code","b51e178d":"code","f8438b0c":"code","b222107b":"code","db9cee0b":"code","f8f08c52":"code","649643f9":"code","f0d659da":"code","ee78f378":"code","80b2523a":"markdown","05877e07":"markdown","49bf03b4":"markdown","2b5e2403":"markdown","558f5dbd":"markdown","a11ff2a3":"markdown","5d0a2d2d":"markdown","7be11041":"markdown","480a21cd":"markdown","4a16711c":"markdown","f32248c9":"markdown","48b0c1bd":"markdown","79f947bd":"markdown","e8b5fa6a":"markdown","527e1927":"markdown","a30bba4f":"markdown","7500d23e":"markdown","d1bf7821":"markdown","41ee79a0":"markdown","4bd7474e":"markdown","a5efc861":"markdown","9247d8ab":"markdown","bd16bd4d":"markdown","57996fa5":"markdown","c5ab3c33":"markdown","3af8a627":"markdown","1cf84dd8":"markdown","15c102ed":"markdown","98ee7c02":"markdown","29eae11c":"markdown","e94064c7":"markdown","2fe81e1d":"markdown","e9502e47":"markdown","6407b1e9":"markdown","ca1c3c73":"markdown","555c28d3":"markdown","f8a146f0":"markdown","77d9eaa7":"markdown","b3491866":"markdown","09f15ad8":"markdown","81bc6279":"markdown","d3398982":"markdown","30527812":"markdown","c89d65b4":"markdown","0e702131":"markdown","f1b28f91":"markdown","630332cb":"markdown","10b71b60":"markdown","5289ad53":"markdown","0cd616a5":"markdown","4ae896b5":"markdown","ad9b8b0c":"markdown","ce3f000a":"markdown","1a33fb46":"markdown","5620b1a8":"markdown","ac751266":"markdown","fdb5e369":"markdown","96abd0c6":"markdown","37147342":"markdown","33ac0603":"markdown","7b176db3":"markdown","54b6286f":"markdown","1b035363":"markdown","76ae8139":"markdown"},"source":{"b301a06d":"# NUMPY \/ PANDAS\nimport numpy as np\nimport pandas as pd\n\n# SCI-KIT LEARN\nimport sklearn\nfrom sklearn.model_selection import (GroupKFold, GroupShuffleSplit, cross_validate, \n                                       RandomizedSearchCV,GridSearchCV)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport sklearn.tree as tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import log_loss, roc_curve, auc\n\n#PDPbox\nfrom pdpbox import pdp\n\n# MATPLOTLIB\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# OTHER\nfrom itertools import product\nimport copy\nimport graphviz\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5821cece":"df = pd.read_csv(\"..\/input\/simulated-data-for-ml-paper\/simulated_TECHCO_data.csv\")","30b48779":"df","293a36e3":"df.time = df.time.astype(int)\ndf.is_male = df.is_male.astype(int)","ca8b1c9b":"df.set_index(['emp_id','time'],drop=False,inplace=True)","ce5eb200":"df.loc[df['turnover']=='Stayed', 'turnover'] = '0 Stayed'\ndf.loc[df['turnover']=='Left', 'turnover'] = '1 Left'\ndf.turnover\ny = df.turnover","2bc43560":"X = df.drop(['turnover'],axis=1)\ncols = [\"time\",\"training_score\",\"avg_literacy\",\"is_male\",\"logical_score\",\"verbal_score\",\"location_age\",\"distance\",\"similar_language\"]\nX = X[cols]","a96b0529":"train_inds, test_inds = next(GroupShuffleSplit(test_size=.3,random_state=111).split(X,y,groups=df.emp_id))","1fc9f753":"X_train, X_test = X.iloc[train_inds], X.iloc[test_inds]\ny_train, y_test = y.iloc[train_inds], y.iloc[test_inds]\nemployee_train = df.emp_id.iloc[train_inds]","1f256544":"folds = list(GroupKFold(n_splits=10).split(X_train,y_train,employee_train))","29b3d37b":"X_train_not_panel = X_train[X_train.time==1].drop(columns=\"time\")\nX_train_not_panel.corr()","83464ca7":"pd.plotting.scatter_matrix(X_train_not_panel,figsize=(16,12),alpha=0.3);","b5a3dd46":"def plot_roc(y_predictions,y_true,name,pos_label):\n    fpr,tpr,thresholds = roc_curve(y_true,y_predictions,pos_label=pos_label)\n    roc_auc = auc(fpr, tpr)\n    plt.title(name)\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","9d0071e3":"def pdplot(X, model, n, var_name,categorical_var=False,which_class = 1):        \n    X_copy = copy.deepcopy(X)\n    #For the continuous variables that will be plotted, create 40-interval arrays.\n    if categorical_var == False:\n        var_grid_vals = np.linspace(X_copy[var_name].min(), X_copy[var_name].max(), num=40)\n    #For the categorical variables that will be plotted, create array of the unique values\n    if categorical_var == True:\n        var_grid_vals = list(set(X_copy[var_name]))\n    \n    samples=np.random.choice(len(X_copy), n, replace=False)\n    \n    predictions = pd.DataFrame()\n      \n    f=plt.figure()\n    \n    for sample in samples:\n        x_vals = list()\n        for i in var_grid_vals:\n            X_copy[var_name]=i\n            y_hat=model.predict_proba(X_copy.iloc[(sample-1):sample])[:,which_class][0]\n            y_hat_log_odds = np.log(y_hat\/(1-y_hat))\n            predictions=predictions.append({'sample':sample,'x_val':i,'pred':y_hat_log_odds},ignore_index=True)\n        sample_preds = predictions[predictions['sample']==sample]\n        plt.plot(sample_preds.x_val, sample_preds.pred, c='C1', alpha=1.0, linewidth=0.1)\n        \n    preds_grouped = predictions.groupby(['x_val']).mean().reset_index()\n    plt.plot(preds_grouped.x_val, preds_grouped.pred, c='C0', linestyle='--')\n    plt.ylabel(r'Log Odds of Turnover Probability (log $\\frac{h_\\theta(x)}{1-h_\\theta(x)}$)')\n    plt.xlabel(var_name)\n    plt.ylim(-8,0)","8db52873":"def plot_twoway_pdp(X,model,var1_name,var2_name,categorical_var1=False,categorical_var2=False,\n                 var1_min=None,var1_max=None,\n                 var2_min=None,var2_max=None,\n                 which_class = 1):\n    #Set the min and max value to plot for both variables. Default is the min and max of the variable\n    if var1_min is None:\n        var1_min=X[var1_name].min() \n    if var1_max is None:\n        var1_max=X[var1_name].max() \n    if var2_min is None:\n        var2_min=X[var2_name].min()\n    if var2_max is None:\n        var2_max=X[var2_name].max() \n        \n    X_copy = copy.deepcopy(X)\n    \n    #For the continuous variables that will be plotted, create 40-interval arrays.\n    if categorical_var1 == False:\n        var1_grid_vals = np.linspace(var1_min, var1_max, num=40)\n    if categorical_var2 == False:\n        var2_grid_vals = np.linspace(var2_min, var2_max, num=40)\n        \n    #For the categorical variables that will be plotted, create array of the unique values\n    if categorical_var1 == True:\n        var1_grid_vals = list(set(X_copy[var1_name]))\n    if categorical_var2 == True:\n        var2_grid_vals = list(set(X_copy[var2_name]))\n    \n    predictions_from_grid = list()\n    x_vals = list()\n    y_vals =list()\n    \n    for i in var1_grid_vals:\n        for j in var2_grid_vals:\n            X_copy[var1_name]=i\n            X_copy[var2_name]=j\n            y_hats = model.predict_proba(X_copy)[:,which_class]\n            predictions_from_grid.append(np.mean(y_hats))   \n            x_vals.append(i)\n            y_vals.append(j)\n\n    plt.figure()\n    plt.scatter(x_vals,y_vals,c=np.log(predictions_from_grid),marker='s',vmin=-8,vmax=-1)\n    plt.xlabel(var1_name)\n    plt.ylabel(var2_name)\n    cbar = plt.colorbar(ticks=range(-8,0))\n    cbar.ax.set_yticklabels(['$10^{'+str(i)+'}$' for i in range(-8,0) ])","c6f407d5":"#Make a dictionary of combinations of hyperparameters to try\nrandom_grid = {'criterion': ['entropy','gini'],\n               'max_depth': np.unique( np.exp(np.linspace(0, 10, 100)).astype(int) ),\n               'min_samples_leaf': np.unique( np.exp(np.linspace(0, 8, 100)).astype(int) ),\n               'min_impurity_decrease': np.exp(np.linspace(-9, -1, 100))}","8aac81e8":"dt_random_search = RandomizedSearchCV(estimator = DecisionTreeClassifier(), \n                                      param_distributions = random_grid,\n                                      random_state=345, n_iter = 100,\n                                      scoring='neg_log_loss',n_jobs=-1,\n                                      cv =folds,return_train_score=True)\ndt_random_search.fit(X=X_train,y=y_train)\ndt_random_search.best_params_","810f440e":"dt=dt_random_search.best_estimator_","daa30d0a":"best_model_index = dt_random_search.best_index_\ndt_train_score = dt_random_search.cv_results_['mean_train_score'][best_model_index]\ndt_validation_score = dt_random_search.cv_results_['mean_test_score'][best_model_index]\ndt_train_std = dt_random_search.cv_results_['std_train_score'][best_model_index]\ndt_validation_std = dt_random_search.cv_results_['std_test_score'][best_model_index]","c9a4f7b8":"decisiontree_test_pred = dt.predict_proba(X_test)[:,1]","66a19b7d":"test_loss_dt = log_loss(y_test.values,decisiontree_test_pred)\nprint(\"Test Loss: %0.4f\" % test_loss_dt)","f13e632f":"decision_tree = graphviz.Source( tree.export_graphviz(dt, out_file=None, feature_names=X_train.columns, filled=False, rounded=True,impurity=True))\ndecision_tree","029c69de":"plot_roc(decisiontree_test_pred,y_test,\"Decision Tree ROC Curve\",pos_label=\"1 Left\")","32bb94bf":"importances = dt.feature_importances_\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances,\n       color=\"b\", align=\"center\")\nplt.xticks(range(X.shape[1]), cols,rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","0cadb95f":"pdplot(X=X_train,var_name='time',n=500,model=dt,which_class = 1)","15d56a0e":"plot_twoway_pdp(X=X_train, model=dt,\n             var1_name='time',var2_name='training_score',\n             var2_min=2.5,which_class=1)","33362855":"#Make a dictionary of which combinations of hyperparameters to try\nrandom_grid = {'criterion': ['entropy'],\n               'max_depth': np.unique( np.exp(np.linspace(0, 10, 100)).astype(int) ),\n               'min_samples_leaf': np.unique( np.exp(np.linspace(0, 8, 100)).astype(int) ),\n               'max_features': [None,'auto','log2'],\n               'min_impurity_decrease': np.exp(np.linspace(-9, -1, 100))}","73b02de4":"rf_random_search = RandomizedSearchCV(estimator = RandomForestClassifier(n_estimators=100), \n                                      param_distributions = random_grid,\n                                      random_state=345, n_iter = 100,\n                                      scoring='neg_log_loss',n_jobs=-1,\n                                      cv =folds,return_train_score=True)\nrf_random_search.fit(X=X_train,y=y_train)\nrf_random_search.best_params_","fc6bee79":"rf = rf_random_search.best_estimator_","e2383121":"best_model_index = rf_random_search.best_index_\nrf_train_score = rf_random_search.cv_results_['mean_train_score'][best_model_index]\nrf_validation_score = rf_random_search.cv_results_['mean_test_score'][best_model_index]\nrf_train_std = rf_random_search.cv_results_['std_train_score'][best_model_index]\nrf_validation_std = rf_random_search.cv_results_['std_test_score'][best_model_index]","1f092452":"randomforest_test_pred = rf.predict_proba(X_test)[:,1]","68d1490e":"test_loss_rf = log_loss(y_test.values,randomforest_test_pred)\nprint(\"Test Loss: %0.4f\" % test_loss_rf)","cc394d5b":"plot_roc(randomforest_test_pred,y_test,\"Random Forest ROC Curve\",pos_label=\"1 Left\")","0ba4f6e8":"importances = rf.feature_importances_\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances,\n       color=\"b\", align=\"center\")\nplt.xticks(range(X.shape[1]), cols,rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","afb1b95f":"pdplot(X=X_train,var_name='time',n=500,model=rf,which_class = 1)","b0d3fe5a":"plot_twoway_pdp(X=X_train, model=rf,\n             var1_name='time',var2_name='training_score',\n             var2_min=2.5,which_class=1)","88ffcfd9":"nnet = MLPClassifier(random_state=345,max_iter=100)\nscaler = StandardScaler()\nnnet_pipeline = make_pipeline(scaler,nnet)","b9d2613a":"#Make a dictionary of which combinations of hyperparameters to try\nrandom_grid = {'mlpclassifier__solver': ['adam'],\n               'mlpclassifier__activation': ['tanh','relu','logistic'],\n               'mlpclassifier__alpha': [0.01,0.05],\n               'mlpclassifier__hidden_layer_sizes': [(30,20,10),(20,20),(25,15),(10,40,10),(20,20,20),(30,10)]}","b482eea9":"# #Commented out for speed\n# nnet_random_search = RandomizedSearchCV(estimator = nnet_pipeline, \n#                                       param_distributions = random_grid, n_iter = 25,\n#                                       #random_state=345, \n#                                       scoring='neg_log_loss',n_jobs=-1,\n#                                       cv =folds,return_train_score=True)\n# nnet_random_search.fit(X=X_train,y=y_train)\n# nnet_random_search.best_params_","fdc4ba80":"nnet = MLPClassifier(solver='adam', \n                     activation='tanh',\n                     alpha=1e-2, \n                     hidden_layer_sizes=(32,12), \n                     random_state=345)\nnnet_pipeline = make_pipeline(scaler,nnet)","f1c03783":"cv_results_nnet = cross_validate(\n    estimator=nnet_pipeline,\n    X=X_train,\n    y=y_train,\n    cv=folds,\n    scoring='neg_log_loss',\n    return_train_score=True\n)\ncv_val_score_nnet = -cv_results_nnet['test_score'].mean()\n\nprint(\"Mean cross-validation score (log loss) for neural network model: %0.4f\" % cv_val_score_nnet)","427bc87f":"nnet_pipeline.fit(X_train,y_train)\nnnet_test_pred = nnet_pipeline.predict_proba(X_test)[:,1]","717358af":"test_loss_nnet = log_loss(y_test.values,nnet_test_pred)\nprint(\"Test Loss: %0.4f\" % test_loss_nnet)","5ceb14e6":"plot_roc(nnet_test_pred,y_test,\"Neural Network ROC Curve\",pos_label=\"1 Left\")","2803b1c3":"pdplot(X=X_train,var_name='time',n=500,model=nnet_pipeline,which_class = 1)","32162c07":"plot_twoway_pdp(X=X_train, model=nnet_pipeline,\n             var1_name='time',var2_name='training_score',\n             var2_min=2.5,which_class=1)","7816338f":"preprocess = make_column_transformer(\n    (OneHotEncoder(), [\"time\"]),\nremainder=\"passthrough\")\n\nlogistic = LogisticRegression(C=1e8)\n\nlogistic_pipeline = make_pipeline(\n    preprocess,\n    logistic)","90a2d712":"cv_results_logistic = cross_validate(\n    estimator=logistic_pipeline,\n    X=X_train,\n    y=y_train,\n    cv=folds,\n    scoring='neg_log_loss',\n    return_train_score=True\n)\ncv_val_score_logistic = -cv_results_logistic['test_score'].mean()\n\nprint(\"Mean cross-validation score (log loss) for logistic model: %0.4f\" % cv_val_score_logistic)","b51e178d":"logistic_pipeline.fit(X_train,y_train)\nlogistic_test_pred = logistic_pipeline.predict_proba(X_test)[:,1]\n\nprint(\"Test Loss: %0.4f\" % log_loss(y_test.values,logistic_test_pred))","f8438b0c":"plot_roc(logistic_test_pred,y_test,\"Logistic ROC Curve\",pos_label=\"1 Left\")","b222107b":"#Note since the variable time is categorical in our logistic_pipeline model, we set categorical_var=True in the pdplot function\npdplot(X=X_train,var_name='time',categorical_var=True,n=500,model=logistic_pipeline)","db9cee0b":"#Note since the variable time is categorical in our logistic_pipeline model, we set categorical_var1=True in the plot_twoway_pdp function\nplot_twoway_pdp(X=X_train, model=logistic_pipeline,\n             var1_name='time',var2_name='training_score',categorical_var1=True,\n             var2_min=2.5,which_class=1)","f8f08c52":"x=[-dt_train_score,-rf_train_score,-cv_results_nnet['train_score'].mean(),-cv_results_logistic['train_score'].mean()]\nx_err=[dt_train_std,rf_train_std,np.std(cv_results_nnet['train_score']),np.std(cv_results_logistic['train_score'])]\ny=[-dt_validation_score,-rf_validation_score,-cv_results_nnet['test_score'].mean(),-cv_results_logistic['test_score'].mean()]\ny_err=[dt_validation_std,rf_validation_std,np.std(cv_results_nnet['test_score']),np.std(cv_results_logistic['test_score'])]","649643f9":"plt.plot([-1,1],[-1,1], c='k', marker='None',linestyle='--',label='_nolegend_')\nfor ax, ax_err, ay, ay_err in zip(x, x_err, y, y_err):\n    plt.errorbar(ax, ay, xerr=ax_err, yerr=ay_err, label='Training',marker='o', linestyle='None')\nplt.xlim(0.06,0.07625)\nplt.ylim(0.06,0.07625)\nplt.xlabel('Training Loss')\nplt.ylabel('Validation Loss')\nplt.legend(['Decision Tree','Random Forest','Neural Network','Logistic Regression'], loc=4)","f0d659da":"x=[-(dt_train_score-cv_results_logistic['train_score'].mean()),-(rf_train_score-cv_results_logistic['train_score'].mean()),-(cv_results_nnet['train_score'].mean()-cv_results_logistic['train_score'].mean()),0]\nx_err=[np.std(dt_train_std-cv_results_logistic['train_score']),np.std(rf_train_std-cv_results_logistic['train_score']),np.std(cv_results_nnet['train_score']-cv_results_logistic['train_score']),0]\ny=[-(dt_validation_score-cv_results_logistic['test_score'].mean()),-(rf_validation_score-cv_results_logistic['test_score'].mean()),-(cv_results_nnet['test_score'].mean()-cv_results_logistic['test_score'].mean()),0]\ny_err=[np.std(dt_validation_std-cv_results_logistic['test_score']),np.std(rf_validation_std-cv_results_logistic['test_score']),np.std(cv_results_nnet['test_score']-cv_results_logistic['test_score']),0]","ee78f378":"plt.plot([-1,1],[-1,1], c='k', marker='None',linestyle='--',label='_nolegend_')\nfor ax, ax_err, ay, ay_err in zip(x, x_err, y, y_err):\n    plt.errorbar(ax, ay, xerr=ax_err, yerr=ay_err, label='Training',marker='o', linestyle='None')\nplt.xlim(-0.010,0.002)\nplt.ylim(-0.0082,0.0025)\nplt.xlabel('Training Loss (Relative to Logistic)')\nplt.ylabel('Validation Loss (Relative to Logistic)')\nplt.legend(['Decision Tree','Random Forest','Neural Network','Logistic Regression'], loc=4)","80b2523a":"### Random Forest","05877e07":"# 1. Import Libraries and Data","49bf03b4":"##### Visualizations: Two-way Partial Dependence Plot","2b5e2403":"Using `RandomizedSearchCV` we try many hyperparameter combinations","558f5dbd":"We set the model using the parameters that returned the best results.","a11ff2a3":"##### Visualizations: ROC Curve","5d0a2d2d":"Make predictions on the holdout test set","7be11041":"Evaluate the test loss","480a21cd":"### Decision Tree","4a16711c":"Import data.","f32248c9":"Fit the model using the training data, and use it to make predictions on the holdout test set","48b0c1bd":"### One-way Partial Dependence Plot Function with overlaid Individual Conditional Expectation (ICE) Plot \n`pdplot()` takes a dataframe (`X`), the estimator (`model`), the name of the variable to plot (`var_name`), and the number of desired samples to plot (`n`). It returns a partial dependence plot, which plots how the model predicts each sampled observation's predicted probability of turnover over the range of possible values of the plotted variable. It also includes `n` individual ICE lines.  Note the argument `which_class`, which tells the function to predict probabilities for a specific class label (in our case, \"Left\").\n  \n**For the off-the-shelf function for plotting partial dependence and ICE plots, use the `pdp_plot()` function from `pdpbox` package.**","79f947bd":"# 4. Algorithm Implementation and Results","e8b5fa6a":"##### Visualizations: ROC Curve","527e1927":"##### Variable Importance (Feature Importance)","a30bba4f":"Using `RandomizedSearchCV` we can try many hyperparameter combinations and select the best performing combination. When using a pipeline, we have to specify to which step the hyperparameters belong. For example rather than writing `'hidden_layer_sizes'` we write `'mlpclassifier__hidden_layer_sizes'` to denote that hidden layer size parameter belongs to the classifier and not the scaler.","7500d23e":"### Neural Network","d1bf7821":"Neural networks don't have an intrinsic variable importance like decision trees and random forests. However, if you want an interpretation of variable importance for neural nets, there are alternative packages that can help (e.g. the`LIME` package)","41ee79a0":"Create `X`, the matrix of variables. Also use the list `cols` to set a consistent ordering of columns","4bd7474e":"Evaluate the test loss","a5efc861":"##### Visualizations: ROC Curve","9247d8ab":"Using `RandomizedSearchCV` we can try many hyperparameter combinations. We make a dictionary (which we call `random_grid`) of different hyperparameter values that we want the algorithm to try.","bd16bd4d":"##### Visualizations: Two-way Partial Dependence Plot","57996fa5":"Use the best model to make predictions on the holdout test set","c5ab3c33":"Split to get indices for training\/cross-validation and holdout test sets. We are using panel data (indices of `emp_id` and `time`) so we sample at the employee level rather than the observation level. If you are just sampling at the observation level, the code for this operation would be `GroupShuffleSplit(test_size=.3)` ","3af8a627":"### Logistic Regression\n","1cf84dd8":"Use the model which returned the best results (slightly modified the hidden layer sizes from (30,10) to (32,12))","15c102ed":"Define `folds`: a list of partitions in the data which will be used for cross-validation. Because we are using panel data where each employee has multiple observations, we sample at the employee level, not at the observation level. We use the function `GroupKFold` to split the data into folds grouped at the employee level. ","98ee7c02":"Plot the cross validation loss for the training and holdout test set on the x and y axis, respectively. Error bars represent standard deviation variance yielded by the k-folds cross-validation.","29eae11c":"This section defines the custom visualization functions for partial dependence plots and ROC curves that we used in the paper. You can also use off-the-shelf packages, such as the `pdpbox` package.","e94064c7":"`RandomizedSearchCV` randomly tries different combinations of Decision Tree hyperparameters from the `random_grid` dictionary. We tell it to try 100 different combinations (`n_iter=100`), to use the `neg_log_loss` score to evaluate each model's performance, and to use `cv=folds` for cross-validation (if we were simply sampling at the observation level rather than at the employee level, we could just set `cv=10`).","2fe81e1d":"Neural networks perform better with standardized variable scaling. In cross-validation, variables should be rescaled using the training set within each fold, so rather than scaling the data before cross-validation, we add scaling as a pre-step to fitting the model in a pipeline using the `make_pipeline` function. Now when we fit the model using the pipeline, it will first standardize the variables using `scaler` before fitting the model `nnet`.","e9502e47":"Make predictions on the holdout test set","6407b1e9":"##### Visualizations: Decision Tree","ca1c3c73":"##### Visualizations: ROC Curve","555c28d3":"##### Visualizations: Two-way Partial Dependence Plot","f8a146f0":"We could keep tuning, but for simplicity we stop here to set the model using the hyperparameters that returned the best results.","77d9eaa7":"# Introduction","b3491866":"Find the cross validation scores (these will be plotted at the end)","09f15ad8":"##### Visualizations: Partial Dependence Plot","81bc6279":"This is the `Python` version of the notebook. For the `R` version, click [here](https:\/\/www.kaggle.com\/ryanthomasallen\/online-appendix-for-cae-2020-r)\n\nThis notebook is provided as supplementary material to the paper [Machine Learning for Pattern Discovery in Management Research](https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=3518780) by Prithwiraj Choudhury, Ryan Allen, and Michael Endres (Strategic Management Journal, 2020).\n\nWe assume users of this notebook are familiar with the paper, which lays out fundamental concepts of machine learning and the empirical setting. We also assume familiarity with Python, including the `pandas` and `scikit-learn` packages.\n\nIn order to preserve the anonymity of TECHCO and its employees, this notebook uses simulated data based on the real data used in the paper. The results in this notebook do not exactly match the results in the paper, but yield qualitatively similar findings.","d3398982":"##### Visualizations: Partial Dependence Plot","30527812":"##### Visualizations: Partial Dependence Plot","c89d65b4":"#### Visualizations","0e702131":"### Two-way Partial Dependence Plot function\n`plot_twoway_pdp()` takes a dataframe (`X`), the estimator (`model`), the name of the variable to plot on the x-axis (`var1_name`), and the name of the variable to plot on the y-axis (`var2_name`). The function returns a two-way PDP, which displays the model's predicted probability of turnover for each point across the range of 40 values for two variables (40\\*40 = 1600 points total).    \n  \nThe argument `which_class` tells the function to predict probabilities for a specific class label (in our case, \"Left\").\n  \n**For an off-the-shelf function for plotting two way partial dependence use the `pdp_interact_plot()` function from `pdpbox` package.**","f1b28f91":"In our logistic regression we include time fixed effects. So we add a preprocessing step that uses `OneHotEncoder()` to transform `time` into a dummy variable for each month.\n  \nThe default for `sklearn.linear_model.LogisticRegression` is to include the L2 regularization term, and it isn't possible to entirely exclude a regularization term. To replicate a traditional logistic regression model with no regularization (like logit in Stata), set the parameter `C` to a very high number to make the strength of the L2 regularization term essentially zero (smaller values of `C` specify stronger regularization).","630332cb":"##### Visualizations: Two-way Partial Dependence Plot","10b71b60":"Find the cross validation scores (these will be plotted at the end)","5289ad53":"# Plots that compare training to test loss for all the models","0cd616a5":"Check Correlations. We should be aware that some algorithms may randomly substitute highly related variables for one another.","4ae896b5":"##### Variable Importance (Feature Importance)","ad9b8b0c":"##### Variable Importance (Feature Importance)","ce3f000a":"Set index as `emp_id` and `time`","1a33fb46":"### Function for Plotting the ROC Curve\n![](http:\/\/)![](http:\/\/)`plot_roc()` takes predictions (`y_predictions`), ground truth (`y_true`), and the name of the model (`name`) as arguments. It returns a plot of an ROC curve.","5620b1a8":"Save the cross-validation scores and standard deviations (will plot later)","ac751266":"Evaluate the holdout test loss","fdb5e369":"Find the cross validation scores (these will be plotted at the end)","96abd0c6":"Now plot the cross validation loss *relative to the logistic regression model* for the training and holdout test set on the x and y axis, respectively. Error bars represent standard deviation variance yielded by the k-folds cross-validation.","37147342":"Convert variables to appropriate data types","33ac0603":"Create training and test sets for `X`, `y`, and employee groupings","7b176db3":"# 2. Preprocess and Partition Data","54b6286f":"# 3. Define Custom Visualization Functions","1b035363":"##### Visualizations: Partial Dependence Plot","76ae8139":"Define the target, y as `turnover`. Set \"Left\" as the positive label."}}