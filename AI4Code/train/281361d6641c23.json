{"cell_type":{"4f7b74cc":"code","22d5f47f":"code","58edc340":"code","7bbdd105":"code","8a577a98":"code","4138a1c7":"code","3f75f76a":"code","0c1b3dbb":"code","ba5e8817":"code","a53d9135":"code","4f2c5ae2":"code","3c90cc50":"code","f7f89bce":"code","3ba32b14":"markdown","89f4d2bc":"markdown","f7cdb858":"markdown","fd6a3060":"markdown","c3b24de3":"markdown","8a524b0b":"markdown"},"source":{"4f7b74cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22d5f47f":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler","58edc340":"from sklearn.datasets import fetch_california_housing\n\ncalifornia = fetch_california_housing()","7bbdd105":"df = pd.DataFrame(california.data, columns=california.feature_names)\ndf['Target'] = california.target\ndf.tail()","8a577a98":"scaler = StandardScaler()\nscaler.fit(df.values[:, :-1]) # Calculate mu, sigma (Exclude target when scaler fit)\ndf.values[:, :-1] = scaler.transform(df.values[:, :-1]) # transform value by applying mu and sigma\n\ndf.tail()","4138a1c7":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","3f75f76a":"data = torch.from_numpy(df.values).float()\n\ndata.shape","0c1b3dbb":"x = data[:, :-1]\ny = data[:, -1:]\n\nprint(x.shape, y.shape)","ba5e8817":"n_epochs = 1000\nbatch_size = 256\nprint_interval = 200\n#learning_rate = 1e-2 # It doesn't need","a53d9135":"model = nn.Sequential(\n    nn.Linear(x.size(-1), 6), # start \n    nn.LeakyReLU(),\n    nn.Linear(6, 5),\n    nn.LeakyReLU(),\n    nn.Linear(5, 4),\n    nn.LeakyReLU(),\n    nn.Linear(4, 3),\n    nn.LeakyReLU(),\n    nn.Linear(3, y.size(-1))\n)\n\nmodel","4f2c5ae2":"# Adam doesn't need learning rate hyper-parameter.\noptimizer = optim.Adam(model.parameters())","3c90cc50":"# |x| = (total_size, input_dim)\n# |y| = (total_size, output_dim)\n\nfor i in range(n_epochs):\n    # Suffle the index to feed-forward.\n    indices = torch.randperm(x.size(0)) # Return a random permutation from indices.\n    x_ = torch.index_select(x, dim=0, index=indices) # Returns a new tensor has the indexes\n    y_ = torch.index_select(y, dim=0, index=indices)\n    \n    x_ = x_.split(batch_size, dim=0) # the last x_ is the remaining number of the x_\n    y_ = y_.split(batch_size, dim=0)\n    # |x_[i]| = (batch_size, input_dim)\n    # |y_[i]| = (batch_size, output_dim)\n    \n    y_hat = []\n    total_loss = 0\n    \n    for x_i, y_i in zip(x_, y_):\n        # |x_i| = |x_[i]|\n        # |y_i| = |y_[i]|\n        y_hat_i = model(x_i)\n        loss = F.mse_loss(y_hat_i, y_i)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n        \n        total_loss += float(loss) # This is very important to prevent memory leak.\n        y_hat += [y_hat_i]\n        \n    total_loss = total_loss \/ len(x_)\n    if (i + 1) % print_interval == 0:\n        print('Epoch %d loss=%.4e' % (i + 1, total_loss))\n        \ny_hat = torch.cat(y_hat, dim=0)\ny = torch.cat(y_, dim=0)\n# |y_hat| = (total_size, output_dim)\n# |y| = (total_size, output_dim)\n    \n    ","f7f89bce":"\ndf = pd.DataFrame(torch.cat([y, y_hat], dim=1).detach().numpy(),\n                 columns=['y', 'y_hat'])\nsns.pairplot(df, height=5)\nplt.show()","3ba32b14":"# **Adam Optimizer**","89f4d2bc":"## **Train Model with PyTorch**","f7cdb858":"## **Let's see the result!**","fd6a3060":"### Load Dataset from sklearn","c3b24de3":"## **Hyper parameter configuration without learning_rate in Adam**\n\n - Adam atomatically applies the learning_rate. \n - Therefore, it is convenient to use Adam at the beginnig.","8a524b0b":"## **Build Model**"}}