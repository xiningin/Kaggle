{"cell_type":{"1fef17d4":"code","f3db1197":"code","b3ffc1dd":"code","d96a4a6a":"code","73dcd2d9":"code","80a6f39e":"code","ce089a84":"code","c4c88033":"code","398b8d7a":"code","89a8ed82":"code","6622db63":"code","a5b4a138":"code","c9a35ecf":"code","d7f69e1c":"code","6966857a":"code","b3a5e1ca":"code","63c72fa3":"code","7003cb45":"markdown","7ba689e0":"markdown","f587d2d9":"markdown","f8e420c6":"markdown","689fc2cf":"markdown","42c6a042":"markdown","59934d35":"markdown","56db5bb2":"markdown","ce2b4ec6":"markdown","aafd2dbd":"markdown","a4be369b":"markdown","970592b4":"markdown","c9631589":"markdown","fa9604b8":"markdown"},"source":{"1fef17d4":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(5)","f3db1197":"data = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndata.head()","b3ffc1dd":"#Standardizing the data before proceeding:\nstandard_scalar = StandardScaler()\ndata_scaled = standard_scalar.fit_transform(data)\ndata = pd.DataFrame(data_scaled, columns=data.columns)\ndata.head()","d96a4a6a":"from sklearn.cluster import KMeans\n\nkm = KMeans(init=\"random\", n_clusters=5)\nkm.fit(data)","73dcd2d9":"km.labels_","80a6f39e":"km.cluster_centers_","ce089a84":"# k-means determine k\ndistortions = []\nK = range(1, 20)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(data)\n    distortions.append(kmeanModel.inertia_)\n    \n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('No of clusters (k)')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","c4c88033":"estimators = [('k_means_8', KMeans(n_clusters=8, init='k-means++')),\n              ('k_means_5', KMeans(n_clusters=5, init='k-means++')),\n              ('k_means_bad_init', KMeans(n_clusters=5, n_init=1, init='random'))]\n\nfignum = 1\ntitles = ['8 clusters', '5 clusters', '5 clusters, bad initialization']\n\nfor name, est in estimators:\n    fig = plt.figure(fignum, figsize=(8, 6))\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n    est.fit(data)\n    labels = est.labels_\n\n    ax.scatter(data.values[:, 3], data.values[:, 0], data.values[:, 2], c=labels.astype(np.float), edgecolor='k')\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel('fixed acidity')\n    ax.set_ylabel('volatile acidity')\n    ax.set_zlabel('citric acid')\n    ax.set_title(titles[fignum - 1])\n    ax.dist = 12\n    fignum = fignum + 1","398b8d7a":"from sklearn.datasets import make_moons\nX3, y3 = make_moons(250, noise=.075, random_state=22)\n\nlabels = KMeans(2, random_state=0).fit_predict(X3)\nplt.scatter(X3[:, 0], X3[:, 1], c=labels, s=50, cmap='cividis')","89a8ed82":"from sklearn.cluster import AgglomerativeClustering\nclustering = AgglomerativeClustering().fit(data)\nclustering","6622db63":"clustering.labels_","a5b4a138":"from scipy.cluster.hierarchy import dendrogram\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n    \nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(data)\n\nplt.figure(fignum, figsize=(10, 6))\nplt.title('Hierarchical Clustering Dendrogram')\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode='level', p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n","c9a35ecf":"from sklearn.datasets import make_moons\nX3, y3 = make_moons(250, noise=.075, random_state=22)\n\nsingle = AgglomerativeClustering(n_clusters=2, linkage='single')\n\nlabels = single.fit_predict(X3)\nplt.scatter(X3[:, 0], X3[:, 1], c=labels, s=50, cmap='cividis')","d7f69e1c":"from sklearn.cluster import SpectralClustering\nmodel = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans').fit(data)\nmodel","6966857a":"clustering.labels_","b3a5e1ca":"from sklearn.datasets import make_moons\nX3, y3 = make_moons(250, noise=.075, random_state=22)\n\nsingle = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n\nlabels = single.fit_predict(X3)\nplt.scatter(X3[:, 0], X3[:, 1], c=labels, s=50, cmap='cividis')","63c72fa3":"import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nnp.random.seed(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(3 * 2 + 3, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndefault_base = {'quantile': .3,\n                'eps': .3,\n                'damping': .9,\n                'preference': -200,\n                'n_neighbors': 10,\n                'n_clusters': 3,\n                'min_samples': 20,\n                'xi': 0.05,\n                'min_cluster_size': 0.1}\n\ndatasets = [\n    (noisy_circles, {'damping': .77, 'preference': -240,\n                     'quantile': .2, 'n_clusters': 2,\n                     'min_samples': 20, 'xi': 0.25}),\n    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n    (varied, {'eps': .18, 'n_neighbors': 2,\n              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n    (aniso, {'eps': .15, 'n_neighbors': 2,\n             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n    (blobs, {}),\n    (no_structure, {})]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params['n_neighbors'], include_self=False)\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params['n_clusters'], linkage='ward',\n        connectivity=connectivity)\n    spectral = cluster.SpectralClustering(\n        n_clusters=params['n_clusters'], eigen_solver='arpack',\n        affinity=\"nearest_neighbors\")\n    dbscan = cluster.DBSCAN(eps=params['eps'])\n    optics = cluster.OPTICS(min_samples=params['min_samples'],\n                            xi=params['xi'],\n                            min_cluster_size=params['min_cluster_size'])\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params['damping'], preference=params['preference'])\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\", affinity=\"cityblock\",\n        n_clusters=params['n_clusters'], connectivity=connectivity)\n    birch = cluster.Birch(n_clusters=params['n_clusters'])\n    gmm = mixture.GaussianMixture(\n        n_components=params['n_clusters'], covariance_type='full')\n\n    clustering_algorithms = (\n        ('MiniBatchKMeans', two_means),\n        ('SpectralClustering', spectral),\n        ('AgglomerativeClustering', average_linkage),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \" +\n                \"connectivity matrix is [0-9]{1,2}\" +\n                \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning)\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\" +\n                \" may not work as expected.\",\n                category=UserWarning)\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, 'labels_'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                             '#f781bf', '#a65628', '#984ea3',\n                                             '#999999', '#e41a1c', '#dede00']),\n                                      int(max(y_pred) + 1))))\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment='right')\n        plot_num += 1\n\nplt.show()","7003cb45":"# What is clustering?\n* Clustering is an unsupervized learning technique where you take the entire dataset and find the \"groups of similar entities\"  within the dataset. Hence there is no labels within the dataset.\n* Useful for organizing very large dataset into meaningful clusters that can be useful and actions can be taken upon. For example, take entire customer base of more than 1M records and try to group into high value customers, low value customers and so on.\n\n### What questions does clustering typically tend to answer?\n* Types of pages are there on the Web?\n* Types of customers are there in my market?\n* Types of people are there on a Social network?\n* Types of E-mails in my Inbox?\n* Types of Genes the human genome has?\n\n\n\n\n## From clustering to classification\n* Clustering is base of all the classification problems. Initially, say we have a large ungrouped number of users in a new social media platform. We know for certain that the number of users will not be equal to the number of groups in the social media, and it will be reasonably finite.\n* Even though each user can vary in fine-grain, they can be reasonably grouped into clusters.\n* Each of these grouped *clusters* become *classes* when we know what group each of these users fall into.","7ba689e0":"![Clustering](https:\/\/d1m75rqqgidzqn.cloudfront.net\/wp-data\/2020\/01\/17162345\/clustering-algorithms-in-Machine-Learning-696x464.jpg)","f587d2d9":"# References:\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n2. https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py\n3. https:\/\/greyatom.com\/programs\/learn-data-science-online-with-projects\/learn\n4. https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n5. https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n\n# Images taken from:\n1. https:\/\/www.mygreatlearning.com\/blog\/clustering-algorithms-in-machine-learning\/\n2. https:\/\/commons.wikimedia.org\/wiki\/File:K-means_convergence.gif\n3. https:\/\/www.solver.com\/xlminer\/help\/hierarchical-clustering-intro\n4. https:\/\/towardsdatascience.com\/spectral-clustering-aba2640c0d5b","f8e420c6":"# Spectral clustering:\n* Works on similarity graphs where each node represents an entity and weight on the edge\n* Consider the structure similar to a graph where all the nodes are connected to all other nodes with edges constituting of weights\n* If we want to split it into two clusters, clearly we want to want to eliminate the edges which has the lowest weight\n\n![image.png](attachment:image.png)","689fc2cf":"### But is 5 clusters good fit?\n* As said above, we need to exactly specify how many clusters we are looking for in this case.\n* The number of clusters is very difficult to guess intuitively. It can only be intuitively be guessed if we know the data well.\n\n#### Solution to number of clusters:\nWe run the k-Means algorithm with different number of clusters and plot the goodness of fit by plotting the inertia parameter from sklearn, which gives **Sum of squared distances of samples to their closest cluster center.**\n","42c6a042":"# Implementation of k-Means using SKLearn","59934d35":"# Introduction\n\nI have made this notebook as an guide to understand implementaiton of clustering on a dataset using different techniques available in sklearn library. We will compare and contrast three most popular clustering techniques: Partition, Hierarchical clustering and spectral clustering.\n\nThe sections in this notebook are:\n1. [Introduction](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Introduction)\n2. [What is clustering?](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#What-is-clustering?)\n3. [Partition clustering](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Partition-clustering:)\n4. [Implementation of k-Means using SKLearn](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Implementation-of-k-Means-using-SKLearn)\n5. [Hierarchical Clustering](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Hierarchical-Clustering)\n6. [Spectral clustering](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Spectral-clustering:)\n7. [Comparing and contrasting clustering](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#Comparing-and-contrasting-clustering:)\n8. [References](https:\/\/www.kaggle.com\/gireeshs\/complete-guide-to-clustering-techniques\/#References:)\n\nI have also written a medium article from this notebook which is [available here](https:\/\/towardsdatascience.com\/beginners-guide-to-clustering-techniques-164d6ad5dbb).","56db5bb2":"# Representation of cluster in 3 dimentions","ce2b4ec6":"# k-Means clustering:\n\n![GIF](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/ea\/K-means_convergence.gif\/617px-K-means_convergence.gif)\n\n* Let the data points X = {x1, x2, x3, ... xn} be N data points that needs to be clustered into K clusters.\n* K falls between 1 and N, where if: \n    * K = 1 then whole data is single cluster, and mean of the entire data is the cluster center we are looking for.\n    * K =N, then each of the data individually represent a single cluster\n    * Typically K falls between 1 and N\n    \n    \n## Formulation as an optimization problem:\n* Let M = {m1, m2, m3, ... mk} be the cluster mean of the K clusters. Each of the m is the representative of the individual clusters that we are looking for. \n* The objective function is that we find such representation for each cluster, that approximate the data the best and the error of approximation is minimum. \n* The objective function that we are trying to minimize is sum squared of the distance between each data point and its representative.\n\n![image.png](attachment:image.png)","aafd2dbd":"For the same moon dataset we can now see that with AgglomerativeClustering, single linkage, we can get good clusters","a4be369b":"# Partition clustering:\n* Before start of clustering if we assume that data is going to fall into x number of clusters, and then partition the data into those many number of clusters then it is called partition clustering. **Number of clusters is known before performing clustering in partition clustering.**\n* **k-Means** is one of the popular partition clustering technique, where the data is partitioned into k unique clusters","970592b4":"# Limitations of k-means clustering:\n* k-Means clustering can only separate linear cluster boundaries, which means that it will fail to recognize far more complicated decision boundaries.","c9631589":"# Comparing and contrasting clustering:\n\n|Partition clustering|Hierarchical clustering|Spectral clustering|\n|------|------|------|\n|Works on data with Euclidean feature spaces|Works on pairwise distance functions in a bottom-up fashion and recursive partitional clustering in a top-down fashion|Works on similarity graphs where each node represents an entity and weight on the edge|\n|Complixity of k-Means is linear O(n)|Complixity is quardratic O(n^2)|Complixity is is linear O(n)|\n|Prior knowledge of number of clusters is necessary|Number of clusters can be interpreted after clustering is done|Prior knowledge of number of clusters is necessary|","fa9604b8":"# Hierarchical Clustering\nThe natural world is made up of hierarchy, like in food chain, organizational structure, biological classification of species, etc,.\nBottom-up hierarchical clustering also known as **agglomerative clustering**.\n![image.png](attachment:image.png)\nThe representation is called as a dentogram of clustering.\n\nThe key hyperparameter in the agglometarive clustering is the called **linkage**. It is the distance between two clusters in general. It is similar to the cluster mean M that is taken for the k-Means clustering. It can be represented in many ways:\n1. Single linkage: The distance between two closest points between the two clusters\n2. Complete linkage: Distance between two farthest points between the two clusters\n3. Average linkage: It is between single and complete linkage. Average between all pair of points is taken. This is robust to noise"}}