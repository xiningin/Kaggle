{"cell_type":{"f7e9055e":"code","6d785f9f":"code","eddf8265":"code","d0c09c41":"code","98591bee":"code","df6352b3":"code","092b48b4":"code","50cbf572":"code","e4ae4c53":"code","628602a7":"code","67ce3fec":"code","f179be56":"code","58239e94":"code","4e629075":"code","75e717a6":"code","ce22aa23":"code","02ba35c8":"code","ba995b3c":"code","10282b67":"code","ae375612":"code","f00bcd1f":"code","34f212e5":"code","7a60d3b8":"code","bcfa233e":"markdown","b4553490":"markdown","3f8ce182":"markdown","2d34662e":"markdown","375f901a":"markdown","38b233ce":"markdown","8f1720cb":"markdown","2afa2fe8":"markdown"},"source":{"f7e9055e":"# tpu v3-8 https:\/\/www.kaggle.com\/docs\/tpu#tpu2\nP = {}\nP['EPOCHS'] = 120\n# \u4e0d\u540cbase \u7684efficeinet \u4f3c\u4e4e\u53ea\u6709\u6a21\u578b\u89c4\u6a21\u7684\u6307\u6570\u4e0d\u540c! \n# \u4e0d\u662f\u7684\uff01\uff01\uff01\uff01\uff01 \u4e0d\u540c baseline \u5bf9\u5e94\u4e86 \u4e0d\u540c\u5206\u8fa8\u7387 https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n# \u6682\u65f6\u6362\u6210b0 \u8c03\u8bd5\u8d77\u6765\u66f4\u5feb\nP['BACKBONE'] = 'efficientnetb0' \n#P['BACKBONE'] = 'efficientnetb2' \nP['NFOLDS'] = 5\nP['SEED'] = 7788\nP['VERBOSE'] = 0\nP['DISPLAY_PLOT'] = True \nP['BATCH_COE'] = 8 # BATCH_SIZE = P['BATCH_COE'] * strategy.num_replicas_in_sync\n#P['BATCH_COE'] = 32 # for gpu\n\nP['TILING'] = [1024,256] # 1024,512 1024,256 1024,128 1536,512 768,384\nP['DIM'] = P['TILING'][1] \nP['DIM_FROM'] = P['TILING'][0]\n\n#P['LR'] = 5e-4\nP['LR'] = 5e-4 # for tpu\nP['OVERLAPP'] = True\nP['STEPS_COE'] = 1\n\nP['patience'] = 20\n# \u4e0d\u7528\u5916\u90e8\u6570\u636e \u52a0\u901f\u8c03\u8bd5\nP['extenal'] = False\n\nimport yaml\nwith open(r'params.yaml', 'w') as file:\n    yaml.dump(P, file)","6d785f9f":"#print(P)\n#!ls\n#!cat params.yaml","eddf8265":"! pip install segmentation_models -q\n%matplotlib inline\n\n%load_ext tensorboard\n# Clear any logs from previous runs\n!rm -rf .\/logs\/\n\nimport os\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport glob\n\nfrom segmentation_models.losses import bce_jaccard_loss\nimport segmentation_models as sm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_custom_objects\n\nfrom kaggle_datasets import KaggleDatasets\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nimport tensorflow_addons as tfa\n\n#import albumentations as albu\n#from functools import partial","d0c09c41":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # no TPU found, detect GPUs\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nBATCH_SIZE = P['BATCH_COE'] * strategy.num_replicas_in_sync\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\nprint(\"BATCH_SIZE: \", str(BATCH_SIZE))","98591bee":"#strategy","df6352b3":"#import tensorflow_datasets as tfds\nGCS_PATH = KaggleDatasets().get_gcs_path('hubmaptfwithtpuefficientunet256tfrecord')\n#GCS_PATH = '..\/input\/hubmap-tf-with-tpu-efficientunet-256-tfrecord'\nALL_TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\n#ALL_TRAINING_FILENAMES","092b48b4":"# \u4f7f\u7528shift\u8fc71024\u7684 train2\u6570\u636e \u8fd9\u91cc\u5982\u679coverlap\u4e3atrue\u4f1a\u5728\u8bad\u7ec3\u9636\u6bb5\u628atrain2\u52a0\u8fdb\u53bb\u4e00\u8d77\u8bad\u7ec3\nif P['OVERLAPP']:\n    OVERLAPP = tf.io.gfile.glob(GCS_PATH + '\/train2\/*.tfrec')\n    ALL_TRAINING_FILENAMES += OVERLAPP\n    \nif P['extenal']:\n    extenal = tf.io.gfile.glob(GCS_PATH + '\/extenal\/*.tfrec')\n    ALL_TRAINING_FILENAMES += extenal","50cbf572":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\nprint('NUM_TRAINING_IMAGES:' )\n#if P['OVERLAPP']:\n#    print(count_data_items(ALL_TRAINING_FILENAMES2)+count_data_items(ALL_TRAINING_FILENAMES)+count_data_items(ALL_TRAINING_FILENAMES3))\n#else:\nprint(count_data_items(ALL_TRAINING_FILENAMES))","e4ae4c53":"import json\nfrom datetime import datetime","628602a7":"#print(\"if without overlapp\", count_data_items(ALL_TRAINING_FILENAMES))\n#print(\"train2\", count_data_items(ALL_TRAINING_FILENAMES2))","67ce3fec":"#_r = tf.random.stateless_uniform(())\n\"\"\"\n# \u6d4b\u8bd5\u4e00\u4e0b\u56fe\u7247\u81ea\u589e\u5bf9\u591a\u56fe\u7247\nx = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0]],\n     [[7.0, 8.0, 9.0],\n      [10.0, 11.0, 12.0]]]\n_seed = (1, 2)\n_a = tf.image.stateless_random_saturation([x, x], 0.7, 1.3\n                                         #, seed\n                                         )\n\"\"\"\n\n#print(_r)\n#plt.imshow(a)\n#plt.imshow(b)\n\n#g1 = tf.random.Generator.from_seed(1)\n#print(g1.normal(shape=[]))\n#g2 = tf.random.get_global_generator()\n#print(g2.normal(shape=[]))\n\n#seed = g2.make_seeds(2)[0]\n#print(seed)\n#color_random = tf.random.uniform(shape=[], minval=0, maxval=10, dtype=tf.int64)\n#color_random % 3 == 1","f179be56":"DIM = P['DIM']\n#cutDIM = 128\n\n\"\"\"\n# \u54ce \u5f04\u4e86\u4e24\u5929 albu \u5728tpu \u4e0b\u5f7b\u5e95\u4e0d\u517c\u5bb9 \u5982\u679c\u5f3a\u884c\u5f00session\u8f6ctensor.ops\u5230np\u4f1a\u5f71\u54cd\u6027\u80fd \u8fd9\u4e2a\u76ee\u524d\u8fd8\u662f\u6ca1\u8f99\n# \u6570\u636e\u81ea\u589e\n# https:\/\/www.kaggle.com\/kool777\/training-hubmap-eda-tf-keras-tpu\ntransforms = albu.Compose([\n    albu.OneOf([\n        albu.RandomBrightness(limit=.2, p=1), \n        albu.RandomContrast(limit=.2, p=1), \n        albu.RandomGamma(p=1)\n    ], p=.5),\n    albu.OneOf([\n        albu.Blur(blur_limit=3, p=1),\n        albu.MedianBlur(blur_limit=3, p=1)\n    ], p=.25),\n    albu.OneOf([\n        albu.GaussNoise(0.002, p=.5),\n        albu.IAAAffine(p=.5),\n    ], p=.25),\n    albu.RandomRotate90(p=.5),\n    albu.HorizontalFlip(p=.5),\n    albu.VerticalFlip(p=.5),\n    albu.Cutout(num_holes=10, \n                max_h_size=int(.1 * DIM), max_w_size=int(.1 * DIM), \n                p=.25),\n    albu.ShiftScaleRotate(p=.25)\n])\n\n\ndef aug_fn(image, img_size, mask):\n    data = {\"image\":image, \"mask\": mask}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img\/255.0, tf.float32)\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    \n    aug_mask = aug_data[\"mask\"]\n    aug_mask = tf.cast(aug_mask, tf.float32)\n    aug_mask = tf.image.resize(aug_mask, size=[img_size, img_size])    \n    return aug_img, aug_mask\n    #return image, mask\n\"\"\"\n\n#https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation\ndef _parse_image_function(example_proto, seed, augment = True):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string)\n    }\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    image = tf.reshape( tf.io.decode_raw(single_example['image'],out_type=np.dtype('uint8')), (DIM,DIM, 3))\n    mask =  tf.reshape(tf.io.decode_raw(single_example['mask'],out_type='bool'),(DIM,DIM,1))\n        \n    if augment:\n        # \u8fd9\u91cc\u4ee3\u7801\u8981\u6539\u4e00\u4e0b \u548c\u53d6 tfrecord\u5206\u5f00\n        # https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation#apply_the_preprocessing_layers_to_the_datasets\n        \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)    \n        # shiftscalerotate 0.25\n        if tf.random.uniform(()) > 0.4:\t        \n            image = tf.image.flip_up_down(image)\t        #if tf.random.uniform(()) > 0.75:\n            mask = tf.image.flip_up_down(mask)\n            \n        #image = tf.image.stateless_random_flip_left_right(image, seed) \n        #mask = tf.image.stateless_random_flip_left_right(mask, seed) \n\n        #image = tf.image.stateless_random_flip_up_down(image, seed)\n        #mask = tf.image.stateless_random_flip_up_down(mask, seed)\n\n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.rot90(image)\n            mask = tf.image.rot90(mask)      \n        # shiftscalerotate 0.25\n        \n        #if tf.random.uniform(()) > 0.75:\n        #    image = tf.image.stateless_random_crop(image, size=[cutDIM, cutDIM, 3], seed=seed)\n        #    mask = tf.image.stateless_random_crop(mask, size=[cutDIM, cutDIM, 1], seed=seed)\n        \n        #  \u968f\u673a\u8c03\u6574\u8272\u8c03\n        if tf.random.uniform(()) > 0.75:\n            image = tf.image.stateless_random_hue(image, 0.2, seed=seed)    \n        \n        #  \u56fe\u7247\u8d28\u91cf \u8fd9\u4e2a\u597d\u50cf\u4f1a\u9020\u6210\u53cd\u5411\u4f18\u5316\n        #if tf.random.uniform(()) > 0.7:\n        #    image = tf.image.stateless_random_jpeg_quality(image, 75, 95, seed)\n            \n        if tf.random.uniform(()) > 0.75:\n            image = tf.image.stateless_random_saturation(image, 0.7, 1.3, seed=seed)\n            \n        if tf.random.uniform(()) > 0.75:\n            image = tf.image.stateless_random_contrast(image, lower=0.8, upper=1.2, seed=seed)\n        \n        if tf.random.uniform(()) > 0.75:\n            image = tf.image.stateless_random_brightness(image, max_delta=0.95, seed=seed)\n        \n        \"\"\"\n            #  \u989c\u8272\n            color_random = tf.random.uniform(shape=[], minval=0, maxval=10, dtype=tf.int64)\n            color_random_resd = color_random % 3;\n            if color_random > 5:\n                # \u4e09\u9009\u4e00\n                if color_random_resd == 2:\n                    image = tf.image.stateless_random_saturation(image, 0.7, 1.3, seed=seed)    \n                elif color_random_resd == 1:\n                    image = tf.image.stateless_random_brightness(image, max_delta=0.95, seed=seed)\n                else: \n                    # color_random_resd == 0:\n                    image = tf.image.stateless_random_contrast(\n                      image, lower=0.8, upper=1.2, seed=seed)\n                image = tf.clip_by_value(image, 0, 1)\n        \"\"\"\n        \n\n        # Blur MedianBlur \u4e8c\u9009\u4e00 \n        \n        \n        #noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=(50)\/(255), dtype=tf.float32)\n        #image = image + noise\n        \n        # albu.GaussNoise(0.002, p=.5),albu.IAAAffine(p=.5), \u4e8c\u9009\u4e00 0.25\n        #image = tf.image.resize(image, [DIM, DIM])\n        #mask = tf.image.resize(mask, [DIM, DIM])\n    return tf.cast(image, tf.float32), tf.cast(mask, tf.float32)\n    \"\"\"\n    # \u81ea\u589e\u5de5\u5177 https:\/\/albumentations.ai\/docs\/examples\/example_multi_target\/\n    # map numpy_function \u7b49\u7684\u8bf4\u660e https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset\n    aug_image, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, DIM, mask], Tout=[tf.float32, tf.float32])\n    # \u9700\u8981\u6062\u590dshape https:\/\/albumentations.ai\/docs\/examples\/tensorflow-example\/#restoring-dataset-shapes\n    aug_image.set_shape((DIM,DIM, 3))\n    aug_mask.set_shape((DIM,DIM, 1))\n    return aug_image, aug_mask\n    \"\"\"\n\ndef load_dataset(filenames, ordered=False, augment = True):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    \n    counter = tf.data.experimental.Counter()\n    dataset = tf.data.Dataset.zip((dataset, (counter, counter)))\n    \n    dataset = dataset.map(lambda ex, i: _parse_image_function(ex, i, augment = augment), num_parallel_calls=AUTO)\n    #dataset = dataset.map(partial(_parse_image_function, augment = augment), num_parallel_calls=AUTO)\n    # \u8fd9\u4e00\u884c\u8ba1\u6570\u4f1a\u6781\u5927\u7684\u5f71\u54cd\u6027\u80fd\n    #dataset_length = [i for i,_ in enumerate(dataset)][-1] + 1        \n    #print(\"dataset_length\", dataset_length)            \n    return dataset\n# \u4e4b\u524d\u8fd9\u91cc\u503c\u90fd\u4e00\u6837 \u73b0\u5728\u6539\u6210\u4e0d\u4e00\u6837\u7684\ndef get_training_dataset(index= P['SEED']):\n    print(\"trainning data load\")\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(128, seed = index)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=True):\n    print(\"validate data load\")\n    dataset = load_dataset(VALIDATION_FILENAMES, ordered=ordered, augment = False)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    #dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset","58239e94":"#dataset = load_dataset(\"gs:\/\/kds-d633b6ed58b88a8006c19265c98f17c827c34cb7d20242fa7111baa9\/train\/0486052bb-355.tfrec\")","4e629075":"\"\"\"\n#\u9047\u5230\u4e86\u4e00\u4e2a\u6d45\u62f7\u8d1d\u7684\u95ee\u9898\uff1aget_training_dataset > load_dataset \u5982\u679c\u91cd\u590d\u8fd0\u884c \u201c\u4f3c\u4e4e\u4f1a\u53d6\u9519 dataset \u7684\u5e76\u884c\u6267\u884c\u5355\u5143\u7684\u6307\u9488\u201c\uff0c\u53d6\u4e4b\u524d\u88ab\u91cd\u590d\u8d4b\u503c\u4e4b\u524d\u7684\u6307\u9488\uff0c\u800c\u90a3\u4e2a\u6307\u9488\u6307\u5411\u7684\u201d\u5e76\u884c\u8ba1\u7b97\u5355\u5143\u201c\u5df2\u7ecf\u88ab\u9500\u6bc1\u4e86\uff0c\u5c31\u4f1a\u5bfc\u81f4\u62a5\u9519\u201ccannot connect alladress\u201d\u9519\u8bef\n\nDIM = P['DIM']\n\ntransforms = albu.Compose([\n    albu.OneOf([\n        albu.RandomBrightness(limit=.2, p=1), \n        albu.RandomContrast(limit=.2, p=1), \n        albu.RandomGamma(p=1)\n    ], p=.5),\n    albu.OneOf([\n        albu.Blur(blur_limit=3, p=1),\n        albu.MedianBlur(blur_limit=3, p=1)\n    ], p=.25),\n    albu.OneOf([\n        albu.GaussNoise(0.002, p=.5),\n        albu.IAAAffine(p=.5),\n    ], p=.25),\n    albu.RandomRotate90(p=.5),\n    albu.HorizontalFlip(p=.5),\n    albu.VerticalFlip(p=.5),\n    albu.Cutout(num_holes=10, \n                max_h_size=int(.1 * DIM), max_w_size=int(.1 * DIM), \n                p=.25),\n    albu.ShiftScaleRotate(p=.25)\n])\n\ndef aug_fn(image, img_size, mask):\n    data = {\"image\":image, \"mask\": mask}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    #aug_img = tf.cast(aug_img\/255.0, tf.float32)\n    #aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    \n    aug_mask = aug_data[\"mask\"]\n    #aug_mask = tf.cast(aug_mask, tf.float32)\n    #aug_mask = tf.image.resize(aug_mask, size=[img_size, img_size])\n    return aug_img, aug_mask\n    #return image, mask\n\nfilenames = \"gs:\/\/kds-d633b6ed58b88a8006c19265c98f17c827c34cb7d20242fa7111baa9\/train\/0486052bb-355.tfrec\"\n\ntestArr = []\ndef _parse_image(example_proto):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string)\n    }\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    image = tf.reshape( tf.io.decode_raw(single_example['image'],out_type=np.dtype('uint8')), (DIM,DIM, 3))\n    mask =  tf.reshape(tf.io.decode_raw(single_example['mask'],out_type='bool'),(DIM,DIM,1))\n    image = tf.cast(image, tf.float32)\n    mask = tf.cast(mask, tf.float32)\n    testArr.append(image)\n    testArr.append(mask)\n    #aug_image, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, size, mask], Tout=[tf.float32, tf.float32])\n    aug_image, aug_mask = aug_fn(image, DIM, mask)\n    \n    aug_image.set_shape((DIM,DIM, 3))\n    aug_mask.set_shape((DIM,DIM, 1))\n    return aug_image, aug_mask \n    #return image, mask \n    \n\ndataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n\n#dataset = dataset.map(partial(_parse_image), num_parallel_calls=AUTO)\ndataset_alb = dataset.map(partial(_parse_image), num_parallel_calls=AUTO)\n\"\"\"\n#print(testArr)","75e717a6":"#import PIL\n#import PIL.Image\n#type(testArr[0])\n\n#arr = np.ndarray(testArr[0])\n#arr_ = np.squeeze(arr)\n\n#plt.imshow(testArr[0])\n#plt.show()\n#PIL.Image.open(str(testArr[0]))","ce22aa23":"#type(element)\n#get_training_dataset()\n#get_validation_dataset()","02ba35c8":"\"\"\"\nplt.imshow(element[0])\nplt.show()\n\"\"\"","ba995b3c":"#get_validation_dataset()","10282b67":"# https:\/\/tensorlayer.readthedocs.io\/en\/latest\/_modules\/tensorlayer\/cost.html#dice_coe\n\"\"\"\ndef dice_coe(output, target, axis = None, smooth=1e-10):\n    output = tf.dtypes.cast( tf.math.greater(output, 0.5), tf. float32 )\n    target = tf.dtypes.cast( tf.math.greater(target, 0.5), tf. float32 )\n    inse = tf.reduce_sum(output * target, axis=axis)\n    l = tf.reduce_sum(output, axis=axis)\n    r = tf.reduce_sum(target, axis=axis)\n\n    dice = (2. * inse + smooth) \/ (l + r + smooth)\n    dice = tf.reduce_mean(dice, name='dice_coe')\n    return dice\n\"\"\"\n\ndef dice_coe(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2 * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# https:\/\/www.kaggle.com\/kool777\/training-hubmap-eda-tf-keras-tpu\ndef tversky(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) \/ (true_pos + alpha * false_neg + beta * false_pos + smooth)\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)\n\ndef dice_loss(y_true, y_pred, smooth=1):\n    return (1 - dice_coe(y_true, y_pred, smooth))\n\ndef bce_dice_loss(y_true, y_pred):\n    return PARAMS['bce_weight'] * binary_crossentropy(y_true, y_pred) + \\\n        (1 - PARAMS['bce_weight']) * dice_loss(y_true, y_pred)\n\nget_custom_objects().update({\"focal_tversky\": focal_tversky_loss})","ae375612":"%%time\n\nM = {}\nmetrics = ['loss','dice_coe'\n           #,'accuracy'\n          ]\nfor fm in metrics:\n    M['val_'+fm] = []\n\nfold = KFold(n_splits=P['NFOLDS'], shuffle=True, random_state=P['SEED'])\nfor fold,(tr_idx, val_idx) in enumerate(fold.split(ALL_TRAINING_FILENAMES)):\n    print('#'*35); print('############ FOLD ',fold+1,' #############'); print('#'*35);\n    print(f'Image Size: {DIM}, Batch Size: {BATCH_SIZE}')\n\n    # CREATE TRAIN AND VALIDATION SUBSETS\n    TRAINING_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in tr_idx]\n    \n    #\u8fd9\u6bb5\u4f3c\u4e4e\u4e0d\u592a\u5bf9\n    \"\"\"    \n    if P['OVERLAPP']:\n        TRAINING_FILENAMES += [ALL_TRAINING_FILENAMES2[fi] for fi in tr_idx]\n    \"\"\"\n\n    VALIDATION_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in val_idx]\n    STEPS_PER_EPOCH = P['STEPS_COE'] * count_data_items(TRAINING_FILENAMES) \/\/ BATCH_SIZE\n\n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():   \n        model = sm.Unet(P['BACKBONE'], encoder_weights='imagenet')\n        #model = sm.Linknet(P['BACKBONE'], encoder_weights='imagenet')\n        #https:\/\/www.kaggle.com\/bigironsphere\/loss-function-library-keras-pytorch \u76ee\u524d\u6700\u597d\u7684lr \u4e3a 1e-3 \u5230  5e-4 \u9644\u8fd1\n        \"\"\"\n        model.compile(optimizer=tfa.optimizers.Lookahead(\n            tf.keras.optimizers.Adam(learning_rate=P['LR']),\n            sync_period=max(6, int(P['patience'] \/ 4))\n          ),\n        \n        \"\"\"\n\n        model.compile(optimizer = tf.keras.optimizers.Adam(lr = P['LR']),\n                      loss = tf.keras.losses.BinaryCrossentropy(),\n                      \n                      #loss='focal_tversky',\n                      \n                      #loss = bce_jaccard_loss,\n                      metrics=[dice_coe\n                              # ,'accuracy'\n                              ])\n    \"\"\"\n      optimizer=tfa.optimizers.Lookahead(\n            tf.keras.optimizers.Adam(learning_rate=P['LR']),\n            sync_period=max(6, int(P['patience'] \/ 4))\n      ),\n    \"\"\"\n\n    # CALLBACKS\n    checkpoint = tf.keras.callbacks.ModelCheckpoint('\/kaggle\/working\/model-fold-%i.h5'%fold,\n                                 verbose=P['VERBOSE'],monitor='val_dice_coe',\n                                                    #patience = 10,\n                                 mode='max',save_best_only=True)\n\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_dice_coe',mode = 'max', patience=P['patience'], restore_best_weights=True)\n    reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=int(P['patience'] \/ 2), min_lr=P['LR'] \/ 1e3)\n\n    print(f'Training Model Fold {fold+1}...')\n\n    log_dir = \"logs\/fit\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    history = model.fit(\n        get_training_dataset(index=fold),\n        epochs = P['EPOCHS'],\n        steps_per_epoch = STEPS_PER_EPOCH,\n        callbacks = [\n            #tensorboard_callback, \n            checkpoint, reduce,early_stop],\n        validation_data = get_validation_dataset(),\n        verbose=P['VERBOSE']\n    )   \n\n    #with strategy.scope():\n    #    model = tf.keras.models.load_model('\/kaggle\/working\/model-fold-%i.h5'%fold, custom_objects = {\"dice_coe\": dice_coe})\n\n    # SAVE METRICS\n    m = model.evaluate(get_validation_dataset(),return_dict=True)\n    for fm in metrics:\n        M['val_'+fm].append(m[fm])\n\n    # PLOT TRAINING\n    # https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords\n    if P['DISPLAY_PLOT']:        \n        plt.figure(figsize=(15,5))\n        n_e = np.arange(len(history.history['dice_coe']))\n        plt.plot(n_e,history.history['dice_coe'],'-o',label='Train dice_coe',color='#ff7f0e')\n        plt.plot(n_e,history.history['val_dice_coe'],'-o',label='Val dice_coe',color='#1f77b4')\n        x = np.argmax( history.history['val_dice_coe'] ); y = np.max( history.history['val_dice_coe'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max dice_coe\\n%.2f'%y,size=14)\n        plt.ylabel('dice_coe',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(n_e,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(n_e,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n        plt.ylabel('Loss',size=14)\n        plt.legend(loc=3)\n        plt.show()","f00bcd1f":"### WRITE METRICS\nM['datetime'] = str(datetime.now())\nfor fm in metrics:\n    M['oof_'+fm] = np.mean(M['val_'+fm])\n    print('OOF '+ fm + ' '+ str(M['oof_'+fm]))\nwith open('metrics.json', 'w') as outfile:\n    json.dump(M, outfile)","34f212e5":"# this need some special configurature for tpu \n#%tensorboard --logdir logs\/fit","7a60d3b8":"#!ls","bcfa233e":"# Refferences:\n1. https:\/\/www.kaggle.com\/wrrosa\/hubmap-tf-with-tpu-efficientunet-512x512-tfrecs (how to create training and inference tfrecords)\n2. https:\/\/www.kaggle.com\/wrrosa\/hubmap-tf-with-tpu-efficientunet-512x512-train (training pipeline)\n3. https:\/\/www.kaggle.com\/wrrosa\/hubmap-tf-with-tpu-efficientunet-512x512-subm (inference with submission)\n4. https:\/\/www.kaggle.com\/vgarshin\/kidney-unet-model-keras-inference?scriptVersionId=58536838 and https:\/\/github.com\/vgarshin\/kaggle_kidney\/blob\/master\/kidney_train.ipynb\n","b4553490":"# tpu\n\n- [Issue]tpu doesn't support numpy_function \n1. issue: https:\/\/github.com\/tensorflow\/tensorflow\/issues\/38762\n\n- tpu traning guide\n1. https:\/\/www.tensorflow.org\/guide\/data_performance\n2. https:\/\/www.tensorflow.org\/guide\/distributed_training\n\n\n# todo \n1. change tf.image to tf sequence layer https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation#resizing_and_rescaling\n2. \u8bed\u8a00\u7279\u6027, tf \u7279\u6027 \u548c np \u7684\u5dee\u5f02 \n3. tf \u7684\u7279\u6027 \u6d41\u7684\u6982\u5ff5 session \u7684\u6982\u5ff5 \u7ef4\u5ea6\u5bf9\u9f50\n4. tpu \u7684\u7279\u6027 tpu\u7684\u6548\u7387\n5. \u5c1d\u8bd5\u591a\u79cdloss https:\/\/github.com\/JunMa11\/SegLoss","3f8ce182":"# Init - parameters, packages, gcs_paths, tpu","2d34662e":"## GCS_PATHS","375f901a":"# Datasets pipeline","38b233ce":"\n\n# Versions\n* V1-V3 efficientnetb4 + unet\n* v4 efficientnetb0 + linknet + dce_jacobian loss\n* v5 efficientnetb0 + unet + bce_jacobian \n* v6-v8 add guassian data augmentation\n* v14 efficientnetb0 + unet 256 tiles\n* v15 efficientnetb0 + linknet\n* v9 fix shuffbug\n* v10 - v49 \n* try to find best learning rate for batchsize 1024 is about 1e-3 to 5e-4 ( version 43 )\n* best backbone is efficientnet b2 due to image resolution \n* gpu with batchsize 32 run over 9 hours limit in version 48\n* v50 read paper and try more loss function\n* v54 add extennal data https:\/\/www.kaggle.com\/baesiann\/glomeruli-hubmap-external-1024x1024\n* v60 tuning params for optimizer adam look forward\n* v64 ~tpu tensorboard for analysis~ only find tpu profiler for gcp","8f1720cb":"# Model","2afa2fe8":"# Model fit"}}