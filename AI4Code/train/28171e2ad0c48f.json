{"cell_type":{"c1589703":"code","da7f7038":"code","19ac0186":"code","a1c7d5bc":"code","242a7c06":"code","cbb9f47f":"code","27a1a791":"code","65ca9011":"code","d9a296b0":"code","48986292":"code","1bed7448":"code","45313b01":"code","73ccf8d6":"markdown"},"source":{"c1589703":"!pip install accelerate","da7f7038":"import os\n#garbage collection\u30e1\u30e2\u30ea\u306e\u958b\u653e\nimport gc\n\nimport sys\nimport math\nimport time\n\n#python\u3067\u30b7\u30fc\u30af\u30d0\u30fc\u3092\u8868\u793a\u3059\u308b\nimport tqdm\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\n\n#Stratified KFold -> \u5c64\u5316\u62bd\u51faK\u5206\u5272\u516c\u5dee\u691c\u8a3c\n#\u76ee\u7684\u5909\u6570\u306e\u504f\u308a\u304c\u4fdd\u6301\u3055\u308c\u308b\u3088\u3046\u306bK\u5206\u5272\u3092\u5b9f\u65bd\u3059\u308b\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\nfrom transformers import (AutoModel,AutoConfig,\n                          AutoTokenizer,get_cosine_schedule_with_warmup)\n\n#\u8272\u95a2\u4fc2?EDA\u7528?\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","19ac0186":"#\u3053\u3053\u304b\u3089\u518d\u958b\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n#print(train_data.head())\n#exerpt\u306e\u6539\u884c\u60c5\u5831\u3092\u524a\u9664\ntrain_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n#np.floor\u306f\u5c11\u6570\u70b9\u4ee5\u4e0b\u3092\u56db\u6368\u4e94\u5165\u3059\u308b\u95a2\u6570. train data\u306e\u30ec\u30b3\u30fc\u30c9\u6570\u306e\u5bfe\u6570+1\u3092\u6574\u6570\u5316\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\n\n#bins\u3067\u6307\u5b9a\u3057\u305f\u6570\u306e\u7fa4\u306btarget\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\n#labels=False\u3067\u306f\u51fa\u529b\u306e\u5024\u57df\u306f\u975e\u8868\u793a\u3068\u306a\u308a, 0\u59cb\u307e\u308a\u306e\u7fa4\u306eid\u306e\u307f\u304coutput\u3055\u308c\u308b\n#loc method\u306f[\u884c,\u5217]\u8981\u7d20\u306e\u9806\u756a\u3067\u6307\u5b9a\u3057\u3066dataframe\u306e\u8981\u7d20\u3092\u62bd\u51fa\u3059\u308b.\n#\u4ee5\u4e0b\u306e\u4f8b\u3060\u3068\u3001\u3059\u3079\u3066\u306e\u884c\u306ebins\u5217\u306b\u5bfe\u3057\u3066\u3001\u7fa4\u756a\u53f7\u3092(\u65b0\u898f\u5217)bins\u306b\u683c\u7d0d\u3057\u3066\u3044\u308b\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\n#bins\/target\u3092\u592b\u3005numpy\u30af\u30e9\u30b9\u306b\u5909\u63db\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n\n#print(\"bins\", type(bins))\n#print(\"target\", target)\n\n#\u4e8c\u4e57\u548c\u5e73\u5747\u8aa4\u5dee\u95a2\u6570\u3092\u5b9a\u7fa9\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","a1c7d5bc":"print(train_data.head())","242a7c06":"#HP\u8a2d\u5b9a, HP\u306e\u5404\u8a73\u7d30\u3082\u3042\u3068\u3067\u8ffd\u3044\u304b\u3051\u308b\nconfig = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':8,\n    'valid_step':10,\n    'max_len':256,\n    'epochs':3,\n    'nfolds':5,\n    'seed':42,\n    'model_path':'..\/input\/clrp-pytorch-roberta-pretrain-roberta-large\/clrp_roberta_large',\n}\n\n#nfolds\u5206\u306e\u683c\u7d0d\u7528dir\u3092\u4f5c\u6210\nfor i in range(config['nfolds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n\n#seed\u8a2d\u5b9a\u306e\u95a2\u6570\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n#seed\u309242\u306b\u8a2d\u5b9a\nseed_everything(seed=config['seed'])\n\n#FOLD\u5217\u3092\u8ffd\u52a0\u3057\u3066\u4eee\u306b-1\u3092\u4e00\u5f8b\u683c\u7d0d\ntrain_data['Fold'] = -1\n#\u5c64\u5316\u62bd\u51fakhold, \u30b7\u30e3\u30c3\u30d5\u30eb\u6709\u52b9, seed\u56fa\u5b9a, \n#kfold\u30af\u30e9\u30b9\u3092\u4f7f\u3063\u305fid\u62bd\u51fa\u306f\u4ee5\u4e0b\u304c\u5206\u304b\u308a\u3084\u3059\u3044.\n#https:\/\/blog.amedama.jp\/entry\/2018\/06\/21\/235951\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold.split\n#Stratified Kfold\u306b\u304a\u3051\u308bsplit\u306b\u304a\u3044\u3066y\u306f\u5c64\u5316\u62bd\u51fa\u306b\u5fc5\u9808\u306e\u30d1\u30e9\u30e1\u30bf, bins\u3092\u5c64\u5316\u62bd\u51fa\u3059\u308b\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    train_data.loc[valid_idx,'Fold'] = k","cbb9f47f":"train_data.head()","27a1a791":"plt.figure(dpi=100)\n#seaborn\u306f\u3053\u306e\u5f62\u3060\u3051\u3067\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u308b\u306e\u3059\u3054\u3044...\n#bins\u3067\u533a\u5207\u3063\u305f\u9069\u6b63\u5e74\u9f62\u7fa4\u306f\u3001\u307b\u307c\u30b7\u30b0\u30de\u30d7\u30ed\u30c3\u30c8\u306e\u69d8\u306a\u5f62\u72b6\u3092\u3057\u3066\u3044\u308b.\nsns.countplot(train_data.bins);","65ca9011":"#Dataset Class\u5b9a\u7fa9\u3001\u521d\u671f\u5024\u3068\u3057\u3066excerpt\u3068targets\u3001max_len, tokenizer\u3092\u6301\u3064\u3002\n#Class\u5b9a\u7fa9\u306b\u304a\u3051\u308b\u30c0\u30d6\u30eb\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\u306b\u7279\u5225\u306a\u610f\u5473\u304c\u3042\u3063\u305f\u304b.\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n    #get item\u3067\u306f\u3001tokenizer\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u308b\u756a\u53f7\u3068target\u5024\u304c\u51fa\u3066\u304f\u308b\uff1f\n    #getitem\u306b\u3064\u3044\u3066https:\/\/qiita.com\/gyu-don\/items\/bde192b129a7b1b8c532\n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    #excerpt\u306e\u9577\u3055\n    def __len__(self):\n        return len(self.excerpt)","d9a296b0":"#AttentionHead\u30af\u30e9\u30b9\u306e\u5b9a\u7fa9\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n    #forward\u3067\u306fattention\u3092\u8a08\u7b97\u3057\u3066\u3001context_vector\u3092\u8fd4\u3059\n    def forward(self, features):\n        #W(argument)\u3067argument\u306b\u5bfe\u3057\u3066nn.linear\u3092\u5b9f\u65bd, hidden_dim\u306b\u5909\u63db\u3057\u3066\u3001tanh\u5909\u63db\n        att = torch.tanh(self.W(features))\n        #\u5165\u529b=>\u96a0\u308c\u5c64\u3078\u5168\u7d50\u5408=>tanh\u5909\u63db\u3092score\u306b\u7dda\u5f62\u5909\u63db\n        score = self.V(att)\n        #attention\u91cd\u307f\u3092softmax\u95a2\u6570\u3067\u4f5c\u6210\n        attention_weights = torch.softmax(score, dim=1)\n        #context_vector\u306fvalue\u306battention\u3092\u3064\u3051\u305f\u3082\u306e\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","48986292":"class Model(nn.Module):\n    #Model\u672c\u4f53\u306f\u4e00\u3064\u524d\u306enotebook\u3067\u4f5c\u6210\u3057\u305fmodel\u3092path\u3067\u53c2\u7167\u3059\u308b\u5f62\u5f0f\n    def __init__(self,path):\n        super(Model,self).__init__()\n        #roberta\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\n        self.roberta = AutoModel.from_pretrained(path)\n        #config\u3092\u8aad\u307f\u8fbc\u307f\uff1f\n        self.config = AutoConfig.from_pretrained(path)\n        #attentionHead\u3092\u8aad\u307f\u8fbc\u307f\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        #Dropout\u30920.1\u306b\u8a2d\u5b9a\n        self.dropout = nn.Dropout(0.1)\n        #1\u6b21\u5143\u3078\u306e\u5168\u7d50\u5408\u5c64?\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self,**xb):\n        #xb\u3092\u30e2\u30c7\u30eb\u306b\u5165\u308c\u308b**\u304c\u5f15\u6570\u306b\u3042\u308b\u306e\u306f\u4efb\u610f\u306e\u6570\u306e\u5f15\u6570\u3092\u6307\u5b9a\u3067\u304d\u308b\u610f\u5473\n        #input xb\u3092roberta\u30e2\u30c7\u30eb\u306b\u89e3\u91c8\u3055\u305b\u3066\u3001head\u3092\u53d6\u3063\u3066\u3001dropout\u3055\u305b\u3066\u3001\u5168\u7d50\u5408\u3057\u3066x\u3092\u8fd4\u3059\n        #forward\u306fattention\u81ea\u4f53\u3092\u8fd4\u3059=\u4e88\u6e2c\u5024\uff1f\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","1bed7448":"#fold\u6570\u3092\u6307\u5b9a\u3057\u3066\u3001run\u3059\u308b\uff1f\ndef run(fold,verbose=True):\n    #loss\u95a2\u6570\u306e\u8a2d\u5b9a\n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    #\u305d\u306e\u307e\u307e\uff1f\n    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs1,targets1) in enumerate(train_loader):\n            #model\u3092train\u3059\u308b\u5ba3\u8a00\n            model.train()\n            #gradien\uff54\u306e\u521d\u671f\u5316\n            optimizer.zero_grad()\n            #inputs1\u306ekey val\u3092\u5909\u5f62\u3057\u3066inputs1\u306b\u518d\u4ee3\u5165\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            #output\u306fmodel\u3067\u51e6\u7406\u3057\u305finputs1\n            outputs1 = model(**inputs1)\n            #loss1\u306f\u8aa4\u5dee\u95a2\u6570\n            loss1 = loss_fn(outputs1,targets1)\n            #\u640d\u5931\u95a2\u6570\u306b\u95a2\u3057\u3066\u3001\u4f1d\u642c\u3078\u306e\u5f71\u97ff\u3092\u5fae\u5206\u8a08\u7b97\n            loss1.backward()\n            #\u5b66\u7fd2\u7387\u3068\u6700\u9069\u5316\u624b\u6cd5\u306b\u57fa\u3065\u3044\u3066\u5b66\u7fd2\u3092\u5b9f\u65bd\u3059\u308b\n            optimizer.step()\n            \n            #loss1\u3092\u4ee3\u5165 \u66f8\u304f train_loader\u8981\u7d20\u306b\u5bfe\u3057\u3066for\u30eb\u30fc\u30d7\u3092\u56de\u3057\u3066\u3044\u308b\u306e\u3067.\n            train_loss += loss1.item()\n            \n            #\u5b66\u7fd2\u7387\u3092\u52d5\u7684\u306b\u5909\u5316\u3055\u305b\u308bscheduler.\n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            #\u7279\u5b9a\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u8a55\u4fa1, valid_step\u307e\u305f\u306f\u30eb\u30fc\u30d7\u306e\u30b1\u30c4\n            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss \/= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        #\u8a55\u4fa1\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u6bd4\u8f03\u3057\u3066, input\u3055\u308c\u305fbest_loss\u306b\u5bfe\u3057\u3066,\n                        #best_loss\u3088\u308a\u540c\u7b49\u304b\u5c0f\u3055\u3044valid_loss\u304c\u5f97\u3089\u308c\u305f\u3089\u305d\u306e\u30e2\u30c7\u30eb\u3068tokenizer\u3092\u30e2\u30c7\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\n                        if verbose:\n                            print(f\"epoch:{epoch} | Train Loss:{train_loss\/(i+1)} | Validation loss:{valid_loss}\")\n                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        torch.save(model.state_dict(),f'.\/model{fold}\/model{fold}.bin')\n                        tokenizer.save_pretrained(f'.\/model{fold}')\n                        \n        return best_loss\n    \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    #fold\u3067\u6307\u5b9a\u3055\u308c\u305ffold\u30ca\u30f3\u30d0\u30fc\u304cvalid\u7528, \u305d\u306e\u307b\u304b\u304ctrain\u7528\u306b\u306a\u3063\u3066\u3044\u308b\n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n    \n    #tokenizer\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080\n    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n    #model\u3092\u8aad\u307f\u8fbc\u3080\n    model = Model(config['model_path'])\n\n    #\u8aad\u307f\u8fbc\u3093\u3060tokenizer\u3067x_train\u3092\u5909\u63db\u3057\u3066dataset\u3068\u3059\u308b\n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    #dataset\u3092dataLoader\u306b\u8aad\u307f\u8fbc\u307e\u305b\u3066train_dl(what dl stand for?)\u3059\u308b.\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n    \n    #valid\u3082\u540c\u69d8\u306b\u51e6\u7406\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    #optimizer\u306e\u8a2d\u5b9a\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    #\u5b66\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u8a2d\u5b9a\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    #accelerator\u3067\u3044\u3044\u611f\u3058\u306b\u51e6\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    \n    #best_loss\u3067\u304b\u3044\u3068\u3053\u308d\u304b\u3089\u521d\u3081\u3066\n    #run\u306e\u4e2d\u3067\u5b9a\u7fa9\u3057\u305ftrain_and_evaluate_loop\u95a2\u6570\u3067\u5b66\u7fd2\u3092epoch\u6570\u3060\u3051\u7e70\u308a\u8fd4\u3059\n    \n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n                                            optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)","45313b01":"#fold\u6570\u3060\u3051\u5b9f\u65bd\u3059\u308b\nfor f in range(config['nfolds']):\n    run(f)","73ccf8d6":"# This note book is Modified for Haruki's learning. \nThis notebook uses the model created in pretrain any model notebook.\n\n1. Pretrain Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-pretrain\n2. Finetune Roberta Model: this notebook, <br\/>\n   Finetune Roberta Model TPU: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm"}}