{"cell_type":{"20b04e45":"code","bdeb5f86":"code","24dbdb8b":"code","fb68dfe8":"code","fc19cb73":"code","0acf893b":"code","984ae572":"code","9ed5a52b":"code","3cd77695":"code","245c2271":"code","f8f5b1b1":"code","2cf7878c":"code","72cdc03a":"code","e487f336":"code","a9eeaf0c":"code","8d9af313":"code","343b06f4":"code","08755d73":"code","d3a00e4a":"code","cd4d8d54":"code","14fb8957":"code","89bb7d4a":"code","b3d041d3":"code","1edc84f1":"code","2561be50":"code","fccd3c38":"code","714ada86":"code","c41c4147":"code","2099e4e3":"code","bfa7d79e":"code","02545bde":"code","520d8d1d":"code","a8704302":"code","992fbe86":"code","ecd1ff7f":"code","547da416":"code","8e68c137":"code","261b029b":"code","e6665ead":"markdown","6ca3978f":"markdown","24a387d8":"markdown","6ffa0aa4":"markdown","8bb84499":"markdown","2b44c65b":"markdown","a8bf3745":"markdown","82111d5c":"markdown","c4220193":"markdown","858bfa81":"markdown","13560e1f":"markdown","94db2820":"markdown","805620ae":"markdown","6498cf26":"markdown","3aeeadff":"markdown","6be1a2ba":"markdown","ed7477ac":"markdown","567bc23c":"markdown","72d91a6c":"markdown","66860050":"markdown","0879c9d1":"markdown","02c9c33d":"markdown","c8b130f2":"markdown","714fccec":"markdown","f8de4434":"markdown","7a8c687e":"markdown","913825da":"markdown","11440d4e":"markdown","214b9aae":"markdown","7d0ffea6":"markdown","1fd85bad":"markdown","01c41149":"markdown","76999c17":"markdown","5e47be82":"markdown","c27d2cec":"markdown","73263225":"markdown","2dec6296":"markdown","42e2af23":"markdown","250b757d":"markdown","580d59d8":"markdown","83a1a939":"markdown","9dc0d125":"markdown","b5be1be6":"markdown","ee6278f9":"markdown","732f1752":"markdown","ea3b5074":"markdown","31f2e270":"markdown","6e53d93c":"markdown","fcb61441":"markdown","a2193e3b":"markdown","85e89cbb":"markdown","e7442202":"markdown","6d4e1d0c":"markdown","70c23857":"markdown","5780d907":"markdown","79d99c7e":"markdown","85ae9cf5":"markdown","2e91f1b5":"markdown","51228bed":"markdown","3f1e3844":"markdown","553317db":"markdown","dac3b2eb":"markdown","2fb76ecf":"markdown","d414c817":"markdown","7c29df3d":"markdown","3ee6114e":"markdown","9eb23439":"markdown","d0b5189d":"markdown","ffac1a31":"markdown","279592af":"markdown","6b70a39b":"markdown","131cdfe7":"markdown","65834f8b":"markdown","fb01895e":"markdown","1e4e8458":"markdown","746c77ff":"markdown","f34d13c7":"markdown","13acc6fc":"markdown","2c048436":"markdown","760348f5":"markdown","ede87a40":"markdown","ea8768c8":"markdown","5658efd9":"markdown","cb758ad4":"markdown","f151952e":"markdown","a9d6dbde":"markdown","ed7b5d98":"markdown","88e2b089":"markdown","fec4dab7":"markdown"},"source":{"20b04e45":"import pandas as pd\nimport numpy as np\nimport itertools\nimport datetime \n#manipulating data and basic python libraries\n\nimport sklearn\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n#Preprocessing data and metrics for error evaluation\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Activation, LSTM, ReLU\nfrom keras.optimizers import Adam\nfrom keras.callbacks import callbacks, EarlyStopping, ReduceLROnPlateau\n#Tensorflow as our deep learning back end\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n#Statistical modeling using Arima\n\nfrom fbprophet import Prophet\n#Facebook prophet modeling\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(palette = 'Set2',style='dark',font=['simhei', 'Arial'])\n#Visualization\n\nimport warnings\nwarnings.simplefilter('ignore')\n#Ignoring warnings","bdeb5f86":"data=pd.read_csv('..\/input\/corona-virus-report\/covid_19_clean_complete.csv')\n\ndata['Date']=pd.to_datetime(data['Date'])\ndata= data.rename(columns={'Country\/Region':'Country','Date':'ds','Province\/State':'Province'}) \ndata.replace('Korea, South','South Korea',inplace=True)\ndata.head()","24dbdb8b":"df=data.groupby(['ds','Country']).agg('sum').reset_index()\ndf['date']=df['ds']\nfor i in range (0,len(df)):\n    df['date'][i]=df['date'][i].strftime(\"%d %b\")\ndf.tail(5)","fb68dfe8":"dfCountry=df.groupby(['Country']).agg('max')\ndfCountry.sort_values('Confirmed',ascending=False,inplace=True)\ndfCountry=dfCountry[0:10]\ndfCountry.reset_index(inplace=True)\n\nf=plt.figure(figsize=(13,7))\nax=sns.barplot(x=dfCountry['Country'],y=dfCountry['Confirmed'], color='Orange',alpha=0.8,label='Confirmed cases')\nax=sns.barplot(x=dfCountry['Country'],y=dfCountry['Recovered']+dfCountry['Deaths'],alpha=0.8, color='purple', label='Terminated cases')\nplt.legend()\nplt.ylabel('')\nplt.xticks(rotation=25)\na=ax.set_title('Cases by country')","fc19cb73":"dfus=df.groupby(['Country']).agg('max')\ndfus.sort_values('Confirmed',ascending=False,inplace=True)\ndfus.reset_index(inplace=True)\nrow_df = pd.DataFrame(dfus.loc[0]).T.drop(columns=['Lat','Long','ds','date'])\ndfus=dfus[1:].drop(columns=['Lat','Long','ds','date']).sum(axis=0)\ndfus=pd.DataFrame([dfus])\ndfus = pd.concat([row_df, dfus])\ndfus.reset_index(inplace=True,drop=True)\ndfus.at[1, 'Country'] = 'Rest of the world'\nf=plt.figure(figsize=(5,7))\nax=sns.barplot(x=dfus['Country'],y=dfus['Confirmed'], color='Orange',alpha=0.8,label='Confirmed cases')\nax=sns.barplot(x=dfus['Country'],y=dfus['Recovered']+dfus['Deaths'],alpha=0.8, color='purple', label='Terminated cases')\nplt.legend()\nplt.ylabel('')\na=ax.set_title('Cases by country')","0acf893b":"dfCountry=dfCountry[1:]\nf=plt.figure(figsize=(13,7))\nax=sns.barplot(x=dfCountry['Country'],y=dfCountry['Confirmed'], color='Orange',alpha=0.8,label='Confirmed cases')\nax=sns.barplot(x=dfCountry['Country'],y=dfCountry['Recovered']+dfCountry['Deaths'],alpha=0.8, color='purple', label='Terminated cases')\nplt.legend()\nplt.ylabel('')\nplt.xticks(rotation=25)\na=ax.set_title('Cases by country')","984ae572":"img=plt.imread('..\/input\/wmappicture\/WMap.png')\n\ndfmap=df.loc[(df['ds']==df['ds'].max())]\ndfmap.drop(columns=['Lat','Long'],inplace=True)\ndfmap=dfa= pd.merge(dfmap, data, on=['Country','ds'], how='inner')\n\nbins=[0,100,1000,10000,100000,1000000,100000000]\nlabels=['1-100','100-1000','1000-10000','10k-100k','100k-1m','1m+']\ndfmap['Category']= pd.cut(dfmap[\"Confirmed_y\"], bins , labels=labels)\n\nf,ax=plt.subplots(figsize=(25,18))\n\nax.imshow(img, zorder=0,extent=( -174,188,-60, 120))\nax=plt.gca()\n\ndfmap=dfmap.loc[dfmap['Confirmed_y']>1]\ndfmap['Confirmed_y']=100*np.log(dfmap['Confirmed_y'])\ndfmap.sort_values(by='Confirmed_y', inplace=True, ascending=True)\n\ng=sns.scatterplot(y=dfmap['Lat'],x=dfmap['Long'], hue=dfmap['Category'],legend='full',s=dfmap['Confirmed_y']*2)\na=g.set(xlabel='', ylabel='', yticklabels='',xticklabels='', title='Distribution of confirmed cases around the world')\ndel(dfmap)","9ed5a52b":"sns.set(palette = 'Dark2',style='darkgrid')\n\ndef pltCountry(case,*argv):\n    f, ax=plt.subplots(figsize=(16,5))\n    labels=argv\n    for a in argv: \n        country=df.loc[(df['Country']==a)]\n        country.reset_index(inplace=True,drop=True)      \n        plt.plot(country['date'],country[case],linewidth=3)\n        plt.xticks(rotation=40)\n        plt.legend(labels)\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(7))\n        ax.set(title='Evolution of the number of %s cases'%case,xlabel='Date',ylabel='Number of %s cases'%case )\n        \n        \npltCountry('Confirmed','China')","3cd77695":"pltCountry('Confirmed', 'Germany','Spain','France','US')\npltCountry('Deaths','Germany','Spain','France','US')\npltCountry('Recovered', 'Germany','Spain','France','US')","245c2271":"pltCountry('Confirmed','South Korea','Italy','Iran')\npltCountry('Deaths','South Korea','Italy','Iran')\npltCountry('Recovered','South Korea','Italy','Iran')","f8f5b1b1":"df['Actual']=df['Confirmed']-(df['Deaths']+df['Recovered'])\n\ndef pltCountryDet(*argv):\n    f,ax=plt.subplots(figsize=(16,5))\n    labels=argv\n    cases=['Confirmed','Deaths','Recovered']\n    for case in cases:\n        for a in argv: \n            country=df.loc[(df['Country']==a)]\n            plt.plot(country['date'],country[case],linewidth=2)\n            ax.xaxis.set_major_locator(ticker.MultipleLocator(7))\n            plt.xticks(rotation=40)\n            plt.legend(cases)\n            ax.set(title='Evolution in %s '%a,xlabel='Date',ylabel='Number of cases' )\n            \n    \npltCountryDet('Germany') ","2cf7878c":"def actualCases(*argv):\n    f,ax=plt.subplots(figsize=(16,5))\n    labels=argv  \n    \n    for a in argv:\n        country=df.loc[(df['Country']==a)]\n        plt.plot(country['date'],country['Actual'],linewidth=2)\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(7))\n        plt.xticks(rotation=40)\n        plt.legend(labels)\n        ax.set(title=' Evolution of actual cases',xlabel='Date',ylabel='Number of cases' )\n\n\nactualCases('France','Italy','Germany')","72cdc03a":"case='Confirmed'\ndef timeCompare(time,*argv):\n    Coun1=argv[0]\n    Coun2=argv[1]\n    f,ax=plt.subplots(figsize=(16,5))\n    labels=argv  \n    country=df.loc[(df['Country']==Coun1)]\n    plt.plot(country['ds'],country[case],linewidth=2)\n    plt.xticks([])\n    plt.legend(labels)\n    ax.set(title=' Evolution of actual cases',ylabel='Number of cases' )\n\n    country2=df.loc[df['Country']==Coun2]\n    country2['ds']=country2['ds']-datetime.timedelta(days=time)\n    plt.plot(country2['ds'],country2[case],linewidth=2)\n    plt.xticks([])\n    plt.legend(labels)\n    ax.set(title=' Evolution of cases with %d days difference '%time ,ylabel='Number of %s cases'%case )\n    \n\ntimeCompare(8,'Italy','France')\ntimeCompare(5,'Italy','Spain')\ntimeCompare(7,'Italy','Germany')\ntimeCompare(7,'Italy','US')","e487f336":"case='Actual'\ntimeCompare(40,'China','Italy')\n","a9eeaf0c":"country=df.loc[(df['Country']=='Italy')]\nf=plt.figure(figsize=(20,7))\ncountry['Confirmed']= country['Confirmed']-country['Confirmed'].shift(1)\ncountry['Deaths']= country['Deaths']-country['Deaths'].shift(1)\ncountry['Recovered']= country['Recovered']-country['Recovered'].shift(1)\ncountry['ds']=pd.to_datetime(country['ds'],format='%Y%m%d')\ncountry=country[32:]\ncountry.reset_index(inplace=True,drop=True)\ncountry['date']=country['ds']\nfor i in range (0,len(country)):\n    country['date'][i]=country['date'][i].strftime(\"%d %b\")\ncountry.drop(index=18,inplace=True) #The march 12th data is not provided in the dataset\nax=sns.barplot(x=country['date'],y=country['Confirmed'], color='Orange',alpha=0.8,label='Confirmed cases')\nax=sns.barplot(x=country['date'],y=country['Recovered']+country['Deaths'],alpha=0.8, color='purple', label='Terminated cases')\nplt.legend()\nplt.ylabel('')\nplt.xticks(rotation=-90)\nplt.xlabel('')\na=ax.set_title('Evolution of daily cases in Italy per day starting on 23rd Feb (reached 100 confirmed cases)')","8d9af313":"country=df.loc[(df['Country']=='US')]\nf=plt.figure(figsize=(20,7))\ncountry['Confirmed']= country['Confirmed']-country['Confirmed'].shift(1)\ncountry['Deaths']= country['Deaths']-country['Deaths'].shift(1)\ncountry['Recovered']= country['Recovered']-country['Recovered'].shift(1)\ncountry['ds']=pd.to_datetime(country['ds'],format='%Y%m%d')\ncountry=country[40:]\ncountry.reset_index(inplace=True,drop=True)\ncountry['date']=country['ds']\nfor i in range (0,len(country)):\n    country['date'][i]=country['date'][i].strftime(\"%d %b\")\nax=sns.barplot(x=country['date'],y=country['Confirmed'], color='Orange',alpha=0.8,label='Confirmed cases')\nax=sns.barplot(x=country['date'],y=country['Recovered']+country['Deaths'],alpha=0.8, color='purple', label='Terminated cases')\nplt.legend()\nplt.ylabel('')\nplt.xticks(rotation=-90)\nplt.xlabel('')\na=ax.set_title('Evolution of daily cases in the US per day starting on March 2nd (reached 100 confirmed cases)')","343b06f4":"#sns.set(palette = 'Set1',style='darkgrid')\n#Function for making a time serie on a designated country and plotting the rolled mean and standard \ndef roll(country,case='Confirmed'):\n    ts=df.loc[(df['Country']==country)]  \n    ts=ts[['ds',case]]\n    ts=ts.set_index('ds')\n    ts.astype('int64')\n    a=len(ts.loc[(ts['Confirmed']>=10)])\n    ts=ts[-a:]\n    return (ts.rolling(window=4,center=False).mean().dropna())\n\n\ndef rollPlot(country, case='Confirmed'):\n    ts=df.loc[(df['Country']==country)]  \n    ts=ts[['ds',case]]\n    ts=ts.set_index('ds')\n    ts.astype('int64')\n    a=len(ts.loc[(ts['Confirmed']>=10)])\n    ts=ts[-a:]\n    plt.figure(figsize=(16,6))\n    plt.plot(ts.rolling(window=7,center=False).mean().dropna(),label='Rolling Mean')\n    plt.plot(ts[case])\n    plt.plot(ts.rolling(window=7,center=False).std(),label='Rolling std')\n    plt.legend()\n    plt.title('Cases distribution in %s with rolling mean and standard' %country)\n    plt.xticks([])\n\ntsC=roll('China')\nrollPlot('China')","08755d73":"#Decomposing the ts to find its properties\nfig=sm.tsa.seasonal_decompose(tsC.values,freq=7).plot()\n","d3a00e4a":"#Function to check the stationarity of the time serie using Dickey fuller test\ndef stationarity(ts):\n    print('Results of Dickey-Fuller Test:')\n    test = adfuller(ts, autolag='AIC')\n    results = pd.Series(test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for i,val in test[4].items():\n        results['Critical Value (%s)'%i] = val\n    print (results)\n\ntsC=tsC['Confirmed'].values\nstationarity(tsC)","cd4d8d54":"\n#Differenciating every elemenet from the previous element to make ts stationary\n#def difference(ts, times=1):\n#    diff = list()\n#    for i in range(times, len(ts)):\n#        val = ts[i] - ts[i - times]\n#        diff.append(val)\n#    return pd.Series(diff)\n\n\n#print('\\n\\nDickey fuller test after de-trend \\n')\n#tsCD=difference(tsC,1)\n#tsCD=tsCD.values \n#plt.plot(tsCD)\n#stationarity(tsCD)","14fb8957":"#auto-Correlation and partial auto-correlation plots\ndef corr(ts):\n    plot_acf(ts,lags=12,title=\"ACF\")\n    plot_pacf(ts,lags=12,title=\"PACF\")\n    \ncorr(tsC)","89bb7d4a":"#Mean absolute percentage error\ndef mape(y, y_pred): \n    y, y_pred = np.array(y), np.array(y_pred)\n    return np.mean(np.abs((y - y_pred) \/ y)) * 100\n\ndef split(ts):\n    #splitting 80%\/20% because of little amount of data\n    size = int(len(ts) * 0.80)\n    train= ts[:size]\n    test = ts[size:]\n    return(train,test)\n\n\n#Arima modeling for ts\ndef arima(ts,test):\n    p=d=q=range(0,6)\n    a=99999\n    pdq=list(itertools.product(p,d,q))\n    \n    #Determining the best parameters\n    for var in pdq:\n        try:\n            model = ARIMA(ts, order=var)\n            result = model.fit()\n\n            if (result.aic<=a) :\n                a=result.aic\n                param=var\n        except:\n            continue\n            \n    #Modeling\n    model = ARIMA(ts, order=param)\n    result = model.fit()\n    result.plot_predict(start=int(len(ts) * 0.7), end=int(len(ts) * 1.2))\n    pred=result.forecast(steps=len(test))[0]\n    #Plotting results\n    f,ax=plt.subplots()\n    plt.plot(pred,c='green', label= 'predictions')\n    plt.plot(test, c='red',label='real values')\n    plt.legend()\n    plt.title('True vs predicted values')\n    #Printing the error metrics\n    print(result.summary())        \n    print('\\nMean absolute error: %f'%mean_absolute_error(test,pred))\n    print('\\nMean absolute percentage error: %f'%mape(test,pred))\n    return (pred)\n\n\n\ntrain,test=split(tsC)\npred=arima(train,test)\n","b3d041d3":"#Taking the US example\ntsU=roll('US')\nrollPlot('US')\n\na=len(tsU.loc[(tsU['Confirmed']>=10)])\ntsU=tsU[-a:]            #we omit the data in earliest stages for irrelevancy \n\n#renaming and preparing data for prophet\ntsU.rename(columns={'Confirmed':'y'},inplace=True) #prophet requires the columns to be named ds and y\ntrain,test= split(tsU) \n\ntrain.reset_index(inplace=True)\ntest.reset_index(inplace=True)","1edc84f1":"#We fit the data and extract the predictions\nmodel=Prophet(daily_seasonality=True)\nmodel.fit(train)\n\npred = model.predict(test)\na=model.plot_components(pred)","2561be50":"f,ax =plt.subplots(figsize=(16,5))\nplt.plot(test['ds'],test['y'],label='True values',c='red',marker='.')\nplt.title('True and predicted values')\na=model.plot(pred, ax=ax)\nax.legend()\n\nprint('Mean absolute error: %d'%mean_absolute_error(test['y'],pred['yhat']))\nprint('Mean absolute  percentage error: %d'%mape(test['y'],pred['yhat']))","fccd3c38":"#In this example, prophet didn't do a good job, we notice how the data is inconsistently growing in this #example, we migh think prophet doesn't perform well with this kind of linear distrubtion. A regression model #might be more accurate for this prediction.","714ada86":"def proph(country,case='Confirmed'):\n    tsU=roll(country,case)\n    tsU.rename(columns={case:'y'},inplace=True) \n    tsU=tsU[30:] \n    train,test= split(tsU)\n\n    train.reset_index(inplace=True)\n    test.reset_index(inplace=True)\n\n    model=Prophet(daily_seasonality=True)\n    model.fit(train)\n\n    pred = model.predict(test)\n    f,ax =plt.subplots(figsize=(16,5))\n    plt.plot(test['ds'],test['y'],label='True values',c='red',marker='.')\n    plt.title('True and predicted values')\n    a=model.plot(pred, ax=ax)\n    ax.legend()\n\n    print('Mean absolute error: %f'%mean_absolute_error(test['y'],pred['yhat']))\n    print('Mean absolute  percentage error: %f'%mape(test['y'],pred['yhat']))\n    return(pred)","c41c4147":"tsI=roll('Italy')\nrollPlot('Italy')\n#Scale and split the data\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(tsI['Confirmed'].values.reshape(-1,1).astype('float32'))\n\ntrain,test=split(scaled)","2099e4e3":"#Transform our ts to a usable df with lookback \n#This means we make our ts a supervised learning modelizable df\ndef lookBack(ts, look_back=2):\n    X=list()\n    Y=list()\n    for i in range(len(ts) - look_back):\n        a = ts[i:(i + look_back), 0]\n        X.append(a)\n        Y.append(ts[i + look_back, 0])\n    return np.array(X), np.array(Y)\n\ntrainX, trainY = lookBack(train)\ntestX, testY = lookBack(test)\n\n","bfa7d79e":"#Reshape our data\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n#Modeling and fitting\nmodel = Sequential()\n\nmodel.add(LSTM(1028, dropout=3, input_shape=(1,2), activation='relu'))\nmodel.add(Dense(1))\n\nearly_stop=EarlyStopping(monitor='val_loss', patience=10, verbose=0)\nlr_reduce=ReduceLROnPlateau(monitor='val-loss',patience=5)\nmodel.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\nhistory=model.fit(trainX,trainY, epochs = 100, batch_size = 12,validation_data=(testX, testY), \n                  shuffle=False, callbacks=[early_stop,lr_reduce])\nmodel.summary()","02545bde":"#plotting the error reduction\nplt.subplots(figsize=(13,5))\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Test')\nplt.title('Error reduction')\na=plt.legend()","520d8d1d":"#plotting our resutls and the error of our scaled data\npred=model.predict(testX)\nf,ax=plt.subplots(figsize=(13,5))\nplt.plot(pred, label='Predict')\nplt.plot(testY, label='True')\nplt.legend()\nplt.title('Predicted vs true scaled values')\n\nprint('Mean absolute error: %f'%mean_absolute_error(testY,pred))\nprint('Mean absolute  percentage error: %f'%mape(testY,pred))","a8704302":"#reversing the scaling\npredictions= scaler.inverse_transform(pred.reshape(-1, 1))\ntest_val = scaler.inverse_transform(testY.reshape(-1, 1))\n#getting the dates back\ndates = tsI.tail(len(testX)).index\n#reshaping\npredictions = predictions.reshape(len(predictions))\ntest_val = test_val.reshape(len(test_val))\n\n#plotting the data\nf, ax=plt.subplots(figsize=(16,5))\nplt.plot(tsI)\nplt.plot(dates, test_val, label= 'True', c='red',marker='.')\nplt.plot(dates, predictions, label= 'Predicted', marker='.' ,c='purple')\nplt.xticks([])\nplt.title('True vs predicted values')\na=plt.legend()\nprint('Mean absolute error: %f'%mean_absolute_error(test_val,predictions))\nprint('Mean absolute  percentage error: %f'%mape(test_val,predictions))","992fbe86":"def lstm(country,case='Confirmed'):\n    ts=roll(country,case)\n    \n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled = scaler.fit_transform(ts[case].values.reshape(-1,1).astype('float32'))\n\n    train,test=split(scaled)\n\n    trainX, trainY = lookBack(train)\n    testX, testY = lookBack(test)\n\n    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n    \n    model = Sequential()\n\n    model.add(LSTM(1028, dropout=3, input_shape=(1,2), activation='relu'))\n    model.add(Dense(1))\n\n    early_stop=EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n    lr_reduce=ReduceLROnPlateau(monitor='val-loss',patience=5)\n    model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n    history=model.fit(trainX,trainY, epochs = 100, batch_size = 12,validation_data=(testX, testY), \n                      shuffle=False, callbacks=[early_stop,lr_reduce])\n    \n    pred=model.predict(testX)\n    \n    predictions= scaler.inverse_transform(pred.reshape(-1, 1))\n    test_val = scaler.inverse_transform(testY.reshape(-1, 1))\n\n    dates = ts.tail(len(testX)).index\n\n    predictions = predictions.reshape(len(predictions))\n    test_val = test_val.reshape(len(test_val))\n\n    f, ax=plt.subplots(figsize=(16,5))\n    plt.plot(ts)\n    plt.plot(dates, test_val, label= 'True', c='red',marker='.')\n    plt.plot(dates, predictions, label= 'Predicted', marker='.' ,c='purple')\n    plt.xticks([])\n    plt.title('True vs predicted values')\n    a=plt.legend()\n    print('Mean absolute error: %f'%mean_absolute_error(test_val,predictions))\n    print('Mean absolute  percentage error: %f'%mape(test_val,predictions))\n    return predictions\n","ecd1ff7f":"rollPlot('Germany')","547da416":"ts=roll('Germany')\nts=ts['Confirmed'].values\ntrain,test=split(ts)\na=arima(train,test)","8e68c137":"a=proph('Germany')","261b029b":"a=lstm('Germany')","e6665ead":"#### Plotting the results of ou model and comparing it to the actual data","6ca3978f":"#### Creating a new dataset with look back tranformation","24a387d8":"#### plotting the error loss evolution to see the behavior of our model","6ffa0aa4":"Import our dataset, set the time column to datetime type, rename columns for easier manipulation and modify some countries names to a more normal format.","8bb84499":"#### Transformation","2b44c65b":"Prophet is an open-source library released by Facebook, it analyses and models the time series in a very simple and direct way.","a8bf3745":"## 3.Auto-correlation","82111d5c":"Projecting the data on a world map, we scaled our data with a log function as the numbers are very divergent, we also binned our numbers into 4 categories and drew them in different colors for more visibility.  \nThe world map is taken from internet :  https:\/\/maproom-wpengine.netdna-ssl.com\/wp-content\/uploads\/Maproom-World-Map-2.png\n","c4220193":"The rolling mean is the mean of a number of cosecutive instances (we took 4, as we have little data) , the rolling mean helps take off the anormalities in the data and draws a smoother ditribution as we can see in the plot, we will use it for our modeling.   \nThe rolling standard mean shows the deviation of a number of consecutive instances (we took 7 for visibility).  \nWe take China as an example.","858bfa81":"![](https:\/\/www.sciencenews.org\/wp-content\/uploads\/2020\/02\/021420_ac_coronainfant_feat-1028x579.jpg)","13560e1f":"#### Comparing the actual cases in China and Italy, the two most infected countries so far","94db2820":"The PACF helps us determinate the AR(p) parameter.\nThe ACF helps us determinate the MA(q) parameter.","805620ae":"In actual cases, China could control and reduce the number of sick people, the amount of sick people in Italy was growing slower than in China, but it suddenly started growing faster.  \nThe number of actual cases in Italy will hopefully start decreasing fast in a couple of days.   \nChina's numbers started droping around the 18th of February and Italy is around 41 days behind China.\n\nThe actual cases in Italy surpassed China's maximum, so it might take more time to control the pandemic in the old continent country.\n\nLet's see the actual evolution of Italy.\n\n\n","6498cf26":"## 2.Transform our time series to a data frame for supervised learning\n","3aeeadff":"## 0.Importing libraries","6be1a2ba":"#### Plotting our results, calculating the error","ed7477ac":"#### Comparing the spread of European countries ","567bc23c":"LSTM is a deep learning supervised learning model implementation, so we have to transform the data in order to provide a usable input and output.","72d91a6c":"By shifting time, we can compare the evolution in different countries.  \nThese graphs might also give us a general idea about the time it will take for some countries to control the pandemic, as we know China has now controlled the pandemic, we can use it as an example to project and compare the evolution.","66860050":"# This graph shows that our intuition was right, the number are almost matching.  \nIn these graphs we can see that:\n   * France evolution was 8 days behind Italy evolution in a slower pace, however an error in the reported numbers showed that the pandemic is actually spreadnig faster than expected in France.\n   * The evolution of the numbers in Spain are only 6 days behind Italy, moreover, we can see that the derivative is a little bit higher, therefore the growth of confirmed cases is faster in Spain.\n   * Germany confirmed case were also groing slightly faster than Italy's, but recently, the evolution pace has decreased.\n   * In the US graph, the slope is much more noticeable and means that the spread is way faster than in European countries.    \n   \n   \nIn general, the virus in European countries is spreading in more or less the same pace.","0879c9d1":"![Coronavirus-HeaderImage.jpg](attachment:Coronavirus-HeaderImage.jpg)","02c9c33d":"# Facebook Prophet","c8b130f2":"According to this example, the first batch of recovered cases start after around 20 days of the infection.","714fccec":"With this feature we can see the evolution of terminated cases in different countries","f8de4434":"We use a function that can be generalized for any country and any case for further use in a simple way","7a8c687e":"In this graph we can see that the number of confirmed cases grows in a similar way in different countries with only a shift in time, let's match the graphs in order to get a foresight from the comparison.","913825da":"Fbprophet requires the dates column to be named 'ds' and data column to be named 'y.  \nWe take the US as an example here.","11440d4e":"To get a clearer visualization, let's make 2 more visible graphs.","214b9aae":"## 5.Summing up the LSTM process in a function","7d0ffea6":"![](https:\/\/storage.googleapis.com\/proudcity\/sanrafaelca\/uploads\/2020\/03\/web-banner-COVID-prevention.jpg)","1fd85bad":"#### Start with importing the required libraries","01c41149":"#### Coronavirus\nAccording to the WHO, coronaviruses are a family of viruses that cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS) and the Middle East respiratory syndrome (MERS).\n\nThese viruses were originally transmitted from animals to people. SARS, for instance, was transmitted from civet cats to humans while MERS moved to humans from a type of camel.\n\nSeveral known coronaviruses are circulating in animals that have not yet infected humans.\n\nThe name coronavirus comes from the Latin word corona, meaning crown or halo. Under an electron microscope, the looks like it is surrounded by a solar corona.\n\nThe novel coronavirus, identified by Chinese authorities on January 7 and since named SARS-CoV-2, is a new strain that had not been previously identified in humans. Little is known about it, although human-to-human transmission has been confirmed.   \n(Source and more info on https:\/\/www.aljazeera.com\/news\/2020\/01\/coronavirus-symptoms-vaccines-risks-200122194509687.html )","76999c17":"#### Reversing the scaling, extracting dates and shaping the results before plotting them","5e47be82":"We define the mean absolute percentage error (MAPE) as a metrics for our model, we will also use the mean absolute error.     \nWe split the data into 2 parts, a 85% training part and a 15% testing part.","c27d2cec":"We use Sklearn MinMaxScaler for scaling, our predefined function for splitting and Italy as an example.","73263225":"Let's now do a model comparison on the same time series data, we apply the 3 models and compare the results.  \nWe take the Germany data.","2dec6296":"## 3.Summing up the process in a function","42e2af23":"In these 3 countries, the pandemic started at more or less th same time, however,the graph shows a vey different evolution from a country to another, South Korea did a very good job facing the pandemic, as it could control the number of cases and keep the deaths very low, Iran and Italy were not as good in controlling the spread of the virus but the recovery process is going smoothly as the number of recovered cases is growing faster. Italy suffered most deaths.","250b757d":"#### We fit the data and plot the components of the predicted data.","580d59d8":"For modeling, we take a random testing of the numbers from 0 to 6 and test them on our data to get a better accuracy, we will thus not accord much importance to the previously found parameters and rather rely on modeling to find parameters better fitting our data.","83a1a939":"## 3.Modeling and fitting","9dc0d125":"The speed of evolution is very different from a country to another, due to precautions, equipment and other readiness to face the virus parameters, the evolution of confirmed cases in the US is the fastest, Spain and France seem like having big difficulties as the number of deaths grows faster than other countries in the same phase.","b5be1be6":"## 1.Rolling mean and standard","ee6278f9":"## 3.LSTM on Germany data","732f1752":"We set our dataset input elements to be taken from the 2 previous elements.","ea3b5074":"## 2.Data stationarity and tranformation","31f2e270":"For the PACF, we notice that after 1, the values don't change much, so the parameter would supposedly be AR(p)=1.\nFor the ACF, there's no big change and we cannot determinate the MA(q) easily.","6e53d93c":"## 1. General view of most affected countries","fcb61441":"##### Calculating and plotting the rolling mean and standards","a2193e3b":"#### World map showing the distribution and density of the confirmed contaminated cases","85e89cbb":"#### Comparing a group of countries with similar evolution: Germany, Spain, France and the US","e7442202":"#### Decomposing the data","6d4e1d0c":"#### Defining metrics, splitting data, modeling, fitting, predicting and testing.","70c23857":"#### In this analysis I will be using the dataset provided by Devakumar kp as it features daily updated data: https:\/\/www.kaggle.com\/imdevskp\/corona-virus-report#covid_19_clean_complete.csv","5780d907":"Sum up the provinces into a bigger entity of countries","79d99c7e":"#### Visualization the detailed evolution in a country ","85ae9cf5":"#### Reshape data, create a sequential instance for our LSTM model and fitting the data","2e91f1b5":"## 1.Coronavirus - Covid-19 - SARS-CoV-2","51228bed":"## 1.Preprocessing","3f1e3844":"Plotting the most infected counrties, showing the number of confirmed cases and number of terminated cases (terminated cases here means the total of recovered and dead cases)","553317db":"## 1.Splitting and scaling the data","dac3b2eb":"#### Plotting the Auto-correlation function (ACF) and Partial auto-correlation function (PACF)","2fb76ecf":"We can see that the number of confirmed cases is indeed getting lower in the last days as the rise of the terminated, specially recovered cases is very comforting.","d414c817":"## 4.Inverse calculating the results","7c29df3d":"## 1.ARIMA on Germany data","3ee6114e":"#### Prevention\nThis might sound repetitive, but it is very important to follow these prevention lines.\n* Wash your hands frequently and carefully\n* Avoid touching your face\n* Stop shaking hands and hugging people\n* Don\u2019t share personal items\n* Cover your mouth and nose when you cough and sneeze\n* Clean and disinfect surfaces\n* Take social distancing seriously\n* Do not gather in groups\n* Avoid eating or drinking in public places\n* Wash fresh groceries\n* Self-quarantine if sick  \n \nMoreover, several countries have taken more strict preventions involving stricter instructions as forced confinement, please make sure to respect these lines as we should all unit to fight this pandemic.  \n(source and more on https:\/\/www.healthline.com\/health\/coronavirus-prevention#Tips-for-prevention)  \n","9eb23439":"Stationarity means that the statistical properties of the data doesn't change over time. The Dickey-Fuller test calculates the p-value of the data to reject the null hypothesis that the time series is non-stationary, at a 0.05 p-value, which gives us strong confidence.","d0b5189d":"#### Testing the data stationarity with the Dickey-Fuller test","ffac1a31":"#### Visualizing the evolution of active cases in different countries","279592af":"The accuracy is very good and the error is very small.  \nHowever, we notice the model took a completely different set of parameters, this comes to confirm that the data wasn't stationary as we assumed with the p-value as the model is not Auto-correlated AR(d=0).","6b70a39b":"## 2.Prophet on Germany data","131cdfe7":"# Models comparison on same data","65834f8b":"Adding a column for the actual cases and displaying all the cases.","fb01895e":"#### Plot of the time evolution of the confirmed cases of China","1e4e8458":"We took a 1028 LSTM layer here, and output it with a dense layer, then compiled the model with an ADAM optimizer. We trained the data in 100 epochs.","746c77ff":"# ARIMA","f34d13c7":"# EDA","13acc6fc":"#### Italy daily new confirmed and terminated cases.","2c048436":"## 2. Comparing the evolution in different countries","760348f5":"#### Barplot of the 10 most infected countries","ede87a40":"The data has 8 columns:\n* Country\n* Province \n* Lat: Lattitude\n* Long: Longitude\n* ds: date\n* Confirmed: Confirmed contaminated cases\n* Deaths: Deaths provoked by the virus\n* Recovered: Recovered from the virus  \n  \nThe dataset has NaN or Null values only in the Province column as some countries don't specify.","ea8768c8":"Here the p-value helps us to reject the null hypothesis of the non-stationarity of the data with confidence, but that assumption is sometimes not enough, we should also consider the time series might not be an AR(auto-regression).   \nThis supposedly means that the parameter i(d) will be 0 and the model would be an ARMA model.","5658efd9":"The decomposition will show the trend, seasonality and residus in our data, we normally don't have seasonality in the data, so we will ignore this parameter.","cb758ad4":"# LSTM","f151952e":"## 2.Analysis introduction\nThis analysis goal is to visualize the daily Coronavirus data and try to further analyse the progression of the virus in different locations.  \n* The first part is a quick EDA as we try to get familiar with the data and visualize it in a simpler way, then try a direct approach to see if we can deduct some forecasst from simple observation and comparison.\n* The second part is the use of different time series algorithms to predict the progression of the cases of the disease. The goal in this project is to determine the best time series algorithm (between ARIMA, Prophet and LSTM) for some distibutions, I will not try to predict the future, however, feel free to use these algorithm for that purpose.  \n  \nThis work will be mainly done through funtions so that it would be easily forked and used for different locations and cases, feel free to use this code for further dicovery.\n\n*This analysis will be updated weekly at the time being according to the coronavirus updates, please refer to the versions if you want to check the previous versions.","a9d6dbde":"## 4.Modeling","ed7b5d98":"We normally use difference to transorm our data and make it stationary, shifting and substracting the data from itself till we obtain stationarity, this process can be skipped here as we already could reject the null hypothesis with confidence.  \nThe integration parameter i(d) would also be determinated here as the d would be equal to number of shiftings done.","88e2b089":"#### Comparing a group of countries with similar evolution: South Korea, Italy, Iran","fec4dab7":"Using the function we defined before, we compare all the cases of the 4 countries"}}