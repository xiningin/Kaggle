{"cell_type":{"993e8e2d":"code","3873d056":"code","bdefc04b":"code","7550126c":"code","0b4df0dc":"code","e2e8dec9":"code","d0723d71":"code","ce146f06":"code","b45b66eb":"code","19d2be1e":"code","72b06bb4":"code","36d29422":"code","a8a9e1a6":"code","5d437956":"code","77a7b234":"code","43c23bd2":"code","5f255207":"code","9658fbbf":"code","a25b9ea2":"code","cf150108":"code","1a65c9f5":"code","99821c63":"markdown","ead7a129":"markdown","1dade5ac":"markdown","e0104b14":"markdown","40e9aa75":"markdown","b06ad223":"markdown","9c3536fb":"markdown","874b13cd":"markdown","e05f3f0e":"markdown","bcbca554":"markdown","af743e53":"markdown","5ea2bbe5":"markdown","4ae339ea":"markdown","7494de9b":"markdown","91c1f98b":"markdown"},"source":{"993e8e2d":"import pandas as pd\nimport numpy as np\nimport pprint\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\n%matplotlib inline\nplt.style.use('ggplot')","3873d056":"Table = pd.read_csv(\"..\/input\/Combined_News_DJIA.csv\")","bdefc04b":"Table.info()","7550126c":"Table.Date = pd.to_datetime(Table.Date) # First convert The Date col to Date format\nfig = plt.figure(figsize=(20,10))\nplt.plot(Table.Date,Table.Label);","0b4df0dc":"Visualizing the actual Series can also be useful ","e2e8dec9":"index_price = pd.read_csv(\"..\/input\/DJIA_table.csv\")\nindex_price.Date = pd.to_datetime(index_price.Date)\nplt.figure(figsize=(10,8))\nplt.plot(index_price.Date, index_price.Close,label = \"DJIA closing price\");\nplt.plot(index_price.Date, index_price.Volume\/100000,label = \"Volumes\");# scale volumes for readability\nplt.legend();\nplt.title(\"DJIA stocks\");","d0723d71":"print(\"Porportion of bullish days: {0:.2f}%\".format(Table.Label.mean()))","ce146f06":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nsnowball = SnowballStemmer(\"english\")","b45b66eb":"stem = lambda word: snowball.stem(word)\npunctuation = ''.join([p for p in punctuation if p not in ['.','!',\"?\",\"-\"]])","19d2be1e":"def clean_text(text,stem=False,remove_punct='punct',remove_stopwords=False,return_list=False):\n    text=str(text)\n    if text.startswith(\"b'\"): # remove the byte types strings which have been converted to text\n        text=text[2:]\n    words = word_tokenize(text)\n        \n    # Optionally use stemmer\n    if stem:\n        words = [stem(w) for w in words]\n    if remove_punct:\n        if remove_punct=='all':\n            words = [re.sub(\"[^a-zA-Z\\.\\?\\!]\",\" \", w) for w in words]\n        elif remove_punct=='punct':\n            punct = set(punctuation)\n            words = [''.join(ch for ch in w if ch not in punct) for w in words]\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if w not in stops]\n    \n    words = [w for w in words if w not in ['','b']]\n    if return_list:\n        return(words)\n    else:\n        return ' '.join(words)","72b06bb4":"print(\"                 Raw                               ---               Cleaned\")\nfor i ,(word1,word2) in enumerate(zip(Table.Top1,Table.Top1.map(lambda x: clean_text(x)))):\n    if i>15: break\n    print(word1[:50],'---',word2[:50])","36d29422":"%%time\ncols = [t for t in list(Table.columns) if t not in ['Label','Date']]\nCleanedTable = Table[cols].apply(lambda i: i.map(lambda x: clean_text(x)))","a8a9e1a6":"corpus = []\nfor i,line in enumerate(CleanedTable.index):\n    line_doc=''\n    for col in CleanedTable:\n        line_doc += \" \"+str(CleanedTable.ix[i,col])\n    corpus.append(line_doc)","5d437956":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\ndata = pd.Series(corpus)\ncv = CountVectorizer(\n    analyzer ='word',\n    ngram_range=(1,3), # we include (1,3) ngrams since they might have a higher predictive power than single words \n    stop_words='english',\n    max_df = 0.7,\n    min_df=5 # from my experience the minimum robust occurence frequency for a word is in the range [4,15]\n)\ntf=TfidfVectorizer(\n    analyzer ='word',\n    ngram_range=(1,3),\n    stop_words='english',\n    max_df = 0.7,\n    min_df=5 \n)\n\ncount_matrix = cv.fit_transform(data)\ntfidf_matrix = tf.fit_transform(data)","77a7b234":"# train test split according to description\nX_train = tfidf_matrix[:1611]\nX_test = tfidf_matrix[1610:]\ny_train = Table.Label[:1611]\ny_test = Table.Label[1610:]","43c23bd2":"%%time\nfrom sklearn.linear_model import LogisticRegression\nimport scipy\nfrom sklearn.model_selection import GridSearchCV\nlm = LogisticRegression()\n\nparam_grid = {\n    'C': np.logspace(0.01,20,10),\n    \"penalty\" :['l1','l2']\n}\n\ngs = GridSearchCV(lm, param_grid,\n                        cv=3,\n                        n_jobs=-1,\n                        scoring=\"roc_auc\")\ngs.fit(X_train, y_train)","5f255207":"lm = gs.best_estimator_\nprint('best params:',gs.best_params_)\nprint('best CV score:', gs.best_score_)\n\nlm.fit(X_train,y_train)\nprint(\"Nb of significative features\",sum(np.abs(lm.coef_)[0]>0))\nprint(\"{0:.2f}% of features excluded by regularization\".format((1-sum(np.abs(lm.coef_)[0]>0)\/len(lm.coef_[0,:]))*100))","9658fbbf":"pred = lm.predict(X_test)\nfrom sklearn.metrics import accuracy_score,auc,roc_auc_score\n\nprint(\"acc:\",accuracy_score(y_test,pred))\nprint(\"auc:\",roc_auc_score(y_test,pred))","a25b9ea2":"var_imp = pd.DataFrame({\"features\":cv.get_feature_names(),\n              \"coefs\":pd.Series(lm.coef_[0,])})\nvar_imp.index = var_imp.features\nvar_imp['color'] = var_imp.coefs.map(lambda l: l>0)\n\nplot_table = pd.concat([var_imp.sort_values(by='coefs').head(20),var_imp.sort_values(by='coefs').tail(20)])\ndic = {True:'g',False:'r'}\nplot_table[\"coefs\"].plot(kind='barh',figsize = (10,13),\n                         color = plot_table.color.map(dic),\n                         title = \"Bag of word Feature Importance\",\n                         label=\"color\");","cf150108":"def print_coef(terms,print_errors=True,threshold=0.5):\n    for term in terms:\n        term=term.lower()\n        try:\n            if np.abs(var_imp.ix[term,\"coefs\"])>threshold:\n                print(\"Coef value for term\",'\"{}\"'.format(term),\"is : {0:.2}\".format(var_imp.ix[term,\"coefs\"]))\n            else:\n                if print_errors:\n                    print('The term \"{}\"is not highly significative'.format(term))\n        except:\n            if print_errors:\n                print('\"{}\" not in variables'.format(term))\n\nterms = ['global crisis','putin says','china trying','job losses',\"germany says\"\n         ,'killed civilians','globalization','afghan military','avian flu','tsunami hit',\"civil war\"]\nprint_coef(terms)","1a65c9f5":"from nltk.corpus import gazetteers\n# we import a list of all country names from nltk \n# and pass it through our function\ncountries = gazetteers.words(fileids=\"countries.txt\")\nprint_coef(countries,print_errors=False,threshold=5)","99821c63":"The accuracy on the test set is very close from a random guess (0.5) and from the number of Positive Labels in the sample, the predictive power of this model is highly questionable.\nCross validation suggests that the best set of hyperparameters induce almost no regularization, meaning that all variables are meaningful...\n\nUsing the CountVectorizer instead similarly yields  poor results\n\nNevertheless we can inspect the model weights and try to see which words are used for prediction.","ead7a129":"The metadata for the file show no missing data ,there is one numeric column Label containing the target we will try to predict. ","1dade5ac":"We now vectorize those documents into a bag of words model counting the occurence of each word of the vocabulary.\nThe second methods use tfidf Vectorizer, this methods weights the words by assuming that:\n\n - The frequency of word within a document represents the importance of this word in the document.\n - Terms occuring often across documents have a low discriminative power and then should be given a lower weight  \n\n You can find more information about this functions in the scikitlearn [Documentation][1] and [User Guide][2]\n\n\n  [1]: http:\/\/We%20now%20vectorize%20those%20documents%20into%20a%20bag%20of%20words%20model%20counting%20the%20occurence%20of%20each%20word%20of%20the%20vocabulary.%20The%20second%20methods%20use%20tfidf%20Vectorizer,%20this%20methods%20weights%20the%20words%20by%20assuming%20that:%20%201.%20The%20frequency%20of%20word%20within%20a%20document%20represents%20the%20importance%20of%20this%20word%20in%20the%20document.%202.%20Terms%20occuring%20often%20across%20documents%20have%20a%20low%20discriminative%20power%20and%20then%20should%20be%20given%20a%20lower%20weight%20%20%20%20%20You%20can%20find%20more%20information%20about%20this%20functions%20in%20the%20scikitlearn%20Documentation%20and%20User%20Guide%20%20%20%20%20http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n  [2]: http:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction","e0104b14":"Here again we can see some curious results, but we won't take the risk to comment them...","40e9aa75":"We will now fit a Logistic Regression with the tfidf matrix as input using gridsearch to find the optimal parameters.","b06ad223":"### An attempt at Modeling Stock market prices with NLP","9c3536fb":"**Apply the cleaning over all news**","874b13cd":"We can now check if some expressions are included in the model and what is their \"impact\" on the DJIA price","e05f3f0e":"We can confirm the poor results of the model since some words seem to have an opposite impact as what we would expect for example \"tsunami hit\" has a positive impact, the coefficients suggest that when germany is speaking this yields has a bad impact on the shares price of american industrial companies (this could be debated...).\nNevertherless some coefficients make sense such as \"afghan military\" or \"avian flu\".\n## Let's check the influence of the mention of Country names in the model ##","bcbca554":"This plot is not very informative but we can observe that **Bullish\/bearish days seem following each other**, this is a well known phenomena in financial econometrics [(see Arch\/Garch models )][1], we will see later if we can do somthing to capture this...\nThis is much clearer using a calendar plot, from the library calmap for example, I added a snapshot below for a few years.\n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Autoregressive_conditional_heteroskedasticity","af743e53":"**We now concatenate all columns in order to get a single document per day, made of all top 25 news We might drop the information related to the rank of the News but we thus have a dictionnary containing all words for a given day**","5ea2bbe5":"## Let's start with a very basic bag-of-words model ##\nThe intuition behind this simple model is that the occurence of some particular words or sequence of words in the reddit News is linked with an event which itself had an impact on the index.\n\n\n - We first need to clean the data, to avoid modeling too much noise we will use the python natural langage toolkit nltk\n\n","4ae339ea":"In this Kernel we will make an attempt at predicting the DJIA trend with various methods ranging from Natural langage Processing to sequence based models.\nWe will also try to inspect the models in order to undestand the methodology involved in each algorithm.","7494de9b":"![enter image description here][1]\n\n\n  [1]: http:\/\/cdn.images.express.co.uk\/img\/dynamic\/22\/590x\/secondary\/dow-jones-stocks-810706.jpg","91c1f98b":"![enter image description here][1]\n\n\n  [1]: https:\/\/cloud.githubusercontent.com\/assets\/22575341\/26532195\/9ba3fdaa-43fa-11e7-825c-e369f75cd4af.PNG"}}