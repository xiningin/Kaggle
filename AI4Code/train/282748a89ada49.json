{"cell_type":{"695c3c5d":"code","b74846fe":"code","21cb0414":"code","cf9288a9":"code","d9b74892":"code","a5ca27e3":"code","44ae28e5":"code","c5908a37":"code","46651e38":"code","85e62e16":"code","5e32204c":"code","5f16f0da":"code","291b8209":"code","435a24a1":"code","897d774b":"code","08258c69":"code","1fd69e8e":"code","a312609f":"code","c652e76e":"code","d773f304":"code","1466951d":"code","3ee1ffc2":"code","1aef13a4":"code","96309690":"code","2dee7517":"code","747b73df":"code","8e4cdc6f":"code","02e81a19":"code","ea3de6d9":"code","529b83e0":"code","9f1546e7":"code","ed49214e":"markdown","5874d3ff":"markdown","c592553f":"markdown","d17ce6c3":"markdown","946a2919":"markdown","f2bf3b59":"markdown","5e330c31":"markdown","d5bee6de":"markdown","d836e044":"markdown","73d168e7":"markdown","77383893":"markdown","cad2e302":"markdown","e703ecac":"markdown","44e77456":"markdown","6f7f966a":"markdown","7c13b207":"markdown","c1d35fec":"markdown","a7652f51":"markdown","95ac758f":"markdown","d1bbdb5c":"markdown","37d60a2f":"markdown"},"source":{"695c3c5d":"# Let`s import all packages that we may need:\n\nimport sys \nimport numpy as np # linear algebra\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","b74846fe":"## Data can be downloaded from: http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00235\/\n## Just open the zip file and grab the file 'household_power_consumption.txt' put it in the directory \n## that you would like to run the code. \n\n\ndf = pd.read_csv('..\/input\/household_power_consumption.txt', sep=';', \n                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='dt')","21cb0414":"df.head() ","cf9288a9":"df.info()","d9b74892":"df.dtypes","a5ca27e3":"df.shape","44ae28e5":"df.describe()","c5908a37":"df.columns","46651e38":"## finding all columns that have nan:\n\ndroping_list_all=[]\nfor j in range(0,7):\n    if not df.iloc[:, j].notnull().all():\n        droping_list_all.append(j)        \n        #print(df.iloc[:,j].unique())\ndroping_list_all","85e62e16":"# filling nan with mean in any columns\n\nfor j in range(0,7):        \n        df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())","5e32204c":"# another sanity check to make sure that there are not more any nan\ndf.isnull().sum()","5f16f0da":"df.Global_active_power.resample('D').sum().plot(title='Global_active_power resampled over day for sum') \n#df.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day', color='red') \nplt.tight_layout()\nplt.show()   \n\ndf.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day for mean', color='red') \nplt.tight_layout()\nplt.show()","291b8209":"### Below I show mean and std of 'Global_intensity' resampled over day \nr = df.Global_intensity.resample('D').agg(['mean', 'std'])\nr.plot(subplots = True, title='Global_intensity resampled over day')\nplt.show()","435a24a1":"### Below I show mean and std of 'Global_reactive_power' resampled over day\nr2 = df.Global_reactive_power.resample('D').agg(['mean', 'std'])\nr2.plot(subplots = True, title='Global_reactive_power resampled over day', color='red')\nplt.show()","897d774b":"### Sum of 'Global_active_power' resampled over month\n# Sum of 'Global_active_power' resampled over month\ndf['Global_active_power'].resample('M').mean().plot(kind='bar')\nplt.xticks(rotation=60)\nplt.ylabel('Global_active_power')\nplt.title('Global_active_power per month (averaged over month)')\nplt.show()","08258c69":"## Mean of 'Global_active_power' resampled over quarter\ndf['Global_active_power'].resample('Q').mean().plot(kind='bar')\nplt.xticks(rotation=60)\nplt.ylabel('Global_active_power')\nplt.title('Global_active_power per quarter (averaged over quarter)')\nplt.show()","1fd69e8e":"## mean of 'Voltage' resampled over month\ndf['Voltage'].resample('M').mean().plot(kind='bar', color='red')\nplt.xticks(rotation=60)\nplt.ylabel('Voltage')\nplt.title('Voltage per quarter (summed over quarter)')\nplt.show()","a312609f":"df['Sub_metering_1'].resample('M').mean().plot(kind='bar', color='brown')\nplt.xticks(rotation=60)\nplt.ylabel('Sub_metering_1')\nplt.title('Sub_metering_1 per quarter (summed over quarter)')\nplt.show()","c652e76e":"# Below I compare the mean of different featuresresampled over day. \n# specify columns to plot\ncols = [0, 1, 2, 3, 5, 6]\ni = 1\ngroups=cols\nvalues = df.resample('D').mean().values\n# plot each column\nplt.figure(figsize=(15, 10))\nfor group in groups:\n\tplt.subplot(len(cols), 1, i)\n\tplt.plot(values[:, group])\n\tplt.title(df.columns[group], y=0.75, loc='right')\n\ti += 1\nplt.show()","d773f304":"## resampling over week and computing mean\ndf.Global_reactive_power.resample('W').mean().plot(color='y', legend=True)\ndf.Global_active_power.resample('W').mean().plot(color='r', legend=True)\ndf.Sub_metering_1.resample('W').mean().plot(color='b', legend=True)\ndf.Global_intensity.resample('W').mean().plot(color='g', legend=True)\nplt.show()","1466951d":"# Below I show hist plot of the mean of different feature resampled over month \ndf.Global_active_power.resample('M').mean().plot(kind='hist', color='r', legend=True )\ndf.Global_reactive_power.resample('M').mean().plot(kind='hist',color='b', legend=True)\n#df.Voltage.resample('M').sum().plot(kind='hist',color='g', legend=True)\ndf.Global_intensity.resample('M').mean().plot(kind='hist', color='g', legend=True)\ndf.Sub_metering_1.resample('M').mean().plot(kind='hist', color='y', legend=True)\nplt.show()","3ee1ffc2":"## The correlations between 'Global_intensity', 'Global_active_power'\ndata_returns = df.pct_change()\nsns.jointplot(x='Global_intensity', y='Global_active_power', data=data_returns)  \n\nplt.show()","1aef13a4":"##\u00a0The correlations between 'Voltage' and  'Global_active_power'\nsns.jointplot(x='Voltage', y='Global_active_power', data=data_returns)  \nplt.show()","96309690":"# Correlations among columns\nplt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\nplt.title('without resampling', size=15)\nplt.colorbar()\nplt.show()","2dee7517":"# Correlations of mean of features resampled over months\n\n\nplt.matshow(df.resample('M').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\nplt.title('resampled over month', size=15)\nplt.colorbar()\nplt.margins(0.02)\nplt.matshow(df.resample('A').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\nplt.title('resampled over year', size=15)\nplt.colorbar()\nplt.show()","747b73df":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdff = pd.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(dff.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(dff.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n ","8e4cdc6f":"## resampling of data over hour\ndf_resample = df.resample('h').mean() \ndf_resample.shape","02e81a19":"## * Note: I scale all features in range of [0,1].\n\n## If you would like to train based on the resampled data (over hour), then used below\nvalues = df_resample.values \n\n\n## full data without resampling\n#values = df.values\n\n# integer encode direction\n# ensure all data is float\n#values = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)\nprint(reframed.head())","ea3de6d9":"# split into train and test sets\nvalues = reframed.values\n\nn_train_time = 365*24\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\n##test = values[n_train_time:n_test_time, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","529b83e0":"\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.2))\n#    model.add(LSTM(70))\n#    model.add(Dropout(0.3))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n\n\n# fit network\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], 7))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","9f1546e7":"## time steps, every step is one hour (you can easily convert the time step to the actual time index)\n## for a demonstration purpose, I only compare the predictions in 200 hours. \n\naa=[x for x in range(200)]\nplt.plot(aa, inv_y[:200], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:200], 'r', label=\"prediction\")\nplt.ylabel('Global_active_power', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show()","ed49214e":"# Splitting the rest of data to train and validation sets","5874d3ff":"## Dealing with missing values  'nan' with a test statistic","c592553f":" ## 1) Note that data include 'nan' and '?' as a string. I converted both to numpy nan in importing stage (above) and treated both of them the same. \n\n##\u00a02) I merged two columns 'Date' and 'Time' to 'dt'. \n\n##\u00a03) I also converted in the above, the data to time-series type, by taking index to be the time. ","d17ce6c3":"\n## This Notebook is a sort of tutorial for the beginners in Deep Learning and time-series data analysis. \n\n##  * The aim is just to show how to build the simplest Long Short-Term Memory (LSTM) recurrent neural network for the data.  \n \n###\u00a0* Jupyter notebook can be downloaded here: https:\/\/github.com\/amirrezaeian\/Individual-household-electric-power-consumption-Data-Set-","946a2919":"#\u00a0Data visualization","f2bf3b59":" ## * It is seen from the above plots that the mean of 'Volage' over month is pretty much constant compared to other features. This is important again in feature selection.","5e330c31":"# Machine-Leaning: LSTM Data Preparation and feature engineering","d5bee6de":"### * In order to reduce the computation time, and also get a quick result to test the model.  One can resmaple the data over hour (the original data are given in minutes). This will reduce the size of data from 2075259 to 34589 but keep the overall strucure of data as shown in the above.   ","d836e044":"# Final remarks","73d168e7":"### * Here I have used the LSTM neural network which is now the state-of-the-art for sequencial problems. \n\n### * In order to reduce the computation time, and get some results quickly, I took the first year of data (resampled over hour) to train the model and the rest of data to test the model.  \n\n### * I put together a very simple LSTM neural-network to show that one can obtain reasonable predictions. However numbers of rows is too high and as a result the computation is very time-consuming (even for the simple model in the above it took few mins to be run on  2.8 GHz Intel Core i7).  The Best is to write the last part of code using Spark (MLlib) running on GPU.  \n\n### * Moreover, the neural-network architecture that I have designed is a toy model. It can be easily improved by adding CNN  and dropout layers.  The CNN is useful here since there are correlations in data (CNN layer is a good way to probe the local structure of data).   ","77383893":"### It is very important to note from above two plots that resampling over larger time inteval, will diminish the periodicity of system as we expect. This is important for machine learning feature engineering. ","cad2e302":"### * Above I showed 7 input variables (input series) and the 1 output variable for 'Global_active_power' at the current  time in hour (depending on resampling). ","e703ecac":"# Correlations among features","44e77456":"###\u00a0Note that in order to improve the model, one has to adjust epochs and batch_size.","6f7f966a":"### * From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose. ","7c13b207":"##\u00a0* Below I resample over day, and show the sum and mean of Global_active_power. It is seen that mean and sum of resampled data set, have similar structure.","c1d35fec":"### * First, I split the prepared dataset into train and test sets. To speed up the training of the model (for the sake of the demonstration), we will only train the model on the first year of data, then evaluate it on the next 3 years of data.","a7652f51":"# Model architecture\n\n### 1)  LSTM with 100 neurons in the first visible layer \n### 3) dropout 20%\n### 4) 1 neuron in the output layer for predicting Global_active_power. \n### 5) The input shape will be 1 time step with 7 features.\n\n### 6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n### 7) The model will be fit for 20 training epochs with a batch size of 70.\n","95ac758f":"## * It is seen from above that with resampling techniques one can change the correlations among features. This is important for feature  engineering.","d1bbdb5c":"### * I will apply recurrent nueral network (LSTM) which is best suited for time-seriers and sequential problem. This approach is the best if we have large data.  \n\n###\u00a0* I will frame the supervised learning problem as predicting the Global_active_power at the current time (t) given the Global_active_power measurement and other features at the prior time step.","37d60a2f":"The description of data can be found here:\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/Individual+household+electric+power+consumption\n\nAttribute Information:\n1.date: Date in format dd\/mm\/yyyy\n\n2.time: time in format hh:mm:ss\n\n3.global_active_power: household global minute-averaged active power (in kilowatt)\n\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n\n5.voltage: minute-averaged voltage (in volt)\n\n6.global_intensity: household global minute-averaged current intensity (in ampere)\n\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."}}