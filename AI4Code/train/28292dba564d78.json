{"cell_type":{"6fc5e59f":"code","909d0290":"code","8c0f7689":"code","c244e4dd":"code","429f7a42":"code","133152f8":"code","0999f2ce":"code","a3257c9a":"code","3683e1d5":"code","f569da61":"code","9ccda0ef":"code","3591097d":"code","7025d5a8":"code","288242a5":"code","740b415c":"code","49332fea":"markdown","4f2c8e9f":"markdown","468afcfe":"markdown","2888694d":"markdown","5ea39239":"markdown","5e0a5850":"markdown","54da763c":"markdown","aa96fc43":"markdown","472f5591":"markdown","a3c3decd":"markdown","275e896a":"markdown","25dbc5a1":"markdown","15ec5180":"markdown","4efc74ae":"markdown","04acfa66":"markdown"},"source":{"6fc5e59f":"import numpy as np\nimport pandas as pd\nfrom sklearn import datasets","909d0290":"# Load Iris dataset\ndata = datasets.load_iris()\ndf = pd.DataFrame(data['data']).set_axis(data['feature_names'], axis=1)\ndf['target'] = data['target']","8c0f7689":"# Only keep data of target 0 & 1 for binary classification \ndf = df[df.target.isin([0,1])]\nprint(f\"Target 0: {data['target_names'][0]}\")\nprint(f\"Target 1: {data['target_names'][1]}\")","c244e4dd":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","429f7a42":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))","133152f8":"def loss(X, y, theta):\n    m = len(y) \n    z = np.dot(X, theta) \n    h = sigmoid(z) \n    # since log(0) = infinity, adding arbitrary small value to h will prevent the cost from blowing up, although h is rarely exactly 0\n    # epsilon = 0.00001 \n    cost = (((-y) * np.log(h))-((1-y) * np.log(1-h)))\/m # taking log of <1 will return negative values, hence multiply -1\n    return cost","0999f2ce":"def gradient(X, y, theta):\n    h = sigmoid(np.dot(X, theta))\n    gradient = np.dot(X.T,(h-y))\/m \n    return gradient # return a vector","a3257c9a":"def fit(X, y, learning_rate, epochs):\n    \n    # Initialize theta with random values \/ zeros\n    theta = np.random.rand(X.shape[1])\n    m = len(X)\n    loss = [] # To store the loss per epoch\n\n    # Iteratively update theta through gradient descent\n    for epoch in range(epochs):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        gradient = np.dot(X.T,(h-y))\/m \n        theta -= learning_rate * gradient\n        cost = (((-y) * np.log(h))-((1-y) * np.log(1-h))).mean()\n        loss.append(cost)\n        \n    # Return optimal theta (coefficients) which will be passed as a parameter to predict() & the losses\n    return theta, loss","3683e1d5":"theta, loss = fit(X,y,0.01,1000)","f569da61":"# The optimal weights for X1 to X4\ntheta","9ccda0ef":"print(f'z = X1 * {theta[0]:.2f} + X2 * {theta[1]:.2f} + X3 * {theta[2]:.2f} + X4 * {theta[3]:.2f}')\nprint('The calculated z will be passed to the sigmoid function and yields anywhere between 0 and 1.')","3591097d":"# Plot the loss\nimport seaborn as sns\nsns.lineplot(x=range(len(loss)), y=loss)","7025d5a8":"def predict(X, theta, threshold):\n    z = np.dot(X, theta)\n    h = sigmoid(z)\n    return np.where(h > 0.5, 1, 0)","288242a5":"# Sample from the dataset for testing the model\nsample = df.sample(5)\nsample","740b415c":"# Make prediction\ntest_X = df.iloc[sample.index,:-1].values\npredict(test_X, theta, 0.5)","49332fea":"To build the Logistic Regression algorithm, there are 3 key building blocks:\n1. Sigmoid Function\n2. Cost (Loss) Function\n3. Gradient Descent Algorithm","4f2c8e9f":"#### Sigmoid function","468afcfe":"![image.png](attachment:81957d82-3190-4538-87dc-e0b86f04c555.png)","2888694d":"#### Cost function","5ea39239":"![image.png](attachment:d330e424-132b-43d7-a92b-bcc3460995cb.png)","5e0a5850":"Taking the derivative of the loss function with respect to each weight will tell us how the loss would change with different parameters, i.e. weights, thereby allowing us to update the weights. The weights are updated by subtracting the learning rate * gradient.","54da763c":"## Putting everything together in fit() & predict()","aa96fc43":"#### Gradient descent algorithm","472f5591":"The Sigmoid function squashes the values to between 0 and 1. It produces a signature S-shaped curve.","a3c3decd":"![image.png](attachment:faccfea2-c1e1-44e5-b711-a4892601387f.png)","275e896a":"![image.png](attachment:71497618-6fdf-4d9f-ae3d-38fbaed17a46.png)","25dbc5a1":"Since this notebook is focus at implementing the algorithm, train-test-split is deliberately ommitted. There is just X and y, and no X_train, y_train, etc.","15ec5180":"*y * log(h)*  --  this part calculates the loss for when actual y = 1\n* if the predicted h from the sigmoid function is 1, the cost\/loss is 0;\n* otherwise, if the predicted h is 0 (or close to 0), log(h) becomes greater and increases the loss\n\n*(1-y) * log(1-h)*  --  this part calculates the loss for when actual y = 0\n* if the predicted h is 1 (or close to 1), loss is non-zero, i.e. (1-0) * log(0) \n* otherwise, if the predicted h is 0, (1-0) * log(1-0) = 0","4efc74ae":"While the formula looks intimidating at first glance, it can be decomposed into 2 parts for simpler understanding.","04acfa66":"## Algorithm building"}}