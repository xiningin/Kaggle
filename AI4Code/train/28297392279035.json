{"cell_type":{"8f3fc173":"code","ab553e5d":"code","ab3df987":"code","a17e9eb1":"code","f929e357":"code","abdc0ffe":"code","2cb8f224":"code","f64c3dc7":"code","aadba313":"code","23413578":"code","6034404c":"code","52aaf796":"code","9dc7605c":"code","d9a3d757":"code","3daae6d6":"markdown","a5c0a85a":"markdown","b335b80a":"markdown","533bbd25":"markdown","371b6b15":"markdown","421457df":"markdown","b63781e6":"markdown","5f78b651":"markdown","f91c0e96":"markdown","01bc173b":"markdown","e9490e74":"markdown","af02ae18":"markdown","34ea3698":"markdown","0800fe3e":"markdown","513c5fc0":"markdown","412fdccd":"markdown","b42a097d":"markdown","b7750938":"markdown","5487b1c4":"markdown","d48da37a":"markdown"},"source":{"8f3fc173":"import pandas as pd\n\ndf = pd.read_json(\"..\/input\/Sarcasm_Headlines_Dataset.json\", lines=True)\ndf.head()","ab553e5d":"import plotly as py\nfrom plotly import graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# Make pie chart to compare the numbers of sarcastic and not-sarcastic headlines\nlabels = ['Sarcastic', 'Not Sarcastic']\ncount_sarcastic = len(df[df['is_sarcastic']==1])\ncount_notsar = len(df[df['is_sarcastic']==0])\nvalues = [count_sarcastic, count_notsar]\n# values = [20,50]\n\ntrace = go.Pie(labels=labels,\n               values=values,\n               textfont=dict(size=19, color='#FFFFFF'),\n               marker=dict(\n                   colors=['#DB0415', '#2424FF'] \n               )\n              )\n\nlayout = go.Layout(title = '<b>Sarcastic vs Not Sarcastic<\/b>')\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","ab3df987":"for i,headline in enumerate (df['headline'], 1):\n    if i > 20:\n        break\n    else:\n        print(i, headline)","a17e9eb1":"import string\nfrom string import digits, punctuation\n\nhl_cleansed = []\nfor hl in df['headline']:\n#     Remove punctuations\n    clean = hl.translate(str.maketrans('', '', punctuation))\n#     Remove digits\/numbers\n    clean = clean.translate(str.maketrans('', '', digits))\n    hl_cleansed.append(clean)\n    \n# View comparison\nprint('Original texts :')\nprint(df['headline'][37])\nprint('\\nAfter cleansed :')\nprint(hl_cleansed[37])\n","f929e357":"# Tokenization process\nhl_tokens = []\nfor hl in hl_cleansed:\n    hl_tokens.append(hl.split())\n\n# View Comparison\nindex = 100\nprint('Before tokenization :')\nprint(hl_cleansed[index])\nprint('\\nAfter tokenization :')\nprint(hl_tokens[index])","abdc0ffe":"# Lemmatize with appropriate POS Tag\n# Credit : www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\n# Init Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\nhl_lemmatized = []\nfor tokens in hl_tokens:\n    lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n    hl_lemmatized.append(lemm)\n    \n# Example comparison\nword_1 = ['skyrim','dragons', 'are', 'having', 'parties']\nword_2 = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_1]\nprint('Before lemmatization :\\t',word_1)\nprint('After lemmatization :\\t',word_2)","2cb8f224":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n# Vectorize and convert text into sequences\nmax_features = 2000\nmax_token = len(max(hl_lemmatized))\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(hl_lemmatized)\nsequences = tokenizer.texts_to_sequences(hl_lemmatized)\nX = pad_sequences(sequences, maxlen=max_token)","f64c3dc7":"index = 10\nprint('Before :')\nprint(hl_lemmatized[index],'\\n')\nprint('After sequences convertion :')\nprint(sequences[index],'\\n')\nprint('After padding :')\nprint(X[index])\n\n","aadba313":"from sklearn.model_selection import train_test_split\n\nY = df['is_sarcastic'].values\nY = np.vstack(Y)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3, random_state = 42)","23413578":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\nembed_dim = 64\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = max_token))\nmodel.add(LSTM(96, dropout=0.2, recurrent_dropout=0.2, activation='relu'))\n# model.add(Dense(128))\n# model.add(Activation('relu'))\n# model.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","6034404c":"epoch = 10\nbatch_size = 128\nmodel.fit(X_train, Y_train, epochs = epoch, batch_size=batch_size, verbose = 2)","52aaf796":"loss, acc = model.evaluate(X_test, Y_test, verbose=2)\nprint(\"Overall scores\")\nprint(\"Loss\\t\\t: \", round(loss, 3))\nprint(\"Accuracy\\t: \", round(acc, 3))\n","9dc7605c":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_test)):\n    \n    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.around(result) == np.around(Y_test[x]):\n        if np.around(Y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.around(Y_test[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1","d9a3d757":"print(\"Sarcasm accuracy\\t: \", round(pos_correct\/pos_cnt*100, 3),\"%\")\nprint(\"Non-sarcasm accuracy\\t: \", round(neg_correct\/neg_cnt*100, 3),\"%\")","3daae6d6":"# Training Process","a5c0a85a":"**4. Test the Model**\n\nAfter we trained our model now we can test our model by count it's accuracy.","b335b80a":"## Objective\nThe goal of this work is to make a classifier model to detect the headlines wether it's a sarcasm or not.","533bbd25":"**1. Text Cleansing**\n\nThe purpose of text cleansing is to remove unnecessary character from our texts. Here we gonna remove *digits* and *punctuations* since they are not needed in our sarcasm detection.","371b6b15":"**1. Preparing the Data**\n\nBefore we start the training process with Keras, we need to convert our data so Keras can read and process it. First we should vectorize our data and convert them into sequences.","421457df":"Next, I want to check the accuracy of each categories since we have unbalanced numbers of data on both categories.","b63781e6":"Below is an example result of what we did before.","5f78b651":"<img src=\"https:\/\/media.giphy.com\/media\/3otPoUkg3hBxQKRJ7y\/giphy.gif\"> ","f91c0e96":"<img src=\"https:\/\/aquariuschannelings.files.wordpress.com\/2017\/06\/sarcasm.jpg?w=930&h=450&crop=1\" width=\"750px\"> \n\n## What is Sarcasm?\n\nAccording to [Wikipedia](https:\/\/simple.wikipedia.org\/wiki\/Sarcasm), Sarcasm is a figure of speech or speech comment which is extremely difficult to define. It is a statement or comment which means the opposite of what it says. It may be made with the intent of humour, or it may be made to be hurtful.","01bc173b":"**3. Training Process**\n\nNow we are ready to train our model.<br>*yeay!!!*","e9490e74":"## Load the data\n\nThe very beginning of everything is loading the data.","af02ae18":"## Text Preprocessing\n\n<img src=\"https:\/\/www.mememaker.net\/api\/bucket?path=static\/img\/memes\/full\/2017\/Apr\/22\/19\/dirty-data-dirty-data-is-everywhere33.jpg\" width=\"300px\">","34ea3698":"**2. Building the Model**\n\nIn this project we will using LSTM model. There are also some variables that called *hyperparameters* which we must set before we train our model and their values are somehow intuitive, no strict rules or standards to set the values. They are *embed dim, number of neurons,* and *dropout rate*.","0800fe3e":"First let's see how our headlines look like to decide what kind of text preprocess that we need to do.","513c5fc0":"**2. Tokenization**\n\nTokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. It's an important process to do in natural language processing since tokenized words will help for words checking or convertion process.","412fdccd":"Next I want to check wether the data has balanced number for each category by using a piechart.","b42a097d":"**3. Lemmatization**\n\nLemmatization is a process to converting the words of a sentence to its dictionary form, which is known as the *lemma*. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.\n\nHere is an example of lemmatization result.","b7750938":"It's shown that the data has already in the lowercase form. So we just need to clean the data, tokenize it, and then do lemmatization process.","5487b1c4":"Next we need to split our data into *training data* and *testing data*. Here we use **X** as a list of data value and **Y** to list of prediction value.","d48da37a":"Well it's seems that our data has unbalance number on each category. It has 13% percent difference (about 3300 data) with *Not Sarcastic* category has more numbers of data. I won't remove some *Not Sarcastic* data to make it balance, instead I will compare the accuracy of each category prediction later."}}