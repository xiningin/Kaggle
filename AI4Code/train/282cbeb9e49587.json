{"cell_type":{"2a948f9d":"code","b2dddffe":"code","8df5e349":"code","a09123f2":"code","35aa0519":"code","0cb8c0f3":"code","3b1e0b32":"code","f2a34316":"code","49f0d4c9":"code","7561147c":"code","ad6fdaa1":"code","70d3cd5e":"code","57980446":"markdown","24a2b70a":"markdown","05e1e7ba":"markdown","49395f22":"markdown"},"source":{"2a948f9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport random\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom IPython import display as ipd\n\nfrom pandas_profiling import ProfileReport as profile\n\nimport pkg_resources as pkg\nprint( f\"pandas_profiling version: {pkg.get_distribution('pandas_profiling').version}\")\n\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.metrics import roc_curve, auc, cohen_kappa_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b2dddffe":"RANDOM_SEED = 42\nDEBUG = False\nPROFILE = False\n\ndef seeding(SEED, use_tf=False):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    if use_tf:\n        tf.random.set_seed(SEED)\n    print('seeding done!!!')\n\nseeding(RANDOM_SEED)\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\nif DEBUG:\n    train = train[:50000]\n    \ntarget = train.target\ntrain.drop(['id','target'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","8df5e349":"print('train:',train.shape)\nprint('test:',test.shape)","a09123f2":"%%time\n\nif PROFILE:\n    train_profile = profile(train, title=\"Train Data\", minimal=True)\n    display(train_profile)","35aa0519":"from sklearn.preprocessing import MinMaxScaler\n\ndef minmax_scale(df, cols):\n    scaler = MinMaxScaler()\n    for col in cols:\n        df[col] = scaler.fit_transform(df[col].values.reshape(-1,1))\n\nskewed_cols = ['f46', 'f59', 'f89']\nminmax_scale( train, skewed_cols)\nminmax_scale( test, skewed_cols)","0cb8c0f3":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train)\ntest_scaled = scaler.transform(test)\n\npca = PCA(n_components=2)\nX_pca_train = pca.fit_transform(train_scaled)\nX_pca_test = pca.transform(test_scaled)\n\nf, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 6))\nax1.scatter(X_pca_train[:,0],X_pca_train[:,1],c=target,cmap='rainbow')\nax2.scatter(X_pca_test[:,0],X_pca_test[:,1],cmap='rainbow')\nplt.show()","3b1e0b32":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","f2a34316":"count_classes = pd.value_counts(target, sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"histogram\")\nplt.xlabel(\"traget\")\nplt.ylabel(\"Frequency\")","49f0d4c9":"%%time\n\ndef run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n    scores = []\n    models = []\n    evals_results = {}  # to record eval results for plotting\n    folds = KFold(n_splits=splits)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n+1} started')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        model = lgb.train(\n            run_params, valid_names=[\"train\", \"valid\"], \n            train_set=lgb.Dataset(X_train, y_train ), \n            num_boost_round = num_boost_round,\n            valid_sets = [lgb.Dataset(X_valid, y_valid)],\n            verbose_eval = verbose_eval,\n            evals_result=evals_results,\n            early_stopping_rounds = early_stopping_rounds,\n        )\n\n        y_predicted = model.predict(X_valid)\n        score = roc_auc_score(y_valid, y_predicted)   \n        print(f'roc_auc_score: {score}')\n\n        models.append(model)\n        scores.append(score)\n    return scores, models, evals_results\n\n\nLEARNING_RATE = 0.00497\nMAX_DEPTH = -1\nNUM_LEAVES = 250    \nTOTAL_SPLITS = 6\nNUM_BOOST_ROUND = 4000\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE_EVAL = 250    \n    \nnegative = target.value_counts()[0]\npositive = target.value_counts()[1]\nscale_pos_weight = negative \/ positive    \n    \nrun_params = {\n    'verbose': -1, \n    'boosting_type': 'gbdt', \n    'objective': 'binary', \n    'metric': ['auc', 'binary_logloss'],\n    'learning_rate': LEARNING_RATE, \n    'num_leaves': NUM_LEAVES, \n    'scale_pos_weight':scale_pos_weight,\n    'feature_fraction': 0.5, \n    'bagging_fraction': 0.5, \n    #'bagging_freq': 4, \n    'max_depth': MAX_DEPTH, \n}\n\nFEATURES = [col for col in train.columns if col.startswith('f')]\nscores, models, evals_results = run_train(train, target, run_params, TOTAL_SPLITS, NUM_BOOST_ROUND, \n                                          VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))","7561147c":"ax = lgb.plot_metric(evals_results, metric='auc')\nplt.show()\n\nax = lgb.plot_metric(evals_results, metric='binary_logloss')\nplt.show()","ad6fdaa1":"predicted = []\nfor model in models:\n    predicted.append(model.predict(test))\n\navg_preds = np.zeros(len(predicted[0]))\nfor pred in predicted:\n    avg_preds += pred\navg_pred = avg_preds \/ len(models)","70d3cd5e":"submission['target'] = avg_pred\nsubmission.to_csv('submission.csv', index=False, float_format='%.6f')\nsubmission.head(20)","57980446":"## Plotting metrics recorded during training","24a2b70a":"## PCA","05e1e7ba":"## EDA","49395f22":"## Dealing with skew data"}}