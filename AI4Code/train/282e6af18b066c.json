{"cell_type":{"69005dbd":"code","b56c85e8":"code","cd2c3cd5":"code","8c4d1d7c":"code","cefd9cbe":"code","a5f850f2":"code","1328a461":"code","be886975":"code","960553a2":"code","a18f9e02":"code","0bd4076e":"code","6d4c514b":"code","2c8cb95c":"code","06107a7b":"code","de0f12af":"code","339d7aba":"code","5827fd22":"code","31b18aec":"code","24b20bd0":"code","e533e3a6":"code","8c37c9f4":"markdown","8a9f0b5c":"markdown","d7581e4b":"markdown","511a9665":"markdown","84179cd3":"markdown"},"source":{"69005dbd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string","b56c85e8":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf = pd.concat([train, test])","cd2c3cd5":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","8c4d1d7c":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Input\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","cefd9cbe":"# Create corpus that without stopwords\n# Each input will be converted to a list of words\n# Return corpus that contains all converted inputs\n\nstop=set(stopwords.words('english'))\n\ndef create_corpus(df):\n    corpus=[]\n    for tweet in df['text']:\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus=create_corpus(df)","a5f850f2":"# Import the pretrained GloVe, here is the 100-dimensional version\n# Retrive the information in txt file\n# Form a embedding dictionary, key is the word, and value is the corresponding embedding vector\n\nembedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","1328a461":"# The maximum length of each tweet is 50\nMAX_LEN=50\n\n# Tokenize the corpus, convert each input into tokens (each unique word will be represented by one token)\n# The argument 'num_words' can be assigned to the 'Tokenizer' class\n# It will keep the most common num_words-1 words based on word frequency\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(corpus)\nsequences=tokenizer.texts_to_sequences(corpus)\n\n# If the original text was longer than 50, truncate at the end\n# If the original text was shorter than 50, pad at the end\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n\n# word_index is a dictionary, the key is one unique word, the value is the corresponding token\nword_index=tokenizer.word_index\nnum_words=len(word_index)+1","be886975":"# Create embedding matrix\n# Initialize the embedding matrix, here the first dimension is the number of unique words, second dimension is the dimension of the GloVe we chose\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in word_index.items():\n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n\n# The embedding matrix represent each unique word in the corpus with a 1 by 100 vector.","960553a2":"# Model structure\n# Layers: Embedding - Dropout - LSTM - Dense\n# The output dimension of last layer is 1, since we are dealing binary classification here\n\nmodel=Sequential()\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimzer=Adam(learning_rate=1e-5)\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\nmodel.summary()","a18f9e02":"# Separate train and test data\ntrain_X=tweet_pad[:train.shape[0]]\nX_test=tweet_pad[train.shape[0]:]\ntrain_y = train['target']\n\n# Separate train and validation data\nX_train,X_val,y_train,y_val=train_test_split(train_X,train_y,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_val.shape)\n\n\nhistory = model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_val,y_val),verbose=2)","0bd4076e":"# Download the tokenizer from BERT\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","6d4c514b":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization","2c8cb95c":"# Prepare the text that feeds to bert layer\n# BERT needs some special format of the input, details here: https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        input_sequence = ['[CLS]'] + text + ['[SEP]']\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0]*pad_len\n        pad_masks = [1]*len(input_sequence) + [0]*pad_len\n        segment_ids = [0]*max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","06107a7b":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","de0f12af":"# Model structure\n# Layers: BERT - Dense\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","339d7aba":"# Load BERT from Tfhub\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","5827fd22":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","31b18aec":"# Save the best model during training\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16)","24b20bd0":"# Use the best model to predict\n\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)","e533e3a6":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","8c37c9f4":"# Submission","8a9f0b5c":"# Data Cleaning - optional","d7581e4b":"# BERT","511a9665":"This notebook used two different pretrained word embedding: GloVe(100 dimensional) and BERT.\n\nHelpful resources: <br>\nhttps:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\/data <br>\nhttps:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras <br>\nhttps:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub <br>\nhttps:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/ <br>\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/ <br>","84179cd3":"# GloVe"}}