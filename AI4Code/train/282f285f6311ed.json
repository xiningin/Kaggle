{"cell_type":{"2896caec":"code","dbeaf941":"code","194e5b06":"code","8524ade8":"code","55c12f92":"code","8a6a877c":"code","35e6ac21":"code","0c580088":"code","746ddd0d":"code","77a595c0":"code","bb70242c":"code","7b4b67af":"code","57420339":"code","c29261ad":"code","7c420b40":"code","1a4bf3e9":"code","562cb29b":"code","8a7b71a3":"code","6be7965d":"markdown","00a463bd":"markdown","2151c0c3":"markdown","31e817ee":"markdown","8d9eaa5a":"markdown","a304fd5e":"markdown","7c3f1b52":"markdown","235c15e1":"markdown","541d9a32":"markdown","b78dcc13":"markdown"},"source":{"2896caec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.widgets import Slider, Button, RadioButtons\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 6.0)  # set default size of plots\n#plt.rcParams['image.aspect'] = 1.3\nplt.rcParams['image.interpolation'] = 'nearest' # \u8bbe\u7f6e interpolation style\n#plt.rcParams['image.cmap'] = 'gray'\nplt.rcParams['savefig.dpi'] = 300  #\u56fe\u7247\u50cf\u7d20\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['font.size'] = 20\nplt.rcParams['font.weight'] = 'normal'\nplt.rcParams['lines.markersize'] = 8\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False #display only a left and bottom box border\nplt.rcParams['legend.edgecolor'] = 'white'\nplt.rcParams['axes.axisbelow'] = False\nplt.rcParams['grid.color'] = 'white'\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dbeaf941":"iris = pd.read_csv('..\/input\/Iris.csv', header = 0)\nprint(iris.info())\nprint(iris.describe())\nprint(iris.head())","194e5b06":"sns.relplot(\n    x=\"SepalLengthCm\",\n    y=\"SepalWidthCm\",\n    hue=\"Species\",\n    dashes=False,\n    markers=True,\n    kind=\"scatter\",\n    data=iris,\n    height=5,\n    aspect=1.3)","8524ade8":"sns.relplot(\n    x=\"PetalLengthCm\",\n    y=\"PetalWidthCm\",\n    hue=\"Species\",\n    dashes=False,\n    markers=True,\n    kind=\"scatter\",\n    data=iris,\n    height=5,\n    aspect=1.3)","55c12f92":"iris.drop('Id', axis=1).plot.hist(edgecolor='white', linewidth=1.2, alpha=0.5, bins=50)","8a6a877c":"iris.drop('Id', axis=1).hist(edgecolor='white', linewidth=1.2, grid=False, alpha=0.5)","35e6ac21":"iris_melt = iris.drop('Id', axis=1).melt(\n    id_vars='Species',\n    value_vars=iris.drop(['Id', 'Species'], axis=1).columns,\n    value_name=\"value\")\nsns.violinplot(\n    data=iris_melt, x='variable', y='value', hue='Species')\nplt.xticks(rotation=45)","0c580088":"sns.heatmap(iris.drop('Id', axis=1).corr(), annot=True, center=0, vmin=-1, vmax=1, cmap='RdBu_r')","746ddd0d":"pca = PCA(n_components=2) #PCA(n_components='mle') #\npca.fit(iris.drop(\"Species\",axis=1))\nprint(pca.explained_variance_ratio_) ","77a595c0":"iris_r = pca.transform(iris.drop(\"Species\",axis=1))\niris_r = pd.DataFrame(iris_r,columns=[\"PC1\",\"PC2\"])\nf = lambda x: round(x,1)\niris_r = iris_r.applymap(f)\nprint(iris_r.head())\niris_r = pd.concat([iris_r,iris[\"Species\"]],axis=1)\nplt.figure(figsize = (10,6))\nsns.stripplot(x=\"PC1\",y=\"PC2\",hue=\"Species\",data=iris_r)\nplt.xticks(rotation=45)","bb70242c":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split # to split the dataset for training and testing\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier","7b4b67af":"train, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train.shape)\nprint(test.shape)","57420339":"train_X = train[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm',\n                 'PetalWidthCm']]  # taking the training data features\ntrain_y = train.Species  # output of our training data\ntest_X = test[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm',\n                 'PetalWidthCm']]  # taking test data features\ntest_y = test.Species  #output value of test data","c29261ad":"model = svm.SVC()  #select the algorithm\nmodel.fit(\n    train_X, train_y\n)  # we train the algorithm with the training data and the training output\nprediction = model.predict(\n    test_X)  #now we pass the testing data to the trained algorithm\nprint('The accuracy of the SVM is:', metrics.accuracy_score(\n    prediction, test_y))  #now we check the accuracy of the algorithm.\n#we pass the predicted output by the model and the actual output","7c420b40":"model = LogisticRegression()\nmodel.fit(train_X, train_y)\nprediction = model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',\n      metrics.accuracy_score(prediction, test_y))","1a4bf3e9":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))\nprint(metrics.classification_report(test_y, prediction))\nprint(metrics.confusion_matrix(test_y, prediction))","562cb29b":"model=KNeighborsClassifier(n_neighbors=6) #this examines 3 neighbours for putting the new data into a class\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))","8a7b71a3":"a_index=list(range(1,11))\na=pd.Series()\nx=[1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_y)))\nplt.plot(a_index, a)\nplt.xticks(x)","6be7965d":"## 3. Decision Tree","00a463bd":"# 1.  Exploratory Data Analysis With Iris","2151c0c3":"## 2. Logistic Regression","31e817ee":"## 4. feature extraction","8d9eaa5a":"## 2. relationship petal_length vs. petal_width","a304fd5e":"## 1. Support Vector Machine (SVM)","7c3f1b52":"## 1. relationship sepal_length vs. sepal_width","235c15e1":"## 4. K-Nearest Neighbours","541d9a32":"## 3. distribution of length & width","b78dcc13":"# 2. Machine learning"}}