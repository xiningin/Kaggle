{"cell_type":{"d92e8d25":"code","83286d53":"code","3c661074":"code","aad6f7ab":"code","ccbb6c50":"code","4b1c1d5b":"code","05a2fac2":"code","1874fd82":"code","734cba01":"code","735b6672":"code","e95645e5":"code","2c162f47":"code","beb24591":"code","1416308d":"markdown","061a0424":"markdown","21eb02b8":"markdown","630915d3":"markdown","5777c27a":"markdown","406437eb":"markdown","4e95ebc9":"markdown","4740aa0e":"markdown","40055cc4":"markdown","0fbbfc48":"markdown"},"source":{"d92e8d25":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting graphs\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","83286d53":"dataset_cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=dataset_cols)","3c661074":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nword_bank = []\ndef preprocess(text):\n  review = re.sub('[^a-zA-Z]',' ',text) \n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  return ' '.join(review)\n","aad6f7ab":"dataset['text'] = dataset['text'].apply(lambda x: preprocess(x))","ccbb6c50":"print(dataset.head())","4b1c1d5b":"from sklearn.preprocessing import LabelEncoder\ny = dataset['target']\nle = LabelEncoder()\ny = le.fit_transform(y)","05a2fac2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size = 0.20, random_state = 0)","1874fd82":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 600)\nX_train_dtm = cv.fit_transform(X_train).toarray()","734cba01":"X_test_dtm = cv.transform(X_test).toarray()","735b6672":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train_dtm, y_train) ","e95645e5":"y_pred = classifier.predict(X_test_dtm)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","2c162f47":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","beb24591":"import seaborn as sns\nfig, ax = plt.subplots()\nsns.heatmap(confusion_matrix(y_test, y_pred, normalize='true'), annot=True, ax=ax)\nax.set_title('Confusion Matrix')\nax.set_ylabel('Real Value')\nax.set_xlabel('Predicted Value')\n\nplt.show()","1416308d":"## Bag of Words Model","061a0424":"## Splitting the dataset into Training and Test set","21eb02b8":"## Making the Confusion Matrix","630915d3":"## Cleaning the text","5777c27a":"## Training the Logistic Regression model on the Training set","406437eb":"## Predicting Test set results","4e95ebc9":"## Importing the Dataset","4740aa0e":"## Importing the Libraries","40055cc4":"## Encoding categorical data   ","0fbbfc48":"# Twitter Sentiment Analysis"}}