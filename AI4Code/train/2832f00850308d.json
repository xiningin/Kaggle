{"cell_type":{"f9f8d611":"code","0285f895":"code","8ff5d367":"code","765feaa9":"code","99525e41":"code","8dfe2165":"code","da755fca":"code","cd47d044":"code","6326a95a":"code","d6c9e051":"code","95031ddf":"code","3089e578":"code","c73c8f18":"markdown","3b30d00a":"markdown","42a95161":"markdown","f5934ebc":"markdown","64e9bc84":"markdown"},"source":{"f9f8d611":"import torch\ndevice = 'gpu' if torch.cuda.is_available() else 'cpu'\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nimport warnings\nwarnings.filterwarnings('ignore')\nimport optuna\nimport gc\nimport os\nroot_path = '\/kaggle\/input\/tabular-playground-series-mar-2021'","0285f895":"train = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(root_path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(root_path, 'sample_submission.csv'))\n\ndataset = pd.concat([train, test], axis = 0, ignore_index = True)\ntrain_len = len(train)\n\nlabel = LabelEncoder()\ncategorical_feature_columns = dataset.drop('id', 1).select_dtypes(exclude=['float64']).columns\n\nfor column in categorical_feature_columns:\n        label.fit(dataset[column])\n        dataset[column] = label.transform(dataset[column])\n\ncategorical_features = list(range(len(categorical_feature_columns)))\n\ntrain_preprocessed = dataset[:train_len]\ntest_preprocessed = dataset[train_len:]\n\nfeatures = train_preprocessed.drop(['id', 'target'], 1).columns.tolist()\n\nassert train_preprocessed.shape[1] == test_preprocessed.shape[1]\n\n#del train, test\ngc.collect()","8ff5d367":"#Set to False if you want to skip it\n\nOPTUNA_OPTIMIZATION = True\n\nN_SPLITS = 3 #Number of folds for validation\nN_TRIALS = 50 #Number of trials to find best hyperparameters\nTIMEOUT = 3600*2","765feaa9":"import tqdm\ndef objective(trial, cv=StratifiedKFold(N_SPLITS, shuffle = True, random_state = 7)):\n    \n    \n    param_lgb = {\n        \"random_state\": trial.suggest_int(\"random_state\", 1, 100),\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"device\" : device,\n        \"boosting_type\": \"gbdt\",\n        \"gpu_use_dp\": True,\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 10000),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    model = LGBMClassifier(**param_lgb)\n    \n    val_aucs = []\n    aucs = []\n    \n    for kfold, (train_idx, val_idx) in tqdm.tqdm(enumerate(cv.split(train_preprocessed[features].values, \n                                                                    train_preprocessed['target'].values))):\n        \n        model.fit(train_preprocessed.loc[train_idx, features], train_preprocessed.loc[train_idx, 'target'])\n        print('Fitted {}'.format(type(model).__name__))\n        val_true = train_preprocessed.loc[val_idx, 'target'].values\n        \n        preds = model.predict(train_preprocessed.loc[val_idx, features])\n        \n        auc = roc_auc_score(val_true, preds)\n        \n        print('Fold: {}\\t AUC: {}\\n'.format(kfold, auc))\n        aucs.append(auc)\n    \n    print('Average AUC: {}'.format(np.average(auc)))\n    return np.average(aucs)","99525e41":"if OPTUNA_OPTIMIZATION:\n    study = optuna.create_study(study_name = 'lgbm_parameter_opt', direction=\"maximize\")\n    #study.optimize(objective, n_trials=N_TRIALS) \n    study.optimize(objective, timeout=TIMEOUT, show_progress_bar=True) \n    \n    trial = study.best_trial\n    \n    print(\"  Value: {}\".format(trial.value))\n    \n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\nelse:\n    trial = {'reg_alpha': 28.07671346302542, 'reg_lambda': 3.7286097228210145e-05,\n             'max_depth': 21, 'num_leaves': 37, 'colsample_bytree': 0.13251356691552435, \n             'subsample': 0.36239658705576194, 'subsample_freq': 43,\n             'min_child_samples': 288}\n","8dfe2165":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_intermediate_values(study))","da755fca":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_optimization_history(study, target_name = 'Average Validation LogLoss'))","cd47d044":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_slice(study, target_name = 'Average Validation LogLoss'))","6326a95a":"if OPTUNA_OPTIMIZATION:\n    display(study.trials_dataframe())","d6c9e051":"if OPTUNA_OPTIMIZATION:\n    final_model = LGBMClassifier(**trial.params)\nelse:\n    final_model = LGBMClassifier(**trial)","95031ddf":"test_preds = []\n\nskf = StratifiedKFold(N_SPLITS, shuffle = True, random_state = 7)\naucs = []\nfor kfold, (train_idx, val_idx) in enumerate(skf.split(train_preprocessed[features].values, \n                                                      train_preprocessed['target'].values)):\n        \n        final_model.fit(train_preprocessed.loc[train_idx, features], \n                        train_preprocessed.loc[train_idx, 'target'])\n        print('Fitted {}'.format(type(final_model).__name__))\n        val_true = train.loc[val_idx, 'target'].values\n        \n        preds = final_model.predict(train_preprocessed.loc[val_idx, features])\n        \n        auc = roc_auc_score(val_true, preds)\n        aucs.append(auc)\n        print('Fold: {}\\t Validation AUC: {}\\n'.format(kfold, auc))\n        \n        test_preds.append(final_model.predict_proba(test_preprocessed[features])[:, 1])\n        \nprint(\"Best Parameters mean AUC: {}\".format(np.mean(aucs)))","3089e578":"test_predictions = np.mean(test_preds, axis = 0)\n\nassert len(test_predictions) == len(test)\n\nsample_submission['target'] = test_predictions\n\nsample_submission.to_csv(\"submission.csv\", index = False)","c73c8f18":"## Tabular Playground Series March 2021\n\n<img src=\"https:\/\/i.imgur.com\/uHVJtv0.png\">\n\n\n\n<br><br>\n\n### Notebook Contents:\n\n0. [**Imports, Data Loading and Preprocessing**](#loading)\n\n1. [**Optuna Hyperparameter Optimization**](#optuna)\n\n2. [**Submission**](#submission)\n","3b30d00a":"Best Params: \n    \n    'reg_alpha': 48.144730345953434 \n    'reg_lambda': 1.2350451395477777e-06 \n    'max_depth': 11 \n    'num_leaves': 147 \n    'colsample_bytree': 0.32482261861770284 \n    'subsample': 0.601096026343747 \n    'subsample_freq': 178 \n    'min_child_samples': 291","42a95161":"<a id=\"loading\"><\/a>\n\n##### 0. Imports, Data Loading and Preprocessing","f5934ebc":"<a id = \"submission\"><\/a>\n\n### Submission","64e9bc84":"<a id=\"optuna\"><\/a>\n\n### Optuna\n\nLook [here](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/) for reference about Optuna library. \n\nLook [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html) for a set of Lightgbm Classifier hyperparameters.\n\n\nSkip and go [here](#hyperparams) to find my best parameters."}}