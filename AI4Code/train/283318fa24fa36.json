{"cell_type":{"8b35a810":"code","8877b5a8":"code","67827063":"code","e83dd13e":"code","4e4ed625":"code","629ff578":"code","5b46f408":"code","dd602940":"code","0a3cb617":"code","808c1366":"code","c3d933f3":"code","aa1ffb5a":"code","fc7394ed":"code","37d28375":"code","b13c6449":"code","f5f5913b":"code","4c9ecf64":"code","9033fe96":"code","413df306":"code","c6613bae":"code","b40d619f":"code","cf55db90":"code","9741d91b":"code","ce3b91be":"code","1e37b867":"code","eecc0d7e":"code","cfbcb60c":"code","f554fd3f":"code","669d7240":"code","f9c3d64e":"code","3e504094":"markdown","b1beb202":"markdown","8ee0cdec":"markdown","ec9e1916":"markdown","5b890e32":"markdown","eb67acf6":"markdown","9611a76a":"markdown","c6ba3017":"markdown","6fc810b6":"markdown","0198eb9a":"markdown","0d50868c":"markdown","e87b6034":"markdown","4f42575c":"markdown","e8ce9d49":"markdown","4a083a1a":"markdown","9223d252":"markdown","2c970886":"markdown","7c89c2e9":"markdown","e1ffa99f":"markdown","6c03c304":"markdown","6219e119":"markdown","50ce768b":"markdown","6cf31e4c":"markdown","79810a6b":"markdown","8bd45cd2":"markdown","13b5a57c":"markdown","82862b06":"markdown","6baf4726":"markdown","6957def7":"markdown","ebf7096a":"markdown","5582a3ef":"markdown","03546230":"markdown","289623b4":"markdown","1f19e711":"markdown","076807da":"markdown","f8a43573":"markdown","60961c00":"markdown","28448280":"markdown","bdf9c43d":"markdown","d0912a43":"markdown"},"source":{"8b35a810":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8877b5a8":"df = pd.read_csv('..\/input\/crime-cleanedcsv\/crime.csv')\ndf.head()","67827063":"print(df.shape, df.drop_duplicates().shape)\ndf = df.drop_duplicates()","e83dd13e":"df.info()","4e4ed625":"df.describe()","629ff578":"df[\"SHOOTING\"].fillna(\"N\", inplace = True)\ndf[\"DAY_OF_WEEK\"] = pd.Categorical(df[\"DAY_OF_WEEK\"], \n              categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],\n              ordered=True)\ndf[\"Lat\"].replace(-1, None, inplace=True)\ndf[\"Long\"].replace(-1, None, inplace=True)\n\ndf.describe()","5b46f408":"def getDate(dateStr, numChar):\n    return dateStr[0:numChar]\n\ndf['DATE'] = df['OCCURRED_ON_DATE'].apply(getDate, numChar = 10)\ndf['YEARMONTH'] = df['OCCURRED_ON_DATE'].apply(getDate, numChar = 7)\n\ndf[['YEARMONTH', 'DATE', 'OCCURRED_ON_DATE']].head()","dd602940":"print('Num of records: {} \\nNum of events: {}'.format(df.shape[0], df[\"INCIDENT_NUMBER\"].nunique()))","0a3cb617":"print(\"It  seemes like there are {} records in database per 1 crime.\".format(round(df.shape[0]\/df[\"INCIDENT_NUMBER\"].nunique(),2)))","808c1366":"tmp = df.groupby(\"INCIDENT_NUMBER\")[\"YEAR\"].count().sort_values(ascending = False)\ntmp.head(10)","c3d933f3":"tmp.value_counts() #Index: num of records per crime, Values: num of occurences of such a case.","aa1ffb5a":"print('It occurs that {}% of our events are \"duplicated\" at least 2 times.'.format(round(100*(282517 - 254996) \/ 282517),2))","fc7394ed":"df[df[\"INCIDENT_NUMBER\"] == \"I162030584\"]","37d28375":"df[df[\"INCIDENT_NUMBER\"] == \"I152080623\"]","b13c6449":"timeOccurencesNormal = df[['INCIDENT_NUMBER','OCCURRED_ON_DATE', 'YEAR', 'MONTH', 'SHOOTING',\n                           'DAY_OF_WEEK', 'HOUR', 'DATE', 'YEARMONTH']]\ntimeOccurencesDedup  = df[['INCIDENT_NUMBER','OCCURRED_ON_DATE', 'YEAR', 'MONTH', 'SHOOTING',\n                           'DAY_OF_WEEK', 'HOUR', 'DATE', 'YEARMONTH']].drop_duplicates()\n\nprint('Sanity check for duplicates: ({}, {})'.format(df['INCIDENT_NUMBER'].nunique(), timeOccurencesDedup.shape[0]))","f5f5913b":"fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (18, 10))\n\nsns.countplot(timeOccurencesNormal[\"YEAR\"], color='lightblue', ax = axes[0,0] )\naxes[0,0].set_title(\"Number of crimes\")\nsns.countplot(timeOccurencesNormal[\"DAY_OF_WEEK\"], color='lightgreen', ax = axes[1,0])\naxes[1,0].set_title(\"Number of crimes\")\nsns.countplot(timeOccurencesNormal[\"HOUR\"], color = 'orange', ax = axes[2,0])\naxes[2,0].set_title(\"Number of crimes\")\n\nsns.countplot(timeOccurencesDedup[\"YEAR\"], color='lightblue', ax = axes[0,1] )\naxes[0,1].set_title(\"Number of crimes (deduplicated)\")\nsns.countplot(timeOccurencesDedup[\"DAY_OF_WEEK\"], color='lightgreen', ax = axes[1,1] )\naxes[1,1].set_title(\"Number of crimes (deduplicated)\")\nsns.countplot(timeOccurencesDedup[\"HOUR\"], color = 'orange', ax = axes[2,1] )\naxes[2,1].set_title(\"Number of crimes (deduplicated)\")\n\nplt.tight_layout()","4c9ecf64":"fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 7))\n\nsns.heatmap(pd.pivot_table(data = timeOccurencesNormal, index = \"DAY_OF_WEEK\", \n                              columns = \"HOUR\", values = \"INCIDENT_NUMBER\", aggfunc = 'count'), \n               cmap = 'Reds', ax = axes[0])\nsns.heatmap(pd.pivot_table(data = timeOccurencesDedup, index = \"DAY_OF_WEEK\", \n                              columns = \"HOUR\", values = \"INCIDENT_NUMBER\", aggfunc = 'count')\n               , cmap = 'Reds', ax = axes[1])","9033fe96":"fig = plt.figure(figsize=(18,6))\n\naxes = fig.add_axes([0.1,0.1,0.8,0.8])\naxes.plot(timeOccurencesNormal.groupby('DATE').count(), \n          c = 'blue', label = \"Original data\")\naxes.plot(timeOccurencesDedup.groupby('DATE').count(), \n          c = 'green', label = \"Dedup data\")\nplt.xticks(rotation = 90)\nplt.legend()\naxes.set_title(\"Number of crimes in a day\")\naxes.set_ylabel(\"Number of crimes\")\n\nlabelsX = timeOccurencesNormal.groupby('DATE').count().index[::30]\nplt.xticks(labelsX, rotation='vertical')\n\n#I've got duplicated legend here, so I used remedy:\n# https:\/\/stackoverflow.com\/questions\/19385639\/duplicate-items-in-legend-in-matplotlib\nhandles, labels = axes.get_legend_handles_labels() \ni = np.arange(len(labels))\nfilter = np.array([])\nunique_labels = list(set(labels))\nfor ul in unique_labels:\n    filter = np.append(filter, [i[np.array(labels) == ul][0]]) \n    \nhandles = [handles[int(f)] for f in filter] \nlabels = [labels[int(f)] for f in filter]\naxes.legend(handles, labels) ","413df306":"fig = plt.figure(figsize=(18,6))\n\naxes = fig.add_axes([0.1,0.1,0.8,0.8])\naxes.plot(timeOccurencesNormal.groupby('YEARMONTH').count(), \n          c = 'blue', label = \"Original data\")\naxes.plot(timeOccurencesDedup.groupby('YEARMONTH').count(), \n          c = 'green', label = \"Dedup data\")\nplt.xticks(rotation = 90)\nplt.legend()\naxes.set_title(\"Number of crimes in a month\")\naxes.set_ylabel(\"Number of crimes\")\n\n#I've got duplicated legend here, so I used remedy:\n# https:\/\/stackoverflow.com\/questions\/19385639\/duplicate-items-in-legend-in-matplotlib\nhandles, labels = axes.get_legend_handles_labels() \ni = np.arange(len(labels))\nfilter = np.array([])\nunique_labels = list(set(labels))\nfor ul in unique_labels:\n    filter = np.append(filter, [i[np.array(labels) == ul][0]]) \n    \nhandles = [handles[int(f)] for f in filter] \nlabels = [labels[int(f)] for f in filter]\naxes.legend(handles, labels)  ","c6613bae":"print('We have {}% of shooting crimes in all events (deduplicated situation).'.format(\n    round(100*timeOccurencesDedup[timeOccurencesDedup['SHOOTING'] == 'Y'].shape[0]\/timeOccurencesDedup.shape[0],2)))","b40d619f":"fig = plt.figure(figsize=(18,6))\n\naxes = fig.add_axes([0.1,0.1,0.8,0.8])\naxes.plot(timeOccurencesNormal[timeOccurencesNormal[\"SHOOTING\"] == \"Y\"].groupby('DATE').count(), \n          c = 'lightblue', label = \"Original data\")\naxes.plot(timeOccurencesDedup[timeOccurencesDedup[\"SHOOTING\"] == \"Y\"].groupby('DATE').count(), \n          c = 'black', label = \"Dedup data\")\nplt.xticks(rotation = 90)\nplt.legend()\naxes.set_title(\"Shooting crimes\")\naxes.set_ylabel(\"Number of crimes with shooting\")\n\nlabelsX = timeOccurencesNormal[timeOccurencesNormal[\"SHOOTING\"] == \"Y\"].groupby('DATE').count().index[::30]\nplt.xticks(labelsX, rotation='vertical')\n\n#I've got duplicated legend here, so I used remedy:\n# https:\/\/stackoverflow.com\/questions\/19385639\/duplicate-items-in-legend-in-matplotlib\nhandles, labels = axes.get_legend_handles_labels() \ni = np.arange(len(labels))\nfilter = np.array([])\nunique_labels = list(set(labels))\nfor ul in unique_labels:\n    filter = np.append(filter, [i[np.array(labels) == ul][0]]) \n    \nhandles = [handles[int(f)] for f in filter] \nlabels = [labels[int(f)] for f in filter]\naxes.legend(handles, labels) ","cf55db90":"fig = plt.figure(figsize=(18,6))\n\naxes = fig.add_axes([0.1,0.1,0.8,0.8])\naxes.plot(timeOccurencesNormal[timeOccurencesNormal[\"SHOOTING\"] == \"Y\"].groupby('YEARMONTH').count(), \n          c = 'blue', label = \"Original data\", marker = \"o\")\naxes.plot(timeOccurencesDedup[timeOccurencesDedup[\"SHOOTING\"] == \"Y\"].groupby('YEARMONTH').count(), \n          c = 'green', label = \"Dedup data\", marker=\"o\")\nplt.xticks(rotation = 90)\nplt.legend()\naxes.set_title(\"Shooting crimes\")\naxes.set_ylabel(\"Number of crimes with shooting\")\n\n#I've got duplicated legend here, so I used remedy:\n# https:\/\/stackoverflow.com\/questions\/19385639\/duplicate-items-in-legend-in-matplotlib\nhandles, labels = axes.get_legend_handles_labels() \ni = np.arange(len(labels))\nfilter = np.array([])\nunique_labels = list(set(labels))\nfor ul in unique_labels:\n    filter = np.append(filter, [i[np.array(labels) == ul][0]]) \n    \nhandles = [handles[int(f)] for f in filter] \nlabels = [labels[int(f)] for f in filter]\naxes.legend(handles, labels) ","9741d91b":"fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 7))\n\nsns.heatmap(pd.pivot_table(data = timeOccurencesNormal[timeOccurencesNormal[\"SHOOTING\"] == \"Y\"], index = \"DAY_OF_WEEK\", \n                              columns = \"HOUR\", values = \"INCIDENT_NUMBER\", aggfunc = 'count'), \n               cmap = 'Reds', ax = axes[0])\nsns.heatmap(pd.pivot_table(data = timeOccurencesDedup[timeOccurencesNormal[\"SHOOTING\"] == \"Y\"], index = \"DAY_OF_WEEK\", \n                              columns = \"HOUR\", values = \"INCIDENT_NUMBER\", aggfunc = 'count')\n               , cmap = 'Reds', ax = axes[1])","ce3b91be":"locationOccurencesNormal = df[['INCIDENT_NUMBER','DISTRICT', 'REPORTING_AREA', 'SHOOTING','Lat', 'Long']]\nlocationOccurencesDedup  = df[['INCIDENT_NUMBER','DISTRICT', 'REPORTING_AREA', 'SHOOTING','Lat', 'Long']].drop_duplicates()\nprint('Sanity check for duplicates: ({}, {})'.format(df['INCIDENT_NUMBER'].nunique(), locationOccurencesDedup.shape[0]))","1e37b867":"# Plot districts\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 7))\nsns.scatterplot(y='Lat',\n                x='Long',\n                hue='DISTRICT',\n                alpha=0.01,\n                data=locationOccurencesNormal, \n                ax = axes[0])\n#plt.ylim(locationOccurencesNormal['Long'].max(), locationOccurencesNormal['Long'].min())\nsns.scatterplot(y='Lat',\n                x='Long',\n                hue='DISTRICT',\n                alpha=0.01,\n                data=locationOccurencesDedup, \n                ax = axes[1])","eecc0d7e":"fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 7))\nsns.scatterplot(y = 'Lat',\n                x = 'Long',\n                alpha = 0.3,\n                data = locationOccurencesNormal[locationOccurencesNormal[\"SHOOTING\"]==\"Y\"], \n                ax = axes[0])\naxes[0].set_title(\"Crime locations\")\nsns.scatterplot(y = 'Lat',\n                x = 'Long',\n                alpha = 0.3,\n                data = locationOccurencesDedup[locationOccurencesDedup[\"SHOOTING\"]==\"Y\"], \n                ax = axes[1])\naxes[1].set_title(\"Crime locations (deduplicated)\")","cfbcb60c":"# Below I choose a YEAR column cause I would like to narrow the data processed \n# and this columns is nice -> doesn't have any null values \ntmp = df.groupby('INCIDENT_NUMBER')['YEAR'].count().sort_values(ascending = False)\ntmp = pd.DataFrame({'INCIDENT_NUMBER': tmp.index, 'NUM_RECORDS': tmp.values})\nseriousCrimes = df.merge(tmp[tmp['NUM_RECORDS'] > 5], on = 'INCIDENT_NUMBER', how = 'inner')\nseriousCrimes = seriousCrimes[['INCIDENT_NUMBER', 'Lat','Long']].drop_duplicates()[['Lat','Long']].dropna()","f554fd3f":"#!pip install folium\n# Used this tutorial: https:\/\/medium.com\/@bobhaffner\/folium-markerclusters-and-fastmarkerclusters-1e03b01cb7b1\nimport folium\nfrom folium.plugins import MarkerCluster\n\nsome_map = folium.Map(location = [seriousCrimes['Lat'].mean(), \n                                  seriousCrimes['Long'].mean()], \n                      zoom_start = 12)\nmc = MarkerCluster()\n#creating a Marker for each point. \nfor row in seriousCrimes.itertuples():\n    mc.add_child(folium.Marker(location = [row.Lat,  row.Long]))\n\nsome_map.add_child(mc)\n\nsome_map","669d7240":"locationTimeOccurencesDedup = df[['INCIDENT_NUMBER', 'SHOOTING','Lat', 'Long', 'DAY_OF_WEEK', 'HOUR']].drop_duplicates()\nprint('Sanity check for duplicates: ({}, {})'.format(df['INCIDENT_NUMBER'].nunique(), locationOccurencesDedup.shape[0]))","f9c3d64e":"fig = plt.figure(figsize=(18,10))\ng = sns.FacetGrid(data = locationTimeOccurencesDedup[(locationTimeOccurencesDedup['HOUR'] >= 1) & (locationTimeOccurencesDedup['HOUR'] <= 7)],\n                                                  row = 'HOUR', col = 'DAY_OF_WEEK')\ng = g.map(sns.scatterplot, 'Long', 'Lat', alpha=0.03)","3e504094":"Looking into details we can discover that one crime can have different UCR_PART and different offence categorization (OFFENSE_CODE, OFFENSE_DESCRIPTION), but duplicated OFFENSE_CODE_GROUP (eg. rows with index 290817, 290818). This can bring some significant changes in visualisations and interpreting. Thus it would be good to carefully deduplicate records depending on questions we would like to answear.","b1beb202":"First let's read in some libraries. Then load our data and explore them a little bit.","8ee0cdec":"### Join time and space","ec9e1916":"Thank you for reading this notebook. I hope you liked it (if so please upvote!) and will get inspired to dig into data in your near future. Maybe you would like to explore this data set a little further? :)\n\nIf you have any comments or questions please find me via Kaggle :)","5b890e32":"Preceding plots show crime locations in selected hour and day of week. I cut the hours to our 'peaceful' period to see if we have any dangerous districts during that time. I don't see any trend in that case. But we can notice that there are more dangerous spots (clubs?) for Weekends about 1-2 AM.","eb67acf6":"### Crimes in space","9611a76a":"### Why duplicated?","c6ba3017":"But what about complicated crimes? Those that include many law violations. In time dimension we could see them as differences in numbers, but here with locations it is not so easily seen... So let's visualise them! ","6fc810b6":"## Why this analysis differ?","0198eb9a":"We had 23 duplicates. Now we would like to know what type of columns do we have. This is also a quick check for lacking values.","0d50868c":"First and main conclusion is that there are far more shootings in the center of Boston than on the suburbs. But on the right plot those suburbs seem to be more safer ones, where shooting happens sporadically...","e87b6034":"Two best analyses of this dataset which I mentioned on the beggining of this notebook made an assumption that the records in our dataset are unique. They are. Howerev in my opinion crimes are NOT.","4f42575c":"If deduplication does not change \"anything\" in overall crimes seen in dimension of time maybe going into details will be more usefull? Let's dig into shooting crimes.","e8ce9d49":"Let's see if something changes in relations with time occurences.","4a083a1a":"Is there anything interesting we can get from joining time with space?","9223d252":"Ommiting a peak in Saturday morning (propably case of one day only) there is a slight trend of range of safer hours 2AM - 2PM in working days and again shifted 4AM - 2PM in Weekends, but it is not as clear as we've seen it in overall crimes. Also deduplication shows more 'outliers'.","2c970886":"### Crimes in time","7c89c2e9":"There is a column 'INCIDENT_NUMBER' having a high potential of being an identity number of an event. Considering that each crime has it's own ID we have some duplicates... How will it change our analysis and conclusions?","e1ffa99f":"It seems to change nothing but the scale of a plot. So there would be a difference in reporting, but conslusions about trends would stay the same.","6c03c304":"Now std seems about right. I would like to add some more data columns for further visualisation. \nWe have date in format 'YYYY-mm-DD HH:MM:SS', 'YYYY', 'mm', but it could be also useful to have it in 'YYYY-mm-DD' and 'YYYY-mm'.","6219e119":"# Crimes in Boston","50ce768b":"This notebook is based on assumption that INCIDENT_NUMBER contains a unique ID of a crime (before usage of any of the conclusions please be aware to confirm that with the data owner). I found previous analysises very useful, inspiring and exhaustive. However I wanted to share my view on duplicated records usecase. Although it could change the analysis very impactful it mainly occured in drops of numbers on this dataset. It can have more impact when going into details - especially type of crimes (which I haven't have time to explore further - maybe will catch up in the future), so I hope that will inspire people to dig deeper. Those 'duplicates' also created new opportunities for dependency analysis of connected types of crimes (tried to perform affinity analysis, but too little cooccurences appeared... :( ). I hope this will come with continuation, too!","6cf31e4c":"So when in the day is it better to have a gun? Are we free at weekends?","79810a6b":"There is an obvious lack of \"SHOOTING\" values and also some missing values in \"STREET\", \"Lat\", \"Long\". Despite that we also know that data are from Boston so \"Lat\" & \"Long\" should have litte std and above we see some \"-1\"s... Let's clean that.","8bd45cd2":"This time also plot day-by-day is very \"lousy\" and the main conclusion which I took from it is that there was maximum 4 shootings in one day instead of 10. But I would like to move to plot by 'YEARMONTH' which is a twist in our analysis. We can see that after deduplication trends has changed, especially in the 2018, where 'duplicated' analysis would report that number of shootings is decreasing where in fact it will be slowly increasing. Also on a blue line we have a maximum peak in December 2017 whereas the true one is in July 2015...","13b5a57c":"There is a difference between working days and weekends. Whereas from Monday to Friday a period of \"lighter\" hours with fewer crimes spans from 1\/2AM - 6\/7AM, in Weekends it seems to be shifted to 3AM - 7\/8AM (due to partying? ;) ).  There is also a buch of very \"busy\" hours (4PM - 6PM) during week which does not occur on Saturday and Sunday (stressed office workers comming out on the streets?). However still comparison between old and deduplicated version of data didn't bring any differences.","82862b06":"Maybe one crime can be assigned to different classes?","6baf4726":"Again we come to the situation where mainly number of events decreased after deduplication, but trends didn't change. \nOn the other hand the second plot shows that we have summer hypes of crimes and significant drop in their rate on the beggining of the year - and it is not a trend in one year only. For me it was more obvious way of seeing that than looking on bar plots aggregated by month without a year information or looking on noisy day-by-day plot above.","6957def7":"We can see that there are crimes that have over 10 records. Max of it is 13. Let's make a summary:","ebf7096a":"## Looking at data","5582a3ef":"Let's see what is inside two top \"complicated\" crimes.","03546230":"Check and eventually remove duplicates.","289623b4":"Let's move to space dimension now. We remember that at the first glance into our data we had some missing values in coordinates - it's good to mention here that those observations are ignored by plots. ","1f19e711":"Are there any time trends in number of crimes at all?","076807da":"Most complex crimes - not suprisingly - happen in the center of Boston...","f8a43573":"How does security in districts looks like?","60961c00":"As we can see there are some safer places. An interesting one is a blue district in the bottom of the plot seems to have only some main streets being very dangerous but not the rest - maybe there is a special structure of buildings (big companies, store houses etc.?). I will leave it for your future exploration. What is good to piont here is that the are no visible effects of deduplication. Let's go to shooting analysis to look for differences.","28448280":"## We came to an end...","bdf9c43d":"Crimes in Boston is a dataset from Kaggle ( https:\/\/www.kaggle.com\/AnalyzeBoston\/crimes-in-boston\/downloads\/crimes-in-boston.zip\/2 ) that I found very useful in practicing data exploratory and visualisation skills. Doing my 'exercise' I found two very inspiring notebooks which showed this data in a very convinient way:\nhttps:\/\/www.kaggle.com\/heesoo37\/boston-crimes-starter-eda\nhttps:\/\/www.kaggle.com\/frankkloster\/bostom-crimes-eda\nPlease do have a look at them cause I won't repeat all the steps done there. As I explored the dataset on my own I hit some new ideas which I would like to share with you :)","d0912a43":"Next I would like to look at dependencies between weekdays, hours and number of cases reported (this is something I haven't found in previous notebooks)."}}