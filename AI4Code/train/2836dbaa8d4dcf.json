{"cell_type":{"8a6337ce":"code","3b8feeed":"code","731d2d2f":"code","3415e1d1":"code","dab0a7c7":"code","7c30334c":"code","b79cb082":"code","cd27b30e":"code","37e0868b":"code","39e662d7":"code","b4367bbe":"code","dffe6b44":"code","2744c507":"code","9b3296fe":"markdown","44bd5653":"markdown","4be81bcc":"markdown","f9516c9e":"markdown","476bd459":"markdown","8ecb9d5f":"markdown"},"source":{"8a6337ce":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/","3b8feeed":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('white')\nsns.set(font_scale=1.2)\n\nimport os\nimport random\nimport sys\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","731d2d2f":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","3415e1d1":"df_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ndisplay(df_train.head(3))\nprint('train data size', df_train.shape)\ndf_train.drop(columns=[\"sig_id\"], inplace=True)\n\ndf_target_ns = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ndisplay(df_target_ns.head(3))\nprint('train target nonscored size', df_target_ns.shape)\n\n\ndf_target_s = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ndisplay(df_target_s.head(3))\nprint('train target scored size', df_target_s.shape)\ndf_target_s.drop(columns=[\"sig_id\"], inplace=True)\n\ndf_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ndisplay(df_test.head(3))\nprint('test data size', df_test.shape)\ndf_test.drop(columns=[\"sig_id\"], inplace=True)\n\n\ndf_sample = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ndisplay(df_sample.head(3))\nprint('sample submission size', df_sample.shape)\n\nremove_vehicle = False\n\nif remove_vehicle:\n    kept_index = df_train['cp_type']=='trt_cp'\n    df_train = df_train.loc[kept_index].reset_index(drop=True)\n    df_target_s = df_target_s.loc[kept_index].reset_index(drop=True)\n\ndf_train[\"cp_type\"] = (df_train[\"cp_type\"]==\"trt_cp\") + 0\ndf_train[\"cp_dose\"] = (df_train[\"cp_dose\"]==\"D1\") + 0\n\ndf_test[\"cp_type\"] = (df_test[\"cp_type\"]==\"trt_cp\") + 0\ndf_test[\"cp_dose\"] = (df_test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = df_test.values","dab0a7c7":"print(df_train.isnull().sum().any()) # True if there are missing values\nprint(df_train.info())","7c30334c":"g_features = [cols for cols in df_train.columns if cols.startswith('g-')]\n\ncolor = ['dimgray','navy','purple','orangered', 'red', 'green' ,'mediumorchid', 'khaki', 'salmon', 'blue','cornflowerblue','mediumseagreen']\n \ncolor_ind=0\nn_row = 6\nn_col = 3\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","b79cb082":"c_features = [cols for cols in df_train.columns if cols.startswith('c-')]\n\nn_row = 6\nn_col = 3\nn_sub = 1 \nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nplt.rcParams[\"legend.loc\"] = 'upper left'\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,c_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,c_features[i]].std()))])\n    \n    plt.xlabel(c_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","cd27b30e":"fig = plt.figure(figsize=(10,4))\nplt.subplots_adjust(right=1.3)\nplt.subplot(1, 2, 1)\nsns.countplot(df_train['cp_time'],palette='nipy_spectral')\nplt.subplot(1, 2, 2)\nsns.countplot(df_train['cp_dose'],palette='nipy_spectral')\nplt.show()","37e0868b":"MAX_EPOCH=200\ntabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )","39e662d7":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","b4367bbe":"scores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 5\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\ny_preds = []\ny_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(df_train, df_target_s)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = df_train.values[train_idx, :], df_target_s.values[train_idx, :]\n    X_val, y_val = df_train.values[val_idx, :], df_target_s.values[val_idx, :]\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=128,\n              num_workers=1, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n\n    ## save y to compute the CV later\n    y_preds.append(preds_val)\n    y_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\ny_preds_all = np.concatenate(y_preds)\ny_targets_all = np.concatenate(y_targets)\ntest_preds_all = np.stack(test_cv_preds)","dffe6b44":"aucs = []\nfor task_id in range(y_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=y_targets_all[:, task_id],\n                              y_score=y_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","2744c507":"all_feat = [col for col in df_sample.columns if col not in [\"sig_id\"]]\ndf_sample[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\ndf_sample.loc[df_test['cp_type']==0, df_sample.columns[1:]] = 0\ndf_sample.to_csv('submission.csv', index=None)\n\ndf_submit = pd.read_csv('.\/submission.csv')\ndf_submit.head()","9b3296fe":"# Submission","44bd5653":"### Gene expression","4be81bcc":"# Modelling","f9516c9e":"### cp_time and cp_dose","476bd459":"### Cell viability","8ecb9d5f":"### Getting data"}}