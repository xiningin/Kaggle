{"cell_type":{"550fceeb":"code","c40dde82":"code","b7914132":"code","8c8c8018":"code","091af4f9":"code","858a3d0f":"code","b9aaeef9":"code","261dd27d":"code","5bfd051d":"code","1ee0500a":"code","ef77ad74":"code","06dd46fc":"code","f4d5447d":"code","e3ea1d98":"code","929babae":"code","c7b06ff8":"code","fa487ff7":"code","41170746":"code","02a7d2f8":"code","bad73b25":"code","89ff2029":"code","3504eb3a":"code","46896eea":"code","99603ed5":"code","73c92762":"code","ad6d1f28":"code","ef64e23f":"code","31773c48":"code","c3a4dc88":"code","0788cf6e":"code","e8169227":"code","16ecf5b6":"code","a34b3537":"code","c83809c1":"code","e99e2a6b":"code","2f4c7095":"code","df3197f4":"code","48259a2d":"code","04bb1bdd":"code","58affc37":"code","2b2f05a7":"markdown","0586a37a":"markdown","72a0b27e":"markdown","366f9d23":"markdown","1c33c753":"markdown","c6e60ae0":"markdown","33a105a5":"markdown","195b99d3":"markdown","af06b372":"markdown","e1bf3868":"markdown","a6fd191e":"markdown","4eb52a90":"markdown","8b6270d6":"markdown","0b6646de":"markdown","58d6932d":"markdown","abf0b2c7":"markdown","776b3164":"markdown","46d1eb63":"markdown","1340bab2":"markdown","2d7fab3b":"markdown","8ee18c93":"markdown","4ebfa39b":"markdown","f69ccd68":"markdown","e9afc4af":"markdown","9f04c9a3":"markdown","3a60e0b2":"markdown","d92154ea":"markdown","86a6b9f1":"markdown","82f7ec7a":"markdown","53178e02":"markdown","29811a9c":"markdown","2685ba0e":"markdown","1a9f5da8":"markdown","0e80ccc2":"markdown","273f5f96":"markdown","ccc2ef93":"markdown"},"source":{"550fceeb":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import plot_confusion_matrix\nfrom scipy.stats import norm, boxcox\nfrom collections import Counter\nfrom scipy import stats\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","c40dde82":"dataset = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","b7914132":"dataset.head()","8c8c8018":"dataset.shape","091af4f9":"dataset.describe()\n","858a3d0f":"dataset.info()","b9aaeef9":"dataset.isnull().values.any()\n","261dd27d":"sns.set_style('whitegrid')\nplt.figure(figsize=(12, 6))\nsns.countplot(x=\"Outcome\", data=dataset, palette='husl');","5bfd051d":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True,linewidth=.8, mask=matrix, cmap=\"rocket\");\n","1ee0500a":"def boxPlotter(columnName):\n    if not columnName == 'Outcome':\n        sns.catplot(x=\"Outcome\", y=columnName, data=dataset, kind=\"box\");\n","ef77ad74":"for column in dataset.columns:\n    boxPlotter(column)","06dd46fc":"pregnancy_count = dataset[\"Pregnancies\"].value_counts().reset_index()\npregnancy_count","f4d5447d":"plt.figure(figsize=(30, 10))\nplt.style.use(\"ggplot\")\nsns.barplot(x=pregnancy_count[\"index\"], y=pregnancy_count[\"Pregnancies\"]);\nplt.title(\"TYPE OF PREGNANCIES WITH COUNT\", fontsize=20)\nplt.xlabel(\"PREGNANCIES\", fontsize=20)\nplt.ylabel(\"COUNT\", fontsize=20)\nplt.show()\n","e3ea1d98":"def distributionPlot(columnName):\n    if not columnName == 'Outcome':\n        plt.figure()\n        ax = sns.distplot(dataset[columnName][dataset.Outcome == 1],\n                        color=\"darkturquoise\", rug=True)\n        sns.distplot(dataset[columnName][dataset.Outcome == 0], color=\"lightcoral\", rug=True);\n        plt.legend(['Diabetes', 'No Diabetes']) ","929babae":"for column in dataset.columns:\n    distributionPlot(column)","c7b06ff8":"sns.pairplot(dataset, hue=\"Outcome\", palette=\"husl\");","fa487ff7":"def skewnessCorrector(columnName):\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n    plt.title(columnName.capitalize() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    print('''After Correcting''')\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n    plt.title(columnName.capitalize() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n","41170746":"skewColumnList = ['DiabetesPedigreeFunction', 'Age']\nfor columns in skewColumnList:\n    skewnessCorrector(columns)\n","02a7d2f8":"!pip install pandas_profiling","bad73b25":"ProfileReport(dataset)","89ff2029":"X = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values","3504eb3a":"X","46896eea":"y","99603ed5":"X.shape","73c92762":"y.shape","ad6d1f28":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\n","ef64e23f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","31773c48":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components = 2)\n# X_train = pca.fit_transform(X_train)\n# X_test = pca.transform(X_test)","c3a4dc88":"# from sklearn.decomposition import KernelPCA\n# kpca = KernelPCA(n_components = 1, kernel = 'rbf')\n# X_train = kpca.fit_transform(X_train)\n# X_test = kpca.transform(X_test)","0788cf6e":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=1)\nX_train = lda.fit_transform(X_train, y_train)\nX_test = lda.transform(X_test)","e8169227":"accuracy_scores = {}\ndef predictor(predictor, params):\n    global accuracy_scores\n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n\n    elif predictor == 'knn':\n        print('TrainingK-Nearest Neighbours on Training Set')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n\n    elif predictor == 'dt':\n        print('Training LDecision Tree Classifier on Training Set')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n\n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n\n    classifier.fit(X_train, y_train)\n\n    print('''Prediciting Test Set Result''')\n    y_pred = classifier.predict(X_test)\n    result = np.concatenate((y_pred.reshape(len(y_pred), 1),\n                             y_test.reshape(len(y_test), 1)), 1)\n    print(result, '\\n')\n    print('''Making Confusion Matrix''')\n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm, '\\n')\n    plot_confusion_matrix(classifier, X_test, y_test, cmap=\"pink\")\n    print('True Positives :', cm[0][0])\n    print('False Positives :', cm[0][1])\n    print('False Negatives :', cm[1][0])\n    print('True Negatives :', cm[0][1], '\\n')\n\n    print('''Classification Report''')\n    print(classification_report(y_test, y_pred,\n          target_names=['0', '1'], zero_division=1))\n\n    print('''Evaluating Model Performance''')\n    accuracy = accuracy_score(y_test, y_pred)\n    print(accuracy, '\\n')\n\n    print('''Applying K-Fold Cross validation''')\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(\n        estimator=classifier, X=X_train, y=y_train, cv=10)\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    accuracy_scores[classifier] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100), '\\n')\n","16ecf5b6":"predictor('lr', {'penalty': 'l1', 'solver': 'liblinear'})","a34b3537":"predictor('svm', {'C': .25, 'gamma': 0.4,\n          'kernel': 'linear', 'random_state': 0})","c83809c1":"predictor('svm', {'C': 0.25, 'gamma': 0.4, 'kernel': 'rbf', 'random_state': 0})","e99e2a6b":"predictor('knn', {'algorithm': 'auto', 'n_jobs': 1,\n          'n_neighbors': 9, 'weights': 'uniform'})","2f4c7095":"predictor('dt', {'criterion': 'gini', 'max_features': 'auto',\n          'splitter': 'best', 'random_state': 0})","df3197f4":"predictor('nb', {})","48259a2d":"predictor('rfc', {'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 50, 'random_state': 0})","04bb1bdd":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))\n","58affc37":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVC',\n               'K-SVC', 'KNN', 'Decisiontree', 'GaussianNB', 'RandomForest']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');","2b2f05a7":"# Loading Dataset","0586a37a":"### Applying LDA","72a0b27e":"## Plotting Bar Chart for Accuracies of different classifiers","366f9d23":"## Plotting Pairplot","1c33c753":"## Accuracy After HyperParamTuning and Without Applying Dimensionality Reduction \n- LogisticRegression: 76.87466948704389,\n- SVC: 77.20253833950291,\n- K-SVC: 76.05499735589636,\n- KNeighborsClassifier: 72.96932839767318,\n- DecisionTreeClassifier: 71.64992067689053,\n- GaussianNB: 75.08461131676361,\n- RandomForestClassifier: 76.53886832363828","c6e60ae0":"## Splitting Dataset into Training Set and Test Set","33a105a5":"## Dimensionality Reduction","195b99d3":"## Plotting Distribution for each Column in dataset","af06b372":"### Applying PCA","e1bf3868":"## Training SVM on Training Set","a6fd191e":"## Skewness Correction\nI found out there were some columns with skewness in the dataset. Here, I'm trying to correct that Skewness","4eb52a90":"## Plotting Count for Qualities","8b6270d6":"## Training K-Nearest Neighbours on Training Set","0b6646de":"### Applying Kernel PCA","58d6932d":"# 2) Using Pandas Profiling","abf0b2c7":"## Visualising Numerical Data","776b3164":"# Exploratory Data Analysis\n","46d1eb63":"## Training Logistic Regression on Training Set","1340bab2":"### Finding which Classifier performed best","2d7fab3b":"## Training Decision Tree on Training Set","8ee18c93":"# Importing Libraries","4ebfa39b":"## Training Kernel SVM on Training Set","f69ccd68":"## Accuracy After HyperParamTuning and Applying PCA\n- LogisticRegression(: 71.48863035430988,\n- SVC: 71.48598625066103,\n- K-SVC: 71.65520888418826,\n- KNeighborsClassifier: 67.42728714965625,\n- DecisionTreeClassifier: 63.85510312004231,\n- GaussianNB: 72.1390798519302,\n- RandomForestClassifier: 64.48704389212057","e9afc4af":"### Type of Pregnancies with Count","9f04c9a3":"## Training Random Forest Classifier on Training Set","3a60e0b2":"## Accuracy After HyperParamTuning and Applying PCA\n- LogisticRegression: 70.35166578529879,\n- SVC: 68.71760973030143,\n- K-SVC: 65.31200423056583,\n- KNeighborsClassifier: 66.13167636171339,\n- DecisionTreeClassifier: 64.68270756213644,\n- GaussianNB: 71.82707562136436,\n- RandomForestClassifier: 67.26335272342675","d92154ea":"## Training Naive Bayes on Training Set","86a6b9f1":"# Training Classifiers on Training Set and drawing Inference","82f7ec7a":"# Summary\n- K-SVC performed best on this data set with an accuracy of 78.33%\n- Logisitic Regression was just behind with an accuracy of an accuracy of 77.35% \n# **Please give your feedback by commenting below.**","53178e02":"## Standardizing Independent Variables","29811a9c":"## Accuracy Before HyperParamTuning and Without Applying Dimensionality Reduction\n- LogisticRegression: 76.87466948704389,\n- SVC: 77.20253833950291,\n- K-SVC: 76.05499735589636,\n- KNeighborsClassifier: 72.31623479640402,\n- DecisionTreeClassifier: 68.725542041248,\n- GaussianNB: 75.08461131676361,\n- RandomForestClassifier: 75.06874669487044}","2685ba0e":"## Accuracy After Applying LDA and Again HyperParamTuning\n- LogisticRegression: 77.35854045478582,\n- SVC: 77.03331570597568,\n- K-SVC: 78.33685880486514,\n- KNeighborsClassifie: 76.55208884188261,\n- DecisionTreeClassifier: 72.15494447382338,\n- GaussianNB: 77.03067160232682,\n- RandomForestClassifier: 72.15494447382338","1a9f5da8":"## Finding Correlation among the variables","0e80ccc2":"## 1) Using Manual Methods","273f5f96":"# Data Preprocessing","ccc2ef93":"## Accuracy After HyperParamTuning and Applying LDA\n- LogisticRegression: 77.35854045478582,\n- SVC: 77.03331570597568,\n- K-SVC: 78.01427815970385,\n- KNeighborsClassifier: 76.22157588577473,\n- DecisionTreeClassifier: 72.15494447382338,\n- GaussianNB: 77.03067160232682,\n- RandomForestClassifier: 72.15494447382338"}}