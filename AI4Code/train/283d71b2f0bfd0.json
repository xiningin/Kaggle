{"cell_type":{"e2cb9fda":"code","d0fa0453":"code","021b442d":"code","03240a05":"code","7f294b23":"code","aed22c69":"code","6de25522":"code","49a4e7c2":"code","c3841c57":"code","b0ceea1e":"code","09652766":"code","d033db1b":"code","0edef185":"code","3a5d105d":"code","628e2112":"code","2ef10810":"code","d44d56b3":"code","806e731f":"code","5b74920e":"code","cba54f99":"code","ee83b675":"code","770ebe5b":"code","d8995b62":"code","78d89a5d":"code","d765a47e":"code","c9f204a0":"markdown","bee64492":"markdown","f10b25d7":"markdown","65577fdb":"markdown","69863cd0":"markdown","fb680ab0":"markdown","6077d46e":"markdown","4ddf3401":"markdown","1ba56784":"markdown","0a1bf041":"markdown","86707671":"markdown","737e901b":"markdown","1eef46ef":"markdown","6b635784":"markdown","eb85f068":"markdown","de14f028":"markdown","8ea2f662":"markdown","3da50190":"markdown","d71e3ce5":"markdown","a2fc64ac":"markdown","565bc9a0":"markdown","7e510a9e":"markdown","09c89c2c":"markdown","497e7b27":"markdown","93a5c6e5":"markdown","c64193b3":"markdown","b8509238":"markdown","ec221b90":"markdown","031da703":"markdown","58f25345":"markdown","51a205e8":"markdown"},"source":{"e2cb9fda":"import tensorflow as tf\nfrom skimage.color import rgb2gray\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, add, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport numpy as np\nimport pandas as pd\nimport glob\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\nnp.random.seed(101)","d0fa0453":"import re\nnumbers = re.compile(r'(\\d+)')\ndef numericalSort(value):\n    parts = numbers.split(value)\n    parts[1::2] = map(int, parts[1::2])\n    return parts","021b442d":"filelist_trainx = sorted(glob.glob('..\/input\/*\/trainx\/*.bmp'), key=numericalSort)\nX_train = np.array([np.array(Image.open(fname)) for fname in filelist_trainx])\n\nfilelist_trainy = sorted(glob.glob('..\/input\/*\/trainy\/*.bmp'), key=numericalSort)\nY_train = np.array([np.array(Image.open(fname)) for fname in filelist_trainy])","03240a05":"plt.figure(figsize=(20,9))\nplt.subplot(2,4,1)\nplt.imshow(X_train[0])\nplt.subplot(2,4,2)\nplt.imshow(X_train[3])\nplt.subplot(2,4,3)\nplt.imshow(X_train[54])\nplt.subplot(2,4,4)\nplt.imshow(X_train[77])\nplt.subplot(2,4,5)\nplt.imshow(X_train[100])\nplt.subplot(2,4,6)\nplt.imshow(X_train[125])\nplt.subplot(2,4,7)\nplt.imshow(X_train[130])\nplt.subplot(2,4,8)\nplt.imshow(X_train[149])\nplt.show()","7f294b23":"plt.figure(figsize=(20,9))\nplt.subplot(2,4,1)\nplt.imshow(Y_train[0], cmap = plt.cm.binary_r)\nplt.subplot(2,4,2)\nplt.imshow(Y_train[3], cmap = plt.cm.binary_r)\nplt.subplot(2,4,3)\nplt.imshow(Y_train[54], cmap = plt.cm.binary_r)\nplt.subplot(2,4,4)\nplt.imshow(Y_train[77], cmap = plt.cm.binary_r)\nplt.subplot(2,4,5)\nplt.imshow(Y_train[100], cmap = plt.cm.binary_r)\nplt.subplot(2,4,6)\nplt.imshow(Y_train[125], cmap = plt.cm.binary_r)\nplt.subplot(2,4,7)\nplt.imshow(Y_train[130], cmap = plt.cm.binary_r)\nplt.subplot(2,4,8)\nplt.imshow(Y_train[149], cmap = plt.cm.binary_r)\nplt.show()","aed22c69":"def jaccard_distance(y_true, y_pred, smooth=100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return (1 - jac)","6de25522":"def iou(y_true, y_pred, smooth = 100):\n    intersection = K.sum(K.abs(y_true * y_pred))\n    sum_ = K.sum(K.square(y_true)) + K.sum(K.square(y_pred))\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return jac","49a4e7c2":"def dice_coe(y_true, y_pred, smooth = 100):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)","c3841c57":"def dice_coef_loss(y_true, y_pred):\n    return -dice_coe(y_true, y_pred)","b0ceea1e":"def precision(y_true, y_pred):\n    '''Calculates the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    '''\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision","09652766":"def recall(y_true, y_pred):\n    '''Calculates the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    '''\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall","d033db1b":"def accuracy(y_true, y_pred):\n    '''Calculates the mean accuracy rate across all predictions for binary\n    classification problems.\n    '''\n    return K.mean(K.equal(y_true, K.round(y_pred)))","0edef185":"def random_rotation(x_image, y_image):\n    rows_x,cols_x, chl_x = x_image.shape\n    rows_y,cols_y = y_image.shape\n    rand_num = np.random.randint(-40,40)\n    M1 = cv2.getRotationMatrix2D((cols_x\/2,rows_x\/2),rand_num,1)\n    M2 = cv2.getRotationMatrix2D((cols_y\/2,rows_y\/2),rand_num,1)\n    x_image = cv2.warpAffine(x_image,M1,(cols_x,rows_x))\n    y_image = cv2.warpAffine(y_image.astype('float32'),M2,(cols_y,rows_y))\n    return x_image, y_image.astype('int')\n\ndef horizontal_flip(x_image, y_image):\n    x_image = cv2.flip(x_image, 1)\n    y_image = cv2.flip(y_image.astype('float32'), 1)\n    return x_image, y_image.astype('int')","3a5d105d":"def img_augmentation(x_train, y_train):\n    x_rotat = []\n    y_rotat = []\n    x_flip = []\n    y_flip = []\n    for idx in range(len(x_train)):\n        x,y = random_rotation(x_train[idx], y_train[idx])\n        x_rotat.append(x)\n        y_rotat.append(y)\n        x,y = horizontal_flip(x_train[idx], y_train[idx])\n        x_flip.append(x)\n        y_flip.append(y)\n    return np.array(x_rotat), np.array(y_rotat), np.array(x_flip), np.array(y_flip)","628e2112":"x_rotated, y_rotated, x_flipped, y_flipped = img_augmentation(X_train, Y_train)","2ef10810":"x_train_full = np.concatenate([X_train, x_rotated, x_flipped])\ny_train_full = np.concatenate([Y_train, y_rotated, y_flipped])","d44d56b3":"img_num = 7\nplt.figure(figsize=(12,12))\nplt.subplot(3,2,1)\nplt.imshow(x_train_full[img_num])\nplt.title('Original Image')\nplt.subplot(3,2,2)\nplt.imshow(y_train_full[img_num], plt.cm.binary_r)\nplt.title('Original Mask')\nplt.subplot(3,2,3)\nplt.imshow(x_train_full[img_num+1])\nplt.title('Rotated Image')\nplt.subplot(3,2,4)\nplt.imshow(y_train_full[img_num+1], plt.cm.binary_r)\nplt.title('Rotated Mask')\nplt.subplot(3,2,5)\nplt.imshow(x_train_full[img_num+2])\nplt.title('Flipped Image')\nplt.subplot(3,2,6)\nplt.imshow(y_train_full[img_num+2], plt.cm.binary_r)\nplt.title('Flipped Mask')\nplt.show()","806e731f":"#x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size = 0.20, random_state = 101)\nkf = KFold(n_splits = 5, shuffle=False)","5b74920e":"# Number of image channels (for example 3 in case of RGB, or 1 for grayscale images)\nINPUT_CHANNELS = 3\n# Number of output masks (1 in case you predict only one type of objects)\nOUTPUT_MASK_CHANNELS = 1\n# Pretrained weights\n","cba54f99":"def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1), activation='relu', name=None):\n    '''\n    2D Convolutional layers\n    \n    Arguments:\n        x {keras layer} -- input layer \n        filters {int} -- number of filters\n        num_row {int} -- number of rows in filters\n        num_col {int} -- number of columns in filters\n    \n    Keyword Arguments:\n        padding {str} -- mode of padding (default: {'same'})\n        strides {tuple} -- stride of convolution operation (default: {(1, 1)})\n        activation {str} -- activation function (default: {'relu'})\n        name {str} -- name of the layer (default: {None})\n    \n    Returns:\n        [keras layer] -- [output layer]\n    '''\n\n    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, use_bias=False)(x)\n    x = BatchNormalization(axis=3, scale=False)(x)\n\n    if(activation == None):\n        return x\n\n    x = Activation(activation, name=name)(x)\n\n    return x\n\n\ndef trans_conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(2, 2), name=None):\n    '''\n    2D Transposed Convolutional layers\n    \n    Arguments:\n        x {keras layer} -- input layer \n        filters {int} -- number of filters\n        num_row {int} -- number of rows in filters\n        num_col {int} -- number of columns in filters\n    \n    Keyword Arguments:\n        padding {str} -- mode of padding (default: {'same'})\n        strides {tuple} -- stride of convolution operation (default: {(2, 2)})\n        name {str} -- name of the layer (default: {None})\n    \n    Returns:\n        [keras layer] -- [output layer]\n    '''\n\n    x = Conv2DTranspose(filters, (num_row, num_col), strides=strides, padding=padding)(x)\n    x = BatchNormalization(axis=3, scale=False)(x)\n    \n    return x\n\n\ndef MultiResBlock(U, inp, alpha = 1.67):\n    '''\n    MultiRes Block\n    \n    Arguments:\n        U {int} -- Number of filters in a corrsponding UNet stage\n        inp {keras layer} -- input layer \n    \n    Returns:\n        [keras layer] -- [output layer]\n    '''\n\n    W = alpha * U\n\n    shortcut = inp\n\n    shortcut = conv2d_bn(shortcut, int(W*0.167) + int(W*0.333) +\n                         int(W*0.5), 1, 1, activation=None, padding='same')\n\n    conv3x3 = conv2d_bn(inp, int(W*0.167), 3, 3,\n                        activation='relu', padding='same')\n\n    conv5x5 = conv2d_bn(conv3x3, int(W*0.333), 3, 3,\n                        activation='relu', padding='same')\n\n    conv7x7 = conv2d_bn(conv5x5, int(W*0.5), 3, 3,\n                        activation='relu', padding='same')\n\n    out = concatenate([conv3x3, conv5x5, conv7x7], axis=3)\n    out = BatchNormalization(axis=3)(out)\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out)\n\n    return out\n\n\ndef ResPath(filters, length, inp):\n    '''\n    ResPath\n    \n    Arguments:\n        filters {int} -- [description]\n        length {int} -- length of ResPath\n        inp {keras layer} -- input layer \n    \n    Returns:\n        [keras layer] -- [output layer]\n    '''\n\n\n    shortcut = inp\n    shortcut = conv2d_bn(shortcut, filters, 1, 1,\n                         activation=None, padding='same')\n\n    out = conv2d_bn(inp, filters, 3, 3, activation='relu', padding='same')\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out)\n\n    for i in range(length-1):\n\n        shortcut = out\n        shortcut = conv2d_bn(shortcut, filters, 1, 1,\n                             activation=None, padding='same')\n\n        out = conv2d_bn(out, filters, 3, 3, activation='relu', padding='same')\n\n        out = add([shortcut, out])\n        out = Activation('relu')(out)\n        out = BatchNormalization(axis=3)(out)\n\n    return out\n\n\ndef MultiResUnet(input_size=(256,256,1)):\n    '''\n    MultiResUNet\n    \n    Arguments:\n        height {int} -- height of image \n        width {int} -- width of image \n        n_channels {int} -- number of channels in image\n    \n    Returns:\n        [keras model] -- MultiResUNet model\n    '''\n\n\n    inputs = Input(input_size)\n\n    mresblock1 = MultiResBlock(32, inputs)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(mresblock1)\n    mresblock1 = ResPath(32, 4, mresblock1)\n\n    mresblock2 = MultiResBlock(32*2, pool1)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(mresblock2)\n    mresblock2 = ResPath(32*2, 3, mresblock2)\n\n    mresblock3 = MultiResBlock(32*4, pool2)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(mresblock3)\n    mresblock3 = ResPath(32*4, 2, mresblock3)\n\n    mresblock4 = MultiResBlock(32*8, pool3)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(mresblock4)\n    mresblock4 = ResPath(32*8, 1, mresblock4)\n\n    mresblock5 = MultiResBlock(32*16, pool4)\n\n    up6 = concatenate([Conv2DTranspose(\n        32*8, (2, 2), strides=(2, 2), padding='same')(mresblock5), mresblock4], axis=3)\n    mresblock6 = MultiResBlock(32*8, up6)\n\n    up7 = concatenate([Conv2DTranspose(\n        32*4, (2, 2), strides=(2, 2), padding='same')(mresblock6), mresblock3], axis=3)\n    mresblock7 = MultiResBlock(32*4, up7)\n\n    up8 = concatenate([Conv2DTranspose(\n        32*2, (2, 2), strides=(2, 2), padding='same')(mresblock7), mresblock2], axis=3)\n    mresblock8 = MultiResBlock(32*2, up8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(\n        2, 2), padding='same')(mresblock8), mresblock1], axis=3)\n    mresblock9 = MultiResBlock(32, up9)\n\n    conv10 = conv2d_bn(mresblock9, 1, 1, 1, activation='sigmoid')\n    \n    model = Model(inputs=[inputs], outputs=[conv10])\n\n    return model","ee83b675":"losses = []\naccuracies = []\nious = []\ndice_cos = []\nprecisions = []\nrecalls = []\nhistories = []\n\ny_train_full = y_train_full[:, :, :, np.newaxis]\n\nfor j, (train_idx, val_idx) in enumerate(kf.split(x_train_full, y_train_full)):\n    \n    print('\\nFold ',j)\n    X_train_cv = x_train_full[train_idx]\n    y_train_cv = y_train_full[train_idx]\n    X_valid_cv = x_train_full[val_idx]\n    y_valid_cv= y_train_full[val_idx]\n    \n    model = MultiResUnet(input_size = (224, 224, INPUT_CHANNELS))\n\n    model.compile(optimizer= Adam(lr = 1e-5), loss= dice_coef_loss,\n                  metrics=[iou, dice_coe, precision, recall, accuracy])\n    \n    model_checkpoint = ModelCheckpoint(str(j+1) + '_skin_leison.hdf5', \n                                       monitor='loss', \n                                       verbose=1, \n                                       save_best_only=True)\n    \n    callbacks_list = [model_checkpoint]\n    history = model.fit(X_train_cv,\n                     y_train_cv,\n                     epochs= 40,\n                     callbacks = callbacks_list,\n                     batch_size= 16,\n                     validation_data=(X_valid_cv, y_valid_cv))\n    \n    model = load_model(str(j+1) + '_skin_leison.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss,\n                                                                           'iou': iou, 'precision': precision, 'recall': recall,\n                                                                           'accuracy': accuracy, 'dice_coe': dice_coe})\n\n    results = model.evaluate(X_valid_cv, y_valid_cv)\n    results = dict(zip(model.metrics_names,results))\n                   \n    accuracies.append(results['accuracy'])\n    losses.append(results['loss'])\n    ious.append(results['iou'])\n    dice_cos.append(results['dice_coe'])\n    precisions.append(results['precision'])\n    recalls.append(results['recall'])\n    histories.append(history)","770ebe5b":"for h, history in enumerate(histories):\n\n    keys = history.history.keys()\n    fig, axs = plt.subplots(1, len(keys)\/\/2, figsize = (25, 5))\n    fig.suptitle('No. ' + str(h+1) + ' Fold Results', fontsize=30)\n\n    for k, key in enumerate(list(keys)[:len(keys)\/\/2]):\n        training = history.history[key]\n        validation = history.history['val_' + key]\n\n        epoch_count = range(1, len(training) + 1)\n\n        axs[k].plot(epoch_count, training, 'r--')\n        axs[k].plot(epoch_count, validation, 'b-')\n        axs[k].legend(['Training ' + key, 'Validation ' + key])","d8995b62":"#model_0.load_weights('unet_1_epoch.h5')\n\nprint('average accuracy : ', np.mean(np.array(accuracies)), '+-', np.std(np.array(accuracies)))\nprint('average loss : ', np.mean(np.array(losses)), '+-', np.std(np.array(losses)))\nprint('average iou : ', np.mean(np.array(ious)), '+-', np.std(np.array(ious)))\nprint('average dice_coe : ', np.mean(np.array(dice_cos)), '+-', np.std(np.array(dice_cos)))\nprint('average precision : ', np.mean(np.array(precisions)), '+-', np.std(np.array(precisions)))\nprint('average recall : ', np.mean(np.array(recalls)), '+-', np.std(np.array(recalls)))","78d89a5d":"model = load_model('1_skin_leison.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss,\n                                                           'iou': iou, 'precision': precision, 'recall': recall,\n                                                           'accuracy': accuracy, 'dice_coe': dice_coe})","d765a47e":"for i in range(3):\n    index=np.random.randint(1,300)\n    pred=model.predict(x_train_full[index][np.newaxis, :, :, :])\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(x_train_full[index])\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(y_train_full[index]))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred) > .5)\n    plt.title('Predicted mask')\n    plt.show()\n","c9f204a0":"Now we join all the augmentations image arrays to the original training arrays.","bee64492":"## Defining Evaluation Metrics","f10b25d7":"Splitting the dataset into training set and test set to verify our model performance without any bias.","65577fdb":"#### Optimizer and Learning Rate  \n* We adopt adam optimization algorithm or adaptive moments, to adjust the learning rate.   \n* It is well known that learning rate is one of the critical hyperparameters that have a signi\ufb01cant impact on classi\ufb01cation performance.     \n* Advantages of Adam optimizer are:\n    - Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum).\n    - Usually works well even with a little tuning of hyperparameter\n* Adam is fairly robust to the choice of hyperparameters, and set the learning rate **\u03b1** as 0.003 to speed up the training procedure in this study as advised in the paper [here](https:\/\/arxiv.org\/pdf\/1703.05165v2.pdf)","69863cd0":"> ![arch](https:\/\/raw.githubusercontent.com\/hashbanger\/Skin_Lesion_Segmentation\/master\/graphics\/unet_arch.png)","fb680ab0":"![recall](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/4c233366865312bc99c832d1475e152c5074891b)","6077d46e":"![precsion](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/26106935459abe7c266f7b1ebfa2a824b334c807)","4ddf3401":"We are going to define to methods for augmentation, one for **random rotation** and one for **horizontal flipping**","1ba56784":"Lets see some of the predictions","0a1bf041":"## The Model","86707671":"## Defining the Loss Function","737e901b":"Let us have a look at our transformations.","1eef46ef":"Here we can load a pretrained model","6b635784":"Defining a function to load the data in sorted order","eb85f068":"#### Dice coefficient\nThe Dice score is not only a measure of how many positives you find, but it also penalizes for the false positives that the method finds, similar to precision. so it is more similar to precision than accuracy.","de14f028":"## Image Augmentation  \nTo build a powerful image classifier using very little training data, image augmentation is usually required to boost the performance of deep networks. Image augmentation artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc.","8ea2f662":"![dc](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*Z1hkDvyhFBogT9EkzVkX2A.png)","3da50190":"#### Intersection over Union  \nThe Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient is a statistic used for gauging the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:","d71e3ce5":"## Loading the Data","a2fc64ac":"![jd](https:\/\/www.geeksforgeeks.org\/wp-content\/ql-cache\/quicklatex.com-44046533fcd54e98cb53619b3390e083_l3.svg)","565bc9a0":"![iou](https:\/\/www.d2l.ai\/_images\/iou.svg)","7e510a9e":"#### Accuracy","09c89c2c":"## Importing the Libraries","497e7b27":"### Model Function","93a5c6e5":"Now let's see their corresponding masks.","c64193b3":"#### Making a Validation Set  \nWe will split our full training set into train and validation set.\nValidation dataset is used to validate the performance after each epoch.","b8509238":"#### Recall  \nRecall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n","ec221b90":"Let's plot an image to see how the original images look.","031da703":"#### Precision    \nPrecision is a good measure to determine, when the costs of False Positive is high.","58f25345":"#### Jaccard Distance  \nThe Jaccard distance, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:","51a205e8":"## Loading the Model"}}