{"cell_type":{"39daec80":"code","69c134f5":"code","68763ffa":"code","1dd9e8ae":"code","28750c19":"code","f56785d1":"code","a763b53c":"code","4dca04ca":"code","3539cf5c":"code","41aab645":"code","10b1cf4e":"code","de55d1fa":"code","5476a201":"code","2afe7c8e":"code","34d5a4af":"code","0586584d":"code","ff0aa364":"code","b1195cf2":"code","eac4cfd1":"code","aab77815":"code","efe5eac7":"code","42d6e910":"markdown","afaf0866":"markdown","a373aaca":"markdown","e23f4bf4":"markdown","ec1c854c":"markdown","ba7cc49e":"markdown","8f18249a":"markdown","dca71324":"markdown","3f26de33":"markdown","fac68179":"markdown","c6c7879d":"markdown","f3fe8521":"markdown","6c40eacf":"markdown","d1f5fa6c":"markdown","3302ad5c":"markdown","59f2a49d":"markdown","888662de":"markdown","f59ce1f3":"markdown","6e7ef705":"markdown","c4e12e17":"markdown","4d4f786b":"markdown","4e6fdbba":"markdown"},"source":{"39daec80":"# Kaggle imports\nimport os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Gdrive and path\nfrom pathlib import Path\n\n# Keras preprocessing\nfrom keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nfrom keras.preprocessing import image\nfrom keras.applications.mobilenet import preprocess_input\nfrom sklearn.utils import class_weight\n\n# MobileNet model\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras import optimizers\nfrom keras.layers import Dense,GlobalAveragePooling2D,Dropout\nfrom keras.models import Model\n\nfrom keras.applications import imagenet_utils","69c134f5":"## Currentlyset to our baseline values\nbatch_size_train_param = 64          # The batch size for the training step. Default = 64, Henriks default = 8\nbatch_size_validate_param = 64       # The batch size for the validation step. Default = 64, Henriks default =  8\nepochs_num_param = 30                # Number of epochs when training the model. Default = 20, Henriks default = 10\nvalidation_split_size_param = 0.2    # How large a part of the dataset is being used for validation. 0.0 = 0%, 1.0 = 100%, default = 0.2\nlearning_rate_param = 3e-5           # The learning rate of the model. Default = 3e-5, Henriks default = 0.001\n# decay_param = 1e-6                 # The decay of the model. Default = 1e-6. No longer used\n# momentum_param = 0.9               # The momentum for the model. Default = 0.9, No longer used ","68763ffa":"class_path = \"\/kaggle\/input\/utkclassesface\/UTKClasses\"\n\nclasses = ['001-010', '011-020', '021-030', '031-040', '041-050', '051-060', '061-070', '071-080', '081-090', '091-100', '101-110', '111-116']","1dd9e8ae":"# From lab 2. Modified as described here: https:\/\/stackoverflow.com\/a\/52372042 \n\n# Useful link for data augmentation https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, #included in our dependencies\n                                   validation_split=validation_split_size_param,\n                                   rotation_range=10,              # Data augmentation experiment\n                                   horizontal_flip=True             # Data augmentation experiment\n                                   ) \n\n# For the exclude last 3 classes experiment \n# Classlist details which classes are to be used for training and validation. \n# Set to equal classes to get all classes, or classes[:9] to exclude the last 3 classes. \nclasslist= classes[:9]\ntrain_generator = train_datagen.flow_from_directory(class_path, # this is where you specify the path to the main data folder\n                                                    classes=classlist,                 # Class exclusion experiment\n                                                    target_size=(200,200),\n                                                    color_mode='rgb',\n                                                    batch_size=batch_size_train_param, # Hyperparameter\n                                                    class_mode='categorical',\n                                                    shuffle=True, \n                                                    subset='training') # set as training data)\n\nvalidation_generator = train_datagen.flow_from_directory(class_path, \n                                                         classes=classlist, # Class exclusion experiment\n                                                         target_size=(200,200),\n                                                         color_mode='rgb',\n                                                         batch_size=batch_size_validate_param, # Hyperparameter\n                                                         class_mode='categorical', \n                                                         shuffle=False, \n                                                        # rotation_range = 10,     # Should these be commented out when doing data augmentation? \n                                                        # vertical_flip = True,    # Should these be commented out when doing data augmentation? \n                                                         subset='validation') # set as validation data\n","28750c19":"print('generator:',train_generator.class_indices)\nprint('your\\'s:',dict((class_name,class_index) for class_index,class_name in enumerate(classlist))) # updated from classes to classlist to reflect the classes actually tested on. ","f56785d1":"mobileNet_full = MobileNet(weights = 'imagenet',\n                      include_top = False,\n                      input_shape = (200, 200, 3))\n\n# mobileNet_full.summary()\n\nmobileNet_full.trainable = True\n\nmodel = mobileNet_full\nx     = model.output #We remove last layer\nx     = GlobalAveragePooling2D()(x)\nx     = Dropout(0.3)(x)\nx     = Dense(1024, activation = 'relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\nx     = Dropout(0.3)(x)\nx     = Dense(512, activation = 'relu')(x) #dense layer 2 (extra)\nx     = Dropout(0.3)(x)\nx     = Dense(256, activation = 'relu')(x) #dense layer 3 (extra)\nx     = Dropout(0.3)(x)\npreds = Dense(9, activation = 'softmax')(x) #final layer with softmax activation. 12 because we have 12 classes \n# Preds must be set to 9 in the exclusion experiment. \n\n# Specify model\nmodel = Model(inputs = mobileNet_full.input, outputs=preds)\nmodel.summary()","a763b53c":"\"\"\"total_num_layers = len(model.layers)\nnum_base_layers = len(conv_base.layers)\nprint(f\"Total number of layers is {total_num_layers}\")\nprint(f\"Number of pretrained base layers is {num_base_layers}\")\n\nfor layer in model.layers[:num_base_layers]:\n    layer.trainable=False\nfor layer in model.layers[num_base_layers:]:\n    layer.trainable=True\n\"\"\"","4dca04ca":"# Class_weights as numpy_array\nclass_weights = class_weight.compute_class_weight(\n           'balanced',\n            np.unique(train_generator.classes), \n            train_generator.classes)\n\n# Use this line to convert into dictionary:\n#class_weights = {i : class_weights[i] for i in range(12)}\n\nprint('Class weights: ', class_weights)","3539cf5c":"# Define method from link:\n\nfrom keras import backend as K\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    A weighted version of keras.objectives.categorical_crossentropy\n    \n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n    \n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"\n    \n    weights = K.variable(weights)\n        \n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    \n    return loss","41aab645":"# Set up optimizer (The Stocastic Gradient Decent optimiser was not recommended by Jintao. Instead we use the Adam optimiser)\n#sgd_optimizer = optimizers.SGD(learning_rate = learning_rate_param, decay = decay_param, momentum = momentum_param, nesterov=True)\n##                                    ^ Hyperparameter                ^ Hyperparameter      ^ Hyperparameter\noptimizer = optimizers.Adam(learning_rate = learning_rate_param)\n\n\n# Stuff for the weighted loss function experiments \n# Weighted loss function. Set loss in the model below to 'weighted_categorical_crossentropy(weights)'\n# weights = class_weights # the twelve or ten weights\n# Baseline loss function: 'categorical_crossentropy' \n\n\n# Compile model - make it trainable\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n\n#step_size_train = train_generator.n\/\/train_generator.batch_size # Number of mini-batches per epoch (training)\n#step_size_val = validation_generator.n\/\/validation_generator.batch_size # Number of mini-batches per epoch (validation)\n\n# Train model for specified no. epochs\n\"\"\"history = model.fit_generator(generator = train_generator,\n                    validation_data = validation_generator,\n                    validation_steps = step_size_val,\n                    steps_per_epoch = step_size_train,\n                    epochs = epochs_num_param)                 ## Hyperparameter \n\"\"\"\n# Model for training link (maybe useful) https:\/\/keras.io\/api\/models\/model_training_apis\/\nhistory = model.fit(train_generator, epochs=epochs_num_param, validation_data=validation_generator)","10b1cf4e":"history.history","de55d1fa":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model_dense_dropout_augment_remove_classes.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model_dense_dropout_augment_remove_classes.h5\")\nprint(\"Saved model to disk\")\n# save model and architecture to single file\nmodel.save(\"fullmodel_dense_dropout_augment_remove_classes.h5\")\nprint(\"Saved full model to disk\")","5476a201":"# assuming you stored your model.fit results in a 'history' variable:\n## history = model.fit(x_train, y_train, epochs=10)\n\n# ALTERNATIVELY USING PANDAS AND JSON OR CSV: \nimport pandas as pd\n\n# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history.history) \n\n# save to json:  \nhist_json_file = 'history_dense_dropout_augment_remove_classes.json' \nwith open(hist_json_file, mode='w') as f:\n    hist_df.to_json(f)\n\n# or save to csv: \nhist_csv_file = 'history_dense_dropout_augment_remove_classes.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","2afe7c8e":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","34d5a4af":"model.summary()","0586584d":"#model_builder = keras.applications.vgg16.VGG16\nimg_size = (200, 200)\n#preprocess_input = keras.applications.vgg16.preprocess_input\ndecode_predictions = keras.applications.mobilenet.decode_predictions\n\nlast_conv_layer_name = \"conv_pw_13_relu\"\n\n# The local path to our target image\nimg_path = keras.utils.get_file(\n    \"minnaface.jpg\", \"https:\/\/i1.rgstatic.net\/ii\/profile.image\/452530338570242-1484903048607_Q128\/Minna-Pakanen.jpg\"\n)\n\ndisplay(Image(img_path))","ff0aa364":"def get_img_array(img_path, size):\n    # `img` is a PIL image of size 224x224\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (224, 224, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 224, 224, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","b1195cf2":"# Prepare image\nimg_array = preprocess_input(get_img_array(img_path, size=img_size))\n\n# Make model\n#model = model_builder(weights=\"imagenet\")\n\n# Remove last layer's softmax\n#model.layers[-1].activation = None\n\n# Print what the top predicted class is\npreds = model.predict(img_array)\n#print(\"Predicted:\", decode_predictions(preds, top=1)[0])\nprint(\"Predicted: \", preds)\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","eac4cfd1":"def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n    display(Image(cam_path))\n\n\nsave_and_display_gradcam(img_path, heatmap)","aab77815":"import keras\nimport numpy\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nnum_of_validation_samples = 4722\n\nY_pred = model.predict(validation_generator, num_of_validation_samples \/\/ batch_size_validate_param+1)\ny_pred = np.argmax(Y_pred, axis=1)\n\nprint('Confusion Matrix')\nmatrix = confusion_matrix(y_pred, validation_generator.classes)\nprint(matrix)\nprint('Classification Report')\ntarget_names = classes[:9]\nprint(classification_report(validation_generator.classes, y_pred, target_names=target_names))","efe5eac7":"import pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf_cm = pd.DataFrame(matrix, range(9), range(9))\n\n#plt.figure(figsize=(8,5))\nsn.set(font_scale=1) # for label size\nsn.heatmap(df_cm\/np.sum(df_cm), annot=True, annot_kws={\"size\": 8}, cmap='Blues') # font size\n\nplt.show()","42d6e910":"To get the last values for last_conv_layer_name, use model.summary() to see the names of all layers in the model","afaf0866":"### Specifying only our layers to train\nWe only which to train on our layers, not the base layers of the MobileNet network. Here we disable training for all but the layers we have specified. \n\n*From lab 2, task 11*\n\n*This is not needed, as we don't include the top layer of MobileNet.*","a373aaca":"## Hyperparameters \nHere we define some hyperparameters in a single location, so it is easier to change them in the future","e23f4bf4":"Checking that the classes are assigned correctly","ec1c854c":"### Training the model \n\nWe are now ready to start training the model using\n- Adam optimizer\n- loss function will be categorical cross entropy\n- evaluation metric will be accuracy\n\n*From lab 2, task 11*","ba7cc49e":"To plot it nicely:","8f18249a":"Start by loading model:","dca71324":"## Setting directory variables","3f26de33":"To create a superimposed image:","fac68179":"## Heatmaps - Grad-CAM\n\n*from: https:\/\/github.com\/klaverhenrik\/Deep-Learning-for-Visual-Recognition-2021\/blob\/main\/Lab10_(part2).ipynb","c6c7879d":"## Confusion matrix\n\n*from: https:\/\/gist.github.com\/RyanAkilos\/3808c17f79e77c4117de35aa68447045*","f3fe8521":"## Saving the history object after training the model for later use. \n\n*From https:\/\/stackoverflow.com\/questions\/41061457\/keras-how-to-save-the-training-history-attribute-of-the-history-object*","6c40eacf":"The Grad-CAM algorithm:","d1f5fa6c":"### Implement weighted categorical crossentropy loss function\n\nNOTE: First iteration (without this code), we used the weights above in model.fit, but still used categorical_crossentropy loss function. Now, we do not set weights in model.fit, but we set the loss function as weighted categorical crossentropy.\n\n*from: https:\/\/gist.github.com\/wassname\/ce364fddfc8a025bfab4348cf5de852d*","3302ad5c":"## Create Image Preprocessor using Keras \n\n*Inspired from: https:\/\/stackoverflow.com\/questions\/42443936\/keras-split-train-test-set-when-using-imagedatagenerator#52372042* \n\n*Excluding subdirectories are inspired from: https:\/\/kylewbanks.com\/blog\/loading-unlabeled-images-with-imagedatagenerator-flowfromdirectory-keras*","59f2a49d":"## Create MobileNet model","888662de":"We use MobileNet trained on imagenet as the base for our baseline model. We disable the top layers, so we can train the model to classify our data. \n\n*From lab 2, task 11, with extra Dense layers as described here: https:\/\/towardsdatascience.com\/transfer-learning-using-mobilenet-and-keras-c75daf7ff299*\n\n*To add dropout to hidden layers: https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/*","f59ce1f3":"Here, we use the algorithm:","6e7ef705":"### Calculate class weights \n\nWe calculate class weights for a weighted categorical crossentropy loss function.\n\nNOTE: In \"Calculate class weights\", convert into dictionary when setting weights in model.fit, but don't when using weighted_categorical_crossentropy\n\n*from: https:\/\/stackoverflow.com\/a\/51485928*\n\n*fix from: https:\/\/stackoverflow.com\/a\/61269444*\n","c4e12e17":"### Save model to disk (**Remember to update filenames to not overwrite!!!!!!!!!**) \nHere it saves bot the model and the weights in individual files, and also in a single file since we don't know which is better \/ more useful for future use. \n\n*From: https:\/\/machinelearningmastery.com\/save-load-keras-deep-learning-models\/* ","4d4f786b":"# Transfer learning from MobileNet to age estimation on UTKFace \n\nThis notebook is an attempt to use transfer learning to use transfer capabilities of an existing model to a new domain. \nIn this case MobileNet trained on imagenet is used as the convolutional base, with a custom five-layer decoder classifying the input into one of 12 classes. \n\nThe goal is to obtain a baseline for age estimation from facial features using the UTKFace dataset for future tests. ","4e6fdbba":"### Summary of the trained model\nGet the history of the model to see how it has performed when training on the ned classification layer. "}}