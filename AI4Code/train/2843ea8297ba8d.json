{"cell_type":{"c975408f":"code","89fe2a39":"code","7a188ac5":"code","43415e55":"code","b704d2e9":"code","42ce61e9":"code","db9dca50":"code","7b11317a":"code","49385329":"code","6b40797c":"code","2c400a2a":"code","a3a689c8":"code","6e438c84":"code","4996cc65":"code","f44c5c7f":"code","454583ff":"code","757e11a3":"code","f326b1ee":"code","83b357c8":"code","ee62a96f":"code","0638aa5f":"code","00659a32":"code","892102cb":"code","73013f6e":"code","b01a2696":"markdown","4dd5afef":"markdown","e2fed1fd":"markdown","8b197190":"markdown","58a8218d":"markdown","96e0a180":"markdown","9f7364b7":"markdown","9bd5be99":"markdown","1684d2b8":"markdown","dcb9ac4d":"markdown","3ee42eb0":"markdown","f4ad360d":"markdown","1c1934b2":"markdown","468360f1":"markdown","9c64c015":"markdown"},"source":{"c975408f":"# data analysis libraries:\nfrom flask import Flask, jsonify, request, render_template\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport sys\nif not sys.warnoptions:\n    import os, warnings\n    warnings.simplefilter(\"ignore\") \n    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n#timer\nimport time\nfrom contextlib import contextmanager\n\n# Importing modelling libraries\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\npd.options.display.float_format = \"{:,.2f}\".format\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} done in {:.0f}s\".format(title, time.time() - t0))","89fe2a39":"# !pip install xgboost\n# !pip install lightgbm>=0.90\n# !pip install catboost==0.23.1","7a188ac5":"!pip install catboost==0.23.1","43415e55":"pwd","b704d2e9":"# Read train and test data with pd.read_csv():\ndata = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")","42ce61e9":"X = data.iloc[:, 3:-1]\ny = data.iloc[:, -1]","db9dca50":"X","7b11317a":"def data_encode(df):\n    df = pd.get_dummies(data = df, columns=[\"Geography\"], drop_first = False)\n    for col in df.select_dtypes(include=['category','object']).columns:\n        codes,_ = df[col].factorize(sort=True)    \n        df[col]=codes\n    return df","49385329":"X = data_encode(X)","6b40797c":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\npickle.dump(X.columns, open(\"columns.pkl\", 'wb'))","2c400a2a":"# Memory Reduction ","a3a689c8":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","6e438c84":"for df in [x_train,x_val]:\n    reduce_mem_usage(df)    ","4996cc65":"r=1309\nmodels = [LogisticRegression(random_state=r),GaussianNB(), KNeighborsClassifier(),\n          SVC(random_state=r,probability=True),BaggingClassifier(random_state=r),DecisionTreeClassifier(random_state=r),\n          RandomForestClassifier(random_state=r), GradientBoostingClassifier(random_state=r),\n          XGBClassifier(random_state=r), MLPClassifier(random_state=r),\n          CatBoostClassifier(random_state=r,verbose = False)]\nnames = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"SVC\",\"Bagging\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]","f44c5c7f":"print('Default model validation accuracies for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val) \n    print(name,':',\"%.3f\" % accuracy_score(y_pred, y_val))","454583ff":"predictors=pd.concat([x_train,x_val])","757e11a3":"results = []\nprint('10 fold Cross validation accuracy and std of the default models for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    kfold = KFold(n_splits=10, random_state=1001)\n    cv_results = cross_val_score(model, predictors, y, cv = kfold, scoring = \"accuracy\")\n    results.append(cv_results)\n    print(\"{}: {} ({})\".format(name, \"%.3f\" % cv_results.mean() ,\"%.3f\" %  cv_results.std()))","f326b1ee":"# Possible hyper parameters\nlogreg_params= {\"C\":np.logspace(-1, 1, 10),\n                    \"penalty\": [\"l1\",\"l2\"], \"solver\":['lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\":[1000]}\n\nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nknn_params= {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nsvc_params= {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n                 \"C\": [1,10,50,100,200,300,1000]}\nbag_params={\"n_estimators\":[50,120,300]}\ndtree_params = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\nrf_params = {\"max_features\": [\"log2\",\"auto\",\"sqrt\"],\n                \"min_samples_split\":[2,3,5],\n                \"min_samples_leaf\":[1,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[50,100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\n\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\nxgb_params ={\n        'n_estimators': [50, 100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4],\n        'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n        \"min_samples_split\": [1,2,4,6]}\n\nmlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\"max_iter\":[1000]}\ncatb_params =  {'depth':[2, 3, 4],\n              'loss_function': ['Logloss', 'CrossEntropy'],\n              'l2_leaf_reg':np.arange(2,31)}\nclassifier_params = [logreg_params,NB_params,knn_params,svc_params,bag_params,dtree_params,rf_params,\n                     gbm_params, xgb_params,mlpc_params,catb_params]               \n                  ","83b357c8":"# Tuning by Cross Validation  \ncv_result = {}\nbest_estimators = {}\nfor name, model,classifier_param in zip(names, models,classifier_params):\n    with timer(\">Model tuning\"):\n        clf = GridSearchCV(model, param_grid=classifier_param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n        clf.fit(x_train,y_train)\n        cv_result[name]=clf.best_score_\n        best_estimators[name]=clf.best_estimator_\n        print(name,'cross validation accuracy : %.3f'%cv_result[name])","ee62a96f":"accuracies={}\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(x_train,y_train).predict(x_val)\n    accuracy=accuracy_score(y_pred, y_val)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy","0638aa5f":"n=3\naccu=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])[:n]\nfirstn=[[k,v] for k,v in best_estimators.items() if k in accu]\nprint(firstn)","00659a32":"# Ensembling First n Score\n\nvotingC = VotingClassifier(estimators = firstn, voting = \"soft\", n_jobs = -1)\nmodel= votingC.fit(x_train, y_train)\nprint(\"\\nAccuracy_score is:\",accuracy_score(model.predict(x_val),y_val))","892102cb":"# save the model so created above into a picle.\npickle.dump(model, open('model.pkl', 'wb')) ","73013f6e":"print(\"\\nAccuracy_score is:\",accuracy_score(model.predict(x_val),y_val))","b01a2696":"After pushing the outputs of this kernel to https:\/\/github.com\/berkarcan\/Heroku-Churn-Ensembled-Model\n\nThanks to the intuitive studies of Muhammet Cakmak(https:\/\/github.com\/muhammet-cakmak), I created the online deployment of the final selected  model.\n\nSee https:\/\/churn-ensembled-dtre-rf-gbm.herokuapp.com\/ \n\nThe screenshot of the web site is:\n\n![](https:\/\/github.com\/berkarcan\/Heroku-Churn-Ensembled-Model\/raw\/master\/heroku_web.jpg)\n","4dd5afef":"<a id = '6'><\/a><br> \n# 4. Modeling, Evaluation and Model Tuning  ","e2fed1fd":"## 4.2 Cross validation accuracy and std of the default models for all the train data <a id = '6.3'><\/a><br>","8b197190":"There is no missing value in the data as seen in section 2.2. In addition, from decriptive statistics we can see that  median and mean values are very similar for most of the numerical variables.","58a8218d":"<font color = 'blue'>\n CONTENTS:  \n    \n   1. [Introduction](#1)\n       * 1.1 [The Heroku deployment link of the final tuned and ensmebled model](#1.1)\n       * 1.2 [Summary Information about the variables and their types in the data](#1.2)\n   2. [Importing Libraries and Loading Data](#2)\n   3. [Data Preprocessing](#3)\n       * 3.1 [Splitting the data as train and validation data](#3.1)  \n       * 3.2 [Handling Categorical Variables](#3.6)\n           * 3.2.1 [Label encoding of gender variable and removing surname](#3.6.1)            \n           * 3.2.3 [One hot encoding of Geography (Country)](#3.6.2)   \n       * 3.3 [Memory Reduction](#3.3)\n   4. [Modeling, Model Evaluation and Model Tuning](#6)\n       * 4.1 [Validation Set Test Accuracy for the default models](#6.2) \n       * 4.2 [Cross validation accuracy and std of the default models for all the train data](#6.3)    \n       * 4.3 [Model tuning using crossvalidation](#6.4)   \n       * 4.4 [Ensembling first n (e.g. 3) models](#6.6) \n\n ","96e0a180":"## 1.1 The Heroku deployment link of the final tuned and ensembled model <a id = '1.1'><\/a><br>","9f7364b7":"* Surname          : The surname of the customer\n* CreditScore      : The credit score of the customer\n* Geography        : The country of the customer(Germany\/France\/Spain)\n* Gender           : The gender of the customer (Female\/Male)\n* Age              : The age of the customer  \n* Tenure           : The customer's number of years in the in the bank \n* Balance          : The customer's account balance\n* NumOfProducts    : The number of bank products that the customer uses \n* HasCrCard        : Does the customer has a card? (0=No,1=Yes) \n* IsActiveMember   : Does the customer has an active mebership (0=No,1=Yes) \n* EstimatedSalary  : The estimated salary of the customer\n* Exited           : Churned or not? (0=No,1=Yes)","9bd5be99":"## 4.1 Validation Set Accuracy for the default models <a id = '6.2'><\/a><br>","1684d2b8":"## 4.4 Ensembling first n (e.g. 3) models <a id = '6.6'><\/a><br>","dcb9ac4d":"# 2. Importing Libraries and Loading Data  <a id = '2'><\/a><br> ","3ee42eb0":"## 1.2 Summary Information about the variables and their types in the data <a id = '1.2'><\/a><br>\n\n","f4ad360d":"# 1. Introduction and the Deployment Link ","1c1934b2":"##### This study predicts which bank customers will churn by means of machine learning modelling techniques. It uses 11 Machine Learning algorithms, tune their parameters and ensemble the best n (e.g. 3) of them using their accuracy scores for the validation set. The full ML work flow version of this kernel with visualizations is available at my other notebook: https:\/\/www.kaggle.com\/berkanacar\/churn-prediction-by-selecting-from-11-tuned-models","468360f1":"# 3. Data Preprocessing <a id = '3'><\/a><br> ","9c64c015":"## 4.3 Model tuning using crossvalidation <a id = '6.4'><\/a><br>"}}