{"cell_type":{"688137ff":"code","a3494d86":"code","9e0b240a":"code","882bb965":"code","c47be89b":"code","e700222d":"code","b8d4b72b":"code","7b2ed3c2":"code","e1e02e1c":"code","f7ae0c8b":"code","e422c3c3":"code","13f32074":"code","31a0888b":"code","4b56b8ca":"code","f6ad3c75":"code","2c7beca2":"code","32fee00d":"code","c0282683":"code","7a04015f":"code","9d99ca31":"code","eea8ba62":"code","2faebf8c":"code","49948240":"code","5e61ea6f":"code","e78621a7":"code","2b0f6986":"markdown","84d0c349":"markdown","5855ef83":"markdown","294788b5":"markdown","2a830ab9":"markdown","febd8c55":"markdown","f6ca731c":"markdown"},"source":{"688137ff":"#%%time\n# Import the Rapids suite here - takes abot 2 mins\n\n#import sys\n#!cp ..\/input\/rapids\/rapids.0.17.0 \/opt\/conda\/envs\/rapids.tar.gz\n#!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n#!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","a3494d86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport gc\nimport sys\nimport random\nfrom tqdm.notebook import * \nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas as pd,numpy as np\n\n\nimport cudf\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.svm import SVC, SVR\nfrom cuml.neighbors import KNeighborsClassifier, NearestNeighbors\n\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing import LabelEncoder\nfrom cuml.experimental.preprocessing import MinMaxScaler\nfrom cuml.linear_model import MBSGDClassifier as cumlMBSGDClassifier\nfrom cuml.naive_bayes import MultinomialNB\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 500)\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings('ignore')","9e0b240a":"# Standar libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Code you have previously used to load data\nfrom numpy import set_printoptions\n%matplotlib inline\nfrom matplotlib import style\nfrom xgboost import XGBRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\n#### A function to calculate errors ####\ndef root_mean_squared_log_error(y_valid, y_preds):\n    \"\"\"Calculate root mean squared error of log(y_true) and log(y_pred)\"\"\"\n    if len(y_preds)!=len(y_valid): return 'error_mismatch'\n    y_preds_new = [math.log(x) for x in y_preds]\n    y_valid_new = [math.log(x) for x in y_valid]\n    return mean_squared_error(y_valid_new, y_preds_new, squared=False)\n#########################################\n\n\n# Path of the file to read. We changed the directory structure to simplify submitting to a competition\ntrain_path = '..\/input\/homedataformlcourse\/train.csv'\n# path to file you will use for predictions\ntest_path = '..\/input\/homedataformlcourse\/test.csv'\n\n# Creating the dataframe --\nhome_data = pd.read_csv(train_path)\n# read test data file using pandas\nX_test = pd.read_csv(test_path)","882bb965":"##### Extracting features ######\nfeatures1 = [x for x in home_data.columns]\nX = home_data[features1]\ny = home_data['SalePrice']\n\n## Copy to modify --\nX_feat_eng=X.copy()\n\n## Numerical features -- \nfeature_numerical_cols = [cname for cname in X_feat_eng.columns if \n                X_feat_eng[cname].dtype in ['int64', 'float64']]\n\n## Categorical features --\nfeature_categorical_cols = [cname for cname in X_feat_eng.columns if\n                    X_feat_eng[cname].nunique() < 50 and \n                    X_feat_eng[cname].dtype in ['object', 'bool']]\n\n\nfeature_numerical_transformer = SimpleImputer(strategy='constant')\n\nfeature_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nfeature_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', feature_numerical_transformer, feature_numerical_cols),\n        ('cat', feature_categorical_transformer, feature_categorical_cols)\n])\n\n#### Model and splitting\n## Let's model and fit:\nfeature_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0.0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=4,\n             min_child_weight=0.0, monotone_constraints='()',\n             n_estimators=1250, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nfeature_clf = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),('feature_model', feature_model)])\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_valid = train_test_split(X_feat_eng, y, random_state=0)\nfeature_clf.fit(feature_X_train, feature_y_train, feature_model__verbose=False) \nfeature_preds = feature_clf.predict(feature_X_valid)","c47be89b":"# Fill in the line below: get names of columns with missing values\ncols_with_missing = [col for col in X_test.columns\n                     if X_test[col].isnull().any()]\n\n# Fill in the lines below: drop columns in training and validation data\nreduced_X_train = feature_X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = feature_X_valid.drop(cols_with_missing, axis=1)\n\n# Fill in the lines below: drop columns in training and validation data\nreduced_X_test = X_test.drop(cols_with_missing, axis=1)","e700222d":"print(reduced_X_train.shape,\nfeature_y_train.shape)","b8d4b72b":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (reduced_X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","7b2ed3c2":"## One rename\ny2=reduced_X_train['SalePrice']\nX_feat_eng2=reduced_X_train.copy()\nX_feat_eng2.drop(['SalePrice'], axis=1, inplace=True)\n\n#### Now forming two groups:\n#### 1) Numerical and 2) Categorical\n#### 3) Feature engineer\nX_feat_eng2['years_since_update'] = X_feat_eng2['YearRemodAdd'] - X_feat_eng2['YearBuilt']\n#X_feat_eng2['geometry'] = X_feat_eng2['LotArea'] \/ X_feat_eng2['LotFrontage']\nX_feat_eng2['land_topology'] = X_feat_eng2['LandSlope'] + '_' + X_feat_eng2['LandContour']\nX_feat_eng2['value_proposition'] = X_feat_eng2['YearBuilt'] * X_feat_eng2['OverallQual']\n#X_feat_eng2['finished_basement'] = X_feat_eng2['BsmtFinSF1'] > 0\n#X_feat_eng2['garage_value'] = X_feat_eng2['YearBuilt'] * X_feat_eng2['GarageCars']\nX_feat_eng2['misc_value'] = X_feat_eng2['Fireplaces'] + X_feat_eng2['OverallQual']\n\n### I dont need GarageCars any more, drop it! --\n#X_feat_eng2 = X_feat_eng2.drop(columns=['GarageCars'])\n\n## Numerical features -- \nfeature_numerical_cols = [cname for cname in X_feat_eng2.columns if \n                X_feat_eng2[cname].dtype in ['int64', 'float64']]\n# After doing the first cycle, I came backk here\n# and put the most important features by hands:\n#feature_numerical_cols=['OverallQual','GrLivArea','1stFlrSF','YearBuilt',\n#                       'Fireplaces','LotArea','YearRemodAdd','TotRmsAbvGrd']\n\n## Categorical features --\nfeature_categorical_cols = [cname for cname in X_feat_eng2.columns if\n                    X_feat_eng2[cname].nunique() < 50 and \n                    X_feat_eng2[cname].dtype in ['object', 'bool']]\n\n\nfeature_numerical_transformer = SimpleImputer(strategy='constant')\n\nfeature_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Final ensemble of features --\nfeature_preprocessor2 = ColumnTransformer(\n    transformers=[\n        ('num', feature_numerical_transformer, feature_numerical_cols),\n        ('cat', feature_categorical_transformer, feature_categorical_cols)\n])","e1e02e1c":"## Now 1srt model to test -- \nfeature_clf2 = Pipeline(steps=[('feature_preprocessor', feature_preprocessor2),('feature_model', feature_model)])\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_feat_eng2, y2, random_state=0)\n#################################################\n#################################################","f7ae0c8b":"## Chosing the best features --\nfeature_clf2.fit(X_train2, y_train2)\nfeature_preds2 = feature_clf2.predict(X_valid2)\nprint('RMSLE:', root_mean_squared_log_error(y_valid2, feature_preds2))","e422c3c3":"from sklearn.metrics import f1_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom sklearn.datasets import make_classification\nfrom functools import partial","13f32074":"space = [\n    Real(0.9, 1.0, name=\"colsample_bylevel\"),\n#    Real(0.4, 0.6, name=\"colsample_bytree\"),\n#    Real(0.0, 0.0001, name=\"gamma\"),\n    Real(0.001, 0.03, name=\"learning_rate\"),\n#    Real(0., 0.1, name=\"max_delta_step\"),\n#    Integer(0, 5, name=\"max_depth\"), ## Influencer parameter\n#    Real(0, 0.1, name=\"min_child_weight\"),\n    Integer(1200, 1400, name=\"n_estimators\"),\n#    Real(0., 0.001, name=\"reg_alpha\"),\n#    Real(0.1, 1, name=\"reg_lambda\"),\n#    Real(0.79, 0.8, name=\"subsample\"),\n#    Real(0.4, 0.5, name=\"base_score\"),\n]","31a0888b":"#feature_numerical_cols+feature_categorical_cols","4b56b8ca":"# function to fit the model and return the performance of the model\ndef return_model_assessment(args, X_train, y_train, X_test, X_valid):\n    global models, train_scores, test_scores, curr_model_hyper_params\n    params = {curr_model_hyper_params[i]: args[i] for i, j in enumerate(curr_model_hyper_params)}\n    #print('Mira los params: ',params)\n    model1 = XGBRegressor(random_state=0,\n                          gpu_id=-1,\n                          predictor='gpu_predictor', \n                          monotone_constraints='()',\n                          n_jobs=0, num_parallel_tree=1,\n                          colsample_bytree=0.6,\n                          max_depth=4,\n                          learning_rate=0.02, \n                          max_delta_step=0,\n                          subsample=0.8,\n                          eta=0.05,\n                          scale_pos_weight=1,\n                          tree_method='gpu_hist', \n                          validate_parameters=1,\n                          verbosity=None)                    \n    model1.set_params(**params)\n    model = Pipeline(steps=[('feature_preprocessor', \n                             feature_preprocessor2),\n                            ('feature_model', model1)])\n    fitted_model = model.fit(X_train, y_train)\n    models.append(fitted_model)\n    train_predictions = model.predict(X_valid)\n    train_score = root_mean_squared_log_error(y_valid2, train_predictions)\n    train_scores.append(train_score)\n    return train_score","f6ad3c75":"%%time\n# collecting the fitted models and model performance\nmodels = []\ntrain_scores = []\ntest_scores = []\ncurr_model_hyper_params = ['colsample_bylevel',\n                           'learning_rate','n_estimators']  \n#    'colsample_bylevel', 'colsample_bytree',\n#                           'gamma', 'learning_rate', 'max_delta_step',\n#                           'max_depth', 'min_child_weight', \n#                           'n_estimators', 'reg_alpha', 'reg_lambda',\n#                           'subsample','base_score']\n\nobjective_function = partial(return_model_assessment,\n                             X_train=X_train2, \n                             y_train=y_train2, \n                             X_test=reduced_X_test, \n                             X_valid=X_valid2)\n\n# running the algorithm\nn_calls = 40 # number of times you want to train your model\nresults = gp_minimize(objective_function, space, \n                      base_estimator=None,\n                      n_calls=n_calls,\n                      n_random_starts=n_calls-1,\n                      random_state=0)","2c7beca2":"train_scores.index(min(train_scores))","32fee00d":"# Extract the best model, based in the mean_squared_log_error\nIndexMin=train_scores.index(min(train_scores))\n# The model -\nbestModel=models[IndexMin]","c0282683":"import plotly.express as px\nmetrics = pd.DataFrame(train_scores)\nmetrics.loc[:,'dataset'] = [\"train_score\"]*n_calls\nmetrics.loc[:,'Iteration Number'] = list(range(1,n_calls+1))\nmetrics.columns = [\"MSLE\", \"dataset\", \"Iteration Number\"]\nfig = px.line(metrics, x=\"Iteration Number\", y=\"MSLE\", color=\"dataset\")\nfig.show()","7a04015f":"%%time\nimport xgboost as xgb\n# Compute shap values using GPU with xgboost\n\nparams1 = {\"random_stat\":0,\n                          \"gpu_id\":-1,\n                          \"predictor\":\"gpu_predictor\",\n                          \"n_jobs\":0, \n                          \"num_parallel_tree\":1,\n                          \"colsample_bytree\":0.6,\n                          \"max_depth\":2,\n                          \"learning_rate\":0.02, \n                          \"max_delta_step\":0,\n                          \"subsample\":0.8,\n                          \"eta\":0.05,\n                          \"scale_pos_weight\":1,\n                          \"tree_method\":\"gpu_hist\", \n                          \"validate_parameters\":1,\n                          \"verbosity\":None\n          }\n\n# GPU accelerated training\ndtrain = xgb.DMatrix(X_train2[feature_numerical_cols], \n                     label=y_train2, \n                     feature_names=feature_numerical_cols)\n%time modelXGB = xgb.train(params1, dtrain,100)","9d99ca31":"%%time\n# Compute shap values using GPU with xgboost\n# model.set_param({\"predictor\":\"cpu_predictor\"})\nmodelXGB.set_param({\"predictor\": \"gpu_predictor\"})\nshap_values = modelXGB.predict(dtrain, pred_contribs=True)","eea8ba62":"%%time\n# Compute shap interaction values using GPU\nshap_interaction_values = modelXGB.predict(dtrain, pred_interactions=True)","2faebf8c":"# We can use the shap package\nimport shap\n\n\n# shap will call the GPU accelerated version as long as the predictor parameter is set to \"gpu_predictor\"\nmodelXGB.set_param({\"predictor\": \"gpu_predictor\"})\nexplainer = shap.TreeExplainer(modelXGB)\n%time shap_values = explainer.shap_values(X_train2[feature_numerical_cols])\n\n# visualize the first prediction's explanation\nshap.force_plot(\n    explainer.expected_value,\n    shap_values[0, :],\n    X_train2[feature_numerical_cols].loc[0, :],\n    feature_names=feature_numerical_cols,\n    matplotlib=True\n)","49948240":"# Show a summary of feature importance\nshap.summary_plot(shap_values, \n                  X_train2[feature_numerical_cols], \n                  plot_type=\"bar\",\n                  feature_names=feature_numerical_cols)","5e61ea6f":"## Have to do exactly what did to the train data --\nreduced_X_test['years_since_update'] = reduced_X_test['YearRemodAdd'] - reduced_X_test['YearBuilt']\nreduced_X_test['land_topology'] = reduced_X_test['LandSlope'] + '_' + reduced_X_test['LandContour']\nreduced_X_test['value_proposition'] = reduced_X_test['YearBuilt'] * reduced_X_test['OverallQual']\nreduced_X_test['misc_value'] = reduced_X_test['Fireplaces'] + reduced_X_test['OverallQual']","e78621a7":"# make predictions which we will submit. \ntest_preds = bestModel.predict(reduced_X_test)\n# The lines below shows how to save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': reduced_X_test.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","2b0f6986":"Run only once if you don't have the suite RAPIDs","84d0c349":"Small modifications from several other's ideas.\n\n1. Removing missing data from the test data.\n2. Combining numerical + categorical data.\n3. Trying to implement some GPU facilities.\n\nModest feature engineering is made.","5855ef83":"# This is a notebook to send to the competition of housing","294788b5":"# Hyperparameters Space study","2a830ab9":"# Test it and send it to competition","febd8c55":"Building a rutine to combine features","f6ca731c":"We finally write the file for the competition"}}