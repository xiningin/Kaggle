{"cell_type":{"55cbf254":"code","833754fc":"code","c2c154f1":"code","336c01e8":"code","428b800d":"code","879fc0d2":"code","e4bcff3a":"code","328f2e45":"code","f106cbab":"code","cd13789f":"code","41887b28":"code","53652497":"code","ab98234b":"code","ae93be6e":"code","79c237e2":"code","30eaf86b":"code","481b53ea":"code","3e5b0e82":"code","38a11e1e":"code","ea4881de":"code","9988decc":"code","8cf9d61b":"code","50e7ad53":"code","b02501b3":"code","48d4c21e":"code","eb3afa13":"code","fb57db35":"code","19caabe5":"code","72bdbc3c":"code","ea52a0b9":"code","bef720d0":"code","b7248ca0":"code","87206bdf":"code","188d831d":"code","983da67e":"markdown","dfe6fa9a":"markdown","0da6bece":"markdown","81d9d073":"markdown","49cc4e00":"markdown","97532d1f":"markdown","35be227c":"markdown","905a485e":"markdown","76a00151":"markdown","5f7c5ad6":"markdown","9251ac79":"markdown","98efcdc7":"markdown","5f324d7c":"markdown","b5c91e50":"markdown","d3b86128":"markdown","8a27bdbc":"markdown","6fc9ecec":"markdown","4f4ebd41":"markdown","287ff3cc":"markdown","c2e42d3f":"markdown","94eedba4":"markdown"},"source":{"55cbf254":"    # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Lambda, Flatten\nfrom keras.optimizers import Adam ,RMSprop\nfrom sklearn.model_selection import train_test_split\nfrom keras import  backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","833754fc":"# create the training & test sets, skipping the header row with [1:]\ntrain = pd.read_csv(\"..\/input\/train.csv\")\nprint(train.shape)\ntrain.head()","c2c154f1":"test= pd.read_csv(\"..\/input\/test.csv\")\nprint(test.shape)\ntest.head()","336c01e8":"X_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\ny_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\nX_test = test.values.astype('float32')","428b800d":"X_train","879fc0d2":"y_train","e4bcff3a":"#Convert train datset to (num_images, img_rows, img_cols) format \nX_train = X_train.reshape(X_train.shape[0], 28, 28)\n\nfor i in range(6, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i]);","328f2e45":"#expand 1 more dimention as 1 for colour channel gray\nX_train = X_train.reshape(X_train.shape[0], 28, 28,1)\nX_train.shape","f106cbab":"X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\nX_test.shape","cd13789f":"mean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32)\n\ndef standardize(x): \n    return (x-mean_px)\/std_px","41887b28":"from keras.utils.np_utils import to_categorical\ny_train= to_categorical(y_train)\nnum_classes = y_train.shape[1]\nnum_classes","53652497":"plt.title(y_train[9])\nplt.plot(y_train[9])\nplt.xticks(range(10));","ab98234b":"# fix random seed for reproducibility\nseed = 43\nnp.random.seed(seed)","ae93be6e":"from keras.models import  Sequential\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D","79c237e2":"model= Sequential()\nmodel.add(Lambda(standardize,input_shape=(28,28,1)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\nprint(\"input shape \",model.input_shape)\nprint(\"output shape \",model.output_shape)","30eaf86b":"from keras.optimizers import RMSprop\nmodel.compile(optimizer=RMSprop(lr=0.001),\n loss='categorical_crossentropy',\n metrics=['accuracy'])","481b53ea":"from keras.preprocessing import image\ngen = image.ImageDataGenerator()","3e5b0e82":"from sklearn.model_selection import train_test_split\nX = X_train\ny = y_train\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\nbatches = gen.flow(X_train, y_train, batch_size=64)\nval_batches=gen.flow(X_val, y_val, batch_size=64)","38a11e1e":"history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3, \n                    validation_data=val_batches, validation_steps=val_batches.n)","ea4881de":"history_dict = history.history\nhistory_dict.keys()","9988decc":"import matplotlib.pyplot as plt\n%matplotlib inline\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss_values, 'bo')\n# b+ is for \"blue crosses\"\nplt.plot(epochs, val_loss_values, 'b+')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.show()","8cf9d61b":"plt.clf()   # clear figure\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc_values, 'bo')\nplt.plot(epochs, val_acc_values, 'b+')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.show()","50e7ad53":"def get_fc_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(optimizer='Adam', loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","b02501b3":"fc = get_fc_model()\nfc.optimizer.lr=0.01","48d4c21e":"history=fc.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","eb3afa13":"from keras.layers import Convolution2D, MaxPooling2D\n\ndef get_cnn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Convolution2D(64,(3,3), activation='relu'),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","fb57db35":"model= get_cnn_model()\nmodel.optimizer.lr=0.01","19caabe5":"history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","72bdbc3c":"gen =ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n                               height_shift_range=0.08, zoom_range=0.08)\nbatches = gen.flow(X_train, y_train, batch_size=64)\nval_batches = gen.flow(X_val, y_val, batch_size=64)","ea52a0b9":"model.optimizer.lr=0.001\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","bef720d0":"from keras.layers.normalization import BatchNormalization\n\ndef get_bn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        BatchNormalization(),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","b7248ca0":"model= get_bn_model()\nmodel.optimizer.lr=0.01\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","87206bdf":"model.optimizer.lr=0.01\ngen = image.ImageDataGenerator()\nbatches = gen.flow(X, y, batch_size=64)\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3)","188d831d":"predictions = model.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DR.csv\", index=False, header=True)","983da67e":"Oh its 3 !","dfe6fa9a":"## Convolutional Neural Network\nCNNs are extremely efficient for images.\n","0da6bece":"*Poonam Ligade*\n\n*1st Feb 2017*\n\n\n----------\n\n\nThis notebook is like note to self.\n\nI am trying to understand various components of Artificial Neural Networks aka Deep Learning.\n\nHope it might be useful for someone else here.\n\nI am designing neural net on MNIST handwritten digits images to identify their correct label i.e number in image.\n\nYou must have guessed its an image recognition task.\n\nMNIST is called Hello world of Deep learning.\n\nLets start!!\n\nThis notebook is inspired from [Jeremy's][1] [Deep Learning][2] mooc and [Deep learning with python][3] book by Keras author [Fran\u00e7ois Chollet][4] .\n\n\n  [1]: https:\/\/www.linkedin.com\/in\/howardjeremy\/\n  [2]: http:\/\/course.fast.ai\/\n  [3]: https:\/\/www.manning.com\/books\/deep-learning-with-python\n  [4]: https:\/\/research.google.com\/pubs\/105096.html","81d9d073":"## Data Visualization\nLets look at 3 images from data set with their labels.","49cc4e00":"## Cross Validation ","97532d1f":"## Fully Connected Model\n\nNeurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. \nAdding another Dense Layer to model.","35be227c":"Lets plot 10th label.","905a485e":"**Import all required libraries**\n===============================","76a00151":"**Preprocessing the digit images**\n==================================","5f7c5ad6":"*Linear Model*\n--------------","9251ac79":"**Load Train and Test data**\n============================","98efcdc7":"**Designing Neural Network Architecture**\n=========================================","5f324d7c":"The output variable is an integer from 0 to 9. This is a **multiclass** classification problem.","b5c91e50":"*One Hot encoding of labels.*\n-----------------------------\n\nA one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. \n\nFor example, 3 would be [0,0,0,1,0,0,0,0,0,0].","d3b86128":"***Compile network***\n-------------------\n\nBefore making network ready for training we have to make sure to add below things:\n\n 1.  A loss function: to measure how good the network is\n    \n 2.  An optimizer: to update network as it sees more data and reduce loss\n    value\n    \n 3.  Metrics: to monitor performance of network","8a27bdbc":"**Feature Standardization**\n-------------------------------------\n\nIt is important preprocessing step.\nIt is used to centre the data around zero mean and unit variance.","6fc9ecec":"More to come . Please upvote if you find it useful.\n\nYou can increase number of epochs on your GPU enabled machine to get better results.","4f4ebd41":"Lets create a simple model from Keras Sequential layer.\n\n1. Lambda layer performs simple arithmetic operations like sum, average, exponentiation etc.\n\n In 1st layer of the model we have to define input dimensions of our data in (rows,columns,colour channel) format.\n (In theano colour channel comes first)\n\n\n2. Flatten will transform input into 1D array.\n\n\n3. Dense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer.\n In the last layer we have to specify output dimensions\/classes of the model.\n Here it's 10, since we have to output 10 different digit labels.","287ff3cc":"## Submitting Predictions to Kaggle.\nMake sure you use full train dataset here to train model and predict on test set.\n","c2e42d3f":"## Data Augmentation\nIt is tehnique of showing slighly different or new images to neural network to avoid overfitting. And  to achieve better generalization.\nIn case you have very small dataset, you can use different kinds of data augmentation techniques to increase your data size. Neural networks perform better if you provide them more data.\n\nDifferent data aumentation techniques are as follows:\n1. Cropping\n2. Rotating\n3. Scaling\n4. Translating\n5. Flipping \n6. Adding Gaussian noise to input images etc.\n","94eedba4":"## Adding Batch Normalization\n\nBN helps to fine tune hyperparameters more better and train really deep neural networks."}}