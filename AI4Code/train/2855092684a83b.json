{"cell_type":{"5f7bc9ad":"code","45a07011":"code","aa0fe8e1":"code","c8b7b7e4":"code","c6ee3974":"code","5ac9c52d":"code","a425d216":"code","06229e23":"code","9bc9bdbc":"code","01ea93a1":"code","4a9850cc":"code","6426876a":"code","39abea49":"code","829b5f83":"code","bb488e81":"code","7fd3047c":"code","e3af78b9":"code","564571dc":"code","d0e6222b":"code","b7259da4":"code","003d1d3d":"code","d289b7fc":"code","42bfa4de":"code","50be9594":"code","32ef03c2":"code","e460bf19":"code","caf08f05":"code","818fe7a4":"code","9b4b60a9":"code","5f939bfd":"code","78ea4844":"code","39c45687":"code","5ea9a64a":"markdown","56ac56f8":"markdown","cfca2302":"markdown","b60d7e25":"markdown","63d47077":"markdown","93f09e74":"markdown","b3a5edd1":"markdown","298aa0d7":"markdown","b194f151":"markdown"},"source":{"5f7bc9ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","45a07011":"train_df = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","aa0fe8e1":"train_df.head()","c8b7b7e4":"# Set multiple plots distribution\nfigs, axs = plt.pyplot.subplots(nrows = 3, ncols = 3, figsize=(30,15))\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df, ax = axs[0][0])\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_df, ax = axs[0][1])\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_df, ax = axs[0][2])\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df, ax = axs[1][0])\nsns.boxplot(x=\"Survived\", y=\"Fare\", data=train_df[(train_df[\"Sex\"] == \"female\") & (train_df[\"Fare\"] < 100)], ax = axs[1][1])\nsns.scatterplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", data=train_df[(train_df[\"Fare\"] < 60) & (train_df[\"Age\"] < 60)], ax = axs[1][2])\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df, ax=axs[2][1])\n","c6ee3974":"def cab_to_deck(cab):\n    if type(cab) is float or cab[0] == 'T':\n        return \"N\"\n    else:\n        return cab[0]\n    \ntrain_df[\"Deck\"] = train_df[\"Cabin\"].apply(cab_to_deck)\ntest[\"Deck\"] = test[\"Cabin\"].apply(cab_to_deck)","5ac9c52d":"cabin_is_nan = train_df[\"Cabin\"].isna().sum() \/ len(train_df[\"Cabin\"])\nprint(\"Percentage of NaN: \", cabin_is_nan*100)\n\nfigs, axs = plt.pyplot.subplots(ncols = 2, figsize = (30,5))\n\nsns.barplot(x=\"Deck\", y=\"Survived\", data=train_df, ax = axs[0])\nsns.countplot(x=\"Deck\", data=train_df, ax = axs[1])","a425d216":"# Criar a Feature\ntrain_df[\"AgeIsNaN\"] = train_df[\"Age\"].isna()\ntest[\"AgeIsNaN\"] = test[\"Age\"].isna()","06229e23":"sns.barplot(x=\"AgeIsNaN\", y=\"Survived\", data=train_df)","9bc9bdbc":"# Substituir valores NaN em Age\ntrain_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace=True)\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace=True)","01ea93a1":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","4a9850cc":"ids = test[\"PassengerId\"]\ntest = test.drop(columns = [\"PassengerId\"])","6426876a":"# Estabelecer um m\u00e1ximo para Fare, 150, para ter melhores resultados ao usar MinMaxScaler\ntrain_df.loc[train_df[\"Fare\"] > 150, \"Fare\"] = train_df.loc[train_df[\"Fare\"] > 150, \"Fare\"].apply(lambda x: 100)\ntest.loc[test[\"Fare\"] > 150, \"Fare\"] = test.loc[train_df[\"Fare\"] > 150, \"Fare\"].apply(lambda x: 100)\n\ntest[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace = True)\n\nsns.distplot(train_df[\"Fare\"])","39abea49":"scaler = MinMaxScaler()\ntrain_df[[\"Age\", \"Fare\"]] = scaler.fit_transform(train_df[[\"Age\", \"Fare\"]])\ntest[[\"Age\", \"Fare\"]] = scaler.fit_transform(test[[\"Age\", \"Fare\"]])","829b5f83":"train_df = train_df.drop(columns = ['Ticket', 'Cabin', 'Name'])\ntest = test.drop(columns = ['Ticket', 'Cabin', 'Name'])","bb488e81":"# Converter as features em dummies\npclass_dummies = pd.get_dummies(train_df[\"Pclass\"], prefix='pclass')\nsex_dummies = pd.get_dummies(train_df[\"Sex\"], prefix='sex')\n#sibsp_dummies = pd.get_dummies(train_df[\"SibSp\"], prefix='sibsp')\n#parch_dummies = pd.get_dummies(train_df[\"Parch\"], prefix='parch')\ndeck_dummies = pd.get_dummies(train_df[\"Deck\"], prefix='deck')\nembarked_dummies = pd.get_dummies(train_df[\"Embarked\"], prefix='embarked')\n\ntrain_df = train_df.join([pclass_dummies, sex_dummies, deck_dummies, embarked_dummies])\n\npclass_dummies = pd.get_dummies(test[\"Pclass\"], prefix='pclass')\nsex_dummies = pd.get_dummies(test[\"Sex\"], prefix='sex')\n#sibsp_dummies = pd.get_dummies(test[\"SibSp\"], prefix='sibsp')\n#parch_dummies = pd.get_dummies(test[\"Parch\"], prefix='parch')\ndeck_dummies = pd.get_dummies(test[\"Deck\"], prefix='deck')\nembarked_dummies = pd.get_dummies(test[\"Embarked\"], prefix='embarked')\n\ntest = test.join([pclass_dummies, sex_dummies, deck_dummies, embarked_dummies])","7fd3047c":"# Apagar antigas features convertidas, Ticket, Cabin e Name\ntrain_df = train_df.drop(columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Deck', 'Embarked'])\ntest = test.drop(columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Deck', 'Embarked'])","e3af78b9":"# Separar as labels dos dados\ny = train_df[\"Survived\"]\nX = train_df.drop(columns = [\"Survived\", \"PassengerId\"])","564571dc":"# Separar os dados em casos de teste e de treino\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","d0e6222b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","b7259da4":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","003d1d3d":"print(logreg.score(X_train, y_train))\nprint(cross_val_score(logreg, X, y, cv = 10).mean())","d289b7fc":"parameters = {'C' : range(1, 20)}\n\ncv = GridSearchCV(logreg, parameters, cv = 10)\n\ncv.fit(X, y)\n\nprint(cv.best_score_)","42bfa4de":"neighbours = KNeighborsClassifier()\nneighbours.fit(X_train,y_train)","50be9594":"print(neighbours.score(X_train, y_train))\nprint(cross_val_score(neighbours, X, y).mean())","32ef03c2":"parameters = {'n_neighbors' : range(1, 35)}\n\ncv = GridSearchCV(neighbours, parameters)\n\ncv.fit(X, y)\n\nprint(cv.best_score_)\nprint(cv.best_params_)","e460bf19":"print(cv.best_estimator_.score(X, y))\nprint(cross_val_score(cv.best_estimator_, X, y, cv=10).mean())","caf08f05":"predictions = cv.best_estimator_.predict(test)\n\nsubmission = pd.DataFrame({'PassengerId' : ids, 'Survived' : predictions})\n\nsubmission.to_csv('Neighbors.csv', index=False)","818fe7a4":"submission","9b4b60a9":"svc = svm.SVC(C = 100)\nsvc.fit(X_train, y_train)","5f939bfd":"print(svc.score(X_train, y_train))\nprint(cross_val_score(svc, X, y).mean())","78ea4844":"parameters = {'C' : range(1, 100, 5), 'kernel' : ['rbf', 'poly'], 'degree' : [2, 3, 5, 7]}\n\ncv = GridSearchCV(svc, parameters)\n\ncv.fit(X, y)\n\nprint(cv.best_score_)\nprint(cv.best_params_)","39c45687":"print(cv.best_estimator_.score(X, y))\nprint(cross_val_score(cv.best_estimator_, X, y, cv=10).mean())","5ea9a64a":"### Nearest Neighbours","56ac56f8":"### SVM","cfca2302":"# Model making\nAgora que j\u00e1 temos todas as features prontas, podemos come\u00e7ar a criar Models. \nVamos experimentar:\n* SVM\n* Logistic Regression\n* Nearest Neighbours\n* Decision Tree\n* Random Forest\n* Gradient Boosted Decision Trees\n\n","b60d7e25":"# Processamento dos Dados #\nAgora vamos s\u00f3 tentar processar um bocado os dados, de modo a poderem ser usados melhores pelos modelos. \n* Vamos aplicar feature scaling a \"Fare\" e \"Age\".\n* One Hot Encoding a Pclass, Sex, SibSp, Parch, Deck e Embarked. \n* Vamos retirar Cabin e Ticket","63d47077":"### Logistic Regression","93f09e74":"# An\u00e1lise dos Dados #\nFazer uns gr\u00e1ficos bonitinhos, s\u00f3 para tentarmos \"perceber\" os dados.","b3a5edd1":"Ou seja, n\u00e3o temos dados para a Cabin, e consequentemente para o Deck, em 77% dos dados. Mas parece o facto de sabermos ou n\u00e3o em que Deck alguem estava \u00e9 bastante relevante.","298aa0d7":"Agora podiamos fazer mais gr\u00e1ficos, com outros fatores. Por exemplo, fazer gr\u00e1ficos s\u00f3 para homens, ou s\u00f3 para adultos. Se tiver paci\u00eancia acrescento-os depois. ","b194f151":"# Feature Creation\nVamos criar:\n* Deck - Extra\u00edda de Cabin. A primeira letra representa o Deck em que ficava a Cabin.\n* AgeIsNaN - Exactamente o que diz. Indica se Age \u00e9 ou n\u00e3o NaN."}}