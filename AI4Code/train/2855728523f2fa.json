{"cell_type":{"11023bce":"code","a8c4671e":"code","0e92c1d2":"code","f650b8ab":"code","b3771c5a":"code","65c4559d":"code","23d458dc":"code","57bf417b":"code","c3037b56":"code","7fa3abe8":"code","9ea5d7d4":"code","3c4c6d0c":"code","40b2f93d":"code","5dac8aa7":"code","bb0e5789":"code","40292ba9":"code","b5832844":"code","664b6962":"code","d61f5aae":"code","a5893e1e":"code","58f1634f":"code","db37be3e":"code","eff5a7d9":"code","99c68202":"code","7c6d11c0":"code","2a41591f":"code","ab14c6e7":"code","8917a233":"code","012fa3e5":"code","cd5229fb":"code","7d0f1927":"code","968749e3":"code","69230b67":"code","e2f45c30":"code","2e8f4bad":"code","cc53bb6b":"code","c7988d0f":"code","a287dae0":"code","57b92076":"code","4451689f":"code","47cf573a":"code","ae04771b":"code","4a2c2772":"code","376274a5":"code","59ae8dca":"code","4fe54b2d":"code","c9c8b8e4":"code","60b0e4c4":"code","2bbe2cc2":"code","c575cbee":"code","72ff181b":"code","5fe77529":"markdown","e712de71":"markdown","dc7055a6":"markdown","8c6740db":"markdown","3f653576":"markdown","80fff666":"markdown","6ccf6c4c":"markdown","ca579a32":"markdown","ed4fd789":"markdown","3d875a12":"markdown","1b451386":"markdown","0ea72b0b":"markdown"},"source":{"11023bce":"import pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","a8c4671e":"from sklearn.model_selection import GroupKFold, KFold\nimport lightgbm as lgb","0e92c1d2":"path = '..\/input\/optiver-realized-volatility-prediction'","f650b8ab":"train = pd.read_csv(os.path.join(path,'train.csv'))","b3771c5a":"test = pd.read_csv(os.path.join(path,'test.csv'))","65c4559d":"train['row_id'] = train['stock_id'].astype('str') + '-' + train['time_id'].astype('str')","23d458dc":"test['row_id'] = test['stock_id'].astype('str') + '-' + test['time_id'].astype('str')","57bf417b":"test","c3037b56":"book = pd.read_parquet(os.path.join(path,'book_train.parquet\/stock_id=0'))\ntrade = pd.read_parquet(os.path.join(path,'trade_train.parquet\/stock_id=0'))","7fa3abe8":"book.shape, trade.shape","9ea5d7d4":"def calculate_wap(book_example):\n    book_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) \/ (\n                                       book_example['bid_size1']+ book_example['ask_size1'])\n    \n    return book_example","3c4c6d0c":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()","40b2f93d":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","5dac8aa7":"import os\nfrom sklearn.metrics import r2_score\nimport glob\nlist_order_book_file_train = glob.glob(os.path.join(path,'book_train.parquet\/*'))","bb0e5789":"def realized_volatility_per_time_id(file_path,prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    \n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  \/ (\n                                      df_book_data['bid_size1']+ df_book_data[\n                                  'ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    \n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]","40292ba9":"def past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in tqdm(list_file):\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\ndf_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                           prediction_column_name='pred')","b5832844":"#merge train and realized per time \ntrain = train.merge(df_past_realized_train, on='row_id',how='left')","664b6962":"train.shape","d61f5aae":"train.head()","a5893e1e":"train_stock_aggs = train.groupby('stock_id')['target'].agg(['mean','var','std']).reset_index()","58f1634f":"train_stock_aggs.head()","db37be3e":"fig = px.line(train_stock_aggs,x='stock_id',y=['mean','std'])\nfig.show()","eff5a7d9":"fig = px.line(train_stock_aggs,x='stock_id',y=['var'])\nfig.show()","99c68202":"ax = sns.lineplot(x = \"stock_id\", y = \"target\", \n                  markers = True, dashes = False, data = train)\nplt.axhline(train['target'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\nplt.xticks(rotation = 45)\nplt.title('stock vs target')\nplt.xlabel('Stock_id')\nplt.ylabel('target')\nplt.legend(loc='best')","7c6d11c0":"ax = sns.lineplot(x = \"stock_id\", y = \"target\", \n                  markers = True, dashes = False, data = train)\nplt.axhline(train['target'].std(), color='red', linestyle='--', linewidth=2, label='Std')\nplt.xticks(rotation = 45)\nplt.title('stock vs target')\nplt.xlabel('Stock_id')\nplt.ylabel('target')\nplt.legend(loc='best')","2a41591f":"ax = sns.lineplot(x = \"stock_id\", y = \"pred\", \n                  markers = True, dashes = False, data = train)\nplt.axhline(train['pred'].std(), color='red', linestyle='--', linewidth=2, label='Std')\nplt.xticks(rotation = 45)\nplt.title('stock vs pred')\nplt.xlabel('Stock_id')\nplt.ylabel('pred')\nplt.legend(loc='best')","ab14c6e7":"list_trade_book_file_train = glob.glob(os.path.join(path,'trade_train.parquet\/*'))","8917a233":"def trade_per_time_id(file_path):\n    df_trade_data = pd.read_parquet(file_path)\n    target_df = df_trade_data.groupby('time_id')['price','order_count','size'].agg(['mean','var','std']).reset_index()\n    target_df['stock_id'] = file_path.split('=')[1]\n    target_df.columns = ['time_id','price_mean','price_var','price_std','order_count_mean','order_count_var','order_count_std',\n                    'size_mean','size_var','size_std','stock_id']\n    \n    return target_df\n    ","012fa3e5":"def trade_per_stock(list_file):\n    df_past_trade = pd.DataFrame()\n    for file in tqdm(list_file):\n        df_past_trade = pd.concat([df_past_trade,\n                                     trade_per_time_id(file)])\n    return df_past_trade\ndf_past_trade_train = trade_per_stock(list_file=list_trade_book_file_train)","cd5229fb":"df_past_trade_train['row_id'] = df_past_trade_train['stock_id'].astype('str') + '-' + df_past_trade_train['time_id'].astype('str')","7d0f1927":"df_past_trade_train.drop(['time_id','stock_id'], axis=1, inplace=True)","968749e3":"df_past_trade_train.shape","69230b67":"train = train.merge(df_past_trade_train, on='row_id',how='left')","e2f45c30":"train","2e8f4bad":"px.imshow(train.drop(['time_id','stock_id','row_id'],axis=1).corr())","cc53bb6b":"#processing test files","c7988d0f":"list_order_book_file_test = glob.glob(os.path.join(path,'book_test.parquet\/*'))\nlist_trade_book_file_test = glob.glob(os.path.join(path,'trade_test.parquet\/*'))","a287dae0":"df_past_realized_test = past_realized_volatility_per_stock(list_file=list_order_book_file_test,\n                                                           prediction_column_name='pred')\n#merge test and realized per time\nprint(df_past_realized_test)\ntest = test.merge(df_past_realized_test, on='row_id',how='left')\ndf_past_trade_test = trade_per_stock(list_file=list_trade_book_file_test)\n\ndf_past_trade_test['row_id'] = df_past_trade_test['stock_id'].astype('str') + '-' + df_past_trade_test['time_id'].astype('str')\n\ndf_past_trade_test.drop(['time_id','stock_id'], axis=1, inplace=True)\nprint(df_past_trade_test)\ntest = test.merge(df_past_trade_test, on='row_id',how='left')","57b92076":"test","4451689f":"group = GroupKFold(n_splits=5)\nkfol = KFold(n_splits=5, shuffle=True, random_state=42)","47cf573a":"X = train[['stock_id','pred','price_var','price_std']]\nY = train['target']","ae04771b":"x_test = test[['stock_id','pred','price_var','price_std']]","4a2c2772":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","376274a5":"# Create out of folds array\noof_predictions = np.zeros(X.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","59ae8dca":"seed = 29\nparams = {\n        'learning_rate': 0.25,        \n        'lambda_l1': 2.154360665259325,\n        'lambda_l2': 6.711089761523827,\n        'num_leaves': 2769,\n        'min_sum_hessian_in_leaf': 20.44437160769411,\n        'feature_fraction': 0.7921473067441019,\n        'feature_fraction_bynode': 0.8083803860191322,\n        'bagging_fraction': 0.9726755660563261,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 1690,\n        'max_depth': 4,\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1,\n    }","4fe54b2d":"for fold, (trn_ind, val_ind) in enumerate(kfol.split(X, Y)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n    y_train, y_val = Y.iloc[trn_ind], Y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n    model = lgb.train(params = params, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 600, \n                          early_stopping_rounds = 67, \n                          verbose_eval = 200,\n                          feval = feval_rmspe)\n    oof_predictions[val_ind] = model.predict(x_val)\n    test_predictions += model.predict(x_test) \/ 5\n    plt.figure(figsize=(12,6))\n    lgb.plot_importance(model, max_num_features=10)\n    plt.title(\"Feature importance\")\n    plt.show()","c9c8b8e4":"rmspe_score = rmspe(Y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","60b0e4c4":"test_predictions","2bbe2cc2":"test['target'] = test_predictions","c575cbee":"test = test[['row_id','target']]","72ff181b":"test.to_csv('submission.csv',index=False)","5fe77529":"**In this notebook will try to figure out what this competition is really asking us to predict**\n\n<br><br>     \n     \n     \n Below are the questions after the first glance of data;\n  \n     1) what is meant by Volatility\n  \n     2) How to read parquet files like book and trade.\n     \n     4) what is book and trade\n      \n     5) Why did they use wap formula as bidask + askbid but not bidbid or askask\n  \n     6) What fields are given in train file and what is given in the test file to predict","e712de71":"**Parquet files**\n\nTo get a good understanding of what is parquet file and how different it is from csv, you can read below article\n\nhttps:\/\/dzone.com\/articles\/how-to-be-a-hero-with-powerful-parquet-google-and\n\n\nIn short, when you have huge data and nested schemas \"parquet is the file format\" to use. \n\nYou can get an idea from the below screen shot;\n\n![image.png](attachment:6b3eb30c-bd33-4650-8395-e79d0685093c.png)\n\n\n\nTo read parquet file we can use pandas library like below\n\n    book = pd.read_parquet(os.path.join(path,'book_train.parquet'))\n\nit will read the whole parquet file, but if you want to read only a particular stock\n\n    book = pd.read_parquet(os.path.join(path,'book_train.parquet\/stock_id=0'))\n\n","dc7055a6":"**Book and Trade**\n\nBook: It is snapshot for a given second in the trading window of 10mins\n\nFor example, the trading had happened 1sec, 5sec, 10sec\n\nSo, the book rows will be from 1,5 and 10 respectively.\n\n\nTrade: It is aggregation of all individual orders at the second.\n\n\n\n","8c6740db":"**Volatility**\n\n\nIt is simply the fluctuation of prices for a given time period. For example, the prices of cryptocurrencies and the  returns on crypto are unpredictable. \n\n","3f653576":"Most of the time_id's are having 112 stocks","80fff666":"columns : pred(realized volatility of past 10min), price_var and price_std(of trade data) are having higher correlation with target.","6ccf6c4c":"**WAP**\n\n\nHere, the competition host tried to explain why the calculation is done like that.\n\nSo, if bidsize>>asksize (prices go up)  and if asksize>>bidsize (prices go down)\n\n<br>\n\n![image.png](attachment:8377bd0e-0320-4fea-a89b-288d4e5fe5bf.png)\n\n","ca579a32":"## Making a prediction with basic fields","ed4fd789":"Does this mean all the higher peaks will always result in higher volatility ??","3d875a12":"**Reading the parquet files**","1b451386":"**processing trade data**","0ea72b0b":"**Prediction**\n\n![image.png](attachment:8e38fa23-1ecc-411a-9191-8a2ec607ac15.png)\n\n\n"}}