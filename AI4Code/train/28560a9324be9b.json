{"cell_type":{"63aef6d2":"code","fffd8922":"code","b7143722":"code","f201640b":"code","c7a8e1ad":"code","f0221477":"code","89eba758":"code","8e94cb5b":"code","bd75dae0":"code","2475912a":"code","e11fc7fc":"code","af5513ee":"code","07dd506b":"code","4cb33944":"code","41c2b6a4":"code","7f43a9e3":"code","8a4623fb":"code","9e7286fc":"code","3b49c340":"code","28610570":"code","d69b9381":"code","540a7b24":"code","e98f75a7":"code","8afb71a0":"code","26ba7504":"code","26110b05":"code","0c8028b0":"code","5ebcf704":"code","740aefad":"code","37fe6e2e":"code","688439c0":"code","80661b1f":"code","1580ebf4":"code","68c7fbec":"code","c3560dbe":"code","2d3fc99c":"code","159db505":"code","2255153c":"code","b5c922b8":"code","ef010b02":"code","81134e0d":"code","49117556":"code","e19bce65":"code","fcbf4659":"code","d8c50524":"code","7b9dde62":"code","385f311c":"code","f5b8fe91":"code","575954e0":"code","68996d1a":"code","ba03811f":"code","8befc910":"code","e0e3ea04":"code","6fa49183":"code","fe4fffac":"code","3589393b":"code","4f5f014b":"code","a29475d5":"code","54aaf546":"code","5b520e0c":"code","df24d248":"code","3063b335":"code","4f88e1d7":"code","bdaf0969":"code","d31650be":"code","cd0c5850":"code","163c9529":"code","24d69b30":"code","498913d3":"code","7240e99a":"code","d4b3494b":"code","a9e0e51c":"code","a27f3479":"code","8c0bb765":"code","effb183d":"code","6d115523":"code","655cb735":"code","eff6e86b":"code","3614967e":"code","269a5c93":"code","656eb501":"code","b66686a4":"code","5d9f3dd8":"code","48c5ba12":"code","a142193f":"code","bc766afb":"code","53d1fda3":"code","a2181694":"code","d931fd4f":"code","9c97dade":"code","cb346adb":"code","2f30de3a":"code","40cb1042":"code","b8c7fa73":"code","f824849f":"code","f3a6b4d4":"code","6c78d67b":"code","5747babb":"code","bea36c13":"code","43a47677":"code","f415d4f5":"code","90fb651b":"code","9b99bc3d":"code","e90ccee4":"code","595bec01":"code","ac779d0d":"code","221bf979":"code","135aa100":"code","39b43235":"code","eaeff4d1":"code","269e1981":"code","2789fe56":"code","c198d4c6":"code","9b818594":"code","543d67a9":"code","2dceed55":"code","ea48524f":"code","157189a8":"code","3e5a6ddb":"code","119f94cc":"code","8a85f7aa":"code","1a03a54c":"code","5ea95fc1":"code","bde2c156":"code","eaf25f1c":"code","393327ca":"code","25c227ef":"code","aff7339b":"code","e127e982":"code","edbb316d":"code","30520888":"code","33270e0b":"code","da3160b0":"code","a4d2290e":"code","f4240201":"code","300d3574":"code","9f50dc1f":"code","7f1501b7":"code","7ffb34d1":"code","e28b45a8":"code","c56aa651":"code","7f8d05fb":"code","8fe45d78":"code","e08bee5b":"code","ebe937bb":"code","95ddc5d6":"code","9632706c":"code","32ff7263":"code","f3becb56":"code","a53f418f":"code","c3ba17ca":"code","937867bf":"code","24df4de9":"code","d5c39b61":"code","a96925b1":"code","a5c41453":"code","d06901f6":"code","60c7f8f5":"code","f8069f36":"code","ac401c8e":"code","4c68290f":"code","14d8c95b":"code","38689d1a":"code","7d146afa":"code","47c9e49d":"code","bd1b2f91":"code","2193add5":"code","06e4c272":"code","8e142f1d":"code","707f2785":"code","175c3b68":"code","69aa24be":"code","2beda275":"code","8193b17f":"code","78d29b81":"code","a18b73b2":"code","c339a474":"code","99d5bfee":"markdown","51dfd64d":"markdown","ffd5b599":"markdown","b019aba3":"markdown","b9f5cdf7":"markdown","63b49f53":"markdown","84b8f939":"markdown","5be93cec":"markdown","475c9699":"markdown","1c672ea8":"markdown","bc368dbb":"markdown","72ed9f78":"markdown","5fdb9122":"markdown","540b3615":"markdown","54db26e9":"markdown","e2a6fff4":"markdown","e9d78145":"markdown","6496e4e3":"markdown","c57d9e2a":"markdown","36d1d7fb":"markdown","9fe92a8f":"markdown","b7e3ef5b":"markdown","e377d57e":"markdown","7969125d":"markdown","f2c4154f":"markdown","73847229":"markdown","b9b4d3bc":"markdown","0002f6c5":"markdown","7ada8db5":"markdown","6eb866f8":"markdown","7887f46f":"markdown","799098b3":"markdown","290935d2":"markdown","d67434ce":"markdown","e2d34cc8":"markdown","d3310f61":"markdown","82a656bf":"markdown","8c8df21a":"markdown","0ee26fee":"markdown","394b5998":"markdown","73ebebea":"markdown","e20fc0e3":"markdown","e327789c":"markdown","4eb4d109":"markdown"},"source":{"63aef6d2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nfrom typing import Tuple, List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline\n\n\n# read data\nin_kaggle = True\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '..\/input\/tabular-playground-series-feb-2021\/train.csv'\n        test_path = '..\/input\/tabular-playground-series-feb-2021\/test.csv'\n        sample_submission_path = '..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data\/train.csv'\n        test_path = 'data\/test.csv'\n        sample_submission_path = 'data\/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path\n\n# cascatter implementation - reused from https:\/\/github.com\/myrthings\/catscatter\/blob\/master\/catscatter.py\n# (c) Myr Barn\u00e9s, 2020\n# More info about this function is available at\n# - https:\/\/towardsdatascience.com\/visualize-categorical-relationships-with-catscatter-e60cdb164395\n# - https:\/\/github.com\/myrthings\/catscatter\/blob\/master\/README.md\ndef catscatter(df,colx,coly,cols,color=['grey','black'],ratio=10,font='Helvetica',save=False,save_name='Default'):\n    '''\n    Goal: This function create an scatter plot for categorical variables. It's useful to compare two lists with elements in common.\n    Input:\n        - df: required. pandas DataFrame with at least two columns with categorical variables you want to relate, and the value of both (if it's just an adjacent matrix write 1)\n        - colx: required. The name of the column to display horizontaly\n        - coly: required. The name of the column to display vertically\n        - cols: required. The name of the column with the value between the two variables\n        - color: optional. Colors to display in the visualization, the length can be two or three. The two first are the colors for the lines in the matrix, the last one the font color and markers color.\n            default ['grey','black']\n        - ratio: optional. A ratio for controlling the relative size of the markers.\n            default 10\n        - font: optional. The font for the ticks on the matrix.\n            default 'Helvetica'\n        - save: optional. True for saving as an image in the same path as the code.\n            default False\n        - save_name: optional. The name used for saving the image (then the code ads .png)\n            default: \"Default\"\n    Output:\n        No output. Matplotlib object is not shown by default to be able to add more changes.\n    '''\n    # Create a dict to encode the categeories into numbers (sorted)\n    colx_codes=dict(zip(df[colx].sort_values().unique(),range(len(df[colx].unique()))))\n    coly_codes=dict(zip(df[coly].sort_values(ascending=False).unique(),range(len(df[coly].unique()))))\n    \n    # Apply the encoding\n    df[colx]=df[colx].apply(lambda x: colx_codes[x])\n    df[coly]=df[coly].apply(lambda x: coly_codes[x])\n    \n    \n    # Prepare the aspect of the plot\n    plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n    plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n    plt.rcParams['font.sans-serif']=font\n    plt.rcParams['xtick.color']=color[-1]\n    plt.rcParams['ytick.color']=color[-1]\n    plt.box(False)\n\n    \n    # Plot all the lines for the background\n    for num in range(len(coly_codes)):\n        plt.hlines(num,-1,len(colx_codes)+1,linestyle='dashed',linewidth=1,color=color[num%2],alpha=0.5)\n    for num in range(len(colx_codes)):\n        plt.vlines(num,-1,len(coly_codes)+1,linestyle='dashed',linewidth=1,color=color[num%2],alpha=0.5)\n        \n    # Plot the scatter plot with the numbers\n    plt.scatter(df[colx],\n               df[coly],\n               s=df[cols]*ratio,\n               zorder=2,\n               color=color[-1])\n    \n    # Change the ticks numbers to categories and limit them\n    plt.xticks(ticks=list(colx_codes.values()),labels=colx_codes.keys(),rotation=90)\n    plt.yticks(ticks=list(coly_codes.values()),labels=coly_codes.keys())\n    plt.xlim(xmin=-1,xmax=len(colx_codes))\n    plt.ylim(ymin=-1,ymax=len(coly_codes))\n    \n    # Save if wanted\n    if save:\n        plt.savefig(save_name+'.png')\n        \n# auxiliary function to build a tailored catscatter plot adapted to the current dataset\ndef build_catscatter_plot(\n    data: pd.DataFrame,\n    cat_x: str,\n    cat_y: str,\n    fig_size_x: int = 80,\n    fig_size_y: int = 80,\n    ratio: int = 10,\n    font_size: int = 80\n):\n    # aggregate record counts by different labels of cat_x and cat_y\n    agg_data = data.groupby([cat_x, cat_y]).size().reset_index(name='record_count')\n    # define the color map\n    colors=['blue', 'grey', 'green']\n    \n    # create the plot\n    plt.figure(figsize=(80,80))\n    catscatter(agg_data , cat_x, cat_y, 'record_count', font='Helvetica', color=colors, ratio=ratio)\n\n    plt.xticks(fontsize=font_size)\n    plt.yticks(fontsize=font_size)\n    plt.show()\n    \n# auxiliary function to build a tailored Plotly Express treemap plot adapted to the current dataset\ndef build_treemap(\n    data: pd.DataFrame,\n    path_cols: List[str],\n    value_col: str,\n    color_col: str,\n    mid_point: float=0.5,\n):\n    prefix = 'Tree Map for Path: '\n    separator = '-'\n    plot_title = \"\".join([prefix, separator.join(path_cols)])\n    \n    plot_title = \"\".join([plot_title, \" (areas sized by \", value_col, \", colored by \", color_col, \")\"])\n    \n    fig = px.treemap(\n        data, \n        path=path_cols, \n        values=value_col, \n        color=color_col, \n        color_continuous_midpoint=mid_point, \n        color_continuous_scale=px.colors.diverging.Portland,\n        title=plot_title\n    )\n    fig.show()\n\n# auxiliary function to build a tailored Plotly Express treemap plot adapted to the current dataset\ndef build_sunburst_plot(\n    data: pd.DataFrame,\n    path_cols: List[str],\n    value_col: str,\n):\n    agg_df = data.groupby(path_cols).agg({value_col: 'count'}).reset_index()\n    prefix = 'Sunburst Targets to Path: '\n    separator = '-'\n    plot_title = \"\".join([prefix, separator.join(path_cols)])\n    fig = px.sunburst(agg_df,\n                  path=path_cols,\n                  values='target',\n                  branchvalues='total', # other value: 'remainder'\n                  height=600,\n                  title=plot_title\n                  )\n    fig.update_layout(\n        font_size=12,\n        title_font_color=\"black\",\n    )\n    fig.show()","fffd8922":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","b7143722":"%%time\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\ndf_train = pd.read_csv(train_set_path)\ndf_test = pd.read_csv(test_set_path)\n\nsubm = pd.read_csv(sample_subm_path)","f201640b":"df_train.info()","c7a8e1ad":"path_cols = ['cat2', 'cat5', 'cat6']","f0221477":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)","89eba758":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] ","8e94cb5b":"build_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","bd75dae0":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","2475912a":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","e11fc7fc":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","af5513ee":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","07dd506b":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","4cb33944":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","41c2b6a4":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","7f43a9e3":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","8a4623fb":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","9e7286fc":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","3b49c340":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","28610570":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","d69b9381":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","540a7b24":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","e98f75a7":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","8afb71a0":"path_cols = ['cat2', 'cat5', 'cat7']","26ba7504":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","26110b05":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","0c8028b0":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","5ebcf704":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","740aefad":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","37fe6e2e":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","688439c0":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","80661b1f":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","1580ebf4":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","68c7fbec":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","c3560dbe":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","2d3fc99c":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","159db505":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","2255153c":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","b5c922b8":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","ef010b02":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","81134e0d":"path_cols = ['cat2', 'cat5', 'cat8']","49117556":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","e19bce65":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","fcbf4659":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","d8c50524":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","7b9dde62":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","385f311c":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","f5b8fe91":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","575954e0":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","68996d1a":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","ba03811f":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","8befc910":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","e0e3ea04":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","6fa49183":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","fe4fffac":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","3589393b":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","4f5f014b":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","a29475d5":"path_cols = ['cat2', 'cat5', 'cat9']","54aaf546":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","5b520e0c":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","df24d248":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","3063b335":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","4f88e1d7":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","bdaf0969":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","d31650be":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","cd0c5850":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","163c9529":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","24d69b30":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","498913d3":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","7240e99a":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","d4b3494b":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","a9e0e51c":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","a27f3479":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","8c0bb765":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","effb183d":"path_cols = ['cat5', 'cat6', 'cat7']","6d115523":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","655cb735":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","eff6e86b":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","3614967e":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","269a5c93":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","656eb501":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","b66686a4":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","5d9f3dd8":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","48c5ba12":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","a142193f":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","bc766afb":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","53d1fda3":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","a2181694":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","d931fd4f":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","9c97dade":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","cb346adb":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","2f30de3a":"path_cols = ['cat5', 'cat6', 'cat8']","40cb1042":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","b8c7fa73":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","f824849f":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","f3a6b4d4":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","6c78d67b":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","5747babb":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","bea36c13":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","43a47677":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","f415d4f5":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","90fb651b":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","9b99bc3d":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","e90ccee4":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","595bec01":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","ac779d0d":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","221bf979":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","135aa100":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","39b43235":"path_cols = ['cat5', 'cat6', 'cat9']","eaeff4d1":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","269e1981":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","2789fe56":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","c198d4c6":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","9b818594":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","543d67a9":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","2dceed55":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","ea48524f":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","157189a8":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","3e5a6ddb":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","119f94cc":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","8a85f7aa":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","1a03a54c":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","5ea95fc1":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","bde2c156":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","eaf25f1c":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","393327ca":"path_cols = ['cat6', 'cat7', 'cat8']","25c227ef":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","aff7339b":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","e127e982":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","edbb316d":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","30520888":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","33270e0b":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","da3160b0":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","a4d2290e":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","f4240201":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","300d3574":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","9f50dc1f":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","7f1501b7":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","7ffb34d1":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","e28b45a8":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","c56aa651":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","7f8d05fb":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","8fe45d78":"path_cols = ['cat6', 'cat7', 'cat9']","e08bee5b":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","ebe937bb":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","95ddc5d6":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","9632706c":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","32ff7263":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","f3becb56":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","a53f418f":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","c3ba17ca":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","937867bf":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","24df4de9":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","d5c39b61":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","a96925b1":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","a5c41453":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","d06901f6":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","60c7f8f5":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","f8069f36":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","ac401c8e":"path_cols = ['cat7', 'cat8', 'cat9']","4c68290f":"# build a sunburst plot with the record count per cat-to-cat buckets\nbuild_sunburst_plot(\n    data=df_train,\n    path_cols=path_cols,\n    value_col='target',\n)\n","14d8c95b":"agg_df = df_train.groupby(path_cols).agg({'target': ['mean', 'count']}).reset_index()\nagg_df.columns = [path_cols[0], path_cols[1], path_cols[2], 'target_mean', 'target_count'] \n\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col=\"target_count\",\n    color_col=\"target_mean\",\n    mid_point=7.0,\n)","38689d1a":"agg_df = df_train.groupby(path_cols).agg({'cont0': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont0',\n    mid_point=0.5,\n)","7d146afa":"agg_df = df_train.groupby(path_cols).agg({'cont1': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont1',\n    mid_point=0.5,\n)","47c9e49d":"agg_df = df_train.groupby(path_cols).agg({'cont2': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont2',\n    mid_point=0.5,\n)","bd1b2f91":"agg_df = df_train.groupby(path_cols).agg({'cont3': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont3',\n    mid_point=0.5,\n)","2193add5":"agg_df = df_train.groupby(path_cols).agg({'cont4': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont4',\n    mid_point=0.5,\n)","06e4c272":"agg_df = df_train.groupby(path_cols).agg({'cont5': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont5',\n    mid_point=0.5,\n)","8e142f1d":"agg_df = df_train.groupby(path_cols).agg({'cont6': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont6',\n    mid_point=0.5,\n)","707f2785":"agg_df = df_train.groupby(path_cols).agg({'cont7': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont7',\n    mid_point=0.5,\n)","175c3b68":"agg_df = df_train.groupby(path_cols).agg({'cont8': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont8',\n    mid_point=0.5,\n)","69aa24be":"agg_df = df_train.groupby(path_cols).agg({'cont9': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont9',\n    mid_point=0.5,\n)","2beda275":"agg_df = df_train.groupby(path_cols).agg({'cont10': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont10',\n    mid_point=0.5,\n)","8193b17f":"agg_df = df_train.groupby(path_cols).agg({'cont11': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont11',\n    mid_point=0.5,\n)","78d29b81":"agg_df = df_train.groupby(path_cols).agg({'cont12': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont12',\n    mid_point=0.5,\n)","a18b73b2":"agg_df = df_train.groupby(path_cols).agg({'cont13': 'mean', 'target': 'count'}).reset_index()\nbuild_treemap(\n    data=agg_df,\n    path_cols=path_cols,\n    value_col='target',\n    color_col='cont13',\n    mid_point=0.5,\n)","c339a474":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","99d5bfee":"We can see that the largest clusters of the observations in the training set (within the space of *'cat2'-'cat5'-'cat9'*) are as follows\n\n- 'cat2'=A -'cat5'=B - 'cat9'=F\n- 'cat2'=A -'cat5'=B - 'cat9'=F","51dfd64d":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat5'-'cat6'-'cat7' space\n- it is therefore not productive to bin the rare classes of 'cat5', 'cat6', and 'cat7' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat5', 'cat6', and 'cat7')","ffd5b599":"## Multi-Variative Associations Between 'cat7', 'cat8', and 'cat9'","b019aba3":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat2'-'cat5'-'cat8' space\n- it is therefore not productive to bin the rare classes of 'cat5', and 'cat8' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat2', 'cat5', and 'cat8')","b9f5cdf7":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat5'-'cat6'-'cat9' space\n- it is therefore not productive to bin the rare classes of 'cat5', 'cat6', and 'cat9' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat5', 'cat6', and 'cat9')","63b49f53":"We see that the individual strong associations of *target* with *'cat5','cat6', and 'cat9'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat5'-'cat6'-'cat9'* dimensions. For instance,\n\n- the subset of 'C-A-I' demonstrated the smallest mean *target* value within the training set (7.00+)\n- the subsets of 'D-C-C' demonstrated the biggest mean *target* value within the training set (8.02+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat5'-'cat6'-'cat9' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat5'-'cat6'-'cat9' feature space","84b8f939":"We see that the individual strong associations of *target* with *'cat2','cat5', and 'cat6'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat2'-'cat5'-'cat6'* dimensions. For instance,\n\n- the subset of 'A-C-A' demonstrated the smallest mean *target* value within the training set (7.1399)\n- the subsets of 'A-B-C' and 'A-D-C' demonstrated the biggest mean *target* value within the training set (7.83+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat2'-'cat5'-'cat6' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat2'-'cat5'-'cat6' feature space","5be93cec":"We can see that the most of the records in the training set are withing the clusters in *'cat2'-'cat5'-'cat6'* as follows\n\n- 'cat2'='A'-'cat5'='B'-'cat6'='A'\n- 'cat2'='A'-'cat5'='D'-'cat6'='A'","475c9699":"We see that the individual strong associations of *target* with *'cat2','cat5', and 'cat7'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat2'-'cat5'-'cat7'* dimensions. For instance,\n\n- the subset of 'A-C' demonstrated the smallest mean *target* value within the training set (7.14+)\n- the subsets of 'B-D' and 'B-B' demonstrated the biggest mean *target* value within the training set (7.76+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat2'-'cat5'-'cat7' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat2'-'cat5'-'cat7' feature space","1c672ea8":"## Multi-Variative Associations Between 'cat5', 'cat6', and 'cat8'","bc368dbb":"## Multi-Variative Associations Between 'cat2', 'cat5', and 'cat9'","72ed9f78":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat6'-'cat7'-'cat8' space\n- it is therefore not productive to bin the rare classes of 'cat7' and 'cat8' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat6', 'cat7', and 'cat8')","5fdb9122":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat2'-'cat5'-'cat9' space\n- it is therefore not productive to bin the rare classes of 'cat5', and 'cat9' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat2', 'cat5', and 'cat9')","540b3615":"We can see that the largest clusters of the observations in the training set (within the space of *'cat5'-'cat6'-'cat9'*) are as follows\n\n- 'cat5'=B - 'cat6'=A - 'cat9'=F\n- 'cat5'=D - 'cat6'=A - 'cat9'=F","54db26e9":"We can see that the largest clusters of the observations in the training set (within the space of *'cat2'-'cat5'-'cat8'*) are as follows\n\n- 'cat2'=A -'cat5'=B - 'cat8'=C\n- 'cat2'=A -'cat5'=B - 'cat8'=E\n- 'cat2'=A -'cat5'=D - 'cat8'=C","e2a6fff4":"We see that the individual strong associations of *target* with *'cat5','cat6', and 'cat8'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat5'-'cat6'-'cat8'* dimensions. For instance,\n\n- the subset of 'C-A-G' demonstrated the smallest mean *target* value within the training set (7.08+)\n- the subsets of 'D-C-C' demonstrated the biggest mean *target* value within the training set (8.02+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat5'-'cat6'-'cat8' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat5'-'cat6'-'cat8' feature space","e9d78145":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat5'-'cat6'-'cat8' space\n- it is therefore not productive to bin the rare classes of 'cat5', 'cat6', and 'cat8' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat5', 'cat6', and 'cat8')","6496e4e3":"## Express Analysis Insights\n\nAs we can see, the simple express EDA analysis (https:\/\/www.kaggle.com\/gvyshnya\/generic-express-eda-with-comprehensive-insights) yielded a lot of useful insights out of the box, in less then 20 minutes of the data crunching. Below are the key finding from the charts generated by *AutoViz* on a generic basis (see https:\/\/www.kaggle.com\/gvyshnya\/generic-express-eda-with-comprehensive-insights for more details).\n\n### Feature-to-Target Relations\n\nWe find that the training set data manifests the following relations between the *target* and feature variables\n\n- It seems like the training set observations with *target* < 3.5 or\/and cont5 < 0.1 could be clearly attributed to as outliers\n- Target variable is a little skewed to the right\n- There is no any numeric feature that is highly correlated with *target*\n- *target* distribution by the labels of the respective cat variables demonstrated that there is a relatively huge association of the *target* with *cat2, cat5, cat6, cat7, cat9*\n- The best association between the *target* values and the cat labels is demonstrated by cat7\n- In turn, there is a weaker association of the target variable with the rest of categorical features\n\n\n### Numeric Feature Findings\n\nIt is demonstrated that\n\n- There is a clear separation of the observations in the training and test sets into well-contained and well separable clusters by the values of *cont1* (6-8 clusters observed, subject to further clustering experiments)\n- Distribution of the continual variables is identic on both the training and testing sets (the details for each variables are provided below)\n- *cont3, cont4, cont5, cont6*, and *cont12* are highly skewed to the left \n- *cont8, cont9, cont10, cont11*, and *cont13* have a polynomial distribution (binomial distribution, presumably)\n- *cont1* and *cont11* are skewed to the right\n- *cont0, cont2* have almost normal distribution\n- as per the review of the respective violin plots on the training set, it could be possible to use the extreme tail values of *cont0, cont5, cont6*, and *cont12* for the outlier removal when training the ML models on the training set\n- there are several quite highly correlated numeric feature pairs detected on the training and test sets (with the Pierson\u2019s correlation coefficient >= 0.6): *cat5-cat8, cat5-cat9*, and *cat5-cat12* (among them, *cat5* has the highest absolute correlation with the target variable on the training set)\n\n\n### Categorical Feature Findings\n\nIt has been detected that\n\n- *cat0* is a two-label categorical variable, and it is unbalance by the label value distribution on the training set (\u2018A\u2019 drastically predominates \u2018B\u2019)\n- *cat1* is a two-label categorical variable, and it is unbalanced a little (\u2018A\u2019 vs. \u2018B\u2019)\n- *cat2* is a two-label categorical variable, and it is unbalance by the label value distribution on the training set (\u2018A\u2019 drastically predominates \u2018B\u2019)\n- *cat3* is a four-label categorical variable, and two of its labels (\u2018C\u2019, \u2018A\u2019) predominate the rest of the labels (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat4* is a four-label categorical variable, and one of its labels (\u2018A\u2019) predominates others (such labels can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat5* is a four-label categorical variable, and two of its labels (\u2018B\u2019, \u2018D\u2019) predominate the rest of the labels (the latter ones can be binned into a single category label \u2018Other\u2019, cont reduce the dimensionality of the respective feature space)\n- *cat6* is an eight-label categorical variable, and one of its labels (\u2018E\u2019) predominates others (such labels can be binned into a single category label \u2018Other\u2019, to reduce the\n- *cat8* is a 7-label categorical variable, and 4 of its categories (\u2018C\u2019, \u2018E\u2019, \u2018G\u2019, and \u2018A\u2019) predominate the rest of the categories on the training set (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat9* is a 15-label categorical variable, and 3 of its labels (\u2018F\u2019, \u2018I\u2019, and \u2018L\u2019) predominate others (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n\n### Categorical-to-Numerical Feature Associations\n\nThere are quite strong associations found between the following categorical and numerical features on the training set\n\n- cont1 by cat3\n- cont5 by cat3\n- cont6 by cat3\n- cont9 by cat3\n- cont10 by cat3\n- cont11 by cat3\n- cont12 by cat3\n- cont0 by cat4\n- cont5 by cat4\n- cont6 by cat4\n- cont8 by cat4\n- cont9 by cat4\n- cont10 by cat4\n- cont11 by cat4\n- cont12 by cat4\n- cont13 by cat4\n- all continual variables by cat5\n- all continual variables by cat6\n- all continual variables by cat7\n- all continual variables by cat8\n- cont0 by cat9\n- cont1 by cat9\n- cont2 by cat9\n- cont5 by cat9\n- cont6 by cat9\n- cont8 by cat9\n- cont9 by cat9\n- cont10 by cat9\n- cont11 by cat9\n- cont12 by cat9\n- cont13 by cat9\n- cont1 by cat1\n\nThe above-mentioned continual-to-categorical feature associations are also confirmed on the test set","c57d9e2a":"## Multi-Variative Associations Between 'cat6', 'cat7', and 'cat9'","36d1d7fb":"We can see that the largest clusters of the observations in the training set (within the space of *'cat6'-'cat7'-'cat9'*) are as follows\n\n- 'cat6'=A - 'cat7'=E - 'cat9'=F\n- 'cat6'=A - 'cat7'=E - 'cat9'=I\n- 'cat6'=A - 'cat7'=E - 'cat9'=L","9fe92a8f":"# Roadmap For Additional EDA Visualizations\n\nThe good insights we quickly got from the express EDA Analysis with *AutoViz* above were very helpful per se. However, they did not address all and every analytical issues we would like to address, when tackling the fundumantal question of what the impact of features on the *target* are.\n\nNow we are going to undertake the additional manual EDA discoveries to review\n\n- pair associations between the selective cat variables\n- multi-variative associations between selective cat variables, factored by the impact of such association on the conditional distributions of the *target* and numeric features on the training set\n- initial analysis on the optimal feature engineering steps to take in the ML experiments phase down the road\n\nWhile doing it, we will be paying the most attention to the cat features highlighted in the express EDA analysis above. These are\n\n- *cat2, cat5, cat6, cat7, and cat9* that have a good association with *target* on the training set\n- *cat8* that has good association with every feature variable both in the training and test sets","b7e3ef5b":"## Multi-Variative Associations Between 'cat6', 'cat7', and 'cat8'","e377d57e":"We can see that the largest clusters of the observations in the training set (within the space of *'cat5'-'cat6'-'cat8'*) are as follows\n\n- 'cat5'=B - 'cat6'=A - 'cat8'=C\n- 'cat5'=B - 'cat6'=A - 'cat8'=E\n- 'cat5'=D - 'cat6'=A - 'cat8'=C\n- 'cat5'=D - 'cat6'=A - 'cat8'=E","7969125d":"We can see that the largest clusters of the observations in the training set (within the space of *'cat5'-'cat6'-'cat7'*) are as follows\n\n- 'cat5'=B - 'cat6'=A - 'cat7'=E\n- 'cat5'=D - 'cat6'=A - 'cat7'=E","f2c4154f":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat6'-'cat7'-'cat9' space\n- it is therefore not productive to bin the rare classes of 'cat7' and 'cat9' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat6', 'cat7', and 'cat9')","73847229":"We see that the individual strong associations of *target* with *'cat2','cat5', and 'cat8'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat2'-'cat5'-'cat8'* dimensions. For instance,\n\n- the subset of 'A-C-G' demonstrated the smallest mean *target* value within the training set (7.07+)\n- the subsets of 'B-D-E' demonstrated the biggest mean *target* value within the training set (7.87+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat2'-'cat5'-'cat8' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat2'-'cat5'-'cat8' feature space","b9b4d3bc":"We see that the individual strong associations of *target* with *'cat5','cat6', and 'cat7'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat5'-'cat6'-'cat7'* dimensions. For instance,\n\n- the subset of 'C-A-E' demonstrated the smallest mean *target* value within the training set (7.14+)\n- the subsets of 'D-C-D' demonstrated the biggest mean *target* value within the training set (8.02+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat5'-'cat6'-'cat7' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat5'-'cat6'-'cat7' feature space","0002f6c5":"## Multi-Variative Associations Between 'cat5', 'cat6', and 'cat7'","7ada8db5":"# Initial Preparations\n\nWe are going to start with the essential pre-requisites as follows\n\n- importing the standard Python packages we need to use down the road\n- programming the useful automation routines for repeatable data visualizations we are going to draw in the Advance Analytical EDA trials down the road","6eb866f8":"We see that the individual strong associations of *target* with *'cat2','cat5', and 'cat9'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat2'-'cat5'-'cat9'* dimensions. For instance,\n\n- the subset of 'A-C-A' demonstrated the smallest mean *target* value within the training set (6.97+)\n- the subsets of 'B-D-L' demonstrated the biggest mean *target* value within the training set (7.94+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat2'-'cat5'-'cat9' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat2'-'cat5'-'cat9' feature space","7887f46f":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat7'-'cat8'-'cat9' space\n- it is therefore not productive to bin the rare classes of 'cat8' and 'cat9' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat7', 'cat8', and 'cat9')","799098b3":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat2'-'cat5'-'cat7' space\n- it is therefore not productive to bin the rare classes of 'cat5', and 'cat7' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat2', 'cat5', and 'cat7')","290935d2":"# Basic Data Overview","d67434ce":"We can see that the largest clusters of the observations in the training set (within the space of *'cat2'-'cat5'-'cat7'*) are as follows\n\n- 'cat2'=A -'cat5'=B - 'cat7'=E\n- 'cat2'=A -'cat5'=D - 'cat7'=E","e2d34cc8":"# Introduction\n\nThis notebook is intended to extract useful insights for the datasets of \u2018Tabular Playground Series - Feb 2021\u2019 competition in Kaggle. For this competition, it is required to tackle the Regression problem to predict a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.\n\nWe are going to perform the complete and comprehensive EDA as follows\n-\tAutomate the generic aspects of EDA with AutoViz, one of the leading freeware Rapid EDA tools in Pythonic Data Science world\n-\tDeep into the problem-specific advanced analytical questions\/discoveries with the custom manual EDA routines programmed on top of standard capabilities of Plotly and Matplotlib\n","d3310f61":"## Multi-Variative Associations Between 'cat5', 'cat6', and 'cat9'","82a656bf":"We find that\n\n- there are statistically meaningful variations in the distribution of every numerical feature on the subsets of the training set within 'cat2'-'cat5'-'cat6' space\n- it is therefore not productive to bin the rare classes of 'cat5' and 'cat6' into the single categories as the significant intelligence could be lost\n- instead of it, we  can try to generate a number of new numeric features (means of each of the raw features, grouped by 'cat2', 'cat5', and 'cat6')","8c8df21a":"# Multi-Variative Analysis of Cat Feature Associations\n\nWe are now going to investigate multi-cat relations among the features in the training set","0ee26fee":"## Multi-Variative Associations Between 'cat2', 'cat5', and 'cat6'","394b5998":"We see that the individual strong associations of *target* with *'cat6','cat7', and 'cat8'* are well displayed in the multi-variate distribution of *target* of the subsets within the *'cat6'-'cat7'-'cat8'* dimensions. For instance,\n\n- the subset of 'A-E-G' demonstrated the smallest mean *target* value within the training set (7.30+)\n- the subsets of 'C' demonstrated the biggest mean *target* value within the training set (7.96+)\n- the rest of the subsets are also quite distinctive in terms of the mean *target* value for each of them\n\n**Statistical Variability of Mean Numeric Feature Values Within 'cat6'-'cat7'-'cat8' Feature Space\n\nNow we are going to see the variability of mean values for every numeric feature by training set clusters, when sliced by category labels within 'cat6'-'cat7'-'cat8' feature space","73ebebea":"We can see that the largest clusters of the observations in the training set (within the space of *'cat7'-'cat8'-'cat9'*) are as follows\n\n- 'cat7'=E - 'cat8'=C - 'cat9'=F\n- 'cat7'=E - 'cat8'=E - 'cat9'=F\n","e20fc0e3":"## Multi-Variative Associations Between 'cat2', 'cat5', and 'cat8'","e327789c":"## Multi-Variative Associations Between 'cat2', 'cat5', and 'cat7'","4eb4d109":"We can see that the largest clusters of the observations in the training set (within the space of *'cat6'-'cat7'-'cat8'*) are as follows\n\n- 'cat6'=A - 'cat7'=E - 'cat8'=E\n- 'cat6'=A - 'cat7'=E - 'cat8'=C"}}