{"cell_type":{"dfd3d30c":"code","be30292e":"code","ce4a1cca":"code","81bcd50b":"code","49dcc9a6":"code","ce5d91fd":"code","7024f294":"code","18526915":"code","d5843fb0":"code","232274d3":"code","8a8caf43":"code","e6c62dda":"code","00227ad7":"code","9c3c042d":"code","e65278df":"code","78723b2f":"code","4fa5465e":"code","07b96569":"code","ea6dcda7":"code","a543487e":"code","67d35564":"code","365583c4":"code","94fac3a2":"code","a8e1755c":"code","a0a06f82":"code","bfce0986":"code","1ba488ea":"code","3637f23c":"code","c7d8f734":"code","37fc63c0":"code","debab88b":"code","a1c18292":"code","b3f814bb":"code","1da7e848":"code","a6394270":"code","dd3e81d6":"code","444d305c":"code","24bf3009":"code","932f8501":"code","d336f473":"code","0adabd16":"code","06aa9cb9":"code","f4747ef0":"code","dbaaaf87":"code","788fedf3":"code","9c07ff3f":"code","7d10f164":"code","cc2ec071":"code","9876315d":"code","9ffea755":"code","81b7e528":"code","02378839":"code","7c08e0ee":"code","a0a3f657":"code","f2d5a2e3":"code","e00fb1c4":"code","69417cdc":"code","b54171f9":"code","081a4d3c":"markdown","8e0ad846":"markdown","86957355":"markdown","2d43d60e":"markdown","21c0f579":"markdown","2166322d":"markdown","c4a06594":"markdown","9e506779":"markdown","d0837590":"markdown","7ccdd710":"markdown","d8983154":"markdown","053b4fd7":"markdown","ace03f89":"markdown","01a7d087":"markdown","7788ad6a":"markdown","978a3d43":"markdown","58bfdb35":"markdown","d4bbb3a5":"markdown","e9770292":"markdown","1444bb43":"markdown"},"source":{"dfd3d30c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","be30292e":"data = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv\")","ce4a1cca":"data.head()","81bcd50b":"data.shape","49dcc9a6":"data.describe()","ce5d91fd":"# \u56fe\u5f62\u53ef\u89c6\u5316,\u67e5\u770b\u6570\u636e\u5206\u5e03\nimport seaborn as sns\n\nsns.countplot(data.target)\n\nplt.show()","7024f294":"new1_data = data[:10000]\nnew1_data.shape","18526915":"# \u56fe\u5f62\u53ef\u89c6\u5316,\u67e5\u770b\u6570\u636e\u5206\u5e03\nimport seaborn as sns\n\nsns.countplot(new1_data.target)\n\nplt.show()","d5843fb0":"# \u968f\u673a\u6b20\u91c7\u6837\u83b7\u53d6\u6570\u636e\n# \u9996\u5148\u9700\u8981\u786e\u5b9a\u7279\u5f81\u503c\\\u6807\u7b7e\u503c\n\ny = data[\"target\"]\nx = data.drop([\"id\", \"target\"], axis=1)","232274d3":"x.head()","8a8caf43":"y.head()","e6c62dda":"# \u6b20\u91c7\u6837\u83b7\u53d6\u6570\u636e\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=0)\n\nX_resampled, y_resampled = rus.fit_resample(x, y)","00227ad7":"x.shape, y.shape","9c3c042d":"X_resampled.shape, y_resampled.shape","e65278df":"# \u56fe\u5f62\u53ef\u89c6\u5316,\u67e5\u770b\u6570\u636e\u5206\u5e03\nimport seaborn as sns\n\nsns.countplot(y_resampled)\n\nplt.show()","78723b2f":"y_resampled.head()","4fa5465e":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_resampled = le.fit_transform(y_resampled)\n","07b96569":"y_resampled","ea6dcda7":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)","a543487e":"x_train.shape, y_train.shape","67d35564":"x_test.shape, y_test.shape","365583c4":"from sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(oob_score=True)\nrf.fit(x_train, y_train)","94fac3a2":"y_pre = rf.predict(x_test)\ny_pre","a8e1755c":"rf.score(x_test, y_test)","a0a06f82":"rf.oob_score_","bfce0986":"# \u56fe\u5f62\u53ef\u89c6\u5316,\u67e5\u770b\u6570\u636e\u5206\u5e03\nimport seaborn as sns\n\nsns.countplot(y_pre)\n\nplt.show()","1ba488ea":"# logloss\u6a21\u578b\u8bc4\u4f30\nfrom sklearn.metrics import log_loss\n\nlog_loss(y_test, y_pre, eps=1e-15, normalize=True)\n","3637f23c":"y_test, y_pre","c7d8f734":"from sklearn.preprocessing import OneHotEncoder\n\none_hot = OneHotEncoder(sparse=False)\n\ny_test1 = one_hot.fit_transform(y_test.reshape(-1, 1))\ny_pre1 = one_hot.fit_transform(y_pre.reshape(-1, 1))\n","37fc63c0":"y_test1","debab88b":"y_pre1","a1c18292":"# logloss\u6a21\u578b\u8bc4\u4f30\n\nlog_loss(y_test1, y_pre1, eps=1e-15, normalize=True)","b3f814bb":"# \u6539\u53d8\u9884\u6d4b\u503c\u7684\u8f93\u51fa\u6a21\u5f0f,\u8ba9\u8f93\u51fa\u7ed3\u679c\u4e3a\u767e\u5206\u5360\u6bd4,\u964d\u4f4elogloss\u503c\ny_pre_proba = rf.predict_proba(x_test)","1da7e848":"y_pre_proba","a6394270":"rf.oob_score_","dd3e81d6":"# logloss\u6a21\u578b\u8bc4\u4f30\n\nlog_loss(y_test1, y_pre_proba, eps=1e-15, normalize=True)","444d305c":"# \u786e\u5b9an_estimators\u7684\u53d6\u503c\u8303\u56f4\ntuned_parameters = range(10, 200, 10)\n\n# \u521b\u5efa\u6dfb\u52a0accuracy\u7684\u4e00\u4e2anumpy\naccuracy_t = np.zeros(len(tuned_parameters))\nprint(accuracy_t)\n\n# \u521b\u5efa\u6dfb\u52a0error\u7684\u4e00\u4e2anumpy\nerror_t = np.zeros(len(tuned_parameters))\nprint(error_t)\n\n# \u8c03\u4f18\u8fc7\u7a0b\u5b9e\u73b0\nfor j, one_parameter in enumerate(tuned_parameters):\n    print(\"j:\",j)\n    print(\"one_parameter\", one_parameter)\n    rf2 = RandomForestClassifier(n_estimators=one_parameter, \n                                 max_depth=10, \n                                 max_features=10, \n                                 min_samples_leaf=10, \n                                 oob_score=True, \n                                 random_state=0, \n                                 n_jobs=-1)\n    \n    rf2.fit(x_train, y_train)\n    \n    # \u8f93\u51faaccuracy\n    accuracy_t[j] = rf2.oob_score_\n    \n    # \u8f93\u51falog_loss\n    y_pre = rf2.predict_proba(x_test)\n    error_t[j] = log_loss(y_test, y_pre, eps=1e-15, normalize=True)\n    \nprint(error_t)\n","24bf3009":"# \u4f18\u5316\u7ed3\u679c\u8fc7\u7a0b\u53ef\u89c6\u5316\nfig,axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 4), dpi=100)\n\naxes[0].plot(tuned_parameters, error_t)\naxes[1].plot(tuned_parameters, accuracy_t)\n\naxes[0].set_xlabel(\"n_estimators\")\naxes[0].set_ylabel(\"error_t\")\naxes[1].set_xlabel(\"n_estimators\")\naxes[1].set_ylabel(\"accuracy_t\")\n\naxes[0].grid(True)\naxes[1].grid(True)\n\n\nplt.show()","932f8501":"# \u786e\u5b9an_estimators\u7684\u53d6\u503c\u8303\u56f4\ntuned_parameters = range(5, 40, 5)\n\n# \u521b\u5efa\u6dfb\u52a0accuracy\u7684\u4e00\u4e2anumpy\naccuracy_t = np.zeros(len(tuned_parameters))\n\n# \u521b\u5efa\u6dfb\u52a0error\u7684\u4e00\u4e2anumpy\nerror_t = np.zeros(len(tuned_parameters))\n\n# \u8c03\u4f18\u8fc7\u7a0b\u5b9e\u73b0\nfor j, one_parameter in enumerate(tuned_parameters):\n    rf2 = RandomForestClassifier(n_estimators=175, \n                                 max_depth=10, \n                                 max_features=one_parameter, \n                                 min_samples_leaf=10, \n                                 oob_score=True, \n                                 random_state=0, \n                                 n_jobs=-1)\n    \n    rf2.fit(x_train, y_train)\n    \n    # \u8f93\u51faaccuracy\n    accuracy_t[j] = rf2.oob_score_\n    \n    # \u8f93\u51falog_loss\n    y_pre = rf2.predict_proba(x_test)\n    error_t[j] = log_loss(y_test, y_pre, eps=1e-15, normalize=True)\n    \n    print(error_t)\n","d336f473":"# \u4f18\u5316\u7ed3\u679c\u8fc7\u7a0b\u53ef\u89c6\u5316\nfig,axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 4), dpi=100)\n\naxes[0].plot(tuned_parameters, error_t)\naxes[1].plot(tuned_parameters, accuracy_t)\n\naxes[0].set_xlabel(\"max_features\")\naxes[0].set_ylabel(\"error_t\")\naxes[1].set_xlabel(\"max_features\")\naxes[1].set_ylabel(\"accuracy_t\")\n\naxes[0].grid(True)\naxes[1].grid(True)\n\n\nplt.show()","0adabd16":"# \u786e\u5b9an_estimators\u7684\u53d6\u503c\u8303\u56f4\ntuned_parameters = range(10, 100, 10)\n\n# \u521b\u5efa\u6dfb\u52a0accuracy\u7684\u4e00\u4e2anumpy\naccuracy_t = np.zeros(len(tuned_parameters))\n\n# \u521b\u5efa\u6dfb\u52a0error\u7684\u4e00\u4e2anumpy\nerror_t = np.zeros(len(tuned_parameters))\n\n# \u8c03\u4f18\u8fc7\u7a0b\u5b9e\u73b0\nfor j, one_parameter in enumerate(tuned_parameters):\n    rf2 = RandomForestClassifier(n_estimators=175, \n                                 max_depth=one_parameter, \n                                 max_features=15, \n                                 min_samples_leaf=10, \n                                 oob_score=True, \n                                 random_state=0, \n                                 n_jobs=-1)\n    \n    rf2.fit(x_train, y_train)\n    \n    # \u8f93\u51faaccuracy\n    accuracy_t[j] = rf2.oob_score_\n    \n    # \u8f93\u51falog_loss\n    y_pre = rf2.predict_proba(x_test)\n    error_t[j] = log_loss(y_test, y_pre, eps=1e-15, normalize=True)\n    \n    print(error_t)\n","06aa9cb9":"# \u4f18\u5316\u7ed3\u679c\u8fc7\u7a0b\u53ef\u89c6\u5316\nfig,axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 4), dpi=100)\n\naxes[0].plot(tuned_parameters, error_t)\naxes[1].plot(tuned_parameters, accuracy_t)\n\naxes[0].set_xlabel(\"max_depth\")\naxes[0].set_ylabel(\"error_t\")\naxes[1].set_xlabel(\"max_depth\")\naxes[1].set_ylabel(\"accuracy_t\")\n\naxes[0].grid(True)\naxes[1].grid(True)\n\n\nplt.show()","f4747ef0":"# \u786e\u5b9an_estimators\u7684\u53d6\u503c\u8303\u56f4\ntuned_parameters = range(1, 10, 2)\n\n# \u521b\u5efa\u6dfb\u52a0accuracy\u7684\u4e00\u4e2anumpy\naccuracy_t = np.zeros(len(tuned_parameters))\n\n# \u521b\u5efa\u6dfb\u52a0error\u7684\u4e00\u4e2anumpy\nerror_t = np.zeros(len(tuned_parameters))\n\n# \u8c03\u4f18\u8fc7\u7a0b\u5b9e\u73b0\nfor j, one_parameter in enumerate(tuned_parameters):\n    rf2 = RandomForestClassifier(n_estimators=175, \n                                 max_depth=30, \n                                 max_features=15, \n                                 min_samples_leaf=one_parameter, \n                                 oob_score=True, \n                                 random_state=0, \n                                 n_jobs=-1)\n    \n    rf2.fit(x_train, y_train)\n    \n    # \u8f93\u51faaccuracy\n    accuracy_t[j] = rf2.oob_score_\n    \n    # \u8f93\u51falog_loss\n    y_pre = rf2.predict_proba(x_test)\n    error_t[j] = log_loss(y_test, y_pre, eps=1e-15, normalize=True)\n    \n    print(error_t)\n","dbaaaf87":"# \u4f18\u5316\u7ed3\u679c\u8fc7\u7a0b\u53ef\u89c6\u5316\nfig,axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 4), dpi=100)\n\naxes[0].plot(tuned_parameters, error_t)\naxes[1].plot(tuned_parameters, accuracy_t)\n\naxes[0].set_xlabel(\"min_sample_leaf\")\naxes[0].set_ylabel(\"error_t\")\naxes[1].set_xlabel(\"min_sample_leaf\")\naxes[1].set_ylabel(\"accuracy_t\")\n\naxes[0].grid(True)\naxes[1].grid(True)\n\n\nplt.show()","788fedf3":"rf3 = RandomForestClassifier(n_estimators=175, max_depth=30, max_features=15, min_samples_leaf=1, \n                             oob_score=True, random_state=40, n_jobs=-1)\n\nrf3.fit(x_train, y_train)","9c07ff3f":"rf3.score(x_test, y_test)","7d10f164":"rf3.oob_score_","cc2ec071":"y_pre_proba1 = rf3.predict_proba(x_test)\n\nlog_loss(y_test, y_pre_proba1)","9876315d":"test_data = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv\")","9ffea755":"test_data.head()","81b7e528":"test_data_drop_id = test_data.drop([\"id\"], axis=1)\ntest_data_drop_id.head()","02378839":"y_pre_test = rf3.predict_proba(test_data_drop_id)","7c08e0ee":"y_pre_test","a0a3f657":"result_data = pd.DataFrame(y_pre_test, columns=[\"Class_\"+str(i) for i in range(1, 10)])","f2d5a2e3":"result_data.head()","e00fb1c4":"result_data.insert(loc=0, column=\"id\", value=test_data.id)","69417cdc":"result_data.head()","b54171f9":"# result_data.to_csv(\".\/data\/otto\/submission.csv\", index=False)","081a4d3c":"## \u6a21\u578b\u8c03\u4f18\n\nn_estimators, max_feature, max_depth, min_samples_leaf","8e0ad846":"\u7ecf\u8fc7\u56fe\u50cf\u5c55\u793a,\u6700\u540e\u786e\u5b9amin_sample_leaf=1\u7684\u65f6\u5019,\u8868\u73b0\u6548\u679c\u4e0d\u9519","86957355":"\u7ecf\u8fc7\u56fe\u50cf\u5c55\u793a,\u6700\u540e\u786e\u5b9an_estimators=175\u7684\u65f6\u5019,\u8868\u73b0\u6548\u679c\u4e0d\u9519","2d43d60e":"## \u628a\u6807\u7b7e\u503c\u8f6c\u6362\u4e3a\u6570\u5b57","21c0f579":"### \u786e\u5b9a\u6700\u4f18\u7684max_depth","2166322d":"\u7ecf\u8fc7\u56fe\u50cf\u5c55\u793a,\u6700\u540e\u786e\u5b9amax_depth=30\u7684\u65f6\u5019,\u8868\u73b0\u6548\u679c\u4e0d\u9519","c4a06594":"# \u6a21\u578b\u8bad\u7ec3\n\n## \u57fa\u672c\u6a21\u578b\u8bad\u7ec3","9e506779":"### \u786e\u5b9a\u6700\u4f18\u7684min_sample_leaf","d0837590":"# \u6570\u636e\u57fa\u672c\u5904\u7406\n\n\u6570\u636e\u5df2\u7ecf\u7ecf\u8fc7\u8131\u654f,\u4e0d\u518d\u9700\u8981\u7279\u6b8a\u5904\u7406","7ccdd710":"\u4e0a\u9762\u62a5\u9519\u539f\u56e0:logloss\u4f7f\u7528\u8fc7\u7a0b\u4e2d,\u5fc5\u987b\u8981\u6c42\u5c06\u8f93\u51fa\u7528one-hot\u8868\u793a,\n    \n\u9700\u8981\u5c06\u8fd9\u4e2a\u591a\u7c7b\u522b\u95ee\u9898\u7684\u8f93\u51fa\u7ed3\u679c\u901a\u8fc7OneHotEncoder\u4fee\u6539\u4e3a\u5982\u4e0b:","d8983154":"## \u786e\u5b9a\u6700\u4f18\u6a21\u578b\n\nn_estimators=175,\n\nmax_depth=30,\n\nmax_features=15,\n\nmin_samples_leaf=1, \n","053b4fd7":"## \u622a\u53d6\u90e8\u5206\u6570\u636e","ace03f89":"## \u5206\u5272\u6570\u636e","01a7d087":"\u4f7f\u7528\u4e0a\u9762\u65b9\u5f0f\u83b7\u53d6\u6570\u636e\u4e0d\u53ef\u884c,\u7136\u540e\u4f7f\u7528\u968f\u673a\u6b20\u91c7\u6837\u83b7\u53d6\u54cd\u5e94\u7684\u6570\u636e","7788ad6a":"\u7ecf\u8fc7\u56fe\u50cf\u5c55\u793a,\u6700\u540e\u786e\u5b9amax_features=15\u7684\u65f6\u5019,\u8868\u73b0\u6548\u679c\u4e0d\u9519","978a3d43":"# 1.\u6570\u636e\u83b7\u53d6","58bfdb35":"### \u786e\u5b9a\u6700\u4f18\u7684max_features","d4bbb3a5":"### \u786e\u5b9a\u6700\u4f18\u7684n_estimators","e9770292":"\u7531\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa,\u8be5\u6570\u636e\u7c7b\u522b\u4e0d\u5747\u8861,\u6240\u4ee5\u9700\u8981\u540e\u671f\u5904\u7406","1444bb43":"# \u751f\u6210\u63d0\u4ea4\u6570\u636e"}}