{"cell_type":{"637ac195":"code","0bdfa404":"code","19cd2048":"code","5342dad7":"code","5b9f0bc4":"code","3ba0561b":"code","a6b0a6f1":"code","4d949eae":"code","e5daae70":"code","24d5cb2b":"code","920f5b29":"code","d8ecddcb":"code","2ac0abc2":"code","869493ad":"code","ee6b969b":"code","09edb944":"code","fbd367d2":"code","ab02e2d6":"code","c4c773f2":"code","756030cc":"code","0bdc3c14":"code","29fbf8b6":"code","dfb77dc3":"code","cdf38915":"code","fc462916":"code","e2241a21":"code","6558342f":"code","62aaf02e":"code","3cd960d1":"code","51edd2f5":"code","dcdfa213":"code","1e0dcb36":"code","a42e9293":"code","a0ca573f":"code","922e2022":"code","6b074059":"code","004f8c18":"code","a8287973":"code","ecf1ea27":"code","a56ccc84":"code","d420644e":"markdown","818c37ac":"markdown","7c807e97":"markdown","a380c0a2":"markdown","016434e4":"markdown","b6791589":"markdown","16481ab5":"markdown","08d742c2":"markdown","98f56161":"markdown","2d28c286":"markdown","7a0d5620":"markdown","309a5038":"markdown","6540a5bd":"markdown","542ed83f":"markdown","fbbd38ee":"markdown","bbcb9e6f":"markdown","ca7ad626":"markdown","51115558":"markdown","81b9f785":"markdown","bcd371c5":"markdown","3789a1c5":"markdown","f66ff30e":"markdown","0524067d":"markdown","97b16ba5":"markdown","9f4ff7dc":"markdown","1670f528":"markdown","6374c35c":"markdown","3b00d988":"markdown","c041ac98":"markdown","1d528f1b":"markdown","2ee57001":"markdown","6d32a303":"markdown","f596ad55":"markdown","a44d8960":"markdown","637e40d2":"markdown","130fe72a":"markdown","4480b9b2":"markdown","dfbee89b":"markdown","f5467ad0":"markdown","f759386f":"markdown","ef24e2e3":"markdown","276d0b51":"markdown","e32811de":"markdown","6c390411":"markdown","179a69b7":"markdown","d5666498":"markdown","0a6d6c26":"markdown","1d020c74":"markdown","5b3b99db":"markdown","d2193efe":"markdown","f3fbc321":"markdown","5dc51f00":"markdown","234d60cc":"markdown","7f81833c":"markdown","d8cfaa55":"markdown","413d34c4":"markdown","8218b629":"markdown","2c32572f":"markdown","31992a15":"markdown","51290a06":"markdown","93a527a1":"markdown","a8042e39":"markdown","4da124d5":"markdown"},"source":{"637ac195":"import pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom matplotlib import pylab as plt\nfrom statsmodels.graphics.gofplots import qqplot\nfrom IPython.core.interactiveshell import InteractiveShell\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=Warning)\nInteractiveShell.ast_node_interactivity = 'all'\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\nset_seed()\n\nsns.set_style('whitegrid')\nsns.set_context('paper', font_scale=1.5)\nplt.style.use('fivethirtyeight')\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_columns\", 25)\npd.set_option(\"display.max_rows\", 20)\n\nprint(\"setup-complete!\")","0bdfa404":"raw = \"..\/input\/nyc-taxi-trip-duration\/NYC.csv\"\ntaxi_data = pd.read_csv(raw, low_memory=False)","19cd2048":"taxi_data # check all data","5342dad7":"taxi_data.info(verbose=True) # getting the information","5b9f0bc4":"categorical = taxi_data.select_dtypes(exclude=[np.number]).columns\nfor cat_col in categorical:\n    print(f\"{cat_col} : {taxi_data[cat_col].nunique()} uniqueness vaiables(s)\")","3ba0561b":"numeric = taxi_data.select_dtypes(include=[np.number]).columns\nfor num_col in numeric:\n    print(f\"{num_col} : {taxi_data[num_col].nunique()} uniqueness variable(s)\")","a6b0a6f1":"# Get the number of missing data points per column\nmissing_values_count = taxi_data.isnull().sum()\n# Look at the missing points in the first ten columns\nmissing_values_count[:10]","4d949eae":"# let's make a correlation matrix for `cop_data`\nsns.set_style(\"whitegrid\") # set the seaborn style\nplt.figure(figsize=(24, 18)) # figure the size\nsns.heatmap(taxi_data.corr(), annot=True) # create a heatmap\nplt.title(\"Taxi Data Correlation\", weight=\"bold\", fontsize=30, fontname=\"monospace\", pad=30) # title\nplt.xticks(weight=\"bold\", fontsize=15) # x-ticks\nplt.yticks(weight=\"bold\", fontsize=15); # y-ticks","e5daae70":"# Let's see the correlation clearly\n(taxi_data.corr()[\"trip_duration\"] # transform it into data corr\n      .sort_values(ascending=False) # sort the values\n      .to_frame() # change it into data frame\n      .T) # transpose","24d5cb2b":"taxi_data.describe(include=[np.number]).astype(\"int\") # checking statistical summary","920f5b29":"# checking and visualizing the type of distribution of a feature column\ndef univariate_analysis(data, color, title1, title2):\n    \n    \"\"\"\n    Showing visualization of univariate\n    analysis with displot and qqplot\n    visualization from seaborn and statsmodel\n    library.\n    \n    Parameters\n    ----------\n    data : DataFrame, array, or list of arrays, optional\n        Dataset for plotting. If ``x`` and ``y`` are absent, this is\n        interpreted as wide-form. Otherwise it is expected to be long-form. \n    title1: The title of the visualization, title1 for displot visualization\n        And title2 for quantile plot from statsmodel.\n    title2: The title of the visualization, title1 for displot visualization\n        And title2 for quantile plot from statsmodel.\n        \n    Returns\n    -------\n    fig : matplotlib figure\n        Returns the Figure object with the plot drawn onto it.\n    \"\"\"\n    \n    fig, (ax1, ax2) = plt.subplots( # subplots\n        ncols=2, # num of cols\n        nrows=1, # num of rows\n        figsize=(20, 6) # set the width and high\n    )\n\n    sns.distplot( # create a distplot visualization\n        data, # data\n        ax=ax1, # axes 1\n        kde=True, # kde\n        color=color # color\n    )\n    \n    ax1.set_title( # set the title 1\n        title1, \n        weight=\"bold\", # weight\n        fontname=\"monospace\", # font-name\n        fontsize=25, # font-size\n        pad=30 # padding\n    )\n    \n    qqplot( # qqplot (quantile plot)\n        data, # data\n        ax=ax2, # axes 2\n        line='s' # line \n    )\n    \n    ax2.set_title( # set the title 2\n        title2, \n        weight=\"bold\", # weight\n        fontname=\"monospace\", # font-name\n        fontsize=25, # font-size\n        pad=30 # padding\n    )\n    \n    return fig # returning the figure","d8ecddcb":"# Dependent Variables\nunivariate_analysis( # call the function\n    data=taxi_data[\"trip_duration\"], # put the data\n    color=\"red\", # pick the color\n    title1=\"Trip Duration Data Distribution\", # title1\n    title2=\"Quantile Plot\"); # title2","2ac0abc2":"# Passenger Count Data\nunivariate_analysis( # call the function\n    data=taxi_data[\"passenger_count\"], # put the data\n    color=\"black\", # pick the color\n    title1=\"Passenger Count Data Distribution\", # title1\n    title2=\"Quantile Plot\"); # title2","869493ad":"# checking skewness value\n# if value lies between -0.5 to 0.5  then it is normal otherwise skewed\nskew_value = taxi_data.skew().sort_values(ascending=False).to_frame().head()\nskew_value","ee6b969b":"taxi_data[[\"p_ymd\", \"p_hms\"]] = taxi_data[\"pickup_datetime\"].str.split(\" \", expand=True) # pickup datetime\ntaxi_data[[\"d_ymd\", \"d_hms\"]] = taxi_data[\"dropoff_datetime\"].str.split(\" \", expand=True) # dropoff datetime\n\ndel taxi_data[\"pickup_datetime\"] # del pickup\ndel taxi_data[\"dropoff_datetime\"] # del dropoff\n\ntaxi_data[[\"p_year\", \"p_month\", \"p_day\"]] = taxi_data[\"p_ymd\"].str.split(\"-\", expand=True).astype(\"int\") # splitting pickup\ntaxi_data[[\"p_hour\", \"p_minute\", \"p_second\"]] = taxi_data[\"p_hms\"].str.split(\":\", expand=True).astype(\"int\") # splitting pickup\n\ndel taxi_data[\"p_ymd\"] # del pickup year month day\ndel taxi_data[\"p_hms\"] # del pickup hour minute second\n\ntaxi_data[[\"d_year\", \"d_month\", \"d_day\"]] = taxi_data[\"d_ymd\"].str.split(\"-\", expand=True).astype(\"int\") # splitting dropoff\ntaxi_data[[\"d_hour\", \"d_minute\", \"d_second\"]] = taxi_data[\"d_hms\"].str.split(\":\", expand=True).astype(\"int\") # splitting dropoff\n\ndel taxi_data[\"d_ymd\"] # del dropoff year month day\ndel taxi_data[\"d_hms\"] # del dropoff hour minute second\ndel taxi_data[\"id\"] # del taxi id\n\ntaxi_data.head(2) # view","09edb944":"taxi_data.drop(taxi_data[taxi_data[\"trip_duration\"] == 1939736].index, inplace = True)\ntaxi_data = taxi_data[taxi_data[\"passenger_count\"]!=0]\ntaxi_data = taxi_data[taxi_data[\"passenger_count\"]<=6]","fbd367d2":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\ndef preprocessing_data(data):\n    \"\"\"Returns Data that has been preprocessed\"\"\"\n    # Prepare\n    categorical_col = [\"store_and_fwd_flag\"]\n    numerical_col = ['vendor_id', 'passenger_count', 'pickup_longitude', \n                     'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',  \n                     'p_year', 'p_month', 'p_day', 'p_hour', 'p_minute', 'p_second', \n                     'd_year', 'd_month', 'd_day', 'd_hour', 'd_minute', 'd_second']\n    \n    # Create a columns transformer\n    column_transformer = make_column_transformer(\n        # Create a columns transformer between 0 and 1\n        (MinMaxScaler(), numerical_col),\n        # Encoder all values in this columns between 0 and 1\n        (OneHotEncoder(handle_unknown=\"ignore\"), categorical_col)\n    )\n    \n    # Create X & y (features and label)\n    X = data.drop(columns=[\"trip_duration\"])\n    y = data[\"trip_duration\"]\n    \n    # Build our train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Fit the columns transformer to our training data\n    column_transformer.fit(X_train)\n    \n    # Transform training and test data with normalization (MinMaxScaler) and encoder (OneHotEncoder, LabelEncoder)\n    X_train_normal = column_transformer.transform(X_train)\n    X_test_normal = column_transformer.transform(X_test)\n    \n    # X_train and X_test (features)\n    X_train, X_test = tf.constant(X_train_normal, dtype=tf.float32), tf.constant(X_test_normal, dtype=tf.float32)\n    \n    # y_train and y_test (label)\n    y_train, y_test = tf.constant(y_train, dtype=tf.float32), tf.constant(y_test, dtype=tf.float32)\n    \n    return X_train, X_test, y_train, y_test\n\n# Call the function\nX_train, X_test, y_train, y_test = preprocessing_data(taxi_data)","ab02e2d6":"# Checking the shape and dimension of rows and columns (features)\nX_train.shape, X_test.shape, X_train.ndim, X_test.ndim","c4c773f2":"# Checking the shape and dimension of rows and columns (label)\ny_train.shape, y_test.shape, y_train.ndim, y_test.ndim","756030cc":"# Checking the type (features and label)\nX_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype","0bdc3c14":"# Checking the len (feature and label)\nlen(X_train), len(X_test), len(y_train), len(y_test)","29fbf8b6":"gpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)","dfb77dc3":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","cdf38915":"tf.test.is_gpu_available()","fc462916":"# Set random seed\nset_seed()\n\n# Create the model\nsmall_model = tf.keras.Sequential([\n    layers.Dense(100, activation=\"relu\", input_shape=[20], name=\"input_1\"),\n    layers.Dense(10, activation=\"relu\", name=\"input_2\"),\n    layers.Dense(1, name=\"output_layer\")\n], name=\"small_model\")\n\n# Compile the model\nsmall_model.compile(\n    loss=\"mae\",\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nsmall_model_history = small_model.fit(\n    X_train, y_train, \n    validation_data=(X_test, y_test),\n    epochs=10\n)","e2241a21":"small_model.summary() # let's see its layer","6558342f":"tf.keras.utils.plot_model(small_model, show_shapes=True) # let's see its architecture","62aaf02e":"# Plot the validation and training data separately\ndef plot_loss_curves(history):\n    \"\"\"\n    Returns separate loss curves for\n    training and validation metrics.\n    \"\"\"\n    # Loss and Val_loss\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    \n    # Mae and Val Mae\n    mae = history.history[\"mae\"]\n    val_mae = history.history[\"val_mae\"]\n    \n    # set the epochs\n    epochs = range(len(history.history[\"loss\"]))\n    \n    # Plot loss\n    plt.figure(figsize=(20, 7))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"Training_loss\", marker=\"o\", markerfacecolor=\"k\")\n    plt.plot(epochs, val_loss, label=\"Val_loss\", marker=\"o\", markerfacecolor=\"b\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, mae, label=\"Training_mae\", marker=\"o\", markerfacecolor=\"k\")\n    plt.plot(epochs, val_mae, label=\"Val_mae\", marker=\"o\", markerfacecolor=\"b\")\n    plt.title(\"MAE\")\n    plt.xlabel(\"Epochs\")\n    plt.legend();","3cd960d1":"small_model.evaluate(X_test, y_test) # evaluate it...","51edd2f5":"plot_loss_curves(small_model_history)","dcdfa213":"# Set random seed\nset_seed()\n\n# Create the model\nmedium_model = tf.keras.Sequential([\n    layers.Dense(200, activation=\"relu\", input_shape=[20], name=\"input_1\"),\n    layers.Dense(200, activation=\"relu\", name=\"input_2\"),\n    layers.Dense(10, activation=\"relu\", name=\"input_3\"),\n    layers.Dense(1, activation=\"relu\", name=\"output_layer\")\n], name=\"medium_model\")\n\n# Compile the model\nmedium_model.compile(\n    loss=\"mae\",\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nmedium_model_history = medium_model.fit(\n    X_train, y_train, \n    validation_data=(X_test, y_test),\n    epochs=15\n)","1e0dcb36":"medium_model.summary() # see its layer","a42e9293":"tf.keras.utils.plot_model(medium_model, show_shapes=True) # let's see its architecture","a0ca573f":"medium_model.evaluate(X_test, y_test) # evaluate it...","922e2022":"plot_loss_curves(medium_model_history)","6b074059":"# Set the seed\nset_seed()\n\n# Create the model\nlarger_model = tf.keras.Sequential([\n    layers.Dense(512, activation=\"relu\", input_shape=[20], name=\"input_1\"),\n    layers.Dense(512, activation=\"relu\", name=\"input_2\"),\n    layers.Dense(512, activation=\"relu\", name=\"input_3\"),\n    layers.Dense(512, activation=\"relu\", name=\"input_4\"),\n    layers.Dense(1, activation=\"relu\", name=\"output_layer\")\n], name=\"larger_model\")\n\n# Compile the model\nlarger_model.compile(\n    loss=\"mae\",\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nlarger_model_history = larger_model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=20\n) ","004f8c18":"larger_model.summary() # see its layer","a8287973":"tf.keras.utils.plot_model(larger_model, show_shapes=True) # let's see its architecture","ecf1ea27":"larger_model.evaluate(X_test, y_test) # evaluate it...","a56ccc84":"plot_loss_curves(larger_model_history)","d420644e":"Let's make a medium model","818c37ac":"#### **Listing Devices including GPU's with Tensorflow**","7c807e97":"### **How many missing data points do we have?**\n\nLet's see how many missing data we have in each column....","a380c0a2":"### **Let's do a simple analysis...**\n\nberfore that let's recap the data...\n\n### **Recap Data:**\n\n* Data has  1,458,644  (rows) and  11  (columns), in my opinion, this data is very good data to be used as a project because this data has a lot of rows, haha, but, if you apply this data to predicting taxi trip duration for the year ( 2021  -  2022 ) in my opinion, this will not work, because you know for yourself that the development of taxis for ( 2016  -  2021 ) is very large, not to mention that many large shuttle companies are replacing public taxis, such as Uber.\n* There are $4$ data of type object, $3$ data of type **int64**, and the rest of data type **float64**.\n* **Uniqueness Categorical Variables:**\n    * `id` : $1458644$ uniqueness vaiables(s)\n    * `pickup_datetime` : $1380222$ uniqueness vaiables(s)\n    * `dropoff_datetime` : $1380377$ uniqueness vaiables(s)\n    * `store_and_fwd_flag` : $2$ uniqueness vaiables(s)\n* **Discrete and Continuous Variables:**\n    * `vendor_id` : $2$ uniqueness variable(s)\n    * `passenger_count` : $10$ uniqueness variable(s)\n    * `pickup_longitude` : $23047$ uniqueness variable(s)\n    * `pickup_latitude` : $45245$ uniqueness variable(s)\n    * `dropoff_longitude` : $33821$ uniqueness variable(s)\n    * `dropoff_latitude` : $62519$ uniqueness variable(s)\n    * `trip_duration` : $7417$ uniqueness variable(s)\n* **Data is not have any missing values.**","016434e4":"# **Data Analysis**\n\nData Analysis is the process of systematically applying statistical and\/or logical techniques to describe and illustrate, condense and recap, and evaluate data. Indeed, researchers generally analyze for patterns in observations through the entire data collection phase *(Savenye, Robinson, $2004$)*.\nanalyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.\n\nOr, the easier, you can say in Data Analysis we (Data Scientist or Data Analyst) what ever you want to call that, in this section, we're looking for the correlation and also the relationships between every data (features and labels) or the variables using and applying the statistical and visualization methods for looking some patterns.","b6791589":"> Great! let's check the Discrete and Continuous Variables...","16481ab5":"---","08d742c2":"### **Uniqueness Categorical Variables**\nLet's have a look at categorical variables. How many unique values of these variables.","98f56161":"We can clearly see an outlier. there is an entry which is significantly different from others. As there is a single row only, let us drop this row. \n\n* There are some trips with even $0$ passenger count.\n* There is only $1$ trip each for $7$ and $9$ passengers.\n\nLet us remove the rows which have $0$ or $7$ or $9$ passenger count.","2d28c286":"### **Discrete and Continuous Variables**\nLet's have a look at Discrete and Continuous variables.","7a0d5620":"---","309a5038":"> Okay! let's check if there any missing value...","6540a5bd":"**regards,**<br>\n**Azmi**","542ed83f":"Let's evaluate it!","fbbd38ee":"# **Objective:**\n\n* Understand the Dataset & cleanup (if required).\n* Build Regression models to predict the duration of taxi trip.\n* Also evaluate the models & compare thier respective scores like R2, RMSE, etc.","bbcb9e6f":"# **Description:**\n\nThe competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set.\n\nThe datset contains the following fields:\n\n#### **Independent Variables**\n\n* `id` - a unique identifier for each trip.\n* `vendorid` - a code indicating the provider associated with the trip record. \n* `pickupdatetime` - date and time when the meter was engaged.\n* `dropoffdatetime` - date and time when the meter was disengaged.\n* `passengercount` - the number of passengers in the vehicle (driver entered value).\n* `pickuplongitude` - the longitude where the meter was engaged .\n* `pickuplatitude` - the latitude where the meter was engaged.\n* `dropofflongitude` - the longitude where the meter was disengaged .\n* `dropofflatitude` - the latitude where the meter was disengaged.\n* `store_and_fwd_flag`\u200a\u2014\u200aThis flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server\u200a\u2014\u200aY=store and forward; N=not a store and forward trip.\n\n#### **Dependent Variables**\n\n* `trip_duration`\u200a\u2014\u200aduration of the trip in seconds.\n ","ca7ad626":"---","51115558":"---\n\n<p style=\"font-name: monospace; line-height: 2; font-size: 30px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Taxi Trip Duration Prediction (Regression) - Knightbearr<\/p>\n\n---","81b9f785":"#### **To Check GPU in Tensorflow**","bcd371c5":"Let's evaluate it!","3789a1c5":"---","f66ff30e":"Let's try to find the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$) using ***Chebychev's Theorem***, and let's try to interprete...","0524067d":"# **On Going...**","97b16ba5":"Let's make a larger model!","9f4ff7dc":"## **Trip Duration**\n\nFind the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$), and let's try to interprete that. and In the Sales Data, the $\\mu = 959$ and the $\\sigma = 5237$, then without further ado let's calculate it.\n\n#### **Calculation:**\n\n* $959 - 2(5237) = -9515$\n* $959 + 2(5237) = 11434$\n\n#### ***Interpretation:***\n\nat least $75$% time trip duration in NYC takes about $3.5$ hours of travel.","1670f528":"> Wow, this data has $1,458,644$ (rows) and $11$ (columns), in my opinion, this data is very good data to be used as a project because this data has a lot of rows, haha, but, if you apply this data to predicting taxi trip duration for the year ($2021$ - $2022$) in my opinion, this will not work, because you know for yourself that the development of taxis for ($2016$ - $2021$) is very large, not to mention that many large shuttle companies are replacing public taxis, such as Uber.","6374c35c":"Let's evaluate it!","3b00d988":"> ok, there are $4$ data of type object, $3$ data of type **int64**, and the rest of data type **float64**, I don't know if there is any null value in this data, let's check, but... before that let's check the Categorical and Numceric uniqueness...","c041ac98":"The histogram is really skewed as we can see. That's why we're using **Chebychev's Theorem**","1d528f1b":"## **Univariate Analysis**\n\nUnivariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.","2ee57001":"Look at that! we're just making a simple model, and it works well! Let's see the layer and its architecture...","6d32a303":"---","f596ad55":"![FB_IMG_15637614668311836.jpg](attachment:1f084ebe-1a57-4ff3-9f65-2a4523d3e3f0.jpg)","a44d8960":"<p style=\"font-name: monospace; line-height: 2; font-size: 25px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Chebychev's Theorem<\/p>\n$$\n\\begin{aligned}\n1 - \\frac{1}{k^2}: k &= 2 -> 1 - \\frac{1}{2^2} = \\frac{3}{4} -> 75 \\\\\n                   k &= 3 -> 1 - \\frac{1}{3^2} = \\frac{8}{9} -> 88.9\n\\end{aligned}\n$$\n<br>\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">How to find Standard Deviation ($\\sigma$)?<\/p>\n<p style=\"font-name: monospace; line-height: 2; font-size: 13px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Here's the Formula:<\/p>\n$$\n\\begin{aligned}\n\\sigma &= \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum{(x - \\mu)^2}}{N}} \\\\\ns &= \\sqrt{s^2} = \\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n - 1}}\n\\end{aligned}\n$$\n<br>\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">How to find Mean ($\\mu$)?<\/p>\n<p style=\"font-name: monospace; line-height: 2; font-size: 13px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Here's the Formula:<\/p>\n$$\n\\begin{aligned}\n\\mu = \\frac{\\sum{x}}{N} \\\\\n\\bar{x} = \\frac{\\sum{x}}{n}\n\\end{aligned}\n$$","637e40d2":"---","130fe72a":"# **Acknowledgement:**\n\nThe dataset is taken from Kaggle:\\\nhttps:\/\/www.kaggle.com\/c\/nyc-taxi-trip-duration\/data","4480b9b2":"---","dfbee89b":"# **Regression with Neural Networks in TensorFlow**\n\nThere are many definitions for a regression problem but in our case we're going to simplify it: predicting a numerical variable based on some other combination of variables, even shorter...predicting a number. \n\n**Regression Analysis** is a set of statistical processes for estimating the relationships between a **dependent variable (often called 'outcome variable') or y \/ Label** and one or more **independent variable (often called 'predictors', 'covariates', or 'features') or X \/ our input Features cols.**\n\n### **Architecture of a Regreesion Model:**\n\n* **Input layer shape:** Same shape as a number of features \n* **Hidden layer(s):** Problem specific, minimum = $1$, maximum = $unlimited$\n* **Neurons per hidden layer:** Problem specific, generally $10$ to $100$\n* **Output layer shape:** Same shape as desired prediction shape \n* **Hidden activation:** Usually `RelU` (rectified linear unit)\n* **Output activation:** None, `RelU`, `logistic\/tanh`\n* **Loss function:** `MSE` (mean square error) or `MAE` (mean absolute error) \/ `Huber` (combination of `MAE`\/`MSE`) if outliers\n* **Optimizer:** `SGD` (stochastic gradient descent), `Adam` etc....","f5467ad0":"---","f759386f":"#### **My Other Notebooks:**\n\n* [Sales Data Deep Analysis](https:\/\/www.kaggle.com\/knightbearr\/sales-data-deep-analysis-knightbearr)\n* [Shopping Cart Database Deep Analysis](https:\/\/www.kaggle.com\/knightbearr\/shopping-cart-database-deep-analysis-knightbearr)","ef24e2e3":"great! look like our model is learning something! let's see its layer and architecture...","276d0b51":"## **Passenger Count**\n\nFind the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$), and let's try to interprete that. and In the Sales Data, the $\\mu = 2$ and the $\\sigma = 1$ if we round it, then without further ado let's calculate it.\n\n#### **Calculation:**\n\n* $2 - 2(1) = 0$\n* $2 + 2(1) = 4$\n\n#### ***Interpretation:***\n\nAt least approximately $75$% passenger count in NYC is around $0 - 4$ passengers count.","e32811de":"> Stranger in the nigghhtt..... God, I love that song, owh, look at that, looks like the data is not have any missing values! okay, let's go to the next steps! ","6c390411":"---","179a69b7":"# **Check Data**\n\nLet's see the data and how it looks.","d5666498":"> Okay, if we look at the visualization and printing results above, we can see, `trip duration` has a pretty good correlation with `pickup_longtitude`, `vendor`, `dropoff_longtitude`, and also `passanger_count`. Okay... let's check the statistical summary... Note that this function can provide statistics for numerical features only.","0a6d6c26":"Okay, now let's move on to the Preprocessing Steps...","1d020c74":"![taxi.jpg](attachment:d47acbd0-fce1-4f2d-a98f-b3b88b6b5fb3.jpg)\n\n[image source](https:\/\/momobil.id\/news\/tips-meminang-mobil-ex-taxi-agar-tidak-kecewa\/)","5b3b99db":"#### **To Check GPU Availability in Tensorflow**","d2193efe":"# **Read-in Data**","f3fbc321":"Let's make a simple model... with multi perceptron...\n\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">A Hidden Layer<\/p>\n<br>\n$$\n\\begin{aligned}\nz_i = W_{o,i} + \\sum_{j=1}^{m} x_j W_{i,j}  \n\\end{aligned}\n$$\n<br>\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Activation Functions<\/p>\n<br>\n$$\n\\begin{aligned}\ng(z) =  max(0, z)\n\\end{aligned}\n$$\n<br>\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Output Layer<\/p>\n<br>\n$$\n\\begin{aligned}\n\\hat{y} = g(W_{o,i} + \\sum_{j=1}^{d_1} g(z) W_{j,i})\n\\end{aligned}\n$$\n<br>\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">Mean Absolute Error Loss<\/p>\n<br>\n$$\n\\begin{aligned}\nMAE = \\frac{1}{n} \\sum_{j=1}^{n} |y_j - \\hat{y}_j| \n\\end{aligned}\n$$","5dc51f00":"### Haha, I'm joking, hope my notebook is useful for you **Kagglers**! thank you for watching! ","234d60cc":"---","7f81833c":"# **Import Libraries**\n\nImport the dependencies libraies that we need for this research...","d8cfaa55":"### You: Forking Without Upvoting and Feedback\n### Me:","413d34c4":"> I will make a cool model tomorrow, wait for it...","8218b629":"#### **Some insights from the above summary:**\n\n* Vendor id has a minimum value of 1 and a maximum value of 2 which makes sense as we saw there are two vendor ids 1 and 2.\n* Passenger count has a minimum of 0 which means either it is an error entered or the drivers deliberately entered 0 to complete a target number of rides.\n* The minimum trip duration is also quite low. We will come back to this later during Univariate Analysis.","2c32572f":"okay, let's move on to the next steps...","31992a15":"<a id=\"9\"><\/a>\n# **Data Preprocessing**\n\nIn terms of scaling values, neural networks tend to prefer normalization.\n\nIf you're not sure on which to use, you could try both and see which performs better.","51290a06":"Let's see its layer and architecture!","93a527a1":"---","a8042e39":"---","4da124d5":"Beautiful! Our data has been normalized and one hot encoded. Now let's build a neural network model on it and see how it goes.\n\n# **Modeling with TensorFlow**\n\n1. **Creating a model** - define the input and output layers, as well as hidden layers of a deep learning model.\n2. **Compile our model** - define the loss function (in others words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evalution metrics (what we can use to interpret the performance of our model).\n3. **Fitting a model** - letting the model try to find patterns betwwen $X$ & $y$ (features and labels).\n\nThe first, don't build a large model, take it easy... step by step....\n\nLet's build a simple model...    "}}