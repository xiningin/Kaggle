{"cell_type":{"be95393f":"code","fb002cf2":"code","53f2f487":"code","1a73fd95":"code","e752eadb":"code","f5cacd7f":"code","8bbc2db3":"code","98f800d8":"code","1c21a101":"code","f2e94783":"code","4b60fc8d":"code","93cf9a91":"code","01b8bd30":"code","4d63d75e":"code","304dc908":"code","49ee70ba":"code","4e8a4a7d":"code","081b8b33":"code","7d493f61":"code","2f1eb195":"code","76edcb95":"code","a48a8e26":"code","9faf54ac":"code","da9b68b8":"code","eb214f43":"code","941a006d":"code","f4548b07":"code","279efa01":"code","fed00728":"code","c993b5d5":"code","9515ba8f":"code","e09cd416":"code","d0a10a4c":"code","987427b3":"code","d7590e7a":"code","017e7f7c":"code","2f643261":"code","3b364739":"code","ae401bfa":"code","2d1ce55b":"code","70e19e27":"code","66d3be35":"code","8b5fd99f":"code","0c96ed81":"code","bc8c4d19":"code","ae0b0268":"code","b22cf6b6":"code","895c37aa":"code","292744f6":"code","8755ff1c":"code","b3534b51":"code","7eae2f2f":"code","ddf4f9bd":"code","2d3f2a95":"code","aed74c11":"code","36f06c91":"code","4e2dd2b0":"code","8e8c3c3c":"code","4ecb4014":"code","400882c2":"code","b22716df":"code","99fe4729":"code","c2c4d4b9":"code","99e9dfac":"code","62a4cc05":"code","e7db4d8a":"code","3d0740ef":"code","5716c075":"code","c78a1675":"code","5b70b885":"code","d41f1ec6":"code","19da73c8":"code","0fc05e92":"code","2eb70013":"code","41c2f6c5":"code","56b5e461":"code","ca07588b":"code","7be33888":"code","6a609671":"code","3e95fe9c":"code","514cd412":"code","b6f4678f":"code","4360be1c":"code","e7a69c71":"code","97474831":"code","bcfd13da":"code","906fa76a":"code","fae98a37":"code","3f28be9a":"code","f78932f0":"code","29c1d225":"code","bc91567c":"code","b30c4eca":"code","357bd0a6":"code","a36ba66e":"code","d49d1d1f":"code","d279b5fc":"code","73808160":"code","ee7b7665":"code","fc2380b2":"code","6a0f4f8e":"code","6c24912c":"code","4ff0cfac":"code","ff0bd0a0":"code","6f701e87":"code","fc190618":"code","f043a9d4":"code","9acae083":"code","a3db2a66":"code","12387ccd":"code","d89e3e09":"code","a449b155":"code","c4e625a9":"code","d7832404":"code","5f687c37":"code","666e32d9":"code","d808582d":"markdown","dd13bb62":"markdown","df6f7536":"markdown","4715be9e":"markdown","5a5be43f":"markdown","d5971027":"markdown","455614f8":"markdown","02864bb9":"markdown","571b5622":"markdown","17291cff":"markdown","229fdd25":"markdown","0e939e59":"markdown","c68b5e70":"markdown","9191e03e":"markdown","bb1c2d7a":"markdown","1dedeabf":"markdown","53f1f3e8":"markdown","83d36eca":"markdown","ec7fcf39":"markdown","cb66e852":"markdown","251663ea":"markdown","f2a0cc53":"markdown","e53f7a2d":"markdown","592d77e6":"markdown","daf88e9c":"markdown","7e9d715e":"markdown","1823cf2f":"markdown","2efdc7ea":"markdown","7fe394ae":"markdown","d4d6f760":"markdown","a5f1acc6":"markdown","b1073322":"markdown","5afb0cfa":"markdown","a1909ea6":"markdown","6a9f23ab":"markdown","cab0b637":"markdown","72c45a8b":"markdown","ed1ea897":"markdown","13e9bee3":"markdown","bd0f7430":"markdown","3b89ebff":"markdown","a9d3d302":"markdown","1870de32":"markdown","f6fd3885":"markdown","5fd9616c":"markdown","aaaae128":"markdown","15796040":"markdown","ad6bceec":"markdown","5e5e46f7":"markdown","33d6a9c5":"markdown","5e35d0ff":"markdown","701677b7":"markdown","707fe803":"markdown","0c2a3359":"markdown","fb1d17b9":"markdown","b9618f41":"markdown","8f2aa3a3":"markdown","6f314c94":"markdown","b7231e5d":"markdown","4615faa3":"markdown","de91a3d2":"markdown","abae0fed":"markdown","87a6e3e3":"markdown","10b5cb97":"markdown","f404f25e":"markdown","34e81086":"markdown","c0d2bdb2":"markdown","7c71742c":"markdown","488a4c11":"markdown","cca07894":"markdown","18c2cc86":"markdown","34a33ac3":"markdown","57df3a61":"markdown","8cacb95f":"markdown","033cc699":"markdown","1e1dbd00":"markdown","61fa2bb5":"markdown","e283cfd5":"markdown","116a62de":"markdown","dcfd2eb6":"markdown","68adf7d5":"markdown","cb1c8e75":"markdown","394054f3":"markdown","c7ff770f":"markdown","d1667a0f":"markdown","e61cc24e":"markdown","fcc3305d":"markdown","34be9162":"markdown","53cd0e36":"markdown","afdcbf7a":"markdown","aa4672b5":"markdown","38c9f750":"markdown","f384585f":"markdown","54cbeca9":"markdown","f20beb5b":"markdown","13832a59":"markdown","feaacea4":"markdown","06b68f4b":"markdown","41565b6d":"markdown","d800b597":"markdown","5a3d0a62":"markdown","d2593d74":"markdown","398835d8":"markdown","19b80a4b":"markdown","b818dd8d":"markdown","06ff499f":"markdown","aa426669":"markdown","c5febf73":"markdown","15635a76":"markdown","be0f9886":"markdown","a4d99767":"markdown"},"source":{"be95393f":"from IPython.core.display import display, HTML\ndisplay(\"https:\/\/www.kaggle.com\/rishih\/present-sir\")\n#Link: [https:\/\/www.kaggle.com\/rishih\/present-sir](https:\/\/www.kaggle.com\/rishih\/present-sir)","fb002cf2":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np","53f2f487":"import pandas as pd\nged=pd.read_csv('..\/input\/ny-ged-plus-locations\/ged-plus-locations.csv')\nschools=pd.read_csv('..\/input\/data-science-for-good\/2016 School Explorer.csv')\nsecure=pd.read_csv('..\/input\/ny-2010-2016-school-safety-report\/2010-2016-school-safety-report.csv')","1a73fd95":"ged.apply(lambda x:sum(x.isnull()))","e752eadb":"ged.head(3)","f5cacd7f":"schools.groupby(['Student Achievement Rating']).agg(np.mean)","8bbc2db3":"pd.set_option('display.max_columns', None)  \na=schools['Address (Full)'].replace({'NEW YORK':'NewYork','CAMBRIA HEIGHTS':'CambriaHeights','SPRINGFIELD GARDENS':'SpringfieldGardens','REGO PARK':'RegoPark','FOREST HILLS':'ForestHills','ROCKAWAY PARK':'ROCKAWAY','HOWARD BEACH':'HowardBeach','QUEENS VILLAGE':'QueensVillage','COLLEGE POINT':'CollegePoint','RICHMOND HILL':'RichmondHill','FLORAL PARK':'FloralPark','OZONE PARK':'OzonePark','LITTLE NECK':'LittleNeck','LONG ISLAND CITY':'LongIslandCity','MIDDLE VILLAGE':'MiddleVillage','ROOSEVELT ISLAND':'RooseveltIsland','STATEN ISLAND':'StatenIsland','JACKSON HEIGHTS':'JacksonHeights','GREENWICH VILLAGE':'GreenwichVillage','GREAT NECK':'GreatNeck','BROAD CHANNEL':'BroadChannel','BRIGHTON BEACH':'BrightonBeach','MANHATTAN BEACH':'ManhattanBeach','ROCKAWAY BEACH':'RockawayBeach','GRAMERCY PARK':'GramercyPark','PABLEO POINT':'PabloPoint','CARROLL GARDENS':'CarrollGardens','KEW GARDENS':'KewGardens'},regex=True)\ndivision=[None]*len(a)\nfor i in range(len(a)):\n    division[i]=str(a[i]).split(\",\",2)[0].split()[-1]\nschools['neighbourhood']=division\ndf=schools[(schools['Economic Need Index']>0.6) | (schools['Rigorous Instruction Rating']=='Approaching Target') | (schools['Rigorous Instruction Rating']=='Not Meeting Target') | (schools['Collaborative Teachers Rating']=='Not Meeting Target') | (schools['Collaborative Teachers Rating']=='Approaching Target') | (schools['Supportive Environment Rating']=='Not Meeting Target') | (schools['Supportive Environment Rating']=='Approaching Target') | (schools['Effective School Leadership Rating']=='Not Meeting Target')| (schools['Effective School Leadership Rating']=='Approaching Target') | (schools['Strong Family-Community Ties Rating']=='Not Meeting Target') | (schools['Strong Family-Community Ties Rating']=='Approaching Target') | (schools['Trust Rating']=='Not Meeting Target') | (schools['Trust Rating']=='Approaching Target') | (schools['Student Achievement Rating']=='Not Meeting Target') | (schools['Student Achievement Rating']=='Approaching Target')]\ndf.head(3)","98f800d8":"df=df[(df['Grade Low']<='09') | (df['Grade High']>='08')]","1c21a101":"# Since we donot need the performance for other grades, we dump those columns\ndf=df.drop(df.columns[41:141], axis=1) \ndf=df.drop(df.columns[[0,1,2,4]],axis=1)","f2e94783":"df.shape","4b60fc8d":"df.apply(lambda x:sum(x.isnull()))","93cf9a91":"df['School Income Estimate']=df['School Income Estimate'].replace({'\\$':'', ',':''},regex=True).astype(float)","01b8bd30":"import seaborn as sns\nimport matplotlib.pyplot as plt","4d63d75e":"plt.figure(figsize=(20,10))\nplt.subplot(211)\nsns.set(rc={'figure.facecolor':'lightgray'})\nsns.boxplot(y=df['School Income Estimate'],x=df[\"Community School?\"])\nplt.title('School Income Estimate vs Community School?')\nplt.subplot(212)\nsns.set(rc={'figure.facecolor':'lightgray'})\nsns.boxplot(y=df['School Income Estimate'],x=df[\"District\"])\nplt.title('School Income Estimate vs District')\nplt.show()","304dc908":"a=df[df['Community School?']=='Yes']['School Income Estimate'].mean()\nnull_index= df[df['Community School?']=='Yes']['School Income Estimate'].isnull().index.tolist()\nfor i in null_index:\n    df.at[i,'School Income Estimate']= a","49ee70ba":"import numpy as np\ngrouped=df.groupby('District')\na=grouped['School Income Estimate'].agg(np.mean)\nnull_index=(df['District'].loc[df['School Income Estimate'].isnull().tolist()]== 4|5|6|7|9|12|16|17|18|19|23|26|32).index.tolist()\n\nfor i in null_index:\n    df.at[i,'School Income Estimate']= a[df['District'][i]]     ","4e8a4a7d":"schools[['Supportive Environment Rating','Rigorous Instruction Rating','Collaborative Teachers Rating','Student Achievement Rating','Trust Rating','Effective School Leadership Rating','Strong Family-Community Ties Rating','District']].groupby(['District'],as_index=False)[['Supportive Environment Rating','Rigorous Instruction Rating','Collaborative Teachers Rating','Student Achievement Rating','Trust Rating','Effective School Leadership Rating','Strong Family-Community Ties Rating']].agg(lambda x: x.value_counts().index[0])\n","081b8b33":"# remove the rows with remaining NaN values\ndf=df.dropna()","7d493f61":"df.shape","2f1eb195":"len(df[df['Community School?']=='No'])","76edcb95":"from collections import Counter\nCounter(df['neighbourhood']).most_common(10)","a48a8e26":"df['Percent of Students Chronically Absent']=df['Percent of Students Chronically Absent'].replace({'\\%':''},regex=True).astype(float)\ndf['Rigorous Instruction %']=df['Rigorous Instruction %'].replace({'\\%':''},regex=True).astype(float)\ndf['Collaborative Teachers %']=df['Collaborative Teachers %'].replace({'\\%':''},regex=True).astype(float)\ndf['Supportive Environment %']=df['Supportive Environment %'].replace({'\\%':''},regex=True).astype(float)\ndf['Effective School Leadership %']=df['Effective School Leadership %'].replace({'\\%':''},regex=True).astype(float)\ndf['Strong Family-Community Ties %']=df['Strong Family-Community Ties %'].replace({'\\%':''},regex=True).astype(float)\ndf['Trust %']=df['Trust %'].replace({'\\%':''},regex=True).astype(float)\ndf['Student Attendance Rate']=df['Student Attendance Rate'].replace({'\\%':''},regex=True).astype(float)","9faf54ac":"import numpy as np\ndf['Percent Black']=df['Percent Black'].replace({'%':''},regex=True).astype(float)\ndf['Percent Asian']=df['Percent Asian'].replace({'%':''},regex=True).astype(float)\ndf['Percent Hispanic']=df['Percent Hispanic'].replace({'%':''},regex=True).astype(float)\ndf['Percent White']=df['Percent White'].replace({'%':''},regex=True).astype(float)\ndf.groupby(['neighbourhood'],as_index=False)[['Percent Black','Percent Hispanic','Percent White','Percent Asian']].agg(np.mean)","da9b68b8":"secure.apply(lambda x:sum(x.isnull()))","eb214f43":"from math import cos, asin, sqrt\n\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    a = 0.5 - cos((lat2-lat1)*p)\/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) \/ 2\n    return 12742 * asin(sqrt(a))\n\ndef closest(data, v):\n    minimum=100000\n    ind=0\n    for index,row in data.iterrows():\n            dist=distance(v['Latitude'],v['Longitude'],row['Latitude'],row['Longitude'])\n            #print(minimum)\n            if minimum > dist:\n                          minimum=dist\n                          ind=index                        \n    return(data['Program Site name'][ind], data['Postcode'][ind],minimum)  \n\n#closest(ged, df[['Latitude','Longitude']].loc[0])\n","941a006d":"a=[]\nb=[]\ne=[]\nfor index,row in df.iterrows():\n    c,f,d=closest(ged, df[['Latitude','Longitude']].loc[index])\n    a.append(c)\n    e.append(f)\n    b.append(d)\ndf['Closest GED Center']=a\ndf['Distance']=b\ndf['Postcode']=e","f4548b07":"secure=secure.dropna(subset=['Geographical District Code'])","279efa01":"plt.figure(figsize=(20,6))\nsns.set(rc={'figure.facecolor':'lightgray'})\nsns.boxplot(y=secure['NoCrim N'],x=secure['Geographical District Code'])\nplt.title('District vs NoCrim N', size=15)\nplt.show()","fed00728":"grouped=secure.groupby('Geographical District Code')\nz=grouped[['Major N','Vio N','Prop N','NoCrim N','Oth N']].agg(np.mean)","c993b5d5":"null_index=secure[secure['Major N'].isnull()].index.tolist()\nfor i in null_index:\n    secure.at[i,'Major N'],secure.at[i,'Vio N'],secure.at[i,'Prop N'],secure.at[i,'NoCrim N'],secure.at[i,'Oth N']=z['Major N'][secure['Geographical District Code'][i]],z['Vio N'][secure['Geographical District Code'][i]],z['Prop N'][secure['Geographical District Code'][i]],z['NoCrim N'][secure['Geographical District Code'][i]],z['Oth N'][secure['Geographical District Code'][i]]    \n    ","9515ba8f":"secure['crime index']=secure['Major N']*0.55 + secure['Vio N']*0.25 + secure['Prop N']*0.1 + secure['NoCrim N']*0.05 + secure['Oth N']*0.05","e09cd416":"# obtaining the crime index of each location\ngrouped=secure.groupby(['Postcode'],as_index=False)['crime index'].agg(np.mean)\n\nfor code in grouped['Postcode'].tolist():\n    df.at[df[df['Postcode']==code].index.tolist(),'crime index']=grouped[grouped['Postcode']==code]['crime index'].tolist()","d0a10a4c":"df.at[df[df['Community School?']=='Yes'].index,'Community School?']=1\ndf.at[df[df['Community School?']=='No'].index,'Community School?']=0","987427b3":"df3=df[['Economic Need Index', 'School Income Estimate','Community School?','Percent Asian', 'Percent Black', 'Percent Hispanic','Percent White', 'Student Attendance Rate',\n       'Percent of Students Chronically Absent', 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %', 'Effective School Leadership %',\n            'Strong Family-Community Ties %','Trust %','Average ELA Proficiency',\n       'Average Math Proficiency', 'Distance', 'crime index']]","d7590e7a":"df3.apply(lambda x:sum(x.isnull()))","017e7f7c":"df3.shape","2f643261":"from matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage","3b364739":"%matplotlib inline\nnp.set_printoptions(precision=5, suppress=True) ","ae401bfa":"# generate the linkage matrix\nZ = linkage(df3,'complete')\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\n\nc, coph_dists = cophenet(Z, pdist(df3))\nc","2d1ce55b":"plt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()\n","70e19e27":"Z[-10:,2]","66d3be35":"plt.figure(figsize=(25, 10))\ndef fancy_dendrogram(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n    \n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n        plt.xlabel('sample index or (cluster size)')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata\n\nfancy_dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=100,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=10,  # useful in small plots so annotations don't overlap\n)\nplt.show()\n","8b5fd99f":"# set cut-off to 21080 \nmax_d = 23000   # max_d as in max_distance","0c96ed81":"plt.figure(figsize=(25, 10))\nfancy_dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=100,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=100,\n    max_d=max_d,  # plot a horizontal cut-off line\n)\nplt.show()","bc8c4d19":"last = Z[-10:, 2]\nlast_rev = last[::-1]\nidxs = np.arange(1, len(last) + 1)\nplt.figure(figsize=(20,8))\nplt.plot(idxs, last_rev)\n\nacceleration = np.diff(last, 2)  # 2nd derivative of the distances\nacceleration_rev = acceleration[::-1]\nplt.plot(idxs[:-2] + 1, acceleration_rev)\nplt.show()\nk = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\nprint(\"clusters:\", k)","ae0b0268":"from scipy.cluster.hierarchy import fcluster\nmax_d = 23000\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters","b22cf6b6":"Counter(clusters)","895c37aa":"k=2\ncluster=fcluster(Z, k, criterion='maxclust')","292744f6":"fcluster(Z, k, criterion='maxclust')","8755ff1c":"z=ged[['Program Site name','Latitude','Longitude','Borough']].dropna()\n\nimport branca.colormap as cm\nimport folium\nfrom folium import plugins\n\nstep = cm.StepColormap(\n    ['aqua','yellow','red'],\n    vmin=0.5, vmax=3.5,\n    index=[0.5,1.5,2.5],\n    caption='step'\n)\n    \nstep","b3534b51":"df['Cluster']=fcluster(Z, max_d, criterion='distance')\ndf = df.reset_index(drop=True)\ndf3 = df3.reset_index(drop=True)","7eae2f2f":"m = folium.Map([df['Latitude'][0], df['Longitude'][0]], zoom_start=9.5,tiles='cartodbdark_matter')\n\ni=0\nfor lat, lon in zip(df['Latitude'], df['Longitude']):\n    folium.CircleMarker([lat, lon], color=step(df['Cluster'][i]), fill=True, radius=0.9).add_to(m)    \n    i+=1\nm","ddf4f9bd":"grouped=df.groupby(['Cluster']).agg(np.mean)\ngrouped.drop(grouped.columns[1:6], axis=1)\n#grouped","2d3f2a95":"grouped=df[['School Name','Cluster']].groupby(['School Name'],as_index=False).agg(lambda x:x.value_counts().index[0]).groupby(['Cluster'])\naug=grouped['School Name'].apply(lambda x: sorted(set(x)))[1]\nnov=grouped['School Name'].apply(lambda x: sorted(set(x)))[2]\napr=grouped['School Name'].apply(lambda x: sorted(set(x)))[3]\n\nz={'School Name':[aug,nov,apr], 'Cluster': [1,2,3]}\nz=pd.DataFrame(z)\nz.style.set_properties(**{'background-color': 'black',\n                           'color': 'gold','border-color': 'white'})","aed74c11":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(df3)","36f06c91":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom collections import defaultdict\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.preprocessing import StandardScaler\n\nclass PFA(object):\n    def __init__(self, n_features, q=None):\n        self.q = q\n        self.n_features = n_features\n\n    def fit(self, X):\n        if not self.q:\n            self.q = X.shape[1]\n\n        sc = StandardScaler()\n        X = sc.fit_transform(X)\n\n        pca = PCA(n_components=self.q).fit(X)\n        A_q = pca.components_.T\n\n        kmeans = KMeans(n_clusters=self.n_features).fit(A_q)\n        clusters = kmeans.predict(A_q)\n        cluster_centers = kmeans.cluster_centers_\n\n        dists = defaultdict(list)\n        for i, c in enumerate(clusters):\n            dist = euclidean_distances([A_q[i, :]], [cluster_centers[c, :]])[0][0]\n            dists[c].append((i, dist))\n\n        self.indices_ = [sorted(f, key=lambda x: x[1])[0][0] for f in dists.values()]\n        self.features_ = X[:, self.indices_]","4e2dd2b0":"import numpy as np\n\npfa = PFA(n_features=10)\npfa.fit(X_std)\n\n# To get the transformed matrix\nX_transformed = pfa.features_\n\n# To get the column indices of the kept features\ncolumn_indices = pfa.indices_\ncolumn_indices","8e8c3c3c":"df3.columns[column_indices].values","4ecb4014":"shst=pd.read_csv('..\/input\/data-science-for-good\/D5 SHSAT Registrations and Testers.csv')\nshst=shst.rename(columns={'DBN':'Location Code'})\nshst=shst.drop(shst.columns[1],axis=1)","400882c2":"shst.head(5)","b22716df":"# ratio of number of students who are enrolled till end of October and who actually take the test\n# participation to enrollment ratio\nshst['PEratio']=shst['Number of students who took the SHSAT']\/shst['Enrollment on 10\/31']\n# registration to enrollment ratio\nshst['REratio']=shst['Number of students who registered for the SHSAT']\/shst['Enrollment on 10\/31']\n# participation to registration ratio\nshst['PRratio']=shst['Number of students who took the SHSAT']\/shst['Number of students who registered for the SHSAT']","99fe4729":"ged_secure=pd.merge(df,shst,on='Location Code')\nged_secure.head(3)","c2c4d4b9":"ged_secure.shape","99e9dfac":"ged_secure.columns","62a4cc05":"ged_secure[['PEratio','REratio','PRratio']].apply(lambda x:sum(x.isnull()))  ","e7db4d8a":"sns.set(rc={'figure.facecolor':'lightgray'})\nsns.boxplot(y=ged_secure['PRratio'],x=ged_secure['Zip'])\nplt.title('PRratio vs Zip')\nplt.show()","3d0740ef":"import numpy as np\ngrouped=ged_secure.groupby('Zip')\na=grouped['PRratio'].agg(np.mean)\nnull_index=ged_secure[ged_secure['PRratio'].isnull()].index.tolist()\n\nfor i in null_index:\n    ged_secure.at[i,'PRratio']= a[ged_secure['Zip'][i]]     ","5716c075":"df3=ged_secure[['Economic Need Index', 'School Income Estimate','Community School?','Percent Asian', 'Percent Black', 'Percent Hispanic','Percent White', 'Student Attendance Rate',\n       'Percent of Students Chronically Absent', 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %', 'Effective School Leadership %',\n            'Strong Family-Community Ties %', 'Trust %','Average ELA Proficiency',\n       'Average Math Proficiency', 'Distance', 'crime index','Enrollment on 10\/31',\n       'Number of students who registered for the SHSAT',\n       'Number of students who took the SHSAT']]","c78a1675":"# generate the linkage matrix\nZ = linkage(df3)\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\n\nc, coph_dists = cophenet(Z, pdist(df3))\nc","5b70b885":"plt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()","d41f1ec6":"Z[-10:,2]","19da73c8":"max_d=400\nplt.figure(figsize=(25, 10))\nfancy_dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=100,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=100,\n    max_d=max_d,  # plot a horizontal cut-off line\n)\nplt.show()","0fc05e92":"last = Z[-10:, 2]\nlast_rev = last[::-1]\nidxs = np.arange(1, len(last) + 1)\nplt.figure(figsize=(20,8))\nplt.plot(idxs, last_rev)\n\nacceleration = np.diff(last, 2)  # 2nd derivative of the distances\nacceleration_rev = acceleration[::-1]\nplt.plot(idxs[:-2] + 1, acceleration_rev)\nplt.show()\nk = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\nprint(\"clusters:\", k)","2eb70013":"from scipy.cluster.hierarchy import fcluster\nmax_d = 400\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters","41c2f6c5":"Counter(clusters)","56b5e461":"ged_secure['Cluster']=fcluster(Z, max_d, criterion='distance')\nged_secure = ged_secure.reset_index(drop=True)\ndf3 = df3.reset_index(drop=True)","ca07588b":"grouped=ged_secure.groupby(['Cluster']).agg(np.mean)\ngrouped.drop(grouped.columns[1:6], axis=1)\n#grouped","7be33888":"grouped=ged_secure[['School Name','Cluster']].groupby(['School Name'],as_index=False).agg(lambda x:x.value_counts().index[0]).groupby(['Cluster'])\naug=grouped['School Name'].apply(lambda x: sorted(set(x)))[1]\nnov=grouped['School Name'].apply(lambda x: sorted(set(x)))[2]\napr=grouped['School Name'].apply(lambda x: sorted(set(x)))[3]\nmar=grouped['School Name'].apply(lambda x: sorted(set(x)))[4]\n#Oct=grouped['School State'].apply(lambda x: sorted(set(x)))['Oct']\nSep=grouped['School Name'].apply(lambda x: sorted(set(x)))[5]\n\nz={'School Name':[aug,nov,apr,mar,Sep], 'Cluster': [1,2,3,4,5]}\nz=pd.DataFrame(z)\nz.style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen','border-color': 'white'})","6a609671":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nx = StandardScaler().fit_transform(df3)\nprint('Covariance matrix \\n%s' %x)","3e95fe9c":"cov_mat = np.cov(x.T)\n\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n","514cd412":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","b6f4678f":"import  plotly\nplotly.tools.set_credentials_file(username='RishiHazra', api_key='3WYShX1Rc0UlKTzCVggk')\nimport plotly.offline as py\nfrom plotly import tools\nfrom plotly.graph_objs import *\npy.init_notebook_mode(connected=True)\n\ntot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = Bar(\n        x=['PC %s' %i for i in range(1,16)],\n        y=var_exp,\n        showlegend=False)\n\ntrace2 = Scatter(\n        x=['PC %s' %i for i in range(1,16)], \n        y=cum_var_exp,\n        name='cumulative explained variance')\n\ndata = Data([trace1, trace2])\n\nlayout=Layout(\n        yaxis=YAxis(title='Explained variance in percent'),\n        title='Explained variance by different principal components')\n\nfig = Figure(data=data, layout=layout)\npy.iplot(fig)","4360be1c":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nx = StandardScaler().fit_transform(df3)","e7a69c71":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom collections import defaultdict\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.preprocessing import StandardScaler\n\nclass PFA(object):\n    def __init__(self, n_features, q=None):\n        self.q = q\n        self.n_features = n_features\n\n    def fit(self, X):\n        if not self.q:\n            self.q = X.shape[1]\n\n        sc = StandardScaler()\n        X = sc.fit_transform(X)\n\n        pca = PCA(n_components=self.q).fit(X)\n        A_q = pca.components_.T\n\n        kmeans = KMeans(n_clusters=self.n_features).fit(A_q)\n        clusters = kmeans.predict(A_q)\n        cluster_centers = kmeans.cluster_centers_\n\n        dists = defaultdict(list)\n        for i, c in enumerate(clusters):\n            dist = euclidean_distances([A_q[i, :]], [cluster_centers[c, :]])[0][0]\n            dists[c].append((i, dist))\n\n        self.indices_ = [sorted(f, key=lambda x: x[1])[0][0] for f in dists.values()]\n        self.features_ = X[:, self.indices_]","97474831":"import numpy as np\n\n\npfa = PFA(n_features=15)\npfa.fit(x)\n\n# To get the transformed matrix\nX = pfa.features_\n\n# To get the column indices of the kept features\ncolumn_indices = pfa.indices_\ncolumn_indices","bcfd13da":"df3.columns[column_indices].values","906fa76a":"from sklearn.model_selection import train_test_split\nfeatures = ged_secure[\n    ['Economic Need Index','School Income Estimate','Community School?','Percent Asian', 'Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Percent of Students Chronically Absent', 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %', 'Effective School Leadership %',\n     'Strong Family-Community Ties %','Trust %','Average ELA Proficiency','Average Math Proficiency', 'Distance', 'crime index']].values\ntargets = ged_secure['PEratio'].values\n\nX_train1, X_test1, y_train1, y_test1 =train_test_split(features, targets, test_size=0.1, random_state=1)\nX_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape","fae98a37":"from sklearn.feature_selection import f_regression\nf_regression(X_train1, y_train1, center=True)","3f28be9a":"a=f_regression(X_train1, y_train1, center=True)[0]\nplt.figure(figsize=[15,5])\nplt.bar(range(len(a)), a,width=0.5)\n\nimport math\nxint = range(0, len(a))\nplt.xticks(xint)\nplt.show()","f78932f0":"from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import r2_score, explained_variance_score\n\ndef regression(regressor, x_train, x_test, y_train):\n    reg = regressor\n    reg.fit(x_train, y_train)\n    \n    y_train_reg = reg.predict(x_train)\n    y_test_reg = reg.predict(x_test)\n    \n    return y_train_reg, y_test_reg\n\ndef scores(regressor, y_train, y_test, y_train_reg, y_test_reg):\n    print(\"_______________________________________\")\n    print(regressor)\n    print(\"_______________________________________\")\n    print(\"R2 score. Train: \", r2_score(y_train, y_train_reg))\n    print(\"R2 score. Test: \", r2_score(y_test, y_test_reg))\n    print(\"---------\")\n    print(\"MSE (Train): \", mean_squared_error(y_train, y_train_reg))\n    print(\"MSE Test: \", mean_squared_error(y_test, y_test_reg))\n    print(\"---------\")\n    print(\"MAE (Train): \", mean_absolute_error(y_train, y_train_reg))\n    print(\"MAE (Test): \", mean_absolute_error(y_test, y_test_reg))\n    print(\"_______________________________________\")","29c1d225":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n#n_estimators=The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\ny_train_gbr1, y_test_gbr1 =regression(GradientBoostingRegressor(max_depth=10, n_estimators=100), \n           X_train1, X_test1, y_train1)\n\nscores('Gradient Boosting Regressor \\nratio of no. of students who are enrolled to the no. of students who take the test', \n       y_train1, y_test1, y_train_gbr1, y_test_gbr1)","bc91567c":"# n_estimators=The number of trees in the forest\ny_train_rfr1, y_test_rfr1 =regression(RandomForestRegressor(n_estimators=22), X_train1, X_test1, y_train1)\n\nscores('Random Forest Regressor \\nratio',y_train1, y_test1, y_train_rfr1, y_test_rfr1)","b30c4eca":"# plot feature importances\nmodel1=GradientBoostingRegressor(max_depth=7, n_estimators=100).fit(X_train1, y_train1)\nmodel2=RandomForestRegressor(n_estimators=22).fit(X_train1, y_train1)\nplt.figure(figsize=[15,5])\nplt.bar(range(len(model1.feature_importances_)), model1.feature_importances_, width=0.5)\nplt.bar(range(len(model2.feature_importances_)), -model2.feature_importances_, width=0.5)\n\nimport math\nimport matplotlib.patches as mpatches\nxint = range(0, len(model1.feature_importances_))\nplt.xticks(xint)\nblue_patch = mpatches.Patch(color='b', label='Gradient Boosting')\ngreen_patch = mpatches.Patch(color='g', label='Random Forest')\nplt.legend(handles=[blue_patch,green_patch])\nplt.show()","357bd0a6":"targets = ged_secure['REratio'].values\nfeatures = ged_secure[\n    ['Economic Need Index','School Income Estimate','Community School?','Percent Asian', 'Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Percent of Students Chronically Absent', 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %', 'Effective School Leadership %',\n     'Strong Family-Community Ties %','Trust %','Average ELA Proficiency','Average Math Proficiency', 'Distance', 'crime index']].values\nX_train1, X_test1, y_train1, y_test1 =train_test_split(features, targets, test_size=0.1, random_state=1)\nX_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape","a36ba66e":"from sklearn.feature_selection import f_regression\nf_regression(X_train1, y_train1, center=True)","d49d1d1f":"a=f_regression(X_train1, y_train1, center=True)[0]\nplt.figure(figsize=[15,5])\nplt.bar(range(len(a)), a,width=0.5)\n\nimport math\nxint = range(0, len(a))\nplt.xticks(xint)\nplt.show()","d279b5fc":"y_train_gbr1, y_test_gbr1 =regression(GradientBoostingRegressor(max_depth=5, n_estimators=100), \n           X_train1, X_test1, y_train1)\n\nscores('Gradient Boosting Regressor \\nratio of no. of students who are enrolled to the no. of students who take the test', \n       y_train1, y_test1, y_train_gbr1, y_test_gbr1)","73808160":"# n_estimators=The number of trees in the forest\ny_train_rfr1, y_test_rfr1 =regression(RandomForestRegressor(n_estimators=25), \n           X_train1, X_test1, y_train1)\n\nscores('Random Forest Regressor \\nratio',y_train1, y_test1, y_train_rfr1, y_test_rfr1)","ee7b7665":"# plot feature importances\n\nmodel1=GradientBoostingRegressor(max_depth=7, n_estimators=100).fit(X_train1, y_train1)\nmodel2=RandomForestRegressor(n_estimators=22).fit(X_train1, y_train1)\nplt.figure(figsize=[15,5])\nplt.bar(range(len(model1.feature_importances_)), model1.feature_importances_, width=0.5)\nplt.bar(range(len(model2.feature_importances_)), -model2.feature_importances_, width=0.5)\n\nimport math\nxint = range(0, len(model1.feature_importances_))\nplt.xticks(xint)\nblue_patch = mpatches.Patch(color='b', label='Gradient Boosting')\ngreen_patch = mpatches.Patch(color='g', label='Random Forest')\nplt.legend(handles=[blue_patch,green_patch])\nplt.show()","fc2380b2":"ged_secure1=ged_secure.dropna()\ntargets = ged_secure1['PRratio'].values\nfeatures = ged_secure1[\n    ['Economic Need Index','School Income Estimate','Community School?','Percent Asian', 'Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Percent of Students Chronically Absent', 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %', 'Effective School Leadership %',\n     'Strong Family-Community Ties %','Trust %','Average ELA Proficiency','Average Math Proficiency', 'Distance', 'crime index']].values\nX_train1, X_test1, y_train1, y_test1 =train_test_split(features, targets, test_size=0.1, random_state=1)\nX_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape","6a0f4f8e":"from sklearn.feature_selection import f_regression\nf_regression(X_train1, y_train1, center=True)","6c24912c":"a=f_regression(X_train1, y_train1, center=True)[0]\nplt.figure(figsize=[15,5])\nplt.bar(range(len(a)), a,width=0.5)\n\nimport math\nxint = range(0, len(a))\nplt.xticks(xint)\nplt.show()","4ff0cfac":"y_train_gbr1, y_test_gbr1 =regression(GradientBoostingRegressor(max_depth=5, n_estimators=100), \n           X_train1, X_test1, y_train1)\n\nscores('Gradient Boosting Regressor \\nratio of no. of students who are enrolled to the no. of students who take the test', \n       y_train1, y_test1, y_train_gbr1, y_test_gbr1)\n","ff0bd0a0":"# n_estimators=The number of trees in the forest\ny_train_rfr1, y_test_rfr1 =regression(RandomForestRegressor(n_estimators=25), \n           X_train1, X_test1, y_train1)\n\nscores('Random Forest Regressor \\nratio', \n       y_train1, y_test1, y_train_rfr1, y_test_rfr1)\n\n# plot feature importances","6f701e87":"model1=GradientBoostingRegressor(max_depth=7, n_estimators=100).fit(X_train1, y_train1)\nmodel2=RandomForestRegressor(n_estimators=22).fit(X_train1, y_train1)\nplt.figure(figsize=[15,5])\nplt.bar(range(len(model1.feature_importances_)), model1.feature_importances_, width=0.5)\nplt.bar(range(len(model2.feature_importances_)), -model2.feature_importances_, width=0.5)\n\nimport math\nxint = range(0, len(model1.feature_importances_))\nplt.xticks(xint)\nimport matplotlib.patches as mpatches\n\nblue_patch = mpatches.Patch(color='b', label='Gradient Boosting')\ngreen_patch = mpatches.Patch(color='g', label='Random Forest')\nplt.legend(handles=[blue_patch,green_patch])\nplt.show()","fc190618":"features = ged_secure1[\n    ['Economic Need Index','Community School?','Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Distance', 'crime index']]\nmin_vec=features.min(axis=0)\nmax_vec=features.max(axis=0)","f043a9d4":"features_normalized=(features.sub(features.mean(axis=0), axis=1))\/ features.std(axis=0)","9acae083":"targets = ged_secure1['PEratio'].values\na=f_regression(features_normalized.values,targets, center=True)[0]\nf_regression(features_normalized.values,targets, center=True)[0]","a3db2a66":"targets = ged_secure1['PRratio'].values\nb=f_regression(features_normalized.values,targets, center=True)[0]\nf_regression(features_normalized.values,targets, center=True)[0]","12387ccd":"targets = ged_secure1['REratio'].values\nc=f_regression(features_normalized.values,targets, center=True)[0]\nf_regression(features_normalized.values,targets, center=True)[0]","d89e3e09":"weights=(a+b+c)\/3","a449b155":"features = df[['School Name', 'Location Code', 'District', 'Latitude', 'Longitude',\n       'Address (Full)', 'City', 'Zip','Economic Need Index','Community School?','Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Distance', 'crime index']]\nmin_vec=features[['Economic Need Index','Community School?','Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Distance', 'crime index']].min(axis=0)\nmax_vec=features[['Economic Need Index','Community School?','Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Distance', 'crime index']].max(axis=0)\n\nfeatures_normalized=(features[['Economic Need Index','Community School?','Percent Black', 'Percent Hispanic','Percent White', \n     'Student Attendance Rate','Distance', 'crime index']].sub(min_vec, axis=1))\/ (max_vec-min_vec)","c4e625a9":"weighed_features=features_normalized*weights\nfeatures['Score']=weighed_features.sum(axis=1)\nfeatures=features.sort_values('Score', ascending=False).drop_duplicates('School Name')","d7832404":"features1=features[:10]\nfeatures1","5f687c37":"map_1 = folium.Map(location=[40.755048, -73.926963],\n                   zoom_start=9.5,\n                   tiles='cartodbdark_matter')\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[0], popup=features1.reset_index(drop=False)['School Name'].loc[0]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[1], popup=features1.reset_index(drop=False)['School Name'].loc[1]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[2], popup=features1.reset_index(drop=False)['School Name'].loc[2]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[3], popup=features1.reset_index(drop=False)['School Name'].loc[3]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[4], popup=features1.reset_index(drop=False)['School Name'].loc[4]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[5], popup=features1.reset_index(drop=False)['School Name'].loc[5]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[6], popup=features1.reset_index(drop=False)['School Name'].loc[6]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[7], popup=features1.reset_index(drop=False)['School Name'].loc[7]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[8], popup=features1.reset_index(drop=False)['School Name'].loc[8]).add_to(map_1)\nfolium.Marker(features1.reset_index(drop=False)[['Latitude','Longitude']].loc[9], popup=features1.reset_index(drop=False)['School Name'].loc[9]).add_to(map_1)\n\n\ni=0\nfor lat, lon in zip(df['Latitude'], df['Longitude']):\n    folium.CircleMarker([lat, lon], color=step(df['Cluster'][i]), fill=True, radius=0.9).add_to(map_1)    \n    i+=1\n\nmap_1","666e32d9":"z=ged[['Program Site name','Latitude','Longitude','Borough']].dropna()\n\nimport branca.colormap as cm\nimport folium\nfrom folium import plugins\n\nstep = cm.StepColormap(\n    ['aqua','yellow','red'],\n    vmin=0.5, vmax=3.5,\n    index=[0.5,1.5,2.5],\n    caption='step'\n)\n    \nstep","d808582d":">Before proceeding further, I'll normalize all features and bring them on the same scale so that none of the features acts to overpower the others. I'll be dividing all the features by their max values.","dd13bb62":"### > Top 10 Schools which require intervention","df6f7536":"Since we won't be using 'Notes' column, we donot preprocess NaN values in ged","4715be9e":"In my previous kernel on PASSNYC, I performed an in-depth exploration in order to understand the dataset which included geography analysis, time series trends, distributions of important variables of the dataset, analysis, and comparisons.\n","5a5be43f":"## 3.1 Hierarchial Clustering","d5971027":"### > How many of them are not community schools?","455614f8":"### > train-test split","02864bb9":"**The top 10 schools represented along with the previous grouping generated by hierarchial clustering.**   \n\n**Click on the marker to the know the name of the Schools.**","571b5622":"### > Using F-Regression:","17291cff":"** I am going to use the hierarchial clustering. **","229fdd25":"### > Normalizing the whole dataset","0e939e59":"### > Taking a look at the neighbourhood in our dataframe","c68b5e70":"* **Most Schools are located in Brooklyn and Bronx where the percentage of Black and Hispanic are high.**","9191e03e":"### > I shall consider the average of all the F-scores as weights of the features.","bb1c2d7a":"### > train-test split","1dedeabf":"* Year 2016 Ratio of number of students who appear for the test to the number of students who are enrolled \n* Year 2016 Ratio of number of students who register for the test to the number of students who are enrolled\n* Year 2016 Ratio of number of students who appear for the test to the number of students who have registered\n* Difference in ratio of number of students who appear for the test to the number of students who are enrolled (2013-2016)\n* Difference in the ratio of number of students who register for the test to the number of students who are enrolled (2013-2016)\n* Difference in ratio of number of students who appear for the test to the number of students who registered for the test (2013-2016)","53f1f3e8":"### > Generate the clusters","83d36eca":"### > Most important features","ec7fcf39":"* In the course of this analysis, I shall proceed with the schools which have ratings: **Approaching Target** or **Not Meeting Target**.","cb66e852":"* **We create a common index called a Security index from all types of crimes committed (which includes Major crimes, Violent crimes, Property crimes, Non criminal crimes and other crimes).**","251663ea":"### > Let's look at the schools in each cluster","f2a0cc53":"### > Cophenetic Correlation Coefficient\nWith help of the cophenet() function. This compares (correlates) the actual pairwise distances of all our samples to those implied by the hierarchical clustering. The closer the value is to 1, the better the clustering preserves the original distances.","e53f7a2d":"* **Blue: CLuster 1**\n* **Yellow: Cluster 2**\n* **Red: Cluster 3**","592d77e6":"### > Using F-Regression:","daf88e9c":"## 3.2 PCA","7e9d715e":"The above shows a truncated dendrogram, which only shows the last p=100.","1823cf2f":"### > Obtaining the most important features ","2efdc7ea":"### **Objective:** \"Which schools have students that would benefit from outreach services and lead to a more diverse group of students taking the SHSAT and being accepted into New York City's Specialized High Schools.\"","7fe394ae":">* **Our efforts should be directed at :**  \n>1) Increase in SHSAT registration (Year 2013-2016)  \n>2) Increase in SHSAT participation (Year 2013-2016)    \n>3) Participation to registration ratio (Year 2016)   \n>4) Participation to enrollment ratio (Year 2016)   ","d4d6f760":"## 0. Pre-processing","a5f1acc6":"# ---------------------------------------------------------------------------------------------------------------","b1073322":"* **If we can rank order schools and be granular about it or show schools on a map and take into account diversity, language, poverty, weather, public transit, whatever, then that's probably a good way to go about it.**","5afb0cfa":"### > Let's look at the schools in each cluster","a1909ea6":"# ---------------------------------------------------------------------------------------------------------------","6a9f23ab":"Now while this manual selection of a cut-off value offers a lot of benefits when it comes to checking for a meaningful clustering and cut-off, there are cases in which we can automate this.","cab0b637":"### > Binarizing the 'Community School?' column","72c45a8b":"* **Replacing School Income Estimate nan values of Community Schools with the Community School average.**","ed1ea897":"**Thus, we see that the following features (which have a high F-score) are most important estimators of our target variable.     \nIn the order of precedence.......  **\n\n**1) School Attendance Rate  **    \n**2) Strong Family-Community Ties %   **    \n**3) Percent of Students Chronically Absent  **    \n**4) crime index    **    \n**5) Economic Need Index   **    \n**6) Percent Black   **        \n**7) Percent Hispanic  **     \n**8) School Income Estimate **  \n\n","13e9bee3":"* **Blue: CLuster 1**\n* **Yellow: Cluster 2**\n* **Red: Cluster 3**","bd0f7430":"## 1. Perform the Hierarchical Clustering\nNow that we have some very simple sample data, let's do the actual clustering on it:","3b89ebff":"### > From the above methods, we conclude that the most important features are as follows:\n\n* **Economic Need Index**\n* **School Attendance Rate**\n* **Distance**\n* **crime index**\n* **Percent Black**\n* **Percent Hispanic**\n* **Percent White (plays a major role as minority)**","a9d3d302":">**Datasets used :**     \n(1) 2016 School Explorer.csv      \n(2) D5 SHSAT Registrations and Testers.csv     \n(3) ged-plus-locations.csv     \n(4) 2010-2016-school-safety-report.csv     ","1870de32":"**Thus, we see that the following features (which have a high F-score) are most important estimators of our target variable.     \nIn the order of precedence.......  **\n\n**1) Distance  **    \n**2) Percent Hispanic   **    \n**3) Percent Black  **    \n**4) Economic Need Index   **    \n**5) Average Math Proficiency   **   ","f6fd3885":"### > Looking at null values","5fd9616c":"**1. Knowing max_d:\nLet's say we determined the max distance with help of a dendrogram, then we can do the following to get the cluster id for each of our samples:**","aaaae128":"### > Finding the closest GED centres to these schools","15796040":"# ---------------------------------------------------------------------------------------------------------------","ad6bceec":"** A total of 15 dimensions account for more than 98% of the data. **","5e5e46f7":"## Exploratory Data Analysis","33d6a9c5":"### > NaN values in the ratio","5e35d0ff":"# This kernel is still in development process. Feel free to post your suggestions. Thank You.","701677b7":"**2. Knowing k:\nAnother way starting from the dendrogram is to say \"i can see i have k=2\" clusters. We can then use:**","707fe803":"## 3. Merging with th SHSAT data of District 5","0c2a3359":"Thus, we see that the following features (which have high importance in either of the methods) are most important estimators of our target variable. **Economic Need Index , Percent Hispanic, Percent Black, Distance**.\n","fb1d17b9":"### > Selecting a Distance Cut-Off aka Determining the Number of Clusters\nAs explained above already, a huge jump in distance is typically what we're interested in if we want to argue for a certain number of clusters. If we have the chance to do this manually, i would always opt for that, as it allows us to gain some insights into the data and to perform some sanity checks on the edge cases. In our case, I would say that the cutoff is 1260 as the jump is prety obvious.","b9618f41":"## [Table of Content :](#as)    \n\n### [1. Preprocessing](#asd)   \n### [2. Hierarchial Clustering](#asd)   \n### [3. PFA : Principal Feature Analysis](#asd)   \n### [4. Regression : Gradient Boosting & Random Forest](#asd)   \n### [5. Obtaining most important features](#asdf)   \n### [6. Obtaining feature weights using SHST dataset](#asdf)   \n### [7. School ranking](#df)  \n","8f2aa3a3":"* Let us formulate a few basic assumptions based on our objective. We are to rank schools in order to encourage more paricipation among students so that the ones on the top benefit from the outreach services. \n\n* Henceforth, we shall consider the following as our target(Y)- values. The other features are our X-values.","6f314c94":"### > PFA (Principal Feature Analysis): Obtaining the most important features ","b7231e5d":">PCA has the disadvantage that measurements from all the original features are used in the projection to the lower dimensional space. PFA, on the other hand performs dimensionality reduction of a feature set by choosing a subset of the original features that contains most of the essential information, using the same criteria as PCA.","4615faa3":"## 4. Obtaining the feature importance from training","de91a3d2":"### > Using Gradient Boosting and\/or Random Forest Regressor\nImportance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity used to select the split points or another more specific error function. The feature importances are then averaged across all of the the decision trees within the model.\n\nRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set.","abae0fed":"### > Retrieve the Clusters\n**Now, let's finally have a look at how to retrieve the clusters, for different ways of determining k. We can use the fcluster function.**","87a6e3e3":"### > Null values of secure dataset","10b5cb97":"### > Automated Cut-Off Selection","f404f25e":"### > train-test split","34e81086":"* **Let's neglect the cluster 4. On careful observation, we can see that Cluster 2 has the highest Percentage of Black and Hispanic students = 40.8% and 50%. Cluster 1 has the highest percentage of Asian = 20.5% and cluster 3 has the highest percentage of White = 29.25%.  **   \n\n* **This explains the high Economic Index of cluster 2 and low Economic Index for cluster 3. Moreover, the crime index is higher in the areas of cluster 2 = 1.06. It's lowest in cluster 1 = 0.88.**   \n\n* **The Distance of the closest GED centres is more in case of cluster 3 = 4.19.**\n\n* **Student Attendance Rate seems to lowest for Cluster 2 = 91.34% with a higher percentage of students chronically absent = 25.77%.**\n\n* **Average ELA and Math Proficiency are lowest for cluster 2.**\n\n* **Fewer students have score 4 in Grade 8 in cluster 2.**     \n\n> This gives us a rough idea of the important features coming into play.\n\n","c0d2bdb2":"* As is evident from the above dataframe, **Exceeding Target and Meeting Target** fare better in terms of proficiency in **Maths and English.** Also, majority of students in schools with rating of Exceeding Target and Meeting Target are either **White or Asian.**","7c71742c":"Let's visualize this in the dendrogram as a cut-off line:","488a4c11":"* I use the **euclidean distance metric** as this produces the **highest Cophenetic Correlation Coefficient.**","cca07894":"* **Replacing the NaN values with average of schools which are not Community Schools.**","18c2cc86":"* Now what do we do with the ramaining NaN values in these columns ?\n* We drop columns with NaN values in these columns as all districts have Exceeding Target and Meeting Target as their majority.","34a33ac3":"# ............................................................................................................................","57df3a61":"### > Using Gradient Boosting and\/or Random Forest Regressor\nImportance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity used to select the split points or another more specific error function. The feature importances are then averaged across all of the the decision trees within the model.\n\nRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set.\n","8cacb95f":"The **GED, General Educational Diploma**, is for those without a High School Diploma. Study and take a battery of tests to certify your aptitude, knowledge and skills. It is designed for those that never finished high school. Find a local test center near you. The GED, which stands for General Educational Development but is also referred to as a General Education Diploma, is a set of tests that when passed certify the test taker (American or Canadian) has met high-school level academic skills.","033cc699":"### > Weighed sum of features","1e1dbd00":"I will be using two methods to rank the features.   \n1) Using F-Regression   \n2) Using Gradient Boosting and\/or Random Forest Regressor   ","61fa2bb5":"* This shows that there should be 6 clusters.","e283cfd5":"* **Replacing the remaining NaN values with district average of certain districts (which have a low variance)**","116a62de":"Thus, we see that the following features (which have high importance in either of the methods) are most important estimators of our target variable.\n**'Distance','Percent Black','Percent Hispanic','Economic Need Index','Distance','Strong Family-Community Ties %','Trust %',**","dcfd2eb6":"> PCA has the disadvantage that measurements from all the original features are used in the projection to the lower dimensional space. PFA, on the other hand performs dimensionality reduction of a feature set by choosing a subset of the original features that contains most of the essential information, using the same criteria as PCA. ","68adf7d5":"# ---------------------------------------------------------------------------------------------------------------","cb1c8e75":"### > Eye Candy\nEven though this already makes for quite a nice visualization, we can pimp it even more by also annotating the distances inside the dendrogram by using some of the useful return values dendrogram():","394054f3":"## 2. PFA (Principal Feature Analysis) ","c7ff770f":"### > Using Gradient Boosting and\/or Random Forest Regressor\n\nImportance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity used to select the split points or another more specific error function. The feature importances are then averaged across all of the the decision trees within the model.\n\nRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set.","d1667a0f":"### > Using F-Regression: \n\nF-regression does the following:\n\n* Start with a constant model, M0\n* Try all models M1 consisting of just one feature and pick the best according to the F statistic\n* Try all models M2 consisting of M1 plus one other feature and pick the best ...","e61cc24e":"# ---------------------------------------------------------------------------------------------------------------","fcc3305d":"### 4.3 Year 2016 Ratio of number of students who appear for the to the number of students who are registered for the test","34be9162":"### 4.2 Year 2016 Ratio of number of students who register for the test to the number of students  who are enrolled ","53cd0e36":">**Background :** The Specialized High Schools Admissions Test (SHSAT) is an examination administered to eighth and ninth grade students residing in New York City and used to determine admission to all but one of the city's nine Specialized High Schools. In 2008, about 29,000 students took the test, and 6,108 students were offered admission to one of the high schools based on the results. On average, 30,000 students take this exam annually. The test is given each year in October and November, and students are informed of their results the following March. Those who receive offers decide by the middle of March whether to attend the school the following September. The test is independently produced and graded by American Guidance Service, a subsidiary of Pearson Education, under contract to the New York City Department of Education.","afdcbf7a":"* Since, I am going to use the crime columns only **(Major N, Oth N, Prop N, NoCrim N, Vio N)**, I'll try and remove the NaN values from the dataset using the mean value of the districts.","aa4672b5":"## 5. Obtaining feature weights","38c9f750":"> * The GED Tests include five subject area tests: **Language Arts\/Writing, Language Arts\/Reading, Social Studies, Science, and Mathematics.**\n>\n>* In addition to **English**, the GED tests are available in **Spanish, French, large print, audiocassette and Braille.**\n>\n>* The GED credential itself is issued by the state, province or territory in which the test taker lives.\n>\n>* Many government institutions and universities regard the GED as the same as a high school diploma with respect to program eligibility and as a prerequisite for admissions.","f384585f":"Let's take apart the dendogram....\n\n* On the x axis you see labels. If you don't specify anything else they are the indices of your samples in X.\n* On the y axis you see the distances (of the 'ward' method in our case).\n\nStarting from each label at the bottom, you can see a vertical line up to a horizontal line. The height of that horizontal line tells you about the distance at which this label was merged into another label or cluster.\n\nLet's have a look at the distances of the last 10 merges:","54cbeca9":"### > Crime Index","f20beb5b":"**Replacing the NaN values in PRratio by the mean value of the PRratio of Zip**","13832a59":"## 6. Ranking the schools","feaacea4":"### > Elbow Method\nIt tries to find the clustering step where the acceleration of distance growth is the biggest (the \"strongest elbow\" of the blue line graph below, which is the highest value of the green graph below):","06b68f4b":"### > Where are these schools located?","41565b6d":"# ................................................................................................................................","d800b597":"Thus, we see that the following features (which have high importance in either of the methods) are most important estimators of our target variable.\n**'School Attendance Rate','Percent of Students Chronically Absent','Percent White','Economic Need Index','Distance','crime index','Average ELA Proficiency','Percent Black','Percent Hispanic'**.\n","5a3d0a62":"The term **'community school'** refers to a type of publicly funded school in the United States that serves as **both an educational institution and a centre of community life.** A community school is both a place and a set of partnerships between the school and other community resources. Its integrated focus on academics, youth development, family support, health and social services and community development leads to improved student learning, stronger families and healthier communities. Using public schools as hubs, community schools bring together many partners to offer a range of support and opportunities to children, youth, families and communities\u2014before, during and after school, and on weekends.   \n\n\n<font color='darkgray'>@ Wikipedia<\/font>","d2593d74":"### > Most important features","398835d8":"### 4.1 Year 2016 Ratio of number of students who appear for the test to the number of students who are enrolled ","19b80a4b":"### > Standardizing","b818dd8d":"# ---------------------------------------------------------------------------------------------------------------","06ff499f":"We can also see that from distances > 21080 up there's a huge jump of the distance to the final merge at a distance of approx 61352. Such distance jumps \/ gaps in the dendrogram are pretty interesting for us. They indicate that something is merged here, that maybe just shouldn't be merged. In other words: maybe the things that were merged here really don't belong to the same cluster, telling us that maybe there's just 4 clusters here.","aa426669":"* **Huge jump after distance=357**","c5febf73":"**Since grade 8 and grade 9 students can only take the SHSAT, we remove all schools which has neither of the grades (upper grade<8 or lower grade>9).**\n","15635a76":"## 7. Conclusion:\nAs is evident from the mapping, most of the schools belong to Cluster 2 which has the highest Percentage of Black and Hispanic students = 40.8% and 50% (as deduced earlier). Moreover, the crime index is higher in the areas of cluster 2 = 1.06. Student Attendance Rate seems to lowest for Cluster 2 = 91.34% with a higher percentage of students chronically absent = 25.77%. Average ELA and Math Proficiency are lowest for cluster 2. ewer students have score 4 in Grade 8 in cluster 2. Also, cluster 2 has the highest Economic Index.    \n\nThus this is consistent with our clustering results.","be0f9886":"# ---------------------------------------------------------------------------------------------------------------","a4d99767":"**Thus, we see that the following features (which have a high F-score) are most important estimators of our target variable.     \nIn the order of precedence.......  **\n\n**1) Distance  **    \n**2) Percent Hispanic   **    \n**3) Percent Black  **    \n**4) Supportive Environment %   **    \n**5) Economic Need Index   **    \n**6) Collaborative Teachers %   **        \n**7) Student Attendance Rate  **     \n**8) crime index **   \n**9) Percent White **"}}