{"cell_type":{"ba68a99d":"code","460b8efc":"code","960c7770":"code","cf83109d":"code","fa7656cc":"code","3aacd9b8":"code","cd72b862":"code","4830b0f1":"code","4df5f9f3":"code","bb371b98":"code","bdf63c39":"code","7a4aa312":"code","99f384c8":"code","387d6a00":"code","04bd2664":"code","2f4e6113":"code","fd2ed8c0":"code","d80dc1ed":"code","ef0faf43":"code","fca55400":"markdown","645d254d":"markdown","8343cd3d":"markdown","04ecb84c":"markdown","d4781e3f":"markdown","473b9227":"markdown","09fa8655":"markdown","152251e8":"markdown","91cc7281":"markdown","eefe1356":"markdown","0deef97f":"markdown","beaaf7f2":"markdown","ae13fdc7":"markdown","113d017f":"markdown"},"source":{"ba68a99d":"!pip install -U -qq tensorflow","460b8efc":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions","960c7770":"df = pd.read_csv('..\/input\/palmer-penguine\/Penguindata.csv')\ndf = df.dropna()\ndf.head()","cf83109d":"categories = df['Species'].unique()\ncat_to_idx = {k:v for k,v in enumerate(categories)}\nidx_to_cat = {v:k for k,v in enumerate(categories)}\n\ndata = df[['Culmen Length (mm)', 'Culmen Depth (mm)']].values\ntargets = df.replace(idx_to_cat)['Species'].values","fa7656cc":"X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, shuffle=True)","3aacd9b8":"labels = cat_to_idx\nlabel_colours = ['blue', 'black', 'red']\n\ndef plot_data(x, y, labels, colours):\n    for c in np.unique(y):\n        inx = np.where(y == c)\n        plt.scatter(x[inx, 0], x[inx, 1], label=labels[c], c=colours[c])\n    plt.title(\"Training set\")\n    plt.xlabel(\"Culmen Length (mm)\")\n    plt.ylabel(\"Culmen Depth (mm)\")\n    plt.legend(loc=\"lower right\")\n    \nplt.figure(figsize=(14, 8))\nplot_data(X_train, y_train, labels, label_colours)\nplt.show()","cd72b862":"def get_prior(y):\n    priors = np.zeros_like(np.unique(y_train))\n    for c_num in range(priors.shape[0]):\n        priors[c_num] = np.sum(y_train == c_num)\n\n    priors = priors\/priors.sum()\n    return tfd.Categorical(probs=priors)\n\nprior = get_prior(y_train)\nprior","4830b0f1":"plt.figure(figsize=(14, 8))\nplt.bar([0, 1, 2], prior.probs.numpy(), color=label_colours, width=0.5)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Prior probability\")\nplt.title(\"Class prior distribution\")\nplt.xticks([0, 1, 2], labels)\nplt.show()","4df5f9f3":"def get_class_conditionals(x, y):\n    locs = np.zeros(shape=(y.max()+1, x.shape[1]))\n    scales = np.zeros(shape=(y.max()+1, x.shape[1]))\n    \n    for c_num in range(y.max()+1):\n        mask = (y == c_num)\n        locs[c_num] = np.mean(x[mask], axis=0)\n        scales[c_num] = np.var(x[mask], axis=0)**0.5\n\n    return tfd.MultivariateNormalDiag(loc=locs, scale_diag=scales)\n\nclass_conditionals = get_class_conditionals(X_train, y_train)\nclass_conditionals","bb371b98":"def get_meshgrid(x0_range, x1_range, num_points=100):\n    x0 = np.linspace(x0_range[0], x0_range[1], num_points)\n    x1 = np.linspace(x1_range[0], x1_range[1], num_points)\n    return np.meshgrid(x0, x1)\n\ndef contour_plot(x0_range, x1_range, prob_fn, batch_shape, colours, levels=None, num_points=100):\n    X0, X1 = get_meshgrid(x0_range, x1_range, num_points=num_points)\n    Z = prob_fn(np.expand_dims(np.array([X0.ravel(), X1.ravel()]).T, axis=1))\n    Z = np.array(Z).T.reshape(batch_shape, *X0.shape)\n    for batch in np.arange(batch_shape):\n        if levels:\n            plt.contourf(X0, X1, Z[batch], alpha=0.2, colors=colours, levels=levels)\n        else:\n            plt.contour(X0, X1, Z[batch], colors=colours[batch], alpha=0.3)\n\nplt.figure(figsize=(14, 8))\nplot_data(X_train, y_train, labels, label_colours)\nx0_min, x0_max = X_train[:, 0].min(), X_train[:, 0].max()\nx1_min, x1_max = X_train[:, 1].min(), X_train[:, 1].max()\ncontour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals.prob, 3, label_colours)\nplt.title(\"Training set with class-conditional density contours\")\nplt.show()","bdf63c39":"def predict_class(prior, class_conditionals, x):\n    cond_prob = class_conditionals.log_prob(np.expand_dims(x, axis=1))\n    joint_likelihood = tf.add(np.log(prior.probs), cond_prob)\n    norm_factor = tf.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)\n    log_prob = joint_likelihood - norm_factor\n\n    return tf.argmax(log_prob, axis=-1).numpy()\n\npredictions = predict_class(prior, class_conditionals, X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Test accuracy: {:.4f}\".format(accuracy))","7a4aa312":"plt.figure(figsize=(14, 8))\nplot_data(X_train, y_train, labels, label_colours)\nx0_min, x0_max = X_train[:, 0].min(), X_train[:, 0].max()\nx1_min, x1_max = X_train[:, 1].min(), X_train[:, 1].max()\ncontour_plot((x0_min, x0_max), (x1_min, x1_max), \n             lambda x: predict_class(prior, class_conditionals, x), \n             1, label_colours, levels=[-0.5, 0.5, 1.5, 2.5],\n             num_points=500)\nplt.title(\"Training set with decision regions\")\nplt.show()","99f384c8":"scales = tf.Variable([1., 1.], dtype=tf.float32, trainable=True)\noptimiser = tf.keras.optimizers.Adam(learning_rate=0.01)\nepochs = 300\n\ntrain_loss_results = []\ntrain_scales_results = []\n\nx = X_train.astype(np.float32)\ny = y_train.astype(np.int32)","387d6a00":"locs = np.zeros(shape=(y.max()+1, x.shape[1]))\nweights = []\n\nfor c_num in range(y.max()+1):\n    mask = (y == c_num)\n    locs[c_num] = np.mean(x[mask], axis=0)\n    weights.append(mask.sum())\n\nlocs = tf.constant(locs, dtype=tf.float32)\ndist = tfd.MultivariateNormalDiag(loc=locs, scale_diag=scales)\n\n# Class Contribution\nweights = np.array(weights, dtype=np.float32)\nweights = weights\/weights.sum()","04bd2664":"for epoch in range(epochs):\n    with tf.GradientTape() as tape:\n        tape.watch(dist.trainable_variables)\n        # Negative Log Likelihood\n        loss = dist.log_prob(x[:,np.newaxis, :])*tf.one_hot(y, depth=3)\n        loss = -tf.reduce_sum(loss)\n        # Gradients\n        grads = tape.gradient(loss, dist.trainable_variables)\n    # Update gradients\n    optimiser.apply_gradients(zip(grads, dist.trainable_variables))\n    \n    train_loss_results.append(loss)\n    train_scales_results.append(dist.stddev().numpy()[0])\n    \n    if epoch%10 == 9:\n            print(\"Epoch: {:03d} \\t Loss: {:0.6f} \\t StdDev_1: {:0.6f} \\t StdDev_2: {:0.6f}\".format(1+epoch, \n                                                                                                    loss, \n                                                                                                    train_scales_results[-1][0],\n                                                                                                    train_scales_results[-1][1]))","2f4e6113":"nlls, scales_arr, class_conditionals = np.array(train_loss_results), np.array(train_scales_results), dist","fd2ed8c0":"print(\"Class conditional means:\")\nprint(class_conditionals.loc.numpy())\nprint(\"\\nClass conditional standard deviations:\")\nprint(class_conditionals.stddev().numpy())","d80dc1ed":"stddevs = np.zeros(shape=(2,2))\n\nfor c_num in range(2):\n    mask = (y_train == c_num)\n    stddevs[c_num] = np.mean((X_train[mask]-class_conditionals.loc.numpy()[c_num])**2, axis=0)\n\nprint(\"Expected conditional standard deviations:\\n\", stddevs)","ef0faf43":"plt.figure(figsize=(14, 8))\nplot_data(X_train, y_train, labels, label_colours)\nx0_min, x0_max = X_train[:, 0].min(), X_train[:, 0].max()\nx1_min, x1_max = X_train[:, 1].min(), X_train[:, 1].max()\ncontour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals.prob, 3, label_colours)\nplt.title(\"Predictions\")\nplt.show()","fca55400":"<h1 id=\"training\" style=\"color:orange; background:white; border:0.5px dotted orange;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\" rel=\" noreferrer nofollow\">\u00b6<\/a>\n    <\/center>\n<\/h1>","645d254d":"## Plot the training set","8343cd3d":"## Parameters","04ecb84c":"## Training weights","d4781e3f":"## Prior","473b9227":"<div width=\"100%\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1446872\/2393125\/294f95f2d596414de70edc35dbe6d0fe\/dataset-cover.png\" width=\"100%\">\n<\/div>","09fa8655":"<h1 id=\"dataset\" style=\"color:purple; background:white; border:0.5px dotted purple;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\" rel=\" noreferrer nofollow\">\u00b6<\/a>\n    <\/center>\n<\/h1>","152251e8":"<h1 id=\"statistics\" style=\"color:green; background:white; border:0.5px dotted green;\"> \n    <center>Statistics\n        <a class=\"anchor-link\" href=\"#statistics\" target=\"_self\" rel=\" noreferrer nofollow\">\u00b6<\/a>\n    <\/center>\n<\/h1>","91cc7281":"## Class conditionals","eefe1356":"## Split dataset","0deef97f":"## Optimize the weights","beaaf7f2":"## Plot contours","ae13fdc7":"## Results","113d017f":"## Predict class"}}