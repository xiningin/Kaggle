{"cell_type":{"7a6fa910":"code","6a493218":"code","91fe4e02":"code","e4e5decd":"code","3d07b929":"code","4af49d2a":"code","b8ea6333":"code","06f72f24":"code","e625db16":"code","7100fd3b":"code","b9837a7f":"code","95e3435e":"code","40c45c04":"code","78ad85d3":"code","a1e31b14":"code","6a711b37":"code","aaafa3a4":"code","ffbcfbed":"code","8246feba":"code","68bd1ac1":"code","645fbea8":"code","50f69c8f":"markdown","c220f08c":"markdown","fe212a6a":"markdown"},"source":{"7a6fa910":"## Importing standard libraries\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6a493218":"## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","91fe4e02":"## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation\nfrom keras.utils.np_utils import to_categorical","e4e5decd":"## Set figure size to 20x10\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,10","3d07b929":"## Read data from the CSV file\n\ndata = pd.read_csv('..\/input\/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')","4af49d2a":"data.shape","b8ea6333":"## Since the labels are textual, so we encode them categorically\n\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)","06f72f24":"## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\n\nX = StandardScaler().fit(data).transform(data)\nprint(X.shape)","e625db16":"## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\n\ny_cat = to_categorical(y)\nprint(y_cat.shape)","7100fd3b":"model = Sequential()\nmodel.add(Dense(256, input_dim=108, init='uniform', activation='relu'))\nmodel.add(Dense(128, init='normal', activation='sigmoid'))\nmodel.add(Dense(99, activation='softmax'))","b9837a7f":"## Developing a layered model for Neural Networks\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n\nmodel = Sequential()\nmodel.add(Dense(512,input_dim=192,  init='uniform', activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='sigmoid'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(99, activation='softmax'))","95e3435e":"## Error is measured as categorical crossentropy or multiclass logloss\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])","40c45c04":"## Fitting the model on the whole training data\nhistory = model.fit(X,y_cat,batch_size=192,\n                    nb_epoch=130,verbose=0, validation_split=0.1)","78ad85d3":"min(history.history['val_acc'])","a1e31b14":"## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\n\nplt.plot(history.history['val_acc'],'o-')\n#plt.plot(history.history['loss'],'o-')\nplt.xlabel('Number of Iterations')\nplt.ylabel('Categorical Crossentropy')\nplt.title('Train Error vs Number of Iterations')","6a711b37":"test = pd.read_csv('..\/input\/test.csv')","aaafa3a4":"index = test.pop('id')","ffbcfbed":"test = StandardScaler().fit(test).transform(test)","8246feba":"yPred = model.predict_proba(test)","68bd1ac1":"## Converting the test predictions in a dataframe as depicted by sample submission\n\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))","645fbea8":"fp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())","50f69c8f":"---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!","c220f08c":"## Using Neural Networks through Keras","fe212a6a":"lets  make it better !**strong text**"}}