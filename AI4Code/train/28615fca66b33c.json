{"cell_type":{"9df293c5":"code","384455c9":"code","a67086e7":"code","0bbd1eeb":"code","a4b199fb":"code","723ae94d":"code","12b4ccf9":"code","9a787e17":"code","cd954ff2":"code","713ee7a6":"code","06f2fb86":"code","9197792c":"code","230b3338":"code","b49f6f36":"code","b4998fe2":"code","6b273ec7":"code","b7c2acd1":"code","de29da9d":"code","46b0e667":"code","aa8bb3b7":"code","542da3f7":"code","d16e3dba":"code","e3c1cfe7":"code","959f48bd":"code","86de6bdd":"code","687578b8":"code","5935e5d5":"code","73971db1":"code","214363bc":"code","4f0ee9a5":"code","6dbaf685":"code","08f87871":"code","b37ef671":"code","1f41e1f9":"code","34a42952":"code","aef98cb2":"code","75ecb02d":"code","1fff96a7":"code","3ec43321":"code","7ddf2695":"code","24e526d4":"code","af47812f":"code","05ecbe3f":"code","a4a4677c":"code","3a08f620":"code","81f7d663":"code","f1ec738a":"code","04f02279":"code","765b128a":"code","3491734f":"code","5f490b71":"code","5e6873bd":"code","e1f4949c":"code","7b5be350":"code","afb9c113":"code","d6418cb5":"code","98ffb34c":"code","64c40d4c":"code","315a68e6":"code","f7f904d2":"code","ff2dac72":"code","86e3f920":"code","7a359418":"code","f943628b":"code","9bbcd3d7":"code","0c8b9ba7":"code","a0c571c0":"code","ff9f4088":"code","5255ea69":"code","9acdb252":"code","010a1546":"code","990a9b93":"code","c2ac5447":"code","329cde1e":"code","f3bbaafe":"markdown","cdbaa1a3":"markdown","1e32cb7a":"markdown","a3f7ed9b":"markdown","5edbe447":"markdown","d978b1f7":"markdown","a246d3d6":"markdown","186f4a70":"markdown","776fc776":"markdown","b86627ce":"markdown","a3ec02f0":"markdown","22fcffa3":"markdown","ed492003":"markdown","d07919b7":"markdown","79af848d":"markdown","10794efc":"markdown","0a38d545":"markdown","345cc6f7":"markdown","df853a21":"markdown","b0fd5f91":"markdown","b559757f":"markdown","0efb757e":"markdown","ee61b07b":"markdown","85796d02":"markdown","5f47a70a":"markdown","da4c019c":"markdown","61e1b4ab":"markdown","4a5415c5":"markdown","bea26785":"markdown","f2423d7a":"markdown","6602f155":"markdown","84f3a35b":"markdown","8ac3ffa1":"markdown","b5cfc7b3":"markdown","2919065b":"markdown","97624504":"markdown","4388bf92":"markdown","39e21654":"markdown","68630d66":"markdown","1db2ac8b":"markdown","fc67482a":"markdown","39088495":"markdown","1036c7c3":"markdown","ee7c86aa":"markdown","6dca3fab":"markdown","e5f37f9a":"markdown","83d05003":"markdown","90af6e61":"markdown","28cd2377":"markdown","fb9e25b9":"markdown","103bf531":"markdown","46453d9f":"markdown","c4e14e4f":"markdown","7805d870":"markdown","9ff9ea3a":"markdown","74085642":"markdown","fb023eab":"markdown","45fcf15c":"markdown","c14e6a03":"markdown","2cdb5c5d":"markdown","0a4cae54":"markdown","43a77183":"markdown","cee6ac43":"markdown","811118c0":"markdown","d7677c15":"markdown","52d406f1":"markdown","840874aa":"markdown","4e997802":"markdown","c032628c":"markdown","3fe566a3":"markdown","a1724a8a":"markdown","3c21cc0b":"markdown","f303dc0e":"markdown","6a617914":"markdown","63a62edc":"markdown","5733c103":"markdown","ab707c4d":"markdown","f7a6f916":"markdown","66624705":"markdown","67fcb2e4":"markdown","18c1efed":"markdown","926670fe":"markdown","45e337c5":"markdown","4c29bb0c":"markdown","0106c6f5":"markdown","7514c147":"markdown","dc8fb70b":"markdown","6f225adb":"markdown","ff9aeab7":"markdown"},"source":{"9df293c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","384455c9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\n\nfrom scipy import stats\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n# To model the Gaussian Navie Bayes classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score # Performance measure \u2013 Accuracy\n\nfrom sklearn import preprocessing","a67086e7":"df  = pd.read_csv('\/kaggle\/input\/bank-personal-loan-modellingthera-bank\/Bank_Personal_Loan_Modelling.csv')\ndf.head()","0bbd1eeb":"personal_loan = df['Personal Loan']\ndf.drop(['Personal Loan'], axis=1, inplace = True)\ndf['Personal Loan'] = personal_loan\ndf.head(5)","a4b199fb":"rows_count, columns_count = df.shape\nprint('Total Number of rows :', rows_count)\nprint('Total Number of columns :', columns_count)","723ae94d":"df.dtypes","12b4ccf9":"df.info()","9a787e17":"df.isnull().sum() ","cd954ff2":"df.isnull().values.any()","713ee7a6":"sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')","06f2fb86":"df.nunique()","9197792c":"df.describe().T","230b3338":"### Five point summary of  attributes and label :-","b49f6f36":"df_transpose = df.describe().T\ndf_transpose[['min', '25%', '50%', '75%', 'max']]","b4998fe2":"sns.pairplot(df.iloc[:,1:]) ","6b273ec7":"# Checking the negative values\ndf[df['Experience'] < 0]['Experience'].value_counts()","b7c2acd1":"# Total records of negative experience\ndf[df['Experience'] < 0]['Experience'].count()","de29da9d":"quantitiveVar = ['Age', 'Income', 'Income', 'CCAvg', 'Mortgage']\nexpGrid = sns.PairGrid(df, y_vars = 'Experience', x_vars = quantitiveVar)\nexpGrid.map(sns.regplot)","46b0e667":"df_Possitive_Experience = df[df['Experience'] > 0]\ndf_Negative_Experience =  df[df['Experience'] < 0]\ndf_Negative_Experience_List = df_Negative_Experience['ID'].tolist()\n\nfor id in df_Negative_Experience_List:\n    age_values = df.loc[np.where(df['ID']==id)][\"Age\"].tolist()[0]\n    education_values = df.loc[np.where(df['ID']==id)][\"Education\"].tolist()[0]\n    possitive_Experience_Filtered = df_Possitive_Experience[(df_Possitive_Experience['Age'] == age_values) & (df_Possitive_Experience['Education'] == education_values)]\n    if possitive_Experience_Filtered.empty :\n        negative_Experience_Filtered = df_Negative_Experience[(df_Negative_Experience['Age'] == age_values) & (df_Negative_Experience['Education'] == education_values)]\n        exp = round(negative_Experience_Filtered['Experience'].median())\n    else:\n        exp = round(possitive_Experience_Filtered['Experience'].median())\n    df.loc[df.loc[np.where(df['ID']==id)].index, 'Experience'] = abs(exp)","aa8bb3b7":"# Total records of negative experience\ndf[df['Experience'] < 0]['Experience'].count()","542da3f7":"df.Experience.describe()","d16e3dba":"sns.distplot(df['ID'])","e3c1cfe7":"sns.distplot(df['Age'])","959f48bd":"sns.distplot(df['Experience'])","86de6bdd":"sns.distplot(df['Income'])","687578b8":"sns.distplot(df['ZIP Code'])","5935e5d5":"sns.distplot(df['CCAvg'])","73971db1":"sns.distplot(df['Education'])","214363bc":"sns.distplot(df['Mortgage'])","4f0ee9a5":"sns.distplot(df['Online'])","6dbaf685":"sns.distplot(df['CreditCard'])","08f87871":"loan_counts = pd.DataFrame(df[\"Personal Loan\"].value_counts()).reset_index()\nloan_counts.columns =[\"Labels\",\"Personal Loan\"]\nloan_counts","b37ef671":"fig1, ax1 = plt.subplots()\nexplode = (0, 0.15)\nax1.pie(loan_counts[\"Personal Loan\"], explode=explode, labels=loan_counts[\"Labels\"], autopct='%1.1f%%',\n        shadow=True, startangle=70)\nax1.axis('equal')  \nplt.title(\"Personal Loan Percentage\")\nplt.show()","1f41e1f9":"sns.catplot(x='Family', y='Income', hue='Personal Loan', data = df, kind='swarm')","34a42952":"sns.boxplot(x='Education', y='Income', hue='Personal Loan', data = df)","aef98cb2":"sns.boxplot(x=\"Education\", y='Mortgage', hue=\"Personal Loan\", data=df)","75ecb02d":"sns.countplot(x=\"Securities Account\", data=df,hue=\"Personal Loan\")","1fff96a7":"sns.countplot(x='Family',data=df,hue='Personal Loan')","3ec43321":"sns.countplot(x='CD Account',data=df,hue='Personal Loan')","7ddf2695":"sns.boxplot(x=\"CreditCard\", y='CCAvg', hue=\"Personal Loan\", data=df)\n","24e526d4":"sns.catplot(x='Age', y='Experience', hue='Personal Loan', data = df, height=8.27, aspect=11\/5)","af47812f":"plt.figure(figsize=(10,4))\nsns.distplot(df[df[\"Personal Loan\"] == 0]['CCAvg'], color = 'r',label='Personal Loan=0')\nsns.distplot(df[df[\"Personal Loan\"] == 1]['CCAvg'], color = 'b',label='Personal Loan=1')\nplt.legend()\nplt.title(\"CCAvg Distribution\")","05ecbe3f":"print('Credit card spending of Non-Loan customers: ',df[df['Personal Loan'] == 0]['CCAvg'].median()*1000)\nprint('Credit card spending of Loan customers    : ', df[df['Personal Loan'] == 1]['CCAvg'].median()*1000)","a4a4677c":"plt.figure(figsize=(10,4))\nsns.distplot(df[df[\"Personal Loan\"] == 0]['Income'], color = 'r',label='Personal Loan=0')\nsns.distplot(df[df[\"Personal Loan\"] == 1]['Income'], color = 'b',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Income Distribution\")","3a08f620":"df.boxplot(return_type='axes', figsize=(20,5))","81f7d663":"plt.figure(figsize = (15,7))\nplt.title('Correlation of Attributes', y=1.05, size=19)\nsns.heatmap(df.corr(), cmap='plasma',annot=True, fmt='.2f')","f1ec738a":"df.head(1)","04f02279":"df = df.drop(['ID','ZIP Code'], axis=1)","765b128a":"df.head(1)","3491734f":"loan_with_experience = df\nloan_without_experience = df.drop(['Experience'], axis=1)","5f490b71":"print('Columns With Experience : ', loan_with_experience.columns)\nprint('Columns Without Experience : ', loan_without_experience.columns)","5e6873bd":"# From Exprenece Dataframe:\nX_Expr = loan_with_experience.drop('Personal Loan', axis=1)\nY_Expr = loan_with_experience[['Personal Loan']]","e1f4949c":"# From Exprenece Dataframe:\nX_Without_Expr = loan_without_experience.drop('Personal Loan', axis=1)\nY_Without_Expr = loan_without_experience[['Personal Loan']]","7b5be350":"# From Experience Dataframe:\nX_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test = train_test_split(X_Expr, Y_Expr, test_size=0.30, random_state=1)\nprint('x train data {}'.format(X_Expr_train.shape))\nprint('y train data {}'.format(y_Expr_train.shape))\nprint('x test data  {}'.format(X_Expr_test.shape))\nprint('y test data  {}'.format(y_Expr_test.shape))","afb9c113":"# From Without Experience Dataframe:\nX_train, X_test, y_train, y_test = train_test_split(X_Without_Expr, Y_Without_Expr, test_size=0.30, random_state=1)\nprint('x train data {}'.format(X_train.shape))\nprint('y train data {}'.format(y_train.shape))\nprint('x test data  {}'.format(X_test.shape))\nprint('y test data  {}'.format(y_test.shape))","d6418cb5":"#X_Exp_train, X_Exp_test, y_Exp_train, y_Exp_test\nlogreg_expr_model = LogisticRegression()\nlogreg_expr_model.fit(X_Expr_train, y_Expr_train)\nprint(logreg_expr_model , '\\n')\n\n# Predicting for test set\nlogreg_expr_y_predicted = logreg_expr_model.predict(X_Expr_test)\nlogreg_expr_score = logreg_expr_model.score(X_Expr_test, y_Expr_test)\nlogreg_expr_accuracy = accuracy_score(y_Expr_test, logreg_expr_y_predicted)\n\nlogestic_confusion_matrix_expr = metrics.confusion_matrix(y_Expr_test, logreg_expr_y_predicted)","98ffb34c":"#X_train, X_test, y_train, y_test\nlogreg_model = LogisticRegression()\nlogreg_model.fit(X_train, y_train)\n\n# Predicting for test set\nlogreg_y_predicted = logreg_model.predict(X_test)\nlogreg_score = logreg_model.score(X_test, y_test)\nlogreg_accuracy = accuracy_score(y_test, logreg_y_predicted)\nlogestic_confusion_matrix = metrics.confusion_matrix(y_test, logreg_y_predicted)","64c40d4c":"# Accuracy\nprint('Logistic Regression Model Accuracy Score W\/O Experience  : %f'  % logreg_accuracy)\nprint('Logistic Regression Model Accuracy Score With Experience : %f'  % logreg_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nLogistic Regression Confusion Matrix W\/O Experience: \\n', logestic_confusion_matrix)\nprint('\\nTrue Possitive    = ', logestic_confusion_matrix[1][1])\nprint('True Negative     = ',   logestic_confusion_matrix[0][0])\nprint('False Possive     = ',   logestic_confusion_matrix[0][1])\nprint('False Negative    = ',   logestic_confusion_matrix[1][0])\nprint('\\nLogistic Regression Confusion Matrix With Experience: \\n', logestic_confusion_matrix_expr)\nprint('\\nTrue Possitive    = ', logestic_confusion_matrix_expr[1][1])\nprint('True Negative     = ',   logestic_confusion_matrix_expr[0][0])\nprint('False Possive     = ',   logestic_confusion_matrix_expr[0][1])\nprint('False Negative    = ',   logestic_confusion_matrix_expr[1][0])\n","315a68e6":"#X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test\nX_train_scaled = preprocessing.scale(X_Expr_train)\nX_test_scaled = preprocessing.scale(X_Expr_test)","f7f904d2":"scaled_logreg_model = LogisticRegression()\nscaled_logreg_model.fit(X_train_scaled, y_Expr_train)\n\n# Predicting for test set\nscaled_logreg_y_predicted = scaled_logreg_model.predict(X_test_scaled)\nscaled_logreg_model_score = scaled_logreg_model.score(X_test_scaled, y_Expr_test)\nscaled_logreg_accuracy = accuracy_score(y_Expr_test, scaled_logreg_y_predicted)\n\nscaled_logreg_confusion_matrix = metrics.confusion_matrix(y_Expr_test, scaled_logreg_y_predicted)\n","ff2dac72":"\nprint('----------------------Final Analysis of Logistic Regression----------------------------\\n')\nprint('After Scalling Logistic Regression Model Accuracy Score with Experience: %f'  % scaled_logreg_accuracy)\nprint('\\nAfter Scalling Logistic Regression Confusion Matrix With Experience: \\n', scaled_logreg_confusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_logreg_confusion_matrix[1][1])\nprint('True Negative     = ',   scaled_logreg_confusion_matrix[0][0])\nprint('False Possive     = ',   scaled_logreg_confusion_matrix[0][1])\nprint('False Negative    = ',   scaled_logreg_confusion_matrix[1][0])\nprint('\\nK-NN classification Report : \\n',metrics.classification_report(y_Expr_test, scaled_logreg_y_predicted))\nconf_table = scaled_logreg_confusion_matrix\na = (conf_table[0,0] + conf_table[1,1]) \/ (conf_table[0,0] + conf_table[0,1] + conf_table[1,0] + conf_table[1,1])\np = conf_table[1,1] \/ (conf_table[1,1] + conf_table[0,1])\nr = conf_table[1,1] \/ (conf_table[1,1] + conf_table[1,0])\nf = (2 * p * r) \/ (p + r)\nprint(\"Accuracy of accepting Loan  : \",round(a,2))\nprint(\"precision of accepting Loan : \",round(p,2))\nprint(\"recall of accepting Loan    : \",round(r,2))\nprint(\"F1 score of accepting Loan  : \",round(f,2))","86e3f920":"#Creating number list from range 1 to 20 of K for KNN\n\nnumberList = list(range(1,20))\nneighbors = list(filter(lambda x: x % 2 != 0 , numberList)) #subsetting just the odd ones\n\n#Declearing a empty list that will hold the accuracy scores\nac_scores = []\n#performing accuracy metrics for value from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    #predict the response\n    knn.fit(X_train, y_train.values.ravel())               \n    y_pred = knn.predict(X_test)\n    #evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    #insert scores to the list\n    ac_scores.append(scores)                \n\nMSE = [1 - x for x in ac_scores] # changing to misclassification error\n\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\n\nprint('Odd Neighbors : \\n', neighbors)\nprint('\\nAccuracy Score : \\n', ac_scores)\nprint('\\nMisclassification error :\\n', MSE)\nprint(\"\\nThe optimal number of neighbor is k=\",optimal_k)\n\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","7a359418":"# instantiating learning model (optimal_k = 3)\nknn_model = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nknn_model.fit(X_train, y_train)\nknn_y_predicted = knn_model.predict(X_test)\nknn_score = knn_model.score(X_test, y_test)\nknn_accuracy = accuracy_score(y_test, knn_y_predicted)\nknn_confusion_matrix = metrics.confusion_matrix(y_test, knn_y_predicted)","f943628b":"# instantiating learning model (optimal_k = 3)\nknn_model_expr = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nknn_model_expr.fit(X_Expr_train, y_Expr_train)\nknn_expr_y_predicted = knn_model_expr.predict(X_Expr_test)\nknn_expr_score = knn_model_expr.score(X_Expr_test, y_Expr_test)\nknn_expr_accuracy = accuracy_score(y_Expr_test, knn_expr_y_predicted)\nknn_confusion_matrix_expr = metrics.confusion_matrix(y_Expr_test, knn_expr_y_predicted)","9bbcd3d7":"# Comparison \nprint('K-NN Model Accuracy Score W\/O Experience  : %f'  % knn_accuracy)\nprint('K-NN Model Accuracy Score With Experience : %f'  % knn_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nK-NN Confusion Matrix W\/O Experience: \\n', knn_confusion_matrix)\nprint('\\nTrue Possitive    = ', knn_confusion_matrix[1][1])\nprint('True Negative     = ',   knn_confusion_matrix[0][0])\nprint('False Possive     = ',   knn_confusion_matrix[0][1])\nprint('False Negative    = ',   knn_confusion_matrix[1][0])\nprint('\\nK-NN Confusion Matrix With Experience: \\n', knn_confusion_matrix_expr)\nprint('\\nTrue Possitive    = ', knn_confusion_matrix_expr[1][1])\nprint('True Negative     = ',   knn_confusion_matrix_expr[0][0])\nprint('False Possive     = ',   knn_confusion_matrix_expr[0][1])\nprint('False Negative    = ',   knn_confusion_matrix_expr[1][0])\n","0c8b9ba7":"#X_train, X_test, y_train, y_test\nX_train_scaled = preprocessing.scale(X_train)\nX_test_scaled = preprocessing.scale(X_test)","a0c571c0":"scaled_knn_model = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nscaled_knn_model.fit(X_train_scaled, y_train)\nscaled_knn_y_predict = scaled_knn_model.predict(X_test_scaled)\nscaled_knn_score = scaled_knn_model.score(X_test_scaled, y_test)\nscaled_knn_accuracy = accuracy_score(y_test, scaled_knn_y_predict)\nscaled_knn_confusion_matrix = metrics.confusion_matrix(y_test, scaled_knn_y_predict)","ff9f4088":"\nprint('----------------------Final Analysis of K-NN----------------------------\\n')\nprint('After Scalling K-NN Model Accuracy Score without Experience: %f'  % scaled_knn_accuracy)\nprint('\\nAfter Scalling K-NN Confusion Matrix Without Experience: \\n', scaled_knn_confusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_knn_confusion_matrix[1][1])\nprint('True Negative     = ',   scaled_knn_confusion_matrix[0][0])\nprint('False Possive     = ',   scaled_knn_confusion_matrix[0][1])\nprint('False Negative    = ',   scaled_knn_confusion_matrix[1][0])\nprint('\\nK-NN classification Report : \\n',metrics.classification_report(y_test, scaled_knn_y_predict))\nknn_conf_table = scaled_knn_confusion_matrix\na = (knn_conf_table[0,0] + knn_conf_table[1,1]) \/ (knn_conf_table[0,0] + knn_conf_table[0,1] + knn_conf_table[1,0] + knn_conf_table[1,1])\np = knn_conf_table[1,1] \/ (knn_conf_table[1,1] + knn_conf_table[0,1])\nr = knn_conf_table[1,1] \/ (knn_conf_table[1,1] + knn_conf_table[1,0])\nf = (2 * p * r) \/ (p + r)\nprint(\"\\nAccuracy of accepting Loan  : \",round(a,2))\nprint(\"precision of accepting Loan : \",round(p,2))\nprint(\"recall of accepting Loan    : \",round(r,2))\nprint(\"F1 score of accepting Loan  : \",round(f,2))","5255ea69":"gnb_model = GaussianNB()\ngnb_model.fit(X_train, y_train)\ngnb_y_predicted = gnb_model.predict(X_test)\ngnb_score = gnb_model.score(X_test, y_test)\ngnb_accuracy = accuracy_score(y_test, gnb_y_predicted)\ngnb_confusion_matrix = metrics.confusion_matrix(y_test, gnb_y_predicted)","9acdb252":"gnb_expr_model = GaussianNB()\ngnb_expr_model.fit(X_Expr_train, y_Expr_train)\ngnb_expr_y_predicted = gnb_expr_model.predict(X_Expr_test)\ngnb_expr_score = gnb_expr_model.score(X_Expr_test, y_Expr_test)\ngnb_expr_accuracy = accuracy_score(y_Expr_test, gnb_expr_y_predicted)\ngnb_expr_confusion_matrix = metrics.confusion_matrix(y_Expr_test, gnb_expr_y_predicted)","010a1546":"# Comparison \nprint('Na\u00efve Bayes Model Accuracy Score W\/O Experience  : %f'  % gnb_accuracy)\nprint('Na\u00efve Bayes Model Accuracy Score With Experience : %f'  % gnb_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nNa\u00efve Bayes Confusion Matrix W\/O Experience: \\n', gnb_confusion_matrix)\nprint('\\nTrue Possitive    = ', gnb_confusion_matrix[1][1])\nprint('True Negative     = ',   gnb_confusion_matrix[0][0])\nprint('False Possive     = ',   gnb_confusion_matrix[0][1])\nprint('False Negative    = ',   gnb_confusion_matrix[1][0])\nprint('\\nNa\u00efve Bayes Confusion Matrix With Experience: \\n', gnb_expr_confusion_matrix)\nprint('\\nTrue Possitive    = ', gnb_expr_confusion_matrix[1][1])\nprint('True Negative     = ',   gnb_expr_confusion_matrix[0][0])\nprint('False Possive     = ',   gnb_expr_confusion_matrix[0][1])\nprint('False Negative    = ',   gnb_expr_confusion_matrix[1][0])\n\n","990a9b93":"scaled_gnb_model = GaussianNB()\nscaled_gnb_model.fit(X_train_scaled, y_train)\nscaled_gnb_y_predict = scaled_gnb_model.predict(X_test_scaled)\nscaled_gnb_score = scaled_gnb_model.score(X_test_scaled, y_test)\nscaled_gnb_accuracy = accuracy_score(y_test, scaled_gnb_y_predict)\nscaled_gnb_connfusion_matrix = metrics.confusion_matrix(y_test, scaled_gnb_y_predict)","c2ac5447":"\nprint('----------------------Final Analysis of Na\u00efve Bayes----------------------------\\n')\nprint('After Scalling Na\u00efve Bayes Model Accuracy Score: %f'  % scaled_gnb_accuracy)\nprint('\\nAfter Scalling Na\u00efve Bayes Confusion Matrix: \\n', scaled_gnb_connfusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_gnb_connfusion_matrix[1][1])\nprint('True Negative     = ',   scaled_gnb_connfusion_matrix[0][0])\nprint('False Possive     = ',   scaled_gnb_connfusion_matrix[0][1])\nprint('False Negative    = ',   scaled_gnb_connfusion_matrix[1][0])\nprint('\\n Gaussian Naive Bayes classification Report : \\n',metrics.classification_report(y_test, gnb_y_predicted))\ngnb_conf_table = scaled_gnb_connfusion_matrix\na = (gnb_conf_table[0,0] + gnb_conf_table[1,1]) \/ (gnb_conf_table[0,0] + gnb_conf_table[0,1] + gnb_conf_table[1,0] + knn_conf_table[1,1])\np = gnb_conf_table[1,1] \/ (gnb_conf_table[1,1] + gnb_conf_table[0,1])\nr = gnb_conf_table[1,1] \/ (gnb_conf_table[1,1] + gnb_conf_table[1,0])\nf = (2 * p * r) \/ (p + r)\nprint(\"\\nAccuracy of accepting Loan   : \",round(a,2))\nprint(\"precision of accepting Loan  : \",round(p,2))\nprint(\"recall of accepting Loan     : \",round(r,2))\nprint(\"F1 score of accepting Loan   : \",round(f,2))","329cde1e":"print('Overall Model Accuracy After scaling:\\n')\nprint ('Logistic Regression : {0:.0f}%'. format(scaled_logreg_accuracy * 100))\nprint ('K-Nearest Neighbors : {0:.0f}%'. format(scaled_knn_accuracy * 100))\nprint ('Naive Bayes         : {0:.0f}%'. format(scaled_gnb_accuracy * 100))\n\nprint('\\nOverall Model Confusion matrix After scaling:\\n')\nprint('\\nLogistic Regression: \\n', scaled_logreg_confusion_matrix)\nprint('\\n     True Possitive    = ', scaled_logreg_confusion_matrix[1][1])\nprint('     True Negative     = ',   scaled_logreg_confusion_matrix[0][0])\nprint('     False Possive     = ',   scaled_logreg_confusion_matrix[0][1])\nprint('     False Negative    = ',   scaled_logreg_confusion_matrix[1][0])\n\nprint('\\nK-Nearest Neighbors: \\n', scaled_knn_confusion_matrix)\nprint('\\n    True Possitive    = ', scaled_knn_confusion_matrix[1][1])\nprint('    True Negative     = ',   scaled_knn_confusion_matrix[0][0])\nprint('    False Possive     = ',   scaled_knn_confusion_matrix[0][1])\nprint('    False Negative    = ',   scaled_knn_confusion_matrix[1][0])\n\nprint('\\nNaive Bayes: \\n', scaled_gnb_connfusion_matrix)\nprint('\\n    True Possitive    = ', scaled_gnb_connfusion_matrix[1][1])\nprint('    True Negative     = ',   scaled_gnb_connfusion_matrix[0][0])\nprint('    False Possive     = ',   scaled_gnb_connfusion_matrix[0][1])\nprint('    False Negative    = ',   scaled_gnb_connfusion_matrix[1][0])\n\n\nprint('\\n\\nReceiver Operating Characteristic (ROC) curve to evalute the classifier output quality.  If area of curve is closer to 1 which means better the model and if area of curve is closer to 0 which means poor the model.')\n\nknn_fpr, knn_tpr, knn_threshold = metrics.roc_curve(y_test, scaled_knn_y_predict)\nknn_roc_auc = metrics.roc_auc_score(y_test, scaled_knn_y_predict)\nfig1_graph = plt.figure(figsize=(15,4))\nfig1_graph.add_subplot(1,3,1)\nplt.plot(knn_fpr, knn_tpr, label='KNN Model (area = %0.2f)' % knn_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\n\nlogistic_fpr, logistic_tpr, logistic_threshold = metrics.roc_curve(y_Expr_test, scaled_logreg_y_predicted)\nlogistic_roc_auc = metrics.roc_auc_score(y_Expr_test, scaled_logreg_y_predicted)\nfig1_graph.add_subplot(1,3,2)\nplt.plot(logistic_fpr, logistic_tpr, label='Logistic Model (area = %0.2f)' % logistic_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\nnb_fpr, nb_tpr, nb_threshold = metrics.roc_curve(y_test, scaled_gnb_y_predict)\nnb_roc_auc = metrics.roc_auc_score(y_test, scaled_gnb_y_predict)\nfig1_graph.add_subplot(1,3,3)\nplt.plot(nb_fpr, nb_tpr, label='Naive-Bayes Model (area = %0.2f)' % nb_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n","f3bbaafe":"### Creating two dataframes with 'Experience' and without 'Experience' repectively :","cdbaa1a3":"<b>Observation :<\/b> Family size does not have any impact in personal loan. But it seems families with size of 3 and 4 are more likely to take loan.","1e32cb7a":"<b>Observation: <\/b> \n- From the above accuracy results we see that accuracy is higher at without 'Experience' (90.93%) than with 'Experience' (90.20%). \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is better at without 'Experience'.\n- <b>Type 1 (False Possitive)<\/b> and <b>Type 2(False Negative)<\/b> errors is less at without Experience.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We will not consider dafaframe 'With Experience' for further iteration.","a3f7ed9b":"<b>observation: <\/b> From above we can say that customers with undergraduate level of education and family greater than 3 are good customers who took loan. Customer who took loan have same income range irrespective of education level. Education of Graduate and above have more chance to take loan.","5edbe447":"## Spliting the data into training and test set in the ratio of 70:30 respectively :-","d978b1f7":"#### Influence of important features on Personal Loan. From the above pair plot we can see some features which has relationship with the target column i.e personal loan :-","a246d3d6":"<b>Comment:<\/b> Out of 5000 data points, 4520 are labeled as 0 and 480 as 1. Percentage of customers who took loan is significantlly greater than customers who did not take loan. I have also show the percentage using in pie chart below.\n","186f4a70":"<b>Observation:<\/b>  Experience is also normaly distributed.","776fc776":" ### Error Fixing(Data Cleaning) :-","b86627ce":"## ::--------------------------- Exploratory Data Analysis ---------------------------- ::","a3ec02f0":"<b>Observation:<\/b> The above graph is unform distribution. ","22fcffa3":"<b>Observation:<\/b> This is a Bernoulli Distrubution. Number of customers who have Online accout is geater than the number of customers who do not have online account","ed492003":"###  Pair plot that includes all the columns of the data frame :-\n\n<b>Note: <\/b> I am not using ID column in tha pair plot as it is not relevent with our analysis. Id column is jut for record index.","d07919b7":"<b>Observation: <\/b> \n- From the above accuracy results we see that accuracy is alomost at without 'Experience' and with 'Experience'. \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is almost same at without 'Experience'.\n- <b>Type I (False Possitive)<\/b> and <b>Type II(False Negative)<\/b> errors is same.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We can consider any dafaframe 'With Experience' or 'Without Experience' for further iteration.","79af848d":"#### Below Code is to find the best K(Neighbors)\n","10794efc":"<b>Comment:<\/b> From the above boxplot we can see there are outliers on few colomns. Mortgage has more number of outlier. Income, CCAvg have also outliers. We will try to fix the outlier by scaling the attributes.","0a38d545":"<b>Comparison :<\/b> Below is the comparison b\/w K-NN Model Accuracy and Confussion Matrix with 'Experience' and W\/O 'Experience'.","345cc6f7":"#### Seperating Target Variable from Independent Variables from Expr and Wihtout Expr dataframe :\n","df853a21":"<b>Observation: <\/b> Customers who have credit card and monthly spending is higher are more likly to take loan.","b0fd5f91":"###  Data distribution in each attribute :-","b559757f":"<b>Observation:<\/b> Undergrad level customers are more than the Graduate and Advanced\/Professional customers.\n ","0efb757e":"# =========== COMPARISON OF ABOVE THREE MODELS ==========","ee61b07b":"#### Dropping 'ID' and 'ZIP Code' :","85796d02":"<b>Comment: <\/b> Now we can see the min is 0.0 which was -3.0 before error fixing.   ","5f47a70a":"<b>Data Description:<\/b>\n    The file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan\ncampaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted\nthe personal loan that was offered to them in the earlier campaign.\n\n<b>Domain:<\/b>\n    Banking\n\n<b>Context:<\/b>\n    This case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as\ndepositors). A campaign that the bank ran last year for liability customers showed a\nhealthy conversion rate of over 9% success. This has encouraged the retail marketing\ndepartment to devise campaigns with better target marketing to increase the success\nratio with minimal budget.\n\n<b>Learning Outcomes:<\/b>\n\n   -  Exploratory Data Analysis\n   -  Data Cleaning\n   -  Data Visualization\n   -  Preparing the data to train a model \n   -  Training and making predictions using a classification model\n   -  Model evaluation\n    \n<b>Objective:<\/b>\n    The classification goal is to predict the likelihood of a liability customer buying personal loans which means we have to build a model which will be used to predict which customer will most likely to accept the offer for personal loan, based on the specific relationship with the bank across various features given in the dataset. Here I will be using the Supervised Learning methods to predict which model is best for this problem amongst Logistic Regresssion, K-Nearest Neighbors(KNN) and Naive Bayes Algorigthm.\n","da4c019c":"<b>Observation :<\/b> Customers who has securies account are more likly to take loan. Majority of customers who does not have loan do not have securities account. ","61e1b4ab":"<b> Comment: <\/b> Now we can see that negative values count is 0 means there is no negative value anymore in the dataframe.\n- Describing the 'Experience' column to check the count, mean, standard deviation and five point summary.","4a5415c5":"<b>Observation: <\/b> Customers who does not have CD account , does not have loan as well. This seems to be majority. But almost all customers who has CD account has loan as well","bea26785":"### Transposing index and columns:-","f2423d7a":"<b>Observation:<\/b>The above distributionis is right skewed distribution because the tail goes to the right. \nMost of the customers do not have mortgage. There are more customers whose mortgage amount is between $80000 to $150000 . Very few customers whos mortgage amount is more than $600000.","6602f155":"<b>Observation: <\/b> Customers who have taken personal loan have higher credit card average than those who did not take loan. So high credit card average seems to be good predictor of whether or not a customer will take a personal loan.","84f3a35b":"<b>Observation: <\/b> \n- From the above accuracy results we see that accuracy is higher with 'Experience' (94.60 %) than without 'Experience' (94.26%). \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is better with 'Experience'.\n- <b>Type 1 (False Possitive)<\/b> and <b>Type 2(False Negative)<\/b> errors is less with experience.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We will not consider dafaframe without 'Experience' for further iteration.","8ac3ffa1":"### Outliers Detection :-","b5cfc7b3":"<b>Comment: <\/b> From the above heatmap graph we can see that there is no missing value in dataset.","2919065b":"## ---------------------------------------END----------------------------------------------","97624504":"<b>Comment:<\/b> The graph show persons who have personal loan have a higher credit card average. Average credit card spending with a median of 3800 dollar indicates a higher probability of personal loan. Lower credit card spending with a median of 1400 dollars is less likely to take a loan.","4388bf92":"<b>Observation:<\/b> Customers who have family size 3 or greater with higher income between 100k to 200k are more likely to take loan.","39e21654":"<b>Decission: <\/b> \n- From above result we can see that the misclassification error is minimum at <b>k = 3<\/b>. \n- Hence I am considering 3 is the optimal k. When k=3 the model accuracy is 0.909.\n- From above graph of misclassification error vs k (with k value on X-axis) we can also see that error is very low when K is 3.","68630d66":"### Checking the unique data :-","1db2ac8b":"<b>Comment:<\/b> Here total missing values count from each column is 0 and we can see there is no missing value in the dataframe.","fc67482a":"<b>Comment:<\/b> Now the traget column is appended at the end of the dataframe.","39088495":"<b>Observation:<\/b> It is a Bernoulli Distrubution. Number of customers without Credit Card is almost double than the number of customers with Credit Card.","1036c7c3":"<b>Comparison :<\/b> Below is the comparison b\/w Logistic Regression Model Accuracy and Confussion Matrix with 'Experience' and W\/O 'Experience'.","ee7c86aa":"<b>Comment:<\/b> Shape of the dataframe is (5000, 14).\nThere are 5000 rows and 14 columns in the dataset. ","6dca3fab":"#### Missing value Visualization :- ","e5f37f9a":"### Shape of the data :- ","83d05003":"## ::----------------------------------- Data Visualization --------------------------------::","90af6e61":"### Correlation using Heatmap :-","28cd2377":"<b>Comment:<\/b> There are 52 records with negative Experience are present in the dataset.\n\n\n\n#### Checking the association of Experience with other quantitive variables :","fb9e25b9":"### Improvement of  the model -------- Iteration 2 For Logistic Regression with Experience--------","103bf531":"<b> Observation: <\/b>The above distributionis is right skewed distribution because the tail goes to the right.","46453d9f":"<b>Comment :<\/b> Here I have used numpy, pandas, matplotlib, seaborn, scipy for EDA and Data Visualization. Also used sklearn for data spliting, model building and for confusion matrix. ","c4e14e4f":"<b>Observation: <\/b>From the above we clearly see that  Age and Experience have very storng association. As Age increases Experience also increases. We can try building our model by droping Experience column. ","7805d870":"## ::------------------------------ K-NN --------------------------------------------::\n\n### Steps to be followed:-\n- We will follow the same procedures as we have followed in Logistic Regression.\n- We will try the model with 'Experience' and without 'Experience'.\n- will run the KNN with number of odd neighbours ranges from 1 to 20 and will find the optimal number of neighbours using the Mis classification error.\n- After finding <b>best K<\/b>, will build the model using 'Experience' and without 'Experience'. ","9ff9ea3a":"- From the above, we can clearly see that k-Nearest Neighbors Alogorithm with scaled data gives us best accuracy of 96%. \n- Also the <b>Type I(False Posssitive)<\/b> and <b>Type II(False Negative)<\/b> errors are least in K-Nearest model. \n- The area in ROC curve for K-NN is 0.82 which is close to 1 which stats that K-NN is the best model in comparesion of Logistic Model and Naive-Bayes Model whose ROC area is 0.79 and 0.73 repectively.\n- Hence among the above three algorithm applied on the underline dataset, K-NN would be the best choice to predict the customers who will accept the personal loan.","74085642":"<b>Observation:<\/b> By looking above plots we can see that 'Age' has very strong and possitive association with 'Experience'. I am also considering 'Education' to fix the negative experience error. Becuase as we know experience relats to the education level. \n\n<b>Decission: <\/b> We can replace each negative 'Experience' value with the median of possitive 'Experience' associated with the particular 'Age' and 'Education' value.\n\n<b>Steps to be followed in the following code: <\/b> \n- Get the record of experience whose value is greater than 0\n- Get the record of experience whose value is lesser than 0\n- Get the list of Cutomer ID whose experience value is negative\n- Get the list of 'Age' values where it finds negative values in 'Experience' column\n- Get the list of 'Education' values where it finds negaitve values in 'Experience' column\n- Next it filters the records matching the above conditions from the dataframe which has data with possive experience and takes the median and store in exp. There could be chance that there will be no possive experience which matches the above condtion. In such case it matches the above condiiton from the dataframe which has the record with negative experience and gets the median and store in exp.\n- Next it will replace the negative experience with the median.\n- After execution of below code we will check the negative experience again.","fb023eab":"<b>Important :<\/b> From the above we can see that the data is having a huge bias(alomot 1:10) towards the category of people not accepting the personal loan. Hence we can build an opinion that our model will tends to perform better towards predicting which customers will not accept the personal loan. However, our goal is to identityfy the customer who can accept the personal loan based on the given features. ","45fcf15c":"<b>Observation:<\/b> \n- <b>Important : <\/b>Here we can see that minumum value of Experience column is <b>-3.0<\/b> which could be a mistake because Experience can not be negative. So I will be fixing it in data cleaning and error fixing part.\n- Binary varibales 'Personal Loan', 'Credit Card', 'Online', 'CD Account', 'Security Account' has clean data.\n- Ordinary Cat variables 'Familty' and 'Education' are also clean.\n- Target variable also looks fine. ","c14e6a03":"<b>Observation :<\/b> From the above box plot we can see that customers whose education level is 1 and did not take loan has higher mortgage than customers who take loan of same education level. Customers whose education level is 2and 3 and did not take loan has lesser mortgage than customers who take loan of same education level. ","2cdb5c5d":"### Model building using  'With Experience' dataframe:-\n- We have already created the dataframe and splited the data logistic regression, will be using the same.\n- Splited data from 'With Experience' dataframe: X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test","0a4cae54":"### Model building using  'Without Experience' dataframe:-\n- Splited data from 'Without Experience' dataframe: X_train, X_test, y_train, y_test","43a77183":"### Model building using  'With Experience' dataframe:-\n- Splited data from 'With Experience' dataframe: X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test","cee6ac43":"### Import the necessary libraries :","811118c0":"### Improvement of  the model -------- Iteration 2 For K-NN without Experience dataset--------","d7677c15":"###  Target column distribution and Data Visualization with Personal Loan Column :-","52d406f1":"<b>comment : <\/b> We can also display the data types of dataframe using df.info() function which gives even more useful info.","840874aa":"<b>Comment:<\/b> This gives if we have any missing values at all. False : No missing value, True: Missing value ","4e997802":"<b>Observation:<\/b> The is uniformaly distrubuted. Data points are more with family size 1 and 2.","c032628c":"###  Data type of each attribute :-","3fe566a3":"#### Read the data as a data frame :- ","a1724a8a":"<b>Observation: <\/b> Customers who have taken personal loan have income than those who did not take. So high income seems to be good predictor of whether or not a customer will take a personal loan.","3c21cc0b":"<b>Comment :<\/b> Here we can see that all the variables are numerical. But the columns 'CD Account', 'Online', 'Family', 'Education' , 'CreditCard' and 'Securities Account' are categorical variable which should be in 'category' type. ","f303dc0e":"<b>Target Column rearrange:- <\/b> As our Target Column(Personal Loan) is in middle of dataframe so for more convinient I have drop the personal loan column from the original place and appended at last of dataframe.","6a617914":"## ::---------------------------- Logistic Regression ---------------------------------------::","63a62edc":"<b> Comment: <\/b> The above distributionis is right skewed distribution because the tail goes to the right. Most of the customers monthly avg. spending on credit cards is between 1k to 2.5K. There are very few customers whose monthly avg. spending on credit card is more than 8k. \n    ","5733c103":"### Improvement of  the model -------- Iteration 2 For Na\u00efve Bayes without Experience dataset--------\n- We have already scaled the attribute in K-NN model building, will be using the same\n- X_train_scaled and X_test_scaled","ab707c4d":"### Model building using  'Without Experience' dataframe:-\n- We have already created the dataframe and splited the data logistic regression, will be using the same.\n- Splited data from 'Without Experience' dataframe: X_train, X_test, y_train, y_test","f7a6f916":"## Dimensionality Reduction : -\n- As we have seen above 'ID' and 'ZIP Code' are not relevent for our model building so we will drop it.\n- 'Age' and 'Experience' are highly correlated so we will build our model <b>with 'Experience'<\/b> and <b>without 'Experience'<\/b> after that we will compare the accurace which will lead us to the conclution that with 'Experience' or without 'Experience' which model is better for prediction. ","66624705":"<b> Features(attributes) Understanding from the above dataframe :- <\/b> \n\n- The ID variable can be ignored as it will not any effect on our model. As we know customer Id is just to maitain the record in serial order. There is no relationship with Id and Loan.\n- Target Variable is <b>Personal Loan<\/b> which describe whether the person has taken loan or not. This is the variable which we need to predict.\n\nNonimal Varibles :\n- ID - Customer ID\n- ZIP Code - Home Address ZIP code of the customer. This variable can also be ignored becasue we can not judge the customers based on thier area or location.\n\nOrdinal Categorical variables :\n- Family - Number of famlily member of the customer\n- Education - Education level of the customer. In our dataset it ranges from 1 to 3 which are Under Graduate, Graduate and Post Graduate respectivly.\n\nInterval Variables :\n- Age        - Age of the customer\n- Experience - Years of experience of customer has\n- Income     - Annula Income of the customer which is in dollars\n- CCAvg      - Avg. spending on credit cards per month which in dollars.\n- Mortgage    - Value of House Mortgage\n\nBinary Categorical Variable :\n- CD Account - Does the customer have CD Account with bank or not?\n- Security Account - Does the customer have Security Account with bank or not?\n- Online  - Does the customer have Online banking facility with bank or not?\n- Credit Card - Does the customer have a credit card issued by Universal Bank or not?\n- Personal Loan - This our target variable which we have to predict. This indicates that the customer has token loan or not?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","67fcb2e4":"### Final Conclusion to Predict the best Model ::-","18c1efed":"<b>Observation:<\/b> Age column is normaly distrubuted.","926670fe":"### Without Experience Column:-","45e337c5":"### Checking the presence of missing values :-","4c29bb0c":"<b>Observation : <\/b> From the above pair plot we can infer the association among the attributes and target column as follows:\n- 'Age' column is normally distributed. Here can see that the mean and midean is almost same which we have also seen in the transpose matrics. Most of the customers age is between 25 to 65 years.\n- 'Experience' is also mormally distributed. Here also mean is amost equal to midean. 'Experience' and 'Age' are stong possitive associasion.\n- 'Income' is positively skewed and it will also have the outlier\n- We dont see any relationship with the ZIP Code and other variables.\n- Family and Education has low association with the 'Personal Loan'. \n- The disribution of CCAvg is also a possotively skewed variable. Majority of the customers average monthly spending is between 1k to 9k.\n- 'Mortgage' is also positively skewed. Majority of the individuals have a mortgage of less than 40K. ","0106c6f5":"<b>Comment:<\/b> Here I have read the Personal Loan  dataset using read_csv() function of pandas. df is a dataframe. I have used head() funtion to display first 5 records of the dataset.","7514c147":"<b>Observation:<\/b> From the above heatmap we can see that :\n- Age and Experience are highly correlated and the correlation is almost 1.\n- 'Income' and 'CCAvg' is moderately correlated.\n- Personal Loan has maximum correlation with 'Income', 'CCAvg', 'CD Account', 'Mortgage', and 'Education'.\n- We can see in above heat map there is association of 'CD Account' with 'Credit Card', 'Securities Account', 'Online', 'CCAvg' and 'Income'.\n- 'Mortgage' has moderate correlation with 'Income' which is about 12%.\n- 'Income' influences 'CCAvg', 'Personal Loan', 'CD Account' and 'Mortgage'.\n","dc8fb70b":"# ::------------------------- Model Building ---------------------------------::","6f225adb":"### With Experience Column:-","ff9aeab7":"## ::-------------------------- Na\u00efve Bayes -------------------------------------::\n\n\n### Steps to be followed:-\n- We will follow the same procedures as we have done in above models.\n- We will try the model with 'Experience' and without 'Experience'."}}