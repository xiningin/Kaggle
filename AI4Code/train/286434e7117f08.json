{"cell_type":{"c0ebe16d":"code","58f84136":"code","6f2ba0da":"code","41d31183":"code","64690b32":"code","4f58e1e6":"code","b9d27240":"code","7355bb36":"code","4b263a53":"code","ad4dcf26":"code","796690e5":"code","c582904a":"code","7ffab67e":"code","c436f479":"code","17b40978":"code","15e4c70d":"code","57324c1c":"code","a939d889":"code","87db3138":"code","ffff1d25":"code","a3da8b7c":"code","06bf5aab":"code","ab3019a5":"code","7eaede1d":"code","b2298452":"code","d666544c":"code","5157859b":"code","efd0d268":"code","7f6cdd79":"code","7df0c587":"code","77613a5b":"code","63695e0c":"code","77c46775":"code","44336329":"markdown","ab6439f6":"markdown"},"source":{"c0ebe16d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58f84136":"df=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf.head()\n#Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.","6f2ba0da":"test=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest.head()","41d31183":"print(df.shape)\nprint(test.shape)","64690b32":"#we split training data into features and target\nX=df.drop(\"label\",axis=1).values\ny=df[\"label\"].values\n#we normalize the data\n","4f58e1e6":"print(X.shape)\nprint(y.shape)\nprint(test.shape)","b9d27240":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,10))\nsns.set_style(\"darkgrid\")\nsns.countplot(x=\"label\",data=df)","7355bb36":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.05, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","4b263a53":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))","ad4dcf26":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","796690e5":"knn=KNeighborsClassifier(n_neighbors=1) # we get the minimum error when n=1","c582904a":"knn.fit(X_train,y_train)\nknn_predictions = knn.predict(X_test)","7ffab67e":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\nprint(confusion_matrix(y_test,knn_predictions))","c436f479":"print(classification_report(y_test, knn_predictions))","17b40978":"print(accuracy_score(y_test, knn_predictions))","15e4c70d":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(X_train,y_train)\nrandom_predictions= random.predict(X_test)","57324c1c":"print(classification_report(y_test, random_predictions))\nprint(confusion_matrix(y_test,random_predictions))\nprint(accuracy_score(y_test, random_predictions)) #Random forest performs better than KNN","a939d889":"from sklearn.naive_bayes import GaussianNB\nbayes=GaussianNB()\nbayes.fit(X_train, y_train)\nbayes_predictions=bayes.predict(X_test)\nprint(classification_report(y_test, bayes_predictions))\nprint(confusion_matrix(y_test, bayes_predictions))\nprint(accuracy_score(y_test, bayes_predictions)) #The predictions are not good","87db3138":"X = X\/255\ntest= test.values\/255","ffff1d25":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n# canal = 1 => For gray scale\nX = X.reshape(-1,28,28,1)\ntest = test.reshape(-1,28,28,1)","a3da8b7c":"# we encode labels to one hot vectors (like [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny = to_categorical(y)\n\nprint(f\"Label size {y.shape}\")","06bf5aab":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.05, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","ab3019a5":"X_visualization = X_train.reshape(X_train.shape[0], 28, 28)\n\nfig, axis = plt.subplots(1, 4, figsize=(20, 10))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_visualization[i], cmap='binary')\n    digit = y_train[i].argmax()\n    ax.set(title = f\"Real Number is {digit}\");\n# we see how our data look like.","7eaede1d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten,Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping,  ReduceLROnPlateau\ncnn=Sequential()\n\n#model.add(Lambda(standardize,input_shape=(28,28,1)))    \ncnn.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1)))\ncnn.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n\ncnn.add(MaxPooling2D(pool_size=(2,2)))\ncnn.add(BatchNormalization())\ncnn.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\ncnn.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n\ncnn.add(MaxPooling2D(pool_size=(2,2)))\ncnn.add(BatchNormalization())    \ncnn.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n    \ncnn.add(MaxPooling2D(pool_size=(2,2)))\n    \ncnn.add(Flatten())\ncnn.add(BatchNormalization())\ncnn.add(Dense(512,activation=\"relu\"))\n    \ncnn.add(Dense(10,activation=\"softmax\"))\n    \ncnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","b2298452":"# With data augmentation to prevent overfitting\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n#datagen.fit(X_train)\ntrain_generator = datagen.flow(X_train, y_train, batch_size=32)\ntest_generator = datagen.flow(X_test, y_test, batch_size=32)","d666544c":"# Fit the model\nhistory = cnn.fit(train_generator, \n                  epochs = 10,\n                  steps_per_epoch = X_train.shape[0] \/\/ 32,\n                  validation_steps = X_test.shape[0] \/\/ 32,\n                  validation_data = test_generator)","5157859b":"sns.set_style(\"darkgrid\")\npd.DataFrame(cnn.history.history).plot(figsize=(15,10))","efd0d268":"y_pred = cnn.predict(X_test)\nX_new = X_test.reshape(X_test.shape[0], 28, 28)\n\nfig, axis = plt.subplots(4, 4, figsize=(12, 14))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_new[i], cmap='binary')\n    ax.set(title = f\"Real Number is {y_test[i].argmax()}\\nPredict Number is {y_pred[i].argmax()}\");","7f6cdd79":"predictions = cnn.predict(test, verbose=2)\npredictions","7df0c587":"new_predictions =np.argmax(predictions, axis=1) # we get the original values instead of one hot coded version\nnew_predictions","77613a5b":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","63695e0c":"submission","77c46775":"submission['Label'] = new_predictions\nsubmission.to_csv(\"my_submission3.csv\", index=False)\nsubmission.head()","44336329":"<font color=\"red\">\nLets try Convolutional Neural Networks:","ab6439f6":",\n,\n                              "}}