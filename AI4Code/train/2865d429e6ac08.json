{"cell_type":{"d4e3a7e0":"code","79129fd2":"code","40653935":"code","72f40091":"code","b3efa6b0":"code","2d59e269":"code","591d1c83":"code","89df62a5":"code","438d39f8":"code","48c616d8":"code","18d60377":"code","295d5d6e":"code","e8bc6fcc":"code","4d29359e":"code","f8507ec6":"code","6dc5a594":"code","41a3cd38":"code","cc747591":"code","31098e25":"code","9e433003":"code","b81f1fb6":"code","ee6e2fc7":"code","50104857":"code","1b864d19":"code","fad49b22":"code","bd663ed4":"code","a2cd23a8":"code","70d4cc57":"code","ad1238d3":"markdown","bc17743c":"markdown","be60890f":"markdown","711198b6":"markdown","6bc3e742":"markdown","fb90fad3":"markdown","abb44e49":"markdown"},"source":{"d4e3a7e0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, KFold","79129fd2":"X, y = load_boston(return_X_y=True)","40653935":"alpha = 0.05","72f40091":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)","b3efa6b0":"def generate_results_dataset(predictions, ci):\n    df = pd.DataFrame()\n    df['prediction'] = preds\n    if ci >= 0:\n        df['upper'] = preds + ci\n        df['lower'] = preds - ci\n    else:\n        df['upper'] = preds - ci\n        df['lower'] = preds + ci\n        \n    return df","2d59e269":"def generate_plot(y_test, preds, ci, positive=True):\n    \n    if ci < 0:\n        ci_pack = np.vstack([preds + ci, preds - ci])\n    else:\n        ci_pack = np.vstack([preds + ci, preds - ci])\n    \n    plt.figure(figsize=(12,9))\n    plt.errorbar([i for i in range(len(preds))], preds, ci_pack, fmt='o', color='black', ecolor='lightgray')\n    plt.plot([i for i in range(len(y_test))], y_test, 'o', c='r')\n    plt.legend(['True Value', 'Prediction', 'Confidence Interval'])\n    plt.show()","591d1c83":"def generate_plot_plus(y_test, preds, bottom, upper,):\n    \n    ci_pack = np.vstack([bottom, upper])\n    \n    plt.figure(figsize=(12,9))\n    plt.errorbar([i for i in range(len(preds))], preds, ci_pack, fmt='o', color='black', ecolor='lightgray')\n    plt.plot([i for i in range(len(y_test))], y_test, 'o', c='r')\n    plt.legend(['True Value', 'Prediction', 'Confidence Interval'])\n    plt.show()","89df62a5":"rf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\nresiduals = y_train - rf.predict(X_train)","438d39f8":"ci = np.quantile(residuals, 1 - alpha)\npreds = rf.predict(X_test)","48c616d8":"df = generate_results_dataset(preds, ci)\ndf.head()","18d60377":"generate_plot(y_test, preds, ci, positive=ci>0)","295d5d6e":"kf = KFold(n_splits=len(y_train)-1, shuffle=True, random_state=42)\nres = []\nfor train_index, test_index in kf.split(X_train):\n    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n    \n    rf.fit(X_train_, y_train_)\n    res.extend(list(y_test_ - rf.predict(X_test_)))","e8bc6fcc":"rf.fit(X_train, y_train)\nci = np.quantile(res, 1 - alpha)","4d29359e":"preds = rf.predict(X_test)\ndf = generate_results_dataset(preds, ci)","f8507ec6":"df.head()","6dc5a594":"generate_plot(y_test, preds, ci, positive=ci>0)","41a3cd38":"kf = KFold(n_splits=len(y_train)-1, shuffle=True, random_state=42)\nres = []\nestimators = []\nfor train_index, test_index in kf.split(X_train):\n    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n    \n    rf.fit(X_train_, y_train_)\n    estimators.append(rf)\n    res.extend(list(y_test_ - rf.predict(X_test_)))","cc747591":"y_pred_multi = np.column_stack([e.predict(X_test) for e in estimators])\ny_pred_multi.shape","31098e25":"ci = np.quantile(res, 1 - alpha)\ntop = []\nbottom = []\n\nfor i in range(y_pred_multi.shape[0]):\n    if ci > 0:\n        top.append(np.quantile(y_pred_multi[i] + ci, 1 - alpha))\n        bottom.append(np.quantile(y_pred_multi[i] - ci, 1 - alpha))\n    else:\n        top.append(np.quantile(y_pred_multi[i] - ci, 1 - alpha))\n        bottom.append(np.quantile(y_pred_multi[i] + ci, 1 - alpha))     ","9e433003":"preds = np.median(y_pred_multi, axis=1)\ndf = pd.DataFrame()\ndf['pred'] = preds\ndf['upper'] = top\ndf['lower'] = bottom","b81f1fb6":"df.head()","ee6e2fc7":"generate_plot_plus(y_test, preds, bottom, top)","50104857":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nres = []\nestimators = []\nfor train_index, test_index in kf.split(X_train):\n    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n    \n    rf.fit(X_train_, y_train_)\n    estimators.append(rf)\n    res.extend(list(y_test_ - rf.predict(X_test_)))","1b864d19":"y_pred_multi = np.column_stack([e.predict(X_test) for e in estimators])\ny_pred_multi.shape","fad49b22":"ci = np.quantile(res, 1 - alpha)\ntop = []\nbottom = []\n\nfor i in range(y_pred_multi.shape[0]):\n    if ci > 0:\n        top.append(np.quantile(y_pred_multi[i] + ci, 1 - alpha))\n        bottom.append(np.quantile(y_pred_multi[i] - ci, 1 - alpha))\n    else:\n        top.append(np.quantile(y_pred_multi[i] - ci, 1 - alpha))\n        bottom.append(np.quantile(y_pred_multi[i] + ci, 1 - alpha))  ","bd663ed4":"preds = np.median(y_pred_multi, axis=1)\ndf = pd.DataFrame()\ndf['pred'] = preds\ndf['upper'] = top\ndf['lower'] = bottom","a2cd23a8":"df.head()","70d4cc57":"generate_plot_plus(y_test, preds, bottom, top)","ad1238d3":"### Naive Approach","bc17743c":"### CV+","be60890f":"# Jackknife+ Confidence Intervals on Regressions\n\nOn this notebook I will implement the Jackknife+ method to finding confidence intervals on regression predictions. I will also implement other methods from the paper and try to reproduce the results obtained. The idea of having confidence intervals on our models predictions is to best assure the uncertainty on it so we know how much we can trust and act on the predictions.","711198b6":"### Jackknife+","6bc3e742":"## Implementation","fb90fad3":"### Jackknife","abb44e49":"## Theoretical Introduction\n\nFour main methods are discussed on the paper. One keeps trying to improve over the other: the naive is to optimistic, the jackknife does not work well on degenerated cases and the jackknife+ can be very slow to compute. Of course, the improvement of the confidence interval will always come with a performance drawback.\n\n### Setting Terminology\nLet $\\hat{q}^+_{n, \\alpha}(v_i)$ be the $(1 - \\alpha)$ quantile of the distribution $v$ and $\\hat{q}^-_{n, \\alpha}(v_i)$ be the $\\alpha$ quantile. Let also $R_i^{LOO} = |Y_i - \\hat{\\mu}_{-i}(X_i)|$ be the i-th leave one-out residual.\n\n### Naive\n\nOn this method, one trains a model on the entire training set and then calculates the residuals obtained from the training set. Then, for every new prediction point, the CI of that new point is given by the quantile of the distribution of residuals from the training set. So:\n\n$$\\hat{\\mu}(x_{i+1}) \\pm (1 - \\alpha) \\text{ quantile of} R(\\hat{\\mu})$$\n\nWe can rewrite it as follows:\n\n$$\\hat{C}^{\\text{naive}}_{n, \\alpha} (X_{n+1}) = \\hat{\\mu}(x_{i+1}) \\pm \\hat{q}^+_{n, \\alpha}(|Y_i - \\hat{\\mu}(X_i)|)$$\n\nThe problem with this methodology is that if the model overfits, the residuals will become close to zero and therefore the confidence interval will go to zero. Even if the residuals are not zero, this method still very optimistic, as we know that the error on the training set will (almost) always be smaller than the one on the test set.\n\n### Jackknife\n\nFor the jackknife method, we compute $n$ models using the leave-one-out method. For each one, we compute the residual of the leaved out sample and then we aggregate the residuals into a distribution. After that, we train a regression on the entire training set and use it to predict the center point of the new test point. Then, the CI is given by the quantile of the distribution of the residuals from the leave-one-out:\n\n$$\\hat{\\mu}(x_{i+1}) \\pm (1 - \\alpha) \\text{ quantile of} R(\\hat{\\mu}_{-1})$$\n\nWe can then write:\n\n$$\\hat{C}^{\\text{jackknife}}_{n, \\alpha} (X_{n+1}) = \\hat{\\mu}(x_{i+1}) \\pm \\hat{q}^+_{n, \\alpha}(R_i^{LOO})$$\n\nThis aims to solve the problem from the previous method since now we are looking at a distribution of residuals from a test sample. Notice, however, that this requires fitting $n$ models, one for each sample of our dataset, which cannot be feasible.\n\nWhen the regression becomes unstable, such as on the scenario where the number of dimensions is close to the number of samples, this method loses its coverage. However, this seems to be a really rare corner case.\n\n### Jackknife+\n\nThe jackknife+ tries to solve the instability of the jackknife on that corner case by using every regressor fit on the leave-one-out instead of a regressor fitted on the entire training set. The idea is that, when using the Jackknife method, the $\\hat{\\mu}$ regressor has always seem one more training point than the the models on the residuals, which makes it not directly comparable to the residuals. Therefore, one can write the Jackknife+ interval as follows:\n\n$$\\hat{C}^{\\text{jackknife+}}_{n, \\alpha} (X_{n+1}) = \\hat{\\mu_{-i}}(x_{i+1}) \\pm \\hat{q}^+_{n, \\alpha}(R_i^{LOO})$$\n\nThe basic idea here is that, when defining the upper and lower bounds for a given instance, we use the median predict of all the models on that instance to find the prediction point, and then we use the residuals quantiles to define the interval as usual.\n\n### CV+\n\nThe CV+ is the application of the Jackknife+ method but using a K-Fold Cross Validation instead of a leave-one-out methodology. This method is best suited when $n$ is too large making the use of a leave one out approach unfeasible."}}