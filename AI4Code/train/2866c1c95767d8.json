{"cell_type":{"0114ebf6":"code","000f4572":"code","fa01d9a7":"code","888348df":"code","50751638":"code","0777cbbe":"code","4a740ce5":"code","770f55d8":"code","89af1a82":"code","a4c74077":"code","8ab11536":"code","fef0fd8a":"code","e098cae9":"code","e1164ea5":"code","c21841d4":"code","5f99e50b":"code","4e71ce8d":"code","95ca299c":"code","1404f35e":"code","ab128744":"code","70e9604a":"code","7ad0a934":"code","213d3118":"code","29f877de":"code","d24e4ff1":"code","18abaf75":"code","fa25db38":"code","533bae00":"code","1aec1e59":"code","54ca40e0":"code","e8ed75fa":"code","82aa7b9c":"code","2916924f":"markdown","b6a1dbc2":"markdown","c9ee9f0e":"markdown","e36922b1":"markdown","ff541511":"markdown","ccf73bc9":"markdown","accae2d2":"markdown","c66c84c4":"markdown","c873bc9f":"markdown","9462703f":"markdown","d6b1c8a7":"markdown","7c5baf22":"markdown"},"source":{"0114ebf6":"# Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\n\nfrom tqdm import tqdm\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","000f4572":"# Import data\ntrain1 = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\", parse_dates=True)\ntrain2 = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_2.csv.zip\", parse_dates=True)","fa01d9a7":"name = []\nfor page in train1['Page']:\n    split = page.split('.')\n    name.append(split[-3])\n\ntrain1['Name'] = name\ntrain1['Language'] = train1['Name'].str[-2:]","888348df":"from statsmodels.tsa.stattools import adfuller\n\ndef stationarity_test(timeseries, rolling_window, language):\n    plt.figure(figsize=(50, 8))\n    \n    rolling_mean = pd.DataFrame.rolling(timeseries, window=rolling_window).mean()\n    rolling_std = pd.DataFrame.rolling(timeseries, window=rolling_window).std()\n\n    plt.plot(timeseries, color='blue', label= 'Original')\n    plt.legend(loc='best')\n    \n    plt.plot(rolling_mean, color='red', label= 'Rolling Mean')\n    plt.legend(loc='best')\n    \n    std = plt.plot(rolling_std, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    \n    plt.title(f'Rolling Mean & Standard Deviation by Language : {language}')\n    plt.show()\n    \n    # Dickey-Fuller test\n    print('Results of Dickey-Fuller Test')\n    print()\n    test = adfuller(timeseries, autolag='AIC')\n    output = pd.Series(test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in test[4].items():\n        output['Critical Value (%s)'%key] = value\n    print(output)\n    print('-'*100)","50751638":"languages = ['es', 'zh', 'fr', 'en', 'ns', 'ru', 'ww', 'de', 'ja']\n\nfor lang in languages:\n    \n    df = train1[train1['Language'] == lang]\n    \n    remove_col = ['Language', 'Name']\n    pv = df.drop(remove_col, axis=1)\n    \n    pivot = pv.melt(id_vars='Page', var_name='Date', value_name='Visits')\n    pivot['Date'] = pd.to_datetime(pivot['Date'])\n    pivot.fillna(0, inplace=True)\n    \n    group_by = pivot[['Date', 'Visits', 'Page']].groupby(['Date'])['Visits'].mean()\n    \n    stationarity_test(group_by, rolling_window=100, language=lang)","0777cbbe":"pivot = train1.drop(['Name', 'Language'], axis=1)\ntrain_pivot = pivot.melt(id_vars='Page', var_name='Date', value_name='Visits')","4a740ce5":"train_pivot.head()","770f55d8":"train_pivot.fillna(0, inplace=True)","89af1a82":"train_pivot['Visits'] = train_pivot['Visits'].astype('float')","a4c74077":"acf = train_pivot[['Date', 'Visits']].groupby(['Date'])['Visits'].mean()\nplot_acf(acf, lags=20)","8ab11536":"plot_pacf(acf, lags=20)","fef0fd8a":"# SMAPE Metric\ndef smape(actual, forecast):\n    result = 1\/len(actual) * np.sum(2*np.abs(forecast-actual) \/ (np.abs(actual) + np.abs(forecast))*100)\n    return result","e098cae9":"def plot_predictions(actual, predicted, figsize, title):\n    \n    plt.figure(figsize=figsize)\n    actual = plt.plot(actual, color='blue', label='Actual')\n    predicted = plt.plot(predicted, color='red', label='Predicted')\n    plt.title(title)\n    plt.legend()\n    plt.show()","e1164ea5":"data = train1.drop(['Page', 'Name', 'Language'], axis=1)\ndata.fillna(0, inplace=True)","c21841d4":"from statsmodels.tsa.ar_model import AutoReg","5f99e50b":"def AutoRegression(data, lags, test_days, visualize_prediction:bool=False):\n    \n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n    \n    train = scaled_data[:-test_days]\n    test = scaled_data[-test_days:]\n    \n    ar = AutoReg(train, lags=lags)\n    model = ar.fit()\n    \n    # Predict\n    ## Training predictions\n    train_start = 0\n    train_end = len(train)\n    train_pred = model.predict(start=train_start, end=train_end)\n    \n    ## Testing predictions\n    test_start = len(train)\n    test_end = len(train) + len(test)\n    test_pred = model.predict(start=test_start, end=test_end)\n    \n    ## Inverse transform\n    train_inverse = scaler.inverse_transform(train_pred.reshape(-1, 1))\n    test_inverse = scaler.inverse_transform(test_pred.reshape(-1, 1))\n    \n    # Plot predictions\n    if visualize_prediction:\n        plot_predictions(train, train_pred, figsize=(20, 6), title='AR Trainig (In Fold) Predictions')\n        plot_predictions(test, test_pred, figsize=(20, 6), title='AR Testing (Out of Fold) Predictions')\n    \n    train_error = smape(train, train_pred)\n    test_error = smape(test, test_pred)\n    \n    print(f'Train Error: {train_error} | Test Error: {test_error}')\n\n    return model","4e71ce8d":"train = data.iloc[0, :].values\nmodel = AutoRegression(train, 30, 31, True)","95ca299c":"from statsmodels.tsa.arima.model import ARIMA","1404f35e":"def Arima(data, p, d, q, test_days, visualize_predictions=False):\n\n    train_date = data[:-31]\n    test_date = data[-31:]\n\n    data = data.astype('float')\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n\n    #scaled_data.index = pd.to_datetime(data.index)\n    train = scaled_data[:-31]\n    test = scaled_data[-31:]\n\n    #print(train), print(type(train)), print(train.shape), print(len(train))\n    train = pd.Series(train.reshape(len(train)), index=pd.to_datetime(train_date.index))\n    test = pd.Series(test.reshape(len(test)), index=pd.to_datetime(test_date.index))\n\n    arima = ARIMA(train, order=[p, d, q])\n    model = arima.fit()\n\n    # Predict\n    ## Training predictions\n    train_start = 0\n    train_end = len(train)\n    train_pred = model.predict(start=train_start, end=train_end, typ='levels')\n\n    ## Testing predictions\n    test_start = len(train)\n    test_end = len(train) + len(test) - 1\n    test_pred = model.predict(start=test_start, end=test_end, typ='levels')\n\n    # Inverse transform\n    train_inverse = scaler.inverse_transform(np.array(train_pred).reshape(-1, 1))\n    test_inverse = scaler.inverse_transform(np.array(test_pred).reshape(-1, 1))\n\n    #print(test_pred)\n\n    if visualize_predictions:\n        plot_predictions(train, train_pred, (20, 5), 'ARIMA Training Predictions')\n        plot_predictions(test, test_pred, (20, 5), 'ARIMA Testing Predictions')\n    \n    train_error = smape(train, train_pred)\n    test_error = smape(test, test_pred)\n    \n    print(f'Train Error: {train_error} | Test Error: {test_error}')\n    \n    return model","ab128744":"row = data.iloc[0, :]","70e9604a":"model = Arima(row, 5, 1, 5, 31, visualize_predictions=True)","7ad0a934":"from fbprophet import Prophet","213d3118":"def prophet(data, test_days, visualize_predictions, plot_prophet_comp:bool=False):\n    \n    train_date = data[:-test_days]\n    test_date = data[-test_days:]\n    \n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n    \n    train = scaled_data[:-test_days]\n    test = scaled_data[-test_days:]\n    \n    df = pd.DataFrame({'ds':pd.to_datetime(train_date.index), 'y':train.reshape(len(train))})\n    \n    model = Prophet()\n    model.fit(df)\n    \n    \n    future = model.make_future_dataframe(periods=test_days)\n    forecast = model.predict(future)\n    \n    train_actual = pd.Series(train.reshape(len(train)), index=pd.to_datetime(train_date.index))\n    train_pred = pd.Series(forecast.yhat.values[:-test_days].reshape(len(train)), index=pd.to_datetime(train_date.index))\n    \n    test_actual = pd.Series(test.reshape(len(test)), index=pd.to_datetime(test_date.index))\n    test_pred = pd.Series(forecast.yhat.values[-test_days:].reshape(len(test)), index=pd.to_datetime(test_date.index))\n    \n    # Visualize Predictions\n    if visualize_predictions:\n        plot_predictions(train_actual, train_pred, title='Prophet In Fold Predictions', figsize=(20, 5))\n        plot_predictions(test_actual, test_pred, title='Prophet Out of Fold Predictions', figsize=(20, 5))\n    \n    if plot_prophet_comp:\n        fig1 = model.plot(forecast)\n        fig2 = model.plot_components(forecast)\n    \n    train_error = smape(train_actual, train_pred)\n    test_error = smape(test_actual, test_pred)\n    \n    print(f'Train Error: {train_error} | Test Error: {test_error}')\n    \n    return model","29f877de":"row_p = data.iloc[0, :]\nmodel_prophet = prophet(row_p, 31, True, True)","d24e4ff1":"import tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras import Sequential\nfrom sklearn.metrics import mean_squared_error","18abaf75":"def create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i+look_back, 0])\n    return np.array(dataX), np.array(dataY)","fa25db38":"def Lstm(x_train, y_train, input_shape, num_lstm, epochs):\n    \n    model = Sequential()\n    model.add(LSTM(num_lstm, input_shape=input_shape))\n    model.add(Dense(1))\n    \n    model.compile(\n        loss='mean_squared_error',\n        optimizer='adam'\n    )\n    \n    es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n    \n    model.fit(x_train, y_train, epochs=epochs, batch_size=1, verbose=0, callbacks=[es])\n    \n    return model","533bae00":"def run_lstm(data, num_lstm, epochs, visualize_predictions=False):\n\n    # Scale the visits\n    scaler = MinMaxScaler()\n    scaled_row = scaler.fit_transform(np.array(data).reshape(-1, 1))\n    \n    # split into training and testing\n    train_data = scaled_row[:-31]\n    test_data = scaled_row[-31:]\n    \n    # Transform the dataset\n    x_train, y_train = create_dataset(train_data.reshape(-1, 1))\n    x_test, y_test = create_dataset(test_data.reshape(-1, 1))\n    \n    # Reshape the data for the lstm model\n    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n    x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n\n    model = Lstm(x_train, y_train, (1, 1), num_lstm, epochs)\n\n    # test the model\n    train_pred = model.predict(x_train)\n    test_pred = model.predict(x_test)\n    \n    train_pred_inverse = scaler.inverse_transform(train_pred)\n    train_true_inverse = scaler.inverse_transform([y_train])\n    test_pred_inverse = scaler.inverse_transform(test_pred)\n    test_true_inverse = scaler.inverse_transform([y_test])\n    \n    if visualize_predictions:\n        plot_predictions(y_train.reshape(-1, 1), train_pred, figsize=(20, 5), title='LSTM Training Predictions')\n        plot_predictions(y_test.reshape(-1, 1), test_pred, figsize=(20, 5), title='LSTM Testing Predictions')\n    \n    # Error\n    train_error_mae = mean_squared_error(y_train.reshape(-1, 1), train_pred)\n    test_error_mae = mean_squared_error(y_test.reshape(-1, 1), test_pred)\n    \n    train_error_smape = smape(y_train, train_pred)\n    test_error_smape = smape(y_test, test_pred)\n    \n    print('Mean Squared Error')\n    print(f'Train Error: {train_error_mae} | Test Error: {test_error_mae}')\n    print('-' * 100)\n    print('SMAPE')\n    print(f'Train Error: {train_error_smape} | Test Error: {test_error_smape}')\n    \n    return model","1aec1e59":"row = data.iloc[0, :]\nmodel_lstm = run_lstm(row, 10, 10, True)","54ca40e0":"from xgboost import XGBRegressor","e8ed75fa":"def xgboost(data, estimators, visualize_predictions):\n    \n    # Split into training and testing\n    train = row[:-31].values\n    test = row[-31:].values\n    \n    x_train, y_train = create_dataset(train.reshape(-1, 1))\n    x_test, y_test = create_dataset(test.reshape(-1, 1))\n    \n    model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n    model.fit(x_train, y_train)\n    \n    train_pred = model.predict(x_train)\n    test_pred = model.predict(x_test)\n    \n    train_error = smape(y_train, train_pred)\n    test_error = smape(y_test, test_pred)\n    \n    if visualize_predictions:\n        plot_predictions(y_train, train_pred, figsize=(15, 4), title='Training Predictions')\n        plot_predictions(y_test, test_pred, figsize=(15, 4), title='Testing Predictions')\n    \n    print(f'Train Error: {train_error} | Test Error: {test_error}')\n    \n    return model","82aa7b9c":"row_xg = data.iloc[1, :]\nmodel_xgb = xgboost(row_xg, 2000, True)","2916924f":"# Time series forecast predictions","b6a1dbc2":"### Augmented Dickey-Fuller test\n\n* This test checks if the data is stationary or not.\n* It is a hypothesis testing in which the null hypothesis is that the data is non-stationary.\n- **p-value > 0.05**: Fail to reject the null hypothesis (H0), the data\u00a0has a unit root and is non-stationary.\n- **p-value <= 0.05**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","c9ee9f0e":"### XGBoost","e36922b1":"### Prophet","ff541511":"* From the above graph we can conlude that the p-value of zh, en, ja is greater than 5% which means that the data is non-stationary, whereas the p-value for other languages is less than 5%, that means they are stationary.","ccf73bc9":"### AR Model","accae2d2":"## Auto Correlation and Partial Correlation","c66c84c4":"## Building and Testing Models\n\n* From the above image we can see that we have geometric decrease in ACF and sudden decrease after 2 significant lags","c873bc9f":"## Statistical tests","9462703f":"* We can see that there is geometric decrease in acf lags and sudden decrease in pacf after 2 lags\n\n![AR and MA based on ACF and PACF](https:\/\/miro.medium.com\/max\/3000\/1*MJIw0Lh2_PYMKJVTUVWAEw.png)","d6b1c8a7":"## LSTM","7c5baf22":"### ARIMA"}}