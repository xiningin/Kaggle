{"cell_type":{"7facde50":"code","d44d7ee0":"code","0b1e17d2":"code","677d18f9":"code","50f3711a":"code","40027d85":"code","0769a095":"code","de3fe191":"code","faec1049":"code","6e982362":"code","18770beb":"code","e3ee2c06":"code","ed2b902e":"code","43830f36":"code","b4a7c664":"code","368081f1":"code","768eefa3":"code","62a00b10":"code","f8a0068b":"code","e73a5ec9":"code","9a2dccf8":"code","52a27803":"code","4060bd14":"code","9b9922f4":"code","e4432acf":"code","65ba23b7":"code","f6769397":"code","bf80b7e5":"code","cd910b26":"code","4fcfeb8b":"code","c1b2097d":"code","ce87c0d9":"code","d5c0ed7c":"code","ea862a31":"code","90a5420a":"code","c45c6e64":"code","1c58c270":"markdown","9a166c85":"markdown","d239a8d3":"markdown","81224e74":"markdown","aa9c4edf":"markdown","f1205d2e":"markdown","e6432a64":"markdown","8f8b807c":"markdown"},"source":{"7facde50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout   # Incase you want to use dropout, but I don't use it because overfitting is pretty hard\nfrom keras import optimizers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d44d7ee0":"np.random.seed(0)\n\ndata = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndata.sample(n=5)","0b1e17d2":"data.dtypes","677d18f9":"data.info()","50f3711a":"import seaborn as sns\nplt.style.use('ggplot')\nsns.set_style('dark')","40027d85":"plt.figure()\n\nax = sns.barplot(x=data.thal.value_counts().index, y=data.thal[data.target==1].value_counts().values, color='g', alpha=0.5)\nax2 = sns.barplot(x=data.thal.value_counts().index, y=data.thal[data.target==0].value_counts().values, color='r', alpha=0.5)","0769a095":"plt.figure()\n\nax = sns.pairplot(data, corner=True)\nplt.show()","de3fe191":"plt.figure()\n\nax = sns.distplot(a=data.oldpeak[data.target==1], bins=15)","faec1049":"plt.figure()\n\nax = sns.distplot(a=data.oldpeak[data.target==0], bins=15)","6e982362":"plt.figure()\nax = sns.barplot(x='slope', y='target', data=data)","18770beb":"plt.figure()\n\nax = sns.barplot(x='ca', y='target', data=data)","e3ee2c06":"plt.figure()\n\nax = sns.barplot(x='thal', y='target', data=data)","ed2b902e":"data.ca.value_counts()","43830f36":"dropindices = data[data.ca == 4].index\ndata.drop(dropindices, inplace=True)\ndata.ca.value_counts()","b4a7c664":"data.index.size","368081f1":"dummy1 = pd.get_dummies(data.cp)\ndummy2 = pd.get_dummies(data.thal)\ndummy3 = pd.get_dummies(data.restecg)\ndummy4 = pd.get_dummies(data.slope)\ndummy5 = pd.get_dummies(data.ca)\nmerge = pd.concat([data,dummy1,dummy2,dummy3,dummy4,dummy5], axis=1)   # This turns the continuous data into binary form, easier for algorithms to understand","768eefa3":"y = merge['target']\nX = merge.drop(['target', 'cp', 'thal', 'restecg', 'slope', 'ca'], axis=1)\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)","62a00b10":"train_X.head(5)","f8a0068b":"test_X.head(5)","e73a5ec9":"train_X.info()","9a2dccf8":"test_X.info()","52a27803":"from sklearn.preprocessing import MinMaxScaler\n\nfeature_scaler = MinMaxScaler()\ntrain_X = feature_scaler.fit_transform(train_X)\ntest_X = feature_scaler.transform(test_X)","4060bd14":"pd.DataFrame(train_X).head()","9b9922f4":"Xtrain, Xval, Ytrain, Yval = train_test_split(train_X, train_y, test_size=0.2, random_state=5)","e4432acf":"Ytrain.shape","65ba23b7":"Yval.shape","f6769397":"model = Sequential()\n\nmodel.add(Dense(train_X.shape[1], input_dim=train_X.shape[1]))   \nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","bf80b7e5":"opt = optimizers.Adam()\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\nhistory = model.fit(Xtrain, Ytrain, epochs=40, validation_data=(Xval, Yval))","cd910b26":"plt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.ylim((0.6, 1.1))\nplt.legend()","4fcfeb8b":"prediction = model.predict(test_X) > 0.5\nprediction = (prediction > 0.5) * 1\naccuracy_nn = metrics.accuracy_score(test_y, prediction) * 100\nprint(accuracy_nn)","c1b2097d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nL = LogisticRegression()\n\nparameters = {'C': [.1, .2, .3, .4, .5, 1, 2, 5, 10]}\n\nlogreg = GridSearchCV(L, parameters, scoring='neg_mean_squared_error')\n\nlogreg.fit(train_X, train_y)\nlogreg.best_params_","ce87c0d9":"model1 = LogisticRegression(C=0.1)\nmodel1.fit(train_X, train_y)\naccuracy1 = model1.score(test_X, test_y)\n\nprint('Logistic Regression Accuracy -->',((accuracy1)*100))","d5c0ed7c":"acc_test_log = round(model1.score(test_X, test_y) * 100, 2)\nprint(acc_test_log)","ea862a31":"# coeff_df = pd.DataFrame(train_X.columns.delete(0))\n# coeff_df.columns = ['Feature']\n# coeff_df[\"Correlation\"] = pd.Series(logreg_model.coef_[0])\n\n# coeff_df.sort_values(by='Correlation', ascending=False)\n\n\n### this block no longer works because of the minmaxscaler turning train_X and test_X into ndarrays\n### wasn't that important anyway","90a5420a":"from sklearn.ensemble import RandomForestClassifier\n\nmodel6 = RandomForestClassifier(criterion = 'entropy',max_features = 'log2',n_estimators = 250)\nmodel6.fit(train_X, train_y)\naccuracy6 = model6.score(test_X, test_y)\n\nprint('Random Forest Classifier Accuracy -->',((accuracy6)*100))","c45c6e64":"from sklearn.naive_bayes import MultinomialNB\nmultiNB = MultinomialNB()\n\nmultiNB.fit(train_X, train_y)\naccuracy5 = multiNB.score(test_X, test_y)\nprint('Multinomial NB Accuracy -->',((accuracy5)*100))","1c58c270":"The RandomForestRegressor is actually quite nice as well, especially when there is very linear data that has been created with the pd.get_dummies function.","9a166c85":"We also have to create our validation data","d239a8d3":"Now we can plot the history of the training","81224e74":"We can now get our prediction based on the test data and see its accuracy","aa9c4edf":"We can now get the mean of the training data and normalize it so that the neural network can \"understand\" it better","f1205d2e":"The below step is highly important to increasing accuracy.\nThis basically creates more features\/colummns for algorithms to learn from. This does require a data scientist to look through the data and see the correlations though. ","e6432a64":"Now we can try to use LogisticRegression and see how well it works in comparison to the neural network.","8f8b807c":"First will be the neural network with Keras"}}