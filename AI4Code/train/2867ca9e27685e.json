{"cell_type":{"50633276":"code","b8577b89":"code","611d90c6":"code","34e01e37":"code","d152b151":"code","d713cc95":"code","b95aa7de":"code","89878dac":"code","e8a72a93":"code","f00ac616":"code","75ac481f":"code","55c1f4a0":"code","5117e836":"code","f911ba9d":"code","2e7f40e3":"code","a1740e1b":"code","a65aa3db":"code","bef0f203":"code","6aeec70e":"code","12effafa":"code","d6cdb229":"code","98d8cc62":"code","c987e18a":"code","4d181339":"code","1272b9d6":"code","e108ad71":"code","079b28b1":"code","8ea356ed":"code","686aa26a":"code","eb3830f2":"code","f42cc2e2":"code","21635008":"code","a7bc4960":"code","b077d975":"code","cecabfdc":"code","1380b9c1":"code","5d7e9e80":"code","f815ce9a":"markdown","e7e441de":"markdown","ed7115f0":"markdown","5dd3a7ec":"markdown","7c243465":"markdown","210aeb83":"markdown","fbe4a018":"markdown","41057cff":"markdown","54514364":"markdown","6b1606e0":"markdown","ae3054eb":"markdown","b73515d4":"markdown","1aca7669":"markdown","1be99a35":"markdown"},"source":{"50633276":"import os\nimport gc\nimport time\n\nimport cv2\nimport albumentations as A\nimport numpy as np\nfrom albumentations.pytorch import ToTensor\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n\nINPUT_DIR = '..\/input\/all-dogs\/all-dogs'","b8577b89":"image_files = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR)]\nprint('total image files: {}'.format(len(image_files)))","611d90c6":"def test(f, n_trials=5):\n    elapsed_times = []\n    for i in range(n_trials):\n        t1 = time.time()\n        f()\n        t2 = time.time()\n        elapsed_times.append(t2-t1)\n    print('Mean: {:.3f}s - Std: {:.3f}s - Max: {:.3f}s - Min: {:.3f}s'.format(\n        np.mean(elapsed_times),\n        np.std(elapsed_times),\n        np.max(elapsed_times),\n        np.min(elapsed_times)\n    ))","34e01e37":"image_files_1000 = image_files[:1000]","d152b151":"def cv2_imread(path):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\nf = lambda: [cv2_imread(f) for f in image_files_1000]","d713cc95":"test(f, n_trials=5)","b95aa7de":"f = lambda: [Image.open(f) for f in image_files_1000]","89878dac":"test(f, n_trials=5)","e8a72a93":"# use them following test\n\ncv2_img_1000 = [cv2_imread(f) for f in image_files_1000]\npil_img_1000 = [Image.open(f) for f in image_files_1000]","f00ac616":"cv2_transform = A.Compose([A.Resize(64, 64, interpolation=cv2.INTER_LINEAR)])\n\nf = lambda: [cv2_transform(image=img)['image'] for img in cv2_img_1000]","75ac481f":"test(f, n_trials=5)","55c1f4a0":"pil_transform = transforms.Compose([transforms.Resize((64, 64), interpolation=2)])\n\nf = lambda: [pil_transform(img) for img in pil_img_1000]","5117e836":"test(f, n_trials=5)","f911ba9d":"# use them following test\n\ncv2_img_1000 = [cv2_transform(image=img)['image'] for img in cv2_img_1000]\npil_img_1000 = [pil_transform(img) for img in pil_img_1000]","2e7f40e3":"cv2_transform = A.Compose([ToTensor()])\n\nf = lambda: [cv2_transform(image=img)['image'] for img in cv2_img_1000]","a1740e1b":"test(f, n_trials=5)","a65aa3db":"pil_transform = transforms.Compose([transforms.ToTensor()])\n\nf = lambda: [pil_transform(img) for img in pil_img_1000]","bef0f203":"test(f, n_trials=5)","6aeec70e":"image_files_1000 = image_files[:1000]","12effafa":"class BaseDataset(Dataset):\n    def __init__(self, files, transform=None):\n        super().__init__()\n        self.files = files\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.files)\n\n\nclass PILDataset(BaseDataset):\n    def __getitem__(self, idx):\n        file = self.files[idx]\n        img = Image.open(file)\n        if self.transform is not None:\n            img = self.transform(img)\n            \n        return img\n\n    \nclass CV2Dataset(BaseDataset):\n    def __getitem__(self, idx):\n        file = self.files[idx]\n        img = cv2.imread(file)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n            \n        return img","d6cdb229":"def dataloader_test(files, transform, test_type='cv2', batch_size=64, n_trials=5):\n    assert test_type in ['cv2', 'pil']\n    \n    if test_type == 'cv2':\n        test_dataset = CV2Dataset(files, transform=transform)\n        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n    else:\n        test_dataset = PILDataset(files, transform=transform)\n        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n    \n    def f():\n        for batch in test_dataloader:\n            pass\n    \n    test(f, n_trials=n_trials)","98d8cc62":"cv2_transform = A.Compose([\n    A.Resize(64, 64, interpolation=cv2.INTER_LINEAR),\n    ToTensor()\n])","c987e18a":"dataloader_test(image_files_1000, cv2_transform, test_type='cv2', batch_size=64, n_trials=5)","4d181339":"pil_transform = transforms.Compose([\n    transforms.Resize((64, 64), interpolation=2),\n    transforms.ToTensor()\n])","1272b9d6":"dataloader_test(image_files_1000, pil_transform, test_type='pil', batch_size=64, n_trials=5)","e108ad71":"cv2_transform = A.Compose([\n    A.SmallestMaxSize(64, interpolation=cv2.INTER_LINEAR),\n    A.CenterCrop(64, 64),\n    ToTensor()\n])","079b28b1":"dataloader_test(image_files_1000, cv2_transform, test_type='cv2', batch_size=64, n_trials=5)","8ea356ed":"pil_transform = transforms.Compose([\n    transforms.Resize(64, interpolation=2),\n    transforms.CenterCrop(64),\n    transforms.ToTensor()\n])","686aa26a":"dataloader_test(image_files_1000, pil_transform, test_type='pil', batch_size=64, n_trials=5)","eb3830f2":"cv2_transform = A.Compose([\n    A.Resize(64, 64, interpolation=cv2.INTER_LINEAR),\n    A.HorizontalFlip(p=0.5),\n    ToTensor()\n])","f42cc2e2":"dataloader_test(image_files_1000, cv2_transform, test_type='cv2', batch_size=64, n_trials=5)","21635008":"pil_transform = transforms.Compose([\n    transforms.Resize((64, 64), interpolation=2),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])","a7bc4960":"dataloader_test(image_files_1000, pil_transform, test_type='pil', batch_size=64, n_trials=5)","b077d975":"cv2_transform = A.Compose([\n    A.Resize(96, 96, interpolation=cv2.INTER_LINEAR),\n    A.RandomCrop(64, 64),\n    ToTensor()\n])","cecabfdc":"dataloader_test(image_files_1000, cv2_transform, test_type='cv2', batch_size=64, n_trials=5)","1380b9c1":"pil_transform = transforms.Compose([\n    transforms.Resize((96, 96), interpolation=2),\n    transforms.RandomCrop(64),\n    transforms.ToTensor()\n])","5d7e9e80":"dataloader_test(image_files_1000, pil_transform, test_type='pil', batch_size=64, n_trials=5)","f815ce9a":"## Resize Speed\nresize images to 64x64 by bilinear-interpolation.\n- cv2: albumentations.Resize\n- PIL: torchvision.transforms.Resize","e7e441de":"Based on an official report [here](https:\/\/github.com\/albu\/albumentations#benchmarking-results), this kernel shows some speed comparisons between cv2 method and pil method. Most of cases, cv2 method is faster than pil one.","ed7115f0":"# DataLoader Comparisons\nDue to time constraints, using only 1,000 images.","5dd3a7ec":"## ToTensor Speed\n- cv2: albumentations.pytorch.ToTensor\n- PIL: torchvision.ToTensor","7c243465":"# Simple Comparisons\nDue to memory constraint, using only 1,000 images.","210aeb83":"## Resize-ToTensor","fbe4a018":"## Loading Speed\n- cv2: cv2.imread\n- PIL: PIL.Image.open","41057cff":"## Assumptions\n- Using pytorch. You can get image data as torch.Tensor through torch.utils.data.DataLoader.\n- Limited memory size. You cannot load all image data on memory so use load method.\n- Image size differs. MUST Resize.","54514364":"## Resize-HorizontalFlip-ToTensor","6b1606e0":"## Resize-CenterCrop-ToTensor","ae3054eb":"# OpenCV+albumentations is faster than PIL+torchvision\nYou can speed up pytorch.utils.data.DataLoader by using cv2.imread + albumentations, instead of PIL.Image.open + torchvision.transforms.","b73515d4":"# Some Settings","1aca7669":"## Resize-RandomCrop-ToTensor","1be99a35":"The work is now in progress. Feel free to post comments or suggestions. Thanks!"}}