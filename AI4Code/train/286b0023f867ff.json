{"cell_type":{"332e3f98":"code","ffe6f252":"code","ad6b3c9c":"code","d573b186":"code","7431d374":"code","c5548a95":"code","1965279c":"code","74971c2c":"code","c1cfa200":"code","37832e98":"code","fdb52f00":"code","d70a6a0b":"code","52d90b01":"code","835c95f1":"code","fa85ccbf":"code","196a7cf9":"code","d5d57a72":"markdown","5a143d94":"markdown","24280cb4":"markdown","dbb30b8c":"markdown","e1d74e63":"markdown","74e9d93f":"markdown","c9fb6f37":"markdown","df0f3fd9":"markdown","6043499f":"markdown"},"source":{"332e3f98":"#all will be used.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport matplotlib.transforms as mtransforms\nimport seaborn as sns\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings\n\nfrom sklearn.model_selection import train_test_split","ffe6f252":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")","ad6b3c9c":"#NULL does not exist. good!\n\ntrain.isnull().sum()[train.isnull().sum() != 0]","d573b186":"#Remove meaningless features.\n\ntrain_df = train.drop(['id'], axis = 1 )\ntest_df = test.drop(['id'], axis = 1)","7431d374":"# data segmentation\nX = train_df.drop('target', axis=1)\ny = train_df['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","c5548a95":"params = {'device_type': 'gpu',\n        'n_estimators': 10000,\n        'learning_rate': 0.2423075935828885,\n        'num_leaves': 2260,\n        'max_depth': 9,\n        'min_data_in_leaf': 8600,\n        'lambda_l1': 70,\n        'lambda_l2': 35,\n        'min_gain_to_split': 0.11775633820897208,\n        'bagging_fraction': 0.6000000000000001,\n        'bagging_freq': 1,\n        'feature_fraction': 0.6000000000000001,}","1965279c":"lgbm_clf = LGBMClassifier(**params)","74971c2c":"start = datetime.datetime.now()\nlgbm = lgbm_clf.fit(X_train,\n                       y_train,\n                       eval_set = [(X_train, y_train), (X_valid, y_valid)], \n                       eval_metric = 'auc',\n                       early_stopping_rounds = 15,\n                       verbose = True)\nend = datetime.datetime.now()\nend-start","c1cfa200":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc","37832e98":"y_pred = lgbm_clf.predict(X_valid)","fdb52f00":"def classifier_eval(y_valid , y_pred) :\n  print('\uc815\ud655\ub3c4(accuracy_score) : ', accuracy_score(y_valid, y_pred))\n  print('\uc815\ubc00\ub3c4(precision_score) : ', precision_score(y_valid, y_pred))\n  print('\uc7ac\ud604\uc728(recall_score) : ', recall_score(y_valid, y_pred))\n  print('F1 : ', f1_score(y_valid, y_pred))\n  print('AUC : ', roc_auc_score(y_valid, y_pred))\n\nclassifier_eval(y_valid, y_pred)","d70a6a0b":"x = np.array([accuracy_score(y_valid, y_pred),\n              precision_score(y_valid, y_pred),\n              recall_score(y_valid, y_pred),\n              f1_score(y_valid, y_pred),\n              roc_auc_score(y_valid, y_pred)])\n\nx","52d90b01":"label = ['accuracy', 'precision', 'recall_score', 'f1_score', 'roc_auc']\n\nindex = np.arange(len(label))\n\n\nplt.bar(index, x, width=0.5)\nplt.title('evaluation index', fontsize=20)\nplt.ylabel('%', fontsize=18)\nplt.xticks(index, label, fontsize=15,rotation=90)    # X\ucd95\uc758 \ubc94\uc704: [xmin, xmax]\nplt.ylim([0, 1])     # Y\ucd95\uc758 \ubc94\uc704: [ymin, ymax]\nplt.show()","835c95f1":"lightGBM_prediction = lgbm_clf.predict(test_df)","fa85ccbf":"submission_lightGBM = pd.DataFrame({'id':test['id'], 'target':lightGBM_prediction})\nsubmission_lightGBM.head()","196a7cf9":"submission_lightGBM.to_csv('.\/submission.csv', index=False)","d5d57a72":"#### 1. A method of giving weight to important data (generally data with incorrect model) such as AdaBoost\n\n#### 2. A method of repeatedly training the difference between correct answers and incorrect answers like a loss function like GBDT","5a143d94":"#### Light GBM's GBM is a Gradient Boosting Model, a tree-based learning algorithm. To put it simply, \n#### the learning method of this GBM can be said to proceed while adding weight to the wrong part.\n\n#### In Gradient Boosting, boosting is a concept that makes several trees, develops the existing model\n#### little by little and adds them at the end, which is different from the random forest bagging technique.","24280cb4":"![LGBM.PNG](attachment:4841ab12-2fe8-4814-a0df-ef19edd7e237.PNG)","dbb30b8c":"**There are two main ways of boosting.**","e1d74e63":"# Light GBM Modeling","74e9d93f":"# Load Data","c9fb6f37":"**Find the best parameters**","df0f3fd9":"# Submission","6043499f":"# Abstract"}}