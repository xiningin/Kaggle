{"cell_type":{"ccbd1bef":"code","439ed2e0":"code","381a1e58":"code","79b9d43b":"code","6acc037f":"code","b4a61ce0":"code","2d54c172":"code","28e5bbba":"code","90e5be67":"code","32ea50f0":"code","ff24a28a":"code","c50bcb50":"code","3ae7cf55":"code","b732de7f":"code","8edc9378":"code","9989a780":"code","f2beceec":"code","78747d5b":"code","198b4a44":"code","adc983ed":"code","38de4a66":"code","6cc6f297":"code","91dcdda4":"code","91c1ee71":"code","83265120":"code","dc75f4bc":"code","18b14c65":"code","92beffc2":"code","a571130a":"code","5141bc93":"code","8bdfa46b":"code","27733cad":"code","7357d1c4":"code","aa0d1319":"code","6f904617":"code","0f8672ca":"markdown","9d149a21":"markdown","7f64181b":"markdown"},"source":{"ccbd1bef":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport PIL\nimport skimage.io\nimport seaborn as sn\nfrom collections import Counter\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras.backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical, Sequence\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation,BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import ResNet50\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings(\"ignore\")","439ed2e0":"path = '\/kaggle\/input\/herbarium-2021-fgvc8\/'\nos.listdir(path)","381a1e58":"samp_subm = pd.read_csv(path+'sample_submission.csv')","79b9d43b":"with open(path+'train\/'+'metadata.json') as f:\n    train_data = json.load(f)\nwith open(path+'test\/'+'metadata.json') as f:\n    test_data = json.load(f)","6acc037f":"def plot_examples():\n    fig, axs = plt.subplots(4, 4, figsize=(20, 20))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    \n    axs = axs.ravel()\n    for i in range(16):\n        img = cv2.imread(path+'train\/'+train_data['images'][i]['file_name'])\n        axs[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        axs[i].set_title(train_data['categories'][i]['family'])\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n    plt.show()","b4a61ce0":"print('Number of train images:', len(train_data['images']))\nprint('Number of test images:', len(test_data['images']))","2d54c172":"train_data['annotations'][0]","28e5bbba":"train_data['categories'][0]","90e5be67":"train_data['images'][0]","32ea50f0":"plot_examples()","ff24a28a":"df_image = pd.json_normalize(train_data['images'])\ndf_annot = pd.json_normalize(train_data['annotations'])\ndf_train_data = pd.DataFrame()\ndf_train_data['file_name'] = df_image['file_name']\ndf_train_data['category_id'] = df_annot['category_id']","c50bcb50":"df_train_data, df_val_data = train_test_split(df_train_data, test_size=0.3)\ndf_train_data.index = range(len(df_train_data.index))\ndf_val_data.index = range(len(df_val_data.index))","3ae7cf55":"df_image = pd.json_normalize(test_data['images'])\ndf_test_data = pd.DataFrame()\ndf_test_data['file_name'] = df_image['file_name']\ndf_test_data['category_id'] = 0","b732de7f":"print('Number of train samples:', len(df_train_data))\nprint('Number of val samples:', len(df_val_data))\nprint('Number of test samples:', len(df_test_data))","8edc9378":"print('Number of categories:', len(df_train_data['category_id'].unique()))","9989a780":"df_train_data['category_id'].value_counts()[0:10]","f2beceec":"q_size = 64\nimg_channel = 3\nnum_classes = 64500\nbatch_size = 32\nepochs = 5","78747d5b":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, labels, batch_size, img_size, img_channel, num_classes):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_channel = img_channel\n        self.num_classes = num_classes\n        self.indexes = np.arange(len(self.list_IDs))\n\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n            \n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, self.img_size, self.img_size, self.img_channel))\n        y = np.zeros((self.batch_size, self.num_classes), dtype=int)\n        for i, ID in enumerate(list_IDs_temp):\n            img = cv2.imread(self.path+ID)\n            img = cv2.resize(img, (self.img_size, self.img_size))\n            X[i, ] = img\/255\n            y[i, ] = to_categorical(self.labels[i], num_classes=self.num_classes)\n        return X, y","198b4a44":"number_samples = 10000\ndf_train_data = df_train_data[0:number_samples]\ndf_val_data = df_val_data[0:number_samples]\ndf_test_data = df_test_data[0:number_samples]","adc983ed":"df_train_data","38de4a66":"train_generator = DataGenerator(path+'train\/', df_train_data['file_name'], df_train_data['category_id'],\n                                batch_size, q_size, img_channel, num_classes)\nval_generator = DataGenerator(path+'train\/',df_val_data['file_name'], df_val_data['category_id'],\n                                batch_size, q_size, img_channel, num_classes)\ntest_generator = DataGenerator(path+'test\/',df_test_data['file_name'], df_test_data['category_id'],\n                                batch_size, q_size, img_channel, num_classes)","6cc6f297":"base_model = tf.keras.applications.VGG16(input_shape=(q_size, q_size, img_channel),include_top=False,weights=\"imagenet\")","91dcdda4":"# Freezing Layers\n\nfor layer in base_model.layers[:-20]:\n    layer.trainable=False","91c1ee71":"# Building Model\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.5))\n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.summary()","83265120":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","dc75f4bc":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","18b14c65":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","92beffc2":"METRICS","a571130a":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 2,verbose = 1,factor = 0.50, min_lr = 1e-4)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=2)","5141bc93":"model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=METRICS)","8bdfa46b":"%time\nhistory = model.fit_generator(generator=train_generator,validation_data=val_generator,epochs = epochs,verbose = 1)","27733cad":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n    \n    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(acc) + 1), acc)\n    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n    ax3.plot(range(1, len(auc) + 1), auc)\n    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n    ax3.set_title('History of AUC')\n    ax3.set_xlabel('Epochs')\n    ax3.set_ylabel('AUC')\n    ax3.legend(['training', 'validation'])\n    \n    ax4.plot(range(1, len(precision) + 1), precision)\n    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n    ax4.set_title('History of Precision')\n    ax4.set_xlabel('Epochs')\n    ax4.set_ylabel('Precision')\n    ax4.legend(['training', 'validation'])\n    \n    ax5.plot(range(1, len(f1) + 1), f1)\n    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n    ax5.set_title('History of F1-score')\n    ax5.set_xlabel('Epochs')\n    ax5.set_ylabel('F1 score')\n    ax5.legend(['training', 'validation'])\n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n               history.history['auc'],history.history['val_auc'],\n               history.history['precision'],history.history['val_precision'],\n               history.history['f1_score'],history.history['val_f1_score']\n              )","7357d1c4":"predict = model.predict_generator(test_generator, verbose=1)","aa0d1319":"predict.argmax(axis=1)\nsamp_subm.loc[0:len(df_test_data.index)-1, 'Predicted'] = predict.argmax(axis=1)[0:len(df_test_data.index)]","6f904617":"Sub = samp_subm.copy()\nSub.to_csv('submission.csv', index=False)","0f8672ca":"\n<h1 style='background-color:#C19A6B; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' > Herbarium 2021 - Half-Earth Challenge - FGVC8 <\/h1>\n\n\n#### Identify plant species of the Americas, Oceania and the Pacific from herbarium specimens\n\n\n\n<img src=\"https:\/\/biokic.asu.edu\/sites\/default\/files\/styles\/panopoly_image_full\/public\/img_2061-fred_irish_collections_1.jpg?itok=PdE3WGEJ\" width=\"1000px\">\n","9d149a21":"## Data Overview\nThe training and test set contain images of herbarium specimens from nearly 65,000 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image.\n\nThe data has been approximately split 80%\/20% for training\/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set contains species with hundreds of examples, but the test set has the number of examples per species capped at a maximum of 10.\n\n\n## Dataset Details\nEach image has different image dimensions, with a maximum of 1000 pixels in the larger dimension. These have been resized from the original image resolution. All images are in JPEG format.\n\n## Dataset Format\nThis dataset uses the COCO dataset format with additional annotation fields. In addition to the species category labels, we also provide region and supercategory information.\n\nThe training set metadata (train\/metadata.json) and test set metadata (test\/metadata.json) are JSON files in the format below. Naturally, the test set metadata file omits the \"annotations\", \"categories,\" and \"regions\" elements.\n\n#### Dataset Link \n\n##### [Here](https:\/\/www.kaggle.com\/c\/herbarium-2021-fgvc8\/overview)\n","7f64181b":"## What is VGG16 model?\nVGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. ... By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes. The model loads a set of weights pre-trained on ImageNet.\n\n<img src=\"https:\/\/storage.googleapis.com\/lds-media\/images\/vgg16-architecture.original.jpg\" width=\"1000px\">\n"}}