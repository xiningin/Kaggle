{"cell_type":{"84f5025f":"code","7d121564":"code","dfde0191":"code","e7287e06":"code","4d01f495":"code","72cf9420":"code","ea191cf1":"code","4c53ee4c":"code","c6ed0fd0":"code","204327c7":"code","45464752":"code","3c392a2a":"code","2a1d77fc":"code","37b711b7":"code","863d2c2c":"code","19185c7d":"markdown"},"source":{"84f5025f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d121564":"import pandas as pd\n\ndatastore = pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json', lines = True)\n\nsentences = datastore['headline'].tolist()\nlabels = datastore['is_sarcastic'].tolist()\nurls = datastore['article_link'].tolist()\n\n#shuffle data to split into train, test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sentences, labels, train_size = 0.8, random_state = 42, shuffle = True)","dfde0191":"datastore.shape\ntrain_dataframe = datastore[0:22895]\ntest_dataframe = datastore[22895:28619]","e7287e06":"print(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","4d01f495":"#tokenize text\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 100\ntraining_size = 20000\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(X_train)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, padding='post', truncating='post')\n\ntesting_sequences = tokenizer.texts_to_sequences(X_test)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding='post', truncating='post')","72cf9420":"import numpy as np\n\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(y_train)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(y_test)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(training_padded, training_labels, epochs=30, validation_data=(testing_padded, testing_labels), verbose=2)","ea191cf1":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","4c53ee4c":"#CNN Model\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory1 = model1.fit(training_padded, training_labels, epochs=30, validation_data=(testing_padded, testing_labels), verbose=2)","c6ed0fd0":"#LSTM model\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory2 = model2.fit(training_padded, training_labels, epochs=30, validation_data=(testing_padded, testing_labels), verbose=2)","204327c7":"#RNN Model\n# encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n# encoder.adapt((datastore['headline']).map(lambda text, label: text))\nmodel3 = tf.keras.Sequential([\n    #encoder,\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\nmodel3.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\nhistory3 = model3.fit(training_padded, training_labels, epochs=10, validation_data=(testing_padded, testing_labels), verbose=2, validation_steps=30)","45464752":"# BERT\nfrom transformers import BertTokenizer\nimport tensorflow_datasets as tfds\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ndef input_for_bert_model(df, tokenizer, max_seq_length):\n    columns_list = df.columns.tolist()\n    input_ids = np.zeros((len(df), max_seq_length))\n    input_attention_masks = np.zeros((len(df), max_seq_length))\n    \n    if 'is_sarcastic' in columns_list:\n        train_labels = np.zeros((len(df), 1))\n        for i, labels in enumerate(df['is_sarcastic']):\n            train_labels[i,:] = labels\n    \n    for i, sequence in enumerate(df['headline']):\n        tokens = tokenizer.encode_plus(\n            sequence,\n            max_length = max_seq_length, # max length of the text that can go to BERT\n            truncation=True, padding='max_length',\n            add_special_tokens = True, # add [CLS], [SEP]\n            return_token_type_ids = False, \n            return_attention_mask = True, # add attention mask to not focus on pad tokens\n            return_tensors = 'tf'\n        )\n        input_ids[i,:], input_attention_masks[i,:] = tokens['input_ids'], tokens['attention_mask']\n    \n    if 'is_sarcastic' in columns_list:\n        return input_ids, input_attention_masks, train_labels\n    else:\n        return input_ids, input_attention_masks\n    \ntrain_ids, train_attention_masks, train_labels = input_for_bert_model(train_dataframe, bert_tokenizer, max_length)\ntest_ids, test_attention_masks, test_labels = input_for_bert_model(test_dataframe, bert_tokenizer, max_length)\ntrain_inputs = {\"input_ids\":train_ids[:22895], \"attention_mask\":train_attention_masks[:22895]}\ntrain_outputs = train_labels[:22895]\nvalid_inputs = {\"input_ids\":train_ids[22895:], \"attention_mask\":train_attention_masks[22895:]}\nvalid_outputs = train_labels[22895:]\ntest_inputs = {\"input_ids\":test_ids, \"attention_mask\":test_attention_masks}\ntest_outputs = test_labels[22895:]","3c392a2a":"#Bert model initialization\nfrom transformers import BertTokenizer, TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\nsequence_output = bert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\nx = tf.keras.layers.Dropout(0.1)(sequence_output)\nout = tf.keras.layers.Dense(1, activation='linear', name=\"outputs\")(x)\nmodel4 = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=out)","2a1d77fc":"optimizer = tf.keras.optimizers.Adam(lr=1e-4)\nmodel4.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\nhistory4 = model4.fit(train_inputs, train_outputs, epochs=10, batch_size=8, validation_data=(valid_inputs, valid_outputs))","37b711b7":"#plotting comparison between 4 models\nimport pandas as pd\nfrom pandas import DataFrame\naccuracy = [max(history1.history['val_accuracy']),max(history2.history['val_accuracy']), max(history3.history['val_accuracy']),max(history4.history['val_accuracy'])]\nloss = [max(history1.history['val_loss']),max(history2.history['val_loss']),max(history3.history['val_loss']),max(history4.history['val_loss'])]\n\ncol={'Accuracy':accuracy,'Loss':loss}\nmodels=['NN','CNN','LSTM']\ngraph_df=DataFrame(data=col,index=models)\ngraph_df","863d2c2c":"graph_df.plot(kind='bar')","19185c7d":"# Sarcasm is nothing but people use positive words in order to convey a negative message and vice versa. Identifying Sarcasm is difficult for humans, how come I teach my machine to identify it? We will try to do it in this notebook, to take a text and identify if it is sarcastic or not.\n![featured_sarcasm.png](attachment:4ec6618a-0d40-450a-826e-b7ead07cc8f9.png)"}}