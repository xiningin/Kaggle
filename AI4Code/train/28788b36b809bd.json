{"cell_type":{"6f58e410":"code","113dbcc1":"code","5bc9891d":"code","3aa5835e":"code","a1213fe9":"code","10cb0af4":"code","b07de0bc":"code","3f2e7b32":"code","926055a9":"code","a70d8567":"code","819a6d19":"code","9699acde":"code","b77015ec":"code","a07a1f32":"code","8b647e05":"code","f3a385e2":"code","46ac93f6":"code","679dd51e":"code","3ca16fd2":"code","b4edcc3b":"code","dc2e0c67":"code","4cbc3668":"markdown","2faf4d5b":"markdown","58384960":"markdown","f7fa2e7e":"markdown"},"source":{"6f58e410":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('Setup complete.')","113dbcc1":"# Load the training dataset and look at the first few records.\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","5bc9891d":"# Look at summary statistics for the training dataset.\ntrain_df.describe()","3aa5835e":"train_df['Cabin'] = train_df['Cabin'].fillna('Unknown')\ntrain_df.groupby(['Cabin']).apply(lambda x: x['Survived'].sum()\/len(x))","a1213fe9":"train_df['n_Cabins'] = train_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ntrain_df.groupby(['n_Cabins', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","10cb0af4":"train_df['CabinPrefix'] = train_df.apply(lambda row: row['Cabin'][0], axis=1)\ntrain_df.groupby(['CabinPrefix', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","b07de0bc":"def split_ticket(ticket):\n    # special case. a few Tickets are only LINE\n    if ticket == 'LINE':\n        return pd.Series(['LINE', 0])\n    \n    parts = ticket.split()  # split the ticket on whitespace\n    if len(parts) == 1:\n        return pd.Series([\"NO_PREFIX\", int(parts[0].strip(' .'))])\n    elif len(parts) == 2:\n        return pd.Series([parts[0].strip(' .'), int(parts[1].strip(' .'))])\n    else:\n        # special case. One Ticket has a prefix separated by a space.\n        return pd.Series([parts[0].strip(' .') + parts[2].strip(' .'), int(parts[2].strip(' .'))])\n\ntrain_df[['Ticket_Prefix', 'Ticket_NUM']] = train_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ntrain_df.groupby(['Ticket_Prefix']).apply(lambda x: x['Survived'].sum()\/len(x))","3f2e7b32":"train_df['Title'] = train_df.apply(lambda row: row['Name'].split()[1], axis=1)\ntrain_df.groupby(['Title']).apply(lambda x: x['Survived'].sum()\/len(x))","926055a9":"from sklearn.preprocessing import LabelEncoder\n\n# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title']\nX = train_df.loc[: ,features]\ny = train_df['Survived']\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\nX['Sex'] = X['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\nX['Embarked'] = X['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the average age.\nX['Age'] = X['Age'].fillna(28.0)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(X['Embarked'])\nX['Embarked'] = emb_encoder.transform(X['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(X['CabinPrefix'])\nX['CabinPrefix'] = cab_encoder.transform(X['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(X['Ticket_Prefix'])\nX['Ticket_Prefix'] = tic_encoder.transform(X['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(X['Title'])\nX['Title'] = title_encoder.transform(X['Title'])\nX","a70d8567":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestClassifier(random_state=0)\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1400, num = 7)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 70, num = 7)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nparam_dict = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\ngrid = GridSearchCV(model, param_grid=param_dict, cv=3, verbose=3, n_jobs=4)\ngrid.fit(X, y)","819a6d19":"grid.best_params_","9699acde":"grid.best_estimator_","b77015ec":"grid.best_score_","a07a1f32":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(test_df.shape)\ntest_df.head()","8b647e05":"# Set aside the test PassengerId values for later.\npassenger_ids = test_df['PassengerId']\n\n# Add a fake value for Survived that we'll remove again later.\ntest_df['Survived'] = np.NaN\nprint(test_df.shape)\ntest_df.head()","f3a385e2":"combo_df = train_df.append(test_df)\nprint(combo_df.shape)","46ac93f6":"# Add the engineered features to the combined dataset\ncombo_df['Cabin'] = combo_df['Cabin'].fillna('Unknown')\ncombo_df['n_Cabins'] = combo_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ncombo_df['CabinPrefix'] = combo_df.apply(lambda row: row['Cabin'][0], axis=1)\ncombo_df[['Ticket_Prefix', 'Ticket_NUM']] = combo_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ncombo_df['Title'] = combo_df.apply(lambda row: row['Name'].split()[1], axis=1)\ncombo_df.describe()","679dd51e":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title', 'Survived']\ncombo_df = combo_df[features]\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\ncombo_df['Sex'] = combo_df['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\ncombo_df['Embarked'] = combo_df['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the median age.\ncombo_df['Age'] = combo_df['Age'].fillna(28.0)\n\n# There was one missing Fare. Fill it with the median fare.\ncombo_df['Fare'] = combo_df['Fare'].fillna(14.45)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(combo_df['Embarked'])\ncombo_df['Embarked'] = emb_encoder.transform(combo_df['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(combo_df['CabinPrefix'])\ncombo_df['CabinPrefix'] = cab_encoder.transform(combo_df['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(combo_df['Ticket_Prefix'])\ncombo_df['Ticket_Prefix'] = tic_encoder.transform(combo_df['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(combo_df['Title'])\ncombo_df['Title'] = title_encoder.transform(combo_df['Title'])\n\ncombo_df.head()","3ca16fd2":"# Split the combined dataframe back into train and test data.\ntrain_df = combo_df[combo_df['Survived'].notna()]\nprint(train_df.shape)\ntrain_df.describe()","b4edcc3b":"test_df = combo_df[combo_df['Survived'].isna()]\ntest_df = test_df.drop('Survived', axis=1)\nprint(test_df.shape)\ntest_df.describe()","dc2e0c67":"# Split training features and target\ntrain_y = train_df['Survived']\ntrain_X = train_df.drop('Survived', axis=1)\n\n# create and fit the model\nmodel = RandomForestClassifier(bootstrap=False, max_depth=30, min_samples_leaf=2,\n                       min_samples_split=5, n_estimators=200, random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_rf_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_rf_predictions_df.to_csv('final_RF_predictions.csv', index=False)","4cbc3668":"Data exploration and feature engineering based on [Titanic - Decision Tree \/ RF \/ GB](https:\/\/www.kaggle.com\/bcruise\/titanic-decision-tree-rf-gb). This notebook builds on that by showing how to do parameter tuning using grid search to get better results with a random forest model.","2faf4d5b":"## Feature Engineering","58384960":"## Grid Search\n\nUse [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) from scikit-learn to find the best parameters from an initial set. The un-tuned random forest model in the original notebook gave us 83.8% accuracy on the train\/validation split using the same features. That dropped all the way to 74% when the same model was used on the full training set to create a submission. Hopefully parameter tuning will improve upon that score.","f7fa2e7e":"## Final submission"}}