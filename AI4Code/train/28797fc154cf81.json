{"cell_type":{"760c9aed":"code","c8bb906e":"code","59f733e6":"code","f5f6874b":"code","9ad19e04":"code","6ca33d1a":"code","d0110959":"code","fee92926":"code","17d570a5":"code","79f912bb":"code","92c1dced":"code","9ad5bae4":"code","eb017899":"code","877b81fb":"code","86b664a4":"code","ba289a3c":"code","6447df84":"code","c3ee9007":"code","3095cf52":"code","6fe182f6":"code","54c6f322":"code","aaa12a2b":"code","52dcb670":"code","0d333d97":"code","d21126a7":"code","6b25485d":"code","64d340ff":"code","f3ec90c5":"code","b2a42b45":"code","28562f5c":"code","11ca5b2d":"code","284401cf":"markdown","3cf6ef03":"markdown","de93a12c":"markdown","441ab88a":"markdown","98349b87":"markdown","9a6107d2":"markdown","a8bbe152":"markdown","3a88375e":"markdown","39c71849":"markdown","ae0b7c35":"markdown"},"source":{"760c9aed":"from os import walk\nimport re\nimport pandas as pd\nimport os\nimport tqdm","c8bb906e":"# La fonction read_xml() permet d'extraire le champs \"CONTENU\" et \"SOMMAIRE\" d'un fichier xml. \n# Une l\u00e9g\u00e8re couche de pr\u00e9processing est effectu\u00e9 pour enlever les balises xml non voulus ainsi que les retour \u00e0 la ligne.\nimport xml.etree.ElementTree as ET\n\n#tree = ET.parse('data\/xml\/JURITEXT000006952571.xml')\ndef read_xml(file) :\n    tree = ET.parse(\"data\/xml\/\"+file)\n    root = tree.getroot()\n    contenu = ET.tostring(root.find('.\/TEXTE\/BLOC_TEXTUEL\/CONTENU'), encoding='utf8').decode('utf8')\n    contenu = re.sub(r\"( *<br \/> *)|(<\/*[A-Z]*>)|(<\\?.*\\?>)\",\"\", contenu)\n    contenu = re.sub(r\"\\n\",\"\",contenu)\n    if root.find('.\/TEXTE\/SOMMAIRE\/SCT') != None:\n        sommaire = root.find('.\/TEXTE\/SOMMAIRE\/SCT').text\n    elif root.find('.\/TEXTE\/SOMMAIRE\/ANA') != None:\n        sommaire = root.find('.\/TEXTE\/SOMMAIRE\/ANA').text\n    else :\n        sommaire = \"None\"\n    return contenu,sommaire","59f733e6":"#arr = os.listdir(\"data\/xml\/\")\n# file_list = []\n# list_article = []\n# for (dirpath, dirnames, filenames) in walk(\"data\/xml\/\"):\n#     file_list.extend(filenames)\n#     break\n# f = open(\"JURISTEXT.csv\",\"w\")\n# for file in file_list:\n#     #print(file)\n#     if(file != None):\n#         contenu,sommaire = read_xml(file)\n#         f.write(contenu + \"#separator#\" + sommaire)\n# f.close()","f5f6874b":"# T\u00e9l\u00e9chargement de la lib\n!pip install gpt_2_simple","9ad19e04":"import pandas as pd\nimport os\nimport gpt_2_simple as gpt2\nfrom datetime import datetime\nimport time\nimport tensorflow as tf\n\n# T\u00e9l\u00e9chargement du mod\u00e8le GPT2 qui servira de base pour notre entrainement\ngpt2.download_gpt2(model_name=\"124M\")","6ca33d1a":"# Import des donn\u00e9es csv\ndf = pd.read_csv(\"..\/input\/juristext\/JURISTEXT.csv\",sep=\"#separator#\", warn_bad_lines=False, header=None, error_bad_lines=False)\ndf.head()","d0110959":"print(df[0][0])","fee92926":"# On n'utilise que le contneu d'un rapport de cassation puis on nettoie d'\u00e9ventuelle donn\u00e9es vides\ndecision = df.loc[:,0]\ndecision = decision.dropna()\ndecision.shape","17d570a5":"# S\u00e9paration du dataset en plusieurs fichiers pour \u00e9viter les erreurs OOMs. \nmax_index=100000\nstep_index=10000\nos.mkdir(\"data\/\")\nos.mkdir(\"data\/txt\")\nos.mkdir(\"data\/npz\")\nfor i in range(int(max_index\/step_index)):\n    # Fichier contenant step_index rapports de cassation\n    text_data = open(f'data\/txt\/decision-{i}.txt', 'w')\n    for item in decision[i*step_index:(i+1)*step_index]:\n        clean = item.replace(\"<CONTENU \/>\", '')\n        if len(clean)>0:\n            text_data.write(\"<|startoftext|> \" + clean + \" <|endoftext|>\\n\")\n    text_data.close()\n    # Compression et pr\u00e9-encodage \n    gpt2.encode_dataset(f'data\/txt\/decision-{i}.txt',  out_path=f'data\/npz\/decision-{i}.npz')","79f912bb":"# Initialisation de la session et du finetuning\nfile_name = \".\/data\/npz\/decision-0.npz\"\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess,\n              dataset=file_name,\n              model_name='124M',\n              steps=500,\n              batch_size=1,\n              learning_rate=0.001,\n              overwrite=True)\n\n# Parcours des diff\u00e9rents fichiers pour le finetuning du mod\u00e8le\nfor i in range(1,int(max_index\/step_index)):\n    file_name=f'data\/npz\/decision-{i}.npz'\n    #tf.reset_default_graph()\n    sess=gpt2.reset_session(sess=sess)\n    gpt2.finetune(sess,\n                  dataset=file_name,\n                  steps=500,\n                  batch_size=1,\n                  learning_rate=0.001,\n                  overwrite=True)","92c1dced":"# Sauvegarde du mod\u00e8le pour un \u00e9ventuel export\nimport shutil\nshutil.make_archive(\"checkpoint2-100000-10000-500step-npz\", 'zip', \"checkpoint\")","9ad5bae4":"gpt2.generate(sess,prefix=\"Il renverse un pi\u00e9ton avec son v\u00e9hicule \", temperature=0, include_prefix=False)","eb017899":"gpt2.generate(sess,prefix=\"Il renverse un pi\u00e9ton avec son v\u00e9hicule \", temperature=0.3, include_prefix=False)","877b81fb":"gpt2.generate(sess,prefix=\"Il renverse un pi\u00e9ton avec son v\u00e9hicule \", temperature=0.7, include_prefix=False)","86b664a4":"gpt2.generate(sess,prefix=\"Il renverse un pi\u00e9ton avec son v\u00e9hicule \", temperature=1, include_prefix=False)","ba289a3c":"# Pour charger le mod\u00e8le entra\u00een\u00e9 :\n#sess2 = gpt2.start_tf_sess()\n#gpt2.load_gpt2(sess2)","6447df84":"import nltk\n\noutput_gen = gpt2.generate(sess,prefix=\"ANNULATION, sur la demande du sieur Prosper X..., avou\u00e9, d'un arr\u00eat rendu \", temperature=0.7, include_prefix=False, length=4096, return_as_list=True, nsamples=100) ","c3ee9007":"referred_list = []\nfor r in df.sample(n=100).loc[:,0]:\n    referred_list.append(r.split(\" \"))\n    \nhypotheses_list = []\nfor h in output_gen:\n    hypotheses_list.append(h.split(\" \"))","3095cf52":"nltk.translate.bleu_score.corpus_bleu(referred_list, hypotheses_list)","6fe182f6":"# !pip install happytransformer","54c6f322":"# from happytransformer import HappyGeneration, GENTrainArgs\n\n# gpt2 = HappyGeneration(\"antoiloui\/belgpt2\") ","aaa12a2b":"# train_args = GENTrainArgs(num_train_epochs=1, learning_rate=2e-05, batch_size=20, fp16=True) \n\n# gpt2.train(\".\/decision.txt\", args=train_args)","52dcb670":"# !pip install transformers","0d333d97":"# from sklearn.model_selection import train_test_split\n# train, test = train_test_split(decision,test_size=0.25) ","d21126a7":"# def build_text_file(dataset, path):\n#     text_data = open(path, 'w')\n#     data = ''\n#     for item in dataset :\n#         data += str(item) + \" \"\n#     text_data.write(data)\n#     text_data.close()\n    \n# build_text_file(train, \"decision_train.txt\")\n# build_text_file(test, \"decision_test.txt\")\n\n# print(\"Train dataset length: \"+str(len(train)))\n# print(\"Test dataset length: \"+ str(len(test)))","6b25485d":"# from transformers import AutoTokenizer\n# train_path = \"decision_train.txt\"\n# test_path = \"decision_test.txt\"\n# tokenizer = AutoTokenizer.from_pretrained(\"dbddv01\/gpt2-french-small\")","64d340ff":"# from transformers import TextDataset,DataCollatorForLanguageModeling\n\n# def load_dataset(train_path,test_path,tokenizer):\n#     train_dataset = TextDataset(\n#           tokenizer=tokenizer,\n#           file_path=train_path,\n#           block_size=128)\n     \n#     test_dataset = TextDataset(\n#           tokenizer=tokenizer,\n#           file_path=test_path,\n#           block_size=128)   \n    \n#     data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer, mlm=False,\n#     )\n#     return train_dataset,test_dataset,data_collator\n\n# train_dataset = TextDataset(\n#           tokenizer=tokenizer,\n#           file_path=train_path,\n#           block_size=128)\n#train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)","f3ec90c5":"# import datasets\n# from transformers import TextDataset,DataCollatorForLanguageModeling\n\n# train = train.dropna()\n# test = test.dropna()\n\n# def tokenize_function(examples):\n#     #print(examples[\"0\"])\n#     return tokenizer(examples[\"0\"])\n\n# train_dataset = datasets.Dataset.from_pandas(train.to_frame())\n# tokenized_dataset_train = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"0\"])\n\n\n# test_dataset = datasets.Dataset.from_pandas(test.to_frame())\n# tokenized_dataset_test = test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"0\"])\n\n# data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer, mlm=False,\n#     )","b2a42b45":"# train.to_dict()","28562f5c":"# from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n\n# model = AutoModelWithLMHead.from_pretrained(\"dbddv01\/gpt2-french-small\")\n\n\n# training_args = TrainingArguments(\n#     output_dir=\".\/gpt2-justice\", #The output directory\n#     overwrite_output_dir=True, #overwrite the content of the output directory\n#     num_train_epochs=3, # number of training epochs\n#     per_device_train_batch_size=32, # batch size for training\n#     per_device_eval_batch_size=64,  # batch size for evaluation\n#     eval_steps = 400, # Number of update steps between two evaluations.\n#     save_steps=800, # after # steps model is saved \n#     warmup_steps=500,# number of warmup steps for learning rate scheduler\n#     prediction_loss_only=True,\n#     )\n\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     data_collator=data_collator,\n#     train_dataset=train_dataset,\n#     eval_dataset=test_dataset,\n# )","11ca5b2d":"# trainer.train()","284401cf":"Les donn\u00e9es utilis\u00e9es proviennent du dataset suivant : \nhttps:\/\/www.data.gouv.fr\/en\/datasets\/cass\/\n\nElles sont fournis au format XML et le code ci-dessous permet d'extraire les donn\u00e9es qui nous int\u00e9ressent pour les stocker sous forme de CSV.","3cf6ef03":"Cette partie du notebook concerne donc l'apprentissage et la g\u00e9n\u00e9ration de texte.\nLe mod\u00e8le GPT2 a \u00e9t\u00e9 choisi et plusieurs impl\u00e9mentations ont \u00e9t\u00e9 test\u00e9s.\n\nGPT2 est un mod\u00e8le entra\u00een\u00e9 sur du texte en anglais, il va donc falloir le finetuner pour qu'il puisse g\u00e9n\u00e9rer du texte en fran\u00e7ais.\nOn va donc utiliser la version la plus l\u00e9g\u00e8re GPT2 124M comme base.\n\n### a) GPT2-Simple\ngpt2-simple est un wrapper python qui facilite le finetuning et la g\u00e9n\u00e9ration du mod\u00e8le GPT2.\nhttps:\/\/github.com\/minimaxir\/gpt-2-simple","de93a12c":"# GENERATION DE DECISION DE JUSTICE\n## I.Extraction des donn\u00e9es","441ab88a":"> ### \/!\\ Ne pas ex\u00e9cuter cette partie si les donn\u00e9es sont d\u00e9j\u00e0 transform\u00e9es\n>\n> En effet, dans ce notebook h\u00e9berg\u00e9 sur kaggle, un dataset de donn\u00e9es d\u00e9j\u00e0 transform\u00e9 est disponible : juristext.","98349b87":"### b) Hugging Face's transformer \nHappy transformer est une librairie \u00e9crite au dessus de celle de Hugging Face et permet d'utiliser diff\u00e9rents mod\u00e8les de NLP.\n\nEn utilisant la documentation de Hugging Face suivante : https:\/\/huggingface.co\/docs\/transformers\/custom_datasets#finetune-with-the-trainer-api\n\n> N'a pas abouti \u00e0 cause d'erreurs OOM","9a6107d2":"### Test des performances\nIndicateur BLEU","a8bbe152":"Le code suivant est comment\u00e9 pour pouvoir faire un commit du notebook kaggle.","3a88375e":"## II. Exploitation des donn\u00e9es","39c71849":"Test de g\u00e9n\u00e9ration de rapport de cours de cassation avec des temp\u00e9ratures diff\u00e9rentes.\nLa temp\u00e9rature permet de d\u00e9finir l'al\u00e9atoire du mod\u00e8le. D\u00e9terministe 0 -> 1 Al\u00e9atoire","ae0b7c35":"### b) Happytransformer\nHappy transformer est une librairie \u00e9crite au dessus de celle de Hugging Face et permet d'utiliser diff\u00e9rents mod\u00e8les de NLP.\nIci on a tent\u00e9 de finetuner un mod\u00e8le GPT2 pr\u00e9entrain\u00e9 pour la langue fran\u00e7aise.\n\n> N'a pas abouti \u00e0 cause d'erreurs OOM"}}