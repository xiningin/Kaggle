{"cell_type":{"08b74332":"code","fe7be39f":"code","94949644":"code","d74aafe3":"code","d14d6c72":"code","00b2da12":"code","04db1985":"code","2d0480d8":"code","8cd9bf74":"code","bc7accec":"code","e76ebd18":"code","84361a59":"markdown","109ae50a":"markdown"},"source":{"08b74332":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe7be39f":"# Import\ndf_ = pd.read_csv('..\/input\/online-retail-ii-data-set-from-ml-repository\/Year 2010-2011.csv',encoding= 'unicode_escape')\ndf = df_.copy()\ndf.head()","94949644":"# Data Preprocessing\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\ndef retail_data_prep(dataframe):\n    dataframe.drop(dataframe[dataframe[\"StockCode\"] == \"POST\"].index, inplace=True)\n    dataframe.dropna(inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"Price\"] > 0]\n    replace_with_thresholds(dataframe, \"Quantity\")\n    replace_with_thresholds(dataframe, \"Price\")\n    return dataframe\n\ndf = retail_data_prep(df)\ndf.head()","d74aafe3":"# # We'll reduce our data for a single country.\ndf_grm = df[df['Country']=='Germany']   \ndf_grm['Country'].unique()\ndf_grm.head()","d14d6c72":"# Let's start placing 'Invoice' variable in rows and 'Description' variable in columns.\ndef create_invoice_product_df(dataframe, id=False):\n    if id:\n        return dataframe.groupby(['Invoice', \"StockCode\"])['Quantity'].sum(). \\\n            unstack(). \\\n            fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n    else:\n        return dataframe.groupby(['Invoice', 'Description'])['Quantity'].sum(). \\\n            unstack(). \\\n            fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n\n# Here I created 2 optional matrix because Invoice-Description matrix's output seems badly.\n# Accordingly, I'll be proceeding with Invoice-StockCode matrix to have more elegant output.\ngrm_inv_sto_df = create_invoice_product_df(df_grm, id=True)\ngrm_inv_sto_df.head()","00b2da12":"# Creating Association Rules: We will use 'apriori' algorithm to indicate association rules.\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrequent_itemsets = apriori(grm_inv_sto_df, min_support=0.01, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\nrules.head()","04db1985":"# The problem. Which variable should I focus on?\n# Well, if we're going to take a specific action, it makes sense to focus on the 'lift' variable.\n# Because on this study, we're working on 'associations'.\n# P.S:In some studies, you may see people focusing on 'confidence'\nsorted_rules = rules.sort_values(\"lift\", ascending=False)\nsorted_rules.head(20)","2d0480d8":"def arl_recommender(rules_df, product_id, rec_count=1):\n    sorted_rules = rules_df.sort_values(\"lift\", ascending=False)\n    recommendation_list = []\n    for i, product in sorted_rules[\"antecedents\"].items():\n        for j in list(product):\n            if j == str(product_id):\n                recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"]))\n    recommendation_list = list({item for item_list in recommendation_list for item in item_list})\n    return recommendation_list[:rec_count]","8cd9bf74":"arl_recommender(rules,'21987',rec_count=3)","bc7accec":"# I will create a function to see recommended products name\ndef check_id(dataframe, stock_code):\n    product_name = dataframe[dataframe[\"StockCode\"] == stock_code][[\"Description\"]].values[0].tolist()\n    print(product_name)","e76ebd18":"check_id(df_grm, '22716')\ncheck_id(df_grm, '20675')\ncheck_id(df_grm, '22728')","84361a59":"#### Here, we will recommend our customers a new product. Therefore, we should find a mathematical way to handle the issue.\n#### Our main goal will be to create invoice-description matrix which displays existence and non-existence of any products in baskets.\n#### To sum up, we will create a binary matrix containing 0 and 1.","109ae50a":"## Output explanation\n##### antecendents: id of the first product or products\n##### consequents: id of the second product or products\n##### antecedent support: probability of first product being observed alone\n##### consequent support: probability of the second product being observed alone\n##### support: probability of 2 products being observed together\n##### confidence: probability of buying 'second' when 'first' is bought.\n##### lift: increase in probability of buying product 'second' when 'first' is bought.\n##### leverage:Leverage effect. Similar to lift but it tends to prioritize higher 'support' values therefore it is biased\n##### conviction: expected frequency (expectation) of 'first' without 'second'"}}