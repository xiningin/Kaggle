{"cell_type":{"2e9bc849":"code","27b894f5":"code","74a4fc94":"code","a73bfa50":"code","c05144c9":"code","14c1128d":"code","69efd06e":"code","d3ef93d8":"code","6ca14028":"code","7e23e8f4":"code","e21a1ee4":"code","b7aa7799":"code","d41354d4":"code","b71ef85c":"code","b6170dd3":"code","a3af0db0":"code","5b320421":"code","4860f001":"code","851ef95b":"code","8a7ea693":"code","373eeffe":"code","dcac9de7":"code","773bdc8c":"code","5c8569c6":"code","f50a7344":"code","7f0f8004":"code","fcfe71dc":"code","4bba1ff8":"code","3c68b8f2":"code","e2f9c952":"code","56baefda":"code","35b6fd6b":"code","1dac9fcf":"code","565353a5":"code","08a00b41":"code","e075022c":"code","9e22f19c":"code","883233ab":"code","ab8e92ea":"code","61011feb":"code","36dd130e":"code","f713fe40":"code","eef0df81":"code","a9f41b86":"code","c8dd5b8b":"code","63f4db73":"code","2c389e81":"code","9103770c":"code","32f3f03a":"code","5c0aff93":"code","e0ce2614":"code","9df52b27":"markdown","c307e586":"markdown","337c5d85":"markdown","29fc40ef":"markdown","aaeddf0a":"markdown","29732a09":"markdown","122f1c6a":"markdown","0c6bfecd":"markdown","b07443a6":"markdown","dd190c85":"markdown","4450ae02":"markdown","20593f9c":"markdown","da8022be":"markdown","d1c4407c":"markdown","74084445":"markdown","4f3e93c6":"markdown","f93c28dd":"markdown","d97b7f28":"markdown","c9f68fb3":"markdown","0ee83322":"markdown","731ef0eb":"markdown"},"source":{"2e9bc849":"# library \nimport numpy as np \nimport pandas as pd \nimport itertools\nfrom scipy import interp\nimport os\nimport datetime\nimport gc\nimport json\nfrom numba import jit\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport time\nfrom contextlib import contextmanager\nimport psutil\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n# ML\nimport lightgbm as lgb\n\n# options\npd.set_option('display.max_columns', 500)","27b894f5":"timer_depth = -1\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    global timer_depth\n    timer_depth += 1\n    yield\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memoryUse = py.memory_info()[0] \/ 2. ** 30\n    print('----'*timer_depth + f'>>[{name}] done in {time.time() - t0:.0f} s ---> memory used: {memoryUse:.4f} GB', '')\n    if(timer_depth == 0):\n        print('\\n')\n    timer_depth -= 1","74a4fc94":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","a73bfa50":"# read\ntrain_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\ntrain_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\nsub = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\n\n# merge \ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\n# reduce_mem_usage\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","c05144c9":"del train_transaction, train_identity, test_transaction, test_identity\n\nprint(\"Train shape : \"+str(train.shape))\nprint(\"Test shape  : \"+str(test.shape))","14c1128d":"# sampling \n\n# train = train.sample(5000)\n# test = test.sample(5000)","69efd06e":"booleanDictionary = {'T':True, 'F':False}\ntrain = train.replace(booleanDictionary)\ntest = test.replace(booleanDictionary)","d3ef93d8":"train['nulls1'] = train.isna().sum(axis=1)\ntest['nulls1'] = test.isna().sum(axis=1)","6ca14028":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n\nfor df in [train, test]:\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds=x)))\n    df['DT_M'] = (df['DT'].dt.year - 2017) * 12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year - 2017) * 52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year - 2017) * 365 + df['DT'].dt.dayofyear\n\n    df['hour'] = df['DT'].dt.hour\n    df['dow'] = df['DT'].dt.dayofweek\n    df['day'] = df['DT'].dt.day","7e23e8f4":"def values_normalization(dt_df, periods, columns):\n    for period in periods:\n        for col in columns:\n            new_col = col +'_'+ period\n            dt_df[col] = dt_df[col].astype(float)  \n\n            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n            temp_min.index = temp_min[period].values\n            temp_min = temp_min['min'].to_dict()\n\n            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n            temp_max.index = temp_max[period].values\n            temp_max = temp_max['max'].to_dict()\n\n            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n            temp_mean.index = temp_mean[period].values\n            temp_mean = temp_mean['mean'].to_dict()\n\n            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n            temp_std.index = temp_std[period].values\n            temp_std = temp_std['std'].to_dict()\n\n            dt_df['temp_min'] = dt_df[period].map(temp_min)\n            dt_df['temp_max'] = dt_df[period].map(temp_max)\n            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n            dt_df['temp_std'] = dt_df[period].map(temp_std)\n\n            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])\/(dt_df['temp_max']-dt_df['temp_min'])\n            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])\/(dt_df['temp_std'])\n            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n    return dt_df\n\n# D1 ~ D15\ni_cols = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']\nfor col in i_cols:\n    df = pd.concat([train[[col,'DT_M']],test[[col,'DT_M']]])\n    df = values_normalization(df,['DT_M'],[col])\n    print(df.head())\n    train = pd.concat([train,df.iloc[:len(train),len(df.columns)-2:]],axis=1)\n    test = pd.concat([test,df.iloc[len(train):,len(df.columns)-2:]],axis=1) \n    del df\n    gc.collect() ","e21a1ee4":"def setbrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\ntrain = setbrowser(train)\ntest = setbrowser(test)","b7aa7799":"def fe_email(df):   \n    \n    emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n    us_emails = ['gmail', 'net', 'edu']\n    \n    df['P_email'] = (df['P_emaildomain']=='xmail.com')\n    df['R_email'] = (df['R_emaildomain']=='xmail.com')\n    \n    df['P_isproton'] = (df['P_emaildomain']=='protonmail.com')\n    df['R_isproton'] = (df['R_emaildomain']=='protonmail.com')\n\n    df['email_check'] = np.where(df['P_emaildomain']==df['R_emaildomain'],1,0)\n    df['email_check_nan_all'] = np.where((df['P_emaildomain'].isna())&(df['R_emaildomain'].isna()),1,0)\n    df['email_check_nan_any'] = np.where((df['P_emaildomain'].isna())|(df['R_emaildomain'].isna()),1,0)    \n    df['email_match_not_nan'] = np.where( (df['P_emaildomain']==df['R_emaildomain']) & (np.invert(df['P_emaildomain'].isna())) ,1,0)\n    \n    df['P_emaildomain_bin'] = df['P_emaildomain'].map(emails)    \n    df['P_emaildomain_suffix'] = df['P_emaildomain'].map(lambda x: str(x).split('.')[-1])    \n    df['P_emaildomain_suffix'] = df['P_emaildomain_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df['P_emaildomain_prefix'] = df['P_emaildomain'].map(lambda x: str(x).split('.')[0])   \n\n    df['R_emaildomain_bin'] = df['R_emaildomain'].map(emails)    \n    df['R_emaildomain_suffix'] = df['R_emaildomain'].map(lambda x: str(x).split('.')[-1])    \n    df['R_emaildomain_suffix'] = df['R_emaildomain_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df['R_emaildomain_prefix'] = df['R_emaildomain'].map(lambda x: str(x).split('.')[0])   \n    \n    return df\n\ntrain = fe_email(train)\ntest = fe_email(test)","d41354d4":"i_cols = ['TransactionID','card1','card2','card3','card4','card5','card6']\n\nfull_df = pd.concat([train[i_cols], test[i_cols]])\n\n## I've used frequency encoding before so we have ints here\n## we will drop very rare cards\nfull_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\nfull_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n\ni_cols = ['card2','card3','card4','card5','card6']\n\n## We will find best match for nan values and fill with it\nfor col in i_cols:\n    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False).reset_index(drop=True)\n    del temp_df['count']\n    temp_df = temp_df.drop_duplicates(keep='first').reset_index(drop=True)\n    temp_df.index = temp_df['card1'].values\n    temp_df = temp_df[col].to_dict()\n    full_df[col] = np.where(full_df[col].isna(), full_df['card1'].map(temp_df), full_df[col])  \n    \ni_cols = ['card1','card2','card3','card4','card5','card6']\nfor col in i_cols:\n    train[col] = full_df[full_df['TransactionID'].isin(train['TransactionID'])][col].values\n    test[col] = full_df[full_df['TransactionID'].isin(test['TransactionID'])][col].values","b71ef85c":"del full_df\ngc.collect()","b6170dd3":"card1_thred = 2","a3af0db0":"# Reset values for \"noise\" card1\nvalid_card = train['card1'].value_counts()\nvalid_card = valid_card[valid_card>card1_thred]\nvalid_card = list(valid_card.index)\n    \ntrain['card1'] = np.where(train['card1'].isin(valid_card), train['card1'], np.nan)\ntest['card1']  = np.where(test['card1'].isin(valid_card), test['card1'], np.nan)","5b320421":"# card3\/5 low freq values \ntrain.loc[train.card3.isin(train.card3.value_counts()[train.card3.value_counts() < 200].index), 'card3'] = \"Others\"\ntest.loc[test.card3.isin(test.card3.value_counts()[test.card3.value_counts() < 200].index), 'card3'] = \"Others\"\n\ntrain.loc[train.card5.isin(train.card5.value_counts()[train.card5.value_counts() < 300].index), 'card5'] = \"Others\"\ntest.loc[test.card5.isin(test.card5.value_counts()[test.card5.value_counts() < 300].index), 'card5'] = \"Others\"","4860f001":"train['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n\ntrain['uid4'] = train['uid3'].astype(str)+'_'+train['P_emaildomain'].astype(str)\ntest['uid4'] = test['uid3'].astype(str)+'_'+test['P_emaildomain'].astype(str)\n\ntrain['uid5'] = train['uid3'].astype(str)+'_'+train['R_emaildomain'].astype(str)\ntest['uid5'] = test['uid3'].astype(str)+'_'+test['R_emaildomain'].astype(str)\n","851ef95b":"train['bank_type'] = train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['bank_type']  = test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['address_match'] = train['bank_type'].astype(str)+'_'+train['addr2'].astype(str)\ntest['address_match']  = test['bank_type'].astype(str)+'_'+test['addr2'].astype(str)\n\nfor col in ['address_match','bank_type']:\n    tmp = pd.concat([train[[col]], test[[col]]])\n    tmp[col] = np.where(tmp[col].str.contains('nan'), np.nan, tmp[col])\n    tmp = tmp.dropna()\n    fq_encode = tmp[col].value_counts().to_dict()   \n    train[col] = train[col].map(fq_encode)\n    test[col]  = test[col].map(fq_encode)\n\ntrain['address_match'] = train['address_match']\/train['bank_type'] \ntest['address_match']  = test['address_match']\/test['bank_type']","8a7ea693":"train['D9_na'] = np.where(train['D9'].isna(), 0, 1)\ntest['D9_na'] = np.where(test['D9'].isna(), 0, 1)\n\ntrain['local_hour'] = train['D9']*24\ntest['local_hour']  = test['D9']*24\n\ntrain['local_hour'] = train['local_hour'] - (train['TransactionDT']\/(60*60))%24\ntest['local_hour']  = test['local_hour'] - (test['TransactionDT']\/(60*60))%24\n\ntrain['local_hour_dist'] = train['local_hour']\/train['dist2']\ntest['local_hour_dist']  = test['local_hour']\/test['dist2']","373eeffe":"i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\ntrain['M_sum'] = train[i_cols].sum(axis=1).astype(np.int8)\ntest['M_sum']  = test[i_cols].sum(axis=1).astype(np.int8)\n\ntrain['M_na'] = train[i_cols].isna().sum(axis=1).astype(np.int8)\ntest['M_na']  = test[i_cols].isna().sum(axis=1).astype(np.int8)","dcac9de7":"i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\ntrain['M_type'] = train[i_cols].astype(str).apply(lambda x: '_'.join(x), axis=1)\ntest['M_type'] = test[i_cols].astype(str).apply(lambda x: '_'.join(x), axis=1)","773bdc8c":"def id_split(df):\n    df['device_name'] = df['DeviceInfo'].str.split('\/', expand=True)[0]\n    df['device_version'] = df['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]\n\n    df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n    df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n\n    df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n    df['id_23'] = df['id_23'].str.split(':', expand=True)[1]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain = id_split(train)\ntest = id_split(test)","5c8569c6":"for df in [train, test]:\n    # Device info\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n\n    # Device info 2\n    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n\n    # Browser\n    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))","f50a7344":"train['TransactionAmt_log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_log'] = np.log(test['TransactionAmt'])","7f0f8004":"train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)","fcfe71dc":"train['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\ntest['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)","4bba1ff8":"i_cols = [\n    'id_02__id_20', \n    'id_02__D8', \n    'D11__DeviceInfo', \n    'DeviceInfo__P_emaildomain', \n    'P_emaildomain__C2', \n    'card2__dist1', \n    'card1__card5', \n    'card2__id_20', \n    'card5__P_emaildomain', \n    'addr1__card1'    \n]\n\nfor feature in i_cols:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = preprocessing.LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","3c68b8f2":"i_cols = [\n    'card1','card2','card3','card5','card4',\n    'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n    'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n    'addr1','addr2',\n    'dist1','dist2',\n    'P_emaildomain', 'R_emaildomain',\n    # 'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09',\n    # 'id_10','id_11','id_13','id_14','id_17','id_18','id_19',\n    # 'id_20','id_21','id_22','id_24','id_25','id_26',\n    # 'id_30','id_31','id_32','id_33','id_34','id_35','id_36',\n    'DeviceInfo', 'DeviceInfo_device', 'DeviceInfo_version',\n    'id_30', 'id_30_device', 'id_30_version',\n    'id_31_device',\n    'id_33',\n    'uid', 'uid2', 'uid3','uid4', 'uid5'\n]\n\nfor feature in i_cols:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))","e2f9c952":"# for feature in ['id_01', 'id_31', 'id_33', 'id_36', 'id_35']:\n#    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n#    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","56baefda":"i_cols = [\n    'ProductCD',\n    'M4', \n    'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType'    \n]\n\nfor col in i_cols:\n    temp_dict = train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train[col+'_target_mean'] = train[col].map(temp_dict)\n    test[col+'_target_mean']  = test[col].map(temp_dict)","35b6fd6b":"def response_rate_by_group(df,x,y):\n    tmp = pd.crosstab(df[x],df[y])\n    tmp['Sum'] = tmp.apply(np.sum,axis=1)\n    tmp['Response_rate'] = tmp.loc[:,1]\/tmp['Sum']\n    tmp = tmp.sort_values(['Response_rate'],ascending=False)\n    print(\"It would be interesting to see if the amount percentual is higher or lower than 3.5% of total!\")\n    return(tmp)\n\n# print(response_rate_by_group(df=train, x=\"card3\", y=\"isFraud\"))\n# print(response_rate_by_group(df=train, x=\"card5\", y=\"isFraud\"))\n\ntrain['card3_high_rate_fraud'] = np.where(train['card3'].isin([185,119,144]),1,0)\ntest['card3_high_rate_fraud'] = np.where(test['card3'].isin([185,119,144]),1,0)\n\ntrain['card5_high_rate_fraud'] = np.where(train['card5'].isin([137,147,141,223,138]),1,0)\ntest['card5_high_rate_fraud'] = np.where(test['card5'].isin([137,147,141,223,138]),1,0)\n\ntrain['card5_137'] = np.where(train['card5'].isin([137]),1,0)\ntest['card5_137'] = np.where(test['card5'].isin([137]),1,0)\n\ntrain['day'] = np.where(train['day'].isin([1,29,30,31]),1,0)\ntest['day'] = np.where(test['day'].isin([1,29,30,31]),1,0)\n\ntrain['hour_from6_to10'] = np.where(train['hour'].isin([6,7,8,9,10]),1,0)\ntest['hour_from6_to10'] = np.where(test['hour'].isin([6,7,8,9,10]),1,0)\n\ntrain['hour_from5_to11'] = np.where(train['hour'].isin([5,6,7,8,9,10,11]),1,0)\ntest['hour_from5_to11'] = np.where(test['hour'].isin([5,6,7,8,9,10,11]),1,0)\n\ntrain['hour_from4_to12'] = np.where(train['hour'].isin([4,5,6,7,8,9,10,11,12]),1,0)\ntest['hour_from4_to12'] = np.where(test['hour'].isin([4,5,6,7,8,9,10,11,12]),1,0)\n\ntrain['P_emaildomain_mail'] = np.where(train['P_emaildomain'].isin(['mail.com']),1,0)\ntest['P_emaildomain_mail'] = np.where(test['P_emaildomain'].isin(['mail.com']),1,0)\n\ntrain['R_emaildomain_icloud'] = np.where(train['R_emaildomain'].isin(['icloud.com']),1,0)\ntest['R_emaildomain_icloud'] = np.where(test['R_emaildomain'].isin(['icloud.com']),1,0)\n\ntrain['addr1_251'] = np.where(train['addr1'].isin([251]),1,0)\ntest['addr1_251'] = np.where(test['addr1'].isin([251]),1,0)\n\ntrain['addr2_65'] = np.where(train['addr2'].isin([65]),1,0)\ntest['addr2_65'] = np.where(test['addr2'].isin([65]),1,0)\n\ntrain['C1_0'] = np.where(train['C1'].isin([0]),1,0)\ntest['C1_0'] = np.where(test['C1'].isin([0]),1,0)\n\ntrain['C1_19'] = np.where(train['C1'].isin([19]),1,0)\ntest['C1_19'] = np.where(test['C1'].isin([19]),1,0)","1dac9fcf":"i_cols = ['card1','card2','card3','card5','uid', 'uid2', 'uid3','uid4', 'uid5']\nfor col in i_cols:\n    for agg_type in ['mean', 'std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        tmp = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        tmp = tmp.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})        \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()       \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)\n        \ni_cols = ['card1','card2','card3','card5','uid', 'uid2', 'uid3','uid4', 'uid5','addr1','addr2']\nfor col in i_cols:\n    for agg_type in ['mean', 'std']:\n        new_col_name = col+'_'+'D15'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'D15']], test[[col,'D15']]])\n        tmp = tmp.groupby([col])['D15'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()       \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)\n        \ni_cols = ['card1','card4']\nfor col in i_cols:\n    for agg_type in ['mean', 'std']:\n        new_col_name = col+'_'+'id_02'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'id_02']], test[[col,'id_02']]])\n        tmp = tmp.groupby([col])['id_02'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})       \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()       \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)\n        \ni_cols = ['card1','card2','card3','card4','card5','uid', 'uid2', 'uid3','uid4', 'uid5']\nfor col in i_cols:\n    for agg_type in ['mean', 'std']:\n        new_col_name = col+'_'+'TransactionAmt_log'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'TransactionAmt_log']], test[[col,'TransactionAmt_log']]])\n        tmp = tmp.groupby([col])['TransactionAmt_log'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()       \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)","565353a5":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')","08a00b41":"for col in ['DT_M', 'DT_W', 'DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n\n    train[col + '_total'] = train[col].map(fq_encode)\n    test[col + '_total'] = test[col].map(fq_encode)\n\nperiods = ['DT_M', 'DT_W', 'DT_D']\n\ni_cols = ['uid', 'uid2', 'uid3','uid4', 'uid5']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n\n        temp_df = pd.concat([train[[col, period]], test[[col, period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n\n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column] = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n\n        train[new_column] \/= train[period + '_total']\n        test[new_column] \/= test[period + '_total']","e075022c":"def cal_woe(train, test, train_target,i_cols):\n    num_events = train_target.sum()\n    num_non_events = train_target.shape[0] - train_target.sum()\n\n    feature_list = []\n    feature_iv_list = []\n    for col in i_cols:\n        with timer('cope with %s' % col):\n            feature_list.append(col)\n\n            woe_df = pd.DataFrame()\n            woe_df[col] = train[col]\n            woe_df['target'] = train_target\n            events_df = woe_df.groupby(col)['target'].sum().reset_index().rename(columns={'target' : 'events'})\n            events_df['non_events'] = woe_df.groupby(col).count().reset_index()['target'] - events_df['events']\n            def cal_woe(x):\n                return np.log( ((x['non_events']+0.5)\/num_non_events) \/ ((x['events']+0.5)\/num_events)  )\n            events_df['WOE_'+col] = events_df.apply(cal_woe, axis=1)\n\n            def cal_iv(x):\n                return x['WOE_'+col]*(x['non_events'] \/ num_non_events - x['events'] \/ num_events)\n            events_df['IV_'+col] = events_df.apply(cal_iv, axis=1)\n\n            feature_iv = events_df['IV_'+col].sum()\n            feature_iv_list.append(feature_iv)\n\n            events_df = events_df.drop(['events', 'non_events', 'IV_'+col], axis=1)\n            train = train.merge(events_df, how='left', on=col)\n            test = test.merge(events_df, how='left', on=col)\n            \n    return train, test\n\ni_cols = [\n    'ProductCD',\n    'M4', \n    'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType'    \n]\n\n# for col in i_cols:\n#    train, test = cal_woe(train, test, train['isFraud'], [col])","9e22f19c":"prefix = ['C','D','Device','M','Transaction','V','addr','card','dist','id']\nfor i, p in enumerate(prefix):\n    \n    column_set = [x for x in train.columns.tolist() if x.startswith(prefix[i])]\n    print(column_set)\n\n    # Take NA count\n    train[p + \"group_nan_sum\"] = train[column_set].isnull().sum(axis=1) \/ train[column_set].shape[1]\n    test[p + \"group_nan_sum\"] = test[column_set].isnull().sum(axis=1) \/ test[column_set].shape[1]\n\n    # Take SUM\/Mean if numeric\n    numeric_cols = [x for x in column_set if train[x].dtype != object]\n    print(numeric_cols)\n    if numeric_cols:\n        train[p + \"group_sum\"] = train[column_set].sum(axis=1)\n        test[p + \"group_sum\"] = test[column_set].sum(axis=1)\n        train[p + \"group_mean\"] = train[column_set].mean(axis=1)\n        test[p + \"group_mean\"] = test[column_set].mean(axis=1)\n        # Zero Count\n        train[p + \"group_0_count\"] = (train[column_set] == 0).astype(int).sum(axis=1) \/ (train[column_set].shape[1] - train[p + \"group_nan_sum\"])\n        test[p + \"group_0_count\"] = (test[column_set] == 0).astype(int).sum(axis=1) \/ (test[column_set].shape[1] - test[p + \"group_nan_sum\"])","883233ab":"# fill in mean for floats\n# for c in train.columns:\n#     if train[c].dtype=='float16' or  train[c].dtype=='float32' or  train[c].dtype=='float64':\n#         train[c].fillna(train[c].mean())\n#         train[c].fillna(train[c].mean())\n\n# fill in -999 for categoricals\n# train = train.fillna(-999)\n# test = test.fillna(-999)","ab8e92ea":"# Encode Str columns\nfor col in list(train):\n    if train[col].dtype=='O':\n        print(col)\n        train[col] = train[col].fillna('unseen_before_label')\n        test[col]  = test[col].fillna('unseen_before_label')\n        \n        train[col] = train[col].astype(str)\n        test[col] = test[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train[col])+list(test[col]))\n        train[col] = le.transform(train[col])\n        test[col]  = le.transform(test[col])\n        \n        #train[col] = train[col].astype('category')\n        #test[col] = test[col].astype('category')","61011feb":"\"\"\"\nle = LabelEncoder()\nfor col in train.select_dtypes(include=['object', 'category']).columns:\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))\n\"\"\"","36dd130e":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() \/ train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() \/ test.shape[0] > 0.9]\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n\nprint(\">> many_null_cols :\",many_null_cols)\nprint(\">> big_top_value_cols :\",big_top_value_cols)\nprint(\">> one_value_cols :\",one_value_cols)\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols + one_value_cols_test))\ncols_to_drop.remove('isFraud')\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)\n\nprint(\">> num of cols_to_drop :\",len(cols_to_drop))","f713fe40":"train.to_csv(\"train.csv\", index=False)\ntest.to_csv(\"test.csv\", index=False)","eef0df81":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","a9f41b86":"rm_cols = [\n    'TransactionID',\n    'uid', 'uid2', 'uid3','uid4', 'uid5',\n    'bank_type',  \n    'DT', 'DT_M', 'DT_W', 'DT_D',  # Temporary Variables\n]\n\ntrain = train.drop(rm_cols, axis=1)\ntest = test.drop(rm_cols, axis=1)","c8dd5b8b":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\nX_test = test.drop(['TransactionDT'], axis=1)","63f4db73":"del train, test\ngc.collect()","2c389e81":"params = {\n    'objective':'binary',\n    'boosting_type':'gbdt',\n    'metric':'auc',\n    'n_jobs':-1,\n    'learning_rate':0.01,\n    'num_leaves': 2**8,\n    'max_depth':-1,\n    'tree_learner':'serial',\n    'colsample_bytree': 0.85,\n    'subsample_freq':1,\n    'subsample':0.85,\n    'max_bin':255,\n    'verbose':-1,\n    'seed': 47,\n    'n_estimators':10000, # 800\n    'early_stopping_rounds':500, # 100 \n    'reg_alpha':0.3,\n    'reg_lamdba':0.243\n} ","9103770c":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, valid_sets = [dtrain, dvalid], verbose_eval=200)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","32f3f03a":"print(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","5c0aff93":"sub['isFraud'] = y_preds\nsub.to_csv(\"submission.csv\", index=False)","e0ce2614":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature')\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits))","9df52b27":"### FE : TransactionAmt","c307e586":"### FE : card ","337c5d85":"### FE : email","29fc40ef":"### FE : device ","aaeddf0a":"### FE : WoE","29732a09":"> ### FE : id_31","122f1c6a":"### FE : user count per period","0c6bfecd":"### FE : repalce missing values","b07443a6":"### FE : feature aggregation ","dd190c85":"### Model","4450ae02":"### FE : missing value","20593f9c":"### FE : D9 (hour)","da8022be":"### FE : drop features ","d1c4407c":"### read dataset","74084445":"### FE : two featrues label encoding","4f3e93c6":"### FE : character feature encoding ","f93c28dd":"### FE : ProductCD and M4 Target mean","d97b7f28":"### FE : M1 ~ M9 (binary encoding, except M4)","c9f68fb3":"### FE : time of day","0ee83322":"### FE : indicator features","731ef0eb":"### FE : count encoding"}}