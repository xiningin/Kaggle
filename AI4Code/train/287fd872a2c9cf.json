{"cell_type":{"a74f2102":"code","22633143":"code","b95251fd":"code","f34f2c54":"code","a7681443":"code","f24b8c4c":"code","3cbd0d02":"code","87e9dea4":"code","78f1f04a":"code","298b9123":"code","c815e766":"code","62528196":"code","ca2be8cc":"code","3f02d2fc":"code","1fb18957":"code","013c32f8":"code","60e6dc5f":"code","d85372bc":"code","02d9cde9":"code","6731bd83":"code","cc70aedb":"code","77b948d1":"code","631d4789":"code","e06361fc":"code","36cfe48c":"code","80a59195":"code","fa9455ec":"markdown","43fdc36e":"markdown","55bb1d33":"markdown","a95de212":"markdown","cd39eda5":"markdown","066f1e27":"markdown","5a0e0e31":"markdown","c28767b2":"markdown","8950d05a":"markdown","7e989f6e":"markdown","51becc00":"markdown","724c0b21":"markdown","b476e375":"markdown","2b138cb2":"markdown","e7d41c17":"markdown","218b5a0a":"markdown","db7c7aea":"markdown","23f12b9d":"markdown"},"source":{"a74f2102":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom scipy.stats import probplot\nfrom fbprophet import Prophet\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","22633143":"os.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/train_2.csv.zip')\nos.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip')\nos.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/key_2.csv.zip')\nos.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/key_1.csv.zip')\nos.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_2.csv.zip')\nos.system('unzip -d \/kaggle\/input \/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_1.csv.zip')","b95251fd":"folder = '..\/input\/'\n\ntrain2 = pd.read_csv(folder+'train_2.csv')","f34f2c54":"train2 = train2.join(train2['Page'].str.rsplit('_',n=3,expand=True)).rename(columns={0:'article',1:'source',2:'access',3:'agent'})\ntrain2 = train2.drop('Page', axis=1)\ntrain2.info()","a7681443":"# %%time\n\n# rcParams['figure.figsize'] = 10, 5\n\n# count = 0\n# idxs = []\n# for i in range(len(train2)):\n#     _row = train2.iloc[i]\n#     if sum(_row.isnull())>0:\n#         count += 1\n#         idxs.append(i)\n# #         if count>5:\n# #             break\n# #print(len(idxs),idxs)\n\n# _tmp = train2\n# for ii,idx in enumerate(idxs):\n#     print(ii,len(idxs))\n#     features,values=[],[]\n#     dss,ys,idxs_null = [],[],[]\n#     # reshape row to dataframe\n#     for col,value in train2.iloc[idx].items():\n#         if col in ['Page','article','source','access','agent']:\n#             features.append(col)\n#             values.append(value)\n#             continue\n#         dss.append(col)\n#         ys.append(value)\n#         if np.isnan(value):\n#             idxs_null.append(len(ys)-1)\n#     _row = pd.DataFrame({'ds':dss,'y':ys})\n#     #print(len(_row))\n#     #print(idxs_null[-1])\n#     #_first = idxs_null[-1]+1\n#     #_row_train = _row[_first:]\n#     #_row_test = _row[:_first]\n#     #print(len(_row_train),len(_row_test))\n#     m = Prophet()\n#     m.fit(_row)\n#     #forecast = m.predict(pd.concat([_row_test[['ds']],_row_train[['ds']]]))\n#     forecast = m.predict(_row[['ds']])\n#     #m.plot(forecast)\n#     for i in idxs_null:\n#         ys[i] = forecast[forecast.ds==_row.iloc[i].ds].iloc[0].yhat\n#     _row = pd.Series(values+ys, index=features+dss)\n#     #print(sum(_row.isnull()))\n#     _tmp.iloc[idx] = _row\n#     #break\n\n# train2 = _tmp\n\n# for i in idxs:\n#     _row = _tmp.iloc[i]\n#     if sum(_row.isnull())>0:\n#         print(sum(_row.isnull()))","f24b8c4c":"cols = list(set(train2.columns.tolist())-set(['article','source','access','agent']))\ntrain2[cols] = train2[cols].fillna(method='bfill', axis=1)\ntrain2[cols] = train2[cols].fillna(method='ffill', axis=1)","3cbd0d02":"train2 = train2.fillna(0)","87e9dea4":"# for col in train2.columns:\n#     if train2[col].dtype != 'object':\n#         train2[col] = train2[col].clip(0)\ntrain2[cols] = train2[cols].clip(0)","78f1f04a":"# \u90e8\u5206\u5217\u505a\u7c7b\u578b\u8f6c\u6362\ntrain2[['article','source','access','agent']] = train2[['article','source','access','agent']].astype('category')\ntrain2[cols] = train2[cols].apply(pd.to_numeric, downcast='unsigned')\ntrain2.info()","298b9123":"visits = train2[cols].stack().reset_index(level=1)\nvisits.columns = ['date','visits']\n#visits.date = visits.date.astype(np.datetime64)\nvisits.date = visits.date.astype('category')\nvisits.visits = visits.visits.astype(np.int32)\nvisits.info()","c815e766":"train2 = train2.drop(cols, axis=1).join(visits)\ndel visits\ntrain2.info(memory_usage='deep')","62528196":"# import seaborn as sns\n# sns.set(style=\"darkgrid\")\n# sns.lineplot(x=\"date\", y=\"visits\", hue=\"article\", \n#              data=train2[['article','date','visits']][(train2.article=='\u8463\u5b50\u5065')|(train2.article=='\u4f55\u5ee3\u6c9b')|(train2.article=='\u674e\u5b97\u4f1f')])","ca2be8cc":"# # 2017-09-11, 2017-09-12\n# for k,group in train2.groupby(['article','source','access','agent']):\n# #for k,group in train2.groupby('article'):\n#     _article,_source,_access,_agent,_visits = k[0],k[1],k[2],k[3],0\n#     #_date1,_date2 = np.datetime64('2017-09-11'),np.datetime64('2017-09-12')\n#     _date1 = np.datetime64('2017-11-13')\n#     train2 = train2.append({'article':_article,'source':_source,'access':_access,'agent':_agent,'visits':_visits,'date':_date1}, ignore_index=True)\n#     #train2 = train2.append({'article':_article,'source':_source,'access':_access,'agent':_agent,'visits':_visits,'date':_date2}, ignore_index=True)\n\n# 2017-09-13 ~ 2017-11-13\nkey2 = pd.read_csv(folder+'key_2.csv')\nkey2 = key2.join(key2['Page'].str.rsplit('_',n=4,expand=True)).rename(columns={0:'article',1:'source',2:'access',3:'agent',4:'date'})\nkey2 = key2.drop(['Page','Id'], axis=1)\nkey2['visits'] = 0\nkey2.visits = key2.visits.astype(np.int8)\n# \u90e8\u5206\u5217\u505a\u7c7b\u578b\u8f6c\u6362\nkey2[['date','article','source','access','agent']] = key2[['date','article','source','access','agent']].astype('category')\nkey2.info(memory_usage='deep')","3f02d2fc":"import gc\ngc.collect()","1fb18957":"from pandas.api.types import union_categoricals\n\n# matrix = pd.DataFrame({})\n# matrix['article'] = pd.Series(union_categoricals([train2.article,key2.article]))\n# matrix['source'] = pd.Series(union_categoricals([train2.source,key2.source]))\n# matrix['access'] = pd.Series(union_categoricals([train2.access,key2.access]))\n# matrix['agent'] = pd.Series(union_categoricals([train2.agent,key2.agent]))\n# matrix['date'] = pd.concat([train2.date,key2.date])\n# matrix['visits'] = pd.concat([train2.visits,key2.visits])\n# del key2,train2\n# matrix.info()\n\nmatrix = pd.DataFrame({})\nprint('article ....')\nmatrix['article'] = pd.Series(union_categoricals([train2.article,key2.article]))\ndel train2['article'],key2['article']\nprint('source ....')\nmatrix['source'] = pd.Series(union_categoricals([train2.source,key2.source]))\ndel train2['source'],key2['source']\nprint('access ....')\nmatrix['access'] = pd.Series(union_categoricals([train2.access,key2.access]))\ndel train2['access'],key2['access']\nprint('agent ....')\nmatrix['agent'] = pd.Series(union_categoricals([train2.agent,key2.agent]))\ndel train2['agent'],key2['agent']\nprint('date ....')\nmatrix['date'] = pd.Series(union_categoricals([train2.date,key2.date]))\nmatrix.date = matrix.date.astype('datetime64')\ndel train2['date'],key2['date']\nprint('visits ....')\nmatrix['visits'] = train2.visits.append(key2.visits, ignore_index=True)\ndel train2['visits'],key2['visits']\ndel train2,key2\nprint('show info ....')\nmatrix.info(memory_usage='deep')","013c32f8":"print(len(matrix[matrix.article=='2NE1'].access.unique()))\nprint(len(matrix[matrix.article=='2NE1'].agent.unique()))\nprint(len(matrix[matrix.article=='2NE1'].source.unique()))\nprint(len(matrix[matrix.article=='2NE1'].date.unique()))\nprint(len(matrix[matrix.article=='2NE1'].visits.unique()))\nprint(len(matrix[matrix.article=='2NE1']))","60e6dc5f":"matrix.sample(10)","d85372bc":"# %%time\n# def split_date(df):\n#     df['year'] = df.date.apply(lambda dt:dt.year).astype(np.int16)\n#     df['month'] = df.date.apply(lambda dt:dt.month).astype(np.int8)\n#     df['quarter'] = df.date.apply(lambda dt:dt.quarter).astype(np.int16)\n#     df['day'] = df.date.apply(lambda dt:dt.day).astype(np.int8)\n#     df['dayofweek'] = df.date.apply(lambda dt:dt.dayofweek).astype(np.int8)\n#     df['dayofyear'] = df.date.apply(lambda dt:dt.dayofyear).astype(np.int16)\n#     df['weekofyear'] = df.date.apply(lambda dt:dt.weekofyear).astype(np.int16)\n#     df['is_month_start'] = df.date.apply(lambda dt:dt.is_month_start).astype(np.int16)\n#     df['is_month_end'] = df.date.apply(lambda dt:dt.is_month_end).astype(np.int16)\n#     df['is_quarter_start'] = df.date.apply(lambda dt:dt.is_quarter_start).astype(np.int16)\n#     df['is_quarter_end'] = df.date.apply(lambda dt:dt.is_quarter_end).astype(np.int16)\n#     df['is_year_start'] = df.date.apply(lambda dt:dt.is_year_start).astype(np.int16)\n#     df['is_year_end'] = df.date.apply(lambda dt:dt.is_year_end).astype(np.int16)\n\n# split_date(matrix)","02d9cde9":"# first_date = matrix.date.min()\n# matrix['day_block_num'] = (matrix.date - first_date).apply(lambda delta:delta.days).astype(np.int16)","6731bd83":"from sklearn.preprocessing import LabelEncoder\n\nles = {}\nfor col in ['article','source','access','agent']:\n    les[col] = LabelEncoder().fit(matrix[col])\n    matrix[col] = les[col].transform(matrix[col])\n    if col == 'article':\n        matrix['article'] = matrix['article'].astype('int16')\n    else:\n        matrix[col] = matrix[col].astype('int8')","cc70aedb":"# matrix['article'] = matrix['article'].astype('int16')\n# matrix['source'] = matrix['source'].astype('int8')\n# matrix['access'] = matrix['access'].astype('int8')\n# matrix['agent'] = matrix['agent'].astype('int8')","77b948d1":"# matrix = matrix.drop('date', axis=1)","631d4789":"_train = matrix[matrix.date<np.datetime64('2017-09-13')]\n_test = matrix[matrix.date>=np.datetime64('2017-09-13')]","e06361fc":"X_train = _train[_train.date<=np.datetime64('2017-07-10')].drop(['date','visits'], axis=1)\nY_train = _train[_train.date<=np.datetime64('2017-07-10')].visits\nX_valid = _train[_train.date>np.datetime64('2017-07-10')].drop(['date','visits'], axis=1)\nY_valid = _train[_train.date>np.datetime64('2017-07-10')].visits","36cfe48c":"def smape(y_true, y_pred):\n    return 2.0 * np.mean(np.abs(y_pred - y_true) \/ (np.abs(y_pred) + np.abs(y_true))) * 100\n\ndef smape_4_xgboost(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    return 'smape', smape(y_true, y_pred)","80a59195":"%%time\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nmodel = XGBRegressor(\n    n_estimators=10000,\n    eta=0.3,\n    seed=10086)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=smape_4_xgboost,\n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 100)","fa9455ec":"#### Full with bfill","43fdc36e":"### Deal with Missing\n\ntrain\u4e2d\u65e5\u671f\u5bf9\u5e94visitors\u6709\u7f3a\u5931\uff0c\u4e14\u7f3a\u5931\u4e0d\u4ee3\u88680\uff0c\u8fd9\u91cc\u8981\u5904\u7406\uff0c\u521d\u6b65\u8003\u8651\u586b\u5145\u65b9\u5f0f\u4f7f\u7528\u4f20\u7edf\u65f6\u5e8f\u6a21\u578b(\u5148\u77e5)\u8fdb\u884c\u9884\u6d4b\u586b\u5145\uff1b","55bb1d33":"### \u5212\u5206\u6570\u636e","a95de212":"## Preprocess\n\n1. \u7f3a\u5931\u5904\u7406\n2. Page\u5217\u5206\u5272\u540e\u5904\u7406\n3. \u964d\u4f4e\u7c7b\u578b\u51cf\u5c11\u5185\u5b58\u5360\u7528\n4. \u6570\u636e\u6574\u7406\u5230\u7b26\u5408train\u548ctest","cd39eda5":"## Modeling","066f1e27":"### Label Encoding","5a0e0e31":"#### Clip to 0 in left","c28767b2":"#### Full with prophet\n\nToo long.","8950d05a":"## Load Data","7e989f6e":"### \u7ecf\u5178\u65f6\u5e8fFE\n\n\u5185\u5b58\u95ee\u9898\uff0c\u5148\u4e0d\u505a\u8fd9\u4e00\u6b65\uff1b","51becc00":"## EDA","724c0b21":"### Data Type Down\n\n\u7f3a\u5931\u5904\u7406\u540e\uff0c\u9700\u8981\u5bf9visitors\u8fdb\u884c\u7c7b\u578b\u4e0b\u964d\uff1b","b476e375":"### Feature Select","2b138cb2":"### \u6784\u5efatest\u6570\u636e\n\n2017\u5e749\u670813\u65e5\u81f32017\u5e7411\u670813\u65e5","e7d41c17":"## FE\n\nlog1p\uff1b","218b5a0a":"### Data Reshape\n\n1. \u6784\u5efatest\n2. \u5c06\u539f\u59cb\u6570\u636e\u683c\u5f0f\u8f6c\u4e3a\u76f4\u63a5\u7528\u4e8etrain\u548ctest\u7684\u683c\u5f0f\uff1b","db7c7aea":"## Submission","23f12b9d":"#### Full with 0 backup"}}