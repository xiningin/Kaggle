{"cell_type":{"66d01414":"code","e43c8ccb":"code","470977a9":"code","96611003":"code","98d4bc37":"code","e8ae5d99":"code","2ce42d97":"code","959784e0":"code","e5f4d240":"code","0dc9ac7f":"code","c2f09acc":"code","f66806b7":"code","98b2fbd5":"code","b83123ff":"code","dd3d989e":"code","7eecc43a":"code","775bb65c":"code","464b8b67":"code","00eab0ab":"code","6e7dae49":"code","8eb3fd10":"code","69e47282":"code","0cafe5da":"code","42af5ca7":"code","a1f437be":"code","fd8c1a2c":"code","c0a8c638":"code","4a69028c":"code","00a7a981":"code","97b7b315":"code","59fca395":"code","bba6ba0d":"code","813a7df4":"code","c6ad1a68":"code","28bb1a65":"code","e8f60578":"code","c83fd03d":"code","a74107c5":"code","e2fc6258":"markdown","f1731777":"markdown","95cae57f":"markdown","69e0534a":"markdown","17ab6eba":"markdown","a943617c":"markdown","b3b54939":"markdown","3c4faaac":"markdown","2f4982be":"markdown","d99042f6":"markdown","c636f4bc":"markdown","6a788cdd":"markdown","cce22521":"markdown","c8257fa2":"markdown","258ccdc4":"markdown","2902a4e0":"markdown","84c18d8e":"markdown","e7c2f7a5":"markdown"},"source":{"66d01414":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","e43c8ccb":"dataset = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","470977a9":"dataset.head()","96611003":"# Converting the non-numerical data into catetgorical data\n\ndataset['gender'] = dataset['gender'].astype('object')\ndataset['ssc_b'] = dataset['ssc_b'].astype('object')\ndataset['hsc_b'] = dataset['hsc_b'].astype('object')\ndataset['hsc_s'] = dataset['hsc_s'].astype('object')\ndataset['degree_t'] = dataset['degree_t'].astype('object')\ndataset['workex'] = dataset['workex'].astype('object')\ndataset['specialisation'] = dataset['specialisation'].astype('object')\n\n#Getting all the categorical columns except the target\ncategorical_columns = dataset.select_dtypes(exclude = 'number').drop('status', axis = 1).columns\n\nprint(categorical_columns)","98d4bc37":"# First considering only numerical values for feature selection\nX = dataset.iloc[:,[2,4,7,10,12,14]].values\nY = dataset.iloc[:,13].values","e8ae5d99":"print(X)","2ce42d97":"print(Y)","959784e0":"len(dataset)","e5f4d240":"dataset.isnull().sum()","0dc9ac7f":"# So salary column contains null values","c2f09acc":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[5]])\nX[:,[5]] = imputer.transform(X[:,[5]])","f66806b7":"print(X)","98b2fbd5":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = dataset.iloc[:,[2,4,7,10,12,14]]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,Y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","b83123ff":"# So we can conclude that 'Salary' and 'ssc_p' are two relavent features for predicting the status of placement for a student","dd3d989e":"# Import the function\n#from scipy.stats import chi2_contingency\n#Testing the relationship\n#chi_res = chi2_contingency(pd.crosstab(dataset['status'], dataset['gender']))\n#print('Chi2 Statistic: {}, p-value: {}'.format(chi_res[0], chi_res[1]))","7eecc43a":"from scipy.stats import chi2_contingency\nchi2_check = []\nfor i in categorical_columns:\n    if chi2_contingency(pd.crosstab(dataset['status'], dataset[i]))[1] < 0.05:\n        chi2_check.append('Reject Null Hypothesis')\n    else:\n        chi2_check.append('Fail to Reject Null Hypothesis')\nres = pd.DataFrame(data = [categorical_columns, chi2_check] \n             ).T \nres.columns = ['Column', 'Hypothesis']\nprint(res)","775bb65c":"# If we choose our p-value level to 0.05, if the p-value test result is more than 0.05 then we fail to reject the Null Hypothesis. \n# This means, there is no relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.\n# And if the p-value test result is less than 0.05 then we reject the Null Hypothesis. \n# This means, there is a relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.","464b8b67":"# So we conclude that 'workex' and 'specialisation' are two important features for predicting status.","00eab0ab":"# So after feature selection of categorical and numerical features, X comes as,\nX = dataset.iloc[:,[2,9,11,14]].values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[3]])\nX[:,[3]] = imputer.transform(X[:,[3]])","6e7dae49":"print(dataset['workex'].unique())\nprint(dataset['specialisation'].unique())\nprint(dataset['status'].unique())","8eb3fd10":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\nX[:,1] = le1.fit_transform(X[:, 1])\nle2 = LabelEncoder()\nX[:,2] = le2.fit_transform(X[:, 2])\nle3 = LabelEncoder()\nY = le3.fit_transform(Y)","69e47282":"print(X[0])","0cafe5da":"print(Y)","42af5ca7":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)","a1f437be":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:,[0,3]] = sc.fit_transform(x_train[:,[0,3]])\nx_test[:,[0,3]] = sc.transform(x_test[:,[0,3]])","fd8c1a2c":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(x_train, y_train)","c0a8c638":"mylist = []\nmylist2 = []\ny_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Logistic Regression')","4a69028c":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclassifier.fit(x_train,y_train)","00a7a981":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('KNN')","97b7b315":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","59fca395":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Naive Bayes')","bba6ba0d":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(x_train, y_train)","813a7df4":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Decision Tree')","c6ad1a68":"from sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state=0)\nclassifier.fit(x_train, y_train)","28bb1a65":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Support Vector')","e8f60578":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(x_train, y_train)","c83fd03d":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Random Forest')","a74107c5":"# Plotting the accuracy score for different models\nplt.rcParams['figure.figsize']=10,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","e2fc6258":"### Label encoding","f1731777":"### Importing the Libraries","95cae57f":"### Encoding Categorical Values","69e0534a":"### Feature Selection of Numerical Values","17ab6eba":"### K Nearest Neighbor","a943617c":"### Importing the Dataset","b3b54939":"### Random Forest Classification","3c4faaac":"### Feature Scaling","2f4982be":"### Support Vector Classification","d99042f6":"### Taking care of missing data in Salary column","c636f4bc":"### Splitting the dataset into training set and test set","6a788cdd":"### Feature Selection of Categorical Data","cce22521":"### Decision Tree Classification","c8257fa2":"### Applying classification models on the Training set","258ccdc4":"### Logistic Regression","2902a4e0":"#### Finding the categories","84c18d8e":"#### Checking for null values","e7c2f7a5":"### Naive Bayes Classification"}}