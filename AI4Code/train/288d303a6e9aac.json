{"cell_type":{"73f5b00a":"code","e67a5308":"code","7dd10879":"code","d50e285d":"code","0fdcfeb4":"code","abfaa9e6":"code","aef70095":"code","a4eafcc9":"code","96effce7":"code","bda258cc":"code","a01fff1a":"code","2bb9b95b":"code","92ad0b80":"code","701e6553":"code","9249c24c":"code","9cb3ce7d":"code","e5b8bf95":"code","f7958a52":"code","13eb5531":"code","3b80b3cc":"code","3c8d6b80":"code","6511bfc5":"code","07619abd":"code","46d3d1a5":"code","02a96540":"code","f6cb3b09":"code","626b8d6c":"code","418816a2":"code","1ee05bc8":"code","d5c101c8":"code","633e0582":"code","381ac4a3":"code","bd8bd5ae":"code","907fa5ee":"code","6ab86118":"code","828103ec":"code","437801c8":"code","b3165955":"code","8bdf56e6":"code","103ade3a":"code","85fcc017":"code","8855eb89":"code","7a8baed5":"code","aca51b41":"code","d3ddaf19":"code","9f333570":"code","771ad6f0":"code","80b4a0c5":"code","b9571a3b":"code","13a9a00a":"code","bb237de2":"code","4917bd6d":"code","0521d750":"code","6256a34c":"code","72b762e8":"code","5e0be6b8":"code","ff440046":"code","8f00b1cc":"code","d110e037":"code","72968d7f":"code","7567c170":"code","44769eb3":"code","75f1f531":"code","a4a1acf9":"markdown","0f79b0ec":"markdown","103e8ccd":"markdown","d232fccb":"markdown","a4b8a083":"markdown","b1124874":"markdown","cc808a3b":"markdown","cbc84e82":"markdown","2c0e2f23":"markdown","b615b967":"markdown","8ace664d":"markdown","51e0c9ab":"markdown","3548ab0c":"markdown","1ec40a73":"markdown","56b12562":"markdown","eeba599f":"markdown","4780812f":"markdown","b2198ebe":"markdown","24632f3b":"markdown","a880bce3":"markdown","c96fc6ba":"markdown","93fe18f1":"markdown","b26eb6cf":"markdown","438b37b5":"markdown","084e9a80":"markdown","165b92f4":"markdown","01b65bf6":"markdown","810ede14":"markdown","244992a9":"markdown","408e50c0":"markdown","f0ecd268":"markdown","53609c09":"markdown","58318790":"markdown","9ac51b2b":"markdown","df1f0146":"markdown","af2667f1":"markdown","44b88067":"markdown","e5a8bcd1":"markdown","8f9d4eef":"markdown","dc92aa33":"markdown"},"source":{"73f5b00a":"#importing necessary Libraries \n\n#working with data\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n## Scikit-learn features various classification, regression and clustering algorithms\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report,f1_score\n\n## Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n## Algo\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')","e67a5308":"#loading Data\nData = pd.read_csv('..\/input\/vehicle\/vehicle-1.csv')","7dd10879":"#checking top 5 rows\nData.head()","d50e285d":"#fetch all columns\nData.columns","0fdcfeb4":"#checking datatypes of each column\nData.dtypes","abfaa9e6":"#shape of data \nshape_Data = Data.shape\nprint('Data set contains \"{x}\" number of rows and \"{y}\" number of columns' .format(x=shape_Data[0],y=shape_Data[1]))","aef70095":"#Data info \n#It gives the information about the number of rows, number of columns, data types , memory usage, \n#number of null values in each columns.\"\nData.info()\n","a4eafcc9":"null_data = Data.isnull().sum()\nnull_data","96effce7":"sns.heatmap(Data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","bda258cc":"#Null values percentage corresponding to the columns\nPercentage_Null = (pd.DataFrame(Data.isnull().sum())\/len(Data))*100\nsns.set(rc={'figure.figsize':(12,6)})\nPercentage_Null.plot.bar()\nplt.xlabel(\"Columns with null values\")\nplt.ylabel(\"Null value percentage\")","a01fff1a":"#Oveview of Data\nData.describe().T","2bb9b95b":"#Replacing the missing values by mean\nfor i in Data.columns[:17]:\n    mean_value = Data[i].mean()\n    Data[i] = Data[i].fillna(mean_value)","92ad0b80":"#Again check for missing values\nnull_data = Data.isnull().sum()\nnull_data","701e6553":"#Skewness is computed for each row or each column , here we will check for column\nskewValue = Data.skew(axis=0) # axis=0 for column\nprint(\"SKEW:\")\nprint(skewValue)","9249c24c":"f, axes = plt.subplots(1, 4,figsize=(15,5))\ncompactness = sns.distplot(Data['compactness'], color=\"green\", kde=True,ax=axes[0])\ncircularity = sns.distplot(Data['circularity'], color=\"blue\", kde=True,ax=axes[1])\ndistance_circularity = sns.distplot(Data['distance_circularity'], color=\"red\",kde=True,ax=axes[2])\naxis_aspect_ratio = sns.distplot(Data['pr.axis_aspect_ratio'], color=\"orange\", kde=True,ax=axes[3])","9cb3ce7d":"f, axes = plt.subplots(1, 4,figsize=(15,5))\nscatter_ratio = sns.distplot(Data['scatter_ratio'], color=\"orange\", kde=True,ax=axes[0])\nradius_ratio = sns.distplot(Data['radius_ratio'], color=\"pink\", kde=True,ax=axes[1])\nlength_aspect_ratio = sns.distplot(Data['max.length_aspect_ratio'], color=\"magenta\", kde=True,ax=axes[2])\nelongatedness = sns.distplot(Data['elongatedness'], color=\"purple\", kde=True,ax=axes[3])\n","e5b8bf95":"f, axes = plt.subplots(1, 4,figsize=(15,5))\npr_axis_rectangularity  = sns.distplot(Data['pr.axis_rectangularity'], color=\"lime\", kde=True,ax=axes[0])\nmax_length_rectangularity = sns.distplot(Data['max.length_rectangularity'], color=\"maroon\", kde=True,ax=axes[1])\nscaled_variance = sns.distplot(Data['scaled_variance'], color=\"olive\",kde=True,ax=axes[2])\nscaled_variance_1_ = sns.distplot(Data['scaled_variance.1'], color=\"LightBlue\", kde=True,ax=axes[3])","f7958a52":"f, axes = plt.subplots(1, 4,figsize=(15,5))\nscaled_radius_of_gyration_1_  = sns.distplot(Data['scaled_radius_of_gyration.1'], color=\"DarkBlue\", kde=True,ax=axes[0])\nscaled_radius_of_gyration = sns.distplot(Data['scaled_radius_of_gyration'], color=\"Cyan\", kde=True,ax=axes[1])\nskewness_about = sns.distplot(Data['skewness_about'], color=\"Aquamarine\",kde=True,ax=axes[2])\nskewness_about_1_ = sns.distplot(Data['skewness_about.1'], color=\"Plum\", kde=True,ax=axes[3])","13eb5531":"f, axes = plt.subplots(1, 2,figsize=(15,5))\nhollows_ratio = sns.distplot(Data['hollows_ratio'], color=\"Lime\",kde=True,ax=axes[0])\nskewness_about_2_ = sns.distplot(Data['skewness_about.2'], color=\"magenta\", kde=True,ax=axes[1])","3b80b3cc":"Data['class'].unique()","3c8d6b80":"Data['class'].value_counts()","6511bfc5":"Class = sns.countplot(data = Data, x = 'class')\nClass.set_xlabel('Types of Vehicle', fontsize=15)","07619abd":"# Label encoder \nfrom sklearn.preprocessing import LabelEncoder\n#Encoding of categorical variable\nlabelencoder_X=LabelEncoder()\nData['class']=labelencoder_X.fit_transform(Data['class'])\nData['class'].value_counts()","46d3d1a5":"corr= Data.corr()\ncorr","02a96540":"sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 3.5})\nplt.figure(figsize=(18,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","f6cb3b09":"corr[(corr>.85) | (corr <(-.85))]","626b8d6c":"#Variation of Vehicles w.r.t. Scatter Ratio\nsns.kdeplot((Data[Data['class'] == 2].scatter_ratio), shade=False, label='Van') # for Van\nsns.kdeplot((Data[Data['class'] == 1].scatter_ratio), shade=True ,label='Car') # for Car\nsns.kdeplot((Data[Data['class'] == 0].scatter_ratio), shade=False , label='Bus') # for Bus\n\nplt.title(\"Variation of Vehicles w.r.t. Scatter Ratio\")","418816a2":"#Variation of Vehicles w.r.t. compactness\n'''\n0-- Bus\n1-- Car\n2 -- Van\n'''\nsns.boxplot(x='class' ,y= 'compactness' ,data=Data)","1ee05bc8":"#Variation of Vehicles w.r.t. circularity\n'''\n0-- Bus\n1-- Car\n2 -- Van\n'''\nsns.boxplot(x='class' ,y= 'circularity' ,data=Data)","d5c101c8":"sns.violinplot(x=\"class\", y=\"distance_circularity\", data=Data,palette='rainbow')","633e0582":"#circularity and scaled_radius_of_gyration, as they are highly correlated.\n'''\n0-- Bus\n1-- Car\n2 -- Van\n'''\n\na=sns.stripplot(x=\"circularity\", y=\"scaled_radius_of_gyration\", data=Data,jitter=True,hue='class',palette='Set1')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","381ac4a3":"#scatter_ratio and scaled_variance.1, as they are highly correlated.\n'''\n0-- Bus\n1-- Car\n2 -- Van\n'''\nplt.figure(figsize=(18,7))\na=sns.swarmplot(x=\"scatter_ratio\", y=\"scaled_variance.1\",hue='class',data=Data, palette=\"Set1\", split=True)\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90,)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","bd8bd5ae":"#Check pairplot for class\nsns.pairplot(Data, hue='class')","907fa5ee":"#Splitting the data between independent and dependent variables\nX=Data.iloc[:,0:18]\ny = Data['class']","6ab86118":"#dropping correlated values which are have either more then 85% or less then -85%\nX_new=X.drop(['circularity','scatter_ratio','scaled_variance'],axis=1)","828103ec":"#since there is lots of variety in the units of features let's scale it\nscaler=StandardScaler().fit(X_new)\nX_scaled=scaler.transform(X_new)","437801c8":"# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size = 0.3, random_state = 10)","b3165955":"#checking the dimensions of the train and test set\nprint(X_train.shape) # shape of train data\nprint(X_test.shape) # shape of test data","8bdf56e6":"from sklearn.svm import SVC\nsvclassifier = SVC(gamma=0.05, C=3,random_state=0) \nsvclassifier.fit(X_train, y_train) # To train the algorithm on training data\n","103ade3a":"y_prediction = svclassifier.predict(X_test) #To make predictions","85fcc017":"#check the accuracy on the training data\nprint('Accuracy on Training data: ',svclassifier.score(X_train, y_train))\n# check the accuracy on the testing data\nprint('Accuracy on Testing data: ', svclassifier.score(X_test , y_test))","8855eb89":"#measure the accuracy of this model's prediction\nprint(\"Confusion Matrix:\\n\",metrics.confusion_matrix(y_prediction,y_test))","7a8baed5":"#Evaluate Model Score\nprint(\"Classification Report:\\n\",metrics.classification_report(y_test,y_prediction))","aca51b41":"#Using K fold to check how my algorighm varies throughout my data if we split it in 10 equal bins\nmodels = []\nmodels.append(('SVM before PCA', SVC(gamma=0.05, C=3)))\n\n# evaluate each model\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=101)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"Name = %s , Mean Accuracy = %f, SD Accuracy = %f\" % (name, cv_results.mean(), cv_results.std()))","d3ddaf19":"fig = plt.figure()\nfig.suptitle('Algorithm Variation')\nax = fig.add_subplot(111)\nplt.plot(results[0],label='SVM before PCA')\nplt.legend()","9f333570":"#Scaling with all Columns(leaving Target)\nscaler=StandardScaler().fit(X)\nX_scaled_PCA=scaler.transform(X)","771ad6f0":"#printing Covariance Matrix\ncovMatrix = np.cov(X_scaled_PCA,rowvar=False)\nprint(covMatrix)","80b4a0c5":"#choosing n_component as 8 coz we saw there are 3-4 attributes having 2 hidden clusters\npca = PCA(n_components=8)\npca.fit(X_scaled_PCA)","b9571a3b":"#eigen values\nprint(pca.explained_variance_)","13a9a00a":"#eigen Vectors\nprint(pca.components_[0])","bb237de2":"#percentage of variance explained by each vector\nprint(pca.explained_variance_ratio_)","4917bd6d":"#visualising it\nplt.bar(list(range(1,9)),pca.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')","0521d750":"#cummilative variance explained via each vector\nplt.step(list(range(1,9)),np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')","6256a34c":"#transforming and storing result in X_Train_PCA\npca_model_test = PCA(n_components=7)\npca_model_test.fit(X_scaled_PCA)\nX_PCA= pca_model_test.transform(X_scaled_PCA)\nX_PCA[0]","72b762e8":"# Split X and y into training and test set in 70:30 ratio\nX_train_PCA, X_test_PCA, y_train_PCA, y_test_PCA = train_test_split(X_PCA,y, test_size = 0.3, random_state = 10)","5e0be6b8":"#model Building\nsvclassifier_PCA = SVC(gamma=0.05, C=3,random_state=0) \nsvclassifier_PCA.fit(X_train_PCA,y_train_PCA)","ff440046":"#To make predictions\ny_prediction_PCA = svclassifier_PCA.predict(X_test_PCA) ","8f00b1cc":"#check the accuracy on the training data\nprint('Accuracy on Training data after PCA: ',svclassifier_PCA.score(X_train_PCA, y_train_PCA))\n#check the accuracy on the test data\nprint('Accuracy on Training data after PCA: ',svclassifier_PCA.score(X_test_PCA, y_test_PCA))","d110e037":"#measure the accuracy of this model's prediction\nprint(\"Confusion Matrix:\\n\",metrics.confusion_matrix(y_prediction_PCA,y_test_PCA))","72968d7f":"#Evaluate Model Score\nprint(\"Classification Report:\\n\",metrics.classification_report(y_test_PCA,y_prediction_PCA))","7567c170":"#Using K fold to check how my algorighm varies throughout my data if we split it in 10 equal bins\nmodels = []\nmodels.append(('SVM After PCA', SVC(gamma=0.05, C=3)))\n\n# evaluate each model\nresults_PCA = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=101)\n\tcv_results_PCA = model_selection.cross_val_score(model, X_train_PCA, y_train_PCA, cv=kfold, scoring=scoring)\n\tresults_PCA.append(cv_results_PCA)\n\tnames.append(name)\n\tprint(\"Name = %s , Mean Accuracy = %f, SD Accuracy = %f\" % (name, cv_results_PCA.mean(), cv_results_PCA.std()))","44769eb3":"fig = plt.figure()\nfig.suptitle(\"SVM Variation with 15 'Attributes' vs 7 'Components'\")\nax = fig.add_subplot(111)\nplt.plot(results[0],label='SVM with 15 attributes')\nplt.plot(results_PCA[0],label='SVM with 7 components')\nplt.legend()","75f1f531":"\nAccuracy_df=pd.DataFrame([{'Model':'SVM with 15 attribute','Mean Accuracy': cv_results.mean(),'Standard Deviation':cv_results.std()},\n                       {'Model':'SVM with 7 Components','Mean Accuracy':cv_results_PCA.mean(),'Standard Deviation':cv_results_PCA.std()}\n                       ] ) \nAccuracy_df=Accuracy_df[['Model','Mean Accuracy','Standard Deviation']]\nAccuracy_df\n","a4a1acf9":"### <font color='red'>Step 1 <\/font> Data pre-processing","0f79b0ec":"### <font color='red'> <\/font> Check for datatypes,columns and shape of data","103e8ccd":"###  Variation of Target variable","d232fccb":"### <font color='red'><\/font> Correlation of the dataset attributes","a4b8a083":"## <font color='red'>Step 4 <\/font> Support Vector Machine using Raw Data","b1124874":"1) scatter_ratio, radius_ratio, elongatedness are approximately normally distributed.<br>\n<b>Skew Value :<\/b><br>\nscatter_ratio = 0.607629 (2 Hidden Clusters)<br>\nradius_ratio = 0.396381<br>\nelongatedness = 0.047875 (2 Hidden Clusters)<br> <br>\n        \n2) max.length_aspect_ratio is Highly right skewed<br>\n<b>Skew Value :<\/b><br>\nmax.length_aspect_ratio =  6.778394 <br>","cc808a3b":"### <font color='red'>Step 2<\/font> Data Analysis","cbc84e82":"<font size =\"4\"> Evaluating the  Algorithm <\/font><br>\nConfusion matrix, precision, recall and F1 measures are the most commonly used metrics for classification tasks. We will find out the same here .","2c0e2f23":"### <font color='red'><\/font> Import data in data frame","b615b967":"#### Following things can be observed from Violin plot of distance_circularity \n2. Bus distribution have right skewness with majority of data around 65-70\n3. Car distribution have left skewness,contains two hidden clusters with majority of data around 90-110\n4. Van sistribution have almost no skewness,contains two hidden clusters with majority of liying data around 70-85","8ace664d":"### Transforming Target Variable","51e0c9ab":"# Unsupervised Learning Project \n### <u>Data Description and Context:<\/u>\nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars. \n\n### <u>Domain:<\/u>\nObject recognition \n\n### <u>Context<\/u>\nThe purpose is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles. \n\n### <u>Attribute Information:<\/u>\n<p>All the features are geometric features extracted from the silhouette.<\/p>  \n<p>All are numeric in nature. <\/p>\n\n### <u>Objective:<\/u>\nApply dimensionality reduction technique \u2013 PCA and train a model using principle components instead of training the model using just the raw data. ","3548ab0c":"1) compactness, circularity, distance_circularity are approximately normally distributed, where Distance_circularity seems to be formed via 2 hidden clusters<br>\n<b>Skew Value :<\/b><br>\ncompactness = 0.381271<br>\ncircularity = 0.262584<br>\ndistance_circularity  = 0.106837<br><br>\n2) pr.axis_aspect_ratio is  right skewed <br>\n<b>Skew Value :<\/b><br>\npr.axis_aspect_ratio = 3.834882","1ec40a73":"#### Following things can be observed from Box plot of circularity \n1. Car,Van distribution is normaly spread as no oulier is present, where else Bus distribution contains outlier.\n2. Bus distribution have right skewness with majority of data around 42-27\n3. Car distribution have no skewness with majority of data around 40-52\n4. Van sistribution have no skewness with majority of liying data around 40-45","56b12562":"### Conclusion\n1. This study of Algorithm variation over the dimension reduction techniques shows how benificial it might get.\n2. From the above DataFrame we can see that with the Raw data that was intially provided to us having 18columns i.e. 17 attributes gives accuracy of (93-99)% over SVM.\n3. If we Build a PCA and reduce dimension from 17 to 7, that is we are almost droping 10 dimensions yet the accuracy is (89-95)%\n4. From this behaviour we can conclude how efficient is our PCA, it made our Algo more reliable, Fast and ready for production.","eeba599f":"#### Following things can be observed from Box plot of compactness \n1. Each distribution is normaly spread as no oulier is present.\n2. Bus distribution have little left skewness with majority of data around 85-95\n3. Car distribution have little Right skewness with majority of data around 90-100\n4. Van sistribution have no skewness with majority of liying data around 90","4780812f":"#### We can conclude that if we take 7 Components we will be covering around 95% of variance","b2198ebe":"### <font color='red'><\/font> Handling the Missing Values","24632f3b":"###  Univariate Analysis","a880bce3":"### <font color='red'>Step 6 <\/font> Applying PCA and Extaracting Components with 95%variance Coverage","c96fc6ba":"## <font color='red'> <\/font> Multivariate analysis w.r.t target variable (class ) <br>\nLet's see  both distribution of single variables and relationships between other variables by the help of PairPlot as includes all the columns of the data frame ","93fe18f1":"The target variable (CLASS)  is categorical and dependent variable, let's see the unique values it contains and then further will see the distribution of those values .","b26eb6cf":"### <font color='red'> <\/font> Bivariate analysis w.r.t target variable (class )\n1. due to this high correlation, we dont need to analyse whole data instead we can just take few as showen below.","438b37b5":" cross_val_score by default runs a K-Fold Cross-Validation when working with a Regression Model whereas it runs a Stratified K-Fold Cross-Validation when dealing with a Classification Model.","084e9a80":"1) pr.axis_rectangularity, max.length_rectangularity , scaled_variance , scaled_variance.1 are approximately normally distributed.<br>\n<b>Skew Value :<\/b><br>\npr.axis_rectangularity = 0.772254 (2 hidden Clusters)<br>\nmax.length_rectangularity = 0.256359 (bi-modal)<br>\nscaled_variance = 0.652753 (2 hidden Clusters)<br>\nscaled_variance.1 = 0.843027 (2 hidden Clusters)<br>\n        ","165b92f4":"Here <br>\n1-- Car <br>\n0-- Bus <br>\n2 -- Van<br>","01b65bf6":"\nThe strength of the correlation matters. The closer the absolute value is to -1 or 1, the stronger the correlation.<br>\n<b>r value    <\/b>        <b>Strength<\/b> <br>\n0.0 \u2013 0.2\tWeak correlation<br>\n0.3 \u2013 0.6\tModerate correlation<br>\n0.7 \u2013 1.0\tStrong correlation<br><br>\n### Heatmap of Correlation ~ Analysis\nFrom above heatmap of correlation, we conclude that : <br>\n1. 'Class' column has weak correlation with other columns <br>\n2.  Strong Correlation between Scatter_ratio and scaled_variance.1 , Scatter_ratio and scaled_variance ,Scatter_ratio and pr.axis_rectangularity , circularity and scaled_radius_of_gyration and many more..  <br>\nFrom the heatmap we can see that many independent attributes  are strongly co-related and it is difficult to determine that which column is least irrelevant . \n3.  We will be setting Correlation cutoff as +-85%, so if correlation is more(+85%) then or less then(-85%) we will drop the coloumn","810ede14":"## <font color='red'>Step 5 <\/font> K- Fold Cross Validation\nA useful technique to check how well a model performs when we apply it on an independent data. It is often used to flag problems caused by overfitting and selection bias.","244992a9":"1. There was MISSING data which is now handled by replacing with the mean . <br>\n2. There are 18 independent variables (compactness,circularity, distance_circularity,radius_ratio, pr.axis_aspect_ratio, max.length_aspect_ratio, scatter_ratio, elongatedness, pr.axis_rectangularity, max.length_rectangularity,scaled_variance, scaled_variance.1,scaled_radius_of_gyration, scaled_radius_of_gyration.1, skewness_about, skewness_about.1,skewness_about.2, hollows_ratio ) and all of them are numeric . <br>\n3. There is one dependent variable (class) which contain categorical data.\n","408e50c0":"Now , we will check normal distribution or any skewness is present or not in the data with the help of graphical representation.","f0ecd268":"By the above two methods , we can see that there are 3 types of vehicles : Van , Car and  Bus . <br>\nCars are almost double in number as compared to bus and van . ","53609c09":"It shows that there are 18 numeric variables (compactness,circularity, distance_circularity,radius_ratio, pr.axis_aspect_ratio, max.length_aspect_ratio, scatter_ratio, elongatedness, pr.axis_rectangularity, max.length_rectangularity,scaled_variance, scaled_variance.1,scaled_radius_of_gyration, scaled_radius_of_gyration.1, skewness_about, skewness_about.1,skewness_about.2, hollows_ratio) and one non-numeric variable (class)","58318790":"#### Following things can be observed from Scatter Ratio graphs\n1. Van distribution have 2 hidden clusters, not skewed, most data lies around 110-130 scatter ration\n2. Bus Distribution is right skewed while most of data lying arounf 130-160\n3. Car Disribution is slightly left skewed with 2 hidden clusters and most data lies around 190-225 scatter ration","9ac51b2b":"### <font color='red'><\/font> Checking for Null Values","df1f0146":"### Data Understanding","af2667f1":"Skewness lets you test by how much the overall shape of a distribution deviates from the shape of the normal distribution.<br>\nSkew value is negative, denotes tail is larger towards left hand side i.e. left skewed.<br>\n Skew value is almost close to 0, uniformly distributed \/ symmetrical distribution . <br>\n Skew value is greater tha zero denoted  it is right skewed as tail is larger towards right hand side .","44b88067":"### <font color='red'>Step 7 <\/font> Repeating SVM Algorithm with PCA(7 components) to see the Efficiency","e5a8bcd1":"## <font color='red'> Step 3<\/font> Scaling and spliting Data","8f9d4eef":"1) skewness_about.2 is approximately uniformly distributed <br>\n<b>Skew Value :<\/b><br>\nskewness_about.2 =  0.249468 <br><br>\n\n2)hollows_ratio is left skewed as it has a negative skew value. <br>\n<b>Skew Value :<\/b><br>\nhollows_ratio =    -0.226341","dc92aa33":"1) scaled_radius_of_gyration is right skewed .<br>\n<b>Skew Value :<\/b><br>\nscaled_radius_of_gyration.1 = 2.088422 <br> <br>\n\n2) scaled_radius_of_gyration , skewness_about  , skewness_about.1 are almost normally distributed.<br>\n<b>Skew Value :<\/b><br>\nscaled_radius_of_gyration = 0.279647 <br>\nskewness_about = 0.779277 <br>\nskewness_about.1 = 0.688423<br>"}}