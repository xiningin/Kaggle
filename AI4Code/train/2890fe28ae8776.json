{"cell_type":{"8ea28956":"code","02dd31a1":"code","6ca6bdc7":"code","ebfcbec3":"code","e6eb4e1b":"code","112bdfb2":"code","6b558984":"code","8cb2ae65":"code","a8b6b9c9":"code","e86a458f":"code","ad7348f6":"code","43803071":"code","a969e480":"code","90c99f60":"code","a8165cd9":"code","e855fbf9":"code","49ffedb8":"code","824f625b":"code","5f96f47d":"code","af1e7744":"code","127cb43a":"code","12415db6":"code","e61d04c9":"code","1439aa38":"code","f24ab442":"code","32e68e93":"code","f8aca001":"code","0ab74ecd":"code","f16a2606":"code","5f88e3e4":"markdown","0c04fd9e":"markdown"},"source":{"8ea28956":"!pip install contractions","02dd31a1":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pdfminer import high_level\nfrom os import listdir\nimport re\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport contractions\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","6ca6bdc7":"# Loading resume files\ndef load_resumes_id():\n    path = \"..\/input\/resumes-123123\/trainResumes\/\"\n    resume_ids =[file for file in listdir(path)]\n    resume_ids.sort()\n    return extract_text(resume_ids)\n\ndef extract_text(resume_ids):\n    resume_text_lst = []\n    for resume_id in resume_ids:\n        local_pdf_filename = \"..\/input\/resumes-123123\/trainResumes\/\" + resume_id\n        page_number = len(list(high_level.extract_pages(local_pdf_filename)))\n        page_number_lst = list(range(page_number))\n        extracted_text = high_level.extract_text(local_pdf_filename, \"\", page_number_lst)\n        resume_text_lst.append(extracted_text)\n    return resume_text_lst\n        ","ebfcbec3":"resume_text_lst = load_resumes_id()","e6eb4e1b":"\ndef degree_contraction(phrase):\n    phrase = re.sub(\"( bachelor of technology )|( b((\\.)|( +)|(\\. +)|( \\.))tech\\S*)\" , \" btech \" , phrase)\n    phrase = re.sub(\"( master of technology )|( m((\\.)|( +)|(\\. +)|( \\.))tech\\S*)\" , \" mtech \" , phrase)\n\n    phrase = re.sub(\"( bachelor of arts )|( b((\\.)|( +)|(\\. +)|( \\.))((arts )|(a\\. )|(a )))\" , \" ba \" , phrase)\n    phrase = re.sub(\"( master of arts )|( m((\\.)|( +)|(\\. +)|( \\.))((arts )|(a\\. )|(a )))\" , \" ma \" , phrase)\n\n    phrase = re.sub(\"( bachelor of business administration )|( b((\\.)|( +)|(\\. +)|( \\.))b((\\.)|( +)|(\\. +)|( \\.))a( |(\\. )))\" , \" bba \" , phrase)\n    phrase = re.sub(\"( master of business administration )|( m((\\.)|( +)|(\\. +)|( \\.))b((\\.)|( +)|(\\. +)|( \\.))a( |(\\. )))\" , \" mba \" , phrase)\n\n    phrase = re.sub(\"( bachelor of management studies )|( b((\\.)|( +)|(\\. +)|( \\.))m((\\.)|( +)|(\\. +)|( \\.))s( |(\\. )))\" , \" bms \" , phrase)\n\n    phrase = re.sub(\"( bachelor of science )|( b((\\.)|( +)|(\\. +)|( \\.))sc( |(\\. )))|( b((\\.)|( +)|(\\. +)|( \\.))s( |(\\. )))\" , \" bsc \" , phrase)\n    phrase = re.sub(\"( master of science )|( m((\\.)|( +)|(\\. +)|( \\.))sc( |(\\. )))|( m((\\.)|( +)|(\\. +)|( \\.))s( |(\\. )))\" , \" msc \" , phrase)\n\n    phrase = re.sub(\"( bachelor of commerce )|( b((\\.)|( +)|(\\. +)|( \\.))com( |(\\. )))\" , \" bcom \" , phrase)\n    phrase = re.sub(\"( master of commerce )|( m((\\.)|( +)|(\\. +)|( \\.))com( |(\\. )))\" , \" mcom \" , phrase)\n\n    phrase = re.sub(\"( bachelor of computer applications )|( b((\\.)|( +)|(\\. +)|( \\.))c((\\.)|( +)|(\\. +)|( \\.))a( |(\\. )))\" , \" bca \" , phrase)\n\n    phrase = re.sub(\"( bachelor of engineering )|( b((\\.)|( +)|(\\. +)|( \\.))e( |(\\. )))\" , \" be \" , phrase)\n\n    phrase = re.sub(\"( bachelor of architecture )|( b((\\.)|( +)|(\\. +)|( \\.))arch( |(\\. )))\" , \"barch\" , phrase)\n\n    \n    # NOW REMOVE 'B' AND 'M' SINGLE LETTERS LEFT.\n    \n    phrase = re.sub(\" [bm] \" , ' ' , phrase)\n    return phrase\n\ndef decontraction (phrase):\n    phrase = degree_contraction(phrase)\n    return contractions.fix(phrase, slang = False)\n\ndef remove_stopwords(phrase):\n    phrase = re.sub(' [a-z] ' , ' ' , phrase)\n    return ' '.join(word for word in phrase.split() if word not in stopwords.words('english'))\n\nprocessed_resumes = []\nstemmer = SnowballStemmer('english')\n\nfor count , resume in tqdm(enumerate(resume_text_lst)):\n\n    #(1).Removing \\t , \\n , - , ( , ) \n    resume = re.sub(\"[\\t\\n\\-)(]\" , \" \" , resume)\n    \n    #(2).Removing starting two words (names)\n    resume = re.sub(\"\\A((\\s*)(\\S+)(\\s+)(\\S+)(\\s+))\" , ' ' , resume)\n    \n    #(3).Removing single letters (leaving B , M) and Numbers and lowercasing\n    resume = re.sub(\" \" , '  ' , resume)\n    resume = re.sub(\"\\d+|(\\s[AC-LN-Z]\\s)\" , '' , resume)\n    resume = re.sub(\"\\s+\" , \" \" , resume)\n    resume = resume.lower()\n    \n    #(4).Contraction-Decontraction \n    resume = decontraction(resume)\n    \n    #(6).Removing punctuation marks and special characters \n    resume = remove_stopwords(resume)\n    \n    #(5).Removing punctuation marks and special characters \n    resume = re.sub('[^a-z0-9+#]+' , \" \" , resume)\n    \n    #(7).Stemming\n    #processed_resumes.append(resume)\n    processed_resumes.append(' '.join(list(map(stemmer.stem , resume.split()))))","112bdfb2":"# Importing the dataset\ndf = pd.read_csv(\"..\/input\/resume-prediction-1235\/train.csv\")\ndf = df.sort_values(by=\"CandidateID\")\ndf.reset_index(drop=True,inplace=True)","6b558984":"processed_resumes = np.array(processed_resumes)\ndf_resumetxt = pd.DataFrame(np.reshape(processed_resumes,(-1,1)), columns=[\"resume-txt_processed\"])","8cb2ae65":"df_final = pd.concat([df, df_resumetxt], axis=1)","a8b6b9c9":"df_final.tail()","e86a458f":"BoW = CountVectorizer(min_df=5)\ntxt_vec = BoW.fit_transform(df_final[\"resume-txt_processed\"]).todense()","ad7348f6":"txt_vec.shape","43803071":"text_vec_norm = Normalizer().fit_transform(txt_vec)","a969e480":"#PCA\npca = PCA(n_components=54)\npca_data = pca.fit_transform(text_vec_norm)","90c99f60":"np.sum(pca.explained_variance_ratio_)","a8165cd9":"pca_data.shape","e855fbf9":"# TSNE\ntsne = TSNE(n_components=2)\nembedded = tsne.fit_transform(text_vec_norm)\ndf_tsne = pd.DataFrame(embedded, columns=[\"dim-1\", \"dim-2\"])\nplt.plot(df_tsne[\"dim-1\"], df_tsne[\"dim-2\"], 'ro')","49ffedb8":"y = df_final[\"Match Percentage\"].to_numpy()\ny = np.reshape(y, (-1, 1))","824f625b":"# train test splitting\nX_train, X_test, y_train, y_test = train_test_split(txt_vec, y, test_size=0.2, train_size=0.8)","5f96f47d":"penalty = [1, 1.2, 1.3,1.4, 1.5, 1.6, 1.8]\ncv_var = []\ncv_mean = []\n\nfor alpha in penalty:\n    model = Lasso(alpha=alpha)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"r2\")\n    cv_var.append(np.var(scores))\n    cv_mean.append(np.mean(scores))","af1e7744":"print(cv_mean)","127cb43a":"model = Lasso(alpha=1.4)\nmodel.fit(X_train, y_train)","12415db6":"weight = model.coef_\nzero_indices = np.argwhere(weight == 0).reshape(1, 275)[0]","e61d04c9":"data_final = np.delete(txt_vec, zero_indices, axis=1)","1439aa38":"data_final.shape","f24ab442":"X_train, X_test, y_train, y_test = train_test_split(data_final, y, test_size=0.2, train_size=0.8)","32e68e93":"model_k = KNeighborsRegressor(n_neighbors=2)\nmodel_k.fit(X_train, y_train)","f8aca001":"model_k.score(X_test, y_test)","0ab74ecd":"model_k.predict(X_test)","f16a2606":"y_test","5f88e3e4":"## Text Preprocessing","0c04fd9e":"## Bag of Words"}}