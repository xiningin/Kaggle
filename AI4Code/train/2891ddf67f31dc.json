{"cell_type":{"9c52ab12":"code","2996a191":"code","e4e55c49":"code","524f0fe9":"code","6eb9d969":"code","c4811657":"code","fc1fa3e2":"code","fbb80927":"code","41d0e0dd":"code","f5a1d5ab":"code","f34b6f79":"code","8b47cff8":"code","10d01f9c":"code","a5ace910":"code","675872cc":"code","cb391a35":"markdown","5baa2153":"markdown","8fa043c3":"markdown","5ae02b74":"markdown","0df4e52f":"markdown","f48512ce":"markdown","f5b6d893":"markdown","0bdd5b43":"markdown","36918a01":"markdown","c29dcf5c":"markdown","16795921":"markdown","414b647d":"markdown","f33a613e":"markdown","eeb7b0d8":"markdown"},"source":{"9c52ab12":"import pandas as pd\n#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")\n\ncsv_=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\nvisualization=csv_['Class'].value_counts()\nprint(visualization)\nplt.figure(figsize=(6,6))\nsns.barplot(visualization.index, visualization.values, alpha=0.8)\nplt.title('classes vs No of datapoints')\nplt.ylabel('no of datapoints')\nplt.xlabel('classes')\nplt.show()","2996a191":"from sklearn.utils import resample\n\n\n# separate minority and majority classes\nnot_fraud = csv_[csv_.Class==0]\nfraud = csv_[csv_.Class==1]\n\n# downsample majority\ndownsampled = resample(not_fraud,replace = False, # sample without replacement\n                                n_samples = len(fraud)) # match minority n\n\n# combine minority and downsampled majority\nundersampling = pd.concat([downsampled, fraud])","e4e55c49":"visualization=undersampling['Class'].value_counts()\nprint(visualization)\nplt.figure(figsize=(6,6))\nsns.barplot(visualization.index, visualization.values, alpha=0.8)\nplt.title('classes vs No of datapoints')\nplt.ylabel('no of datapoints')\nplt.xlabel('classes')\nplt.show()","524f0fe9":"# import library\nfrom imblearn.under_sampling import TomekLinks\nfrom collections import Counter\ndf=csv_\n\n# Separate input features and target\ny = df.Class\nx = df.drop('Class', axis=1)\n\ntl = TomekLinks(sampling_strategy='majority')\n\n# fit predictor and target variable\nx_tl, y_tl = tl.fit_resample(x, y)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_tl))","6eb9d969":"from sklearn.utils import resample\n\n# separate minority and majority classes\nnot_fraud = csv_[csv_.Class==0]\nfraud = csv_[csv_.Class==1]\n\n# upsample minority class\nupsample = resample(fraud,replace=True, # sample with replacement\n                          n_samples=len(not_fraud) # match number in majority class\n                          )\n\n# combine majority and upsampled minority\noversampled = pd.concat([not_fraud, upsample])","c4811657":"visualization=oversampled['Class'].value_counts()\nprint(visualization)\nplt.figure(figsize=(6,6))\nsns.barplot(visualization.index, visualization.values, alpha=0.8)\nplt.title('classes vs No of datapoints')\nplt.ylabel('no of datapoints')\nplt.xlabel('classes')\nplt.show()","fc1fa3e2":"#stroning the datraframe into df\ndf=csv_\n\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_df, y_df = smt.fit_resample(X, y)\nsmote_df = pd.concat([X_df, y_df], axis=1)","fbb80927":"visualization=smote_df['Class'].value_counts()\nprint(visualization)\nplt.figure(figsize=(6,6))\nsns.barplot(visualization.index, visualization.values, alpha=0.8)\nplt.title('classes vs No of datapoints')\nplt.ylabel('no of datapoints')\nplt.xlabel('classes')\nplt.show()","41d0e0dd":"#storing the datraframe into df\ndf=csv_\n\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n\n\nclf = LogisticRegression(class_weight='balanced').fit(X, y)","f5a1d5ab":"from sklearn.utils import class_weight\nimport numpy as np\nclass_weight.compute_class_weight('balanced', np.unique(y), y)","f34b6f79":"df=csv_\n\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27,stratify=y)","8b47cff8":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","10d01f9c":"clf = LogisticRegression()\nclf.fit(X_train, y_train)","a5ace910":"def plotPrecisionRecall(y_test,y_pred):\n    C = confusion_matrix(y_test, y_pred)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    B =(C\/C.sum(axis=0))\n    labels = ['0','1']\n\n\n    print(\"-\"*20, \"Precision matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True,annot_kws={\"size\": 16}, fmt='g', xticklabels=labels, yticklabels=labels)\n    plt.show()\n\n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True,annot_kws={\"size\": 16}, fmt='g', xticklabels=labels, yticklabels=labels)\n    \n    plt.show()","675872cc":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\nmatrix=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(matrix)\nsns.set(font_scale=1.4)#for label size\nlabels = ['0','1']\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.show()\nplotPrecisionRecall(y_test,y_pred)","cb391a35":"## Table of content\n* 1.Introduction\n    * 1.1. what is imbalance data\n    * 1.2. Problem with imbalance data\n* 2.Under sampling\n    * 2.1 Random under-sampling\n    * 2.2 Tomek links\n* 3.over sampling\n    * 3.1 Random over-sampling\n    * 3.2 SMOTE\n   \n* 4.weight_class\n* 5.Use the right evaluation metrics\n* 6.collect more data","5baa2153":"### 2.1 Random under-sampling","8fa043c3":"## 5. Use the right evaluation metrics","5ae02b74":"* while working in an imbalanced domain <i><b>accuracy<\/b><\/i> is not an appropriate measure to evaluate model performance. \n    For eg: A classifier which achieves an accuracy of 99 % <br>\n    even the model misclassified all the 0 class datapoint still the accuracy is 99%\n\n\nbetter measurement for imbalance dataset\n* macro f1-score\n* macro precision\n* macro recall <br>\n","0df4e52f":"### 1.2 Problem with imbalance\n* while working in an imbalanced  accuracy is not an appropriate measure to evaluate model performance. \n    For eg: A classifier which achieves an accuracy of 99 % <br>\n    even the model misclassified all the 0 class datapoint still the accuracy is 99%\n\n* It also cause overfitting of model\n","f48512ce":"### 3.1 Random over-sampling","f5b6d893":"### 3.2. SMOTE\nThis technique generates synthetic data for the minority class.\n\nSMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![smote_.png](attachment:c9a06f9a-86ce-49c8-9fcd-ba1bd07ddff5.png)","0bdd5b43":"### Disadvantage\n* As the same datapoint is copied we are training the model on same datapoints","36918a01":"## 6.collect more data\n\nby collecting more data of minority class can help in handling the imbalance data","c29dcf5c":"## 4.weight_class\n\n![class_weight_inp_image.png](attachment:d34907ce-9125-49c2-9d5b-02b474c0be0d.png)","16795921":"## 1.Introduction\n### 1.1. what is imbalance data\nwhen one class datapoint is more than other class datapoint <br>\nFor example Let say we have credit card dataset here we have two class fraud and not-fraud<br>\ntotal number of datapoint=284807 <br>\nnot-fraud point=284315 <br>\nfraud point=492 <br>\n","414b647d":"### 2.2 Tomek links\n\nTomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n\nTomek\u2019s link exists if the two samples are the nearest neighbors of each other\n\n![Tomek_links.png](attachment:f8f90d70-3afd-45e0-825c-1391b69a51c8.png)","f33a613e":"## 3.Over-Sampling\nOver-Sampling balances dataset by increasing the size of rare samples <br>\nIt is used where data cannot be increase","eeb7b0d8":"## 2.Under-sampling\nUnder-sampling balances the dataset by reducing the size of the minor class <br>\nIt is used where data is sufficient"}}