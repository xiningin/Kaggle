{"cell_type":{"652c79a2":"code","deae52f6":"code","5f4dbcfb":"code","e9866425":"code","3b4bfc5e":"code","e472cb68":"code","bc08942b":"code","c6eb3efc":"code","a0ac8b78":"code","933bb58d":"code","3a451991":"code","426c5b97":"code","7da5a805":"code","ea0369ee":"code","52a7359d":"code","762672ca":"code","1db35336":"code","56fb81ac":"code","0afa9614":"code","c8d9e68d":"code","bd978546":"code","b06c35b2":"code","66c244fd":"code","c28206d5":"code","01fd8989":"code","1bc93502":"code","d27d86c6":"code","b40070b1":"code","ccc4bfa9":"markdown","6d2c0065":"markdown","9da4867a":"markdown","4152c6a1":"markdown","c2871144":"markdown","ca4d81f8":"markdown","83877fca":"markdown","39edcced":"markdown","72c501b1":"markdown","f0c0fa77":"markdown","4711dce0":"markdown","8bc3e23b":"markdown","05b36ba5":"markdown","29fe98a4":"markdown","4d1f73b6":"markdown","dd2f3236":"markdown","8dd95891":"markdown","b6d8a6bf":"markdown","dac57af1":"markdown","703d37a3":"markdown","0d5dfea8":"markdown","e119a1d3":"markdown","0d0861ca":"markdown","47e5fec7":"markdown","67c1f299":"markdown","051515f3":"markdown"},"source":{"652c79a2":"#1.0 Clear memory\n%reset -f","deae52f6":"\n\n\n# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import kurtosis, skew\n\n# 1.3 Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import preprocessing \n# 1.3 Data transformation classes\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import StandardScaler \n \n\n\n# 1.4 Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost.sklearn import XGBClassifier\n\n# 1.6 Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\n# 1.7 Model evaluation metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# 1.8\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\n\n\n# 1.9 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier \n\n# 2.0 Misc\nimport os, gc\n\nfrom scipy.stats import uniform\n\n#Graphing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom matplotlib.colors import LogNorm\n\n# to display all outputs of one cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#hide warning\nimport warnings\nwarnings.filterwarnings('ignore')","5f4dbcfb":"os.chdir(\"\/kaggle\/input\/rossmann-store-sales\")\nos.listdir()","e9866425":"dftrain=pd.read_csv(\"train.csv\")\ndfstore=pd.read_csv('store.csv')\ndftrain.head()\ndfstore.head()\n","3b4bfc5e":"dftrain.columns[dftrain.isnull().any()]\n\n#no column has null value so need to fix null values\n","e472cb68":"\ndfstore.columns[dfstore.isnull().any()]\ndfstore.isnull().sum()","bc08942b":"df_train_store = dftrain.merge(dfstore, on = 'Store', copy = False)","c6eb3efc":"df_train_store=df_train_store[df_train_store.Open != 0]\ndf_train_store=df_train_store[df_train_store.Sales > 0]","a0ac8b78":"df_train_store.shape\n#decreased by 2 lakh approx","933bb58d":"#get year month day from date column & drop column\nimport calendar\ndf_train_store['Date']=pd.to_datetime(df_train_store['Date'])\ndf_train_store['Year']=df_train_store['Date'].dt.year\ndf_train_store['month']=df_train_store['Date'].dt.month\ndf_train_store['weekofyear']=df_train_store['Date'].dt.weekofyear\ndf_train_store['month_name']=df_train_store['month'].apply(lambda x: calendar.month_abbr[x])\n\n\n#df_train_store.drop(['Date'], axis = 1, inplace= True)","3a451991":"df_train_store['CompetitionOpen'] = 12 * (df_train_store.Year - df_train_store.CompetitionOpenSinceYear) + (df_train_store.month - df_train_store.CompetitionOpenSinceMonth)\ndf_train_store['CompetitionOpen'] = df_train_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\ndf_train_store.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis = 1,  inplace = True)\n\ndf_train_store['PromoOpen'] = 12 * (df_train_store.Year - df_train_store.Promo2SinceYear) + (df_train_store.weekofyear - df_train_store.Promo2SinceWeek) \/ float(4)\ndf_train_store['PromoOpen'] = df_train_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\ndf_train_store.drop(['Promo2SinceYear', 'Promo2SinceWeek'], axis = 1,  inplace = True)","426c5b97":"\n#drop na values\ndf_train_store.dropna(inplace = True)\n\n#convert PromoInterval and month_name in string\ndf_train_store['PromoInterval']=df_train_store['PromoInterval'].astype(str)\n","7da5a805":"\ndef checkpromomonth(row):\n if (row['month_name'] in row['PromoInterval']):\n    return 1\n else:\n    return 0\ndf_train_store['IsPromoMonth'] =  df_train_store.apply(lambda row: checkpromomonth(row),axis=1)\n\n#Drop Date,month_name,PromoInterval\ndf_train_store.drop(['Date', 'month_name','PromoInterval'], axis = 1,  inplace = True)","ea0369ee":"#convert num columns into float 32\ndf_train_store.dtypes.value_counts()\nnum_columns= df_train_store.select_dtypes(exclude=[object]).columns \ncat_columns=df_train_store.select_dtypes(include=[object]).columns \nfor col in num_columns:\n    df_train_store[col]=df_train_store[col].astype('float32')\n\nle = preprocessing.LabelEncoder()\nfrom sklearn import preprocessing\nfor col in cat_columns:\n    df_train_store[col]=le.fit_transform(df_train_store[col].astype('str'))\n","52a7359d":"\nimport math\nplt.figure(figsize=(15,18))\nnoofrows= math.ceil(len(num_columns)\/3)\n\n\n#set false.Other wise error if  bandwidth =0 \nsns.distributions._has_statsmodels=False\n\nfor i in range(len(num_columns)):\n plt.subplot(noofrows,3,i+1)\n out=sns.distplot(df_train_store[num_columns[i]]) \n    \nplt.tight_layout()\n\n","762672ca":"df_train_store.Sales.mean()\ndf_train_store.loc[(df_train_store.Sales >= df_train_store.Sales.mean()),'aboveAvgSale']=1\ndf_train_store.loc[(df_train_store.Sales < df_train_store.Sales.mean()),'aboveAvgSale']=0\ndf_train_store.aboveAvgSale.value_counts()\n#define y\ny=df_train_store.aboveAvgSale.astype('int')\ny1=np.log1p(df_train_store['Sales'])\ndf_train_store.drop(['Sales','aboveAvgSale'], axis = 1,  inplace = True)\n\nnum_columns=num_columns.drop(labels=['Sales'])\nct=ColumnTransformer([\n    ('abc',RobustScaler(),num_columns),\n    ('abc1',OneHotEncoder(),cat_columns),\n    ],\n    remainder=\"passthrough\"\n    )\nct.fit_transform(df_train_store)\nX=df_train_store","1db35336":"#store features\ncolnames = X.columns.tolist()","56fb81ac":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.30)","0afa9614":"#credit : https:\/\/www.kaggle.com\/tushartilwankar\/sklearn-rf\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1.\/(y[ind]**2)\n    return w\n\ndef RMSPE(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe ","c8d9e68d":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 15)\nrf.fit(X_train, y_train)\n\n","bd978546":"from sklearn.metrics import accuracy_score\n\n\ny_pred = rf.predict(X_test)\nerror = (RMSPE(y_test,y_pred))\nerror\nprint(\"RMSPE of Random Forest %\",error * 100)\n\n#top 10 features of Random Forest\nfeat_importances_rf = pd.Series(rf.feature_importances_, index=colnames)\nfeat_importances_rf.nlargest(10).sort_values(ascending = True).plot(kind='barh')\nplt.xlabel('importance')\nplt.title('Feature Importance in Random Forest')","b06c35b2":"ss=preprocessing.StandardScaler\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        # Specify other parameters here\n            )\n            ]\n# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)\n# What parameters in the pipe are available for tuning\npipe_xg.get_params()\n\nparameters = {'xg__learning_rate':  [0.03, 0.05], # learning rate decides what percentage\n                                                  #  of error is to be fitted by\n                                                  #   by next boosted tree.\n                                                  # See this answer in stackoverflow:\n                                                  # https:\/\/stats.stackexchange.com\/questions\/354484\/why-does-xgboost-have-a-learning-rate\n                                                  # Coefficients of boosted trees decide,\n                                                  #  in the overall model or scheme, how much importance\n                                                  #   each boosted tree shall have. Values of these\n                                                  #    Coefficients are calculated by modeling\n                                                  #     algorithm and unlike learning rate are\n                                                  #      not hyperparameters. These Coefficients\n                                                  #       get adjusted by l1 and l2 parameters\n              'xg__n_estimators':   [200,  300],  # Number of boosted trees to fit\n                                                  # l1 and l2 specifications will change\n                                                  # the values of coeff of boosted trees\n                                                  # but not their numbers\n\n              'xg__max_depth':      [4,6],\n              'pca__n_components' : [10,15]\n              }  \n\n","66c244fd":"import time\nclfgs = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =2 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )\n#Start fitting data to pipeline\nstart = time.time()\nclfgs.fit(X_train, y_train)\n   ","c28206d5":"y_pred = clfgs.predict(X_test)\ny_pred\n\n# 7.5 Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy using GridSearchCV: {accuracy * 100.0}\"             \n\n# 7.6 Confusion matrix\n\n\nfrom sklearn.metrics import confusion_matrix\ncm_gs = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm_gs, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Predicted vs Actual -GridSearchCV ')\n\n# 7.7 F1 score\nf1_score(y_test,y_pred, pos_label = 1)      \nf1_score(y_test,y_pred, pos_label = 0)      \n\n# 7.8 ROC curve\nplot_roc_curve(clfgs, X_test, y_test)\n\n\n# Get feature importances from GridSearchCV best fitted 'xg' model\n#     See stackoverflow: https:\/\/stackoverflow.com\/q\/48377296\nclfgs.best_estimator_.named_steps[\"xg\"].feature_importances_\nclfgs.best_estimator_.named_steps[\"xg\"].feature_importances_.shape\n\n","01fd8989":"#Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,300),\n              'xg__max_depth':      range(3,10),\n              'pca__n_components' : range(10,17)}\n\n\n\n# 8.1 Tune parameters using random search\n#     Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          \n                                            \n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               \n                                             \n                        )\n\n\n# \nrs.fit(X_train, y_train)\n","1bc93502":"y_pred = rs.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy using randomized search: {accuracy * 100.0}\"        \nf1_score(y_test,y_pred, pos_label = 1) ","d27d86c6":"model_gs = XGBClassifier(\n                    learning_rate = clfgs.best_params_['xg__learning_rate'],\n                    max_depth = clfgs.best_params_['xg__max_depth'],\n                    n_estimators=clfgs.best_params_['xg__max_depth']\n                    )\n\n# 9.1 Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__max_depth']\n                    )\n\n\n# Modeling with both parameters\n\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\n\n#Predictions with both models\ny_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\n\n#Accuracy from both models\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nprint(\"Accuracy with GridSearch XGB model:\",accuracy_gs*100)\nprint(\"Accuracy with Random search XGB model:\",accuracy_rs*100)\n\nrmspe_gs = RMSPE(y_pred_gs,y_test)\nrmspe_rs = RMSPE(y_pred_rs,y_test)\nprint(\"RMSPE of GridSearch XGB modelt %\",rmspe_gs * 100)\nprint(\"RMSPE of Random search XGB modelt %\",rmspe_rs * 100)","b40070b1":"\n\n#  Plt now\n\n%matplotlib inline\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\n# Importance type: 'weight'\nplot_importance(\n                model_gs,\n                importance_type = 'weight'   # default\n                )\n#  Importance type: 'gain'\n#        # Normally use this\nplot_importance(\n                model_rs,\n                importance_type = 'gain', \n                title = \"Feature impt by gain\"\n                )\nplt.show()","ccc4bfa9":"### Call libraries","6d2c0065":"##### feature importances from Grid Search and Random Search","9da4867a":"##### accuracy and predictions of RandomSearchCV","4152c6a1":"### Random forest","c2871144":"#### Check train database if any NULL value","ca4d81f8":"#####  check RMSPE & Feature Importance of Random Forest","83877fca":"#### About Data","39edcced":"### Files:\n\n* **train.csv**: historical data including Sales\n* **test.csv**: historical data excluding Sales\n* **sample_submission.csv**: a sample submission file in the correct format\n* **store.csv**: supplemental information about the stores\n\n\n************************************************************************\n### Data fields\n* ** Most of the fields are self-explanatory. The following are descriptions for those that aren't.**:\n\n* **Id** - an Id that represents a (Store, Date) duple within the test set\n* **Store** - a unique Id for each store\n* **Sales** - the turnover for any given day (this is what you are predicting)\n* **Customers** - the number of customers on a given day\n* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n* **StoreType** - differentiates between 4 different store models: a, b, c, d\n* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended\n* **CompetitionDistance** - distance in meters to the nearest competitor store\n* **CompetitionOpenSince[Month\/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n* **Promo** - indicates whether a store is running a promo on that day\n* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* **Promo2Since[Year\/Week]** - describes the year and calendar week when the store started participating in Promo2\n* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","72c501b1":"###### AUC is Outstanding so discrimination between positive class and negative class","f0c0fa77":"#### Define Root Mean Square Percentage Error\n\nquadratic scoring rule that also measures the average magnitude of the error. It\u2019s the square root of the average of squared differences between prediction and actual observation.","4711dce0":"### XGBoost","8bc3e23b":"#### calculate competition open and promo open in months","05b36ba5":"-----------------------------------------------------------------------------------------------------","29fe98a4":"*  **There are outliers for most of columns so we will use RobustScaler for num_columns** \n* **OneHotEncoder for cat_columns**\n* **define avgsale\n","4d1f73b6":"##### accuracy and predictions of GridSearchCV","dd2f3236":"*********************************************************************************************","8dd95891":"#### *Observations*  : \n* ** Customer is most important feature in Random forest,Grid search,Random search\n* ** Competition is second most important feature in Random forest,Grid search\n* ** RMSPE is least in random forest","b6d8a6bf":"# Rossmann Store Sales using Random Forest and XGBoost\n#### Author : Rohini Garg","dac57af1":"### consider only open stores and sale > 0 .Because data is already too big","703d37a3":"####  Calculate compition open and promo time in months","0d5dfea8":"#### Tune parameters using  gridsearch","e119a1d3":"### Objective of  analysis of Rossmann Store Sales?\n#### Forecast sales using store, promotion, and competitor data","0d0861ca":"## Read Data","47e5fec7":"#### Tuning parameters using randomized search","67c1f299":"### merge data frames","051515f3":"#### split data in 7:3 ratio so set test size=30"}}