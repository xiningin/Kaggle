{"cell_type":{"ab9e21f2":"code","26044d48":"code","88a26b47":"code","2965ea75":"code","955ed3da":"code","f25ba503":"code","6a3dd081":"code","50e9bd81":"code","9cf27535":"code","058ca3cd":"code","124eb96a":"code","096d80b7":"code","e0d18acd":"code","38f5d7e7":"code","6bfd170f":"code","f82d7a11":"code","e86eacdb":"code","c63bb292":"code","001348ea":"code","dd9f5bec":"code","12e76b17":"code","558c790f":"code","aad7a850":"code","9236d284":"code","a8d2158a":"code","04f1cade":"code","9a705cf5":"code","ced176b2":"code","baee77f7":"code","7baaa69f":"code","ed91a854":"code","c4375f42":"code","ac84e180":"code","fd6f912f":"code","3b2194ee":"code","9bef9c7c":"code","7fa11a73":"code","628c4cd9":"code","a0708993":"code","b501dec2":"code","4dc2f174":"code","be114732":"code","f52ede30":"code","74c755a4":"code","5fd64821":"code","018feb11":"code","a2cc5a69":"code","344e9bb0":"code","13cf5956":"code","a893e38f":"code","c8c403ea":"code","a94129dd":"code","6d1dd95a":"code","931694a6":"code","288483ef":"code","1951b3d8":"code","e81e6ae3":"code","332f993b":"code","5b7a8c71":"code","cd4844a8":"code","52db1f47":"code","3a6b68d3":"code","785128ac":"code","41808819":"code","b5e253ce":"code","71bfc6d6":"code","04074f5e":"code","689a535c":"code","4553bf00":"code","9f7cbc72":"code","fdc9b2d3":"code","dd6a3b59":"code","f1b4c732":"code","eedbf7ea":"code","07c9df1e":"code","3956fc8a":"code","4fe6107b":"code","5dfbed4d":"code","1751e71d":"code","1a0ac5a1":"code","1a39890e":"code","a0a73a7f":"code","c4aa24ee":"code","64d49bcb":"code","cd568797":"code","47c5670a":"code","d0fd78b1":"code","7516763d":"code","106b9e19":"code","25a7ec82":"code","a3ea42ce":"code","0cf4e188":"code","0d656e5a":"code","7d9d4ddb":"code","8acb4fc2":"code","9b317e22":"code","f917a422":"code","38fa8d7e":"code","b0342da5":"code","2895edc5":"code","f703b50e":"code","086c2a58":"code","829cf014":"code","03f8563f":"code","600457b7":"code","fa97dd66":"code","099b4bf7":"code","4433a8fb":"code","da0a27f5":"code","0dbcfab2":"code","5ebfc5d7":"code","8a0ac814":"code","1fed79af":"code","4db9835a":"code","760e2002":"code","a292b94d":"code","14ccde80":"code","9f022107":"code","08bcc2d0":"code","d0a90aef":"code","63cdf533":"code","ba0f83ab":"code","788554ee":"code","22bef44c":"code","b015bb8d":"code","453de8a7":"code","67b31e9b":"markdown","fc517ddb":"markdown","831f49bd":"markdown","04d2f3a8":"markdown","704a529c":"markdown","0157836d":"markdown","d88a81ed":"markdown","5cbc4d06":"markdown","48b1a918":"markdown","4643d8f6":"markdown","2b39de31":"markdown","0307363d":"markdown","d17359ec":"markdown","07e76191":"markdown","c1e41183":"markdown","67b589d5":"markdown","50b833a0":"markdown","8f0b274a":"markdown","6c7d302a":"markdown","3b4ef5df":"markdown","b61e8ebf":"markdown","339ae6db":"markdown","ebd26330":"markdown","082ab288":"markdown","50fce47d":"markdown","407fb568":"markdown","0dcbdd1a":"markdown","0fcb5abb":"markdown","fb40b5bd":"markdown","6ac5bb40":"markdown","091bcc86":"markdown","6cdae91d":"markdown","116c8675":"markdown"},"source":{"ab9e21f2":"#Importing Required Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_ind\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.stats.diagnostic import linear_rainbow\nimport scipy.stats as stats\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom statsmodels.tools.eval_measures import rmse\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom statsmodels.tsa.stattools import adfuller\n#from pandas.plotting import autocorrelation_plo\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nimport statsmodels\nfrom pandas.tseries.offsets import DateOffset","26044d48":"plt.rcParams[\"figure.figsize\"] = (18,8) # Setting the size of the images used here.","88a26b47":"train = pd.read_csv('..\/input\/walmart-sales-prediction\/train.csv', parse_dates=['Date'])  # Training data\nfeatures = pd.read_csv('..\/input\/walmart-sales-prediction\/features.csv', parse_dates=['Date']) # Features\nstores = pd.read_csv('..\/input\/walmart-sales-prediction\/stores.csv')  # Store Details","2965ea75":"train_stores = pd.merge(left=train, right=stores, how='left', on='Store')  # Merging Train data and Stores Details","955ed3da":"df = pd.merge(left=train_stores, right=features, how='left', on=['Store','Date','IsHoliday'])  # Merging Train data, Stores and Features\ndf.head()","f25ba503":"print(f'Total number of rows --> {df.shape[0]}')\nprint(f'Total number of columns --> {df.shape[1]}')","6a3dd081":"df.info()  # Information about the columns","50e9bd81":"df.describe().T  # Five Point Summary","9cf27535":"df.columns   # Columns present in the dataset","058ca3cd":"#Plotting a heatmap to check the missing values\nsns.heatmap(data = df.isna(), yticklabels=False, cbar=False, cmap='Wistia')\nplt.title('Missing Values')\nplt.show()","124eb96a":"#Number of missing values\ndf.isna().sum().sort_values(ascending=False).head()","096d80b7":"#Percentage of missing values\ndf.isna().sum().sort_values(ascending=False).head()\/df.shape[0]*100","e0d18acd":"#Imputing the missing values with 0 as it means there is no discount available there\ndf.fillna(0, inplace=True)","38f5d7e7":"#Plotting a heatmap again to confirm that there are no missing values\nsns.heatmap(data = df.isna(), yticklabels=False, cbar=False, cmap='Wistia')\nplt.title('Missing Values')\nplt.show()","6bfd170f":"df_markdown = df[['MarkDown1', 'MarkDown2', 'MarkDown3','MarkDown4', 'MarkDown5', 'Weekly_Sales']].copy()  # Creating a new dataframe with all MarkDowns and Weekly Sales.\ndf_markdown['Is_MarkDown'] = df_markdown.iloc[:,0:5].sum(axis=1).apply(lambda x: 0 if x == 0 else 1) # Add one new column is_MarkDown","f82d7a11":"df_is_markdown = df_markdown.loc[df_markdown['Is_MarkDown']==1, 'Weekly_Sales']  # Weekly Sales with MarkDown\ndf_no_markdown = df_markdown.loc[df_markdown['Is_MarkDown']==0, 'Weekly_Sales']  # Weekly Sales without MarkDown","e86eacdb":"df_is_markdown.shape, df_no_markdown.shape  # NUmber of records with and without MarkDown","c63bb292":"print(f'Average Weekly Sales with Markdown --> {df_is_markdown.mean():.2f}\\nAverage Weekly Sales without Markdown --> {df_no_markdown.mean():.2f}')","001348ea":"ttest_ind(df_is_markdown, df_no_markdown)  # Two Sample Independent T Test performed on weekly sales with MarkDown andd weekly sales without MarkDown","dd9f5bec":"mask = np.triu(np.ones_like(df.corr(), dtype=bool))\nsns.heatmap(data=df.corr(), annot=True, cmap='afmhot_r', mask=mask)  # Heatmap for correlation\nplt.title('Correlation Matrix')\nplt.show()","12e76b17":"cols_outlier = df[['Weekly_Sales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature','MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'Unemployment']]\nfig, axes = plt.subplots(4,3,figsize=(18,12))\nfig.suptitle('Outliers in the numerical features',fontsize=18, color = '#06917e', x = 0.5, y = 1.05)\nindex = [(i,j) for i in range(4) for j in range(3)]\nindex_count=0\nfor col in cols_outlier.columns:\n    sns.boxplot(x=col, ax=axes[index[index_count]], data=df, palette='afmhot_r')\n    index_count += 1\n    plt.tight_layout()\nplt.show()","558c790f":"sns.distplot(df['Weekly_Sales'], bins=30, kde=True)  # Distribution of Target Variable 'Weekly_Sales'\nplt.show()","aad7a850":"sns.boxplot(x='Weekly_Sales', y='IsHoliday', data=df, orient='h', palette='afmhot_r')  # Effect of IsHoliday on Weekly Sales\nplt.show()","9236d284":"fig, axes = plt.subplots(3,2,figsize=(18,12))\nax_index = [(i,j) for i in range(3) for j in range(2)]\nindex_number = 0\nfig.suptitle('Effect of various factors on Weekly Sales',fontsize=18, color = '#06917e', y = 1.05)\nfor i in ['Unemployment','IsHoliday','Size','CPI','Temperature','Fuel_Price']:\n    sns.scatterplot(x=i, y='Weekly_Sales', data=df, ax=axes[ax_index[index_number]], palette='afmhot_r')\n    index_number += 1\n    plt.tight_layout()","a8d2158a":"sns.boxplot(x='Size', y='Type', data=df, palette='afmhot_r')\nplt.title('Size of the Store with respect to Type')\nplt.show()","04f1cade":"TypewiseSize = df.groupby(by='Type')['Size']\nprint(\"Median Size for Type A Stores --> \",TypewiseSize.get_group('A').median())\nprint(\"Median Size for Type B Stores --> \",TypewiseSize.get_group('B').median())\nprint(\"Median Size for Type C Stores --> \",TypewiseSize.get_group('C').median())","9a705cf5":"sns.boxplot(y='Type',x='Weekly_Sales', data=df, orient='h', palette='afmhot_r')\nplt.title('Type wise Weekly Sales')\nplt.show()","ced176b2":"TypewiseSales = df.groupby(by='Type')['Weekly_Sales']\nprint(\"Median Weekly Sales for Type A Stores --> \",TypewiseSales.get_group('A').median())\nprint(\"Median Weekly Sales for Type B Stores --> \",TypewiseSales.get_group('B').median())\nprint(\"Median Weekly Sales for Type C Stores --> \",TypewiseSales.get_group('C').median())","baee77f7":"#Average Sales per stores\navg_sales_per_store = df.groupby(by='Store')['Weekly_Sales'].mean()\nsns.barplot(x = avg_sales_per_store.index, y=avg_sales_per_store)\nplt.title('Average Sales per Store')\nplt.show()","7baaa69f":"avg_sales_per_store.sort_values(ascending = False).head()  # Top 5 most average weekly sales stores.","ed91a854":"avg_sales_per_store.sort_values(ascending = False).tail()  # Bottom 5 less average weekly sales stores.","c4375f42":"#Average Sales per Department\navg_sales_per_dept = df.groupby(by='Dept')['Weekly_Sales'].mean()\nsns.barplot(x = avg_sales_per_dept.index, y=avg_sales_per_dept)\nplt.title('Average Sales per Department')\nplt.xticks(rotation = 90)\nplt.show()","ac84e180":"avg_sales_per_dept.sort_values().head(6)  # Bottom 6 departments with least average weekly sales. ","fd6f912f":"#Total Sales per stores\ntotal_sales_per_store = df.groupby(by='Store')['Weekly_Sales'].sum()\nsns.barplot(x = total_sales_per_store.index, y=total_sales_per_store)\nplt.title('Total Sales per Store')\nplt.show()","3b2194ee":"total_sales_per_year = df.groupby(by=[df['Date'].dt.year, 'Type'])['Weekly_Sales'].sum()\ng = total_sales_per_year.unstack().plot(kind='bar')\nfor p in g.patches:\n    g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.085, p.get_height()),ha='center', va='bottom',color= 'black')\nplt.title('Total Sales per Year - Type Wise')\nplt.xticks(rotation=0)\nplt.show()","9bef9c7c":"sns.barplot(x=df['Date'].dt.isocalendar().week, y=\"Weekly_Sales\", data=df, ci=None)\nplt.title('Week wise Total Sales')\nplt.show()","7fa11a73":"df_2010 = df.loc[ (df['Date'].dt.year==2010), ['Date', 'Weekly_Sales'] ].groupby(by='Date').sum()\ndf_2011 = df.loc[ (df['Date'].dt.year==2011), ['Date', 'Weekly_Sales'] ].groupby(by='Date').sum()\ndf_2012 = df.loc[ (df['Date'].dt.year==2012), ['Date', 'Weekly_Sales'] ].groupby(by='Date').sum()\n\na10 = pd.DataFrame(data = {'Week_num':df_2010.index.isocalendar().week , 'Sales_2010':df_2010['Weekly_Sales']})\na11 = pd.DataFrame(data = {'Week_num':df_2011.index.isocalendar().week , 'Sales_2011':df_2011['Weekly_Sales']})\na12 = pd.DataFrame(data = {'Week_num':df_2012.index.isocalendar().week , 'Sales_2012':df_2012['Weekly_Sales']})\n\nx = pd.merge(a11, a10, how='outer', on='Week_num')\ny = pd.merge(a12, x, how='outer', on='Week_num')\n\nfor i in y.columns[1:]:\n    plt.plot(y['Week_num'], y[i], label=i)\nplt.ylabel(\"Sales in millions dollars\")\nplt.xlabel(\"Week of the Year\")\nplt.xticks(np.arange(1,53))\nplt.yticks(np.arange(20000000, 85000000, 5000000))\nplt.title('Weekly Sales over the Years')\nplt.legend()\nplt.grid()\nplt.show()","628c4cd9":"df['Type'].value_counts(normalize=True).plot(kind='pie', autopct='%.2f', explode=[0.05,0.05,0.05])\nplt.legend(df['Type'].value_counts(normalize=True).index, loc = 'upper right')\nplt.title('Distribution of Store Types')\nplt.show()","a0708993":"month_wise_avg_sales=df.groupby(df['Date'].dt.month)['Weekly_Sales'].mean()\nplt.title('Month wise Average Sales')\ng = sns.barplot(x=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], y=month_wise_avg_sales)\nfor p in g.patches:\n    g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),ha='center', va='bottom',color= 'black')","b501dec2":"df1 = df.drop(columns=['Date'])  # Dropping Date column as most of the values are unique","4dc2f174":"df1['IsHoliday'] = df1['IsHoliday'].apply(lambda x : 1 if x==True else 0)  # Label Encoding","be114732":"df1 = pd.get_dummies(df1, drop_first=True)  # get_dummies for 'Type' column\ndf1.head()","f52ede30":"inp = df1.drop('Weekly_Sales',1)  # Independent Features\nout = df1['Weekly_Sales']  # Dependent Features","74c755a4":"sc=StandardScaler()\ninp_sc=sc.fit_transform(inp.iloc[:,2:])\ninp_sc=pd.DataFrame(inp_sc,columns=inp.iloc[:,2:].columns)\ninp_sc = pd.concat((inp.iloc[:,0:2],inp_sc),axis=1)\ninp_sc.head(2)","5fd64821":"xtrain,xtest,ytrain,ytest = train_test_split(inp_sc,out,test_size = 0.3, random_state = 40)  # Splitting data into Train and Test","018feb11":"# Creating a base model using OLS\ninpc = sm.add_constant(inp_sc)\nols = sm.OLS(out,inpc)\nols_mod = ols.fit()\nols_mod.summary()","a2cc5a69":"# Linear Base Model using Scikit-learn\nlr=LinearRegression()\nlr.fit(xtrain,ytrain)\nypred=lr.predict(xtest)\nprint('R-Square Value:',r2_score(ytest,ypred))\nrmse=np.sqrt(mean_squared_error(ytest,ypred))\nprint('RMSE:',rmse)","344e9bb0":"#Checking for Assumptions:\n#Multi-collinearity\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(inp_sc.values,i) for i in range(inp_sc.shape[1])]\nvif['Features'] = inp_sc.columns\nvif.sort_values('VIF', ascending= False)","13cf5956":"#Auto Correlation\ninpc = sm.add_constant(xtrain)\nols = sm.OLS(ytrain,inpc)\nols_mod = ols.fit()\nols_mod.summary()","a893e38f":"#Linearity\ninpc = sm.add_constant(inp_sc)\nols = sm.OLS(out,inpc)\nols_mod = ols.fit()\nstat,p_value = linear_rainbow(res = ols_mod, frac = 0.5)\nstat,p_value","c8c403ea":"#Homoscedasticity\nsns.scatterplot(ols_mod.predict(),ols_mod.resid)\nplt.show()","a94129dd":"#Feature Selection\nlr=LinearRegression()\nbackward=sfs(estimator=lr,k_features='best',forward=False,scoring='r2')\nsfs_backward=backward.fit(inp_sc,out)\nfeat_back=sfs_backward.k_feature_names_\nprint('Best Features using Backward Elimination:\\n',feat_back)","6d1dd95a":"lr=LinearRegression()\nforward=sfs(estimator=lr,k_features='best',forward=True,scoring='r2')\nsfs_forward=forward.fit(inp_sc,out)\nfeat_forw=sfs_forward.k_feature_names_\nprint('Best Features using Forward Selection:\\n',feat_forw)","931694a6":"lr=LinearRegression()\nrfe=RFECV(estimator=lr)\nrfe_mod=rfe.fit(inp_sc,out)\nrfe_mod.ranking_","288483ef":"rank=pd.DataFrame()\nrank['Features']=xtrain.columns\nrank['RANK']=rfe_mod.ranking_\nfeat_rfe=rank[rank['RANK']==1]['Features']\nrank.sort_values(by='RANK')\nprint('Best Features using Recursive Feature Elimination:\\n',feat_rfe)","1951b3d8":"feat_back =list(feat_back)\nfeat_forw =list(feat_forw)\nfeat_rfe=list(feat_rfe)","e81e6ae3":"#Building Model using features got by Backward Elimination\nlr=LinearRegression()\nlr.fit(xtrain[feat_back],ytrain)\nypred=lr.predict(xtest[feat_back])\n\nr2=r2_score(ytest,ypred)\nrmse=np.sqrt(mean_squared_error(ytest,ypred))\n\nres_back=[r2,rmse]\nres_back","332f993b":"#Building Model using features got by Forward Selection\nlr=LinearRegression()\nlr.fit(xtrain[feat_forw],ytrain)\nypred=lr.predict(xtest[feat_forw])\n\nr2=r2_score(ytest,ypred)\nrmse=np.sqrt(mean_squared_error(ytest,ypred))\n\nres_forw=[r2,rmse]\nres_forw","5b7a8c71":"#Building Model using features got by Recursive Feature Elimination\nlr=LinearRegression()\nlr.fit(xtrain[feat_rfe],ytrain)\nypred=lr.predict(xtest[feat_rfe])\n\nr2=r2_score(ytest,ypred)\nrmse=np.sqrt(mean_squared_error(ytest,ypred))\n\nres_rfe=[r2,rmse]\nres_rfe","cd4844a8":"score_card=pd.DataFrame()\nscore_card['Backward_Elmination']=res_back\nscore_card['Forward_Selection']=res_forw\nscore_card['RFE']=res_rfe\nscore_card.index=['Rsquare','RMSE']\nscore_card","52db1f47":"# Cross Validation Score using RFE\nlr=LinearRegression()\nres=cross_val_score(lr,inp_sc[feat_rfe],out,cv=3,scoring='neg_mean_squared_error')\nrmse=np.sqrt(abs(res))\nbe=np.mean(rmse)\nve=np.std(rmse)\ncvv=np.std(rmse)\/np.mean(rmse)\nres_lr=[be,ve,cvv]\nres_lr","3a6b68d3":"inp_sc=inp_sc[feat_rfe]","785128ac":"#Regularization\n# Ridge Model\nridge=Ridge()\nparam={'alpha':[0.0001,0.001,0.005,0.01,0.5,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]}\ngrid=GridSearchCV(ridge,param_grid=param,cv=3,scoring='neg_mean_squared_error')\nmod_hyp=grid.fit(inp_sc,out) \nprint(mod_hyp.best_params_) \nprint(abs(mod_hyp.best_score_))","41808819":"ridge = Ridge(alpha = 100)\nres = cross_val_score(ridge,inp_sc,out,cv = 3,scoring = 'neg_mean_squared_error')\nrmse = np.sqrt(abs(res))\nbe = np.mean(rmse) #bias error\nve = np.std(rmse) #variance error\ncve = be\/ve #coefficient of variance\nres_rid = [be,ve,cve]\nres_rid","b5e253ce":"# Lasso Model\nlasso=Lasso()\nparam={'alpha':[0.0001,0.001,0.005,0.01,0.5,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]}\ngrid=GridSearchCV(lasso,param_grid=param,cv=3,scoring='neg_mean_squared_error')\nmod_hyp=grid.fit(inp_sc,out) \nprint(mod_hyp.best_params_) \nprint(abs(mod_hyp.best_score_))","71bfc6d6":"lasso = Lasso(alpha = 100)\nres = cross_val_score(lasso,inp_sc,out,cv = 3,scoring = 'neg_mean_squared_error')\nrmse = np.sqrt(abs(res))\nbe = np.mean(rmse) #bias error\nve = np.std(rmse) #variance error\ncve = be\/ve #coefficient of variance\nres_las = [be,ve,cve]\nres_las","04074f5e":"#ElasticNet Model\nenet=ElasticNet()\nparam={'alpha':[0.0001,0.001,0.005,0.01,0.5,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]}\ngrid=GridSearchCV(enet,param_grid=param,cv=3,scoring='neg_mean_squared_error')\nmod_hyp=grid.fit(inp_sc,out) \nprint(mod_hyp.best_params_) \nprint(abs(mod_hyp.best_score_))","689a535c":"enet = ElasticNet(alpha = 0.5)\nres = cross_val_score(enet,inp_sc,out,cv = 3,scoring = 'neg_mean_squared_error')\nrmse = np.sqrt(abs(res))\nbe = np.mean(rmse) #bias error\nve = np.std(rmse) #variance error\ncve = be\/ve #coefficient of variance\nres_enet = [be,ve,cve]\nres_enet","4553bf00":"score_card=pd.DataFrame() \nscore_card['LR']=res_lr \nscore_card['Ridge']=res_rid \nscore_card['Lasso']=res_las \nscore_card['ElasticNet']=res_enet \nscore_card.index=['Bias Error','Variance Error', 'Coefficient of Variance'] \nscore_card","9f7cbc72":"x = inp_sc\ny = out","fdc9b2d3":"from statsmodels.tools.eval_measures import rmse","dd6a3b59":"dtree=  DecisionTreeRegressor()\ndtree.fit(xtrain,ytrain)\n\nytrain_pred = dtree.predict(xtrain)\nytest_pred = dtree.predict(xtest)\n\nprint('RMSE score of train data: ', rmse(ytrain, ytrain_pred) )\nprint('R^2 score of train data: ',r2_score(ytrain, ytrain_pred) )\n\nprint('RMSE score of test data: ', rmse(ytest, ytest_pred) )\nprint('R^2 score of test data: ', r2_score(ytest, ytest_pred) )","f1b4c732":"result_rmse_score = pd.DataFrame(index=['Training','Testing'])\nresult_r2_score = pd.DataFrame(index=['Training','Testing'])","eedbf7ea":"result_rmse_score['DT Base Model'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['DT Base Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","07c9df1e":"dtree = DecisionTreeRegressor() # estimator\n\n\nparam_dist = {'max_depth' : np.arange(5,20),\n             'min_samples_leaf':[15,17,20,25,30,35,40],\n              'min_samples_split':[2,5,8,10,12,15],\n              'criterion':['mse']}\n\n\nrsearch  = RandomizedSearchCV(dtree, param_distributions = param_dist, cv=4) \n\nrsearch.fit(x,y)\nrsearch.best_params_","3956fc8a":"dtree_rand_tuned = DecisionTreeRegressor(**rsearch.best_params_)\ndtree_rand_tuned.fit(xtrain,ytrain)\n\n\nytrain_pred = dtree_rand_tuned.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred) )\nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred = dtree_rand_tuned.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred) )\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","4fe6107b":"result_rmse_score['DT Tuned Model'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['DT Tuned Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","5dfbed4d":"result_rmse_score","1751e71d":"result_r2_score","1a0ac5a1":"rf1 = RandomForestRegressor()\nrf1.fit(xtrain, ytrain)\n\n\nytrain_pred =  rf1.predict(xtrain)\nprint('RMSE of Train Data: ', rmse(ytrain, ytrain_pred))\nprint('R^2 score of Train Data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred =  rf1.predict(xtest)\nprint('RMSE of Test Data: ', rmse(ytest, ytest_pred))\nprint('R^2 score of Test Data: ', r2_score(ytest, ytest_pred))","1a39890e":"result_rmse_score['RF Base Model'] = [rmse(ytrain, ytrain_pred),rmse(ytest, ytest_pred) ]\nresult_r2_score['RF Base Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","a0a73a7f":"result_rmse_score","c4aa24ee":"result_r2_score","64d49bcb":"rf2 = RandomForestRegressor()\n\n\nparam_dist = { 'n_estimators':sp_randint(50,100),\n              'max_features': sp_randint(1,14),\n              'max_depth' : sp_randint(5,20),\n             'min_samples_leaf':sp_randint(10,50),\n              'min_samples_split':sp_randint(2,50)}\n\n\nrsearch_rf  = RandomizedSearchCV(estimator=rf2, param_distributions = param_dist, cv=4, random_state=4) \n\nrsearch_rf.fit(x,y)\nrsearch_rf.best_params_","cd568797":"rsearch_rf.best_score_","47c5670a":"rf_tuned = RandomForestRegressor(**rsearch_rf.best_params_)\n\nrf_tuned.fit(xtrain,ytrain)\n\nytrain_pred = rf_tuned.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = rf_tuned.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","d0fd78b1":"result_rmse_score['RF Tuned Model'] = [rmse(ytrain, ytrain_pred),rmse(ytest, ytest_pred) ]\nresult_r2_score['RF Tuned Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","7516763d":"result_rmse_score","106b9e19":"result_r2_score","25a7ec82":"result_rmse_score","a3ea42ce":"rf_tuned.fit(xtrain,ytrain)\npd.DataFrame(index=xtrain.columns, data=rf_tuned.feature_importances_, columns=['feat']).sort_values(by='feat',ascending=False)","0cf4e188":"et = ExtraTreesRegressor()\net.fit(xtrain, ytrain)\n\nytrain_pred =  et.predict(xtrain)\nprint('RMSE of Train Data: ', rmse(ytrain, ytrain_pred))\nprint('R^2 score of Train Data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred =  et.predict(xtest)\nprint('RMSE of Test Data: ', rmse(ytest, ytest_pred))\nprint('R^2 score of Test Data: ', r2_score(ytest, ytest_pred))","0d656e5a":"result_rmse_score['ETR Base Model'] = [rmse(ytrain, ytrain_pred),rmse(ytest, ytest_pred) ]\nresult_r2_score['ETR Base Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","7d9d4ddb":"result_rmse_score","8acb4fc2":"result_r2_score","9b317e22":"etr2 = ExtraTreesRegressor()\n\n\nparam_dist = { 'n_estimators':sp_randint(50,100),\n              'max_features': sp_randint(1,14),\n              'max_depth' : sp_randint(5,20),\n             'min_samples_leaf':sp_randint(10,50),\n              'min_samples_split':sp_randint(2,50)}\n\n\nrsearch_etr  = RandomizedSearchCV(estimator=etr2, param_distributions = param_dist, cv=4, random_state=4) \n\nrsearch_etr.fit(x,y)\nrsearch_etr.best_params_","f917a422":"etr_tuned = ExtraTreesRegressor(**rsearch_etr.best_params_)\netr_tuned.fit(xtrain, ytrain)\n\nytrain_pred = etr_tuned.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = etr_tuned.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","38fa8d7e":"result_rmse_score['ETR Tuned Model'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['ETR Tuned Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","b0342da5":"result_rmse_score","2895edc5":"result_r2_score","f703b50e":"etr_tuned.fit(xtrain, ytrain)\netr_tuned.feature_importances_","086c2a58":"pd.DataFrame(index=xtrain.columns, data=etr_tuned.feature_importances_, columns=['feat']).sort_values(by='feat',ascending=False)","829cf014":"ada = AdaBoostRegressor(random_state=48)\nada.fit(xtrain,ytrain)\n\nytrain_pred = ada.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = ada.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","03f8563f":"result_rmse_score['AdaBoost'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['AdaBoost'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","600457b7":"grad = GradientBoostingRegressor(random_state=48)\ngrad.fit(xtrain,ytrain)\n\nytrain_pred = grad.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = grad.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","fa97dd66":"result_rmse_score['Gradient Boost'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['Gradient Boost'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","099b4bf7":"lgbmr = LGBMRegressor(random_state=48)\nlgbmr.fit(xtrain,ytrain)\n\nytrain_pred = lgbmr.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = lgbmr.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","4433a8fb":"result_rmse_score['LightGBM'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['LightGBM'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","da0a27f5":"lgbmr.fit(xtrain, ytrain)\nlgbmr.feature_importances_","0dbcfab2":"pd.DataFrame(index=xtrain.columns, data=lgbmr.feature_importances_, columns=['feat']).sort_values(by='feat',ascending=False)","5ebfc5d7":"xg = XGBRegressor(random_state=48)\nxg.fit(xtrain,ytrain)\n\nytrain_pred = xg.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = xg.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","8a0ac814":"result_rmse_score['XGBoost'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['XGBoost'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","1fed79af":"xg.fit(xtrain, ytrain)\nxg.feature_importances_","4db9835a":"pd.DataFrame(index=xtrain.columns, data=xg.feature_importances_, columns=['feat']).sort_values(by='feat',ascending=False)","760e2002":"from sklearn.neighbors import KNeighborsRegressor","a292b94d":"knn = KNeighborsRegressor()\nknn.fit(xtrain,ytrain)\n\nytrain_pred = knn.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = knn.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","14ccde80":"result_rmse_score['KNN'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['KNN'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","9f022107":"result_rmse_score","08bcc2d0":"result_r2_score","d0a90aef":"rf_tuned = RandomForestRegressor(**rsearch_rf.best_params_)\netr_tuned = ExtraTreesRegressor(**rsearch_etr.best_params_)\nlgbmr = LGBMRegressor(random_state=48)\nxg = XGBRegressor(random_state=48)\n\nestimators = [('rf_tuned', rf_tuned),('etr_tuned',etr_tuned),('lgbmr', lgbmr), ('xg',xg)]\n\nstack1 = VotingRegressor(estimators=estimators)\n\nstack1.fit(xtrain, ytrain)\n\nytrain_pred = stack1.predict(xtrain)\nprint('RMSE on train data: ', rmse(ytrain, ytrain_pred)) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = stack1.predict(xtest)\nprint('RMSE on test data: ', rmse(ytest, ytest_pred))\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))\n\nresult_rmse_score['Stacked Model'] = [rmse(ytrain, ytrain_pred), rmse(ytest, ytest_pred) ]\nresult_r2_score['Stacked Model'] = [r2_score(ytrain, ytrain_pred),r2_score(ytest, ytest_pred) ]","63cdf533":"result_rmse_score","ba0f83ab":"result_r2_score","788554ee":"result_rmse_score.T","22bef44c":"rmse_1 =  result_rmse_score.drop(columns=['AdaBoost']).T\nr2_1 =  result_r2_score.drop(columns=['AdaBoost']).T","b015bb8d":"plt.figure(figsize=(18,5))\nplt.plot(rmse_1['Training'], label='Train')\nplt.plot(rmse_1['Testing'], label='Test')\nplt.title('RMSE score')\nplt.legend()","453de8a7":"plt.figure(figsize=(18,5))\nplt.plot(r2_1['Training'], label='train')\nplt.plot(r2_1['Testing'], label='Test')\nplt.title('R2 score')\nplt.legend()","67b31e9b":"**RFE is giving comparatively bettr result.**","fc517ddb":"#### LightGBM:","831f49bd":"#### Creating tuned model with best params of RandomizedCV object","04d2f3a8":"### Extra Tree Regressor\n#### Base Model","704a529c":"<h2 style='font-family:rockwell; color:#06917e'> Linear Model:<\/h2>","0157836d":"#### Gradient Boost:","d88a81ed":"**As the value of Durbin-Watson test near 2, we can say there is no auto correlation present.**","5cbc4d06":"#### Tuning the Extra Tree Regressor","48b1a918":"#### XGBoost:","4643d8f6":"**Average number of departments in each type**","2b39de31":"## Stacking Algorithms","0307363d":"### RMSE Scorecard:","d17359ec":"#### Tuning with RandomizedCV","07e76191":"**From the graph, we can say that the model is homoscedastic.**","c1e41183":"<h2 style='font-family:rockwell; color:#06917e'> Non-Linear Models:<\/h2>","67b589d5":"**As the P-value greater than 0.05, It is following linearity.**","50b833a0":"**As we can see from the above graph, few of the sales have almost no sales**","8f0b274a":"### Decision Tree Regressor\n#### Base Model","6c7d302a":"## Ensemble Technique:\n### Bagging:","3b4ef5df":"### K-Nearest Neighbor Regressor:","b61e8ebf":"**As we are getting best result for Random Forest Regressor and LightGBM, we will use these two ensemble methods in stacking to get best result.**","339ae6db":"### Random Forest\n#### Base Random Forest Model","ebd26330":"### R-square Scorecard:","082ab288":"<h2 style='font-family:rockwell; color:#06917e'> Exploratory Data Analysis<\/h2>","50fce47d":"**From the models, we can see that Linear Regression model is giving comparatively better result. But the r-square value is very less.**\n\n**So we will try Non-Linear Models.**","407fb568":"**As all values are less than 5, There is no multi-colinearity present in the data.**","0dcbdd1a":"#### Tuning Random forest model with RandomizedCV","0fcb5abb":"#### Creating tuned model with best params of RandomizedCV object","fb40b5bd":"**As we can see from the ttest, there is a significant difference between the Weekly Sales with Markdown and the Weekly Sales without Markdown.**","6ac5bb40":"### Boosting:\n#### AdaBoost:","091bcc86":"#### Creating tuned model with best params of RandomizedCV object","6cdae91d":"### RMSE Scorecard:","116c8675":"### R-square Scorecard:"}}