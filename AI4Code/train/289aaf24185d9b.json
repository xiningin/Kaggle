{"cell_type":{"d9a151f7":"code","11348728":"code","199d5284":"code","1605b446":"code","5eb3ad24":"code","a548a7e6":"code","2ac1892b":"code","e331c2ec":"code","408bd9dc":"code","37b14852":"code","a0ba6630":"code","e6f232f2":"code","0c77de10":"code","411890ba":"code","e04556d3":"code","d966f48f":"code","6f504302":"code","c4f91428":"code","62a92c3f":"code","9d7afda2":"code","6b65e322":"code","b2c1032d":"code","26e24978":"code","e9d57c75":"markdown","025f76ca":"markdown","96cd5f5b":"markdown","ab775bbb":"markdown","772cb45e":"markdown","c2b68841":"markdown","c79a6693":"markdown","1a5d5d8f":"markdown"},"source":{"d9a151f7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nimport gc\nimport warnings\nimport joblib\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold,cross_val_score,GridSearchCV,train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor, StackingRegressor\nfrom lightgbm import LGBMRegressor\n#from xgboost import XGBRegressor\n#from catboost import CatBoostRegressor\nplt.style.use('ggplot')\nwarnings.filterwarnings(\"ignore\")","11348728":"os.getcwd()","199d5284":"dataset = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndataset.head()","1605b446":"dataset.info()","5eb3ad24":"features = [col for col in dataset.columns.tolist() if col not in ['price','car_ID']]\nnumerical_features = []\ncategorical_features = [] \ntarget = dataset.price\ndataset = dataset[features]\n\nfor col in features:\n    if dataset[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n        numerical_features.append(col);\n    else:\n        categorical_features.append(col)\n        \nprint('There are {:} numerical features.'.format(len(numerical_features)))\nprint('There are {:} categorical features.'.format(len(categorical_features)))","a548a7e6":"dataset.CarName.value_counts()","2ac1892b":"dataset.loc[0,'CarName'].split(' ')[0]","e331c2ec":"for item in range(len(dataset)):\n    dataset.loc[item,'CarName'] = dataset.loc[item,'CarName'].split(' ')[0]\n    \ndataset[categorical_features].head()","408bd9dc":"dataset.CarName.value_counts()","37b14852":"rare_name = ['isuzu','porsche','jaguar','chevrolet',\n             'alfa-romero','maxda','vw','renault',\n             'mercury','vokswagen','vokswagen','toyouta','Nissan','porcshce']\n\nfor item in range(len(dataset)):\n    if dataset.loc[item,'CarName'] in rare_name:\n        dataset.loc[item,'CarName'] = 'rare'\n    else:\n        pass","a0ba6630":"dataset[categorical_features].describe().T","e6f232f2":"dataset = pd.get_dummies(dataset, columns=categorical_features)\ndataset.head()","0c77de10":"dataset['carvolume'] = dataset['carlength'] * dataset['carwidth'] * dataset['carheight']\ndataset['totalmpg'] = dataset['citympg'] + dataset['highwaympg']\nnumerical_features.append('carvolume')\nnumerical_features.append('totalmpg')","411890ba":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\n\ncategorical_features = [col for col in dataset.columns.tolist() if col not in numerical_features]\n\nscaler = StandardScaler()\nscaler = scaler.fit(dataset[numerical_features])\ndataset_scaler = scaler.transform(dataset[numerical_features])\ndataset_scaler = pd.DataFrame(dataset_scaler)\ndataset_scaler.columns = numerical_features\ndataset_scaler = pd.concat([dataset_scaler,dataset[categorical_features]], axis=1)\ndataset_scaler.head()","e04556d3":"features = dataset_scaler.values\nfeatures.shape, target.shape","d966f48f":"seeds = [42, 73, 111, 123, 2021]    \nSEED = seeds[0]\nNUM_TRAIN_SAMPLES = features.shape[0]","6f504302":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)  \n    \ndef count_data_items(data):\n    return len(data)","c4f91428":"lgb_params = {'bagging_fraction': 0.8, 'boosting_type': 'gbdt', \n              'colsample_bytree': None, 'feature_fraction': 0.7, \n              'lambda_l1': 2, \n              'lambda_l2': 2, \n              'learning_rate': 0.1, \n              'max_depth': -1, \n              'metrics': 'rmse', \n              'min_child_samples': None, \n              'min_child_weight': 15.586, \n              'min_data_in_leaf': 6, \n              'min_sum_hessian_in_leaf': None,\n              'n_estimators': 5000, \n              'num_leaves': 70,\n              'reg_alpha': None,\n              'reg_lambda': None, \n              'seed': 7, \n              'subsample': None,\n              'n_jobs' : 16,\n              'verbose': -1}","62a92c3f":"def train_and_evaluate(folds = 5):\n    oof_targets = np.zeros(NUM_TRAIN_SAMPLES)\n    oof_predictions = np.zeros(NUM_TRAIN_SAMPLES)\n    previous_number_of_files = 0\n    total_number_of_files = 0\n    \n    seed_everything(SEED)\n    kfold = KFold(folds, shuffle=True, random_state=SEED)\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(features, target)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1} begin :')\n        \n        train_x, train_y = features[trn_ind], target[trn_ind]\n        val_x, val_y  = features[val_ind], target[val_ind]\n        \n        model = LGBMRegressor(**lgb_params)\n        model.fit(train_x, train_y, eval_set=(val_x, val_y), early_stopping_rounds=200, verbose=False)\n        gc.collect() \n        \n        # make pesudo label\n        pesudo_label = model.predict(val_x)\n        X_agg = np.concatenate([train_x,val_x],axis=0)\n        y_agg = np.concatenate([train_y,pesudo_label],axis=0)\n        # train again\n        model = LGBMRegressor(**lgb_params)\n        model.fit(X_agg, y_agg, eval_set=(val_x, val_y), early_stopping_rounds=200, verbose=False)\n        gc.collect() \n        \n        joblib.dump(model,f'.\/lgb_{fold}_PLabel.pkl')\n        \n        number_of_files = count_data_items(val_y)\n        total_number_of_files += number_of_files\n        oof_targets[previous_number_of_files:total_number_of_files] = val_y\n        probabilities = model.predict(val_x)\n        oof_predictions[previous_number_of_files:total_number_of_files] = probabilities\n        previous_number_of_files += number_of_files\n        \n        print('\\n')\n        print('-'*50)\n        fold_r2_score = r2_score(val_y, probabilities)\n        fold_rmse_score = np.sqrt(mean_squared_error(val_y, probabilities))\n        print(f'Our fold {fold + 1} rmse score validation is {fold_rmse_score}')\n        print(f'Our fold {fold + 1} r2 score validation is {fold_r2_score}')\n        \n    print('\\n')\n    print('-'*50)\n    oof_r2_score = r2_score(oof_targets, oof_predictions)\n    oof_rmse_score = np.sqrt(mean_squared_error(oof_targets, oof_predictions))\n    print(f'Our out of folds rmse score is {oof_rmse_score}')\n    print(f'Our out of folds r2 score is {oof_r2_score}')\n    \n    print('Saving out of folds to disk...')\n    target_columns = ['Tc']\n    prediction_columns = [col + ' Prob' for col in target_columns]\n    oof_targets_df = pd.DataFrame(oof_targets, columns=target_columns)\n    oof_predictions_df = pd.DataFrame(oof_predictions, columns=prediction_columns)\n    \n    oof_dataset = pd.concat([oof_targets_df, oof_predictions_df], axis=1)\n    oof_dataset.to_csv(f'.\/lgbm_oof_{SEED}.csv', index=False)","9d7afda2":"# Train & prediction\ntrain_and_evaluate(folds = 5)","6b65e322":"from sklearn.metrics import mean_squared_error as mse\ndef PerformanceCalculator(trueVals, predVals, name):\n    plt.plot([0,0.001,0.01,1], [0,0.001,0.01,1], color = 'blue')\n    plt.scatter(trueVals, predVals, color = 'green')\n    er = mse(trueVals, predVals)\n    er = pow(er, 0.5)\n    er = int(er * 10000) \/ 10000\n    r2 = np.round(r2_score(trueVals, predVals),4)\n    plt.title('RMSE: ' + str(er) + ' for '+ name)\n    plt.plot([2500,50000], [2500,50000], '--', lw=2, c='r')\n    plt.xlim(2500,50000)\n    plt.ylim(2500,50000)\n    print('R2: ' + str(r2) + ' for '+ name)\n    plt.show()","b2c1032d":"oof_dataset = pd.read_csv(f'.\/lgbm_oof_{SEED}.csv')\noof_dataset.head()","26e24978":"PerformanceCalculator(oof_dataset.Tc, oof_dataset['Tc Prob'], 'LGBM single model')","e9d57c75":"oof train loop **with plabel**","025f76ca":"### Preprocessing","96cd5f5b":"Feature Split","ab775bbb":"Scaling","772cb45e":"Add features","c2b68841":"### Modeling","c79a6693":"visualization","1a5d5d8f":"CarName extract"}}