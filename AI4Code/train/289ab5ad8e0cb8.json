{"cell_type":{"1e2ae89b":"code","b856ddc4":"code","fa84d930":"code","45ff7e73":"code","a11f2aa2":"code","46ec7db7":"code","35453d53":"code","b0f4f36b":"code","fb654e5e":"code","11b6aeb2":"code","3ca2930e":"code","cde5b8e7":"code","2fe36bba":"code","ebbb32ff":"code","f04544f0":"code","53305ec6":"code","170941fa":"code","137abbf1":"code","7f7f5f24":"code","1e356889":"code","d57dbbb1":"code","b60e820d":"code","c829bef1":"code","7a9014f7":"code","52ea9a5e":"code","1de685c5":"code","f5fef588":"code","9bb2c50c":"code","6d1ee5f7":"code","4747bff5":"code","1e4712f7":"code","344c7882":"markdown","bfc24c42":"markdown","9b1395b6":"markdown","8bb21db7":"markdown"},"source":{"1e2ae89b":"import numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport shap\nfrom sklearn import preprocessing","b856ddc4":"plt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 12, 7","fa84d930":"# Make environment and get data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","45ff7e73":"# Dropping assetName just to focus exclusively on one categorical variable\nmarket_train_df.drop('assetName', axis=1, inplace=True)","a11f2aa2":"def make_test_train(df, split=0.80):\n    # Label encode the assetCode feature\n    X = df[df.universe==1]\n    le = preprocessing.LabelEncoder()\n    X = X.assign(assetCode = le.fit_transform(X.assetCode))\n    \n    # split test and train\n    train_ct = int(X.shape[0]*split)\n    y_train, y_test = X['returnsOpenNextMktres10'][:train_ct], X['returnsOpenNextMktres10'][train_ct:]\n    X = X.drop(['time', 'returnsOpenNextMktres10'], axis=1)\n    X_train, X_test = X.iloc[:train_ct,], X.iloc[train_ct:,]\n    return X, X_train, X_test, y_train, y_test","46ec7db7":"# Make the encoding and split\nX, X_train, X_test, y_train, y_test = make_test_train(market_train_df)","35453d53":"def make_lgb(X_train, X_test, y_train, y_test, categorical_cols = ['assetCode']):\n    # Set up LightGBM data structures\n    train_cols = X_train.columns.tolist()\n    dtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols)\n    dvalid = lgb.Dataset(X_test.values, y_test, feature_name=train_cols, categorical_feature=categorical_cols)\n    return dtrain, dvalid","b0f4f36b":"# Set up the LightGBM data structures\ndtrain, dvalid = make_lgb(X_train, X_test, y_train, y_test)","fb654e5e":"# Set up the LightGBM params\nlgb_params = dict(\n    objective='regression_l1', learning_rate=0.1, num_leaves=127, max_depth=-1, bagging_fraction=0.75,\n    bagging_freq=2, feature_fraction=0.5, lambda_l1=1.0, seed=1015\n)","11b6aeb2":"# Fit and predict\nevals_result = {}\nm = lgb.train(\n    lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), \n    verbose_eval=25, early_stopping_rounds=20, evals_result=evals_result\n)","3ca2930e":"# Plot reported feature importance\nlgb.plot_importance(m);","cde5b8e7":"lgb.plot_importance(m, importance_type='gain');","2fe36bba":"shap_explainer = shap.TreeExplainer(m)","ebbb32ff":"%%time\nsample = X.sample(frac=0.50, random_state=100)\nshap_values = shap_explainer.shap_values(sample)","f04544f0":"%%time\nshap.summary_plot(shap_values, sample)","53305ec6":"# Create some random assetCodes (this is a nice snippet fro McKinney, Wes. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython (p. 340). O'Reilly Media. Kindle Edition.)\nimport random; random.seed(0)\nimport string\nnum_stocks = 1250\ndef rands(n):\n    choices = string.ascii_uppercase\n    return ''.join([random.choice(choices) for _ in range(n)])\nassetCodes = np.array([rands(5) for _ in range(num_stocks)])","170941fa":"# Spoof intraday and overnight returns\ndays_in_year = 260\ntotal_days = days_in_year*7\non_vol_frac = 0.2  # overnight volatility fraction\n\nannualized_vol = 0.20\nopen_to_close_returns = np.random.normal(0.0, scale=annualized_vol*(1-on_vol_frac)\/np.sqrt(days_in_year), size=(total_days, num_stocks))\nclose_to_open_returns = np.random.normal(0, scale=annualized_vol*(on_vol_frac)\/np.sqrt(days_in_year), size=(total_days, num_stocks))\nopen_to_open_returns = close_to_open_returns + open_to_close_returns \nclose_to_close_returns = close_to_open_returns + np.roll(open_to_close_returns, -1)\n\n# Make price series\nprices_close = 100*np.cumprod(1+close_to_close_returns, axis=0)\nprices_open = prices_close*(1+close_to_open_returns)","137abbf1":"import itertools\n\n# Make into a DataFrame\ndates = pd.date_range(end=pd.Timestamp('2017-12-31'), periods=total_days)\nspoofed_df = pd.DataFrame(\n    data={'close': prices_close.flatten('F'), 'open': prices_open.flatten('F')},\n    index = pd.MultiIndex.from_tuples(\n        list(itertools.product(assetCodes, dates)), names=('assetCode', 'time')\n    )\n)\nspoofed_df['universe'] = 1.0","7f7f5f24":"spoofed_df.head()\n\n# Looks good!","1e356889":"spoofed_df = spoofed_df.reset_index().sort_values(['assetCode','time']).set_index(['assetCode', 'time'])","d57dbbb1":"# make sure we did the open\/close transform properly. Looks good.\nspoofed_df.loc['MYNBI', ['open', 'close']]['1Q2013'].plot();","b60e820d":"# # Make the \"return\" based features\n\nspoofed_df = spoofed_df.assign(\n     returnsClosePrevRaw1 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.close\/x.close.shift(1) -1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsOpenPrevRaw1 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.open\/x.open.shift(1) -1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsOpenPrevRaw10 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: (x.open\/x.open.shift(10)) - 1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsClosePrevRaw10 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.close\/x.close.shift(10)-1)\n     .reset_index(0, drop=True)\n)\n","c829bef1":"# Make the target variable\nspoofed_df = spoofed_df.assign(\n    returnsOpenNextMktres10 = spoofed_df.groupby(level='assetCode').\n    apply(lambda x: (x.open.shift(-10)\/x.open)-1)\n    .reset_index(0, drop=True)\n)","7a9014f7":"# Drop the edges where we don't have data to make returns\nspoofed_df = spoofed_df.reset_index().dropna()","52ea9a5e":"# Split the data\nX, X_train, X_test, y_train, y_test = make_test_train(spoofed_df)\n\n# Set up LightGBM data structures\ndtrain, dvalid = make_lgb(X_train, X_test, y_train, y_test)","1de685c5":"# Fit and predict\nevals_result = {}\nm = lgb.train(\n    lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), \n    verbose_eval=25, early_stopping_rounds=20, evals_result=evals_result\n)","f5fef588":"lgb.plot_importance(m);","9bb2c50c":"lgb.plot_importance(m, importance_type='gain');","6d1ee5f7":"shap_explainer = shap.TreeExplainer(m)","4747bff5":"%%time\nsample = X.sample(frac=0.50, random_state=100)\nshap_values = shap_explainer.shap_values(sample)","1e4712f7":"%%time\nshap.summary_plot(shap_values, sample)","344c7882":"Let's see what SHAP thinks. Per the [GitHub repo](https:\/\/github.com\/slundberg\/shap), **SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the [SHAP NIPS paper](http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions) for details).","bfc24c42":"So **in random data** we find that `assetCode` is predictive. In training, this category encodes something like the average of the target variable across the entire training set. It's like if I told you the total return for AAPL and NFLX were +50% and -25% over the entire training set (in-sample). Thus to make a good prediction in-sample, I would use the knowledge of the ticker to predict a positive target for each day for AAPL and a negative target for each day for NFLX. These would be decent predictions becuase we knew the sign and relative magnitudes of the total period. Of course out of sample, we would not expect this to hold. **So, bottom line, don't encode `assetCode` or `assetName`.**","9b1395b6":"So indeed,  consistent with other public kernels, the model believes `assetCode` is a valuable feature. \n\nHowever, this result is from leaked information. Let's try this model on **completely random data**.\n\n## Spoof Completely Random Dataset\n\nBelow I create a random DataFrame in the shape of the `market_data_df` DataFrame. Then I fit the same LightGBM model as above. Since this is random data, there should be no material predictive features, right?","8bb21db7":"# The fallacy of encoding assetCode\nBy @marketneutral\n\nThere have been a few kernel shares of gradient-boosted decision trees (\"GBDT\"; e.g., `lightgbm`) applied directly to the data \"as is\" in this competition. The results of this show that `assetCode` (and `assetName`), as a categorical variable, is a substantially important feature. Does this make sense? If you simply know the ticker symbol should that add predictive power to the model? On the face of it, it seems implausible and that the use of `assetCode` rather is simply leaking future information and producing an overfit model. I investigate this idea in this kernel.\n\n## Minimal Reproduction\nFirst, let's do a minimal reproduction. There is no effort here to do parameter tuning, or to create a great model per se. Here we just want to fit the bare minimum GBDT model and see what features the model thinks are important. We just want to reproduce in a minimal way that the model will find `assetCode` and `assetName` to be very important."}}