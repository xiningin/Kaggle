{"cell_type":{"828f4e83":"code","e0487337":"code","370fa06c":"code","4cf8aabb":"code","d7c59e60":"code","e307b368":"code","60610b34":"code","3507d5ac":"code","740f0be0":"code","68b9217f":"code","a9208dd2":"code","9fb8a7f3":"code","ca10a157":"code","3e296b9b":"code","1f35a287":"code","99db279e":"code","bcf7ddb4":"code","0fafdc49":"code","b4a9dd78":"code","058d5864":"code","d99ae4ad":"code","fc0778d6":"code","99560c0e":"code","d1cc50fb":"code","c945a274":"code","a4a213e1":"code","b2ed0e5c":"code","4a7dd83b":"code","b082e04e":"code","15c60379":"code","c64ea282":"code","42c2486e":"code","41380d55":"code","1ce65dab":"code","d2a66c3a":"code","0916fdba":"code","b7c2dd9f":"code","9e1617a1":"code","c90665a2":"code","8fad50dc":"code","010c519e":"code","bc72f089":"code","1e5a3682":"code","66bd547a":"code","d03f77d4":"code","d27c4353":"code","a80ba69f":"code","310d5ff5":"code","9c4fa221":"code","b67c3817":"code","7a852667":"code","79cad181":"code","2fe56dee":"markdown","d655aee8":"markdown","c6878144":"markdown","b3d7f6aa":"markdown","6aa2f0a6":"markdown","e0536e7f":"markdown","d2a3565a":"markdown","144e4a50":"markdown","fdd936f2":"markdown","26a58528":"markdown","61cd31b1":"markdown","8812270d":"markdown","58ef2d11":"markdown","aed58197":"markdown","e47598a6":"markdown","5c755283":"markdown"},"source":{"828f4e83":"def read_file(filepath):\n    \n    with open(filepath) as f:\n        str_text = f.read()\n    \n    return str_text","e0487337":"read_file('..\/input\/melville-moby-dick\/melville-moby_dick.txt')","370fa06c":"import spacy\nnlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n\nnlp.max_length = 1198623","4cf8aabb":"def separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","d7c59e60":"d = read_file('..\/input\/melville-moby-dick\/melville-moby_dick.txt')\ntokens = separate_punc(d)","e307b368":"tokens","60610b34":"len(tokens)","3507d5ac":"max=int(len(tokens)*.75)\nmax","740f0be0":"tokens=tokens[:max]","68b9217f":"# organize into sequences of tokens\ntrain_len = 25+1 # 50 training words , then one target word\n\n# Empty list of sequences\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    \n    # Grab train_len# amount of characters\n    seq = tokens[i-train_len:i]\n    \n    # Add to list of sequences\n    text_sequences.append(seq)","a9208dd2":"' '.join(text_sequences[0])","9fb8a7f3":"' '.join(text_sequences[1])","ca10a157":"' '.join(text_sequences[2])","3e296b9b":"len(text_sequences)","1f35a287":"max=int(len(text_sequences)*.75)\nmax","99db279e":"text_sequences=text_sequences[:max]","bcf7ddb4":"from keras.preprocessing.text import Tokenizer","0fafdc49":"# integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)","b4a9dd78":"sequences[0]","058d5864":"tokenizer.index_word","d99ae4ad":"for i in sequences[0]:\n    print(f'{i} : {tokenizer.index_word[i]}')","fc0778d6":"tokenizer.word_counts","99560c0e":"vocabulary_size = len(tokenizer.word_counts)","d1cc50fb":"import numpy as np","c945a274":"sequences = np.array(sequences)","a4a213e1":"sequences","b2ed0e5c":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding,Dropout","4a7dd83b":"def create_model(vocabulary_size, seq_len):\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n    model.add(LSTM(150, return_sequences=True))\n    model.add(LSTM(150))\n    model.add(Dense(150, activation='relu'))\n\n    model.add(Dense(vocabulary_size, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n   \n    model.summary()\n    \n    return model","b082e04e":"from keras.utils import to_categorical","15c60379":"sequences","c64ea282":"# First 49 words\nsequences[:,:-1]","42c2486e":"# last Word\nsequences[:,-1]","41380d55":"X = sequences[:,:-1]","1ce65dab":"y = sequences[:,-1]","d2a66c3a":"y = to_categorical(y, num_classes=vocabulary_size+1)","0916fdba":"seq_len = X.shape[1]","b7c2dd9f":"seq_len","9e1617a1":"# define model\nmodel = create_model(vocabulary_size+1, seq_len)","c90665a2":"from pickle import dump,load","8fad50dc":"# fit model\nmodel.fit(X, y, batch_size=512, epochs=300,verbose=1)","010c519e":"# save the model to file\nmodel.save('.\/epochBIG.h5')\n# save the tokenizer\ndump(tokenizer, open('.\/epochBIG', 'wb'))","bc72f089":"from random import randint\nfrom pickle import load\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences\n\n","1e5a3682":"def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    '''\n    INPUTS:\n    model : model that was trained on text data\n    tokenizer : tokenizer that was fit on text data\n    seq_len : length of training sequence\n    seed_text : raw string text to serve as the seed\n    num_gen_words : number of words to be generated by model\n    '''\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # Pad sequences to our trained rate (50 words in the video)\n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","66bd547a":"text_sequences[0]","d03f77d4":"import random\nrandom.seed(101)\nrandom_pick = random.randint(0,len(text_sequences))","d27c4353":"random_seed_text = text_sequences[random_pick]","a80ba69f":"random_seed_text","310d5ff5":"seed_text = ' '.join(random_seed_text)","9c4fa221":"seed_text","b67c3817":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)","7a852667":"full_text = read_file('..\/input\/melville-moby-dick\/melville-moby_dick.txt')","79cad181":"for i,word in enumerate(full_text.split()):\n    if word == 'inkling':\n        print(' '.join(full_text.split()[i-20:i+20]))\n        print('\\n')","2fe56dee":"### Exploring Generated Sequence","d655aee8":"### Train \/ Test Split","c6878144":"### Training the Model","b3d7f6aa":"We use only 75% of token only","6aa2f0a6":"We use only 75% of text_sequences only","e0536e7f":"### Convert to Numpy Matrix","d2a3565a":"### Tokenize and Clean Text","144e4a50":"## Functions for Processing Text\n\n### Reading in files as a string text","fdd936f2":"# Generating New Text","26a58528":"## Create Sequences of Tokens","61cd31b1":"# Creating an LSTM based model","8812270d":"---\n\n----","58ef2d11":"\n___\n# Text Generation with Neural Networks","aed58197":"# Keras","e47598a6":"### Grab a random seed sequence","5c755283":"### Keras Tokenization"}}