{"cell_type":{"0e35291d":"code","c0170094":"code","75efb0f1":"code","ed7c2733":"code","7d20f3de":"code","490cfd8d":"code","eae0abc4":"code","99bd56a9":"code","bbcab14b":"code","3ee485ed":"code","ad140df4":"code","49b9ee92":"code","86a7f5ac":"code","631c30c1":"code","cad6e895":"code","7b230114":"code","8125ceab":"code","bbb08f2e":"code","55e7424c":"code","414b3c91":"code","0a6ab72a":"code","f84c99ca":"code","70827343":"code","5ab59e60":"code","7bc4122e":"code","2e7946df":"code","bdf21ba9":"code","83d492cc":"code","414da2c4":"code","ef56b0fe":"code","fc5e0a89":"code","2781e6c1":"code","1be61fdd":"code","dfb7027e":"code","929d8136":"code","283d0ab4":"code","2176e786":"code","445cb671":"code","9e910740":"code","c6bc60b3":"code","da965df4":"code","2cfdc1e1":"code","153a7c91":"code","a91f43c8":"code","a82ff628":"code","888d9b82":"code","351242b1":"code","aa804c5b":"code","a3b4839d":"code","14c46c89":"code","4af1811b":"code","db50498b":"code","1963edd6":"code","1ea6594a":"code","8db8139f":"code","aa684b67":"code","1ff57db8":"markdown","3e957c0b":"markdown","e791f4c0":"markdown","3914d256":"markdown","aa84bec0":"markdown","aabc4b42":"markdown","663e20f9":"markdown"},"source":{"0e35291d":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nsys.path.append('..')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","c0170094":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom joblib import dump, load\n\nBOOK1 = '..\/input\/daebook1c\/'\nBOOK2 = '..\/input\/daebook2c\/'\nCTT = 7","75efb0f1":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.merge(train_targets,extra_targets,on ='sig_id')\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets.to_list()\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt \n\n","ed7c2733":"input_directory = '..\/input\/lish-moa\/'","7d20f3de":"train,target_cols = train_short_form_loader(input_directory +'train_features.csv',input_directory+'train_targets_scored.csv')\ntest = test_short_form_loader(input_directory +\"test_features.csv\")\n","490cfd8d":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","eae0abc4":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer","99bd56a9":"from sklearn.compose import make_column_transformer,ColumnTransformer","bbcab14b":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","3ee485ed":"from sklearn.base import BaseEstimator,TransformerMixin\n\nclass CatIntMapper( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ,col,dicti):\n        self.col = col\n        self.dicti = dicti\n        \n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n       \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n    \n    def transform( self, X):\n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n\nclass NamedOutTWrapper( BaseEstimator, TransformerMixin ):\n    \n    def __init__(self,transformer,columns,inplace=False,prefix='_' ):\n        \n        self.transformer = transformer\n        self.cols = columns\n        self.inplace =  inplace\n        self.prefix = prefix\n        self.transformer_name = self._get_transformer_name()\n        \n    def fit(self, X, y = None):\n            \n        self.transformer =   self.transformer.fit(X[self.cols] , y )\n            \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n       \n        \n        transformed_columns = self.transformer.fit_transform(X[self.cols] , y )\n        out=pd.DataFrame(index=X.index)\n        \n       \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n           \n            for i,values in enumerate(transformed_columns.transpose()):\n            \n                out[ self.transformer_name + self.prefix + str(i)] = values\n        \n       \n        \n            return   pd.concat([X,out],axis=1)\n    \n    def transform( self, X):\n        \n        transformed_columns = self.transformer.transform(X[self.cols]  )\n        \n        out=pd.DataFrame(index=X.index)\n        \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n            for i,values in enumerate(transformed_columns.transpose()):\n\n                out[ self.transformer_name + self.prefix + str(i)] = values\n\n             \n        return   pd.concat([X,out],axis=1)\n            \n    \n    def _get_transformer_name(self):\n        return str(self.transformer.__class__).split('.')[-1][0:-2]\n\n\nclass IdentityTransformer:\n    '''Duummy_tansformer as a filler'''\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        return  X\n      \n    \n    def transform( self, X):\n       \n        return  X    \n\nclass SuppressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n      \n        \n        return   X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1) \n    \n    def transform( self, X):\n       \n       \n        return    X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1)\n","ad140df4":"def multifold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state,**kwargs)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_columns])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds\n","49b9ee92":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","86a7f5ac":"class DAE_Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=1100,hidden_size2=1300):\n        super(DAE_Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size2))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        #self.dropout3 = nn.Dropout(drop_rate2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, hidden_size))\n        \n      #  self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout4 = nn.Dropout(drop_rate3)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_features))\n\n        \n    def forward(self, x,mode='DAE'):\n      #  x = self.batch_norm1(x)\n       # x1 = self.dropout1(x1)\n        x1 = F.relu(self.dense1(x))\n        \n            \n        x2 = self.batch_norm2(x1)\n      #  x = self.dropout2(x)\n        x2 = F.relu(self.dense2(x2))\n        \n        x3 = self.batch_norm3(x2)\n      \n        x3 = F.relu(self.dense3(x3))\n        \n        out = self.dense4(x3)\n        \n        if mode == 'DAE':\n            return out\n        else:\n            return x1,x2,x3\n    \n#     def forwardh2(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x\n    \n#     def forwardh3(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x","631c30c1":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n","cad6e895":"\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n","7b230114":"train = pd.read_csv(f'{input_directory}\/train_features.csv')","8125ceab":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","bbb08f2e":"#GENES","55e7424c":"Rankg_g_tansform =  NamedOutTWrapper( QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),columns= GENES+CELLS,inplace=True)","414b3c91":"PCA_g_tansform =  NamedOutTWrapper(PCA(20),columns= GENES,prefix ='_g' )","0a6ab72a":"PCA_c_tansform =  NamedOutTWrapper(PCA(20),columns= CELLS,prefix ='_c' )","f84c99ca":"#transformers_list=[map_controls,map_dose,map_time,PCA_g_tansform,PCA_c_tansform,Rankg_g_tansform]","70827343":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n","5ab59e60":"\nCatDropper =ColumnDropper(cols=['cp_type','cp_time','cp_dose'])","7bc4122e":"transformers_list=[map_controls,map_dose,map_time,Rankg_g_tansform,CatDropper]","2e7946df":"exp_name = 'test_DAE_0.3_all_together'","bdf21ba9":"def run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=False,**kwargs):\n    seed_everything(seed)\n    if not  inference_only:\n        train_dataset = MoADataset(X_train, y_train)\n        valid_dataset = MoADataset(X_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = DAE_Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  X_train.shape[1],\n       # hidden_size=hidden_size,\n        **kwargs\n    )\n    \n    \n    model.load_state_dict(torch.load( f\"{BOOK1}FOLD{fold}_{exp_name}.pth\",map_location=torch.device('cpu')))#map_location='cuda:0'))#,freeze_first_layer=True)\n    print('### LOADED FOLD%i MODEL'%fold)\n    \n    model.to(DEVICE)\n    \n    if not  inference_only:\n        oof = inference_infer_features_fn(model, validloader, DEVICE)    \n    else:\n        oof= 0\n    \n\n    predictions = infer_features_fn(model, testloader, DEVICE)\n    \n    predictions = predictions\n    \n    return oof, predictions","83d492cc":"transformers_list","414da2c4":"\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 1000\nBATCH_SIZE = 640\nLEARNING_RATE = 2e-3\nWEIGHT_DECAY = 1e-8\nNFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nGAMMA=0.5\nFACTOR=0.75\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\nhidden_size=1100\nhidden_size2=1300\nPATIENCE=10\nTHRESHOLD = 5e-3","ef56b0fe":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, scheduler, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    if scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n        scheduler.step(final_loss)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\n\ndef infer_features_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs,mode='get_features')\n        \n#         print(len(outputs))\n        \n#         for i in range(len(outputs)):\n#             print(outputs[i].shape)\n            \n#         print(torch.cat(outputs,axis=1).shape)\n        \n        preds.append(torch.cat(outputs,axis=1).detach().cpu().numpy())\n        \n        \n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","fc5e0a89":"#SEED = [0,12347,565657,123123,78591]\nSEED = [0]\ntrain,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')\ntest = test_short_form_loader(\"..\/input\/lish-moa\/test_features.csv\")\n\ntrain = pd.concat([train,test])\ntrain[target_cols]= train[target_cols].fillna(0)\ntest = train.copy()\n#pipeline_test = make_pipeline(*transformers_list)\n#pipeline_test.fit(train)\n#test = pipeline_test.transform(test)\n    \n\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    train = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n     \n        #pipeline_val = pk.load(open(f\"..\/input\/tmultiv5rnkgpcag50smth1e3unpropertrafo\/preprocessing_SEED{seed}_FOLD{fold}.pth\",'rb'))\n        \n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n        #pipeline_val.fit(X_train)\n        \n        ###############################\n        #### SAVE\/LOAD PREPROCESSING #######\n        #dump(pipeline_val,'pipeline_val2_fold%i.joblib'%fold)\n        pipeline_val = load(BOOK2+'pipeline_val2_fold%i.joblib'%fold)\n        print('LOADED PIPELINE_VAL2_FOLD%i'%fold)\n        print(pipeline_val)\n        ###############################\n       \n        ##X_train = pipeline_val.fit_transform(X_train,y_train)\n        #X_train = pipeline_val.fit_transform(X_train)\n        \n        X_train = pipeline_val.transform(X_train)\n                \n        #feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        \n        \n        \n        \n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n       \n        \n        X_test = test[feature_cols]\n            \n        \n        X_test = pipeline_val.transform(X_test).values\n        \n        #X_test = X_test.values\n        \n        \n        pred_ = run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=True)\n        \n        print('## extracted features', pred_[1].shape, len(train) )\n        transformed_features = pd.DataFrame(pred_[1],index=test.index)\n        transformed_features.columns = [str(i) for i in range(len(transformed_features.columns))]\n        transformed_features.reset_index().to_feather('.\/features_0.2_altogether_fold%i.fth'%fold)\n        \n        if fold>=(CTT-1): break","2781e6c1":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","1be61fdd":"from sklearn.base import BaseEstimator, TransformerMixin","dfb7027e":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","929d8136":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    ","283d0ab4":"class DaeAdder( BaseEstimator, TransformerMixin ):\n    def __init__(self,filename):\n        \n        self.filename=filename\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def fit_transform(self,X,y=None):\n        \n        Dae_features = pd.read_feather(self.filename).set_index('sig_id')\n        \n        return X.merge(Dae_features,how='left', on='sig_id')\n        \n        \n    def transform(self,X):\n        \n        Dae_features = pd.read_feather(self.filename).set_index('sig_id')\n        \n        return X.merge(Dae_features,how='left', on='sig_id')\n        \n        ","2176e786":"\n\nclass SupressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        X = X[X['cp_type']!=0]\n        X = X.drop('cp_type', axis=1)\n        return X \n    \n    def transform( self, X):\n        X = X[X['cp_type']!=0]\n        X = X.drop('cp_type', axis=1)\n        return X\n    \nclass CatIntMapper( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ,col,dicti):\n        self.col = col\n        self.dicti = dicti\n        \n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n       \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n    \n    def transform( self, X):\n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n\nclass NamedOutTWrapper( BaseEstimator, TransformerMixin ):\n    \n    def __init__(self,transformer,columns,inplace=False,prefix='_' ):\n        \n        self.transformer = transformer\n        self.cols = columns\n        self.inplace =  inplace\n        self.prefix = prefix\n        self.transformer_name = self._get_transformer_name()\n        \n    def fit(self, X, y = None):\n            \n        self.transformer =   self.transformer.fit(X[self.cols] , y )\n            \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n       \n        \n        transformed_columns = self.transformer.fit_transform(X[self.cols] , y )\n        out=pd.DataFrame(index=X.index)\n        \n       \n        if self.inplace:\n            out = X[self.cols].copy()\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n           \n            for i,values in enumerate(transformed_columns.transpose()):\n            \n                out[ self.transformer_name + self.prefix + str(i)] = values\n        \n       \n        \n            return   pd.concat([X,out],axis=1)\n    \n    def transform( self, X):\n        \n        transformed_columns = self.transformer.transform(X[self.cols]  )\n        \n        out=pd.DataFrame(index=X.index)\n        \n        if self.inplace:\n            out = X[self.cols].copy()\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n            for i,values in enumerate(transformed_columns.transpose()):\n\n                out[ self.transformer_name + self.prefix + str(i)] = values\n\n             \n        return   pd.concat([X,out],axis=1)\n            \n    \n    def _get_transformer_name(self):\n        return str(self.transformer.__class__).split('.')[-1][0:-2]\n\n\nclass IdentityTransformer:\n    '''Duummy_tansformer as a filler'''\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        return  X\n      \n    \n    def transform( self, X):\n       \n        return  X    \n\nclass SuppressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n      \n        \n        return   X.loc[X['cp_type']=='trt_cp'].drop('cp_type', axis=1) \n    \n    def transform( self, X):\n       \n       \n        return    X.loc[X['cp_type']=='trt_cp'].drop('cp_type', axis=1)\n\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\n#Custom transformer that breaks dates column into year, month and day into separate columns and\n#converts certain features to binary \nclass VarianceFilter( BaseEstimator, TransformerMixin ):\n    def __init__(self,threshold):\n        self.threshold = threshold\n    def fit(self,X,y=None):\n        mask = X.var()<= self.threshold\n        self.drop_cols = set([ col for val,col in zip(mask,X.columns) if val])\n        self.drop_cols.discard('cp_type')\n        return self\n    def transform(self,X):\n        \n        return X.drop(self.drop_cols,axis=1)\n\n\ndef apply_pipe_together(pipeline,train,test,ct=None):\n    #@add warning when intesection is not the whole\n    data = pd.concat([train,test])\n\n    #data = pipeline.fit_transform(data)\n    #pipeline.fit(data)\n    \n    ###############################\n    #### SAVE\/LOAD PREPROCESSING #######\n    #dump(pipeline,'pipe_together_ct%i.joblib'%ct)\n    pipeline = load(BOOK2+'pipe_together_ct%i.joblib'%ct)\n    print('LOAD PIPE_TOGETHER_CT%i'%ct)\n    print(pipeline)\n    ###############################\n    \n    data = pipeline.transform(data)\n    \n    train = data.loc[data.index.intersection(train.index)]\n    test = data.loc[data.index.intersection(test.index)]\n    \n    return pipeline,train,test","445cb671":"\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n","9e910740":"from torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","c6bc60b3":"def multifold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state,**kwargs)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_columns])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds\n","da965df4":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size1=388,hidden_size2=512,drop_rate1=0.0,drop_rate2=0.3,drop_rate3=0.3):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size1))\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size1)\n        self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size1, hidden_size2))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        self.dropout3 = nn.Dropout(drop_rate3)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, num_targets))\n\n\n    def forward(self, x):\n        \n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","2cfdc1e1":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, scheduler, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    if scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n        scheduler.step(final_loss)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","153a7c91":"exp_name = 'DAE'","a91f43c8":"VALIDATE = True","a82ff628":"def run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose=False,ct=None,**kwargs):\n    \n    seed_everything(seed)\n    \n   \n    \n    train_dataset = MoADataset(X_train, y_train)\n    valid_dataset = MoADataset(X_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  y_train.shape[1],hidden_size1=hidden_size1,hidden_size2=hidden_size2,\n       **kwargs\n    )\n    \n    model.to(DEVICE)\n    \n    #initialize_from_past_model(model, f\"..\/results\/original_torch_moa_smoothed_lrplateau_5_folds_AUX_SEED{seed}_FOLD{fold}.pth\")#,freeze_first_layer=True)\n    \n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, pct_start=0.1, div_factor=1e3, \n     #                                         max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n    \n    loss_val = nn.BCEWithLogitsLoss()\n    #loss_tr =  nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    #todo el guardado de los resultados se puede mover a kfold que si tiene info de los indices\n    #oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    \n    \n    if 0:\n    #for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        if verbose & (epoch%10==0):\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model,scheduler, loss_val, validloader, DEVICE)\n        if verbose & (epoch%10==0):\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof = valid_preds\n        \n            #torch.save(model.state_dict(), f\"{exp_name}_SEED{seed}_FOLD{fold}.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n   \n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    ###############################\n    #### SAVE\/LOAD MODEL ###############\n    #torch.save(model.state_dict(), f\"{exp_name}_SEED{seed}_FOLD{fold}_CT{ct}.pth\")\n    \n    model = Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  y_train.shape[1],\n        hidden_size1=hidden_size1,\n        hidden_size2=hidden_size2,\n       **kwargs\n    )\n    \n    model.load_state_dict(torch.load(f\"{BOOK2}{exp_name}_SEED{seed}_FOLD{fold}_CT{ct}.pth\"))\n    model.to(DEVICE)\n    \n    print('LOAD MODEL_FOLD%i_CT%i'%(fold,ct))\n    ###############################\n    \n    if VALIDATE:\n        valid_loss, oof = valid_fn(model,scheduler, loss_val, validloader, DEVICE)\n    else:\n        oof = np.zeros((len(X_valid),y_train.shape[1]))\n    \n    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    \n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","888d9b82":"#params for one cycle schedule\nDEVICE =  torch.device('cuda:0')\nEPOCHS = 36\nBATCH_SIZE = 1024\nLEARNING_RATE = 6e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7              #<-- Update\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\n\nhidden_size1=2048\n\nhidden_size2=2048","351242b1":"#params ={}","aa804c5b":"def run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=False,ct=None, **kwargs):\n    \n    \n    train = folds\n    test_ = test\n    \n    #oof = np.zeros((len(folds), len(target_cols)))\n    oof = train[target_cols].copy()\n    oof = oof*0\n    predictions = pd.DataFrame(0,columns=target_cols,index=test.index)\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n      \n            \n        # IDENTITY TRANSFORMATION DONT NEED TO SAVE\n        #######################\n        X_train = pipeline_val.fit_transform(X_train,y_train)\n        \n        feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n        X_test = pipeline_val.transform(test_)\n        test_index = X_test.index\n        X_test = X_test[feature_cols].values\n            \n        oof_, pred_ = run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose,ct=ct,**kwargs)\n        \n#         print(X_valid.shape)\n#         print(oof_.shape)\n#         print( oof.loc[valid_index].head())\n        \n        oof.loc[valid_index] = oof_\n        \n        \n        \n        predictions.loc[test_index] += pred_ \/ NFOLDS\n        \n        \n    return oof, predictions","a3b4839d":"params={}","14c46c89":"# Averaging on multiple SEEDS\n#SEED = [0,12347,565657]\n#SEED = [0,12347,565657,123123,78591]\n\nk = 0\nSEED = [0]\n\nDae0_2 = DaeAdder(filename='features_0.2_altogether_fold%i.fth'%k)\ntransformers_list=[map_dose,map_time,Dae0_2,SuppressControls()]\ntrain,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')\ntest = test_short_form_loader(\"..\/input\/lish-moa\/test_features.csv\")\npipeline_test = make_pipeline(*transformers_list)\npipeline_test,train , test = apply_pipe_together(pipeline_test,train,test,ct=k)\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor k in range(CTT):\n    \n    if k!=0:\n        Dae0_2 = DaeAdder(filename='features_0.2_altogether_fold%i.fth'%k)\n        transformers_list=[map_dose,map_time,Dae0_2,SuppressControls()]\n        train,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')\n        test = test_short_form_loader(\"..\/input\/lish-moa\/test_features.csv\")\n        pipeline_test = make_pipeline(*transformers_list)\n        pipeline_test,train , test = apply_pipe_together(pipeline_test,train,test,ct=k)\n\n    transformers_list=[IdentityTransformer()]\n\n    for seed in SEED:\n   \n        folds = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n        oof_, predictions_ = run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=True,ct=k,**params)\n        oof += oof_ \/ len(SEED) \/ CTT\n        predictions += predictions_ \/ len(SEED) \/ CTT\n        \n    print()\n\n        \n#train[target_cols] = oof\ntest[target_cols] = predictions","4af1811b":"#valid_results = train.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n#valid_results\n\ny_true = train[target_cols].values\ny_pred = oof\n\nscore = 0\nfor i in range(len(target_cols)):\n   # print(log_loss(y_true[:, i], y_pred[:, i])\/ len(target_cols))\n    score_ = log_loss(y_true[:, i], y_pred.iloc[:, i],labels=[0,1])\n    #score_ = log_loss(y_true[:, i], y_pred[:, i],labels=[0,1])\n    #if score_ > 0.02:\n     #   print(score_)\n    score +=( score_ \/ len(target_cols))\n    \nprint('### NOTE THIS CV USES LAST MODEL IN EPOCH NOT BEST ###')\nprint(\"CV log_loss: \", score)\n    ","db50498b":"train,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')\ny_true = train[target_cols].values\n\ny_pred = train[target_cols].copy()\ny_pred[target_cols] = 0\ny_pred.loc[oof.index] = oof\ny_pred.loc[train.cp_type=='ctl_vehicle'] = 0\n\nscore = 0\nfor i in range(len(target_cols)):\n   # print(log_loss(y_true[:, i], y_pred[:, i])\/ len(target_cols))\n    score_ = log_loss(y_true[:, i], y_pred.iloc[:, i],labels=[0,1])\n    #if score_ > 0.02:\n     #   print(score_)\n    score +=( score_ \/ len(target_cols))\n    \nprint('### NOTE THIS CV USES LAST MODEL IN EPOCH NOT BEST ###')\nprint(\"CV log_loss: \", score)\n    ","1963edd6":"sample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission.set_index('sig_id',inplace=True)\ntest_features.set_index('sig_id',inplace=True)\ntest_features = test_features.loc[sample_submission.index]\n\nsub = sample_submission.drop(columns=target_cols).merge(test[target_cols], on='sig_id', how='left').fillna(0)\n#sub.set_index('sig_id',inplace=True)\nsub.loc[test_features['cp_type']=='ctl_vehicle', target_cols] =0\nsub.to_csv('.\/submission.csv', index=True)\nsub.head()","1ea6594a":"s = sub.loc[test_features['cp_type']=='ctl_vehicle', target_cols].sum().sum()\nprint('Sub control rows =',s)","8db8139f":"import matplotlib.pyplot as plt\n\nx = sub.loc[test_features['cp_type']!='ctl_vehicle', target_cols].values\nprint('MIN pred',np.min(x),'MAX pred',np.max(x))\nplt.figure(figsize=(20,4))\nplt.subplot(1,2,1)\nplt.hist( x[x<0.01], bins=100)\nplt.subplot(1,2,2)\nplt.hist( x[x>0.9], bins=100)\nplt.show()","aa684b67":"!rm features*\n!ls","1ff57db8":"# CV folds","3e957c0b":"# DAE Model","e791f4c0":"# Dataset Classes","3914d256":"## defining transformations","aa84bec0":"## training and final prediction","aabc4b42":"# PREDICTION MODEL","663e20f9":"# feature Selection transformers"}}