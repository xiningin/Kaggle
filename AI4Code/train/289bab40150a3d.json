{"cell_type":{"56f65a20":"code","d0073e12":"code","098754b1":"code","b36ca06e":"code","50ba90d2":"code","8716a8ed":"code","d42551c7":"code","bde14679":"code","bf9457e8":"code","f7a8374d":"code","d8eb046e":"code","5737c28d":"code","896af73e":"code","726e516a":"markdown","495113f4":"markdown","21fcbf6e":"markdown","aaf486d2":"markdown","000c758a":"markdown","b6b28343":"markdown","7b2af905":"markdown","95f421f6":"markdown"},"source":{"56f65a20":"import joblib\nimport numpy as np\nimport os\nimport pandas as pd \nfrom pathlib import Path\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nfrom warnings import simplefilter\nsimplefilter('ignore')","d0073e12":"from pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.core.lightning import LightningModule\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n                          get_constant_schedule_with_warmup, AdamW)","098754b1":"model_name = 'roberta_v7'\n\ndata_dir = Path('..\/input\/commonlitreadabilityprize')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\n\npretrained_path = '..\/input\/roberta-base\/'\n\nbuild_dir = Path('.\/build')\noutput_dir = build_dir \/ model_name\n\ntrn_encoded_file = output_dir \/ 'trn.enc.joblib'\ntokenizer_file = output_dir \/ 'tokenizer.joblib'\nval_predict_file = output_dir \/ f'{model_name}.val.txt'\nsubmission_file = 'submission.csv'\n\nid_col = 'id'\ntarget_col = 'target'\ntext_col = 'excerpt'\n\nmax_len = 200\nn_fold = 5\nn_est = 20\nn_stop = 2\nbatch_size = 8\nseed = 42","b36ca06e":"output_dir.mkdir(parents=True, exist_ok=True)","50ba90d2":"seed_everything(seed)","8716a8ed":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","d42551c7":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\ny = trn[target_col].values\nprint(trn.shape, y.shape)\ntrn.head()","bde14679":"tokenizer = RobertaTokenizerFast.from_pretrained(pretrained_path, do_lower_case=True)\nmodel_config = RobertaConfig.from_pretrained(pretrained_path)\nmodel_config.output_hidden_states = True","bf9457e8":"class Data(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        self.labeled = target_col in df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        texts = self.df[text_col][idx]\n        token = tokenizer(texts, max_length=max_len, truncation=True, padding='max_length', \n                          return_tensors='pt', add_special_tokens=True)\n        ids = torch.tensor(token['input_ids'], dtype=torch.long).squeeze()\n        mask = torch.tensor(token['attention_mask'], dtype=torch.long).squeeze()\n        if self.labeled:\n            target = torch.tensor(self.df[target_col][idx], dtype=torch.float)\n        \n        return (ids, mask, target) if self.labeled else (ids, mask)","f7a8374d":"class ReadabilityModel(LightningModule):\n    \n    def __init__(self, conf):\n        super().__init__()\n        self.config = conf\n        self.model = RobertaModel.from_pretrained(pretrained_path, config=self.config)\n        self.dropout = nn.Dropout(0.1)\n        self.num_targets = 1\n        self.clf = nn.Linear(768, self.num_targets)\n        torch.nn.init.normal_(self.clf.weight, std=0.02)\n    \n    def forward(self, inputs):\n        ids, mask = inputs\n        out = self.model(ids, attention_mask=mask)\n        out = out['hidden_states']\n        x = out[-1]\n        x = self.dropout(x)\n        x = torch.mean(x, 1, True)\n        preds = self.clf(x)\n        preds = preds.squeeze(-1).squeeze(-1)\n\n        return preds\n    \n    def training_step(self, batch, batch_idx):\n        ids, mask, y = batch\n        p = self([ids, mask])\n        loss = self.loss_fn(p, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        ids, mask, y = batch\n        p = self([ids, mask])\n        loss = self.loss_fn(p, y)\n        self.log('val_loss', loss)\n        \n    def configure_optimizers(self):\n        optimizer = AdamW(self.model.parameters(), lr=1e-5, weight_decay=0.01)\n        lr_scheduler = get_constant_schedule_with_warmup(optimizer, 100)\n        return [optimizer], [lr_scheduler]\n    \n    def loss_fn(self, p, y):\n        return torch.sqrt(nn.MSELoss()(p, y))","d8eb046e":"cv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],), dtype=float)\nfor i_cv, (i_trn, i_val) in enumerate(cv.split(trn), 1):\n    model = ReadabilityModel(model_config)\n    trn_loader = DataLoader(Data(trn.iloc[i_trn]), shuffle=True, batch_size=batch_size)\n    val_loader = DataLoader(Data(trn.iloc[i_val]), shuffle=False, batch_size=batch_size * 8)\n\n    trainer = Trainer(max_epochs=n_est, gpus=-1, logger=False, checkpoint_callback=False,\n                      callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=n_stop)])\n    trainer.fit(model, trn_loader, val_loader)\n\n    val_loader = DataLoader(Data(trn.iloc[i_val].drop(target_col, axis=1)), shuffle=False, \n                            batch_size=batch_size * 8)\n    tst_loader = DataLoader(Data(tst), shuffle=False, batch_size=batch_size * 8)\n    p[i_val] = np.concatenate(trainer.predict(model, val_loader))\n    p_tst += np.concatenate(trainer.predict(model, tst_loader)) \/ n_fold\n    \n    trainer.save_checkpoint(f'{model_name}_cv{i_cv}.ckpt')","5737c28d":"print(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(val_predict_file, p, fmt='%.6f')","896af73e":"sub = pd.read_csv(sample_file, index_col=id_col)\nsub[target_col] = p_tst\nsub.to_csv(submission_file)\nsub.head()","726e516a":"# Changelogs\n\n| Version | CV Score | Public Score | Changes | Comment |\n|---------|----------|--------------|---------|---------|\n| v7 | 0.651698 | to be updated | initial baseline | |","495113f4":"# Comments on PyTorch Lightning as a Keras user\n\n* Not so many examples to refer to.\n* No simple `model.predict()`??? - `trainer.predict()` returns a list of `np.array` size of `batch_size`.\n* Unlike Keras, `trainer.fit()` doesn't re-initialize callbacks (e.g. EarlyStopping() should be re-initialized for each CV separately) --> took a while to debug this. :(\n\nOverall, it's nice to have less boilerplate than PyTorch, but PyTorch Lightning needs more comprehensive examples and tutorials.","21fcbf6e":"# Tokenization Using RoBERTa","aaf486d2":"This notebook shows how to train a neural network model with pre-trained RoBERTa in Pytorch Lightning. \n\nThis competition is a code competition without access to internet. So we add the pretrained model through @abhishek's [`roberta-base` Kaggle Datasets](https:\/\/www.kaggle.com\/abhishek\/roberta-base) instead.\n\nThis notebook shares the same structure as in [TF\/Keras BERT Baseline (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/tf-keras-bert-baseline-training-inference), and is built on top of two other notebooks:\n* [BERT & PyTorch [CommonLit Readability] Simple](https:\/\/www.kaggle.com\/shivanandmn\/bert-pytorch-commonlit-readability-simple) by @shivanandmn\n* [RoBERTa meets TPUs](https:\/\/www.kaggle.com\/yassinealouini\/roberta-meets-tpus#Application:-Tweet-Sentiment-Extraction) by @yassinealouini\n\nHope it helps.","000c758a":"# Model Training with Cross-Validation","b6b28343":"## Print CV RMSE and Save CV Predictions","7b2af905":"If you find it helpful, please upvote the notebook. Also check out my other notebooks below:\n\n* [TF\/Keras BERT Baseline (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/tf-keras-bert-baseline-training-inference): shares the TF\/Keras BERT baseline with 5-fold CV\n* [All Zero Submission](https:\/\/www.kaggle.com\/jeongyoonlee\/all-zero-submission): shows the public LB score for all zero submission\n* [DAE with 2 Lines of Code with Kaggler](https:\/\/www.kaggle.com\/jeongyoonlee\/dae-with-2-lines-of-code-with-kaggler): shows how to generate Denoising AutoEncoder features using `Kaggler`\n\nHappy Kagglging~!","95f421f6":"# Submission"}}