{"cell_type":{"4a679e9d":"code","a8a1ee16":"code","b12c754a":"code","cbca7c80":"code","52a25374":"code","be87c01a":"code","a0a7c926":"code","ca43ca0a":"code","63f17dea":"code","a4d15367":"code","5c298afd":"code","a3db6932":"code","b30bf4b4":"code","dff2cdd2":"code","a91f140b":"code","03340d36":"code","a1a28133":"markdown","2f099617":"markdown","8b4167c8":"markdown","f4dbfcec":"markdown","b3de90a8":"markdown"},"source":{"4a679e9d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","a8a1ee16":"class Linear_Regression():\n\n  # initiating the parameters (learning rate & no. of iterations)\n    def __init__ (self, learning_rate, no_of_iterations):\n        self.learning_rate = learning_rate\n        self.no_of_iterations = no_of_iterations\n\n    def fit(self, X, Y ):\n\n    # number of training examples & number of features\n\n        self.m, self.n = X.shape  # number of rows & columns\n\n        # initiating the weight and bias \n\n        self.w = np.zeros(self.n)\n        self.b = 0\n        self.X = X\n        self.Y = Y\n\n    # implementing Gradient Descent\n\n        for i in range(self.no_of_iterations):\n\n              self.update_weights()\n\n\n    def update_weights(self):\n\n        Y_prediction = self.predict(self.X)\n\n        # calculate gradients\n\n        dw = - (2 * (self.X.T).dot(self.Y - Y_prediction)) \/ self.m\n\n        db = - 2 * np.sum(self.Y - Y_prediction)\/self.m\n\n        # upadating the weights\n\n        self.w = self.w - self.learning_rate*dw\n        self.b = self.b - self.learning_rate*db\n\n\n    def predict(self, X):\n        return X.dot(self.w) + self.b\n","b12c754a":"df = pd.read_csv('..\/input\/salary-data-simple-linear-regression\/Salary_Data.csv')","cbca7c80":"df.head()","52a25374":"df.shape","be87c01a":"X = df.drop('Salary', axis = 'columns')\ny = df.Salary","a0a7c926":"#Why?\n#Assiging values in X & Y\n#X = dataset.iloc[:, :-1].values\n#y = dataset.iloc[:, -1].values\nX = X.to_numpy()\ny = y.to_numpy()","ca43ca0a":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)","63f17dea":"model = Linear_Regression(learning_rate = 0.01,no_of_iterations = 100)","a4d15367":"model.fit(X_train, y_train)","5c298afd":"y_preds = model.predict(X_test)","a3db6932":"print(f\"Slope - weight: {model.w[0]}\")\nprint(f\"intercept - bias: {model.b}\")","b30bf4b4":"plt.scatter(X_test, y_test)\nplt.plot(X_test,y_preds,'#e377c2')","dff2cdd2":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","a91f140b":"mean_squared_error(y_test, y_preds)","03340d36":"r2_score(y_train, model.predict(X_train))","a1a28133":"![image.png](attachment:ab8161af-503b-456e-bf77-56d8e71f5106.png)","2f099617":"![image.png](attachment:cfc6fb6f-6353-45e4-8f31-e7a1b90cefbb.png)","8b4167c8":"Linear Regression:\n\n**Y = wX + b**\n\nY : Dependent Variable\n\nX : Independent Variable\n\nw : weight\n\nb : bias","f4dbfcec":"**Gradient Descent:**\n\nGradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n\nw  =  w - \u03b1*dw\n\nb  =  b - \u03b1*db","b3de90a8":"**Learning Rate:**\n\nLearning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}}