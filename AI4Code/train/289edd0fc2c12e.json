{"cell_type":{"42040a19":"code","6aadfe0e":"code","037beecc":"code","34571587":"code","db5ff124":"code","cd5f6719":"code","38bc183c":"code","59544408":"code","9191f24d":"code","77849867":"code","343940c5":"code","72bc5ff0":"code","a7a4edf3":"code","1bbf4fd5":"code","fb09c992":"code","f38bddbb":"code","84d0126b":"code","da241ce2":"code","ce05eae8":"code","3aad71e2":"code","1f09e93b":"code","ae0e91ab":"code","270ad310":"code","97a3c1b0":"code","51be6e91":"code","1eed8e5c":"code","0e2e513b":"code","fdb415c1":"code","4d9f5993":"code","c43bffdf":"code","3c2d2ace":"code","fb8cec21":"code","87e00474":"code","4ae8abe8":"code","7f700392":"code","765f0227":"code","520559b7":"code","5f7d5b2c":"code","e12230e0":"code","1ecf9fbc":"code","74c7531a":"markdown","722c0a07":"markdown","d92d04a8":"markdown","681c3e68":"markdown","e71676e7":"markdown","057b2646":"markdown","584df33c":"markdown","afe410d6":"markdown","c25ca4df":"markdown"},"source":{"42040a19":"# getting started with the model \n# importing required libraries\/packages \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import time for training details\nfrom time import time\nt0 = time()","6aadfe0e":"# Importing and Reading the Dataset\ndf_stroke= pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","037beecc":"df_stroke_row_count, df_stroke_column_count=df_stroke.shape\nprint('Total number of rows:', df_stroke_row_count)\nprint('Total number of columns:', df_stroke_column_count)","34571587":"df_stroke.describe()","db5ff124":"df_stroke.info()","cd5f6719":"df_stroke.isna().sum()","38bc183c":"print (\"Unique values are:\\n\",df_stroke.nunique())","59544408":"#checking Datatypes\ndf_stroke.dtypes","9191f24d":"#counting outliers for bmi\n\nbmi_outliers=df_stroke[df_stroke['bmi']>50]\nbmi_outliers['bmi'].shape","77849867":"#replacing outlier entries with mean of bmi\ndf_stroke[\"bmi\"] = df_stroke[\"bmi\"].apply(lambda x: df_stroke.bmi.mean() if x>50 else x)","343940c5":"# Replacing null values of bmi with mean of bmi column\n\ndf_stroke.bmi.replace(to_replace=np.nan,value=df_stroke.bmi.mean(), inplace=True)\nprint (df_stroke.shape)","72bc5ff0":"# missing values \ndf_stroke.isna().sum()","a7a4edf3":"# Dropping not necessary Id column\ndf_stroke.drop(columns='id',inplace=True)","1bbf4fd5":"df_stroke.head().iloc[:5]","fb09c992":"df_stroke.gender.value_counts()","f38bddbb":"# number of 'other' is very small, converting the value to 'Male'\ndf_stroke['gender']=df_stroke['gender'].replace('Other','Male')","84d0126b":"df_stroke.replace({ 'gender': {'Male':1 ,'Female':0}} ,inplace=True)\ndf_stroke.replace({ 'ever_married': {'No':0 ,'Yes':1}} ,inplace=True)\ndf_stroke.replace({ 'Residence_type': {'Rural':0 ,'Urban':1}} ,inplace=True)\ndf_stroke.replace({ 'smoking_status': {'Unknown':0 ,'never smoked':1,'formerly smoked':2,'smokes':3}} ,inplace=True)\ndf_stroke.replace({ 'work_type': {'Private':0 ,'Self-employed':1,'children':2,'Govt_job':3,'Never_worked':4}} ,inplace=True)","da241ce2":"#checking Datatypes\ndf_stroke.dtypes","ce05eae8":"#Getting an idea about the distribution of gender \np = sns.countplot(data=df_stroke, x = 'gender', palette='PuBuGn')","3aad71e2":"fig, ax = plt.subplots(4,2, figsize = (14,14))\n((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = ax\n\nlabels = df_stroke['gender'].value_counts().index.tolist()[:2]\nvalues = df_stroke['gender'].value_counts().tolist()[:2]\nax1.pie(x=values, labels=labels, autopct=\"%1.1f%%\",colors=['#FF9933','#19FF66'],shadow=True, startangle=45,explode=[0.01, 0.1])\nax1.set_title(\"Gender\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"No Hypertension\", \"Hypertension\"]\nvalues = df_stroke['hypertension'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.1f%%\",colors=['#FF9933','#19FF66'],shadow=True,startangle=45,explode=[0.1, 0.15])\nax2.set_title(\"Hypertension\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"No Heart Disease\", \"Heart Disease\"]\nvalues = df_stroke['heart_disease'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.1f%%\",colors=['#AA80FF','#FF9933'],shadow=True, startangle=45,explode=[0.1, 0.15])\nax3.set_title(\"Heart disease\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"Married\", \"Not Married\"]\nvalues = df_stroke['ever_married'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.1f%%\", colors=['#FF9933','#AA80FF'],shadow=True,startangle=45,explode=[0, 0.05])\nax4.set_title(\"Marriage\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"Private\", \"Self-Employed\", \"Children\", \"Govt Job\", \"Never Worked\"]\nvalues = df_stroke['work_type'].value_counts().tolist()\nax5.pie(x=values, labels=labels, autopct=\"%1.1f%%\", colors=['#66b3ff','#FF9933','#19FF66','#FF6699','#000066'],shadow=True, startangle=45,explode=[0.1, 0.1, 0.1, 0.1, 0.2])\nax5.set_title(\"Work Type\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"Urban\", \"Rural\"]\nvalues = df_stroke['Residence_type'].value_counts().tolist()\nax6.pie(x=values, labels=labels, autopct=\"%1.1f%%\", colors=['#66b3ff','#19FF66'],shadow=True,startangle=45,explode=[0.05, 0.05])\nax6.set_title(\"Residence Type\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"Never Smoked\", \"Unknown\", \"Smoked\", \"Smokes\"]\nvalues = df_stroke['smoking_status'].value_counts().tolist()\nax7.pie(x=values, labels=labels, autopct=\"%1.1f%%\", shadow=True, startangle=45,explode=[0.05, 0.07, 0.1, 0.1],colors=['#AA80FF','#66b3ff','#19FF66','#FF6699'])\nax7.set_title(\"Smoking Status\", fontdict={'fontsize': 12},fontweight ='bold')\n\nlabels = [\"No Stroke\", \"Stroke\"]\nvalues = df_stroke['stroke'].value_counts().tolist()\nax8.pie(x=values, labels=labels, autopct=\"%1.1f%%\", shadow=True, startangle=45,explode=[0.01, 0.3],colors=['#19FF66','#FF9933'])\nax8.set_title(\"Stroke\", fontdict={'fontsize': 12},fontweight ='bold')\n\nplt.tight_layout()\nplt.show()","1f09e93b":"df_st=['gender','hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status', 'stroke']\nfig, axs = plt.subplots(4, 2, figsize=(14,20))\naxs = axs.flatten()\nfor i, col_name in enumerate(df_st):\n    sns.countplot(x=col_name, data=df_stroke, ax=axs[i], hue =df_stroke['stroke'],palette='mako_r')","ae0e91ab":"#correlation map for features\nf,ax = plt.subplots(figsize=(12, 12))\nax.set_title('Correlation map for variables')\nsns.heatmap(df_stroke.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax,cmap=\"icefire\")","270ad310":"#Defining X and y\nX = df_stroke.drop(['stroke'], axis=1)\ny = df_stroke['stroke']\n\n# creating dataset split for prediction\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_test:', y_test.shape)","97a3c1b0":"import warnings\nwarnings.filterwarnings('ignore')\n# 1. Using Random Forest Classifier\nt0 = time()\n# Load random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a random forest Classifier\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_rf = round(clf.score(X_test,y_test) * 100, 2)\nrf_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', rf_time)\n\n#Print Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","51be6e91":"#2. Gaussian Naive Bayes Classifier\nt0 = time()\n#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n# Train the Classifier\/fitting the model\ngnb.fit(X_train, y_train)\n\n# predict the response\ny_pred = gnb.predict(X_test)\nacc_gnb = round(gnb.score(X_test,y_test) * 100, 2)\ngnb_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Gaussian Na\u00efve Bayes Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', gnb_time)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","1eed8e5c":"#import Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(max_depth=10)\n\n# Train the Classifier\/fitting the model\nclf = clf.fit(X_train,y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_dt = round(clf.score(X_test,y_test) * 100, 2)\ndt_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score \n\n# evaluate accuracy\nprint (\"Decision Tree Accuracy:\", metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', dt_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","0e2e513b":"#kNN\nimport sys, os\n\n# Import kNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the Classifier\/fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test,y_test) * 100, 2)\nknn_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"kNN Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', knn_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","fdb415c1":"import warnings\nwarnings.filterwarnings('ignore')\n#Support Vector Machines trial\nimport sys, os\n\n#Import svm model\nfrom sklearn import svm\nfrom sklearn.svm import SVC\n\n#Create a svm Classifier\nclf = SVC(C=1, kernel='rbf')\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_svm = round(clf.score(X_test,y_test) * 100, 2)\nsvm_time=(round(time() - t0, 3))\n# evaluate accuracy\nprint(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', svm_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","4d9f5993":"# visualizing accuracies for all ML Algorithms using Matplotlib\npredictors_group = ('Random Forest', 'GaussianNB', 'DecisionTree','kNN','SVM')\nx_pos = np.arange(len(predictors_group))\naccuracies1 = [acc_rf, acc_gnb, acc_dt,acc_knn, acc_svm]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='blue')\nplt.xticks(x_pos, predictors_group, rotation='vertical')\nplt.ylabel('Accuracy (%)')\nplt.title('Classifier Accuracies')\nplt.show()","c43bffdf":"from sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create an instance of Pipeline\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, max_depth=4))\nstrtfdKFold = StratifiedKFold(n_splits=10)\nkfold = strtfdKFold.split(X_train, y_train)\nscores = []\n\nfor k, (train, test) in enumerate(kfold):\n    pipeline.fit(X_train.iloc[train, :], y_train.iloc[train])\n    score_skf = pipeline.score(X_train.iloc[test, :], y_train.iloc[test])\n    scores.append(score_skf)\n    acc_skf = round(score_skf * 100, 2) # calculating accuracy for plot\n    print('Fold: %2d, Training\/Test Split Distribution: %s, Accuracy: %.3f' % (k+1, np.bincount(y_train.iloc[train]), score_skf))\n    print('\\n\\nCross-Validation accuracy: %.3f +\/- %.3f' %(np.mean(scores), np.std(scores)))","3c2d2ace":"#trial CV knn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create an instance of Pipeline\npipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\nstrtfdKFold = StratifiedKFold(n_splits=10)\nkfold = strtfdKFold.split(X_train, y_train)\nscores = []\n\nfor k, (train, test) in enumerate(kfold):\n    pipeline.fit(X_train.iloc[train, :], y_train.iloc[train])\n    score_skf = pipeline.score(X_train.iloc[test, :], y_train.iloc[test])\n    scores.append(score_skf)\n    acc_skf_knn = round(score_skf * 100, 2) # calculating accuracy for plot\n    print('Fold: %2d, Training\/Test Split Distribution: %s, Accuracy: %.3f' % (k+1, np.bincount(y_train.iloc[train]), score_skf))\n    print('\\n\\nCross-Validation accuracy: %.3f +\/- %.3f' %(np.mean(scores), np.std(scores)))","fb8cec21":"from sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# data pre-processing\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n# use StratifiedShuffleSplit()\nsss = StratifiedShuffleSplit(n_splits=4, test_size=0.5,\n                             random_state=0)\nsss.get_n_splits(X, y)\nscores = []\n\nclf = RandomForestClassifier(n_estimators=40, max_depth=7)\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred) * 100)\n    acc_sss = np.mean(scores)\n    # get accurracy of each prediction\nprint(scores)\nprint('Avg Cross Validation accuracy for Shuffle split: %.3f +\/- %.3f' % (np.mean(scores),np.std(scores)))","87e00474":"#trial knn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# data pre-processing\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n# use StratifiedShuffleSplit()\nsss = StratifiedShuffleSplit(n_splits=4, test_size=0.5,\n                             random_state=0)\nsss.get_n_splits(X, y)\nscores = []\n\nclf = KNeighborsClassifier(n_neighbors=3)\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred) * 100)\n    acc_sss_knn = np.mean(scores)\n    # get accurracy of each prediction\nprint(scores)\nprint('Avg Cross Validation accuracy for Shuffle split: %.3f +\/- %.3f' % (np.mean(scores),np.std(scores)))","4ae8abe8":"# visualizing accuracies \npredictors_group = ('Random Forest', 'GaussianNB', 'DecisionTree','kNN','SVM', 'RF-StratifiedKFold','RF-StratifiedShuffleSplit','knn-StratifiedKFold','knn-StratifiedShuffleSplit')\nx_pos = np.arange(len(predictors_group))\naccuracies1 = [acc_rf, acc_gnb, acc_dt,acc_knn, acc_svm,acc_skf,acc_sss,acc_skf_knn,acc_sss_knn ]\ncolors = ['b','b','b','b','b','r','r','r','r']\nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color=colors)\nplt.xticks(x_pos, predictors_group, rotation='vertical')\nplt.ylabel('Accuracy (%)')\nplt.title('Classifier Accuracies')\nplt.show()","7f700392":"plt.figure(figsize=(10,6))\nx = df_stroke.age\ny =df_stroke.bmi\nplt.xlabel('Age')\nplt.ylabel('bmi')\nplt.scatter(x,y)","765f0227":"df_stroke.drop(df_stroke[df_stroke.age < 20].index, inplace=True)","520559b7":"print (df_stroke.shape)","5f7d5b2c":"#Defining X and y\nXn = df_stroke.drop(['stroke'], axis=1)\nyn = df_stroke['stroke']\n\n# creating dataset split for prediction\nfrom sklearn.model_selection import train_test_split\nXn_train, Xn_test , yn_train , yn_test = train_test_split(Xn,yn,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('Xn_train:', Xn_train.shape)\nprint('yn_train:', yn_train.shape)\nprint('Xn_test:', Xn_test.shape)\nprint('yn_test:', yn_test.shape)","e12230e0":"import warnings\nwarnings.filterwarnings('ignore')\n# 1. Using Random Forest Classifier\nt0 = time()\n# Load random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a random forest Classifier\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier\/fitting the model\nclf.fit(Xn_train, yn_train)\n\n# predict the response\nyn_pred = clf.predict(Xn_test)\nacc_rf_n = round(clf.score(Xn_test,yn_test) * 100, 2)\nrf_time_n=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(yn_test, yn_pred)*100,\"%\")\nprint('Training time', rf_time_n)\n\n#Print Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = pd.DataFrame(confusion_matrix(yn_test, yn_pred))\nsns.heatmap(cm, annot=True,cmap=\"mako_r\")","1ecf9fbc":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of Pipeline\n\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, max_depth=4))\nscores = cross_val_score(pipeline, X=Xn_train, y=yn_train, cv=10, n_jobs=1)\n \nprint('Cross Validation accuracy scores with updated df: %s' % scores)\nprint('updated Avg Cross Validation accuracy: %.3f +\/- %.3f' % (np.mean(scores),np.std(scores)))","74c7531a":"<blockquote>\ud83d\udccc According to World Health Organization, 15 million people suffer strokes worldwide each year. Of these, 5 million die and another 5 million are permanently disabled. Every year, more than 795,000 people in the United States have a stroke.\n<br>\nThe objective of this project is to construct a machine learning model for predicting stroke and to evaluate the accuracy of the model. We are going to apply different machine learning algorithms to see which algorithms produce reliable results with good accuracy.<\/blockquote>","722c0a07":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing Libraries & Packages \ud83d\udcda <\/centre><\/strong><\/h3>","d92d04a8":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Checking for missing values \u270f\ufe0f <\/centre><\/strong><\/h3>","681c3e68":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Plotting all accuracies \ud83d\udcca <\/centre><\/strong><\/h3>","e71676e7":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Dataset split for prediction \u23f3 <\/centre><\/strong><\/h3>","057b2646":"![Colorful Business Data General Linkedin Banner(3).jpg](attachment:21dcd0cb-53b6-4e90-967d-55fb29603575.jpg)","584df33c":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Checking for outliers \u270f\ufe0f <\/centre><\/strong><\/h3>","afe410d6":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Data Exploration for the Dataset \ud83d\udd0d <\/centre><\/strong><\/h3>","c25ca4df":"### <h3 style=\"background-color:#4895ef;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing & Reading the dataset \ud83d\udcdd <\/centre><\/strong><\/h3>"}}