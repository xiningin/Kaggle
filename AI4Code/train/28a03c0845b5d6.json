{"cell_type":{"cf82131b":"code","6f244860":"code","0c8711c1":"code","fc3651a5":"code","209a062f":"code","760481f4":"code","ad734a91":"code","ec8960c7":"code","523d25e4":"code","1409b4ad":"code","62c2fa25":"code","0f82d3ef":"code","02444683":"code","c37618b0":"code","1b62537d":"code","32247934":"code","95ee21bb":"code","fa51361c":"code","038b2c97":"code","82b6f093":"code","23b148b0":"code","b46efeb8":"code","9e9687da":"code","a7ee2694":"code","a5c9f734":"code","e090547f":"code","5c549040":"code","89c549a3":"code","bf387bed":"code","7acb7a5a":"code","b65fe954":"code","31e8ff87":"code","ae569498":"code","f78ea1f3":"code","aa0a08bf":"code","a2b5f5b5":"code","29c60ea2":"code","d3408ba9":"code","51538468":"code","8b24e488":"markdown","676589b7":"markdown","164efc8d":"markdown","bb59fee4":"markdown","8356631f":"markdown","9083fd57":"markdown","dc4481e5":"markdown","270d461b":"markdown","8c71b07d":"markdown","6c618128":"markdown","bd8641c2":"markdown","c566ea45":"markdown","d35d87e0":"markdown"},"source":{"cf82131b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6f244860":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","0c8711c1":"train[:5]","fc3651a5":"train.describe()","209a062f":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","760481f4":"train = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\noutliers = [30, 88, 462, 631, 1322]\ntrain = train.drop(train.index[outliers])","ad734a91":"sns.distplot(train['SalePrice'])\nprint('Skewness: %f' % train['SalePrice'].skew())\nprint('Kurtosis: %f' % train['SalePrice'].kurt())","ec8960c7":"#the qq-plot before log-transfomation\nfrom scipy import stats\nfrom scipy.stats import norm\nfig = plt.figure(figsize=(15,5)) #\u8fd9\u91cc\u8981\u8bb0\u5f97\u5199\u7b49\u4e8e\nplt.subplot(1,2,1) #\u5982\u679c\u9700\u8981\u4e00\u884c\u591a\u5e45\u56fe \u9700\u8981\u5728\u8fd9\u91cc\u5148\u5236\u5b9a\u662f\u54ea\u4e00\u5e45\u56fe\nsns.distplot(train[\"SalePrice\"], fit=norm)\nmu, sigma = norm.fit(train['SalePrice'])\n#plt.legend(['Norm dist. mu = %f, sigma = %f' %(mu,sigma)], loc='upper right')\nplt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)])\nplt.subplot(1,2,2)\nstats.probplot(train['SalePrice'],plot=plt)\nplt.title('Before transfomation')\nprint('mu = {:.2f},\\nsigma = {:.2f}'.format(mu,sigma))\n\n#Do the transformation\ntrain.SalePrice = np.log1p(train.SalePrice)\ny_train = train.SalePrice.values\ny_train_orig = train.SalePrice\n#the reason why we do this is because the models like linear regression and SVM need the data to be norm distribution.\n\n#after the transformation\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train.SalePrice, fit=norm)\nmu, sigma = norm.fit(train.SalePrice)\nplt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(mu, sigma)])\nplt.subplot(1,2,2)\nplt.ylabel('Frequency')\nstats.probplot(train.SalePrice, plot=plt)\nplt.title('After transformation')\nprint('\\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))","523d25e4":"train_X = train.drop('SalePrice',axis=1)\ndata_features = pd.concat((train_X, test)).reset_index(drop=True)\nprint(data_features.shape)\ndata_features.columns\n#We concatenate the train set and the test set since we need to handle the data both on the train set and the test set.","1409b4ad":"data_features[['MSSubClass', 'MSZoning']]","62c2fa25":"data_features_na = data_features.isnull().sum().sort_values(ascending=False)\ndata_features_na = data_features_na[data_features_na>0]\ndata_features_na","0f82d3ef":"percent = (data_features.isnull().sum()\/data_features.isnull().count()).sort_values(ascending=False)\npercent = percent[percent>0]\npd.concat([data_features_na, percent],axis=1,keys=['total', 'percent'])","02444683":"#fill the missing values\nx = data_features.loc[(data_features['LotArea'].notnull()), 'LotArea']\ny = data_features.loc[(data_features['LotFrontage'].notnull()), 'LotFrontage']\nt = (x<25000) & (y<150)\ncoef = np.polyfit(x[t], y[t],1)\nformula = np.poly1d(coef)\npoly_y = formula(data_features['LotArea'])\ncondition_frontage = (data_features['LotFrontage'].isnull())\ndata_features.loc[condition_frontage,'LotFrontage'] = formula(data_features.loc[condition_frontage,'LotArea'])\n\ngarage_var = ['GarageYrBlt','GarageCond','GarageFinish','GarageQual']\ncondition1 = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\nfor col in garage_var:\n    data_features.loc[condition1,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]\n    \ncondition2 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())\ndata_features.loc[condition2,'BsmtExposure'] = 'No'\n\ncondition3 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())\ndata_features.loc[condition3,'BsmtCond'] = 'TA'\n\ncondition4 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())\ndata_features.loc[condition4,'BsmtQual'] = data_features.loc[(data_features['BsmtExposure'] == 'No'),'BsmtQual'].mode()[0]\n\ncondition5 = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull())\ndata_features.loc[condition5, 'BsmtFinType2'] = 'Unf'\n\nbsmt_var = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']\ngarage_var = ['GarageType','GarageCond','GarageFinish','GarageQual']\nNONE_var = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\nfor col in bsmt_var, garage_var,NONE_var:\n    data_features[col] = data_features[col].fillna('None')\ndata_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)\n\ncondition6 = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull())\ndata_features.loc[condition6,'MasVnrType'] = 'Stone'\n\ndata_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')\ndata_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)\n\ndata_features['MSZoning'] = data_features['MSZoning'].groupby(\n    data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))\n\nNA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageArea', 'GarageCars','MasVnrArea']\nfor col in NA_for_0:\n    data_features[col] = data_features[col].fillna(0)\n    \ncommon_for_NA = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\nfor col in common_for_NA:\n    data_features[col].fillna(data_features[col].mode()[0], inplace = True)\n    \ndata_features['Functional'] = data_features['Functional'].fillna('Typ')\ndata_features['Utilities'] = data_features['Utilities'].fillna('None')","c37618b0":"#Now the check the missing values\nmissing_data = data_features.isnull().sum().sort_values(ascending = False)\nmissing_data = missing_data[missing_data > 0]\nmissing_data","1b62537d":"#Some number features stand for categories.\nstr_var = [\"MSSubClass\",'MoSold','YrSold']\nfor var in str_var:\n    data_features[var] = data_features[var].apply(str)","32247934":"#Mapping values for categorical features.\n#I map values as much as possible though other kernels choose some of them to transform.\ndata_features[\"MSSubClass\"] = data_features.MSSubClass.map({'180':1, \n                                        '30':2, '45':2, \n                                        '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, \n                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n\ndata_features[\"MSZoning\"] = data_features.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n\ndata_features[\"Street\"] = data_features.Street.map({'Grvl':1, 'Pave':2})\n\ndata_features[\"Alley\"] = data_features.Alley.map({'None':0, 'Grvl':1, 'Pave':2})\n\ndata_features[\"LotShape\"] = data_features.LotShape.map({'Reg':1, 'IR1':2, 'IR2':3, 'IR3':3})\n\ndata_features[\"LandContour\"] = data_features.LandContour.map({'Bnk':1, 'Lvl':2, 'Low':3, 'HLS':4})\n\ndata_features[\"Utilities\"] = data_features.Utilities.map({'NoSeWa':0, 'None':0,'AllPub':1})\n\ndata_features[\"LotConfig\"] = data_features.LotConfig.map({'Inside':1, 'Corner':1, 'CulDSac':2, 'FR2':1, 'FR3':2})\n\ndata_features[\"LandSlope\"] = data_features.LandSlope.map({'Gtl':1, 'Mod':2, 'Sev':2})\n\ndata_features[\"Neighborhood\"] = data_features.Neighborhood.map({'MeadowV':1,\n                                               'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})\n    \ndata_features[\"Condition1\"] = data_features.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})\n    \ndata_features[\"BldgType\"] = data_features.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n    \ndata_features[\"HouseStyle\"] = data_features.HouseStyle.map({'1.5Unf':1, \n                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n                                           '1Story':3, 'SLvl':3,\n                                           '2Story':4, '2.5Fin':4})\n    \ndata_features['RoofStyle'] = data_features.RoofStyle.map({'Gambrel':1, 'Gable':2, 'Flat':3, 'Hip':3, 'Mansard':3, 'Shed':4})\n\ndata_features[\"Exterior1st\"] = data_features.Exterior1st.map({'BrkComm':1,\n                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n                                             'BrkFace':4, 'Plywood':4,\n                                             'VinylSd':5,\n                                             'CemntBd':6,\n                                             'Stone':7, 'ImStucc':7})\n    \ndata_features[\"MasVnrType\"] = data_features.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n    \ndata_features[\"ExterQual\"] = data_features.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \ndata_features[\"Foundation\"] = data_features.Foundation.map({'Slab':1, \n                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n                                           'Wood':3, 'PConc':4})\n    \ndata_features[\"BsmtQual\"] = data_features.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n\ndata_features[\"BsmtCond\"] = data_features.BsmtCond.map({'Fa':2, 'None':0, 'TA':3, 'Gd':4, 'Po':1})\n    \ndata_features[\"BsmtExposure\"] = data_features.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n    \ndata_features[\"Heating\"] = data_features.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n    \ndata_features[\"HeatingQC\"] = data_features.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\ndata_features['CentralAir'] = data_features.CentralAir.map({'N':1, 'Y':2})\n\ndata_features[\"Electrical\"] = data_features.Electrical.map({'Mix':1, 'FuseP':2, 'FuseF':3, 'FuseA':4, 'SBrkr':5})\n    \ndata_features[\"KitchenQual\"] = data_features.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \ndata_features[\"Functional\"] = data_features.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n    \ndata_features[\"FireplaceQu\"] = data_features.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \ndata_features[\"GarageType\"] = data_features.GarageType.map({'CarPort':1, 'None':1,\n                                           'Detchd':2,\n                                           '2Types':3, 'Basment':3,\n                                           'Attchd':4, 'BuiltIn':5})\n    \ndata_features[\"GarageFinish\"] = data_features.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n\ndata_features[\"GarageQual\"] = data_features.GarageQual.map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':4})\n    \ndata_features[\"PavedDrive\"] = data_features.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n\ndata_features[\"PoolQC\"] = data_features.PoolQC.map({'None':0, 'Gd':1, 'Fa':1, 'Ex':2})\n    \ndata_features[\"SaleType\"] = data_features.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n                                       'CWD':2, 'Con':3, 'New':3})\n    \ndata_features[\"SaleCondition\"] = data_features.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})","95ee21bb":"#For variables about years,we use labelencoder method to transform. \n#You can also use one-hot encode but I don't like the dimensional disaster...\nle = preprocessing.LabelEncoder()\n#ohe = preprocessing.OneHotEncoder()\nYear_var = ['YearBuilt','YearRemodAdd','GarageYrBlt']\nfor var in Year_var:\n    data_features[var] = le.fit_transform(data_features[var])\ndata_features[Year_var]","fa51361c":"data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea']\ndata_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure']\ndata_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\ndata_features[\"AllSF\"] = data_features[\"GrLivArea\"] + data_features[\"TotalBsmtSF\"]\ndata_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath']\ndata_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']","038b2c97":"train1 = data_features[:len(y_train)]\ntrain1.loc[:,'SalePrice'] = y_train\ncorr1 = train1.corr()['SalePrice'].sort_values(ascending = False)\ncorr15 = corr1[:15]\ncorr15","82b6f093":"#Polynomial on the top 10 features.For the features have connection themself, only use one to do polynomail.\ncorr10 = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea'])\ncorr10_var = corr10.index.tolist()\nfor col in corr10_var:\n    data_features[col + '-2'] = data_features[col] **2\n    data_features[col + '-3'] = data_features[col] **3\n    data_features[col + '-sqrt'] = np.sqrt(data_features[col])\n\n","23b148b0":"cat_features = data_features.select_dtypes(include = ['object']).columns\nnum_features = data_features.select_dtypes(exclude = ['object']).columns\nprint(cat_features)\nprint('Categorial features :' + str(len(cat_features)) + '\\n')\n\nprint(num_features)\nprint('Numerical features :' + str(len(num_features)))","b46efeb8":"data_num = data_features[num_features]\ndata_cat = data_features[cat_features]\ndata_num.head()\ndata_cat.head()","9e9687da":"skew_features = data_num.skew().sort_values(ascending = False)\nskew_features","a7ee2694":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.base import BaseEstimator, TransformerMixin","a5c9f734":"class skewness__dummies(BaseEstimator, TransformerMixin):\n    def __init__(self, skew = 0.5):\n            pass\n    def fit(self, X, y = None):\n            return self\n    def transform(self, X, y = None):\n            num_fea = X.select_dtypes(exclude = ['object'])\n            skew_fea = X[abs(num_fea.skew()) > skew ].index\n            X[skew_fea] = boxcox1p(X.loc[:,skew_fea], boxcox_normmax(X.loc[:,skew_fea]))\n            X = pd.get_dummies(X)\n            return X\n            ","e090547f":"skew_features = skew_features[abs(skew_features) > 0.5]\nprint('The mean skewness of the variables is{}'.format(np.mean(data_num.skew())))\nprint('There are {} features have to boxcox1p transform'.format(len(skew_features)))\nskew_features","5c549040":"skew_features_index = skew_features.index\nfor features in skew_features_index:\n    data_num.loc[:,features] = boxcox1p(data_num.loc[:,features], boxcox_normmax(data_num.loc[:,features] + 1) )\n    data_num.loc[:,features] = boxcox1p(data_num.loc[:,features], boxcox_normmax(data_num.loc[:,features] + 1))\n    \nprint('After the transformation the mean skewness of the features is {}'.format(np.mean(data_num.skew())))\ndata_num.skew().sort_values(ascending = False)","89c549a3":"len(data_features)","bf387bed":"final_features = pd.get_dummies(data_features)\nX_train = final_features.iloc[:len(y_train),:]\nX_test = final_features.iloc[len(y_train):,:]\nprint('The shape of train set is{},y set is{},and the shape of test set is{}'.format(X_train.shape,y_train.shape,X_test.shape))","7acb7a5a":"type(X)","b65fe954":"X.isnull().sum().sort_values(ascending = False)","31e8ff87":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","ae569498":"kfolds = KFold(n_splits=18, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","f78ea1f3":"models = {'XGBoost':XGBRegressor(n_estimators=1000,\n                                max_depth=6,\n                                objective ='reg:squarederror')}","aa0a08bf":"models.keys()","a2b5f5b5":"for name in models.keys():\n    model = models[name]\n    model.fit(X_train, y_train)\n    scores = cv_rmse(model,X_train, y_train)\n    print(scores.mean)","29c60ea2":"scores.mean()","d3408ba9":"sample_submission.iloc[:,1] = (np.expm1(model.predict(X_test)))\nsample_submission","51538468":"sample_submission.to_csv(\"submission.csv\", index=False)","8b24e488":"We have to split data into the numerical variables and categorial features.\n* For numerical features, we have to adjust their skewness.\n* For categorical features(those we don't encode them ), we use get_dummies to encode them with one-hot.","676589b7":"From the results below we know that the predicted variable has skew","164efc8d":"# step 5 Numerical and Categorial features","bb59fee4":"# Step3 The missing values\nAfter handling the predicted variables, now we have to handle the variables. First we have to observe the missing values.","8356631f":"Now we use box-cox transformation to get rid of the skewness of the variables which is different from the predicted variable.\n\nAnd we just handle the variables whose abs skewness > 0.5","9083fd57":"# Step8 Preparing the data","dc4481e5":"# \u8fd9\u91cc\u662f\u5206\u5272\u7ebf \u4e0b\u9762\u7684\u5185\u5bb9\u90fd\u662f\u6284\u7684\u73a9\u610f","270d461b":"# Step6 Data visualization","8c71b07d":"# Step2 Predicted values - Skewness\nFirst of all we should observe the most important variable - the predicted variable SalePrice\n\nWe focus on the skewness and the kurtosis","6c618128":"# Preface\n* Hi! This is my first attempt in data science competition.This project not only put my knowledge from books to practice and also improve my coding skills.\n* During the whole journey, I read lots of amazing kernels which help me in every espects. I list some of them below, and you can explore more helpful kernels in kaggle.And this is the main reason why I want to write this kernel and share it to all of you.I believe what counts in this community is everyone's contributions to this.","bd8641c2":"# Step1 Import data\n\nLet's take the first look on these features ","c566ea45":"For numerical features , we need to observe their skewness just like the predicted variable.","d35d87e0":"## Step4 Data preprocessing and Feature Engineering"}}