{"cell_type":{"17e9bc0e":"code","227395fe":"code","1d7c2d81":"code","83e13856":"code","5e68b9fe":"code","1932bc30":"code","ab54a906":"code","d38ccab9":"code","932c345c":"code","bd1ed895":"code","8195813e":"code","c4449114":"code","caf21e78":"code","0f47908b":"code","a781436f":"code","62da541b":"code","43e8bc49":"code","8e94684b":"code","5f352533":"code","22349ee0":"code","1dc69fc0":"code","e36768b4":"code","c47dcd01":"code","e68408b4":"code","3fad08f2":"code","b8ffc1e3":"code","71c3827e":"code","7010ce72":"code","b6d2822d":"code","0237719d":"code","33eca888":"code","a4d89d70":"code","34b37047":"code","4b3c7a8e":"code","1fadcad0":"code","8425eff8":"code","29d69a74":"code","4b6da224":"code","44c7203f":"code","540a576e":"code","60caf896":"code","3d00a2b5":"code","2c4f929d":"markdown","6b3775cc":"markdown","2f6c16f9":"markdown","bef35d0c":"markdown","cd2a679a":"markdown","079c5524":"markdown","ab51aea2":"markdown","421db73f":"markdown","e5ef8875":"markdown","b9312858":"markdown","0662c2ee":"markdown","65c439f8":"markdown","dbdb6e02":"markdown","1e4f629d":"markdown","db08de31":"markdown","854c61e1":"markdown","acc56ed6":"markdown","061f8798":"markdown","52823947":"markdown","e8815269":"markdown","1405bc6c":"markdown","6dca03f4":"markdown"},"source":{"17e9bc0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","227395fe":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","1d7c2d81":"train['text']=train['text'].apply(lambda x : x.lower())\ntest['text']=test['text'].apply(lambda x : x.lower())\n\ndef lower_keywords(keywords):\n    if keywords == keywords:\n        keywords=keywords.lower()\n    return keywords\n        \ntrain['keyword']=train['keyword'].apply(lambda x : lower_keywords(x))\ntest['keyword']=test['keyword'].apply(lambda x : lower_keywords(x))\n\ntest.head()","83e13856":"def keywords_to_list(keywords):\n    if keywords!=keywords: # nan value is not equal to itself\n        return []\n    else:\n        return keywords.split('%20')\n            \ntrain['keyword']=train['keyword'].apply(lambda x : keywords_to_list(x))\ntest['keyword']=test['keyword'].apply(lambda x : keywords_to_list(x))\n\ntest.head()","5e68b9fe":"import re\ntrain['hashtag'] = train['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\ntest['hashtag'] = test['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n\ntest.head(20)","1932bc30":"!pip install tweet-preprocessor\nimport preprocessor","ab54a906":"train['text'] = train['text'].apply(lambda x: preprocessor.clean(x))\ntest['text'] = test['text'].apply(lambda x: preprocessor.clean(x))\n\ndef clear_list(lista):\n    try:\n        for i,ele in enumerate(lista):\n            lista[i]=preprocessor.clean(ele)\n        return lista\n    except:\n        print(lista)\n\ntrain['hashtag'] = train['hashtag'].apply(lambda x: clear_list(x))\ntest['hashtag'] = test['hashtag'].apply(lambda x: clear_list(x))\n\ntrain['keyword'] = train['keyword'].apply(lambda x: clear_list(x))\ntest['keyword'] = test['keyword'].apply(lambda x: clear_list(x))\n\ntest.head()","d38ccab9":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntrain['text'] = train['text'].apply(lambda x: decontracted(x))\ntest['text'] = test['text'].apply(lambda x: decontracted(x))","932c345c":"import re\n\ndef remove_punc(lista):\n    for i,ele in enumerate(lista):\n        lista[i] = re.sub(r'[^\\w\\s]', '', ele)\n        lista[i] = re.sub('_', ' ', lista[i]) # the previous row doesn't remove underscore\n    return lista\n\ntrain['text']=train['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ntrain['text']=train['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\ntest['text']=test['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ntest['text']=test['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\n\ntrain['hashtag']=train['hashtag'].apply(lambda x: remove_punc(x))\ntest['hashtag']=test['hashtag'].apply(lambda x: remove_punc(x))\n\ntrain['keyword']=train['keyword'].apply(lambda x: remove_punc(x))\ntest['keyword']=test['keyword'].apply(lambda x: remove_punc(x))\n\ntest.head()","bd1ed895":"import nltk\n\ntrain['text']=train['text'].apply(lambda x: nltk.word_tokenize(x))\ntest['text']=test['text'].apply(lambda x: nltk.word_tokenize(x))\n\ntest.head(10)","8195813e":"from nltk.corpus import stopwords\nstop_words=stopwords.words('english')\nstop_words.append('u') # 'i love u' is the semantically the same as 'i love you'\nstop_words.append('one') # want to remove numbers\nstop_words.append('two')\nstop_words.append('three')\nstop_words.append('four')\nstop_words.append('five')\nstop_words.append('six')\nstop_words.append('seven')\nstop_words.append('eight')\nstop_words.append('nine')\nstop_words.append('ten')\n\ndef remove_stop_words(lista):\n    pt=0 # don't use a for loop because len(lista) keeps changing as we remove stop words.\n    while pt<len(lista):\n        if lista[pt] in stop_words:\n            lista.remove(lista[pt])\n        else:\n            pt+=1\n    return lista\n\ntrain['text']=train['text'].apply(lambda x : remove_stop_words(x))\ntrain['hashtag']=train['hashtag'].apply(lambda x : remove_stop_words(x))\ntrain['keyword']=train['keyword'].apply(lambda x : remove_stop_words(x))\n\ntest['text']=test['text'].apply(lambda x : remove_stop_words(x))\ntest['hashtag']=test['hashtag'].apply(lambda x : remove_stop_words(x))\ntest['keyword']=test['keyword'].apply(lambda x : remove_stop_words(x))","c4449114":"from nltk import WordNetLemmatizer ","caf21e78":"WordNetLemmatizer().lemmatize('us')","0f47908b":"def lemmatize_list(lista):\n    for i, ele in enumerate(lista):\n        lista[i]=WordNetLemmatizer().lemmatize(ele)\n    return lista\n\ntrain['text']=train['text'].apply(lambda x : lemmatize_list(x))\ntrain['hashtag']=train['hashtag'].apply(lambda x : lemmatize_list(x))\ntrain['keyword']=train['keyword'].apply(lambda x : lemmatize_list(x))\n\ntest['text']=test['text'].apply(lambda x : lemmatize_list(x))\ntest['hashtag']=test['hashtag'].apply(lambda x : lemmatize_list(x))\ntest['keyword']=test['keyword'].apply(lambda x : lemmatize_list(x))\n\ntest.head()","a781436f":"train.to_csv('preprocess_train.csv')\ntest.to_csv('preprocess_test.csv')","62da541b":"maxi=0\n\nfor ele in train['text']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['text']:\n    maxi=max(maxi,len(ele))\n    \nmaxi_text=maxi\nmaxi_text","43e8bc49":"maxi=0\n\nfor ele in train['hashtag']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['hashtag']:\n    maxi=max(maxi,len(ele))\n    \nmaxi\nmaxi_hashtag=maxi\nmaxi_hashtag","8e94684b":"maxi=0\n\nfor ele in train['keyword']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['keyword']:\n    maxi=max(maxi,len(ele))\n    \nmaxi_keyword=maxi\nmaxi_keyword","5f352533":"!pip3 install spacy\n!python3 -m spacy download en_core_web_lg","22349ee0":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","1dc69fc0":"import math\ndef distance(vec1,vec2):\n    sum=0\n    for i in range(len(vec1)):\n        sum+=(vec1[i]-vec2[i])**2\n    return math.sqrt(sum)","e36768b4":"doc = nlp(\"father grandfather\") # change your words here ","c47dcd01":"doc[0].vector.shape","e68408b4":"print(distance(doc[0].vector, doc[1].vector)) # the smaller the more similar\nprint(doc[0].similarity(doc[1])) # the larger the more similar","3fad08f2":"m=train.shape[0]\nstore_train=np.zeros((m,23+13+2,300))\nfor i in range(m): # m\n    if i % 100 == 99:\n        print(i)\n    for j in range(len(train['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n        store_train[i,j,:]=nlp(train['text'][i][j])[0].vector\n    for j in range(len(train['hashtag'][i])):\n        try:\n            store_train[i,23+j,:]=nlp(train['hashtag'][i][j])[0].vector\n        except:\n            store_train[i,23+j,:]=nlp(train['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n    for j in range(len(train['keyword'][i])):\n        store_train[i,36+j,:]=nlp(train['keyword'][i][j])[0].vector","b8ffc1e3":"np.save('store_train.npy',store_train)","71c3827e":"m=test.shape[0]\nstore_test=np.zeros((m,23+13+2,300))\nfor i in range(m): # m\n    if i % 100 == 99:\n        print(i)\n    for j in range(len(test['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n        store_test[i,j,:]=nlp(test['text'][i][j])[0].vector\n    for j in range(len(test['hashtag'][i])):\n        try:\n            store_test[i,23+j,:]=nlp(test['hashtag'][i][j])[0].vector\n        except:\n            store_test[i,23+j,:]=nlp(test['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n    for j in range(len(test['keyword'][i])):\n        store_test[i,36+j,:]=nlp(test['keyword'][i][j])[0].vector","7010ce72":"np.save('store_test.npy',store_test)","b6d2822d":"store_train_text=store_train[:,:maxi_text,:]\nstore_train_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\nstore_train_keyword=store_train[:,-maxi_keyword:,:]","0237719d":"print(store_train_text.shape)\nprint(store_train_hashtag.shape)\nprint(store_train_keyword.shape)","33eca888":"store_test_text=store_train[:,:maxi_text,:]\nstore_test_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\nstore_test_keyword=store_train[:,-maxi_keyword:,:]","a4d89d70":"import tensorflow as tf\nimport keras.backend as K\nfrom keras.layers import Input, Dropout, GRU, BatchNormalization, TimeDistributed, Reshape, Dense, Conv1D, Concatenate\nfrom keras import Model\nimport keras","34b37047":"# input_text=Input(shape=(store_train_text.shape[1],store_train_text.shape[2]))\n# input_hashtag=Input(shape=(store_train_hashtag.shape[1],store_train_hashtag.shape[2]))\n# input_keyword=Input(shape=(store_train_keyword.shape[1],store_train_keyword.shape[2]))\n\n# mid1=GRU(units=128, return_sequences=True)(input_text)\n# mid1=Dropout(0.8)(mid1)\n# mid1=BatchNormalization()(mid1)  \n\n# mid1=GRU(units=16, return_sequences=True)(mid1)\n# mid1=Dropout(0.8)(mid1)\n# mid1=BatchNormalization()(mid1)  \n\n# mid1=GRU(units=1, return_sequences=False)(mid1)\n# #mid1=Dropout(0.8)(mid1)\n# #mid1=BatchNormalization()(mid1)\n# print(mid1.shape)\n\n# # mid1=Dropout(0.8)(mid1)\n# # mid1=TimeDistributed(Dense(1, activation = \"relu\"))(mid1)\n# # mid1=Reshape((mid1.shape[1],))(mid1)\n# # # mid1 has shape (m,23)\n\n# mid2=TimeDistributed(Dense(128, activation = \"relu\"))(input_hashtag)\n# # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n# print(mid2.shape)\n# mid2=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid2)\n# mid2=Reshape((mid2.shape[1],))(mid2)\n# # now mid2 has shape (m,13)\n# #print(mid2.shape)\n\n# # mid3=Conv1D(30, kernel_size=1, strides=1, padding='valid')(input_keyword)\n# # # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n# # mid3=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid3)\n# # mid3=Reshape((mid3.shape[1],))(mid3)\n\n# # mid=Concatenate(axis=-1)([mid1,mid2,mid3])\n# # #print(mid1.shape,mid2.shape,mid3.shape)\n# # output=Dense(2, activation=\"softmax\")(mid)\n# # #print(output.shape)\n\n# # model=Model(inputs=[input_text,input_hashtag, input_keyword], outputs=outputs)\n# # model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy')","4b3c7a8e":"inp=Input(shape=(store_train.shape[1],store_train_text.shape[2]))\n\nmid=GRU(units=300, return_sequences=True)(inp)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=GRU(units=300, return_sequences=True)(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=GRU(units=300, return_sequences=True)(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=Dropout(0.6)(mid)\nmid=TimeDistributed(Dense(1,activation='relu'))(mid)\nmid=Reshape((mid.shape[1],))(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid) \noutp=Dense(2,activation='softmax')(mid)\n\n\nmodel=Model(inputs=inp, outputs=outp) ","1fadcad0":"np.random.seed(3)\nlis1=np.array([[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9]])\nlis2=np.array([1,2,3,4,5,6,7,8,9])\nnp.random.shuffle(lis1)\nnp.random.seed(3)\nnp.random.shuffle(lis2)\nprint(lis1)\nprint(lis2)","8425eff8":"store_train=np.load('store_train.npy')\n\nm=store_train.shape[0]\ntrain_Y=np.zeros((m,2))\nfor i in range(m):\n    train_Y[i,train.iloc[i]['target']]=1","29d69a74":"sed=13\nnp.random.seed(sed)\nnp.random.shuffle(store_train)\nnp.random.seed(sed)\nnp.random.shuffle(train_Y)","4b6da224":"model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics='accuracy')","44c7203f":"model.fit(store_train[0:-500,:,:], train_Y[0:-500,:], batch_size=64, epochs=50, verbose=1)","540a576e":"model.evaluate(store_train[-500:,:,:], train_Y[-500:,:])","60caf896":"test_Y=model.predict(store_test)\n\ntest_label=[]\n\nfor i in range(test_Y.shape[0]):\n    if test_Y[i,1]>=0.5:\n        test_label.append(1)\n    else:\n        test_label.append(0)","3d00a2b5":"submission=pd.DataFrame({'id': test['id'], 'target':test_label})\nprint(submission.head(10))\n\nfilename = 'submission_nlp_tweets.csv'\n\nsubmission.to_csv(filename,index=False)","2c4f929d":"I copied the code from the follow url by Yann Dubois <br>\nhttps:\/\/stackoverflow.com\/questions\/43018030\/replace-apostrophe-short-words-in-python","6b3775cc":"**0. lower case**","2f6c16f9":"**7. remove stop words**","bef35d0c":"**18. submit**","cd2a679a":"**9. save the preprocessed files**","079c5524":"The author Fanglida Yan has used code from these references in the notebook. <br>\nhttps:\/\/towardsdatascience.com\/basic-tweet-preprocessing-in-python-efd8360d529e <br>\nhttps:\/\/www.youtube.com\/watch?v=hhjn4HVEdy0 <br>\n\n0. lower case <br>\n1. turn key words into lists <br>\n2. extract hashtags and create a new feature column <br>\n3. remove digits (01234), urls (http:\/\/...), mentions (@...) and hashtags (#...) <br>\n4. recover abbreviations (change they'll to they will, etc) <br>\n5. remove punctuations <br>\n6. tokenization <br>\n7. remove stop words <br>\n8. lemmatization <br>\n12. word embedding","ab51aea2":"**12. use word embedding and create training set, the training set has dimension (m,23+13+2,300)**","421db73f":"**16. evaluate the cross validation set**","e5ef8875":"**13. slice the train and test sets**","b9312858":"understand  np.random.shuffle and np.random.seed","0662c2ee":"**8. lemmatization. ('us' is lemmatized to 'u')** ","65c439f8":"**1. turn keywords into lists**","dbdb6e02":"**5. remove punctuations**","1e4f629d":"**6. tokenization**","db08de31":"**11. install and understand word embedding**","854c61e1":"**10. find maximum tweet length, maximum hashtag length, maximum keywords length**","acc56ed6":"**2. extract hashtags and create a new feature column**","061f8798":"**14. build the model**","52823947":"**3. remove digits (01234), urls (http:\/\/...), mentions (@...) and hashtags (#...)**","e8815269":"**15. create labels for the training sets**","1405bc6c":"**17. predict the test set**","6dca03f4":"**4. recover abbreviations (change they'll to they will, etc)**"}}