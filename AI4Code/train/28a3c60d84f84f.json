{"cell_type":{"221b81cc":"code","3d425191":"code","bcd0e275":"code","932e7dc4":"code","96ad13af":"code","dfb06b9a":"code","8b39783a":"code","af69f795":"code","b1ba84a0":"code","9557e51b":"code","0786146a":"code","5ce885ab":"code","8c9ee42e":"code","f93e71a5":"code","59a48902":"code","d1968c6b":"code","26be0d99":"code","e49ee88f":"code","778ee459":"code","0d6e332f":"code","19c07417":"code","88fad725":"code","8a151806":"code","90b50dfb":"code","c5a8b51a":"code","70523bcb":"markdown","a22e05a1":"markdown","4e72d840":"markdown","f8dc3784":"markdown","5a89f3f0":"markdown","bd22b824":"markdown","aad1f619":"markdown","e31f5f65":"markdown"},"source":{"221b81cc":"import tensorflow as tf\nprint(tf.__version__)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d425191":"_URL = \"https:\/\/storage.googleapis.com\/ztm_tf_course\/food_vision\/pizza_steak.zip\"\n\nzip_file = tf.keras.utils.get_file(origin=_URL,\n                                   fname=\"pizza_steak.zip\",\n                                   extract=True)\n\nbase_dir = os.path.join(os.path.dirname(zip_file), 'pizza_steak')","bcd0e275":"for dirpath, dirnames, filenames in os.walk(base_dir):\n  print(\"Parent Directory Path : \",dirpath)\n  print(\"Sub-Directories\",f\"({len(dirnames)}) : \",dirnames)\n  print(\"Contained Files\",f\"({len(filenames)}) : \",filenames)\n  print()","932e7dc4":"train_dir = os.path.join(base_dir,'train')\ntest_dir  = os.path.join(base_dir, 'test')","96ad13af":"int2string = []\nclass_label = 0 \nfor class_name in os.listdir(train_dir):\n  class_dir = train_dir + \"\/\" + class_name\n  if(os.path.isdir(class_dir)):\n    os.rename(class_dir, train_dir + \"\/\" + f\"{class_label}\")\n    class_label = class_label + 1\n    int2string.append(class_name)","dfb06b9a":"int2string.index(\"pizza\"), int2string.index(\"steak\")","8b39783a":"for class_name in os.listdir(test_dir):\n  class_dir = test_dir + \"\/\" + class_name\n  if(os.path.isdir(class_dir)):\n    os.rename(class_dir, test_dir + \"\/\" + f\"{int2string.index(class_name)}\")","af69f795":"for dirpath, dirnames, filenames in os.walk(base_dir):\n  print(\"Parent Directory Path : \",dirpath)\n  print(\"Sub-Directories\",f\"({len(dirnames)}) : \",dirnames)\n  print(\"Contained Files\",f\"({len(filenames)}) : \",filenames)\n  print()","b1ba84a0":"train_ds_files = tf.data.Dataset.list_files(train_dir + \"\/*\/*\", shuffle = False)\ntest_ds_files = tf.data.Dataset.list_files(test_dir + \"\/*\/*\", shuffle = False)","9557e51b":"#Let's visualize our images\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random","0786146a":"IMG_SHAPE = 224","5ce885ab":"def process_and_scale(filepath):\n    label = tf.strings.split(filepath, os.sep)[-2]\n    label = tf.strings.to_number(label, out_type=tf.dtypes.float32)\n    img = tf.io.read_file(filepath)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_flip_up_down(img)\n    img = tf.image.random_brightness(img,0.2)\n    #img = tf.image.random_contrast(img,0.2,0.5)\n    #img = tf.image.random_hue(img, 0.2)\n    #img = tf.image.random_saturation(img, 5, 10) \n    img = tf.image.resize(img,[IMG_SHAPE,IMG_SHAPE])\n    \n    return img\/255., label","8c9ee42e":"for img_path in train_ds_files:\n  img, label = process_and_scale(img_path)\n  if img.shape == (224,224,1):\n    plt.imshow(tf.squeeze(img).numpy())\n    img_path = tf.compat.as_str_any(img_path.numpy())\n    print(img_path)\n    os.remove(img_path)       #remove noise\/corrupt data","f93e71a5":"train_ds_files = tf.data.Dataset.list_files(train_dir + \"\/*\/*\", shuffle = False)\ntest_ds_files = tf.data.Dataset.list_files(test_dir + \"\/*\/*\", shuffle = False)","59a48902":"train_size = len(train_ds_files)\ntest_size = len(test_ds_files)\nf\"Train Size: {train_size}\", f\"Test Size: {test_size}\"","d1968c6b":"num_epochs = 10","26be0d99":"train_ds = train_ds_files.shuffle(train_size).repeat(num_epochs).take(train_size)\ntrain_ds = train_ds.map(process_and_scale, num_parallel_calls = tf.data.AUTOTUNE).batch(16).prefetch(tf.data.AUTOTUNE)\n\ntest_ds  = test_ds_files.shuffle(test_size).repeat(num_epochs).take(test_size)\ntest_ds  = test_ds.map(process_and_scale, num_parallel_calls = tf.data.AUTOTUNE).batch(16).prefetch(tf.data.AUTOTUNE)","e49ee88f":"from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Concatenate, Activation, Add\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model","778ee459":"input_placeholder = Input(shape = (224,224,3))\nconv1_out = Conv2D(filters = 32, \n           kernel_size = (3,3),\n           strides=(2,2),\n           activation = 'relu',\n           padding = 'valid')(input_placeholder) #(111,111,32)\nconv2_out = Conv2D(filters=32,\n           kernel_size = (3,3),\n           strides = (1,1),\n           activation='relu',\n           padding = 'valid')(conv1_out) #(109, 109, 32)\nconv3_out = MaxPool2D(pool_size = (3,3),\n                      strides=(2,2),\n                      padding='valid')(conv2_out) #(54,54,32)\nconv4_out = Conv2D(filters=64,\n           kernel_size = (1,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv2_out) #(109,109,64)\n\nconv5_out = Conv2D(filters=64,\n           kernel_size = (3,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv4_out) #(109,109,64)\nconv6_out = Conv2D(filters=64,\n           kernel_size = (1,3),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv5_out) #(109,109,64)\nconv7_out = Conv2D(filters=96,\n           kernel_size = (3,3),\n           strides = (2,2),\n           activation='relu',\n           padding = 'valid')(conv6_out) #(54,54,96)\n\nconcat1_out = Concatenate(axis = -1)([conv3_out, conv7_out]) #(54,54,128)\nconcat1_out = Conv2D(filters=256,\n              kernel_size = (3,3),\n              strides = (1,1),\n              activation='relu',\n              padding = 'same')(conv6_out) #(54,54,256)\nconcat1_relu =  Activation('relu')(concat1_out)\n\nconv8_out = Conv2D(filters=32,\n           kernel_size = (1,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(concat1_relu) #(54,54,32)\nconv9_out = Conv2D(filters=32,\n           kernel_size = (1,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(concat1_relu) #(54,54,32)\n\nconv10_out = Conv2D(filters=32,\n           kernel_size = (3,3),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv9_out) #(54,54,32)\n\nconv11_out = Conv2D(filters=32,\n           kernel_size = (1,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(concat1_relu ) #(54,54,32)\nconv12_out = Conv2D(filters=48,\n           kernel_size = (3,3),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv11_out) #(54,54,48)\n\nconv13_out = Conv2D(filters=64,\n           kernel_size = (3,3),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(conv12_out) #(54,54,64)\n\nconcat2_out = Concatenate(axis=-1)([conv8_out, conv10_out,conv13_out]) #(54,54,128)\nconv14_out = Conv2D(filters=256,\n           kernel_size = (1,1),\n           strides = (1,1),\n           activation='relu',\n           padding = 'same')(concat2_out) #(54,54,256)\n\naddedTensor = Add()([concat1_relu, tf.math.multiply(conv14_out,0.1)])\nrelu_out = Activation('relu')(addedTensor)\n\nx = MaxPool2D(pool_size=(2,2))(relu_out)\nx = Flatten()(x)\nout = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inputs=[input_placeholder],\n                outputs=[out])","0d6e332f":"model.compile(loss=\"binary_crossentropy\",\n              optimizer=  tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])","19c07417":"model.summary()","88fad725":"history = model.fit(train_ds,\n                    epochs=num_epochs, #10,\n                    steps_per_epoch=len(train_ds), #2*len(train_ds),\n                    validation_data=test_ds,\n                    validation_steps=len(test_ds)) #2*len(test_ds),","8a151806":"# Plot loss per iteration\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()","90b50dfb":"# Plot accuracy per iteration\nplt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.legend()","c5a8b51a":"# Save model\nmodel.save('Food_Vision_Model')","70523bcb":"## **Download The Dataset**","a22e05a1":"## **Model Training**","4e72d840":"## **Save Model**","f8dc3784":"## **Input Pipeline**:\n* Shuffle the filepaths\n* Repeat\n* Map\n* Take batches of data\n* Prefetch for efficient data loading","5a89f3f0":"## **Remove Noise\/Corrupt Data**","bd22b824":"## **Rename Image Directories within Train and Test directories**","aad1f619":"## **Mapping function that takes Image Path as an argument and returns Image Tensor + Label**","e31f5f65":"## **Model Architecture - Inspired From Inception-Resnet V2**"}}