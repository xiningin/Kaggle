{"cell_type":{"ee957ddb":"code","269ba76c":"code","e0b4d23f":"code","647f6fbb":"code","e10ffe39":"code","41c24025":"code","6a0b2e53":"code","579677ee":"code","aebdafcf":"code","d7687704":"code","4da80c2b":"code","57ade31a":"code","e2323560":"code","1b2a5f79":"code","1e746fa6":"code","98e9659a":"code","919c9f0c":"code","270760e6":"code","7ee20f2b":"code","b3c9666e":"code","101330c1":"code","79c91d3e":"code","fdb6891a":"code","17454006":"code","eda29394":"code","6f3598ab":"code","a0597256":"code","8c2cabe2":"code","984a4159":"code","c963f3d2":"code","b88e6ffe":"code","e39b3305":"code","1bc33794":"code","fe7d3b83":"code","03403d13":"code","c3fdc524":"code","033364fe":"code","edd9bc02":"code","15eea8a8":"code","785c6acb":"code","5cd5e0bf":"code","b7ee65d1":"code","a59d0a1c":"code","773cd64c":"code","6b78a982":"code","7d5bf7e8":"code","2a6a116e":"code","b4829df0":"code","1e353ee9":"code","24c67dd0":"code","972adad8":"code","f0c60080":"code","d628ab51":"code","60ef732e":"code","3b93ccf5":"code","8386b5cc":"code","e5d942fb":"code","31de5368":"code","2e46fde6":"code","5b623e97":"code","a096c7ed":"code","a4a29e09":"code","3df655bf":"markdown","bcb91ee9":"markdown","c9cff019":"markdown","8b0841ea":"markdown","f8951ca3":"markdown","857ae42a":"markdown","35a1c401":"markdown","4047928f":"markdown","7c139bae":"markdown","da32761c":"markdown","b8b7feaf":"markdown","854d2f7c":"markdown","9c196be8":"markdown","84242c74":"markdown","ecacb8af":"markdown","5dc4dbe2":"markdown","bbf5f8f7":"markdown","743e3a2d":"markdown","36c97f27":"markdown","a0d6704b":"markdown","ab854cae":"markdown","94272aff":"markdown","31677387":"markdown","2f28728b":"markdown","bb38eccd":"markdown","5d3f20c4":"markdown","91d11bd7":"markdown","b971e9fc":"markdown","56f4faa7":"markdown","abc0fbdf":"markdown","45182e43":"markdown","3c5c0c0d":"markdown","637ee2fd":"markdown","cc77a723":"markdown","7f56460c":"markdown","d2f61834":"markdown","4ba4693d":"markdown"},"source":{"ee957ddb":"# Main libraries\nimport os\nimport pandas as pd\nimport numpy as np","269ba76c":"# Visualization libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.express as px","e0b4d23f":"# Libraries for recommendation systems\nfrom collections import defaultdict\nfrom surprise import SVD\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split","647f6fbb":"!pip install translate\n!pip install google-trans-new","e10ffe39":"from google_trans_new import google_translator\nfrom translate import Translator","41c24025":"import warnings\nwarnings.filterwarnings('ignore')","6a0b2e53":"pd.set_option(\"display.max_rows\", 50, \"display.max_columns\", 50)\npd.set_option('display.max_colwidth', None)\nplt.style.use('bmh')\n# create contants\nRS=612","579677ee":"%%time\n#Loading Data files\nreview_1 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_1.csv', encoding='iso-8859-1')\nreview_2 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_2.csv', encoding='iso-8859-1')\nreview_3 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_3.csv', encoding='iso-8859-1')\nreview_4 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_4.csv', encoding='iso-8859-1')\nreview_5 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_5.csv', encoding='iso-8859-1')\nreview_6 = pd.read_csv('..\/input\/recommendation-system\/phone_user_review_file_6.csv', encoding='iso-8859-1')","aebdafcf":"review_1.head()","d7687704":"review_2.head()","4da80c2b":"print(f'review_1: Rows: {review_1.shape[0]} and Columns: {review_1.shape[1]}\\n')\nprint(f'review_2: Rows: {review_2.shape[0]} and Columns: {review_2.shape[1]}\\n')\nprint(f'review_3: Rows: {review_3.shape[0]} and Columns: {review_3.shape[1]}\\n')\nprint(f'review_4: Rows: {review_4.shape[0]} and Columns: {review_4.shape[1]}\\n')\nprint(f'review_5: Rows: {review_5.shape[0]} and Columns: {review_5.shape[1]}\\n')\nprint(f'review_6: Rows: {review_6.shape[0]} and Columns: {review_6.shape[1]}\\n')\nprint(f'Total rows: {review_1.shape[0]+review_2.shape[0]+review_3.shape[0]+review_4.shape[0]+review_5.shape[0]+review_6.shape[0]}')","57ade31a":"print('Check whether the column names are same in all the dataframes: ')\nall(np.unique(review_1.columns.tolist()) == np.unique(review_1.columns.tolist()+\n                                                      review_2.columns.tolist()+\n                                                      review_3.columns.tolist()+\n                                                      review_4.columns.tolist()+\n                                                      review_5.columns.tolist()+\n                                                      review_6.columns.tolist()))","e2323560":"#Merge the data into a single dataframe \nreviews = pd.concat([review_1,review_2,review_3,review_4,review_5,review_6], ignore_index=True)\ndel review_1, review_2, review_3, review_4, review_5, review_6\nprint(f'reviews: Rows: {reviews.shape[0]} and Columns: {reviews.shape[1]}\\n')\nprint('Top 5 rows of the data: ')\ndisplay(reviews.head())\nprint('Bottom 5 rows of the data: ')\ndisplay(reviews.tail())","1b2a5f79":"reviews.info()","1e746fa6":"df=reviews.isna().sum().round(2)\ndf1 = (df*100\/reviews.shape[0]).round(2)\nprint('Missing count and percentages for each column are: \\n',df.astype('str') +' ('+ df1.astype('str')+'%)')\n\n#fig = px.bar(x=df.index, y=df1,text = df.astype('str') +'('+ df1.astype('str')+'%)',\n#             title=\"Count (text) and Percentage(yaxis) of missing values in all the features (data: reviews)\")\n#fig.update_xaxes(title_text= 'Features')\n#fig.update_yaxes(title_text= 'Percentage of Missing values')\n#fig.show()\ndel df, df1","98e9659a":"print('Number of unique values in each feature: \\n',reviews.nunique())","919c9f0c":"# Top 10 Non-english users\nreviews[reviews['lang']!='en']['author'].value_counts(ascending=False)[:10]","270760e6":"# Top 10 Non-english products\nreviews[reviews['lang']!='en']['product'].value_counts(ascending=False)[:10]","7ee20f2b":"# print the frequency count of phone_url column\nreviews['phone_url'].value_counts(ascending=False).head(50)","b3c9666e":"# frequency count of 'product' column for 'samsung-galaxy-s-iii' type phone\nreviews[reviews[\"phone_url\"]=='\/cellphones\/samsung-galaxy-s-iii\/'][['product']].value_counts().head(15)","101330c1":"# frequency count of 'product' column for 'apple-iphone-5s' type phone\nreviews[reviews[\"phone_url\"]=='\/cellphones\/apple-iphone-5s\/'][['product']].value_counts().head(15)","79c91d3e":"# frequency count of 'product' column for 'samsung-galaxy-s6' type phone\nreviews[reviews[\"phone_url\"]=='\/cellphones\/samsung-galaxy-s6\/'][['product']].value_counts().head(15)","fdb6891a":"# frequency count of 'product' column for 'samsung-galaxy-s5' type phone\nreviews[reviews[\"phone_url\"]=='\/cellphones\/samsung-galaxy-s5\/'][['product']].value_counts().head(15)","17454006":"reviews['phone'] = reviews['phone_url'].str.split(\"\/\").apply(lambda col: col[2]).replace('-', ' ', regex=True)\nreviews['product'] = reviews['phone']\nreviews['phone'].unique()","eda29394":"reviews['product'].shape","6f3598ab":"product = reviews['product'].value_counts()[:10]\nprint('Distribution of number of ratings per item (Clipped at 10): \\n',product)\nsns.barplot(y=product.index,x=product)\nplt.tight_layout()\nplt.show()","a0597256":"users = reviews['author'].value_counts(dropna=False)[:10]\nprint('Distribution of number of ratings per user(Clipped at 10): \\n',users)\nusers.index = users.index.map(str)\nsns.barplot(y=users.index,x=users)\nplt.tight_layout()\nplt.show()","8c2cabe2":"unknowns = ['Anonymous','einer Kundin','einem Kunden', 'unknown','Anonymous ']\nreviews['author'].replace(to_replace = unknowns, \n                          value = 'Anonymous', \n                          inplace=True)","984a4159":"users = reviews['author'].value_counts(dropna=False)[:10]\nprint('Distribution of number of ratings per user(Clipped at 10): \\n',users)\nusers.index = users.index.map(str)\nsns.barplot(y=users.index,x=users)\nplt.tight_layout()\nplt.show()","c963f3d2":"# Let's check the score column of the data\nprint('Uniqe values in the \"score\" feature: \\n',reviews.score.unique())","b88e6ffe":"relevant_features=['author','product','score']\n# irrelvant_features=['phone_url','date','lang','country','source','domain','score_max','extract']","e39b3305":"print('Number of duplicate rows: ', reviews.duplicated().sum())","1bc33794":"orgnl_rows = reviews.shape[0]\norgnl_columns = reviews.shape[1]","fe7d3b83":"revs1 = reviews.copy()\n\n# Delete data which is not useful anymore, to save memory\ndel reviews\n\n# Step1: remove irrelevant features\nrevs1 = revs1.loc[:,relevant_features]\nprint(f'Step1: revs1 Shape after removing irrelevant features: Rows: {revs1.shape[0]} and Columns: {revs1.shape[1]}\\n')\n\n# Step2: Round-off score feature to nearest integer\nrevs1['score'] = revs1['score'].round(0).astype('Int64')\nprint('Step2: Round-off: Unique values in the \"score\" feature(after rounding-off): \\n',list(revs1.score.unique()))\n\n# Step3: Impute missing values in score feature with median\nrevs1['score'] = revs1['score'].fillna(revs1['score'].median())\nprint('\\nStep3: Imputation of \"score\"  with median and \"author\" with \"Anonymous\"')\n\n# Step4: remove samples with missing values in 'Product' and 'author' feature and also 'Anonymous' values\nrevs1.dropna(inplace=True)\nrevs1 = revs1[revs1[\"author\"] != 'Anonymous']\nprint(f'\\nStep4: revs1 Shape(after removing missing values): Rows: {revs1.shape[0]} and Columns: {revs1.shape[1]}\\n')\n\n# Step5: remove duplicates, if any\nrevs1 = revs1.drop_duplicates()\nprint(f'Step5: revs1 Shape(after removing duplicates): Rows: {revs1.shape[0]} and Columns: {revs1.shape[1]}\\n')\n\nprint(f'Overall {round(100 - revs1.shape[0]*100\/orgnl_rows,2)}% samples are dropped\\n')\nrevs1.head()","03403d13":"# separate 1 million data samples\nrevs_1m = revs1.sample(n=1000000, random_state=RS)\nprint(f'revs2 Shape: Rows: {revs_1m.shape[0]} and Columns: {revs_1m.shape[1]}\\n')","c3fdc524":"# 1. Most rated features\nprint('Most rated features\/products: \\n\\n',revs_1m['product'].value_counts().head())","033364fe":"# find out which rating is given highest number of times\nsns.countplot(data=revs_1m , x='score')\nplt.show()","edd9bc02":"revs1[revs1['score']==10]['author'].value_counts().head(10)","15eea8a8":"# 2. Users with highest number of reviews\nprint('Users with highest number of reviews: \\n\\n',revs_1m['author'].value_counts().head())","785c6acb":"# 3. Select data with products having >50 ratings and users who have given > 50 ratings\nauthor50 = revs1['author'].value_counts()\nauthor50 = author50[author50>50].index.tolist() # list of authors with > 50 ratings\nprint('Number of authors who have given >50 rating: ', len(author50))\n\nproduct50 = revs1['product'].value_counts()\nproduct50 = product50[product50>50].index.tolist() # list of products with > 50 ratings\nprint('Number of products with >50 rating: ', len(product50))\n\nrevs_50 = revs1[(revs1['author'].isin(author50)) & (revs1['product'].isin(product50))]\nprint(f'\\nrevs_50: Rows: {revs_50.shape[0]} and Columns: {revs_50.shape[1]}\\n')\ndel author50, product50\nrevs_50.head()","5cd5e0bf":"revs_50['author'].unique()[:100]","b7ee65d1":"revs_50['product'].unique()[:100]","a59d0a1c":"#target = 3000\ntop50_product = revs1['product'].value_counts()[0:50].rename('rating_count').to_frame()\ntop50_product['mean_ratings']=revs1[revs1['product'].isin(top50_product.index.tolist())].groupby(['product'])['score'].mean().astype('float64').round(1)\ntop50_product.sort_values(by='mean_ratings',inplace=True)\nprint('Number of products with >'+str(target)+' rating: ', len(top50_product))","773cd64c":"fig = px.scatter(top50_product, x=top50_product.index, y=\"mean_ratings\", size=\"rating_count\", size_max=60,\n                  height=800,title=\"Visualisation of mean ratings vs rating count for highest rated 50 phones\")\nfig.show()","6b78a982":"def popularity_rec(data):\n    ratings_mean_count = pd.DataFrame(data.groupby('product')['score'].mean())\n    ratings_mean_count['rating_counts'] = data.groupby('product')['score'].count()\n    ratings_mean_count = ratings_mean_count.sort_values(by=['score','rating_counts'], ascending=[False,False])\n    print('Top 5 recommendations for the products are: \\n')\n    display(ratings_mean_count.head())\n    return","7d5bf7e8":"# Using the data from the most popular phones amongst the most frequent users\npopularity_rec(revs_50)","2a6a116e":"# if we consider the original data (excluding 'Anonymous' users)\npopularity_rec(revs1)","b4829df0":"field_length = revs1.author.astype(str).map(len)\nprint (revs1.iloc[field_length.argmax(),0])","1e353ee9":"# Rearrange columns for SVD and prepare train and testsets\nrevs50_ = Dataset.load_from_df(revs_50[['author','product','score']], Reader(rating_scale=(1, 10)))\ntrainset, testset = train_test_split(revs50_, test_size=.25,random_state=RS)\n\nprint('top 3 values from trainset: \\n')\nfor key,value in {k: v for k, v in trainset.ur.items() if k <= 2}.items(): print(key,'-> ',value,'\\n')\nprint('\\ntop 3 values from testset: ', *testset[0:3], sep='\\n\\n')","24c67dd0":"# Objective: To get top_n recommendation for each user\ndef get_top_n(predictions, n=5):\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the n highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","972adad8":"%%time\n# fit and predict using svd\ndef svd_func(train, test):\n    svd = SVD(random_state=RS)\n    svd.fit(train)\n    svd_pred = svd.test(test)\n    return svd_pred, svd\n\nsvd_pred, svd = svd_func(trainset,testset)\nprint('First few prediction values: \\n',svd_pred[0:2])\nprint('\\nRMSE value(test-set): ',round(accuracy.rmse(svd_pred),2),'\\n') # compute RMSE\nsvd_rmse = round(accuracy.rmse(svd_pred),2)","f0c60080":"%%time\n# fit and predict using knn\ndef knn_item(train, test):\n    knn_i = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': False})\n    knn_i.fit(train)\n    knn_i_pred = knn_i.test(test)\n    return knn_i_pred, knn_i\n\nknn_i_pred, knn_i = knn_item(trainset, testset)\nprint('First few prediction values: \\n',knn_i_pred[0:2])\nprint('\\nRMSE value(Item-based Model, test-set): ',round(accuracy.rmse(knn_i_pred),2),'\\n') # compute RMSE\nknn_i_rmse = round(accuracy.rmse(knn_i_pred),2)","d628ab51":"%%time\n# fit and predict using knn\ndef knn_user(train, test):\n    knn_u = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': True})\n    knn_u.fit(train)\n    knn_u_pred = knn_u.test(test)\n    return knn_u_pred, knn_u\n\nknn_u_pred, knn_u = knn_user(trainset, testset)\nprint('First few prediction values: \\n',knn_u_pred[0:2])\nprint('\\nRMSE value(User-based Model, test-set): ',round(accuracy.rmse(knn_u_pred),2),'\\n') # compute RMSE\nknn_u_rmse = round(accuracy.rmse(knn_u_pred),2)","60ef732e":"# Comparison of RMSE scores from different collaorative algorithms\nsns.barplot(x=['svd_rmse','knn_i_rmse', 'knn_u_rmse'],y=[svd_rmse,knn_i_rmse, knn_u_rmse])","3b93ccf5":"svd_pred_df=pd.DataFrame(svd_pred, columns=['uid', 'iid', 'rui', 'est', 'details'])\nprint('average prediction for test users: ',svd_pred_df['est'].mean())\nprint('average rating  by test users: ',svd_pred_df['rui'].mean())\nprint('average prediction error for test users: ',(svd_pred_df['rui']-svd_pred_df['est']).abs().mean())","8386b5cc":"knn_i_pred_df=pd.DataFrame(knn_i_pred, columns=['uid', 'iid', 'rui', 'est', 'details'])\nprint('average prediction for test users: ',knn_i_pred_df['est'].mean())\nprint('average rating  by test users: ',knn_i_pred_df['rui'].mean())\nprint('average prediction error for test users: ',(knn_i_pred_df['rui']-knn_i_pred_df['est']).abs().mean())","e5d942fb":"knn_u_pred_df=pd.DataFrame(knn_u_pred, columns=['uid', 'iid', 'rui', 'est', 'details'])\nprint('average prediction for test users: ',knn_u_pred_df['est'].mean())\nprint('average rating  by test users: ',knn_u_pred_df['rui'].mean())\nprint('average prediction error for test users: ',(knn_u_pred_df['rui']-knn_u_pred_df['est']).abs().mean())","31de5368":"%%time\n#recommend top 5 products for test users\ntop_5 = get_top_n(knn_i_pred,5)\nprint('Top 5 recommendations for all test users are: \\n')\nfor key,value in top_5.items(): print(key,'-> ',value,'\\n') # to print all the recommendations for all the users\n#print('Top 5 recommendations for 3 users are: \\n')\n#for key,value in {k: v for k, v in top_5.items() if k in ['Amazon Customer','Cliente Amazon',\"Client d'Amazon\"]}.items(): print(key,'-> ',value,'\\n')","2e46fde6":"%%time\nsvd_cv = cross_validate(svd,revs50_, measures=['RMSE'], cv=5, verbose=False)\nprint('\\n Mean svd cv score:', round(svd_cv['test_rmse'].mean(),2),'\\n')\nsvd_cv","5b623e97":"%%time\nknn_i_cv = cross_validate(knn_i,revs50_, measures=['RMSE'], cv=5, verbose=False)\nprint('\\n Mean knn_i_cv score:', round(knn_i_cv['test_rmse'].mean(),2),'\\n')\nknn_i_cv","a096c7ed":"%%time\nknn_u_cv = cross_validate(knn_u,revs50_, measures=['RMSE'], cv=5, verbose=False)\nprint('\\n Mean knn_u_cv score:', round(knn_u_cv['test_rmse'].mean(),2),'\\n')\nknn_u_cv","a4a29e09":"# Comparison of RMSE scores(mean cv) from different collaorative algorithms\nsns.barplot(y=['svd_cv_rmse','knn_i_cv_rmse', 'knn_u_cv_rmse'],\n            x=[svd_cv['test_rmse'].mean(),knn_i_cv['test_rmse'].mean(), knn_u_cv['test_rmse'].mean()])","3df655bf":"**12: What other possible methods can you think of which can further improve the recommendation for different users ?**  <a id=\"q12\"><\/a>\n[Go to top](#toc)\n> Other from Popularity and Collaborative Filtering, hybrid recommendation methods like Content+Collaborative method, Demographic, Utility based, and Knowledge based recommendation system can also be used.","bcb91ee9":"[Go to top](#toc)","c9cff019":"### 1.5. Data split <a id=\"data_split\"><\/a>\n[Go to top](#toc)","8b0841ea":"## 4.3. Collaborative filtering model using kNNWithMeans_User based <a id=\"knnwithmeans_user_based\"><\/a>\n[Go to top](#toc)","f8951ca3":"## 2. Analysis <a id=\"analysis\"><\/a>\n[Go to top](#toc)","857ae42a":"### 1.4. Modification 1: revs1: Data cleaning, Imputation and rounding-off <a id=\"clean_impute_round-off\"><\/a>\n[Go to top](#toc)","35a1c401":"## 7. Summary (findings and Inferences) <a id=\"summary_inferences\"><\/a>\n[Go to top](#toc)\n\n1. Most popular phone (rated 10 by highest number of people):  \n        * Overall: verykool t742\n        * Amongst top users: samsung e1120       \n2. Overall data is highly skewed towards 'Amazon customers' from different countries. This may also be because 'Amazon' is the biggest trader for phones in the world. Although correct 'user' names from 'Amazon' should have used.\n3. Most of the authors have given the rating of '10' or '8'\n4. Both knn_i(item-based) and knn_u(user-based) have roughly similar RMSE","4047928f":"Following observations are made:  \n1. Most active user is 'Amazon customer'\n2. 'Anonymous' and 'unknown' users are those whose names are not known. Thus we can use this to impute blank values in 'author' column\n3. Many names are similar but in different languages like 'Amazon customer' and 'Cliente Amazon'. Let's search for these first and cleanup the differences due to language","7c139bae":"**10: In what business scenario you should use popularity based Recommendation Systems ?** <a id=\"q10\"><\/a>\n[Go to top](#toc)\n\n> Popularity based recommendation systems can be useful in multiple scenarios like:  \n> 1. When there is no data about the user and items.\n> 2. When it is required to show most popular items in different categories along with personalized results like: \n>     * Most popular punjabi songs or most popular english songs on a music website\/app\n>     * Most popular trend in cwestern wear or traditional wear\n>     * Most popular holiday packages for honeymoon trips, or bike trips or himalayan trips etc","da32761c":"* Except **score** and **score_max** (which are of float type) all other features are of object type \n* feature **date** should be of datetype\n* Also, **score, score_max, extract and author**: columns seems to have Null values","b8b7feaf":"# Description: \n* **DOMAIN:** Smartphone, Electronics  \n\n* **CONTEXT:** India is the second largest market globally for smartphones after China. About 134 million smartphones were sold across India in the year 2017 and is estimated to increase to about 442 million in 2022. India ranked second in the average time spent on mobile web by smartphone users across Asia Pacific. The combination of very high sales volumes and the average smartphone consumer behaviour has made India a very attractive market for foreign vendors. As per Consumer behaviour, 97% of consumers turn to a search engine when they are buying a product vs. 15% who turn to social media. If a seller succeeds to publish smartphones based on user\u2019s behaviour\/choice at the right place, there are 90% chances that user will enquire for the same. This Case Study is targeted to build a recommendation system based on individual consumer\u2019s behaviour or choice.\n\n* **DATA DESCRIPTION:**  \n\u2022 author : name of the person who gave the rating  \n\u2022 country : country the person who gave the rating belongs to  \n\u2022 data : date of the rating  \n\u2022 domain: website from which the rating was taken from  \n\u2022 extract: rating content  \n\u2022 language: language in which the rating was given  \n\u2022 product: name of the product\/mobile phone for which the rating was given  \n\u2022 score: average rating for the phone  \n\u2022 score_max: highest rating given for the phone  \n\u2022 source: source from where the rating was taken  \n\n* **PROJECT OBJECTIVE:** We will build a recommendation system using popularity based and collaborative filtering methods to recommend mobile phones to a user which are most popular and personalised respectively.","854d2f7c":"names like 'einer Kundin', 'einem Kunden','Anonymous' and 'unknown' can be interpreted in the same way i.e. an 'unknown customer'. Let's replace these names too","9c196be8":"### 1.3. Basic analysis <a id=\"basic_analysis\"><\/a>\n[Go to top](#toc)","84242c74":"Thus these three features have approx 4.5% missing values\n'score' and 'score_max' have exactly same number of missing values","ecacb8af":"## 4. Collaborative filtering based models <a id=\"collaborative_filtering\"><\/a>\n[Go to top](#toc)","5dc4dbe2":"Exra information is generally:  \n* phone memory: 8Gb\/16GB\/32GB etc\n* phone colour: Marble white, Blue, Red etc\n* carrier: AT&T, Verizon etc\n\nAnother observation is that these specifications are not present in all the product names, for eg: \nthere is no-way available to differentiate between the 2 products below:  \n'Samsung Galaxy S III Cellular Phone' and   \n'Samsung Galaxy S III SPH-L710 - 16GB - Marble White (Sprint) Smartphone'\n\nThus differentiating information is not same in all the product details. \nAlso, the goal is to recommend a phone not the carrier. and other specs like color etc are of low importance in recommendation. The only consistent differentiating information in all the product names is the 'phone manufacturer and model number', which can also be extracted from 'phone_url' column.\nLet's check for other phone names as well","bbf5f8f7":"i.e. score_max for all the phones is 10 throughout.","743e3a2d":"## 9. Results with cross_validation techniques <a id=\"cross_validation\"><\/a>\n[Go to top](#toc)","36c97f27":"## #. Import libraries <a id=\"import_libraries\"><\/a>\n[Go to top](#toc)","a0d6704b":"## 6. Average ratings for test users <a id=\"average_rating\"><\/a>\n[Go to top](#toc)","ab854cae":"**11: In what business scenario you should use CF based Recommendation Systems ?**  <a id=\"q11\"><\/a>\n[Go to top](#toc)\n> Collaborative filtering is useful in scenarios like: \n> 1. Giving personalised recommendation to the user, when user history or item data is available. Some examples can be:\n>     * Personalized movie recommendation of movie sites like Netflix, Amazon Prime, Youtube etc","94272aff":"## 3. Recommend top 5 mobile phones using popularity based model <a id=\"popularity_model\"><\/a>\n[Go to top](#toc)","31677387":"Thus, for cv scores too, knn_i is giving a better performance","2f28728b":"### 4.1. Collaborative filtering model using SVD <a id=\"svd\"><\/a>\n[Go to top](#toc)","bb38eccd":"### 1.2. Merge dataset <a id=\"merge_data\"><\/a>\n[Go to top](#toc)","5d3f20c4":"## 8. Recommend top 5 products for test users <a id=\"recommend_top_5\"><\/a>\n[Go to top](#toc)","91d11bd7":"As can be seen, same pattern is visible for the most comun types of phones. Thus it is better to use phone name and model number rather than other details mentioned in 'product' column","b971e9fc":"## Table of Contents <a id=\"toc\"><\/a>\n* [# Import Libraries](#import_libraries)  \n1. [Data Prepration and cleaning](#data_prepration)  \n    1.1. [Load the dataset](#load_data)  \n    1.2. [Merge dataset](#merge_data)  \n    1.3. [Basic Analysis](#basic_analysis)  \n    1.4. [Modification 1: revs1: Data cleaning, Imputation and rounding-off](#clean_impute_round-off)  \n    1.5. [Data split](#data_split)  \n2. [Analysis](#analysis)\n3. [Recommend top 5 mobile phones using popularity based model](#popularity_model)\n4. [Collaborative filtering based models](#collaborative_filtering)  \n    4.1 [SVD](#svd)  \n    4.2 [kNNWithMeans_Item based](#knnwithmeans_item_based)  \n    4.3 [kNNWithMeans_User based](#knnwithmeans_user_based)  \n5. [Show RMSE value and comparison](#rmse)  \n6. [Average ratings for test users](#average_rating)  \n7. [Summary (findings and Inferences)](#summary_inferences)\n8. [Recommend top 5 products for test users](#recommend_top_5)  \n9. [Results with cross_validation techniques](#cross_validation)  \n10. [In what business scenario you should use popularity based Recommendation Systems?](#q10)  \n11. [In what business scenario you should use CF based Recommendation Systems ?](#q11)\n12. [What other possible methods can you think of which can further improve the recommendation for different users ?](#q12)","56f4faa7":"## 5. Show RMSE value and comparison <a id=\"rmse\"><\/a>\n[Go to top](#toc)","abc0fbdf":"### 4.2. Collaborative filtering model using kNNWithMeans_Item based <a id=\"knnwithmeans_item_based\"><\/a>\n[Go to top](#toc)","45182e43":"Best RMSE score is given by knn (item based), so let's use it for further analyssi","3c5c0c0d":"Let's round it off to nearest integer","637ee2fd":"Thus, a multiple similar names, with different details exist in product list. For eg:  \n* Huawei P8lite zwart \/ 16 GB and  \n* Huawei P8 Lite Smartphone, Display 5\" IPS, Processore Octa-Core 1.5 GHz, Memoria Interna da 16 GB, 2 GB RAM, Fotocamera 13 MP, monoSIM, Android 5.0, Bianco [Italia]  \nare exactly same models","cc77a723":"## 1. Data preparation and basic cleaning <a id=\"data_prepration\"><\/a>\n[Go to top](#toc)","7f56460c":"[Go to top](#toc)","d2f61834":"### 1.1.  Load the dataset <a id=\"load_data\"><\/a>\n[Go to top](#toc)","4ba4693d":"Another observation is that 'phone_url' column also contains the phone name and model information. Let's check what extra information is present in 'product column'"}}