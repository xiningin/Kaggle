{"cell_type":{"fedf62c7":"code","081fb7e7":"code","8d8f9e74":"code","2af10b55":"code","cf51d8c9":"code","d879afbe":"code","b742eae0":"code","27b479a5":"code","0d71b042":"code","fce52476":"code","bcc5281d":"code","f3150f0f":"code","c7590f61":"code","67cd877c":"code","5bf5db55":"code","c5b97024":"code","2a283d52":"code","3566670c":"code","28518cd0":"code","7a7e779b":"code","0fda8a41":"code","3fa1b696":"code","fcf01a69":"code","957df5ec":"code","4ed6045b":"code","d6ae70a9":"code","cafb8a23":"code","db32e4a2":"code","36d20562":"code","4c0c7d51":"code","ba014915":"code","98234b19":"code","dd1f9ace":"code","c7df669e":"code","9b9c9f03":"code","ceecbdd4":"code","3842e686":"code","17e7cbea":"code","07310c10":"code","346972dc":"code","9ebe0957":"code","a0d3ac3a":"code","91b8290e":"code","2c7075eb":"code","98802ee7":"code","48cf0a2e":"code","35a39368":"code","a8defc14":"code","ee1cc107":"code","3719d9a4":"code","3db06fb0":"code","ba74b32a":"code","4730d2f5":"code","f4b5eca0":"code","aaea6fb4":"code","143c7a92":"code","87e6753e":"code","b6e16fd1":"code","0f05ca19":"code","f582c945":"code","5b1511dc":"code","cbed6052":"code","1a884ce1":"code","c7f61cf0":"markdown","047084a3":"markdown","0439fd74":"markdown","196982e3":"markdown","dd55bbbb":"markdown","7b1ea053":"markdown","d07e672c":"markdown","29065ded":"markdown","8015dd6e":"markdown","1b27e3fa":"markdown","4007bc0f":"markdown","f21ef1b7":"markdown","177d55c8":"markdown","9c536fdc":"markdown","64fe941d":"markdown","854b5e8b":"markdown","e71b48a7":"markdown","08a2bb31":"markdown","de634439":"markdown","63ec44fa":"markdown","075e3b6e":"markdown","63900f16":"markdown","f8bdbc33":"markdown","1627db96":"markdown","3fa127ca":"markdown"},"source":{"fedf62c7":"!pip install imutils\n","081fb7e7":"import numpy as np \nimport pandas as pd \nimport os\nfrom os import listdir\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport imutils    \n\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.layers import Conv2D,Input,ZeroPadding2D,BatchNormalization,Flatten,Activation,Dense,MaxPooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle #shuffling the data improves the model","8d8f9e74":"image_dir=\"..\/input\/brain-mri-images-for-brain-tumor-detection\/\"","2af10b55":"os.makedirs('..\/output\/kaggle\/working\/augmented-images', exist_ok = True)\nos.makedirs('..\/output\/kaggle\/working\/augmented-images\/yes', exist_ok = True)\nos.makedirs('..\/output\/kaggle\/working\/augmented-images\/no', exist_ok = True)\n\naugmented_data_path ='..\/output\/kaggle\/working\/augmented-images\/'\n\naugmented_yes =augmented_data_path+'yes'\naugmented_no = augmented_data_path+'no'\n\nIMG_SIZE = (224,224)","cf51d8c9":"def augment_data(file_dir, n_generated_samples, save_to_dir):\n    data_gen = ImageDataGenerator(rotation_range=10, \n                                  width_shift_range=0.1, \n                                  height_shift_range=0.1, \n                                  shear_range=0.1, \n                                  brightness_range=(0.3, 1.0),\n                                  horizontal_flip=True, \n                                  vertical_flip=True, \n                                  fill_mode='nearest'\n                                 )\n\n    for filename in listdir(file_dir):\n        image = cv2.imread(file_dir + '\/' + filename)\n        # reshape the image\n        image = image.reshape((1,)+image.shape)\n        save_prefix = 'aug_' + filename[:-4]\n        i=0\n        for batch in data_gen.flow(x=image, batch_size=1, save_to_dir=save_to_dir,save_prefix=save_prefix, save_format='jpg'):\n                i += 1\n                if i > n_generated_samples:\n                    break","d879afbe":"dir = os.listdir('..\/output\/kaggle\/working\/augmented-images\/yes') \n\nif len(dir) == 0: \n\n    # augment data for the examples with label equal to 'yes' representing tumurous examples\n    augment_data(file_dir=image_dir+'yes',n_generated_samples=6, save_to_dir=augmented_data_path+'yes')\n    # augment data for the examples with label equal to 'no' representing non-tumurous examples\n    augment_data(file_dir=image_dir+'no', n_generated_samples=9, save_to_dir=augmented_data_path+'no')\nelse:\n    print(len(dir))","b742eae0":"def load_data(dir_list):\n\n    # load all images in a directory\n    X = []\n    y = []\n#     image_width, image_height = image_size\n    \n    for directory in dir_list:\n        for filename in listdir(directory):\n            image = cv2.imread(directory+'\/'+filename)\n#             image = crop_brain_contour(image, plot=False)\n#             image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n#             # normalize values\n#             image = image \/ 255.\n#             # convert image to numpy array and append it to X\n            X.append(image)\n            # append a value of 1 to the target array if the image\n            # is in the folder named 'yes', otherwise append 0.\n            if directory[-3:] == 'yes':\n                y.append([1])\n            else:\n                y.append([0])\n                \n    X = np.array(X)\n    y = np.array(y)\n    \n    # Shuffle the data\n    X, y = shuffle(X, y)\n    \n    print(f'Number of examples is: {len(X)}')\n    print(f'X shape is: {X.shape}')\n    print(f'y shape is: {y.shape}')\n    \n    return X, y","27b479a5":"def split_data(X, y, test_size=0.2):\n       \n    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","0d71b042":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","fce52476":"\nX_train, y_train = load_data([augmented_yes, augmented_no])","bcc5281d":"plot_samples(X_train, y_train, ['yes','no'], 20)","f3150f0f":"RATIO_LIST = []\nfor set in (X_train):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","c7590f61":"del RATIO_LIST","67cd877c":"def crop_brain_contour(image, plot=False):\n    \n    # Convert the image to grayscale, and blur it slightly\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n\n    # Find contours in thresholded image, then grab the largest one\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    c = max(cnts, key=cv2.contourArea)\n    # extreme points\n    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n    extRight = tuple(c[c[:, :, 0].argmax()][0])\n    extTop = tuple(c[c[:, :, 1].argmin()][0])\n    extBot = tuple(c[c[:, :, 1].argmax()][0])\n    \n    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]            \n\n    if plot:\n        plt.figure()\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.tick_params(axis='both', which='both', top=False, bottom=False, left=False, right=False,labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n        plt.title('Original Image')\n        plt.subplot(1, 2, 2)\n        plt.imshow(new_image)\n        plt.tick_params(axis='both', which='both',top=False, bottom=False, left=False, right=False,labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n        plt.title('Cropped Image')\n        plt.show()\n    \n    return new_image","5bf5db55":"img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/yes\/Y108.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","c5b97024":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","2a283d52":"def Croping_Data(train):\n\n    # load all images in a directory\n    X = []\n    y = []\n    \n    for img in train:\n        image = crop_brain_contour(img, plot=False)\n        X.append(image)\n                \n    X = np.array(X)\n    \n    return X","3566670c":"X = Croping_Data(X_train)","28518cd0":"plot_samples(X, y_train, ['yes','no'], 20)","7a7e779b":"def Resize_Data(train):\n\n    # load all images in a directory\n    X = []\n    y = []\n    \n    IMG_WIDTH, IMG_HEIGHT = (240, 240)\n    \n    for img in train:\n        image = cv2.resize(img, dsize=(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)\n        # normalize values\n        image = image \/ 255.\n        # convert image to numpy array and append it to X\n        X.append(image)\n                \n    X = np.array(X)\n    \n    return X","0fda8a41":"augmented_yes =augmented_data_path+'yes'\naugmented_no = augmented_data_path+'no'\n\nIMG_WIDTH, IMG_HEIGHT = (240, 240)\n\nX = Resize_Data(X)\ny = y_train\n\n","3fa1b696":"plot_samples(X, y_train, ['yes','no'],20)","fcf01a69":"img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/no\/1 no.jpeg')\nimg_arr = (np.round(np.array(img)*255)).astype(np.uint8)\n\n# flatten\nimg_arr = img_arr.flatten()\n\nyes_img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/yes\/Y1.jpg')\nyes_img_arr = (np.round(np.array(yes_img)*255)).astype(np.uint8)\n\n# flatten\nyes_img_arr = yes_img_arr.flatten()","957df5ec":"# plot histogram\nplt.hist(img_arr, bins = 256, range = [0,256])\nplt.title(\"Number of pixels in each intensity value for no\")\nplt.xlabel(\"Intensity\")\nplt.ylabel(\"Number of pixels\")\nplt.show()\n\nplt.imshow(img)\nplt.show()","4ed6045b":"plt.hist(yes_img_arr, bins = 256, range = [0,256])\nplt.title(\"Number of pixels in each intensity value fo yes\")\nplt.xlabel(\"Intensity\")\nplt.ylabel(\"Number of pixels\")\nplt.show()\n\nplt.imshow(yes_img)\nplt.show()","d6ae70a9":"del img \ndel img_arr \n\n\ndel yes_img \ndel yes_img_arr \n","cafb8a23":"X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y, test_size=0.05)","db32e4a2":"del X \ndel y","36d20562":"print (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of validation examples = \" + str(X_val.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))","4c0c7d51":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\ny = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","ba014915":"def build_model(input_shape):\n    X_input = Input(input_shape) \n    X = ZeroPadding2D((2, 2))(X_input) \n    \n    X = Conv2D(32, (7, 7), strides = (1, 1))(X)\n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X) \n    \n    X = MaxPooling2D((4, 4))(X) \n    X = MaxPooling2D((4, 4))(X) \n    X = Flatten()(X) \n    X = Dense(1, activation='sigmoid')(X) \n    model = Model(inputs = X_input, outputs = X)\n    \n    return model","98234b19":"IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\nmodel=build_model(IMG_SHAPE)\nmodel.summary()","dd1f9ace":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x=X_train, y=y_train, batch_size=32, epochs=22, validation_data=(X_val, y_val)) #22","c7df669e":"history = model.history.history","9b9c9f03":"def plot_metrics(history):\n    \n    train_loss = history['loss']\n    val_loss = history['val_loss']\n    train_acc = history['accuracy']\n    val_acc = history['val_accuracy']\n    \n    # Loss\n    plt.figure()\n    plt.plot(train_loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.title('Loss')\n    plt.legend()\n    plt.show()\n    \n    # Accuracy\n    plt.figure()\n    plt.plot(train_acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n    plt.show()","ceecbdd4":"plot_metrics(history)","3842e686":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","17e7cbea":"import itertools\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nlabels = ['yes','no']\n# validate on val set\npredictions = model.predict(X_val)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)","07310c10":"# validate on val set\npredictions = model.predict(X_test)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)","346972dc":"for i in range(5):\n    plt.figure()\n    plt.imshow(X_test[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(f'Actual class: {y_test[i]}\\nPredicted class: {predictions[i]}')\n    plt.show()","9ebe0957":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x=X_train, y=y_train, batch_size=32, epochs=44, validation_data=(X_val, y_val)) #44","a0d3ac3a":"history = model.history.history\n\nplot_metrics(history)","91b8290e":"# validate on val set\npredictions = model.predict(X_val)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)\n","2c7075eb":"# validate on val set\npredictions = model.predict(X_test)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)\n","98802ee7":"for i in range(5):\n    plt.figure()\n    plt.imshow(X_test[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(f'Actual class: {y_test[i]}\\nPredicted class: {predictions[i]}')\n    plt.show()","48cf0a2e":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping","35a39368":"# load base model\nvgg16_weight_path = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg16_model = VGG16(\n    weights=vgg16_weight_path,\n    include_top=False, \n    input_shape=IMG_SHAPE\n)","a8defc14":"NUM_CLASSES = 1\n\nmodel = Sequential()\nmodel.add(vgg16_model)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nmodel.layers[0].trainable = False\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(lr=1e-4),\n    metrics=['accuracy']\n)\n\nmodel.summary()","ee1cc107":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x=X_train, y=y_train, batch_size=32, epochs=22, validation_data=(X_val, y_val)) #22","3719d9a4":"history = model.history.history","3db06fb0":"plot_metrics(history)","ba74b32a":"# validate on val set\npredictions = model.predict(X_val)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)\n","4730d2f5":"# validate on val set\npredictions = model.predict(X_test)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)\n","f4b5eca0":"\nfor i in range(5):\n    plt.figure()\n    plt.imshow(X_test[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(f'Actual class: {y_test[i]}\\nPredicted class: {predictions[i]}')\n    plt.show()","aaea6fb4":"from keras.preprocessing import image\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.applications.vgg19 import  decode_predictions, preprocess_input\nimport matplotlib.image as mpimg\nfrom keras import backend as K","143c7a92":"img_path = '..\/input\/brain-mri-images-for-brain-tumor-detection\/yes\/Y100.JPG'\norg_img = cv2.imread(img_path)\nplt.imshow(org_img)\nplt.show()","87e6753e":"img = image.load_img(img_path, target_size=(224, 224))\nplt.imshow(img)\nplt.show()","b6e16fd1":"x = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx.shape","0f05ca19":"x = preprocess_input(x)\nmodel = VGG16(weights='imagenet')\npreds = model.predict(x)\ncam_predictions = pd.DataFrame(decode_predictions(preds, top=3)[0],columns=['col1','category','probability']).iloc[:,1:]","f582c945":"argmax = np.argmax(preds[0])\noutput = model.output[:, argmax]\nmodel.summary()","5b1511dc":"last_conv_layer = model.get_layer('block5_conv3')\ngrads = K.gradients(output, last_conv_layer.output)[0]\npooled_grads = K.mean(grads, axis=(0, 1, 2))\niterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\npooled_grads_value, conv_layer_output_value = iterate([x])\nfor i in range(512):\n    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]","cbed6052":"heatmap = np.mean(conv_layer_output_value, axis=-1)\nheatmap = np.maximum(heatmap, 0)\nheatmap \/= np.max(heatmap)\nplt.matshow(heatmap)\nplt.show()","1a884ce1":"import cv2\nimport matplotlib.image as mpimg\n\nimg = cv2.imread(img_path)\n\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nhif = .8\nsuperimposed_img = heatmap * hif + img\n\n\n\noutput = 'output.jpeg'\ncv2.imwrite(output, superimposed_img)\nimg=mpimg.imread(output)\n\nplt.imshow(img)\nplt.axis('off')\nplt.title(predictions[0])","c7f61cf0":"Intensity of whitness is very hight which refelect that he have brain tumor","047084a3":"## <a id='build'>4. CNN Modeling<\/a>","0439fd74":"### Adding K-fold Cross Validation","196982e3":"### <a id='build'>4.1 Model building<\/a>","dd55bbbb":"## <a id='build'>4.2 Pretrained Model - VGG 16<\/a>","7b1ea053":"# <a id='concl'>7. References<\/a>\n\n* https:\/\/www.kaggle.com\/ethernext\/brain-tumour-detection-with-cnn-96-accuracy\n* https:\/\/www.kaggle.com\/keras\/vgg19\/home?select=vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n* https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl\n* https:\/\/www.kaggle.com\/loaiabdalslam\/brain-tumor-detection-cnn\n* https:\/\/www.kaggle.com\/ruslankl\/brain-tumor-detection-v1-0-cnn-vgg-16\n* https:\/\/github.com\/nickbiso\/Keras-Class-Activation-Map\/blob\/master\/Class%20Activation%20Map(CAM).ipynb\n* https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/","d07e672c":"VGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:\n\nThis network contains total 16 layers in which weights and bias parameters are learnt.\nA total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.\nThe number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).\nThe informative features are obtained by max pooling layers applied at different steps in the architecture.\nThe dense layers comprises of 4096, 4096, and 1000 nodes each.\nThe cons of this architecture are that it is slow to train and produces the model with very large size.\nThe VGG16 architecture is given below:\n![](https:\/\/tech.showmax.com\/2017\/10\/convnet-architectures\/image_0-8fa3b810.png)","29065ded":"## Preprocessing the data\n\nIn order to crop the specific part of the image containing tumour,cropping technique via OpenCv is used, the details can be found here.[How to find extreme points in OpenCv?](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/)","8015dd6e":"# <a id='concl'>6. Conclusions<\/a>\n\nThis project was a combination of CNN model classification problem (to predict wheter the subject has brain tumor or not) & Computer Vision problem (to automate the process of brain cropping from MRI scans). The final accuracy is **about 99%.**","1b27e3fa":"## Visualization of data","4007bc0f":"## Augmentation of images \n\n**Data augmentation** is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. \n\nData augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nAbout the data:\nThe dataset contains 2 folders: yes and no which contains **253 Brain MRI Images**. The folder yes contains 155 Brain MRI Images that are tumorous andno contains 98 Brain MRI Images that are non-tumorous. After applying augmentation we will have **2065 samples**\n\n![](https:\/\/i.ibb.co\/tpBWTX2\/Screen-Shot-2020-08-01-at-9-11-51-PM.png)\n\n","f21ef1b7":"**Too many epochs might cause overfitting!**\n\nFollowing is an intuitive explanation.\n\nAny data = Behavior Pattern + Noise\n\nOur objective is to find the pattern using Deep Learning model in the form of weights and biases for various nodes.\n\nThe objective of the DL model is to minimum error or maximum accuracy. In the process of minimizing error, it would have learnt the pattern but the error would not reach a absolute zero. If the training is continued for more epochs, model tries to reach zero error where **it is starting to learn the noise of training data.**\n\nThis noise can change from data sample to sample and hence you would observe the following curves when number of epochs is plotted against accuracy \/ error for training vs validation sets.\n\nYes. Accuracy would drop marginally but our objective was NOT to be good only on training data. Hence generalization \/ regularization is required for the model. Early Stopping is one technique that can help.\n\n![](https:\/\/i.stack.imgur.com\/jKgDa.png)","177d55c8":"Intensity of whitness is low which refelect that he don't have tumor.","9c536fdc":"# <a id='env'>2. Setting up the Environment<\/a>","64fe941d":"# <a id='concl'>5. CAM<\/a>\n\nWe propose a technique for generating class activation maps using the global average pooling (GAP) in CNNs. A class activation map for a particular category indicates the discriminative image regions used by the CNN to identify that category. The procedure for generating these maps is illustrated as follows:\n![](http:\/\/cnnlocalization.csail.mit.edu\/framework.jpg)\n\nClass activation maps could be used to intepret the prediction decision made by the CNN. The left image below shows the class activation map of top 5 predictions respectively, you can see that the CNN is triggered by different semantic regions of the image for different predictions. The right image below shows the CNN learns to localize the common visual patterns for the same object class.\n\n![](http:\/\/cnnlocalization.csail.mit.edu\/example.jpg)\n\n*Source: from MIT: http:\/\/cnnlocalization.csail.mit.edu*","854b5e8b":"As you can see, images have different width and height and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is (224,224) some wide images may look weird after resizing. Histogram of ratio distributions (ratio = width\/height):","e71b48a7":"# <a id='import'>3. Data Import and Preprocessing<\/a>","08a2bb31":"**<center><font size=5>Brain Tumor Detection<\/font><\/center>**\n***\n**Authors**: Hussien Elgabry, Moustafa Bahnasawy and Youssef Aziz\n\n**date**: 8th August, 2020\n\n**Table of Contents**\n- <a href='#intro'>1. Project Overview and Objectives<\/a> \n    - <a href='#dataset'>1.1. Data Set Description<\/a>\n    - <a href='#tumor'>1.2. What is Brain Tumor?<\/a>\n- <a href='#env'>2. Setting up the Environment<\/a>\n- <a href='#import'>3. Data Import and Preprocessing<\/a>\n- <a href='#cnn'>4. CNN Model<\/a>\n    - <a href='#aug'>4.1. Model Building<\/a>\n    - <a href='#build'>4.2. Pretrained Model - VGG 16<\/a>\n- <a href='#cnn'>5. CAM<\/a>\n- <a href='#concl'>6. Conclusions<\/a>\n- <a href='#concl'>7. References<\/a>","de634439":"### Making directory for augmented images","63ec44fa":"## After applying the cropping function","075e3b6e":"### Plotting of aaccuracy","63900f16":" Right now all images are in one folder with yes and no subfolders. I will split the data into train, val and test folders which makes its easier to work for me. The new folder heirarchy will look as follows:","f8bdbc33":"**A directory is formed using os.makedirs() function for augmented images(yes\/ no). Note- custom directory is obtained in outputs folder.**","1627db96":"# <a id='intro'>1. Project Overview and Objectives<\/a>\n\nThe main purpose of this project was to build a CNN model that would classify if subject has a tumor or not based on MRI scan.We used some models architecture and weights to train the model for this binary problem. We used `accuracy` as a metric to justify the model performance which can be defined as:\n\n$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n\n<br>\n\n- **validation set** - is the set used during the model training to adjust the hyperparameters.\n- **test set** - is the small set that We don't touch for the whole training process at all. It's been used for final model performance evaluation.\n\n\n## <a id='dataset'>1.1. Data Set Description<\/a>\n\nThe image data that was used for this problem is [Brain MRI Images for Brain Tumor Detection](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection). It conists of MRI scans of two classes:\n\n* `NO` - no tumor, encoded as `0`\n* `YES` - tumor, encoded as `1`\n\n## <a id='tumor'>1.2. What is Brain Tumor?<\/a>\n\n> A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: cancerous (malignant) tumors and benign tumors. Cancerous tumors can be divided into primary tumors, which start within the brain, and secondary tumors, which have spread from elsewhere, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the part of the brain involved. These symptoms may include headaches, seizures, problems with vision, vomiting and mental changes. The headache is classically worse in the morning and goes away with vomiting. Other symptoms may include difficulty walking, speaking or with sensations. As the disease progresses, unconsciousness may occur.\n>\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5f\/Hirnmetastase_MRT-T1_KM.jpg)\n>\n> *Brain metastasis in the right cerebral hemisphere from lung cancer, shown on magnetic resonance imaging.*\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Brain_tumor)","3fa127ca":"**Will be discussed seperatelly**"}}