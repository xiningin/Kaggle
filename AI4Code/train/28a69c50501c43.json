{"cell_type":{"5d440340":"code","d21115aa":"code","bbcd7807":"code","887630ee":"code","b921753e":"code","4e511c72":"code","f5ca5cd5":"code","52f58ada":"code","83cdc74e":"code","4141e75a":"code","dbbe810a":"code","1bb2fd81":"code","6d613f36":"code","2a710803":"code","ce9af481":"code","01058afc":"code","1e16ae9c":"code","53c2be0f":"code","181f42a2":"code","487bd3ec":"code","a2b95aca":"code","64ba4aea":"code","77403f99":"code","60268949":"code","488c9bf4":"code","98b68c42":"code","17426541":"code","045fb268":"code","2dc2f436":"code","41c10488":"code","254f0e71":"code","70a91147":"code","dc13dd6d":"markdown","384d404d":"markdown","31bc119a":"markdown","287df339":"markdown","ab102d04":"markdown","0fe5aed8":"markdown","ef905b59":"markdown"},"source":{"5d440340":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","d21115aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bbcd7807":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nstructures = pd.read_csv('..\/input\/structures.csv')\nscalar_coupling_contributions = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')\n\nprint('Train dataset shape is -> rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\nprint('Test dataset shape is  -> rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\nprint('Sub dataset shape is  -> rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\nprint('Structures dataset shape is  -> rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\nprint('Scalar_coupling_contributions dataset shape is  -> rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\n                                                                                   scalar_coupling_contributions.shape[1]))","887630ee":"train = pd.merge(train, scalar_coupling_contributions, how = 'left',\n                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])","b921753e":"print('The training set has shape {}'.format(train.shape))\nprint('The test set has shape {}'.format(test.shape))","4e511c72":"# Distribution of the target\ntrain['scalar_coupling_constant'].plot(kind='hist', figsize=(20, 5), bins=1000, title='Distribution of the target scalar coupling constant')\nplt.show()","f5ca5cd5":"# Number of of atoms in molecule\nfig, ax = plt.subplots(1, 2)\ntrain.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Train Set)',\n                                                                      ax=ax[0])\ntest.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Test Set)',\n                                                                     ax=ax[1])\nplt.show()","52f58ada":"## Additional Data\nmst = pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\nmul = pd.read_csv('..\/input\/mulliken_charges.csv')\npote = pd.read_csv('..\/input\/potential_energy.csv')\nscc = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')","83cdc74e":"mst.head(3)","4141e75a":"mul.head(3)","dbbe810a":"# Plot the distribution of mulliken_charges\nmul['mulliken_charge'].plot(kind='hist', figsize=(15, 5), bins=500, title='Distribution of Mulliken Charges')\nplt.show()","1bb2fd81":"# Plot the distribution of potential_energy\npote['potential_energy'].plot(kind='hist',\n                              figsize=(15, 5),\n                              bins=500,\n                              title='Distribution of Potential Energy',\n                              color='b')\nplt.show()","6d613f36":"scalar_coupling_contributions.head()","2a710803":"scc.groupby('type').count()['molecule_name'].sort_values().plot(kind='barh',\n                                                                color='red',\n                                                               figsize=(15, 5),\n                                                               title='Count of Coupling Type in Train Set')\nplt.show()","ce9af481":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\nscc['fc'].plot(kind='hist', ax=ax.flat[0], bins=500, title='Fermi Contact contribution', color=\"red\")\nscc['sd'].plot(kind='hist', ax=ax.flat[1], bins=500, title='Spin-dipolar contribution', color=\"blue\")\nscc['pso'].plot(kind='hist', ax=ax.flat[2], bins=500, title='Paramagnetic spin-orbit contribution', color=\"green\")\nscc['dso'].plot(kind='hist', ax=ax.flat[3], bins=500, title='Diamagnetic spin-orbit contribution', color=\"orange\")\nplt.show()","01058afc":"import seaborn as sns\nsns.pairplot(data=train.sample(5000), hue='type', vars=['fc','sd','pso','dso','scalar_coupling_constant'])\nplt.show()","1e16ae9c":"## Target vs atom count\natom_count_dict = structures.groupby('molecule_name').count()['atom_index'].to_dict()","53c2be0f":"train['atom_count'] = train['molecule_name'].map(atom_count_dict)\ntest['atom_count'] = test['molecule_name'].map(atom_count_dict)","181f42a2":"# Map the atom structure data into train and test files\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)","487bd3ec":"# https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark\ntrain_p_0 = train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)","a2b95aca":"# make categorical variables\natom_map = {'H': 0,\n            'C': 1,\n            'N': 2}\ntrain['atom_0_cat'] = train['atom_0'].map(atom_map).astype('int')\ntrain['atom_1_cat'] = train['atom_1'].map(atom_map).astype('int')\ntest['atom_0_cat'] = test['atom_0'].map(atom_map).astype('int')\ntest['atom_1_cat'] = test['atom_1'].map(atom_map).astype('int')","64ba4aea":"# One Hot Encode the Type\ntrain = pd.concat([train, pd.get_dummies(train['type'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['type'])], axis=1)","77403f99":"train.head(5)","60268949":"train['dist_to_type_mean'] = train['dist'] \/ train.groupby('type')['dist'].transform('mean')\ntest['dist_to_type_mean'] = test['dist'] \/ test.groupby('type')['dist'].transform('mean')","488c9bf4":"# Configurables\nFEATURES = ['atom_index_0', 'atom_index_1',\n            'atom_0_cat',\n            'x_0', 'y_0', 'z_0',\n            'atom_1_cat', \n            'x_1', 'y_1', 'z_1', 'dist', 'dist_to_type_mean',\n            'atom_count',\n            '1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN'\n           ]\nTARGET = 'scalar_coupling_constant'\nCAT_FEATS = ['atom_0','atom_1']\nN_ESTIMATORS = 2000\nVERBOSE = 500\nEARLY_STOPPING_ROUNDS = 200\nRANDOM_STATE = 529\n\nX = train[FEATURES]\nX_test = test[FEATURES]\ny = train[TARGET]","98b68c42":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\n\nlgb_params = {'num_leaves': 128,\n              'min_child_samples': 64,\n              'objective': 'regression',\n              'max_depth': 6,\n              'learning_rate': 0.1,\n              \"boosting_type\": \"gbdt\",\n              \"subsample_freq\": 1,\n              \"subsample\": 0.9,\n              \"bagging_seed\": 11,\n              \"metric\": 'mae',\n              \"verbosity\": -1,\n              'reg_alpha': 0.1,\n              'reg_lambda': 0.4,\n              'colsample_bytree': 1.0\n         }\nRUN_LGB = True\nif RUN_LGB:\n    n_fold = 5\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=RANDOM_STATE)\n\n    # Setup arrays for storing results\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n\n    # Train the model\n    for fold_n, (train_idx, valid_idx) in enumerate(folds.split(X)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        model = lgb.LGBMRegressor(**lgb_params, n_estimators = N_ESTIMATORS, n_jobs = -1)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric='mae',\n                  verbose=VERBOSE,\n                  early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\n        y_pred_valid = model.predict(X_valid)\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        # feature importance\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = FEATURES\n        fold_importance[\"importance\"] = model.feature_importances_\n        fold_importance[\"fold\"] = fold_n + 1\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n        prediction \/= folds.n_splits\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n        oof[valid_idx] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        prediction += y_pred","17426541":"if RUN_LGB:\n    # Plot feature importance as done in https:\/\/www.kaggle.com\/artgor\/artgor-utils\n    feature_importance[\"importance\"] \/= folds.n_splits\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n    plt.figure(figsize=(15, 20));\n    ax = sns.barplot(x=\"importance\",\n                y=\"feature\",\n                hue='fold',\n                data=best_features.sort_values(by=\"importance\", ascending=False));\n    plt.title('LGB Features (avg over folds)');","045fb268":"prediction_final = model.predict(X_test)","2dc2f436":"prediction_final","41c10488":"test.shape","254f0e71":"prediction_final.shape","70a91147":"sub[\"scalar_coupling_constant\"] = prediction_final\nsub.to_csv('submission.csv', index=False)\nsub.head()","dc13dd6d":"# LIGHTGBM -5 Fold Cross validation","384d404d":"## Libraries used","31bc119a":"## Data Exploration","287df339":"## Reading Data","ab102d04":"## Data merge","0fe5aed8":"## Feature Creation","ef905b59":"## Baseline models"}}