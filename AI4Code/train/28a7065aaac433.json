{"cell_type":{"a47778f6":"code","6cfa4e93":"code","2e775510":"code","32fd6af2":"code","f3516953":"code","98562401":"code","aaa78aa2":"code","37712888":"code","08d080ca":"code","b538395d":"code","e65676a1":"code","04b49abe":"code","db595628":"code","600159bb":"code","c1241afa":"code","8cf0d783":"code","af0ff9f8":"code","0e7399cc":"code","c08a817e":"code","4920b279":"code","90af0a9f":"code","a09739cb":"code","9deb66f0":"code","edb2feca":"code","22f36d75":"code","7cc46393":"code","2f346f10":"code","7bfb4fbf":"code","9a096c57":"code","39b4fd47":"code","ca2b51e8":"code","752c9047":"code","e5e3947f":"code","0c3696f2":"code","46c40e9e":"code","c3e7eb0e":"code","95f07e75":"code","fbe4c63c":"code","3556a035":"code","0b2143e0":"code","cd6ad26e":"code","ce64fc35":"code","fd92d41f":"code","2ebc0b63":"code","65890ed3":"code","e74c9eeb":"code","28101255":"code","58c89ea2":"code","1a402dae":"code","17c08d56":"code","c1c3486e":"code","30dd9b6c":"code","9c639134":"code","7713a5c8":"code","8fb557e4":"code","59894b19":"code","fa888549":"code","a7f684b2":"code","51e3db03":"code","626c8190":"code","d1f5b0f4":"code","778da968":"code","2ed8ad09":"code","fa828a3d":"code","4238f7b6":"code","c7b57a10":"code","4f19336d":"code","17c20fc0":"code","1b45ed20":"code","77d01a1d":"code","7b593d71":"code","6f6a3207":"code","d74d6f05":"code","8ebba860":"code","b4c6af20":"code","df993e35":"code","b51935f8":"code","c0bed527":"code","42f2b5aa":"code","cd263ad3":"code","15bf50c7":"code","6cc9fc3c":"code","96b5b4f7":"code","c4d29a26":"code","4ed2a18e":"code","8c2e8f3e":"code","ae45837c":"code","9597a737":"code","ee6160ab":"code","c0f00801":"code","3039f68a":"code","dcd45a3f":"code","cbe7216e":"code","82d52847":"code","09070cd7":"code","89028803":"code","5048b0d2":"code","0c598f27":"markdown","4959c21e":"markdown","6efcc4ec":"markdown","520c7128":"markdown","2a11566a":"markdown","c115b9fd":"markdown","cb7bb32c":"markdown","f815afe9":"markdown","efed2c95":"markdown","d0a53a62":"markdown","f0aee9b8":"markdown","4c5fd403":"markdown","c9589c8c":"markdown","8db8da97":"markdown","3453aff7":"markdown","0e1502de":"markdown","912abf8c":"markdown","e6022343":"markdown","9679cde8":"markdown","068072c5":"markdown","a999146a":"markdown","7d9bb532":"markdown","c1650a10":"markdown","c7a0a574":"markdown","f7e94448":"markdown","6f9e1d8a":"markdown","6e810675":"markdown","87a1d3dd":"markdown","27d1c7d2":"markdown","166a0f8f":"markdown","4839c530":"markdown","ac4af353":"markdown","cbf95969":"markdown","5242ca7a":"markdown","2bb0cfbc":"markdown","8950ba47":"markdown","3db3ee35":"markdown","39e86519":"markdown","a6f9058d":"markdown","d846b3a1":"markdown"},"source":{"a47778f6":"# !pip3 install graphviz\n# !pip3 install dask\n# !pip install \"dask[complete]\" \n# !pip3 install toolz\n# !pip3 install cloudpickle\n# !pip3 install folium\n# !pip install gpxpy\n!pip3 install pyroomacoustics","6cfa4e93":"import os\nimport time\nimport math\nimport scipy\nimport pickle\nimport warnings\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pylab as plt\n\n# This library helps to open street map\nimport folium \n\n# Similar to pandas, but helps in parallel computing\n# Below are some resources for getting familiar with Dask\n# https:\/\/www.youtube.com\/watch?v=ieW3G7ZzRZ0\n# https:\/\/github.com\/dask\/dask-tutorial\n# https:\/\/github.com\/dask\/dask-tutorial\/blob\/master\/07_dataframe.ipynb\nimport dask.dataframe as dd\n\n# This library is used while we calculate the straight line distance between two (lat, lon) pairs \n# in miles. Get the haversine distance\nimport gpxpy.geo \n\n# Used to determine the size of plots\nfrom matplotlib import rcParams \n\n# https:\/\/pyroomacoustics.readthedocs.io\/en\/pypi-release\/pyroomacoustics.doa.detect_peaks.html\n# Used for detection of peaks\nfrom pyroomacoustics.doa.detect_peaks import detect_peaks\n\n# Download migwin: https:\/\/mingw-w64.org\/doku.php\/download\/mingw-builds\n# Install it in your system and keep the path, migw_path ='installed path'\n# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\n\nfrom prettytable import PrettyTable\nfrom datetime import datetime as dt\n\nmatplotlib.use('nbagg');\nwarnings.filterwarnings(\"ignore\");\n%matplotlib inline","2e775510":"# To find the running time of the entire kernel\nglobalstart = dt.now()","32fd6af2":"# Looking at the features\nmonth = dd.read_csv(\"..\/input\/nyc-yellow-taxi-trip-data\/yellow_tripdata_2015-01.csv\")\nmonth.columns","f3516953":"month.visualize()","98562401":"month.fare_amount.sum().visualize()","aaa78aa2":"# The table below shows few datapoints along with all our features\nmonth = dd.read_csv('..\/input\/nyc-yellow-taxi-trip-data\/yellow_tripdata_2015-01.csv')\nprint(month.columns)\n\nmonth.head()","37712888":"# Plotting pickup cordinates which are outside the bounding box of New-York \n# We will collect all the points outside the bounding box of NYC into outlier_locations\noutlier_locations = month[((month.pickup_longitude <= -74.15) | (month.pickup_latitude <= 40.5774)| \\\n    (month.pickup_longitude >= -73.7004) | (month.pickup_latitude >= 40.9176))]\nprint(\"Number of Outlier Locations:\", len(outlier_locations))\n\n# Creating a map with a base location\n# Read more about the folium here: http:\/\/python-visualization.github.io\/folium\/\n\n# Note: We don't need to remember any of these, we don't need an in-depth knowledge on these \n# maps and plots\n\nmap_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n\n# We will spot only the first 1000 outliers on the map. Plotting all the outliers will take more time\nsample_locations = outlier_locations.head(1000)\nfor i,j in sample_locations.iterrows():\n    if int(j['pickup_latitude']) != 0:\n        folium.Marker(list((j['pickup_latitude'],j['pickup_longitude']))).add_to(map_osm)\n\nmap_osm","08d080ca":"# Plotting dropoff cordinates which are outside the bounding box of New-York \n# We will collect all the points outside the bounding box of NYC into outlier_locations\noutlier_locations = month[((month.dropoff_longitude <= -74.15) | (month.dropoff_latitude <= 40.5774)| \\\n    (month.dropoff_longitude >= -73.7004) | (month.dropoff_latitude >= 40.9176))]\nprint(\"Number of Outlier Locations:\", len(outlier_locations))\n\n# Creating a map with a base location\n# Read more about the folium here: http:\/\/python-visualization.github.io\/folium\/\n\n# Note: We don't need to remember any of these, we don't need an in-depth knowledge on these \n# maps and plots\n\nmap_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n\n# We will spot only the first 1000 outliers on the map, plotting all the outliers will take more time\nsample_locations = outlier_locations.head(1000)\nfor i,j in sample_locations.iterrows():\n    if int(j['pickup_latitude']) != 0:\n        folium.Marker(list((j['dropoff_latitude'],j['dropoff_longitude']))).add_to(map_osm)\n\nmap_osm","b538395d":"# The timestamps are converted to unix so as to get duration (trip-time) & speed.\n# Also, the pickup-times in unix are used while binning \n\n# In the data, we have time in the format \"YYYY-MM-DD HH:MM:SS\" \n# We convert this string to python time format and then into unix time stamp\n# https:\/\/stackoverflow.com\/a\/27914405\ndef convert_to_unix(s):\n    return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())\n\n# We return a data-frame which contains the columns\n# 1.'passenger_count'   : Self-explanatory\n# 2.'trip_distance'     : Self-explanatory\n# 3.'pickup_longitude'  : Self-explanatory\n# 4.'pickup_latitude'   : Self-explanatory\n# 5.'dropoff_longitude' : Self-explanatory\n# 6.'dropoff_latitude'  : Self-explanatory\n# 7.'total_amount'      : Total fare that was paid\n# 8.'trip_times'        : Duration of each trip\n# 9.'pickup_times       : Pickup time converted into unix time \n# 10.'Speed'            : Velocity of each trip\n\ndef return_with_trip_times(month):\n    duration = month[['tpep_pickup_datetime','tpep_dropoff_datetime']].compute()\n    \n    # Pickups and dropoffs to unix time\n    duration_pickup = [convert_to_unix(x) for x in duration['tpep_pickup_datetime'].values]\n    duration_drop = [convert_to_unix(x) for x in duration['tpep_dropoff_datetime'].values]\n    \n    # Calculate the duration of trips\n    # Division by 60 converts the difference from seconds to minutes\n    durations = (np.array(duration_drop) - np.array(duration_pickup))\/float(60)\n\n    # Append durations of trips and speed in miles\/hr to a new dataframe\n    new_frame = month[['passenger_count','trip_distance','pickup_longitude','pickup_latitude',\n        'dropoff_longitude','dropoff_latitude','total_amount']].compute()\n    \n    new_frame['trip_times'] = durations\n    new_frame['pickup_times'] = duration_pickup\n    new_frame['Speed'] = 60*(new_frame['trip_distance']\/new_frame['trip_times'])\n    \n    return new_frame\n\nframe_with_durations = return_with_trip_times(month)\nframe_with_durations.head()","e65676a1":"# The skewed box-plot shows us the presence of outliers \nplt.figure(figsize = (8, 6))\nsns.boxplot(y=\"trip_times\", data = frame_with_durations)","04b49abe":"# Calculating 0-100th percentile to find the correct percentile value for removal of outliers\nvar = frame_with_durations[\"trip_times\"].values\nvar = np.sort(var, axis = None)\n\nfor i in range(0,100,10):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is\", var[-1])","db595628":"# Looking further from the 99th percentile\nvar = frame_with_durations[\"trip_times\"].values\nvar = np.sort(var,axis = None)\n\nfor i in range(90,100):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is\", var[-1])","600159bb":"# Removing data based on our analysis and TLC regulations\nframe_with_durations_modified = frame_with_durations[(frame_with_durations.trip_times > 1) & \\\n    (frame_with_durations.trip_times < 720)]","c1241afa":"frame_with_durations_modified.head()","8cf0d783":"# Box-plot after removal of outliers\nplt.figure(figsize = (8, 6))\nsns.boxplot(y=\"trip_times\", data = frame_with_durations_modified)","af0ff9f8":"# PDF of trip-times after removing the outliers\ntt_sample = frame_with_durations_modified['trip_times'].sample(n = 200000)\n\nplt.figure(figsize = (8, 6))\nsns.distplot(tt_sample, hist=True, kde=True)","0e7399cc":"# Converting the values to log-values to check for log-normal distribution\nframe_with_durations_modified['log_times'] = [math.log(i) for i in frame_with_durations_modified['trip_times'].values]","c08a817e":"# PDF of log-values of trip-times\nltt_sample = frame_with_durations_modified['log_times'].sample(n = 200000)\n\nplt.figure(figsize = (8, 6))\nsns.distplot(ltt_sample, hist=True, kde=True)","4920b279":"# Q-Q plot for checking if trip-times follow the log-normal distribution\n# https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.probplot.html\nltt_sample = frame_with_durations_modified['log_times'].sample(n = 200000)\n\nplt.figure(figsize = (8, 6))\nscipy.stats.probplot(ltt_sample, plot = plt)","90af0a9f":"# Check for any outliers in the data after trip duration outliers have been removed\n# Box-plot for speeds with outliers\nframe_with_durations_modified['Speed'] = 60 * (frame_with_durations_modified['trip_distance'] \/ \\\n    frame_with_durations_modified['trip_times'])\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(y=\"Speed\", data = frame_with_durations_modified)","a09739cb":"# Calculating speed values at each percentile: 0,10,20,30,40,50,60,70,80,90,100 \nvar = frame_with_durations_modified[\"Speed\"].values\nvar = np.sort(var,axis = None)\nfor i in range(0,100,10):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","9deb66f0":"# Calculating speed values at each percentile: 90,91,92,93,94,95,96,97,98,99,100\nvar = frame_with_durations_modified[\"Speed\"].values\nvar = np.sort(var,axis = None)\nfor i in range(90,100):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","edb2feca":"# Calculating speed values at each percentile: 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\nvar = frame_with_durations_modified[\"Speed\"].values\nvar = np.sort(var,axis = None)\nfor i in np.arange(0.0, 1.0, 0.1):\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","22f36d75":"# Removing further outliers based on the 99.9th percentile value\nframe_with_durations_modified = frame_with_durations[(frame_with_durations.Speed > 0) & \\\n    (frame_with_durations.Speed < 45.31)]","7cc46393":"# Average speed of cabs in New-York\nsum(frame_with_durations_modified[\"Speed\"]) \/ float(len(frame_with_durations_modified[\"Speed\"]))","2f346f10":"# Up to now, we have removed the outliers based on trip durations and cab speeds\n# Let's try if there are any outliers in trip distances\n# Box-plot showing outliers in trip-distance values\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(y = \"trip_distance\", data = frame_with_durations_modified)","7bfb4fbf":"# Calculating trip distance values at each percentile: 0,10,20,30,40,50,60,70,80,90,100 \nvar = frame_with_durations_modified[\"trip_distance\"].values\nvar = np.sort(var,axis = None)\nfor i in range(0,100,10):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","9a096c57":"# Calculating trip distance values at each percentile: 90,91,92,93,94,95,96,97,98,99,100\nvar = frame_with_durations_modified[\"trip_distance\"].values\nvar = np.sort(var,axis = None)\nfor i in range(90,100):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","39b4fd47":"# Calculating trip distance values at each percentile: 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\nvar = frame_with_durations_modified[\"trip_distance\"].values\nvar = np.sort(var,axis = None)\nfor i in np.arange(0.0, 1.0, 0.1):\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","ca2b51e8":"# Removing further outliers based on the 99.9th percentile value\nframe_with_durations_modified=frame_with_durations[(frame_with_durations.trip_distance > 0) & \\\n    (frame_with_durations.trip_distance < 23)]","752c9047":"# Box-plot after removal of outliers\nplt.figure(figsize = (8, 6))\nsns.boxplot(y=\"trip_distance\", data = frame_with_durations_modified)","e5e3947f":"# Up to now we have removed the outliers based on trip durations, cab speeds, and trip distances\n# Let's try if there are any outliers in the total_amount\n# Box-plot showing outliers in fare\nplt.figure(figsize = (8, 6))\nsns.boxplot(y=\"total_amount\", data = frame_with_durations_modified)","0c3696f2":"# Calculating total fare amount values at each percentile: 0,10,20,30,40,50,60,70,80,90,100 \nvar = frame_with_durations_modified[\"total_amount\"].values\nvar = np.sort(var,axis = None)\nfor i in range(0,100,10):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","46c40e9e":"# Calculating total fare amount values at each percentile: 90,91,92,93,94,95,96,97,98,99,100\nvar = frame_with_durations_modified[\"total_amount\"].values\nvar = np.sort(var,axis = None)\nfor i in range(90,100):\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","c3e7eb0e":"# Calculating total fare amount values at each percentile: 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\nvar = frame_with_durations_modified[\"total_amount\"].values\nvar = np.sort(var,axis = None)\nfor i in np.arange(0.0, 1.0, 0.1):\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","95f07e75":"# The below plot shows us the fare values (sorted) to find a sharp increase, to remove those values \n# as outliers. Plot the fare amount excluding the last two values in sorted data\nplt.figure(figsize = (8, 6))\nplt.plot(var[:-2])\nplt.show()","fbe4c63c":"# A very sharp increase in fare values can be seen. Plotting last three total fare values\n# And we can observe that there is a shared increase in the values\nplt.figure(figsize = (8, 6))\nplt.plot(var[-3:])\nplt.show()","3556a035":"# Now looking at values not including the last two points, we again find a drastic increase at \n# around 1000 fare values. We plot the last 50 values, excluding the last two values\nplt.figure(figsize = (8, 6))\nplt.plot(var[-50:-2])\nplt.show()","0b2143e0":"# Removing all outliers based on our univariate analysis above\ndef remove_outliers(new_frame):\n    a = new_frame.shape[0]\n    print(\"Number of pickup records:\", a)\n    \n    temp_frame = new_frame[((new_frame.dropoff_longitude >= -74.15) & (new_frame.dropoff_longitude <= -73.7004) &\\\n        (new_frame.dropoff_latitude >= 40.5774) & (new_frame.dropoff_latitude <= 40.9176)) & \\\n        ((new_frame.pickup_longitude >= -74.15) & (new_frame.pickup_latitude >= 40.5774)& \\\n        (new_frame.pickup_longitude <= -73.7004) & (new_frame.pickup_latitude <= 40.9176))]\n    b = temp_frame.shape[0]\n    print(\"Number of outlier coordinates lying outside NY boundaries:\", (a-b))\n\n    temp_frame = new_frame[(new_frame.trip_times > 0) & (new_frame.trip_times < 720)]\n    c = temp_frame.shape[0]\n    print(\"Number of outliers from trip times analysis:\", (a-c))\n    \n    temp_frame = new_frame[(new_frame.trip_distance > 0) & (new_frame.trip_distance < 23)]\n    d = temp_frame.shape[0]\n    print (\"Number of outliers from trip distance analysis:\", (a-d))\n    \n    temp_frame = new_frame[(new_frame.Speed <= 45.31) & (new_frame.Speed >= 0)]\n    e = temp_frame.shape[0]\n    print (\"Number of outliers from speed analysis:\", (a-e))\n    \n    temp_frame = new_frame[(new_frame.total_amount < 1000) & (new_frame.total_amount > 0)]\n    f = temp_frame.shape[0]\n    print (\"Number of outliers from fare analysis:\", (a-f))\n    \n    \n    new_frame = new_frame[((new_frame.dropoff_longitude >= -74.15) & (new_frame.dropoff_longitude <= -73.7004) &\\\n        (new_frame.dropoff_latitude >= 40.5774) & (new_frame.dropoff_latitude <= 40.9176)) & \\\n        ((new_frame.pickup_longitude >= -74.15) & (new_frame.pickup_latitude >= 40.5774)& \\\n        (new_frame.pickup_longitude <= -73.7004) & (new_frame.pickup_latitude <= 40.9176))]\n    new_frame = new_frame[(new_frame.trip_times > 0) & (new_frame.trip_times < 720)]\n    new_frame = new_frame[(new_frame.trip_distance > 0) & (new_frame.trip_distance < 23)]\n    new_frame = new_frame[(new_frame.Speed < 45.31) & (new_frame.Speed > 0)]\n    new_frame = new_frame[(new_frame.total_amount < 1000) & (new_frame.total_amount > 0)]\n    \n    print (\"Total outliers removed\", a-new_frame.shape[0])\n    return new_frame","cd6ad26e":"print (\"Removing outliers in the month of Jan-2015\")\nframe_with_durations_outliers_removed = remove_outliers(frame_with_durations)\nprint(\"Fraction of data points that remain after removing outliers\", \n    float(len(frame_with_durations_outliers_removed)) \/ len(frame_with_durations))","ce64fc35":"# Trying different cluster sizes to choose the right K in K-means\ncoords = frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']].values\nneighbours = []\n \ndef find_min_distance(cluster_centers, cluster_len):\n    nice_points = 0\n    wrong_points = 0\n    less2 = []\n    more2 = []\n    # Stored & Calculated in miles\n    min_dist = 1000\n    \n    for i in range(0, cluster_len):\n        nice_points = 0\n        wrong_points = 0\n        for j in range(0, cluster_len):\n            if j != i:\n                distance = gpxpy.geo.haversine_distance(cluster_centers[i][0], cluster_centers[i][1],\n                    cluster_centers[j][0], cluster_centers[j][1])\n                # `distance` is in meters, so, we have converted it into miles\n                min_dist = min(min_dist, distance\/(1.60934*1000))\n                if (distance\/(1.60934*1000)) <= 2: nice_points += 1\n                else: wrong_points += 1\n        less2.append(nice_points)\n        more2.append(wrong_points)\n    neighbours.append(less2)\n    print (\"On choosing a cluster size of:\", cluster_len, \"\\nAvg. #Clusters within the vicinity (i.e. intercluster-distance < 2):\", \n        np.ceil(sum(less2)\/len(less2)), \"\\nAvg. #Clusters outside the vicinity (i.e. intercluster-distance > 2):\", \n        np.ceil(sum(more2)\/len(more2)), \"\\nMin inter-cluster distance:\", min_dist, \"\\n\")\n\n# https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#mini-batch-kmeans\ndef find_clusters(increment):\n    kmeans = MiniBatchKMeans(n_clusters=increment, batch_size=10000, random_state=42).fit(coords)\n    frame_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n    cluster_centers = kmeans.cluster_centers_\n    cluster_len = len(cluster_centers)\n    return cluster_centers, cluster_len\n\nfor increment in range(10, 100, 10):\n    cluster_centers, cluster_len = find_clusters(increment)\n    find_min_distance(cluster_centers, cluster_len)            ","fd92d41f":"# Getting 40 clusters using K-Means\nkmeans = MiniBatchKMeans(n_clusters=40, batch_size=10000, random_state=0).fit(coords)\nframe_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])","2ebc0b63":"# Plotting the cluster centers on OSM\ncluster_centers = kmeans.cluster_centers_\ncluster_len = len(cluster_centers)\nmap_osm = folium.Map(location = [40.734695, -73.990372], tiles='Stamen Toner')\nfor i in range(cluster_len):\n    folium.Marker(list((cluster_centers[i][0], cluster_centers[i][1])), \n        popup=(str(cluster_centers[i][0])+str(cluster_centers[i][1]))).add_to(map_osm)\nmap_osm","65890ed3":"# Visualising the clusters on a map\ndef plot_clusters(frame):\n    city_long_border = (-74.03, -73.75)\n    city_lat_border = (40.63, 40.85)\n    fig, ax = plt.subplots(ncols=1, nrows=1, figsize = (12, 10))\n    ax.scatter(frame.pickup_longitude.values[:100000], frame.pickup_latitude.values[:100000], s=10, \n        lw=0, c=frame.pickup_cluster.values[:100000], cmap='tab20', alpha=0.2)\n    ax.set_xlim(city_long_border)\n    ax.set_ylim(city_lat_border)\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    plt.show()\n\nplot_clusters(frame_with_durations_outliers_removed)","e74c9eeb":"def add_pickup_bins(frame, month, year):\n    unix_pickup_times = [i for i in frame['pickup_times'].values]\n    unix_times = [[1420070400,1422748800,1425168000,1427846400,1430438400,1433116800],\\\n                    [1451606400,1454284800,1456790400,1459468800,1462060800,1464739200]]\n    start_pickup_unix = unix_times[year-2015][month-1]\n    \n    tenminutewise_binned_unix_pickup_times=[(int((i-start_pickup_unix)\/600)+33) for i in unix_pickup_times]\n    frame['pickup_bins'] = np.array(tenminutewise_binned_unix_pickup_times)\n    return frame","28101255":"# Clustering, making pickup bins and grouping by pickup cluster and pickup bins\njan_2015_frame = add_pickup_bins(frame_with_durations_outliers_removed,1,2015)\njan_2015_groupby = jan_2015_frame[['pickup_cluster','pickup_bins','trip_distance']] \\\n    .groupby(['pickup_cluster','pickup_bins']).count()\njan_2015_groupby = jan_2015_groupby.rename(columns = {'trip_distance': 'pickups'})","58c89ea2":"# Data Preparation for the months of Jan,Feb and March 2016\n# Here, `month` refers to the Dask Dataframe\ndef datapreparation(month, kmeans, month_no, year_no):\n    # Add the trip-times\n    frame_with_durations = return_with_trip_times(month)\n    \n    # Remove the outliers\n    frame_with_durations_outliers_removed = remove_outliers(frame_with_durations)\n    print(\"\\n\")\n    \n    # Estimating Clusters\n    frame_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n\n    # Performing the Group-By operation\n    final_updated_frame = add_pickup_bins(frame_with_durations_outliers_removed, month_no, year_no)\n    final_groupby_frame = final_updated_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()\n    final_groupby_frame = final_groupby_frame.rename(columns = {'trip_distance': 'pickups'})\n    return final_updated_frame, final_groupby_frame\n    \nmonth_jan_2016 = dd.read_csv('..\/input\/nyc-yellow-taxi-trip-data\/yellow_tripdata_2016-01.csv')\nmonth_feb_2016 = dd.read_csv('..\/input\/nyc-yellow-taxi-trip-data\/yellow_tripdata_2016-02.csv')\nmonth_mar_2016 = dd.read_csv('..\/input\/nyc-yellow-taxi-trip-data\/yellow_tripdata_2016-03.csv')\n\njan_2016_frame, jan_2016_groupby = datapreparation(month_jan_2016, kmeans, 1, 2016)\nfeb_2016_frame, feb_2016_groupby = datapreparation(month_feb_2016, kmeans, 2, 2016)\nmar_2016_frame, mar_2016_groupby = datapreparation(month_mar_2016, kmeans, 3, 2016)","1a402dae":"def return_unq_pickup_bins(frame):\n    values = []\n    for i in range(0,40):\n        new = frame[frame['pickup_cluster'] == i]\n        list_unq = list(set(new['pickup_bins']))\n        list_unq.sort()\n        values.append(list_unq)\n    return values","17c08d56":"# For every month, we get all indices of 10-min intervals in which atleast one pickup happened\njan_2015_unique = return_unq_pickup_bins(jan_2015_frame)\njan_2016_unique = return_unq_pickup_bins(jan_2016_frame)\nfeb_2016_unique = return_unq_pickup_bins(feb_2016_frame)\nmar_2016_unique = return_unq_pickup_bins(mar_2016_frame)","c1c3486e":"# For each cluster, number of 10-min intervals with 0 pickups\nfor i in range(40):\n    print(\"For the\", i, \"th cluster, number of 10-min intervals with 0 pickups:\", 4464 - len(set(jan_2015_unique[i])))","30dd9b6c":"# Fills a value of 0 for every bin, where no pickup data is present \n# count_values: #pickps that happened in each region for each 10-min interval\n# There won't be any value if there are no pickups\n# values: Unique bins' indices for every cluster\n\n# For every 10-min interval (pickup_bin), we will check if it is there in our unique bin\n# If it is there, we will add the count_values[index] to smoothed data\n# If not, we will add 0 to the smoothed data\n# We finally return smoothed data\n\ndef fill_missing(count_values, values):\n    smoothed_regions = []\n    ind = 0\n    for r in range(0,40):\n        smoothed_bins = []\n        for i in range(4464):\n            if i in values[r]:\n                smoothed_bins.append(count_values[ind])\n                ind += 1\n            else:\n                smoothed_bins.append(0)\n        smoothed_regions.extend(smoothed_bins)\n    return smoothed_regions","9c639134":"# Fills the average value for every bin, where no pickup data is present \n# count_values: #pickps that happened in each region for each 10-min interval\n# There won't be any value if there are no pickups\n# values: Unique bins' indices for every cluster\n\n# For every 10-min interval (pickup_bin), we will check if it is there in our unique bin\n# If it is there, we will add the count_values[index] to smoothed data\n# If not, we will add smoothed data (which is calculated based on the methods that are discussed \n# in the above markdown cell), and then, we finally return the smoothed data\n\ndef smoothing(count_values,values):\n    # Stores list of final smoothed values of each region\n    smoothed_regions = [] \n    ind = 0\n    repeat = 0 \n    smoothed_value = 0\n    for r in range(0,40):\n        smoothed_bins = [] # Stores the final smoothed values\n        repeat = 0\n        for i in range(4464):\n            # Prevents iteration for a value which is already visited\/resolved\n            if repeat != 0: repeat -= 1; continue\n            \n            # Checks if the pickup-bin exists. Appends the value of the pickup bin if it exists\n            if i in values[r]: smoothed_bins.append(count_values[ind]) \n                \n            # If the pickup bin doesn't exist\n            else:\n                if i != 0:\n                    right_hand_limit = 0\n                    for j in range(i, 4464):\n                        # Searches for the left-limit or the pickup-bin value which has a pickup value\n                        if  j not in values[r]: continue\n                        else: right_hand_limit = j; break\n                    # Case 1: When we have the last\/last few values are found to be missing, & hence we have no right-limit here\n                    if right_hand_limit == 0:\n                        smoothed_value = count_values[ind-1]*1.0 \/ ((4463-i)+2)*1.0                               \n                        for j in range(i, 4464):                              \n                            smoothed_bins.append(math.ceil(smoothed_value))\n                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n                        repeat = (4463-i)\n                        ind -= 1\n                    else:\n                    # Case 2: When we have the missing values between two known values\n                        smoothed_value = (count_values[ind-1] + count_values[ind])*1.0 \/ ((right_hand_limit-i)+2)*1.0             \n                        for j in range(i, right_hand_limit+1):\n                            smoothed_bins.append(math.ceil(smoothed_value))\n                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n                        repeat=(right_hand_limit-i)\n                else:\n                    # Case 3: When we have the first\/first few values are found to be missing, hence we have no left-limit here\n                    right_hand_limit = 0\n                    for j in range(i,4464):\n                        if j not in values[r]: continue\n                        else: right_hand_limit = j; break\n                    smoothed_value = count_values[ind]*1.0 \/ ((right_hand_limit-i)+1)*1.0\n                    for j in range(i, right_hand_limit+1):\n                        smoothed_bins.append(math.ceil(smoothed_value))\n                    repeat = (right_hand_limit - i)\n            ind += 1\n        smoothed_regions.extend(smoothed_bins)\n    return smoothed_regions","7713a5c8":"# Filling Missing values of Jan-2015 with 0\njan_2015_fill = fill_missing(jan_2015_groupby['pickups'].values, jan_2015_unique)\n\n# Smoothing Missing values of Jan-2015\njan_2015_smooth = smoothing(jan_2015_groupby['pickups'].values, jan_2015_unique)","8fb557e4":"# Number of 10-min indices for Jan 2015: 24*31*60\/10 = 4464\n# Number of 10-min indices for Jan 2016: 24*31*60\/10 = 4464\n# Number of 10-min indices for Feb 2016: 24*29*60\/10 = 4176\n# Number of 10-min indices for Mar 2016: 24*31*60\/10 = 4464\n# For each cluster we will have 4464 values, therefore 40*4464 = 178560 (Length of the jan_2015_fill)\nprint(\"Number of 10-min intervals among all the clusters:\", len(jan_2015_fill))","59894b19":"# Smoothing vs Filling\n# Sample plot that shows two variations of filling missing values\n# We have taken the #pickups for cluster region 2\nplt.figure(figsize = (10,5))\nplt.plot(jan_2015_fill[4464:8920], label=\"Filled with zeroes\")\nplt.plot(jan_2015_smooth[4464:8920], label=\"Filled with avg. values\")\nplt.legend()\nplt.show()","fa888549":"# Jan-2015 data is smoothed, Jan, Feb & Mar 2016 data missing values are filled with zeroes\njan_2015_smooth = smoothing(jan_2015_groupby['pickups'].values, jan_2015_unique)\njan_2016_smooth = fill_missing(jan_2016_groupby['pickups'].values, jan_2016_unique)\nfeb_2016_smooth = fill_missing(feb_2016_groupby['pickups'].values, feb_2016_unique)\nmar_2016_smooth = fill_missing(mar_2016_groupby['pickups'].values, mar_2016_unique)\n\n# Making list of all the values of pickup data in every bin for a period of 3 months, \n# and storing them region-wise. \n# regions_cum: It will contain 40 lists, each list will contain 4464 + 4176 + 4464 values,\n# which represents the #pickups, that have happened for three months in 2016 data\nregions_cum = []\n\n# Number of 10-min indices for Jan 2015: 24*31*60\/10 = 4464\n# Number of 10-min indices for Jan 2016: 24*31*60\/10 = 4464\n# Number of 10-min indices for Feb 2016: 24*29*60\/10 = 4176\n# Number of 10-min indices for Mar 2016: 24*31*60\/10 = 4464\n\nfor i in range(0, 40):\n    regions_cum.append(jan_2016_smooth[4464*i:4464*(i+1)] + feb_2016_smooth[4176*i:4176*(i+1)] + \\\n        mar_2016_smooth[4464*i:4464*(i+1)])\n\nprint(len(regions_cum), len(regions_cum[0]))","a7f684b2":"# A function to generate unique colors\ndef uniqueish_color():\n    return plt.cm.gist_ncar(np.random.random())\n\nfirst_x = list(range(0,4464))\nsecond_x = list(range(4464,8640))\nthird_x = list(range(8640,13104))\n\nfor i in range(40):\n    plt.figure(figsize = (10,4))\n    plt.plot(first_x,regions_cum[i][:4464], color = uniqueish_color(), label='Jan 2016')\n    plt.plot(second_x,regions_cum[i][4464:8640], color = uniqueish_color(), label='Feb 2016')\n    plt.plot(third_x,regions_cum[i][8640:], color = uniqueish_color(), label='Mar 2016')\n    plt.legend()\n    plt.show()","51e3db03":"Y = np.fft.fft(np.array(jan_2016_smooth)[0:4460])\nfreq = np.fft.fftfreq(4460, 1)\nn = len(freq)\nplt.figure()\nplt.plot(freq[:int(n\/2)], np.abs(Y)[:int(n\/2)])\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Amplitude\")\nplt.show()","626c8190":"indexes_jan2016 = detect_peaks(regions_cum[0][:4464], mph = 200, show=True)\nindexes_feb2016 = detect_peaks(regions_cum[0][4464:8640], mph = 200, show=True)\nindexes_mar2016 = detect_peaks(regions_cum[0][8640:], mph = 200, show=True)","d1f5b0f4":"def extract_fft_features(cluster, month):\n    \"\"\"\n    The `fft` function when returns the DFT, the sampling frequencies are in the form of \n    +- f1, -+ f2, and so on. So, when we will be considering the 3 frequencies corresponding to\n    the 3 highest peaks, we will consider the positive counterparts only. (If you want, you can \n    consider the negative counterparts only, as well)\n    \"\"\"\n    \n    # Creating an empty list for the 4 features\n    features = []\n    \n    if month == 0:\n        Y = np.fft.fft(regions_cum[cluster][0:4464])\n        freq = np.fft.fftfreq(4464, 1)\n    elif month == 1:\n        Y = np.fft.fft(regions_cum[cluster][4464:8640])\n        freq = np.fft.fftfreq(4176, 1)\n    elif month == 2:\n        Y = np.fft.fft(regions_cum[cluster][8640:])\n        freq = np.fft.fftfreq(4464, 1)\n    \n    # `indexes` contains the indices of the peaks\n    indexes = detect_peaks(Y, mph = 1000, show = True)\n    \n    # Number of peaks is the first feature\n    features.append(len(indexes))\n    \n    if len(indexes) != 0:\n        # Getting the Amplitude and Frequency of the Peaks using the `indexes`\n        amp_peaks = np.abs(Y[indexes])\n        freq_peaks = freq[indexes]\n\n        # Getting the Indices which would sort the amp_peaks array\n        # Since the Indices sort the array in the Increasing Order, hence reversing the array of indices\n        sorted_ind = np.argsort(amp_peaks)\n        sorted_ind = sorted_ind[ : :-1]\n\n        # Contains the frequencies of the Highest Peaks in order\n        high_peak_freq = np.take(freq_peaks, sorted_ind)\n        \n        for i in range(0, 6, 2):\n            if i < len(high_peak_freq): features.append(np.abs(high_peak_freq[i]))\n            else: features.append(0)\n\n    else: features.extend([0, 0, 0])\n    return features\n    \n    \n# Creating an empty numpy array, sto store the FFT-based features\nfft_feat = np.empty([40, 13104, 4])    \n\nfor i in range(40):\n    features = extract_fft_features(i, 0)\n    fft_feat[i][:4464] = features\n    features = extract_fft_features(i, 1)\n    fft_feat[i][4464:8640] = features\n    features = extract_fft_features(i, 2)\n    fft_feat[i][8640:] = features","778da968":"# Preparing the Dataframe only with x(i) values as Jan 2015 data and y(i) values as Jan 2016\nratios_jan = pd.DataFrame()\nratios_jan['Given'] = jan_2015_smooth\nratios_jan['Prediction'] = jan_2016_smooth\nratios_jan['Ratios'] = ratios_jan['Prediction']*1.0 \/ ratios_jan['Given']*1.0","2ed8ad09":"def MA_R_Predictions(ratios):\n    predicted_ratio = (ratios['Ratios'].values)[0]\n    error, predicted_values, predicted_ratio_values = [], [], []\n    window_size = 3\n    for i in range(0, 4464*40):\n        if i % 4464 == 0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_val = int(((ratios['Given'].values)[i])*predicted_ratio)\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(predicted_val)\n        error.append( abs(predicted_val-(ratios['Prediction'].values)[i]) )\n        if i+1 >= window_size:\n            predicted_ratio = sum((ratios['Ratios'].values)[(i+1)-window_size:(i+1)]) \/ window_size\n        else:\n            predicted_ratio = sum((ratios['Ratios'].values)[0:(i+1)]) \/ (i+1)\n            \n    ratios['MA_R_Predicted'] = predicted_values\n    ratios['MA_R_Error'] = error\n    \n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error]) \/ len(error)\n    return ratios, mape_err, mse_err","fa828a3d":"def MA_P_Predictions(ratios):\n    predicted_value = (ratios['Prediction'].values)[0]\n    error, predicted_values = [], []\n    window_size = 1\n    for i in range(0, 4464*40):\n        predicted_values.append(predicted_value)\n        error.append(abs(predicted_value-(ratios['Prediction'].values)[i]))\n        if i+1 >= window_size:\n            predicted_value = int(sum((ratios['Prediction'].values)[(i+1)-window_size:(i+1)]) \/ window_size)\n        else:\n            predicted_value = int(sum((ratios['Prediction'].values)[0:(i+1)]) \/ (i+1))\n            \n    ratios['MA_P_Predicted'] = predicted_values\n    ratios['MA_P_Error'] = error\n    \n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error]) \/ len(error)\n    return ratios, mape_err, mse_err","4238f7b6":"def WA_R_Predictions(ratios):\n    predicted_ratio=(ratios['Ratios'].values)[0]\n    alpha, window_size = 0.5, 5\n    error, predicted_values, predicted_ratio_values = [], [], []\n    for i in range(0, 4464*40):\n        if i%4464 == 0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_val = int(((ratios['Given'].values)[i])*predicted_ratio)\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(predicted_val)\n        error.append( abs(predicted_val-(ratios['Prediction'].values)[i]) )\n    \n        sum_values = 0\n        sum_of_coeff = 0\n        if i+1 >= window_size:\n            for j in range(window_size, 0, -1):\n                sum_values += j*(ratios['Ratios'].values)[i-window_size+j]\n                sum_of_coeff += j\n        else:\n            for j in range(i+1, 0, -1):\n                sum_values += j*(ratios['Ratios'].values)[j-1]\n                sum_of_coeff += j\n                \n        predicted_ratio = sum_values \/ sum_of_coeff\n            \n    ratios['WA_R_Predicted'] = predicted_values\n    ratios['WA_R_Error'] = error\n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error])\/len(error)\n    return ratios, mape_err, mse_err","c7b57a10":"def WA_P_Predictions(ratios):\n    predicted_value=(ratios['Prediction'].values)[0]\n    error, predicted_values = [], []\n    window_size = 2\n    for i in range(0, 4464*40):\n        predicted_values.append(predicted_value)\n        error.append(abs(predicted_value-(ratios['Prediction'].values)[i]))\n        \n        sum_values = 0\n        sum_of_coeff = 0\n        if i+1 >= window_size:\n            for j in range(window_size, 0, -1):\n                sum_values += j*(ratios['Prediction'].values)[i-window_size+j]\n                sum_of_coeff += j\n\n        else:\n            for j in range(i+1,0,-1):\n                sum_values += j*(ratios['Prediction'].values)[j-1]\n                sum_of_coeff+=j\n        \n        predicted_value = int(sum_values \/ sum_of_coeff)\n    \n    ratios['WA_P_Predicted'] = predicted_values\n    ratios['WA_P_Error'] = error\n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error])\/len(error)\n    return ratios, mape_err, mse_err","4f19336d":"def EA_R1_Predictions(ratios):\n    predicted_ratio=(ratios['Ratios'].values)[0]\n    alpha = 0.6\n    error, predicted_values, predicted_ratio_values = [], [], []\n    for i in range(0, 4464*40):\n        if i % 4464 == 0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_val = int(((ratios['Given'].values)[i])*predicted_ratio)\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(predicted_val)\n        error.append(abs((predicted_val - (ratios['Prediction'].values)[i])))\n        predicted_ratio = (alpha*predicted_ratio) + (1-alpha)*((ratios['Ratios'].values)[i])\n    \n    ratios['EA_R1_Predicted'] = predicted_values\n    ratios['EA_R1_Error'] = error\n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error])\/len(error)\n    return ratios, mape_err, mse_err","17c20fc0":"def EA_P1_Predictions(ratios):\n    predicted_value= (ratios['Prediction'].values)[0]\n    alpha = 0.3\n    error, predicted_values = [], []\n    for i in range(0, 4464*40):\n        if i % 4464 == 0:\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_values.append(predicted_value)\n        error.append(abs((predicted_value-(ratios['Prediction'].values)[i])))\n        predicted_value =int((alpha*predicted_value) + (1-alpha)*((ratios['Prediction'].values)[i]))\n    \n    ratios['EA_P1_Predicted'] = predicted_values\n    ratios['EA_P1_Error'] = error\n    avg_pred_val = (sum(ratios['Prediction'].values) \/ len(ratios['Prediction'].values))\n    mape_err = (1 \/ len(error)) * (sum(error) \/ avg_pred_val)\n    mse_err = sum([e**2 for e in error])\/len(error)\n    return ratios, mape_err, mse_err","1b45ed20":"mape_err = [0]*10\nmse_err = [0]*10\nratios_jan, mape_err[0], mse_err[0] = MA_R_Predictions(ratios_jan)\nratios_jan, mape_err[1], mse_err[1] = MA_P_Predictions(ratios_jan)\nratios_jan, mape_err[2], mse_err[2] = WA_R_Predictions(ratios_jan)\nratios_jan, mape_err[3], mse_err[3] = WA_P_Predictions(ratios_jan)\nratios_jan, mape_err[4], mse_err[4] = EA_R1_Predictions(ratios_jan)\nratios_jan, mape_err[5], mse_err[5] = EA_P1_Predictions(ratios_jan)","77d01a1d":"x = PrettyTable()\n\nx.field_names = [\"Baseline Model\", \"MAPE\", \"MSE\"]\nx.add_rows([\n    [\"Simple Moving Averages (Ratios)\", mape_err[0], mse_err[0]],\n    [\"Simple Moving Averages (2016 Values)\", mape_err[1], mse_err[1]],\n    [\"Weighted Moving Averages (Ratios)\", mape_err[2], mse_err[2]],\n    [\"Weighted Moving Averages (2016 Values)\", mape_err[3], mse_err[3]],\n    [\"Exponential Weighted Moving Averages (Ratios)\", mape_err[4], mse_err[4]],\n    [\"Exponential Weighted Moving Averages (2016 Values)\", mape_err[5], mse_err[5]]\n])\n\nprint (\"Error Metric Matrix (Forecasting Methods) - MAPE & MSE\")\nprint(x)","7b593d71":"# Number of 10-min indices for Jan 2016: 24*31*60\/10 = 4464\n# Number of 10-min indices for Feb 2016: 24*29*60\/10 = 4176\n# Number of 10-min indices for Mar 2016: 24*31*60\/10 = 4464\n# regions_cum: It will contain 40 lists, each list will contain (4464+4176+4464) = 13104 values \n# which represents the #pickups that have happened for three months in 2016.\n\n# Since, we are considering the #pickups for the last 5 time-bins, hence, we are omitting the \n# first 5 time bins from our dataframe, and therefore, our prediction starts from 5th 10-min interval\n\n\n# We take #pickups that have happened in last 5 10-min intervals\nnumber_of_time_stamps = 5\n\n# It is a list of lists\n# It will contain 13099 #pickups for each cluster\noutput = []\n\n# tsne_lat will contain 13099 times latitude of cluster center for every cluster\n# Ex: [[cent_lat 13099times], [cent_lat 13099times], [cent_lat 13099times] .... 40 lists]\n# It is a list of lists\ntsne_lat = []\n\n\n# tsne_lon will contain 13099 times longitude of cluster center for every cluster\n# Ex: [[cent_long 13099times], [cent_long 13099times], [cent_long 13099times] .... 40 lists]\n# It is a list of lists\ntsne_lon = []\n\n# We will code each day as below\n# Sun = 0, Mon = 1, Tue = 2, Wed = 3, Thu = 4, Fri = 5, Sat = 6\n# For every cluster, we will be adding 13099 values, each value represent to which day of \n# the week that pickup bin belongs to.\n# It is a list of lists\ntsne_weekday = []\n\n# It's a numpy array of shape (40 * 13099, 5) = (523960, 5)\n# Each row corresponds to an entry in out data\n# For the first row we will have [f0,f1,f2,f3,f4], where fi = #pickups happened in (i+1)th \n# 10-min interval (bin). The second row will have [f1,f2,f3,f4,f5]\n# The third row will have [f2,f3,f4,f5,f6], and so on...\ntsne_feature = []\ntsne_feature = [0] * number_of_time_stamps\n\n# Jan 2016 is Thursday, so we start our day from 4: \"(int(k\/144))%7+4\"\n# regions_cum is a list of lists \n# [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], ... 40 lists]\n\nfor i in range(0, 40):\n    tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099)\n    tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n    tsne_weekday.append([int(((int(k\/144))%7+4)%7) for k in range(5,4464+4176+4464)])\n    tsne_feature = np.vstack((tsne_feature, [\n        regions_cum[i][r : r + number_of_time_stamps] \\\n        for r in range(0, len(regions_cum[i]) - number_of_time_stamps)\n    ]))\n    output.append(regions_cum[i][5:])\n\n# Removing the first dummy row\ntsne_feature = tsne_feature[1:]\n\nprint(len(tsne_lat[0])*len(tsne_lat))\nprint(tsne_feature.shape[0])\nprint(len(tsne_weekday)*len(tsne_weekday[0]))\nprint(len(output)*len(output[0]))","6f6a3207":"# Adding the FFT-Based Features, Removing the first 5 rows for each of the clusters\n# fft_feat_rem5 = fft_feat[ : ,5: , :]\n# print(fft_feat_rem5.shape)\n\n# fft_feat_prep = fft_feat_rem5.reshape((523960, 4))\n# print(fft_feat_prep.shape)\n\n# tsne_feature = np.hstack([tsne_feature, fft_feat_prep])\n# print(tsne_feature.shape)","d74d6f05":"# From the baseline models we said that the EWMA gives us the least error\n# We will try to add the same EWMA at t as a feature to our data\n# EWMA => P'(t) = alpha * P'(t-1) + (1-alpha) * P(t-1) \nalpha = 0.3\n\n# It is a temporary array that stores EWMA for each 10-min interval, \n# For each cluster it will get reset. For every cluster it contains 13104 values\npredicted_values = []\n\n# It is similar like tsne_lat\n# It is a list of lists\n# [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lists]\npredict_list = []\ntsne_flat_exp_avg = []\n\nfor r in range(0,40):\n    for i in range(0,13104):\n        if i==0:\n            predicted_value = regions_cum[r][0]\n            predicted_values.append(0)\n            continue\n        predicted_values.append(predicted_value)\n        predicted_value = int((alpha*predicted_value) + (1-alpha)*(regions_cum[r][i]))\n    predict_list.append(predicted_values[5:])\n    predicted_values = []","8ebba860":"# train-test split: 70%-30% split\nprint(\"Size of train data:\", int(13099*0.7))\nprint(\"Size of test data:\", int(13099*0.3))","b4c6af20":"# Extracting first 9169 timestamp values i.e., 70% of 13099 (total timestamps) for our training data\ntrain_features =  [tsne_feature[13099*i:(13099*i+9169)] for i in range(0,40)]\n# temp = [0]*(12955 - 9068)\ntest_features = [tsne_feature[13099*i+9169:13099*(i+1)] for i in range(0,40)]","df993e35":"print(\"Number of Data-clusters:\", len(train_features), \"\\nNumber of data points in train data\", \n    len(train_features[0]), \"\\nEach data-point contains\", len(train_features[0][0]),\"features\")\nprint(\"\\n\")\nprint(\"Number of Data-clusters:\", len(test_features), \"\\nNumber of data points in test data\", \n    len(test_features[0]), \"\\nEach data-point contains\", len(test_features[0][0]), \"features\")","b51935f8":"# Extracting first 9169 timestamp values i.e., 70% of 13099 (total timestamps) for our training data\ntsne_train_flat_lat = [i[:9169] for i in tsne_lat]\ntsne_train_flat_lon = [i[:9169] for i in tsne_lon]\ntsne_train_flat_weekday = [i[:9169] for i in tsne_weekday]\ntsne_train_flat_output = [i[:9169] for i in output]\ntsne_train_flat_exp_avg = [i[:9169] for i in predict_list]","c0bed527":"# Extracting the rest of the timestamp values i.e., 30% of 13099 (total timestamps) for our test data\ntsne_test_flat_lat = [i[9169:] for i in tsne_lat]\ntsne_test_flat_lon = [i[9169:] for i in tsne_lon]\ntsne_test_flat_weekday = [i[9169:] for i in tsne_weekday]\ntsne_test_flat_output = [i[9169:] for i in output]\ntsne_test_flat_exp_avg = [i[9169:] for i in predict_list]","42f2b5aa":"# The above variables contain values in the form of list of lists (i.e., list of values of \n# each region), here we make all of them in one list.\ntrain_new_features = []\nfor i in range(0, 40):\n    train_new_features.extend(train_features[i])\n    \ntest_new_features = []\nfor i in range(0, 40):\n    test_new_features.extend(test_features[i])","cd263ad3":"# Converting lists of lists into a single list i.e., flatten\n# a  = [[1,2,3,4],[4,6,7,8]]\n# print(sum(a,[]))\n# [1, 2, 3, 4, 4, 6, 7, 8]\n\ntsne_train_lat = sum(tsne_train_flat_lat, [])\ntsne_train_lon = sum(tsne_train_flat_lon, [])\ntsne_train_weekday = sum(tsne_train_flat_weekday, [])\ntsne_train_output = sum(tsne_train_flat_output, [])\ntsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])","15bf50c7":"# Converting lists of lists into a single list i.e., flatten\n# a  = [[1,2,3,4],[4,6,7,8]]\n# print(sum(a,[]))\n# [1, 2, 3, 4, 4, 6, 7, 8]\n\ntsne_test_lat = sum(tsne_test_flat_lat, [])\ntsne_test_lon = sum(tsne_test_flat_lon, [])\ntsne_test_weekday = sum(tsne_test_flat_weekday, [])\ntsne_test_output = sum(tsne_test_flat_output, [])\ntsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])","6cc9fc3c":"# Preparing the data-frame for our train data\n\n# Considering FFT-Based Features\n# columns = ['ft_5','ft_4','ft_3','ft_2','ft_1', 'len_peaks', 'freq_1', 'freq_2', 'freq_3']\n# Not Considering FFT-Based Features\ncolumns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n\ndf_train = pd.DataFrame(data = train_new_features, columns = columns) \ndf_train['lat'] = tsne_train_lat\ndf_train['lon'] = tsne_train_lon\ndf_train['weekday'] = tsne_train_weekday\ndf_train['exp_avg'] = tsne_train_exp_avg\nprint(df_train.shape)","96b5b4f7":"df_train.head()","c4d29a26":"# Preparing the data-frame for our test data\ndf_test = pd.DataFrame(data=test_new_features, columns=columns) \ndf_test['lat'] = tsne_test_lat\ndf_test['lon'] = tsne_test_lon\ndf_test['weekday'] = tsne_test_weekday\ndf_test['exp_avg'] = tsne_test_exp_avg\nprint(df_test.shape)","4ed2a18e":"df_test.head()","8c2e8f3e":"# No hyper-parameter tuning to do\nlr_reg = LinearRegression().fit(df_train, tsne_train_output)\ny_pred = lr_reg.predict(df_train)\nlr_train_predictions = [round(value) for value in y_pred]\ny_pred = lr_reg.predict(df_test)\nlr_test_predictions = [round(value) for value in y_pred]","ae45837c":"# params = {\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'min_samples_leaf': [1, 2, 3, 4],\n#     'min_samples_split': [1, 2, 3, 4],\n#     'n_estimators': [i for i in range(0, 101, 20)],\n#     'n_jobs': [-1]\n# }\n# rfr = RandomForestRegressor()\n# model = RandomizedSearchCV(rfr, params, verbose = 1)\n# model.fit(df_train, tsne_train_output)\n# print(model.best_params_)\n\n# Best Params found using RandomizedSearch\n# {'n_jobs': -1, 'n_estimators': 40, 'min_samples_split': 3, \n# 'min_samples_leaf': 4, 'max_features': 'sqrt'}","9597a737":"# Training the Model with the best hyper-parameters found using the above Randomized Search\nregr1 = RandomForestRegressor(max_features='sqrt', min_samples_leaf = 4,\n    min_samples_split = 3, n_estimators = 40, n_jobs = -1)\nregr1.fit(df_train, tsne_train_output)","ee6160ab":"# Predicting on train & test data using our trained Random Forest model \ny_pred = regr1.predict(df_train)\nrndf_train_predictions = [round(value) for value in y_pred]\ny_pred = regr1.predict(df_test)\nrndf_test_predictions = [round(value) for value in y_pred]","c0f00801":"# Feature importances based on analysis using Random Forest\nprint (df_train.columns)\nprint (regr1.feature_importances_)","3039f68a":"# params = {\n#     'learning_rate': [0.01, 0.1, 1],\n#     'n_estimators': [i for i in range(1, 1001, 250)],\n#     'max_depth': [2, 3, 4],\n#     'min_child_weight': [2, 3, 4],\n#     'gamma': [0, 0.1, 0.5],\n#     'subsample': [0.5, 0.8, 1],\n#     'reg_alpha': [100, 200],\n#     'reg_lambda': [100, 200],\n#     'colsample_bytree': [0.4, 0.8, 1.0],\n#     'n_jobs': [-1]\n# }\n# xgbr = xgb.XGBRegressor()\n# model = RandomizedSearchCV(xgbr, params, verbose = 2)\n# model.fit(df_train, tsne_train_output)\n# print(model.best_params_)\n\n# Best Params found using RandomizedSearch\n# {'subsample': 0.8, 'reg_lambda': 200, 'reg_alpha': 200, 'n_jobs': -1, \n# 'n_estimators': 1000, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.1, \n# 'gamma': 0, 'colsample_bytree': 0.8}","dcd45a3f":"# Training the Model with the best hyper-parameters found using the above Randomized Search\nx_model = xgb.XGBRegressor(\n    learning_rate=0.1, n_estimators=1000, max_depth=3, min_child_weight=3,\n    gamma=0, subsample=0.8, reg_alpha=200, reg_lambda=200, colsample_bytree=0.8, n_jobs=-1\n)\nx_model.fit(df_train, tsne_train_output)","cbe7216e":"# Predicting on train & test data using our trained XgBoost regressor model\ny_pred = x_model.predict(df_train)\nxgb_train_predictions = [round(value) for value in y_pred]\ny_pred = x_model.predict(df_test)\nxgb_test_predictions = [round(value) for value in y_pred]","82d52847":"# Feature importances based on analysis using XgBoost\nprint (df_train.columns)\nprint(x_model.feature_importances_)","09070cd7":"train_mape = []\ntest_mape = []\n\ntrain_mape.append((mean_absolute_error(tsne_train_output,df_train['ft_1'].values)) \/ (sum(tsne_train_output)\/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output,df_train['exp_avg'].values)) \/ (sum(tsne_train_output)\/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output,rndf_train_predictions)) \/ (sum(tsne_train_output)\/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output, xgb_train_predictions)) \/ (sum(tsne_train_output)\/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output, lr_train_predictions)) \/ (sum(tsne_train_output)\/len(tsne_train_output)))\n\ntest_mape.append((mean_absolute_error(tsne_test_output, df_test['ft_1'].values)) \/ (sum(tsne_test_output)\/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, df_test['exp_avg'].values)) \/ (sum(tsne_test_output)\/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, rndf_test_predictions)) \/ (sum(tsne_test_output)\/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, xgb_test_predictions)) \/ (sum(tsne_test_output)\/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, lr_test_predictions)) \/ (sum(tsne_test_output)\/len(tsne_test_output)))","89028803":"x = PrettyTable()\n\nx.field_names = [\"Model\", \"Train MAPE\", \"Test MAPE\"]\nx.add_rows([\n    [\"Baseline Model (2016 Values)\", train_mape[0], test_mape[0]],\n    [\"Exponential Weighted Moving Averages (2016 Values)\", train_mape[1], test_mape[1]],\n    [\"Random Forest Regressor\", train_mape[2], test_mape[2]],\n    [\"XgBoost Regressor\", train_mape[3], test_mape[3]],\n    [\"Linear Regression\", train_mape[4], test_mape[4]]\n])\n\nprint (\"Error Metric Matrix (Forecasting Methods) - MAPE & MSE\")\nprint(x)","5048b0d2":"print(\"Time taken to run this entire kernel is:\", dt.now() - globalstart)","0c598f27":"### 2.1.1. Plotting the Cluster Centers","4959c21e":"## Data Information\n- The data used in the attached datasets were collected and provided to the **NYC Taxi and Limousine Commission (TLC)**. \n- You can get the data from [here](https:\/\/www1.nyc.gov\/site\/tlc\/about\/tlc-trip-record-data.page). \n\n## Information on Taxis\n#### Yellow Taxi: Yellow Medallion Taxicabs\nThese are the famous NYC yellow taxis that provide transportation exclusively through street-hails. The number of taxicabs is limited by a finite number of medallions issued by the TLC. You access this mode of transportation by standing in the street and hailing an available taxi with your hand. The pickups are not pre-arranged.\n\n#### For Hire Vehicles (FHVs)\nFHV transportation is accessed by a pre-arrangement with a dispatcher or limo company. These FHVs are not permitted to pick up passengers via street hails, as those rides are not considered pre-arranged.\n\n#### Green Taxi: Street Hail Livery (SHL)\nThe SHL program will allow livery vehicle owners to license and outfit their vehicles with green borough taxi branding, meters, credit card machines, and ultimately the right to accept street hails in addition to pre-arranged rides.\nCredits: Quora\n\n#### Footnote:\nIn the given notebook we are considering only the yellow taxis data for the months of Jan 2015 & Jan - Mar 2016\n\n## Data Collection\n- In this dataset, we have collected the data for 4 months particularly:\n    - Jan 2015: 1.8 GB\n    - Jan 2016: 1.6 GB\n    - Feb 2016: 1.7 GB\n    - Mar 2016: 1.8 GB\n- Based upon the models, either we will be using the 2015 data as the training dataset and the 2016 data as the test set, or, we will be using the 2016 data solely.\n\n## Quick Tutorial on Dask\n- Unlike Pandas, operations on dask.dataframes don't trigger immediate computation. \n- Instead they add key-value pairs to an underlying Dask Graph, which are basically stored in the form of Python Dictionaries. \n- Recall that in the diagram below, circles are operations and rectangles are results, and to see these visualizations, you need to install graphviz.\n- Here one might wonder, why I have used Dask, when the size of the entire dataset < 7 GB, even though Kaggle Kernels provide us with 16 GBs of RAM.\n- This is just because I wanted to explore this new library. Additionally, it would help me to run this kernel on my local work-station as well.\n- It's worth noting here that Dask is intended to work with 10-100 GBs of datasets, by making intelligent use of parallel computing.","6efcc4ec":"### 4.2.2. Random Forest Regressor Model","520c7128":"- Next, we use the Moving averages of the 2016 values themselves, to predict the future value using $\\begin{align}P_{t} = ( P_{t-1} + P_{t-2} + P_{t-3} .... P_{t-n} )\/n \\end{align}$\n- Once again, the window-size (n) is a hyper-parameter, which is tuned manually.\n- And it is found that the window-size of 1 is optimal for getting the best results using Moving Averages with previous 2016 values, therefore we get $\\begin{align}P_{t} = P_{t-1} \\end{align}$","2a11566a":"### 4.2.3. XgBoost Regressor Model","c115b9fd":"## 3.4. Comparison between Baseline Models\n- We have chosen our error metric for comparison between models as <b>MAPE (Mean Absolute Percentage Error)<\/b>, so that we can know, on an average how good is our model with predictions\n- <b>MSE (Mean Squared Error)<\/b> is also used, so that we have a clearer understanding as to how well our forecasting model performs with outliers, & it helps us to make sure that there is not much of an error margin between our predicted and the actual value.","cb7bb32c":"## Features in the Dataset\n<table border=\"1\">\n\t<tr>\n\t\t<th>Field Name<\/th>\n\t\t<th>Description<\/th>\n\t<\/tr>\n\t<tr>\n\t\t<td>VendorID<\/td>\n\t\t<td>\n\t\tA code indicating the TPEP provider that provided the record.\n\t\t<ol>\n\t\t\t<li>Creative Mobile Technologies<\/li>\n\t\t\t<li>VeriFone Inc.<\/li>\n\t\t<\/ol>\n\t\t<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>tpep_pickup_datetime<\/td>\n\t\t<td>The date and time when the meter was engaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>tpep_dropoff_datetime<\/td>\n\t\t<td>The date and time when the meter was disengaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Passenger_count<\/td>\n\t\t<td>The number of passengers in the vehicle. This is a driver-entered value.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Trip_distance<\/td>\n\t\t<td>The elapsed trip distance in miles reported by the taximeter.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Pickup_longitude<\/td>\n\t\t<td>Longitude where the meter was engaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Pickup_latitude<\/td>\n\t\t<td>Latitude where the meter was engaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>RateCodeID<\/td>\n\t\t<td>The final rate code in effect at the end of the trip.\n\t\t<ol>\n\t\t\t<li> Standard rate <\/li>\n\t\t\t<li> JFK <\/li>\n\t\t\t<li> Newark <\/li>\n\t\t\t<li> Nassau or Westchester<\/li>\n\t\t\t<li> Negotiated fare <\/li>\n\t\t\t<li> Group ride<\/li>\n\t\t<\/ol>\n\t\t<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Store_and_fwd_flag<\/td>\n\t\t<td>This flag indicates whether the trip record was held in vehicle memory before sending to the vendor,<br> aka \u201cstore and forward,\u201d because the vehicle did not have a connection to the server.\n\t\t<br>Y= store and forward trip\n\t\t<br>N= not a store and forward trip\n\t\t<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Dropoff_longitude<\/td>\n\t\t<td>Longitude where the meter was disengaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Dropoff_ latitude<\/td>\n\t\t<td>Latitude where the meter was disengaged.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Payment_type<\/td>\n\t\t<td>A numeric code signifying how the passenger paid for the trip.\n\t\t<ol>\n\t\t\t<li> Credit card <\/li>\n\t\t\t<li> Cash <\/li>\n\t\t\t<li> No charge <\/li>\n\t\t\t<li> Dispute<\/li>\n\t\t\t<li> Unknown <\/li>\n\t\t\t<li> Voided trip<\/li>\n\t\t<\/ol>\n\t\t<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Fare_amount<\/td>\n\t\t<td>The time-and-distance fare calculated by the meter.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Extra<\/td>\n\t\t<td>Miscellaneous extras and surcharges. Currently, this only includes. the $0.50 and $1 rush hour and overnight charges.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>MTA_tax<\/td>\n\t\t<td>0.50 MTA tax that is automatically triggered based on the metered rate in use.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Improvement_surcharge<\/td>\n\t\t<td>0.30 improvement surcharge assessed trips at the flag drop. the improvement surcharge began being levied in 2015.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Tip_amount<\/td>\n\t\t<td>Tip amount \u2013 This field is automatically populated for credit card tips.Cash tips are not included.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Tolls_amount<\/td>\n\t\t<td>Total amount of all tolls paid in trip.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Total_amount<\/td>\n\t\t<td>The total amount charged to passengers. Does not include cash tips.<\/td>\n\t<\/tr>\n<\/table>","f815afe9":"# NYC Yellow Taxi Trip Data: Pickup Density Prediction\n- Hola amigos, this notebook covers my code for the **NYC Yellow Taxi Trip Data** dataset, in which I have trained some models to **predict the pickup density**. The dataset can be found [here](https:\/\/www.kaggle.com\/elemento\/nyc-yellow-taxi-trip-data).\n- This dataset can be used in other interesting ways as well, but I chose to use it this way. Feel free to use this dataset in other ways, and do let me know in the [Discussion Section](https:\/\/www.kaggle.com\/elemento\/nyc-yellow-taxi-trip-data\/discussion) of the dataset. Feel free to check out the dataset for more information.\n- This work is largely inspired by the **Applied AI Course**, and I would like to say a special thanks to the entire team of Applied AI.","efed2c95":"- Exponential Weighted Moving Averages using Previous 2016 Values:\n$\\begin{align}P^{'}_{t} = \\alpha*P_{t-1} + (1-\\alpha)*P^{'}_{t-1}  \\end{align}$","d0a53a62":"## 2.3. Smoothing\n- Get the unique bins where pickup values are present for each region\n- For each cluster region, we will collect all the indices of 10-min intervals, in which at least 1 pickup has happened. We got an observation that there are some pickup bins that doesn't have any pickups.","f0aee9b8":"### 2.4.2. Integrating Fourier Transform Based Features\n- We will be using the `fft` and the `fftfreq` functions from the `numpy.fft` module. Using the `fft` function, we will get the **1D Discrete Fourier Transform (DFT)** of our time-series data, and using the `fftfreq` function, we will get the **sampling frequencies for the DFT**.\n- Now, in order to integrate the Fourier Transform based features, we will perform the following steps:\n    - We will consider the data month-wise and cluster-wise.\n    - For each such dataset, we will find the DFT and the sampling frequencies of the DFT.\n    - Now, we will perform **peak detection** on the DFT, using which, we will get the #peaks and the indices of the peaks. As a threshold, we will be using the **Minimum Peak Height (mph)** as **1000**.\n    - So, the first feature will be the number of such peaks.\n    - Now, using the indices of the peaks, we will find the corresponding sampling frequencies, and we will use the sampling frequencies corresponding to the highest 3 peaks as 3 of the features.\n    - If for some dataset, we have less than 3 peaks, then, we will simply put 0 for those frequencies.","4c5fd403":"## 1.6. Total Fare","c9589c8c":"### 2.4.1. Visualizing Detection of Peaks\n- Reference: https:\/\/pyroomacoustics.readthedocs.io\/en\/pypi-release\/pyroomacoustics.doa.detect_peaks.html\n- In the below code cell, we have visualized the detection of peaks for cluster indexed as 0, for the months of Jan-Mar 2016.\n- The detected peaks indicate the time-bins for which, we got the maximum pickups, and at the same time, they also detect the maximum pickups for a given cluster.","8db8da97":"## 4.3. Calculating the Error Metric Values for the Various Models","3453aff7":"## 1.1. Pickup Latitude and Pickup Longitude\n- It is inferred from the source https:\/\/www.flickr.com\/places\/info\/2459115 that New York is bounded by the location cordinates (lat,long) - (40.5774, -74.15) & (40.9176,-73.7004)\n- Hence, any cordinates not within this cordinate range, are not considered by us, as we are only concerned with pickups which originate within New York.","0e1502de":"## 2.2. Time Binning\n- Reference for Unix Time-Stamp: https:\/\/www.unixtimestamp.com\/\n| Unix Time Stamp | YYYY-MM-DD HH:MM:SS |\n| --- | --- |\n| 1420070400 | 2015-01-01 00:00:00 | \n| 1422748800 | 2015-02-01 00:00:00 | \n| 1425168000 | 2015-03-01 00:00:00 |\n| 1427846400 | 2015-04-01 00:00:00 | \n| 1430438400 | 2015-05-01 00:00:00 | \n| 1433116800 | 2015-06-01 00:00:00 |\n| 1451606400 | 2016-01-01 00:00:00 | \n| 1454284800 | 2016-02-01 00:00:00 | \n| 1456790400 | 2016-03-01 00:00:00 |\n| 1459468800 | 2016-04-01 00:00:00 | \n| 1462060800 | 2016-05-01 00:00:00 | \n| 1464739200 | 2016-06-01 00:00:00 |\n\n- Reference for EST: https:\/\/www.timeanddate.com\/time\/zones\/est\n- **(int((i-start_pickup_unix)\/600) + 33)**: The unix time is in GMT\/UTC, so, we are converting it to EST (Eastern Standard Time)","912abf8c":"## 3.3. Exponential  Weighted Moving Averages\n- Reference: https:\/\/en.wikipedia.org\/wiki\/Moving_average#Exponential_moving_average\n- Through weighted averaged we have satisfied the analogy of giving higher weights to the latest value and decreasing weights to the subsequent ones, but we still do not know which is the correct weighting scheme as there are infinitely many possibilities in which we can assign weights in a non-increasing order and tune the hyperparameter window-size. \n- To simplify this process, we use **Exponential Moving Averages** which is a more logical way towards assigning weights and at the same time, also using an optimal window-size.\n- In exponential moving averages, we use a single hyperparameter alpha $\\begin{align}(\\alpha)\\end{align}$ which is a value between 0 & 1 and based on the value of the hyperparameter alpha the weights and the window sizes are configured.\n- For eg. If $\\begin{align}\\alpha=0.9\\end{align}$ then the number of days on which the value of the current iteration is based is ~ $\\begin{align}1\/(1-\\alpha)=10\\end{align}$ i.e. we consider values 10 days prior before we predict the value for the current iteration. \n- Also, the weights are assigned using $\\begin{align}2\/(N+1)=0.18\\end{align}$, where N = number of prior values being considered, hence from this it is implied that the first or latest value is assigned a weight of 0.18 which keeps exponentially decreasing for the subsequent values.\n\n\n$\\begin{align}R^{'}_{t} = \\alpha*R_{t-1} + (1-\\alpha)*R^{'}_{t-1}  \\end{align}$","e6022343":"#### Observations:\n- As even the 99.9th percentile value doesnt look like an outlier, as there is not much difference between the 99.8th percentile and 99.9th percentile, we move on to do graphical analyis from the numerical analysis, we have been doing so far.","9679cde8":"#### Observations:\n- The avg speed in Newyork speed is **12.45 miles\/hr**, so a cab driver can travel 2 miles per 10 min on an average.\n- We will be using this observation in determining the time-bins and the regions. \n\n## 1.5. Trip Distance","068072c5":"## 2.4. Time series and Fourier Transforms\n- Getting Peaks: https:\/\/blog.ytotech.com\/2015\/11\/01\/findpeaks-in-python\/\n- FFT Function: https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.fft.fft.html\n- FFTfreq: https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.fft.fftfreq.html ","a999146a":"- We have added 2 more columns `pickup_cluster` (to which cluster it belongs to) and `pickup_bins` (to which 10 min interval the trip belongs to)\n- Here, the `pickups` represents the number of pickups that happened in that particular 10-min interval. \n- This data frame has two indices\n    - Primary Index: pickup_cluster (Cluster Number)\n    - Secondary Index : pickup_bins (We have divided the entire month's time into 10-min intervals 31\\*24\\*60\/10 = 4464bins)\n- Upto now, we have cleaned and prepared the data for Jan 2015. Now, we will do the same operations for months Jan, Feb, March of 2016\n1. Get the dataframe which includes only required colums\n2. Adding trip times, speed, unix time-stamp of pickup_time\n4. Remove the outliers based on trip_times, speed, trip_duration, total_amount\n5. Add pickup_cluster to each data point\n6. Add pickup_bin (index of 10-min interval to which that trip belongs to)\n7. Group by data, based on 'pickup_cluster' and 'pickup_bin'","7d9bb532":"#### Observations:\n- As can be seen from the above QQ-Plot, the `trip_times`, do not follow a log-normal distribution completely.\n- From [-3 Std-Dev, +3 Std-Dev], it is more or less a log-normal distribution, but beyond that, the `trip_times` differ from the log-normal distribution to some extent.\n\n## 1.4. Speed","c1650a10":"#### Observations:\n- The observations here are similar to those obtained while analysing pickup latitude and longitude\n\n## 1.3. Trip Durations:\n- According to NYC Taxi & Limousine Commision Regulations, the maximum allowed trip duration in a 24 hour interval is 12 hours.","c7a0a574":"# 2. Data Preparation\n## 2.1. Clustering\/Segmentation\n- While clustering, the inter-cluster distance for considering 2 clusters to be in the vicinity of each other is set as **2 miles**.\n- In section 1.4, we observed that on an average, a cabbie can cover 2 miles in approximately 10 mins, and since, 2 miles is an acceptable distance to find more #pickups, hence, we have set the 2 miles threshold.\n- We need to choose number of clusters so that, there are more number of cluster regions that are close to any cluster center and make sure that the minimum inter-cluster distance should not be very less.","f7e94448":"#### Now, there are two ways to fill up these values:\n- Fill the missing value with 0's\n- Fill the missing values with the avg values\n    - Case 1: (Values missing at the start)\n        - _ _ _ x => ceil(x\/4), ceil(x\/4), ceil(x\/4), ceil(x\/4) \n        - _ _ x => ceil(x\/3), ceil(x\/3), ceil(x\/3)\n    - Case 2: (Values missing in middle) \n        - x _ _ y => ceil((x+y)\/4), ceil((x+y)\/4), ceil((x+y)\/4), ceil((x+y)\/4) \n        - x _ _ _ y => ceil((x+y)\/5), ceil((x+y)\/5), ceil((x+y)\/5), ceil((x+y)\/5), ceil((x+y)\/5)\n    - Case 3: (Values missing at the end)  \n        - x _ _ _  => ceil(x\/4), ceil(x\/4), ceil(x\/4), ceil(x\/4) \n        - x _  => ceil(x\/2), ceil(x\/2)","6f9e1d8a":"## 3.2. Weighted Moving Averages\n- The Moving Avergaes Model used above gave equal importance to all the values in the window used, but we know intuitively that the future is more likely to be similar to the latest values and less similar to the older values. \n- Weighted Averages converts this analogy into a mathematical relationship giving the highest weight while computing the averages to the latest previous value and decreasing weights to the subsequent older ones.\n- Weighted Moving Averages using Ratio Values: $\\begin{align}R_{t} = ( N*R_{t-1} + (N-1)*R_{t-2} + (N-2)*R_{t-3} .... 1*R_{t-n} )\/(N*(N+1)\/2) \\end{align}$\n- In the below code cells, the window-size (n) is a hyper-parameter, which is tuned manually.\n- It is found that the window-size of 5 is optimal for getting the best results using Weighted Moving Averages with previous Ratio values, therefore, we get $\\begin{align} R_{t} = ( 5*R_{t-1} + 4*R_{t-2} + 3*R_{t-3} + 2*R_{t-4} + R_{t-5} )\/15 \\end{align}$","6e810675":"- Getting the predictions of EWMA to be used as a feature in cumulative form.\n- Upto now we computed 8 features for every data-point that starts from 50th-min of the day\n    1. Cluster center lattitude\n    2. Cluster center longitude\n    3. Day of the week \n    4. f_t_1: #pickups that have happened in the previous (t-1)th 10-min interval\n    5. f_t_2: #pickups that have happened in the previous (t-2)th 10-min interval\n    6. f_t_3: #pickups that have happened in the previous (t-3)th 10-min interval\n    7. f_t_4: #pickups that have happened in the previous (t-4)th 10-min interval\n    8. f_t_5: #pickups that have happened in the previous (t-5)th 10-min interval","87a1d3dd":"# Installing & Importing Packages","27d1c7d2":"#### Observations:\n- In the above code cells, we can see how we have incorporrated the information conveyed by the geographical location and the time-bin, into 9 features of our dataset.\n- So, now, if we are given a test-point, we will first consider the coordinates, which we will pass to our MiniBatchKMeans model, which will predict the cluster for these coordinates.\n- Using the predicted cluster, we will find out the coordinates of the predicted cluster's center, since, we have the coordinates of each of the cluster's center (pre-computed, while training).\n- The cluster's center's latitude and longitude will form up 2 of our 9 features.\n- Now, since, we have the time-bin, we can easily calculate the weekday. Also, since, we would have the pickup density of the last 5 time-bins for the given cluster, another 5 features will be obtained.\n- And in a similar fashion, we would have the `exp_avg`, which will be computed based-off the pickup density of the previous time-bins.\n- In this way, using just the time-bins and the geographical location of the cabbie, we can find out the 9 features, using which, we can predict the pickup density for the given region and time-bin.\n\n## 4.2. Modelling using Regression Models\n### 4.2.1. Linear Regression Model","166a0f8f":"## 3.1. Simple Moving Averages\n- The first model used is the **Moving Averages Model** which uses the previous 'n' values in order to predict the next value\n- Using Ratio Values - $\\begin{align}R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3} .... R_{t-n} )\/n \\end{align}$\n- In the below code cells, the window-size (n) is a hyper-parameter, which is tuned manually.\n- It is found that the window-size of 3 is optimal for getting the best results using Moving Averages with previous ratio values, therefore, we get $\\begin{align}R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3})\/3 \\end{align}$","4839c530":"# ML Problem Formulation\n### Time-series Forecasting and Regression\n- To find number of pickups, given location cordinates (latitude and longitude) and time, in the query region and surrounding regions.\n-  To solve the above problem, we would be using data collected in Jan 2015 to predict the pickups in Jan-Mar 2016.\n\n# Performance metrics\n1. Mean Absolute percentage error (MAPE)\n2. Mean Squared error (MSE)\n\n### MAPE\n- Reference: https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_percentage_error\n- In this case-study, we are using an alternate version of MAPE, which accounts for the cases, when the absolute value *(i.e., the denominator value)* is 0.\n- In the alternate version, instead of using the absolute value in the denominator, we use the **average of the absolute values** in the denominator, and since, this is a constant, hence, we can take this quantity out from the summation.\n\n# 1. Data Cleaning\n- In this section we will be doing univariate analysis and removing outlier\/illegitimate values which may be caused due to some error.\n- We will do the entire univariate analysis on Jan 2015 data, and whatever pre-processing we do based on the insights, will be applied to the data from all the 4 months.","ac4af353":"#### Why we choose these methods and which method will be used for which data?\n\n- Consider we have data of some time-bin in 1st Jan 2015, 10 _ _ _ 20, i.e., 10 pickups happened in 0th 10-min interval, 0 pickup happened in 1st, 2nd & 3rd 10-min interval, and 20 pickups happened in 4th 10-min interval\n- In ***fill_missing*** method, we replace these values like 10, 0, 0, 0, 20, where as in ***smoothing*** method, we replace these values as 6, 6, 6, 6, 6.\n- If you can check the #pickups that happened in the first 50-min, they are same in both the cases, but if you can observe, we are looking at the future values when we are using smoothing, i.e., we are looking at the future #pickups,  which might cause a data leakage.\n- So, we use smoothing for Jan 2015 data, since it acts as our training data, and we use simple fill_misssing method for 2016 data.","cbf95969":"## 1.7. Remove all outlier\/erroneous points\n- Based upon the numerical\/graphical univariate analysis, we performed with the *pickup coordinates, dropoff coordinates, trip durations, trip distances, speeds and total fares*, we will make a function to apply all those pre-processing steps.","5242ca7a":"#### Observations:\n- I integrated the above 4 features in my dataset, and tested it with the Linear Regression, Random Forest and the XgBoost Regression models, but the **MAPE only increased** (lesser is good).\n- Hence, in the final version of this notebook, I have **left out** the Fourier Transform Based features, however, if you would like to play with them, or use them in another way, you just need to uncomment a few lines of code, and **Voila**!\n- One of those code cells is present in the section **4.1.1.**, and the only remaining line of code that you need to uncomment is present in the section **4.1.2.**\n\n# 3. Modelling: Baseline Models\nNow we get into modelling, in order to forecast the pickup densities for the months of Jan, Feb and March of 2016 for which we are using multiple models with two variations \n1. Using Ratios of the 2016 data to the 2015 data i.e., $\\begin{align} R_{t} = P^{2016}_{t} \/ P^{2015}_{t} \\end{align}$\n2. Using previous known values of the 2016 data itself to predict the future values","2bb0cfbc":"#### Observations:\n- As you can see above that there are some points just outside the boundary but there are a few that are in either South america, Mexico or Canada.\n\n## 1.2. Dropoff Latitude & Dropoff Longitude\n- It is inferred from the source https:\/\/www.flickr.com\/places\/info\/2459115 that New York is bounded by the location cordinates (lat,long) - (40.5774, -74.15) & (40.9176,-73.7004)\n- Hence, any cordinates not within this cordinate range, are not considered by us, as we are only concerned with dropoffs which are within New York.","8950ba47":"### 2.1.2. Plotting the Clusters","3db3ee35":"#### Inference:\n- The main objective was to find a optimal min. distance (which roughly estimates to the radius of a cluster) between the clusters, and we got it as 40.\n- If we check for 50 clusters, we can observe that there are two clusters which are only 0.3 miles apart from each other, so, we choose 40 clusters to solve the further problems.","39e86519":"- Weighted Moving Averages using Previous 2016 Values: $\\begin{align}P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )\/(N*(N+1)\/2) \\end{align}$\n- Once again, the window-size (n) is a hyper-parameter, which is tuned manually.\n- And it is found that the window-size of 2 is optimal for getting the best results using Weighted Moving Averages with previous 2016 values, therefore, we get $\\begin{align} P_{t} = ( 2*P_{t-1} + P_{t-2} )\/3 \\end{align}$","a6f9058d":"### 4.1.2. Performing train-test split","d846b3a1":"- **Note**: The above comparisons are made using Jan 2015 and Jan 2016 data only\n- From the above results, we can infer that the best forecasting model for our predictions would be:\n$\\begin{align}P^{'}_{t} = \\alpha*P_{t-1} + (1-\\alpha)*P^{'}_{t-1}  \\end{align}$ i.e **Exponential Weighted Moving Averages using 2016 Values**.\n\n# 4. Regression Models\n## 4.1. Preparing Data & Performing Train-Test Split\n- Before we start predictions using the tree-based & other regression models, we take 3 months of 2016 pickup data and split it such that, for every region we have 70% data in train and 30% in test, ordered date-wise for every region.\n- The below code cells prepares data in cumulative form which will be later split into test and train sets.\n\n### 4.1.1. Preparing Data"}}