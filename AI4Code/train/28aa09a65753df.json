{"cell_type":{"f04db6c0":"code","5973d161":"code","f75d4d1f":"code","c385ee41":"code","66ed8dfd":"code","8ec76f9a":"code","5315853e":"code","6cdc1457":"code","bab46826":"code","4f835608":"code","48a29aee":"code","58164e34":"code","26111966":"code","ba01cba2":"code","ad360422":"code","13c1c15d":"code","930aec00":"code","757ae1f8":"code","1ebfcf8e":"code","5f5eaa33":"code","a63c521f":"code","e85a9e29":"code","9ba2da20":"code","3f411415":"code","ee1dcbae":"code","25981cae":"code","0670a387":"code","d5521f9d":"code","9c307d27":"code","a9535a1b":"code","e2d84c60":"code","9a2e6f9e":"code","517dd83d":"code","e8c626d2":"code","a3c00a7f":"code","2171901d":"code","75198c48":"code","918054eb":"code","13b2278d":"code","df3e5f32":"code","8ed4a24f":"code","a6faea20":"code","42bbf440":"code","99b8abe2":"code","6f4f5187":"code","221bbe35":"code","d7d0278c":"code","12c8452a":"markdown","7030333e":"markdown","f1f582ed":"markdown","53e3d18c":"markdown","387e7d9d":"markdown","c5f4bbe8":"markdown","3a960ec9":"markdown","9bfa1854":"markdown","3fec7e3e":"markdown","0723dabe":"markdown","3b92eca9":"markdown","6015df4e":"markdown","4093c189":"markdown","171b947e":"markdown","40d9a424":"markdown","2eea1f95":"markdown","934c70c6":"markdown","948d54ed":"markdown","c111ea3e":"markdown","6a3def80":"markdown","6ce258a0":"markdown","bf53b885":"markdown","a1c5026b":"markdown","2d9fdf28":"markdown","24a0c859":"markdown","4cbd96b7":"markdown","f7470ae2":"markdown","82373d3e":"markdown","75723def":"markdown","99238161":"markdown","b8600a42":"markdown","5e283377":"markdown","ba6bfbad":"markdown","5394c47c":"markdown","ec8e7387":"markdown","cd3fbb42":"markdown","1a17842d":"markdown","31600fc4":"markdown","651a025c":"markdown","aa121359":"markdown","35ef80a9":"markdown","128535b9":"markdown","aa7194a0":"markdown","9473248e":"markdown","fe438d6d":"markdown","8ef2d534":"markdown","0ee156b9":"markdown","0440c3bc":"markdown"},"source":{"f04db6c0":"import sys\nimport io\nimport pdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport xgboost as xgb\n#from google.colab import files\nimport re\n","5973d161":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ncommon_path = '..\/input\/titanic'\ntrain_data = pd.read_csv(common_path+'\/train.csv')\ntest_data = pd.read_csv(common_path+'\/test.csv')","f75d4d1f":"print(train_data.head())\nprint(test_data.head())\n\n#print(train_data.shape)\n#print(train_data.tail())\n#print(train_data.columns)\n#print(train_data.describe())\nprint(train_data.info())\n#print(train_data.isnull().sum())\n\nprint(test_data.info())","c385ee41":"var='Survived' #indica se a pessoa sobreviveu ou n\u00e3o\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 62% das pessoas morreram\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\n\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xticklabels(labels=['Died', 'Survived'])","66ed8dfd":"var='Sex'\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 65% das pessoas eram homens\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_xlabel('Sex')\nax.set_ylabel('Number of People')","8ec76f9a":"var='Age'\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\n#print('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, maior concentra\u00e7\u00e3o de pessoas com idades entre 20 e 40 anos\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nprint(train_data[var].describe(), '\\n')\nax=train_data[var].hist(bins=30)\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Age')\n\nprint('Porcentagem de pessoas com menos de 15 anos (crian\u00e7as?): ', len(train_data[train_data['Age']<15])\/len(train_data)*100, len(train_data[train_data['Age']<15]))\nprint('Porcentagem de pessoas com menos de 18 anos: ', len(train_data[train_data['Age']<18])\/len(train_data)*100, len(train_data[train_data['Age']<18]))\nprint('Porcentagem de pessoas do sexo masculino com menos de 18 anos: ', len(train_data[(train_data['Age']<18) & (train_data['Sex']=='male')])\/len(train_data)*100, len(train_data[(train_data['Age']<18) & (train_data['Sex']=='male')]))\nprint('Porcentagem de pessoas do sexo feminino com menos de 18 anos: ', len(train_data[(train_data['Age']<18) & (train_data['Sex']=='female')])\/len(train_data)*100, len(train_data[(train_data['Age']<18) & (train_data['Sex']=='female')]))\nprint('Porcentagem de pessoas entre 18 e 40 anos: ', len(train_data[(train_data['Age']>=18) & (train_data['Age']<=40)])\/len(train_data)*100, len(train_data[(train_data['Age']>=18) & (train_data['Age']<=40)]))\nprint('Porcentagem de pessoas entre 41 e 65 anos: ', len(train_data[(train_data['Age']>=41) & (train_data['Age']<=65)])\/len(train_data)*100, len(train_data[(train_data['Age']>=41) & (train_data['Age']<=65)]))\nprint('Porcentagem de pessoas com mais de 65 anos: ', len(train_data[(train_data['Age']>=66)])\/len(train_data)*100, len(train_data[(train_data['Age']>=66)]))","5315853e":"var='Pclass' #indica a classe no navio (1, 2 e 3)\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 55% das pessoas ocuparam a classe 3 (mais barata), 25% a classe 2 (intermedi\u00e1ria) e 20% a primeira classe (mais ricos)\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Class')\nax.tick_params(axis='x', labelrotation=0)","6cdc1457":"var='Fare' #indica o valor da passagem\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde a maior parte das pessoas pagaram entre 0 e 100. Feature variando entre 0 e 512.\n#print('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nprint(train_data[var].describe(), '\\n')\n\nax=train_data[var].hist(bins=50)\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Fare')\n\nprint('Porcentagem de pessoas que pagaram at\u00e9 50: ', len(train_data[train_data['Fare']<50])\/len(train_data)*100)\nprint('Porcentagem de pessoas que pagaram at\u00e9 100: ', len(train_data[train_data['Fare']<100])\/len(train_data)*100)\nprint('\\n')\n\nprint('Valores de passagens para classe 3:\\n', train_data['Fare'][train_data['Pclass']==3].describe(), '\\n')\nprint('Valores de passagens para classe 2:\\n',train_data['Fare'][train_data['Pclass']==2].describe(), '\\n')\nprint('Valores de passagens para classe 1:\\n',train_data['Fare'][train_data['Pclass']==1].describe(), '\\n')","bab46826":"var='Parch' #indica com quantos acompanhantes a pessoa estava viajando, sendo eles pais ou filhos\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 76% das pessoas estavam viajando sem os pais ou filhos\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Number of Parents and Children')\nax.tick_params(axis='x', labelrotation=0)","4f835608":"var='SibSp' #indica com quantos acompanhantes a pessoa estava viajando, sendo eles irm\u00e3os ou c\u00f4njuge\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 78% das pessoas estavam viajando sem irm\u00e3os ou c\u00f4njuge\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Number of Siblings and Spouse')\nax.tick_params(axis='x', labelrotation=0)","48a29aee":"train_data['Parch_SibSp']=train_data['Parch']+train_data['SibSp']\ntrain_data['Parch_SibSp'].describe()\n\nvar='Parch_SibSp' #indica com quantos acompanhantes a pessoa estava viajando\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 60% das pessoas estavam viajando sozinhas\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\n\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Number of Companions')\nax.tick_params(axis='x', labelrotation=0)\n\nprint('Total de pessoas com um ou mais acompanhantes: ', len(train_data[(train_data['Parch_SibSp']>0)]))\nprint('Total de pessoas com mais de um acompanhante, sendo pai ou filho e irm\u00e3o ou c\u00f4njuge: ', len(train_data[(train_data['Parch']>0) & (train_data['SibSp']>0)]), len(train_data[(train_data['Parch']>0) & (train_data['SibSp']>0)])\/len(train_data)*100)","58164e34":"var='Embarked' #indica com quantos acompanhantes a pessoa estava viajando, sendo eles irm\u00e3os ou c\u00f4njuge\nprint('Total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts()))\nprint('Porcentagem total de valores da feature %s:\\n%s\\n' % (var, train_data[var].value_counts(normalize=True))) #feature com valores desbalanceados, onde 78% das pessoas estavam viajando sem irm\u00e3os ou c\u00f4njuge\nprint('Classes de valores \u00fanicos da feature %s:\\n%s\\n'  % (var, train_data[var].unique())) #descri\u00e7\u00e3o dos valores \u00fanicos da feature\nax=train_data[var].value_counts().plot.bar()\nax.set_title('Distribui\u00e7\u00e3o de valores para a feature '+var)\nax.set_ylabel('Number of People')\nax.set_xlabel('Boarding Place')\nax.tick_params(axis='x', labelrotation=0)","26111966":"#groupby\ndisplay(train_data.groupby(['Sex', 'Survived', 'Pclass']).mean())","ba01cba2":"sns.set()\nplt.figure(figsize=(10, 8))\nsns.boxplot(y='Age', x='Sex', data=train_data)\nplt.show()\n\nplt.figure(figsize=(10, 8))\nsns.violinplot(y='Age', x='Sex', data=train_data)\nplt.show()","ad360422":"sns.set()\nplt.figure(figsize=(10, 8))\nsns.boxplot(y='Age', x='Survived', data=train_data)","13c1c15d":"display(pd.crosstab([train_data['Sex']], train_data['Survived']))\n\ndisplay(pd.crosstab([train_data['Sex']], train_data['Survived'], normalize='index'))","930aec00":"pd.crosstab([train_data['Sex']], train_data['Survived']).plot.bar()\nplt.ylabel('Number of People')","757ae1f8":"#display(pd.crosstab([train_data['Sex'], train_data['Pclass']], train_data['Survived'], values=train_data['Age'], aggfunc='mean'))\n\ndisplay(pd.crosstab([train_data['Sex'], train_data['Pclass']], train_data['Survived']))\n\npd.crosstab([train_data['Sex'], train_data['Survived']], train_data['Pclass']).plot.bar()\nplt.ylabel('Number of People')\nplt.tick_params(axis='x', labelrotation=45)\nplt.show()","1ebfcf8e":"sns.set()\nplt.figure(figsize=(10, 8))\nsns.boxplot(y='Fare', x='Pclass', data=train_data)","5f5eaa33":"##Homens acompanhados de um ou mais pais ou filhos versus sobreviv\u00eancia\ntrain_data[(train_data['Sex']=='male') & (train_data['Parch']>0)]['Survived'].value_counts(normalize=False).plot.bar(label='With Parch')\ntrain_data[(train_data['Sex']=='male') & (train_data['Parch']==0)]['Survived'].value_counts(normalize=False).plot.bar(alpha=0.8, color='darkorange', width=0.3, label='WO Parch')\nplt.xlabel('Survived')\nplt.ylabel('Number of Men - Feature Parch')\nplt.legend()\nplt.tick_params(axis='x', labelrotation=0)\nplt.show()\n\n##Homens acompanhados de um ou mais irm\u00e3os ou conjuge versus sobreviv\u00eancia\ntrain_data[(train_data['Sex']=='male') & (train_data['SibSp']>0)]['Survived'].value_counts(normalize=False).plot.bar(label='With SibSp')\ntrain_data[(train_data['Sex']=='male') & (train_data['SibSp']==0)]['Survived'].value_counts(normalize=False).plot.bar(alpha=0.8, color='darkorange', width=0.3, label='WO SibSp')\nplt.xlabel('Survived')\nplt.ylabel('Number of Men - Feature SibSp')\nplt.legend()\nplt.tick_params(axis='x', labelrotation=0)\nplt.show()\n\n##Homens acompanhados de um ou mais pais, filhos, irm\u00e3os ou conjuge versus sobreviv\u00eancia\ntrain_data[(train_data['Sex']=='male') & (train_data['Parch_SibSp']>0)]['Survived'].value_counts(normalize=False).plot.bar(label='With Parch_SibSp')\ntrain_data[(train_data['Sex']=='male') & (train_data['Parch_SibSp']==0)]['Survived'].value_counts(normalize=False).plot.bar(alpha=0.8, color='darkorange', width=0.3, label='WO Parch_SibSp')\nplt.xlabel('Survived')\nplt.ylabel('Number of Men - Feature Parch_SibSp')\nplt.legend()\nplt.tick_params(axis='x', labelrotation=0)\nplt.show()\n\n##Homens da primeira classe acompanhados de um ou mais pais, filhos, irm\u00e3os ou conjuge versus sobreviv\u00eancia\ntrain_data[(train_data['Sex']=='male') & (train_data['Pclass']==1) & (train_data['Parch_SibSp']>0)]['Survived'].value_counts(normalize=False).plot.bar(label='With Parch_SibSp')\ntrain_data[(train_data['Sex']=='male') & (train_data['Pclass']==1) & (train_data['Parch_SibSp']==0)]['Survived'].value_counts(normalize=False).plot.bar(alpha=0.8, color='darkorange', width=0.3, label='WO Parch_SibSp')\nplt.xlabel('Survived')\nplt.ylabel('Number of 1st Class Men - Feature Parch_SibSp')\nplt.legend()\nplt.tick_params(axis='x', labelrotation=0)\nplt.show()","a63c521f":"corr=train_data.iloc[:, 1:].corr()\ndisplay(corr)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","e85a9e29":"##Transformation of nominal features to numerical features\ndef sex_to_bin(val):\n    if val=='female':\n        return 1\n    else:\n        return 0\n\ndef embarked_to_num(val):\n    if val=='S':\n        return 0\n    elif val=='C':\n        return 1\n    elif val=='Q':\n        return 2\n\ntrain_data['Sex_bin']=train_data['Sex'].map(sex_to_bin)\ntest_data['Sex_bin']=test_data['Sex'].map(sex_to_bin)\n\ntrain_data['Embarked_num']=train_data['Embarked'].map(embarked_to_num)\ntest_data['Embarked_num']=test_data['Embarked'].map(embarked_to_num)\n\n#Booleans\ntrain_data['Cabin_null']=train_data['Cabin'].isnull().astype(int) \ntest_data['Cabin_null']=test_data['Cabin'].isnull().astype(int)\n\n#train_data['Embarked_S']=(train_data['Embarked']=='S').astype(int) \n#test_data['Embarked_S']=(test_data['Embarked']=='S').astype(int)\n\n#train_data['Embarked_C']=(train_data['Embarked']=='C').astype(int) \n#test_data['Embarked_C']=(test_data['Embarked']=='C').astype(int) ","9ba2da20":"##titles of names\nnames=train_data['Name'].unique()\nname_re = re.compile('\\w+\\.')\n#name_pref=list(set(name_re.findall(str(names)))) #remove duplicated strings\nname_pref=[name_re.findall(str(n)) for n in names]\ndict_name_pref={}\nfor n in name_pref:\n  if n[0] in dict_name_pref:\n    dict_name_pref[n[0]]+=1\n  else:\n    dict_name_pref[n[0]]=1\nname_pref=sorted(dict_name_pref.items(), key=lambda x: x[1], reverse=True)\nprefs=[]\nfor n in name_pref:\n  if n[1]>1:\n    prefs.append(n)\nprint(prefs)\n##[(' Mr. ', 517), (' Miss. ', 182), (' Mrs. ', 125), (' Master. ', 40), (' Dr. ', 7), (' Rev. ', 6), (' Major. ', 2), (' Mlle. ', 2), (' Col. ', 2)]\n\ntrain_data['Name_have_Miss']=train_data['Name'].str.contains(' Miss. ').astype(int)\ntrain_data['Name_have_Mrs']=train_data['Name'].str.contains(' Mrs. ').astype(int)\n\ntrain_data['Name_have_Mr']=train_data['Name'].str.contains(' Mr. ').astype(int)\ntrain_data['Name_have_Master']=train_data['Name'].str.contains(' Master. ').astype(int)\ntrain_data['Name_have_Dr']=train_data['Name'].str.contains(' Dr. ').astype(int)\ntrain_data['Name_have_Rev']=train_data['Name'].str.contains(' Rev. ').astype(int)\n\ntest_data['Name_have_Miss']=test_data['Name'].str.contains(' Miss. ').astype(int)\ntest_data['Name_have_Mrs']=test_data['Name'].str.contains(' Mrs. ').astype(int)\n\ntest_data['Name_have_Mr']=test_data['Name'].str.contains(' Mr. ').astype(int)\ntest_data['Name_have_Master']=test_data['Name'].str.contains(' Master. ').astype(int)\ntest_data['Name_have_Dr']=test_data['Name'].str.contains(' Dr. ').astype(int)\ntest_data['Name_have_Rev']=test_data['Name'].str.contains(' Rev. ').astype(int)","3f411415":"corr=train_data.iloc[:, 1:].corr()\ndisplay(corr)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","ee1dcbae":"print(train_data.columns)\n\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_bin', 'Embarked_num', 'Cabin_null', 'Name_have_Miss', 'Name_have_Mrs', 'Name_have_Mr', 'Name_have_Master', 'Name_have_Dr', \n             'Name_have_Rev']","25981cae":"X = train_data[features]\ny = train_data['Survived']\ndisplay(X.head())\ndisplay(y.head())","0670a387":"X_test=test_data[features]\ndisplay(X_test.head())","d5521f9d":"#train\nmeds=X['Age'].median()\nX['Age']=X['Age'].fillna(meds)\nX['Embarked_num']=X['Embarked_num'].fillna(X['Embarked_num'].value_counts().index[0])\n\n#test\nX_test['Age']=X_test['Age'].fillna(meds)\nX_test['Embarked_num']=X_test['Embarked_num'].fillna(X['Embarked_num'].value_counts().index[0])\nX_test['Fare']=X_test['Fare'].fillna(-1)\n","9c307d27":"#np.random.seed(0)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.5, stratify=y, random_state=376) #test_size=0.3, 0.2\nprint(train_X.head())","a9535a1b":"##modelo dummy 1\nmodelo_dummy = DummyClassifier('most_frequent', random_state=376)\nmodelo_dummy.fit(train_X, train_y) #treino\nprevisao_dummy = modelo_dummy.predict(val_X) #predi\u00e7\u00e3o\naccuracy_score(val_y, previsao_dummy) #c\u00e1lculo de acur\u00e1cia","e2d84c60":"##modelo dummy 2 (with only women and children)\ncontrol_predictions=((val_X['Sex_bin']==1) | (val_X['Age']<15)).astype(np.int64)\naccuracy_score(val_y, control_predictions)","9a2e6f9e":"##modelo dummy 3\ncontrol_predictions=((val_X['Sex_bin']==1) | (val_X['Age']<15) | ((val_X['Sex_bin']==0) & (val_X['Pclass']==1) & (val_X['Parch']==0) & (val_X['SibSp']==0))).astype(np.int64)\naccuracy_score(val_y, control_predictions)","517dd83d":"##modelo de regress\u00e3o log\u00edstica\nmodelo_rlogistica = LogisticRegression(max_iter=1000, random_state=376)\nmodelo_rlogistica.fit(train_X, train_y) #treino\nmodelo_rlogistica.score(val_X, val_y) #predi\u00e7\u00e3o e c\u00e1lculo de acur\u00e1cia","e8c626d2":"##modelo de \u00e1rvore de decis\u00e3o\nmodelo_arvore = DecisionTreeClassifier(max_depth = 3, random_state=376)\nmodelo_arvore.fit(train_X, train_y)\nmodelo_arvore.score(val_X, val_y)","a3c00a7f":"fig, ax = plt.subplots(figsize=(15, 10), facecolor='k')\ntree.plot_tree(modelo_arvore,\n               ax=ax,\n               fontsize=10,\n               rounded=True,\n               filled=True,\n               feature_names=train_X.columns,\n               class_names=['Not survived', 'Survived'])\n\nplt.show()","2171901d":"train=[]\nvalid=[]\nfor i in range(1,30):\n    modelo_arvore = DecisionTreeClassifier(max_depth = i, random_state=376)\n    modelo_arvore.fit(train_X, train_y)\n    train.append(modelo_arvore.score(train_X, train_y))\n    valid.append(modelo_arvore.score(val_X, val_y))\n  \n#compara\u00e7\u00e3o de acur\u00e1cia\nfor i in range(len(train)): \n  print('max_depth:', i+1, 'score treino:', train[i], 'score valida\u00e7\u00e3o:', valid[i])","75198c48":"plt.figure(figsize=(12, 8))\nax=sns.lineplot(x=range(1,30), y = train, label='treino')\nax=sns.lineplot(x=range(1,30), y = valid, label='valida\u00e7\u00e3o')\nax.set(xlabel='max_depth', ylabel='Score')","918054eb":"modelo_arvore = DecisionTreeClassifier(max_depth = 14, random_state=376)\nmodelo_arvore.fit(train_X, train_y)\nmodelo_arvore.score(val_X, val_y)","13b2278d":"#rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=376)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=376)\nrf_model.fit(train_X, train_y)\nrf_model.score(val_X, val_y)","df3e5f32":"xgb_model = xgb.XGBClassifier(\n #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1,\n random_state=376).fit(train_X, train_y)\n\nxgb_model.score(val_X, val_y)","8ed4a24f":"list_acc=[]\n\nkf=RepeatedKFold(n_splits=2, n_repeats=10, random_state=376)\n\nfor l_train, l_val in kf.split(X):\n  #print(l_train.shape[0], l_val.shape[0])\n  train_X, val_X = X.iloc[l_train], X.iloc[l_val]\n  train_y, val_y = y.iloc[l_train], y.iloc[l_val]\n\n  ##train\n  #ml_model = LogisticRegression(max_iter=1000, random_state=376)\n\n  ml_model = xgb.XGBClassifier(\n  #learning_rate = 0.02,\n  n_estimators= 2000,\n  max_depth= 4,\n  min_child_weight= 2,\n  #gamma=1,\n  gamma=0.9,                        \n  subsample=0.8,\n  colsample_bytree=0.8,\n  objective= 'binary:logistic',\n  nthread= -1,\n  scale_pos_weight=1,\n  random_state=376)\n\n  ml_model.fit(train_X, train_y)\n\n  ##test and score\n  rf_val_predictions = ml_model.predict(val_X)\n  acc=ml_model.score(val_X, val_y)\n  list_acc.append(acc)\n\n#print(np.mean(list_acc), np.min(list_acc), np.max(list_acc))","a6faea20":"%matplotlib inline\n%pylab inline\npylab.hist(list_acc)","42bbf440":"sns.boxplot(y=list_acc)","99b8abe2":"val_X_check = train_data.iloc[l_val].copy()\nval_X_check['Prediction'] = rf_val_predictions\n#val_X_check.head()\n\nX_errors=val_X_check[val_X_check['Survived']!=val_X_check['Prediction']] #show only the errors between prediciton and expected output\nX_errors=X_errors[features+['Prediction', 'Survived']]\ndisplay(X_errors.head())\n\nX_women=X_errors[X_errors['Sex_bin']==1]\nX_men=X_errors[X_errors['Sex_bin']==0]\n#print(X_women.sort_values('Survived'))\nprint(X_men.sort_values('Survived'))\n","6f4f5187":"##training\nml_model = xgb.XGBClassifier(\n #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1,\n random_state=376)\n\nml_model.fit(X, y)\nxgb_val_predictions = ml_model.predict(X_test)\n","221bbe35":"#ml_model = DecisionTreeClassifier(max_depth = 3, random_state=376)\n\nml_model = RandomForestClassifier(n_estimators=100, random_state=376)\nml_model.fit(X, y)\nrf_predictions = ml_model.predict(X_test)\n\nml_model = LogisticRegression(max_iter=1000, random_state=376)\nml_model.fit(X, y)\nlr_predictions = ml_model.predict(X_test)\n\nml_model = xgb.XGBClassifier(\n #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1,\n random_state=376)\n\nml_model.fit(X, y)\nxgb_predictions = ml_model.predict(X_test)\n\nml_val_predictions=0.3*rf_predictions+0.3*lr_predictions+0.4*xgb_predictions\nml_val_predictions=[(lambda x:1 if x>=0.5 else 0)(x) for x in ml_val_predictions]","d7d0278c":"output = pd.DataFrame({'PassengerId': test_data.PassengerId,\n                       'Survived': ml_val_predictions})\n\n#output = pd.Series(rf_val_predictions, index=test_data['PassengerId'], name='Survived')\n\noutput.to_csv('submission.csv', header=True, index=False)\n!head -n10 submission.csv\n#files.download('submission.csv')","12c8452a":"# **Feature transformation**\n\nTransformation of nominal features to numerical features.\n\nOperations applied to both train and test datasets.\n","7030333e":"> **Rela\u00e7\u00e3o entre as features Sex, Survived e Pclass:**\n\n* Considerando que mais homens morreram, e haviam mais homens na classe 3, proporcionalmente a cada categoria de Pclass, percebe-se que os homens da primeira classe tiveram prioridade no salvamento.\n\n* Os homens da primeira classe, mesmo em menos n\u00famero, foram os que mais sobreviveram.\n\n> **Resposta Q2: Considerando os homens que sobreviveram, existe alguma rela\u00e7\u00e3o com a classe ocupada? Proporcionalmente ao total de homens em cada categoria de Pclass, os homens da primeira classe sobreviveram em maior n\u00famero.**\n\n> **Resposta Q2: E as mulheres que n\u00e3o sobreviveram, pertenciam a alguma classe espec\u00edfica? Assim como no caso anterior, proporcionalmente ao total de mulheres em cada categoria de Pclass, mulheres da primeira classe sobreviveram em maior n\u00famero. Poucas mulheres da primeira e segunda classe morreram, as mulheres que morreram em geral pertenciam a classe 3.**","f1f582ed":"> **Feature SibSp:** Indica com quantos acompanhantes a pessoa estava viajando, sendo eles irm\u00e3os ou c\u00f4njuge.\n\n* Sem dados faltantes.\n* Valores desbalanceados, onde 78% (608) das pessoas estavam viajando sem irm\u00e3os ou c\u00f4njuge; \n* 23% (209) estavam viajando com apenas um desses acompanhantes.","53e3d18c":"# **Loading CSV files into Colab from a local drive**","387e7d9d":"Modelo de \u00e1rvore de decis\u00e3o DecisionTreeClassifier com max_depth = 8.\n\n> **Como acur\u00e1cia, o modelo de \u00e1rvore de decis\u00e3o atingiu 0,805 (~81% das previs\u00f5es corretas).**","c5f4bbe8":"> **Rela\u00e7\u00e3o entre as features Parch, SibSp, Parch_SibSp e Survived em rela\u00e7\u00e3o aos homens:**\n\n* Comparando homens acompanhados e sem nenhum acompanhante, nota-se que homens n\u00e3o acompanhados sobreviveram mais em n\u00fameros absolutos.\n\n* Contudo, haviam mais homens n\u00e3o acompanhados do que acompanhados no navio. \n\n* Assim, homens n\u00e3o acompanhados morreram mais tamb\u00e9m. \n\n> **Resposta Q4: Desconsiderando mulheres e crian\u00e7as, homens com acompanhantes tenderam a sobreviver mais? N\u00e3o, homens n\u00e3o acompanhados sobreviveram em maior n\u00famero.**\n\n**> Resposta Q4: Ou ainda, homens ricos e acompanhados tiveram mais chances? N\u00e3o, mesmo para homens ricos o padr\u00e3o se manteve, homens n\u00e3o acompanhados sobreviveram mais.**","3a960ec9":"# **ML model**\n\n*Datasets:*\n\nTrain: train_X, train_y\n\nValidation: val_X, val_y\n\nTest: X_test","9bfa1854":"# **Correlation between features II**\n","3fec7e3e":"> **Rela\u00e7\u00e3o entre as features Age e Survived:**\n\n* **Percebe-se uma similaridade entre a distribui\u00e7\u00e3o das idades das pessoas que sobreviveram e que morreram.**\n\n* No entanto, mais pessoas morreram (62% - 549) do que sobreviveram (38% - 342).\n\n* **Assim, notamos que apesar da prioridade dada as crian\u00e7as, algumas tamb\u00e9m morreram.**","0723dabe":"# **Logistic Regression algorithm**\n\nModelo de regress\u00e3o log\u00edstica modelo_rlogistica com max_iter = 1000.\n\nAjuste do modelo: modelo_rlogistica.fit(train_X, train_y)\n\nC\u00e1lculo de acur\u00e1cia: fun\u00e7\u00e3o modelo_rlogistica.score(val_X, val_y)\n\nScore() quantas predi\u00e7\u00f5es o modelo acertou na base de teste.\n\n> **Como acur\u00e1cia, o modelo de regress\u00e3o log\u00edstica atingiu 0,823 (~82% das previs\u00f5es corretas).**","3b92eca9":"**An\u00e1lise das features mais relevantes:**\n\n> **Feature Survived:** Indica se o passageiro sobreviveu ou n\u00e3o ao naufr\u00e1gio.\n\n* Sem dados faltantes.\n* Valores desbalanceados, onde 62% (549) das pessoas morreram e 38% (342) sobreviveram.\n* Nota-se que a sobreviv\u00eancia est\u00e1 relacionada ao n\u00famero de botes e coletes salva-vidas dispon\u00edveis no navio. Fica evidente que n\u00e3o havia material de resgate para todos os passageiros. \n* Considera-se tamb\u00e9m alguns casos onde as pessoas n\u00e3o conseguiram chegar \u00e0 \u00e1rea externa, permanecendo presas dentro do navio.","6015df4e":"> **Aparentemente s\u00f3 h\u00e1 um cen\u00e1rio de correla\u00e7\u00e3o: Consiste na correla\u00e7\u00e3o entre as features Parch e Parch_SibSp, e Parch_SibSp e SibSp.**\n\n* Isso se deve o fato de Parch_SibSp ser a soma dessas duas features.\n\n* Parch e SibSp apresentam correla\u00e7\u00e3o baixa.\n\n* Assim, seria poss\u00edvel considerar somente Parch_SibSp ou desconsider\u00e1-la para considerar Parch e SibSp.","4093c189":"> **Feature Parch:** Indica com quantos acompanhantes a pessoa estava viajando, sendo eles pais ou filhos.\n\n* Sem dados faltantes.\n* Valores desbalanceados, onde 76% (678) das pessoas estavam viajando sem os pais ou filhos;\n* 13% (118) estavam viajando com apenas um desses acompanhantes;\n* 9% (80) viajavam com dois desses acompanhantes.","171b947e":"# **DummyClassifier algorithm: First Baseline**\n\nO algoritmo base DummyClassifier necessita da defini\u00e7\u00e3o da estrat\u00e9gia utilizada para ajustar o modelo.\n\nDummyClassifier('most_frequent')\n\n> Ser\u00e1 empregada a estrat\u00e9gia de *dados mais frequentes*: dado o valor mais frequente da vari\u00e1vel resposta (Survived), o modelo Dummy vai inferir que todos os registros da base de dados assumem aquele valor na feature target Survived. \n\n> Predi\u00e7\u00e3o: fun\u00e7\u00e3o *predict()*. Recebe como par\u00e2metro a base de valida\u00e7\u00e3o val_X.\n\n> C\u00e1lculo da acur\u00e1cia: *accuracy_score function*. Recebe como par\u00e2metros a base teste da vari\u00e1vel resposta (val_y) e as previs\u00f5es do modelo.\n\nprevisao_dummy = modelo_dummy.predict(x_teste)\naccuracy_score(y_teste, previsao_dummy)\n\n> **Como acur\u00e1cia, o modelo DummyClassifier atingiu 0.617 (~62% das previs\u00f5es corretas).**","40d9a424":"# **Data split between train and validation datasets**\n\nPara o train_test_split define-se:\n(x_treino, x_teste, y_treino, y_teste)\n\nFun\u00e7\u00e3 de divis\u00e3o:\ntrain_test_split(X, y, test_size = 0.5)\n\nOnde,\n\nX -> vetor features\n\ny -> vari\u00e1vel resposta Survived\n\n> teste_size = 0.5 -> tamanho para a base de teste (valida\u00e7\u00e3o). A propor\u00e7\u00e3o entre treino e teste varia de acordo com o volume de dados dispon\u00edvel. Mas, usualmente, encontra-se propor\u00e7\u00f5es 50\/50, 30\/70, 25\/75 ou 20\/80. \n\n> Aqui, ser\u00e1 testado com 0.5 ou 50%, o que significa que a base de treino ser\u00e1 composta pelos 50% restantes.","2eea1f95":"> **Feature Parch e SibSp: Soma das duas features para representar o total de acompanhantes de um passageiro, sendo eles pais, filhos, irm\u00e3os ou c\u00f4njuge.**\n\n* Valores desbalanceados, onde 60% (537) das pessoas estavam viajando sozinhas;\n* 18% (161) estavam viajando com apenas um acompanhante.\n* 11% (102) estavam viajando com dois acompanhantes.\n\n* Total de pessoas com um ou mais acompanhantes: 354 (40%) pessoas.\n* Total de pessoas com mais de um acompanhante, sendo pelo menos um pai ou filho e um irm\u00e3o ou c\u00f4njuge:  142 (16%) pessoas.\n\n* **Q4: Desconsiderando mulheres e crian\u00e7as, homens com acompanhantes tenderam a sobreviver mais? Ou ainda, homens ricos e acompanhados tiveram mais chances?**","934c70c6":"# **Cross Validation: *Kfold***","948d54ed":"**Varia\u00e7\u00e3o como forma de otimiza\u00e7\u00e3o do par\u00e2metro max_depth da \u00e1rvore para tentar encontrar resultados melhores**\n\nVaria\u00e7\u00e3o do max_depth dentro de um intervalo de valores: entre 1 e 30 camadas.","c111ea3e":"**Train Definition**\n","6a3def80":"# **Titanic - Machine Learning from Disaster**\n\nTitanic Dataset from Kaggle Competition","6ce258a0":"# **Second Baseline**\n\nOutro baseline est\u00e1 relacionado ao problema em quest\u00e3o: Sabe-se que mulheres e crian\u00e7as tiveram prioridade no salvamento, confirmado pela an\u00e1lise pr\u00e9via realizada.\n\nCom base na etapa de EDA, o segundo baseline consiste em considerar que:\n\n* Modelo dummy 2:\n1. Mulheres sobreviveram.\n\n2. Crian\u00e7as menores de 15 anos sobreviveram.\n\n* Modelo dummy 3:\n\n1. Mulheres sobreviveram.\n\n2. Crian\u00e7as menores de 15 anos sobreviveram.\n\n3. Homens da classe 1 e sem acompanhantes sobreviveram.\n\n4. O restante n\u00e3o sobreviveu.\n\n> **Como acur\u00e1cia, o segundo baseline dummy 2 atingiu 0.796 (~80% das previs\u00f5es corretas).**","bf53b885":"**Representa\u00e7\u00e3o da \u00e1rvore de decis\u00e3o com profundidade 3:**","a1c5026b":"# **Test step**\n\nIncluding the entire train and test sets with XGBoost algorithm.","2d9fdf28":"* Feature Sex (str) para Sex_bin (bin\u00e1ria)\n\n* Feature Embarked (str) para Embarked_num (num\u00e9rica)\n\n* Feature Cabin_null representando valores ausentes de Cabin. \n","24a0c859":"# **ML steps**\n\nProblema a ser resolvido: Considerando as features do dataset, predizer um dado passageiro sobreviveu (1) ou n\u00e3o (0) de acordo com a feature-alvo Survived.\n\n**Processo de ajuste do modelo de ML:**\n\n1. Divis\u00e3o da base em treino e valida\u00e7\u00e3o (split).\n\n2. Ajuste dos dados de treino (fit).\n\n3. Predi\u00e7\u00e3o e c\u00e1lculo da acur\u00e1cia para o conjunto de valida\u00e7\u00e3o (score).\n\n4. Predi\u00e7\u00e3o no conjunto real de teste (prediction).","4cbd96b7":"> **Feature Age:**\n\n> Distribui\u00e7\u00e3o dos valores:\\\n**count:**     714\\\n**missing:**   177\\\n**mean:**      29.7 anos\\\n**std:**       14.5 anos\\\n**min:**      0.4 anos\\\n**25%:**       20.1 anos\\\n**50%:**       28 anos\\\n**75%:**      38 anos\\\n**max:**       80 anos\n\n* **Dados faltantes (177).**\n* Valores desbalanceados, variando entre 0.4 e 80 anos.\n* 50% das pessoas apresentam menos de 28 anos.\n* 75% das pessoas apresentam menos de 38 anos.\n* Maior concentra\u00e7\u00e3o de pessoas com idades entre 18 e 40 anos.\n\n> * Porcentagem de pessoas com menos de 15 anos (crian\u00e7as?):  8.75% (78)\n* Porcentagem de pessoas com menos de 18 anos:  12.68% (113)\n* Porcentagem de pessoas do sexo masculino com menos de 18 anos:  6.51% (58)\n* Porcentagem de pessoas do sexo feminino com menos de 18 anos:  6.17% (55)\n* Porcentagem de pessoas entre 18 e 40 anos:  50.62% (451)\n* Porcentagem de pessoas entre 41 e 65 anos:  15.71% (140)\n* Porcentagem de pessoas com mais de 65 anos:  0.9% (8)","f7470ae2":"# **Correlation between features**\n\n> Determina\u00e7\u00e3o do grau de relacionamento entre duas features. Caso os pontos das vari\u00e1veis, representados num plano cartesiano (x, y) ou gr\u00e1fico de dispers\u00e3o, apresentem uma dispers\u00e3o similar ao longo de uma reta imagin\u00e1ria, diz-se que os dados apresentam correla\u00e7\u00e3o linear que pode ser negativa (pr\u00f3xima \u00e0 -1) ou positiva (pr\u00f3xima \u00e0 1). Caso n\u00e3o apresentem, o c\u00e1lculo de correla\u00e7\u00e3o ser\u00e1 igual \u00e0 0.\n\n> **Por\u00e9m, correla\u00e7\u00e3o n\u00e3o implica em causalidade.**\n\nO fato de vari\u00e1veis estarem correlacionados n\u00e3o quer dizer que uma *influ\u00eancia* no comportamento da outra. O padr\u00e3o de comportamento pode estar associado a fatores externos sem rela\u00e7\u00e3o entre si.","82373d3e":"> **Rela\u00e7\u00e3o entre as features Pclass e Fare:**\n\n> **Resposta Q3: O pre\u00e7o da passagem est\u00e1 relacionado com a classe ocupada? Sim, a primeira classe engloba passagens mais caras. Os outliers de cada classe podem ser explicados talvez pela data da compra (passagens compradas no dia tendem a ser mais caras, por exemplo).**","75723def":"**Plotting results**","99238161":"# **Relation between features**\n","b8600a42":"> **Feature Sex:**\n\n* Sem dados faltantes.\n* Valores desbalanceados, onde 65% (577) das pessoas eram homens e 35% (314) eram mulheres.\n\n> **Q1: Sabe-se que mulheres e crian\u00e7as tenderam a serem salvas primeiro. Existe rela\u00e7\u00e3o entre as pessoas que n\u00e3o sobreviveram e o sexo masculino?**","5e283377":"**Save predictions in format used for Titanic competition scoring**","ba6bfbad":"> **Feature Embarked:** Indica a esta\u00e7\u00e3o de embarque do passageiro.\n\n* **Dados faltantes (2).**\n* Valores desbalanceados, onde 72% (644) pessoas embarcaram no porto S;\n* 19% (168) embarcaram em C;\n* 9% (77) embarcaram em Q.","5394c47c":"# **EDA step**\n\n> **Total de dados:** 891 registros\\\n> **Total de features:** 12 features\\\n> **Feature alvo:** Survived\n\n* Cada registro representa um passageiro do Titanic.\n\n* A feature alvo (Survived) indica se um determinado passageiro sobreviveu ou n\u00e3o ao naufr\u00e1gio. \n\n* Survived \u00e9 utilizada na formula\u00e7\u00e3o do problema de ML, que visa prever se um determinado passageiro sobreviveu ou n\u00e3o considerando os outros 11 atributos do dataset.\n\n**Missing values:**\n\n* **Train dataset:** features Age, Cabin, Embarked\n\n* **Test dataset:** features Age, Fare, Cabin","ec8e7387":"> **Feature Pclass:** Indica a classe ocupada no navio, sendo elas: classe 1 (primeira classe), classe 2 (intermedi\u00e1ria) e classe 3 (mais econ\u00f4mica).\n\n* Sem dados faltantes.\n* Valores desbalanceados, onde 55% (491) das pessoas ocupam a classe 3\u00aa classe (mais econ\u00f4mica), 21% (184) ocupam a 2\u00aa classe (intermedi\u00e1ria) e 24% (216) est\u00e3o na primeira classe (mais cara).\n\n> **Q2: Considerando os homens que sobreviveram, existe alguma rela\u00e7\u00e3o com a classe ocupada? E as mulheres que n\u00e3o sobreviveram, pertenciam a alguma classe espec\u00edfica?**","cd3fbb42":"> **Rela\u00e7\u00e3o entre as features Sex e Survived:**\n\n* **Considerando que haviam no navio mais homens do que mulheres, proporcionalmente a cada categoria, percebe-se que mais mulheres sobreviveram em rela\u00e7\u00e3o aos homens.**\n\n> **Resposta Q1: Sabe-se que mulheres e crian\u00e7as tenderam a serem salvas primeiro. Existe rela\u00e7\u00e3o entre as pessoas que n\u00e3o sobreviveram e o sexo masculino? Sim, mais homens morreram em compara\u00e7\u00e3o com as mulheres.**","1a17842d":"**Test Definition**","31600fc4":"> **Feature Fare:** Indica o valor da passagem utilizada pelo passageiro.\n\n> Distribui\u00e7\u00e3o dos valores:\\\n**count:**    891\\\n**missing:**  0\\\n**mean:**     32.2\\\n**std:**      49.7\\\n**min:**      0\\\n**25%:**      7.9\\\n**50%:**      14.5\\\n**75%:**      31\\\n**max:**      512.3\n\n* Valores desbalanceados, variando entre 0 e 512:\n* 75% das pessoas pagaram valores inferiores a 31.\n* Maior parte das pessoas pagaram valores de passagem entre 0 e 50.\n\n* Porcentagem de pessoas que pagaram at\u00e9 50:  81.9%\\\n* Porcentagem de pessoas que pagaram at\u00e9 100:  94.1%\n\n> **Discrimina\u00e7\u00e3o dos valores de passagens para pessoas da classe 3 (classe mais econ\u00f4mica):**\\\ncount:     491\\\nmean:      13.675550\\\nstd:       11.778142\\\nmin:        0.000000\\\n25%:        7.750000\\\n50%:        8.050000\\\n75%:       15.500000\\\nmax:       69.550000\n\n> **Discrimina\u00e7\u00e3o dos valores de passagens para pessoas da classe 2:**\\\ncount:     184\\\nmean:     20.662183\\\nstd:       13.417399\\\nmin:       0.000000\\\n25%:       13.000000\\\n50%:       14.250000\\\n75%:       26.000000\\\nmax:       73.500000\n\n> **Discrimina\u00e7\u00e3o dos valores de passagens para pessoas da classe 1 (classe mais cara):**\\\ncount:     216\\\nmean:      84.154687\\\nstd:      78.380373\\\nmin:       0.000000\\\n25%:       30.923950\\\n50%:       60.287500\\\n75%:       93.500000\\\nmax:       512.329200\n\n> **Q3: O pre\u00e7o da passagem est\u00e1 relacionado com a classe ocupada?**","651a025c":"> **Rela\u00e7\u00e3o entre as features Age e Sex:**\n\n* **Atrav\u00e9s dos gr\u00e1ficos, percebe-se uma similaridade entre a distribui\u00e7\u00e3o das idades de homens e mulheres.**\n\n* No entanto, no navio haviam mais homens do que mulheres, conforme an\u00e1lise pr\u00e9via: 65% (577) das pessoas eram homens e 35% (314) eram mulheres.\n\n* **Assim, a sobreviv\u00eancia de mais mulheres n\u00e3o est\u00e1 relacionada com a idade das pessoas e sim com a prioridade dada ao sexo feminino.**\n","aa121359":"# **Decision Tree algorithm**\n\nModelo de \u00e1rvore de decis\u00e3o DecisionTreeClassifier com max_depth = 3.\n\n> **Como acur\u00e1cia, o modelo de \u00e1rvore de decis\u00e3o atingiu 0,778 (~78% das previs\u00f5es corretas).**","35ef80a9":"# **Random Forest algorithm**\n\nRandom Forest gera v\u00e1rias \u00e1rvores de decis\u00e3o atrav\u00e9s de amostras aleat\u00f3rias do conjunto de dados para, posteriormente, realizar as predi\u00e7\u00f5es.\n\nRandom Forest com n_estimators=100.\n\n> **Como acur\u00e1cia, o modelo de \u00e1rvore de decis\u00e3o atingiu 0,814 (~81% das previs\u00f5es corretas).**","128535b9":"# **Error analysis of features**\n\nComparing the predicted results with the expected output in respect of the other features.","aa7194a0":"# **XGBoost algorithm**\n\nXGBoost foi idealizado para otimizar algoritmos de \u00e1rvore de larga-escala.\n\nXGBoost com:\n* max_depth = 4: How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n* gamma = 0.9: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n* eta: step size shrinkage used in each boosting step to prevent overfitting.\n\n> **Como acur\u00e1cia, o modelo de \u00e1rvore de decis\u00e3o atingiu 0,836 (~84% das previs\u00f5es corretas).**","9473248e":"**Features para o modelo de ML:**\n\nX = Features de treino\\\ny = Feature-alvo","fe438d6d":"**Problema de overfitting:**\n\nObservando as diferen\u00e7as entre os scores de treino e teste, percebe-se que inicialmente os valores de treino e teste s\u00e3o muito pr\u00f3ximos, mas conforme a profundiade da \u00e1rvore aumenta, os valores de acur\u00e1cia para os dados de treino aumentam muito, enquanto esses mesmos valores para os dados de teste, caem (a diferen\u00e7a entre eles aumenta).\n\nA medida em que o n\u00famero de camadas aumenta, o modelo consegue capturar muito bem as caracter\u00edsticas dos dados de treino, em particular.\n\nEm contrapartida, o modelo n\u00e3o consegue generalizar de forma satisfat\u00f3ria as caracter\u00edsticas dos dados de valida\u00e7\u00e3o, ent\u00e3o a acur\u00e1cia vai caindo em rela\u00e7\u00e3o ao conjunto de treino ou se mant\u00e9m est\u00e1vel.\n\n**Conforme o gr\u00e1fico, o par\u00e2metro ideal seria max_depth = 14.**","8ef2d534":"# **Test step with ensemble**\n\nModelo de ensemble consiste em um algoritmo que re\u00fane diferentes modelos de ML para prever algum resultado, usando algoritmos de modelagem diferentes ou diferentes conjuntos de dados de treinamento. \n\nO ensemble agrega a previs\u00e3o de cada modelo, gerando uma previs\u00e3o final. A motiva\u00e7\u00e3o para usar modelos de ensemble de ML \u00e9 reduzir o erro de generaliza\u00e7\u00e3o da previs\u00e3o. Desde que os modelos de base sejam diversos e independentes, o erro de predi\u00e7\u00e3o do modelo diminui quando o modelo de ennsemble \u00e9 empregado. \n\nA abordagem busca o consenso das massas ao fazer uma previs\u00e3o. Mesmo que o ensemble englobe v\u00e1rios algoritmos de ML, este atuar\u00e1 como um \u00fanico modelo. A maioria das solu\u00e7\u00f5es pr\u00e1ticas de ML utiliza t\u00e9cnicas de ensemble.\n\n**Ensemble ML model with Logistic Regression (w=0.3), Random Forest (w=0.3) and XGBoost (w=0.4).**","0ee156b9":"* T\u00edtulos existentes na feature Name utilizados como features individuais indicando poss\u00edvel import\u00e2ncia de um certo passageiro.\n\n* T\u00edtulos mais frequentes usados: (' Mr. ', 517), (' Miss. ', 182), (' Mrs. ', 125), (' Master. ', 40), (' Dr. ', 7), (' Rev. ', 6)\n","0440c3bc":"**Treat Data Missing**"}}