{"cell_type":{"bf311bcf":"code","c9b7bc5c":"code","a37240b5":"code","f1c754e9":"code","9935e3dd":"code","b0d64e5a":"code","2461ce3f":"code","33cbdb58":"code","a4720025":"code","f0bcaac1":"code","23457bba":"code","75d6aeae":"code","daa0d383":"code","e470f5b4":"code","20cd76c0":"code","752a22e4":"code","079096bd":"code","2eaba990":"code","2dd43561":"code","83d4c745":"code","6920e397":"code","295a653e":"code","a24a0d02":"code","8b8c5976":"code","65e0c50f":"code","6c3a7675":"code","ccd21a86":"code","6190cf0d":"code","007fd2b8":"code","313224ac":"code","470d6be3":"code","594677e4":"code","e04c7f61":"code","4fa2b245":"code","2d439937":"code","43faa0d3":"code","f375a2a7":"code","23c7413e":"code","6d2370d4":"code","b16b5eb3":"code","c53dfc86":"code","47e2cff5":"code","fcd35308":"code","2dbaed50":"code","1f510cce":"code","39a9e8de":"code","4b9abedb":"code","34a47a82":"code","e4b4cd64":"code","547989d6":"code","d567f5d0":"code","cd8b4e6a":"code","30469e5f":"code","b3f80587":"code","fc083ceb":"code","30442873":"code","0ecc688b":"code","30a33f65":"code","63af3fd0":"markdown","ec46884f":"markdown","8c34053c":"markdown","1905f937":"markdown","2f84e1fa":"markdown","d0aa48ef":"markdown","7f968eef":"markdown","ed224593":"markdown","69563589":"markdown","30e58560":"markdown","8e3e4f25":"markdown","ef3bf21d":"markdown","39ef1dd1":"markdown","b4d8e39b":"markdown","9f3e9172":"markdown","d800edee":"markdown","33f8bbb2":"markdown","266c1536":"markdown","3400bc95":"markdown","2c17e27a":"markdown","5901c983":"markdown","e1785de3":"markdown","54482688":"markdown","7300dbad":"markdown","ee56765c":"markdown","93a90700":"markdown","03313f20":"markdown"},"source":{"bf311bcf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #to plot charts\nimport seaborn as sns #used for data visualization\nimport warnings #avoid warning flash\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9b7bc5c":"df=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","a37240b5":"df.head() #get familier with dataset, display the top 5 data records","f1c754e9":"df.shape #getting to know about rows and columns we're dealing with - 768 rows , 9 columns","9935e3dd":"df.columns #learning about the columns","b0d64e5a":"df.dtypes #knowledge of data type helps for computation","2461ce3f":"df.info() #Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.","33cbdb58":"df.describe() #helps us to understand how data has been spread across the table.\n# count :- the number of NoN-empty rows in a feature.\n# mean :- mean value of that feature.\n# std :- Standard Deviation Value of that feature.\n# min :- minimum value of that feature.\n# max :- maximum value of that feature.\n# 25%, 50%, and 75% are the percentile\/quartile of each features. ","a4720025":"#dropping duplicate values - checking if there are any duplicate rows and dropping if any\ndf=df.drop_duplicates()","f0bcaac1":"#check for missing values, count them and print the sum for every column\ndf.isnull().sum() #conclusion :- there are no null values in this dataset","23457bba":"#checking for 0 values in 5 columns , Age & DiabetesPedigreeFunction do not have have minimum 0 value so no need to replace , also no. of pregnancies as 0 is possible as observed in df.describe\nprint(df[df['BloodPressure']==0].shape[0])\nprint(df[df['Glucose']==0].shape[0])\nprint(df[df['SkinThickness']==0].shape[0])\nprint(df[df['Insulin']==0].shape[0])\nprint(df[df['BMI']==0].shape[0])","75d6aeae":"#replacing 0 values with median of that column\ndf['Glucose']=df['Glucose'].replace(0,df['Glucose'].mean())#normal distribution\ndf['BloodPressure']=df['BloodPressure'].replace(0,df['BloodPressure'].mean())#normal distribution\ndf['SkinThickness']=df['SkinThickness'].replace(0,df['SkinThickness'].median())#skewed distribution\ndf['Insulin']=df['Insulin'].replace(0,df['Insulin'].median())#skewed distribution\ndf['BMI']=df['BMI'].replace(0,df['BMI'].median())#skewed distribution","daa0d383":"sns.countplot('Outcome',data=df)","e470f5b4":"#histogram for each  feature\ndf.hist(bins=10,figsize=(10,10))\nplt.show()","20cd76c0":"plt.figure(figsize=(16,12))\nsns.set_style(style='whitegrid')\nplt.subplot(3,3,1)\nsns.boxplot(x='Glucose',data=df)\nplt.subplot(3,3,2)\nsns.boxplot(x='BloodPressure',data=df)\nplt.subplot(3,3,3)\nsns.boxplot(x='Insulin',data=df)\nplt.subplot(3,3,4)\nsns.boxplot(x='BMI',data=df)\nplt.subplot(3,3,5)\nsns.boxplot(x='Age',data=df)\nplt.subplot(3,3,6)\nsns.boxplot(x='SkinThickness',data=df)\nplt.subplot(3,3,7)\nsns.boxplot(x='Pregnancies',data=df)\nplt.subplot(3,3,8)\nsns.boxplot(x='DiabetesPedigreeFunction',data=df)","752a22e4":"from pandas.plotting import scatter_matrix\nscatter_matrix(df,figsize=(20,20));\n# we can come to various conclusion looking at these plots for example  if you observe 5th plot in pregnancies with insulin, you can conclude that women with higher number of pregnancies have lower insulin","079096bd":"corrmat=df.corr()\nsns.heatmap(corrmat, annot=True)","2eaba990":"df_selected=df.drop(['BloodPressure','Insulin','DiabetesPedigreeFunction'],axis='columns')","2dd43561":"from sklearn.preprocessing import QuantileTransformer\nx=df_selected\nquantile  = QuantileTransformer()\nX = quantile.fit_transform(x)\ndf_new=quantile.transform(X)\ndf_new=pd.DataFrame(X)\ndf_new.columns =['Pregnancies', 'Glucose','SkinThickness','BMI','Age','Outcome']\ndf_new.head()","83d4c745":"plt.figure(figsize=(16,12))\nsns.set_style(style='whitegrid')\nplt.subplot(3,3,1)\nsns.boxplot(x=df_new['Glucose'],data=df_new)\nplt.subplot(3,3,2)\nsns.boxplot(x=df_new['BMI'],data=df_new)\nplt.subplot(3,3,3)\nsns.boxplot(x=df_new['Pregnancies'],data=df_new)\nplt.subplot(3,3,4)\nsns.boxplot(x=df_new['Age'],data=df_new)\nplt.subplot(3,3,5)\nsns.boxplot(x=df_new['SkinThickness'],data=df_new)","6920e397":"target_name='Outcome'\ny= df_new[target_name]#given predictions - training data \nX=df_new.drop(target_name,axis=1)#dropping the Outcome column and keeping all other columns as X","295a653e":"X.head() # contains only independent features ","a24a0d02":"y.head() #contains dependent feature","8b8c5976":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=0)#splitting data in 80% train, 20%test","65e0c50f":"X_train.shape,y_train.shape","6c3a7675":"X_test.shape,y_test.shape","ccd21a86":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import GridSearchCV","6190cf0d":"#List Hyperparameters to tune\nknn= KNeighborsClassifier()\nn_neighbors = list(range(15,25))\np=[1,2]\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n#convert to dictionary\nhyperparameters = dict(n_neighbors=n_neighbors, p=p,weights=weights,metric=metric)\n\n#Making model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=knn, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring='f1',error_score=0)","007fd2b8":"best_model = grid_search.fit(X_train,y_train)","313224ac":"#Best Hyperparameters Value\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])","470d6be3":"#Predict testing set\nknn_pred = best_model.predict(X_test)","594677e4":"print(\"Classification Report is:\\n\",classification_report(y_test,knn_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,knn_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,knn_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,knn_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,knn_pred))","e04c7f61":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid_nb = {\n    'var_smoothing': np.logspace(0,-2, num=100)\n}\nnbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)","4fa2b245":"best_model= nbModel_grid.fit(X_train, y_train)","2d439937":"nb_pred=best_model.predict(X_test)","43faa0d3":"print(\"Classification Report is:\\n\",classification_report(y_test,nb_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,nb_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,nb_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,nb_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,nb_pred))","f375a2a7":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score","23c7413e":"model = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']","6d2370d4":"# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)","b16b5eb3":"grid_result = grid_search.fit(X, y)","c53dfc86":"svm_pred=grid_result.predict(X_test)","47e2cff5":"print(\"Classification Report is:\\n\",classification_report(y_test,svm_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,knn_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,knn_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,knn_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,svm_pred))","fcd35308":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import GridSearchCV\ndt = DecisionTreeClassifier(random_state=42)","2dbaed50":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [5, 10, 20,25],\n    'min_samples_leaf': [10, 20, 50, 100,120],\n    'criterion': [\"gini\", \"entropy\"]\n}","1f510cce":"grid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")","39a9e8de":"best_model=grid_search.fit(X_train, y_train)","4b9abedb":"dt_pred=best_model.predict(X_test)","34a47a82":"print(\"Classification Report is:\\n\",classification_report(y_test,dt_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,dt_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,dt_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,dt_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,dt_pred))","e4b4cd64":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","547989d6":"# define models and parameters\nmodel = RandomForestClassifier()\nn_estimators = [1800]\nmax_features = ['sqrt', 'log2']","d567f5d0":"# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)","cd8b4e6a":"best_model = grid_search.fit(X_train, y_train)","30469e5f":"rf_pred=best_model.predict(X_test)","b3f80587":"print(\"Classification Report is:\\n\",classification_report(y_test,rf_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,knn_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,knn_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,knn_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,rf_pred))","fc083ceb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score","30442873":"reg = LogisticRegression()\nreg.fit(X_train,y_train)                         ","0ecc688b":"lr_pred=reg.predict(X_test)","30a33f65":"print(\"Classification Report is:\\n\",classification_report(y_test,lr_pred))\nprint(\"\\n F1:\\n\",f1_score(y_test,lr_pred))\nprint(\"\\n Precision score is:\\n\",precision_score(y_test,lr_pred))\nprint(\"\\n Recall score is:\\n\",recall_score(y_test,lr_pred))\nprint(\"\\n Confusion Matrix:\\n\")\nsns.heatmap(confusion_matrix(y_test,lr_pred))","63af3fd0":"## 9.6 Logistic Regression:- \nLogistical regression is selected when the dependent variable is categorical, meaning they have binary outputs, such as \"true\" and \"false\" or \"yes\" and \"no.\" \n\nLogistic regression does not really have any critical hyperparameters to tune. Sometimes, you can see useful differences in performance or convergence with different solvers (solver).Regularization (penalty) can sometimes be helpful.\n\nAfter trying to tune this for 4 hours and achieving 0.00003% of increased accuracy, I've given up and didn't apply grid search for Logistic regression. If you have any better method please comment and help me!\ud83e\udd72","ec46884f":"# INDEX :- \n**1. Importing Required Libraries**\n\n**2. Loading the Dataset**\n\n**3. Exploratory Data Analysis**\n\na. Understanding the dataset\n- Head of the dataset\n- Shape of the data set\n- Types of columns\n- Information about data set\n- Summary of the data set\n\nb. Data Cleaning\n- Dropping duplicate values\n- Checking NULL values\n- Checking for 0 value\n         \n**4. Data Visualization**\n####  Here we are going to plot :-\n - Count Plot :- to see if the dataset is balanced or not\n - Histograms :- to see if data is normally distributed or skewed\n - Box Plot :- to analyse the distribution and see the outliers\n - Scatter plots :- to understand relationship between any two variables\n - Pair plot :- to create scatter plot between all the variables\n \n**5. Feature Selection**\n\n**6. Handling Outliers**\n\n**7. Split the Data Frame into X and y**\n\n**8. TRAIN TEST SPLIT**\n\n**9. Build the Classification Algorithm**\n\n9.1  KNN\n\n9.2  Naive Bayes\n\n9.3  SVM\n\n9.4  Decision Tree\n\n9.5  Random Forest\n\n9.6  Logistic Regression\n\n#### The models include the following:- \n\n  a. Hyper Parameter Tuning using GridSearch CV \n  \n  b. Fit Best Model\n  \n  c. Predict on testing data using that model\n  \n  d. Performance Metrics :- Confusion Matrix, F1 Score, Precision Score, Recall Score\n  \n\n  \n# If you like my work, feel free to upvote and comment !\n### I'd love to hear your thoughts about this notebook! Constructive Criticism is always welcomed\ud83d\ude04 Do share any better method, model or techniques I should try and improvements in this notebook(if any). This is my very first notebook and I hope you like my work \u2728 Thankyou for your time\ud83d\ude4f\n","8c34053c":"# 3. Exploratory Data Analysis\n## a. Understanding the dataset\n- Head of the dataset\n- Shape of the data set\n- Types of columns\n- Information about data set\n- Summary of the data set","1905f937":"- The train-test split is a technique for evaluating the performance of a machine learning algorithm.\n\n- Train Dataset: Used to fit the machine learning model.\n- Test Dataset: Used to evaluate the fit machine learning model.\n\n- Common split percentages include:\n\nTrain: 80%, Test: 20%\n\nTrain: 67%, Test: 33%\n\nTrain: 50%, Test: 50%\n\nI've used 80% train and 20% test\n\nRead more about it here :- https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/","2f84e1fa":"### **Conclusion** :- We observe that number of people who do not have diabetes is far more than people who do which indicates that our data is imbalanced.","d0aa48ef":"# 6. Handling Outliers ","7f968eef":"# 4. Data Visualization\n## Here we are going to plot :-\n- Count Plot :- to see if the dataset is balanced or not\n- Histograms :- to see if data is normally distributed or skewed\n- Box Plot :- to analyse the distribution and see the outliers\n- Scatter plots :- to understand relationship between any two variables\n- Pair plot :- to create scatter plot between all the variables","ed224593":"## 9.2 Naive Bayes :-\n\nNaive Bayes is classification approach that adopts the principle of class conditional independence from the Bayes Theorem. This means that the presence of one feature does not impact the presence of another in the probability of a given outcome, and each predictor has an equal effect on that result","69563589":"## 9.5 Random Forest :- \nThe \"forest\" references a collection of uncorrelated decision trees, which are then merged together to reduce variance and create more accurate data predictions.","30e58560":"## b. Data Cleaning\n- Dropping duplicate values\n- Checking NULL values\n- Checking for 0 value and replacing it :- It isn't medically possible for some data record to have 0 value such as Blood Pressure or Glucose levels. Hence we replace them with the mean value of that particular column.\n\nRead more about this here :- https:\/\/towardsdatascience.com\/the-ultimate-guide-to-data-cleaning-3969843991d4","8e3e4f25":"# 1. Import Required Libraries","ef3bf21d":"### NOTE :-\nSome of the columns have a skewed distribution, so the mean is more affected by outliers than the median. Glucose and Blood Pressure have normal distributions hence we replace 0 values in those columns by mean value. SkinThickness, Insulin,BMI have skewed distributions hence median is a better choice as it is less affected by outliers.\n\nRefer Histograms down below to see the distribution.\n\nRead more here :- \n\nhttps:\/\/vitalflux.com\/pandas-impute-missing-values-mean-median-mode\/\n\nhttps:\/\/www.clinfo.eu\/mean-median\/\n\n","39ef1dd1":"### **Conclusion** :- We observe that only glucose and Blood Pressure are normally distributed rest others are skewed and have outliers","b4d8e39b":"## 9.1 K Nearest Neighbours :-\n\nKNN algorithm, is a non-parametric algorithm that classifies data points based on their proximity and association to other available data.","9f3e9172":"# Diabetes Prediction\n## What is Diabetes?\nDiabetes is a chronic disease that occurs when the pancreas is no longer able to make insulin, or when the body cannot make good use of the insulin it produces. Learning how to use Machine Learning can help us predict Diabetes. Let\u2019s get started!\n\n## About this project :- \n\n- The objective of this project is to classify whether someone has diabetes or not.\n- Dataset consists of several Medical Variables(Independent) and one Outcome Variable(Dependent)\n- The independent variables in this data set are :-'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'\n- The outcome variable value is either 1 or 0 indicating whether a person has diabetes(1) or not(0).\n\n## About the Dataset\n\n- Pregnancies :- Number of times a woman has been pregnant\n- Glucose :- Plasma Glucose concentration of 2 hours in an oral glucose tolerance test\n- BloodPressure :- Diastollic Blood Pressure (mm hg)\n- SkinThickness :- Triceps skin fold thickness(mm)\n- Insulin :- 2 hour serum insulin(mu U\/ml)\n- BMI :- Body Mass Index ((weight in kg\/height in m)^2)\n- Age :- Age(years)\n- DiabetesPedigreeFunction :-scores likelihood of diabetes based on family history) \n- Outcome :- 0(doesn't have diabetes) or 1 (has diabetes)","d800edee":"# 9.3 Support Vector Machine :- \n\nIt is typically leveraged for classification problems, constructing a hyperplane where the distance between two classes of data points is at its maximum. This hyperplane is known as the decision boundary, separating the classes of data points (e.g., has diabetes vs doesn't have diabetes ) on either side of the plane.","33f8bbb2":"# 5. Feature Selection","266c1536":"## 9.4 Decision Tree ","3400bc95":"### **CONCLUSION** :- We observe that min value of some columns is 0 which cannot be possible medically.Hence in the data cleaning process we'll have to replace them with median\/mean value depending on the distribution. Also in the max column we can see insulin levels as high as 846! We have to treat outliers.","2c17e27a":"# 5. Split the Data Frame into X and y","5901c983":"# 9. Classification Algorithms\n\n- KNN\n- Naive Bayes\n- SVM\n- Decision Tree\n- Random Forest\n- Logistic Regression\n\n### The models include the following:-\n\n#### a. Hyper Parameter Tuning using GridSearch CV\n\n**1. What Is Hyperparameter Tuning?**\n\nHyperparameters are the variables that the user specify usually while building the Machine Learning model. thus, hyperparameters are specified before specifying the parameters or we can say that hyperparameters are used to evaluate optimal parameters of the model. the best part about hyperparameters is that their values are decided by the user who is building the model. For example, max_depth in Random Forest Algorithms, k in KNN Classifier.\nHyperparameter tuning is the process of tuning the parameters present as the tuples while we build machine learning models. \n\n**2. What is GridSearch ?**\n\nGrid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.\n\n**3. What Steps To Follow For Hyper Parameter Tuning?**\n\n1. Select the type of model we want to use like RandomForestClassifier, regressor or any other model\n2. Check what are the parameters of the model\n3. Select the methods for searching the hyperparameter\n4. Select the cross-validation approach\n5. Evaluate the model using the score\n\n#### b. Fit Best Model\n\n#### c. Predict on testing data using that model\n\n#### d. Performance Metrics :- Confusion Matrix, F1 Score, Precision Score, Recall Score\n**Confusion Matrix**\nIt is a tabular visualization of the model predictions versus the ground-truth labels. \n\n![Screenshot 2021-11-17 183814.jpg](attachment:8262feb0-4386-4e13-a7b1-28097c5717f2.jpg)\n\n**F1 Score :-**\nIt\u2019s the harmonic mean between precision and recall. \n\n![Screenshot 2021-11-17 184716.jpg](attachment:c426429c-ce5e-4b57-abad-f3ec48c95344.jpg)\n\n**Precision Score**\nPrecision is the fraction of predicted positives\/negatives events that are actually positive\/negatives.\n\n![Screenshot 2021-11-17 190044.jpg](attachment:6d36beef-92e4-4732-8dcd-51c798d9e5da.jpg)\n\n\n**Recall Score**\nIt is the fraction of positives\/negative events that you predicted correctly. \n\n![re.jpg](attachment:3641709b-9970-4ef9-97ad-c150302b8e36.jpg)\n\n#### I've given preference to F1 Scoring because :- \n\n1. When you have a small positive class, then F1 score makes more sense.In this case the positive class number is almost half of the negative class.\n\n2. F1-score is a better metric when there are imbalanced classes as in the above case.\n\n3. F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall\n\nReference :- https:\/\/machinelearningmastery.com\/hyperparameters-for-classification-machine-learning-algorithms\/\n\n","e1785de3":"**Pearson's Correlation Coefficient** : Helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.","54482688":"# 2. Loading the dataset ","7300dbad":"Outliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Hence it is of utmost importance to deal with them. In this case removing outliers can cause data loss so we have to deal with it using various scaling and transformation techniques.","ee56765c":"# 7. TRAIN TEST SPLIT","93a90700":"### **CONCLUSION** :- Observe the last row 'Outcome' and note its correlation scores with different features. We can observe that Glucose, BMI and Age are the most correlated with Outcome. BloodPressure, Insulin, DiabetesPedigreeFunction are the least correlated, hence they don't contribute much to the model so we can drop them. Read more about this here :- https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e I have used 3'rd technique method mentioned here.","03313f20":"**1 \u2014 What is an Outlier?**\n\nAn outlier is a data point in a data set that is distant from all other observations.\n\n**2 \u2014 How can we Identify an outlier?**\n\n- Using Box plots\n\n- Using Scatter plot\n\n- Using Z score\n\nI've used Box Plots above in data visualization step to detect outliers.\n\n**3 \u2014 How am I treating the outliers ?**\n\nQuantile Transformer :- This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. \n\nLets do a simple Standard Scaler vs Quantile Transformation. Given this data set:- \n\n![1.jpg](attachment:839ace54-71e4-40e9-9f4e-8e2f432bf784.jpg)\n\nWe perform StandardScaler() on this and get\n\n![2.jpg](attachment:23bc74db-a90b-448b-8b4d-0ed4d416eede.jpg)\n\nThe Y-axis has 8 units whereas X-axis has only 3.5 units, indicating that Outliers have affected the scales\n\nAfter applying Quantile Transformation , we get \n\n![3.jpg](attachment:42d6d738-75e3-46d8-acf3-5300f695acd3.jpg)\n\nThe Y-axis and X-axis are equally scaled. The outliers are still present in this dataset but their impact has been reduced. One of these examples has led me to use this transformer.\n\nCourtesy :- Freecodecamp and CalmCode \n\nLearn more about it here :- https:\/\/www.youtube.com\/watch?v=0B5eIE_1vpU , https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/feature-scaling-techniques-in-python-a-complete-guide\/"}}