{"cell_type":{"beb47205":"code","d6d22ef1":"code","920da807":"code","22237249":"code","59ded1c3":"code","e0a93430":"code","ef7f0c44":"code","89f06452":"code","5d78fecc":"code","c0bb1d37":"code","b61259b9":"code","b63a945f":"code","f3be4d17":"code","4fc27c97":"code","8c4a4a86":"code","d874151e":"code","d7b58a3b":"code","1777f725":"code","51d865de":"code","3182e9a7":"code","406a9232":"markdown"},"source":{"beb47205":"# 1. Import Libraries\n# Visualization Libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preprocessing Libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n\n# ML Libraries\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Evaluation Metrics\nfrom yellowbrick.classifier import ClassificationReport\nfrom sklearn import metrics","d6d22ef1":"# Read dataset and display first 5 row\ndf = pd.read_csv('..\/input\/train.csv', error_bad_lines=False) \ndf.head(5)","920da807":"# 2. Preliminary Analysis\ndf.info()","22237249":"# 3. Data Preparation\n# Remove irrelevant\/not meaningfull attributes\ndf = df.drop(['Descript'], axis=1)\ndf = df.drop(['Resolution'], axis=1)\n\ndf.info()","59ded1c3":"# Splitting the Date to Day, Month, Year, Hour, Minute, Second\ndf['date2'] = pd.to_datetime(df['Dates'])\ndf['Year'] = df['date2'].dt.year\ndf['Month'] = df['date2'].dt.month\ndf['Day'] = df['date2'].dt.day\ndf['Hour'] = df['date2'].dt.hour\ndf['Minute'] = df['date2'].dt.minute\ndf['Second'] = df['date2'].dt.second \ndf = df.drop(['Dates'], axis=1) \ndf = df.drop(['date2'], axis=1)\ndf.head(5)","e0a93430":"# Convert Categorical Attributes to Numerical\ndf['PdDistrict'] = pd.factorize(df[\"PdDistrict\"])[0]\ndf['Address'] = pd.factorize(df[\"Address\"])[0]\ndf['DayOfWeek'] = pd.factorize(df[\"DayOfWeek\"])[0]\ndf['Year'] = pd.factorize(df[\"Year\"])[0]\ndf['Month'] = pd.factorize(df[\"Month\"])[0]\ndf['Day'] = pd.factorize(df[\"Day\"])[0]\ndf['Hour'] = pd.factorize(df[\"Hour\"])[0]\ndf['Minute'] = pd.factorize(df[\"Minute\"])[0]\ndf['Second'] = pd.factorize(df[\"Second\"])[0] \ndf.head(5)","ef7f0c44":"# Display targer class\nTarget = 'Category'\nprint('Target: ', Target)","89f06452":"# Plot Bar Chart visualize Crime Types\nplt.figure(figsize=(14,10))\nplt.title('Amount of Crimes by Category')\nplt.ylabel('Crime Category')\nplt.xlabel('Amount of Crimes')\n\ndf.groupby([df['Category']]).size().sort_values(ascending=True).plot(kind='barh')\n\nplt.show()","5d78fecc":"# Display all unique classes\nClasses = df['Category'].unique()\nClasses","c0bb1d37":"#Encode target labels into categorical variables:\ndf['Category'] = pd.factorize(df[\"Category\"])[0] \ndf['Category'].unique()","b61259b9":"# 4. Feature Selection using Filter Method \n# Split Dataframe to target class and features\nX_fs = df.drop(['Category'], axis=1)\nY_fs = df['Category']\n\n#Using Pearson Correlation\nplt.figure(figsize=(20,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","b63a945f":"#Correlation with output variable\ncor_target = abs(cor['Category'])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.02]\nrelevant_features","f3be4d17":"# At Current Point, the attributes is select manually based on Feature Selection Part. \nFeatures = [\"Address\",\"Year\",\"Hour\", \"Minute\" ]\nprint('Full Features: ', Features) ","4fc27c97":"# 5. Split dataset to Training Set & Test Set\nx, y = train_test_split(df, \n                        test_size = 0.2, \n                        train_size = 0.8, \n                        random_state= 3)\n\nx1 = x[Features]    #Features to train\nx2 = x[Target]      #Target Class to train\ny1 = y[Features]    #Features to test\ny2 = y[Target]      #Target Class to test\n\nprint('Feature Set Used    : ', Features)\nprint('Target Class        : ', Target)\nprint('Training Set Size   : ', x.shape)\nprint('Test Set Size       : ', y.shape)","8c4a4a86":"# 6. Random Forest\n# Create Model with configuration\nrf_model = RandomForestClassifier(n_estimators=70, # Number of trees\n                                  min_samples_split = 30,\n                                  bootstrap = True, \n                                  max_depth = 50, \n                                  min_samples_leaf = 25)\n\n# Model Training\nrf_model.fit(X=x1,\n             y=x2)\n\n# Prediction\nresult = rf_model.predict(y[Features])","d874151e":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== Random Forest Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","d7b58a3b":"# 7. Neural Network\n# Create Model with configuration \nnn_model = MLPClassifier(solver='adam', \n                         alpha=1e-5,\n                         hidden_layer_sizes=(40,), \n                         random_state=1,\n                         max_iter=1000                         \n                        )\n\n# Model Training\nnn_model.fit(X=x1,\n             y=x2)\n\n# Prediction\nresult = nn_model.predict(y[Features]) ","1777f725":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== Neural Network Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","51d865de":"# 8. Naive Bayes\n# Create Model with configuration \nnb_model = GaussianNB() \n\n# Model Training\nnb_model.fit(X=x1, \n             y=x2)\n\n# Prediction\nresult = nb_model.predict(y[Features])","3182e9a7":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== Naive Bayes Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","406a9232":"**In this notebook, we demonstrate the application of Random Forest, Naive Bayes and Neural Network to perform classification task with San Francisco Crime Dataset**\n\nThe steps of the classification task:\n1. Import Libraries\n1. Preliminary Analysis\n1. Data Preparation\n1. Feature Selection\n1. Dataset Splitting\n1. Random Forest Modelling\n1. Neural Network Modelling\n1. Naive Bayes Modelling"}}