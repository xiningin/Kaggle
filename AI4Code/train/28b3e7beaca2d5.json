{"cell_type":{"d756fcf8":"code","eb2f8831":"code","f41514f8":"code","b0485657":"code","74b21336":"code","71fca0eb":"code","ec08c468":"code","568a7082":"code","410bab47":"code","6ed0a1e1":"code","4ab43e6a":"code","53b565e0":"code","c1d1e0ac":"code","e3d4aa6b":"code","6951a948":"code","dc6f1c2a":"code","48f852ad":"code","c5af6696":"code","6d26ea53":"code","fba16f07":"code","6baf33f9":"code","7f3f7232":"code","46252273":"code","ea6481c6":"code","10ae94b4":"code","d11fbaaf":"code","003de693":"code","d1daa3cf":"code","84f0d235":"code","2f42faca":"code","7bf2f042":"code","92052335":"code","41903e0a":"code","b20f63d0":"code","7c9f362d":"code","5146360c":"code","d128d8ba":"code","ac0b8605":"code","d829d0ed":"code","c2ee74ce":"code","80c02cf3":"code","d29ecfdd":"code","26774c37":"code","60fe3a98":"code","b9f07c29":"code","bac7d319":"code","bf7f57b6":"code","7145acf7":"code","5968bd3a":"code","03a79b5b":"code","afebecd0":"code","177c87d4":"markdown","76352aed":"markdown","dce474e8":"markdown","89b29fb8":"markdown","03533830":"markdown","68c7981a":"markdown","7f034955":"markdown","533041a6":"markdown","62e12e3e":"markdown","f7dafd20":"markdown","1c0c4c9c":"markdown","6538f515":"markdown","02236d0c":"markdown","6a2339cb":"markdown","b2a39ad5":"markdown","8a1589f0":"markdown","cbf0c6ac":"markdown","30faff87":"markdown","5588e419":"markdown","79a7c18a":"markdown","0dcae290":"markdown","3d422cc9":"markdown","c30bb205":"markdown","ab7e452a":"markdown","ea9dda8c":"markdown"},"source":{"d756fcf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb2f8831":"# Numpy and pandas for data manupulation\nimport numpy as np\nimport pandas as pd\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f41514f8":"# List files available\nprint(os.listdir(\"..\/input\/home-credit-default-risk\"))","b0485657":"# train data\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Train data shape:',app_train.shape)\napp_train.head()","74b21336":"# Test Data\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Test data shape:',app_test.shape)\napp_test.head()","71fca0eb":"app_train['TARGET'].value_counts()","ec08c468":"app_train['TARGET'].plot.hist()","568a7082":"# Function to calculate missing values by columns\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # percentage of missing values\n    mis_val_percent = 100 * mis_val\/len(df)\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val,mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns={0:'Missing Values',1:'Percentage of missing values'})\n    \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('Percentage of missing values',ascending=False).round(1)\n    \n    # Print some summary information\n    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n    \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","410bab47":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","6ed0a1e1":"app_train.dtypes.value_counts()","4ab43e6a":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique)","53b565e0":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.'%le_count)\n","c1d1e0ac":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape:', app_train.shape)\nprint('Test Features shape:', app_test.shape)","e3d4aa6b":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train,app_test = app_train.align(app_test, join='inner',axis=1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","6951a948":"(app_train['DAYS_BIRTH']\/-365).describe()","dc6f1c2a":"app_train['DAYS_EMPLOYED'].describe()","48f852ad":"app_train['DAYS_EMPLOYED'].plot.hist(label='Days Employed Histogram')\nplt.xlabel('Days Employment')","c5af6696":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n\nprint('There are %d anomalous days of employment' % len(anom))\n# It turns out that the anomalies have a lower rate of default.","6d26ea53":"# fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.\n\n# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_train[\"DAYS_EMPLOYED\"].replace({365243 : np.nan}, inplace=True)\n\napp_train[\"DAYS_EMPLOYED\"].plot.hist(title = 'Days Employment Histogram')\nplt.xlabel('Days Employment')\n\n","fba16f07":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","6baf33f9":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n#Display Corellations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","7f3f7232":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","46252273":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH']\/-364, edgecolor = 'k',bins=25)\nplt.title('Age of Client')\nplt.xlabel('Age (years)')\nplt.ylabel('count')","ea6481c6":"# kde plots\nplt.figure(figsize=(10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH']\/365, label = 'Target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH']\/365, label = 'Target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)')\nplt.xlabel('Density')\nplt.title('Distribution of Ages')","10ae94b4":"# Age information into a separate dataframe\nage_data = app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']\/365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], np.linspace(20,70,num = 11))\nage_data.head(10)","d11fbaaf":"# Group by the bin and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","003de693":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str),100*age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75)\nplt.xlabel('Age Group(Years)')\nplt.ylabel('Failure to repay percentage')\nplt.title('Failure to Repay by Age Group')","d1daa3cf":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corr = ext_data.corr()\next_data_corr","84f0d235":"plt.figure(figsize=(8,8))\n\nsns.heatmap(ext_data_corr, cmap = plt.cm.RdYlBu_r,vmin=-0.25,vmax=0.6,annot=True)\nplt.title('Correlation Heatmap');","2f42faca":"# kde plot for each of the ext data colored bey the target\nplt.figure(figsize=(12,10))\n\n# iterate through the sources\nfor i,source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    # create a new subplot for each source\n    plt.subplot(3,1,1+i)\n    \n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0, source], label='Target==0')\n    # plot for loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1, source], label='Target==1')\n                \n    # Lable the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel(source)\n    plt.ylabel('Density')\n    \n    plt.tight_layout(h_pad = 2.5)\n    ","7bf2f042":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","92052335":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns=['TARGET'])\n\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)\n\n# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","41903e0a":"# This creates a considerable number of new features. \n# To get the names we have to use the polynomial features get_feature_names method.\npoly_transformer.get_feature_names(input_features=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]","b20f63d0":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(15))\nprint(poly_corrs.tail(15))","7c9f362d":"## Merging the polinomial features to our original train set\n\n# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                             columns = poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features,on = 'SK_ID_CURR', how='left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test,on = 'SK_ID_CURR', how='left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","5146360c":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] =  app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']\n\napp_test_domain['CREDIT_INCOME_PERCENT'] =  app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']\n","d128d8ba":"plt.figure(figsize=(12,20))\n# iterate through the new features\n\nfor i,feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    # create new subplot for each source\n    plt.subplot(4,1,1+i)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0,feature], label = 'target == 0')\n    \n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1,feature], label = 'target == 1')\n\n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel(feature);\n    plt.ylabel('Density');\n\n    # For better visibility and clear plot\n    plt.tight_layout(h_pad = 2.5)\n    ","ac0b8605":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# feature names\nfeatures = list(train.columns)\n\n# copy the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy='median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\n# Repeat the same with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","d829d0ed":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train,train_labels)","c2ee74ce":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","80c02cf3":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['Target'] = log_reg_pred\nsubmit.head(10)","d29ecfdd":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","26774c37":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest.fit(train,train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importance = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:,1]","60fe3a98":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","b9f07c29":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = SimpleImputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","bac7d319":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","bf7f57b6":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","7145acf7":"app_train_domain = app_train_domain.drop(columns='TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = SimpleImputer(strategy='median')\n\n# Train the imputer\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Train the scaler\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","5968bd3a":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","03a79b5b":"pwd","afebecd0":"ls","177c87d4":"# Read in Data","76352aed":"## Column Types","dce474e8":"## Correlations","89b29fb8":"## Baseline","03533830":"### Label Encoding and One-Hot Encoding","68c7981a":"# Feature Engineering","7f034955":"## Encoding Categorical Variables","533041a6":"### Aligning Training and Testing Data","62e12e3e":"### Without Feature Engineering","f7dafd20":"## Domain Knowledge Features","1c0c4c9c":"## Pair Plot","6538f515":"## Examine the Distribution of the Target Column","02236d0c":"#### For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition","6a2339cb":"### Without Feature Engineering","b2a39ad5":"## Random Forest","8a1589f0":"## Anomalies","cbf0c6ac":"## Logistic Regression","30faff87":"## Polynomial Features","5588e419":"### With Feature Engineering","79a7c18a":"## Examine Missing Values","0dcae290":"### With Domain Feature Engineering","3d422cc9":"# Predictions","c30bb205":"# Exploratory Data Analysis","ab7e452a":"## Effect of Age on Repayment","ea9dda8c":"## Exterior Sources"}}