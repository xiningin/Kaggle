{"cell_type":{"3a4d444c":"code","956e9c64":"code","d3e5fb46":"code","ae7522a2":"code","fbc6c67c":"code","19cafd6e":"code","7327c9b3":"code","838b949f":"code","f22d979b":"code","2f1f1a40":"code","4bbabcd4":"code","e5fab75c":"code","0654197f":"code","91d7045f":"code","cb9d0ad6":"code","eeb407eb":"code","60464d30":"code","5a70b67f":"code","46e59038":"code","efa11c97":"code","e5e0e007":"markdown","4ff3e302":"markdown","993c9fc6":"markdown","9b89091b":"markdown","2699dd44":"markdown","38d46e17":"markdown","6696e9d6":"markdown","a3cef5a2":"markdown","250c170e":"markdown"},"source":{"3a4d444c":"# import\n\nimport os\nimport bisect\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm\nfrom scipy.io.wavfile import write","956e9c64":"# 16 bit: \u221232,768 to 32,767.\n\n\ndef mu_law_companding_transformation(x, mu=255):\n    return np.sign(x) * (np.log(1 + mu * np.abs(x)) \/ np.log(1 + mu))\n\n\ndef inverse_mu_law_companding_transformation(y, mu=255):\n    return np.sign(y) * (((1 + mu) ** np.abs(y) - 1) \/ mu)\n\n# quantize to 8bit\ndef quantize(wav, bit):\n    wav = mu_law_companding_transformation(wav, 2**bit - 1)\n    return ((wav + 1) * 2**(bit - 1)).astype(int)\n\n# recover to 16bit\ndef inv_quantize(wav, bit):\n    wav = (wav \/ 2**(bit - 1) - 1).astype(float)\n    return inverse_mu_law_companding_transformation(wav, bit**2 - 1)","d3e5fb46":"wav, sr = librosa.load(\"..\/input\/freesound-audio-ds-project\/ProcessedAudio\/baby cry\/1.wav\",sr=11025)\nprint(sr)\nquantized_wav = quantize(wav, 8)\nrecovered_wav = inv_quantize(quantized_wav, 8)\n\nfig, ax = plt.subplots(3, 1)\nax[0].set_title(\"original wave (16 bit)\")\nax[0].plot(wav * 32_767)\nax[1].set_title(\"quantized wave (8 bit)\")\nax[1].plot(quantized_wav)\nax[2].set_title(\"recovered wave (16 bit)\")\nax[2].plot(recovered_wav * 32_767)","ae7522a2":"# recursively locate all the wav files\n\ndef get_files(dir_):\n    def _get_files(fps, dir_):\n        _, ds, fs = next(os.walk(dir_))\n        for f in fs:\n            if f.split('.')[-1] == 'wav':\n                fps.append(os.path.join(dir_, f))\n        for d in ds:\n            _get_files(fps, os.path.join(dir_, d))\n            \n    files = []\n    _get_files(files, dir_)\n    return files\n\nfiles = get_files(\"..\/input\/freesound-audio-ds-project\/ProcessedAudio\/baby cry\")","fbc6c67c":"# create dataset in ndarray format\n# preprocess applied accordingly\n\ndef create_dataset(dataset_path, files, sr=11025):\n    dataset = []\n    for f in tqdm(files):\n        wav, _ = librosa.load(f, sr=sr)\n#         wav = librosa.util.normalize(wav)\n        quantized_wav = quantize(wav, 8)    # 8 bit\n        quantized_wav = np.clip(quantized_wav, 0, 2**8 - 1)\n        dataset.append(quantized_wav)\n    np.savez(dataset_path, *dataset)\n    \ncreate_dataset(\"dataset\", files[:1])    # TODO: files[x: y] -> files","19cafd6e":"class MNISTAudio(Dataset):\n    def __init__(\n        self, \n        path=\"dataset.npz\", \n        src_len=1024+64, \n        tgt_len=64, \n        num_class=256\n    ):\n        self.path = path\n        self.src_len = src_len\n        self.tgt_len = tgt_len\n        self.num_class = num_class\n        \n        self.dataset_index = self.index_dataset()\n        \n    def index_dataset(self):\n        # locate starting index of audio\n        dataset_index = [0]\n        dataset = np.load(self.path, mmap_mode='r')\n        for i in range(len(dataset)):\n            dataset_index.append(dataset_index[-1] \n                                 + dataset['arr_' + str(i)].shape[0])\n        return dataset_index\n    \n    def __getitem__(self, idx):\n        d_idx = bisect.bisect_right(self.dataset_index, idx)\n        dataset = np.load(self.path, mmap_mode='r')\n        \n        if idx + self.src_len + 1 <= self.dataset_index[d_idx]:\n            start_pos = idx - self.dataset_index[d_idx - 1]\n            end_pos = start_pos + self.src_len + 1\n            data = dataset['arr_' + str(d_idx - 1)][start_pos: end_pos]\n        else:\n            start_pos = idx - self.dataset_index[d_idx - 1]\n            end_pos = idx + self.src_len + 1 - self.dataset_index[d_idx]\n            data = np.concatenate((dataset['arr_' + str(d_idx - 1)][start_pos: ], \n                                   dataset['arr_' + str(d_idx)][: end_pos]))\n            \n        src = torch.tensor(data[: self.src_len])\n        src = F.one_hot(src, self.num_class).type(torch.float).transpose(0, 1)\n        tgt = torch.tensor(data[-self.tgt_len:])\n        \n        return {\"src\": src, \"tgt\": tgt}\n        \n    def __len__(self):\n        return self.dataset_index[-1] - self.src_len\n    \n","7327c9b3":"# dataset2 = MNISTAudio(\"dataset.npz\", 64, 32, 256)\n# sum((dataset[0]['src'] == dataset2[0]['src']))\n# # index = torch.tensor([[1, 2, 3]])\/","838b949f":"\n# print(dataset.dataset_index)\n# print(len(dataset.dataset_index))\n## check with float value if aligned correctly\n# print(dataset[0])\n# print(dataset.dataset['arr_0'][: 20])\n# print(dataset[18282])\n# print(dataset.dataset['arr_1'][: 20])\n# print(dataset[18272])\n# print(dataset[18272]['src'][:, 0])\n# print(dataset[18272]['src'].size())\n# print(np.concatenate((dataset.dataset['arr_0'][-10: ], dataset.dataset['arr_1'][: 10])))\n\n# for i in np.linspace(0, 1468083, 1000):\n#     i = int(i)\n#     if dataset[i]['src'].size()[-1] != 64:\n#         print(\"error\")\n#         print(dataset[i]['src'].size())\n#         print(dataset[i]['tgt'].size())\n#         print(i)\n#         break\n\n# dataloader = DataLoader(dataset, batch_size=2)\n# batch = next(iter(dataloader))\n# batch['src'].size()","f22d979b":"# dilated causal convolution\n\nclass CausalConv1d(nn.Module):\n    def __init__(self, \n                 in_channels, \n                 out_channels, \n                 kernel_size, \n                 dilation=1, \n                 **kwargs):\n        super(CausalConv1d, self).__init__()\n        self.padding = (kernel_size - 1) * dilation\n        self.conv = nn.Conv1d(in_channels, \n                              out_channels, \n                              kernel_size, \n                              padding=self.padding, \n                              dilation=dilation, \n                              bias=False, \n                              **kwargs)\n    \n    def forward(self, input_):\n        return self.conv(input_)[:, :, :-self.padding] if self.padding else self.conv(input_)\n\n\n# cc = CausalConv1d(1, 1, 2, dilation=2, bias=False)\n# x = torch.arange(1, 11, dtype=torch.float).view(2, 1, 5)\n# x[:, :, [1, 3]] = 0.0\n# print(x[0])\n# cc(x[:1])\n","2f1f1a40":"# gated activation units\nclass GatedActivationUnit(nn.Module):\n    def __init__(self):\n        super(GatedActivationUnit, self).__init__()\n    \n    def forward(self, input_):\n        return torch.tanh(input_) * torch.sigmoid(input_)\n\n\n# actv = GatedActivationUnit()\n# actv(torch.ones(1))\n","4bbabcd4":"class WaveNet(nn.Module):\n    def __init__(\n        self,\n        num_block=4,\n        num_layer=10,\n        class_dim=256,\n        residual_dim=32,\n        dilation_dim=32,\n        skip_dim=256,\n        kernel_size=2,\n        bias=False\n    ):\n        super(WaveNet, self).__init__()\n        self.start_conv = nn.Conv1d(in_channels=class_dim, \n                                    out_channels=residual_dim, \n                                    kernel_size=1, \n                                    bias=bias)\n        \n        self.stack = nn.ModuleList()\n        for b in range(num_block):\n            dilation = 1\n            for k in range(num_layer):\n                \n                layer = nn.Sequential(\n                    CausalConv1d(in_channels=residual_dim,\n                                 out_channels=dilation_dim, \n                                 kernel_size=kernel_size,\n                                 dilation=dilation),\n                    GatedActivationUnit(),\n                    nn.Conv1d(in_channels=dilation_dim, \n                              out_channels=residual_dim, \n                              kernel_size=1, \n                              bias=bias)\n                )\n                \n                self.stack.append(layer)\n                dilation *= 2\n        \n        self.end_conv = nn.Sequential(\n            nn.ReLU(), \n            nn.Conv1d(in_channels=residual_dim, \n                      out_channels=skip_dim, \n                      kernel_size=1, \n                      bias=bias),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=skip_dim, \n                      out_channels=class_dim, \n                      kernel_size=1, \n                      bias=bias)\n        )\n        \n        \n    def forward(self, input_):\n        residual = self.start_conv(input_)\n        skips = torch.zeros_like(residual)\n        \n        for layer in self.stack:\n            skip = layer(residual)\n            residual = residual + skip\n            skips = skips + skip\n        logit = self.end_conv(skips)\n        return logit\n\n    \n        ","e5fab75c":"# n, c, l = 1, 2, 3\n# x = torch.zeros(n, c, l)\n# # x = \n# # output...\n# # softmax\n# # sample from the index\n# n, c, l\n# logit = torch.randn(32, 256, 10)\n# i = 0\n# dist = F.softmax(logit[:, :, i], 1)\n# print(dist.size())\n# torch.multinomial(dist, num_samples=1).size()\n\n# # ...","0654197f":"# x = torch.zeros(1, 256, 100).scatter_(1, torch.tensor(quantized_wav[:100]).view(1, 1, -1), 1.0)\n# print(x.size())\n# model = WaveNet()\n# model(x).size()","91d7045f":"# k = torch.zeros(1, 1, 6)\n# k[0, 0, [1]] = 1\n# print(k)\n# c = CausalConv1d(1, 1, 3, 2, bias=False)\n# c(k)","cb9d0ad6":"\ndef save_ckpt(path, model, optimizer, misc):\n    model.eval()\n    model.cpu()\n    torch.save({\n        'last_epoch': misc['epoch'],\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'losses': misc['losses'],\n    }, path)\n    \n\ndef generate_audio(model, audio_num, audio_len, num_class, receptive_field=1024):\n    model.eval()\n    n, c, l = audio_num, num_class, audio_len\n    device = next(model.parameters()).device\n    input_ = torch.zeros(n, c, 1, device=device)   # [N, C, L + 1]\n    \n    for i in tqdm(range(l)):\n        pred = model(input_[:, :, -receptive_field: ])\n        dist = F.softmax(pred[:, :, -1], dim=1)\n        sample = torch.multinomial(dist, num_samples=1)\n        one_hot = F.one_hot(sample.view(n, 1), num_class).type(torch.float).permute(0, 2, 1)\n        input_ = torch.cat((input_, one_hot), dim=-1)\n#         input_ = torch.cat((input_, torch.zeros(n, c, 1, device=device).scatter_(1, sample.view(n, 1, 1), 1.0)), -1)\n        \n    audio = input_.argmax(1).to('cpu').numpy()\n    audio = inv_quantize(audio, 8)\n    return audio\n\n# audio = generate_audio(model, 1, 100, 256)\n# plt.plot(audio[0])","eeb407eb":"from dataclasses import dataclass\n\n@dataclass\nclass Option:\n    DEVICE: str = 'cuda'\n    epoch: int = 20\n    lr: float = 1e-3\n    batch_sz: int = 128\n    num_class: int = 256\n    clip: float = 1.0\n    max_itr: int = 1_000\n    \n    src_len:int = 1024 + 64\n    tgt_len:int = 64\n    \n    num_block: int = 4\n    num_layer: int = 10\n    residual_dim: int = 32\n    dilation_dim: int = 128\n    skip_dim: int = 256\n    kernel_size: int = 2\n    bias: bool = False\n    \n    loss_update_itr: int = 100\n    ckpt_path: str = \"\"\n    dataset_path: str = \"dataset.npz\"\n    ckpt_dir: str = \"ckpt\"\n\nopt = Option()\nos.makedirs(opt.ckpt_dir, exist_ok=True)\n\n# prepare dataset and dataloader\ndataset = MNISTAudio(opt.dataset_path, opt.src_len, opt.tgt_len, opt.num_class)    # TODO: give parameter accordingly\ndataloader = DataLoader(dataset, \n                        batch_size=opt.batch_sz, \n                        shuffle=True, \n                        num_workers=2)\npbar = tqdm(range(opt.epoch * min(opt.max_itr, len(dataloader))))\n\n# prepare model\nmodel = WaveNet(    \n    num_block = opt.num_block,\n    num_layer = opt.num_layer,   # 10,\n    class_dim = opt.num_class,\n    residual_dim = opt.residual_dim,\n    dilation_dim = opt.dilation_dim,\n    skip_dim = opt.skip_dim,\n    kernel_size = opt.kernel_size,\n    bias=opt.bias\n)\n\n# prepare optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=opt.lr)\n\nlosses = []\nlast_epoch = 0\n\n# load from checkpoint\nif opt.ckpt_path:\n    ckpt = torch.load(opt.ckpt_path)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    losses = ckpt['losses']\n    last_epoch = ckpt['last_epoch']\n\n    \n# train\n\nfor e in range(last_epoch, opt.epoch):\n    model.train()          # save_ckpt set model mode to eval and cpu\n    model.to(opt.DEVICE)\n    accum_loss = 0\n    \n    for idx, batch in enumerate(dataloader):\n        src, tgt = batch['src'].to(opt.DEVICE), batch['tgt'].to(opt.DEVICE)\n        pred = model(src)[:, :, -opt.tgt_len: ]\n        loss = loss_fn(pred, tgt)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)\n        optimizer.step()\n\n        accum_loss += loss.item()\n        pbar.update()\n        \n        if idx == opt.max_itr:\n            break\n\n        if (idx + 1) % opt.loss_update_itr == 0:\n            avg_loss = accum_loss \/ opt.loss_update_itr\n            losses.append(avg_loss)\n            accum_loss = 0\n            pbar.set_description(f\"Epoch {round(e + idx \/ min(opt.max_itr, len(dataloader)), 2)} | Loss: {round(avg_loss, 5)}\")\n    print()    # new line for pbar\n    # save checkpoint\n    save_ckpt(os.path.join(opt.ckpt_dir, str(e) + '.pt'), model, optimizer, {\"epoch\": e + 1, \"losses\": losses})","60464d30":"plt.title(\"loss\")\nplt.plot(losses)","5a70b67f":"model.to(opt.DEVICE)\naudio = generate_audio(model, 16, 40_000, 256)\nfig, ax = plt.subplots(3, 1)\n\nfor i in range(3):\n    ax[i].plot(audio[i])\n    write(f'output_{i}.wav', 11025, audio[i])","46e59038":"plt.plot(inv_quantize(np.load(\"dataset.npz\")[\"arr_0\"], 8))\nwrite('audio_0.wav', 11025, inv_quantize(np.load(\"dataset.npz\")[\"arr_0\"], 8))\n","efa11c97":"wav, _ = librosa.load(files[0], sr=11025)\nplt.plot(wav)\n# write('audio_0.wav', 11025, wav)","e5e0e007":"# WaveNet\n\n[paper] \\\nhttps:\/\/arxiv.org\/pdf\/1609.03499.pdf \\\nThe WaveNet proposed by Aaron van den Oord in 2016 autoregressively generates audio. Its novelty comes from convolutional architecture to train on temporal data.\n\n[implementation] \\\nhttps:\/\/github.com\/vincentherrmann\/pytorch-wavenet\/blob\/master\/audio_data.py \\\n_(Implementation details are heavily inspired by the above github repo)_","4ff3e302":"### Gated Activation Units\n\n$$z = \\tanh (W_{f, k} * x) \\odot \\sigma (W_{g, k} * x)$$","993c9fc6":"### $\\mu$-law companding transformation\n\n$$f(x_t) = sign(x_t) {{\\ln (1 + \\mu |x_t|)}\\over{\\ln (1 + \\mu)}}, \\quad -1 \\leq x \\leq 1$$\n$$f^{-1}(y_t) = sign(y_t) {{(1 + \\mu)^{|y_t|} - 1}\\over{\\mu}}, \\quad -1 \\leq y \\leq 1$$\n$\\mu$-law companding transformation function and its inverse function. This allows to perform tractable classificaiton task","9b89091b":"## Train\n\nClassification Task: Train with softmax loss","2699dd44":"## Data\n\nThe notebook is currently trained on audio-cats-and-dogs but will be trained on VCTK in the end. The below is the list of dataset tried with the notebook.\n* ~~audio-mnist~~\n* audio-cats-and-dogs\n\n_[Jan 20, 2022] Changed dataset to audio-cats-and-dogs from audio-mnist as the training on audio-mnist results in generating signals with no sound. I posit the reason to be the long tail and head with no sound for each data._\n\n### Preparing data for training\nFormat input(src) and label(tgt) in the following way.\n\n<img src=\"https:\/\/i.ibb.co\/Yp2KG8B\/src-tgt-drawio.png\" width=\"600\"\/>\n\nOne-hot encoding is applied to the src. Thus, the shape of src is [N, C, L] and label is [N, L].","38d46e17":"### WaveNet Model\n\n\n<img src=\"https:\/\/nvidia.github.io\/OpenSeq2Seq\/html\/_images\/wavenet.png\" width=400>\n\n_make sure to show dimension_","6696e9d6":"## Model","a3cef5a2":"### Dilated Causal Convolution\n<img src=\"https:\/\/production-media.paperswithcode.com\/methods\/Screen_Shot_2020-05-24_at_12.11.35_AM_RLzUMeS.png\" width=\"400\"\/>\n\n\n\n[link] https:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/README.md\n\n[link2] https:\/\/discuss.pytorch.org\/t\/causal-convolution\/3456\/4\n","250c170e":"## Preprocess\n* quantization ($\\mu$-law companding transformation)\n* ~~normalization~~"}}