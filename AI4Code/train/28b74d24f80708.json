{"cell_type":{"76d0dcad":"code","4b1dbcd3":"code","3bcd2d6d":"code","e75b8cc8":"code","23c51af1":"code","385cbd68":"code","300aa19e":"code","0260d67d":"code","6386f4a4":"code","18189001":"code","dd3d9b61":"code","24fef5e6":"code","ce99780b":"code","a6c65abc":"code","ee0dad52":"code","642111d6":"code","aa51e45a":"code","6df7dc18":"code","c9737d52":"markdown","de1671cb":"markdown","d95f5a17":"markdown","0364725e":"markdown","ffb20c80":"markdown"},"source":{"76d0dcad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b1dbcd3":"!pip install pyspark","3bcd2d6d":"from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nspark = SparkSession.builder.getOrCreate()","e75b8cc8":"spark","23c51af1":"f= open(\"test1.csv\",\"w+\")\nfor i in range(10):\n     f.write(\"This is line %d\\r\\n\" % (i+1))\nf.close()","385cbd68":"!>test1.csv\n!cat test1.csv\n!echo \"Name,Age\">>test1.csv\n!echo \"sudhanshu,31|\">>test1.csv\n!echo \"sanjay,30\">>test1.csv\n!echo \"sunny,29\">>test1.csv\n!cat test1.csv","300aa19e":"df_pyspark=spark.read.csv('test1.csv')","0260d67d":"df_pyspark=spark.read.csv('test1.csv',inferSchema=True, header=True)\ndf_pyspark.show()","6386f4a4":"# dealing with bad records\n\n#mode=dropmalformed remove the dirty records \n#mode=failfast   fails the load and don't insert anything\n#mode=permissive  convert them to null\n#.option(\"badRecordsPath\",\"\/tmp\/badRecordsPath\")  moves them to other folder\n#.schema(\"ID Integer,Name String,DOJ Date,Salary Integer\")","18189001":"arrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{}) ]\n\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\ndf.printSchema()\ndf.show()","dd3d9b61":"from pyspark.sql.functions import explode\ndf2 = df.select(df.name,explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()","24fef5e6":"from pyspark.sql.functions import explode\ndf3 = df.select(df.name,explode(df.properties))\ndf3.printSchema()\ndf3.show()","ce99780b":"from pyspark.sql.functions import explode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,explode_outer(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,explode_outer(df.properties)).show()","a6c65abc":"from pyspark.sql.functions import posexplode\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode(df.properties)).show()","ee0dad52":"from pyspark.sql.functions import posexplode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode_outer(df.knownLanguages)).show()\n\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode_outer(df.properties)).show()","642111d6":"data = [\"Project Gutenberg\u2019s\",\n        \"Alice\u2019s Adventures in Wonderland\",\n        \"Project Gutenberg\u2019s\",\n        \"Adventures in Wonderland\",\n        \"Project Gutenberg\u2019s\"]\nrdd=spark.sparkContext.parallelize(data)\nfor element in rdd.collect():\n    print(element)","aa51e45a":"rdd2=rdd.flatMap(lambda x: x.split(\" \"))\nfor element in rdd2.collect():\n    print(element)","6df7dc18":"\n# flatmap by dataframe\narrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})]\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n\nfrom pyspark.sql.functions import explode\ndf2 = df.select(df.name,explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()","c9737d52":"### Bad records handling\n<!-- # https:\/\/medium.com\/@11amitvishwas\/how-to-handle-bad-records-corrupt-records-in-apache-spark-392f2991cbb5 -->","de1671cb":"### scd types\n\n\nType 0 \u2013 Fixed Dimension\nNo changes allowed, dimension never changes\nType 1 \u2013 No History\nUpdate record directly, there is no record of historical values, only current state\nType 2 \u2013 Row Versioning\nTrack changes as version records with current flag & active dates and other metadata\nType 3 \u2013 Previous Value column\nTrack change to a specific attribute, add a column to show the previous value, which is updated as further changes occur\nType 4 \u2013 History Table\nShow current value in dimension table but track all changes in separate table","d95f5a17":"### Explode Explode_out posexplode posexplode_out","0364725e":"### File Types:\n\navro: when want to change schema multiple times ,middle speed\n\ntext: write fast\n\nparquet: faster read in spark\n\norc: faster read in hive\n\n\nstg: write faster\n\ncore tables: read faster","ffb20c80":"### Map vs flatmap"}}