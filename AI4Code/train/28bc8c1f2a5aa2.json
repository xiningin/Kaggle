{"cell_type":{"6de387c9":"code","4d641bc6":"code","1dcbbc37":"code","0fa1fef3":"code","a1811537":"code","96a9911a":"code","d6b66e88":"code","81c4b226":"code","fd1a08d1":"code","322040f3":"code","869d5f73":"code","5348df34":"code","09e6f977":"code","4b530f40":"code","4ede8837":"code","35105dac":"code","ef629535":"code","4f2b1a82":"code","3a7f2885":"code","f44a2d6d":"code","1edb5d3e":"code","749ddcd1":"code","41d9304f":"code","fd482c13":"code","f8bf405c":"code","fd8aad44":"code","a200f8e5":"code","772418b5":"code","9267fd8f":"code","70636702":"code","b52a6062":"code","470808bc":"code","ac8b4b48":"code","280b525b":"code","ef93a580":"code","dc2e7015":"code","45886849":"code","7729ee9f":"code","5a6d0600":"code","20d72364":"code","e95f4165":"code","0eee8753":"code","a8d55f12":"code","56ae8d99":"code","ebaed3e1":"code","baa692ad":"code","d38ae801":"code","ac0ae0d6":"code","67e4f64e":"code","56872eec":"code","f36f8818":"code","11722bf5":"code","2fa8ee28":"code","02b91565":"code","797ed3f4":"code","6e28af93":"code","060c3f15":"code","b0d3c604":"code","81480f0e":"code","9a2743ce":"code","146f124d":"code","13c17d6a":"code","2361f290":"code","ca603ae4":"code","c354007a":"code","bad36429":"code","b2b14f9b":"code","5374c95f":"code","335d7c2d":"code","21f7c0c1":"code","bf511134":"code","81e2974f":"code","c7a2dd71":"code","2d8396a0":"code","b23321eb":"code","af63bf82":"code","18e3e009":"code","b449ce78":"code","95765582":"markdown","cddb1b5a":"markdown","2a83c099":"markdown","0c86df3a":"markdown","a426994e":"markdown","752a0982":"markdown","dc089212":"markdown","a8e244dc":"markdown","16fac4aa":"markdown","8b8b624b":"markdown","7bea47b5":"markdown","078501b3":"markdown","1d860ac8":"markdown","e4780556":"markdown","cbda6844":"markdown","516d0002":"markdown"},"source":{"6de387c9":"# Set this param to false for better performance, at thos ecost of longer run time. \nFAST_RUN = True","4d641bc6":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nfrom sklearn.ensemble import IsolationForest\n\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","1dcbbc37":"%%time\nif FAST_RUN:\n    print(\"using sampled data , fast run\")\n    train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID').sample(frac=0.3)\nelse:\n    train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')# ,nrows=12345)\n\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')#,nrows=123)\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n\ntrain.head()","0fa1fef3":"# ## join train+test for easier feature engineering:\n# df = pd.concat([train,test],sort=False)\n# print(df.shape)","a1811537":"list(train.columns)\n\nif FAST_RUN:\n    COLUMN_GROUP_PREFIXES = [\"card\",\"M\",\"id\"]\n    \nelse: COLUMN_GROUP_PREFIXES = [\"card\",\"C\",\"D\",\"M\",\"V\",\"id\"]  # ,\"V\" # \"C\" , \"V\" # V has many values, slow, but does contribute. \n\ndef column_group_features(df):\n    \"\"\"\n    Note: surprisingly slow! \n    TODO: Check speed, e.g. with `$ pip install line_profiler`\n    \"\"\"\n    df[\"total_missing\"] = df.isna().sum(axis=1)\n    print(\"total_missing\",df[\"total_missing\"].describe(percentiles=[0.5]))\n    df[\"total_unique_values\"] = df.nunique(axis=1,dropna=False)\n    print(\"total_unique_values\",df[\"total_unique_values\"].describe(percentiles=[0.5]))\n    \n    for p in COLUMN_GROUP_PREFIXES:\n        col_group = [col for col in df if col.startswith(p)]\n        print(\"total cols in subset:\", p ,len(col_group))\n        df[p+\"_missing_count\"] = df[col_group].isna().sum(axis=1)\n        print(p+\"_missing_count\", \"mean:\",df[p+\"_missing_count\"].describe(percentiles=[]))\n        df[p+\"_uniques_count\"] = df[col_group].nunique(axis=1,dropna=False)\n        print(p+\"_uniques_count\", \"mean:\",df[p+\"_uniques_count\"].describe(percentiles=[]))\n#         df[p+\"_max_val\"] = df[col_group].max(axis=1)\n#         df[p+\"_min_val\"] = df[col_group].min(axis=1)\n    print(\"done \\n\")\n    return df\n\n\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df,do_categoricals=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            if do_categoricals==True:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","96a9911a":"# %%time\n\ntrain = reduce_mem_usage(train,do_categoricals=False)\ntest = reduce_mem_usage(test,do_categoricals=False)","d6b66e88":"%%time\n\ntrain = column_group_features(train)\nprint(\"train features generated\")\n\ntest = column_group_features(test)\n\ntrain.head()","81c4b226":"import datetime\n\nSTART_DATE = \"2017-11-02\"\n\n# Preprocess date column\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\ntrain['time'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['time'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n## check if time of day is morning\/early night, and\/or weekend\/holiday:\ntrain[\"hour_of_day\"] = train['time'].dt.hour\ntest[\"hour_of_day\"] = test['time'].dt.hour\n\n## check if time of day is morning\/early night, and\/or weekend\/holiday: (day of the week with Monday=0, Sunday=6.)\ntrain[\"day_of_week\"] = train['time'].dt.dayofweek\ntest[\"day_of_week\"] = test['time'].dt.dayofweek\n\nprint(train['time'].describe())\nprint(test['time'].describe())","fd1a08d1":"## no clear correlation, but we expect any such features to be categorical in nature, not ordinal\/continous. the model can findi t\ntrain[[\"isFraud\",\"hour_of_day\",\"day_of_week\"]].sample(frac=0.07).corr()","322040f3":"# ID_COLS = [col for col in train if col.startswith(\"id\")] # + (['DeviceType', 'DeviceInfo'])\n\nID_COLS =['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', \n 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26',\n 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n 'DeviceType', 'DeviceInfo']\nprint(ID_COLS)","869d5f73":"train[ID_COLS].sample(12).drop_duplicates()","5348df34":"train['DeviceType'].value_counts()","09e6f977":"train['DeviceInfo'].value_counts()","4b530f40":"print(\"percent of rows with duplicated identities in data: {:.5}% \".format(100*train[ID_COLS].duplicated().sum()\/train.shape[0]))","4ede8837":"train.duplicated(subset=ID_COLS).sum()","35105dac":"ID_COLS_EXT =['id_01', 'id_02', 'id_03', 'id_04', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n 'DeviceType', 'DeviceInfo',\"ProductCD\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n              'addr1', \n              'addr2'\n      , 'P_emaildomain', \n#               'R_emaildomain'\n              ]\n\n\nID_COLS =[ 'DeviceType',\n          \"ProductCD\",\n          \"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n       'P_emaildomain', \n#           'addr1', \n              ]\n\n\n# much siompler, basically just \"did they use this trans.amount. Would benefit from loh10 rounding the amount\" : \nMINI_ID_COLS = [\"card1\",\"card2\"] # ,\"card3\",\"card4\",\"card5\",\"card6\",\"ProductCD\"  ","ef629535":"train[ID_COLS].head(8).drop_duplicates()","4f2b1a82":"train[MINI_ID_COLS].drop_duplicates().shape[0]","3a7f2885":"train[MINI_ID_COLS].head().drop_duplicates()","f44a2d6d":"print(\"% EXT rows with duplicated identities: {:.4}% \".format(100*train[ID_COLS_EXT].duplicated().sum()\/train.shape[0]))\n\nprint(\"Without TransactionAmt,  % rows with duplicated identities: {:.4}% \".format(100*train[ID_COLS].duplicated().sum()\/train.shape[0]))\n\n\nprint(\"% duplicatecard: {:.4}% \".format(100*train[MINI_ID_COLS].duplicated().sum()\/train.shape[0]))\n","1edb5d3e":"train.columns[420:]","749ddcd1":"df_all = pd.concat([train,test],sort=True)\nprint(df_all.shape)","41d9304f":"# df_all[\"duplicated_extended\"] = df_all[ID_COLS_EXT].duplicated().astype(int)\n\ndf_all[\"duplicated_base\"] = df_all[ID_COLS].duplicated().astype(int)\n\ndf_all[\"duplicated_card\"] = df_all[MINI_ID_COLS].duplicated().astype(int)","fd482c13":"%%time\n## size includes NaN values, count does not.\n\ndf_all[\"card_hash\"] = df_all[MINI_ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\ndf_all[\"card_hash_total_counts\"] = df_all.groupby(\"card_hash\")[\"total_missing\"].transform(\"size\") \n\n\ndf_all[\"multIDcols_hash\"] = df_all[ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\ndf_all[\"multIDcols_total_counts\"] = df_all.groupby(ID_COLS)[\"total_missing\"].transform(\"size\") \n\n# df_all[\"hash_PlusTransamt\"] = df_all[ID_COLS_EXT].apply(lambda x: hash(tuple(x)), axis = 1)\n# df_all[\"hash_PlusTransamt_total_counts\"] = df_all.groupby(\"hash_PlusTransamt\")[\"total_missing\"].transform(\"size\") \n\n# # count # transaction amount reoccurred +- trans type\n\n# df_all[\"hash_Transamt\"] = df_all[MINI_ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\n# df_all[\"hash_Transamt_total_counts\"] = df_all.groupby(\"hash_Transamt\")[\"total_missing\"].transform(\"size\") \n\n\n\n# # drop some  the hashed ids that uses transaction amount\n# df_all.drop(['hash_PlusTransamt',\"hash_Transamt\"], axis=1, inplace=True)\n","f8bf405c":"train.head(2)","fd8aad44":"%%time\n\n## Spending vs mean\ndf_all[\"TransactionAmt_count\"] = df_all.groupby([\"TransactionAmt\"])[\"card1\"].transform(\"size\")\n\n## more (count) feats\n\ndf_all[\"TransactionAmt_count\"] = df_all.groupby([\"TransactionAmt\"])[\"card1\"].transform(\"size\")\n\ndf_all[\"card1_count\"] = df_all.groupby([\"card1\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card2_count\"] = df_all.groupby([\"card2\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card3_count\"] = df_all.groupby([\"card3\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card5_count\"] = df_all.groupby([\"card5\"])[\"ProductCD\"].transform(\"size\")\n\ndf_all[\"R_email_count\"] = df_all.groupby([\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"P_email_count\"] = df_all.groupby([\"P_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"addr1_count\"] = df_all.groupby([\"addr1\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"addr2_count\"] = df_all.groupby([\"addr2\"])[\"ProductCD\"].transform(\"size\")\n# joint column count\ndf_all[\"P_R_emails_count\"] = df_all.groupby([\"P_emaildomain\",\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"joint_addresses_count\"] = df_all.groupby([\"addr1\",\"addr2\"])[\"ProductCD\"].transform(\"size\")","a200f8e5":"## transactions per hour\n## https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#resampling\n## https:\/\/stackoverflow.com\/questions\/45922291\/average-number-of-actions-per-day-of-the-week-using-pandas\n\n## results seem inconsistent between the 2 functions. need to debug. \n# df_all.head(8234).set_index(\"time\").resample(\"H\").size()\n# df_all.head(2234).groupby([df_all.time.dt.hour,df_all.time.dt.dayofyear])[\"ProductCD\"].transform(\"size\")\n\ndf_all[\"events_this_hour_cnt\"] = df_all.groupby([df_all.time.dt.hour,df_all.time.dt.dayofyear])[\"ProductCD\"].transform(\"size\")\n\n####\n\n# mean spend for an hour of day and DoW. could add more aggregations\ndf_all[\"mean_spend_hour_day\"] = df_all.groupby([df_all.time.dt.hour,df_all.time.dt.dayofweek])[\"TransactionAmt\"].transform(\"mean\")\n# spend vs mean\ndf_all[\"normalized_spend_vs_hour_day_mean\"] = df_all[\"TransactionAmt\"].div(df_all[\"mean_spend_hour_day\"])\n\n# spend vs that specific day of year\ndf_all[\"normalized_spend_vs_dayOfYear\"] = df_all[\"TransactionAmt\"].div(df_all.groupby([df_all.time.dt.day])[\"TransactionAmt\"].transform(\"mean\"))\n","772418b5":"## Frequency Encoding\ntemp = df_all['card1'].value_counts().to_dict()\ndf_all['card1_counts'] = df_all['card1'].map(temp)","9267fd8f":"## Additional mean Aggregations \/ Group Statistics\ntemp = df_all.groupby('card1')['TransactionAmt'].agg(['median']).rename({'median':'TransactionAmt_card1_median'},axis=1)\ndf_all = pd.merge(df_all,temp,on='card1',how='left')\n\n\ntemp = df_all.groupby('card_hash')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card_hash_mean'},axis=1)\ndf_all = pd.merge(df_all,temp,on='card_hash',how='left')","70636702":"list(df_all.columns)","b52a6062":"########################### TransactionDT\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\n## USA holidays\ndf_all['is_holiday'] = (df_all['time'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)","470808bc":"df_all.columns[432:]","ac8b4b48":"ADDED_FEATS = [\n       'duplicated_base', 'duplicated_card', 'card_hash',\n       'card_hash_total_counts', 'multIDcols_total_counts', 'multIDcols_hash',\n       'TransactionAmt_count', 'card1_count', 'card2_count', 'card3_count',\n       'card5_count', 'R_email_count', 'P_email_count', 'addr1_count',\n       'addr2_count', 'P_R_emails_count', 'joint_addresses_count',\n       'events_this_hour_cnt', 'mean_spend_hour_day',\n       'normalized_spend_vs_hour_day_mean', 'normalized_spend_vs_dayOfYear',\n    'TransactionAmt_card1_median', 'TransactionAmt_card_hash_mean', 'card1_counts'\n    ,'is_holiday']","280b525b":"# df_all = reduce_mem_usage(df_all,do_categoricals=False)","ef93a580":"print(train.shape)\ntrain = train.join(df_all[ADDED_FEATS])\nprint(train.shape)\nprint()\nprint(test.shape)\ntest = test.join(df_all[ADDED_FEATS])\nprint(test.shape)","dc2e7015":"# check if columns have unary vals\nnunique = test.apply(pd.Series.nunique)\ncols_to_drop = nunique[nunique == 1].index\ntest.drop(cols_to_drop, axis=1,inplace=True)\n\ntrain.drop(cols_to_drop, axis=1,inplace=True)\nprint(train.shape)\nprint(test.shape)","45886849":"import gc","7729ee9f":"# Drop target, fill in NaNs ?\n# consider dropping the TransactionDT column as well...\nX_train = train.drop(['isFraud',\"time\",'TransactionDT'], axis=1).copy()\nX_test = test.drop([\"time\",'TransactionDT'], axis=1).copy()\n\ny_train = train['isFraud'].copy() # recopy\n\n\ndel train, test\ngc.collect()\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","5a6d0600":"### xgboost can handlke nans itself, often better (more efficient memory use) BUT our  test data has diff nan distrib ? \n\nX_train.fillna(-999,inplace=True)\nX_test.fillna(-999,inplace=True)","20d72364":"%%time\n\nX_train = reduce_mem_usage(X_train,do_categoricals=True)\nX_test = reduce_mem_usage(X_test,do_categoricals=True)","e95f4165":"# df_all = pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]).drop([\"TransactionDT\"],axis=1).dropna(axis=1)\n# TR_ROWS = X_train.shape[0]\n# NO_NAN_COLS = df_all.columns\n# print(\"num of no nan cols\",len(NO_NAN_COLS))\n# print(\"columns with no missing vals: \\n\",NO_NAN_COLS)\n# print(df_all.shape)","0eee8753":"%%time\nclf = IsolationForest(random_state=82,  max_samples=0.05, bootstrap=False, n_jobs=2,\n                          n_estimators=100,max_features=0.98,behaviour=\"new\",contamination= 0.035)\nclf.fit(pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]))\n# del (df_all)","a8d55f12":"## add anomalous feature.\n## Warning! this is brittle! be careful with the columns!!\n\n# X_train[\"isolation_overall_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\n# X_test[\"isolation_overall_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\nX_train[\"isolation_overall_score\"] =clf.decision_function(X_train)\nX_test[\"isolation_overall_score\"] =clf.decision_function(X_test)\n\n\nprint(\"Fraud only mean anomaly score\",X_train.loc[y_train==1][\"isolation_overall_score\"].mean())\nprint(\"Non-Fraud only mean anomaly score\",X_train.loc[y_train==0][\"isolation_overall_score\"].mean())","56ae8d99":"df_all[\"isolation_overall_score\"] = pd.concat([X_train[\"isolation_overall_score\"],X_test[\"isolation_overall_score\"] ])\n\ndf_all[\"isolation_overall_score\"].describe()","ebaed3e1":"# train only on non fraud samples\n\nclf = IsolationForest(random_state=31,  bootstrap=True,  max_samples=0.05,n_jobs=3,\n                          n_estimators=100,behaviour=\"new\",max_features=0.96) #\n# clf.fit(X_train[NO_NAN_COLS].loc[y_train==1].values)\nclf.fit(X_train.loc[y_train==1].values)\n\n# X_train[\"isolation_pos_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\n# X_test[\"isolation_pos_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\nX_train[\"isolation_pos_score\"] =clf.decision_function(X_train)\nX_test[\"isolation_pos_score\"] =clf.decision_function(X_test)\n\n# del (clf)\n\nprint(\"Fraud only mean pos-anomaly score\",X_train.loc[y_train==1][\"isolation_pos_score\"].mean())\nprint(\"Non-Fraud only mean pos-anomaly score\",X_train.loc[y_train==0][\"isolation_pos_score\"].mean())","baa692ad":"df_all[\"isolation_pos_score\"] = pd.concat([X_train[\"isolation_pos_score\"],X_test[\"isolation_pos_score\"] ])\n\ndf_all[\"isolation_pos_score\"].describe()","d38ae801":"## some hyperparams, made faster for fast runs. \n\nif FAST_RUN:\n    EPOCHS = 2\n    model_num_estimators = 300\n    model_lr = 0.2\nelse: \n    EPOCHS = 4 # use more for better perf, but greater risk of overfitting\n    model_num_estimators = 700\n    model_lr = 0.06\n    \n    \nkf = KFold(n_splits = EPOCHS, shuffle = False)\nkf_time = TimeSeriesSplit(n_splits = EPOCHS) # temporal validation. use this to evaluate performance better , not necessarily as good for OOV ensembling though!","ac0ae0d6":"# %%time\n\n\n# y_preds = np.zeros(sample_submission.shape[0])\n# y_oof = np.zeros(X_train.shape[0])\n# for tr_idx, val_idx in kf.split(X_train, y_train):\n#     clf = xgb.XGBClassifier(#n_jobs=2,\n#         n_estimators=500,  # 500 default\n#         max_depth=9, # 9\n#         learning_rate=0.05,\n#         subsample=0.9,\n#         colsample_bytree=0.9,\n# #         tree_method='gpu_hist' # #'gpu_hist', - faster, less exact , \"gpu_exact\" - better perf\n# #         ,min_child_weight=2 # 1 by default\n#     )\n    \n#     X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n#     y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n#     clf.fit(X_tr, y_tr)\n#     y_pred_train = clf.predict_proba(X_vl)[:,1]\n#     y_oof[val_idx] = y_pred_train\n#     print('ROC AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    \n#     y_preds+= clf.predict_proba(X_test)[:,1] \/ EPOCHS","67e4f64e":"%%time\n## second approach:\n\nclf = xgb.XGBClassifier(n_jobs=3,\n    n_estimators=model_num_estimators,  # 500 default\n    max_depth=11, # 9\n    learning_rate=model_lr, # 0.05 better\n    subsample=0.9,\n    colsample_bytree=0.9\n    ,tree_method= 'hist' # #'gpu_hist', - faster, less exact , \"gpu_exact\" - better perf , \"auto\", 'hist' (cpu)\n        ,min_child_weight=2 # 1 by default\n)\n\n\nclf.fit( X_train,y_train)\ny_preds = clf.predict_proba(X_test)[:,1]","56872eec":"import matplotlib.pyplot as plt\n%matplotlib inline\n# fi = pd.DataFrame(index=clf.feature_names_)\nfi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0004].sort_values('importance').head(50).plot(kind='barh', figsize=(14, 32), title='Feature Importance')\nplt.show()","f36f8818":"# make submissions\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('dan_xgboost_cpu_hist_singlemodel.csv')","11722bf5":"print(\"mean AUC\",cross_val_score(clf, X_train,y_train, cv=kf, scoring='roc_auc').mean())\n","2fa8ee28":"print(\"mean temporal CV AUC\",cross_val_score(clf, X_train,y_train, cv=kf_time, scoring='roc_auc').mean())","02b91565":"X_train[\"random_cv_preds\"] = cross_val_predict(clf, X_train,y_train, method='predict_proba')[:,1]\nX_test[\"random_cv_preds\"] = y_preds","797ed3f4":"# X_train[\"temporal_cv_preds\"] = cross_val_predict(clf, X_train,y_train, cv=kf_time.split(X_train),  method='predict_proba')[:,1]\n# X_test[\"temporal_cv_preds\"] = y_preds","6e28af93":"%%time\n## second approach:\n\nclf = xgb.XGBClassifier(n_jobs=2,\n    n_estimators=model_num_estimators,  # 500 default\n    max_depth=15, # 19\n    learning_rate=model_lr, # 0.06 is better\n    subsample=0.9,\n    colsample_bylevel=0.9\n    ,tree_method= 'auto' # #'gpu_hist', \/ \"hist\" - faster, less exact , \"gpu_exact\" - better perf , \"auto\", 'hist' (cpu)\n     ,min_child_weight=3 # 1 by default\n    ,missing=-999\n#     ,scale_pos_weight=3\n)\n\n\nclf.fit( X_train,y_train)\ny_preds = clf.predict_proba(X_test)[:,1]","060c3f15":"fi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0004].sort_values('importance').head(40).plot(kind='barh', figsize=(14, 32), title='Feature Importance')\nplt.show()","b0d3c604":"sample_submission['isFraud'] = y_preds\nsample_submission.to_csv('dan_xgboost_stack_model2.csv')","81480f0e":"df_all[\"random_cv_preds\"] = pd.concat([X_train[\"random_cv_preds\"],X_test[\"random_cv_preds\"] ])\n# df_all[\"temporal_cv_preds\"] = pd.concat([X_train[\"temporal_cv_preds\"],X_test[\"temporal_cv_preds\"] ])","9a2743ce":"df_all.head()","146f124d":"df_all.columns[:65]","13c17d6a":"df_all.columns[:100]","2361f290":"df_all.columns[100:200]","ca603ae4":"df_all.columns[200:300]","c354007a":"df_all.columns[300:390]","bad36429":"df_all.columns[390:]","b2b14f9b":"df_all.head()","5374c95f":"len(df_all.columns[432:])","335d7c2d":"df_all.columns[431:]","21f7c0c1":"EXTRA_FEAT_NAMES = df_all.columns[432:]","bf511134":"df_all[EXTRA_FEAT_NAMES].tail()","81e2974f":"df_all[EXTRA_FEAT_NAMES].to_csv(\"extra_fraud_feats_baseV2.csv\")#.gz\",compression=\"gzip\")\n\n# del df_all","c7a2dd71":"df_all.drop(['TransactionDT'],axis=1).loc[~df_all['isFraud'].isna()].sample(frac=0.005).to_csv(\"sample_fraud_train_augV1.csv\")","2d8396a0":"df_all.drop(['TransactionDT'],axis=1).loc[~df_all['isFraud'].isna()].to_csv(\"fraud_train_augV1.csv\")\nprint(\"train full saved\")\ndf_all.loc[df_all['isFraud'].isna()].drop(['TransactionDT','isFraud'],axis=1).to_csv(\"fraud_test_augV1.csv\")\nprint(\"test full saved\")","b23321eb":"TRANSACT_COLS = ['isFraud', 'time', 'card_hash', \n       'multIDcols_hash','normalized_spend_vs_hour_day_mean',\n       'normalized_spend_vs_dayOfYear', 'isolation_overall_score',\n       'isolation_pos_score','TransactionAmt','P_emaildomain', 'ProductCD', 'R_emaildomain','addr1', 'addr2',\n                'id_01', 'id_02', 'id_03', 'id_04', 'id_05',\n       'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13',\n       'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n       'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n       'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37',\n       'id_38',\"V307\",\"V179\",\"V120\",\"V258\", \"V185\"]","af63bf82":"df_all.head()","18e3e009":"df_all.loc[~df_all['isFraud'].isna()][TRANSACT_COLS].to_csv(\"tr_eventsTS.csv.gz\",compression=\"gzip\")","b449ce78":"df_all.loc[df_all['isFraud'].isna()][TRANSACT_COLS].to_csv(\"test_eventsTS.csv.gz\",compression=\"gzip\")","95765582":"### Store the extracted, novel features in a new dataframe for export\/sharing\n\n* Store it before any memory saving resizing if possible\n* can concat the anomaly model based features to it, or seperately. \n\n* Remember: `TransactionID` is the index, not a column","cddb1b5a":"#### transactions only for TS","2a83c099":"##### Model training\n\n* todo: do cross_val_predict (sklearn) using sklearn api for convenience\n* Temporal split :  use sklearn's TimeSeriesSplit (or manual) for early stopping\/validation + validation\n\n\n* First version - cross validated predictions - ensemble approach\n* secodn appproach - single model","0c86df3a":"### label-encode & model build\n* TODO: compare to OHE? +- other encoding\/embedding methods","a426994e":"#### put our new features back into train, test. \n\n* Warning - requires that we manually add these columns here - could be improved. \n","752a0982":"## datetime features\n* try to guess date and datetime delta unit, then add features\n* TODO: strong features potential already found offline, need to validate\n* Try 01.12.2017 as start date: https:\/\/www.kaggle.com\/kevinbonnes\/transactiondt-starting-at-2017-12-01\n* ALT : https:\/\/www.kaggle.com\/terrypham\/transactiondt-timeframe-deduction ","dc089212":"### Add some features\n* missing values count\n    * TODO: nans per cattegory\/group (e..g V columns)\n    * Could be more efficient with this code, but that's aimed at columnar, not row level summation: https:\/\/stackoverflow.com\/questions\/54207038\/groupby-columns-on-column-header-prefix\n* Add some of the time series identified in external platform\n* ToDo: anomaly detection features. \n* proxy for lack of an identifier, duplicate values. \n    * TODO: try to understand what could be a proxy for a key\/customer\/card identifier (then apply features based on that).\n    \n    \n* ToDo: readd feature of identical transactions: this is typically a strong feature, but (surprisingly) gave no signal in this dataset. Both with and without transaction amount (and with transaction time removed ofc).\n\n\n* COLUMN_GROUP_PREFIXES - I don't calculate all due to memory issues\/kernel instability. Not a very strogn feature, but can help. ","a8e244dc":"## Anomaly detection features\n* Isolation forest approach for now, can easily be improved with semisupervised approach, additional models, TSNE etc'\n* Based on this kernel: https:\/\/www.kaggle.com\/danofer\/anomaly-detection-for-feature-engineering-v2\n\n* Note: potential improvement: train additional model on only positive (non fraud) samples on concatenated train+test. \n\n\n\n##### Isolation forest (anomaly detection)\n* https:\/\/www.kaggle.io\/svf\/1100683\/56c8356ed1b0a6efccea8371bc791ba7\/__results__.html#Tree-based-techniques )\n* contamination = % of anomalies expected  (fraud class % in our case)\n\n* isolation forest doesn't work on nan values!\n    * TODO: model +- transaction amount. +- nan imputation (at least\/especially for important columns)","16fac4aa":"## A model stacking feature\n#### warning - may overfit!!\n* temporal CV self score (vs random CV). ","8b8b624b":"## Identity hash: \"magic feature\"?\n* Top missing feature is user identity (for feature engineering). \n* Previously (not shown), I extracted features based on  duplicate counts using most of the rows, and showed 1% + of rows as being duplicates (with `TransactionDT`, +- `TransactionAmt` excluded) - but this did not give a good feature (surprisingly).\n    * Fraud is often characterized by similar behavior\/repeat behavior. \n* I will create a proxy for identity from the identity\/ `id` data! \n    * This can doubtless be improved by keeping\/dropping some. e.g. DeviceType , DeviceInfo may be too specific.\n    * `id_34` has \"match_status:\" values - may be only feature we need, or metric of noise (in which case we may want to drop it)\n    \n#### We see that due to the sparsity of the identifier data, 75% of the transactions are duplicates!! \n* This is probably why this wasn't used to create  a \"user ID\" in the first place.\n* Presumably, it might still be salvageable using other data\n\n* This kernel looks at just some card features and gets an approximate of missing car numbers: https:\/\/www.kaggle.com\/smerllo\/identify-unique-cards-id","7bea47b5":"## make concatenated train+Test for this feature \n* delete it after due to memory issues if using kernel\n* can use for anomaly detection features\n\n\n\ntodo: hash for counting duplicates (counts based on group by columns with missing values is tricky. We don't care about collisions much.\n\n* integer (efficient) Hashing : https:\/\/stackoverflow.com\/questions\/25757042\/create-hash-value-for-each-row-of-data-with-selected-columns-in-dataframe-in-pyt\n     * `df.apply(lambda x: hash(tuple(x)), axis = 1)`","078501b3":"#### Simple model based feature importance plot\n* TODO: shapley, interactions\n\n* It looks like our grouped missing values are **valuable**, although the datetime features seemingly didn't (likely, some of the anonymized variables already capture them). They may have some marginal contribution.\n    * toDo: check that run models with and without them","1d860ac8":"#### Continued Identity hash: \n* Let's try using some of the card data as well. (Ideally we'd use something like bank account or location, but we lack that :( )\n\n* using transaction amt changes this from ~74% (when addin in card type) to 51% . Still a lot of redundancy, and it's a bad variable (too easy to change\/game), but better than nothing ?\n* adding in the addr1\/2 and email domains gets us to 30%. (The same without `TransactionAmt` goes back to 60% duplicates)\n    * Adding dist 1\/2 gets us to 19%, but i'm scared of them - no way of knowing if they mean distance or location or something else\n    \n    \n    \n    ########### Ideal solution: brute force a half dozen combinations as features, add them all or one by one, check feature importance\/feature selection, keep the best one(s). \n    * I leave this for others kernels :) \n    \n    * does hash respect nans? This kernel ensures they're kept : https:\/\/www.kaggle.com\/smerllo\/identify-unique-cards-id","e4780556":"#### FE kernel - I forgot to add these originally!\n\nhttps:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/108575\n* add mean transaction (even if I extract it anyway afterwards, externally).\n* card 1 counts..","cbda6844":"#### holiday possible features\n* Depends on our start date\n* Additional possible holidays - cyber monday, black friday... \n* Holiday code from : https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-for-local-test\n* Original also did datetime aggregations, may be different than the ones I do here\/above.. ","516d0002":"\n* I've added a few simple & generic fraud\/anomaly features, such as anomality, isolation, missing values per group, datetime, occurences count and hash based identity reconstruction.\n* The kernel will be updated with additional features.\n* Isolation forest for anomaly detection feature from: https:\/\/www.kaggle.com\/danofer\/anomaly-detection-for-feature-engineering-v2\n\n* Model code is from the baseline + [gpu\/Xhulu's kernel](https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s)\n\n* Start date set to 2.11.2017 instead of 20.12.2017, based on this kernel : https:\/\/www.kaggle.com\/terrypham\/transactiondt-timeframe-deduction\n\n* To get my ~95 submission, just run this with more XGBoost iterations and depth \/ hyperparams (e.g. as in my kernel : https:\/\/www.kaggle.com\/danofer\/xgboost-using-optuna-fastauc-features) , also add V,C to the COLUMN_GROUP_PREFIXES\n\n\n\n###### PERFORMANCE notes: If running locally, set the `n_jobs` param in the isolation forests (+- XGBoost) to -1\/-2 (it's problematic in kaggle), and if you have a GPU, set XGBoost to use `gpu_hist` - it's much much faster\n ###### If runing locally - `FAST_RUN = False`  , otherwise you'll have poor results "}}