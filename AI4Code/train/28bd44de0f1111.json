{"cell_type":{"ded97811":"code","0875bc24":"code","14d0ab37":"code","0d9c1372":"code","8bd2dc67":"code","3fb62f89":"code","6c6bc8d0":"code","de94d50a":"code","ad7edb3b":"code","1f7d7153":"code","a03c374f":"code","ea3c8c99":"code","387c7fee":"code","e319787a":"code","8cef2c9e":"code","b0ddf7ce":"code","f1d31965":"code","0841049e":"markdown","01ce05e6":"markdown","efdac9cb":"markdown","e8b79d5c":"markdown","1b1f69d2":"markdown","921afac7":"markdown","5d082bcc":"markdown","e2b16cf2":"markdown","432c040d":"markdown","521b19f7":"markdown","86682cda":"markdown","03654a92":"markdown","f77fb852":"markdown","92e53e8d":"markdown","30c31909":"markdown","039eff6a":"markdown","c2649821":"markdown","44850eeb":"markdown","b5f383ee":"markdown","9338d7fd":"markdown","aba6948c":"markdown","a29b6ddd":"markdown","512d094b":"markdown","ba1ae6ff":"markdown"},"source":{"ded97811":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans #Clustering use case\nfrom sklearn import metrics as mt #Check metrics on the model\nfrom sklearn.preprocessing import RobustScaler, normalize #Scaling options\nfrom sklearn.decomposition import PCA #Explaining variance\n\nfrom yellowbrick.cluster import SilhouetteVisualizer #Visualize Silhouette Scores\n\nimport matplotlib.pylab as plt #Plotting graphs\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D #Plotting graphs in 3D\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0875bc24":"credit = pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\") #Load the dataset into a dataframe\ncredit.head() #Take a peek at the dataframe","14d0ab37":"print(credit.count()) #Gets the counts of the columns\n\n#There are some with inconsistent values, being CREDIT_LIMIT and MINIMUM_PAYMENTS\n\nprint(credit[\"CREDIT_LIMIT\"].isnull().any()) #Check for nulls in CREDIT_LIMIT\nprint(credit[\"MINIMUM_PAYMENTS\"].isnull().any()) #Check for nulls in MINIMUM_PAYMENTS","0d9c1372":"credit[\"MINIMUM_PAYMENTS\"] = credit[\"MINIMUM_PAYMENTS\"].fillna(0) #Fills nulls in MINIMUM_PAYMENTS with 0\ncredit[\"CREDIT_LIMIT\"] = credit[\"CREDIT_LIMIT\"].fillna(credit[\"CREDIT_LIMIT\"].mean()) #Fills the null in CREDIT_LIMIT with the mean limit\nprint(credit.count()) #Print the counts","8bd2dc67":"metric = [\"BALANCE\", \"PURCHASES\", \"PAYMENTS\", \"CUST_ID\"] #Laying out the metrics stated above plus cust_id for indexing\nmetricsDf = credit[metric] #Take the metrics from the credit dataframe\nmetricsDf = metricsDf.set_index(\"CUST_ID\") #Set the customer ID to the index\nmetricsDf = metricsDf.astype(\"float\") #Converts the types to float to make kmeans more applicable\nmetricsDf.head() #Display the metrics dataframe","3fb62f89":"fig = plt.figure() #Build the figure\nax = Axes3D(fig) #Make it 3D\n\n#Define the x y z to be the dataset columns\nx = list(metricsDf.iloc[:,0])\ny = list(metricsDf.iloc[:,1])\nz = list(metricsDf.iloc[:,2])\n\n#Define the axes labels\nnames = metricsDf.columns\nax.set_xlabel(names[0])\nax.set_ylabel(names[1])\nax.set_zlabel(names[2])\n\nax.scatter(x, y, z, c = \"purple\", marker = \"o\") #Create a scatterplot of the data\nplt.show() #Show the graph","6c6bc8d0":"#Scaling with a robust scaler, as messing with it and the min\/max and standard scalers showed it to be the best here\nrobust = RobustScaler()\ntrainRobust = robust.fit_transform(metricsDf)\n\nfig = plt.figure() #Build the figure\nax = Axes3D(fig) #Make it 3D\n\n#Define the x y z to be the dataset columns\nx = trainRobust[:,0]\ny = trainRobust[:,1]\nz = trainRobust[:,2]\n\n#Define the axes labels\nnames = metricsDf.columns\nax.set_xlabel(names[0])\nax.set_ylabel(names[1])\nax.set_zlabel(names[2])\n\nax.scatter(x, y, z, c = \"purple\", marker = \"o\") #Create a scatterplot of the robust data\nplt.show() #Show the graph","de94d50a":"normalRobust = normalize(trainRobust) #Normalize the robust scaled data\nfig = plt.figure() #Build the figure\nax = Axes3D(fig) #Make it 3D\n\n#Define the x y z to be the dataset columns\nx = normalRobust[:,0]\ny = normalRobust[:,1]\nz = normalRobust[:,2]\n\n#Define the axes labels\nnames = metricsDf.columns\nax.set_xlabel(names[0])\nax.set_ylabel(names[1])\nax.set_zlabel(names[2])\n\nax.scatter(x, y, z, c = \"purple\", marker = \"o\") #Create a scatterplot with the normalized data\nplt.show() #Show the graph","ad7edb3b":"pcaTrain = PCA().fit(trainRobust) #A PCA for just the fit data\npcaNormal = PCA().fit(normalRobust) #A PCA for the normalized data\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(12,4)) #Creates a subplot for both explained variances\n\n#Sets the x and y labels on each graph\naxes[0].set_xlabel(\"Component Number\")\naxes[0].set_ylabel(\"Percent Variance\")\naxes[1].set_xlabel(\"Component Number\")\naxes[1].set_ylabel(\"Percent Variance\")\n\n#Sets the titles of the graph, one normal, the other non-normal\naxes[0].set_title(\"Non-normal Explained Variance\")\naxes[1].set_title(\"Normal Explained Variance\")\n\n#Sets a gray grid below for easier reading\naxes[0].grid(alpha=0.25)\naxes[1].grid(alpha=0.25)\naxes[0].set_axisbelow(True)\naxes[1].set_axisbelow(True)\n\n#Plots the explained variance for the normal and non-normal data\naxes[0].plot(np.cumsum(pcaTrain.explained_variance_ratio_))\naxes[1].plot(np.cumsum(pcaNormal.explained_variance_ratio_))\n\nplt.show() #Shows the plots","1f7d7153":"pcaSet = PCA(n_components = 2) #Initialize a PCA with two components\npcaSet = pcaSet.fit_transform(normalRobust) #Insert the normalized data, squashing it into two components\npcaDf = pd.DataFrame(data = pcaSet, columns = [\"Component 1\", \"Component 2\"]) #Insert the PCA set into a dataframe\npcaDf.head() #Take a peek at the dataframe","a03c374f":"plt.figure() #Build a figure\n\n#Set the axes to the components\naxes[0].set_xlabel(\"Component 1\")\naxes[0].set_ylabel(\"Component 2\")\naxes[0].set_title(\"2 Component PCA\")\n\n#Take a look at the orb\nplt.scatter(pcaDf[\"Component 1\"], pcaDf[\"Component 2\"], c=\"purple\")","ea3c8c99":"results = {}\nnumClusters = 11\n\nfor k in range(2 , numClusters):\n    print(\"-\"*100) #Separate the iterations\n    results[k] = {} #Collect the results at K\n    kmeans = KMeans(n_clusters = k, random_state = 0).fit(pcaDf) #Run the kmeans for k\n    silhouette = mt.silhouette_score(pcaDf, kmeans.labels_, metric = \"euclidean\") #Get the silhouette score\n    \n    #Put the scores into the results dictionary\n    results[k][\"Silhouette\"] = silhouette\n    results[k][\"Score\"] = kmeans.score\n    results[k][\"Inertia\"] = kmeans.inertia_\n    \n    #Print the results\n    print(\"Clusters: {}\".format(k))\n    print(\"Silhouette Score: {}\".format(silhouette))","387c7fee":"clusters = [2, 3, 4, 5, 6, 7, 8] #Choose a couple clusters to visualize\nfor cluster in clusters:\n    kmeans = KMeans(n_clusters = cluster, random_state = 0) #Run KMeans\n    visualize = SilhouetteVisualizer(kmeans) #Set a visualizer for KMeans\n    visualize.fit(pcaDf) #Fit the visualizer to the data\n    visualize.poof() #Display the visualization","e319787a":"plt.figure() #Build a figure\n\n#Set the axes to the components\naxes[0].set_xlabel(\"Component 1\")\naxes[0].set_ylabel(\"Component 2\")\naxes[0].set_title(\"2 Component PCA\")\n\n#Take a look at the orb\nplt.scatter(pcaDf[\"Component 1\"], pcaDf[\"Component 2\"], c = KMeans(n_clusters = 3).fit_predict(pcaDf), cmap =plt.cm.winter) \nplt.show() ","8cef2c9e":"fig = plt.figure() #Build the figure\npcaSetTrain = PCA(n_components = 2) #Initialize a PCA with two components\npcaSetTrain = pcaSetTrain.fit_transform(metricsDf) #Insert the normalized data, squashing it into two components\npcaDfTrain = pd.DataFrame(data = pcaSetTrain, columns = [\"Component 1\", \"Component 2\"]) #Insert the PCA set into a dataframe\n\n#Scatter based off the original set\nplt.scatter(pcaDfTrain[\"Component 1\"], pcaDfTrain[\"Component 2\"], c = KMeans(n_clusters = 3).fit_predict(pcaDfTrain), cmap =plt.cm.winter, marker = \"o\") \nplt.show() ","b0ddf7ce":"info = metricsDf.copy() #Get a copy of the original data\ninfo[\"Cluster\"] = KMeans(n_clusters = 3).fit_predict(pcaDf) #Add a column for which cluster each was classified as\ninfo.head() #Show the new dataframe","f1d31965":"sums = [0, 0, 0, 0, 0, 0, 0, 0, 0] #A list to hold the sum of each classification, 0-2 for cluster 0, 3-5 for cluster 1, and 6-8 for 2\ncounts = [0, 0, 0] #Counts for each cluster, the cluster number being the same as the index.\nclust = info[\"Cluster\"].copy() #Get the cluster classifications\n\n#For loop to get the sums of each field based off each cluster classification\nfor i in range(0,len(clust)):\n    currentCluster = clust[i] #Get the current cluster\n    \n    #If in cluster 0\n    if currentCluster == 0:\n        #Sum the balances, purchases, and payments for cluster 0\n        sums[0] = sums[0] + info[\"BALANCE\"][i]\n        sums[1] = sums[1] + info[\"PURCHASES\"][i]\n        sums[2] = sums[2] + info[\"PAYMENTS\"][i]\n        counts[0] += 1\n        \n    #If in cluster 1\n    elif currentCluster == 1:\n        #Sum the balances, purchases, and payments for cluster 1\n        sums[3] = sums[3] + info[\"BALANCE\"][i]\n        sums[4] = sums[4] + info[\"PURCHASES\"][i]\n        sums[5] = sums[5] + info[\"PAYMENTS\"][i]\n        counts[1] += 1\n    \n    #If in cluster 2\n    else:\n        #Sum the balances, purchases, and payments for cluster 2\n        sums[6] = sums[6] + info[\"BALANCE\"][i]\n        sums[7] = sums[7] + info[\"PURCHASES\"][i]\n        sums[8] = sums[8] + info[\"PAYMENTS\"][i]\n        counts[2] += 1\n    \nmean0 = [sums[0]\/counts[0], sums[1]\/counts[0], sums[2]\/counts[0]] #Calculate the means for cluster 0\nmean1 = [sums[3]\/counts[1], sums[4]\/counts[1], sums[5]\/counts[1]] #Calculate the means for cluster 1\nmean2 = [sums[6]\/counts[2], sums[7]\/counts[2], sums[8]\/counts[2]] #Calculate the means for cluster 2\n\n#Print the mean arrays for each cluster\nprint(\"             Balance             Purchases         Payments\")\nprint(\"Cluster 0: \", mean0)\nprint(\"Cluster 1: \", mean1)\nprint(\"Cluster 2: \", mean2)","0841049e":"---","01ce05e6":"# Visualize and Transform Data","efdac9cb":"# Deriving data from the clusters","e8b79d5c":"---","1b1f69d2":"# Create a PCA-modified set","921afac7":"According to the graph, one component can explain 90% of the variance on the non-normal data and between 80% and 85% on the normalized data. However, both sets can explain the variance 100% with two components, so it is a toss up on whichever one I choose to use. I like the ball better, so I choose the normalized data.","5d082bcc":"---","e2b16cf2":"The three groups appear to be the less engaged (lower in all fronts), the heavy spenders (high spending and credit payments, but a medium amount held by the bank as a balance), and the savers (high balances for purchases, but less spending in the form of purchases and payments\n\nBased on the clusters and these means, I feel it is fair to assume that Balance and Payments are the two components strongly considered in the models by how separate their values are. This tells me that the bank's marketing strategy should focus more on getting users to deposit more and pay more. I had also noticed previously that most of this dataset is pretty high in tenure, so a strong strategy in targeting new people is offering benefits based on how much they deposit into the bank. As for payments, these are likely to be referring to credit card payments in this dataset. Since payments increase with increased credit use, building trust in the bank to be willing to use that much credit seems like a strong strategy in that regard.","432c040d":"# Check for nulls","521b19f7":"3 clusters look best. All are about the same width and peak over the average","86682cda":"Updated Thought Process:\n\nUpon experimentation, these were not the best choices to make. The Tenure essentially created different graphs based on the tenure number because it is only in years. The frequency parameters also made for spread all across the board instead of more clear clusters, so the numeric values may give more insights.\n\nI now think purchases, balance, and payments are the best play because they exemplify the baseline services of a bank (getting people to have more with the bank, pay them more, and spend more overall to have to pay back) without those key problems.","03654a92":"---","f77fb852":"---","92e53e8d":"---","30c31909":"Nulls have been filled. Most fields are also numeric, so no blank strings to check for. Proceed to the next part.","039eff6a":"# Assign Cluster Centers","c2649821":"Sources used for learning in case I want to view them again: https:\/\/www.youtube.com\/playlist?list=PLcFcktZ0wnNnzAQZL4m_-1Xc66MfnPegq , https:\/\/www.kaggle.com\/vipulgandhi\/kmeans-detailed-explanation","44850eeb":"Minimum payments appears to have multiple nulls. I think it is fair to replace the nulls with 0 here.\nCredit Limits only has one null. I think i will just take the mean of credit limits for this one.","b5f383ee":"# Silhouette Analysis","9338d7fd":"# Column Use Choices","aba6948c":"Original Thought Process: \n\nThis is a bank that wants to build a market strategy. The google definition of market strategy is \"A marketing strategy refers to a business's overall game plan for reaching prospective consumers and turning them into customers of the products or services the business provides\" (For my future self in case I forget). A bank makes money from roping people in for longer amounts of time as well as when making purchases. So, I think the strongest indicators are purchase frequency, balance frequency, and tenure (I do not think the actual amounts of the balance and purchases matter in this case, as it is overall strategy and not \"make as much money as possible\").","a29b6ddd":"---","512d094b":"Coded by Luna McBride\n\nThis one does not have as much of an objective as others. I just saw it and felt compelled.","ba1ae6ff":"# Clustering Project: Bank Data"}}