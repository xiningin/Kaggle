{"cell_type":{"b24a42e8":"code","bf1548a4":"code","53f4fdc4":"code","85bbc721":"code","93a02391":"code","cc0e2af6":"code","9ca76617":"code","356e7f35":"code","7d9d131e":"code","6276f1e8":"code","44dc6ff9":"code","5737cbc8":"code","a520fa4e":"code","c8614306":"code","fe67f1a4":"code","0e595bf1":"code","e749c887":"markdown","ecf6715d":"markdown","87105736":"markdown","1e101be9":"markdown","2bcb732e":"markdown","31f4f903":"markdown","ab0820e1":"markdown","5659c0d7":"markdown","eeafb2ac":"markdown","709af96a":"markdown","7a1affa9":"markdown"},"source":{"b24a42e8":"import torch\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\n\nDIR = '..\/input\/animefacedataset\/'\n\nimage_size = 64\nbatch_size = 64\n\ntrain_ds = ImageFolder(DIR, transform=transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor()]))\n\ntrain_dl = DataLoader(train_ds, batch_size)","bf1548a4":"def show_images(images):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images.detach(), nrow=8).permute(1, 2, 0))\n\ndef show_batch(dl):\n    for images, _ in dl:\n        show_images(images)\n        break","53f4fdc4":"show_batch(train_dl)","85bbc721":"import os\nimport pandas as pd\n\niters = os.scandir('..\/input\/animefacedataset\/images')\n\ndf = pd.DataFrame(columns=['file_name', 'label'])\n\nfor i in iters:\n    loc = f\"..\/input\/animefacedataset\/images\/{i.name}\"\n    df = df.append({'file_name': loc, 'label': 0}, ignore_index=True)\n\ndf.to_csv('file_names.csv', index=False)","93a02391":"#include <torch\/torch.h>\n#include <opencv2\/opencv.hpp>\n\n#include <iostream>\n\nusing namespace torch;\n\n\/\/ Reshaped Image Size\nconst int64_t kImageSize = 64;\n\n\/\/ The size of the noise vector fed to the generator.\nconst int64_t kLatentDim = 100;\n\n\/\/ The batch size for training.\nconst int64_t kBatchSize = 64;\n\n\/\/ Number of workers\nconst int64_t kNumOfWorkers = 4;\n\n\/\/ Enforce ordering\nconst bool kEnforceOrder = false;\n\n\/\/ The number of epochs to train.\nconst int64_t kNumberOfEpochs = 30;\n\n\/\/ Where to find the CSV with file locations.\nconst string kCsvFile = \"..\/file_names.csv\";\n\n\/\/ After how many batches to create a new checkpoint periodically.\nconst int64_t kCheckpointEvery = 200;\n\n\/\/ How many images to sample at every checkpoint.\nconst int64_t kNumberOfSamplesPerCheckpoint = 64;\n\n\/\/ After how many batches to log a new update with the loss value.\nconst int64_t kLogInterval = 10;\n\n\/\/ Learning Rate\nconst double kLr = 2e-4;\n\n\/\/ Beta1\nconst double kBeta1 = 0.5;\n\n\/\/ Beta2\nconst double kBeta2 = 0.999;","cc0e2af6":"auto ReadCsv(std::string& location) {\n    std::fstream in(location, std::ios::in);\n    std::string line;\n    std::string name;\n    std::string label;\n    std::vector<std::tuple<std::string, int64_t>> csv;\n\n    while (getline(in, line)) {\n        std::stringstream s(line);\n        getline(s, name, ',');\n        getline(s, label, ',');\n\n        csv.push_back(std::make_tuple(\"..\/\" + name, stoi(label)));\n    }\n    return csv;\n};","9ca76617":"struct FaceDataset : torch::data::Dataset<FaceDataset>\n{\n\n    std::vector<std::tuple<std::string \/*file location*\/, int64_t \/*label*\/>> csv_;\n\n    FaceDataset(std::string& file_names_csv)\n        \/\/ Load csv file with file locations and labels.\n        : csv_(ReadCsv(file_names_csv)) {\n\n    };\n\n    \/\/ Override the get method to load custom data.\n    torch::data::Example<> get(size_t index) override {\n\n        std::string file_location = std::get<0>(csv_[index]);\n        int64_t label = std::get<1>(csv_[index]);\n\n        \/\/ Load image with OpenCV.\n        cv::Mat img = cv::imread(file_location);\n\n        \/\/ Resize to 64*64\n        cv::resize(img, img, cv::Size(kImageSize,kImageSize));\n\n        \/\/ Convert the image and label to a tensor.\n        \/\/ Here we need to clone the data\n        Tensor img_tensor = torch::from_blob(img.data, { img.rows, img.cols, 3 }, torch::kByte).clone();\n        img_tensor = img_tensor.permute({ 2, 0, 1 }); \/\/ convert to CxHxW\n        Tensor label_tensor = torch::full({ 1 }, label);\n\n        return { img_tensor, label_tensor };\n    };\n\n    \/\/ Override the size method to infer the size of the data set.\n    torch::optional<size_t> size() const override {\n\n        return csv_.size();\n    };\n};","356e7f35":"struct DCGANGeneratorImpl : nn::Module {\n    DCGANGeneratorImpl(int kLatentDim)\n        : conv1(nn::ConvTranspose2dOptions(kLatentDim, 512, 4)\n            .bias(false)),\n        batch_norm1(512),\n        conv2(nn::ConvTranspose2dOptions(512, 256, 4)\n            .stride(2)\n            .padding(1)\n            .bias(false)),\n        batch_norm2(256),\n        conv3(nn::ConvTranspose2dOptions(256, 128, 4)\n            .stride(2)\n            .padding(1)\n            .bias(false)),\n        batch_norm3(128),\n        conv4(nn::ConvTranspose2dOptions(128, 64, 4)\n            .stride(2)\n            .padding(1)\n            .bias(false)),\n        batch_norm4(64),\n        conv5(nn::ConvTranspose2dOptions(64, 3, 4)\n            .stride(2)\n            .padding(1)\n            .bias(false))\n    {\n        register_module(\"conv1\", conv1);\n        register_module(\"conv2\", conv2);\n        register_module(\"conv3\", conv3);\n        register_module(\"conv4\", conv4);\n        register_module(\"conv5\", conv5);\n        register_module(\"batch_norm1\", batch_norm1);\n        register_module(\"batch_norm2\", batch_norm2);\n        register_module(\"batch_norm3\", batch_norm3);\n        register_module(\"batch_norm4\", batch_norm4);\n    }\n\n    Tensor forward(Tensor x) {\n        x = relu(batch_norm1(conv1(x)));\n        x = relu(batch_norm2(conv2(x)));\n        x = relu(batch_norm3(conv3(x)));\n        x = relu(batch_norm4(conv4(x)));\n        x = tanh(conv5(x));\n        return x;\n    }\n\n    nn::ConvTranspose2d conv1, conv2, conv3, conv4, conv5;\n    nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3, batch_norm4;\n};\n\nTORCH_MODULE(DCGANGenerator);","7d9d131e":"int main() {\n    manual_seed(42);\n\n    torch::Device device(torch::kCPU);\n    if (torch::cuda::is_available()) {\n        std::cout << \"CUDA available! Training on GPU.\" << std::endl;\n        device = torch::Device(torch::kCUDA);\n    }\n\n    DCGANGenerator generator(kLatentDim);\n    generator->to(device);\n\n    nn::Sequential discriminator(\n        \/\/ Layer 1\n        nn::Conv2d(\n            nn::Conv2dOptions(3, 64, 4).stride(2).padding(1).bias(false)),\n        nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n        \/\/ Layer 2\n        nn::Conv2d(\n            nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)),\n        nn::BatchNorm2d(128),\n        nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n        \/\/ Layer 3\n        nn::Conv2d(\n            nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)),\n        nn::BatchNorm2d(256),\n        nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n        \/\/ Layer 4\n        nn::Conv2d(\n            nn::Conv2dOptions(256, 512, 4).stride(2).padding(1).bias(false)),\n        nn::BatchNorm2d(512),\n        nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n        \/\/ Layer 5\n        nn::Conv2d(\n            nn::Conv2dOptions(512, 1, 4).stride(1).padding(0).bias(false)),\n        nn::Sigmoid());\n    discriminator->to(device);","6276f1e8":"    std::string file_names_csv = kCsvFile;\n    auto dataset = FaceDataset(file_names_csv)\n        .map(data::transforms::Normalize<>(0.5, 0.5))\n        .map(data::transforms::Stack<>());\n\n    const int64_t batches_per_epoch =\n        std::ceil(dataset.size().value() \/ static_cast<double>(kBatchSize));\n\n    auto data_loader = data::make_data_loader<data::samplers::RandomSampler>(\n        dataset,\n        data::DataLoaderOptions().workers(kNumOfWorkers).batch_size(kBatchSize).enforce_ordering(kEnforceOrder));\n\n    optim::Adam generator_optimizer(\n        generator->parameters(), optim::AdamOptions(kLr).betas(std::make_tuple(kBeta1, kBeta2)));\n    optim::Adam discriminator_optimizer(\n        discriminator->parameters(), optim::AdamOptions(kLr).betas(std::make_tuple(kBeta1, kBeta2)));","44dc6ff9":"    int64_t checkpoint_counter = 1;\n    for (int64_t epoch = 1; epoch <= kNumberOfEpochs; ++epoch) {\n        int64_t batch_index = 0;\n        for (auto& batch : *data_loader) {\n            \/\/ Train discriminator with real images.\n            discriminator->zero_grad();\n            Tensor real_images = batch.data.to(device);\n            Tensor real_labels =\n                torch::empty(batch.data.size(0), device).uniform_(0.8, 1.0);\n            Tensor real_output = discriminator->forward(real_images);\n            Tensor d_loss_real =\n                binary_cross_entropy(real_output, real_labels);\n            d_loss_real.backward();\n\n            \/\/ Train discriminator with fake images.\n            Tensor noise =\n                torch::randn({ batch.data.size(0), kLatentDim, 1, 1 }, device);\n            Tensor fake_images = generator->forward(noise);\n            Tensor fake_labels = torch::zeros(batch.data.size(0), device);\n            Tensor fake_output = discriminator->forward(fake_images.detach());\n            Tensor d_loss_fake =\n                torch::binary_cross_entropy(fake_output, fake_labels);\n            d_loss_fake.backward();\n\n            Tensor d_loss = d_loss_real + d_loss_fake;\n            discriminator_optimizer.step();\n\n            \/\/ Train generator.\n            generator->zero_grad();\n            fake_labels.fill_(1);\n            fake_output = discriminator->forward(fake_images);\n            Tensor g_loss =\n                torch::binary_cross_entropy(fake_output, fake_labels);\n            g_loss.backward();\n            generator_optimizer.step();\n            batch_index++;\n            if (batch_index % kLogInterval == 0) {\n                std::printf(\n                    \"\\r[%2ld\/%2ld][%3ld\/%3ld] D_loss: %.4f | G_loss: %.4f\\n\",\n                    epoch,\n                    kNumberOfEpochs,\n                    batch_index,\n                    batches_per_epoch,\n                    d_loss.item<float>(),\n                    g_loss.item<float>());\n            }\n            if (batch_index % kCheckpointEvery == 0) {\n                \/\/ Checkpoint the model and optimizer state.\n                torch::save(generator, \"generator-checkpoint.pt\");\n                torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n                torch::save(discriminator, \"discriminator-checkpoint.pt\");\n                torch::save(\n                    discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n                \/\/ Sample the generator and save the images.\n                Tensor samples = generator->forward(torch::randn(\n                    { kNumberOfSamplesPerCheckpoint, kLatentDim, 1, 1 }, device));\n                torch::save(\n                    (samples + 1.0) \/ 2.0,\n                    torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n                std::cout << \"\\n-> checkpoint \" << ++checkpoint_counter << '\\n';\n            }\n        }\n    }\n\n    std::cout << \"Training complete!\" << std::endl;\n}","5737cbc8":"from IPython.display import Image\nImage('..\/input\/gen-images\/generated-images-0001.png')","a520fa4e":"Image('..\/input\/gen-images\/generated-images-0002.png')","c8614306":"Image('..\/input\/gen-images\/generated-images-0005.png')","fe67f1a4":"Image('..\/input\/gen-images\/generated-images-0010.png')","0e595bf1":"Image('..\/input\/gen-images\/generated-images-0025.png')","e749c887":"## Generator module\n\n`register_module()` is needed to use the parameters() method later on. A macro`TORCH_MODULE` is a wrapper over a `std::shared_ptr<DCGANGeneratorImpl>`.","ecf6715d":"To write a custom dataset, first, we need to create csv file with paths and labels (in case of classification).","87105736":"Create function to read locations and labels.","1e101be9":"Import libraries and define constants.","2bcb732e":"## Anime Face Dataset\n\nLoad images with OpenCV, resize them, convert to tensor, clone data to prevent memory deallocation after leaving the scope and permute channels.","31f4f903":"# Dataset Overview\n\nWe have a dataset which consists of 63632 high-quality anime faces. Sizes vary from 90x90 to 120x120.\n\nLet's visualize a resized(64x64) batch.","ab0820e1":"# Deep Convolutional Generative Adversarial Network (Pytorch C++)\n\n\n![](https:\/\/www.researchgate.net\/publication\/331282441\/figure\/fig3\/AS:729118295478273@1550846756282\/Deep-convolutional-generative-adversarial-networks-DCGAN-for-generative-model-of-BF-NSP.png)\n\n\nHello everyone.\n\nIn this notebook, we\u2019ll use pure C++ interface to train DCGAN to generate images using [Anime Face Dataset](https:\/\/www.kaggle.com\/splcher\/animefacedataset).\n\nOur additional libraries we'll be:\n* [Torch](https:\/\/pytorch.org\/)\n* [OpenCV](https:\/\/opencv.org\/)\n* [CUDA](https:\/\/developer.nvidia.com\/cuda-toolkit)","5659c0d7":"## Training Loop\n\nSet Seed, use GPU, if one is available, then define discriminator and move models to GPU.","eeafb2ac":"Let\u2019s instantiate a `FaceDataset`, apply two transformations .First, we normalize the images so that they are in the range of -1 to +1 (from an original range of 0 to 1). Second, we apply the Stack collation, which takes a batch of tensors and stacks them into a single tensor along the first dimension.\n\nNext, we create a data loader and pass it this dataset. Then, add options to them.\n\nFinally, create two optimizers.","709af96a":"Let's take a look after 1st, 2nd, 5th, 10th and 25th epochs of training.","7a1affa9":"We first evaluate the discriminator on real images using label smoothing(`uniform_(0.8, 1.0)`), compute the loss, repeat for the fake images, update parameters, after that, re-evaluate the discriminator on the fake images with `fake_labels` tensor with all ones and take generator's step.\n\nCreate checkpoints every `kCheckpointEvery`, save a batch of images."}}