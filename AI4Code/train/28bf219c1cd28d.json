{"cell_type":{"cb0eb4d9":"code","305b6d43":"code","983bd81f":"code","12b00ddb":"code","61ca4cd1":"code","f59a3c55":"code","63e9050c":"code","2e050155":"code","fde5611d":"code","99f87ed2":"code","8ed9b672":"code","adda667a":"code","57643ec3":"code","93fa3d9f":"code","165ac20d":"code","112b2b57":"code","4d51e648":"code","fb2d2fd8":"code","9072f425":"code","4f275631":"code","80885402":"code","10126213":"code","65a831e3":"code","ac933927":"code","ac379983":"code","924acedb":"code","817bfe97":"code","1f7d26a5":"code","70e4ca6a":"code","44e0c8d4":"code","5449caa3":"markdown","ab33468f":"markdown","c7620ef5":"markdown","730f8791":"markdown","13aa2e27":"markdown","45abb696":"markdown","54972d63":"markdown","4461411e":"markdown","829d2d9e":"markdown","0ad175fe":"markdown","d6175472":"markdown","538c2a07":"markdown","10587c51":"markdown","403f2288":"markdown","6754a4b3":"markdown","818a7383":"markdown","d34ba031":"markdown","685c5930":"markdown","b139526c":"markdown","5bcc6257":"markdown","6869cf3f":"markdown","42cafbef":"markdown","2c71cb08":"markdown","e6d85b1c":"markdown","a14f57d8":"markdown","1a74c192":"markdown","264e716d":"markdown","2be14318":"markdown"},"source":{"cb0eb4d9":"#C\u00c9LULA KE-LIB-01\nimport numpy as np\nimport keras as K\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom keras.utils import to_categorical\nimport h5py\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'","305b6d43":"#C\u00c9LULA KE-LIB-02\ndef loadDatasets():\n    train_dataset = h5py.File('..\/input\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # features de treinamento\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # labels de treinamento\n\n    test_dataset = h5py.File('..\/input\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # features de teste\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # labels de teste\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # lista das classes\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","983bd81f":"#IMPLEMENTE O C\u00d3DIGO AQUI (1 linha)\n  X_train, y_train, X_test, y_test, classes = loadDatasets()","12b00ddb":"#IMPLEMENTE O C\u00d3DIGO AQUI (1 linha)\nX_train[0]","61ca4cd1":"    index = 10\n    plt.figure()\n    plt.imshow(X_train[index])","f59a3c55":"#IMPLEMENTE O C\u00d3DIGO AQUI\nfor index in range(1,11):\n    #index = 10\n    print (\"Index\", index)\n    plt.figure()\n    plt.imshow(X_train[index])","63e9050c":"#C\u00c9LULA KE-LIB-03\nnp.random.seed(4)\ntf.set_random_seed(13)\n\n#IMPLEMENTE O C\u00d3DIGO AQUI (2 linhas)\nX_train = X_train \/ 255\nX_test = X_test \/ 255","2e050155":"#C\u00c9LULA KE-LIB-04\n#IMPLEMENTE O C\u00d3DIGO AQUI (2 linhas)\nX_train_flatten =  X_train.reshape(X_train.shape[0], -1)\nX_test_flatten  = X_test.reshape(X_test.shape[0], -1)","fde5611d":"#IMPLEMENTE O C\u00d3DIGO AQUI (2 linhas)\nprint('Teste: '+str(X_test_flatten))\nprint('Teste: '+str(X_train_flatten))","99f87ed2":"#C\u00c9LULA KE-LIB-05\nlayers_dims = [12288, 20, 1] # modelo de 2 camadas densas\n\ntf.logging.set_verbosity(tf.logging.ERROR) #desliga os warnings do tensorflow\n\n#Limpa o modelo previamente usado, sen\u00e3o ir\u00e1 acumular camadas\nK.backend.clear_session()\n\n#Inicializador\ninit = K.initializers.glorot_uniform()\n\n#IMPLEMENTE O C\u00d3DIGO AQUI\n\n#Criando o otimizador\ntheOptimizer = K.optimizers.RMSprop()\n\n#Construindo o modelo (topologia)\nmodel = K.models.Sequential()\n\n#Exemplo de input_shape na camada de entrada da rede\n# model = Sequential()\n# model.add(Dense(32, input_shape=(16,)))\n# o modelo ir\u00e1 receber como entrada arrays no formato (*, 16)\n# e como sa\u00edda ter\u00e1 arrays no formato (*, 32)\n\n#Rede Densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dense(units=layers_dims[1], input_shape=(layers_dims[0],), kernel_initializer=init , activation='relu'))    \nmodel.add(K.layers.Dense(units=1, activation='sigmoid'))\n\n# Compile o modelo - IMPLEMENTE O C\u00d3DIGO AQUI    \nmodel.compile( loss='binary_crossentropy', optimizer=theOptimizer, metrics=['accuracy'])\nmodel.summary()","8ed9b672":"#C\u00c9LULA KE-LIB-06\n#IMPLEMENTE O C\u00d3DIGO AQUI\nbatch_size = 19\nmax_epochs = 2500\n\nprint(\"Iniciando treinamento... \")\n#IMPLEMENTE O C\u00d3DIGO AQUI\nh = model.fit(x = X_train_flatten, y = y_train.T, batch_size= batch_size, epochs= max_epochs, verbose=1)\n\nprint(\"Treinamento finalizado \\n\")","adda667a":"#C\u00c9LULA KE-LIB-07\n#IMPLEMENTE O C\u00d3DIGO AQUI\neval_train1 = model.evaluate(X_train_flatten, y_train.T, verbose=0)\nprint(\"Erro m\u00e9dio do treinamento: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_train1[0], eval_train1[1]*100))\n\neval_test1 = model.evaluate(X_test_flatten, y_test.T, verbose=0)\nprint(\"Erro m\u00e9dio do teste: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_test1[0], eval_test1[1]*100))","57643ec3":"X_train, y_train, X_test, y_test, classes = loadDatasets()\n\nX_train = X_train \/ 255\nX_test = X_test \/ 255\n\nX_train_flatten = X_train.reshape(X_train.shape[0], -1)   # The \"-1\" makes reshape flatten the remaining dimensions\nX_test_flatten  = X_test.reshape(X_test.shape[0], -1)","93fa3d9f":"#C\u00c9LULA KE-LIB-08\nlayers_dims = [12288, 20, 7, 5, 1] # modelo de 4 camadas densas\n\ntf.logging.set_verbosity(tf.logging.ERROR) #desliga os warnings do tensorflow\n\n#Limpa o modelo previamente usado, sen\u00e3o ir\u00e1 acumular camadas\nK.backend.clear_session()\n\n#Inicializador\ninit = K.initializers.glorot_uniform()\n\n#IMPLEMENTE O C\u00d3DIGO AQUI\n#Criando o otimizador\ntheOptimizer = K.optimizers.RMSprop()\n\n#Construindo o modelo (topologia)\nmodel = K.models.Sequential()\n\n#Rede Densa - IMPLEMENTE O C\u00d3DIGO AQUI\n## Rede com as camadas definidas no vetor layerdims\nmodel.add(K.layers.Dense(units=layers_dims[1], kernel_initializer=init, input_shape=(layers_dims[0],),activation='relu'))\nmodel.add(K.layers.Dense(units=layers_dims[2], activation='relu'))\nmodel.add(K.layers.Dense(units=layers_dims[3], activation='relu'))\nmodel.add(K.layers.Dense(units=layers_dims[4], activation='sigmoid'))\n\n#Compile o modelo - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.compile(loss='binary_crossentropy',optimizer=theOptimizer,metrics=['accuracy'])\nmodel.summary()\n    ","165ac20d":"#C\u00c9LULA KE-LIB-09\n#IMPLEMENTE O C\u00d3DIGO AQUI\nbatch_size = 19\nmax_epochs = 2500\n\nprint(\"Iniciando treinamento... \")\n#IMPLEMENTE O C\u00d3DIGO AQUI\nh = model.fit(x=X_train_flatten, y=y_train.T, batch_size=batch_size, epochs=max_epochs, verbose=1)\n\nprint(\"Treinamento finalizado \\n\")","112b2b57":"#C\u00c9LULA KE-LIB-10\n#IMPLEMENTE O C\u00d3DIGO AQUI\neval_train2 = model.evaluate(X_train_flatten, y_train.T, verbose=0)\nprint(\"Erro m\u00e9dio do conjunto de treinamento: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_train2[0], eval_train2[1]*100))\n\neval_test2 = model.evaluate(X_test_flatten, y_test.T, verbose=0)\nprint(\"Erro m\u00e9dio do conjunto de teste: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_test2[0], eval_test2[1]*100))","4d51e648":"X_train, y_train, X_test, y_test, classes = loadDatasets()\n\nimage_shape = (64,64,3)\n\nX_train = X_train.reshape(-1, 64, 64, 3)\nX_train = X_train \/ 255\n\nX_test = X_test.reshape(-1, 64, 64, 3)\nX_test = X_test \/ 255\n\ny_train = y_train.reshape(-1, 1)\n\ny_test = y_test.reshape(-1, 1)","fb2d2fd8":"#C\u00c9LULA KE-LIB-11\nlayers_dims = [12288, 20, 7, 5, 1] # modelo de 4 camadas densas\n\ntf.logging.set_verbosity(tf.logging.ERROR) #desliga os warnings do tensorflow\n\n#Limpa o modelo previamente usado, sen\u00e3o ir\u00e1 acumular camadas\nK.backend.clear_session()\n\n#Inicializador\ninit = K.initializers.glorot_uniform()\n\n#Criando o otimizador\n#IMPLEMENTE O C\u00d3DIGO AQUI\ntheOptimizer = K.optimizers.Adam()\n\n#Construindo o modelo (topologia)\nmodel = K.models.Sequential()\n\n#Camada convolucional - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init, activation='relu', input_shape=image_shape))\nmodel.add(K.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init, activation='relu'))\n\n#MaxPooling - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.MaxPooling2D(pool_size=(2, 2))) \n\n#Dropout de 25% na camada anterior - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dropout(0.25))\n\n#Flatten para entrada da rede densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Flatten())\n\n#Camada Densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dense(units=100, kernel_initializer=init, activation='relu',input_shape=(209,)))\n  \n#Dropout de 50% na camada anterior - IMPLEMENTE O C\u00d3DIGO AQUI \nmodel.add(K.layers.Dropout(0.5))\n\n#Camada densa - IMPLEMENTE O C\u00d3DIGO AQUI     \nmodel.add(K.layers.Dense(units=10, kernel_initializer=init, activation='relu'))\nmodel.add(K.layers.Dense(units=1, kernel_initializer=init, activation='sigmoid'))\n    \n#Compile o modelo - IMPLEMENTE O C\u00d3DIGO AQUI \nmodel.compile(loss='binary_crossentropy', optimizer=theOptimizer, metrics=['acc'])\nmodel.summary()","9072f425":"#C\u00c9LULA KE-LIB-12\n\n#IMPLEMENTE O C\u00d3DIGO AQUI\nbatch_size = 19\nmax_epochs = 50\n\nprint(\"Iniciando treinamento... \")\nh = model.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, verbose=1)\n\nprint(\"Treinamento finalizado \\n\")","4f275631":"#C\u00c9LULA KE-LIB-13\n#IMPLEMENTE O C\u00d3DIGO AQUI\neval_train3 = model.evaluate(X_train, y_train, verbose=0)\nprint(\"Erro m\u00e9dio do treinamento: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_train3[0], eval_train3[1]*100))\n\neval_test3 = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Erro m\u00e9dio do teste: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_test3[0], eval_test3[1]*100))","80885402":"#IMPLEMENTE O C\u00d3DIGO AQUI (1 linha)\nplt.plot(range(len(h.history['loss'])), h.history['loss'])","10126213":"#Tabela de resultados dos modelos na fase de treinamento\nprint('Tabela de resultados dos modelos na fase de treinamento')\nresults_train = pd.DataFrame(columns=['Modelo','Perda','Acur\u00e1cia'])\nresults_train['Modelo'] = ['Rede Neural 2 camada','Rede Neural 4 camada','Rede Convolucional 6 camada']\nresults_train['Perda'] = [eval_train1[0],eval_train2[0],eval_train3[0]]\nresults_train['Acur\u00e1cia'] = [eval_train1[1],eval_train2[1],eval_train3[1]]\nresults_train","65a831e3":"\n#Tabela de resultados dos modelos na fase de teste\nprint('Tabela de resultados dos modelos na fase de teste')\nresults_test = pd.DataFrame(columns=['Modelo','Perda','Acur\u00e1cia'])\nresults_test['Modelo'] = ['Rede Neural 2 camada','Rede Neural 4 camada','Rede Convolucional 6 Camadas']\nresults_test['Perda'] = [eval_test1[0],eval_test2[0],eval_test3[0]]\nresults_test['Acur\u00e1cia'] = [eval_test1[1],eval_test2[1],eval_test3[1]]\nresults_test","ac933927":"X_train, y_train, X_test, y_test, classes = loadDatasets()\n\nimage_shape = (64,64,3)\n\nX_train = X_train.reshape(-1, 64, 64, 3)\nX_train = X_train \/ 255\n\nX_test = X_test.reshape(-1, 64, 64, 3)\nX_test = X_test \/ 255\n\ny_train = y_train.reshape(-1, 1)\n\ny_test = y_test.reshape(-1, 1)","ac379983":"#C\u00c9LULA KE-LIB-11\nlayers_dims = [12288, 20, 7, 5, 1] # modelo de 4 camadas densas\n\ntf.logging.set_verbosity(tf.logging.ERROR) #desliga os warnings do tensorflow\n\n#Limpa o modelo previamente usado, sen\u00e3o ir\u00e1 acumular camadas\nK.backend.clear_session()\n\n#Inicializador\ninit = K.initializers.glorot_uniform()\n\n#Criando o otimizador\n#IMPLEMENTE O C\u00d3DIGO AQUI\ntheOptimizer = K.optimizers.Adadelta()\n\n#Construindo o modelo (topologia)\nmodel = K.models.Sequential()\n\n#Camada convolucional - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init, activation='relu', input_shape=image_shape))\n\n#Camada convolucional - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init, activation='relu'))\n\n#MaxPooling - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.MaxPooling2D(pool_size=(2, 2))) \n\n#Dropout de 25% na camada anterior - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dropout(0.25))\n\n#Flatten para entrada da rede densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Flatten())\n\n#Camada Densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dense(units=100, kernel_initializer=init, activation='relu',input_shape=(209,)))\n  \n#Dropout de 50% na camada anterior - IMPLEMENTE O C\u00d3DIGO AQUI \n#model.add(K.layers.Dropout(0.5))\n\n#Camada densa - IMPLEMENTE O C\u00d3DIGO AQUI     \nmodel.add(K.layers.Dense(units=10, kernel_initializer=init, activation='relu'))\n#Camada Densa - IMPLEMENTE O C\u00d3DIGO AQUI\nmodel.add(K.layers.Dense(units=1, kernel_initializer=init, activation='sigmoid'))\n    \n#Compile o modelo - IMPLEMENTE O C\u00d3DIGO AQUI \nmodel.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])\nmodel.summary()","924acedb":"#C\u00c9LULA KE-LIB-12\n\n#IMPLEMENTE O C\u00d3DIGO AQUI\nbatch_size = 19\nmax_epochs = 50\n\nprint(\"Iniciando treinamento... \")\nh = model.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, verbose=1)\n\nprint(\"Treinamento finalizado \\n\")","817bfe97":"#C\u00c9LULA KE-LIB-13\n#IMPLEMENTE O C\u00d3DIGO AQUI\neval_train4 = model.evaluate(X_train, y_train, verbose=0)\nprint(\"Erro m\u00e9dio do conjunto de treinamento: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_train4[0], eval_train4[1]*100))\n\neval_test4 = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Erro m\u00e9dio do conjunto de teste: Perda {0:.4f}, acuracia {1:.4f}\".format(eval_test4[0], eval_test4[1]*100))","1f7d26a5":"#IMPLEMENTE O C\u00d3DIGO AQUI (1 linha)\nplt.plot(range(len(h.history['loss'])), h.history['loss'])","70e4ca6a":"#Tabela de resultados dos modelos na fase de treinamento\nprint('Tabela de resultados dos modelos na fase de treinamento')\nresults_train = pd.DataFrame(columns=['Modelo','Otimizador','Perda','Acur\u00e1cia'])\nresults_train['Modelo'] = ['Rede Neural 2 camada','Rede Neural 4 camada','Rede Convolucional','Rede Convolucional']\nresults_train['Otimizador'] = ['RMSProp','RMSProp','Adam','Adadelta']\nresults_train['Perda'] = [eval_train1[0],eval_train2[0],eval_train3[0],eval_train4[0]]\nresults_train['Acur\u00e1cia'] = [eval_train1[1],eval_train2[1],eval_train3[1],eval_train4[1]]\n\nresults_train","44e0c8d4":"#Tabela de resultados dos modelos na fase de teste\nprint('Tabela de resultados dos modelos na fase de teste')\nresults_test = pd.DataFrame(columns=['Modelo','Otimizador','Perda','Acur\u00e1cia'])\nresults_test['Modelo'] = ['Rede Neural 2 camada','Rede Neural 4 camada','Rede Convolucional','Rede Convolucional']\nresults_test['Otimizador'] = ['RMSProp','RMSProp','Adam','Adadelta']\nresults_test['Perda'] = [eval_test1[0], eval_test2[0], eval_test3[0], eval_test4[0]]\nresults_test['Acur\u00e1cia'] = [eval_test1[1], eval_test2[1], eval_test3[1], eval_test4[1]]\n\nresults_test","5449caa3":"### Para inicializar uma CNN, utilize um comando no padr\u00e3o\n\n        model.add(K.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init, activation='relu', input_shape=image_shape))\n\n**Nota:** observe que o parametro input_shape indica a tupla referente \u00e0s dimensoes da imagem (64 x 64 x 3 canais) e este par\u00e2metro deve ser usado **APENAS NA PRIMEIRA camada da CNN** (e ignorado nas demais camadas do modelo)","ab33468f":"# <center> Atividade 1 - Rede Neural Densa com 1 camada oculta <\/center>","c7620ef5":"## 1.2. Instru\u00e7\u00f5es:\n\n- Verifique a dimens\u00e3o de um dado de entrada ( X_train[0] )","730f8791":"## 1.7. Instru\u00e7\u00f5es:\n\n- Construa uma topologia de uma Rede Neural Densa com 2 camadas:\n\n        layers_dims = [12288, 20, 1] # modelo de 2 camadas densas\n\nDados da topologia:\n- Otimizador **RMSProp**\n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada intermediaria: **ReLu** \n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda: **Sigmoid** \n- Utilize treinamento em lote com **batch_size= 19**\n- Limite o treinamento a **2500** \u00e9pocas\n- Fun\u00e7\u00e3o de Perda (Loss): **binary_crossentropy** (estamos lidando com classifica\u00e7\u00e3o bin\u00e1ria)","13aa2e27":"## Construindo a Topologia da Rede\n\n## 1.4. Instru\u00e7\u00f5es:\n\n- Fa\u00e7a a normaliza\u00e7\u00e3o dos dados de ENTRADA dos conjuntos de treinamento e teste pelo maior valor de pixel (255)","45abb696":"# Compare os resultados obtidos.","54972d63":"\n\n# <center> Atividade 2 - Rede Neural Densa com 2 camadas ocultas <\/center>","4461411e":"# <center>Observa\u00e7\u00e3o\n  <center>\n      \n      \n      \n      \nMesmo com o aumento do numero de camadas a rede continua resultando em overfit","829d2d9e":"\u00c9 poss\u00edvel visualizar o gr\u00e1fico de evolu\u00e7\u00e3o do erro ao longo do treinamento a partir do seguinte c\u00f3digo:\n\n    plt.plot(range(len(h.history['loss'])), h.history['loss'])\n    \n**Nota:** a vari\u00e1vel 'h' \u00e9 a vari\u00e1vel que foi retornada pelo comando *model.fit(...)*","0ad175fe":"# <center>Resposta\n  <center>\nA primeira as duas primeiras rede apresentam elevado overfitting, no entanto, a segunda rede de 4 camadas apresenta um overfitting mais agravado, as redes convolucionais apesar de ainda apresentar overfitting, apresentam um resultado muito melhor, sendo que o otimizador Adadelta se apresentou como mais adequado \u00e0 aplica\u00e7\u00e3o.\n\n","d6175472":"# Material de aula - Redes Neurais e Deep Learning","538c2a07":"\n## <center> Exerc\u00edcio 1 - Keras: Classifica\u00e7\u00e3o de imagens com CNN <\/center>","10587c51":"## 1.3. Instru\u00e7\u00f5es:\n\n- Exiba algumas imagens, no intervalo de 0 a 10, a fim de verificar como s\u00e3o as imagens do dataset","403f2288":"## 3.1. Instru\u00e7\u00f5es:\n\nConstrua uma topologia de rede CONVOLUCIONAL da seguinte forma:\n\n- Otimizador **Adam ou RMSProp**\n\n    - 1 Camada Convolucional (Conv2D) com 32 Filtros, strides= (1,1) e padding='same' com fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada Convolucional (Conv2D) com 64 Filtros, strides= (1,1) e padding='same' com fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada MaxPooling2D com janela (2,2)\n    - 1 Fun\u00e7\u00e3o de Dropout de 25% (0,25)\n\n    - 1 Fun\u00e7\u00e3o *Flatten*\n    - 1 Camada densa com 100 unidades e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Fun\u00e7\u00e3o de Dropout de 50% (0,5)\n    - 1 Camada densa com 10  unidades e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada densa com 1   unidade  e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **Sigmoid**\n\n- Utilize treinamento em lote com **batch_size= 19**\n- Limite o treinamento a **50** \u00e9pocas\n- Fun\u00e7\u00e3o de Perda (Loss): **binary_crossentropy** (estamos lidando com classifica\u00e7\u00e3o bin\u00e1ria)    ","6754a4b3":"> ## 3.2. Instru\u00e7\u00f5es:\n\nRefa\u00e7a o treinamento com a topologia acima, **ELIMINANDO a fun\u00e7\u00e3o de Dropout de 50% da Camada densa**\n\n- Otimizador **Adadelta**\n\n    - 1 Camada Convolucional (Conv2D) com 32 Filtros, strides= (1,1) e padding='same' com fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada Convolucional (Conv2D) com 64 Filtros, strides= (1,1) e padding='same' com fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada MaxPooling2D com janela (2,2)\n    - 1 Fun\u00e7\u00e3o de Dropout de 25% (0,25)\n\n    - 1 Fun\u00e7\u00e3o *Flatten*\n    - 1 Camada densa com 100 unidades e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - <strike> 1 Fun\u00e7\u00e3o de Dropout de 50% (0,5) <\/strike>\n    - 1 Camada densa com 10  unidades e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **ReLu**\n    - 1 Camada densa com 1   unidade  e fun\u00e7\u00e3o de ativa\u00e7\u00e3o **Sigmoid**\n\n- Utilize treinamento em lote com **batch_size= 19**\n- Limite o treinamento a **50** \u00e9pocas\n- Fun\u00e7\u00e3o de Perda (Loss): **binary_crossentropy** (estamos lidando com classifica\u00e7\u00e3o bin\u00e1ria)    ","818a7383":"## 2.1. Instru\u00e7\u00f5es:\n\n- Construa uma nova topologia de rede com 4 camadas:\n- Refa\u00e7a o treinamento e compare os resultados\n\n    layers_dims = [12288, 20, 7, 5, 1] # modelo de 4 camadas densas\n    \nDados da topologia:\n- Otimizador **RMSProp**\n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da primeira camada oculta: **ReLu** \n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da segunda camada oculta: **ReLu** \n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da terceira camada oculta: **ReLu** \n- Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda: **Sigmoid** \n- Utilize treinamento em lote com **batch_size= 19**\n- Limite o treinamento a **2500** \u00e9pocas\n- Fun\u00e7\u00e3o de Perda (Loss): **binary_crossentropy** (estamos lidando com classifica\u00e7\u00e3o bin\u00e1ria)    ","d34ba031":"Instru\u00e7\u00f5es para a realiza\u00e7\u00e3o desta atividade:\n\n1. Fa\u00e7a o download deste notebook Jupyter para sua esta\u00e7\u00e3o de trabalho\n2. Realize as atividades solicitadas dentro do notebook, nos espa\u00e7os apropriados e indicados\n3. Salve o notebook no seguinte padr\u00e3o: <nome_sobrenome_RA>.ipynb (por exemplo: **rodrigo_caropreso_123456.ipynb ** )\n4. Envie o notebook com a atividade realizada na sala de aula da disciplina (Google Classroom)\n","685c5930":"# Compare os resultados obtidos com a Rede Convolucional comparada \u00e0s Redes Neurais das Atividades 1 e 2","b139526c":"A ambas as redes apresentam um overfitting e poucas diferen\u00e7as nos resultados de perda e acuracia.\n","5bcc6257":"## 1.8. Instru\u00e7\u00f5es:\n\n- Execute o treinamento da rede com o comando no formato\n\n        h = model.fit(x=X_train_flatten, y=y_train.T, batch_size=batch_size, epochs=max_epochs, verbose=1)\n        \n**Nota:** Pelo formato dos dados o valor de entrada deve ser inserido na forma **[N_amostras, N_Entradas]** enquanto o vetor de sa\u00edda deve ser inserido no formato **[N_amostras, N_saidas]**","6869cf3f":"## 1.9. Instru\u00e7\u00f5es:\n\n- Fa\u00e7a a verifica\u00e7\u00e3o dos resultados dos conjuntos de **treinamento E teste** a partir do comando\n\n        eval = model.evaluate(X_train_flatten, y_train.T, verbose=0)\n\n**Nota:**  observe que o comando acima faz a valida\u00e7\u00e3o do conjunto de treinamento apenas. Al\u00e9m disso, o formato dos dados de entrada segue o mesmo padr\u00e3o utilizado no comando de treinamento","42cafbef":"## 1.5. Instru\u00e7\u00f5es:\n\n- Fa\u00e7a o ajuste de dimensionalidade do tensor de entrada para 1 dimens\u00e3o (flatenning)\n\n**Nota:** n\u00e3o esque\u00e7a que a quantidade de entradas da rede ser\u00e1 dada pelo tensor de entrada 'achatado em 1 dimens\u00e3o' ser\u00e1 dada por: $ 64 \\cdot 64 \\cdot 3 = 12288$\n\n**Nota:** \u00c9 poss\u00edvel redimensionar um tensor de N dimens\u00f5es com o comando reshape, na forma:\n\n    X_train_flatten = X_train.reshape(X_train.shape[0], -1)","2c71cb08":"## 1.1. Instru\u00e7\u00f5es:\n\n- Fa\u00e7a a carga do dataset atrav\u00e9s do m\u00e9todo \n\n        X_train, y_train, X_test, y_test, classes = loadDatasets()","e6d85b1c":"## O c\u00f3digo abaixo exibe uma imagem do conjunto de treinamento para a classe 'n\u00e3o-gato'\n\n    index = 10\n    plt.figure()\n    plt.imshow(X_train[index])","a14f57d8":"### O conjunto de dados de treinamento est\u00e1 armazenado no formato h5 e cont\u00e9m imagens de gatos e imagens de objetos que n\u00e3o s\u00e3o gatos. \n\n### O Objetivo da atividade \u00e9 construir uma Rede capaz de fazer a classifica\u00e7\u00e3o bin\u00e1ria entre gatos VERSUS N\u00e3o-gatos","1a74c192":"## 1.6. Instru\u00e7\u00f5es:\n\n- Verifique as dimensoes dos vetores X_train_flatten e y_train_flatten","264e716d":"\n# <center> Atividade 3 - Rede Neural Convolucional com 6 camadas <\/center>","2be14318":"# <center>Observa\u00e7\u00e3o\n\n<center>\n\n\nModelo completamente overfitted****"}}