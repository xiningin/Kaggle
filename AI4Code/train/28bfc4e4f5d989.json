{"cell_type":{"e7f68a84":"code","2fb77364":"code","caeca0c8":"code","f45f7721":"code","0c623a22":"code","fbf522bb":"code","6f474ddd":"code","d425b9cd":"code","c32352ff":"code","66e13283":"code","7adb73cd":"code","dbe17b4c":"code","56a7165d":"code","1235e25c":"code","573c33d5":"code","50431ccf":"code","938a10b5":"code","6844a5de":"code","b6241ece":"code","58fc9c35":"code","53900b2f":"code","f6aa7f58":"code","058f262f":"code","b9a0716a":"code","9420af66":"markdown","b088ac18":"markdown","c6a48fb7":"markdown","8f389fde":"markdown","6ff95988":"markdown"},"source":{"e7f68a84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fb77364":"from __future__ import unicode_literals\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import regularizers","caeca0c8":"data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndata = data.drop(['id','location'],axis=1)\ndata.head()","f45f7721":"#Stopwords\nstop = (stopwords.words('english'))\npunctuation = list(string.punctuation)\nfor i in punctuation:\n    stop.append(i)","0c623a22":"#Cleaning Data\n\nstemmer = SnowballStemmer('english',ignore_stopwords=True)\nlemmatizer = WordNetLemmatizer()\n\ndef remove_stopwords(text):\n    sentences = []\n    for word in text.split():\n        if word.lower().strip() not in stop and len(word)>3:\n            word = lemmatizer.lemmatize(word)\n            sentences.append(word.lower().strip())\n    return \" \".join(sentences)\n\ndef remove_punctuations(text):\n    punc = re.compile(r'[%s]'%string.punctuation)\n    return punc.sub(r'',text)\n                      \ndef remove_urls(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_squarebrackets(text):\n    square = re.compile(r'\\[.*?\\]')\n    return square.sub(r'',text)\n\ndef remove_tags(text):\n    tags = re.compile(r'<.*?>')\n    return tags.sub(r'',text)\n    \ndef remove_numbers(text):\n    num = re.compile(r'\\w*\\d\\w*')\n    return num.sub(r'',text)\n    \ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'',text)\n\ndef clean(text):\n    text = remove_punctuations(text)\n    text = remove_urls(text)\n    text = remove_tags(text)\n    text = remove_squarebrackets(text)\n    text = remove_emoji(text)\n    text = remove_numbers(text)\n    text = remove_stopwords(text)\n    return text\n","fbf522bb":"data['text'] = data['text'].apply(lambda x:clean(x))","6f474ddd":"#Count of tweets\nsns.countplot(data['target'],palette='RdBu_r')\nplt.title(\"Non-Disaster vs Disaster Tweets\")\nprint(\"No of Non-Disaster Tweets: \" ,data['target'].value_counts()[0])\nprint(\"No of Disaster Tweets: \" ,data['target'].value_counts()[1])","d425b9cd":"#Wordcloud\nfig,ax = plt.subplots(figsize=(12,16))\nplt.axis('off')\n\nplt.subplot(2,1,1)\ntext = \" \".join(data[data['target']==0]['text'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Non-Disaster Tweet\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(2,1,2)\ntext = \" \".join(data[data['target']==1]['text'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Disaster Tweet\")\nplt.axis('off')\nplt.imshow(wordcloud)","c32352ff":"#No Of Characters In A Tweet\nfig,ax = plt.subplots(figsize=(12,6))\nfig.suptitle(\"NO OF CHARACTERS IN A TWEET\")\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Disaster Tweets\")\nwords = data[data['target']==0]['text'].str.len()\nsns.distplot(words,kde=True)\n\nplt.subplot(1,2,2)\nplt.title(\"Disaster Tweets\")\nwords = data[data['target']==1]['text'].str.len()\nsns.distplot(words,kde=True)","66e13283":"#Average Word Length In A Tweet\nfig,ax = plt.subplots(figsize=(12,6))\nfig.suptitle(\"AVERAGE WORD LENGTH IN A Tweet\")\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Disaster Tweets\")\nword_length = data[data['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word_length.map(lambda x:np.mean(x)),kde=True)\n\nplt.subplot(1,2,2)\nplt.title(\"Disaster Tweets\")\nword_length = data[data['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word_length.map(lambda x:np.mean(x)),kde=True)","7adb73cd":"#Keywords\nplt.figure(figsize=(14,6))\nsns.barplot(x=data['keyword'].value_counts()[:20],y=data['keyword'].value_counts()[:20].index,palette='RdBu_r')\nplt.xlabel(\"Count\")\nplt.ylabel(\"Keyword\")","dbe17b4c":"#Split the data\nx_train,x_test,y_train,y_test = train_test_split(data['text'],data['target'],test_size=0.2,random_state=0)","56a7165d":"#Tokenizer\nvocab_size=10000\nembedding_dim=200\nmax_length=100\ntrunc_type=\"post\"\npad_type=\"post\"\noov_tok=\"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(list(x_train)+list(x_test))\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\ntrain_padded = pad_sequences(train_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)\n\ntest_sequences = tokenizer.texts_to_sequences(x_test)\ntest_padded = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)","1235e25c":"print(\"words =\", len(word_index))\nprint(\"train =\",len(train_padded))\nprint(\"test =\",len(test_padded))","573c33d5":"#GloVe Embeddings\nembeddings_index={}\nwith open(\"..\/input\/glove6b\/glove.6B.200d.txt\",'r',encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nembeddings_matrix = np.zeros((len(word_index)+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","50431ccf":"#Neural Network\nmodel = Sequential()\n\nmodel.add(Embedding(len(word_index)+1,embedding_dim,input_length=max_length,weights=[embeddings_matrix]))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(Bidirectional(LSTM(128,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\nmodel.summary()","938a10b5":"#Callbacks\nearlystop = EarlyStopping(monitor='val_loss',patience=3,verbose=1)\nlearning_reduce = ReduceLROnPlateau(patience=2,monitor=\"val_acc\",verbose=1,min_lr=0.00001,factor=0.5,cooldown=1)\ncallbacks = [earlystop,learning_reduce]","6844a5de":"epoch=10\nhistory = model.fit(train_padded,y_train,epochs=epoch,validation_data=(test_padded,y_test),callbacks=callbacks)","b6241ece":"#Plot\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')\n","58fc9c35":"y_pred = model.predict_classes(test_padded)\nprint(\"Accuracy: \",accuracy_score(y_test,y_pred).round(3))\nprint(\"Precision: \",precision_score(y_test,y_pred).round(3))\nprint(\"Recall: \",recall_score(y_test,y_pred).round(3))\nprint(\"F1-Score: \",f1_score(y_test,y_pred).round(3))","53900b2f":"#Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\ncm = pd.DataFrame(cm , index = ['Non-Disaster','Disaster'] , columns = ['Non-Disaster','Disaster'])\nsns.heatmap(cm,cmap= \"Blues\",annot=True,fmt='')\nplt.title(\"Confusion Matrix\")","f6aa7f58":"#Test Data\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest['text'] = test['text'].apply(lambda x:clean(x))\n\ntesting_sequences = tokenizer.texts_to_sequences(test['text'])\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)\n\npredictions = model.predict(testing_padded)\n\nsubmission['target'] = (predictions>0.5).astype(int)","058f262f":"submission.head()","b9a0716a":"submission.to_csv(\"submission.csv\", index=False, header=True)","9420af66":"# Preparing Data","b088ac18":"# Submission","c6a48fb7":"# Importing Libraries","8f389fde":"# Exploratory Data Analysis","6ff95988":"# Classification Model"}}