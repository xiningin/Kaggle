{"cell_type":{"4828c10e":"code","fd6649e3":"code","c730659f":"code","2f2c4ab4":"code","e61ac583":"code","bdc01f64":"code","29e362f3":"code","5ba4f05a":"code","b30384b1":"code","b43d4e0c":"code","1a25e0f3":"code","19d15196":"code","634c28e2":"code","4e80cf1b":"code","022a3668":"code","571cce36":"code","6c5251c2":"code","35c20e36":"code","df508102":"code","052605b7":"code","9e08b0bd":"code","893ab6ce":"code","8f7f71c5":"markdown","e6dd1e14":"markdown"},"source":{"4828c10e":"# SVM is an ML model which tries to separate two different classes with a hyperplane.\n# SVMs again are naturally binary classifiers\n\n# Data points of 'd' dimensions are separated by (d-1) dimensional hyperplane using the method \n# \"Maximum-Margin Hyperplane\"\n\n# There can be many hyperplanes which separate the two classes, but maximum-margin hyperplane will have the highest\n# distances from the nearest points of each classes\n\n# Assuming the class labels as 1 and -1, the nearest points of each classes will go through the planes parallel \n# to max-margin hyperplane wx - b = 0, will be wx-b=1 and wx-b=-1.\n\n# Support Vector Machines are generalized linear models which means they are trying to find a linear hyperplane\n# which can separate the data. \n\n# SVMs have been modified to find non-linear hyperplanes between the data.\n\n# So, how do we do that?\n# SVMs are able to do non-linear classifications, by actually mapping the current 'd' dimensional data to \n# infinite or 'k' dimensional data space where k > d. After the mapping, SVM tries to find a linear hyperplane \n# which can classifiy the data in new 'k' dimensional data space.\n\n# Always remember that finding a hyperplane which can correctly separate different classses of data will be much \n# easier in high dimensional space and there are possibility of infinite hyperplanes.\n\n# I am not saying that will be computation friendly, but there is much probaility of that happeninig and your\n# degree of freedom is more.\n# So, linear hyperplane in 'k' dimension will actually be non-linear in 'd' dimensional space. That's how we achieve\n# non-linear classification.\n\n# How do we map the points to higher dimension space?\n# We achieve this using a method called kernel trick. Kernel function is a function which will map the lower \n# dimensional points to a higher dimensional space in such a way that we will be able to find a hyperplane in 'k' \n# dimensional space which can easily separate the data. \n\n# Come let me make it a bit clearer by a small example","fd6649e3":"# We have data like below which can not be seprated with a linear hyperplane.\n# But if you see properly, we can actually make circle in 2-D which will be able to do a great job.","c730659f":"from sklearn.datasets import make_circles\nimport seaborn as sns\nX, y = make_circles(n_samples=1000, shuffle=True, noise= .15, factor = .5)\nsns.scatterplot(X[:, 0], X[:, 1], hue=y)","2f2c4ab4":"# Let's first see whether an SVM of with linear lernel would be able to separate them well or not.","e61ac583":"from sklearn.svm import LinearSVC, SVC\nmodel = LinearSVC()\nmodel.fit(X, y)","bdc01f64":"from mlxtend.plotting import plot_decision_regions\nplot_decision_regions(X, y, clf=model, legend=2)","29e362f3":"from sklearn.metrics import accuracy_score\naccuracy_score(y_true=y, y_pred=model.predict(X))","5ba4f05a":"# linear kernel, which doesn't do the mapping of the data to higher dimensional space try to do linear separation\n# But we already know that the data is not linearly separable","b30384b1":"# Now lets work with non-linear SVMs or SVM with non-linear kernel functions\nfrom sklearn.svm import SVC\nmodel = SVC(gamma=\"scale\")\nmodel.fit(X, y)\n# To visualize the decision boundaries properly I have take only 50 points, you can plot for full data also.\nplot_decision_regions(X[:100, :], y[:100], clf=model, legend=2)","b43d4e0c":"# SVC was able to separate the points because kernel used here is radial basis function which can find the \n# the non-linear planes easily. RBF is default for SVC in sklearn.\n\n# Now, there are other kernels also, lets see how they perform for this data.","1a25e0f3":"import matplotlib.pyplot as plt\n# plt.figure(figsize=(20, 10))\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n# Different types of kernel supported by sklearn\n# You can have your custom kernel also\nkernel_types=[\"linear\", \"rbf\", \"poly\", \"sigmoid\"]\nfig, ax = plt.subplots()\nfor i in range(0, 4):\n    model = SVC(kernel=kernel_types[i])\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    acc_score = accuracy_score(y_true=y, y_pred=y_pred)\n    plt.title(kernel_types[i] + \" kernel || acc_score: \" + str(round(acc_score, 2)))\n    plot_decision_regions(X[:100, :], y[:100], clf=model, legend=2)\n    plt.show()","19d15196":"# We can see for different kernels, the decision boundary is different because of the different kernel functions.\n\n# Each of them try to find the best hyperplane in high dimension which can properly separate the dataset.\n\n# Also, observe that rbf is able to do the best in terms of separating the data. How is it able to do it ? What else\n# it can do ? You can read about it in net. That would deifinitely be good information in underastanding the kernels\n# in general.","634c28e2":"# Rather than using the actual kernel parameter, we can use Linear SVM wiht one featurization\n# We add another dimension to the data which is basically z = x^2 + y^2\n# We came up with this fucntion as our data is of circle kind. \n# Lets see whether Linear SVC is actually able to improve the score with this extra feature.","4e80cf1b":"# Adding new dimension to our data\nimport numpy as np\nnew_dim = ((X[:, 0]*X[:, 0]) + (X[:, 1]*X[:, 1])).reshape(-1, 1)\nX_t = np.hstack((X, new_dim))\ny_t = y","022a3668":"# We have three dimesions now\nX_t.shape","571cce36":"# Plot scatter plot with third dimension as y-axis\n# We can observe this is more like a parabola and we can actually make a line parallel to x-axis which can \n# separate the data.\n# Now, we can linearly separate the data\nsns.scatterplot(X_t[:, 0],X_t[:, 2], hue=y_t) ","6c5251c2":"# We can visualize this in 3-d in a better way.\nimport plotly.express as px\nfig = px.scatter_3d(x=X_t[:, 0], y=X_t[:, 1], z=X_t[:, 2],color=y_t)\nfig","35c20e36":"# Let' see whether in actual, our model is able to perform well with this new dimension\nmodel = LinearSVC()\nmodel.fit(X_t, y_t)\ny_pred = model.predict(X_t)\naccuracy_score(y_true=y, y_pred=y_pred)","df508102":"# We can see that now our model fitted to the data very well.\n# Let's try to plot the hyperplane also in 3-d.","052605b7":"import chart_studio.plotly as py\nimport plotly.graph_objs as go\n\nX_t = np.hstack((X_t, y_t.reshape(-1, 1)))\n\nz = lambda x,y: (-model.intercept_[0]-model.coef_[0][0]*x-model.coef_[0][1]*y) \/ model.coef_[0][2]\n\ntmp = np.linspace(-2,2,51)\nx,y = np.meshgrid(tmp,tmp)\n                  \nfig = go.FigureWidget()\nfig.add_surface(x=x, y=y, z=z(x,y), colorscale='Greys', showscale=False)\nfig.add_scatter3d(x=X_t[y_t==0, 0], y=X_t[y_t==0, 1], z=X_t[y_t==0, 2], mode='markers', marker={'color': 'blue'})\nfig.add_scatter3d(x=X_t[y_t==1, 0], y=X_t[y_t==1, 1], z=X_t[y_t==1, 2], mode='markers', marker={'color': 'yellow'})\nfig.show()","9e08b0bd":"# As we can see, our model is able to distinguish between the classes and find a hyperplane which can separate the \n# the data points.","893ab6ce":"# FOR THIS KERNEL, I WILL TAKE A STOP HERE.\n# IF YOU HAVE ANY THOUGHTS, PLEASE LET ME KNOW IN THE COMMENTS. IT WILL HELP ME IMPROVE.","8f7f71c5":"### Feature Engineering is what SVM Kernel is","e6dd1e14":"### SVM"}}