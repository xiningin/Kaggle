{"cell_type":{"2344f51c":"code","084a2256":"code","502cf54f":"code","5af69138":"code","64afced7":"code","a51de0e7":"code","c10c88fd":"code","da250ee9":"code","98e1dab4":"code","8f1486d6":"code","b2b2647b":"code","213b172a":"code","f25d95e6":"markdown","c39a35a1":"markdown","0874d20c":"markdown","a1ed9514":"markdown","492aed1f":"markdown","cbc6beb8":"markdown","9ea15601":"markdown","bf08b8c3":"markdown","3d71ce99":"markdown","c4cc48b3":"markdown"},"source":{"2344f51c":"import numpy as np\n\ndef reduce_mem_usage(df):\n    \"\"\"\n    Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n    \"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n        col_typename = str(col_type)\n        is_numeric = col_typename.startswith('int') or \\\n            col_typename.startswith('float') or \\\n            col_typename.startswith('bool')\n        if is_numeric:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if col_typename.startswith('int') or col_typename.startswith('bool'):\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","084a2256":"from kaggle.competitions import twosigmanews\nimport gc\n\nenv = twosigmanews.make_env()\nenv._var07 = reduce_mem_usage(env._var07)\nenv._var10 = reduce_mem_usage(env._var10)\ngc.collect()","502cf54f":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import FunctionTransformer, LabelBinarizer, StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.decomposition import TruncatedSVD\nfrom xgboost import XGBClassifier","5af69138":"class DateSplit(TimeSeriesSplit):\n    def split(self, X, y, *args, **kwargs):\n        assert isinstance(X, pd.DataFrame)\n        dates = sorted(X['date'].unique())\n        for train_idx, _ in super(self.__class__, self).split(dates):\n            train_before = dates[train_idx[-1]]\n            train_mask = (X['date'] <= train_before).values\n            val_mask = (X['date'] > train_before).values\n            yield np.where(train_mask)[0], np.where(val_mask)[0]\n            \n            \ndef metric_aggregation(_, daily_metrics):\n    return daily_metrics.mean() \/ daily_metrics.std()\n\n\nclass DailyMetricEstimator(BaseEstimator):\n    def __init__(self, base_model, pass_collumns=None):\n        self.base_model = base_model\n        self.pass_collumns = pass_collumns\n        \n    def _get_numeric_columns(self, df):\n        columns = []\n        for column in df.columns:\n            typename = str(df[column].dtype)\n            if typename.startswith('int') or typename.startswith('float') or typename.startswith('bool'):\n                columns.append(column)\n        return columns\n                \n    def _get_pass_collumns(self, X):\n        if self.pass_collumns is not None:\n            pass_collumns = self.pass_collumns\n        else:\n            pass_collumns = self._get_numeric_columns(X)\n        return pass_collumns        \n    \n    def fit(self, X, y):\n        assert isinstance(X, pd.DataFrame)\n        pass_collumns = self._get_pass_collumns(X)\n        y_classes = 1.0 * (y > 0.0)\n        self.base_model.fit(X[pass_collumns], y_classes)\n    \n    def predict(self, X):\n        assert isinstance(X, pd.DataFrame)\n        X = X.copy(deep=False)\n        pass_collumns = self._get_pass_collumns(X)\n        X['confidenceValue'] = 2.0 * self.base_model.predict(X[pass_collumns]) - 1.0\n        X['metric'] = X['returnsOpenNextMktres10'] * X['universe'] * X['confidenceValue']\n        metric = X.groupby('date')['metric'].sum().values\n        return metric\n\n\ndef twosigma_cross_val_score(model, X, y, pass_collumns=None):\n    decorated_model = DailyMetricEstimator(model, pass_collumns)\n    return cross_val_score(decorated_model, X, y, cv=DateSplit(), scoring=make_scorer(metric_aggregation))","64afced7":"class LabelBinarizerPipelineFriendly(LabelBinarizer):\n    def fit(self, X, y=None):\n        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n        super(LabelBinarizerPipelineFriendly, self).fit(X)\n        \n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n    \n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)","a51de0e7":"class Repeater(BaseEstimator):\n    def fit(self, *args, **kwargs):\n        return self\n    \n    def predict(self, X, *args, **kwargs):\n        return X\n    \n    def transform(self, X, *args, **kwargs):\n        return X\n    \n    def fit_transform(self, X, *args, **kwargs):\n        return self.fit(X).transform(X)","c10c88fd":"def make_submission(model, X, extract_features, pass_collumns=None):\n    def get_numeric_columns(df):\n        columns = []\n        for column in df.columns:\n            typename = str(df[column].dtype)\n            if typename.startswith('int') or typename.startswith('float') or typename.startswith('bool'):\n                columns.append(column)\n        return columns\n                \n    if pass_collumns is None:\n        pass_collumns = get_numeric_columns(X)\n    \n    y = 1.0 * (X['returnsOpenNextMktres10'] > 0.0)\n    \n    model.fit(X, y)\n    for market_obs_df, news_obs_df, predictions_template_df in env.get_prediction_days():\n        X_val = extract_features(market_obs_df, news_obs_df)\n        confidence = model.predict(X_val) * 2.0 - 1.0\n        prediction = pd.DataFrame({\n            'assetCode': X_val['assetCode'],\n            'confidenceValue': confidence,\n        })\n        env.predict(prediction)\n    env.write_submission_file()\n","da250ee9":"def extract_features(market_df, news_df):\n    market_df['date'] = pd.to_datetime(market_df['time'].dt.date)\n    news_df['date'] = pd.to_datetime(news_df['time'].dt.date)\n    return reduce_mem_usage(market_df)\n\n\ndf = extract_features(*env.get_training_data())\ndf.head()","98e1dab4":"twosigma_cross_val_score(DummyClassifier(), df, df['returnsOpenNextMktres10'])","8f1486d6":"market_numeric_features = ['volume', 'close', 'open', 'returnsClosePrevRaw1',\n                           'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                           'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n                           'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                           'returnsOpenPrevMktres10']\npass_collumns = market_numeric_features + ['assetCode']\nclf = Pipeline([\n    ('features', FeatureUnion([\n        ('assetCodeEncoder', Pipeline([\n            ('selector', FunctionTransformer(lambda X: X['assetCode'], \n                                             validate=False)),\n            ('label', LabelBinarizerPipelineFriendly(sparse_output=True)),\n            ('t-svd', TruncatedSVD(10)),\n            ('repeater', Repeater()),\n        ])),\n        ('numericFeatures', Pipeline([\n            ('selector', FunctionTransformer(lambda X: X[market_numeric_features], \n                                             validate=False)),\n            ('fillNa', FunctionTransformer(lambda X: X.fillna(0), \n                                           validate=False)),\n            ('scale', StandardScaler()),\n            ('repeater', Repeater()),\n        ]))\n    ])),\n    ('classifier', XGBClassifier(n_estimators=50, tree_kind='hist'))\n])","b2b2647b":"twosigma_cross_val_score(clf, df, df['returnsOpenNextMktres10'], pass_collumns=pass_collumns)","213b172a":"make_submission(clf, df, extract_features, pass_collumns)","f25d95e6":"This kernel contain just some basic pipeline with scikit-learn \"pipeline\" building, cross-validation (with implemented contest metric) and some memory tricks.\n\n# Environment preparation","c39a35a1":"In my case, at this stage it used ~5.5Gb memory instead of 7-8Gb without reducing dataframes. \n\n# scikit-learn \"utilities\" functions","0874d20c":"## Pipeline-friendly LabelBinarizer\n\nStandard```scikit-learn```'s ```LabelBinarizer``` gets fit-arguments related exception if you're trying to use it as feature extractor in ```Pipeline```. Here you could see customized class.","a1ed9514":"## Building submission\n\nFinally, next function will train model on your dataset and build submission. It's consuming next params:\n\n- model\n- dataframe with features and target (see description below)\n- function that consume market data and news data and return such dataframe\n- list of collumns used by model","492aed1f":"# Dummy model\n\nFirstly I'll check DummyClassifier to check cross-validation works:","cbc6beb8":"## repeater \"Estimator\"\nI'm using next class to build pipelines like\n```\nPipeline([\n    ('features', FeatureUnion([\n        ('feature1pipeline', Pipeline([\n            ...\n        ]),\n        ('feature2pipeline', Pipeline([\n            ...\n        ]),\n    ])),\n    ('clf', Predictor()),\n])\n```\nwithout it - building of ```feature1pipeline``` and ```feature2pipeline``` will be failed (seems like ```Pipeline``` must have extimator at last stage, not transformer).\nIt could be usefull when we're going to apply different complex pipelines to different features.\nE.g. :\n- encode embeddings for categorical features\n- scale numeric feature","9ea15601":"# Extracting features\n\nBelow I'll define feature-extraction function (now it'll only use market data) and extract features for train set:","bf08b8c3":"# XGBoost baseline\n\nNext one is baseline with xgboost on market data. Note few things (at least, it was in my case):\n- training isn't fast enough, so maybe you'll need to work next way:\n  - cross-validate model\n  - then if yound score good enought - comment call for cross-validation and save score before commiting your work","3d71ce99":"## Validation approach\n\nI'll use validation based on time-series splitting of dates. So it'll work next way:\n- split dates to N ranges\n- when on each range:\n  - fit on dates before edge\n  - validate on later dates\n\nBut:\n- I need to predict sign of 10-day returns, not returns\n- in metric funtion I can pass only predictions and real values. \n\nAnd to calculate metric value I need more information then just true returns and predicted. I also need \"universe\" feature and dates, which can't be passed to scorer. So I'll make next thing:\n- create wrapper \"estimator\", which:\n  - fit wrapped model to predict returns sign\n  - after building prediction:\n    - calculate ```recordMetric = confidenceValue * returns * universe```\n    - then aggregate daily metric: ```dailyMetric = sum(recordMetric[day] for day in dates)```\n    - return such aggregated metric instead of prediction\n- then in scoring function I'll calculate next thing: ```metric = mean(dailyMetric) \/ std(dailyMetric)```\n\nFinally, I'll wrap all this things usage in cross-validation function.\n  ","c4cc48b3":"# Building final submission"}}