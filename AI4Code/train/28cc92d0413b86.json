{"cell_type":{"82799472":"code","984e7767":"code","e0ab5ffc":"code","d1069f80":"code","0dad24f3":"code","725ef1e5":"code","b7f06755":"code","45d373de":"code","abfad0bb":"code","700d3ecb":"code","73169289":"code","919dac5c":"code","35f431f1":"code","a8d3c58b":"code","de9f21bc":"code","bc2320d5":"code","b93b6b33":"code","ea154855":"code","37100f9e":"code","b32c7cf5":"code","6c510e04":"code","3a4829c2":"code","4ecddb3b":"code","0ab9e1a0":"code","366fc5f4":"code","93e9dd6d":"code","58f1c8ed":"code","51290fcf":"code","fbdb4ed3":"code","c99c4b58":"code","ae4a37da":"code","8b182601":"code","810c8762":"code","e804b7d2":"code","319f6e9b":"code","aee23cca":"code","6781f513":"code","b67dac03":"code","c2a386f7":"markdown","eeaa3927":"markdown","1111fb7c":"markdown","f524c668":"markdown","b98305b4":"markdown","ba94df58":"markdown","ba9e8f13":"markdown","80e43c9b":"markdown","f98d229b":"markdown","398bd6e1":"markdown","6efbc916":"markdown","b25d7b80":"markdown","f6e24b38":"markdown","96c97208":"markdown","b8739424":"markdown"},"source":{"82799472":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as scp # scientific data crunching tools\nimport seaborn as sns # nice visualizations \n\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import ensemble\nfrom sklearn.model_selection import GridSearchCV\n\nimport itertools\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nsns.set(context = \"notebook\")\n\n# Print numpy arrays human readable\n# Thx2: https:\/\/stackoverflow.com\/a\/2891805\nnp.printoptions(precision=3, suppress=True)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nDataFilePath = ''\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        filepth = os.path.join(dirname, filename)\n        print(filepth)\n        if ('data' in filename) and ('csv' in filename):\n            DataFilePath = filepth\n            \nif len(DataFilePath) < 1:\n    print('Oh no! We have no data file! :(')\nelse:\n    print('Data File:\\n', DataFilePath)\n\n# Any results you write to the current directory are saved as output.","984e7767":"CandyDataSetDF = pd.read_csv(DataFilePath)\nCandyDataSetDF.head()","e0ab5ffc":"CandyDataSetDF.describe()","d1069f80":"# Define a function that will provide labels for the thirs of data points\n\ndef get_third_label(value, borders):\n    \"\"\"This function expects a value (value:float) that it can compare against \n    two other values given (borders:list of two numbers).\n    It returnds a string containing either min, med or max, depending on in which\n    third the given value lies.\"\"\"  \n    \n    third_label = ''\n    \n    if len(borders) > 2:\n        #print('No more than two values!')\n        raise ValueError('get_third_label: No more than two border values!')\n    \n    if value <= borders[0]:\n        third_label = 'min'\n    if (value > borders[0]) and (value <= borders[1]):\n        third_label = 'med'\n    if value > borders[1]:\n        third_label = 'max'\n    \n    if third_label == '':\n        raise RuntimeError('get_third_label: Could not assign a label!')\n        \n    return(third_label)\n","0dad24f3":"SPDFboundaries = CandyDataSetDF[['sugarpercent', 'pricepercent']].describe(percentiles = [.33, .66])\n\nprint('### Boundaries for Sugar')\nprint(SPDFboundaries['sugarpercent']['min'], SPDFboundaries['sugarpercent']['33%'], '- 1\/3 for Sugar') \nprint(SPDFboundaries['sugarpercent']['33%'], SPDFboundaries['sugarpercent']['66%'], '- 2\/3 for Sugar') \nprint(SPDFboundaries['sugarpercent']['66%'], SPDFboundaries['sugarpercent']['max'], '- 3\/3 for Sugar') \nprint('### Boundaries for Price')\nprint(SPDFboundaries['pricepercent']['min'], SPDFboundaries['pricepercent']['33%'], '- 1\/3 for Price') \nprint(SPDFboundaries['pricepercent']['33%'], SPDFboundaries['pricepercent']['66%'], '- 2\/3 for Price') \nprint(SPDFboundaries['pricepercent']['66%'], SPDFboundaries['pricepercent']['max'], '- 3\/3 for Price') \n\n\ntmppricelabels = list()\nfor idx, row in CandyDataSetDF.iterrows():\n    tmppricelabels.append(get_third_label(\n        row['sugarpercent'], [SPDFboundaries['sugarpercent']['33%'],SPDFboundaries['sugarpercent']['66%']]))\n\nCandyDataSetDF['sugarlabel'] = tmppricelabels \n\ntmpsugarlabels = list()\nfor idx, row in CandyDataSetDF.iterrows():\n    tmpsugarlabels.append(get_third_label(\n        row['pricepercent'], [SPDFboundaries['pricepercent']['33%'],SPDFboundaries['pricepercent']['66%']]))\n\nCandyDataSetDF['pricelabel'] = tmpsugarlabels    \n\nCandyDataSetDF.head()","725ef1e5":"sns.distplot(CandyDataSetDF['winpercent'], bins=20, color = 'blue')","b7f06755":"scp.stats.kstest(CandyDataSetDF['winpercent'],'norm')","45d373de":"sns.distplot(CandyDataSetDF['sugarpercent'], bins=20, color = 'red')","abfad0bb":"scp.stats.kstest(CandyDataSetDF['sugarpercent'],'norm')","700d3ecb":"sns.distplot(CandyDataSetDF['pricepercent'], bins=20, color = 'green')","73169289":"scp.stats.kstest(CandyDataSetDF['pricepercent'],'norm')","919dac5c":"# Thx2: https:\/\/becominghuman.ai\/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f\n# Thx2: https:\/\/becominghuman.ai\/@awhan.mohanty\n\n# What is the difference between flatten and ravel functions in numpy?\n# Thx2: https:\/\/stackoverflow.com\/a\/28930580\/12171415\n\n# How to use Data Scaling Improve Deep Learning Model Stability and Performance\n# Thx2: https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nSVRGBXscaler = StandardScaler()\nSVRGBYscaler = StandardScaler()\n\nXindy = np.array(CandyDataSetDF.iloc[:,1:10])\nYdept = np.array(CandyDataSetDF['winpercent'])\n\n# The data appears to be centered and scaled properly, however not good enough for MLPR and SVM, need a standard distibution for best results\nX4SVGB = SVRGBXscaler.fit_transform(Xindy)\nY4SVGB = SVRGBYscaler.fit_transform(Ydept.reshape(-1, 1)).flatten()\n","35f431f1":"CandyDataSetDF.head()","a8d3c58b":"# Expand the DataFrame with categorial variables for price and sugar\n# Thx2: https:\/\/kaijento.github.io\/2017\/04\/22\/pandas-create-new-column-sum\/\n# Thx2: https:\/\/thispointer.com\/python-how-to-use-if-else-elif-in-lambda-functions\/\n\n\n\nMLPR_Xbinscaler = MinMaxScaler(feature_range=(0.000001,0.999999)) #for ReLu activation function\nMLPR_Ybinscaler = MinMaxScaler(feature_range=(0.000001,0.999999))\nMLPR_XOHscaler = OneHotEncoder(categories='auto')\nMLPR_yscaler = MinMaxScaler(feature_range=(0.01,0.99))\n\n# Furthermore Neural Nets like to have a test and training set\nXtr, Xte, Ytr, Yte = train_test_split(Xindy, Ydept, test_size = 20) # Twenty samples in the test set to control outliers\n\nXOHlabels = MLPR_XOHscaler.fit_transform(CandyDataSetDF[['sugarlabel', 'pricelabel']])\nXOHtr = np.hstack([Xtr,XOHlabels.toarray()[0:len(Xtr),:]])\nXOHte = np.hstack([Xte,XOHlabels.toarray()[0:(len(Xindy)-len(Xtr)),:]])\n\nMLPR_Xbinscaler.fit(XOHtr)\nXNNTrain = MLPR_Xbinscaler.transform(XOHtr)\nXNNTest = MLPR_Xbinscaler.transform(XOHte)\n\nMLPR_Ybinscaler.fit(Ytr.reshape(-1, 1))\nYNNTrain = MLPR_Ybinscaler.transform(Ytr.reshape(-1, 1)).flatten()\nYNNTest = MLPR_Ybinscaler.transform(Yte.reshape(-1, 1)).flatten()","de9f21bc":"# Create a view of only the ratio scaled columns\nCandyRatioVar = CandyDataSetDF[['sugarpercent', 'pricepercent', 'winpercent']]\n\nCandyRatioCorr = CandyRatioVar.corr(method = 'pearson')\n\nsns.heatmap(CandyRatioCorr, vmin=0, vmax=1)\nCandyRatioCorr","bc2320d5":"from sklearn.neighbors import LocalOutlierFactor\n\nmylof = LocalOutlierFactor(contamination = 0.1) #Only necessary to supress the version 0.22 warning message\nmylof.fit(np.array(CandyDataSetDF.iloc[:,1:12])) # Resembles Xdept\nsns.lineplot(data = mylof.negative_outlier_factor_)\nmylof.fit(np.array(CandyDataSetDF.iloc[:,1:13])) # Resembles all numerical values in a line\nsns.lineplot(data = mylof.negative_outlier_factor_)","b93b6b33":"NNlof = LocalOutlierFactor(contamination = 0.1, n_neighbors = 8)\nNNlof.fit(XNNTrain) # Training values \nsns.lineplot(data = NNlof.negative_outlier_factor_)","ea154855":"NNlof.fit(XNNTest) # Testing values \nsns.lineplot(data = NNlof.negative_outlier_factor_)","37100f9e":"myregr = linear_model.LinearRegression()\nmyregr.fit(Xindy, Ydept)","b32c7cf5":"sns.relplot(data = pd.DataFrame(data = myregr.coef_, index = CandyDataSetDF.iloc[:,1:10].columns ), \n            kind = \"line\", legend = False, aspect = 3)","6c510e04":"# Thx2: https:\/\/stackoverflow.com\/questions\/36306555\/scikit-learn-grid-search-with-svm-regression\n# Python2 code and for sklearn.svm.SVC, so some changes had to be made to make this work\n# for Python3 and sklearn.svm.SVR\n\n# About the parameter C (penalty parameter)\n# Thx2: https:\/\/stats.stackexchange.com\/a\/159051\n\nc = list(np.linspace(0.0000001, 10, 100, endpoint = True))\nparam_gridSVR = dict(C = c)\n","3a4829c2":"mysvregr = SVR(max_iter = -1, kernel = 'linear', gamma = 'auto')\n\ngridsvr = GridSearchCV(mysvregr, param_gridSVR, cv = 5)\ngridsvr.fit(X4SVGB, Y4SVGB) \nprint('AND THE WINNER IS:\\n', gridsvr.best_params_)\ngridsvrDF = pd.DataFrame(gridsvr.cv_results_)","4ecddb3b":"myplt = sns.relplot(data = pd.DataFrame(data = gridsvr.best_estimator_.coef_.reshape(9), index = CandyDataSetDF.iloc[:,1:10].columns), \n                    kind = \"line\", legend = False, aspect = 3)","0ab9e1a0":"# Thx2: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n# Worked:\n# mygradboostregr = ensemble.GradientBoostingRegressor(n_estimators = 500, max_depth = 4, min_samples_split = 2, learning_rate = 0.01, loss = 'ls')\n\n# Tune parameters for small datasets\nmygradboostregr = ensemble.GradientBoostingRegressor(max_depth = 2)  # minimal max_depth for keeping overfitting low\nmygradboostregr.fit(X4SVGB, Y4SVGB)","366fc5f4":"sns.relplot(data = pd.DataFrame(data = mygradboostregr.feature_importances_, index = CandyDataSetDF.iloc[:,1:10].columns),\n            kind = \"line\", legend = False, aspect = 3)","93e9dd6d":"# Define the parameters that stay constant and are not default.\nparam_fixMLPR = dict(\n    #alpha =\n    early_stopping = False,\n    nesterovs_momentum = False,\n    learning_rate = 'constant',\n    #solver = 'sgd',\n    activation = 'relu',\n    learning_rate_init = 0.001,\n    momentum = 0.9,\n    max_iter = 100000,\n    hidden_layer_sizes = (200,),\n    verbose = False\n)\n\n# Prepare the parameters for grid search \n\nparam_gridMLPR = dict(\n    #activation = ['identity', 'logistic', 'tanh', 'relu'],\n    #momentum = [0.25, 0.5, 0.9],\n    #learning_rate_init = [0.001, 0.0001],\n    #learning_rate = ['adaptive', 'constant'],\n    #nesterovs_momentum = [True, False],\n    #early_stopping = [True, False],\n    solver = ['adam', 'sgd'],\n    #hidden_layer_sizes = [(20,), (50,), (100,), (200,), (100, 50), (100, 50, 50), (100, 100, 50, 50)], #, (500,)    \n    alpha = [0.1, 1, 10],    # Better higher penality for small sample sizes\n)","58f1c8ed":"myMLPR = MLPRegressor(**param_fixMLPR)\n\ngridMLPR = GridSearchCV(myMLPR, param_gridMLPR, verbose = 2, cv = 5) # Five folds\ngridMLPR.fit(XNNTrain, YNNTrain) \nprint('AND THE WINNER IS:\\n', gridMLPR.best_params_)\ngridMLPRDF = pd.DataFrame(gridMLPR.cv_results_)\n\nmyMLPR = gridMLPR.best_estimator_","51290fcf":"myOLDerrs = np.absolute(YNNTrain) - np.absolute(myMLPR.predict(XNNTrain)) # makes little sense but kept for backward comparability\nmyNEWerrs = np.absolute(YNNTrain - myMLPR.predict(XNNTrain)) \nprint(\"OLD Home Brew Error Estimator\", np.mean(myOLDerrs))\nprint(\"NEW Home Brew Error Estimator\", np.mean(myNEWerrs))\n# sns.relplot(data = pd.DataFrame(data = myNEWerrs, index = CandyDataSet['competitorname']),\n#            kind = \"line\", legend = False, aspect = 3)\n\n# Get the error score against the test set\nmyMLPR.score(XNNTest, YNNTest)","fbdb4ed3":"MLPRparams = myMLPR.get_params()\nif MLPRparams['solver'] in ['sgd', 'adam']:\n    MyDFLoss = pd.DataFrame(data = myMLPR.loss_curve_, columns=['Error'])\n    sns.relplot(data = pd.DataFrame(data = myMLPR.loss_curve_, columns=['Error']),\n                kind = \"line\", legend = False, aspect = 3)","c99c4b58":"# Thx2: https:\/\/docs.python.org\/3\/library\/functions.html#bin\n\nmyinputarr = list()\nCandyNames = list()\nFoundNames = list()\nFoundCandy = False\n\nXLabelIndy = np.vstack((XOHtr,XOHte))\n\n# Calculate the number of possible binary combinations and desired permutations for price and sugar content.\n# 100 sugar and 100 price equals 100^2 combinations\nNumBinPermutations = pow(2,9) * pow(3,2) # 6 new dimensions since one hot encoding for price (3) and sugar (3)\n#NumSPPermutations = pow(100,2)\n#NumTotPermutations = NumBinPermutations * NumSPPermutations\n\nmyNPinputBin = np.empty(shape=(NumBinPermutations, (9+6)))\nmyOneHotStrings = ['100', '010', '001']\nmyOHcntS = 0\nmyOHcntP = 0\nbincnt = 0\n\nfor mynum in range(NumBinPermutations):\n    mybinarystr = \"{:09b}\".format(bincnt) #, fill=0, witdth=20')\n    bincnt += 1\n    if  bincnt >= pow(2,9):\n         bincnt = 0\n    mybinarystr += (myOneHotStrings[myOHcntS] + myOneHotStrings[myOHcntP])\n    #print(mybinarystr)\n    myOHcntS += 1\n    if myOHcntS > 2: # Permutate the one hot combinations\n        myOHcntS = 0\n        myOHcntP += 1\n        if myOHcntP > 2:\n            myOHcntP = 0\n    for mypos in range(len(mybinarystr)):\n        myinputarr.append(int(mybinarystr[mypos]))\n    #print(mynum, len(mybinarystr), mybinarystr, myinputarr)\n    for (x,testarr) in enumerate(XLabelIndy):\n        if myinputarr == list(testarr): #Before: [0:9]\n            #print(CandyDataSetDF.iloc[x,0])\n            #print(mybinarystr)\n            FoundNames.append(CandyDataSetDF.iloc[x,0])\n            FoundCandy = True\n    if FoundCandy == True:\n        CandyNames.append(FoundNames)\n        #print(FoundNames)\n        FoundNames = []\n        FoundCandy = False\n    else: \n        CandyNames.append(mybinarystr)\n        #print(mybinarystr)\n    \n    myNPinputBin[mynum] = myinputarr\n    \n    #Keep variables tidy\n    myinputarr = []\n    mybinarystr = ''\n    \nprint('Processed {} binary permutations, generating an {} numpy array.'.format(mynum+1, np.shape(myNPinputBin)))","ae4a37da":"res = myMLPR.predict(MLPR_Xbinscaler.transform(myNPinputBin)) #fit_transform(myNPinputTot)\nnetmax = np.amax(res)\nidx = np.argmax(res)\nprint('The winning combination is:')\nprint(myNPinputBin[idx], 'with the index {} and the value {}'.format(idx, MLPR_Ybinscaler.inverse_transform(netmax.reshape(-1, 1))))","8b182601":"# #################################\n# Now create a nice DataFrame with all possible permutations of the binary columns\n# Sort with index also sorted:\n# Thx2: https:\/\/stackoverflow.com\/a\/52720936\n\nMLPPredictDF = pd.DataFrame(data = myNPinputBin[:,0:9], columns = CandyDataSetDF.iloc[:,1:10].columns)\n\n# Transform the one hot fields back to DataFrame Format\n\nMLPPredictDF['sugarlabel'] = MLPR_XOHscaler.inverse_transform(myNPinputBin[:,9:16])[:,0]\n    \nMLPPredictDF['pricelabel'] = MLPR_XOHscaler.inverse_transform(myNPinputBin[:,9:16])[:,1]\n\nMLPPredictDF['prediction'] = myMLPR.predict(MLPR_Xbinscaler.transform(myNPinputBin)) # yes, calculating a second time is not elegant but... ah well...\n\nMLPPredictDF['PredNames'] = CandyNames \n\nMLPPredictDFsort = MLPPredictDF.sort_values(by = 'prediction', ascending = False)\n\nMLPPredictDFsort.reset_index(drop=True, inplace=True)\n\nsns.lineplot(x = MLPPredictDFsort.index, y = MLPPredictDFsort['prediction'], sort = True)\n","810c8762":"MLPPredictDFsort.head()","e804b7d2":"from time import gmtime, strftime\n\ncurgmtstr = strftime(\"%Y%m%d%H%M%S\", gmtime())\n\nMLPPredictDFsort.to_excel(\"prediction_\" + curgmtstr + \".xlsx\")\n\n# Exporting the whole myNPinputTot array via a DataFrame in an EXCEL caused:\n# ValueError: This sheet is too large! Your sheet size is: 5120000, 12 Max sheet size is: 1048576, 16384\n# WORKAROUND: Save to CSV:\n# MLPPredictDFsort.to_csv(\"prediction_\" + curgmtstr + \".csv\")\n\ngridsvrDF.to_excel(\"SVRegSearch_\" + curgmtstr + \".xlsx\")\ngridMLPRDF.to_excel(\"MLPRSearch_\" + curgmtstr + \".xlsx\")\n#np.savetxt('NP_buffer_' + curgmtstr + '.csv',buffer) ###################### DEBUG\n\nMLPRparams\n\n","319f6e9b":"##################### DEBUG\n# For exporting files directly from the kernel\n# Thx2: https:\/\/www.kaggle.com\/general\/65351#600457\n\n#import os\n#os.chdir(r'\/kaggle\/working')","aee23cca":"curgmtstr","6781f513":"!ls -l","b67dac03":"##################### DEBUG\n#from IPython.display import FileLink\n#FileLink(r'XXXXXXXX.xlsx')","c2a386f7":"___\n\n# Training a MLP regression now\n\nBasically the results of the various regression analysis show us that we agree to disagree with only one exception: **chocolate IS important**!\n\nSince we have so different results this indicates that something more complex is hidden in this data. When it is coming to complex relationships between data points there is a gold standard: **artificial neural networks**!\n\nHowever, this comes with a price. We don't get convenient weight indicies for the different ingredients, their relationship is more complex. So this leaves us only with the choice of providing the trained network examples for interference. Thus getting a prediciton of the estimated success according to the trained model.\n\nThe parameters I determined by hand were:\n```Python\nMLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n             hidden_layer_sizes=(100, 100, 50, 50), learning_rate='adaptive',\n             learning_rate_init=0.0001, max_iter=2000, momentum=0.9,\n             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n             random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n             validation_fraction=0.1, verbose=False, warm_start=False)\n```\nor simpler\n```Python\nmyMLPR = MLPRegressor(solver='sgd', learning_rate='adaptive', learning_rate_init=0.0001,\n                      hidden_layer_sizes = (100, 100, 50, 50), \n                      activation= 'relu', max_iter=2000, verbose=False)\n```\n---\n\n## Now let's fine out what grid search can do here...\n\nThe parameters were determined after several grid search runs.\n\nSo below is only the final competition of some winning values against my initial 'gut feeling' setup\n\n* 2019-10-04: Left out _'tanh'_ activation, only eats computing time, never won\n* 2019-10-05: invscaling takes too long and never won.\n* 2019-10-06: Back to the roots and go from the initial configuration into grid search.\n* v29: With new transformations for MLPR it makes sense to include the acivation functions again in the grid search.","eeaa3927":"# Ensemble learning with gradient boosting","1111fb7c":"# Check for outliers\nDuring various test runs with hyperparameters that seemed to work good, I got sometimes a good model fit, then again negative scores. I suspect that there are outliers in the dataset.","f524c668":"# Arrange the data in numpy arrays","b98305b4":"___\n# Run the model\n","ba94df58":"---\n# Feed all combinations of ingredients in the MLP model\nEssentially we can produce all possible combinations by 9 bit encoding, because of nine binary attributes\nSo, we build a function that will genereate a numpy array with all possible 9 bit combinations that we can \nthen feed into the MLP network for approximation to find the best candidate.\n\n### [v23 Include pricing and sugar content](https:\/\/www.kaggle.com\/docacs\/the-halloween-frankencandy-generator?scriptVersionId=21561735#Run-sugar-and-price-through-the-model)\nFurthermore I decided that it makes sense to feed in sugar content and pricing data in the model. This leaves us with the problem of what data to give as input. Simple! Since everything is convientiently scaled in the 0 to 100% range we simply that all possible combinations between 1 and 100. \n\nThis leaves us with 2^9 possibillites from the binary permutations and 100^2 permuations from 100 variations for sugar content and 100 variatons for pricing.\n\nThat are **2^9 * 100^2** combinations, resulting in **5,120,000** permutations that are fed as input.\n\n### [v34 Changed to one hot encoding, complete refactoring...](https:\/\/www.kaggle.com\/docacs\/the-halloween-frankencandy-generator?scriptVersionId=21832995#Arrange-the-data-in-numpy-arrays)","ba9e8f13":"___\nSave some stuff in file formats known to consultants","80e43c9b":"# Having a look at the standard correlations","f98d229b":"# Logbook\n* v22: The last cells are now work in progress, trying to get the permuations for sugar content and price work. **No saving of files currently!** because the Numpy array is not filtered yet and thus over five million lines long! EXCEL can only handle under two million lines.\n* v23: Something seems to go terribly wrong. But at least the code is running without errors. Good news is that sugar and price are now in the prediction. Bad news is that it does not seem to work because for all permutations the winning price and the winning sugar content is always 0.99\n* v24: All is working now, however the results of the MLP regression are not plausible. Added some debug code.\n* v25: The notebook is still a big mess, nevertheless **the MLPR model produces plausible results now!!!** The integration of sugar and price in the model was successful but harder than thought. My eternal gratitude goes to *Awhan Mohanty* who wrote an enlighting blog post about how to work with MLPR models in modern times: <https:\/\/becominghuman.ai\/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f>\n* v26: Cancelled by accident.\n* v27: Finally the notebook works technically. However the results don't make much sense in many of runs. Have changed the configuration of hidden layers, see below.\n* v28: Corrected some small but mean errors.\n* v29: Using `MLPRscaler = MinMaxScaler(feature_range=(-0.99,0.99))` for scaling the data for the Multi Layer Perceptron Regression seems more sensible than `MLPRscaler = StandardScaler()`. Furthermore I should have rememebered to transform the `Ydept = np.array(CandyDataSet['winpercent'])` values too to fit the range betwen -1 to 1. Neural Networks still work pretty much like in the 90's, even with Python and sklearn...\n* v30: Two important lessons learned: **A) Fewer hidden layers (1!) in MLPR with high L2 penality (alpha) seem to work better** and the **seemingly random MLPR results can be explained by outliers.** Every time training and test sets are selected other data points get into the training set. Thus making the result unstable depending on how many outliers get in the training or test set.\n* v31: Basically confirmed that the estimations are very dependent on the starting configuration, like random picked training data and weight initialization for MLPR. Added one outlier analysis too.\n* v32: Toyed a around with developing an assessement of the ranking quality by rank corellation. Tough nut is still that binary patters can represent multiple candy brands around the whole rating spectrum. ### Did an assessement of four exemplary candy rankings, the retest reliability is not too bad, seems right when just looking at 1\/3 chunks of the spectrum. ### Toyed a bit around with scaling, because the extreme values for sugar and pricing prediction still concern me. \n* v33: Going for one hot encoding for sugar and price. Reason: It will solve the mapping issues. \n    - a) Quite much products share the same binary combination. \n    - b) Hopefully the predictions for sugar and pricing will show more variations than now. \n    - c) Encoding will be done for [low, middle, high], leading to three blocks for price and sugar content respectively. \n    - d) Suspicion about the mapping issues: The optimiazation process optimizes for extreme values because the majority of features is constituted of extremes [0,1] \n* v34: One hot encoding finally implemented! :) The notebook runs now without errors. Since the output in the DataFrame and EXCEL file still just contains the raw binary values it is hard to say if something really improved. Lablels of the plots have to be fixed too. \n* v35: General maintenance to make the notebook run smoothly under the new 'one hot encoding' paradigm.\n    - a) Integrated the categorial labels in the EXCEL output.\n    - b) Fixed the incorrect output of product names in EXCEL output.\n        - Todo: The assignment of names is still based on the pure binary combinations. Implement a mapping to the sugar and price one hot categories.\n* v36: Minor but important fix for the names, see v35 b) 'Todo'\n    - a) FIXED: Implement a correct mapping of the product names ingesting the sugar and price one hot categories.\n* v37: Run an extensive grid search\n* v38: Cancelled, apperently out of memory?! (Currently the same notebook running in draft, grid search since over 2:30 hours and 342 MB of 16 GB...)\n* v39: Simply give it a 2nd try... **Also failed.** Kernel was killed after running longer than 9 hours. Workaround: I have saved this notebook and uploaded it to another (paid) cloud computer. Such complex estimations will run there.\n    - Todo: Export winning model parameters and relevant metrics to a file.\n* v40: Had some large grid search runs on the Google Cloud platform in the last days. It is hard to find really good winning combinations beacause the dependency on the test set for R\u00b2 calculation. Hence there are many outliers it always depends on the random draw if a good representative sample will be drawn for the test set. I don't want to hand pick them because this will be way too biased. Two things seems to help, thanks to [Soumen Pramanik](https:\/\/www.kaggle.com\/soumenpramanik) for these:\n    - Make a fixed sample rate of 20 test samples to have a more representative testing set than the previous 10%, which results in only 8 samples.\n    - Widen the grid search to a five fold search to take more samples into account.\n* v41: Decided to stop working on this notebook, after I run a notebook that calculates the rank correlations of all output files genererated, it turned out that the old models were the best and notebooks from around v20 only produces worse results. So back to the drawing board! I forked Version 17 of this notebook to a new one called [The Ultimate Halloween Frankencandy Generator](https:\/\/www.kaggle.com\/docacs\/the-ultimate-halloween-frankencandy-generator). Will take the good parts of this project and successively integrate them into the fork. From now on here I will basically only doing some cosmetics to improve the readability of this notebook.\n* v42: Basically just some cosmetics\n    ","398bd6e1":"# Introduction\nThis notebook is my first attempt to tackle the [Ultimate Halloween Candy Power Ranking](https:\/\/fivethirtyeight.com\/features\/the-ultimate-halloween-candy-power-ranking\/) challenge. Well, not an official Kaggle challenge, more of a personal one. What would you do if you have to recommend ideal Halloween candy creations to a product development team? The approach in the article in [FiveThirtyEight](https:\/\/fivethirtyeight.com\/) was to use linear regression models. I disagree with this approach. As I showed below, various linear regression models come to quite different conclusions. I am convinced that this problem can be tackled by [Artificial Neural Networks](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network), which are able to address the complex non linear interactions. This kernel is a predecessor to the [The Ultimate Halloween Frankencandy Generator](https:\/\/www.kaggle.com\/docacs\/the-ultimate-halloween-frankencandy-generator), which I will make public on the day of All Hallows' Eve 2019. \\*SCNR\\*","6efbc916":"# Support Vector Regression\n* Fitting a bit more exciting Support Vector Regression.\n* Finding best parameters via grid search. \n\nThe optimal value for C was 2 from a `np.linspace(1,100,100)` run, so let's get a bit more granular here.","b25d7b80":"___\n* v25a: ` {'activation': 'relu', 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'momentum': 0.5, 'nesterovs_momentum': False, 'solver': 'sgd'}` Remark: `early_stopping` was set `True`\n* v25b: `{'activation': 'relu', 'early_stopping': True, 'learning_rate': 'constant', 'learning_rate_init': 0.0001, 'momentum': 0.2, 'nesterovs_momentum': True, 'solver': 'sgd'}`\n\nSeems like Grid Search is always finding slightly different solutions. Either it is due to overfitting and random selection of training set or the hidden layers are not big enough to encompass the complexity. Following now Geoffrey Hinton's advice to think deep *SCNR* and switching the hidden layer size from currently `(50, 11, 50, 11)` to `(50, 100, 100, 100, 100, 50)` which is the biggest so far.\n\nDidn't work out that well. Always negative error score.\n\n* v28a: `{'alpha': 10, 'learning_rate_init': 0.0001, 'solver': 'sgd'}` - Scaled back the number of hidden layers to the original configuration of `(100, 100, 50, 50)`, now at least **0.28401955712609395** model error score. **Alpha is quite high** and seems to indicate that we have a problem with overfitting.\n* v29a: `{'alpha': 10, 'learning_rate_init': 0.0001, 'solver': 'adam'}` for `hidden_layer_sizes = (33, 16, 8)` leads to **score = 0.28304160519242905**\n* v29b: After transforming everything (X,Y) for the MLPR, which I should have done from the beginning (btw.), we are now at ` {'activation': 'relu', 'alpha': 1, 'learning_rate_init': 0.0001, 'solver': 'adam'}` with a **score = 0.1969986587464747**\n* v30a: The grid search seems to move towards high alpha values, prevents overfitting, and just one layer `{'alpha': 10, 'hidden_layer_sizes': 50, 'solver': 'adam'}` the choice was here `hidden_layer_sizes = [(11), (50), (50,11), (50,10,5)]` **score = 0.06063702646125613**\n* v30b: `{'alpha': 1, 'hidden_layer_sizes': (50,)}` **score = 0.18784317971924147**\n* v31a: `{'alpha': 0.1, 'hidden_layer_sizes': (200,)}` **score = 0.5359156939969579**\n* v31b: `{'alpha': 0.1, 'hidden_layer_sizes': (50,)}` **score = 0.31754689413737824**\n* v31c: `{'alpha': 1, 'hidden_layer_sizes': (200,)}` **score = 0.30148634242724603**\n* v31d: `{'alpha': 1, 'hidden_layer_sizes': (100,)}` **score = 0.5531706684269215**\n* v31e: `{'alpha': 0.1, 'hidden_layer_sizes': (500,)}` **score = 0.2967485893676107** --> MEMORY ERROR, no data :(\n\n#### Consequence of above memory error is now to leave the hidden layer size of 500 out\n* v32a: `{'alpha': 1, 'hidden_layer_sizes': (100,)}` **score = 0.4329253400103321**\n* v32b: `{'alpha': 10, 'hidden_layer_sizes': (20,)}` **score = 0.4415032631493838**\n* v32c: `{'alpha': 0.1, 'hidden_layer_sizes': (200,)}` **score = 0.3580575500953781** After rescaling to `MLPRscaler = MinMaxScaler(feature_range=(0.000001,0.999999)) #for ReLu activation function`\n\n#### Switched now to one hot encoding for price and sugar with three levels: min, med, max\n* v34a: `{'alpha': 1, 'hidden_layer_sizes': (100,)}` **score = 0.5687749663324637**\n* v34b: `{'alpha': 0.1, 'hidden_layer_sizes': (200,)}` **score = 0.4517328576112521**\n* v35a: `{'alpha': 1, 'hidden_layer_sizes': (200,)}` **score = 0.23603677162202819**\n* v36a: `{'alpha': 1, 'hidden_layer_sizes': (100,)}` **score = 0.49201928580048937**\n\n### Run extensive model comparisons via grid search\n* v37a: `{'alpha': 1, 'early_stopping': True, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'momentum': 0.5, 'nesterovs_momentum': True, 'solver': 'adam'}` **score = 0.672827549652748**\n\n**score = XXXXX**","f6e24b38":"# Distributions and test against normality assumption\nTesting if the continous variables follow a standard distribution and make some visualizations to explore the data","96c97208":"# Create sorted DataFrame with Results ","b8739424":"# Linear Regression\nFitting a boring and uneventful regression model"}}