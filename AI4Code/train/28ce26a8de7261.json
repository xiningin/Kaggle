{"cell_type":{"dae2d87e":"code","54b9f50c":"code","079a2158":"code","3152536f":"code","563d9dd9":"code","43db8a4d":"code","dc990d2c":"code","893d7bab":"code","f6346954":"code","2c5e5aaf":"code","2896fd2f":"code","d155daf3":"code","83cc269d":"code","a3b7cf92":"markdown","3ffcfda4":"markdown","6f4a2972":"markdown","cc6d3d21":"markdown","e050e016":"markdown","35d12bf7":"markdown","6a12a240":"markdown","9d2ab9c9":"markdown","9e804873":"markdown","1d21ee4b":"markdown","de6549b4":"markdown","d4152aec":"markdown","2c2f0a20":"markdown"},"source":{"dae2d87e":"import pandas as pd\ndata = pd.read_csv('..\/input\/turkish cyberbullying.csv')\ndata.head()","54b9f50c":"from nltk.corpus import stopwords\nimport re","079a2158":"# storing stopwords of Turkish (set structure for speed)\nstops = set(stopwords.words('turkish'))\nprint(stops)","3152536f":"# pattern string in order to exclude non-Turkish-letter characters \n# such as punctuations and numbers\nexc_letters_pattern = '[^a-z\u00e7\u011f\u0131\u015f\u00f6\u00fc]'","563d9dd9":"def text_to_wordlist(text, remove_stopwords=False, return_list=False):\n    # 1. convert to lower\n    text = text.lower()\n    # 2. replace special letters\n    special_letters = {'\u00ee':'i', '\u00e2': 'a'}\n    for sp_let, tr_let in special_letters.items():\n        text = re.sub(sp_let, tr_let, text)\n    # 3. remove non-letters\n    text = re.sub(exc_letters_pattern, ' ', text)\n    # 4. split\n    wordlist = text.split()\n    # 5. remove stopwords\n    if remove_stopwords:\n        wordlist = [w for w in wordlist if w not in stops]\n    # 6.\n    if return_list:\n        return wordlist\n    else:\n        return ' '.join(wordlist)","43db8a4d":"clean_messages = []\nfor message in data['message']:\n    clean_messages.append(text_to_wordlist(\n        message, remove_stopwords=True, return_list=False))","dc990d2c":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    clean_messages, data['cyberbullying'], test_size=0.33, random_state=1)","893d7bab":"from sklearn.feature_extraction.text import CountVectorizer\n\n# limit vocabulary size as at most 5000; thus, words with \n# the least frequency are not included in the vocabulary\nvectorizer = CountVectorizer(analyzer=\"word\",\n                             tokenizer=None,\n                             preprocessor=None,\n                             stop_words=None,\n                             max_features=5000)\n\n# since we assume that we are not familiar with test set in advance,\n# we use our training set in the construction of the vocabulary \ntrain_data_features = vectorizer.fit_transform(x_train)\n\n# convert it to numpy array since it is easier to work with\ntrain_data_features = train_data_features.toarray()","f6346954":"from sklearn.ensemble import RandomForestClassifier\n\n# training Random Forest\nforest = RandomForestClassifier(n_estimators=100)\n_ = forest.fit(train_data_features, y_train)","2c5e5aaf":"# converting test data to BOW features\ntest_data_features = vectorizer.transform(x_test)\ntest_data_features = test_data_features.toarray()","2896fd2f":"y_pred = forest.predict(test_data_features)\n\npredictions = pd.DataFrame(\n    data={\"message\": x_test, \"cyberbullying_true\": y_test, \"cyberbullying_pred\": y_pred})","d155daf3":"# correct_count = sum(y_pred == y_test)\ncorrect_count = (predictions[\"cyberbullying_pred\"] == predictions[\"cyberbullying_true\"]).sum()\nprint(\"Accuracy is %{:.3f}\".format(100 * correct_count \/ len(y_test)))\n\nfrom sklearn.metrics import confusion_matrix\ncf = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix')\nprint('\\tPredictions')\nprint('\\t{:>5}\\t{:>5}'.format(0,1))\nfor row_id, real_row in enumerate(cf):\n    print('{}\\t{:>5}\\t{:>5}'.format(row_id, real_row[0], real_row[1]))","83cc269d":"wrong_predictions = predictions[predictions[\"cyberbullying_pred\"] != predictions[\"cyberbullying_true\"]]\n\nwrong_predictions.to_csv(\"wrong_predictions.csv\", index=False, quoting=3)\n# wrong_predictions.head()\nwrong_predictions.iloc[10:15]\n\n","a3b7cf92":"## Model Evaluation\nWe have trained our model; so, we can evaluate its performance on the test set. At first, we should convert test data to BOW features. ","3ffcfda4":"## Feature Extraction - Bag of Words\nAs features, we use Bag Of Words (BOW), where a document is represented with counts of words from a predefined vocabulary. Here, our vocabulary is the set of all words within the training data. Thus, we can traverse all documents in the training data, figure out set of words and store them as our vocabulary. Then we can represent each document with the counts of words from that vocabulary. Fortunately, *scikit-learn* has already a BOW implementation, *CountVectorizer*.","6f4a2972":"# Text Cleaning\nFirst, it is better to process text of messages. For text processing, let's import the following modules:\n* **nltk** (Natural Langauge ToolKit), which is the well-known NLP (natural language processing) module for Python. We will use it to remove *stopwords*, which are generally common words in that language that do not contribute to the meaning much. For example,  'gibi' and 'de' are stopwords of Turkish. \n* **re**, the built-in regular expression library.\n\nYou can install nltk via pip with the following commands:\n> pip install nltk","cc6d3d21":"The test data is ready. Therefore, our model can predict whether they contain cyberbullying or not. We can examine its accuracy and its confusion matrix.","e050e016":"Let's examine wrong predictions. ","35d12bf7":"# Classification\n## Data Split\nTo train our model and test its performance, we need to split our dataset. we can use *scikit-learn* module here.\nYou can install *sklearn* with the following *pip* command or you can check [its documentation](http:\/\/scikit-learn.org\/stable\/install.html):\n>pip install -U scikit-learn","6a12a240":"Let's clean messages.","9d2ab9c9":"We will use *nltk* to remove *stopwords*, which are generally common words in a language that do not contribute to the meaning much. For example,  'gibi' and 'de' are stopwords of Turkish. You can see the stopwords of Turkish, provided by *nltk*.","9e804873":"As you can see on the examples above, the data has 2 columns, *message* and *cyberbullying*. The former one is the message itself while the latter one is a binary integer representing whether a message contains cyberbullying or not.","1d21ee4b":"We process the text with the following operations:\n1. First, text is converted to lowercase. Thus, words with uppercase are not considered different words from their lowercase equivalents.\n2. Replace special characters with their corresponding Turkish characters.\n3. Remove non-Turkish-letter characters such as digits or punctuation marks.\n4. Split text into a list of words\n5. Remove Turkish stopwords (optional)\n6. Return as a list or as one string (depends the parameter)\n\nLet's define it as a function.","de6549b4":"## Model Training\nOur training data is ready; so, we can train our model. Here I have used a Random Forest classifier as our model, which is available in *scikit-learn*. ","d4152aec":"I have prepared this kernel on detection of cyberbullying in Turkish language with machine learning. Here it is approached as a binary classification problem. I have employed [this dataset](https:\/\/www.kaggle.com\/abozyigit\/turkish-cyberbullying). \nThe [dataset](https:\/\/www.kaggle.com\/abozyigit\/turkish-cyberbullying) contains some online Turkish messages and whether they contain or not any cyberbullying purpose. Let's start with examining few instances from our data.","2c2f0a20":"Let's define other necessary objects to clean words."}}