{"cell_type":{"4cf91875":"code","65dc8711":"code","f17c300f":"code","7c6ab187":"code","678e81fa":"code","a70ff6ec":"code","596a21ff":"code","387059f4":"code","00afdfae":"code","8424396c":"code","e18c9f89":"code","f8934310":"code","48c9ba7d":"code","85bb99ad":"code","ac029a29":"code","25575ef2":"markdown","f0abdadb":"markdown","ab273351":"markdown"},"source":{"4cf91875":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","65dc8711":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","f17c300f":"target = train['target']\nsns.countplot(target)\ntrain.drop(['target'], inplace =True,axis =1)","7c6ab187":"def concat_df(train, test):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train, test], sort=True).reset_index(drop=True)\ndf_all = concat_df(train, test)\nprint(train.shape)\nprint(test.shape)\nprint(df_all.shape)\ndf_all.head()","678e81fa":"features = ['keyword','location']\nfor feat in features : \n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(df_all[feat].isnull().sum())+ \" for the combined dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(train[feat].isnull().sum())+ \" for the train dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(test[feat].isnull().sum())+ \" for the test dataset\")","a70ff6ec":"# To check if there are any keywords which are missing in the train set but present in the test set\nkeyw_train = train['keyword'].unique()\nkeyw_test = test['keyword'].unique()\nprint(set(keyw_train)==set(keyw_test))","596a21ff":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsentences = train['text']\n# 80% of total data\ntrain_size = int(7613*0.8)\ntrain_sentences = sentences[:train_size]\ntrain_labels = target[:train_size]\n\ntest_sentences = sentences[train_size:]\ntest_labels = target[train_size:]\n\n# Setting our parameters for the tokenizer (currently using default, we will tune them once we have optimised the rest of the model)\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(train_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(test_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n","387059f4":"import tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(14, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","00afdfae":"num_epochs = 10\nhistory = model.fit(padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels))","8424396c":"# Let us analyse our model performance in an accuracy vs epoch graph\nimport matplotlib.pyplot as plt\n\ndef plot(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\nplot(history, \"accuracy\")\nplot(history, 'loss')","e18c9f89":"tokenizer_1 = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer_1.fit_on_texts(train['text'])\nword_index = tokenizer_1.word_index\nsequences = tokenizer_1.texts_to_sequences(train['text'])\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntrue_test_sentences = test['text']\ntesting_sequences = tokenizer_1.texts_to_sequences(true_test_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","f8934310":"model_2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_2.summary()\nnum_epochs = 10\nhistory = model_2.fit(padded, target, epochs=num_epochs, verbose=2)","48c9ba7d":"# Now let us deal with testing data\noutput = model_2.predict(testing_padded)\npred_plot =  pd.DataFrame(output, columns=['target'])\npred_plot.plot.hist()","85bb99ad":"final_output = []\nfor val in pred_plot.target:\n    if val > 0.5:\n        final_output.append(1)\n    else:\n        final_output.append(0)","ac029a29":"submission['target'] = final_output\n# submission['id'] = test['id']\nsubmission.to_csv(\"final.csv\", index=False)\nsubmission.head()","25575ef2":"**This is just the baseline prediction, stay tuned for a updated version with data cleaning, feature generation, and more!  **","f0abdadb":"Before we jump on to applying our testing data let us retrain our model with the entire train set","ab273351":"A great course to get you started:\n\n[Natural Language Processing in TensorFlow](https:\/\/www.coursera.org\/learn\/natural-language-processing-tensorflow\/)\n"}}