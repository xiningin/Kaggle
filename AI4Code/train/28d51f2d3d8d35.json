{"cell_type":{"8e3cea16":"code","1b221315":"code","28e72577":"code","73dac5b2":"code","b0a8d4c2":"code","a19fce19":"code","c0aa80b0":"code","c755135c":"code","73543aa4":"code","bebd0dc4":"code","d1eb99b3":"code","59a1dc42":"code","a7b8336f":"code","4d5b583e":"code","7ced45ca":"code","b4bf591e":"code","515679e6":"code","5696ae63":"code","20889bf6":"code","f2fc4b1f":"markdown","83cac5c5":"markdown","a7f2ef08":"markdown","2a8b73a2":"markdown","22ae8bdf":"markdown","14aed89d":"markdown","69c44269":"markdown","2794b992":"markdown"},"source":{"8e3cea16":"# import library\nimport numpy as np # numpy\n\nfrom sklearn import datasets # load dataset\nfrom sklearn.model_selection import train_test_split # split dataset\nfrom sklearn.preprocessing import StandardScaler # standard scaler\nfrom sklearn.linear_model import SGDClassifier # import SGDClassifier\nfrom sklearn.metrics import accuracy_score # check accuracy\n\nfrom sklearn.linear_model import Perceptron # import Perceptron\nfrom sklearn.linear_model import LogisticRegression # import LogisticRegression\nfrom sklearn.svm import SVC # import SVM \nfrom sklearn.tree import DecisionTreeClassifier # import Decision Tree\nfrom sklearn.neighbors import KNeighborsClassifier # import KNeighborClassifier\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt","1b221315":"# load dataset\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class label :', np.unique(y))","28e72577":"# split training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","73dac5b2":"# check unique value's count\nprint('y label count :', np.bincount(y))\nprint('y_train label count :', np.bincount(y_train))\nprint('y_test label count :', np.bincount(y_test))","b0a8d4c2":"# standardize\nsc = StandardScaler()\nsc.fit(X_train) # calculate mu and sigma\nX_train_std = sc.transform(X_train) # standardize\nX_test_std = sc.transform(X_test)\n\nX_combined_std = np.vstack((X_train_std, X_test_std)) # vlookup combine\ny_combined_std = np.hstack((y_train, y_test))  # hlookup combine\n\nX_combined = np.vstack((X_train, X_test)) # vlookup combine\ny_combined = np.hstack((y_train, y_test))  # hlookup combine","a19fce19":"# define function about visualizing\ndef plot_decision_regions(X, y, classifier, test_idx = None, resolution = 0.02):\n    \n    # set marker and colormap\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    # draws a decision boundary.\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha = 0.3, cmap = cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(X[y == cl, 0], X[y == cl, 1],\n                   alpha = 0.8, c = colors[idx],    # alpha : size of marker\n                   marker = markers[idx], label = cl,\n                   edgecolor = 'black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                   c = '', edgecolor = 'black', alpha = 1,\n                   s = 100, label = 'test set')","c0aa80b0":"# Perceptron modeling\nppn = Perceptron(max_iter = 40, eta0 = 0.1, tol = 1e-3, random_state = 1)\nppn.fit(X_train_std, y_train)\ny_pred = {}\ny_pred['Perceptron'] = ppn.predict(X_test_std)","c755135c":"# Decision regions\nplot_decision_regions(X_combined_std, y_combined_std,\n                    classifier = ppn, test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.title('Perceptron Decision Regions')\nplt.tight_layout()\nplt.show()","73543aa4":"# Logistic Regression modeling\nLR = LogisticRegression(solver = 'liblinear', multi_class = 'auto',\n                       C = 100, random_state = 1)\nLR.fit(X_train_std, y_train)\ny_pred['Logistic Regression'] = LR.predict(X_test_std)","bebd0dc4":"# Decision regions\nplot_decision_regions(X_combined_std, y_combined_std,\n                    classifier = LR, test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.title('Logistic Regression Decision Regions')\nplt.tight_layout()\nplt.show()","d1eb99b3":"# Linear SVM modeling\nsvm = SVC(kernel = 'linear', C = 1, random_state = 1)\nsvm.fit(X_train_std, y_train)\ny_pred['Linear SVM'] = svm.predict(X_test_std)","59a1dc42":"# Decision regions\nplot_decision_regions(X_combined_std, y_combined_std,\n                    classifier = svm, test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.title('Linear SVM Decision Regions')\nplt.tight_layout()\nplt.show()","a7b8336f":"# Kernel SVM modeling\nsvm2 = SVC(kernel = 'rbf', gamma = 0.2, C = 1, random_state =1)\nsvm2.fit(X_train_std, y_train)\ny_pred['kernel SVM'] = svm2.predict(X_test_std)","4d5b583e":"# Decision regions\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm2,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.title('Kernel SVM Decision Regions')\nplt.tight_layout()\nplt.show()","7ced45ca":"# Decission Tree modeling\ntree = DecisionTreeClassifier(criterion = 'gini',\n                             max_depth = 4,\n                             random_state = 1)\ntree.fit(X_train, y_train)\ny_pred['Decision Tree'] = tree.predict(X_test)","b4bf591e":"# Decision regions\nplot_decision_regions(X_combined, y_combined,\n                     classifier = tree,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [cm]')\nplt.ylabel('petal width [cm]')\nplt.legend(loc = 'upper left')\nplt.title('Decision Tree Decision Regions')\nplt.tight_layout()\nplt.show()","515679e6":"knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski')\nknn.fit(X_train_std, y_train)\ny_pred['KNN'] = knn.predict(X_test_std)","5696ae63":"# Decision regions\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = knn,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [cm]')\nplt.ylabel('petal width [cm]')\nplt.legend(loc = 'upper left')\nplt.title('KNN Decision Regions')\nplt.tight_layout()\nplt.show()","20889bf6":"for key, value in y_pred.items():\n    print('The accuracy of {} : {}'.format(key, (y_test == value).sum() \/ len(y_test)))","f2fc4b1f":"# Comparing decision regions with supervised learning algorithm\n\nThis notebook goal:\n- understanding basic principle of each supervised learning algorithm.\n- Checking decision regions of each supervised learning algorithm.\n\nYou can learn about Decision tree principle in [my blog](https:\/\/konghana01.tistory.com\/category\/English).","83cac5c5":"## SVM\n\n##### The svm model is classified based on margin.\n\nA simple explanation of support vector machines is a algorithm to classify classes through decision boundaries that maximize **margins**. \n\n**Margin** means the distance between the superplane (decision boundary) that separates the class and the training sample closest to this superplane.\n\n<b>Therefore, it is very important to see and understand decision boundaries.\n<\/b>\n\nI recommend someone who want to know about SVM's principle to visit [my Linear SVM notebook](https:\/\/www.kaggle.com\/choihanbin\/iris-classification-with-sklearn-svm-linear)","a7f2ef08":"## Logistic Regression\n\nThe biggest difference between Perceptron is Logistic regression use <b>sigmoid function<\/b> as activation function instead of linear function. \n\nAlso Logistic Regression use **Log likelihood function** as cost function. The advantage of defining the log likelihood function is that it imposes a higher cost on the wrong prediction.\n\nIf you need more information about Logistic Regression, you can visit [my Logistic Regression notebook](https:\/\/www.kaggle.com\/choihanbin\/logistic-regression-principle)","2a8b73a2":"## KNN\n\nKNN Algorithm Learning Course\n1. Select the number k and distance measurement criteria.\n2. Locate the k nearest neighbor in the sample you want to classify.\n3. Assign class labels by majority vote.\n\nAs you can see in the learning process, only 'K' and 'distance measurement criteria' are required to learn KNN algorithms. Generally, the distance measurement criterion is 'Eucladian distance'.\n\n**Manhattan Distance** : \n$$\\sum_{i=0}^{n}|p_i - q_i|$$\n\n**Eucladian distance** : \n$$ \\sqrt{ \\sum_{i=0}^{n}(p_i - q_i)^2 } $$\n\n**minkowski** :\n$$ _p\\sqrt{ \\sum_{k}|x_{k}^{i} - x_{k}^{i}|^p }  $$ \n\n* If parameter p is specified as 2, it becomes Eucladian distance, and if p is specified as 1, it becomes Manhattan distance.","22ae8bdf":"## Summary\n\n#### Decision Regions\n- Perceptron can be seen to divide the decision regions linearly from the middle. \n- Logistic regression analysis shows that the decision regions are divided at any one point. \n- Linear SVM show that the decision regions are completely linear. \n- Kernel SVM show that the decision regions are in nonlinear form. \n- Decision trees divide the decision regions into squares. \n- KNN divides decision regions by similar but slightly more irregular lines to SVMs.\n\n","14aed89d":"## Decision Tree\n\nThe criteria for classifying decision trees are information gain. Information gains can be determined based on impurity. As the name suggests, impurity is an indicator of how various classes are mixed into the node.\n\n##### Information Gain\n$$ IG(D_p,f) = I(D_p) - \\sum_{j=1}^{m}\\frac{N_j}{N_p}I(D_j) $$\n\n\nSimply put, the information gain is determined by the sum of the impurity of the child nodes and the difference between the parent nodes. The lower the impurity of the child nodes, the greater the information gain.\n\n\nThe frequently used indicators of impurity in decision trees are **Entropy**, **Gini impurity**, and **Classification error**.\n\n\nI recommend someone who want to know about Decision Tree's principle to visit [my Decision Tree notebook](https:\/\/www.kaggle.com\/choihanbin\/principle-of-decision-tree-by-classifying-iris)","69c44269":"## Perceptron\n\nTo sum up Perceptron process briefly,\n1. Initialize weights to zero or randomly small values.\n2. Proceed to the next task of each training sample drawer.\n    - Calculates the output value yhat.\n    - Update weights.\n\ncan be expressed as.\n\n\n\nIf you need more information about Perceptron, you can visit [my Perceptron notebook](https:\/\/www.kaggle.com\/choihanbin\/iris-classification-with-sklearn-perceptron)","2794b992":"### Some supervised learning algorithm needs standardized train dataset.\n- Perceptron\n- Logistic Regreesion\n- SVM\n- KNN"}}