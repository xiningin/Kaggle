{"cell_type":{"2777399b":"code","98393c58":"code","6ecccf10":"code","e73d4d99":"code","a95b76b8":"code","44207211":"code","d7feb587":"code","6a85fd7a":"code","c679ea31":"code","8260498d":"code","e2e88402":"code","3943eef8":"code","897b9d70":"code","a5b274f5":"code","617d500f":"code","06dcb3cc":"code","b8e3e4ca":"markdown"},"source":{"2777399b":"import pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n","98393c58":"train = pd.read_csv('..\/input\/train.csv')\nprint(train.shape)\ntrain.head()","6ecccf10":"train.columns","e73d4d99":"test = pd.read_csv('..\/input\/test.csv')\nprint(test.shape)","a95b76b8":"test.columns","44207211":"y_train = train.winPlacePerc\nX_train = train.drop(['Id', 'groupId', 'matchId', 'winPlacePerc'], axis=1)","d7feb587":"models = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('RIDGE', Ridge()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('ELN', ElasticNet()))\nmodels.append(('DT', DecisionTreeRegressor()))\nscoring = 'r2'\nseed = 1000\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train,  cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","6a85fd7a":"import pprint\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\n# Create the random grid\nrandom_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\nrandom_grid","c679ea31":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = DecisionTreeRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\nprint( rf_random.best_estimator_ )\nprint( rf_random.best_score_ )\nprint( rf_random.best_params_ )","8260498d":"rf = DecisionTreeRegressor(criterion='mse', max_depth=20, max_features='auto',\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=4,\n           min_samples_split=10, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best').fit(X_train, y_train)","e2e88402":"X_test = test.drop(['Id', 'groupId', 'matchId'], axis=1)","3943eef8":"submission = pd.read_csv('..\/input\/sample_submission.csv')","897b9d70":"submission.head()","a5b274f5":"submission.winPlacePerc = rf.predict(X_test)","617d500f":"submission.head()","06dcb3cc":"submission.to_csv('submission.csv', index=False)","b8e3e4ca":"## Base Model"}}