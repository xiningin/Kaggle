{"cell_type":{"093c6025":"code","e454daa0":"code","5e991d80":"code","3b9c3d49":"code","14b221c8":"code","30c4f506":"code","2abbf980":"code","3c09e055":"code","cbf52b60":"code","2f1de712":"code","215c9ffa":"code","a9e2a796":"code","2ec29008":"code","d763faa2":"code","12d6c7a5":"code","c46211d7":"code","51a70de9":"code","f2bf3856":"code","e3982363":"code","bdf36057":"code","0cb9fd56":"code","064acef2":"code","c3b5c54e":"code","3b5e7304":"code","fa916622":"markdown","b4253c39":"markdown","7ba286ff":"markdown","a8f5acf7":"markdown","299c3370":"markdown","ade1a10e":"markdown","97c8c369":"markdown","82f91701":"markdown","f438870d":"markdown","1dfbcfe9":"markdown"},"source":{"093c6025":"# https:\/\/www.pyimagesearch.com\/2017\/NUM_CLASSES\/11\/image-classification-with-keras-and-deep-learning\/\n# https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n# https:\/\/machinelearningmastery.com\/save-load-keras-deep-learning-models\/\n#https:\/\/www.learnopencv.com\/image-classification-using-convolutional-neural-networks-in-keras\/\n!pip install tensorflow-gpu\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random\nimport sys\nimport cv2\nimport matplotlib\nimport tensorflow\n\nfrom subprocess import check_output\nfrom datetime import datetime\n#tensorflow.keras.something\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Embedding\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# Image Cropping Imports\nfrom PIL import ImageFile \nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom skimage import io\nfrom skimage.transform import resize\n\n","e454daa0":"#list the files in the input directory\n#classes : 0 - No DR, 1 - Mild, 2 - Moderate, 3 - Severe, 4 - Proliferative DR\n\n# print(os.listdir(\"..\/input\"))\n# print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #trainLabels.csv\n# print(check_output([\"pwd\", \"\"]).decode(\"utf8\")) # \/kaggle\/working\/\n","5e991d80":"def classes_to_int(label):\n    # label = classes.index(dir)\n    label = label.strip()\n    if label == \"No DR\":  return 0\n    if label == \"Mild\":  return 1\n    if label == \"Moderate\":  return 2\n    if label == \"Severe\":  return 3\n    if label == \"Proliferative DR\":  return 4\n    print(\"Invalid Label\", label)\n    return 5\n\ndef int_to_classes(i):\n    if i == 0: return \"No DR\"\n    elif i == 1: return \"Mild\"\n    elif i == 2: return \"Moderate\"\n    elif i == 3: return \"Severe\"\n    elif i == 4: return \"Proliferative DR\"\n    print(\"Invalid class \", i)\n    return \"Invalid Class\"\n\n\nNUM_CLASSES = 5\n\n# we resize images\nWIDTH = 128\nHEIGHT = 128\nDEPTH = 3\ninputShape = (HEIGHT, WIDTH, DEPTH)\n\n# initialize number of epochs to train for, initial learning rate and batch size\nEPOCHS = 15\nINIT_LR = 1e-3\nBS = 32\n\n#global variables\nImageNameDataHash = {}\nuniquePatientIDList = []","3b9c3d49":"def readTrainData(trainDir):\n    global ImageNameDataHash\n    # loop over the input images\n    images = os.listdir(trainDir)\n    print(\"Number of files in \" + trainDir + \" is \" + str(len(images)))\n    for imageFileName in images:\n        if (imageFileName == \"trainLabels.csv\"):\n            continue\n        # load the image, pre-process it, and store it in the data list\n        imageFullPath = os.path.join(os.path.sep, trainDir, imageFileName)\n        #print(imageFullPath)\n        img = load_img(imageFullPath)\n        arr = img_to_array(img)  # Numpy array with shape (233,233,3)\n        dim1 = arr.shape[0]\n        dim2 = arr.shape[1]\n        dim3 = arr.shape[2]\n        if (dim1 < HEIGHT or dim2 < WIDTH or dim3 < DEPTH):\n            print(\"Error image dimensions are less than expected \"+str(arr.shape))\n        arr = cv2.resize(arr, (HEIGHT,WIDTH)) #Numpy array with shape (HEIGHT, WIDTH,3)\n        #print(arr.shape) # 128,128,3\n        dim1 = arr.shape[0]\n        dim2 = arr.shape[1]\n        dim3 = arr.shape[2]\n        if (dim1 != HEIGHT or dim2 != WIDTH or dim3 != DEPTH):\n            print(\"Error after resize, image dimensions are not equal to expected \"+str(arr.shape))\n        #print(type(arr))\n        # scale the raw pixel intensities to the range [0, 1] - TBD TEST\n        arr = np.array(arr, dtype=\"float\") \/ 255.0\n        imageFileName = imageFileName.replace('.jpeg','')\n        ImageNameDataHash[str(imageFileName)] = np.array(arr) \n    return","14b221c8":"from datetime import datetime\nprint(\"Loading images at...\"+ str(datetime.now()))\nsys.stdout.flush()\nreadTrainData(\"\/kaggle\/working\/..\/input\/\")\nprint(\"Loaded \" + str(len(ImageNameDataHash)) + \" images at...\"+ str(datetime.now())) # 1000","30c4f506":"#csv contains image\tlevel\n#10_left 0\n#10_right 0\nimport csv\ndef readTrainCsv():\n    raw_df = pd.read_csv('\/kaggle\/working\/..\/input\/trainLabels.csv', sep=',')\n    print(type(raw_df)) #<class 'pandas.core.frame.DataFrame'>\n    row_count=raw_df.shape[0] #gives number of row count row_count=35126 \n    col_count=raw_df.shape[1] #gives number of col count col count=2\n    print(\"row_count=\"+str(row_count)+\" col count=\"+str(col_count))\n    raw_df[\"PatientID\"] = ''\n    header_list = list(raw_df.columns)\n    print(header_list) # ['image', 'level', 'PatientID']\n    # double check if level of left and right are same or not\n    ImageLevelHash = {}\n    patientIDList = []\n    for index, row in raw_df.iterrows():\n        # 0 is image, 1 is level, 2 is PatientID, 3 is data\n        key = row[0] + ''\n        patientID = row[0] + ''\n        patientID = patientID.replace('_right','')\n        patientID = patientID.replace('_left','')\n        #print(\"Adding patient ID\"+ patientID)\n        raw_df.at[index, 'PatientID'] = patientID\n        patientIDList.append(patientID)\n        ImageLevelHash[key] = str(row[1]) # level\n                \n    global uniquePatientIDList\n    uniquePatientIDList = sorted(set(patientIDList))\n    count=0;\n    for patientID in uniquePatientIDList:\n        left_level = ImageLevelHash[str(patientID+'_left')]\n        right_level = ImageLevelHash[str(patientID+'_right')]\n        #right_exists = str(patientID+'_right') in raw_df.values\n        if (left_level != right_level):\n            count = count+1\n            #print(\"Warning for patient=\"+ str(patientID) + \" left_level=\" + left_level+ \" right_level=\" +right_level)\n    print(\"count of images with both left and right eye level not matching=\"+str(count)) # 2240\n    print(\"number of unique patients=\"+str(len(uniquePatientIDList))) # 17563\n    return raw_df","2abbf980":"random.seed(10)\nprint(\"Reading trainLabels.csv...\")\ndf = readTrainCsv()","3c09e055":"# df has 3 columns ['image', 'level', 'PatientID']\nkeepImages =  list(ImageNameDataHash.keys())\ndf = df[df['image'].isin(keepImages)]\nprint(len(df)) # 1000","cbf52b60":"#convert hash to dataframe\nimageNameArr = []\ndataArr = []\nfor index, row in df.iterrows():\n    key = str(row[0])\n    if key in ImageNameDataHash:\n        imageNameArr.append(key)\n        dataArr.append(np.array(ImageNameDataHash[key])) # np.array\n\ndf2 = pd.DataFrame({'image': imageNameArr, 'data': dataArr})\ndf2_header_list = list(df2.columns) \nprint(df2_header_list) # ['image', 'data']\nprint(len(df2)) # 1000\n#print(df2.describe(include='all'))\n#print(df2.sample(3)) # 3 rows x 2 columns","2f1de712":"if len(df) != len(df2):\n    print(\"Error length of df != df2\")\n    \nfor idx in range(0,len(df)):\n    if (df.loc[df.index[idx], 'image'] != df2.loc[df2.index[idx], 'image']):\n        print(\"Error \" + df.loc[df.index[idx], 'image'] +\"==\" + df2.loc[df2.index[idx], 'image'])\n        \nprint(df2.dtypes)\nprint(df.dtypes)","215c9ffa":"df = pd.merge(df2, df, left_on='image', right_on='image', how='outer')\ndf_header_list = list(df.columns) \nprint(df_header_list) # 'image', 'data', level', 'PatientID'\nprint(len(df)) # 1000\nprint(df.sample())","a9e2a796":"def create_directory(directory):\n    '''\n    Creates a new folder in the specified directory if the folder doesn't exist.\n    INPUT\n        directory: Folder to be created, called as \"folder\/\".\n    OUTPUT\n        New folder in the current directory.\n    '''\n    if not os.path.exists(directory):\n        os.makedirs(directory)","2ec29008":"def crop_and_resize_images(path, new_path, cropx, cropy, img_size=256):\n    '''\n    Crops, resizes, and stores all images from a directory in a new directory.\n    INPUT\n        path: Path where the current, unscaled images are contained.\n        new_path: Path to save the resized images.\n        img_size: New size for the rescaled images.\n    OUTPUT\n        All images cropped, resized, and saved from the old folder to the new folder.\n    '''\n    create_directory(new_path)\n    dirs = [l for l in os.listdir(path) if l != '.DS_Store']\n    total = 0\n\n    for item in dirs:\n        img = io.imread(path+item)\n        y,x,channel = img.shape\n        startx = x\/\/2-(cropx\/\/2)\n        starty = y\/\/2-(cropy\/\/2)\n        img = img[starty:starty+cropy,startx:startx+cropx]\n        img = resize(img, (256,256))\n        io.imsave(str(new_path + item), img)\n        total += 1\n        print(\"Saving: \", item, total)\n","d763faa2":"# sample0 = df.loc[df.index[0], 'data']\n# print(sample0)\n# print(type(sample0)) # <class 'numpy.ndarray'>\n# print(sample0.shape) # 128,128,3\n# from matplotlib import pyplot as plt\n# plt.imshow(sample0, interpolation='nearest')\n# plt.show()\n# print(\"Sample Image\")","12d6c7a5":"X = df['data']\nY = df['level']\n\n#print(type(X)) # 'pandas.core.series.Series'\n#X = np.array(X, dtype=\"float\") \/ 255.0 -- TBD moved to top\nY = np.array(Y)\n\nY =  to_categorical(Y, num_classes=NUM_CLASSES)","c46211d7":"print(\"Parttition data into 75:25...\")\nsys.stdout.flush()\nprint(\"Unique patients in dataframe df=\" + str(df.PatientID.nunique())) # 500\nunique_ids = df.PatientID.unique()\nprint('unique_ids shape='+ str(len(unique_ids))) #500\n\ntrain_ids, valid_ids = train_test_split(unique_ids, test_size = 0.25, random_state = 10) #stratify = rr_df['level'])\ntrainid_list = train_ids.tolist()\nprint('trainid_list shape=', str(len(trainid_list))) # 375\n\ntraindf = df[df.PatientID.isin(trainid_list)]\nvalSet = df[~df.PatientID.isin(trainid_list)]","51a70de9":"traindf = traindf.reset_index(drop=True)\nvalSet = valSet.reset_index(drop=True)","f2bf3856":"trainX = traindf['data']\ntrainY = traindf['level']\n\nvalX = valSet['data']\nvalY = valSet['level']\n\n#(trainX, valX, trainY, valY) = train_test_split(X,Y,test_size=0.25, random_state=10)\nprint('trainX shape=', trainX.shape[0], 'valX shape=', valX.shape[0]) # 750, 250\n","e3982363":"trainY =  to_categorical(trainY, num_classes=NUM_CLASSES)\nvalY =  to_categorical(valY, num_classes=NUM_CLASSES)","bdf36057":"#construct the image generator for data augmentation\nprint(\"Generating images...\")\nsys.stdout.flush()\naug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \\\n    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\\\n    horizontal_flip=True, fill_mode=\"nearest\")","0cb9fd56":"def createModel():\n    print (\"num classes\", NUM_CLASSES)\n    model = Sequential()\n    # first set of CONV => RELU => MAX POOL layers\n    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=inputShape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    # Output layer\n    model.add(Embedding(output_dim=NUM_CLASSES, activation='softmax'))\n#     keras.layers.Embedding(output_dim=NUM_CLASSES)\n    # returns our fully constructed deep learning + Keras image classifier \n    opt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\n    # use binary_crossentropy if there are two classes\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    return model","064acef2":"print(\"Reshaping trainX at...\"+ str(datetime.now()))\n#print(trainX.sample()) \nprint(type(trainX)) # <class 'pandas.core.series.Series'>\nprint(trainX.shape) # (750,)\nfrom numpy import zeros\nXtrain = np.zeros([trainX.shape[0],HEIGHT, WIDTH, DEPTH])\nfor i in range(trainX.shape[0]): # 0 to traindf Size -1\n    Xtrain[i] = trainX[i]\nprint(Xtrain.shape) # (750,128,128,3)\nprint(\"Reshaped trainX at...\"+ str(datetime.now()))","c3b5c54e":"print(\"Reshaping valX at...\"+ str(datetime.now()))\nprint(type(valX)) # <class 'pandas.core.series.Series'>\nprint(valX.shape) # (250,)\nfrom numpy import zeros\nXval = np.zeros([valX.shape[0],HEIGHT, WIDTH, DEPTH])\nfor i in range(valX.shape[0]): # 0 to traindf Size -1\n    Xval[i] = valX[i]\nprint(Xval.shape) # (250,128,128,3)\nprint(\"Reshaped valX at...\"+ str(datetime.now()))","3b5e7304":"# initialize the model\nprint(\"compiling model...\")\nsys.stdout.flush()\nmodel = createModel()\n\n# print the summary of model\nfrom tensorflow.keras.utils import print_summary\nprint_summary(model, line_length=None, positions=None, print_fn=None)\n\n# add some visualization\nfrom IPython.display import SVG\nfrom tensorflow.keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","fa916622":"### Pre-process the images ","b4253c39":"### Scale the raw pixel intensities to the range [0, 1] then convert labels to vectors","7ba286ff":"### Further look into the data...","a8f5acf7":"### Initialise Model","299c3370":"### Split data into training and testing using 75% training and 25% for validation... refer to https:\/\/www.kaggle.com\/kmader\/tf-data-tutorial-with-retina-and-keras\n\n### Then reset index for new dataframes","ade1a10e":"### Allow for conversion of labels to categorical variables, pre-processing the size of images in the data-set and the epochs and batch size of the model.","97c8c369":"### Begin by importing requirements","82f91701":"### The function below builds the model, we will be using 4 layers?? ","f438870d":"### Begin Data Augmentation","1dfbcfe9":"### Print a sample image from the dataset and look at the sape of the "}}