{"cell_type":{"147b0c08":"code","c09cd163":"code","62e06a3a":"code","74746248":"code","ead0855c":"code","7a49c699":"code","64a930ea":"code","e163928c":"code","b44a00d3":"code","fcfffe22":"code","978ca857":"code","bc4d32d4":"code","866d58fc":"code","03020cba":"code","ae41c042":"code","454fd256":"code","0a7b329f":"code","83df3a7b":"code","d8482cd0":"code","115ba5cc":"code","482f4a05":"code","ab76fc4a":"code","081bcc1d":"code","d4666441":"code","7297fa41":"code","ef942b6d":"code","af466621":"code","4aee131f":"code","6b09c9a5":"code","c8c4708e":"code","29b90834":"code","61b3a030":"code","b5dc383a":"code","4cb7d9b3":"code","edcce0ec":"code","b831717a":"code","c71dd9d9":"code","66757c4e":"code","35fe0be2":"code","bc9104c7":"code","1bf85628":"code","b7b9b5b4":"code","eaba5874":"code","f5fadf47":"code","696b22aa":"code","23b0db74":"code","6f3d1b5b":"code","7eedabdb":"code","53db9a44":"code","fdca4c87":"code","8b11e090":"code","a5e025f0":"code","90b20cf4":"code","4c1735eb":"code","98eec766":"code","1e439e23":"code","2489439b":"code","369f140e":"code","ef9241b9":"code","1f98e46c":"code","4595d8eb":"code","233101fb":"code","d0b95118":"code","c3591bdc":"code","f27c0863":"code","40c49094":"code","7a35add7":"code","35aed90c":"code","1681a84f":"code","da82bbd6":"code","7c244d99":"code","6b126dd9":"code","347a5957":"code","57dccedf":"code","0c7de937":"code","447c3ccd":"code","9e7efd63":"code","432e9dec":"code","e147cabd":"markdown","ccc637a4":"markdown","6fa8f543":"markdown","4d5b98e9":"markdown","12c361fe":"markdown","0896bf34":"markdown","69bdce4a":"markdown","8d70bf6b":"markdown","b008e362":"markdown","cf8e7059":"markdown","8e3b4993":"markdown","3f382136":"markdown","69e3b676":"markdown","967909c2":"markdown","90b9b5ea":"markdown","108658f6":"markdown","d6fd34ce":"markdown","95490a0d":"markdown","3c47207b":"markdown","6fd62528":"markdown","01abb6e2":"markdown","fff0c9ba":"markdown","5276f0d9":"markdown","aaff3db3":"markdown","06267c5a":"markdown","dbec1d67":"markdown","1208509c":"markdown","a700be64":"markdown","0704b52b":"markdown","f6e28545":"markdown","e6b94611":"markdown","3da30897":"markdown","86799bd0":"markdown","22d92251":"markdown","cc556a45":"markdown","ebd05bae":"markdown","6d6c63c8":"markdown","81ad281b":"markdown","4efa2331":"markdown","749304ec":"markdown","9c37b033":"markdown","ac680ef6":"markdown","fae5a48b":"markdown","0a23f3c6":"markdown","a889ce16":"markdown","49b7dc75":"markdown","6482e5ec":"markdown","84e720f4":"markdown","73021901":"markdown","a55bb0fa":"markdown","c426933d":"markdown","a25dfb49":"markdown","6f8bdd1e":"markdown","0879915f":"markdown","d53c8930":"markdown","b5afe241":"markdown","4ea33d63":"markdown","35992796":"markdown","dedca079":"markdown","30e7a6ab":"markdown","33726366":"markdown","d637211d":"markdown","d967bd8f":"markdown"},"source":{"147b0c08":"# For Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\n\n# For Data Manipulation\nimport numpy as np \nimport pandas as pd\nimport sklearn\nfrom itertools import cycle\n\n\n# For Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# For Classification Results\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.exceptions import NotFittedError\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import PCA\n\n# Importing Models\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN #K-Nearest Neighbors\nfrom sklearn.svm import SVC #Support Vector Classifier\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest Classifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import GridSearchCV","c09cd163":"df = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","62e06a3a":"ax = df[\"quality\"].value_counts().plot.bar(figsize=(7,5))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \nprint(df[\"quality\"].value_counts(normalize=True)*100)","74746248":"df.describe()","ead0855c":"df.isnull().sum() #No missing values","7a49c699":"df[\"is good\"] = 0\ndf.loc[df[\"quality\"]>=7,\"is good\"] = 1","64a930ea":"ax = df[\"is good\"].value_counts().plot.bar(figsize=(7,5))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 0.5), color=\"white\")\n    \nprint(df[\"is good\"].value_counts(normalize=True)*100)","e163928c":"features = df.columns[:-2]\noutput = df.columns[-1]\nprint(\"Features: \\n{}, \\n\\nLabels: \\n{}\".format(features.values,output))","b44a00d3":"# sns.pairplot(df[features],palette='coolwarm')\n# plt.show()","fcfffe22":"for f in features:\n    print('Feature:{}\\n Skew = {} \\n\\n'.format(f,df[f].skew()))","978ca857":"corr = df[features].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","bc4d32d4":"for f in features:\n    df.boxplot(column=f, by=output)\n    plt.title(f)\nplt.show()","866d58fc":"X = df[features].values\ny = df[output].values","03020cba":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30)\nprint('Training size: {}, Testing size: {}'.format(X_train.size,X_test.size))","ae41c042":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","454fd256":"def get_probabilty_output(X_test, model_fitted, value_count=10):\n    def highlight_max(data, color='yellow'):\n        attr = 'background-color: {}'.format(color)\n        if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n            is_max = data == data.max()\n            return [attr if v else '' for v in is_max]\n        else:  # from .apply(axis=None)\n            is_max = data == data.max().max()\n            return pd.DataFrame(np.where(is_max, attr, ''), index=data.index, columns=data.columns)\n        \n    y_scores = model_fitted.predict_proba(X_test)\n    prob_df = pd.DataFrame(y_scores*100).head(value_count)\n    styled_df = prob_df.style.background_gradient(cmap='Reds')\n    styled_df = styled_df.highlight_max(axis=1, color='green')\n    return styled_df","0a7b329f":"def get_classification_report(y_test,predictions,average=\"macro\"):\n    #Confusion Matrix\n    cm = confusion_matrix(y_test, predictions)\n    sns.heatmap(cm, annot=True)\n    plt.title(\"Confusion Matrix\")\n    \n    acc = accuracy_score(y_test, predictions)\n    pre = precision_score(y_test, predictions, average=average)\n    rec = recall_score(y_test, predictions, average=average)\n    # Prediction Report\n    print(classification_report(y_test, predictions, digits=3))\n    print(\"Overall Accuracy:\", acc)\n    print(\"Overall Precision:\", pre)\n    print(\"Overall Recall:\", rec)\n    \n    return acc,pre,rec\n    ","83df3a7b":"def get_classification_ROC(X,y,model,test_size,model_fitted=False,random_state=0):\n    \n    def check_fitted(clf): \n        return hasattr(clf, \"classes_\")\n    \n    if(len(np.unique(y)) == 2):\n        #Binary Classifier\n        if not check_fitted(model):\n            model = model.fit(X,y)\n        \n        plot_roc_curve(model, X, y)\n        y_score = model.predict_proba(X)[:, 1]\n        fpr, tpr, threshold = roc_curve(y, y_score)\n        auc = roc_auc_score(y, y_score)\n        return auc\n#         print(\"False Positive Rate: {} \\nTrue Positive Rate: {} \\nThreshold:{}\".format(fpr,tpr,threshold))\n    \n    else:\n        #Multiclass Classifier\n        y_bin = label_binarize(y, classes=np.unique(y))\n        n_classes = y_bin.shape[1]\n\n        # shuffle and split training and test sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=test_size, random_state=random_state)\n\n        # Learn to predict each class against the other\n        classifier = OneVsRestClassifier(model)\n        model_fitted = classifier.fit(X_train, y_train)\n        try:\n            y_score = model_fitted.decision_function(X_test)\n        except:\n            y_score = model_fitted.predict_proba(X_test)\n\n\n\n        # Compute ROC curve and ROC area for each class\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n\n        plt.figure()\n        lw = 2\n        plt.plot(fpr[2], tpr[2], color='darkorange',\n                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver operating characteristic averaged')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n\n\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n        # Finally average it and compute AUC\n        mean_tpr \/= n_classes\n\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        # Plot all ROC curves\n        plt.figure(figsize=(10,10))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label='micro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"micro\"]),\n                 color='deeppink', linestyle=':', linewidth=4)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n                 label='macro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"macro\"]),\n                 color='navy', linestyle=':', linewidth=4)\n\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'blue', 'purple', 'green'])\n        for i, color in zip(range(n_classes), colors):\n            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                     label='ROC curve of class {0} (area = {1:0.2f})'\n                     ''.format(i, roc_auc[i]))\n\n        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('multi-class ROC (One vs All)')\n        plt.legend(loc=\"lower right\")\n        plt.show()","d8482cd0":"def visualisation_through_PCA(X_PCA, y, model_PCA, model_name=\"Classification Model\"):\n    X_set, y_set = X_PCA, y\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, model_PCA.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n                 alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue', 'yellow', 'purple', 'grey')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c = ListedColormap(('red', 'green', 'blue', 'yellow', 'purple', 'grey'))(i), label = j)\n    plt.title(model_name)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend()\n    plt.show()","115ba5cc":"pca = PCA(n_components = 2)\nX_train_PCA_2 = pca.fit_transform(X_train)\nX_test_PCA_2 = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Variance Explained by each of the Principal Components: {:.{prec}f}% and {:.{prec}f}%, \\nTotal Variance Explained: {:.{prec}f}%\".format((explained_variance*100)[0],\n                                                                                                                                               (explained_variance*100)[1],\n                                                                                                                                                  explained_variance.sum()*100,prec=3))","482f4a05":"parameters_LR = {\n    \"solver\" : ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n    \"penalty\" : ('l1', 'l2', 'elasticnet', 'none'),\n    \"C\" : [0.01, 0.1, 1, 10, 1000]\n    \n}\n\nmodel_LR = LogisticRegression()\nmodel_LR_with_best_params = GridSearchCV(model_LR, parameters_LR)\nmodel_LR_with_best_params.fit(X_train,y_train)\nmodel_LR_best_params = model_LR_with_best_params.best_params_","ab76fc4a":"model_LR_best_params","081bcc1d":"predictions_LR = model_LR_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_LR[:10])\nprint(\"Actual:\",y_test[:10])","d4666441":"get_probabilty_output(X_test=X_test, model_fitted=model_LR_with_best_params, value_count=15)","7297fa41":"acc_LR,pre_LR,rec_LR = get_classification_report(y_test,predictions_LR)","ef942b6d":"auc_LR = get_classification_ROC(X_test,y_test,model_LR_with_best_params,test_size=0.3,random_state=0)","af466621":"# model_LR_PCA = LogisticRegression(random_state = 0)\n# model_LR_PCA.fit(X_train_PCA_2, y_train)\n# predictions_LR_PCA = model_LR_PCA.predict(X_test_PCA_2)","4aee131f":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_LR_PCA, model_name=\"Logisitic Regression (Training Set)\")","6b09c9a5":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_LR_PCA, model_name=\"Logisitic Regression (Test Set)\")","c8c4708e":"parameters_KNN = {\n    \"n_neighbors\" : [2,5,7,15],\n    \"weights\" : ('uniform','distance'),\n    \"algorithm\" : ('auto','ball_tree','kd_tree','brute'),\n    'p': [1,2,5]\n    \n    \n}\n\nmodel_KNN = KNN(n_jobs=-1)\nmodel_KNN_with_best_params = GridSearchCV(model_KNN, parameters_KNN)\nmodel_KNN_with_best_params.fit(X_train,y_train)\nmodel_KNN_best_params = model_KNN_with_best_params.best_params_","29b90834":"model_KNN_best_params","61b3a030":"predictions_KNN = model_KNN_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_KNN[:10])\nprint(\"Actual:\",y_test[:10])","b5dc383a":"get_probabilty_output(X_test=X_test, model_fitted=model_KNN_with_best_params)","4cb7d9b3":"acc_KNN,pre_KNN,rec_KNN = get_classification_report(y_test,predictions_KNN)","edcce0ec":"auc_KNN = get_classification_ROC(X_test,y_test,model_KNN_with_best_params,test_size=0.3,random_state=0)","b831717a":"# model_KNN_PCA = KNN(5)\n# model_KNN_PCA.fit(X_train_PCA_2, y_train)\n# predictions_KNN_PCA = model_KNN_PCA.predict(X_test_PCA_2)","c71dd9d9":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_KNN_PCA, model_name=\"k-Nearest Neighbors (Training Set)\")","66757c4e":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_KNN_PCA, model_name=\"k-Nearest Neighbors (Test Set)\")","35fe0be2":"parameters_SVC = {\n    \"C\": [0.1, 1, 10],\n    \"kernel\": ('linear','poly','rbf'),\n    \"degree\": [2,4] \n    \n}\n\nmodel_SVC = SVC(probability=True)\nmodel_SVC_with_best_params = GridSearchCV(model_SVC, parameters_SVC)\nmodel_SVC_with_best_params.fit(X_train,y_train)\nmodel_SVC_best_params = model_SVC_with_best_params.best_params_","bc9104c7":"model_SVC_best_params","1bf85628":"predictions_SVC = model_SVC_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_SVC[:10])\nprint(\"Actual:\",y_test[:10])","b7b9b5b4":"get_probabilty_output(X_test=X_test, model_fitted=model_SVC_with_best_params)","eaba5874":"acc_SVC,pre_SVC,rec_SVC = get_classification_report(y_test,predictions_SVC)","f5fadf47":"auc_SVC = get_classification_ROC(X_test,y_test,model_SVC_with_best_params,test_size=0.3,random_state=0)","696b22aa":"# model_SVC_PCA = model_SVC = SVC(kernel=kernel, random_state=random_state, probability=True)\n# model_SVC_PCA.fit(X_train_PCA_2, y_train)\n# predictions_SVC_PCA = model_SVC_PCA.predict(X_test_PCA_2)","23b0db74":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_SVC_PCA, model_name=\"Support Vector Classifier (Training Set)\")","6f3d1b5b":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_SVC_PCA, model_name=\"Support Vector Classifier (Test Set)\")","7eedabdb":"model_NB = GaussianNB()\nmodel_NB.fit(X_train, y_train)","53db9a44":"predictions_NB = model_NB.predict(X_test)\nprint(\"Predictions:\",predictions_NB[:10])\nprint(\"Actual:\",y_test[:10])","fdca4c87":"get_probabilty_output(X_test=X_test, model_fitted=model_NB)","8b11e090":"acc_NB,pre_NB,rec_NB = get_classification_report(y_test,predictions_NB)","a5e025f0":"auc_NB = get_classification_ROC(X_test,y_test,model_NB,test_size=0.3,random_state=0)","90b20cf4":"# model_NB_PCA = GaussianNB()\n# model_NB_PCA.fit(X_train_PCA_2, y_train)\n# predictions_NB_PCA = model_NB_PCA.predict(X_test_PCA_2)","4c1735eb":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_NB_PCA, model_name=\"Naive Bayes Classifier (Training Set)\")","98eec766":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_NB_PCA, model_name=\"Naive Bayes Classifier (Test Set)\")","1e439e23":"parameters_DT = {\n    'criterion':('gini','entropy'),\n    'max_features': ('auto','sqrt','log2')\n}\n\n\nmodel_DT = DecisionTreeClassifier()\nmodel_DT_with_best_params = GridSearchCV(model_DT, parameters_DT)\nmodel_DT_with_best_params.fit(X_train,y_train)\nmodel_DT_best_params = model_DT_with_best_params.best_params_\nmodel_DT_with_best_params.fit(X_train,y_train)","2489439b":"model_DT_best_params","369f140e":"predictions_DT = model_DT_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_DT[:10])\nprint(\"Actual:\",y_test[:10])","ef9241b9":"get_probabilty_output(X_test=X_test, model_fitted=model_DT_with_best_params)","1f98e46c":"acc_DT,pre_DT,rec_DT = get_classification_report(y_test,predictions_DT)","4595d8eb":"auc_DT = get_classification_ROC(X_test,y_test,model_DT_with_best_params,test_size=0.3,random_state=0)","233101fb":"# model_DT_PCA = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n# model_DT_PCA.fit(X_train_PCA_2, y_train)\n# predictions_DT_PCA = model_DT_PCA.predict(X_test_PCA_2)","d0b95118":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_DT_PCA, model_name=\"Decision Tree Classifier (Training Set)\")","c3591bdc":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_DT_PCA, model_name=\"Decision Tree Classifier (Test Set)\")","f27c0863":"parameters_RF = {\n    'criterion':('gini','entropy'),\n    'max_features': ('auto','sqrt','log2'),\n    'n_estimators': [100,150,200,250,300]\n}\n\n\nmodel_RF = RandomForestClassifier(n_jobs=-1)\nmodel_RF_with_best_params = GridSearchCV(model_RF, parameters_RF)\nmodel_RF_with_best_params.fit(X_train,y_train)\nmodel_RF_best_params = model_RF_with_best_params.best_params_\nmodel_RF_with_best_params.fit(X_train,y_train)","40c49094":"model_RF_best_params","7a35add7":"predictions_RF = model_RF_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_DT[:10])\nprint(\"Actual:\",y_test[:10])","35aed90c":"get_probabilty_output(X_test=X_test, model_fitted=model_RF_with_best_params)","1681a84f":"acc_RF,pre_RF,rec_RF = get_classification_report(y_test,predictions_RF)","da82bbd6":"auc_RF = get_classification_ROC(X_test,y_test,model_RF_with_best_params,test_size=0.3,random_state=0)","7c244d99":"# model_RF_PCA = RandomForestClassifier(n_estimators = 10, criterion=\"entropy\", random_state=0)\n# model_RF_PCA.fit(X_train_PCA_2, y_train)\n# predictions_RF_PCA = model_RF_PCA.predict(X_test_PCA_2)","6b126dd9":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_RF_PCA, model_name=\"Random Forest Classifier (Training Set)\")","347a5957":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_RF_PCA, model_name=\"Random Forest Classifier (Test Set)\")","57dccedf":"result = pd.DataFrame(\n    [[\"LogisticRegression\",auc_LR,acc_LR,pre_LR,rec_LR],\n    [\"kNearestNeighbor\",auc_KNN,acc_KNN,pre_KNN,rec_KNN],\n    [\"SupportVectorClassifier\",auc_SVC,acc_SVC,pre_SVC,rec_SVC],\n    [\"NaiveBayes\",auc_NB,acc_NB,pre_NB,rec_NB],\n    [\"DecisionTree\",auc_DT,acc_DT,pre_DT,rec_DT],\n    [\"RandomForest\",auc_RF,acc_RF,pre_RF,rec_RF]],\n    columns=[\"Classifier\",\"AUC\",\"Accuracy\",\"Precision\",\"Recall\"]\n)\n\nresult","0c7de937":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.AUC\nsns.barplot(x=x, y=y)\nplt.title(\"AUC Score Comparision\")\nplt.show()","447c3ccd":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Accuracy\nsns.barplot(x=x, y=y)\nplt.title(\"Accuracy Comparision\")\nplt.show()","9e7efd63":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Precision\nsns.barplot(x=x, y=y)\nplt.title(\"Precision Comparision\")\nplt.show()","432e9dec":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Recall\nsns.barplot(x=x, y=y)\nplt.title(\"Recall Comparision\")\nplt.show()","e147cabd":"Although, we can see the explained variance is ~46% hence, it is **NOT** suggested to go for just 2 components. But since we need to visualise the the dataset and classification boundries, we will go for 2 components. ","ccc637a4":"### Result","6fa8f543":"### Analysing Feature Distribution","4d5b98e9":"## Data Scaling","12c361fe":"### Predicting","0896bf34":"## Generating Principal Components of the feature dataset ","69bdce4a":"### Fitting model on Principal Component dataset","8d70bf6b":"## Visualisation Through PCA","b008e362":"# Naive Bayes Classifier","cf8e7059":"### Fitting model on Principal Component dataset","8e3b4993":"#### by Logistic Regression, k-Nearest Neighbor, Support Vector Classifier, Naive Bayes, Decision Tree and Random Forest","3f382136":"### Visualisation through PCA","69e3b676":"## Exploring Features ","967909c2":"## Probability Output","90b9b5ea":"### Results","108658f6":"### Fitting model on Principal Component dataset","d6fd34ce":"## Data Splitting","95490a0d":"### Predicting","3c47207b":"### Creating Model","6fd62528":"### Creating Model","01abb6e2":"### Creating Model","fff0c9ba":"### Results","5276f0d9":"# Wine Quality Classification ","aaff3db3":"### Visualising through PCA","06267c5a":"### Predicting","dbec1d67":"# Logisitic Regression ","1208509c":"We can see there are many ","a700be64":"### Visualising PCA","0704b52b":"### Visualising through PCA","f6e28545":"### Analysing Feature Correlation","e6b94611":"It is suggested to make the ```quality``` a binary variable. Let's say for ```quality``` >= 7 is good quality or ```is good``` = 1 and for ```quality``` < 7 is not of good quality or ```is good``` = 0","3da30897":"### Predicting ","86799bd0":"Let's start defining classification models <br> Models which are implemented: <br>\n* Logitic Regression\n* k-Nearest Neighbors\n* Support Vector Classifier\n* Naive Bayes\n* Decision Tree\n* Random Forest","22d92251":"### Results","cc556a45":"### Result","ebd05bae":"# Random Forest Classifier","6d6c63c8":"# Functions for Analysing Results","81ad281b":"# Support Vector Classifier (SVC)","4efa2331":"## Checking for missing values","749304ec":"### Fitting model on Principal Component dataset","9c37b033":"## Classification Report","ac680ef6":"In this notebook, I will be implementing multiple classfication algorithms on a binary (or multiclass if required) classification problem. I created simple functions <br>for generating ROC curve for both binary or multiclass classification (using One-vs-Rest),\n<br>for visualising results in 2 dimensions via Principal Component Analysis,\n<br>for getting classification report,\n<br>for looking classification results row wise along with each class' prediction probabilty.<br><br>\nFeel free to use it as a template for any other classification problem. Do upvote :)","fae5a48b":"## Checking for Dataset skewness","0a23f3c6":"We can see the dataset is skewed (unbalanced). <br>\nOf whole dataset **~5% belong to class 4, 8 and 3 combined**","a889ce16":"### Creating Model","49b7dc75":"### Fitting model on Principal Component dataset","6482e5ec":"# Classification Models","84e720f4":"## Importing Libraries","73021901":"## Classification ROC","a55bb0fa":"### Visualising through PCA","c426933d":"# Final Results","a25dfb49":"## Model Comparision","6f8bdd1e":"### Results","0879915f":"## Reading Dataset","d53c8930":"### Visualising through PCA","b5afe241":"### Predicting","4ea33d63":"# Decision Tree Classifier ","35992796":"### Creating Model","dedca079":"### Predicting","30e7a6ab":"There are no missing values. Pretty clean dataset!","33726366":"### Fitting model on Principal Component dataset","d637211d":"# k-Nearest Neighbor Classifier","d967bd8f":"### Creating Model"}}