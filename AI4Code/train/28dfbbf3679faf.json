{"cell_type":{"34fe07eb":"code","4850d56c":"code","dd88ec2a":"code","d0fbdcda":"code","938b2016":"code","d28c571e":"code","ac7a48d9":"code","07c606fc":"code","1d400124":"code","89d5dbd1":"code","e2c44a23":"code","3d814442":"code","8ed07363":"code","c874ea44":"code","296e66b5":"code","1d7fc85d":"code","d9b33926":"code","8f30367e":"code","e254a5e4":"code","3d928724":"code","ab515fde":"code","db25713f":"code","c0635bfa":"code","f445a650":"code","f08ca53a":"code","370943bd":"code","088b1fc0":"code","bc2cfec5":"code","20cee26e":"code","b2c01f11":"code","3c43ad4b":"markdown","af2355a6":"markdown","295ecd6e":"markdown","7ae670f7":"markdown","33c60e79":"markdown","cbfabf55":"markdown","a16f2bad":"markdown","ea54e2c8":"markdown","9f841b51":"markdown","05410829":"markdown","67bae21a":"markdown","6a73167f":"markdown","83e62081":"markdown","b22b5dcb":"markdown","164db4f4":"markdown","49f1834f":"markdown"},"source":{"34fe07eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4850d56c":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(train.shape)\ntype(train)","dd88ec2a":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(test.shape)","d0fbdcda":"t_train = train['label']\nx_train = train.drop(labels = ['label'], axis = 1)","938b2016":"x_train2 = x_train.to_numpy()\nt_train2 = t_train.to_numpy()\nx_test2 = test.to_numpy()\nx_train2","d28c571e":"def softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) \/ np.sum(np.exp(x), axis=0)\n        return y.T \n\n    x = x - np.max(x) # \uc624\ubc84\ud50c\ub85c \ub300\ucc45\n    return np.exp(x) \/ np.sum(np.exp(x))\n    \ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # \ud6c8\ub828 \ub370\uc774\ud130\uac00 \uc6d0-\ud56b \ubca1\ud130\ub77c\uba74 \uc815\ub2f5 \ub808\uc774\ube14\uc758 \uc778\ub371\uc2a4\ub85c \ubc18\ud658\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size","ac7a48d9":"class Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dW = None\n        self.db = None\n        \n    def forward(self, x):\n        self.x = x\n        out = np.dot(x, self.W) + self.b\n        return out\n        \n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis = 0)\n        return dx\n    # X.shape == grad(X).shape !!!\n    \nclass ReLU:\n    def __init__(self):\n        self.mask = None\n        \n    def forward(self, x):\n        self.mask = (x <= 0) # T\/F \ub85c \ucd9c\ub825.\n        out = x.copy()\n        out[self.mask] = 0 # True\uc778 \uc6d0\uc18c\uc5d0\ub9cc 0\uc73c\ub85c \ub300\uccb4.\n        return out\n    \n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n    \n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n    \n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n    \n    def backward(self, dout = 1):\n        batch_size = self.t.shape[0]\n        dx = (self.y- self.t) \/ batch_size\n        return dx\n\n    # why divided batch size in softmax with loss function's backpropagation ?\n    # https:\/\/stats.stackexchange.com\/questions\/358786\/mean-or-sum-of-gradients-for-weight-updates-in-sgd ----> good!\n    # https:\/\/stats.stackexchange.com\/questions\/183840\/sum-or-average-of-gradients-in-mini-batch-gradient-decent\/183990","07c606fc":"'''\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n        self.params = {}\n        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n        self.params['b2'] = np.zeros(output_size)\n        \n        self.layers = OrderedDict()\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['Relu1'] = ReLU()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        \n        self.lastLayer = SoftmaxWithLoss()\n        \n    def predict(self, x):\n        for layer in self.layer.values():\n            x = layer.forward(x)\n            \n        return(x)\n    \n    def loss(self, x, t):\n        y = self.predict(x)\n        return self.lastLayer.forward(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis = 1)\n        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n        \n        accuracy = np.sum(y == t) \/ float(x.shape[0])\n        return accuracy\n\n    def gredient(self, x, t):\n        \n        # forward\n        self.loss(x, t)\n        \n        # backward\n        dout = 1\n        dout = self.lastLayer.backward(dout)\n        \n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n            \n        grads = {}\n        grads['W1'] = self.layers['Affine1'].dW\n        grads['b1'] = self.layers['Affine1'].db\n        grads['W2'] = self.layers['Affine2'].dW\n        grads['b2'] = self.layers['Affine2'].db\n        \n        return grads\n'''","1d400124":"# net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n# net.params['b1'].shape, net.params['b2'].shape","89d5dbd1":"sample200 = x_train2[ 0:200 , :]\nsample200, sample200.shape\n\ntsample200 = t_train2[ 0:200, ]\ntsample200","e2c44a23":"weight_init_std = 0.01","3d814442":"aff1 = Affine(W = np.random.randn(784,100) * weight_init_std, b = np.zeros(100))","8ed07363":"a1 = aff1.forward(sample200) #(200, 784) * (784, 100) = (200 * 100)\na1, a1.shape\n# aff1.forward(sample200).shape","c874ea44":"relu1 = ReLU()\nz1 = relu1.forward(a1)\nz1.shape, z1","296e66b5":"relu1.mask","1d7fc85d":"aff2 = Affine(W = np.random.randn(100, 10) * weight_init_std, b = np.zeros(10))\na2 = aff2.forward(z1)\na2.shape","d9b33926":"a2","8f30367e":"y1 = softmax(a2)\ny1, y1.shape","e254a5e4":"y1[0,], np.sum(y1[0,])","3d928724":"tsample200 # label \ud615\uc2dd (1D)\n# one hot encoding \ud615\ud0dc\ub85c \ubcc0\uacbd. (2D)\ntsample200_1hot = np.zeros((tsample200.size, tsample200.max() + 1)) # (m, n) array\ntsample200_1hot[np.arange(tsample200.size), tsample200] = 1 ## **** index\ub0b4 \uc778\uc790\ub85c array\uac00 \ub4e4\uc5b4\uac00????? *****\ntsample200, tsample200_1hot","ab515fde":"cross_entropy_error(y1, tsample200_1hot)","db25713f":"'''\nclass SoftmaxWithLoss:\n    ...\n    def backward(self, dout = 1):\n        batch_size = self.t.shape[0] -------(x)\n        dx = (self.y- self.t) \/ batch_size\n        return dx\n'''","c0635bfa":"batch_size = 10 # tsample200_1hot.shape[0] # 200\ndy1 = (y1- tsample200_1hot) \/ batch_size\ndy1","f445a650":"'''\nclass Affine:\n    ...\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis = 0)\n        return dx\n'''","f08ca53a":"dy1\naff2.backward(dy1)\n\naff2.W.T.shape\n\ndx2 = np.dot(dy1,aff2.W.T)\ndW2 = np.dot(aff2.x.T, dy1)\ndb2 = np.sum(dy1, axis = 0)\n\ndx2.shape, dW2.shape, db2.shape#, dx2, dW2, db2","370943bd":"'''\nclass ReLU:\n    ...    \n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx        \n'''","088b1fc0":"dx2","bc2cfec5":"relu1.backward(dx2)\n\ndx2[relu1.mask] = 0\ndr1 = dx2\ndr1 # compare with prev. dx2","20cee26e":"dx1 = np.dot(dr1, aff1.W.T)\ndW1 = np.dot(aff1.x.T, dr1)\ndb1 = np.sum(dr1, axis = 0)\n\ndx1.shape, dW1.shape, db1.shape","b2c01f11":"# dW1, db1, dW2, db2\nlearning_rate = 0.01\n\n# W1 := W1 - learning_rate * dW1\naff1.W - learning_rate * dW1\naff1.b - learning_rate * db1 \naff2.W - learning_rate * dW2\naff2.b - learning_rate * db2 \n","3c43ad4b":"# **[7] update weights and bias**","af2355a6":"mini batch size = 10\n\n<mini batch\ub97c \ud65c\uc6a9\ud55c gradient calculation>\n1. \ubbf8\ub2c8\ubc30\uce58 \ud68d\ub4dd (\ubbf8\ub2c8\ubc30\uce58 \uc0ac\uc774\uc988\ub9cc\ud07c \ub370\uc774\ud130 sampling)\n2. \uae30\uc6b8\uae30 \uacc4\uc0b0\n3. \ub9e4\uac1c\ubcc0\uc218 \uac31\uc2e0\n4. \ud559\uc2b5 \uacbd\uacfc \uae30\ub85d\n\n! gradient \uc21c\uc11c : loss ft > aff2 > relu > aff1","295ecd6e":"## [6-3] backward(relu1)","7ae670f7":"## [6-4] backward(aff1)","33c60e79":"## [6-2] backward(aff2)","cbfabf55":"# **[2] Relu**","a16f2bad":"## [6-1] backward(loss functions)","ea54e2c8":"# **[1] Affine1**","9f841b51":"# **sampling (0:200)**","05410829":"# **define functions**","67bae21a":"# FOR NOVICE 1 (including me) - Deep Learning from Scratch ch1 ~ ch5 (my exercise)\n\n! I'm novice, either :)\n\n**The purpose of this notebook is to understand the basics of how neural networks work while implementing code. The basic neural network implemented the Two Layer Network introduced in the book using a minimal function of forward, backward, and weight updates.**\n\nThis notebook implemented the contents of deep learning from scratch 1 to chapter 5 with MNIST. I omitted the detailed explanation of the code, but if you have an approximate understanding of the contents of the book, it is expected that you will understand it by following this code in order. If you learn step-by-step while understanding how the code works, you should be able to understand most of the content up to Chapter 5.\n\nThe actual MNIST set was implemented step by step with code centered on numpy, without using existing packages or modules. All the codes are codes I studied while referring mostly to the codes provided in the book. The code was not created by me, so feel free to use it for learning whenever you need it.\n\nIn the first part, I tried to implement the calculation process in the function sequentially, without using each of the functions defined in the book (ex. def gradient(self, x, f): ... ). (As you look at the code below, function definitions are already done, but the function in the 'TwoLayerNet' class is not used.)\n\nAfter understanding numerical_gradient, please move on to gradient(backpropagation).\n\n> 1. After sampling 200 in TRAIN, the weight was assigned to random normal n and fit. After that, we focused on the process from calculating and updating the gradient of the loss function using the error wave inversion method.\n> 2. The process of updating weights in mini-batch (batch size = 10) in the entire TRAIN has been implemented one by one. In this case, the previously defined functions (source: the code of the book) were used. \n\nI will add more as I study little by little.\n\n!Please understand that my native language is not English.\n\n**!if you fine something wrong in my code, please feel free to let me know!!**\n\n----------------------------------------------------------------------------------------\n**\uc774 \ub178\ud2b8\ubd81\uc740 \uc81c\uac00 \uacf5\ubd80\ud55c \ucf54\ub4dc\ub97c \uacf5\uc720\ud55c \uac83\uc785\ub2c8\ub2e4**\n\n! \uc800\ub3c4 \ucd08\uc2ec\uc790\uc785\ub2c8\ub2e4..\u314e\n\n**\uc774 \ub178\ud2b8\ubd81\uc758 \ubaa9\uc801\uc740 \ucf54\ub4dc\ub97c \uad6c\ud604\ud558\uba74\uc11c \uae30\ucd08\uc801\uc778 \uc2e0\uacbd\ub9dd \uc791\ub3d9 \uc6d0\ub9ac\ub97c \uc774\ud574\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uae30\ucd08\uc801\uc778 \uc2e0\uacbd\ub9dd\uc740 \ucc45\uc5d0\uc11c \uc18c\uac1c\ud55c Two Layer Network\ub97c forward, backward \uadf8\ub9ac\uace0 \uac00\uc911\uce58 \uc5c5\ub370\uc774\ud2b8\uae4c\uc9c0\uc758 \ub0b4\uc6a9\uc744 \ucd5c\uc18c\ud55c\uc758 \ud568\uc218\ub97c \uc0ac\uc6a9\ud574 \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4.**\n\n\uc774 \ub178\ud2b8\ubd81\uc740 deep learning from scratch 1\uc758 \ub0b4\uc6a9 \uc911 chapter 5\uae4c\uc9c0 \ub0b4\uc6a9\uc744 MNIST\ub85c \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. \ucf54\ub4dc\uc758 \uc790\uc138\ud55c \uc124\uba85\uc740 \uc0dd\ub7b5\ud558\uc600\uc9c0\ub9cc, \ucc45\uc758 \ub0b4\uc6a9\uc744 \ub300\ub7b5\uc801\uc73c\ub85c \uc774\ud574\ud558\uc168\ub2e4\uba74 \uc774 \ucf54\ub4dc\ub97c \uc21c\uc11c\ub300\ub85c \ub530\ub77c\uac00\uba74\uc11c \uc774\ud574\ud560 \uac83\uc73c\ub85c \uae30\ub300\ub429\ub2c8\ub2e4. \ucf54\ub4dc\uac00 \uc2e4\ud589\ub418\ub294 \uc6d0\ub9ac\ub97c \uc774\ud574\ud558\uba74\uc11c \ucc28\uadfc\ucc28\uadfc \ud559\uc2b5\ud558\uc2e0\ub2e4\uba74 5\uc7a5\uae4c\uc9c0\uc758 \ub0b4\uc6a9\uc744 \ub300\ubd80\ubd84 \uc54c \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4.\n\n\uc2e4\uc81c MNIST set\uc744 \ud604\uc874\ud558\ub294 \ud328\ud0a4\uc9c0\ub098 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0, numpy\ub97c \uc911\uc2ec\uc73c\ub85c \ucf54\ub4dc\ub85c \ud55c \uc2a4\ud15d\uc529 \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \ucf54\ub4dc\ub294 \ucc45\uc5d0\uc11c \uc81c\uacf5\ud55c \ucf54\ub4dc\ub97c \ub300\ubd80\ubd84 \ucc38\uace0\ud558\uba74\uc11c \uc81c\uac00 \uacf5\ubd80\ud55c \ucf54\ub4dc\uc785\ub2c8\ub2e4. \ucf54\ub4dc\ub294 \uc81c\uac00 \ucc3d\uc791\ud55c \uac83\uc774 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 \ud544\uc694\ud558\uc2dc\uba74 \uc5b8\uc81c\ub4e0\uc9c0 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. \n\n\ub610\ud55c, \uccab \ubd80\ubd84\uc740 \ucc45\uc5d0\uc11c \uc815\uc758\ud55c \uac01 \ud568\uc218\ub4e4 (ex) def gradient(self, x, f): ... )\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0, \ud568\uc218 \uc548\uc758 \uacc4\uc0b0 \uacfc\uc815\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \uad6c\ud604\ud558\ub3c4\ub85d \ub178\ub825\ud588\uc2b5\ub2c8\ub2e4. (\uc544\ub798 \ucf54\ub4dc\ub97c \ubcf4\uba74, \ud568\uc218 \uc815\uc758\ub294 \ubbf8\ub9ac \ud588\uc9c0\ub9cc TwoLayerNet class \ub0b4 \ud568\uc218\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc74c.)\n\nnumerical_gradient\ub97c \uc774\ud574\ud55c \ub4a4 gradient(backpropagation)\ub85c \ub118\uc5b4\uac00\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n\n> 1. TRAIN\uc5d0\uc11c 200\uac1c\ub97c sampling \ud55c\ub4a4 \uac00\uc911\uce58\ub294 normal random n\ub85c \ubd80\uc5ec\ud55c \ub4a4 \uc801\ud569\ud558\uc600\uc2b5\ub2c8\ub2e4. \uadf8 \ub4a4, \uc624\ucc28\ud30c\uc5ed\uc804\ubc95\uc73c\ub85c loss function\uc758 gradient\ub97c \uacc4\uc0b0\ud558\uace0 \uac31\uc2e0\ud558\ub294 \uac83 \uae4c\uc9c0 \uacfc\uc815\uc5d0 \uc9d1\uc911\ud558\uc600\uc2b5\ub2c8\ub2e4.\n> 2. \uc804\uccb4 TRAIN\uc5d0\uc11c \ubbf8\ub2c8\ubc30\uce58(batch size = 10)\ub85c \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud558\ub294 \uacfc\uc815\uc744 \ud558\ub098\ud558\ub098 \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub54c\ub294 \uc55e\uc11c \uc815\uc758\ud55c \ud568\uc218\ub4e4 (\ucd9c\ucc98 : \ucc45\uc758 \ucf54\ub4dc)\uc744 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.\n\n\uc870\uae08\uc529 \uacf5\ubd80\ud574\uac00\uba74\uc11c \ucd94\uac00\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n**\ud2c0\ub9b0 \ubd80\ubd84 \uc788\uc73c\uba74 \ud3b8\ud558\uac8c \ub9d0\uc500\ud574\uc8fc\uc138\uc694!**\n\n![fig1](http:\/\/www.kaggle.com\/zizonpingu\/mnist-for-novice-fig-1)\n","6a73167f":"# **[5] loss function(cross entropy)**","83e62081":"# **[6] calculate loss functions's gradient**","b22b5dcb":"## before reading\n\ntrain data_size = 200\n\nbatch_size = 10\n\n**X, W, B :**\n\n    Affine > Relu > Affine > Softmax > loss(cross entropy) > gradient > W, B update >>>>> Affine > ..iter..> Softmax > result\n    |                                                                                |\n    |___________________________________one cycle____________________________________|\n\n    **-obj names definition-**\n    At first iteration,\n    class(class names)          : aff1 >   relu1    >   aff2               (ft) -> softmax -> cross_entropy_error\n    class's forward(obj names)  : a1   >   r1       >   a2                 (ft) -> y1\n    class's backward(obj names) : dy1  >   dx2      >   dr1   >   dx1 \n                                       dW2, db2               dW1, db1\n\n\n**===two layers===**\n\n    1 picture - 784 size.\n\n    input_size = 784 \\ hidden_size = 100 \\ output_size = 10 (0-9)\n\n    so, the structure of first iter...\n\n    --|input|--------|aff1|--------|relu1|--------|aff2|-------|softmax)|----|output|--\n\n    (200, 784) --> (784, 100) --> (100, 100) --> (100, 10) --> (10, 10) --> (200, 10)","164db4f4":"# **[3] Affine2**","49f1834f":"# **[4] Sortmax**"}}