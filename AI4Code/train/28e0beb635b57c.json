{"cell_type":{"cbe934d1":"code","fc6262e5":"code","5e9e8203":"code","615c0359":"code","82cf295d":"code","8c580092":"code","360454b5":"code","4f3d16f6":"code","ff4c36ca":"code","8bdefba4":"code","115986e0":"code","39521856":"code","8c964792":"code","4955ea91":"code","0ca2a0ff":"code","89a59446":"code","d181959b":"markdown","3574fb51":"markdown","16535b9e":"markdown","a2497391":"markdown","4b54ab41":"markdown","0d66065e":"markdown","890eb590":"markdown","9d47f45e":"markdown","99da71ee":"markdown","bd8745b1":"markdown","f03d49f0":"markdown","24e55410":"markdown","de9c8fd6":"markdown"},"source":{"cbe934d1":"import numpy as np\nimport cv2\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\nfrom keras.models  import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\nimport random,os,glob\nimport matplotlib.pyplot as plt","fc6262e5":"dir_path = '..\/input\/garbage classification\/Garbage classification'","5e9e8203":"img_list = glob.glob(os.path.join(dir_path, '*\/*.jpg'))","615c0359":"len(img_list)","82cf295d":"train=ImageDataGenerator(horizontal_flip=True,\n                         vertical_flip=True,\n                         validation_split=0.1,\n                         rescale=1.\/255,\n                         shear_range = 0.1,\n                         zoom_range = 0.1,\n                         width_shift_range = 0.1,\n                         height_shift_range = 0.1,)\n\ntest=ImageDataGenerator(rescale=1\/255,\n                        validation_split=0.1)\n\ntrain_generator=train.flow_from_directory(dir_path,\n                                          target_size=(300,300),\n                                          batch_size=32,\n                                          class_mode='categorical',\n                                          subset='training')\n\ntest_generator=test.flow_from_directory(dir_path,\n                                        target_size=(300,300),\n                                        batch_size=32,\n                                        class_mode='categorical',\n                                        subset='validation')\n\nlabels = (train_generator.class_indices)\nprint(labels)\n\nlabels = dict((v,k) for k,v in labels.items())\nprint(labels)","8c580092":"for image_batch, label_batch in train_generator:\n  break\nimage_batch.shape, label_batch.shape","360454b5":"print (train_generator.class_indices)\n\nLabels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n\nwith open('labels.txt', 'w') as f:\n  f.write(Labels)\n","4f3d16f6":"model=Sequential()\n#Convolution blocks\n\nmodel.add(Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n#model.add(SpatialDropout2D(0.5)) # No accuracy\n\nmodel.add(Conv2D(64,(3,3), padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n#model.add(SpatialDropout2D(0.5))\n\nmodel.add(Conv2D(32,(3,3), padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n\n#Classification layers\nmodel.add(Flatten())\n\nmodel.add(Dense(64,activation='relu'))\n#model.add(SpatialDropout2D(0.5))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,activation='relu'))\n\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6,activation='softmax'))\n\nfilepath=\"trained_model.h5\"\ncheckpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint1]\n\n","ff4c36ca":"model.summary()","8bdefba4":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc']) # RMS PROP - No accuracy\n\n#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n","115986e0":"history = model.fit_generator(train_generator,\n                              epochs=100,\n                              steps_per_epoch=2276\/\/32,\n                              validation_data=test_generator,\n                              validation_steps=251\/\/32,\n                              workers = 4,\n                              callbacks=callbacks_list) \n#41 epoch - 75% #73- 76.9%\n#78 epoch - 80%","39521856":"from keras.preprocessing import image\n\nimg_path = '..\/input\/garbage classification\/Garbage classification\/plastic\/plastic75.jpg'\n\nimg = image.load_img(img_path, target_size=(300, 300))\nimg = image.img_to_array(img, dtype=np.uint8)\nimg=np.array(img)\/255.0\n\nplt.title(\"Loaded Image\")\nplt.axis('off')\nplt.imshow(img.squeeze())\n\np=model.predict(img[np.newaxis, ...])\n\n#print(\"Predicted shape\",p.shape)\nprint(\"Maximum Probability: \",np.max(p[0], axis=-1))\npredicted_class = labels[np.argmax(p[0], axis=-1)]\nprint(\"Classified:\",predicted_class)\n\n","8c964792":"classes=[]\nprob=[]\nprint(\"\\n-------------------Individual Probability--------------------------------\\n\")\n\nfor i,j in enumerate (p[0],0):\n    print(labels[i].upper(),':',round(j*100,2),'%')\n    classes.append(labels[i])\n    prob.append(round(j*100,2))\n    \ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(classes))\n    plt.bar(index, prob)\n    plt.xlabel('Labels', fontsize=12)\n    plt.ylabel('Probability', fontsize=12)\n    plt.xticks(index, classes, fontsize=12, rotation=20)\n    plt.title('Probability for loaded image')\n    plt.show()\nplot_bar_x()","4955ea91":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# ________________ Graph 1 -------------------------\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\n# ________________ Graph 2 -------------------------\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,max(plt.ylim())])\nplt.title('Training and Validation Loss')\nplt.show()","0ca2a0ff":"import tensorflow as tf\nimport keras\nfile = \"Garbage.h5\"\nkeras.models.save_model(model,file)\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(file)\ntflite_model=converter.convert()\nopen(\"garbage.tflite\",'wb').write(tflite_model)","89a59446":"from IPython.display import FileLinks\nFileLinks('.')","d181959b":"# Getting files from kernel","3574fb51":"## Dataset Input","16535b9e":"[Callback model Checkpoint Reference](https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/)","a2497391":"### Writing the labels file","4b54ab41":"# Import Libraries","0d66065e":"# Let's Train","890eb590":"# Testing PREDICTION \n##### Note: Path is of training dataset (pl. don't mind)","9d47f45e":"# Image Augmentation","99da71ee":"# Accuracy Graph","bd8745b1":"### Compiling Model using categorical cross entropy loss function & Adam Optimizer","f03d49f0":"# Building CNN & Saving keras model","24e55410":"## Converting to TFLite\n#### Note: Image Size is 300","de9c8fd6":"# Summarizing our model"}}