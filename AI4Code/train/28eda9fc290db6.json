{"cell_type":{"31e28141":"code","bfc8165e":"code","1360f81c":"code","c6fa0053":"code","4fe6b3a0":"code","fbc522c9":"code","44833672":"code","27ccfb3a":"code","4e58bb77":"code","44f7a2f6":"code","5575f6a1":"code","b19c45bb":"code","2b822af1":"code","a63f027d":"code","afa07b08":"code","45063a80":"code","ee41c8e0":"code","24798465":"code","954bb029":"code","77356866":"code","ef1907b4":"code","d255cfa2":"code","b01eda93":"code","2baed1ab":"code","8c33331b":"code","90d4d93b":"code","ba798b0b":"code","11ac9375":"code","a755b3ff":"code","e9cbac50":"code","044a68ec":"markdown","f6d1de9d":"markdown","16921c91":"markdown","5e18df44":"markdown","5d0003f9":"markdown","afafa7d3":"markdown","b859c3fd":"markdown","2f3039dc":"markdown","eaf0d4fd":"markdown","207bb753":"markdown","6d0a5279":"markdown","67618e12":"markdown","df0362fb":"markdown","62741ca0":"markdown","a9b001b9":"markdown","b3b2d38d":"markdown","379e11cf":"markdown","7eef2a5b":"markdown","7fd40051":"markdown","ab4503d5":"markdown","8449ef7b":"markdown","0dcbd194":"markdown","42b02fd9":"markdown","1f2147c8":"markdown","40af6520":"markdown","cd51659a":"markdown","d8d3ba86":"markdown","e84826ee":"markdown","f1a6f149":"markdown","daf775a5":"markdown","6535acd4":"markdown","94b1f834":"markdown","eb4131f3":"markdown","ba014e96":"markdown","3cfa1eac":"markdown","6ce0c0ce":"markdown"},"source":{"31e28141":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.datasets import mnist\nimport keras.utils.np_utils as ku\nimport keras.models as models\nimport keras.layers as layers\nfrom keras import regularizers\nimport numpy.random as nr\n\nimport keras\nfrom keras.layers import Dropout\nfrom keras.optimizers import rmsprop, Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bfc8165e":"train=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain.head()","1360f81c":"target=train['label']\ntrain.drop('label',axis=1, inplace=True)\n","c6fa0053":"plt.figure(figsize=(15,5))\nsns.countplot(target, color='crimson')\nplt.title('The distribution of the digits in the dataset', weight='bold', fontsize='18')\nplt.xticks(weight='bold', fontsize=16)\nplt.show()","4fe6b3a0":"train=train\/255\ntest=test\/255","fbc522c9":"Train=train.values.reshape(-1,28,28,1)\nTest=test.values.reshape(-1,28,28,1)","44833672":"plt.figure(figsize=(15,8))\nfor i in range(60):\n    plt.subplot(6,10,i+1)\n    plt.imshow(Train[i].reshape((28,28)),cmap='binary')\n    plt.axis(\"off\")\nplt.show()","27ccfb3a":"Target=ku.to_categorical(target, num_classes=10)","4e58bb77":"print(\"The shape of the labels before One Hot Encoding\",target.shape)\nprint(\"The shape of the labels after One Hot Encoding\",Target.shape)\nprint(\"We have 10 columns for the 10 digits\")","44f7a2f6":"print(\"Shape of the first image with label: '1' after OHE\")\nprint(Target[0])","5575f6a1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(Train, Target, test_size=0.10, random_state=42)","b19c45bb":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","2b822af1":"nn=models.Sequential()","a63f027d":"## Add some convolutional layers to extract features = Feature map\n\nnn.add(layers.Conv2D(16, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nnn.add(layers.MaxPooling2D((2, 2)))\nnn.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nnn.add(layers.MaxPooling2D((2, 2)))\nnn.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\nnn.add(layers.MaxPooling2D((2, 2)))\nnn.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\nnn.add(layers.MaxPooling2D((2, 2)))\n\n\n## latten the output of the convolutional layers so that fully connected network can be applied\nnn.add(layers.Flatten())\n\n## Finally, fully connected layers to classify the digits using the extracted features\nnn.add(layers.Dense(64, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\nnn.add(Dropout(0.5))\nnn.add(layers.Dense(64, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\nnn.add(Dropout(0.5))\nnn.add(layers.Dense(10, activation = 'softmax'))\n\nnn.summary()","afa07b08":"nn1=models.Sequential()","45063a80":"## Add some convolutional layers to extract features = Feature map\nnn1=models.Sequential()\nnn1.add(layers.Conv2D(16, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nnn1.add(BatchNormalization())\nnn1.add(layers.MaxPooling2D((2, 2)))\nnn1.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nnn1.add(BatchNormalization())\nnn1.add(layers.MaxPooling2D((2, 2)))\nnn1.add(BatchNormalization())\nnn1.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\nnn1.add(BatchNormalization())\nnn1.add(layers.MaxPooling2D((2, 2)))\n\n## latten the output of the convolutional layers so that fully connected network can be applied\nnn1.add(layers.Flatten())\n\n## Finally, fully connected layers to classify the digits using the extracted features\nnn1.add(layers.Dense(64, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\nnn1.add(Dropout(0.5))\nnn1.add(layers.Dense(64, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\nnn1.add(Dropout(0.5))\nnn1.add(layers.Dense(10, activation = 'softmax'))\n\nnn1.summary()","ee41c8e0":"nn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","24798465":"nn1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","954bb029":"filepath = 'my_model_file.hdf5' # define where the model is saved\ncallbacks_list = [\n        keras.callbacks.EarlyStopping(\n            monitor = 'val_loss', # Use accuracy to monitor the model\n            patience = 5 # Stop after 5 steps with lower accuracy\n        ),\n        keras.callbacks.ModelCheckpoint(\n            filepath = filepath, # file where the checkpoint is saved\n            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n            save_best_only = True)]# Only save model if it is the best","77356866":"datagen = ImageDataGenerator(\n        rotation_range=15,\n        zoom_range = 0.15,\n        width_shift_range=0.15,\n        height_shift_range=0.15)\ndatagen.fit(x_train)","ef1907b4":"history = nn.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n                              epochs = 25, validation_data = (x_test,y_test),\n                              steps_per_epoch=len(x_train) \/ 128, \n                              callbacks=callbacks_list)","d255cfa2":"history2 = nn1.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n                              epochs = 25, validation_data = (x_test,y_test),\n                              steps_per_epoch=len(x_train) \/ 128, verbose=1,\n                              callbacks=callbacks_list)","b01eda93":"predicted = nn.predict(x_test)","2baed1ab":"predicted2 = nn1.predict(x_test)","8c33331b":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#First Model\nax1 = plt.subplot2grid((2,2),(0,0))\ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\nx = list(range(1, len(test_loss) + 1))\nplt.plot(x, test_loss, color = 'cyan', label = 'Test loss')\nplt.plot(x, train_loss, label = 'Training losss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Model 1: Loss vs. Epoch',weight='bold', fontsize=18)\nax1 = plt.subplot2grid((2,2),(0,1))\ntrain_acc = history.history['accuracy']\ntest_acc = history.history['val_accuracy']\nx = list(range(1, len(test_acc) + 1))\nplt.plot(x, test_acc, color = 'cyan', label = 'Test accuracy')\nplt.plot(x, train_acc, label = 'Training accuracy')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Accuracy vs. Epoch', weight='bold', fontsize=18)  \n\n#Second Model\n\nax1 = plt.subplot2grid((2,2),(1,0))\ntrain_loss = history2.history['loss']\ntest_loss = history2.history['val_loss']\nx = list(range(1, len(test_loss) + 1))\nplt.plot(x, test_loss, color = 'cyan', label = 'Test loss')\nplt.plot(x, train_loss, label = 'Training losss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Model 2: Loss vs. Epoch',weight='bold', fontsize=18)\n\nax1 = plt.subplot2grid((2,2),(1,1))\ntrain_acc = history2.history['accuracy']\ntest_acc = history2.history['val_accuracy']\nx = list(range(1, len(test_acc) + 1))\nplt.plot(x, test_acc, color = 'cyan', label = 'Test accuracy')\nplt.plot(x, train_acc, label = 'Training accuracy')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Model 2: Accuracy vs. Epoch', weight='bold', fontsize=18)  \n\nplt.show()","90d4d93b":"from sklearn.metrics import confusion_matrix\n\ny_class = np.argmax(predicted, axis = 1) \n\ny_check = np.argmax(y_test, axis = 1) \n\ncmatrix = confusion_matrix(y_check, y_class)\n\nplt.figure(figsize=(15,8))\nplt.title('Confusion matrix of the test\/predicted digits ', weight='bold', fontsize=18)\nsns.heatmap(cmatrix,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n","ba798b0b":"#We use np.argmax with y_test and predicted values: transform them from 10D vector to 1D\nclass_y = np.argmax(y_test,axis = 1) \nclass_num=np.argmax(predicted, axis=1)\n#Detect the errors\nerrors = (class_num - class_y != 0)\n#Localize the error images\npredicted_er = predicted[errors]\ny_test_er = y_test[errors]\nx_test_er = x_test[errors]\n\n                \n#Plot the misclassified numbers\nplt.figure(figsize=(15,9))\n\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    plt.imshow(x_test_er[i].reshape((28,28)),cmap='binary')\n    plt.title( np.argmax(predicted_er[i]), size=13, weight='bold', color='red')\n    plt.axis(\"off\")\n\n\nplt.show()","11ac9375":"plt.figure(figsize=(15,8))\n\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    plt.imshow(Train[i].reshape((28,28)),cmap='binary')\n    plt.axis(\"off\")\n\nplt.show()","a755b3ff":"final = nn.predict(Test)\nfinal = np.argmax(final,axis = 1)\nfinal = pd.Series(final, name=\"Label\")","e9cbac50":"submission = pd.concat([pd.Series(range(1,len(Test)+1),name = \"ImageId\"),final],axis = 1)\n\nsubmission.to_csv(\"CNN_digit_recognizer.csv\", index=False)\n","044a68ec":">### 3.1 Loss and accuracy","f6d1de9d":"Good news! Just few misclassified digits, we can see that most of the mistakes are between digits that are kind of similar \n\n>### 3.3 Examples of misclassied digits:\n\n* Here we have 36 misclassified digits:","16921c91":"After reshaping the features and encoding the labels, we split them to train and test sets\n","5e18df44":"# 1- Introduction:\n\nIn this competition, the goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. 10 digits are represented in this dataset [0 1 2 3 4 5 6 7 8 9 ]. We will try to preprocess the images and the labels and then feed the convolutional neural nets with those preprocessed images to classify them. The output will be the prediction of the digit represented in the image.","5d0003f9":"**Early Stopping terminated the training at 29 epochs (Check the output of the cell above)**. The model reached 99% accuracy in the 27th epoch but didn't improve afterwards.","afafa7d3":"> ## 2.1 Building the model:","b859c3fd":">Create our CSV file and submit to competition","2f3039dc":">### 3.2 Confusion matrix","eaf0d4fd":"# 3 CNN Model Evaluation","207bb753":">### 2.4 Training the model:","6d0a5279":"Models created with Keras, and most other deep learning frameworks, operate on floating point numbers. The gray scale pixel values of the images are coded as integers in the range  {0,255}\n\n* These pixel values must be coerced to floating point and then standardized to be in a range  {0.0,1.0}\n\n* As is the case for training many machine learning models, it is best to use standardized values for training deep neural networks.\n\n","67618e12":"Here is a glimpse of what we will be dealing with:\n\n* Images of handwritten Digits from 0 to 9 \n* We will feed those images to the CNN in order to learn and predict the test images.\n* ** We have below an example of 60 digit images from this dataset**","df0362fb":"We have the predicted values in red colors along with the misclassified images. What's interesting here is that the error is more from the humans than from the machine, some digits are written in an uncomprehensive way, it's difficult even for us to read some of these handwritings. I would have guessed the same as the machine in many of these cases.\n\n* The digits shown in the section \"**1.3 Data visualization**\" are readable and clear in comparison to these misclassified digits. We plot them again in the figure below to notice the difference.","62741ca0":"The labels are coded as integers corresponding to the digit in the image. We use One Hot Encoding to encode each label","a9b001b9":"# 4 Submission","b3b2d38d":"# COMPONENTS OF THIS PROJECT\n### 1- Introduction\n\n### 1- Data exploration\n\n### 2- Convolutional neural networks\n\n### 3- CNN model evaluation","379e11cf":"> **2 CNN model: With Batch normalization**\n* Batch normalization is used to increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n* It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer\u2019s activations. \n","7eef2a5b":">Finally, we are done with our analysis. We fit the models on the test data\n","7fd40051":">### 2.3 Data augmentation:\n\nIn order to avoid overfitting, we generate new images out of the existing images with some changes (rotation, zoom, width and height shift)","ab4503d5":"\n> ## 2.1 Model parameters:\n>### Regularization:\n\n>> #### l2 Regularization\nWe will use l2 regularization, l2 regularization applies a penalty proportional to the l2 or Euclidean norm of the model weights to the loss function (Also called Ridge regression in sklearn).\n\n> #### Dropout\nWe will also apply dropout regularization to train our CNN. We'll  define a neural network with a dropout layer with  \ud835\udc5d=0.5 (Half of the nodes are dropped).\n\n>### Activation functions\n>>* The **ReLu** function:  rectilinear activation function is used for the hidden units.\n\n>>* The **softmax** activation function is used for multi-class classifiers (Classification of 10 digits)\n\n>### Padding\nThe coverage of the convolution opertor can be expanded by zero padding in the spatial dimenstion. With the zero padding added, the convolution operator covers the entire spatial dimension of the input tensor. \n\n>>* We add zero padding to convolutional layers with the parameter **padding = same**. It adds zero padding\n\n>### CNN Architecture\n* A multi-layer convolutional neural network to create a **feature map.**\n> * **Con2D** is used to build feature maps from the data\n> * **MaxPooling** is used to reduce dimensionality. In MaxPooling, the output value is just the maximum of the input values in each patch (for ex. The maximum pixel in a span of 3 pixels).\n* A fully-connected hidden layer to perform the **classification.**\n* An output layer to **identify the digits**.\n","8449ef7b":"The table shows the 784 pixels of the pictures with the digits. We will need some preprocessing to reshape and scale the values.","0dcbd194":"We have here the distribution of the digits in this dataset. They are all equally represented. We don't have any class imbalance here so we can go forward with our preprocessing.","42b02fd9":"> ## 1.1 Target and features\n\n\nWe set the target and the features:\n* Target: Labels\n* Features: train set without labels","1f2147c8":"# 2 Convolutional neural networks","40af6520":"* **Model 1**: The train\/test accuracy and loss results look good. Our model didn't overfit.\n* **Model 2**: Batch normalization reduced overfitting.\n* Next, we check the confusion matrix to see the mistakes done by the classifier","cd51659a":"We start by opening our train and test datasets","d8d3ba86":"> ### 1.2 Standardize the values","e84826ee":"> **1 CNN model: Without Batch normalization**","f1a6f149":"> ### 1.4 Encoding target:","daf775a5":">### 2.5 Digits prediction","6535acd4":"> ### 1.5 Split: Train\/Test","94b1f834":"### Optimization:\n\nWe will use **RMSprop**:\n* RMSprop accumulates a measure of the squared gradient to change the learning rate. An exponential decay is applied to the accumulated squared gradient to ensure that more recent experience dominates the learning rate.\n\n","eb4131f3":"** Early stopping** terminates the training of the neural network model at an epoch before it becomes over-fit. We set the patience to epochs=5\n * The first callback, EarlyStopping, is for the early stopping method.\n * The second call back checkpoints or saves the current model.","ba014e96":"> ### 1.3 Data visualization","3cfa1eac":"Effectively, the misclassified digit are not clear in comparison with the rest of the digits. We reached a 99% accuracy and it could have been higher if the misclassified digits were written in a comprehensive way.","6ce0c0ce":"> **(Check the output button for the training description)**"}}