{"cell_type":{"7c11f2ec":"code","8fed36a0":"code","dc8e01c4":"code","213207e6":"code","211ebc68":"code","bc8208f4":"code","111a8bd0":"code","fba3c4ee":"code","44d817bc":"code","09f10491":"code","a993da0e":"code","5b52408b":"code","071e06e6":"code","4bf3b5c6":"code","4b08ec6c":"code","45686a3e":"code","9d2acc60":"code","904f94c2":"code","e7c48177":"code","09dda95c":"code","e7c7ef0f":"code","9fa5f73e":"code","ec54d9cf":"markdown","a5239b8d":"markdown","84326c97":"markdown","a155ffe0":"markdown","04863aa1":"markdown","be1e487c":"markdown","3bd7e98f":"markdown","0799df44":"markdown","d998950f":"markdown"},"source":{"7c11f2ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport keras.backend as K\nfrom keras.layers import Input, Dense, Activation, Reshape\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport re\n\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nPATH= '..\/input'\nprint(os.listdir(f\"{PATH}\"))\n\n# Any results you write to the current directory are saved as output.","8fed36a0":"#df_raw = pd.read_csv(f'{PATH}\/ara.txt', delimiter='\\t', error_bad_lines=False, header=None, names=['en', 'ar'])\ndf_raw = pd.read_csv(f'{PATH}\/fra.txt', delimiter='\\t', error_bad_lines=False, header=None, names=['en', 'fr'])","dc8e01c4":"df_raw.head()","213207e6":"lang1 = 'en'\nlang2 = 'fr' # 'ar'\n#english_max_sentence_length = 250\n#arabic_max_sentence_length = 260\ndf_raw.count()[0], df_raw[lang1].str.len().max(), df_raw[lang2].str.len().max()","211ebc68":"# english characters \nenglish_chars = [char for char in 'abcdefghijklmnopqrstuvwxyz1234567890']\nenglish_chars_num = len(english_chars)\nenglish_char_stop = np.zeros((english_chars_num))\nenglish_cleanup_regex = '[?!\\.,\/]'\n\n# arabic characters \narabic_chars = [char for char in '1234567890\u0627\u0628\u062a\u062b\u062c\u062d\u062e\u062f\u0630\u0631\u0632\u0633\u0634\u0635\u0636\u0637\u0638\u0639\u063a\u0641\u0642\u0643\u0644\u0645\u0646\u0647\u0648\u064a\u0621\u0622\u0671\u0623\u0625\u0629\u0624\u0626\u0649']\narabic_chars_num = len(arabic_chars)\narabic_char_stop = np.zeros((arabic_chars_num))\narabic_cleanup_regex = '[\u0640\u060c\u061b\u061f\u066b\u066c\u0660]'\n\n# french characters \nfrench_chars = [char for char in 'abcdefghijklmnopqrstuvwxyz1234567890']\nfrench_chars_num = len(english_chars)\nfrench_char_stop = np.zeros((english_chars_num))\nfrench_cleanup_regex = '[?!\\.,\/]'\n\n# parameters for the source and destination languages\nparams = {\n    'en': {\n        'chars': english_chars,\n        'chars_num': english_chars_num,\n        'char_stop': english_char_stop,\n        'cleanup_regex': english_cleanup_regex, \n        'max_sentence_length': 70\n    },\n    'fr': {\n        'chars': french_chars,\n        'chars_num': french_chars_num,\n        'char_stop': french_char_stop,\n        'cleanup_regex': french_cleanup_regex, \n        'max_sentence_length': 70\n    }\n}\n","bc8208f4":"# transform a sentence into an matrix of dimension (max_sentence_en, num_english_characters)\ndef one_hot_encoding(encodings, sentence, lang):\n    \"\"\"one hot encoding using the given characters and encoding length\n    \"\"\"\n    if lang not in params.keys():\n        print('unknown language', lang)\n        return\n    p = params[lang]\n    chars, length, max_len, stop_char = p['chars'], p['chars_num'], p['max_sentence_length'], p['char_stop']\n    for index in range(len(sentence)):\n        char = sentence[index]\n        # consider only characters in dictionnary\n        if char in chars:\n            idx = chars.index(char)\n            onehot = np.zeros((length))\n            onehot[idx] = 1\n            encodings[index] = onehot\n    # append padding to the end of the encoded sentence\n    padding = max_len - len(sentence)\n    for i in range(padding):\n        encodings[len(sentence) + i] = stop_char\n\n# shift the given encoding array to the right\ndef shift_right(encodings, lang):\n    # check if the language is known or not\n    if lang not in params.keys():\n        print('unknown language', lang)\n        return\n    target, stop = [], []\n    p = params[lang]\n    target = np.zeros((num_pairs, p['max_sentence_length'], p['chars_num']))\n    stop = p['char_stop']    \n    # shift\n    rows, sentences, chars = encodings.shape\n    for r in range(rows):\n        for s in range(sentences-1):\n            target[r][s] = encodings[r][s+1]\n        target[r][sentences-1] = stop\n    return target\n","111a8bd0":"# for each language\nlanguages = params.keys()\nfor lang in languages:\n    # lowercase everything\n    df_raw[lang] = df_raw[lang].str.lower()\n    # trim sentences\n    df_raw[lang] = df_raw[lang].str.strip()\n    # remove stop words\n    df_raw[lang] = df_raw[lang].map(lambda x: re.sub(params[lang]['cleanup_regex'], '', x))\n    # filter-out long sentences\n    df_raw = df_raw[(df_raw[lang].str.len()<=params[lang]['max_sentence_length'])]\n\nnum_pairs = df_raw.count()[0]\ndf_raw.head()","fba3c4ee":"# analyze sentences length\ndef plot_sentence_dist(df_raw, lang):\n    df_agg = pd.DataFrame(df_raw[lang].map(lambda x: len(x)))\n    df_agg[lang+'_count'] = 1\n    return df_agg.groupby(lang).agg('count')\n\ndf_agg1 = plot_sentence_dist(df_raw, lang1)\ndf_agg2 = plot_sentence_dist(df_raw, lang2)\n\ndf_agg1[lang1+'_count'].describe(), df_agg2[lang2+'_count'].describe()","44d817bc":"# plot source langauge length distribution\nplt.subplot(1, 2, 1)\nplt.plot(df_agg1)\nplt.title('Sentences length')\nplt.ylabel('count')\nplt.xlabel(lang1)\n\n# plot destination langauge length distribution\nplt.subplot(1, 2, 2)\nplt.plot(df_agg2)\nplt.title('Sentences length')\nplt.xlabel(lang2)\n\nplt.tight_layout()\nplt.show()","09f10491":"#df_agg1[lang1+'_count']\nplt.figure(figsize=(15,5))\nplt.plot(df_agg1[lang1+'_count'].keys(), df_agg1[lang1+'_count'].values, marker='o', label=lang1)\nplt.plot(df_agg2[lang2+'_count'].keys(), df_agg2[lang2+'_count'].values, marker='o', label=lang2)\nplt.legend()\nplt.xlabel('Length of sentence')\nplt.ylabel('Count of occurrences')\nplt.tight_layout()\nplt.show()","a993da0e":"encodings = {\n    'fr': np.zeros((num_pairs, params['fr']['max_sentence_length'], params['fr']['chars_num'])),\n    'en': np.zeros((num_pairs, params['en']['max_sentence_length'], params['en']['chars_num']))\n}\n# one hot encode sentence characters\nindex = 0\nfor row_idx, row in df_raw.iterrows():\n    for lang in encodings.keys():\n        one_hot_encoding(encodings[lang][index], row[lang], lang)\n    index += 1\nprint(encodings['en'].shape, encodings['fr'].shape)","5b52408b":"# A 3D array of shape (num_pairs, max_sentence_en, num_english_characters) containing a one-hot vectorization of the English sentences.\nencoder_input_data = encodings['en']\n\n# A 3D array of shape (num_pairs, max_sentence_ar, num_arabic_characters) containg a one-hot vectorization of the Arabic sentences.\ndecoder_input_data = encodings['fr']\n\n# Same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\ndecoder_target_data = shift_right(encodings['fr'], 'fr')\n\ndecoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape","071e06e6":"decoder_input_data.shape, decoder_input_data[0][1]","4bf3b5c6":"decoder_target_data.shape, decoder_target_data[0][0]","4b08ec6c":"# language model\nmax_token_lang1 = params['en']['chars_num']\nmax_sentence_length_lang1 = params['en']['max_sentence_length']\nmax_token_lang2 = params['fr']['chars_num']\nmax_sentence_length_lang2 = params['fr']['max_sentence_length']\n\n\nbatch_size = 64  # Batch size for training.\nepochs = 10  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\n\n# initializer the shape of the input for the Encoder\/Decoder\nencoder_input = Input(shape=(None, max_token_lang1))\ndecoder_input = Input(shape=(None, max_token_lang2))\n\n# Encoding-Decoding LSTM\nencoder = LSTM(latent_dim, return_state=True)\ndecoder = LSTM(latent_dim, return_state=True, return_sequences=True)\n\n# run english input sentence througth the encoder\nencoder_input_reshaped = Reshape((max_sentence_length_lang1, max_token_lang1))(encoder_input)\n_, h, c = encoder(encoder_input_reshaped)\n# pass the hidden\/context states from encoder to the decoder, along with the target Arabic sentence\nencoder_input_reshaped = Reshape((max_sentence_length_lang2, max_token_lang2))(decoder_input)\nout, _, _ = decoder(encoder_input_reshaped, initial_state=[h, c])\n# generate the final output\nout = Dense(max_token_lang2)(out)\ndecoder_outtput = Activation('softmax')(out)\n# create the Keras model\nmodel = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_outtput])\n\nadam = Adam() #Adam(lr=0.1, decay=0.0005)\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])","45686a3e":"model.summary()","9d2acc60":"samples = decoder_input_data.shape[0]","904f94c2":"model.fit(\n    x=[encoder_input_data[:samples], decoder_input_data[:samples]],\n    y=[decoder_target_data[:samples]],\n    batch_size=batch_size,\n    shuffle = True,\n    epochs=epochs,\n    validation_split=0.1\n)","e7c48177":"def store(model):\n    \"\"\"Store to disk a model along with its weights\n    # Arguments:\n        model: the Keras model to be stored\n    \"\"\"\n    # serialize model to JSON\n    model_json = model.to_json()\n    with open(\"model.json\", \"w\") as json_file:\n        json_file.write(model_json)\n    # serialize weights to HDF5\n    model.save_weights(\"model.h5\")\n    print(\"Saved model to disk\")\n\ndef restore():\n    \"\"\"Restore from disk a model along with its weights\n    # Returns:\n        model: the Keras model which was restored\n    \"\"\"\n    # load json and create model\n    json_file = open('model.json', 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    loaded_model = model_from_json(loaded_model_json)\n    # load weights into new model\n    loaded_model.load_weights(\"model.h5\")\n    print(\"Loaded model from disk\")\n    return loaded_model","09dda95c":"store(model)","e7c7ef0f":"# evaluate loaded model on test data\n#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n#score = loaded_model.evaluate(X, Y, verbose=0)\n#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","9fa5f73e":"df_agg1[lang1+'_count'].keys()","ec54d9cf":"### Pre-process text data ","a5239b8d":"### Helper methods for storing models","84326c97":"### check decoder target is a right shift of input","a155ffe0":"![e4cdf91c-063f-11e6-8844-c89a9e134339.png](attachment:e4cdf91c-063f-11e6-8844-c89a9e134339.png)","04863aa1":"### Read sentence pairs","be1e487c":"### Seq2Seq input","3bd7e98f":"### Inference phase","0799df44":"### Training phase","d998950f":"## LSTM-based Architecture"}}