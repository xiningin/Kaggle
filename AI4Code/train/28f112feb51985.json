{"cell_type":{"4130cd59":"code","29973f07":"code","53bf79f4":"code","af582863":"code","90d1a466":"code","24680bba":"code","d1da141f":"code","2510d444":"code","b7cd6440":"code","a3a4497e":"code","2bf07712":"code","88ddc70b":"code","a2577ec6":"code","6f356bb5":"code","b0c93fa4":"code","17843ddf":"code","504052a6":"code","0fb22967":"code","0505cb1e":"code","cdb743d2":"code","4d11deb6":"code","80eb35f5":"code","96f7df23":"code","3baa0e7c":"code","e3e0ca63":"code","aba0d2c9":"code","81029a8d":"code","9040167b":"code","62173ed8":"code","5e043602":"code","cc5d61ef":"code","10e2db3e":"code","4eb7f9e0":"code","a30afd77":"code","3f518eea":"code","d993d38c":"code","3177a599":"code","4cee0c94":"code","563f1dab":"markdown","db27d9e4":"markdown","f431e1ba":"markdown","959d11d4":"markdown","e8cfd274":"markdown","b8bdd19e":"markdown","48c75665":"markdown","c3c5bc95":"markdown","4fc6cc15":"markdown","e15e0284":"markdown"},"source":{"4130cd59":"# common preprocessing libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n\n# model preprocessing libraries\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.preprocessing import  MinMaxScaler, RobustScaler, StandardScaler, LabelEncoder as le\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\n\n# model libraries\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n","29973f07":"# load data\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_data","53bf79f4":"# finding is there null values in some columns\ntrain_data.info()","af582863":"# checking the scales of features\ntrain_data.describe()","90d1a466":"# fill NaN values \ntrain_data.fillna(method='pad',inplace=True)\ntest_data.fillna(method='pad',inplace=True)\n","24680bba":"# confirming about null values\ntrain_data.info()","d1da141f":"# correlation visualisation using heatmap\nsns.set(style='darkgrid')\nfig=plt.figure(figsize=(10,10))\nsns.heatmap(train_data.corr(),annot=True,linewidths=0.3)","2510d444":"sns.countplot(train_data['Survived'])","b7cd6440":"y_train = train_data['Survived']\n","a3a4497e":"# drop columns which seems irrelevant for x_train and x_test\nx_train = train_data.drop(['PassengerId','Name','Cabin','Survived','Ticket'],axis=1)\nx_test = test_data.drop(['PassengerId','Name','Cabin','Ticket'],axis=1)\nx_train","2bf07712":"# making the columns which need to be rescaled\nx1_train = pd.DataFrame()\nx1_test = pd.DataFrame()\nx1_train['Age'] = x_train['Age']\nx1_train['Fare'] = x_train['Fare']\n\n\nx1_test['Age'] = x_test['Age']\nx1_test['Fare'] = x_test['Fare']\n","88ddc70b":"## Features Scaling\n\nsd1 = MinMaxScaler()\nsd2= MinMaxScaler()\nnorm1 = sd1.fit(x1_train)\nnorm2 = sd2.fit(x1_test)\nx1_train = pd.DataFrame(norm1.transform(x1_train),columns=x1_train.columns)\nx1_test = pd.DataFrame(norm2.transform(x1_test),columns=x1_test.columns)\nx1_train.describe()","a2577ec6":"\nsns.boxplot(y=x1_train['Fare'],x=y_train)","6f356bb5":"from scipy.stats import norm\nfig=plt.figure(figsize=(10,10))\nsns.distplot(x1_train['Fare'][y_train==0],color='g',label='notsurvived')\nsns.distplot(x1_train['Fare'][y_train==1],color='r',label='survived')\nplt.legend(loc='best')\n\n","b0c93fa4":"# convert to logarathmic scale to reduce skewness\nx1_train['Fare'] = x1_train['Fare'].map(lambda i:np.log(i) if i>0 else 0)\nx1_test['Fare'] = x1_test['Fare'].map(lambda i:np.log(i) if i>0 else 0)\n","17843ddf":"\nfrom scipy.stats import norm\nfig=plt.figure(figsize=(10,10))\nsns.distplot(x1_train['Fare'][y_train==0],color='g',label='notsurvived')\nsns.distplot(x1_train['Fare'][y_train==1],color='r',label='survived')\nplt.legend(loc='best')\n","504052a6":"fig= plt.figure(figsize=(15,15))\nsns.jointplot(x='Fare',y='Age',data=x1_train[y_train==0],color='g')\nsns.jointplot(x='Fare',y='Age',data=x1_train[y_train==1],color='r')\nsns.jointplot(x='Fare',y='Age',data=x1_test,color='b')","0fb22967":"# changing the new columns with existing train and test columns\nx_train['Age'] = x1_train['Age']\nx_train['Fare'] = x1_train['Fare']\nx_test['Age'] = x1_test['Age']\nx_test['Fare'] = x1_test['Fare']","0505cb1e":"## Applying label encoding for all data\nencode=le()\nx_train['Sex'] = encode.fit_transform(x_train['Sex'])\nx_test['Sex'] = encode.fit_transform(x_test['Sex'])\nx_train['Embarked'] = encode.fit_transform(x_train['Embarked'])\nx_test['Embarked'] = encode.fit_transform(x_test['Embarked'])\n\n\nx_train","cdb743d2":"\"\"\"\n# implementing PCA\n#covar_matrix = PCA(n_components = 5)\n\n\n\n##covar_matrix.fit(x_train)\n#variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n\n#var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n#var #cumulative sum of variance explained with [n] features\n\n\"\"\"","4d11deb6":"#x_train=covar_matrix.fit_transform(x_train)\n#x_train.drop('SibSp',axis=1,inplace=True)\n#x_test.drop('SibSp',axis=1,inplace=True)\n","80eb35f5":"# Applying p-value to check feature dependence\nimport statsmodels.api as sm\nregressor_OLS = sm.OLS(endog = y_train, exog = x_train).fit()\nregressor_OLS.summary()","96f7df23":"kfold = StratifiedKFold(n_splits = 5 )\n","3baa0e7c":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(kernel = 'rbf',probability = True))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(ExtraTreesClassifier(random_state=2,max_depth = None,min_samples_split= 2,min_samples_leaf = 1,bootstrap = False,n_estimators =320), random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression())\nclassifiers.append(XGBClassifier(random_state=random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\n\"\"\"\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier,x_train, y = y_train, scoring = 'accuracy', cv = kfold , n_jobs =-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"Adaboost\"\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MLP\",\"KNeighboors\",\"LogisticRegression\",\"xgboost\",\"LDA\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\ncv_res\n\n\"\"\"","e3e0ca63":"# feeding raw models into stacking ensemble as the metal model will extract tht best out of each one\nfrom vecstack import stacking\nfrom sklearn.metrics import accuracy_score,f1_score\n\nS_train, S_test = stacking(classifiers,                   \n                           x_train, y_train, x_test,   \n                           regression= False,\n                          \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=True,\n         \n                           save_dir=None, \n             \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","aba0d2c9":"S_train","81029a8d":"S_train.shape","9040167b":"argmax_train = []\nargmax_test = []\nfor i in range(0,S_train.shape[1],2):\n    argmax_train.append( np.argmax(S_train[:,i:i+2],axis=1))\n    argmax_test.append( np.argmax(S_test[:,i:i+2],axis=1))","62173ed8":"argmax_train = np.array(argmax_train,dtype= np.int64).T\nargmax_test = np.array(argmax_test,dtype= np.int64).T\n","5e043602":"argmax_train","cc5d61ef":"# here using overall probabilities for meta model\n## from sklearn.metrics import f1_score\nmodelc = LogisticRegression()\n    \nmodel1c = modelc.fit(S_train, y_train)\ny_pred1c = model1c.predict_proba(S_train)\ny_predc = model1c.predict_proba(S_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1c,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1c,axis=1)))\n","10e2db3e":"# here using predictions for metal model\n## from sklearn.metrics import f1_score\nmodel = XGBClassifier(random_state=2, objective = 'reg:linear', n_jobs=-1, learning_rate= 0.5, \n                      n_estimators=30, max_depth=20)\n    \nmodel1 = model.fit(argmax_train, y_train)\ny_pred1 = model1.predict_proba(argmax_train)\ny_pred = model1.predict_proba(argmax_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1,axis=1)))\n","4eb7f9e0":"## checking the distribution of prediction\nsns.distplot(y_pred)\nsns.distplot(y_predc,color = 'r')","a30afd77":"# loading sample submission file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission\n# making submission file\nsubmission1 = pd.DataFrame()\nsubmission1['PassengerId'] = submission['PassengerId']\nsubmission1['Survived'] = np.argmax(y_pred+y_predc,axis=1)\n# making submission\nsubmission1.to_csv('submission1.csv',index=False)","3f518eea":"submission1","d993d38c":"# prediction on train data for classification report\npredictions_train=model1c.predict_proba(S_train)\npred_train=np.argmax(predictions_train,axis=1)\npred_train","3177a599":"# confusion matrix\nconf = confusion_matrix(y_train,pred_train)\nsns.heatmap(conf,annot= True)\nconf","4cee0c94":"# classification report\nrepo = classification_report(y_train,pred_train)\nprint(repo)","563f1dab":"now look nice","db27d9e4":"**If you find this notebook helpful Please Upvote**","f431e1ba":"## MODEL BUILDING","959d11d4":"### preparing submissions","e8cfd274":"### Features visualisations\n","b8bdd19e":"*Kfold for cross validation*","48c75665":" *looks like there are too much outlier in fare column*\n","c3c5bc95":"*Again plotting to check the scaled distribution*","4fc6cc15":"### Stacking base learners \nhere we are assuming that the meta model will extract best from everyone","e15e0284":"### Loadind Data"}}