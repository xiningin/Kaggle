{"cell_type":{"62d8926e":"code","78125c55":"code","1313db04":"code","ea218686":"code","51474d8d":"code","1962438b":"code","3bfacf33":"code","89d28b25":"code","8788533e":"code","fb1c8882":"code","49e86a77":"code","4bc65cf9":"code","32bbec4d":"code","3ba82ab8":"code","603a5b49":"code","bc912221":"code","42b93c2b":"code","adfbc841":"code","47d1559a":"code","d504ec65":"code","39eb286f":"markdown","ca1b1e76":"markdown","f079d867":"markdown","54e4a191":"markdown","c35a3922":"markdown","4ce350af":"markdown","0468415f":"markdown","1ff61456":"markdown","78214440":"markdown","2baeeddd":"markdown","44a72c26":"markdown","d7751670":"markdown","0d063563":"markdown","f9db0a20":"markdown","c8c61f17":"markdown","7cd56d19":"markdown","dbf6a905":"markdown","0217eb1e":"markdown","e9c12fc6":"markdown"},"source":{"62d8926e":"%%html\n<style>\nh1 { color: #7c795d; text-align:center; font-align:center; font-family: 'Trocchi', serif; font-size: 45px; font-weight: normal; line-height: 48px; margin: 0; }\n\nh2 { color: #7c795d; text-align:center; font-align:center; font-family: 'Trocchi', serif; font-size: 20px; font-weight: normal; line-height: 48px; margin: 0; }\n\nh3 { color: #7c795d; text-align:center; font-align:center; font-family: 'Trocchi', serif; font-size: 16px; font-weight: normal; line-height: 48px; margin: 0; }\n\nh4 { color: #7c795d; text-align:center; font-align:center; font-family: 'Trocchi', serif; font-size: 14px; font-weight: normal; line-height: 48px; margin: 0; }\n      \n<\/style>\n<hr>\n<h1>Optiver Market Volatility Prediction<\/h1>\n<h2>By Thomas Meli<\/h2>\n<hr>","78125c55":"%%html\n<style>\nblockquote {\nfont-family: Helvetica, Arial, serif;\nfont-size: 14px;\nfont-style: italic;\nwidth: 90%;\nmargin: 0.25em 0 2em;\npadding: 0.25em 40px;\nline-height: 1.45;\nposition: relative;\ncolor: #383838;\n}\n\nblockquote:before {\nfont-family: Georgia, serif;\ndisplay: block;\ncontent: \"\\201C\";\nfont-size: 100px;\nposition: absolute;\nleft: -10px;\ntop: -35px;\ncolor: #ccc;\n}\n\nblockquote cite {\ncolor: #870237;\nfont-size: 13px;\ndisplay: block;\nmargin-top: 5px;\n}\n \nblockquote cite:before {\ncontent: \"\\2014 \\2009\";\n}\n\n<\/style>\n\n<blockquote>\n<p><br><\/p>\n<p><b>Realized volatility<\/b> is the assessment of variation in returns for an investment product by analyzing its historical returns within a defined time period. Assessment of degree of uncertainty and\/or potential financial loss\/gain from investing in a firm may be measured using variability\/ volatility in stock prices of the entity. <\/p>\n<p>The realized volatility or actual volatility in the market is caused by two components- a continuous volatility component and a jump component, which influence the stock prices. Continuous volatility in a stock market is affected by the intra-day trading volumes. For example, a single high volume trade transaction can introduce a significant variation in the price of an instrument. <\/p>\n<cite>https:\/\/www.wallstreetmojo.com\/realized-volatility\/<\/cite>\n<\/blockquote>","1313db04":"%%html\n\n<hr>\n<h2>Introduction<\/h2>\n<hr>","ea218686":"import numpy as np # linear algebra\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\n\nimport pandas as pd \npd.set_option(\"precision\", 3)  # Display precision\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n\nimport tensorflow as tf\nimport sklearn as sk\nfrom IPython.display import display, HTML, IFrame\n\nimport plotly.express as px\n\nimport os","51474d8d":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')","1962438b":"full_train_w_features = pd.read_csv(\"..\/input\/optiver-full-train-ml-ready\/full_train.csv\").drop(\"('seconds_in_bucket', 'min')_book\", axis = 1)\nfull_train_w_features = full_train_w_features[full_train_w_features.stock_id==0]\nfull_train_w_features.replace([np.inf, -np.inf], 0, inplace=True)  # Replace infinities from pct_change with zero.\nfull_train_w_features = full_train_w_features.dropna()\n\nprint(\"The dataframe we will be investigating is a dataframe with statistically engineered features and the target already merged.\\n\")\nprint(f\"Shape of engineered features where stock_id = 0 is {full_train_w_features.shape} \\n\\n\")\nfull_train_w_features.head(2)","3bfacf33":"corr = full_train_w_features.corr()\ntarget_corr = pd.DataFrame(corr[\"target\"]).rename(columns = {\"target\":\"pearson\"})\n\nspearman_corr = full_train_w_features.corr(method = \"spearman\")\nspearman_target_corr = pd.DataFrame(spearman_corr[\"target\"]).rename(columns = {\"target\":\"spearman\"})\n\nkendall_corr = full_train_w_features.corr(method = \"kendall\")\nkendall_target_corr = pd.DataFrame(kendall_corr[\"target\"]).rename(columns = {\"target\":\"kendall\"})","89d28b25":"merged_corr = pd.concat([target_corr, spearman_target_corr, kendall_target_corr], axis = 1).drop(\"stock_id\").sort_values(\"pearson\", ascending=False)","8788533e":"merged_corr.shape","fb1c8882":"merged_corr.dropna().style.background_gradient(cmap ='coolwarm', \n                                      axis = 0,\n                                      vmin=-1,\n                                      vmax=1) \\\n                   .set_properties(**{'font-size': '14px'}) \\\n    .set_caption(\"3 Correlation Coefficients of Statistical Features\") \\\n    .set_properties(padding=\"20px\", border=\"2px solid white\")","49e86a77":"sns.set_theme()\nsns.set_style(\"whitegrid\")","4bc65cf9":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n\n\nsns.regplot(data=full_train_w_features, x=\"('bid_price2', 'min')\", y=\"target\", ax = ax1)\nax1.set_title(\"Minimum Bid Price2\")\n\nsns.regplot(x=\"('wap2', 'min')\", y=\"target\", data=full_train_w_features, ax=ax2)\nax2.set_title(\"Minimum Weighted Average Price Computed from '2' Columns\")\n\nsns.regplot(x=\"('wap_log_return', 'Median_abs_deviation')\", y=\"target\", data = full_train_w_features, ax=ax3)\nax3.set_title(\"Median Absolute Deviation of Avg. Weighted Avg. Price\")\n\nsns.regplot(x=\"('ask_price2', 'max')\", y=\"target\", data = full_train_w_features, ax=ax4)\nax4.set_title(\"Max Asking Price 2\")\n\nplt.tight_layout(pad=3)\nplt.show()","32bbec4d":"std_features = pd.DataFrame(\n    StandardScaler().fit_transform(full_train_w_features.fillna(0)),\n    columns = full_train_w_features.columns\n)","3ba82ab8":"std_corr = pd.DataFrame(std_features.corr()['target']) \\\n            .sort_values('target', ascending=False)","603a5b49":"std_corr.head()","bc912221":"stock_ids = full_train_w_features['stock_id']\ntime_ids = full_train_w_features['time_id']\n\npolyfeaturizer = PolynomialFeatures(degree=2).fit(full_train_w_features.drop(['stock_id', 'time_id'], axis=1))\ntrain_polyfeats = pd.DataFrame(polyfeaturizer.transform(full_train_w_features.drop(['stock_id', 'time_id'], axis=1)), \n                               columns = polyfeaturizer.get_feature_names())\n\npoly_target = train_polyfeats['x0']\ntrain_polyfeats = train_polyfeats.rename(columns={\"x0\":\"target\"}).drop(\"1\", axis=1)\ntrain_polyfeats.shape","42b93c2b":"train_polyfeats.head()","adfbc841":"poly_corr = train_polyfeats.corr().sort_values(by=\"target\", ascending=False)\npoly_corr.to_csv(\"polynomial_features_corr.csv\", index = True)","47d1559a":"#poly_corr.head(20)","d504ec65":"#poly_corr.tail(20)","39eb286f":"**3 Correlation Values with target - some notes on the different wap values**\n\n* wap1 = derived from bid1, ask1, etc.\n* wap2 = derived from bid2, ask2, etc.\n* wap = averaged from wap1 and wap2\n\nAll values were calulated with a groupby aggregation.  See comments with Yirun.","ca1b1e76":"## Exploring the Data (Visual guide to the data coming soon)\n\nWe are primarily given Book Data and Trade Data.  Tip - You may want to keep the file format in parquet or convert to csv offline because the csv conversion in the notebook tends to kill it.\n\n**Book Data** - is information about the asking price and the selling price over time.  This can be used for feature generation.\n\n**Trade Data** - is information about the trade price of the stock over time as well as the size of the trade and the number of orders.","f079d867":"![Market Pic](https:\/\/www.thomasmeli.tech\/wp-content\/uploads\/2021\/06\/Monitor-Stock-Business-Trading-Exchange-Finance-1863880.jpg)","54e4a191":"## Are Standardized Features More Highly Correlated than the most correlated above? - No ##","c35a3922":"---\n## TL:DR - Insight Summary\n* Standard Deviations of Price Variables Most POSITIVELY Highly Correlated with Target in Stock = 0\n* Minimum Prices Variables Most NEGATIVELY Highly Correlated with Target in Stock = 0\n* All time IDs in book and trade match and none are missing.\n* Total \"Seconds in Bucket\" values are different in trade and book by an order of magnitude.\n* There are no missing cells in either dataframe.","4ce350af":"---\n### Insight 1: Standard Deviations of Price Variables Most POSITIVELY Highly Correlated with Target in Stock = 0\n### Insight 2: Minimum Prices Variables Most NEGATIVELY Highly Correlated with Target in Stock = 0\n---","0468415f":"Much of the data is already standardized (or the std. dev. is an explicit feature and probably does not need standardization)","1ff61456":"**Introduction**\n\n**Purpose of competition: Given 10 minutes of book data, predict the volatility in the next 10 minutes.  The units of the prediction are in a weighted average price of those 10 minutes.**\n\nAs explained in the introduction notebook, Optiver is, among other things, a **market maker** - that is, they are a mediator that takes information about buying prices and selling prices and provides bids to see who is willing to buy and sell.\n\n**They are interested in predicting volatility accurately** since increased volatility allows more flexibility in trading and allows them to make more appealing offers between sellers and buyers.  Although for consumers, volatility can be more uncertain and risky, for traders, volatility also makes assets more liquid (mobile).  Since it refers to the amount of dispersion (spread) of an asset's returns, a higher volatility makes gains as well as losses higher.  Traders prefer it because it makes more profit possible.\n\nOne notion of **volatility** refers to **the standard deviation of the percent change of the price returns of an asset.**  At the most basic level, you can find it by performing **.pct_change().std()** in a dataframe along with some timestep (daily volatility, weekly (5 trading days), monthly (21 trading days), yearly (252 trading days) etc.).  **The date range is important for the notion of realized vs. implied volatility.**   However, in this project you are advised to use a standardized value and use log returns.\n\nWe are also given **the book data which represents the intentions of buyers and sellers.** The more dense this book is, the more variance there may be in bids and asks in relation to the trade that (possibly) happens.  So this can also be used to measure the volatility as well if we find the difference between the best ask price and the best bid price - known as **the bid-ask spread**.  These prices are weighted depending on the level and size of the orders.\n\nIn common terms, volatility measures the variance of returns of an asset, the the more variance there is, the more uncertainty there is and the riskier the asset is.  In quantitative analysis, in order to make these metrics comparable across assets and across different time measures, they are standardized over time (annualized) and returns are converted into log returns to make assets comparable.  ","78214440":"## Links to Pandas Profiles.\n\nI intend this notebook to be a resource that will have pandas profiles up for\n* Now: just the basic dataset.\n* As I go: Features generated such as various technical indicators.\n\n**Externally Processed Pandas Profile explorative Reports**\n\nMinimal reports are attached to this notebook.  All reports and figures will be attached to this dataset. \n\n* **Optiver Visual Data Reports:** https:\/\/www.kaggle.com\/tpmeli\/optivervisualreports.","2baeeddd":"#### Thanks for reading through my (ongoing) notebook.  I hope it was useful for you :) ","44a72c26":"## Are Any Interactions Between These Variables Higher in Correlation with Target Than The Original Variables? (presentation in process - see output for polynomial correlation csv for now) ##","d7751670":"### Important Point: Time ID is NOT sequential\n\n![https:\/\/www.thomasmeli.tech\/wp-content\/uploads\/2021\/07\/time_id_important.jpg](https:\/\/www.thomasmeli.tech\/wp-content\/uploads\/2021\/07\/time_id_important.jpg)\n\nAs I shared [here](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250706), the information we have about the next 10 minutes is really contained in target, which has the volatility of the next 10 minutes.\n\nThere are a lot of cool ideas to still get some information from this.\n\n* prices is normally distributed, hence the volatility may actually allow us to infer information about the 'missing' next dataset.\n* It might be useful to label each row in each bucket and have a sense if interesting things are happening towards the beginning or end of the bucket.\n\nI'll get working on these ideas and publish some ideas in a new notebook. \n\nLet me know if you want me to explore any other ideas.","0d063563":"<hr>","f9db0a20":"## External Resources and further information\n\n**Read more about:**\n* Why volatility is so important to investors - https:\/\/www.investopedia.com\/articles\/financial-theory\/08\/volatility.asp\n* Bid ask spread: https:\/\/www.investopedia.com\/trading\/basics-of-the-bid-ask-spread\/\n* Parquet files: https:\/\/miuv.blog\/2018\/08\/21\/handling-large-amounts-of-data-with-parquet-part-1\/","c8c61f17":"<hr>","7cd56d19":"**Load code and imports**","dbf6a905":"## What are the features that are highly correlated with the target, but least correlated with each other? (in process)","0217eb1e":"## Seaborn Regression Plots of Higher Correlated Features and Targets","e9c12fc6":"---\n## Automated Pandas Profiles Attached Below with Basic Data Exploration\n---"}}