{"cell_type":{"79077d72":"code","11369baa":"code","8dda4e4d":"code","3b101b0c":"code","80ce9f8c":"code","b4e822da":"code","a14c9826":"code","28cdc5be":"code","3b37241a":"code","dc6f8661":"code","48348b82":"code","a9f0b97a":"code","dd9771f4":"code","ed6b8f30":"code","9cf0831b":"code","887206a8":"code","54ff955c":"code","5accacec":"code","821e5804":"code","e130583d":"code","2e89683c":"code","f1b431cc":"code","d43da63e":"code","a148621a":"code","375f5022":"code","0b630b73":"code","8de01473":"code","247eeb52":"code","b031d9ad":"code","a3f28ebd":"code","5c90a698":"code","67546d56":"code","1ae02574":"markdown","ad5c1652":"markdown","a9a44567":"markdown","8592e044":"markdown","16a4c4bb":"markdown","466e072f":"markdown","5f2f46d8":"markdown","176f5a3c":"markdown"},"source":{"79077d72":"# data visualization and manipulation libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline \n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Data Augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# DL libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n\n# CNN libraries\nfrom keras.layers import Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\nimport tensorflow as tf\nimport random\n\n# manipulating images and getting numpy arrays of pixel values of images.\nimport cv2                  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom PIL import Image\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","11369baa":"DIR = \"..\/input\/flowers-recognition\/flowers\/flowers\/\"\n\nfolders = os.listdir(DIR)\nfolders","8dda4e4d":"# \uacbd\ub85c \uc124\uc815\nFLOWER_DAISY_DIR = '..\/input\/flowers-recognition\/flowers\/flowers\/daisy'\nFLOWER_SUNFLOWER_DIR = '..\/input\/flowers-recognition\/flowers\/flowers\/sunflower'\nFLOWER_TULIP_DIR = '..\/input\/flowers-recognition\/flowers\/flowers\/tulip'\nFLOWER_DANDELION_DIR = '..\/input\/flowers-recognition\/flowers\/flowers\/dandelion'\nFLOWER_ROSE_DIR = '..\/input\/flowers-recognition\/flowers\/flowers\/rose'","3b101b0c":"# \ub77c\ubca8\ub9c1 \ud568\uc218\ndef assign_label(img, flower_type):\n    return flower_type","80ce9f8c":"# train data \uc0dd\uc131 \ud568\uc218\ndef make_train_data(flower_type, DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label = assign_label(img, flower_type) \n        path = os.path.join(DIR, img)\n        \n        try:\n            img = cv2.imread(path, cv2.IMREAD_COLOR) # \uc774\ubbf8\uc9c0 \uc77d\uae30\n            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) # size \ubcc0\ud658\n        \n            Images.append(np.array(img)) # numpy\ub85c \uc800\uc7a5\n            Labels.append(str(label)) # \ub77c\ubca8 \uc800\uc7a5\n        \n        # jpg \ud30c\uc77c\uc774 \uc544\ub2cc \uacbd\uc6b0 \uc5d0\ub7ec \ud578\ub4e4\ub9c1\n        except Exception as e:\n            print(\"error image\")\n            print(e)","b4e822da":"Images = [] # image\nLabels = [] # label\nIMG_SIZE = 150 # img size \uc124\uc815","a14c9826":"make_train_data('Daisy', FLOWER_DAISY_DIR)\nmake_train_data('Sunflower', FLOWER_SUNFLOWER_DIR)\nmake_train_data('Tulip', FLOWER_TULIP_DIR)\nmake_train_data('Rose', FLOWER_ROSE_DIR)\nmake_train_data('Dandelion', FLOWER_DANDELION_DIR)","28cdc5be":"print(\"Total image number :\", len(Images))","3b37241a":"plt.figure(figsize = (8, 6))\nsns.set_style('dark')\nsns.countplot(Labels)\nplt.title(\"Number of images by flower types\", y = 1.02, size = 15)\nplt.show()","dc6f8661":"f, ax = plt.subplots(4, 2, figsize = (15, 15))\n\nfor i in range(4):\n    for j in range (2):\n        l = random.randint(0, len(Labels))\n        ax[i, j].imshow(Images[l])\n        ax[i, j].set_title(Labels[l], size = 15)\n        \nplt.tight_layout()","48348b82":"# Label Encoding\nle = LabelEncoder()\nY = le.fit_transform(Labels)\nY","a9f0b97a":"# One-hot Encoding\nY = to_categorical(Y, 5)\nY","dd9771f4":"# Normalization\nX = np.array(Images) \/ 255","ed6b8f30":"X.shape, Y.shape","9cf0831b":"# Splitting into Training and Validation Sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify = Y, random_state = 42)","887206a8":"X_train.shape, Y_train.shape, X_test.shape, Y_test.shape","54ff955c":"# Data Augmentation\n\ndatagen=ImageDataGenerator(\n       \n        rotation_range=10,  # \ub79c\ub364\ud558\uac8c 10\ub3c4 \ud68c\uc804\n        zoom_range = 0.1, # \ub79c\ub364\ud558\uac8c zoom\n        width_shift_range=0.2,  # \ub79c\ub364\ud558\uac8c \uc218\ud3c9\uc73c\ub85c shift\n        height_shift_range=0.2,  # \ub79c\ub364\ud558\uac8c \uc218\uc9c1\uc73c\ub85c shift\n        horizontal_flip=True,  # \ub79c\ub364\ud558\uac8c \uc218\ud3c9\uc73c\ub85c flip\n        vertical_flip=False)  # \ub79c\ub364\ud558\uac8c \uc218\uc9c1\uc73c\ub85c flip        \n\ndatagen.fit(X_train)","5accacec":"# LR \ucf5c\ubc31\ud568\uc218\nfrom keras.callbacks import ReduceLROnPlateau\n\nreduceLR = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.1)","821e5804":"# Modelling CNN\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size = (3,3), padding = 'same', activation ='relu', input_shape = (IMG_SIZE, IMG_SIZE, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64, kernel_size = (3,3), padding = 'same', activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \nmodel.add(Conv2D(128, kernel_size = (3,3), padding = 'same', activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation = \"softmax\"))","e130583d":"model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","2e89683c":"model.summary()","f1b431cc":"batch_sizes = 128\nepochs = 30\n\nhistory = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizes),\n                              epochs=epochs, \n                              validation_data=(X_test,Y_test),\n                              verbose = 1, \n                              steps_per_epoch=X_train.shape[0]\/\/128,\n                              callbacks=[reduceLR])\n\n# history = model.fit(X_train, Y_train,\n#           batch_size=batch_sizes,\n#           epochs=epochs,\n#           verbose=1, \n#           validation_data=(X_test, Y_test),\n#           callbacks=[reduceLR])","d43da63e":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","a148621a":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","375f5022":"from tensorflow.keras.applications import InceptionResNetV2","0b630b73":"inception_resnet_v2 = InceptionResNetV2(\n    include_top=False, weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n)\n\ninception_resnet_v2.trainable=False","8de01473":"model_inception_resnet_v2 = Sequential([\n    inception_resnet_v2, \n    \n    Flatten(),\n    Dense(512, activation = 'relu'),\n    Dropout(0.2),\n    Dense(128, activation = 'relu'),\n    Dropout(0.2),\n    Dense(5, activation='softmax')])","247eeb52":"model_inception_resnet_v2.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","b031d9ad":"batch_sizes = 128\nepochs = 70\n\nhistory = model_inception_resnet_v2.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizes),\n                                                  epochs=epochs, \n                                                  validation_data=(X_test,Y_test),\n                                                  verbose = 1, \n                                                  steps_per_epoch=X_train.shape[0]\/\/128,\n                                                  callbacks=[reduceLR])","a3f28ebd":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","5c90a698":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","67546d56":"print(\"Test Loss :\" , model_inception_resnet_v2.evaluate(X_test, Y_test)[0])\nprint(\"Test Accuracy:\" , model_inception_resnet_v2.evaluate(X_test, Y_test)[1]*100 , \"%\")","1ae02574":"- \ube44\uad50\uc801 \uace0\ub978 \ubd84\ud3ec\ub97c \uc9c0\ub2d8","ad5c1652":"# \ub77c\uc774\ube0c\ub7ec\ub9ac \ubd88\ub7ec\uc624\uae30","a9a44567":"# Transfer Learning - InceptionResNetV2","8592e044":"# \ubaa8\ub378 \uc131\ub2a5 \ud655\uc778\ud558\uae30","16a4c4bb":"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ubc0f \uc900\ube44","466e072f":"# \ubaa8\ub378\ub9c1","5f2f46d8":"# \uc2dc\uac01\ud654","176f5a3c":"# \ub370\uc774\ud130 \uc900\ube44\ud558\uae30"}}