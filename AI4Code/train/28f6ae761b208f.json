{"cell_type":{"8832d2a8":"code","6bfc80c9":"code","ec0c87d5":"code","2809b197":"code","a39135f9":"code","87dfdb81":"code","da43211a":"code","f2a63c05":"code","59663350":"code","0fe0b0ab":"code","02f5cef7":"code","09e78d65":"code","c6847566":"code","d44596f5":"code","fe0f941a":"code","3cc41c55":"code","04b5f906":"code","1f2b7e1a":"code","2f0ada09":"markdown"},"source":{"8832d2a8":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler","6bfc80c9":"class Config:\n    num_classes=1\n    epochs=10\n    margin=0.5\n    model_name = '..\/input\/robertalarge'\n    batch_size = 128\n    lr = 1e-4\n    weight_decay=0.01\n    scheduler = 'CosineAnnealingLR'\n    max_length = 128\n    accumulation_step = 1\n    patience = 1","ec0c87d5":"class ToxicDataset(Dataset):\n    def __init__(self, comments, tokenizer, max_length):\n        self.comment = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        \n    def __len__(self):\n        return len(self.comment)\n    \n    def __getitem__(self, idx):\n\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                self.comment[idx].lower(),\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n\n        \n        input_ids = inputs_more_toxic['input_ids']\n        attention_mask = inputs_more_toxic['attention_mask']\n\n       \n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n        }","2809b197":"class ToxicModel(nn.Module):\n    def __init__(self, model_name, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.output = nn.LazyLinear(self.args.num_classes)\n    \n        \n    def forward(self, toxic_ids, toxic_mask):\n        \n        out = self.model(\n            input_ids=toxic_ids,\n            attention_mask=toxic_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        outputs = self.output(out)\n\n        return outputs","a39135f9":"def get_predictions(model, dataloader):\n    model.eval()\n    \n    PREDS=[]\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:        \n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n\n            outputs = model(input_ids, attention_mask)\n\n            PREDS.append(outputs.view(-1).cpu().detach().numpy())\n\n            bar.set_postfix(Stage='Inference')  \n        \n        PREDS = np.hstack(PREDS)\n        gc.collect()\n\n        return PREDS","87dfdb81":"df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","da43211a":"args = Config()","f2a63c05":"tokenizer = AutoTokenizer.from_pretrained(args.model_name)","59663350":"def washing_machine(comments):\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus","0fe0b0ab":"# df=df[:500]","02f5cef7":"def validate():\n#     comments1 = washing_machine(df['less_toxic'].values)\n#     comments2 = washing_machine(df['more_toxic'].values)\n\n    comments1 = df['less_toxic'].values\n    comments2 = df['more_toxic'].values\n\n    test_dataset1 = ToxicDataset(comments1, tokenizer, max_length=args.max_length)\n    test_loader1 = DataLoader(test_dataset1, batch_size=2*args.batch_size,\n                             num_workers=2, shuffle=False, pin_memory=True)\n    test_dataset2 = ToxicDataset(comments2, tokenizer, max_length=args.max_length)\n    test_loader2 = DataLoader(test_dataset2, batch_size=2*args.batch_size,\n                             num_workers=2, shuffle=False, pin_memory=True)\n\n    preds1 = inference(test_loader1)\n\n    preds2 = inference(test_loader2)\n\n    score = []\n    for o1,o2 in zip(preds1, preds2):\n        s = 1 if o1<o2 else 0\n        score.append(s)\n    print(np.mean(score))","09e78d65":"def inference(dataloader):\n    final_preds = []\n    args = Config()\n    ### bert-base-uncased\n    base_path='..\/input\/rob-large\/'\n    \n    for fold in range(5):\n        model = ToxicModel(args.model_name, args)\n        model = model.cuda()\n        if fold==4 or fold==3:\n            base_path='..\/input\/notebookf6da51ad47\/'\n        \n        path = base_path + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(model, dataloader)\n        final_preds.append(preds)\n    \n    \n    # bertweet base\n    base_path='..\/input\/bertweet\/'\n    args.model_name = '..\/input\/bertweetbase'\n    for fold in range(5):\n        model = ToxicModel(args.model_name, args)\n        model = model.cuda()\n        \n        path = base_path + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(model, dataloader)\n        final_preds.append(preds)\n    \n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","c6847566":"# validate()","d44596f5":"sub = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","fe0f941a":"sub.head(1)","3cc41c55":"sub_dataset = ToxicDataset(sub['text'].values, tokenizer, max_length=args.max_length)\nsub_loader = DataLoader(sub_dataset, batch_size=2*args.batch_size,\n                        num_workers=2, shuffle=False, pin_memory=True)","04b5f906":"sub_preds = inference(sub_loader)","1f2b7e1a":"sub['score'] = sub_preds\nsub[['comment_id', 'score']].to_csv('submission.csv', index=False)","2f0ada09":"# Prediction and submission"}}