{"cell_type":{"776cd143":"code","0d39303b":"code","809f912d":"code","4eb9898f":"code","624fa54a":"code","819fa167":"code","98857c37":"code","65524bca":"code","298731a6":"code","6f01965d":"code","a53325eb":"code","a1f5a0a9":"code","d39a1d85":"code","cb9deb60":"markdown"},"source":{"776cd143":"import os\nimport random\nimport time\nfrom os import walk\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom skimage.feature import local_binary_pattern\nfrom skimage.measure import find_contours\nfrom skimage.morphology import binary_dilation\nfrom sklearn.svm import SVC\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset\nimport torch.nn.functional as F","0d39303b":"# Parameters and constants\nAVAILABLE_WRITERS = 672\nRESULTS_FILE = 'results.txt'\nTIME_FILE = 'time.txt'\nOVERLAPPING_METHOD = 0\nLINES_METHOD = 1\nSUPPORT_VECTOR_CLASSIFIER = 0\nNEURAL_NETWORK_CLASSIFIER = 1\nHISTOGRAM_BINS = 256\nNN_LEARNING_RATE = 0.003\nNN_WEIGHT_DECAY = 0.01\nNN_DROPOUT = 0.25\nNN_EPOCHS = 200\nNN_BATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","809f912d":"def show_images(images, titles=None):\n    n_ims = len(images)\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n    fig = plt.figure()\n    n = 1\n    for image, title in zip(images, titles):\n        a = fig.add_subplot(1, n_ims, n)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image)\n        a.set_title(title)\n        n += 1\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n    plt.show()","4eb9898f":"def preprocess_image(img, feature_extraction_method=OVERLAPPING_METHOD):\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        img_copy = img.copy()\n        if len(img.shape) > 2:\n            img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n        img_copy = cv2.medianBlur(img_copy, 5)\n        img_copy = cv2.threshold(img_copy, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n        img_copy = img_copy[min_vertical:max_vertical]\n        return img_copy\n\n    if feature_extraction_method == LINES_METHOD:\n        img_copy = img.copy()\n        if len(img.shape) > 2:\n            grayscale_img = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n        else:\n            grayscale_img = img.copy()\n        img_copy = cv2.threshold(grayscale_img, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n        img_copy = img_copy[min_vertical:max_vertical]\n        grayscale_img = grayscale_img[min_vertical:max_vertical]\n        filter_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n        img_copy_sharpened = cv2.filter2D(img_copy, -1, filter_kernel)\n        return img_copy_sharpened, grayscale_img","624fa54a":"def get_corpus_boundaries(img):\n    crop = []\n    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (100, 1))\n    detect_horizontal = cv2.morphologyEx(img, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n    contours = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    prev = -1\n    for i, c in enumerate(contours):\n        if np.abs(prev - int(c[0][0][1])) > 800 or prev == -1:\n            crop.append(int(c[0][0][1]))\n            prev = int(c[0][0][1])\n    crop.sort()\n    max_vertical = crop[1] - 20\n    min_vertical = crop[0] + 20\n    return min_vertical, max_vertical","819fa167":"def segment_image(img, num, grayscale_img=None):\n    if grayscale_img is not None:\n        grayscale_images = []\n        img_copy = np.copy(img)\n        kernel = np.ones((1, num))\n        img_copy = binary_dilation(img_copy, kernel)\n        bounding_boxes = find_contours(img_copy, 0.8)\n        for box in bounding_boxes:\n            x_min = int(np.min(box[:, 1]))\n            x_max = int(np.max(box[:, 1]))\n            y_min = int(np.min(box[:, 0]))\n            y_max = int(np.max(box[:, 0]))\n            if (y_max - y_min) > 50 and (x_max - x_min) > 50:\n                grayscale_images.append(grayscale_img[y_min:y_max, x_min:x_max])\n        return grayscale_images\n    images = []\n    img_copy = np.copy(img)\n    kernel = np.ones((1, num))\n    img_copy = binary_dilation(img_copy, kernel)\n    bounding_boxes = find_contours(img_copy, 0.8)\n    for box in bounding_boxes:\n        x_min = int(np.min(box[:, 1]))\n        x_max = int(np.max(box[:, 1]))\n        y_min = int(np.min(box[:, 0]))\n        y_max = int(np.max(box[:, 0]))\n        if (y_max - y_min) > 10 and (x_max - x_min) > 10:\n            images.append(img[y_min:y_max, x_min:x_max])\n    return images","98857c37":"def overlap_words(words, avg_height):\n    overlapped_img = np.zeros((3600, 320))\n    index_i = 0\n    index_j = 0\n    max_height = 0\n    for word in words:\n        if word.shape[1] + index_j > overlapped_img.shape[1]:\n            max_height = 0\n            index_j = 0\n            index_i += int(avg_height \/\/ 2)\n        if word.shape[1] < overlapped_img.shape[1] and word.shape[0] < overlapped_img.shape[0]:\n            indices = np.copy(overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]])\n            indices = np.maximum(indices, word)\n            overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]] = indices\n            index_j += word.shape[1]\n            if max_height < word.shape[0]:\n                max_height = word.shape[0]\n    overlapped_img = overlapped_img[:index_i + int(avg_height \/\/ 2), :]\n    return overlapped_img","65524bca":"def get_textures(image):\n    index_i = 0\n    index_j = 0\n    texture_size = 100\n    textures = []\n    while index_i + texture_size < image.shape[0]:\n        if index_j + texture_size > image.shape[1]:\n            index_j = 0\n            index_i += texture_size\n        textures.append(np.copy(image[index_i: index_i + texture_size, index_j: index_j + texture_size]))\n        index_j += texture_size\n    return textures","298731a6":"def model_generator(features, labels, feature_extraction_method=OVERLAPPING_METHOD,\n                    classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n    histograms = []\n\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        for texture_array in features:\n            for texture in texture_array:\n                lbp = local_binary_pattern(texture, 8, 3, 'default')\n                histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n                histograms.append(histogram)\n\n    elif feature_extraction_method == LINES_METHOD:\n        for line in features:\n            lbp = local_binary_pattern(line, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            histograms.append(histogram)\n\n    if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n        model = SVC(kernel='linear')\n        model.fit(histograms, labels)\n        return model\n\n    if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n        model = nn.Sequential(nn.Linear(HISTOGRAM_BINS, 128),\n                              nn.ReLU(),\n                              nn.Dropout(p=NN_DROPOUT),\n                              nn.Linear(128, 64),\n                              nn.ReLU(),\n                              nn.Dropout(p=NN_DROPOUT),\n                              nn.Linear(64, 3))\n        model.to(DEVICE)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adamax(model.parameters(), lr=NN_LEARNING_RATE, weight_decay=NN_WEIGHT_DECAY)\n        inputs = torch.Tensor(histograms)\n        labels = torch.tensor(labels, dtype=torch.long) - 1\n        dataset = TensorDataset(inputs, labels)\n        train_loader = torch.utils.data.DataLoader(dataset, batch_size=NN_BATCH_SIZE, shuffle=True)\n        for epoch in range(NN_EPOCHS):\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n                output = model(inputs)\n                loss = criterion(output, labels)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        return model","6f01965d":"def predict(model, test_image, feature_extraction_method=OVERLAPPING_METHOD, classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        img = preprocess_image(test_image)\n        words = segment_image(img, 3)\n        avg_height = 0\n        for word in words:\n            avg_height += word.shape[0] \/ len(words)\n        overlapped_img = overlap_words(words, avg_height)\n        textures = get_textures(overlapped_img)\n        prediction = np.zeros(4)\n        for texture in textures:\n            lbp = local_binary_pattern(texture, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n                prediction[model.predict([histogram])] += 1\n            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n                with torch.no_grad():\n                    model.eval()\n                    histogram = torch.Tensor(histogram)\n                    probabilities = F.softmax(model.forward(histogram), dim=0)\n                    _, top_class = probabilities.topk(1)\n                    prediction[top_class + 1] += 1\n        return np.argmax(prediction)\n\n    if feature_extraction_method == LINES_METHOD:\n        img, grayscale_img = preprocess_image(test_image, feature_extraction_method)\n        grayscale_lines = segment_image(img, 100, grayscale_img)\n        prediction = np.zeros(4)\n        for line in grayscale_lines:\n            lbp = local_binary_pattern(line, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n                prediction[model.predict([histogram])] += 1\n            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n                with torch.no_grad():\n                    model.eval()\n                    histogram = torch.Tensor(histogram)\n                    probabilities = F.softmax(model.forward(histogram), dim=0)\n                    _, top_class = probabilities.topk(1)\n                    prediction[top_class + 1] += 1\n        return np.argmax(prediction)","a53325eb":"def read_random_images(root):\n    images = []\n    labels = []\n    test_images = []\n    test_labels = []\n    for i in range(3):\n        found_images = False\n        while not found_images:\n            images_path = root\n            random_writer = random.randrange(AVAILABLE_WRITERS)\n            if random_writer < 10:\n                random_writer = \"00\" + str(random_writer)\n            elif random_writer < 100:\n                random_writer = \"0\" + str(random_writer)\n            images_path = os.path.join(images_path, str(random_writer))\n            if not os.path.isdir(images_path):\n                continue\n            _, _, filenames = next(walk(images_path))\n            if len(filenames) <= 2 and i == 2 and len(test_images) == 0:\n                continue\n            if len(filenames) >= 2:\n                found_images = True\n                chosen_filenames = []\n                for j in range(2):\n                    random_filename = random.choice(filenames)\n                    while random_filename in chosen_filenames:\n                        random_filename = random.choice(filenames)\n                    chosen_filenames.append(random_filename)\n                    images.append(cv2.imread(os.path.join(images_path, random_filename)))\n                    labels.append(i + 1)\n                if len(filenames) >= 3:\n                    random_filename = random.choice(filenames)\n                    while random_filename in chosen_filenames:\n                        random_filename = random.choice(filenames)\n                    chosen_filenames.append(random_filename)\n                    test_images.append(cv2.imread(os.path.join(images_path, random_filename)))\n                    test_labels.append(i + 1)\n    test_choice = random.randint(0, len(test_images) - 1)\n    test_image = test_images[test_choice]\n    test_label = test_labels[test_choice]\n    return images, labels, test_image, test_label","a1f5a0a9":"def extract_features(images, labels, feature_extraction_method=OVERLAPPING_METHOD):\n    if feature_extraction_method == LINES_METHOD:\n        lines_labels = []\n        lines = []\n        for image, label in zip(images, labels):\n            image, grayscale_image = preprocess_image(image, feature_extraction_method)\n            grayscale_lines = segment_image(image, 100, grayscale_image)\n            for line in grayscale_lines:\n                lines.append(line)\n                lines_labels.append(label)\n        return lines, lines_labels\n\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        textures = []\n        textures_labels = []\n        for image, label in zip(images, labels):\n            image = preprocess_image(image)\n            words = segment_image(image, 3)\n            avg_height = 0\n            for word in words:\n                avg_height += word.shape[0] \/ len(words)\n            overlapped_img = overlap_words(words, avg_height)\n            new_textures = get_textures(overlapped_img)\n            textures.append(new_textures)\n            for j in range(len(new_textures)):\n                textures_labels.append(label)\n        return textures, textures_labels","d39a1d85":"epochs = 100\nroot='..\/input\/iam-handwritten-forms-dataset\/data'\nfeature_extraction_method=OVERLAPPING_METHOD\nclassifier_type=SUPPORT_VECTOR_CLASSIFIER\ncorrect_predictions = 0\ntotal_execution_time = 0\nfor epoch in range(epochs):\n    images, labels, test_image, test_label = read_random_images(root)\n    start_time = time.time()\n    features, features_labels = extract_features(images, labels, feature_extraction_method)\n    model = model_generator(features, features_labels, feature_extraction_method, classifier_type)\n    prediction = predict(model, test_image, feature_extraction_method, classifier_type)\n    execution_time = time.time() - start_time\n    total_execution_time += execution_time\n    if prediction == test_label:\n        correct_predictions += 1\n    print(\"Epoch #{} | Execution time {} seconds | Model accuracy {}\".format(epoch + 1, round(execution_time, 2), round((correct_predictions \/ (epoch + 1)) * 100, 2)))\nprint(\"Model accuracy = {}% using {} sample tests.\".format((correct_predictions \/ epochs) * 100, epochs))\nprint(\"Total execution time = {} using {} sample tests.\".format(round(total_execution_time, 2), epochs))","cb9deb60":"# A writer identifier implementation that uses randomly generated training and test data from the IAM dataset\n\n## Project Pipeline\n\nThis project adopts the following machine learning pipeline:\n\n![image.png](attachment:image.png)\n\n## Preprocessing\n\nInput images are processed according to the chosen features extraction method, but whatever the chosen extraction method is, the image essentially goes through the same procedures. The preprocessing module simply crops the image to its useful content (i.e the handwritten text corpus) and returns a binarized version of the cropped image and a grayscale version depending on the type of the features extraction method.\n\n## Features Extraction\n\nThe features extraction phase is inspired by [this paper](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0045790617322401). Two similar methods are implemented for extracting features from raw images. The first is implemented as proposed in the paper where the image is segmented into words images which are then overlapped into a single image to represent the texture of a particular writer\u2019s handwriting. Texture images are then segmented into texture blocks that are each represented as an input vector for the next phase using their local binary pattern histograms.\n\nThe other feature extraction method works by simply feeding the model in the next phase raw segmented images (e.g sentences, words) of the cropped image. Despite being simple, this method takes much more time to execute than the overlapping method mentioned above as it takes an average execution time of 10 seconds per iteration while the overlapping method takes around 3.5 seconds per iteration, in addition to that, the overlapping method yields better accuracy by a slight yet accountable margin making matter disadvantageous for the lines method.\n\n## Model Generation\n\nThe output of the previous step is feed to a model generator function that\u2019s responsible for both generating and training a model. You can either choose a support vector machine classifier or a neural network classifier to classify test images. Both yield exceptional results when running on 1000 random samples, the neural network classifier has a slight edge over the SVM classifier in terms of accuracy as it achieves 98.8% accuracy compared to 98.2%. On the other side of the coin, the SVM classifier is faster than the neural network one having an average iteration runtime of 3.4 seconds instead of 14.7 seconds but keep in mind that neural network classifiers performance can be improved by GPU acceleration which indeed reduced the neural network classifier average iteration runtime from 14.7 seconds to 8.7 seconds.\n\n## Training and Test Samples Generation\n\nAt each epoch, 3 random writers directories are chosen from the dataset where 2 random images are read from each writer's directory and an extra image to be used for testing is randomly chosen from the three writers' directories. This process ends up with 7 labelled images, 6 for training (2 for each writer) and 1 for testing."}}