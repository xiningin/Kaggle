{"cell_type":{"d9e51dc2":"code","8ecdf49b":"code","5aad4313":"code","af68d233":"code","b9663a3c":"code","58ff0201":"code","70ff5b08":"code","f555b059":"code","d502397e":"code","ae7c6034":"code","dfe2ba85":"code","ae780f23":"code","423890b7":"code","6b07460c":"code","2a2906cc":"code","802a325e":"code","1b7d6fc0":"code","de2968a4":"code","ec0ce1a0":"code","ac7784da":"code","9911bccc":"code","72048945":"code","215d312a":"code","e022a1b4":"code","a24bd1ce":"code","194c166e":"code","526c6b03":"code","720cf508":"code","23c74068":"code","cb64366c":"code","a235d386":"code","d5efeec2":"code","94f79b70":"code","d24e8ea5":"code","06048ef8":"code","633bd7c3":"code","38f49be3":"code","c64db2b2":"code","7730fc28":"code","77ce15f7":"code","afd7bc92":"markdown","cc78ab11":"markdown","1aa4524a":"markdown","9655bc5d":"markdown","63e33d3b":"markdown","08365fb7":"markdown","e3cdb571":"markdown","bbb9ac0b":"markdown","6023dce7":"markdown","49cba1e8":"markdown","f2a57805":"markdown","2db2fd4c":"markdown","554c2463":"markdown","aa52fd63":"markdown","70da0412":"markdown","c9538be1":"markdown"},"source":{"d9e51dc2":"%%time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt","8ecdf49b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5aad4313":"auto = pd.read_csv(\"\/kaggle\/input\/vehiceldata\/AutoData.csv\")\nprint(\"Dataset with rows {} and columns {}\".format(auto.shape[0],auto.shape[1]))\nauto.head()","af68d233":"auto.info() # so in this datasets we have 8 float data types, 7 interger data types and 10 object type data.\n# to check if there is any null values","b9663a3c":"auto.isnull().sum(axis=0) # so it's comparebly very clean data with no null values. \n# Now we will move to EDA part of the datasets.","58ff0201":"auto.head() # so considering all the features we have to predict the price.","70ff5b08":"%%time\n# here we are seperating object and numerical data types \nobj_col = []\nnum_col = []\nfor col in auto.columns:\n    if auto[col].dtype=='O':\n        obj_col.append(col)\n    else:\n        num_col.append(col)","f555b059":"print(\"Object data type features \",obj_col)\nprint(\"Numerical data type features \",num_col)\n","d502397e":"from numpy import median\nfor col in obj_col[1:]:\n    plt.figure(figsize=(10,8))\n    sns.violinplot(auto[col],auto[\"price\"])\n    plt.title(\"Price vs \"+col,fontsize=20)\n    plt.xlabel(col,fontsize=12)\n    plt.ylabel(\"Price\",fontsize=12)\n    plt.show()\n#sns.despine()\n# violin plots give best of both worlds \n# it gives boxplot and distribution of data like whether the data is skewed or not.\n# if normally distributed then it's the best you can get.\n# you can also use barplots in this case.","ae7c6034":"plt.figure(figsize=(15,12))\nsns.heatmap(auto.corr(),annot=True,cmap='RdBu_r')\nplt.title(\"Correlation Of Each Numerical Features\")\nplt.show()","dfe2ba85":"for col in num_col[:-1]:\n    plt.figure(figsize=(10,8))\n    sns.jointplot(x = auto[col],y = auto[\"price\"],kind='reg')\n    plt.xlabel(col,fontsize = 15)\n    plt.ylabel(\"Pr\u00efce\",fontsize = 15)\n    plt.grid()\n    plt.show()","ae780f23":"from sklearn.model_selection import train_test_split\nX_tr,X_ts,y_tr,y_ts = train_test_split(auto.drop([\"price\"],axis=1),auto[\"price\"],test_size = 0.2,random_state=42)\nprint(\"Train Data shape \",X_tr.shape)\n#X_tr.head()\nprint(\"Test Data shape \",X_ts.shape)\n#X_ts.head()","423890b7":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse = False,handle_unknown=\"ignore\")\nX_tr_obj = ohe.fit_transform(X_tr[obj_col])\nX_ts_obj = ohe.transform(X_ts[obj_col])\nprint(X_tr_obj.shape)\nprint(X_ts_obj.shape)","6b07460c":"features = ohe.get_feature_names().tolist()","2a2906cc":"X_tr_obj = pd.DataFrame(X_tr_obj,columns= features)\nX_ts_obj = pd.DataFrame(X_ts_obj,columns= features)","802a325e":"auto[\"make\"].value_counts()","1b7d6fc0":"X_tr_obj[\"x0_Nissan versa\"]","de2968a4":"from sklearn.preprocessing import MinMaxScaler\nmin_max = MinMaxScaler()\nX_tr = min_max.fit_transform(X_tr[num_col[:-1]])\nX_ts = min_max.transform(X_ts[num_col[:-1]])\nprint(X_tr.shape)\nprint(X_ts.shape)","ec0ce1a0":"X_tr = pd.DataFrame(X_tr,columns=num_col[:-1])\nX_ts = pd.DataFrame(X_ts,columns=num_col[:-1])","ac7784da":"X_tr = pd.concat([X_tr_obj,X_tr[num_col[:-1]]],axis=1)\nX_ts = pd.concat([X_ts_obj,X_ts[num_col[:-1]]],axis=1)\nprint(X_tr.shape)\nprint(X_ts.shape)","9911bccc":"X_tr.head()","72048945":"from sklearn.metrics import mean_squared_error,r2_score,explained_variance_score\n\n# calculation part\nmodel = LinearRegression()\nmodel.fit(np.array(X_tr[\"enginesize\"]).reshape(-1,1),np.array(y_tr).reshape(-1,1))\ny_pred = model.predict(np.array(X_ts[\"enginesize\"]).reshape(-1,1))\n\n\n# plotting part\nplt.figure(figsize=(10,6))\nsns.scatterplot(x = X_ts[\"enginesize\"],y = y_ts,label = \"Actual Points\",palette=\"set1\")\nplt.plot(X_ts[\"enginesize\"],y_pred,label = \"Estimated Line\")\nplt.title(\"Price Vs Engine Size\",fontsize=20)\nplt.xlabel(\"Engine Size\",fontsize = 15)\nplt.ylabel(\"Price\",fontsize = 15)\nplt.legend()\nplt.grid()\nplt.show()\nprint(\"R2 Score using engine size features is -->\",r2_score(y_ts,y_pred))","215d312a":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE","e022a1b4":"#selecting top 10 features \nlr = LinearRegression(n_jobs=-1)\nrfe = RFE(estimator=lr,n_features_to_select=10)\nrfe.fit(X_tr,y_tr)","a24bd1ce":"selected_feat = X_tr.columns[rfe.ranking_==1]","194c166e":"selected_feat","526c6b03":"X_tr_cat = sm.add_constant(X_tr[selected_feat]) # adding constant ","720cf508":"pd.Series([variance_inflation_factor(X_tr_cat.values, i) \n               for i in range(X_tr_cat.shape[1])], \n              index=X_tr_cat.columns)","23c74068":"model = sm.OLS(np.array(y_tr),X_tr_cat).fit()\nmodel.summary()","cb64366c":"X_tr_obj = X_tr_obj[[\"x6_rear\",\"x7_dohc\",\"x7_l\",\"x7_ohc\",\"x8_eight\",\"x8_twelve\",\"x8_four\"]]","a235d386":"lr = LinearRegression(n_jobs=-1)\nrfe = RFE(estimator=lr,n_features_to_select=5)\nrfe.fit(X_tr[num_col[:-1]],y_tr)","d5efeec2":"selected_feat = X_tr[num_col[:-1]].columns[rfe.ranking_==1]","94f79b70":"selected_feat","d24e8ea5":"X_tr_num = sm.add_constant(X_tr[selected_feat])","06048ef8":"pd.Series([variance_inflation_factor(X_tr_num.values, i) \n               for i in range(X_tr_num.shape[1])], \n              index=X_tr_num.columns)","633bd7c3":"model = sm.OLS(np.array(y_tr),X_tr_num).fit()\nmodel.summary()","38f49be3":"X_tr_main = pd.concat([X_tr_obj,X_tr_num],axis=1)\nX_tr_main.head()","c64db2b2":"model = sm.OLS(np.array(y_tr),X_tr_main).fit()\nmodel.summary()","7730fc28":"top_features = X_tr_main[[\"x6_rear\",\"x7_ohc\",\"x8_four\",\"enginesize\",\"stroke\"]]","77ce15f7":"lr = LinearRegression(n_jobs=-1)\nlr.fit(top_features,y_tr)\nX_ts_main = X_ts[[\"x6_rear\",\"x7_ohc\",\"x8_four\",\"enginesize\",\"stroke\"]]\ny_pred = lr.predict(X_ts_main)\ny_pred_tr = lr.predict(top_features)\nprint(\"r2 score on test data is --> \",r2_score(y_ts,y_pred))\nprint(\"r2 score on train data is --> \",r2_score(y_tr,y_pred_tr))","afd7bc92":"### Importing some useful modules","cc78ab11":"## Multiple Regression Models","1aa4524a":"####  From above we can see only using one features i.e enginesize we can get r2_score of 0.80. This features we will be using in coming models.","9655bc5d":"### Linear Regression","63e33d3b":"### One Hot Encoding of Categorical Features","08365fb7":"#### Categorical Features","e3cdb571":"### Min-Max Scaling of Numerical Features","bbb9ac0b":"1. So from above eda and visualizations we are aware that enginesize is the most important numerical features which contribute more in predicting the target values.\n2. So we have used only engine size features to predict the r2 score which is 0.804","6023dce7":"### Split the dataset using 80:20 ratio","49cba1e8":"#### Numerical Features","f2a57805":"## Result Based On RFE(Reverse Features Selection)","2db2fd4c":"1. There is certain group of features which is highly correlated i.e multicollinearity exist.\n2. One cluster having high correlation between them is wheelbase,carlength,carwidth if you think mathematically the base is nothing but lenth*width.\n3. Second Cluster having high correlation between them is enginesize,horsepower,curbweightand citympg(miles per gallon) if you think as enginesize increases horsepower also increases.And if engine is large enough then it's automatically goinf to increase the total mass of body.\n","554c2463":"### EDA(Exploratory Data Analysis)","aa52fd63":"#### Reading CSV File","70da0412":"1. We have selected some categorical features and numerical features and concat them to get r2 scores of 0.85 on test data.\n2. But as we have seen in above code snippets that we only used enginesize features and got 0.80 r2 scores.\n3. We can also try to calculate adjusted r2 score.\n4. So in above codes we have selected features based on p-values, as p-values approaches zero we can say that \n5. Prob(features contribute more | null hypothesis is true)<0.05 where 0.05 is statistical significant value.\n6. where our h0(null hypothesis) is that there is no correlation between target variable and selected feature.","c9538be1":"## Conclusion"}}