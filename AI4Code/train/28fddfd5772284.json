{"cell_type":{"d6bbf0b2":"code","b4e7d666":"code","6aa8cc64":"code","a8de5507":"code","e7087260":"code","3c54d5f9":"code","fb2812ab":"code","48ec7b3a":"code","8cba43ca":"code","36036e13":"code","59035f93":"code","d26383a3":"code","4b78321e":"code","5da570ef":"code","dba1c24f":"code","65adade8":"code","bc1bbf3e":"code","7a447e22":"code","66e9ace1":"code","a7658760":"code","31392846":"code","9ba1d419":"markdown","f2df7309":"markdown","02ca94b9":"markdown","cca22588":"markdown","08988f91":"markdown","bf95400f":"markdown","26b9f934":"markdown","6c77fa99":"markdown","ce379a09":"markdown"},"source":{"d6bbf0b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4e7d666":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\n\nprint('Tensorflow Version {}'.format(tf.__version__))","6aa8cc64":"import pandas as pd\n\ntrain_dataset = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_dataset = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nprint('Total size of the train dataset is {}'.format(train_dataset.shape))\nprint('Total size of the test dataset is {}'.format(test_dataset.shape))","a8de5507":"y_train = train_dataset['label']\n\nX_train = train_dataset.drop(['label'], axis=1)\n\n\nprint('Size of X_train {}'.format(X_train.shape))\nprint('Size of y_train {}'.format(y_train.shape))","e7087260":"train_dataset.isna().any().describe()","3c54d5f9":"test_dataset.isna().any().describe()","fb2812ab":"# Normalization of the data\nX_train = X_train\/255.0\ntest_dataset = test_dataset\/255.0","48ec7b3a":"# Reshaping of the dataset\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest_dataset = test_dataset.values.reshape(-1,28,28,1)\n\nprint('Shape of the train dataset is {}'.format(X_train.shape))\nprint('Shape of the test dataset is {}'.format(test_dataset.shape))","8cba43ca":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train, num_classes=10)\n\nprint('Shape of y_train after one hot encoding {}'.format(y_train.shape))\n","36036e13":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, shuffle=True, test_size=0.2)\n\nprint('Size of train dataset is {}'.format(x_train.shape))\nprint('Size of the val dataset is {}'.format(x_val.shape))","59035f93":"#visulaization of training set\nimport random\ni = random.randint(10,5000)\nplt.imshow(x_train[i][:,:,0])\n    ","d26383a3":"# setting up model parameters\ninput_shape = (28,28,1)\nbatch_size = 64\nnum_classes = y_train.shape[1]\nepochs = 20","4b78321e":"# CNN model\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), padding ='same', activation='relu', input_shape=input_shape),\n    tf.keras.layers.Conv2D(32, (5,5), padding ='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(num_classes, activation='softmax')   \n    \n])\n\nmodel.compile (optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['accuracy'])","5da570ef":"# creating a callback for 99.55% accuracy\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('acc') is not None and logs.get('acc')> 0.995):\n            print(\"\\n Reached 99.5% of accuracy!!\")\n            self.model.stop_training=True\n            \ncallback= myCallback()","dba1c24f":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)\n\ndatagen.fit(x_train)","65adade8":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n                  epochs=epochs,\n                  validation_data= (x_val, y_val),\n                  callbacks = [callback]\n                   )   ","bc1bbf3e":"history.history.keys()","7a447e22":"fig, ax = plt.subplots(1,2,figsize=(25,5))\n\n#Loss Curve\nax[0].plot(history.history['loss'], color='r', label='Training Loss')\nax[0].plot(history.history['val_loss'], color='b', label='Validation Loss')\nax[0].legend(loc='best')\nax[0].set_title('Loss curve for Training and validation DataSet', color='k', size=15)\n\n#Accuracy Curve\nax[1].plot(history.history['accuracy'], color='r', label='Training Accuracy')\nax[1].plot(history.history['val_accuracy'], color='b', label='Validation Accuracy')\nax[1].legend(loc='best')\nax[1].set_title('Accuracy curve for Training and Validation Dataset', size=15)","66e9ace1":"y_val_predict = model.predict(x_val)\n\ny_val_predict_class = np.argmax(y_val_predict, axis=1)\n\n\ny_val_true_class = np.argmax(y_val, axis=1)\n\n\nconfusion_matrix = tf.math.confusion_matrix(y_val_true_class, y_val_predict_class)\n\n\n# heatmap for confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix, annot=True, fmt='g')","a7658760":"# predict result of test dataset\n\npredict_test = model.predict(test_dataset)\nresults = np.argmax(predict_test, axis=1)\n\n\nprint(results)","31392846":"# converting to csv for submission\nresults = pd.Series(results, name='Label')\n\nfinal_result = pd.concat([pd.Series(range(1, test_dataset.shape[0]+1), name='ImageId'), results], axis=1)\n\n\nfinal_result.to_csv('CNN_digit_classification_TensorFlow.csv', index=False)","9ba1d419":"# 4. Data Augmentation","f2df7309":"# 1.3 Normalizing and Reshaping","02ca94b9":"If you like this notebook, please upvote!! Happy Learning ","cca22588":"# 1.4 One-Hot-Encoding\n\nOne hot encoding of label feature","08988f91":"# 3. CNN Sequential Model","bf95400f":"# 2. Train and Test Split","26b9f934":"# 1.2 Missing Values","6c77fa99":"# 1. Data Preprocessing ","ce379a09":"# 5. Predictions"}}