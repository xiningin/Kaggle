{"cell_type":{"04cebaf8":"code","6c507884":"code","67358154":"code","beaac96b":"code","05c1c5ea":"code","fe8316d6":"code","e81414cc":"code","6ea58c33":"code","9c703de7":"code","9aa2b908":"code","d7580052":"code","3003441a":"code","d70886c7":"code","65a54622":"markdown","a0f98ccd":"markdown","78640666":"markdown","b5d1e0f5":"markdown","53907ab9":"markdown","dc0a9bff":"markdown","2f23f2ba":"markdown","be34bff7":"markdown","9047eeff":"markdown","d8a0b565":"markdown"},"source":{"04cebaf8":"\"\"\"cpu\"\"\"\n# !python -m pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cpu\/torch1.9\/index.html\n\"\"\"gpu\"\"\"    \n!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install detectron2==0.5 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html\n\n!pip install timm\n!git clone https:\/\/github.com\/emiz6413\/SwinT_detectron2.git swin\n!curl -OL https:\/\/github.com\/xiaohu2015\/SwinT_detectron2\/releases\/download\/v1.1\/faster_rcnn_swint_T.pth  # pretrained\n!curl -OL https:\/\/github.com\/emiz6413\/SwinT_detectron2\/releases\/download\/v1.3\/model_0021209.pth  # trained","6c507884":"import os\nfrom pathlib import Path\nfrom ast import literal_eval\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nfrom detectron2.data import (DatasetCatalog, \n                             MetadataCatalog, \n                             build_detection_test_loader\n                            )\nfrom detectron2.data.datasets.coco import convert_to_coco_json\nfrom detectron2.utils.visualizer import Visualizer, ColorMode\nfrom detectron2.structures import BoxMode\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.engine import DefaultTrainer, default_setup, hooks\nfrom detectron2.modeling import GeneralizedRCNNWithTTA\nfrom detectron2.evaluation import COCOEvaluator, DatasetEvaluators, inference_on_dataset\n\nfrom swin.swint import add_swint_config\n\nlogger = setup_logger()","67358154":"data_root = Path('\/kaggle\/input\/tensorflow-great-barrier-reef\/')\ntrain_df = pd.read_csv(str(data_root\/'train.csv'))\ntrain_df['annotations'] = train_df['annotations'].apply(literal_eval)","beaac96b":"def gbl_dataset(df, img_root):\n    dataset = []\n    for i, row in df.iterrows():\n        file_name = str(img_root\/\"video_{}\/{}.jpg\".format(*row['image_id'].split('-')))\n        width, height = Image.open(file_name).size\n        image_id = i\n        annotations = [dict(bbox=[bbox['x'], bbox['y'], bbox['width'], bbox['height']],\n                            bbox_mode=BoxMode.XYWH_ABS,\n                            category_id=0)\n                       for bbox in row['annotations']]\n        \n        dataset.append(\n            dict(file_name=file_name,width=width,\n                 height=height,\n                 image_id=image_id,\n                 annotations=annotations\n                )\n        )\n    return dataset\n\ndef gbl_dataset_wrapper(df, img_root):\n    def wrapper():\n        return gbl_dataset(df, img_root)\n    return wrapper","05c1c5ea":"_train_df = train_df.query(\"video_id != 2\")\n_val_df = train_df.query(\"video_id == 2\")\ntrain_ds = gbl_dataset_wrapper(_train_df, Path('\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'))\nval_ds = gbl_dataset_wrapper(_val_df, Path('\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'))\nDatasetCatalog.pop(\"gbl_train_dataset\", None)\nDatasetCatalog.pop(\"gbl_val_dataset\", None)\nDatasetCatalog.register(\"gbl_train_dataset\", train_ds)\nDatasetCatalog.register(\"gbl_val_dataset\", val_ds)\nMetadataCatalog.get(\"gbl_train_dataset\").thing_classes = [\"starfish\"]\nMetadataCatalog.get(\"gbl_val_dataset\").thing_classes = [\"starfish\"]\n#Convert validation dataset to coco format and dump it for evaluation\nconvert_to_coco_json('gbl_val_dataset', output_file='.\/output\/inference\/gbl_val_dataset_coco_format.json', allow_cached=False)","fe8316d6":"gbl_ds = DatasetCatalog.get(\"gbl_train_dataset\")\nmetadata = MetadataCatalog.get('gbl_train_dataset')","e81414cc":"for data in gbl_ds:\n    if len(data['annotations']):\n        break\nim = cv2.cvtColor(cv2.imread(data['file_name']), cv2.COLOR_BGR2RGB)\nv = Visualizer(im, \n               metadata=MetadataCatalog.get('gbl_train_dataset'),\n               scale=0.5)\nout = v.draw_dataset_dict(data)\nim = Image.fromarray(out.get_image())\nim","6ea58c33":"class Trainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        return COCOEvaluator(dataset_name=dataset_name,\n                             tasks=[\"bbox\"],\n                             distributed=True,\n                             output_dir=output_folder)\n    \n    @classmethod\n    def build_tta_model(cls, cfg, model):\n        return GeneralizedRCNNWithTTA(cfg, model)\n    \n    @classmethod\n    def test_with_TTA(cls, cfg, model):\n        # In the end of training, run an evaluation with TTA\n        # Only support some R-CNN models.\n        logger.info(\"Running inference with test-time augmentation ...\")\n        model = self.build_tta_model(cfg, model)\n        evaluators = [\n            cls.build_evaluator(\n                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n            )\n            for name in cfg.DATASETS.TEST\n        ]\n        res = cls.test(cfg, model, evaluators)\n        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n        return res","9c703de7":"TRAIN_STEPS = 4242 # only 4242 images with annotation\ncfg = get_cfg()\nadd_swint_config(cfg)\ncfg.merge_from_file('swin\/configs\/SwinT\/faster_rcnn_swint_T_FPN_3x_.yaml')\ncfg.DATASETS.TRAIN = (\"gbl_train_dataset\",)\ncfg.DATASETS.TEST = (\"gbl_val_dataset\",)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\ncfg.MODEL.WEIGHTS = None\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.MAX_ITER = TRAIN_STEPS * 10\ncfg.SOLVER.STEPS = []\ncfg.SOLVER.CHECKPOINT_PERIOD = TRAIN_STEPS\ncfg.TEST.EVAL_PERIOD = TRAIN_STEPS \nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = Trainer(cfg)\ntrainer.resume_or_load(resume=False)","9aa2b908":"\"\"\"\n# load pretrained weights\nother_weights = torch.load('faster_rcnn_swint_T.pth')['model']\nself_weight = trainer.model.state_dict()\nfor name, param in self_weight.items():\n    if name in other_weights:\n        if other_weights[name].shape == param.shape:\n            self_weight[name] = other_weights[name]\n        else:\n            print(f\"size mismatch at {name}\")\n    else:\n        print(f\"layer {name} does not exist\")\ntrainer.model.load_state_dict(self_weight)\ntrainer.train()\n\"\"\"","d7580052":"trainer.model.load_state_dict(torch.load('model_0021209.pth')['model'])\ntrainer.test(cfg, trainer.model)","3003441a":"val_ds = DatasetCatalog.get(\"gbl_val_dataset\")\ntrainer.model.eval()\nmetadata = MetadataCatalog.get('gbl_train_dataset')","d70886c7":"for _ in range(5):\n    idx = np.random.randint(0, len(val_ds))\n    data = val_ds[idx]\n    print(data['file_name'])\n    im = cv2.imread(data['file_name'])\n    im_tensor = torch.from_numpy(im).permute(2,0,1)  # h, w, c -> c, h, w\n    h, w, _ = im.shape\n    with torch.no_grad():\n        pred = trainer.model([{\"image\": im_tensor.cuda(), \"width\": w, \"height\": h}])\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(pred[0][\"instances\"].to(\"cpu\"))\n    plt.figure()\n    plt.imshow(out.get_image())","65a54622":"## 2.1 Define a dataset function","a0f98ccd":"## SwinTransformer RCNN training and evaluation code \nThis code is based on https:\/\/github.com\/xiaohu2015\/SwinT_detectron2<br>\n\nI splitted video 0 and 1 to training and video 2 to validation.<br>\nSo far, I could only train the model for 5 epochs and only achieved:<br>\n**bbox\/AP = 15.2<br>**\n**bbox\/AP50 = 35.7<br>**\n**bbox\/AP75 = 8.0<br>**\non the validation set.\n\nHowever, I'd like to share my work and will continue working on it to further improve.","78640666":"## 3.1 Define a custom Trainer to evaluate on custom dataset","b5d1e0f5":"## 1. Load data","53907ab9":"## 4.2 Visualize a few prediction examples","dc0a9bff":"## 3.2 Set a config","2f23f2ba":"## 2.2 Check the Dataset\nvisualize the dataset for verification","be34bff7":"## 4.1 Evaluate\nevaluate on validation set","9047eeff":"## 3.3 Train\n\nI could only run for 5 epochs due to runtime quota","d8a0b565":"## Install requirements\ninstall Detectron2, timm and Detectron2 version implementation of SwinTransformer"}}