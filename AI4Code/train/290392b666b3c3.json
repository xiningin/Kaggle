{"cell_type":{"1d0a83b9":"code","e92beb0f":"code","ba94f5c1":"code","0b6adbe9":"code","9b0b0681":"code","7065b3b7":"code","3b1ad797":"code","f4299dc0":"code","3976f5e7":"code","a756fd77":"code","3f7d538d":"code","b0aa188e":"code","9016719f":"code","7c98d35e":"code","24cfb0da":"code","cf6eed76":"code","b8856e88":"code","58cac73c":"code","1b7c52bb":"code","7f44cba6":"code","a5c4b6a4":"code","d058e4be":"code","f698756e":"code","28e9e903":"code","be28ff03":"code","42c5ab23":"code","ef5dd44e":"code","49defa3c":"code","ccc640e3":"code","d46d119d":"code","aa0d44f9":"code","fd1270da":"code","ccdd6da5":"code","01ba8871":"code","686e8daa":"code","0a364709":"code","fcc1bddf":"code","fdf92891":"code","dc59ea5b":"code","720f53e6":"code","a2c2e6ea":"code","f9a0b2de":"code","bef53fbf":"code","82f8eb88":"code","dd5dde64":"code","b975e7fd":"code","bae1d982":"code","d6fbdd17":"code","2ab2a7ff":"code","6b94e3d6":"code","45e05489":"code","273a6ba1":"code","7169f53d":"code","1528987c":"code","27fc5cca":"code","6adb4d77":"code","00771f32":"code","774c9a01":"code","bc550a75":"code","bb23bcbc":"code","d23c985f":"code","ab4b46fe":"code","8ca16da2":"code","006d5705":"code","794f8c8e":"code","47b972ba":"code","710a6f99":"code","09968646":"markdown","1fc1a966":"markdown","5f445390":"markdown","f5308794":"markdown","e9dcc2bd":"markdown","53ea5e03":"markdown","b6809d69":"markdown","9ff89599":"markdown","4a9fcd15":"markdown","6b60e97f":"markdown","9c4641b8":"markdown","3beb6d50":"markdown","861ebed0":"markdown","e770990d":"markdown","16161e84":"markdown","695e2bdb":"markdown","3a4e2316":"markdown","bdf31a10":"markdown","c5cb21b1":"markdown","1baf811a":"markdown","60b14a79":"markdown","ec6d5328":"markdown","c600294c":"markdown","268f9414":"markdown","c390f49d":"markdown","05453f2a":"markdown","a57edb2d":"markdown","d5b13d7f":"markdown","5262349e":"markdown","1e5c0ba2":"markdown","654f332f":"markdown","130bd5bb":"markdown","85e4a888":"markdown","d655bea0":"markdown","e05179d8":"markdown","9cd6c073":"markdown","6b9d5d20":"markdown","776f337f":"markdown","89e33ad8":"markdown","cab73655":"markdown","6728d3cc":"markdown","8047f36e":"markdown","f558332d":"markdown","e5446a06":"markdown","9388d6fd":"markdown","2440bfab":"markdown"},"source":{"1d0a83b9":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/2020-cost-of-living\/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","e92beb0f":"import seaborn as sns","ba94f5c1":"sns.color_palette(\"Greens\",  as_cmap=True)","0b6adbe9":"Greens_palette = sns.color_palette(\"Greens\", 20)\nsns.palplot(Greens_palette)","9b0b0681":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndoc1 = \"apple banana everyone like likey watch card holder\"\ndoc2 = \"apple banana coupon passport love you\"\n\nprint(jaccard(doc1, doc2))","7065b3b7":"# Most basic stuff for EDA.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\n\nimport string\nimport re\n\n# Libraries for text preprocessing.\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport os\nimport random\nimport warnings\nimport time\nimport datetime\nfrom tqdm.autonotebook import tqdm\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n# Modelling Packages\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tokenizers import ByteLevelBPETokenizer\nfrom transformers import RobertaModel, RobertaConfig\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import logging\nlogging.set_verbosity_warning()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","3b1ad797":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","f4299dc0":"# Taking general look at the both datasets.\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","3976f5e7":"# Checking observation and feature numbers for train and test data.\n\nprint(train.shape)\nprint(test.shape)","a756fd77":"# Displaying target distribution.\n\nGreens_palette_3 = sns.color_palette(\"Greens\", 3)\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5), dpi=100)\nsns.countplot(train['sentiment'],palette = Greens_palette_3, ax=axes[0])\naxes[1].pie(train['sentiment'].value_counts(),\n            labels=['neutral', 'positive', 'negative'],\n            autopct='%1.2f%%',\n            shadow=True,\n            colors = Greens_palette_3,\n            explode=(0.05, 0.05, 0.05),\n            startangle=60)\nfig.suptitle('Twitter sentiment Extaction in Train', fontsize=20)\nplt.show()","3f7d538d":"fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5), dpi=100)\nsns.countplot(test['sentiment'],palette = Greens_palette_3,ax=axes[0])\naxes[1].pie(test['sentiment'].value_counts(),\n            labels=['neutral', 'positive', 'negative'],\n            autopct='%1.2f%%',\n            shadow=True,\n            colors = Greens_palette_3,\n            explode=(0.05, 0.05, 0.05),\n            startangle=60)\nfig.suptitle('Twitter sentiment Extaction in Test', fontsize=20)\nplt.show()","b0aa188e":"# Check for missing values\n\ntrain.isnull().value_counts(), test.isnull().value_counts()","9016719f":"# Remove missing values\n\ntrain.dropna(inplace=True)","7c98d35e":"# Some basic helper functions to clean text by punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\ndef remove_comma(readData):\n    text = re.sub('[-=+,#\/\\?:^$.@*\\\"\u203b~&%\u318d!\u300f\\\\\u2018|\\(\\)\\[\\]\\<\\>`\\'\u2026\u300b]', '', readData)\n    return text\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions (train Text)\n\ntrain['text_clean'] = train['text'].apply(lambda x: remove_URL(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_punct(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_html(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_emoji(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_comma(x))                     \n\n# Applying helper functions (train Selected Text)\n\ntrain['ST_clean'] = train['selected_text'].apply(lambda x: remove_URL(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_punct(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_html(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_emoji(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_comma(x))\n\n# Applying helper functions (test Text)\n\ntest['text_clean'] = test['text'].apply(lambda x: remove_URL(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_punct(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_html(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_emoji(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_comma(x))      ","24cfb0da":"train.head()","cf6eed76":"test.head()","b8856e88":"# Tokenizing the tweet base texts.\n\ntrain['tokenized_text'] = train['text_clean'].apply(word_tokenize)\ntrain['tokenized_ST'] = train['ST_clean'].apply(word_tokenize)\ntest['tokenized_text'] = test['text_clean'].apply(word_tokenize)\ndisplay(train.sample(5))\ndisplay(test.sample(5))","58cac73c":"# Lower casing clean text.\n\ntrain['lower_text'] = train['tokenized_text'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrain['lower_ST'] = train['tokenized_ST'].apply(\n    lambda x: [word.lower() for word in x])\n\ntest['lower_text'] = test['tokenized_text'].apply(\n    lambda x: [word.lower() for word in x])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","1b7c52bb":"# Removing stopwords.\n\ntrain['stopwords_removed_text'] = train['lower_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['stopwords_removed_ST'] = train['lower_ST'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntest['stopwords_removed_text'] = test['lower_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","7f44cba6":"# Applying part of speech tags.\n\ntrain['pos_tags_text'] = train['stopwords_removed_text'].apply(nltk.tag.pos_tag)\ntrain['pos_tags_ST'] = train['stopwords_removed_ST'].apply(nltk.tag.pos_tag)\ntest['pos_tags_text'] = test['stopwords_removed_text'].apply(nltk.tag.pos_tag)\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","a5c4b6a4":"# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ntrain['wordnet_pos_text'] = train['pos_tags_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntrain['wordnet_pos_ST'] = train['pos_tags_ST'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntest['wordnet_pos_text'] = test['pos_tags_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","d058e4be":"# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ntrain['lemmatized_text'] = train['wordnet_pos_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrain['lemmatized_text'] = train['lemmatized_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['lemma_str_text'] = [' '.join(map(str, l)) for l in train['lemmatized_text']]\n\ntrain['lemmatized_ST'] = train['wordnet_pos_ST'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrain['lemmatized_ST'] = train['lemmatized_ST'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['lemmatized_str_ST'] = [' '.join(map(str, l)) for l in train['lemmatized_ST']]\n\ntest['lemmatized_text'] = test['wordnet_pos_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntest['lemmatized_text'] = test['lemmatized_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntest['lemma_str_text'] = [' '.join(map(str, l)) for l in test['lemmatized_text']]\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","f698756e":"# Creating a new feature for the visualization.\n# text_clean Lengths visualization.\ntrain['Character Count'] = train['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[:2, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#008d62')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[2:, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#008d62')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, ax=ax3, color='#008d62')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    plt.suptitle(f'{title}', fontsize=24)\n\n# Greens_palette_3 = sns.color_palette(\"Greens\", 3)\n# sns.palplot(Greens_palette_3)","28e9e903":"plot_dist3(train[train['sentiment'] == 'positive'], 'Character Count',\n           'Characters Per \"positive\" Tweet')","be28ff03":"plot_dist3(train[train['sentiment'] == 'neutral'], 'Character Count',\n           'Characters Per \"neutral\" Tweet')","42c5ab23":"plot_dist3(train[train['sentiment'] == 'negative'], 'Character Count',\n           'Characters Per \"negative\" Tweet')","ef5dd44e":"# ST_clean Lengths visualization.\n\ntrain['Character Count ST'] = train['ST_clean'].apply(lambda x: len(str(x)))","49defa3c":"plot_dist3(train[train['sentiment'] == 'positive'], 'Character Count ST',\n           'Characters Per \"positive\" Tweet')","ccc640e3":"plot_dist3(train[train['sentiment'] == 'neutral'], 'Character Count ST',\n           'Characters Per \"neutral\" Tweet')","d46d119d":"plot_dist3(train[train['sentiment'] == 'negative'], 'Character Count ST',\n           'Characters Per \"negative\" Tweet')","aa0d44f9":"# Test data : text_clean Lengths visualization.\n\ntest['Character Count'] = test['text_clean'].apply(lambda x: len(str(x)))","fd1270da":"plot_dist3(test[test['sentiment'] == 'positive'], 'Character Count',\n           'Characters Per \"positive\" Tweet')","ccdd6da5":"plot_dist3(test[test['sentiment'] == 'neutral'], 'Character Count',\n           'Characters Per \"neutral\" Tweet')","01ba8871":"plot_dist3(test[test['sentiment'] == 'negative'], 'Character Count',\n           'Characters Per \"negative\" Tweet')","686e8daa":"def plot_word_number_histogram(textne, textpo, textng):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textne.str.split().map(lambda x: len(x)), ax=axes[0], color='#008d62')\n    sns.distplot(textpo.str.split().map(lambda x: len(x)), ax=axes[1], color='#008d62')\n    sns.distplot(textng.str.split().map(lambda x: len(x)), ax=axes[2], color='#008d62')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('neutral')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('positive')\n    axes[2].set_xlabel('Word Count')\n    axes[2].set_title('negative')\n    \n    fig.suptitle('Punctuations in tweets', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","0a364709":"plot_word_number_histogram(train[train['sentiment'] == 'neutral']['text'],\n                           train[train['sentiment'] == 'positive']['text'],\n                           train[train['sentiment'] == 'negative']['text'])","fcc1bddf":"plot_word_number_histogram(train[train['sentiment'] == 'neutral']['selected_text'],\n                           train[train['sentiment'] == 'positive']['selected_text'],\n                           train[train['sentiment'] == 'negative']['selected_text'])","fdf92891":"plot_word_number_histogram(test[test['sentiment'] == 'neutral']['text'],\n                           test[test['sentiment'] == 'positive']['text'],\n                           test[test['sentiment'] == 'negative']['text'])","dc59ea5b":"def plot_word_len_histogram(textne, textpo, textng):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textne.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[0], color='#008d62')\n    sns.distplot(textpo.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[1], color='#008d62')\n    sns.distplot(textng.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[2], color='#008d62')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('neutral')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('positive')\n    axes[2].set_xlabel('Word Count')\n    axes[2].set_title('negative')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()","720f53e6":"plot_word_len_histogram(train[train['sentiment'] == 'neutral']['text'],\n                        train[train['sentiment'] == 'positive']['text'],\n                        train[train['sentiment'] == 'negative']['text'])","a2c2e6ea":"plot_word_len_histogram(train[train['sentiment'] == 'neutral']['selected_text'],\n                        train[train['sentiment'] == 'positive']['selected_text'],\n                        train[train['sentiment'] == 'negative']['selected_text'])","f9a0b2de":"plot_word_len_histogram(test[test['sentiment'] == 'neutral']['text'],\n                        test[test['sentiment'] == 'positive']['text'],\n                        test[test['sentiment'] == 'negative']['text'])","bef53fbf":"lis = [\n    train[train['sentiment'] == 'neutral']['lemma_str_text'],\n    train[train['sentiment'] == 'positive']['lemma_str_text'],\n    train[train['sentiment'] == 'negative']['lemma_str_text']\n]","82f8eb88":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","dd5dde64":"# Displaying most common words in train data.\n\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:20]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\naxes[0].set_title('neutral in Tweets')\naxes[1].set_title('positive in Tweets')\naxes[2].set_title('negative in Tweets')\n\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\naxes[2].set_xlabel('Count')\naxes[2].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams TOP 20 in train data', fontsize=24, va='baseline')\nplt.tight_layout()\n\n\n\n# Displaying most common words in test data.\n\nlis_test = [\n    test[test['sentiment'] == 'neutral']['lemma_str_text'],\n    test[test['sentiment'] == 'positive']['lemma_str_text'],\n    test[test['sentiment'] == 'negative']['lemma_str_text']\n]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_test, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:20]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\naxes[0].set_title('neutral in Tweets')\naxes[1].set_title('positive in Tweets')\naxes[2].set_title('negative in Tweets')\n\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\naxes[2].set_xlabel('Count')\naxes[2].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams TOP 20 in test data', fontsize=24, va='baseline')\nplt.tight_layout()","b975e7fd":"def ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\n        axes[0].set_title('neutral in Tweets')\n        axes[1].set_title('positive in Tweets')\n        axes[2].set_title('negative in Tweets')\n        \n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        axes[2].set_xlabel('Count')\n        axes[2].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()\n        \n        \ndef ngrams_test(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis_test, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\n        axes[0].set_title('neutral in Tweets')\n        axes[1].set_title('positive in Tweets')\n        axes[2].set_title('negative in Tweets')\n        \n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        axes[2].set_xlabel('Count')\n        axes[2].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","bae1d982":"ngrams(2, 'Most Common Bigrams in train')\nngrams_test(2, 'Most Common Bigrams in test')","d6fbdd17":"ngrams(3, 'Most Common Trigrams in train')\nngrams_test(3, 'Most Common Bigrams in test')","2ab2a7ff":"mask = np.array(Image.open('..\/input\/masksforwordclouds\/twitter_mask3.jpg'))","6b94e3d6":"fig, axes = plt.subplots(1,3, figsize=(24,12))\nsentiment_list = np.unique(train['sentiment'])\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Greens\").generate(\" \".join(train[train['sentiment']==sentiment]['lemma_str_text']))\n    \n    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.3,0.8,\"WordCloud by sentiment per selected text in train Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=22)\nplt.show()","45e05489":"fig, axes = plt.subplots(1,3, figsize=(24,12))\nsentiment_list = np.unique(test['sentiment'])\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Greens\").generate(\" \".join(test[test['sentiment']==sentiment]['lemma_str_text']))\n    \n    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.3,0.8,\"WordCloud by sentiment per selected text in test Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=22)\nplt.show()","273a6ba1":"# Train\ntrain_df = train[['textID','text','selected_text','sentiment']]\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\n# Test\ntest_df = test[['textID','text','sentiment']]\ntest_df['text'] = test_df['text'].astype(str)","7169f53d":"train_df.head(3)","1528987c":"test_df.head(3)","27fc5cca":"if torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","6adb4d77":"def set_seed(seed):\n    # Set random seed for reproducibility\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n        \nseed = 777\nset_seed(seed)","00771f32":"MAX_LENGTH = 100\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 16\nEPOCHS = 10","774c9a01":"PATH = \"..\/input\/roberta-base\"\n\ntokenizer = ByteLevelBPETokenizer(\n            vocab = f'{PATH}\/vocab.json', \n            merges= f'{PATH}\/merges.txt', \n            add_prefix_space = True,\n            lowercase=True)\n\ntokenizer.enable_truncation(max_length=512)\n\n\nclass TextDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n        #data loading\n        self.df = df\n        self.selected_text = \"selected_text\" in df\n        self.tokenizer = tokenizer\n        self.max_length = MAX_LENGTH\n        \n    def __len__(self):\n        #len(dataset) i.e., the total number of samples\n        return len(self.df)\n\n    \n    def get_data(self, row):\n        #processing the data\n        text = \" \"+\" \".join(row.text.lower().split()) # clean the text\n        encoded_input = self.tokenizer.encode(text) # the sentence to be encoded\n        \n        sentiment_id = {\n                'positive': 1313,\n                'negative': 2430,\n                'neutral': 7974\n            }  # stating the ids of the sentiment values \n        \n        #print ([list((i, encoded_input[i])) for i in range(len(encoded_input))])\n\n        \n        input_ids = [101] + [sentiment_id[row.sentiment]] + [102] + encoded_input.ids + [102]\n                 \n        #ID offsets       \n        offsets = [(0, 0)] * 3 + encoded_input.offsets + [(0, 0)]  # since first 3 are [CLS] ...sentiment tokens... [SEP]\n        \n        \n        pad_len = self.max_length - len(input_ids)    \n        if pad_len > 0:\n            input_ids += ([0] * pad_len)\n            offsets += ([(0, 0)] * pad_len)\n                       \n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        \n        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n        masks = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0))  \n        \n        masks = torch.tensor(masks, dtype=torch.long)\n        offsets = torch.tensor(offsets, dtype=torch.long)\n                \n        return input_ids, masks, text, offsets\n    \n    def get_target_ids(self, row, text, offsets):\n        # preparing data only for the training\n        selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n\n        string_len = len(selected_text) - 1\n        \n        idx0 = None\n        idx1 = None\n            \n        for ind in (position for position, line in enumerate(text) if line == selected_text[1]):\n            if \" \" + text[ind: ind+string_len] == selected_text:\n                idx0 = ind\n                idx1 = ind + string_len - 1\n                break\n                \n        char_targets = [0] * len(text)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n                \n        # Start and end tokens\n        target_idx = []\n        for k, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                try:\n                    target_idx.append(k)\n                except:\n                    continue\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n                \n        return selected_text, targets_start, targets_end\n    \n   \n    \n    def __getitem__(self, index): \n        \n        # addressing each row by its index\n        # dataset[index] i.e., generates one sample of data\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, text, offsets = self.get_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['text'] = text\n        data['offsets'] = offsets\n        data['sentiment'] = row.sentiment\n        \n        if self.selected_text:   \n            # checking if selected text exists\n            # This part only exists in the training\n            selected_text,start_index, end_index = self.get_target_ids(row, text, offsets)\n            data['start_index'] = start_index\n            data['end_index'] = end_index\n            data['selected_text'] = selected_text\n                \n        return data","bc550a75":"e = TextDataset(train_df, tokenizer, MAX_LENGTH)\ne[1]","bb23bcbc":"class TextModel(nn.Module):\n     \n    def __init__(self):\n        super(TextModel, self).__init__()\n        \n        # RoBERTa encoder \n        config = RobertaConfig.from_pretrained(\n            '..\/input\/roberta-base\/config.json', output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            '..\/input\/roberta-base\/pytorch_model.bin', config=config)\n\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n    \n        self.drop0 = nn.Dropout(0.5)\n        self.l0 = nn.Linear(config.hidden_size * 2,config.hidden_size) \n        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        \n        self.drop1 = nn.Dropout(config.hidden_dropout_prob)\n        self.l1 = nn.Linear(config.hidden_size, 2) \n        # The output will have two dimensions- start and end logits\n        \n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        torch.nn.init.normal_(self.l0.bias, 0)\n        \n    def forward(self, ids, masks): \n        # token_type_ids\n        # Return the hidden states from the RoBERTa backbone\n        # Type: torch tensor\n        last_hidden_state, pooled_output, hidden_states = self.roberta(input_ids=ids, attention_mask=masks, return_dict=False)\n        \n        # input_ids.shape and attention_mask.shape both will be of the size (batch size x seq length)\n        # print(last_hidden_state.shape) : torch.Size([24, 100, 768])\n        # But why 768? \n        # This is the number of hidden units in the feedforward-networks. We can verify that by checking the config.\n        # Concatenate the last two hidden states\n        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)   \n        # out = torch.mean(out, 0) # take the mean along axis 0\n        \n        # adding dropouts and linear layers\n        out = self.drop0(out)\n        out = F.relu(self.l0(out))\n        out = self.drop1(out) \n        out = self.l1(out) \n        \n        # splitting the tensor into two logits \n        start_logits, end_logits = out.split(1, dim=-1)\n        \n        # dimension along which to split the tensor.\n        # Return a tensor with all the dimensions of input of size 1 removed, for both the logits.\n        start_logits = start_logits.squeeze()  \n        \n        # Squeezing a tensor removes the dimensions or axes that have a length of one\n        end_logits = end_logits.squeeze() \n        \n        return start_logits, end_logits\n\n# Training and validation dataloaders\ndef train_val_dataloaders(df, train_idx, val_idx, batch_size):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TextDataset(train_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=0,   # to avoid multi-process, keep it at 0\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TextDataset(val_df, tokenizer, MAX_LENGTH),\n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\n# Test dataloader\ndef test_loader(df, batch_size=TEST_BATCH_SIZE):\n    loader = torch.utils.data.DataLoader(\n        TextDataset(test_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)    \n    return loader","d23c985f":"def loss_function(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","ab4b46fe":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef find_jaccard_score(text, selected_text, sentiment, offsets, start_logits, end_logits): #start_idx, end_idx\n    start_pred = np.argmax(start_logits) # Predicted start index using argmax\n    end_pred = np.argmax(end_logits) # Predicted end index using argmax\n    if (end_pred <= start_pred) or sentiment == 'neutral' or len(text.split()) < 2:\n        enc = tokenizer.encode(text)\n        prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])   \n    else:\n        prediction = get_selected_text(text, start_pred, end_pred, offsets)\n    true = selected_text\n    \n    return jaccard(true, prediction), prediction","8ca16da2":"train_loss = []\nval_loss = []\njac_train = []\njac_val = []\n\ndef TrainModel(model, dataloaders_dict, optimizer, num_epochs, scheduler, device, filename): \n\n    # Set device as `cuda` (GPU)\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        for key in ['train', 'val']:\n            if key == 'train':\n                model.train()\n                dataloaders = dataloaders_dict['train']\n            else:\n                model.eval()\n                dataloaders = dataloaders_dict['val']\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            # Set tqdm to add loading screen and set the length\n            loader = tqdm(dataloaders, total=len(dataloaders))\n            #print(len(dataloaders))\n                        \n            # loop over the data iterator, and feed the inputs to the network\n            # Train the model on each batch\n            for (idx, data) in enumerate(loader):\n                ids = data['ids']\n                masks = data['masks']\n                text = data['text']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_index']\n                end_idx = data['end_index']\n                sentiment = data['sentiment']\n\n                model.zero_grad()\n                optimizer.zero_grad()\n                \n                ids = ids.to(device, dtype=torch.long)\n                masks = masks.to(device, dtype=torch.long)\n                start_idx = start_idx.to(device, dtype=torch.long)\n                end_idx = end_idx.to(device, dtype=torch.long)\n\n                with torch.set_grad_enabled(key == 'train'): \n\n                    start_logits, end_logits = model(ids, masks) \n                    \n                    loss = loss_function(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if key == 'train':\n                        if idx != 0: \n                            loss.backward() # Perform a backward pass to calculate the gradients\n                        optimizer.step() # Update parameters and take a step using the computed gradient\n                        scheduler.step() # Update learning rate schedule                        \n                        \n                        # Clip the norm of the gradients to 1.0.\n                        # This is to help prevent the \"exploding gradients\" problem.\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n                        \n                    epoch_loss += loss.item() * len(ids)\n                    \n                    # Move logits to CPU\n                    # detaching these outputs so that the backward passes stop at this point\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    selected_text = data['selected_text']\n                    \n                    filtered_sentences = []\n                    for i, t_data in enumerate(text):                 \n                        jaccard_score, filtered_output = find_jaccard_score(\n                            t_data,\n                            selected_text[i],\n                            sentiment[i],\n                            offsets[i],\n                            start_logits[i], \n                            end_logits[i])\n                        epoch_jaccard += jaccard_score\n                        filtered_sentences.append(filtered_output)\n            \n            # Calculate the average loss over the training data\n            epoch_loss = epoch_loss \/ len(dataloaders.dataset)\n            # Calculate the average jaccard score over the training data\n            epoch_jaccard = epoch_jaccard \/ len(dataloaders.dataset)\n            \n            print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, key, epoch_loss, epoch_jaccard))\n            \n            # Store the loss value for plotting the learning curve.\n            if key == 'train':\n                train_loss.append(epoch_loss)\n                jac_train.append(epoch_jaccard)\n                \n            else:\n                val_loss.append(epoch_loss)\n                jac_val.append(epoch_jaccard)\n    \n    torch.save(model.state_dict(), filename)","006d5705":"skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)","794f8c8e":"for fold, (idxTrain, idxVal) in enumerate(skf.split(train_df, train_df.sentiment), start=1):\n    \n    print('#'*10)\n    print('# FOLD %i #'%(fold))\n    print('#'*10)\n    \n    model = TextModel()\n    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01, correct_bias=False)\n    dataloaders_dict = train_val_dataloaders(train_df, idxTrain, idxVal, batch_size=TRAIN_BATCH_SIZE)\n    num_training_steps = int(len(train_df) \/ EPOCHS * TRAIN_BATCH_SIZE)\n    \n    # default #use a linear scheduler with no warmup steps\n    scheduler = get_linear_schedule_with_warmup(\n                  optimizer,\n                  num_warmup_steps=0, \n                  num_training_steps=num_training_steps\n                )    \n    TrainModel(\n        model, \n        dataloaders_dict, \n        optimizer, \n        EPOCHS,\n        scheduler,\n        device,\n        f'bert_fold{fold}.pth')","47b972ba":"%%time\n\nt_loader = test_loader(test_df)\npredictions = []\nmodels = []\nfor fold in range(skf.n_splits):\n    model = TextModel()\n    model.to(device)\n    model.load_state_dict(torch.load(f'.\/bert_fold{fold+1}.pth'))\n    model.eval()\n    models.append(model)\n\nloader = tqdm(t_loader, total=len(t_loader))\nfor (idx, data) in enumerate(loader):\n    ids = data['ids'].to(device)\n    masks = data['masks'].to(device)\n    text = data['text']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    \n    for i, t_data in enumerate(text):\n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred >= end_pred:\n            enc = tokenizer.encode(t_data)\n            prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])\n        else:\n            prediction = get_selected_text(t_data, start_pred, end_pred, offsets[i])\n        predictions.append(prediction)","710a6f99":"sub_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nsub_df['selected_text'] = predictions\n# post-processing trick\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\n\nsub_df.sample(10)","09968646":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>In Bigrams and Trigrams, too, train and test had similar results.<\/b><\/div>","1fc1a966":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Both train and test show similar distributions in positive, neutral, and negative.<\/b><\/div>","5f445390":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Word Lengths <\/b>","f5308794":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Seed Setting <\/b>","e9dcc2bd":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>There are two words that appear in both document 1 and 2 in total: banana and apple. Now dividing the number of intersections by the number of assemblies calculates the jacquard similarity.<\/b><\/div>","53ea5e03":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #226D28\"><b>\u2705 Load Green palette <\/b><\/p>","b6809d69":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Introduction<\/b> \ud83c\udf40\n\n<center><img src=\"https:\/\/junpyopark.github.io\/assets\/img\/sentiment.png\"\/ width=\"800\" height=\"700\" ><\/center>\n\n\n<div class=\"alert success-alert\">\n\u2714 <b>General information about competitions : <\/b>\n\n<br>In this competition, we must predict whether phrases such as **\"Myridiculous dog is amazing\"** will have a positive or negative effect.\nVarious NLP frameworks exist, such as BERT, to make these predictions.\nMy notebook will introduce **basic and common data clearing methods to help predict NLP models.**\nI will also write modeling using RoBERTa, so please look forward to it!\n<\/div>","9ff89599":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Why should I lower? <\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>This is because words can be saved by changing them to lowercase letters and reducing the number of words through subsequent processes.<\/b><\/div>","4a9fcd15":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Submission<\/b> \ud83c\udf40","6b60e97f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 What is RoBERTa ? <\/b>\n    \n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>By adjusting the hyperparameters and learning data sizes of BERT, we are a model that shows the performance of post-BERT as well as that of post-BERT, which is comparable to that of post-BERT, or much better. In particular, RoBERTa demonstrates that by rebuilding BERT like this, the masked language model alone is sufficiently capable of competing with autoregressive language modeling such as XLNet.<\/b><\/div>","9c4641b8":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Most Common Bigrams <\/b>","3beb6d50":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 What is the difference between BERT and RoBERTa? <\/b>\n    \n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Learning with longer, larger batch with more data<\/b><br><br>\u2714 <b>Remove next sentence prediction objective<\/b><br><br>\u2714 <b>Learning with longer sequences<\/b><br><br>\u2714 <b>Change masking dynamically<\/b><\/div>\n\n<center><img src=\"https:\/\/vanche.github.io\/assets\/images\/roberta_spanbert\/roberta.png\"\/ width=\"700\" height=\"500\" ><\/center>","861ebed0":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>N-gram Analysis<\/b> \ud83c\udf40","e770990d":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Modelling <\/b>","16161e84":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>The train and test data sets each have 4, 3 features, 2,7481 for train and 3,534 for test.<\/b><\/div>","695e2bdb":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>References<\/b> \ud83c\udf40\n    \n<div class=\"alert success-alert\"><b>\n\u2714 https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds\/data <br><br>\n\u2714 https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch <br><br>\n\u2714 https:\/\/www.kaggle.com\/aditidutta\/tweet-sentiment-extraction-pytorch <br><br>\n\u2714 https:\/\/brunch.co.kr\/@choseunghyek\/7 <br><br>\n\u2714 https:\/\/vanche.github.io\/spanbert_roberta\/ <b>\n<\/div>","3a4e2316":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Let's find out about Lemmatizing ! <\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>First of all, Stemming and Lemmatizing, which we commonly know, are similar, but there is a big difference.<\/b><\/div>\n\n<br>\n\n<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQgUAJcKqcD-WlJa3rz6-5FInyAG_EpoXxHIQ&usqp=CAU\"\/ ><\/center>\n\n<br>\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Stemming sometimes creates words that do not exist, and Lemmatizing sometimes creates words that do exist.That's what it's like.<\/b><\/div>\n\n<br>\n\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Conclusion<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>In summary, both Stemming and Lemmatizing are root extractions, which can be extracted from the dictionary, and Lemma can be extracted from the dictionary.<\/b><\/div>","bdf31a10":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Data Cleaning<\/b> \ud83c\udf40\n\n<p style=\"font-family:Comic Sans MS; font-size:130%; color: #008d62; text-align:center;\"><b>Before visualizing the data through the basic Data Cleaning process, we will optimize it for \nanalysis purposes.<\/b><\/p>\n<br><br>\n \n<div class=\"alert success-alert\">\n    \n\u2714 <b>The basic order is as follows : <\/b>\n    \n* Check for missing values\n\n* Removed urls, emojis and punctuations\n\n* Tokenized base text and selected_text\n\n* Lower cased clean text\n\n* Removed stopwords\n\n* Applied part of speech tags\n\n* Converted part of speeches to wordnet format\n\n* Applying word lemmatizer\n\n* Converted tokenized text to string again\n    \n<\/div>","c5cb21b1":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Loss Function <\/b>","1baf811a":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Most Common Words <\/b>","60b14a79":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Table of Contents<\/b> \ud83c\udf40\n\n<div class=\"alert success-alert\">\n    \n\u2714 <b>Importing Neccesary Packages<\/b><br><br>\n\ni)  Most basic stuff for EDA, packages for text processing, Libraries for text preprocessing<br><br>\n  \nii) We will import basic libraries for data discovery and packages for the Bert model<br><br>\n  \n\u2714 <b>Road Data<\/b><br><br>\n\ni)  We will need train.csv, test.csv<br><br>\n    \nii) These include text ID, text, sentiment, selected_text columns that can predict sentiment<br><br>\n    \n\u2714 <b>EDA<\/b><br><br>\n\ni)  We will see how target values are distributed according to sentiment<br><br>\n    \nii) Let's look at the train and test data set in general<br><br>\n    \n\u2714 <b>Cleaning Data<\/b><br><br>\n\ni)  Check for missing values, Removed urls, emojis and punctuations, Removed stopwords etc.<br><br>\n\nii) We proceed with data preprocessing for accurate predictive models<br><br>\n    \n\u2714 <b>Visualizing the Data<\/b><br><br>\n\ni)  Tweet Lengths, Word Lengths, Word Counts, Most Common Words<br><br>\n    \nii) Increase overall understanding of data through visualization<br><br>\n    \n\u2714 <b>N-gram Analysis<\/b><br><br>\n\ni)  An approach that considers only a few words and a sequence data representation approach<br><br>\n    \nii) Let's adjust n in n-gram and look at the changes<br><br>\n    \n\u2714 <b>Wordclouds<\/b><br><br>\n    \n\u2714 <b>Modelling using RoBERTa<\/b><br><br>\n \n\u2714 <b>References<\/b>\n<\/div>","ec6d5328":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>One NAN exists in train data. Therefore, it removes it.<\/b><\/div>","c600294c":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Tweet Lengths <\/b>","268f9414":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Data Loader <\/b>","c390f49d":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Exploratory Data Analysis<\/b> \ud83c\udf40","05453f2a":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Why change to Wordnet format ? <\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Wordnet is an ontology built by Princeton University about English words in the past, which outlines how they relate to each other. Rather than organizing individual meanings by word like a dictionary, organizing them around the relationship between words can increase their utilization.<\/b><\/div>","a57edb2d":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Evaluation Function<\/b>","d5b13d7f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Twenty Most Common Words appear similar in train and test.<\/b><\/div>","5262349e":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>All right! The data is complete as we wish. Now let's look at the visualization to see what insights are hidden.<\/b><\/div>","1e5c0ba2":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Modelling using RoBERTa<\/b> \ud83c\udf40","654f332f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Feature Selection <\/b>\n    \n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Our EDA work was meaningful enough, but we decided to use existing 'text', 'selected_text' and 'sentiment' to find the output we wanted in the competition.<\/b><\/div>","130bd5bb":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Most Common Trigram <\/b>","85e4a888":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Training Function<\/b>","d655bea0":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>You can see that emoji, url, etc. has been erased well.<\/b><\/div>","e05179d8":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Inference<\/b>","9cd6c073":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Word Counts <\/b>","6b9d5d20":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Visualizing the Data<\/b> \ud83c\udf40","776f337f":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Data and Packages Improts<\/b> \ud83c\udf40","89e33ad8":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>Word Cloud provides pretentious information.<\/b><br><br> \u2714 <b>We see the need for the elimination of words in train and test that negatively affect the analysis such as 'im', 'u'.<\/b><br><\/div>","cab73655":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights<\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>In Word counts, train and test show skwness.<\/b><\/div>","6728d3cc":"<p style=\"font-family:Comic Sans MS; font-size:180%; color: #008d62; text-align:center;\"><b>Pls, \"UPVOTE\" if this code helped ! \ud83d\udc40<\/b><\/p>","8047f36e":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 What is Part-of-speech Tagging ? <\/b><\/p>\n\n><div class=\"alert success-alert\" role=\"alert\">\u2714 <b>It refers to identifying and tagging the parts of the words in a sentence. It is output in the form of a tuple, and is output in the form of a (word, tag). where tags are POS tags.<\/b><\/div>\n\n<br><br>\n\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 The types of Part-of-speech Tag are shown in the table below : <\/b><\/p>\n\n<center><img src=\"https:\/\/mblogthumb-phinf.pstatic.net\/MjAyMDA0MDZfMTUz\/MDAxNTg2MTQzOTE2MDc3._q5jz1Y50qyH23mv5VsU_Vz_s6At_CnVQl-HbyL873wg.Bk1q6ZSCUOJ5rxy5yZGBKTaBpnVbnPdvu_A3a1vyzfEg.PNG.bycho211\/1.png?type=w800\"\/ width=\"500\" height=\"700\" ><\/center>","f558332d":"<div class=\"alert success-alert\">\n\u2714 <b>Understanding evaluation metrics : What's a jacquard? : <\/b>\n\n<br>Let's say there are two sets, A and B. An intersection is a set of elements that are common in two sets. That is, **the idea of jacquard similarity is that if we find the ratio of the intersection in the union, we can find the similarity of the two sets A and B.**\nJacquard similarity has a value **between 0 and 1**, if two sets are equal, a value of 1, and if there are no common elements in both sets, a value of 0. When J is the function to obtain jacquard similarity, the jacquard similarity function J is shown below\n\n<center><img src=\"data:image\/png;base64,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\"\/ width=\"500\" height=\"500\" ><\/center>\n\n<\/div>\n\n\n\n","e5446a06":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Training<\/b>","9388d6fd":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83c\udf40 <b>Word Cloud<\/b> \ud83c\udf40","2440bfab":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>\u2705 Let me understand through a simple example <\/b><\/p>"}}