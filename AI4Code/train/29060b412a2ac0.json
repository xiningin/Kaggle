{"cell_type":{"5b89aca0":"code","af3f8c31":"code","f2a70ace":"code","46dac6e5":"code","1b6bd20f":"code","ce6cb314":"code","35478cbf":"code","307abec3":"code","99838601":"code","ceaf78dd":"code","6d5b5abc":"code","44ba9992":"code","a5c72cde":"code","e074e71e":"code","d602c356":"code","386d5daf":"code","04c4a127":"code","ef6c6b6c":"code","1b236bc8":"code","4b5dc959":"code","8c771d53":"code","a5827e20":"code","92bb550d":"code","bb9a7b7c":"markdown","36359896":"markdown","ad2ad08e":"markdown","ebcadf08":"markdown","da56e96f":"markdown","aebdbd90":"markdown","f55f8714":"markdown","313c7c40":"markdown","51b4200a":"markdown","9c1b6c0d":"markdown","dc5fa3dc":"markdown","a39fd63c":"markdown","039b187e":"markdown"},"source":{"5b89aca0":"import os\nimport numpy as np\nimport pandas as pd \nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nfrom os import listdir\nfrom os.path import isfile, join\nfrom PIL import Image\nimport glob\n\n\nseed = 2019\nnp.random.seed(seed)\n%matplotlib inline","af3f8c31":"tf.__version__","f2a70ace":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","46dac6e5":"    dirname = '\/kaggle\/input'\n    train_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/train')\n    train_normal_pth = os.path.join(train_path, 'NORMAL')\n    train_dme_pth = os.path.join(train_path, 'DME')\n    train_drusen_pth = os.path.join(train_path, 'DRUSEN')\n    train_cnv_pth = os.path.join(train_path, 'CNV')\n    \n    test_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/test')\n    test_normal_pth = os.path.join(test_path, 'NORMAL')\n    test_dme_pth = os.path.join(test_path, 'DME')\n    test_drusen_pth = os.path.join(test_path, 'DRUSEN')\n    test_cnv_pth = os.path.join(test_path, 'CNV')\n    \n    val_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/val')\n    val_normal_pth = os.path.join(val_path, 'NORMAL')\n    val_dme_pth = os.path.join(val_path, 'DME')\n    val_drusen_pth = os.path.join(val_path, 'DRUSEN')\n    val_cnv_pth = os.path.join(val_path, 'CNV')","1b6bd20f":"test_normal_pth","ce6cb314":"def plot_imgs(item_dir, num_imgs=4):\n    all_item_dirs = os.listdir(item_dir)\n    item_files = [os.path.join(item_dir, file) for file in all_item_dirs][:num_imgs]\n\n    plt.figure(figsize=(16, 16))\n    for idx, img_path in enumerate(item_files):\n        plt.subplot(1, 4, idx+1)\n\n        img = plt.imread(img_path)\n        plt.imshow(img, cmap='gray')\n\n    plt.tight_layout()","35478cbf":"plot_imgs(train_normal_pth)","307abec3":"plot_imgs(train_dme_pth)","99838601":"plot_imgs(train_drusen_pth)","ceaf78dd":"plot_imgs(train_cnv_pth)","6d5b5abc":"def Images_details_Print_data(data, path):\n    print(\" ====== Images in: \", path)    \n    for k, v in data.items():\n        print(\"%s:\\t%s\" % (k, v))\n\ndef Images_details(path):\n    files = [f for f in glob.glob(path + \"**\/*.*\", recursive=True)]\n    data = {}\n    data['images_count'] = len(files)\n    data['min_width'] = 10**100  # No image will be bigger than that\n    data['max_width'] = 0\n    data['min_height'] = 10**100  # No image will be bigger than that\n    data['max_height'] = 0\n\n\n    for f in files:\n        im = Image.open(f)\n        width, height = im.size\n        data['min_width'] = min(width, data['min_width'])\n        data['min_height'] = min(height, data['min_height'])\n        data['max_width'] = max(width, data['max_height'])\n        \n        data['max_height'] = max(height, data['max_height'])\n\n    Images_details_Print_data(data, path)\n","44ba9992":"Images_details(train_normal_pth)\nImages_details(train_dme_pth)\nImages_details(train_drusen_pth)\nImages_details(train_cnv_pth)","a5c72cde":"Images_details(test_normal_pth)\nImages_details(test_dme_pth)\nImages_details(test_drusen_pth)\nImages_details(test_cnv_pth)","e074e71e":"Images_details(val_normal_pth)\nImages_details(val_dme_pth)\nImages_details(val_drusen_pth)\nImages_details(val_cnv_pth)","d602c356":"input_path = \"\/kaggle\/input\/kermany2018\/OCT2017 \/\"\n\nfor _set in ['train', 'test', 'val']:\n    normal = len(os.listdir(input_path + _set + '\/NORMAL'))\n    dme = len(os.listdir(input_path + _set + '\/DME'))\n    drusen = len(os.listdir(input_path + _set + '\/DRUSEN'))\n    cnv = len(os.listdir(input_path + _set + '\/CNV'))\n    print('{}, Normal images: {}, DME images: {}, DRUSEN images: {}, CNV images: {}'.format(_set, normal, dme, drusen, cnv))","386d5daf":"def process_data(img_dims, batch_size):\n    # Data generation objects\n    train_datagen = ImageDataGenerator(\n        rescale = 1.\/255,\n      #  featurewise_center=True,\n      #  featurewise_std_normalization=True,\n        zoom_range = 0.3,\n        horizontal_flip = True)\n    \n    test_datagen = ImageDataGenerator(\n      #  featurewise_center=True,\n      #  featurewise_std_normalization=True,\n        rescale=1.\/255)\n    \n    # This is fed to the network in the specified batch sizes and image dimensions\n    train_gen = train_datagen.flow_from_directory(\n    directory = train_path, \n    target_size = (img_dims, img_dims), \n    batch_size = batch_size, \n    class_mode = 'categorical', \n    shuffle=True)\n\n    test_gen = test_datagen.flow_from_directory(\n    directory=test_path, \n    target_size=(img_dims, img_dims), \n    batch_size=batch_size, \n    class_mode='categorical', \n    shuffle=True)\n    \n    # I will be making predictions off of the test set in one batch size\n    # This is useful to be able to get the confusion matrix\n    test_data = []\n    test_labels = []\n\n    for cond in ['\/NORMAL\/', '\/DME\/', '\/DRUSEN\/', '\/CNV\/']:\n        for img in (os.listdir(test_path + cond)):\n            img = plt.imread(test_path + cond + img)\n            img = cv2.resize(img, (img_dims, img_dims))\n            img = np.dstack([img, img, img])\n            img = img.astype('float32') \/ 255\n            if cond=='\/NORMAL\/':\n                label = 0\n            elif cond=='\/DME\/':\n                label = 1\n            elif cond=='\/DRUSEN\/':\n                label = 2\n            elif cond=='\/CNV\/':\n                label = 3\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    \n    return train_gen, test_gen, test_data, test_labels","04c4a127":"PARAMS = {'img_dims': 160,\n          'epochs': 10,\n          'batch_size': 32,\n          'optimizer': 'adam',\n          'loss': 'categorical_crossentropy',\n          'metrics': 'accuracy',\n          }","ef6c6b6c":"\n\ntrain_gen, test_gen, test_data, test_labels = process_data(PARAMS['img_dims'], PARAMS['batch_size'])\ninputs = Input(shape=(PARAMS['img_dims'], PARAMS['img_dims'], 3))\n\n# First conv block\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Second conv block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Third conv block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Fourth conv block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Fifth conv block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# FC layer\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n# Output layer\noutput = Dense(units=4, activation='softmax')(x)\n\n# Creating model and compiling\nmodel = Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=PARAMS['optimizer'], loss=PARAMS['loss'], metrics=[PARAMS['metrics']])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","1b236bc8":"model.summary()","4b5dc959":"plot_model(model)","8c771d53":"history = model.fit_generator(\n           train_gen, steps_per_epoch=train_gen.samples \/\/ PARAMS['batch_size'], \n           epochs=PARAMS['epochs'],\n           \n           validation_data=test_gen, \n           validation_steps=test_gen.samples \/\/ PARAMS['batch_size'],\n           callbacks=[checkpoint, lr_reduce])","a5827e20":"print ('Train Accuracy', np.mean(history.history['accuracy']))\nprint ('Train Loss', np.mean(history.history['loss']))\nprint ('Test Accuracy', np.mean(history.history['val_accuracy']))\nprint ('Test Loss', np.mean(history.history['val_loss']))","92bb550d":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","bb9a7b7c":"# Network architecture","36359896":"# Details about images dimensions","ad2ad08e":"## DME","ebcadf08":"## Summary\nThe model is converging which can be observed from the decrease in loss and validation loss with epochs. Also it is able to reach over 99% validation accuracy in just 10 epochs.","da56e96f":"## CNV","aebdbd90":"# Data augmentation\nThe practice of data augmentation is an effective way to increase the size of the training set.\n\nAugmenting the training examples allow the network to \u201csee\u201d more diversified, but still representative, data points during training.\n\nThere's two data generators: one for training data, and the other for validation data. \nA data generator is capable of loading the required amount of data (a mini batch of images) directly from the source folder, convert them into training data (fed to the model) and training targets (a vector of attributes \u2014 the supervision signal).","f55f8714":"Some theory about Spatial Separable Convolutions.\n\nConceptually, this is the easier one out of the two, and illustrates the idea of separating one convolution into two well, so I\u2019ll start with this. Unfortunately, spatial separable convolutions have some significant limitations, meaning that it is not heavily used in deep learning.\n\nThe spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height. (The other dimension, the \u201cdepth\u201d dimension, is the number of channels of each image). A spatial separable convolution simply divides a kernel into two, smaller kernels.\n\nThe most common case would be to divide a 3x3 kernel into a 3x1 and 1x3 kernel, like so:\n\n\n\nNow, instead of doing one convolution with 9 multiplications, we do two convolutions with 3 multiplications each (6 in total) to achieve the same effect. With less multiplications, computational complexity goes down, and the network is able to run faster.\n\n\n\nOne of the most famous convolutions that can be separated spatially is the Sobel kernel, used to detect edges:\n\n\n\nThe main issue with the spatial separable convolution is that not all kernels can be \u201cseparated\u201d into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.\n\nDepthwise Separable Convolutions\n\nUnlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be \u201cfactored\u201d into two smaller kernels. Hence, it is more commonly used. This is the type of separable convolution seen in keras.layers.SeparableConv2D or tf.layers.separable_conv2d.\n\nThe depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension \u2014 the number of channels \u2014 as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the \u201cred\u201d channel interprets the \u201credness\u201d of each pixel, the \u201cblue\u201d channel interprets the \u201cblueness\u201d of each pixel, and the \u201cgreen\u201d channel interprets the \u201cgreenness\u201d of each pixel. An image with 64 channels has 64 different interpretations of that image.\n\nSimilar to the spatial separable convolution, a depthwise separable convolution splits a kernel into 2 separate kernels that do two convolutions: the depthwise convolution and the pointwise convolution. But first of all, let\u2019s see how a normal convolution works.\n\nNormal Convolution:\n\nIf you don\u2019t know how a convolution works from a 2-D perspective, read this article or check out this site. A typical image, however, is not 2-D; it also has depth as well as width and height. Let us assume that we have an input image of 12x12x3 pixels, an RGB image of size 12x12. Let\u2019s do a 5x5 convolution on the image with no padding and a stride of 1. If we only consider the width and height of the image, the convolution process is kind of like this: 12x12 \u2014 (5x5) \u2014 >8x8. The 5x5 kernel undergoes scalar multiplication with every 25 pixels, giving out1 number every time. We end up with a 8x8 pixel image, since there is no padding (12\u20135+1 = 8).\n\nHowever, because the image has 3 channels, our convolutional kernel needs to have 3 channels as well. This means, instead of doing 5x5=25 multiplications, we actually do 5x5x3=75 multiplications every time the kernel moves. Just like the 2-D interpretation, we do scalar matrix multiplication on every 25 pixels, outputting 1 number. After going through a 5x5x3 kernel, the 12x12x3 image will become a 8x8x1 image.\n\n![](https:\/\/miro.medium.com\/max\/1218\/1*fgYepSWdgywsqorf3bdksg.png)\n\nWhat if we want to increase the number of channels in our output image? What if we want an output of size 8x8x256? Well, we can create 256 kernels to create 256 8x8x1 images, then stack them up together to create a 8x8x256 image output.\n\n![](https:\/\/miro.medium.com\/max\/1447\/1*XloAmCh5bwE4j1G7yk5THw.png)\n\nhis is how a normal convolution works. I like to think of it like a function: 12x12x3 \u2014 (5x5x3x256) \u2014 >12x12x256 (Where 5x5x3x256 represents the height, width, number of input channels, and number of output channels of the kernel). Not that this is not matrix multiplication; we\u2019re not multiplying the whole image by the kernel, but moving the kernel through every part of the image and multiplying small parts of it separately.\n\nA depthwise separable convolution separates this process into 2 parts: a depthwise convolution and a pointwise convolution.\n\nPart 1 \u2014 Depthwise Convolution:\n\nIn the first part, depthwise convolution, we give the input image a convolution without changing the depth. We do so by using 3 kernels of shape 5x5x1.\n\n![](https:\/\/miro.medium.com\/max\/1308\/1*yG6z6ESzsRW-9q5F_neOsg.png)\n\nEach 5x5x1 kernel iterates 1 channel of the image (note: 1 channel, not all channels), getting the scalar products of every 25 pixel group, giving out a 8x8x1 image. Stacking these images together creates a 8x8x3 image.\n\nPart 2 \u2014 Pointwise Convolution:\n\nRemember, the original convolution transformed a 12x12x3 image to a 8x8x256 image. Currently, the depthwise convolution has transformed the 12x12x3 image to a 8x8x3 image. Now, we need to increase the number of channels of each image.\n\nThe pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.\n\n![](https:\/\/miro.medium.com\/max\/1222\/1*37sVdBZZ9VK50pcAklh8AQ.png)\n\nWe can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.\n\n![](https:\/\/miro.medium.com\/max\/1407\/1*Q7a20gyuunpJzXGnWayUDQ.png)\n\nAnd that\u2019s it! We\u2019ve separated the convolution into 2: a depthwise convolution and a pointwise convolution. In a more abstract way, if the original convolution function is 12x12x3 \u2014 (5x5x3x256) \u219212x12x256, we can illustrate this new convolution as 12x12x3 \u2014 (5x5x1x1) \u2014 > (1x1x3x256) \u2014 >12x12x256.\n\nAlright, but what\u2019s the point of creating a depthwise separable convolution?\n\nLet\u2019s calculate the number of multiplications the computer has to do in the original convolution. There are 256 5x5x3 kernels that move 8x8 times. That\u2019s 256x3x5x5x8x8=1,228,800 multiplications. What about the separable convolution? In the depthwise convolution, we have 3 5x5x1 kernels that move 8x8 times. That\u2019s 3x5x5x8x8 = 4,800 multiplications. In the pointwise convolution, we have 256 1x1x3 kernels that move 8x8 times. That\u2019s 256x1x1x3x8x8=49,152 multiplications. Adding them up together, that\u2019s 53,952 multiplications.\n\n52,952 is a lot less than 1,228,800. With less computations, the network is able to process more in a shorter amount of time.\n\nHow does that work, though? The first time I came across this explanation, it didn\u2019t really make sense to me intuitively. Aren\u2019t the two convolutions doing the same thing? In both cases, we pass the image through a 5x5 kernel, shrink it down to one channel, then expand it to 256 channels. How come one is more than twice as fast as the other?\n\nAfter pondering about it for some time, I realized that the main difference is this: in the normal convolution, we are transforming the image 256 times. And every transformation uses up 5x5x3x8x8=4800 multiplications. In the separable convolution, we only really transform the image once \u2014 in the depthwise convolution. Then, we take the transformed image and simply elongate it to 256 channels. Without having to transform the image over and over again, we can save up on computational power.\n\nIt\u2019s worth noting that in both Keras and Tensorflow, there is a argument called the \u201cdepth multiplier\u201d. It is set to 1 at default. By changing this argument, we can change the number of output channels in the depthwise convolution. For example, if we set the depth multiplier to 2, each 5x5x1 kernel will give out an output image of 8x8x2, making the total (stacked) output of the depthwise convolution 8x8x6 instead of 8x8x3. Some may choose to manually set the depth multiplier to increase the number of parameters in their neural net for it to better learn more traits.\n\nAre the disadvantages to a depthwise separable convolution? Definitely! Because it reduces the number of parameters in a convolution, if your network is already small, you might end up with too few parameters and your network might fail to properly learn during training. If used properly, however, it manages to enhance efficiency without significantly reducing effectiveness, which makes it a quite popular choice.\n\nAuthor: Chi-Feng Wang","313c7c40":"## Normal","51b4200a":"# Plot learning curves","9c1b6c0d":"# Path sets","dc5fa3dc":"# Let's see what xray photos look like","a39fd63c":"## DRUSEN","039b187e":"# Fitting the model"}}