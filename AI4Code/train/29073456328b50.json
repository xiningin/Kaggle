{"cell_type":{"50c9e2ab":"code","2934149a":"code","4d697a1a":"code","ac38f144":"code","e6b460d1":"code","87c7c306":"code","f93f636a":"code","b20df88a":"code","39150616":"code","b3464c64":"code","f1f2c275":"code","65d9f573":"code","9112ac2d":"code","7f58edb9":"code","6e0eb997":"code","843c959d":"code","a470d473":"code","d859efb0":"code","56884f72":"code","04d1b725":"code","783140a0":"code","476ccdda":"code","de6e8f8e":"code","7acf5cdf":"code","4f1faae1":"code","1289e3cd":"code","40a2ea8e":"code","baeb510d":"code","a99bb689":"code","b4a9b89d":"code","542cab45":"code","94e8d2f1":"code","c737b3a7":"code","ccb44f95":"code","ba648e0c":"code","f06c72a0":"code","76a9f15c":"code","dc6d570e":"code","6da771e5":"markdown","df586eb5":"markdown","ca72c65c":"markdown","2acfc3de":"markdown","3e6c22bd":"markdown","4f82311b":"markdown","12794d98":"markdown","50728205":"markdown","b977917f":"markdown","32892081":"markdown"},"source":{"50c9e2ab":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport math as m\n\nimport matplotlib.pyplot as plt","2934149a":"data = pd.read_csv(\"..\/input\/tibau-wind-speed\/data_tibau.csv\", names=['time', 'velocity', 'direction'])\ndata","4d697a1a":"data['time'] = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S')","ac38f144":"data['year'] = data['time'].dt.strftime(\"%Y\") ## Extracting year to get mean velocity ","e6b460d1":"data","87c7c306":"data[['velocity', 'year']].groupby(by='year').mean()","f93f636a":"mean_velocity = data[['velocity', 'year']].groupby(by='year').mean()\nmean_velocity","b20df88a":"vel = list(data['velocity'])\nscaled_velocity = list()\n\nvel_min = data['velocity'].min()\nvel_max = data['velocity'].max()\n\nfor i in vel:\n    scaled_velocity.append((i - vel_min)\/(vel_max - vel_min))\n\ndata['velocity'] = scaled_velocity ## Extracting max and min velocity, this way we preprocess the data for predictions. Later we'll reescale the predicted data","39150616":"mean_velocity = 4.90","b3464c64":"rm = 0.15 ## dead radius\nro = 1.18 ## Estimated air density\nP = 830 ## desired potency\ncp = 0.4 ## standard 0.35 - 0.45\nvisc_ar = 1.82*pow(10,-5) ## air viscosity\nn = 0.90 ## gearbox efficiency\nPi = m.pi\nmi = 7 ## tip-air speed\nB = 3 ## blades\nCl = 1.70 ## S1223 Profile\nCd = 0.0163 ## drag\nsections = 15 ## sections","f1f2c275":"R = m.sqrt(pow(rm,2)+P\/(cp*n*0.5*ro*Pi*pow(mean_velocity,3))) #------------------------------ raio do rotor\nw = mi*mean_velocity\/R #--------------------------------------------------------------velocidade angular\nR,w","65d9f573":"radius = []\nratio = []\nre = [] ##  Reynolds\n\nfor i in range(sections):\n    r_section = R*rm + (i*((R - R*rm)\/(sections-1))) ## Formula to avoid going to the tip\n    ratio_section = r_section\/R\n    radius.append(r_section)\n    ratio.append(ratio_section)\n    reynolds = ro*mean_velocity*radius[i]\/visc_ar\n    re.append(reynolds) ## Calculate Reynolds, and the radius of the section\n\nradius[sections-1] = radius[sections-2]+(radius[sections-1]-radius[sections-2])*0.1 \n## raio igual da na posicao final da erro e quanto mais proximo da penultima, melhor o resultado\n\nradius, ratio, re","9112ac2d":"chord_schmitz = []\n\nfor i in range(sections):\n    ang = mi*radius[i]\n    atan = R \/ ( ang )\n    sen_2 = (1\/3) * m.atan( atan )\n\n    section_chord = (16*Pi*radius[i]) \/ (B*Cl)  * pow(m.sin( sen_2 ),2)\n    chord_schmitz.append(section_chord)\n\nchord_schmitz\n\n## Used to calculate the chord at each section","7f58edb9":"for j in range(sections):\n    \n    error_a = 1 ## Start relative error\n    error_a_ = 1 \n    a = 0.001 # Start a and a'\n    a_ = 0.001\n    i = 0 ## Count\n    ac = 0.2 ## ac correction from Schmitz\n    \n    while (error_a > 0.001) and (error_a_ > 0.001):\n \n        phi = m.atan(((1-a)\/(1+a_))*(mean_velocity\/(radius[j]*w))) ## Relative attack angle\n            \n        Cx = Cl*(m.sin(phi)) - Cd*(m.cos(phi)) ##  Tangential coef.\n        Cy = Cl*(m.cos(phi)) + Cd*(m.sin(phi)) ## Normal coef.\n\n\n        f = (B\/2) * (R-radius[j])\/(radius[j]*m.sin(phi))\n        F = (2\/Pi) * (m.acos(m.exp(-f)))\n        sol = (chord_schmitz[j]*B)\/(2*Pi*radius[j]) ## Terms to calculate the new a and a' from the Blade Element Method\n\n        if a > 0.2: ## Correction for values of a greater than 0.2\n            K = (4*F*pow(m.sin(phi),2))\/(sol*Cy)\n            a_new = 0.5 * (2 + K*(1 - 2*ac) - m.sqrt( pow((K*(1-2*ac)+2),2) + 4*(K*pow(ac,2) -1 ) ))\n        else:\n            a_new = 1 \/ (((4*F*pow(m.sin(phi),2))\/(sol*Cy)) + 1)\n\n        a_new_ = 1 \/ (( (4*F*m.sin(phi)*m.cos(phi)) \/ (sol*Cx)) - 1)\n\n        error_a = m.fabs(((a_new - a)\/a))\n        error_a_ = m.fabs(((a_new_ - a_)\/a_)) ## Calculate new error\n\n        a = a_new\n        a_ = a_new_ ## Substitute values\n  \n        i = i + 1\n\n    print(\"Section \", j, \"Chord: \", np.round(chord_schmitz[j],6), \", Phi:\", np.round(m.degrees(phi),2))","6e0eb997":"data ## Checking the data","843c959d":"## Index 9948 is the first day of May, so we'll separate the data before May, 2021 and After it.\n## This way we'll use the data before this data to train and the May month to test.\n## For the test, we'll start at 9941, as this will include the 7 previous data readings before the start of May","a470d473":"velocity_train = np.array(data['velocity'][0:9948])\nvelocity_train.shape","d859efb0":"velocity_test = np.array(data['velocity'][9941:])\nvelocity_test.shape  ## Divided the data from before and after may, 2021","56884f72":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense  ## Importing relevant libraries for DL","04d1b725":"def split_data(sequence, n_steps):\n    X, Y = list(), list()\n    for i in range(len(sequence)):\n        initial = i\n        end = i + n_steps\n        if end > len(sequence)-1:\n            break\n        x_seq = sequence[initial:end]\n        y_seq = sequence[end]\n        X.append(x_seq)\n        Y.append(y_seq)\n    return np.array(X), np.array(Y)\n\n## Creating a sequence of data, this will feed the neural network. n_steps is equal to the number of samples we want to\n## use to predict a new value. In this example, we'll try to predict only one value based on 7 previous values. In a future work, we'll try more than one.","783140a0":"n_steps = 7\nX1, Y1 = split_data(velocity_train, n_steps)","476ccdda":"from sklearn.model_selection import train_test_split as tts\nX_train, X_val, Y_train, Y_val = tts(X1, Y1, test_size=0.10) ## This will be our validation data, as we already have our test data","de6e8f8e":"n_features = 1\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features)) ## n_features is the number of values we'll predict\nX_val = X_val.reshape((X_val.shape[0], X_val.shape[1], n_features)) ## Reshape the data","7acf5cdf":"## model = tf.keras.models.load_model('07_06_2020_20h55.h5') ## In case we need to load a previous saved model","4f1faae1":"model = keras.Sequential([keras.layers.Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)),\n                          keras.layers.Dense(1)])  ## Creation of the LSTM layer and one single output","1289e3cd":"model.compile(optimizer='adam',\n             loss='mse') ## Loss function","40a2ea8e":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(min_delta=0.00005, patience=50) ## This will make sure our model is improving, and if not, the fit will stop","baeb510d":"history = model.fit(x=X_train,\n                   epochs=200,\n                    y=Y_train,\n                    validation_data=(X_val,Y_val),\n                    verbose=2,\n                    callbacks=[early_stopping])","a99bb689":"## Uncomment if want to save another model\n\n## model.save(\"07_06_2020_21h15.h5\") ","b4a9b89d":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1) ## Saving fit history for plotting","542cab45":"fig, ax = plt.subplots(figsize=(15,9))\n\nax.plot(epochs, loss, label='Training MSE')\nax.plot(epochs,val_loss, label='Validation MSE')\nax.set_title(\"Training and Validation Accuracy\")\nax.legend()","94e8d2f1":"def predict_data(sequence, n_steps): \n    \n    x_real, y_real = split_data(sequence, 7)\n    \n    predicted, real = list(), list()\n    \n    count = 0\n    \n    for i, j in zip(x_real,y_real):\n        i = i.reshape(1,7,1)\n        prediction = model.predict(i)[0][0] ## This way we extract the value\n        print(\"Real: \", j, \"Predicted: \", prediction)\n        real.append(j)\n        predicted.append(prediction)\n        \n    return predicted, real\n\n## Creating a function to receive data, and then predict it. It does the same of the split data, it just adds the prediction part","c737b3a7":"predicted, real = predict_data(velocity_test, n_steps) ## Predict data in the test dat (May)","ccb44f95":"re_vel = list(predicted)\nre_vel_real = list(real)\n\nrescaled_velocity = list()\nrescaled_real_velocity = list()\n\nfor i,j in zip(re_vel, re_vel_real):\n    rescaled_velocity.append(i * (vel_max - vel_min) + vel_min)\n    rescaled_real_velocity.append(j * (vel_max - vel_min) + vel_min)\n    \n    ## Reescaling the data, based on our max and min velocity","ba648e0c":"May = pd.DataFrame({\"time\":np.array(data['time'][9948:]), \"real\":rescaled_real_velocity, \"predictions\":rescaled_velocity})\nMay\n\n## Creating a May dataframe to compare results","f06c72a0":"import plotly\nimport plotly.graph_objs as go\nimport plotly.offline as py","76a9f15c":"trace = [go.Scatter(x=May.time,\n                     y=May.real,\n                     name='Real'),\n          \n          go.Scatter(x=May.time,\n                     y=May.predictions,\n                     name='Predicted')]    \n          \nlayout = go.Layout(title=\"Prediction x Real values of Wind Speed\")\n\nfig = go.Figure(data=trace, layout=layout)\n          \npy.iplot(fig)","dc6d570e":"power_predict = list()\npower_real = list()\n\nfor i,j in zip(May['predictions'], May['real']):\n    if i > 3:\n        power_predict.append(0.20*0.5*ro*pow(i,3)*pow((R-rm),2)*Pi)  \n        power_real.append(0.20*0.5*ro*pow(j,3)*pow((R-rm),2)*Pi)\n        ## rm is the \"dead radius\" where the turbines pratically doesn't \"generate\" energy\n        ## 0.20 takes into account the betz limit and efficiency for 3 blades and gearbox\n    else:\n        power_predict.append(0)\n        power_real.append(0)\n\nprint(\"The total energy generated for this wind turbines is estimated as: \", np.sum(power_predict),\n      \"W. The real production would be: \", np.sum(power_real),\" W\")","6da771e5":" -------------------------------------------------------------------------------------------------------------- ","df586eb5":"## This cell marks the start of the Blade Element Method, it's used to design a Wind turbine, for more information please check Aerodynamics of Wind Turbines","ca72c65c":"## This is the end of the Blade Element Method","2acfc3de":"## Great results!","3e6c22bd":"<img src=\"https:\/\/datasciencerecruit.files.wordpress.com\/2021\/06\/image-1.png?w=839\">","4f82311b":"#### If you want to see more in depth thoughts on this forecast, please visit my blog: https:\/\/datasciencerecruit.com\/ and the full code here: https:\/\/github.com\/ramontanoeiro\/Wind-energy-forecast","12794d98":"- This data was collected using GreenEnergy - Storm Glass API for latitude and longitude at: lat, lng = -6.168371, -35.096696","50728205":"## Chord - Schmitz","b977917f":" ----------------------------------------------------------------------------","32892081":"## Schmitz"}}