{"cell_type":{"02cc16c0":"code","12b3773f":"code","eb847cc0":"code","c7987b56":"code","e82cd809":"code","f3681e81":"code","15f5d444":"code","4419271e":"code","aaa301e8":"code","04306e31":"code","7edbfa25":"code","cb50088f":"code","1626b3cc":"code","4e8471aa":"code","a5dd5c50":"code","a1e441a3":"code","d6f8ee6f":"code","7fa85d2a":"code","668822f6":"code","1ae1dd56":"code","d6b372a8":"code","d082028a":"code","7caf240d":"code","bc280f76":"code","435d41fb":"code","38eed260":"code","51b19929":"code","80c7b0e7":"code","5d8a087a":"code","eea40fe7":"code","50a072cd":"code","4b31e108":"code","7ee27f88":"code","3c4fe67f":"code","ded1c41b":"code","facddfd3":"code","b9b4c43a":"code","d78aca79":"code","008b9ebc":"code","72f01e7c":"code","3e142308":"code","f8218199":"code","212df130":"code","2a0451e4":"code","e67235ff":"code","71e50137":"code","5e3faf3b":"code","627bac6e":"code","8044bbab":"code","bdac58fe":"code","f81f89f9":"code","ac02ddee":"code","720baf55":"code","003db62b":"code","1909165f":"code","e72742b3":"code","8d339556":"code","76d93b63":"code","15116dd2":"code","e49c7d95":"code","8bccb17d":"code","98919b6d":"code","6c7f5e7d":"code","778aaee1":"code","540d817f":"code","f942089f":"code","af0d88fa":"code","6ec3ff4d":"code","d9496e69":"code","f3cdb0a7":"code","398709dc":"code","9168e038":"code","60333e41":"code","2421c638":"code","56a0dbbf":"code","64112e03":"code","d4e9f993":"code","3b129cfa":"code","5a1e82b4":"code","3edf85e0":"code","f255af48":"code","7c311a63":"code","ecb090bb":"code","fbae37b5":"code","9ff77fbf":"code","00c84365":"code","12b08133":"code","5a012a51":"code","997d6ec0":"code","13d10e86":"code","4d80b9d8":"code","7d9bc424":"code","fedfeb1b":"code","6ec7e221":"code","3eb31cdb":"code","3fb16686":"code","8ab3249b":"code","b8ba4f85":"code","bf0835dd":"markdown","0142795c":"markdown","0ab3f383":"markdown","d673458c":"markdown","5fd1fffb":"markdown","08d1f9fb":"markdown","89f72769":"markdown","30795e1c":"markdown","d8af7483":"markdown","7fbf1e5c":"markdown","a648d401":"markdown","2b62b48f":"markdown","e34b0218":"markdown","05ee239c":"markdown","2b608900":"markdown","f04b9f2b":"markdown","1b7227d0":"markdown","54ca560a":"markdown","1e076bbd":"markdown","ca1cb816":"markdown","45407a15":"markdown","4a107276":"markdown","c962c660":"markdown","0e9970d8":"markdown","3c9b94c7":"markdown","2208c83d":"markdown","1d1e82cb":"markdown","1fc7e256":"markdown","9f0fbae9":"markdown","12ca7d49":"markdown","753eb716":"markdown","2b790b2a":"markdown","329d6bd9":"markdown","11f7a601":"markdown","f10cd007":"markdown","33b24835":"markdown","b6de2727":"markdown","70c8c362":"markdown","19e9fbb9":"markdown","0e2576cc":"markdown","740abd51":"markdown","7a67752d":"markdown","70d820d0":"markdown","a85911bb":"markdown","f76eb917":"markdown","d26e9773":"markdown","e5ab2c70":"markdown","1fdbb01f":"markdown","e8cb9d51":"markdown"},"source":{"02cc16c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', None)","12b3773f":"#%%time\n#sub_df= pd.read_csv('\/kaggle\/input\/new-york-city-taxi-fare-prediction\/sample_submission.csv')\n#test_df= pd.read_csv('\/kaggle\/input\/new-york-city-taxi-fare-prediction\/test.csv')\n#df= pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv')\n#df.sample(5)","eb847cc0":"data_dir = '..\/input\/new-york-city-taxi-fare-prediction'\n!ls -lh {data_dir}","c7987b56":"%%time\n!wc -l {data_dir}\/train.csv","e82cd809":"%%time\n!wc -l {data_dir}\/test.csv","f3681e81":"%%time\n!wc -l {data_dir}\/sample_submission.csv","15f5d444":"# Training set\n!head {data_dir}\/train.csv","4419271e":"# Test set\n!head {data_dir}\/test.csv","aaa301e8":"#  Sample sub\n!head {data_dir}\/sample_submission.csv","04306e31":"df_test = pd.read_csv(data_dir+'\/test.csv',parse_dates=['pickup_datetime'])\ndf_test","7edbfa25":"df_test.info()","cb50088f":"df_test.key.nunique()","1626b3cc":"import random\n## to select random index no from training dataset","4e8471aa":"# Change this\nsample_frac = 0.20\n# we are loading 20% data : Wall time: 28min 35s ; memory usage: 327.6 MB\n# 10% data loading : Wall time: 14min 38s","a5dd5c50":"%%time\nselected_cols = 'fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count'.split(',')\ndtypes = {\n    'fare_amount': 'float16',\n    'pickup_longitude': 'float32',\n    'pickup_latitude': 'float32',\n    'dropoff_longitude': 'float32',\n    'passenger_count': 'uint8'\n}\n## this function will return True for (1 -sample_frac) thus these rows will be skipped\ndef skip_row(row_idx):\n    if row_idx == 0:\n        return False\n    return random.random() > sample_frac  ## \n\nrandom.seed(7)\ndf = pd.read_csv(data_dir+\"\/train.csv\", \n                 usecols=selected_cols, \n                 dtype=dtypes, \n                 parse_dates=['pickup_datetime'], \n                 skiprows=skip_row)\ndf_original =df.copy()\ndf","a1e441a3":"## Exporting 20% data to csv for further prediction\ndf.to_csv('20% ofnew-york-city-taxi-fare-predicition.csv')","d6f8ee6f":"df.info()","7fa85d2a":"df.isnull().sum()","668822f6":"%%time\n#### Checking for duplicates\ndf.duplicated().sum()","1ae1dd56":"%%time\n# There are 42 duplicates, Lets remove them\ndf.drop_duplicates()  ## dropes duplicates\ndf.duplicated().sum()","d6b372a8":"df.describe()","d082028a":"import numpy as np\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km","7caf240d":"def add_trip_distance(df):\n    df['trip_distance'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])","bc280f76":"%%time\nadd_trip_distance(df)","435d41fb":"def add_dateparts(df, col):\n    df['year'] = df[col].dt.year\n    df['month'] = df[col].dt.month\n    df['day'] = df[col].dt.day\n    df['weekday'] = df[col].dt.weekday\n    df[col + '_hour'] = df[col].dt.hour","38eed260":"%%time\nadd_dateparts(df, 'pickup_datetime')","51b19929":"jfk_lonlat = -73.7781, 40.6413\nlga_lonlat = -73.8740, 40.7769\newr_lonlat = -74.1745, 40.6895\nmet_lonlat = -73.9632, 40.7794\nwtc_lonlat = -74.0099, 40.7126","80c7b0e7":"def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):\n    lon, lat = landmark_lonlat\n    df[landmark_name + '_drop_distance'] = haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])","5d8a087a":"%%time\nfor name, lonlat in [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]:\n    add_landmark_dropoff_distance(df, name, lonlat)","eea40fe7":"def remove_outliers(df):\n    return df[(df['fare_amount'] >= 1.) & \n              (df['fare_amount'] <= 500.) &\n              (df['pickup_longitude'] >= -75) & \n              (df['pickup_longitude'] <= -72) & \n              (df['dropoff_longitude'] >= -75) & \n              (df['dropoff_longitude'] <= -72) & \n              (df['pickup_latitude'] >= 40) & \n              (df['pickup_latitude'] <= 42) & \n              (df['dropoff_latitude'] >=40) & \n              (df['dropoff_latitude'] <= 42) & \n              (df['passenger_count'] >= 1) & \n              (df['passenger_count'] <= 6)]","50a072cd":"%%time\ndf = remove_outliers(df)","4b31e108":"## Checking for fare value to be -ve\ndf[df['fare_amount']<0]","7ee27f88":"## Checking for fare value to be greater than 500\ndf[df['fare_amount']>500]","3c4fe67f":"df[df['pickup_longitude']==0]","ded1c41b":"## Lets check for distance = 0\ndf[df['trip_distance']==0]","facddfd3":"df[df['passenger_count']>6]","b9b4c43a":"df.pickup_datetime.min(), df.pickup_datetime.max()","d78aca79":"%%time\ndf = df.drop('pickup_datetime', axis=1)\ndf.head(2)","008b9ebc":"df.hist(figsize=(22,21), bins=20);","72f01e7c":"# 1. What is the busiest day of the week?\ndf.weekday.mode()","3e142308":"# 2. What is the busiest time of the day?\ndf.pickup_datetime_hour.mode()","f8218199":"# 3. In which month are fares the highest? >>> winters have high fare and Jan has highest fare\ndf_fare= df.sort_values(ascending=False, by= 'fare_amount').head(50)\ndf_fare.month.hist();","212df130":"# 4. Which pickup locations have the highest fares?\nsns.scatterplot(x='pickup_longitude', y= 'pickup_latitude', hue='fare_amount',data=df_fare);","2a0451e4":"# 5. Which drop locations have the highest fares?\nsns.scatterplot(x='dropoff_longitude', y= 'dropoff_latitude',hue='fare_amount',data=df_fare);","e67235ff":"# 6. What is the average ride distance?\ndf.trip_distance.mean()","71e50137":"### Plotting log , lat for pickup\nsns.scatterplot(x='pickup_longitude', y= 'pickup_latitude', data=df )","5e3faf3b":"### Plotting log , lat for pickup\n#sns.scatterplot(x='dropoff_longitude', y= 'dropoff_latitude', data=df , hue='fare_amount')","627bac6e":"df_test.hist(figsize=(8,7), bins=20);","8044bbab":"df_test[df_test['pickup_longitude']==0]","bdac58fe":"df_test[df_test['passenger_count']>6]","f81f89f9":"df_test.pickup_datetime.min(), df_test.pickup_datetime.max()","ac02ddee":"%%time\ndf.corr()","720baf55":"df.describe()","003db62b":"import seaborn as sns","1909165f":"#%%time\n#sns.lineplot(y='fare_amount', x='pickup_datetime', data= df_original)","e72742b3":"from sklearn.model_selection import train_test_split","8d339556":"train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)","76d93b63":"len(train_df), len(val_df)","15116dd2":"df.columns","e49c7d95":"input_cols = ['pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n       'trip_distance', 'year', 'month', 'day', 'weekday',\n       'pickup_datetime_hour', 'jfk_drop_distance', 'lga_drop_distance',\n       'ewr_drop_distance', 'met_drop_distance', 'wtc_drop_distance']\ntarget_col = 'fare_amount'","8bccb17d":"train_inputs = train_df[input_cols]\ntrain_targets = train_df[target_col]\ntrain_inputs.head(3)","98919b6d":"train_targets.head(3)","6c7f5e7d":"val_inputs = val_df[input_cols]\nval_targets = val_df[target_col]","778aaee1":"display(val_inputs.head(3))\ndisplay(val_targets.head(3))","540d817f":"### Feature enigeerning on Test dataset\nadd_dateparts(df_test, 'pickup_datetime')\nadd_trip_distance(df_test)\n\nfor name, lonlat in [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]:\n    add_landmark_dropoff_distance(df_test, name, lonlat)\ndf_test.head(2)","f942089f":"test_inputs = df_test[input_cols]\ntest_inputs.head(3)","af0d88fa":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression","6ec3ff4d":"#%%time\nlinreg_model = LinearRegression()\nlinreg_model.fit(train_inputs, train_targets)\ntrain_preds = linreg_model.predict(train_inputs)\nval_preds = linreg_model.predict(val_inputs)\ntrain_rmse = mean_squared_error(train_targets, train_preds, squared=False)\nval_rmse = mean_squared_error(val_targets, val_preds, squared=False)\nprint('RMSE Score on Validation data',val_rmse)\nprint('RMSE Score on Validation data',train_rmse)","d9496e69":"df.fare_amount.median()","f3cdb0a7":"from sklearn.metrics import r2_score\nr2_train= r2_score(train_targets, train_preds)\nr2_val = r2_score(val_targets, val_preds)\nprint('R2 Score on Validation data',r2_val)\nprint('R2 Score on Validation data',r2_train)","398709dc":"def evaluate(model):\n    train_preds = model.predict(train_inputs)\n    train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n    val_preds = model.predict(val_inputs)\n    val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n    return train_rmse, val_rmse, train_preds, val_preds","9168e038":"def predict_and_submit(model, fname):\n    test_preds = model.predict(test_inputs)\n    sub_df = pd.read_csv(data_dir+'\/sample_submission.csv')\n    sub_df['fare_amount'] = test_preds\n    sub_df.to_csv(fname, index=None)\n    return sub_df","60333e41":"from sklearn.linear_model import Ridge","2421c638":"model1 = Ridge(random_state=42)","56a0dbbf":"%%time\nmodel1.fit(train_inputs, train_targets)","64112e03":"evaluate(model1)","d4e9f993":"predict_and_submit(model1, 'ridge_submission.csv')","3b129cfa":"predict_and_submit(linreg_model, 'Linear_submission.csv')","5a1e82b4":"from sklearn.ensemble import RandomForestRegressor","3edf85e0":"%%time\nmodel2 = RandomForestRegressor(max_depth=10, n_jobs=-1, random_state=7, n_estimators=50)\nmodel2.fit(train_inputs, train_targets)","f255af48":"evaluate(model2)","7c311a63":"predict_and_submit(model2, 'rf_submission.csv')","ecb090bb":"from xgboost import XGBRegressor","fbae37b5":"%%time\nmodel3 = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror')\nmodel3.fit(train_inputs, train_targets)","9ff77fbf":"evaluate(model3)","00c84365":"predict_and_submit(model3, 'xgb_submission.csv')","12b08133":"import matplotlib.pyplot as plt\n\ndef test_params(ModelClass, **params):\n    \"\"\"Trains a model with the given parameters and returns training & validation RMSE\"\"\"\n    model = ModelClass(**params).fit(train_inputs, train_targets)\n    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n    return train_rmse, val_rmse\n\ndef test_param_and_plot(ModelClass, param_name, param_values, **other_params):\n    \"\"\"Trains multiple models by varying the value of param_name according to param_values\"\"\"\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = dict(other_params)\n        params[param_name] = value\n        train_rmse, val_rmse = test_params(ModelClass, **params)\n        train_errors.append(train_rmse)\n        val_errors.append(val_rmse)\n    \n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","5a012a51":"best_params = {\n    'random_state': 7,\n    'n_jobs': -1,\n    'objective': 'reg:squarederror'\n}","997d6ec0":"%%time\n### No of trees\ntest_param_and_plot(XGBRegressor, 'n_estimators', [100, 250, 500], **best_params)","13d10e86":"best_params['n_estimators'] = 250","4d80b9d8":"%%time \n#### Max Depth\ntest_param_and_plot(XGBRegressor, 'max_depth', [3, 4, 5], **best_params)","7d9bc424":"best_params['max_depth'] = 5","fedfeb1b":"%%time\n#### Learning Rate\ntest_param_and_plot(XGBRegressor, 'learning_rate', [0.05, 0.1, 0.25], **best_params)","6ec7e221":"best_params['learning_rate'] = 0.25","3eb31cdb":"# Final Model Creation\nxgb_model_final = XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42,\n                               n_estimators=500, max_depth=5, learning_rate=0.1, \n                               subsample=0.8, colsample_bytree=0.8)","3fb16686":"%%time\nxgb_model_final.fit(train_inputs, train_targets)","8ab3249b":"evaluate(xgb_model_final)","b8ba4f85":"predict_and_submit(xgb_model_final, 'xgb_tuned_submission.csv')","bf0835dd":"# 2. **EDA** cont...","0142795c":"## Basic Terminal Navigation Commands: \n\n**ls** : To get the list of all the files or folders.  \n**ls -l**: Optional flags are added to ls to modify default behavior, listing contents in extended form -l is used for \u201clong\u201d output  \n**ls -a**: Lists of all files including the hidden files, add -a  flag   \n**cd**: Used to change the directory.  \n**du**: Show disk usage.  \n**pwd**: Show the present working directory.  \n**man**: Used to show the manual of any command present in Linux.  \n**rmdir**: It is used to delete a directory if it is empty.  \n**ln file1 file2**: Creates a physical link.  \n**ln -s file1 file2**: Creates a symbolic link.  \n**locate**: It is used to locate a file in Linux System  \n**echo**:  This command helps us move some data, usually text into a file.      \n**df**: It is used to see the available disk space in each of the partitions in your system.      \n**tar**: Used to work with tarballs (or files compressed in a tarball archive)       \n\n### [For more details...](https:\/\/www.geeksforgeeks.org\/basic-shell-commands-in-linux\/)","0ab3f383":"## About the data:\nData fields\nID\n* **key** - Unique string identifying each row in both the training and test sets. Comprised of pickup_datetime plus a unique integer, but this doesn't matter, it should just be used as a unique ID field. Required in your submission CSV. Not necessarily needed in the training set, but could be useful to simulate a 'submission file' while doing cross-validation within the training set.\n### Features**\n* **pickup_datetime** - timestamp value indicating when the taxi ride started.\n* **pickup_longitude** - float for longitude coordinate of where the taxi ride started.\n* **pickup_latitude** - float for latitude coordinate of where the taxi ride started.\n* **dropoff_longitude** - float for longitude coordinate of where the taxi ride ended.\n* **dropoff_latitude** - float for latitude coordinate of where the taxi ride ended.\n* **passenger_count** - integer indicating the number of passengers in the taxi ride.\n### Target\nfare_amount - float dollar amount of the cost of the taxi ride. This value is only in the training set; this is what you are predicting in the test set and it is required in your submission CSV.","d673458c":"## Loading data\nSince we can't load training data full \nlets load it in pieces","5fd1fffb":"### Add Distance From Popular Landmarks\n\n- JFK Airport\n- LGA Airport\n- EWR Airport\n- Times Square\n- Met Meuseum\n- World Trade Center\n\nWe'll add the distance from drop location. ","08d1f9fb":"Seems like 500 estimators has the lowest validation loss. However, it also takes a long time. Let's stick with 250 for now.","89f72769":"### Why fare has min value as -ve and max value = infinite","30795e1c":"## Test","d8af7483":"I wanted to see relationship between -ve fare value with distance thus decided to perform feature engineering before hand ","7fbf1e5c":"### iv) XGradient Boosting\n\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn","a648d401":"* I was curious if distance and log, lat had any correlation between them\n* I also want to check for correlation between extracts of time\n* `Fare` had a correlation of **0.819645** with `trip distance`","2b62b48f":"RF Model Execution time :: Wall time: 41min 6s   :: CPU times: user 2h 37min 31s  \nWow Random Forest is giving so accurate results with RMSE score of **0.01** and **0.0131** which means prediction is off by few cents and model is not overfitting the data","e34b0218":"# 2. Explore the Dataset\n\n- Basic info about training set\n- Basic info about test set\n- Exploratory data analysis & visualization\n- Ask & answer questions","05ee239c":"### Now lets load training data\nI will avoid key column","2b608900":"### Extract Inputs and Outputs","f04b9f2b":"# Ask & answer questions about the dataset: \n\n1. What is the busiest day of the week?\n2. What is the busiest time of the day?\n3. In which month are fares the highest?\n4. Which pickup locations have the highest fares?\n5. Which drop locations have the highest fares?\n6. What is the average ride distance?\n\nEDA + asking questions will help you develop a deeper understand of the data and give you ideas for feature engineering.","1b7227d0":"## 5.1. Train Hardcoded & Baseline Models\n\n- Hardcoded model: always predict average fare\n- Baseline model: Linear regression \n\nFor evaluation the dataset uses RMSE error: \nhttps:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\/overview\/evaluation","54ca560a":"## 4. Prepare Dataset for Training\n\n- Split Training & Validation Set\n- Fill\/Remove Missing Values\n- Extract Inputs & Outputs\n   - Training\n   - Validation\n   - Test","1e076bbd":"Fortunatly this doesn't exit in test dataset so we can **remove** these data from training set","ca1cb816":"There are 463 incidents with fare less than 0, I assume they might have used some **coupons** or might carry from past when they paid in surplus... \nBut since we don't have customer i.d. or cab id we can't infer these","45407a15":"### Just wanna check if this pattern is present in test dataset.","4a107276":"### iii) Random Forest\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","c962c660":"There are 210749 rows with log lat = 0,0\nand 200326 rows with 0 trip distance ","0e9970d8":"### Add Distance Between Pickup and Drop\n\nWe can use the haversine distance: \n- https:\/\/en.wikipedia.org\/wiki\/Haversine_formula\n- https:\/\/stackoverflow.com\/questions\/29545704\/fast-haversine-approximation-python-pandas","3c9b94c7":"* numpy.finfo(numpy.float16).precision **> 3**\n* numpy.finfo(numpy.float32).precision **> 6** (about 8 digit)\n* numpy.finfo(numpy.float64).precision **> 15**\n* numpy.finfo(numpy.float128).precision **> 18**\n* A UINT8 is an **8-bit `unsigned integer` (range: `0 through 255` decimal)** > Generally Taxi can accommodate single digit passangers so 255 is still over kill.","2208c83d":"Test and Submission have a difference of 1 row... we will look into this but this could me mostly an empty line","1d1e82cb":"`Why key is an object data type to me this these number in the dataframe?`","1fc7e256":"Why fare has min value as -ve and max value = infinite","9f0fbae9":"### Ridge Regression","12ca7d49":"## 5.2 Train & Evaluate Different Models\n\nWe'll train each of the following & submit predictions to Kaggle:\n\n- Ridge Regression\n- Random Forests\n- Gradient Boosting","753eb716":"### Train & Evaluate Hardcoded Model\n\ngeneral approach is to create a simple model that always predicts the average.\nBut we will use **linear regression **","2b790b2a":"# 3.1 Removing Outliers\nWe'll use the following ranges:\n\n- `fare_amount`: 1 to 500\n- `longitudes`: -75 to -72\n- `latitudes`: 40 to 42\n- `passenger_count`: 1 to 6","329d6bd9":"# 5 Modeling","11f7a601":"## Validation","f10cd007":"### Observations:\n\n- This is a supervised learning regression problem\n- Training data is 5.5 GB in size\n- Training data has 55 million rows (`55,423,856 rows`) \n- Test set is much smaller (`9,914 rows`)\n- The training set has 8 columns:\n    - `key` (a unique identifier)\n    - `fare_amount` (target column)\n    - `pickup_datetime`\n    - `pickup_longitude`\n    - `pickup_latitude`\n    - `dropoff_longitude`\n    - `dropoff_latitude`\n    - `passenger_count`\n- The test set has all columns except the target column `fare_amount`.\n- The submission file should contain the `key` and `fare_amount` for each test sample.\n- Evaluation is donw with **RMSE**\n","33b24835":"# 3. Feature Engineering\nAfter some exploraation I realised I sholud perform **Feature Engineering** to understand the data properly  \nas I saw fare to be -ve in around 463 rows and fare value more than 500 few time and also infinite twice or trice\n1. add a feature for distance between pickup place and drop place\n2. Need to perform feature extraction for time","b6de2727":"### Lets look at the 1st few lines of each dataset","70c8c362":"Time taken to execute XGBoost :: Wall time: 41min 23s :: CPU times: user 2h 28min 42s  \nXGBoost performed worse than Random forest it has a std of 1.2 to 1.35 usd per prediction","19e9fbb9":"training data is 5.5GB so it fails to load \nso lets analyse data with shell commands","0e2576cc":"### Extract Parts of Date\n\n- Year\n- Month\n- Day\n- Weekday\n- Hour\n","740abd51":"## Training","7a67752d":"Time taken by model : Wall time: 3.18 s\n* Model is not overfitting as both RMSE score is 5.138\n* This mean fare prediction is off by $ 5.138 which is `similar to Linear Regression` \n","70d820d0":"# What did I learnt from this notebook\n1. Always create a outline of the project as it `give us direction`\n2. How to handle **Large dataset**\n\n# What is the main objective of this Notebook\n1. Perform EDA and create a Baseline Model on sample data (20% of training set)\n2. Reduce the size of training set and than train the a model better than this Notebook\n    a. In next notebook use stacking and blending to achieve more respectable result","a85911bb":"Dataset has no missing values","f76eb917":"### Split Training & Validation Set\n\nWe'll set aside 20% of the training data as the validation set, to evaluate the models we train on previously unseen data. \n\nSince the test set and training set have the same date ranges, we can pick a random 20% fraction.","d26e9773":"* Rmse = 5.15384799403991 mean our prediction is off by 5.153 per prediction which is not good as **fare.median is 8.5**\n* our base model isn't overfitting as validation score is similar to training set","e5ab2c70":"why there are log and lat with 0,0","1fdbb01f":"## 8. Tune Hyperparmeters\n\nhttps:\/\/towardsdatascience.com\/mastering-xgboost-2eb6bce6bc76\n\n\nWe'll train parameters for the XGBoost model. Here\u2019s a strategy for tuning hyperparameters:\n\n- Tune the most important\/impactful hyperparameter first e.g. n_estimators\n\n- With the best value of the first hyperparameter, tune the next most impactful hyperparameter\n\n- And so on, keep training the next most impactful parameters with the best values for previous parameters...\n\n- Then, go back to the top and further tune each parameter again for further marginal gains\n\n- Hyperparameter tuning is more art than science, unfortunately. Try to get a feel for how the parameters interact with each other based on your understanding of the parameter\u2026\n\nLet's define a helper function for trying different hyperparameters.","e8cb9d51":"There are 210749 rows with log lat = 0,0"}}