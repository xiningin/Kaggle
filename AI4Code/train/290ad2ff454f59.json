{"cell_type":{"f8c8b4e0":"code","c2de39aa":"code","0bd843e5":"code","fbe9e090":"code","b62ff1cb":"code","39d3a25c":"code","701ee8c1":"code","2d106b11":"code","edc3f8be":"code","974f4cd0":"code","befb5e0a":"code","ca78231e":"code","ecf59c98":"code","58fe1ad7":"code","7bb1d339":"code","252beae3":"code","c9dbf9ec":"code","42d6ad4e":"code","86372674":"code","b7b4fee2":"code","62c64ffa":"code","bcbf7b3d":"code","1dec0610":"code","3078bb71":"code","c8e1cb6a":"code","41ac5cd0":"code","3e84d3a6":"code","1f83b814":"code","72067ed6":"code","88844f94":"code","6d451184":"code","62cc756f":"code","1698e5ef":"code","39144b68":"code","10d39c3b":"code","649d8ed8":"code","ba886ec4":"code","477d614a":"code","adfaf561":"code","df46571e":"code","fd6fa189":"code","b1bfb882":"code","c9b2deef":"code","36199d3b":"code","db5cd429":"code","bb4141dc":"code","6507a9ff":"code","fdf5d72d":"code","92c6a4da":"code","4e12e3f7":"code","17aade18":"code","12e6d41e":"code","bd75debd":"code","45b6f89b":"code","1a46b46f":"code","bd165400":"code","38303a94":"code","6ec1ed96":"code","b2ea69eb":"code","1b4c3b61":"code","965e1722":"code","316de418":"code","25d01b1c":"markdown","49b447bc":"markdown","59ff4da2":"markdown","286e0fe5":"markdown","d137af9d":"markdown","f7bde9b2":"markdown","8279279a":"markdown","9128c277":"markdown","53e67529":"markdown","c5f80027":"markdown","a723fbf8":"markdown","80e33e53":"markdown","aa88861b":"markdown","7bd36e6b":"markdown","0a1ec018":"markdown","e158f731":"markdown","84f1d17b":"markdown","049d4925":"markdown","0ac6b544":"markdown","560e200b":"markdown","f13d7ca3":"markdown","0beff96f":"markdown","9bf4d02f":"markdown","722b3d47":"markdown","4b61df66":"markdown","f4181048":"markdown","be51476a":"markdown","a4587d89":"markdown","e377ba7f":"markdown","623dc78b":"markdown","c68986a3":"markdown","da30fcc9":"markdown","63d7a1f0":"markdown","00929aff":"markdown","72e01374":"markdown","afd375d4":"markdown","f86685d3":"markdown","8e299b75":"markdown","844d2189":"markdown","e96ab9dc":"markdown","0f842d98":"markdown","1a2b5675":"markdown","86243609":"markdown","dd72392c":"markdown","ddeb866e":"markdown","024cafad":"markdown","beb97bc0":"markdown","77cbf4fc":"markdown","1e3c0f6f":"markdown","0db16cc0":"markdown","4b4bc97a":"markdown","94419d64":"markdown","7ed9823a":"markdown","c935b1d9":"markdown","da5c1ad1":"markdown","c1dda825":"markdown","bb3184f0":"markdown"},"source":{"f8c8b4e0":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nsns.set_style(\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c2de39aa":"dfs = dict()\ndfs[\"campaign_desc\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/campaign_desc.csv\")\ndfs[\"campaign_desc\"].head()","0bd843e5":"dfs[\"campaign_desc\"][\"DUR\"] = dfs[\"campaign_desc\"].END_DAY-dfs[\"campaign_desc\"].START_DAY\nfig = plt.figure(figsize=(10,5))\nsns.barplot(x=\"CAMPAIGN\",y=\"DUR\",data=dfs[\"campaign_desc\"],orient=\"v\",order=dfs[\"campaign_desc\"].sort_values(by=\"DUR\",ascending=False).CAMPAIGN.values)\nplt.show()","fbe9e090":"dfs[\"campaign_table\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/campaign_table.csv\")\ndfs[\"campaign_table\"].head()","b62ff1cb":"total_households = 2500\nlen(dfs[\"campaign_table\"].household_key.unique())","39d3a25c":"no_of_received_campaigns_per_house = dfs[\"campaign_table\"].household_key.value_counts()\nno_of_received_campaigns = no_of_received_campaigns_per_house.value_counts()\nno_of_received_campaigns[0] = total_households-len(no_of_received_campaigns_per_house)\nno_of_received_campaigns = pd.DataFrame(list(zip(no_of_received_campaigns.index,no_of_received_campaigns)),columns=[\"Number of Campaigns Received\",\"Number of Households Reached To\"]).sort_values(by=\"Number of Campaigns Received\",ascending=True)\nplt.figure(figsize=(10,5))\nsns.barplot(y=\"Number of Campaigns Received\",x=\"Number of Households Reached To\",data=no_of_received_campaigns,orient=\"h\",order=no_of_received_campaigns[\"Number of Campaigns Received\"])\nplt.show()","701ee8c1":"freq_campaigns = pd.DataFrame(list(zip(dfs[\"campaign_table\"].CAMPAIGN.value_counts().index, dfs[\"campaign_table\"].CAMPAIGN.value_counts())),columns=[\"Campaign\",\"Frequency\"])\nfig = plt.figure(figsize=(10,7))\nsns.barplot(y=\"Campaign\",x=\"Frequency\",data = freq_campaigns,orient=\"h\",order=freq_campaigns.Campaign)","2d106b11":"dfs[\"coupon\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/coupon.csv\")\ndfs[\"coupon\"].head()","edc3f8be":"dfs[\"coupon_redempt\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/coupon_redempt.csv\")\ndfs[\"coupon_redempt\"].head()","974f4cd0":"len(dfs[\"coupon_redempt\"].household_key.unique())","befb5e0a":"redeem_frequency = pd.DataFrame(list(zip(dfs[\"coupon_redempt\"].CAMPAIGN.value_counts().index, dfs[\"coupon_redempt\"].CAMPAIGN.value_counts())),columns=[\"Campaign\",\"No of Redeems\"])\nfig = plt.figure(figsize=(10,7))\nsns.barplot(y=\"Campaign\",x=\"No of Redeems\",data = redeem_frequency,orient=\"h\",order=redeem_frequency.Campaign)\nplt.show()","ca78231e":"\nredems_per_camp = dfs[\"coupon_redempt\"].CAMPAIGN.value_counts().sort_values()\/dfs[\"campaign_table\"].CAMPAIGN.value_counts().sort_values()\nredems_per_camp = pd.DataFrame(list(zip(redems_per_camp.index,redems_per_camp)),columns=[\"Campaign\",\"Redeem_Rate\"]).sort_values(by=\"Redeem_Rate\",ascending=False)\nplt.figure(figsize=(10,7))\nsns.barplot(y=\"Campaign\",x=\"Redeem_Rate\",data=redems_per_camp,orient=\"h\",order=redems_per_camp.Campaign)\nplt.show()","ecf59c98":"dfs[\"transaction_data\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/transaction_data.csv\")\ndfs[\"transaction_data\"].head()","58fe1ad7":"grouped_sum = dfs[\"transaction_data\"].groupby(\"household_key\").sum()","7bb1d339":"av_purc = round(dfs['transaction_data'].SALES_VALUE.sum()\/len(grouped_sum),1)\nav_purc","252beae3":"av_tot_prod = round(grouped_sum[\"QUANTITY\"].mean(),1)\nav_tot_prod","c9dbf9ec":"av_uniq_prod = round(len(dfs[\"transaction_data\"].groupby([\"household_key\",\"PRODUCT_ID\"]).sum()[\"QUANTITY\"])\/len(grouped_sum),1)\nav_uniq_prod","42d6ad4e":"av_days_visited = round(len(dfs[\"transaction_data\"].groupby([\"household_key\",\"DAY\"]).count())\/len(grouped_sum),1)\nav_days_visited","86372674":"sales_per_store = dfs[\"transaction_data\"].groupby(\"STORE_ID\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\nsales_per_store = pd.DataFrame(list(zip(sales_per_store.index,sales_per_store)),columns=[\"Store ID\",\"Total Sales (USD)\"])\nfig_store = plt.figure(figsize=(10,5))\nsns.barplot(y=\"Store ID\",x=\"Total Sales (USD)\",data = sales_per_store[:20],order=sales_per_store[:20][\"Store ID\"],orient=\"h\")\nplt.show()","b7b4fee2":"purc_per_cust = dfs[\"transaction_data\"].groupby(\"household_key\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\npurc_per_cust = pd.DataFrame(list(zip(purc_per_cust.index,purc_per_cust)),columns=[\"household_key\",\"Total Purchase (USD)\"])\nfig_store = plt.figure(figsize=(10,5))\nsns.barplot(y=\"household_key\",x=\"Total Purchase (USD)\",data = purc_per_cust[:20],order=purc_per_cust[:20][\"household_key\"],orient=\"h\")\nplt.show()","62c64ffa":"dfs[\"hh_demographic\"] = pd.read_csv(\"\/kaggle\/input\/dunnhumby-the-complete-journey\/hh_demographic.csv\")\ndfs[\"hh_demographic\"].head()","bcbf7b3d":"def pie_categorical(data):\n    #function to plot the histogram of categorical variables in pie graph\n    features = data.columns\n    #plot pie charts of categorical variables\n    fig_pie_cat = plt.figure(figsize=(15,15))\n    count = 1\n    #calculate dynamic numbers of subplot rows and columns\n    cols = int(np.ceil(np.sqrt(len(features))))\n    rows = int(np.ceil(len(features)\/cols))\n    for i in features:\n        ax = fig_pie_cat.add_subplot(rows,cols,count)\n        data[i].value_counts().plot(kind=\"pie\",autopct=\"%.1f%%\",ax=ax)\n        plt.ylabel(\"\")\n        plt.title(i,fontweight=\"bold\",fontsize=8)\n        count += 1\n\ndef hist_numeric(data):\n    #function to plot the histogram of numeric variables\n    features = data.columns\n    fig_hists = plt.figure(figsize=(15,15))\n    fig_hists.subplots_adjust(hspace=0.5,wspace=0.5)\n    count = 1\n    #calculate dynamic numbers of subplot rows and columns\n    cols = int(np.ceil(np.sqrt(len(features))))\n    rows = int(np.ceil(len(features)\/cols))\n    for i in features:\n        ax = fig_hists.add_subplot(rows,cols,count)\n        data[i].plot(kind=\"hist\",alpha=.5,bins=25,edgecolor=\"navy\",legend=False,ax=ax)\n        ax.set_xlabel(\"\")\n        ax.set_title(i,fontweight=\"bold\",fontsize=10)\n        count += 1","1dec0610":"pie_categorical(dfs[\"hh_demographic\"].drop(\"household_key\",axis=1))","3078bb71":"out_weeks_threshold = 2\n\n#weekly customer purchase amount\nweekly_purchase = dfs[\"transaction_data\"].groupby([\"household_key\",\"WEEK_NO\"]).sum()[\"SALES_VALUE\"]\nweekly_purchase = weekly_purchase.unstack()\n\n#customer churned by their last 2 week's purchasing behaviour\n#go through all households and calculate out weeks\ntarget = []\ncommon_houses = set(dfs[\"hh_demographic\"].household_key) & set(dfs[\"transaction_data\"].household_key)\nfor house in common_houses:\n  target.append(102-weekly_purchase.loc[house].dropna().index[-1])\n\ntarget = pd.DataFrame(list(zip(common_houses,target)),columns=[\"household_key\",\"No. of Churned Weeks\"])\nax = sns.distplot(target[\"No. of Churned Weeks\"],kde=False)\nax.axvline(x=out_weeks_threshold,c=\"red\",label=\"Out weeks threshold\")\nax.legend()\nax.set_yscale(\"log\")\nax.set_ylabel(\"Frequency (Log)\")\nplt.show()","c8e1cb6a":"target[\"isChurned\"] = target[\"No. of Churned Weeks\"]>=out_weeks_threshold\ntarget.drop(\"No. of Churned Weeks\",axis=1,inplace=True)","41ac5cd0":"target.isChurned.value_counts().plot(kind=\"pie\",autopct=\"%.1f%%\",labels=[\"Non-Churned\",\"Churned\"])\nplt.ylabel(\"\")\nplt.title(\"Churn Variable Distribution\",fontweight=\"bold\")\nplt.show()","3e84d3a6":"household_per_campaign = dfs[\"campaign_table\"].groupby(\"CAMPAIGN\")[\"household_key\"].apply(list)\ntotal_campaigns = len(dfs[\"campaign_table\"].CAMPAIGN.unique())\ndf_camp = pd.DataFrame(np.full((total_households,total_campaigns),0),columns=[\"Camp_\"+str(i) for i in range(1,total_campaigns+1)],index=range(1,total_households+1))\nfor camp in household_per_campaign.index:\n    df_camp.loc[household_per_campaign[camp],\"Camp_\"+str(camp)] = 1\n\ndf_camp[\"household_key\"] = df_camp.index","1f83b814":"temp = dfs[\"campaign_table\"].household_key.value_counts().sort_index()\nno_of_received_campaigns = pd.DataFrame(list(zip(temp.index,temp.values.astype(int))),columns=[\"household_key\",\"no_of_received_campaigns\"])","72067ed6":"\nno_of_received_campaigns_vs_sumsales = pd.merge(no_of_received_campaigns,purc_per_cust,on=\"household_key\")\nsns.lmplot(x=\"no_of_received_campaigns\",y=\"Total Purchase (USD)\",data=no_of_received_campaigns_vs_sumsales)\nplt.show()","88844f94":"no_of_received_campaigns_vs_sumsales.drop(\"household_key\",axis=1).corr()","6d451184":"temp = dfs[\"coupon_redempt\"].groupby(\"household_key\")[\"CAMPAIGN\"].apply(list)\nrede_camp_history_per_household = pd.DataFrame(list(zip(temp.index,temp.values)),columns=[\"household_key\",\"redeemed_CAMPAIGN_list\"])","62cc756f":"temp = dfs[\"coupon_redempt\"].household_key.value_counts().sort_index()\nno_of_rede_per_household = pd.DataFrame(list(zip(temp.index,temp.values.astype(int))),columns=[\"household_key\",\"no_of_redeems\"])","1698e5ef":"camp_list = dfs[\"campaign_table\"].groupby(\"household_key\")[\"DESCRIPTION\"].apply(list)\nmost_freq_type = pd.DataFrame(list(zip(camp_list.index,[pd.Series(i).value_counts().idxmax() for i in camp_list])),columns=[\"household_key\",\"most_freq_camp_type\"])","39144b68":"store_per_house = dfs[\"transaction_data\"].groupby(\"household_key\")[\"STORE_ID\"].apply(list).apply(np.unique)\n\nchurn_houses = set(target.household_key[target.isChurned == True].sort_values().values) & set(dfs[\"hh_demographic\"].household_key.unique())\nchurn_stores = []\nfor i in churn_houses:\n  churn_stores.extend(store_per_house[i])\n\nchurn_stores = pd.Series(churn_stores).value_counts()\/len(churn_houses)\n\ndf_store = dfs[\"transaction_data\"].groupby([\"STORE_ID\",\"household_key\"]).count()[\"BASKET_ID\"].unstack(\"STORE_ID\")\ndf_store.replace(np.nan,0,inplace=True)\ndf_store[df_store>0] = 1\n\n#select the top stores with high number of churners\ndf_store = df_store.loc[:,churn_stores.index[:20]]\ndf_store.columns=[\"Store_\"+str(i) for i in df_store.columns]","10d39c3b":"purc_per_cust = dfs[\"transaction_data\"].groupby(\"household_key\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\npurc_per_cust = pd.DataFrame(list(zip(purc_per_cust.index,purc_per_cust)),columns=[\"household_key\",\"Total Purchase (USD)\"])","649d8ed8":"data = pd.merge(dfs[\"hh_demographic\"],df_camp,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,no_of_received_campaigns,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,df_store,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,no_of_rede_per_household,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,most_freq_type,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,purc_per_cust,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,target,on=\"household_key\",how=\"left\")","ba886ec4":"data.shape","477d614a":"data.head()","adfaf561":"data.no_of_received_campaigns.replace(np.nan,0,inplace=True)\ndata.no_of_received_campaigns = data.no_of_received_campaigns.astype(int)\ndata.no_of_redeems.replace(np.nan,0,inplace=True)","df46571e":"data.KID_CATEGORY_DESC.replace([\"None\/Unknown\",\"3+\"],[0,3],inplace=True)\ndata.KID_CATEGORY_DESC = data.KID_CATEGORY_DESC.astype(int)\ndata.HOUSEHOLD_SIZE_DESC.replace(\"5+\",5,inplace=True)\ndata.HOUSEHOLD_SIZE_DESC = data.HOUSEHOLD_SIZE_DESC.astype(int)\ndata[\"Total Purchase (USD)\"] = data[\"Total Purchase (USD)\"].astype(int)","fd6fa189":"data.info()","b1bfb882":"data.dtypes.value_counts()","c9b2deef":"ax = sns.barplot(x=\"AGE_DESC\",y=\"isChurned\",data=data,order=[\"19-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\",\"65+\"])\nax.axhline(y=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","36199d3b":"ax = sns.barplot(x=\"MARITAL_STATUS_CODE\",y=\"isChurned\",data=data,order=[\"A\",\"B\",\"U\"])\nax.axhline(y=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","db5cd429":"fig = plt.figure(figsize=(8,5))\nax = sns.barplot(x=\"INCOME_DESC\",y=\"isChurned\",data=data,\n                 order=[\"Under 15K\",\"15-24K\",\"25-34K\",\"35-49K\",\"50-74K\",\"75-99K\",\"100-124K\",\"125-149K\",\"150-174K\",\"175-199K\",\"200-249K\",\"250K+\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.axhline(y=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","bb4141dc":"ax = sns.barplot(y=\"HOMEOWNER_DESC\",x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","6507a9ff":"ax = sns.barplot(y=pd.cut(data.no_of_redeems,bins=[-0.1,0,1,35],duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","fdf5d72d":"ax = sns.barplot(y=pd.qcut(data.no_of_received_campaigns,5,duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","92c6a4da":"ax = sns.barplot(y=\"HH_COMP_DESC\",x=\"isChurned\",data=data,orient=\"h\",order=['Unknown','Single Female','Single Male','2 Adults No Kids','2 Adults Kids','1 Adult Kids'])\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","4e12e3f7":"ax = sns.barplot(y=\"HOUSEHOLD_SIZE_DESC\",x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","17aade18":"ax = sns.barplot(y=\"most_freq_camp_type\",x=\"isChurned\",data=data,orient=\"h\",order=['TypeA','TypeB','TypeC'])\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","12e6d41e":"ax = sns.barplot(y=pd.qcut(data[\"Total Purchase (USD)\"],5,duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()\/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","bd75debd":"corrs = abs(data[[\"Camp_\"+str(i) if i<31 else \"isChurned\" for i in range(1,32)]].corr()[\"isChurned\"])\ncorrs.drop(\"isChurned\",inplace=True)\ncorrs = pd.DataFrame(list(zip(corrs.index,corrs)),columns=[\"Campaign No.\",\"Correlation with Target\"]).sort_values(by=\"Correlation with Target\",ascending=False)\nfig_store = plt.figure(figsize=(10,8))\nsns.barplot(y=\"Campaign No.\",x=\"Correlation with Target\",data = corrs,orient=\"h\")\nplt.show()","45b6f89b":"cols =list(df_store.columns)\ncols.extend([\"isChurned\"])\ncorrs = abs(data[cols].corr()[\"isChurned\"])\ncorrs.drop(\"isChurned\",inplace=True)\ncorrs = pd.DataFrame(list(zip(corrs.index,corrs)),columns=[\"Store ID\",\"Correlation with Target\"]).sort_values(by=\"Correlation with Target\",ascending=False)\nfig_store = plt.figure(figsize=(10,6))\nsns.barplot(y=\"Store ID\",x=\"Correlation with Target\",data = corrs,orient=\"h\")\nplt.show()","1a46b46f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\n#one hot encoding categorical data for modelling\nencoded = pd.get_dummies(data[data.columns[data.dtypes==object]])\ndata_encoded = pd.concat([encoded, data[data.columns[data.dtypes != object]]],axis=1)\n\n#Features (X) and the target (y)\nX = data_encoded.drop(\"isChurned\",axis=1)\ny = data_encoded.isChurned\n\n#lets start with the default hyperparameters and hold-out mechanism for train\/test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n#XGBoost\nxgb_mdl = XGBClassifier().fit(X_train.values,y_train.values)\nxgb_mdl","bd165400":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, average_precision_score\n\n#XGBoost\ny_pred_train = xgb_mdl.predict(X_train.values)\ny_pred = xgb_mdl.predict(X_test.values)\n\nprint(\"Train Data Classification Report:\\n\")\nprint(classification_report(y_train,y_pred_train))\n\nprint(\"Test Data Classification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\n#generate a confusion matrix to visualise precision, recall, misclassification and false alarms\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = list(set(y)), columns = list(set(y)))\n\n#visualise the confusion matrix in the heatmap form\nplt.figure()\nsns.heatmap(cm, annot = True, fmt=\"d\",\n            cmap=sns.color_palette(\"GnBu\")).set(xlabel='predicted values', \n                                                ylabel='real values', \n                                                title = 'Confusion Matrix')","38303a94":"roc_auc_score(y_test,y_pred)\n# average_precision_score(y_test,y_pred)","6ec1ed96":"import sklearn\nsorted(sklearn.metrics.SCORERS.keys())","b2ea69eb":"from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n\n#handle class imbalance problem by undersampling (not used)\n# no_churn_down_sampled = data_encoded[data_encoded.isChurned == False].sample(sum(data_encoded.isChurned))\n# data_encoded_balanced = pd.concat([no_churn_down_sampled,data_encoded[data_encoded.isChurned]])\n#shuffle the dataset to avoiding sampling biases\n# data = data_encoded_balanced.sample(frac=1)\n\n#shuffle the dataset to avoiding sampling biases\ndata = data_encoded.sample(frac=1)\nX = data.drop(\"isChurned\",axis=1)\ny = data.isChurned\n\n#generate an XGB classifier\nmdl = XGBClassifier()\n\n#parameter ranges\nparam_list = {\n    'silent': [False],\n    'max_depth': range(2,51),\n    'learning_rate': [0.001, 0.01, 0.1, 0.15],\n    'subsample': np.arange(0,1.1,.1),\n    'colsample_bytree': np.arange(0,1.1,.1),\n    'colsample_bylevel': np.arange(0,1.1,.1),\n    'min_child_weight': [0.5, 0.7, 1.0, 2.0, 3.0],\n    'gamma': [0, 0.25, 0.5, 0.75, 0.9, 1.0],\n    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0],\n    'n_estimators': [2, 5, 10, 20, 50, 100],\n    'scale_pos_weight': [1, 1.5, 2, 6, 6.1, 6.3, 6.5, 8],\n    'max_delta_step': [1, 2, 3, 5, 10]\n}\n\nkfold = 5\ncv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=10)\n\n#Randomized Search\n# cv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"roc_auc\",n_jobs=-1).fit(X.values,y.values)\ncv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"balanced_accuracy\",n_jobs=-1).fit(X.values,y.values)\n# cv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"average_precision\",n_jobs=-1).fit(X.values,y.values)\n\n#use the best estimator after the hyperparameter optimisation\nmdl_best = cv.best_estimator_","1b4c3b61":"cv.best_params_","965e1722":"from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n\ncv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=20)\nscores = cross_validate(mdl_best,X.values,y.values,cv=cv_strat,verbose=3,n_jobs=-1,return_train_score=True,\n                        scoring={\"roc_auc\":\"roc_auc\",\n                                 \"recall\":\"recall\",\n                                 \"precision\":\"precision\",\n                                 \"accuracy\":\"accuracy\",\n                                 \"balanced_accuracy\":\"balanced_accuracy\",\n                                 \"average_precision\":\"average_precision\"}) \n\npd.DataFrame(pd.DataFrame(scores).mean(),columns=[\"Score\"]).drop([\"fit_time\",\"score_time\"])","316de418":"feat_imp = pd.DataFrame(list(zip(data.columns,mdl_best.feature_importances_)),columns=[\"Feature\",\"Importance\"]).sort_values(by=\"Importance\",ascending=False)\nfig_store = plt.figure(figsize=(10,20))\nsns.barplot(y=\"Feature\",x=\"Importance\",data = feat_imp,orient=\"h\")\nplt.show()","25d01b1c":"Let's see how our features interact with the target variable. Start with the \"Age\" variable:\n\n* Households with the age of 55-64 tend to churn less then the rest of other age groups\n* No monotonic increase or decrease observed with age","49b447bc":"Even though getting lower accuracy in total, now we are getting much better results for the minority class as recall and ROC AUC (receiver operating characteristics, area under curve) increased significantly.\n\nThe score table above gives all metric results for both test and train sets. Train and test results are close which is an indication that our model did not overfit.\n\nLet's check which parameters were more important to separate churners\/non churners. The barchart below list the features ordered by their importance values for the XGB Classifier.\n\nAs expected, \"Total Purchase\", \"Number of Received Campaigns\", \"Campaign no. 25\", and \"Numer of redeems\" features are the most important features for the classifier. As we recall from feature engineering section, these features were either a good separator or highly correlated to the target variable.","59ff4da2":"Average total number of products purchased by a household within two years is:","286e0fe5":"On the other side, a very small portion of the households received double-digit number of campaigns. We will investigate how this will effect churn rates in the following sections. The barchart below groups the households and shows counts of the group populations.","d137af9d":"Hyperparameter optimisation is completed. Now lets train and test the XGBoost Classifier with the optimised hyperparameters","f7bde9b2":"### Last touches on the dataset\nGenerate the final dataframe to be used for predictive modelling","8279279a":"Graph below gives the most frequent campaigns. Campaign number 18, 13 and 8 being the most frequent ones reaching 1000 and more households each (no campaigns applied to same customer more than once therefore Frequency axis gives unique number of households).","9128c277":"# The Complete Journey Dataset Analysis: Churn Prediction\n\nIn this notebook we will make a churn prediction model using a retail dataset. Let's start importing necessary packages for the analysis.","53e67529":"Lets first start defining the functions for exploratory analysis of demographic variables:","c5f80027":"### Coupon Redemptions\nIs a data table, ordered by household_key, gives which household redeemed what coupon number. Also the day of the redeem and the campaign number is given for each household.","a723fbf8":"Average number of store visits per household within two years is:","80e33e53":"Now we have 91 features and a target variable in the final dataframe","aa88861b":"### Campaign\nCampaign dataset contains identifying information for the marketing campaigns each household participated in.","7bd36e6b":"Let's calculate and visualise how long each campaign lasted within two years period:\n\nCampaign No:15 lasts the longest with a staggering 160 days figure, where other campaigns are fairly close to each other ranging from 30 to 70 days\nAverage campaign duration is 37 days (median)","0a1ec018":"Change NaN or None\/Unknown values to zero for the columns of \"number of received campaigns\", \"kid category\" and the \"number of redeems\"","e158f731":"Feature 5: Most Frequent Campaign Type (A,B,C) received by each household:","84f1d17b":"Average total number of unique products purchased by a household within two years is:","049d4925":"Feature 6: Top 20 stores with high number of households which have more high out weeks:","0ac6b544":"Correlation between the stores with high number of churner customers and the Target (isChurned) are calculated and displayed in the barchart below:","560e200b":"Feature 2: Total number of received campaigns per household:","f13d7ca3":"Top 20 customers based on total purchase amount (USD) is calculated and displayed below. Top customer is with the ID of 1023 who made the most purchases amongst 2500 house holds with almost $40,000.","0beff96f":"## Exploratory Data Analysis\nThe dataset consist of eight tables in separate .csv files, we will go through six of them which we will use in this notebook. The dataset covers a two year span purchase transactions of 2500 households. Also demographics information of households, campaign and coupon redemption informations are available. In the modeling phase we will join these tables to make our final dataset.\n\n* Campaign Descriptions (campaigndesc.csv)\n* Campaigns (campaign_table.csv)\n* Coupons (coupon.csv)\n* Coupon Redemptions (coupon_redempt.csv)\n* Transactions (transaction_data.csv)\n* Demographics (hh_demographic.csv)\n\n### Campaign Descriptions\nCampaign description data is a look up table, containing the start and the end days of each campaign (30 campaigns in total). Also it gives which campaign belongs to what category (Type A, B and C).","9bf4d02f":"Out of 2500 households, 1584 of them received a campaign once, the rest never received a campaign.","722b3d47":"### Demographics Data\nDemographics data contains household demographical information such as age group, marital status and househols size. We will be using this table as a base and add few columns after feature engineering. Target variable for the churn prediction will be added in the final dataset before moving on to the predictive modelling.","4b61df66":"Home Ownership vs Churn Rate:\n\n* No significant differences between owners and renters","f4181048":"Most frequent campaigns (18, 13 and 8, all belongs to TypeA) attracts the most number of redeems as expected. The barchart below gives the number of redeems per campaign.","be51476a":"## Churn Prediction\nLet's start working on the \"Churn Prediction\" problem from here.\n\nDatasets have been examined and found out that it does not contain a column which indicates whether customer(household) is churned or not. Therefore we will have to define our own churn definition and move on to the modelling.\n\nChurn rate works well for subscription-based products or ones that have regular nature of interactions like Netflix subscription. It\u2019s clear that customer has churned at the moment when he cancels or misses the next planned payment.\n\nWhen we know which customers have churned we can ask them for reasons and prioritize fixes for them. But for not regular, transactional products churn rate is hard to measure since we don\u2019t know which customers are churned and which are dormant.\n\nA generally accepted retail churn rate is between five to sevent percent per year. Less than five percent is a great goal, but a churn rate over ten percent is cause for concern. Even as you acquire more customers, your business can't grow unless you have a greater volume of incoming customers than outgoing ones.\n\nSatisfying existing customers is actually more profitable than obtaining new ones. It costs five times more to obtain a new customer than it does to retain an existing customer. Decreasing your churn rate by five percent increases profits up to 125%.\n\nLet's define transactional churn:\n\nA customer will be considered as churned if not purchased from a store 2 weeks or more.\n\n2 weeks out threshold is chosen as it splits households around 85%\/15% No Churn\/Churn.","a4587d89":"Feature 4: The number of redemptions made by each household:","e377ba7f":"Marital Status vs Churn rate:\n\n* Married couples (Group A) tend to churn more compared to singles (Group B)\n* Unknown (Group U) households increase the chun rate of the population","623dc78b":"### Transactional Data\nTransactional data contains purchase history of each household. It contains the product ID and sales value, store ID and all other transactional features.","c68986a3":"Campaign Type vs Churn rate:\n\n* TypeC campaign fails as all households which was approached with campaign C are churned. However note that it has extremely lower sample size (1% of all population)\n* TypeB is the most successful one with the lowest churn rate","da30fcc9":"Churn rate tends to decrease as the number of family members increases","63d7a1f0":"### ML Model Training & Testing\nIn this section we will train a Machine Learning model with the training data. The machine learning model is chosen as XGBoost (Extreme Gradient Boosting) as they are known to be performing well with imbalanced datasets like ours.","00929aff":"We first transformed our categorical variables using one-hot encoding algorithm (get_dummies) to be able to use in our classifier. Then separated the data into train and test 75%\/25%. Then generated an XGBoost classifier with it's default parameters and trained it with the training set.\n\nNow let's test our trained classifier with test data:","72e01374":"Top 20 stores based on total sales amount (USD) is calculated and plotted below. Stores with IDs of 367 and 406 made the most sales amongst 582 stores with over $200,000 each.","afd375d4":"Let's see how households split with the defined churn:","f86685d3":"\nLets see if the \"number\" of campaigns being received affects a household's purchase behaviour.\n\nThe correlation between the number of campaigns received by a household and their total purchase amount is visualised in a scatter plot below. Linear line in the graph shows the regression model fitted to the scattered data. One can say the aforementioned variables are linearly related.","8e299b75":"The barchart below gives the redeem rates of each campaign in a descending order. Campaign 13 and 18 are clearly have higher redeem rates (~60%) as compared to the rest of campaigns.\n\nBased on these results, campaign organisers did well by promoting the right campaigns (13 and 18) more frequently.","844d2189":"Churn rate vs Number of campaigns received by a household\n\n* Churn rate decreases as the number of campaigns received by a household increases","e96ab9dc":"Out of 2500 households, only 434 of them (17%) redeemed coupons within this period.","0f842d98":"Number of Redeems vs Churn Rate:\n\nLets group our data based on household's number of redeems. We will use pandas.cut which groups the number of redeems into the bins. Graph below separates 0 redeemers, redeemed once and the final group redeemed more than once:\n\n* Churn rate decreases as the number of redeems increase","1a2b5675":"### Coupons\nIs a lookup table which lists all the coupons sent to customers as part of a campaign, as well as the products for which each coupon is redeemable","86243609":"Change other object type elements in the columns to integer for modeling purpose","dd72392c":"### Feature engineering\nLet's generate some features from the \"Campaign Table\", \"Transaction Data\" and \"Coupon Redempt\" tables to be used in the churn prediction model:\n\nFeature 1: List of campaigns received by each household:","ddeb866e":"_scale_pos_weight_ is the parameter used for biasing (weighing more) of the minority class samples. The higher the value the more bias towards minority class. In our dataset, the ratio between majority class samples and the minority class is 6.15 which coheres with this parameter (optimised value came out as 6).\n\nLets test our optimised model with the data:","024cafad":"Household_key is the ID of a household, therefore will not be used in the modelling. The rest of the data will be used.","beb97bc0":"The scatter plot and the correlation value above (71%) indicates that the total purchase amount of a household is positively correlated to the number of campaigns received by them.\n\nFeature 3: List of campaigns resulted in coupon redemption:","77cbf4fc":"Correlation between Campaign Number and the Target (isChurned) are calculated and displayed in the barchart below:\n\n* Campaign 25 as a feature expected to be performing well in the model as it has the highest correlation with the target variable","1e3c0f6f":"Now lets do some descriptive analysis on the transactional data. First, let's group the data by household numbers:","0db16cc0":"Feature 7: Amount of purchase of a household within two years:","4b4bc97a":"Even though accuracy for the test set is 88% it is misleading as our target variable is skewed towards not churned (86% are not churned). Even a very basic model which selects majority class all times would score 86% accuracy.\n\nTherefore we will have to focus on how well our model performs on the minority class (churned households). On the test set we have 201 samples only 23 of them being churned. Our model could not manage to detect any of them, therefore test set recall has come out as 0%. This is the part we need to aim to increase.\n\nNow lets try to enhance our model by optimising hyperparameters by using \"Randomised Search\" with \"Cross-Validation\".\n\nWe need to determine the evaluation criteria (scorer) to be optimised for the Randomised Search. Lets see the available scorers:\n\n* average precision, balanced accuracy, roc_auc, f1 are the scorers which are commonly used for imbalanced dataset classifications as they handle situation well as compared to other metrics","94419d64":"\nSince all variables are categorical in the demographics dataset, we will use our _piecategorical function. The pie charts below gives the distribution of each categorical variable in the demographics dataset.\n\nSome interesting findings:\n\n* Majority of the customers age in between 35-54 (~60%)\n* Married couples are almost 3 times the singles\n* Almost half of the population have a yearly salary between $35-74K\n* Majority of the customers own a house (63%)\n* Majority of the customers does not have a child (~70%)","7ed9823a":"Total purchase of a household vs Churn rate:\n\n* Total purchase amount of a customer is a clear separator, expected to perform well as a feature in the churn prediction model\n* Churn rate decreases as total amount of purchase of a household (within 2 years) increases","c935b1d9":"Income vs Churn Rate:\n\n* No monotonic increase or decrease observed over the income groups\n* However as seen in the barchart below, no churn was observed when a household's income reaches to 175K and more (27 samples out of 801 households)","da5c1ad1":"Not very clear, however churn rate tends to decrease when children starts to appear in households","c1dda825":"Average amount of purchase by a household within these two years is:","bb3184f0":"Now lets generate our target variable for modelling:\n\n* Churned -> True\n* Not Churned -> False"}}