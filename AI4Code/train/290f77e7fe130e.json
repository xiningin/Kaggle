{"cell_type":{"1f924996":"code","39e7c0d8":"code","ca053173":"code","b1427a19":"code","12a32c81":"code","a9f56083":"code","d9c1e253":"code","0ff64e44":"code","2b527a30":"code","a893282e":"code","9bb242cc":"code","169d2944":"code","3cba084f":"code","4467b132":"code","8d24790d":"code","931d1e8b":"code","1a804459":"code","edcb468a":"code","c4b8a8cf":"code","30b10e00":"code","ea007eb9":"code","5297f106":"code","0e48fb87":"markdown","b35744c4":"markdown","fe8776f7":"markdown","341817c3":"markdown","288e61de":"markdown","a939081d":"markdown","9f9dbe8b":"markdown","81e5950d":"markdown","08f98ac7":"markdown","d9c763c0":"markdown","039df2b6":"markdown","4b4c7555":"markdown","2ccdbaab":"markdown","326f4e9e":"markdown","5f86c834":"markdown","c278a9bb":"markdown","cda04eee":"markdown","7796c908":"markdown","b239088c":"markdown","b8317613":"markdown","3f666c1d":"markdown","8191e540":"markdown"},"source":{"1f924996":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport pylab as pl\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39e7c0d8":"df = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","ca053173":"df","b1427a19":"cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\noutput_col = [\"output\"]","12a32c81":"dfc = df\n\ndfc = pd.get_dummies(dfc, columns = cat_cols, drop_first = True)\n\nX = dfc.drop(['output'],axis=1)\ny = dfc[['output']]","a9f56083":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d9c1e253":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","0ff64e44":"yhat = LR.predict(X_test)\nyhat","2b527a30":"from sklearn.metrics import jaccard_score\njaccard_score(y_test, yhat,pos_label=0)","a893282e":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))","9bb242cc":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['output=1','output=0'],normalize= False,  title='Confusion matrix')","169d2944":"from sklearn.neighbors import KNeighborsClassifier","3cba084f":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","4467b132":"yhat = neigh.predict(X_test)\nyhat","8d24790d":"Ks = 50\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train.values.ravel())\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    yhat=yhat.reshape(61,1)\n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n#Best results for K=11 then K= 12 13 14 then K=6","931d1e8b":"from sklearn.tree import DecisionTreeClassifier\n#X_train, X_test, y_train, y_test","1a804459":"haTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nhaTree # it shows the default parameters","edcb468a":"haTree.fit(X_train,y_train)","c4b8a8cf":"predTree = haTree.predict(X_test)","30b10e00":"print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))","ea007eb9":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","5297f106":"clf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(\"SVM accuracy score: \", accuracy_score(y_test, y_pred))","0e48fb87":"Data Standardization give data zero mean and unit variance (technically should be done after train test split)","b35744c4":"### 3.3 Feature Selection","fe8776f7":"### 4.1.2 Metrics","341817c3":"### 3.2 One Hot Encoding","288e61de":"# 3. Pre-processing","a939081d":"### 3.1 Convert Categorical features to numerical values","9f9dbe8b":"# Conclusion","81e5950d":"### 4.1.1 Modeling and Predicting","08f98ac7":"### Check Tableau repo at: https:\/\/public.tableau.com\/app\/profile\/saadeddine.loughzali\/viz\/HeartAttackProject-Analysis\/CasesSummary","d9c763c0":"### 4.1.1 Modeling and Predicting","039df2b6":"## 4.3 Decision Tree","4b4c7555":"# 2. Visualization","2ccdbaab":"# 1.Import Data","326f4e9e":"let's build an accurate model. Then use the test set to report the accuracy of the model\nlet's try the following algorithms:\n\n*   K Nearest Neighbor(KNN)\n*   Decision Tree\n*   Support Vector Machine\n*   Logistic Regression","5f86c834":"## 4.1. Logistic Regression","c278a9bb":"## 4.2 K Nearest Neighbor(KNN)\n","cda04eee":"# 4. Classification","7796c908":"## 4.4 Support Vector Machine","b239088c":"## Highest scores:\n#### Logistic Regression: 0.72\n#### KNN: 0,75 K=11\n#### Decision Tree: 0,67\n#### SVM: 0,85","b8317613":"### Train \/ Test Split","3f666c1d":"### 3.4 Normalize Data","8191e540":"### 4.2.1 Modeling, Predicting and metrics"}}