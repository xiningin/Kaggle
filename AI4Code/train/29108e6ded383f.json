{"cell_type":{"1ca437da":"code","08e6c55d":"code","a7484d01":"code","86b756d9":"code","c416986d":"code","b5210bcd":"code","c71def53":"code","9aa7b05e":"code","11ff65a7":"code","fe37ca43":"code","af24661a":"code","2d5aebd2":"code","62efe604":"code","faa36870":"code","b67d5999":"code","6d6627a8":"code","2ee1a102":"code","acfe796f":"code","3324bd51":"code","7c1e45f8":"code","621e7bbc":"code","b6745194":"code","53d42bd0":"code","3c83b2d3":"code","8ed28bd0":"code","2c1ea2ef":"code","0064de40":"code","b31575bb":"code","050df19a":"code","fecbc9be":"code","cdc5787e":"code","574e2f26":"markdown","c25d2f90":"markdown","28b1a7a2":"markdown","5e8afa20":"markdown","813f8c19":"markdown","52b2f786":"markdown","93b4fceb":"markdown","837ba883":"markdown","6831317e":"markdown","c2723e7f":"markdown","e4fce91d":"markdown","afaaebed":"markdown","77784da4":"markdown","90730945":"markdown","301ef6c2":"markdown","adf623b0":"markdown","e5c4bf9a":"markdown","6d1b774f":"markdown","a4bdfb3d":"markdown","400e21f0":"markdown","f5a606bb":"markdown","b0b89132":"markdown","d9cfa04d":"markdown","63bc258f":"markdown","023c9ac3":"markdown","c909c137":"markdown","79286bab":"markdown"},"source":{"1ca437da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08e6c55d":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import neighbors, tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import SGDRegressor, LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error","a7484d01":"df = pd.read_csv('\/kaggle\/input\/gpu-runtime\/sgemm_product.csv')\nsns.set()","86b756d9":"print(\"Number of nulls for each column:\")\nprint(df.isnull().sum())\nprint(df.shape)","c416986d":"df['Runtime']=df[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)\ndf = df.drop(columns =['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'], axis = 1)\ndf.head()","b5210bcd":"df.info()","c71def53":"sns.set()\ndf.hist(figsize=(14,16))","9aa7b05e":"df.describe().T","11ff65a7":"sns.boxplot(x=df['Runtime']);","fe37ca43":"Q1=df['Runtime'].quantile(0.25)\nQ3=df['Runtime'].quantile(0.75)\nIQR = Q3 - Q1\nMIN=Q1-1.5*IQR\nMAX=Q3+1.5*IQR\ndf = df[(df.Runtime>MIN) & (df.Runtime<MAX)]\ndf.describe().T","af24661a":"sns.boxplot(x=df['Runtime']);","2d5aebd2":"sns.distplot(df['Runtime'])","62efe604":"df['target']=np.log(df.Runtime)\nsns.distplot(df['target'])","faa36870":"plt.figure(figsize=(14,14))\nax = sns.heatmap(df.corr(),annot=True, linewidths=.5, cmap=plt.cm.Blues)\nplt.title('Variable Correlation')","b67d5999":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df.corr()[['target']].sort_values(by='target', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with target', fontdict={'fontsize':18}, pad=16);","6d6627a8":"df_target = df[['target']]\nY = df_target.to_numpy().ravel()\ndf_features = df.drop(columns=['target','Runtime'])\nX = df_features.to_numpy()","2ee1a102":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nprint(X)","acfe796f":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\nlist_alpha = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\nlist_MSE = []\n\nfor i in list_alpha:\n    regr = SGDRegressor(alpha = i)\n    regr.fit(X_train, Y_train)\n    Y_pred = regr.predict(X_test)\n    MSE = mean_squared_error(Y_test, Y_pred)\n    list_MSE.append(MSE)\n    print(f'MSE with alpha={i}: {MSE}  \/  N iterations to converge: {regr.n_iter_}')\n    \nplt.plot(list_alpha,list_MSE)\nplt.xlabel('Alpha')\nplt.ylabel('MSE')\nplt.show()","3324bd51":"for i in range(0,10):\n    X = df_features.sample(axis = 1,random_state=i,n=8) \n    print(f'Using features: {X.columns}')\n    X = X.to_numpy()\n    X = scaler.fit_transform(X)\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\n\n    regr = SGDRegressor(alpha = 0.0001)\n    MSE = cross_val_score(regr, X_train, Y_train, cv=5, scoring = \"neg_mean_squared_error\").mean()\n    MSE = abs(MSE)\n    print(f'MSE: {MSE}')","7c1e45f8":"best_features = ['MWG', 'SA','NWG',  'VWM','MDIMC', 'NDIMC','SB', 'STRM','NDIMB' ,'VWN', 'KWI','MDIMA','KWG', 'STRN']","621e7bbc":"list_MSE = []\n\nfor i in range(1,16):\n    X = df_features[best_features[:i]]\n    print(f'Using features: {X.columns}')\n    X = X.to_numpy()\n    X = scaler.fit_transform(X)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\n\n    regr = SGDRegressor(alpha = 0.0001)\n    MSE = cross_val_score(regr, X_train, Y_train, cv=5, scoring = \"neg_mean_squared_error\").mean()\n    MSE = abs(MSE)\n    list_MSE.append(MSE)\n    print(f'MSE: {MSE}')\n    \nplt.plot(range(1,16),list_MSE)\nplt.xlabel('N features')\nplt.ylabel('MSE')\nplt.show()","b6745194":"best_features.reverse()\nlist_MSE = []\n\nfor i in range(1,16):\n    X = df_features[best_features[:i]]\n    print(f'Using features: {X.columns}')\n    X = X.to_numpy()\n    X = scaler.fit_transform(X)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\n\n    regr = SGDRegressor(alpha = 0.0001)\n    MSE = cross_val_score(regr, X_train, Y_train, cv=5, scoring = \"neg_mean_squared_error\").mean()\n    MSE = abs(MSE)\n    list_MSE.append(MSE)\n    print(f'MSE: {MSE}')\n    \nplt.plot(range(1,16),list_MSE)\nplt.xlabel('N features')\nplt.ylabel('MSE')\nplt.show()","53d42bd0":"X = scaler.fit_transform(df_features)\ndf_target = pd.qcut(df['target'].values, q=4, labels=False)\nY = df_target.ravel()\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)","3c83b2d3":"unique, counts = np.unique(Y, return_counts=True)\nprint(dict(zip(unique, counts)))","8ed28bd0":"tree_classifier = tree.DecisionTreeClassifier()\ntree_score = cross_val_score(tree_classifier, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nscore = tree_score.mean()\nprint(f'Cross validation accuracy with K-fold=5: {score}')\n\ntree_classifier.fit(X_train, Y_train)\nY_pred = tree_classifier.predict(X_test)\n\nprint(f'Test accuracy: {metrics.accuracy_score(Y_test, Y_pred)}')\n\ndisp = metrics.plot_confusion_matrix(tree_classifier, X_test, Y_test, cmap=plt.cm.Blues)\ndisp.ax_.set_title('Test Results')","2c1ea2ef":"knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn_score = cross_val_score(knn_classifier, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nscore = knn_score.mean()\nprint(f'Cross validation accuracy with K-fold=5: {score}')\n\nknn_classifier.fit(X_train, Y_train)\nY_pred = knn_classifier.predict(X_test)\n\nprint(f'Test accuracy: {metrics.accuracy_score(Y_test, Y_pred)}')\n\ndisp = metrics.plot_confusion_matrix(knn_classifier, X_test, Y_test, cmap=plt.cm.Blues)\ndisp.ax_.set_title('Test Results')","0064de40":"logistic_classifier = LogisticRegression()\nlogistic_score = cross_val_score(logistic_classifier, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nscore = logistic_score.mean()\nprint(f'Cross validation accuracy with K-fold=5: {score}')\n\nlogistic_classifier.fit(X_train, Y_train)\nY_pred = logistic_classifier.predict(X_test)\n\nprint(f'Test accuracy: {metrics.accuracy_score(Y_test, Y_pred)}')\n\ndisp = metrics.plot_confusion_matrix(logistic_classifier, X_test, Y_test, cmap=plt.cm.Blues)\ndisp.ax_.set_title('Test Results')","b31575bb":"random_classifier = RandomForestClassifier()\nrandom_score = cross_val_score(random_classifier, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nscore = random_score.mean()\nprint(f'Cross validation accuracy with K-fold=5: {score}')\n\nrandom_classifier.fit(X_train, Y_train)\nY_pred = random_classifier.predict(X_test)\n\nprint(f'Test accuracy: {metrics.accuracy_score(Y_test, Y_pred)}')\n\ndisp = metrics.plot_confusion_matrix(random_classifier, X_test, Y_test, cmap=plt.cm.Blues)\ndisp.ax_.set_title('Test Results')","050df19a":"my_dict = {'Tree': tree_score, '5-NN': knn_score, 'Logisitic':logistic_score, 'Forest': random_score}\n\nfig, ax = plt.subplots()\nax.boxplot(my_dict.values())\nax.set_xticklabels(my_dict.keys())\nplt.ylabel('Accuracy')","fecbc9be":"parameters = {'criterion':['gini', 'entropy'],\n              'max_features':[0.1, 0.3, 0.5],\n              'n_estimators': range(50, 201, 50)}","cdc5787e":"rf = RandomForestClassifier()\nclf = GridSearchCV(estimator = rf,  param_grid = parameters,scoring = 'accuracy',cv = 5,verbose=0)\ngrid_search = clf.fit(X_train, Y_train)\n\n# best score achieved during the GridSearchCV\nprint('GridSearch CV best score : {:.4f}\\n'.format(grid_search.best_score_))\n\n# print parameters that give the best results\nprint('Parameters that give the best results :','\\n', (grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\nprint('\\n\\nEstimator that was chosen by the search :','\\n', (grid_search.best_estimator_))","574e2f26":"# Dataset Information\n\nThis data set measures the running time of a matrix-matrix product A \u00b7 B = C, where all matrices have size 2048 x 2048, using a parameterizable SGEMM GPU kernel with 241600 possible parameter combinations. For each tested combination, 4 runs were performed and their results are reported as the 4 last columns. All times are measured in milliseconds.\n\nThe experiment was run on a desktop workstation running Ubuntu 16.04 Linux with an Intel Core i5 (3.5GHz), 16GB RAM, and a NVidia Geforce GTX 680 4GB GF580 GTX-1.5GB GPU.\n\n#### Attribute Information:\n\nIndependent variables:\n- 1-2. MWG, NWG: per-matrix 2D tiling at workgroup level: {16, 32, 64, 128} (integer)\n- 3. KWG: inner dimension of 2D tiling at workgroup level: {16, 32} (integer)\n- 4-5. MDIMC, NDIMC: local workgroup size: {8, 16, 32} (integer)\n- 6-7. MDIMA, NDIMB: local memory shape: {8, 16, 32} (integer)\n- 8. KWI: kernel loop unrolling factor: {2, 8} (integer)\n- 9-10. VWM, VWN: per-matrix vector widths for loading and storing: {1, 2, 4, 8} (integer)\n- 11-12. STRM, STRN: enable stride for accessing off-chip memory within a single thread: {0, 1} (categorical)\n- 13-14. SA, SB: per-matrix manual caching of the 2D workgroup tile: {0, 1} (categorical)\n\nOutput:\n- 15-18. Run1, Run2, Run3, Run4: performance times in milliseconds for 4 independent runs using the same parameters. They range between 13.25 and 3397.08.","c25d2f90":"### Normalitzation\n\nSo as to normalize our data and bring all the variables to the same range we should use an scaler. Due to not having outliers we might have no problem to use MinMaxScaler from sklearn. MinMaxScaler rescales the data set such that all feature values are in the range [0,1].\n\nFirst of all we have to split our data into features (X) and target (Y).","28b1a7a2":"We can also check the type of our features using the function info from pandas.","5e8afa20":"First of all we will check if there are any nulls in our dataset.","813f8c19":"# Classification\n\nFirst of all we have to convert this problem into a multi-classification problem, we will assign each value to the quartile it is part of.","52b2f786":"#### Nearest Neighbor","93b4fceb":"As we can see once we reach the seven most correlated features together we get the lowest error, and the rest of the features do not affect the performance. So now we could opt just to train our model with less features.\n\nFurthermore, to prove that the seven most correlated features are the ones that imply the best performance we can also try the same experiment but beginning with the least correlated.","837ba883":"We opt to not take into account those values that form part of the outliers.","6831317e":"### Target transformation\n\nAs we see in the Runtime histogram we have a logarithmic distribution, so it is a good idea to opt for a logarithmic transformation.","c2723e7f":"# Exploratory Data Analysis\n\nRead data from our csv file.","e4fce91d":"# Regression\n\nWe will apply a regression model using Stochastic Gradient Descent from sklearn.\n\n### Learning rate\n\nFirst of all we are going to study the affect of the parameter alpha (learning rate). ","afaaebed":"We suppose that Nearest Neighbor performance is worse because of the known \"curse of dimensionality\", due to the high number of features it is difficult to find the right weights and to determine which features are not important for classification. \n\nFurthermore, Nearest Neighbor is a distance-based algorithm, so when working with large datasets its performance degrades.\n\n#### Logistic Regression","77784da4":"### Outliers\n\nWe see that although the Runtime mean is 217.57 and the median just 69.79, its maximum value is 3341.51, so it is worth checking if there are any outliers in our data.\n\nLets check it with a boxplot and remove it if it is necessary.\n\n<img src=\"https:\/\/miro.medium.com\/max\/8000\/1*0MPDTLn8KoLApoFvI0P2vQ.png\" width=\"500\" align=\"left\">","90730945":"And as proven before we see that it is when we begin to use the most correlated features that the error gets lower.\n\nUnfortunately we do not achieve a lower error than the one with every feature, but with less features to train our model we may reduce our training time.","301ef6c2":"We also see that Logistic Regression performance is the worse so far.\n\nLogistic Regression only estimates a linear boundary. So, when there is non-linear separation of labels, Logistic regression could fail badly. As a result we can estimate that our classification is non-linear separable.\n\n#### Random Forest","adf623b0":"This way we achieve a normal distribution of our target variable.\n\nAs a result, now we can visualize a heatmap that will show us the correlation between our features and our target variable.","e5c4bf9a":"We see that some features set have a higher error than others, so now we are going to study the relevance of each feature training our model from the most correlated feature to the least.\n\nFirstly we will train it with just the most correlated feature, and in each iteration we will add the next most correlated until we have the whole dataset.","6d1b774f":"# Preprocessing","a4bdfb3d":"We see that we have no nulls, so it will not be necessary to do any null value treatment.\n\nWe opt to simplify the target variable in just one column because the four runs are from the same program.","400e21f0":"### Model selection\n\nIn the following experiment we will evaluate the following models:\n- Decision Tree\n- Nearest Neighbor\n- Logistic Regression\n- Random Forest\n\n#### Decision Tree","f5a606bb":"Now, without the outliers, we can check the distribution of our target.","b0b89132":"We should check if our dataset is balanced so as to know which metrics will be more reliable.","d9cfa04d":"As we can see the lower alpha we get the lower MSE but it usually takes longer to converge.\n\n### Feature selection\n\nSo far we have trained our model with every feature we have, now we will try to train it with 8 random features, using 0.0001 as alpha. \n\nAdditionally, the following experiments will be implemented using cross validation, with K-fold=5, so as to prevent possible overfitting and get more reliable results.","63bc258f":"We see as checked before that there are no nulls and also every feature is an int, except our target that is float, so it should not be necessary to do any special feature treatment. \n\nWe can also visualize the distribution of each feature.","023c9ac3":"And as expected, we can see that every int variable is categorical while the target is continuous. We also realize that the target variable has some values that are worth studying in case they were outliers.\n\nAdditionally, we can see some more basic stats using the function describe from pandas.","c909c137":"We can also visualize the correlation with our target variable in a sorted and clearer way.","79286bab":"We can conclude that Decision Tree and Random Forest give the best accuracy because every feature is categorical.\n\nEven though Random Forest is an ensemble method from Decision Tree, its performance is not better, so we will opt to perform an hyperparameter search over Random Forest to know if it can get a better accuracy.\n\n### Hyperparameter Search"}}