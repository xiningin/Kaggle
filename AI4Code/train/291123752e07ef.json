{"cell_type":{"a5515bbf":"code","63003771":"code","a1543be4":"code","6566c247":"code","376a8ac8":"code","3ee2da24":"code","cef3676b":"code","1f33755a":"code","1c16e06d":"code","c731a6b1":"code","3cacfe23":"code","16aea6e6":"code","e947c50a":"code","bcc86d12":"code","857ea114":"code","09e74f34":"code","71849992":"code","196d6ff2":"code","5f0055f0":"code","75ca8642":"code","8ff565e9":"code","444664ae":"code","e375846d":"code","cea46e85":"code","7b9a3c38":"code","0d7c2fa7":"code","1f7140fe":"code","c2c23bed":"code","aee7af27":"code","dc7a3464":"code","3343af92":"code","296394d6":"code","d293c6b3":"code","c8b392aa":"code","c6f06bf1":"code","dbd83a8c":"code","6d79b9ac":"code","e7a0866a":"code","dd378e4e":"code","9ea173e0":"code","173ca9b8":"code","cc125e17":"code","79515b58":"code","803137d7":"code","1542fabf":"code","7d95f794":"code","23acb343":"code","416a23d8":"code","de7e3e10":"code","89c5f9a3":"code","3541e700":"code","e4e922af":"code","6aa4bf5b":"code","1281239a":"code","ae5de08d":"code","88098fe7":"code","991fb862":"code","01cc4523":"markdown","692dfd7a":"markdown","f8c0dada":"markdown","5e341b7c":"markdown","c7361be2":"markdown","be71beeb":"markdown","3e9e7a7f":"markdown","284e50a4":"markdown","9b89ba21":"markdown","08309936":"markdown","3487e648":"markdown","fc89563a":"markdown","d255cca0":"markdown","ceaf1e1e":"markdown","bc6fddc9":"markdown","2079c1a9":"markdown","467926c2":"markdown","14c56170":"markdown","306ee458":"markdown","8c029d6f":"markdown","6e9329c5":"markdown","893b04ce":"markdown","e136e65f":"markdown","736cca85":"markdown","29cde0b7":"markdown","1d6ec4fb":"markdown","0ab478b2":"markdown","f38e09a3":"markdown","89739051":"markdown","a0656ba4":"markdown","6a7e559d":"markdown","f55dc9c0":"markdown","6aba0b41":"markdown","52e6f8fc":"markdown","811d80bf":"markdown","ff2b6d76":"markdown","673c64dc":"markdown","7b7cfcdb":"markdown","0714cf2c":"markdown","64c19491":"markdown","fb799612":"markdown","d3a8bd3d":"markdown","ad44ea81":"markdown","d5ffc68f":"markdown","83199fc7":"markdown","6da3b887":"markdown","24e23633":"markdown","8c44b9f8":"markdown","a4021948":"markdown","79b1560b":"markdown","ddb646cf":"markdown","c65fd4be":"markdown","ac358a9a":"markdown","e26ce9d7":"markdown","8d1e18cb":"markdown","04b9e71b":"markdown","cfe329eb":"markdown","97d442ba":"markdown","c0237108":"markdown","ae06633b":"markdown","e5394ed8":"markdown","2e4d811d":"markdown","92387e71":"markdown","3a90f41a":"markdown","72cbe7a2":"markdown","30efe098":"markdown","7597a4e1":"markdown"},"source":{"a5515bbf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nfrom scipy import stats\nfrom scipy.stats import skew\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 100)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os,random, math, psutil, pickle\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom tqdm import tqdm\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Activation\nfrom keras.optimizers import Adam, Adagrad\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","63003771":"def merge_meta(df, m_df):\n    temp_df = df[['building_id']]\n    temp_df = temp_df.merge(m_df, on=['building_id'], how='left')\n    del temp_df['building_id']\n    return pd.concat([df, temp_df], axis=1)\n\ndef merge_weather(df, w_df):\n    temp_df = df[['site_id','timestamp']]\n    temp_df = temp_df.merge(w_df, on=['site_id','timestamp'], how='left')\n    del temp_df['site_id'], temp_df['timestamp']\n    return pd.concat([df, temp_df], axis=1)","a1543be4":"building_meta_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\")\ntrain_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\")\nweather_train_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\")","6566c247":"train_df = merge_meta(train_df, building_meta_df)\ntrain_df = merge_weather(train_df, weather_train_df)","376a8ac8":"train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])","3ee2da24":"train_df['age'] = train_df['year_built'].max() - train_df['year_built'] + 1\ntrain_df = train_df.drop('year_built', axis=1)","cef3676b":"# Convert wind direction into categorical feature. We can split 360 degrees into 16-wind compass rose. \n# See this: https:\/\/en.wikipedia.org\/wiki\/Points_of_the_compass#16-wind_compass_rose\ndef rose_petalize(x):\n    if np.isnan(x):\n        return x\n    else: \n        val=int((x\/22.5))\n        arr=[i for i in range(0,16)]\n        return arr[(val % 16)]\n\n# Since this is a cyclical variable, convert it to x and y on a circle.\n# See this: http:\/\/blog.davidkaleko.com\/feature-engineering-cyclical-features.html\ndef sinify_petals(X):\n    return np.sin(X*(2.*np.pi\/16))\ndef cosify_petals(X):\n    return np.cos(X*(2.*np.pi\/16))\n    \ntrain_df['wind_direction'] = train_df['wind_direction'].apply(rose_petalize)\ntrain_df['wind_sin'] = sinify_petals(train_df['wind_speed'])\ntrain_df['wind_cos'] = cosify_petals(train_df['wind_speed'])\n#train_df = train_df.drop('wind_direction')","1f33755a":"beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\n# The original wind speed column is left in the dataframe, as it is \n# not certain that the new categorical alternative will be better. \nfor item in beaufort:\n    train_df.loc[(train_df['wind_speed']>=item[1]) & (train_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]","1c16e06d":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","c731a6b1":"train_df = reduce_mem_usage(train_df)","3cacfe23":"train_df.to_pickle('train_df.pkl')\n#test_df.to_pickle('test_df.pkl')\n\ndel train_df#, test_df\ngc.collect()","16aea6e6":"train_df = pd.read_pickle('train_df.pkl')","e947c50a":"fig = plt.figure(figsize=(20,13))\nfor i, (name, meter_group) in enumerate(train_df.groupby('meter')):  \n    ax = fig.add_subplot(2, 2, i+1)\n    meter_group = meter_group[meter_group['meter_reading'] < 5000]\n    sns.distplot(meter_group['meter_reading']).set_title('Distribution for meter {}'.format(name))","bcc86d12":"import statsmodels.api as sm\n\nbuilding_id = 500\ndata = train_df[train_df['building_id'] == building_id]\ndata = data[data['meter'] == 0]\nfig = plt.figure(figsize=(20,6))\n_ = sm.graphics.tsa.plot_pacf(data['meter_reading'], lags=300, ax=plt.gca())\nplt.xlabel('Lag (hour)');\nplt.ylabel('Autocorrelation');","857ea114":"id_df = train_df.groupby(['building_id']).mean()\nsns.distplot(id_df['age'], bins = 15, kde = True, rug = False, norm_hist=False);","09e74f34":"fig = plt.figure(figsize=(20,35))\nfig.subplots_adjust(hspace=0.6, wspace=0.4)\nn = 1\nfor name, site_group in train_df.groupby(['site_id']):\n    ax = fig.add_subplot(8, 2, n)\n    id_df = site_group.groupby(['building_id']).mean()\n    nan_amount = id_df['age'].isnull().sum()\n    nan_ratio = nan_amount \/ len(id_df)\n    # Some out the sites that have no buildings with ages.\n    if nan_ratio < 1:\n        percent = np.around((1-nan_ratio)*100, 1)\n        text = 'Site {}, {} buildings, of which {} percent have age.'.format(name, len(id_df), percent)\n        sns.distplot(id_df['age'], bins = 15, kde = True, rug = False, norm_hist=False, ax=ax).set_title(text)\n        n = n+1","71849992":"fig = plt.figure(figsize=(20,13))\nn = 1\nfor name, site_group in train_df.groupby('meter'):  # use enumerate\n    ax = fig.add_subplot(2, 2, n)\n    id_df = site_group.groupby(['building_id']).mean()\n    id_df.plot(kind='scatter', x='age', y='meter_reading', color='red', alpha=0.4, title='Meter: {}'.format(name), ax=ax)\n    n = n+1","196d6ff2":"def plot_age_energy_sitewise(meter):\n    fig = plt.figure(figsize=(15,40))\n    fig.subplots_adjust(hspace=0.7)\n    n = 1\n    this_meter_df = train_df[train_df['meter'] == meter]\n    for name, site_group_df in this_meter_df.groupby('site_id'):  # use enumerate\n        buildings_df = site_group_df.groupby(['building_id']).mean()\n        ax = fig.add_subplot(8, 2, n)\n        buildings_df.plot(kind='scatter', x='age', y='meter_reading', color='red', alpha=0.4, title='Site: {}'.format(name), ax=ax)\n        n = n+1\n        \nplot_age_energy_sitewise(0)","5f0055f0":"def use_energy_plot(df, meter):\n    meter_df = df[df['meter'] == meter]\n    \n    temp_df = meter_df.copy()\n    le = LabelEncoder()\n    meter_df['primary_use'] = le.fit_transform(meter_df['primary_use']).astype(np.int8)\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    \n    buildings_df = meter_df.groupby(['building_id']).mean()\n    del temp_df\n    \n    plt.subplots(figsize = (10,7))\n    sns.barplot(x = 'primary_use', \n                y = 'meter_reading', \n                data=buildings_df, \n                linewidth= 5,\n                capsize = .1\n               )\n    plt.title(\"Energy usage vs. primary use for meter {}\".format(meter), fontsize = 16, pad=20)\n    plt.xticks(le.transform(le.classes_), le.classes_, rotation=90)\n    plt.ylabel(\"Energy usage\", fontsize = 15);\n    plt.xlabel(None);\n\nuse_energy_plot(train_df, 0)\nuse_energy_plot(train_df, 1)\nuse_energy_plot(train_df, 2)\nuse_energy_plot(train_df, 3)","75ca8642":"temp_df = train_df.copy()\nle = LabelEncoder()\ntemp_df['primary_use'] = le.fit_transform(temp_df['primary_use']).astype(np.int8)\nle_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n\nbuildings_df = temp_df.groupby(['building_id']).mean()\ndel temp_df\n\nplt.subplots(figsize = (15,10))\nsns.barplot(x = 'primary_use', \n            y = 'floor_count', \n            data=buildings_df, \n            linewidth=5,\n            capsize = .1\n           )\nplt.title(\"Floor count vs. primary use\", fontsize = 16, pad=20)\nplt.xticks(le.transform(le.classes_), le.classes_, rotation=90)\nplt.ylabel(\"Floor count\", fontsize = 15);\nplt.xlabel(None);","8ff565e9":"# Optimizing by numpy-vectorization.\n\ndef closest_index(a,x):\n    closest_index = np.abs(a-x).argmin()\n    if a[closest_index] - x >= 0:\n        return closest_index\n    else:\n        return min((closest_index + 1, len(a) - 1))\n\ndef weather_energy_matrix(df, feature):    \n    temperature_bin_edges = range(int(df['air_temperature'].min()), int(df['air_temperature'].max()))\n    feature_bin_edges = range(int(df[feature].min()), int(df[feature].max()))[1:]\n    \n    df = df[['meter_reading', 'air_temperature', feature]].dropna()\n    X = df['meter_reading'].to_numpy()\n    T = df['air_temperature'].to_numpy()\n    F = df[feature].to_numpy()\n\n    # Assigning each feature and temperature value into a bin (taking the upper bin edge).\n    F_ = [closest_index(feature_bin_edges,f) for f in F]\n    T_ = [closest_index(temperature_bin_edges,t) for t in T]\n    shape = (len(temperature_bin_edges), len(feature_bin_edges))\n    sums_matrix, counts_matrix = np.zeros(shape), np.zeros(shape)\n    for x,t,f in zip(X,T_,F_):\n        sums_matrix[t,f] += x \n        counts_matrix[t,f] += 1\n    M = pd.DataFrame(data=sums_matrix\/counts_matrix, index=temperature_bin_edges, columns=feature_bin_edges)\n    return M\n    \ndef plot_weather_energy_heatmaps(df, feature, meter):\n    meter_df = df[df['meter'] == meter]\n    n = 1\n    fig = plt.figure(figsize=(40,60))\n    fig.subplots_adjust(hspace=1)\n    for site_id, site_group_df in meter_df.groupby('site_id'):\n        M = weather_energy_matrix(site_group_df, feature)\n        if ~M.isna().all().all():\n            ax = fig.add_subplot(8, 2, n)\n            sns.heatmap(M, cmap='Reds')\n            ax.set_title('Meter {}, site {}'.format(meter, site_id))\n            plt.xlabel(feature)\n            plt.ylabel('Air temperature')\n        label_freq = 3\n        for ind, label in enumerate(ax.get_yticklabels()):\n            label.set_visible((ind % label_freq) == 0)\n        for ind, label in enumerate(ax.get_xticklabels()):\n            label.set_visible((ind % label_freq) == 0)\n        n += 1","444664ae":"plot_weather_energy_heatmaps(train_df, 'beaufort_scale', 0)","e375846d":"# This kernel takes too long to commit, so switched off.\n'''\nplot_weather_energy_heatmaps(train_df, 'wind_direction', 0)\n'''","cea46e85":"fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True, sharey=True, figsize=(16, 12))\nfor sid in weather_train_df.site_id.unique():\n    row = int(sid \/ 4)\n    col = sid%4\n    tmp_df = weather_train_df[weather_train_df.site_id==sid]\n    tmp_df['timestamp'] = pd.to_datetime(tmp_df['timestamp'])\n    tmp_df = tmp_df.set_index('timestamp')\n    missing = 100 \/ len(tmp_df) * tmp_df['wind_direction'].isnull().sum()\n    tmp_df.groupby(tmp_df.index.hour)['wind_direction'].median().plot(ax=axes[row,col])\n    if missing !=0:\n        axes[row, col].set_title(f\"site {sid}, null:{missing :.2f}%\", fontsize=12, color=\"darkred\")\n    else:\n        axes[row, col].set_title(f\"site {sid}\", fontsize=12, color=\"darkgreen\")\n    axes[row, col].set_xlabel(\"\")\nfig.suptitle(f\"wind_direction per hour of day\", fontsize=18)\nfig.subplots_adjust(top=0.92)","7b9a3c38":"import missingno as msno\n\ndf = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\")\ndf['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])\nmsno.matrix(df.set_index(['timestamp']))","0d7c2fa7":"weather_train_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\")\nweather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])\nweather_train_df = weather_train_df.set_index(['timestamp'])\nfor i in range(15):\n    site_df = weather_train_df[weather_train_df['site_id'] == i]\n    msno.matrix(site_df.iloc[:,1:], freq='M')","1f7140fe":"# I am not including the month in here, as the year should already be well covered by the week of year.\ndef cyc_time(df):\n    df['hour_of_day'] = df['timestamp'].dt.hour.astype(np.int8)\n    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype(np.int8)\n    df['week_of_year'] = df['timestamp'].dt.weekofyear.astype(np.int8)\n    return df\n\ntrain_df = cyc_time(train_df)","c2c23bed":"train_df['hour_sin'] = np.sin(train_df['hour_of_day']*(2.*np.pi\/24))\ntrain_df['hour_cos'] = np.cos(train_df['hour_of_day']*(2.*np.pi\/24))\n\ntrain_df['day_sin'] = np.sin(train_df['day_of_week']*(2.*np.pi\/7))\ntrain_df['day_cos'] = np.cos(train_df['day_of_week']*(2.*np.pi\/7))\n\ntrain_df['week_sin'] = np.sin(train_df['week_of_year']*(2.*np.pi\/52))\ntrain_df['week_cos'] = np.cos(train_df['week_of_year']*(2.*np.pi\/52))","aee7af27":"# Using numpy in addition to pandas due to the latter's slowness.\n\ndef shift_features(df, features, shift_n):\n    print('Shifting {}'.format(features))\n    total_values_array = np.empty((1,len(features)+1))\n    for x, g in df.groupby(('building_id', 'meter')):\n        \n        shifted_features = g[features].shift(shift_n).to_numpy()\n        shifted_features_with_index = np.column_stack((g.index, shifted_features))\n        # Concatenating this building's data (as extra rows) to all the data.\n        total_values_array = np.r_[total_values_array, shifted_features_with_index]\n    \n    # Exclude the first row, as it is the empty starting array.\n    V = total_values_array[1:,:]\n    \n    # Sort rows according to index to match with dataframe's indices.\n    V = V[V[:,0].argsort()]\n\n    # Adding the data to the dataframe.\n    for i, f in enumerate(features):\n        df.loc[V[:,0].astype(int), 'shifted {} {}'.format(shift_n,f)] = V[:,i+1]    \n    \n    return df","dc7a3464":"features_to_shift = [\n                        'meter_reading',\n                        'air_temperature',\n                    ]\n\n\n#train_df = shift_features(train_df, features_to_shift, 1)\n#train_df = shift_features(train_df, features_to_shift, 2)\n#train_df = shift_features(train_df, features_to_shift, 3)\n#train_df = shift_features(train_df, features_to_shift, 4)","3343af92":"# Getting the weather one as a separate function from the energy,\n# as it is much faster to do it from the weather_df itself rather\n# than the merged train_df.\ndef roll_weather(df, features, long_window, short_window):\n    for x, g in df.groupby('site_id'):\n        rolling_now = g[features].rolling(window=short_window, center=False).mean()\n        rolling_mean = g[features].rolling(window=long_window, center=False).mean()\n        rolling_std = g[features].rolling(window=long_window, center=False).std()\n        target_values = ((rolling_now - rolling_mean) \/ rolling_std)\n        df.loc[target_values.index, target_values.columns] = target_values\n        return_df = df[features].add_prefix('rolling {}\/{} '.format(short_window, long_window))\n        return_df[['site_id', 'timestamp']] = df[['site_id', 'timestamp']]\n        return_df['timestamp'] = pd.to_datetime(return_df['timestamp'])\n        \n        # Filling the first NaNs (left by the rolling) with the original data.\n        return_df.loc[:long_window, features] = df.loc[:long_window, features]\n    return return_df\n\n\n    \n# Using numpy in addition to pandas due to the latter's slowness.\ndef roll_energy(df, long_window, short_window):\n\n    all_target_values = []\n    all_index_values = []\n    for x, g in df.groupby(('building_id', 'meter')):\n        rolling_now = g['energy_per_area'].rolling(window=short_window, center=False).mean()\n        rolling_mean = g['energy_per_area'].rolling(window=long_window, center=False).mean()\n        rolling_std = g['energy_per_area'].rolling(window=long_window, center=False).std()\n        target_values = ((rolling_now - rolling_mean) \/ rolling_std).to_numpy()\n        \n        # Filling the first NaNs (left by the rolling) with the original data.\n        target_values[:long_window] = g.iloc[:long_window, df.columns.get_loc('energy_per_area')]\n        \n        all_target_values = np.append(all_target_values, target_values)\n        all_index_values = np.append(all_index_values, g.index)\n        \n    V = np.column_stack((all_index_values, all_target_values))\n\n    # Ordering the column of indices so that it would match with the dataframe.\n    V = V[V[:,0].argsort()]\n    \n    col_name = 'rolling {}\/{} energy per area'.format(short_window, long_window)\n    df[col_name] = V[:,1]\n    \n    return df","296394d6":"# Takes too long to commit this kernel, so switch off.\n'''\nlong_window = 20\nshort_windows = [2]\n\n# Using the original weather dataframe to do the rolling \n# due to the slowness of doing it for each building using the merged dataframe.\nweather_features_to_roll = ['air_temperature', 'wind_speed', 'cloud_coverage', 'precip_depth_1_hr', 'sea_level_pressure']\n    \nfor w in short_windows:\n    train_df = roll_energy(train_df, long_window, w)\n    rolled_weather = roll_weather(weather_train_df, weather_features_to_roll, long_window, w)\n    train_df = merge_weather(train_df, rolled_weather) #pd.concat([train_df, rolled_weather], axis=1)\n    \ndel weather_df\ngc.collect()\n'''","d293c6b3":"selected_features =  ['timestamp',\n                     'shifted 1 meter_reading',\n                     #'shifted 2 meter_reading',\n                      #'shifted 3 meter_reading',\n                      #'shifted 4 meter_reading',\n                      'shifted 1 air_temperature',\n                      #'shifted 2 air_temperature',\n                      #'shifted 3 air_temperature',\n                      #'shifted 4 air_temperature', \n                      'air_temperature',\n                      'hour_sin', 'hour_cos', \n                      'day_sin', 'day_cos',\n                      'week_sin', 'week_sin']","c8b392aa":"def sequential_trial_sets():\n    kfold = KFold(n_splits=5)\n    cvscores = []\n    b_id = 589\n    n_hours = 600\n\n    building_n_df = train_df[train_df['building_id'] == b_id][:n_hours]\n    building_n_df = building_n_df[building_n_df['meter'] == 0]\n    building_n_df = building_n_df[~building_n_df['meter_reading'].isna()].reset_index(drop=True)\n    building_n_df = building_n_df[~building_n_df[selected_features].isna().any(axis=1)].reset_index(drop=True)\n\n    y = building_n_df['meter_reading']\n    X = building_n_df[selected_features]\n    \n    return X, y, train_test_split(X, y, test_size=0.3, random_state=3)","c6f06bf1":"def try_sequential():\n    X, y, (X_train, X_test, y_train, y_test) = sequential_trial_sets()\n    all_dates = X['timestamp']\n    test_dates = X_test['timestamp']\n    X_train = X_train.drop('timestamp', axis=1)\n    X_test = X_test.drop('timestamp', axis=1)\n\n    model = Sequential(\n            [\n                Dense(50, activation=\"relu\", input_dim=len(X_train.columns)),\n                Dense(1, activation=\"linear\")\n            ]\n        )\n    model.compile(optimizer=Adam(), loss=\"mean_squared_error\")\n    model.fit(X_train, y_train, epochs=10, verbose=1)\n    y_pred = model.predict(X_test)\n    plt.figure(figsize=(16,5))\n    plt.scatter(test_dates, y_pred, s=4, label='Predicted', alpha=1)\n    plt.scatter(all_dates, y, color='r', s=13, label='Actual', alpha=0.2)\n    plt.title('Predicted first {} hours for building {}'.format(n_hours, b_id))\n    plt.legend()\n    \n#try_sequential()","dbd83a8c":"le = LabelEncoder()\ntrain_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)","6d79b9ac":"categoricals = [\"building_id\",  \"meter\", 'day_of_week', 'week_of_year', 'hour_of_day']#, 'primary_use', 'hour_of_day']\n\nnumericals = [\"air_temperature\",\n              #\"dew_temperature\",\n              #'cloud_coverage',\n              #'square_feet',\n              #'floor_count'\n             ]\n\nshifted_numericals = ['shifted 1 meter_reading',\n                      #'shifted 2 meter_reading',\n                      #'shifted 3 meter_reading',\n                      #'shifted 4 meter_reading',\n                      'shifted 1 air_temperature',\n                      #'shifted 2 air_temperature',\n                      #'shifted 3 air_temperature',\n                      #'shifted 4 air_temperature',\n                     ] \n            \n\nselected_features = categoricals + numericals + shifted_numericals","e7a0866a":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'subsample_freq': 1,\n            'learning_rate': 0.3,\n            'bagging_freq': 5,\n            'num_leaves': 330,\n            'feature_fraction': 0.9,\n            'lambda_l1': 1,  \n            'lambda_l2': 1\n            }","dd378e4e":"def gbm_trial_sets():\n    building_n_df = train_df[train_df['building_id'] == b_id]\n    building_n_df = building_n_df[building_n_df['meter'] == 0]\n\n    y = building_n_df['meter_reading']\n    X = building_n_df[selected_features + ['timestamp']]\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3) #split_data(X, y, 0.3)\n\n    all_dates, val_dates = X['timestamp'], X_val['timestamp']\n    X_train, X_val = X_train.drop('timestamp', axis=1), X_val.drop('timestamp', axis=1)\n    \n    return X_train, X_val, y_train, y_val","9ea173e0":"def try_gbm():\n    X_train, X_val, y_train, y_val = gbm_trial_sets()\n    \n    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=500,\n                    valid_sets=(lgb_train, lgb_eval),\n                    early_stopping_rounds=50,\n                    verbose_eval = 50)\n    y_pred = gbm.predict(X_val)\n\n    plt.figure(figsize=(16,5))\n    plt.scatter(val_dates, y_val, color='r', s=13, alpha=0.2)\n    plt.scatter(val_dates, y_pred, s=2, alpha=1)\n    x1,x2,y1,y2 = plt.axis()\n    plt.axis(('2016-01-01 00:00:00', '2016-01-25 00:00:00', y1, y2))\n    \n#try_gbm()","173ca9b8":"from scipy.stats import pearsonr, spearmanr\n\n# Excluding the columns that are not weather or are cyclical.\nfeatures = train_df.columns.values[8:]\nfeatures = np.setdiff1d(features, ['energy_per_area', \n                                   'age', \n                                   'floor_count',\n                                   'square_feet',\n                                   'wind_direction', \n                                   'hour_of_day',\n                                   'day_of_week',\n                                   'week_of_year',\n                                   'beaufort_scale',\n                                   'wind_sin', 'wind_cos', \n                                   'hour_sin', 'hour_cos', \n                                   'day_sin', 'day_cos',\n                                   'week_sin', 'week_cos'])\n\ndef multicollinearity(df):\n    fig = plt.figure(figsize=(10,7))\n    corr = df[features].corr()\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    sns.heatmap(corr, mask=mask, annot=False, cmap='PiYG')\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5) \n\ndef meter_correlations(df, meter):\n    data = df[df['meter'] == meter]\n    all_corres = np.empty([1,len(features)])\n    all_corres[:] = np.nan\n    for i, g in data.groupby('building_id'):\n        building_corres = []\n        for f in features:\n            feature_data = g[['meter_reading', f]]\n            feature_data = feature_data[~feature_data[f].isna()]\n            # Proceed only if there is sufficient data of this feature.\n            sufficient_n = 200\n            is_suf = (~feature_data[f].isna()).sum() > sufficient_n\n            is_inf = any(np.isinf(feature_data[f]))\n            if is_suf & ~is_inf:\n                building_corres.append(pearsonr(feature_data['meter_reading'], feature_data[f])[0])\n            else:\n                building_corres.append(np.nan)\n        all_corres = np.vstack((all_corres, building_corres))\n    return np.nanmean(all_corres, axis=0)","cc125e17":"# I will just paste an image that I got before, using this same code, instead of\n# lengthening the commit time by doing it again.\n'''\ncorres = [meter_correlations(train_df, i) for i in range(4)]\nfig, ax = plt.subplots(figsize=(20,5))\nax = sns.heatmap(corres, xticklabels=features, yticklabels=range(4), cmap='PiYG', annot=False, ax=ax, vmin=-1, vmax=1)\n# Removing for the plot's truncation caused by matplotlib 3.1.1 \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5) \n'''","79515b58":"multicollinearity(train_df)","803137d7":"def backwards():\n    y = train_df['meter_reading']\n    X = train_df[selected_features]\n\n    lgb_train = lgb.Dataset(X, y, categorical_feature=categoricals)\n    del y, X\n    gc.collect()\n\n    history = lgb.cv(params,\n                    lgb_train,\n                    shuffle=True,\n                    stratified=False,\n                    nfold=5,\n                    num_boost_round=500,\n                    categorical_feature=categoricals,\n                    early_stopping_rounds=30,\n                    verbose_eval = 30)\n\n    'Last RMSE: {}'.format(history['rmse-mean'][-1])\n\n#backwards():","1542fabf":"def prepare_test():\n    test_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\")\n    building_meta_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\")\n    weather_test_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\")\n\n    test_df = merge_meta(test_df, building_meta_df)\n    test_df = merge_weather(test_df, weather_test_df)\n    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n    test_df = cyc_time(test_df)\n\n    features_to_shift = ['air_temperature']\n    test_df = shift_features(test_df, features_to_shift, 1)\n    #test_df = shift_features(test_df, features_to_shift, 2)\n    #test_df = shift_features(test_df, features_to_shift, 3)\n    #test_df = shift_features(test_df, features_to_shift, 4)\n    \n    test_df = reduce_mem_usage(test_df)\n    test_df.to_pickle('test_df.pkl')\n    gc.collect()\n    \n    return test_df","7d95f794":"#test_df = prepare_test()\n#test_df = pd.read_pickle('test_df.pkl')","23acb343":"def gbm_bag(df, features, cats):\n    folds = 5\n    seed = 387\n    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n    models = []\n    i=0\n    for train_index, val_index in kf.split(df[features], df['building_id']):\n        i += 1\n        #print('Fold {}'.format(i))\n        train_X, val_X = df.loc[train_index, features], df.loc[val_index, features]\n        train_y, val_y = df.loc[train_index, 'meter_reading'], df.loc[val_index, 'meter_reading']\n        #train_y, val_y = np.log1p(df.loc[train_index, 'meter_reading']), np.log1p(df.loc[val_index, 'meter_reading'])\n        lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=cats)\n        lgb_val = lgb.Dataset(val_X, val_y)\n        gbm = lgb.train(params,\n                        lgb_train,\n                        num_boost_round=500,\n                        valid_sets=lgb_val,\n                        early_stopping_rounds=30,\n                        verbose_eval = -1)\n        models.append(gbm)\n    return models","416a23d8":"def bagging_predict(bag, X):\n    return np.mean([model.predict(X) for model in bag]) ","de7e3e10":"categoricals = ['day_of_week', 'week_of_year', 'hour_of_day']\nnumericals = [\"air_temperature\"]\nshifted_numericals = ['shifted 1 meter_reading'] \n\ntrain_df_features = categoricals + numericals + shifted_numericals\ntest_df_features = categoricals + numericals","89c5f9a3":"scores = []\n\ndef train_predict_separately():\n    for (_, train_g), (i, test_g) in zip(train_df.groupby(['building_id', 'meter']), test_df.groupby(['building_id', 'meter'])):\n\n        # This if is useful when having to stop and restart this script.\n        if (i[0] > -1):\n            print('Building {}'.format(i))\n\n            bag = gbm_bag(train_g.reset_index(), train_df_features, categoricals[2:])\n            scores.append([m.best_score for m in bag])\n\n            energy_shifts = 1\n            temperature_shifts = 1\n            preds = np.array([])\n\n            # Adding the last values from the previous year into the preds array,  \n            # as they are the shifted values of the first values of the new year.\n            start_energies = train_g.loc[train_g.index[-energy_shifts:], 'meter_reading'] #np.log1p(train_g.loc[train_g.index[-energy_shifts:], 'meter_reading']) \n            start_temperatures = train_g.loc[train_g.index[-temperature_shifts:], 'air_temperature']\n            preds = np.concatenate((preds, start_energies))\n            temperatures = np.append(start_temperatures.to_numpy(), test_g['air_temperature'].to_numpy())\n\n            # Iterating over the numpy array instead of using g.iterrows(),\n            # as the latter is too slow.\n            for j, row in enumerate(test_g[test_df_features].to_numpy()):\n\n                shifted_energies = preds[-energy_shifts:]\n                shifted_temperatures = temperatures[j:j+temperature_shifts]\n                row = np.concatenate((row, shifted_energies))\n                row = np.concatenate((row, shifted_temperatures))\n\n                # Reshaping due to LightGBM's 2D array requirement.\n                row = np.array(row).reshape((1,-1))\n                preds = np.append(preds, bagging_predict(bag, row))\n\n            test_df.loc[test_g.index, 'meter_reading'] = preds[energy_shifts:] #expm1(preds[energy_shifts:])","3541e700":"# Takes too long to commit with this, so flipped off.\n'''\ntrain_predict_separately()\n\n# Mean score from validation sets.\nprint(np.mean([X[0]['valid_0'].popitem()[1] for X in scores]))\n\ntest_df = reduce_mem_usage(test_df)\ntest_df.to_pickle('test_df.pkl')\ngc.collect()\n'''","e4e922af":"from dateutil.relativedelta import relativedelta\n\ndef test_for_building(b_id):\n    train_building_df = train_df[(train_df['building_id'] == b_id) & (train_df['meter'] == 0)]\n    test_building_df = test_df[(test_df['building_id'] == b_id) & (test_df['meter'] == 0)]\n\n    train_building_df = train_building_df.set_index('timestamp')['2016-04-01 00:00:00' : '2016-07-01 00:00:00']\n    test_building_df = test_building_df.set_index('timestamp')['2017-04-01 00:00:00' : '2017-07-01 00:00:00']\n\n    plt.figure(figsize=(16,4))\n    plt.scatter(train_building_df.index, train_building_df['meter_reading'], c='r', s=13, alpha=0.2,)\n    plt.scatter([ x - relativedelta(years=1) for x in test_building_df.index ], test_building_df['meter_reading'], s=2, alpha=1)","6aa4bf5b":"'''\ntest_for_building(601)\n'''","1281239a":"train_df_features = categoricals + numericals + shifted_numericals\ntest_df_features = categoricals + numericals","ae5de08d":"def train_predict_holistically():\n    \n    bag = gbm_bag(train_df, train_df_features, categoricals)\n    \n    for (_, train_g), (i, test_g) in zip(train_df.groupby(['building_id', 'meter']), test_df.groupby(['building_id', 'meter'])):\n        if (i[0] > -1):\n            print(i)\n            energy_shifts = 1\n            temperature_shifts = 1\n            preds = np.array([])\n\n            # Adding the last values from the previous year into the preds array,  \n            # as they are the shifted values of the first values of the new year.\n            start_energies = train_g.loc[train_g.index[-energy_shifts:], 'meter_reading'] \n            start_temperatures = train_g.loc[train_g.index[-temperature_shifts:], 'air_temperature']\n            preds = np.concatenate((preds, start_energies))\n            temperatures = np.append(start_temperatures.to_numpy(), test_g['air_temperature'].to_numpy())\n\n            # Iterating over the numpy array instead of using g.iterrows(),\n            # as the latter is too slow.\n            for j, row in enumerate(test_g[test_df_features].to_numpy()):\n\n                shifted_energies = preds[-energy_shifts:]\n                shifted_temperatures = temperatures[j:j+temperature_shifts]\n                row = np.concatenate((row, shifted_energies))\n                row = np.concatenate((row, shifted_temperatures))\n\n                # Reshaping due to LightGBM's 2D array requirement.\n                row = np.array(row).reshape((1,-1))\n                preds = np.append(preds, bagging_predict(bag, row))\n\n            test_df.loc[test_g.index, 'meter_reading'] = preds[4:]","88098fe7":"'''\ntrain_predict_holistically()\ntest_for_building(601)\n'''","991fb862":"#test_df[['row_id', 'meter_reading']].to_csv('submission.csv')","01cc4523":"Setting up the data:","692dfd7a":"**Weather at each site**","f8c0dada":"# Feature selection\n\nIt is time to curate the optimal set of features from the ones given and the ones created above. I will use two approches: correlation, and backward elimination. \n\n### Correlation","5e341b7c":"![ll](https:\/\/preview.redd.it\/12kjs65k2ah21.jpg?width=640&crop=smart&auto=webp&s=24b5dcf14075ff7c19e201514a6751770f71511a)\n\n# Mission\n\nAccording to ASHRAE,\n\n> Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing.\n\nOur task (yes, your and mine) is to embark on a quest to find such a suitable model. It will be used to predict the energy usages in buildings of various types all over the world for about 16 months, based on all their energy usage data from 2016. Besides various attributes of the buildings, this task obviously requires also weather data, as besides the time of day and week, weather is intuitively the most important determinant of energy use. Therefore the chosen buildings, of which there are over 9000, are clustered near 15 weather stations.","c7361be2":"### Cyclical time features","be71beeb":"# Encoding","3e9e7a7f":"### STDs away from rolling mean","284e50a4":"There is very strong correlation with the previous hour's reading. There is also considerable negative correlation with the values at multiples of 24 h before.","9b89ba21":"The code in this section is from this excellent kernel: https:\/\/www.kaggle.com\/caesarlupum\/ashrae-ligthgbm-simple-fe\/notebook#7.-Handling-missing-values","08309936":"The most common age is around 50 years. We can also see that there are three primary categories of age: about 0 to 30, 30 to 75, and above 75. We should try to encode the age with these.\n\nNext, lets look at the age distributions at each site.","3487e648":"This is not a clear sign of victory, though: I have yet to find out how biased the model is towards the training set.","fc89563a":"# Exploratory modeling\n\n![ll](https:\/\/media.giphy.com\/media\/jKhxkINMo429G\/giphy.gif)\n\n### Trying a Keras Sequential model","d255cca0":"# Setting up","ceaf1e1e":"While somewhat interesting per se, this does not yield any insights for the feature engineering.","bc6fddc9":"Looking at the KDEs (the continuous lines), we see that the distribution of most sites is slightly skewed towards buildings less than 50 years old.","2079c1a9":"### Age vs. energy use\n\n**A mean of all buildings, for each meter type.**\n","467926c2":"### Converting year built into age","14c56170":"### Backwards elimination\n\nThis process is cumulative, i.e., when a feature is marked with REMOVE, the ones after that are done with it removed. Each new RMSE score is hence compared to the last one marked with REMOVE.","306ee458":"**With all:** 29553\n\n**Without floor count:** 29716   REMOVE\n\n**Without square feet:** 30025   REMOVE\n\n**Without hour of day:**  27523  REMOVE\n\nThis is surprising, given the intuitively huge significance of the time of day for energy use.\n\n**Without day of week:**   34521  KEEP   \n\n**Without week of year:**  42546  KEEP \n\n**Withou cloud coverage:**   27273  REMOVE\n\n**Without primary use**:    26958    REMOVE\n\n**Without dew temperature:**   29371   KEEP\n\n**Without shifted 4 air temperature:**  27791   KEEP\n\n**Without shifted 4 meter reading:** 27323  KEEP\n\n\nHere I have the issue of not knowing what threshold I should use for removing or keeping a feature. For example, the first increase in the RMSE above, when the floor count is removed, is only 0.6%. There must be a robust technique for finding the threshold.","8c029d6f":"# Modeling and prediction","6e9329c5":"### Predicting with a single bag of models for the whole dataset","893b04ce":"### Trying linear regression in LightGBM\n\nInspired by https:\/\/www.kaggle.com\/caesarlupum\/ashrae-ligthgbm-simple-fe\/notebook#10.-Baseline","e136e65f":"**Notes on individual features**\n\n*Dew temperature*: discard due to high collinearity with air temp.\n\n*Cloud coverage*: a non-trivial case. On one hand, it seems that it should have a significant impact when the air temperature is high; after all, people yearn for shadow in summer heat. However, there is  no definite threshold above which it would be valid to use the cloud coverage. Using the actual function that relates cloud coverage (and radiation angle) to temperature and projected energy could of course bring big value to the model, but the point of this project is rather to find a heuristic model. In other words, though an intuitive comprehension of the underlying physics is indispensable for this mission, I shall refrain from the actual equations, and focus on learning data science. As such, taking into account the awful sparsity of the cloud coverage feature at most of the sites, it is best to drop it from the upcoming modeling.\n\n*Cyclicals*: I did not include the cyclical features (wind direction, hour of day and period of year) here because correlation is not valid with them. However, the time-based ones must be significantly descriptive of the energy; this is apparent in the daily and yearly cycles of the series. The wind direction also could be important, as there are specific cold or hot winds in certain areas of the world (for example, in Europe when the wind is from the north, it is usually colder than otherwise, as it comes from Siberia). Although such heat or coldness is probably usually far better encapsulated in the air temperature, I suspect that the wind direction is worth trying.\n\n**Choose our team:**\n\n* n-1 energy\n\n* Air temp\n\n* Cyclicals: Hour of day, Day of week, and Week of year.","736cca85":"### Wind: En-compassing direction and Beau-tifying speed","29cde0b7":"Checking what the predictions look like:","1d6ec4fb":"![image.png](attachment:image.png)\n\n**<center>Looking promising.<\/center>**\n![ll](https:\/\/media.giphy.com\/media\/YFis3URdQJ6qA\/giphy.gif)","0ab478b2":"# Process insights\n\n* I had to comment out some code, as well as embed many of the plots manually instead of letting the code do it, as otherwise committing this kernel would have taken too long. In addition, I even had to comment some code that would not have taken long (the Sequential test and the LightGBM test) as the kernel ran out of memory during them, crashing the commit. A cleaner solution would have been to split the project into two or three kernels, as described [here](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/114941).","f38e09a3":"**All sites for meter 0**\n\nOnly meter 0 has enough data to warrant a sitewise analysis of the age vs. the meter reading:","89739051":"Before submitting, lets check whether the predictions at least *could* be accurate by comparing the form of one building's prediction to that of its readings during the equivalent timeframe in the previous year. This will give an intuitive sense of its bias and variance. ","a0656ba4":"# Feature engineering","6a7e559d":"Note that performing the same without bagging, i.e., with a standard model for each building, would yield excessive bias, that is, low reguralization; it would be odd if the very same patterns truly repeated from year to year, like here:\n![image.png](attachment:image.png)","f55dc9c0":"### Creating the submission file\n\nI am here using the results from the approach of a separate model for each building. (Meaning that I did not run the whole kernel sequentially; I left out the section just above this one, as otherwise the test_df would contain the predictions from the wrong approach).","6aba0b41":"### Preparing the testing set\n\nPutting the testing set through the same functions as the training set.","52e6f8fc":"### Converting timestamps to datetimes","811d80bf":"### Energy usage distribution","ff2b6d76":"# TODO \n\n* Outlier analysis. A fun little activity would be the finding of faulty meters: https:\/\/www.kaggle.com\/juanmah\/ashrae-outliers\n\n* Try the 24th hour meter reading, which has a big negative correlation with the current hour, as a feature.\n\n* Test how much imputation would improve performance.\n\n* When the time series contains huge constant regions, the models tend to unfortunately learn from it, even though there is no reason why the regions should repeat every year. Remove them and try again!","673c64dc":"**Building metadata**","7b7cfcdb":"### Sinified cyclicals\n\nThis is needed for deep learning, but not for linear regression. [Here](http:\/\/blog.davidkaleko.com\/feature-engineering-cyclical-features.html) is a great explanation of it.","0714cf2c":"### Downloading and merging the training data","64c19491":"Meters 1, 2 and 3 have each one or two building uses with huge variances compared to the rest of the uses. Other than that, I am currently unable to extract any insights from these.","fb799612":"### Predicting with a separate model for each building\n\nIn backwards elimination above, I did not use this method, but the in the next subsection; this method takes such a long time to finish that I have not enough time to conduct the backwards elimination process with this. As such, I shall instead use the set of 5 features chosen in the alternative feature selection approach, namely the correlation analysis.","d3a8bd3d":"![ll](https:\/\/media.giphy.com\/media\/o14YPU6vooy0o\/giphy.gif)\n\nAwful! It turns out that the scaling of the target by log1p before training the model causes this naughty behavior, as it makes the values too small. Why do small values prevent proper regression? It is a mystery, one of the great questions of the universe.  \n\nHere they are with the log1p removed (commented out and replaced with new lines above):\n![image.png](attachment:image.png)","ad44ea81":"(The empty plots are sites that lack ages.) Again, there are no strong relationships; for most of them, newer buildings tend to have higher readings, but most buildings are at about the same level. The only interesting phenomenon is the distribution of site 5: the site could be some special region where many buildings have been intentionally opened on the same years. I fail to imagine what sort of district the latter could be. Most likely the stack are caused by slipshod data collection. ","d5ffc68f":"### Primary use\n\n**vs. energy usage**","83199fc7":"### Building age","6da3b887":"Any insights that would prove useful in the modeling? Perhaps, but lets leave that for later. Unless you really insist.","24e23633":"### Wind direction per hour of day\n\nCopied from https:\/\/www.kaggle.com\/chmaxx\/ashrae-eda-and-visualization-wip#more-to-come...","8c44b9f8":"![image.png](attachment:image.png)","a4021948":"### Weather\n\nLets look at the relationships between weather features and the energy usage. Since many of the weather features are quite collinear, and there are so many that looking at all of their mutual interactions would take too much time and space, I will restrict this analysis to the relationship between the intuitively most important one, temperature and its relationship with intuitively second-most significant one, wind speed. (It is clear that across the four types of energy usage, the wind speed, precipitation etc. do not matter much if the air is at room temperature. Hence the temperature is the supreme weather variable.)","79b1560b":"![ll](https:\/\/media.giphy.com\/media\/H5RSLry1qMrzq\/giphy.gif)","ddb646cf":"Making and trying the model:","c65fd4be":"# Pickling","ac358a9a":"# Exploration","e26ce9d7":"![image.png](attachment:image.png)\n\n![ll](https:\/\/media.giphy.com\/media\/zXMRfbsHOAire\/giphy.gif)\n\nThere are two possible reasons for this:\n1. Linear regression cannot use the categorical feature Building ID to discern which building it is predicting for.\n2. The various scales of the data for the buildings, i.e., some buildings have far smaller ranges of the target variable than other buildings. \n\nThe lesson here is obvious: **never use a single model for a dataset consisting of multiple time series.**","8d1e18cb":"### Shifted series\n\nI expect the temps and the wind speed to have moderate shifted correlation with the meter reading. Others not much.","04b9e71b":"![ll](https:\/\/vignette.wikia.nocookie.net\/metroid\/images\/8\/8a\/Savestation.gif\/revision\/latest\/scale-to-width-down\/180?cb=20190728014411)","cfe329eb":"![image.png](attachment:image.png)\n\n![ll](https:\/\/media.giphy.com\/media\/JTxQbJ5RMgVHi\/giphy.gif)","97d442ba":"**vs. floor count**","c0237108":"# Missing data","ae06633b":"### Meter reading autocorrelation","e5394ed8":"# Reducing memory usage","2e4d811d":"![image.png](attachment:image.png)","92387e71":"![ll](https:\/\/media.giphy.com\/media\/NJZMSqRY3rG9i\/giphy.gif)","3a90f41a":"These yield no insights. Probably because there is so much variation between the time series of the different buildings that their mean values simply tend towards a the same value across the heatmap.","72cbe7a2":"The energy per area is quite independent of building age for all of the meters. However, for meters 0 and 1, there is a lot of noise for buildings newer than about 70 and 50 years respectively.\n\nNote the singular outliers of meters 1 and 3. ","30efe098":"### Defining the model\n\nSince LightGBM cannot be used as a base estimator in BaggingRegressor (as it does not belong to scikit), it has to be bagged manually.\n\nAs I want to maximize regularization, I will use only the validation set as the LightGBM's validation set, not the training set too.","7597a4e1":"![image.png](attachment:image.png)"}}