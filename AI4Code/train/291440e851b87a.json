{"cell_type":{"8fa1f4ce":"code","f3f81744":"code","dfe137c2":"code","750db7b2":"code","ffc6299b":"code","6d146862":"code","45805e8d":"code","af013a61":"code","e705edde":"code","9107bdd3":"code","54e01330":"code","fa98c2f7":"code","67ed56f7":"code","d3c8d2f5":"code","8431432a":"code","6c6513d6":"code","830d146e":"code","3206acf1":"code","df360c8a":"code","3fd4e754":"code","179f3dff":"code","0b700982":"code","ed19c756":"code","ef15f53e":"code","1d64ffd3":"code","1a0fc656":"code","ed125cb8":"code","0104f120":"code","a9040d53":"code","38c10177":"code","e164087b":"code","c8f84717":"code","505cc7dc":"code","2ef92404":"code","aea754dd":"code","ff11fb26":"code","dd369fdc":"code","47c2a667":"code","52050661":"code","b475f404":"code","2f5a5302":"code","1e8267f3":"code","9bf6ad1a":"code","b1e633c6":"code","56ddbc6b":"code","9249e9cb":"code","c2aacb9a":"code","92a22779":"code","7bbc9a63":"code","e6babbcc":"code","61e9e01d":"code","a2f11ab9":"code","25441ba0":"code","13c7403d":"code","27260a5d":"code","021d13a4":"code","7e303340":"code","26567423":"code","c7041c22":"code","30d99be5":"code","81633d1f":"code","da7f636c":"markdown","fe76568b":"markdown","ba9e7a8d":"markdown","80b74ffb":"markdown","a958082c":"markdown","5b870c40":"markdown","eefe3b79":"markdown","72a3a929":"markdown","3308e7cd":"markdown","051b5dc9":"markdown","e61ecfd8":"markdown","f031ee34":"markdown","f6d8584c":"markdown","2cd0ca0a":"markdown","abc1abf7":"markdown"},"source":{"8fa1f4ce":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f3f81744":"import pandas as pd \nimport numpy as np ","dfe137c2":"train_df = pd.read_csv('\/kaggle\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-with-disaster-tweets-cleaning-data\/test_data_cleaning.csv')","750db7b2":"train_df.head()","ffc6299b":"train_df.info()","6d146862":"train_df.nunique()","45805e8d":"test_df.head()","af013a61":"test_df.info()","e705edde":"test_df.nunique()","9107bdd3":"train_df.head()","54e01330":"train_df.drop(columns=['id','keyword','location'], axis=1, inplace=True)","fa98c2f7":"test_df.head()","67ed56f7":"test_df.drop(columns=['id','keyword','location'],axis=1, inplace=True)","d3c8d2f5":"print(train_df.shape, test_df.shape)","8431432a":"from sklearn.model_selection import train_test_split","6c6513d6":"X_train, X_valid, y_train, y_valid = train_test_split(train_df['text'],train_df['target'], test_size=0.2, random_state=111)","830d146e":"print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)","3206acf1":"from tensorflow.keras.preprocessing.text import Tokenizer","df360c8a":"vocab_size = 1000\noov_token = \"<OOV>\"\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)","3fd4e754":"tokenizer.fit_on_texts(X_train)","179f3dff":"X_train = tokenizer.texts_to_sequences(X_train)\nX_valid = tokenizer.texts_to_sequences(X_valid)","0b700982":"for i in range(10):\n    print(len(X_train[i]))","ed19c756":"X_train[0]","ef15f53e":"for i in range(10):\n    print(len(X_valid[i]))","1d64ffd3":"X_valid[0]","1a0fc656":"from tensorflow.keras.preprocessing.sequence import pad_sequences","ed125cb8":"max_length = 120\ntrunc_type = 'post'\npad_type = 'post'","0104f120":"X_train_padded = pad_sequences(X_train, maxlen=max_length, truncating=trunc_type, padding=pad_type)\nX_valid_padded = pad_sequences(X_valid, maxlen=max_length, truncating=trunc_type, padding=pad_type)","a9040d53":"X_train_padded[:2]","38c10177":"X_valid_padded[:2]","e164087b":"print(X_train_padded.shape, X_valid_padded.shape)","c8f84717":"print(type(X_train_padded), type(X_valid_padded))\nprint(type(y_train), type(y_valid))","505cc7dc":"y_train = np.array(y_train)\ny_valid = np.array(y_valid)","2ef92404":"print(type(X_train_padded), type(X_valid_padded))\nprint(type(y_train), type(y_valid))","aea754dd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten","ff11fb26":"embedding_dim = 16\n# vocab_size = 1000\n# max_length = 120","dd369fdc":"model = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_length),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(64, dropout=0.5)),\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","47c2a667":"model.summary()","52050661":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","b475f404":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","2f5a5302":"filepath = 'my_checkpoint.ckpt'\ncp = ModelCheckpoint(\n    filepath=filepath,\n    save_weights_only=True,\n    save_best_only=True,\n    monitor='val_loss',\n    verbose=1\n)","1e8267f3":"ep = EarlyStopping(\n    monitor='val_loss', \n    patience=5,\n)","9bf6ad1a":"epochs=30\nmodel.fit(\n    X_train_padded, y_train,\n    validation_data = (X_valid_padded, y_valid),\n    callbacks=[cp,ep],\n    epochs=epochs\n)","b1e633c6":"model.load_weights(filepath)","56ddbc6b":"model.evaluate(X_valid_padded, y_valid)","9249e9cb":"X_valid[0]","c2aacb9a":"model.save('.\/model\/basic_val_loss_0.4475.h5')","92a22779":"import tensorflow as tf","7bbc9a63":"mymodel = tf.keras.models.load_model('.\/model\/basic_val_loss_0.4475.h5')","e6babbcc":"mymodel.summary()","61e9e01d":"X_test = tokenizer.texts_to_sequences(test_df['text'])","a2f11ab9":"X_test_padded = pad_sequences(X_test, maxlen=max_length, truncating=trunc_type, padding=pad_type)","25441ba0":"y_test_raw = model.predict(X_test_padded)","13c7403d":"y_test_raw","27260a5d":"y_test = list(map(lambda x : 1 if x > 0.5 else 0, y_test_raw))","021d13a4":"set(y_test)","7e303340":"y_test[:5]","26567423":"test_df['predict'] = y_test","c7041c22":"test_df","30d99be5":"test_df[test_df['predict']==1]","81633d1f":"test_df[test_df['predict']==0]","da7f636c":"### 2-a. Drop Columns","fe76568b":"## Step 7. Model Evaluate & Save","ba9e7a8d":"```\nStep 1. Library Import & Data Load\nStep 2. Data Preprocessing\n     2-a. Drop Columns\n     2-b. Tokenizer\n     2-c. Pad Sequences\n     2-d. Match Data type to numpy.ndarray\nStep 3. Modeling\nStep 4. Model Compile\nStep 5. Callbacks\nStep 6. Model Fit\nStep 7. Model Evaluate & Save\nStep 8. Reload Model\nStep 9. Predict Test Data\n```\n","80b74ffb":"## Step 1. Library Import & Data Load","a958082c":"# NLP with Disaster Tweets","5b870c40":"## Step 6. Model Fit","eefe3b79":"## Step 8. Reload Model","72a3a929":"## Step 9. Predict Test Data ","3308e7cd":"## Step 3. Modeling","051b5dc9":"## Step 4. Model Compile","e61ecfd8":"## Step 5. Callbacks","f031ee34":"## Step 2. Data Preprocessing","f6d8584c":"### 2-b. Tokenizer","2cd0ca0a":"### 2-d. Match Data type to numpy.ndarray","abc1abf7":"### 2-c. Pad Sequences"}}