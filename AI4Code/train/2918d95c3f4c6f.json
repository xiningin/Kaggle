{"cell_type":{"e6840680":"code","6320de8f":"code","b1f83597":"code","0b8e3a35":"code","685b401f":"code","b979685c":"code","6994cdc1":"code","ae927be4":"code","43cbb113":"markdown","605fa9d0":"markdown","b89659c7":"markdown","58083a65":"markdown","025f8acc":"markdown","fc2b69c8":"markdown","22f316c9":"markdown"},"source":{"e6840680":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import layers","6320de8f":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \npseudolabels_df = pd.read_csv(\"..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\ntrain_df = train_df[train_df.Cover_Type != 5]\ntrain_df = train_df[train_df.Cover_Type != 4]\ntrain_df = pd.concat([train_df, pseudolabels_df], axis=0)\ntrain_df.reset_index(drop=True)\n\nle = LabelEncoder()\ntarget = le.fit_transform(train_df.Cover_Type)","b1f83597":"features = [feat for feat in test_df.columns if feat != 'Id' and feat != 'Cover_Type']\n\ndata_pipe_transformer = make_pipeline(\n    StandardScaler()\n)\n\npreprocessor = make_column_transformer(\n    (data_pipe_transformer, features)\n)","0b8e3a35":"# if you want to traing on GPU just set it to False\n\nTPU = True","685b401f":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Replicas:', strategy.num_replicas_in_sync)","b979685c":"# I use skipped connection which improve score a little bit\n\ndef my_model(X):\n    il = layers.Input(shape=(X.shape[-1]), name=\"input\")\n    x = layers.Dense(128, activation='selu')(il)\n    x1 = layers.BatchNormalization()(x)\n    x = layers.Dense(64, activation='selu')(x1)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(layers.Concatenate()([x, x1]))\n    x = layers.Dense(units=64, activation='relu')(x) \n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(64, activation='selu')(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(len(le.classes_), activation=\"softmax\", name=\"output\")(x)\n\n    model = tf.keras.Model([il], output)\n    return model","6994cdc1":"EPOCHS = 90 \nVERBOSE = 2 \nRUNS = 1 \nBATCH_SIZE = 1024 \nFOLDS = 10\n\nnp.random.seed(2021)\ntf.random.set_seed(2021)\n\nscore_list, test_pred_list, history_list = [], [], []\noof_list = [np.full((len(train_df), len(le.classes_)), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df.Cover_Type)):\n        print(f\"Fold {run}.{fold}\")\n\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        y_tr = target[train_idx]\n        y_va = target[val_idx]\n        X_tr = X_tr[features]\n        X_va = X_va[features]\n        \n        X_tr = preprocessor.fit_transform(X_tr)\n        X_va = preprocessor.transform(X_va)\n        \n        # TPU model\n        if TPU:\n            with strategy.scope():\n                model = my_model(X_tr)\n                model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n        else:\n            \n            # GPU model\n            model = my_model(X_tr)\n            model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n  \n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_acc\", patience=10, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n\n  \n        history = model.fit(X_tr, y_tr, \n                            validation_data=(X_va, y_va), \n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE, \n                            validation_batch_size=len(X_va),\n                            shuffle=True,\n                            callbacks=[lr, es])\n        history_list.append(history.history)\n           \n        y_va_pred = model.predict(X_va, batch_size=len(X_va))\n        oof_list[run][val_idx] = y_va_pred\n        y_va_pred = le.inverse_transform(np.argmax(y_va_pred, axis=1))\n\n        accuracy = accuracy_score(train_df.iloc[val_idx].Cover_Type, y_va_pred)\n\n        print(f\"Fold {run}.{fold} | Epochs: {len(history_list[-1]['loss'])} | Accuracy: {accuracy:.5f}\")\n        \n        test_pred_list.append(model.predict(preprocessor.transform(test_df[features]), batch_size=BATCH_SIZE))\n        \n        del model, y_va_pred\n        gc.collect()","ae927be4":"sub = test_df[['Id']].copy()\nsub['Cover_Type'] = le.inverse_transform(np.argmax(sum(test_pred_list), axis=1)) \nsub.to_csv('tps12-pseudeo-submission.csv', index=False)","43cbb113":"# 4. TRAINING","605fa9d0":"## TPS-12 Tensorflow NN (model traning on GPU - to speed up learning process) and pseudolabeling\n\nSpecial thanks to [AMBROSM](https:\/\/www.kaggle.com\/ambrosm) for hist great notebook [TPSDEC21-01-Keras Quickstart](https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart) which was inspiration for this one.\n\nThis notebooks contains some noew ideas and improvements:\n- pseudolabeling - made it separately - you can find it and use for own trainings in my dataset [TPS-12 Pseudolabels](https:\/\/www.kaggle.com\/remekkinas\/tps12-pseudolabels) - please vote on my database (it will be updated)\n- TPU training (to speed up process)\n- Tensorflow API isntead of Keras Sequential","b89659c7":"# 3. MODEL CONFIGURATION","58083a65":"# 1. DATA PREPARATION\n\n## 1.1 LOAD DATA AND PREPARE\n","025f8acc":"# 5. SUBMISSION","fc2b69c8":"# 2. TPU CONFIGURATION","22f316c9":"## 1.2 DEFINE DATA PIPELINE PREPROCESSOR"}}