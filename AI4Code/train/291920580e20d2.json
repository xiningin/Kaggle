{"cell_type":{"95179ca7":"code","b0de8295":"code","19a3bf64":"code","34fdccf1":"code","fdd70a65":"code","34297e4c":"code","4debc0df":"code","a5b58a7a":"code","32fe5797":"code","6f04b278":"code","66a7755f":"code","005339d5":"code","498ef6f8":"code","23eaee27":"code","6d3204ff":"code","3dd18530":"code","eeb0cf3d":"code","43f181b8":"code","b441b885":"code","53f8f5ce":"code","32cac027":"code","16914a84":"code","0ec0b3b8":"code","4b230a37":"code","964a219d":"code","b3f85fd9":"code","b9685d32":"code","95c644a5":"code","77da4c95":"code","28be1d06":"code","e99866b8":"markdown","c4526ed4":"markdown","29529c52":"markdown","9beae6ff":"markdown","a9b2071c":"markdown","f58555e2":"markdown","b51d80aa":"markdown","3b20496c":"markdown","0ed45bdc":"markdown","aaf2c4f8":"markdown","0084d63a":"markdown","a84a1cde":"markdown"},"source":{"95179ca7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b0de8295":"import matplotlib.pyplot as py\nimport seaborn as sns\nfrom nltk.corpus import stopwords as stop\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.feature_extraction.text import TfidfVectorizer","19a3bf64":"df = pd.read_json(\"\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json\", lines=True)","34fdccf1":"df.head()","fdd70a65":"df.describe()","34297e4c":"df.info()","4debc0df":"df = df[df['date']>pd.Timestamp(2018,1,1)]","a5b58a7a":"df.shape","32fe5797":"df = df[df['headline'].apply(lambda x:len(x.split())>5)]\ndf.shape","6f04b278":"df.sort_values('headline', inplace=True, ascending=False)\nprint(df.shape)\ndf_duplicated=df.duplicated('headline',keep=False)\nprint(df.shape)\ndf = df[~df_duplicated]\nprint(df.shape)","66a7755f":"# lets check if any of the cell is empty or has an ambigiious value\ndf.isna().sum()","005339d5":"df.describe()","498ef6f8":"index = df['category'].value_counts().index\nindex.shape","23eaee27":"values = df['category'].value_counts().values\ntype(values)","6d3204ff":"py.bar(df['category'].value_counts().index, df['category'].value_counts(),width=0.8)\npy.show()","3dd18530":"news_per_month= df.resample('M', on= \"date\")['headline'].count()\nnews_per_month","eeb0cf3d":"py.figure()\npy.title('Month wise distribution')\npy.xlabel('Month')\npy.ylabel('Number of articles')\npy.bar(news_per_month.index.strftime('%b'), news_per_month, width=0.8)","43f181b8":"sns.distplot(df['headline'].str.len(), hist=False)","b441b885":"df['day and month']= df['date'].dt.strftime(\"%a\")+'_'+df['date'].dt.strftime('%b')","53f8f5ce":"df.index= range(df.shape[0])\ndf_temp=df.copy()","32cac027":"stop_words = stop.words('english')","16914a84":"for i  in range(len(df_temp[\"headline\"])):\n    string=\"\"\n    for word in df_temp[\"headline\"][i].split():\n        word = (\"\".join(e for e in word if e.isalpha()))\n        word = word.lower()\n        if not word in stop_words:\n            string += word +\" \" \n    if i%500 == 0:\n        print(i)\n    df_temp.at[i,'headline']= string.strip()","0ec0b3b8":"from nltk.stem import WordNetLemmatizer\nlemitizer = WordNetLemmatizer()","4b230a37":"for i in range(len(df_temp['headline'])):\n    string=\"\"\n    for w in df_temp['headline'][i]:\n        string += lemitizer.lemmatize(w,pos='v')+\" \"\n    print(string)\n    df_temp.at[i,'headline'] += string.strip()","964a219d":"df_temp['headline'][0]","b3f85fd9":"vectorize = CountVectorizer()\nvectorize_features = vectorize.fit_transform(df_temp['headline'])\nvectorize_features.shape","b9685d32":"pd.set_option('display.max_colwidth', -1)\n\n# to get the biggest possible headline to display","95c644a5":"def bag_of_words_model(row_index, output_values):\n    # to find the distance of the featutres of row_index corresponding to all the other rows\n    couple_dist =  pairwise_distances(vectorize_features, vectorize_features[row_index])\n    indices = np.argsort(couple_dist.ravel())[0:output_values]\n    df1 = pd.DataFrame(\n        {'publish_date':df['date'][indices].values, \n         'headline':df['headline'][indices].values,\n         'euclidean distance':couple_dist[indices].ravel()\n        }\n    )\n    print(\"headline: \",df['headline'][indices[0]])\n    return (df1.iloc[1:,])\n\nbag_of_words_model(133, 11)","77da4c95":"vectorizer = TfidfVectorizer(min_df=0)\ntfidf_headline_features = vectorize.fit_transform(df_temp['headline'])","28be1d06":"def tfidf_based_model(row_index, num_similar_items):\n    couple_dist = pairwise_distances(tfidf_headline_features,tfidf_headline_features[row_index])\n    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n    df1 = pd.DataFrame({'publish_date': df['date'][indices].values,\n               'headline':df['headline'][indices].values,\n                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',df['headline'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    \n    #return df.iloc[1:,1]\n    return df1.iloc[1:,]\n\ntfidf_based_model(133, 11)","e99866b8":"Hence, we see that no cell is empty","c4526ed4":"From the above, we were able to see that we are really not getting much accuracy, as ","29529c52":"# Preprocesing starts","9beae6ff":"# Training","a9b2071c":"in the above code we will remove all short headlines, as in future when we will remove stop words, they may become null\nIn the above, we were able to see that we had 53 such headlines, so we removed all that data","f58555e2":"In the above code, we first sorted the data in descending and then exactracted the duplicate data. And at the end, removed the duplicates by using '~'","b51d80aa":"So after doing pre-processing, we found out that we are left with 26 categories, 8453 headlines and 865 authors","3b20496c":"## Bag of Words Algo","0ed45bdc":"# NLTK","aaf2c4f8":"So from the above command, we got the know that there are a total of 200853 records,and all the majoz news are about Politics having the headline \"Sunday Roundup\". There are in total 41 categories, 199344 headlines and 27993 authors.","0084d63a":"# Importing Libraries and setting up","a84a1cde":"## TFIDF Algorithm"}}