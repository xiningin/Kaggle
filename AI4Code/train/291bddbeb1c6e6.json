{"cell_type":{"e9383ef1":"code","7b64f4d3":"code","dec1ef6e":"code","33696e91":"code","3b0abc7f":"code","2a623d60":"code","5311145e":"code","1bdef6c1":"code","b2364b17":"code","a7922114":"code","f93ef053":"code","8b5d72a8":"code","35f0ace6":"code","cb1c25fa":"code","3e53ce3c":"code","18654a17":"code","0ae3f57a":"code","ea377c12":"code","f63bc0c1":"code","13c2b62d":"code","e666efa1":"code","053d5431":"code","f7f12bbb":"code","abde366a":"code","264b0acb":"code","05af9fe6":"code","6b542644":"code","631bb791":"code","60078d03":"markdown","4eee6310":"markdown","bd2de446":"markdown","6d6bf710":"markdown","6cebdf2a":"markdown","347cd276":"markdown","45e73c51":"markdown","0f45dfea":"markdown","067c0582":"markdown","d0b88c12":"markdown","8af83fa3":"markdown","48b19797":"markdown"},"source":{"e9383ef1":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt \n\npath_input='..\/input\/gender-classification-dataset\/'\npath_output='.\/'","7b64f4d3":"data=pd.read_csv(path_input+'gender_classification_v7.csv')\ndata.head()","dec1ef6e":"data.info()","33696e91":"mask = np.zeros_like(data.corr())\nmask[np.tril_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = data.corr()\nsns.heatmap(corr, vmax=1, square=True,annot=True,cmap='viridis', mask=mask.T)\n\nplt.title('Correlation between different fearures')","3b0abc7f":"sns.lmplot(x=\"forehead_width_cm\", y=\"forehead_height_cm\", hue=\"gender\", data=data, palette = 'inferno_r', height = 7)","2a623d60":"sns.lmplot(x=\"nose_wide\", y=\"nose_long\", hue=\"gender\", data=data, palette = 'inferno_r', height = 7)","5311145e":"sns.lmplot(x=\"lips_thin\", y=\"distance_nose_to_lip_long\", hue=\"gender\", data=data, palette = 'inferno_r', height = 7)","1bdef6c1":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, classification_report,confusion_matrix","b2364b17":"Y = data.gender\nX = data.drop(['gender'], axis = 1)\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state =np.random.RandomState(0))","a7922114":"from sklearn.model_selection import GridSearchCV\ndef tune_model(model,params):\n    modelCV=GridSearchCV(estimator=model,\n                     param_grid=params,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=-1,\n                     verbose=1)\n    modelCV.fit(x_train,y_train)\n    print(\"best parameters : \\n{}\\n\".format(modelCV.best_params_))\n    print(\"accuracy : \\n{}\\n\".format(modelCV.best_score_))","f93ef053":"reglog_params={\"C\":np.logspace(-3,3,7),\n      \"penalty\":['l1', 'l2', 'elasticnet', 'none']}\n\nreglog=LogisticRegression()","8b5d72a8":"tune_model(reglog,reglog_params)","35f0ace6":"reglog=LogisticRegression(C=0.1,penalty='l2')\nreglog.fit(x_train,y_train)","cb1c25fa":"def plot_results(labels,preds,model_name,plot_type='all'):\n    def plot_confusion_matrix(labels, preds,model_name):\n        plt.figure(1, figsize= (10, 10))\n        plt.title(\"Confusion matrix for \"+model_name)\n        mat = confusion_matrix(labels, preds)\n        sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n        plt.xlabel('true label')\n        plt.ylabel('predicted label')\n        plt.show()\n        plt.close()\n\n    def plot_classif_report(labels,preds,model_name):\n        clf=classification_report(labels, preds,output_dict=True)\n        plt.figure(1, figsize= (10,10))\n        ax = plt.axes()\n        sns.heatmap(pd.DataFrame(clf).iloc[:-1, :].T,annot=True)#annot=True\n        ax.set_title(\"Classification report for \"+model_name)\n        plt.show()\n        plt.close()\n    if 'confusion' in plot_type:\n        plot_confusion_matrix(labels,preds,model_name)\n    elif 'report' in plot_type:\n        plot_classif_report(labels,preds,model_name)\n    elif 'all' in plot_type:\n        plot_confusion_matrix(labels,preds,model_name)\n        plot_classif_report(labels,preds,model_name)","3e53ce3c":"pred=reglog.predict(x_test)\nplot_results(y_test,pred,'LogisticRegression',plot_type='all')","18654a17":"from sklearn.svm import SVC","0ae3f57a":"svc=SVC()\nparams_svc={'kernel': ['rbf','linear'],\n             'gamma': [1e-3, 1e-4, 1e-5],\n                     'C': [1, 10, 100, 1000]}\n                                \ntune_model(svc,params_svc)","ea377c12":"svc=SVC(kernel='rbf',gamma=0.001,C=10)\nsvc.fit(x_train,y_train)\npred=svc.predict(x_test)\nplot_results(y_test,pred,'Support Vector Classifier',plot_type='all')","f63bc0c1":"from sklearn.neighbors import KNeighborsClassifier","13c2b62d":"knn=KNeighborsClassifier()\nparams_knn={'n_neighbors': [k for k in range(1,20)]}\n                                \ntune_model(knn,params_knn)","e666efa1":"knn=KNeighborsClassifier(n_neighbors=15)\nknn.fit(x_train,y_train)\npred=knn.predict(x_test)\nplot_results(y_test,pred,'K Nearest Neighbours',plot_type='all')","053d5431":"from sklearn.ensemble import RandomForestClassifier","f7f12bbb":"rf=RandomForestClassifier()\nparams_rf={'n_estimators': [k for k in range(50,1000,200)],\n          'max_depth' : [None, 2, 4, 8],\n          'max_features':['auto', 'sqrt']}\ntune_model(rf,params_rf)","abde366a":"rf=RandomForestClassifier(max_depth=4, max_features='auto',n_estimators=450)\nrf.fit(x_train,y_train)\npred=rf.predict(x_test)\nplot_results(y_test,pred,'Random Forest Classifier',plot_type='all')","264b0acb":"import xgboost as xgb","05af9fe6":"boost=xgb.XGBClassifier()\nparams_boost={\n    'objective': ['binary:logistic'],\n    'eval_metric':['error'],\n    'nthread':[-1],\n    'min_child_weight':(3, 20),\n      'gamma':(0, 5),\n      'subsample':(0.7, 1),\n      'colsample_bytree':(0.1, 1),\n      'max_depth': (3, 10),\n      'learning_rate': (0.01, 0.5)\n       }\ntune_model(boost,params_boost)","6b542644":"boost = xgb.XGBClassifier(colsample_bytree=0.1,\n                          eval_metric='error',\n                          learning_rate=0.5,\n                          min_child_weight=3,\n                          nthread=-1,\n                          gamma=5,\n                          subsample=1,\n                          max_depth=3,\n                         )\nboost.fit(x_train,y_train)\npred=boost.predict(x_test)\nplot_results(y_test,pred,'XGBOOST',plot_type='all')","631bb791":"from sklearn.ensemble import StackingClassifier\n\nestimators = [\n        ('log', reglog),\n        ('svm', svc),\n        ('knn',knn), \n       ('rf',rf),\n       ('xgb',boost)]\n\nstack = StackingClassifier(\n    estimators=estimators,\n    final_estimator=xgb.XGBClassifier(colsample_bytree=0.1,\n                          eval_metric='error',\n                          learning_rate=0.5,\n                          min_child_weight=3,\n                          nthread=-1,\n                          gamma=5,\n                          subsample=1,\n                          max_depth=3,),\n    n_jobs=-1,\n    cv=10)\n\nstack.fit(x_train,y_train)\npred=stack.predict(x_test)\nplot_results(y_test,pred,'Stacking Classifier',plot_type='all')","60078d03":"# Random Forest Classifier ","4eee6310":"## XGBOOST Classifier","bd2de446":"# Modeling","6d6bf710":"## K Nearest Neighbours","6cebdf2a":"## Support Vector Machine","347cd276":"### Model selection\n\nHere we will tune parameters of a few models. For each, we will use GridSearchCV with cross-validations (CV). Here is the function to tune a model returning best parameters and score","45e73c51":"Best classifier : XGBOOST with 97.7% accuracy","0f45dfea":"After tuning parameters we update the model","067c0582":"## Logistic Regression","d0b88c12":"# Data Exploration","8af83fa3":"# Gender Classification","48b19797":"# Classifier Stacking"}}