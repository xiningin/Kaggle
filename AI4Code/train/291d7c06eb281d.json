{"cell_type":{"04f0b921":"code","5073d81d":"code","cbf8b5a3":"code","d2eff823":"code","d0ac5e83":"code","d4cb44d1":"code","c51f9c02":"code","4c698042":"code","424c41a7":"code","8a074b75":"code","cc78a595":"code","2cfaadba":"code","82a573e4":"code","6d3ab983":"code","7f0a55fa":"code","53be86db":"code","0845a03b":"code","17f5e92f":"code","845ba9bd":"code","a998d2e7":"code","517e528d":"code","9e6163b9":"code","e84272a2":"code","177f000e":"code","114a3169":"code","5c898d54":"code","2d7cb1ed":"code","62a62cb5":"code","c713ffc6":"code","bffba9df":"code","090fe3be":"code","c7236f5e":"code","c50f95dd":"code","46d8f126":"code","94b41a67":"code","92c632f3":"code","8f91b6ac":"code","802a2750":"code","49962d4d":"code","9720b940":"code","ef7d92af":"code","3b9f2827":"code","83583aa8":"code","d39824d4":"code","1b835b56":"code","af3f14c7":"code","7b452bad":"code","e52584a4":"code","3009ae84":"code","38d92037":"code","0e16bd7a":"code","db8fe22d":"code","9f7bb136":"code","00819b93":"code","922fd590":"code","631bcc63":"code","b60ecd01":"code","e5de513c":"code","6324f761":"code","3978b50a":"code","b63a95f2":"code","b78a3a49":"code","6e472973":"code","7ceacff0":"code","e692ef53":"code","b775f982":"code","80baf4c6":"code","68f0660e":"code","98230579":"code","8994c65e":"code","d235b34b":"code","a6c9ce57":"markdown","b9c8f369":"markdown","5844f4bd":"markdown","5f2a6982":"markdown","f30b3ca1":"markdown","fc2596fd":"markdown","bb9b67a8":"markdown","a9e01fce":"markdown","c9ca4910":"markdown","bb13174f":"markdown","6e8ca4a0":"markdown","b72f35d6":"markdown","5c008f85":"markdown","b68396ce":"markdown","0cde71c5":"markdown","7427f95f":"markdown","e64033a2":"markdown","e36a9ee1":"markdown","a8925869":"markdown","b8cf705b":"markdown","ad2c98bb":"markdown","9caa23cf":"markdown","329d9a37":"markdown","c1ddd108":"markdown","6a5a6f30":"markdown","4b4947d2":"markdown","b11fd38c":"markdown","31786f6a":"markdown","898e6f8a":"markdown","8a9e8318":"markdown"},"source":{"04f0b921":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport random\n# for reproducibility , to get the same results when evry your run\nnp.random.seed(2021) \n\nimport re\nimport string\nfrom collections import Counter\n\nimport scipy.spatial\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.linear_model import SGDClassifier, LinearRegression\nfrom sklearn.svm import LinearSVC\nimport xgboost as xgb\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\n\n\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nimport sys\nimport os\nimport glob\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","5073d81d":"PATH = '..\/input\/good-reads-quotes'","cbf8b5a3":"quotes_multi_label_data = pd.read_csv(PATH + '\/popular_quotes.csv')\nquotes_multi_label_data.head()","d2eff823":"def plot_freq(data, st):\n    '''\n        take the data, and st refeere to kind of sentiment\n    '''\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data= data , x= 'counts', y= 'word')\n    plt.title(f'Top 20 words in {st} quotes')\n    plt.show();","d0ac5e83":"stop_words = stopwords.words('english')\nstemmer    = nltk.SnowballStemmer(\"english\")","d4cb44d1":"def clean_text(text):\n    '''\n        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        and remove words containing numbers.\n    '''\n    # text = re.findall('\u201c([^\"]*)\u201d', text)[0] # extract text for quotations\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","c51f9c02":"def preprocess_data(text):\n    # text = remove_quotations(text)                                            # extract text for quotations\n    text = clean_text(text)                                                     # Clean puntuation, urls, and so on\n    text = ' '.join(word for word in text.split() if word not in stop_words)    # Remove stopwords\n    text = ' '.join(stemmer.stem(word) for word in text.split())                # Stemm all the words in the sentence\n    return text","4c698042":"print(quotes_multi_label_data['tags'][0])\nprint(type(quotes_multi_label_data['tags'][0]))\nprint(quotes_multi_label_data['tags'][0][0]) # print char instate of str as a tag word","424c41a7":"# make a preprocessing pipeline \nquotes_multi_label_data['clean_text'] = quotes_multi_label_data['quotes'].apply(preprocess_data)\n\n# remove ' and , from the string and [] and spliting the tags\nquotes_multi_label_data['tags'] = quotes_multi_label_data['tags'].apply(lambda tags: tags.replace(\"'\",\"\").replace(\",\",\"\")[1:-1].split())\n\n# get the len of number of tags in each quote\nquotes_multi_label_data['n_tags'] = quotes_multi_label_data['tags'].apply(lambda tags: len(tags))\nquotes_multi_label_data.head()","8a074b75":"print(quotes_multi_label_data['tags'][0])\nprint(type(quotes_multi_label_data['tags'][0]))\nprint(quotes_multi_label_data['tags'][0][0]) # print char instate of str as a tag word","cc78a595":"sns.histplot(data= quotes_multi_label_data, x= 'n_tags');","2cfaadba":"tag_list = [word for sent in quotes_multi_label_data['tags'] for word in sent]","82a573e4":"len(tag_list)","6d3ab983":"freq_tags = Counter(tag_list)\nfreq_tags_sorted = sorted(freq_tags.items(), key=lambda pair: pair[1], reverse=True)\nfreq_tags_df = pd.DataFrame(freq_tags_sorted, columns=['word', 'counts'])\nfreq_tags_df[:20].T","7f0a55fa":"plot_freq(freq_tags_df[:20], 'Most_tags')","53be86db":"all_original_tags = list(set(tag_list))\n\n# this ->\" \".join(tag for tag in most_20_tags) .. to make it like a text for working well, and split them againe\n# all_cleaned_tags = preprocess_data(\" \".join(tag for tag in all_original_tags)).split()\nall_cleaned_tags = [preprocess_data(tag) for tag in all_original_tags]\n\n# {'cleaned_tag\": 'original_tag'}\ntags_dict = dict([(preprocess_data(tag), tag) for tag in all_original_tags])\n","0845a03b":"def simple_multi_label(text):\n    text = preprocess_data(text)\n    labels_ = []\n    for word in text.split():\n        if word in all_cleaned_tags:\n            labels_.append(tags_dict[word])\n    return labels_","17f5e92f":"quote = \"I love reading books and also playing football!\"\nprint(f'The predicted labels: {simple_multi_label(quote)}')","845ba9bd":"quote = \"A friend is someone who knows all about you and still loves you.\"\nprint(f'The predicted labels: {simple_multi_label(quote)}')","a998d2e7":"from fuzzywuzzy import fuzz","517e528d":"q1 = quotes_multi_label_data['quotes'][7]\nt1 = quotes_multi_label_data['tags'][7]\nprint(q1)\nprint(t1)","9e6163b9":"top_20_tag = list(freq_tags_df['word'][:20])\ntop_20_tag","e84272a2":"# we just need at most top 10 tags\ntop_10_tag = top_20_tag[:10]\ntop_10_tag","177f000e":"tags_scores = []\nsim_tags = []\nfor tag in top_10_tag:\n    score = fuzz.ratio(q1,tag)\n#     print(f'The similarity of {tag} tag is: {score}') # compares the entire string similarity, in order\n    tags_scores.append((score, tag))\n    \ntags_scores.sort(reverse = True)\n# print(tags_scores)\nfor item in tags_scores:\n    sim_tags.append(item[1])\nprint(sim_tags[:5])","114a3169":"tags_scores = []\nsim_tags = []\nfor tag in top_10_tag:\n    score = fuzz.partial_ratio(q1,tag)\n#     print(f'The similarity of {tag} tag is: {score}') # compares the entire string similarity, in order\n    tags_scores.append((score, tag))\n    \ntags_scores.sort(reverse = True)\n# print(tags_scores)\nfor item in tags_scores:\n    sim_tags.append(item[1])\nprint(sim_tags[:5])","5c898d54":"tags_scores = []\nsim_tags = []\nfor tag in top_10_tag:\n    score = fuzz.token_sort_ratio(q1,tag)\n#     print(f'The similarity of {tag} tag is: {score}') # compares the entire string similarity, in order\n    tags_scores.append((score, tag))\n    \ntags_scores.sort(reverse = True)\n# print(tags_scores)\nfor item in tags_scores:\n    sim_tags.append(item[1])\nprint(sim_tags[:5])","2d7cb1ed":"def fuzzywuzzy_sim(q):\n    tags_scores = []\n    sim_tags = []\n    for tag in top_10_tag:\n        score = fuzz.partial_ratio(q,tag)\n        tags_scores.append((score, tag))\n\n    tags_scores.sort(reverse = True)\n    for item in tags_scores:\n        sim_tags.append(item[1])\n    return sim_tags[:5]","62a62cb5":"q = \"If you don't belong, don't be long!\"\nprint(f'The predicted labels: {fuzzywuzzy_sim(q)}')","c713ffc6":"q = \"We always needs the close people who says you are on the road, just keep going and don\u2019t turn around.\"\nprint(f'The predicted labels: {fuzzywuzzy_sim(q)}')","bffba9df":"q = \"Even superman sometimes needed to superwoman soul!\"\nprint(f'The predicted labels: {fuzzywuzzy_sim(q)}')","090fe3be":"'inspirational' in top_20_tag, 'inspiration' in top_20_tag, 'inspirational-quotes' in top_20_tag","c7236f5e":"top_20_tag.remove('inspiration'), top_20_tag.remove('inspirational-quotes')","c50f95dd":"'inspirational' in top_20_tag, 'inspiration' in top_20_tag, 'inspirational-quotes' in top_20_tag","46d8f126":"def customize_tags(tags):\n    \n    '''\n        Here we will make a custome list of tags , if the original list of tags <= 5 take all original tags\n        else expand it to be of len 5\n    '''   \n    tmp_list = [] \n    if len(tags) <= 5:\n        return tags\n    tmp_list = list(set(tags) & set(top_10_tag)) \n    tmp_list += top_10_tag[:2]\n    tmp_list = list(set(tmp_list))\n    return tmp_list\n\n\ndef make_top_5_tags(li):\n    '''\n        Make a random 5 tags from top 10 tags, to make it easy for modeling.\n        Just take random 5 tags from the top 10 tags, and most of of top 10 tags in each row.\n    '''\n    return random.sample(top_10_tag, 5)\n                \n","94b41a67":"quotes_multi_label_data['customize_tags'] = quotes_multi_label_data['tags'].apply(customize_tags)\nquotes_multi_label_data['customize_top_5_tags'] = quotes_multi_label_data['tags'].apply(lambda tags: random.sample(top_10_tag,5))\nquotes_multi_label_data.head(10)\n","92c632f3":"# test the function\nquotes_multi_label_data['tags'][1],quotes_multi_label_data['customize_tags'][1],quotes_multi_label_data['customize_top_5_tags'][1]","8f91b6ac":"# test the function\nquotes_multi_label_data['tags'][3], quotes_multi_label_data['customize_tags'][3],quotes_multi_label_data['customize_top_5_tags'][3]","802a2750":"multi_label = MultiLabelBinarizer()\ny = multi_label.fit_transform(quotes_multi_label_data['customize_top_5_tags'])","49962d4d":"multi_label.classes_","9720b940":"pd.DataFrame(y, columns= multi_label.classes_)","ef7d92af":"X_train, X_test, y_train, y_test = train_test_split(quotes_multi_label_data['clean_text'], y, test_size= .2, random_state= 2021)\nX_train.shape, X_test.shape, y_train.shape,  y_test.shape","3b9f2827":"tf_idf = TfidfVectorizer(analyzer= 'word', max_features= 10000, ngram_range= (1, 3))\nX_train = tf_idf.fit_transform(X_train)\nX_test = tf_idf.transform(X_test)\nX_train.shape, X_test.shape","83583aa8":"sgd = SGDClassifier()\nlr = LinearRegression()\nsvc = LinearSVC()","d39824d4":"def jaccard_score(y_true, y_pred):\n    jaccard = np.minimum(y_true, y_pred).sum(axis= 1) \/ np.maximum(y_true, y_pred).sum(axis= 1)\n    return jaccard.mean() * 100\n\ndef print_score(y_pred, clf):\n    print(\"clf:\", clf.__class__.__name__)\n    print(\"Jaccard Score:\", round(jaccard_score(y_test, y_pred),3))\n    print('-'*20)","1b835b56":"for classifier in [sgd, lr, svc]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print_score(y_pred, classifier)","af3f14c7":"def ml_predicted_tags(sent):\n    lebels = []\n    xt = tf_idf.transform([sent])\n    labels = list(multi_label.inverse_transform(clf.predict(xt))[0])\n    return labels","7b452bad":"q = \"I'm happy when i am reading the holy Quran\"\nprint(f'The predicted labels for ml_predicted_tags : {ml_predicted_tags(q)}')\nprint(f'The predicted labels for simple_multi_label : {simple_multi_label(q)}')\nprint(f'The predicted labels for fuzzywuzzy_sim : {fuzzywuzzy_sim(q)}')","e52584a4":"# sgd = SGDClassifier()\n# clf = OneVsRestClassifier(sgd)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n\n\nsgd = SGDClassifier()\nclf = OneVsRestClassifier(sgd)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n","3009ae84":"def ml_predicted_tags(q):\n    tags = []\n    x = tf_idf.transform([q])\n    tags = list(multi_label.inverse_transform(clf.predict(x))[0]) \n    return tags","38d92037":"q = \"i'm happy because i'm reading the hole Quran\"\ncleaned_q = preprocess_data(q)\nprint(ml_predicted_tags(cleaned_q))","0e16bd7a":"def some_pred_funcs(q):\n    l1 = ml_predicted_tags(q)\n    l2 = simple_multi_label(q)\n    l3 = fuzzywuzzy_sim(q)\n    l4 = list(set(l1+l2+l3))\n    return l4","db8fe22d":"q = \"If you don't belong, don't be long!\"\nsome_pred_funcs(q)","9f7bb136":"import tensorflow as tf\nimport torch\nimport torchvision\nfrom transformers import AutoTokenizer, AutoConfig\n\nfrom transformers import TFRobertaModel, RobertaConfig, RobertaTokenizerFast\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\n\n","00819b93":"# Name of the Roberta model to use\nmodel_name = 'roberta-base'\n\n# Load transformers config and set output_hidden_states to False\nconfig = RobertaConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load Roberta tokenizer\ntokenizer = RobertaTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Transformers BERT model\ntransformer_model = TFRobertaModel.from_pretrained(model_name, config = config)","922fd590":"X_train, X_test, y_train, y_test = train_test_split(quotes_multi_label_data['quotes'], y, test_size= .2, random_state= 2021)\nX_train.shape, X_test.shape, y_train.shape,  y_test.shape","631bcc63":"# This is work well\nall_quotes = X_train.values\n\n# Encode our concatenated data\nencoded_qoutes = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_quotes]\n\n# Find the maximum length\nmax_length_all_quotes_training = max([len(sent) for sent in encoded_qoutes])\n\nprint('Max length in traning: ', max_length_all_quotes_training)","b60ecd01":"# we choosen 38 because is the max lenght in evaluation\nmax_length_all_quotes_training = 30\nprint('Max length in traning: ', max_length_all_quotes_training)\n\n# Load the MainLayer\nroberta = transformer_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length_all_quotes_training,), name='input_ids', dtype='int32')\nattention_mask = Input(shape=(max_length_all_quotes_training,), name='attention_mask', dtype='int32') \n\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers Roberta model as a layer in a Keras model\nroberta_model = roberta(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(roberta_model, training=False)\n\n# Then build your model output\nlabels = Dense(units=10, activation='sigmoid', kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='issue')(pooled_output)\n# outputs = Dense(3, activation='softmax')(labels)\noutputs = {'label': labels}\n\n# And combine it all in a model object\n\nmodel = Model(inputs=inputs, outputs=outputs, name='Roberta_MultiLabel_MultiClass')\n\n# Take a look at the model\nmodel.summary()","e5de513c":"top_10_tag\nquotes_multi_label_data.head()","6324f761":"import math\nvalid_split = 0.2\nbatch_size_no = 32\nepochs_no = 16\ninitial_lr = 5e-05\nstep_no = math.ceil(len(quotes_multi_label_data)\/32)","3978b50a":"# Set an optimizer\noptimizer = Adam(\n    learning_rate=initial_lr,\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'label': BinaryCrossentropy(from_logits = True)}\n# loss = {'label': CategoricalCrossentropy(from_logits = False)} # when i using it = False, get low accuracy!!\nmetric = {'label': BinaryAccuracy('accuracy')}\n\n# Compile the model\nmodel.compile(optimizer = optimizer, loss = loss, metrics = metric)\n\n# Ready output data for the model\n# y_labels = to_categorical(y_train)\ny_labels = y_train\n\n\n# Tokenize the input (takes some time)\nx = tokenizer(\n#     text=quotes_multi_label_data.clean_text.tolist(),\n    text=X_train.tolist(),\n    add_special_tokens=True,\n    max_length=max_length_all_quotes_training,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\n\ncheckpoint_filepath = 'roberta_keras_weights.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","b63a95f2":"%%time\n# Fit the model\nhistory = model.fit(\n    x={'input_ids': x['input_ids']},\n    y={'label': y_labels},\n    validation_split=valid_split,\n    batch_size=batch_size_no,\n    epochs=epochs_no,\n    callbacks=[model_checkpoint_callback]\n    )\n","b78a3a49":"test_y_labels = y_test\ntest_y_labels.shape","6e472973":"# Encode our concatenated data\nencoded_test_quotes = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_test]\nmax_length_text_tweet_eval = max([len(sent) for sent in encoded_test_quotes]) \n\nmax_length_text_quotes_eval = 30\n\n# Ready test data\ntest_y_labels = y_test\n\ntest_x = tokenizer(\n    text=X_test.tolist(),\n    add_special_tokens=True,\n    max_length = max_length_text_quotes_eval,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\n\n# Run evaluation\nmodel_eval = model.evaluate(\n    x={'input_ids': test_x['input_ids']},\n    y={'label': test_y_labels}\n)","7ceacff0":"input_pred = tokenizer(\n    text=X_test.tolist(),\n    add_special_tokens=True,\n    max_length=max_length_text_quotes_eval,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\nprint(\"input_pred['input_ids'].shape:\",input_pred['input_ids'].shape)\n\ny_pred = model.predict(input_pred['input_ids'])","e692ef53":"print(y_pred['label'].shape)\ny_pred = np.array([np.argmax(item) for item in y_pred['label']])\nprint(len(y_pred))","b775f982":"tokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(quotes_multi_label_data.quotes.values)","80baf4c6":"sent = \"Be yourserlf,everyone else is already taken.\"\ntxt = clean_text(sent)\n# print(txt)\ntxt = tokenizer.texts_to_sequences([txt])\nprint(txt)\ntxt = pad_sequences(txt, maxlen=max_length_text_quotes_eval, padding='post' , dtype='int32')\nprint(txt)\n# get probability for each class \nl = model.predict(txt,batch_size=1)['label'][0].tolist()\nprint(top_10_tag)\nprint(l)","68f0660e":"def dl_predicted_tags(q):\n    txt = clean_text(sent)\n    txt = tokenizer.texts_to_sequences([txt])\n    txt = pad_sequences(txt, maxlen=max_length_text_quotes_eval, padding='post' , dtype='int32')\n    # get probability for each class \n    ts = model.predict(txt,batch_size=1)['label'][0].tolist()\n    predicted_tags_index = [ts.index(t) for t in ts if t >= .5]\n    predicted_tags_labels = [top_10_tag[i] for i in predicted_tags_index]\n    return predicted_tags_labels","98230579":"q = \"Be yourserlf,everyone else is already taken.\"\nprint(f'The predicted labels: {simple_multi_label(q)}')\nprint(f'The predicted labels: {fuzzywuzzy_sim(q)}')\nprint(f'The predicted labels: {ml_predicted_tags(q)}')\nprint(f'The predicted labels: {dl_predicted_tags(q)}')","8994c65e":"def comp_tags(q):\n    simple_tags = simple_multi_label(q)\n    fuzzwazzy   = fuzzywuzzy_sim(q)\n    ml_tags     = ml_predicted_tags(q)\n    dl_tags     = dl_predicted_tags(q)\n    comp_tags   = list(set(simple_tags + fuzzwazzy + ml_tags+ dl_tags))\n    return comp_tags[:10] if len(comp_tags) > 10 else comp_tags","d235b34b":"q = \"A friend is someone who knows all about you and still loves you\"\nprint(f'The predicted labels: {comp_tags(q)}')","a6c9ce57":"#### Build The RoBERTa Model","b9c8f369":"Here we found 3 tags have the same tag!\n[inspirational-quotes, inspirational, inspiration], we will just keep on of them 'inspirational'","5844f4bd":"## Deep Learning Model","5f2a6982":"### Custimize tags","f30b3ca1":"**The accuracy it's too low, because the data is samll 2481 row data.**","fc2596fd":"### Model Testing","bb9b67a8":"### Distribution on number of tags","a9e01fce":"# 1- Importing libs\n","c9ca4910":"# 2- Pre-processing","bb13174f":"Makes a list of top 10 tags and iterate over each original tags, if any one from the top_10 add it.","6e8ca4a0":"#### Test all predicted methods\n","b72f35d6":"**Here we converted the list as a string to actual list type**","5c008f85":"## Machine Learning Model","b68396ce":"# 3- Modeling","0cde71c5":"**fuzz.partial_ratio**","7427f95f":"### Building The Model","e64033a2":"## Simple Predecting Method","e36a9ee1":"**By the experimental i'll use `partial_ratio`**","a8925869":"## FuzzyWuzzy ","b8cf705b":"**fuzz.token_sort_ratio**","ad2c98bb":"### TF-IDF Vectorizer","9caa23cf":"**Here the tags col has a str type instate of list for each quote, so we will convert it to list**","329d9a37":"#### Evaluate the model\n","c1ddd108":"**fuzz.ratio**","6a5a6f30":"* Fuzzywuzzy is a Python library uses **Levenshtein Distance** to calculate the differences between sequences in a simple-to-use package.\n\n* **We will use this lib for sent similarity between the quote and top 10 tags.**\n* **ratio**, compares the entire string similarity, in order.\n* **partial_ratio**, compares partial string similarity.\n* **token_sort_ratio**, ignores word order and ignores duplicated words.\n","4b4947d2":"### Referances:\n[Multi-Label Text Classification](https:\/\/pianalytix.com\/multi-label-text-classification\/)\n\n\n[TF-IDF from scratch in python on a real-world dataset](https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n\n\n[A quick tutorial on how to deploy your Streamlit app to Heroku.\n](https:\/\/towardsdatascience.com\/a-quick-tutorial-on-how-to-deploy-your-streamlit-app-to-heroku-874e1250dadd)\n\n\n[A quick tutorial on how to deploy your Streamlit app to Heroku-video](https:\/\/www.youtube.com\/watch?v=zK4Ch6e1zq8)","b11fd38c":"### Model Testing","31786f6a":"**The simplest way to make a multi-label classification is just to take all labels in the dataset and check if the piece of a word in the exact quote is belongs to the whole labels and get this label\\s!**\n\n**The sequance:**\n* Make a list of all original unique labels.\n* Make a list of all unique labels but cleaned. \n* Make a dict with {'cleaned_tag\": 'original_tag'}.\n* After taking the quote, make a preprocess on it then check if any word belong the cleaned_labels, if it .. get the original labels from the dict.\n","898e6f8a":"## Choosen my ML model","8a9e8318":"### Make a list of frequent tags"}}