{"cell_type":{"95d2612e":"code","9f079265":"code","b701145f":"code","43963a47":"code","69bfbec5":"code","289ed5db":"code","880424f4":"code","9d455313":"code","f0791acd":"code","e5648462":"code","99738926":"code","ea31355c":"code","be3a1b58":"code","708d59b4":"code","8862ffdc":"code","c69ac210":"code","b09d6cf6":"code","d3d8985e":"code","002d6631":"code","1878a71a":"code","673ef262":"code","9fc61f57":"code","2659f4eb":"code","4a42032c":"code","25174c58":"code","4fa062bf":"code","7ad62381":"code","c897425b":"code","314dbb93":"code","401c4a5c":"code","b8cca908":"code","769c91ea":"code","95c40edb":"code","197898c6":"code","96d5d49e":"code","f74aabb2":"code","217df03b":"code","ae7abb1f":"code","0ac937c3":"code","1bcc1047":"code","8acd21c2":"code","c21fb55c":"code","1e157e9d":"code","04e5ba1d":"code","564e603d":"code","83efecfb":"code","0a019b0c":"markdown","54655994":"markdown","9d665e7d":"markdown","2149f0be":"markdown","bb276bcd":"markdown","70b383a1":"markdown","f38dd8b1":"markdown","db0cec1a":"markdown","bc710ddb":"markdown","a44a00c2":"markdown","541c918a":"markdown","a50aea9d":"markdown","c522c206":"markdown","24de6416":"markdown","6f481fa5":"markdown","672c6f70":"markdown","4650767e":"markdown","c4449c5d":"markdown","a7fbd52b":"markdown","cf3b66d1":"markdown","1283da28":"markdown","0ef4e81d":"markdown"},"source":{"95d2612e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","9f079265":"car_details = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/CAR DETAILS FROM CAR DEKHO.csv')\ncar_details.head()","b701145f":"#Get the count of rows for each column\ncar_details.info()","43963a47":"# A good library to get basic EDA done\nimport seaborn as sns\nprint(car_details.describe())\n\nsns.distplot(car_details.selling_price, color = 'blue')","69bfbec5":"sns.boxplot(car_details.iloc[:,1])","289ed5db":"sns.pairplot(car_details)","880424f4":"# car_details[\"is_duplicate\"]= car_details.duplicated()\n# car_details[\"is_duplicate\"].value_counts()\n#car_details.drop(car_details[car_details['is_duplicate'] == True].index, inplace = True)\n#By default, for each set of duplicated values, the first occurrence is set on False and all others on True.\n#value_counts is a Series method rather than a DataFrame method\n#car_details[(car_details.name == 'Hyundai Verna SX') & (car_details.year == 2007)]","9d455313":"car_details.drop_duplicates(inplace = True)\ncar_details1 = car_details.reset_index()\nlen(car_details)","f0791acd":"car_details1.describe()","e5648462":"categorical_variables = car_details1.select_dtypes(exclude=[\"number\"]).columns","99738926":"# Categorical plots in the shape of Violin\nfor i in categorical_variables:\n    if(len(car_details1[i].unique())<10 and len(car_details1[i].unique())>0):\n        sns.catplot(x=\"selling_price\", y=i,kind=\"violin\", split=True, data=car_details1)","ea31355c":"#Pair plot for integer variables\nsns.pairplot(car_details1.iloc[:,2:5])","be3a1b58":"car_details1['name'].value_counts()","708d59b4":"#Taking company name and parent model name, not focused on exact model type\nfor i in range(len(car_details1)):\n    car_details1.loc[i,'name_model'] = ' '.join(car_details1.loc[i,'name'].split()[:2]) #split and join the string","8862ffdc":"car_details1.name_model.value_counts()","c69ac210":"#Drop models whose count is less than 5\ncounts = car_details1['name_model'].value_counts()\n\ncar_details1 = car_details1[~car_details1['name_model'].isin(counts[counts < 5].index)]","b09d6cf6":"categorical_variables\n# We will not encode the name variable. As it will result to more 188 columns","d3d8985e":"#one-hot encoding, adding dummy-variables\ncar_details2 = pd.get_dummies(car_details1, columns=categorical_variables[1:])","002d6631":"car_details2.info()","1878a71a":"#Now, drop the columns not required for further analysis\ncar_details2.drop(columns = ['index','name'], inplace = True)\ncar_details2.head()","673ef262":"# Label encoding of model names\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(list(car_details2.name_model))\ncar_details2.name_model = le.transform(list(car_details2.name_model))\ncar_details2.head()","9fc61f57":"car_details2.info()\n#Now all the columns are in numeric format","2659f4eb":"#Let's figure out the heat map among the features\nplt.figure(figsize=(24,24)) # For a appropriate size\n\nsns.heatmap(car_details2.corr(),annot=True,cmap='summer') #Annotation = enables value of each box visible","4a42032c":"#sns.pairplot(car_details2)","25174c58":"#For models from Sklearn\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(car_details2), columns = car_details2.columns)","4fa062bf":"target = train.selling_price\nfeatures = train.drop(columns = ['selling_price'])","7ad62381":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, stratify = features.name_model)","c897425b":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nlinreg.score(X_test,y_test)","314dbb93":"from sklearn.model_selection import cross_val_score\nnp.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10))","401c4a5c":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'alpha': np.logspace(-3, 3, 13)}","b8cca908":"grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True, iid=False)\ngrid.fit(X_train, y_train)","769c91ea":"grid.score(X_test, y_test)","95c40edb":"np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10))","197898c6":"from sklearn.neighbors import KNeighborsRegressor\nneighbors = range(1, 30, 2)\n\ntraining_scores = []\ntest_scores = []\nfor n_neighbors in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=n_neighbors).fit(X_train, y_train)\n    training_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))","96d5d49e":"plt.figure()\nplt.plot(neighbors, training_scores, label=\"training scores\")\nplt.plot(neighbors, test_scores, label=\"test scores\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","f74aabb2":"knn = KNeighborsRegressor(n_neighbors=7)\nscore = cross_val_score(knn, X_train, y_train, cv=10)\nprint(f\"best cross-validation score: {np.max(score):.3}\")\n\nknn.fit(X_train, y_train)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")","217df03b":"from sklearn.svm import SVR\nsvr = SVR()\nsvr.fit(X_train, y_train)\nprint(f\"test-set score: {svr.score(X_test, y_test):.3f}\")","ae7abb1f":"svr1 = SVR(kernel='poly')\nsvr1.fit(X_train, y_train)\nprint(f\"test-set score: {svr1.score(X_test, y_test):.3f}\")","0ac937c3":"from sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor()\nsgd.fit(X_train, y_train)\nprint(f\"test-set score: {sgd.score(X_test, y_test):.3f}\")","1bcc1047":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\nprint(f\"test-set score: {dtr.score(X_test, y_test):.3f}\")","8acd21c2":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\nprint(f\"test-set score: {rfr.score(X_test, y_test):.3f}\")","c21fb55c":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\nprint(f\"test-set score: {gbr.score(X_test, y_test):.3f}\")","1e157e9d":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train,y_train)\nprint(f\"test-set score: {xgb.score(X_test, y_test):.3f}\")","04e5ba1d":"from sklearn.ensemble import ExtraTreesRegressor\netr = ExtraTreesRegressor()\netr.fit(X_train, y_train)\nprint(f\"test-set score: {etr.score(X_test, y_test):.3f}\")","564e603d":"from sklearn.ensemble import VotingRegressor\nvr = VotingRegressor(estimators=[('rfr', rfr), ('gbr', gbr), ('xgb', xgb)])\nvr.fit(X_train,y_train)\nprint(f\"test-set score: {vr.score(X_test, y_test):.3f}\")","83efecfb":"from sklearn.metrics import mean_squared_error\n\nfor clf in (rfr, gbr, xgb, vr):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__,  mean_squared_error(y_test, y_pred))","0a019b0c":"## 3. Preparing to modeling <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","54655994":"### 4.10 Extra Tree Regressor <a class=\"anchor\" id=\"5.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","9d665e7d":"Categorical Plots","2149f0be":"## 2. EDA <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","bb276bcd":"### 4.8 Gradient Boosting <a class=\"anchor\" id=\"4.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","70b383a1":"### 4.11 Voting Regressor <a class=\"anchor\" id=\"4.11\"><\/a>\n\n[Back to Table of Contents](#0.1)","f38dd8b1":"# Used Car Data Price Prediction\n\nBuild of the 10+ most popular ML regression models to predict the price of used Car in Indian Market.\n\nLet's design the flow of computation:\n1. Supervised, Unsupervised, Reinforcement Learning? Ans. Supervised because provided with labeled data.\n2. Classification, Regression, something else? Ans. Univariate Multiple Regression because we need to predict a single variable, selling price on the basis of multiple features.\n3. Batch learning or online learning techniques? Ans. Plain Batch learning as there is no continous inflow of data apart from provided once.\n4. Performance Measure? Ans. Root Mean Square Error(RMSE), a typical performance measure for regression problems. Will also consider, using Mean Absolute Error(MAE).\n5. Hypothesis: Selling price of car will be more if less driven, of premium brand, sold by first owner, of latest year with automatic transmission.","db0cec1a":"### 4.6 Decision Tree Regressor <a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","bc710ddb":"### 4.7 Random Forest <a class=\"anchor\" id=\"4.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","a44a00c2":"Due to many car models with less data associated. I will consider cars on the basis of their Company & car name.","541c918a":"## 1. Import libraries & dataset <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","a50aea9d":"The or and and python statements require truth-values. For pandas these are considered ambiguous so you should use \"bitwise\" | (or) or & (and) operations:","c522c206":"### 4.1 Linear Regression <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","24de6416":"So, we have details of car with second hand selling price for a particular year","6f481fa5":"This gives a sense of correlation among variables with integer values. As, price of car is maximum with less driven and sold in latest year","672c6f70":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries & dataset](#1)\n1. [EDA](#2)\n1. [Preparing to modeling](#3)\n1. [ML models](#4)\n    -  [Linear Regression](#4.1)\n    -  [Ridge Regression](#4.2)\n    -  [K Neighbors Regressor](#4.3)\n    -  [SVR](#4.4)\n    -  [Stochastic Gradient Descent](#4.5)\n    -  [Decision Tree Regressor](#4.6)\n    -  [Random Forest](#4.7)\n    -  [Gradient Boosting](#4.8)\n    -  [XG Boost](#4.9)\n    -  [ExtraTreesRegressor](#4.10)\n    -  [VotingRegressor](#4.11)","4650767e":"### 4.2 Ridge Regression <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c4449c5d":"### 4.3 K Neighbors Regressor <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a7fbd52b":"### 4.5 Stochastic Gradient Descent <a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","cf3b66d1":"There are duplicate rows and let's drop them out","1283da28":"### 4.4 SVR <a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","0ef4e81d":"### 4.9 XG Boost <a class=\"anchor\" id=\"4.9\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}