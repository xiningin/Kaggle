{"cell_type":{"5f2c512a":"code","e247d52b":"code","9a846b20":"code","488716e4":"code","e597f8c7":"code","72827485":"code","66f6d0a0":"code","1901960b":"code","fa6aadaf":"code","637daff0":"code","871f72cc":"code","8702a881":"code","19fa1632":"code","2a4f5093":"code","f6dbe918":"code","ea44687f":"code","95b86937":"code","ce024279":"code","c7cd3c9e":"code","2ac67b52":"code","55b46f95":"code","93c7e38b":"code","0cca2ec7":"code","86df477f":"code","c8a66e81":"code","15765882":"code","5fa44590":"code","bc216ff1":"code","ba314c1d":"code","98ac3e7f":"code","d02a00fe":"code","f2183186":"code","8177e5ea":"markdown","babb56cb":"markdown","d61af95b":"markdown","7289ee8b":"markdown","4a448aee":"markdown","10df19ea":"markdown","f841189c":"markdown","52247ca8":"markdown","e24ac81c":"markdown","8de7a89d":"markdown","db112f69":"markdown","bfd98004":"markdown","6e4549d8":"markdown","8cd758b1":"markdown","83c9c05d":"markdown","6e08b561":"markdown","ce2a8fd1":"markdown","7c031c65":"markdown","83a5a570":"markdown","5fe2b3ba":"markdown","e6134f14":"markdown","65070be5":"markdown","266151fb":"markdown","48036082":"markdown"},"source":{"5f2c512a":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","e247d52b":"#READ THE DATA\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nclear_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_sub_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ncombine = [train_df, test_df]","9a846b20":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","488716e4":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e597f8c7":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","72827485":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","66f6d0a0":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","1901960b":"list1 = [ \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","fa6aadaf":"sns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\",data = train_df, kind = \"box\")\nplt.show()","637daff0":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","871f72cc":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', height=2.2, aspect=1.6,)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep',order=[1,2,3])\ngrid.add_legend()","8702a881":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","19fa1632":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","2a4f5093":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","f6dbe918":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","ea44687f":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","95b86937":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","ce024279":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","c7cd3c9e":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(\"S\")\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2ac67b52":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","55b46f95":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\n\ntest_df[\"Sex\"] = test_df[\"Sex\"].astype(\"category\")\ntest_df = pd.get_dummies(test_df, columns=[\"Sex\"])","93c7e38b":"def clean_data(data):\n    data.drop(labels = [\"Ticket\", \"Cabin\"], axis = 1, inplace = True)\n    data.dropna(how=\"any\", inplace = True)","0cca2ec7":"clean_data(train_df)\nclean_data(test_df)\ntrain_df.shape","86df477f":"#Divide data\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","c8a66e81":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n    ","15765882":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","5fa44590":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","bc216ff1":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","ba314c1d":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","98ac3e7f":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","d02a00fe":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes','Linear SVC','Percepteron'],\n    'Score': [acc_knn, acc_log, acc_gaussian, acc_linear_svc,acc_perceptron]})\nmodels.sort_values(by='Score', ascending=False)","f2183186":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n# submission.to_csv('..\/output\/submission.csv', index=False)","8177e5ea":"Fare feature seems to have correlation with survived feature (0.26).","babb56cb":"## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","d61af95b":"After that we don't need Name feature. And we also don't need PassengerId feature in the training dataset.","7289ee8b":"### Correlation between categorical and numerical features\nPoint plot chart shows me the correlation between gender and embarked features.\nWe can assume these with this chart:\n- Passengers who pay a lot of monet had better survival\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).","4a448aee":"## Analyze by visualizing data\n\nThe most important point for people to understand a data is to be able to see it visually.\nAt this point, I will try to determine the relationship between the data and make inferences from them by using the graphics, which are the most important tools at our disposal.\n\n### Correlating numerical features\n\nLets start by understanding correlations between numerical features and our solution goal (Survived).","10df19ea":"### Creating new feature by combining other features\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","f841189c":"### Completing Embarked Feature\nEmbarked feature takes S, Q, C values. We simply fill these with the most common occurance which is 'S'.","52247ca8":"#### Converting Categorical Features to Numerical","e24ac81c":"**Which features contain blank, null or empty values?**\n\nThese will require correcting.\n\n- Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n- Cabin > Age are incomplete in case of test dataset.\n\n**What are the data types for various features?**\n\nHelping us during converting goal.\n\n- Seven features are integer or floats. Six in case of test dataset.\n- Five features are strings (object).","8de7a89d":"I ranked all solutions and KNN gave best score for me so I choose it.","db112f69":"Converting Embarked letters to numerical","bfd98004":"We can replace these complicated titles with most common types to make easier analyze","6e4549d8":"When we look at the histogram chart we can easly see that there is correlation between age and survived. It gives me idea about that I should use in my model age feature.\nI can assume these when I look at the chart:\n- Age <=4 had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.","8cd758b1":"### New Feature From Title\nAt the beginning of the analysis we looked at the dataset see that title feature has errors. We can solve this error by extracting new feature. It also gives me new column to analyze.\n\nIn the following code I extract Title feature using RegEX \"([A-Za-z]+)\\.\" .\n","83c9c05d":"**There is mixed data types**\n\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n\n**There is features that contains errors**\n\n- Name feature may contain errors such as quotes, brackets and alternative short names.","6e08b561":"1st class passengers are older than 2nd, and 2nd is older than 3rd class.\n\n","ce2a8fd1":"## Model And Prediction\nI'm done with data. Now I'm ready to use it with some algorithms for prediction and modeling. There are more than fifty predictive modelling algorithms to choose from. So I must understand my problem solution requirement. Titanic problem is a regression and classification problem.Supervised Learning plus Classification and Regression, we can find some algorithms compatible with our problem like theese:\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Naive Bayes classifier\n- Decision Tree","7c031c65":"### Dataset Fields\n* PassengerId: Unique Id for each passenger\n* Survived: Binary value for survival (0 = No, 1 = Yes)\n* Pclass: Ticket class for each passenger (1 = 1st Class, 2 = 2nd Class, 3 = 3rd Class)\n* Sex: Gender of each passenger\n* Age: Age of each passenger in years\n* SibSp: Number of siblings or spouses aboard the Titanic\n* Parch: Number of parents or children aboard the Titanic\n* Ticket: Ticket number for the passenger\n* Fare: Price of the ticker\n* Cabin: Cabin number of the passenger\n* Embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","83a5a570":"### Completing Or Dropping Missing Features \n\nOne of the problem is that missing, empty rows. There are some solution that I can use. Easy way is that. I can remove them or fill them with some values.\n\nSome features like Ticket and Cabin I will not use.\n    ","5fe2b3ba":"\n**We can categorize features by looking data:**\n* Categorical: Survived, Sex, and Embarked. \n* Ordinal: Pclass.\n* Continous: Age, Fare. \n* Discrete: SibSp, Parch.\n\n\n","e6134f14":"We can combine many charts at the same time see how Pclass affect people to survive.\nWe may assume these:\n- Pclass=3 had most passengers, however most did not survive.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. \n- Most passengers in Pclass=1 survived. \n- Pclass varies in terms of Age distribution of passengers.","65070be5":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","266151fb":"I can convert them to ordinal.","48036082":"# Titanic Data\n\n\n## Workflow stages\n\n1. Question or problem definition.\n2. Wrangle, prepare, cleanse the data.\n3. Analyze, identify patterns, and explore the data.\n4. Visualize, report, and present the problem solving steps and final solution.\n\n\n## Question and Problem\n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nWe will analyse this data and our aim is that Is there any relationship between variables If it is ,how affect people to survive ?\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal?\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."}}