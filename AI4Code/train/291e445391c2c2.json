{"cell_type":{"69b9f745":"code","b14238ad":"code","88ac1fff":"code","3763f1df":"code","a1ef3c97":"code","e6d977a6":"code","8c78597d":"code","ea8b40d9":"code","b54423f4":"code","5b9f261a":"code","b4995fa4":"code","3a0d1c4c":"code","48dbb95c":"code","58dd8c98":"code","71be7879":"code","1fdaa47b":"code","b0b1a3fa":"code","eaf6fa43":"code","b7af9f87":"markdown","3d076a5c":"markdown","8854e248":"markdown","a75c95a1":"markdown","368b548d":"markdown","cea53ed4":"markdown","a7e0b465":"markdown","8ef632ab":"markdown","9509306a":"markdown","129edd7d":"markdown","df8abc87":"markdown"},"source":{"69b9f745":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b14238ad":"# Load the data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngendersub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")#for verification","88ac1fff":"#A view at the training data\ntrain.shape","3763f1df":"features=['Pclass', 'Sex', 'SibSp', 'Parch']","a1ef3c97":"train[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e6d977a6":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8c78597d":"train = train.replace({'Sex' : { 'male' : 1, 'female' : 0}})\ntest = test.replace({'Sex' : { 'male' : 1, 'female' : 0}})","ea8b40d9":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b54423f4":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5b9f261a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","b4995fa4":"#defining variables\nX=train[features]\ny=train['Survived']\nZ=test[features]\n#correct predictions\nytrue = gendersub[\"Survived\"]","3a0d1c4c":"lr = LogisticRegression()\nlr.fit(X,y)\nypred1 = lr.predict(Z)\nprint(\"LR \",accuracy_score(ytrue, ypred1, normalize=True, sample_weight=None))\nprint(confusion_matrix(ytrue, ypred1))","48dbb95c":"dtc = DecisionTreeClassifier()\ndtc.fit(X,y)\nypred2 = dtc.predict(Z)\nprint(\"DTC \",accuracy_score(ytrue, ypred2, normalize=True, sample_weight=None))\nprint(confusion_matrix(ytrue, ypred2))","58dd8c98":"rfc = RandomForestClassifier()\nrfc.fit(X,y)\nypred3 = rfc.predict(Z)\nprint(\"RFC \",accuracy_score(ytrue, ypred3, normalize=True, sample_weight=None))\nprint(confusion_matrix(ytrue, ypred3))","71be7879":"knn = KNeighborsClassifier()\nknn.fit(X,y)\nypred4 = knn.predict(Z)\nprint(\"KNN\", accuracy_score(ytrue, ypred4, normalize=True, sample_weight=None))\nprint(confusion_matrix(ytrue, ypred4))","1fdaa47b":"gbk = GradientBoostingClassifier()\ngbk.fit(X, y)\nypred5 = gbk.predict(Z)\n\nprint(\"GBK\", accuracy_score(ytrue, ypred5, normalize=True, sample_weight=None))\nprint(confusion_matrix(ytrue, ypred5))","b0b1a3fa":"ypred1","eaf6fa43":"#Lets save our output file \nids = test['PassengerId']\npredictions = lr.predict(Z)\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","b7af9f87":"# **2) Selecting Features**\nSelecting features is an important part for prediction accuracy.\nFor keeping things a bit simple I've dropped columns with empty values.\nSo the features I've selected are :\nPclass,\nSex,\nSibSp,\nParch","3d076a5c":"*Looks like passengers with higher class are more likely to survive*","8854e248":"# **1) Read Data**","a75c95a1":"# **Output**\n*This is the output from Logistic Regression Model*","368b548d":"# **3) Importing Models**\nI'm using these models\n* Logistic Regression\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Gradient Boosting Classifier","cea53ed4":"Let's have a look at how good our selected features are:","a7e0b465":"Now since Sex is not a numeric value we have to convert it first.\n(*So assigning 0 to Female and 1 to Male*)\n","8ef632ab":"*Here passengers with 3 or less then 3 children are more likely to survive*","9509306a":"# **Conclusion**\nSo this is the order of accuracy each model provides with highest on top\n* Logistic Regression - 0.992822966507177\n* Gradient Boosting Classifier - 0.9712918660287081\n* Random Forest Classifier - 0.9258373205741627\n* KNN or k-Nearest Neighbors - 0.9114832535885168\n* Decision Tree Classifier - 0.9090909090909091","129edd7d":"*Female passengers are more likely to survive than Male passengers*","df8abc87":"*Here passengers with 2 or less then 2 siblings\/spouse are more likely to survive*"}}