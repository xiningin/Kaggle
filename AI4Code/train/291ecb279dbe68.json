{"cell_type":{"2ad0c254":"code","e640dce3":"code","cf005e42":"code","8fdd60a5":"code","4586edf4":"code","7f38e1b5":"code","0e9f6c55":"code","89d2be54":"code","8668e2c8":"code","993d1082":"code","6fdcfcab":"code","cacc2f5c":"code","ead7d041":"code","d91b6eff":"code","171e1a2b":"code","2a06de96":"code","fee1ee8b":"code","4f963e58":"code","76e0b6d4":"code","3c631d75":"code","a6242a0d":"code","87ebf4b7":"code","7f52c81f":"code","4853bb4d":"code","a61a7d36":"markdown","5d65f743":"markdown","08726887":"markdown","460e6590":"markdown","acb405d5":"markdown","dd992cc6":"markdown","77c6646a":"markdown","6fce38c5":"markdown"},"source":{"2ad0c254":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import skew, kurtosis\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Data Processing libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Importing models\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e640dce3":"data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","cf005e42":"data.head(5)","8fdd60a5":"data.info()","4586edf4":"# No Null Values?\ndata.isnull().sum()","7f38e1b5":"# Print out columns that are not necessarily discrete\nfor col in data.select_dtypes(np.number).columns:\n    if data[col].nunique() > 5:\n        print(col, data[col].nunique())","0e9f6c55":"# Print out column names\nprint(data.columns.tolist())","89d2be54":"# Save the discrete values (values lesser than 5 count as being discrete)\ndisc_vars = 'sex cp fbs restecg exng slp caa thall output'.split()\ndisc_vars","8668e2c8":"# Check the distributions of each discrete variable\nfig, axes = plt.subplots(3,3,figsize=(14,14))\nfor var,ax in zip(disc_vars,axes.flat):\n    sns.countplot(data=data,x=var,hue='output',ax=ax)","993d1082":"# Capture the 'less discrete' features\ncont_vars = [var for var in data.columns if var not in disc_vars]\nprint(len(cont_vars))\ncont_vars","6fdcfcab":"fig, axes = plt.subplots(3,2,figsize=(14,14))\nfor var,ax in zip(cont_vars, axes.flat):\n    sns.distplot(data[var],ax=ax,bins=45)","cacc2f5c":"# Fixing the positive skewness with log transform\ndata['oldpeak'] = np.log1p(data['oldpeak'])","ead7d041":"sns.distplot(data['oldpeak'])","d91b6eff":"# Checking for Categorical Variables\ndata.select_dtypes(exclude=np.number).columns.tolist()","171e1a2b":"# Make a copy so we can see the difference with cross validation if scaling, encoding made a difference\ndata_copy = data.copy()\n\n# Get rid of the target values so we can use a pipeline to transform the features\ndata_copy.drop('output',axis=1,inplace=True)\ny = data['output']","2a06de96":"# Make a pipeline for scaling and transforming our data\ndisc = [i for i in disc_vars if i != \"output\"]\npipeline = ColumnTransformer([\n    (\"numeric\",StandardScaler(),cont_vars),\n    (\"discrete\",OneHotEncoder(),disc)\n])","fee1ee8b":"# Fit the Transformations\ndata_copy = pipeline.fit_transform(data_copy)","4f963e58":"# Test whether the transformations made a difference using cross validation and a simple LR model\nmodel = LogisticRegression()\nprint(\"Scores with transformed dataset:\")\nprint(cross_val_score(model,data_copy,y,cv=10).mean())\nmodel2 = LogisticRegression()\nprint(\"Scores with plain dataset:\")\nprint(cross_val_score(model2,data.drop('output',axis=1),y,cv=10).mean())","76e0b6d4":"model = RandomForestClassifier()\nprint(\"Scores with transformed dataset:\")\nprint(cross_val_score(model,data_copy,y,cv=10).mean())\nmodel2 = RandomForestClassifier()\nprint(\"Scores with plain dataset:\")\nprint(cross_val_score(model2,data.drop('output',axis=1),y,cv=10).mean())","3c631d75":"from sklearn.model_selection import train_test_split\n\ndata = data_copy\nX_train,X_test,y_train,y_test = train_test_split(data,y,test_size=0.2,random_state=42)","a6242a0d":"# Make our models and train them\n# Lets add in LinearSVC too\nfrom sklearn.svm import LinearSVC\nrf = RandomForestClassifier(n_estimators=120,random_state=42)\nsvm = SVC(max_iter=100)\nmlp = MLPClassifier(random_state=42)\nlog_reg = LogisticRegression(random_state=42)\nknn = KNeighborsClassifier()\nada = AdaBoostClassifier()\nlsvc = LinearSVC(max_iter=100, tol=20, random_state=42)\n\nestimators = [rf,svm,mlp,log_reg,knn,ada,lsvc]","87ebf4b7":"for estimator in estimators:\n    print(\"Training the \",estimator)\n    estimator.fit(X_train,y_train)\n    print(estimator.score(X_test,y_test))\nprint(\"Done\")","7f52c81f":"named_estimators = [\n    (\"rf\",rf),(\"svm\",svm),(\"log_reg\",log_reg),(\"knn\",knn)\n]\nvoting_classifier = VotingClassifier(named_estimators)\n\nprint(\"Training...\")\nvoting_classifier.fit(X_train,y_train)\nprint(\"Done.\\n Score: \")\n# See how it scores\nprint(voting_classifier.score(X_test,y_test))","4853bb4d":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix for the Hard Voting Classifier:\")\nprint(confusion_matrix(voting_classifier.predict(X_test),y_test))\nprint(\"Confusion Matrix for the KNN Classifier:\")\nprint(confusion_matrix(knn.predict(X_test),y_test))","a61a7d36":"The Confusion Matrices are not dissimilar so we can safely say that the KNN Classifier is the victor.","5d65f743":"Some observations:\n- Samples with a 'thall' value of 2 are VERY likely to be positive instances\n- Samples with an 'exng' value of 1 are more likely to be positive instances\n- People with 'restecg' value 1 are slightly more likely to be at risk\n- Samples with an 'slp' value of 2 are very likely to be positive instances\n- Samples with a 'caa' value of 0 are VERY likely to be positive instances\n- It seems that females (sex=0) are more likely to be positive instances\n- fbs really is not a useful feature\n- Those with a 'cp' value of 2 are more at risk\n- Our target class has a good distribution of different classes","08726887":"So our transformed dataset looks to feed better to models, though not by much. Perhaps this is made more apparent using more powerful models like RandomForests.","460e6590":"So there are no Null Values, no Categorical Variables. Recall we still have some variables that take on discrete values so we should One-Hot Encode them. On top of this we should scale our data.","acb405d5":"Apart from 'oldpeak' everything else has a balanced distribution. This special feature is skewed so we have to normalize it.","dd992cc6":"Again it does not look like much of an improvement but we shall stick with it and train an ensemble.","77c6646a":"KNN worked very well here- for our ensemble we shall only keep the four best estimators.","6fce38c5":"The Ensemble was disappointing. The KNN Classifier won out in the end with an accuracy of $90.16$%"}}