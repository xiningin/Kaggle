{"cell_type":{"6e7a2486":"code","b434490c":"code","d280c92a":"code","7e908e2a":"code","cf0308d3":"code","b7752192":"code","86877d93":"code","6aac934f":"code","77ebcec6":"code","426f019c":"code","de50197a":"code","09611e4e":"code","f79e118a":"code","8c9cd841":"code","3649655c":"code","b3eb12bf":"code","fd952f41":"code","e34619c2":"code","5c5b542c":"code","f1dd9a27":"code","603b1bd7":"code","ce1750df":"code","06f40ce8":"code","5704a43b":"code","509b3573":"code","2e76bd6f":"code","7d5c4e78":"code","7a2c6cfb":"code","0e1acf81":"code","372373af":"code","9c6a20df":"markdown","f3baf5fb":"markdown","0d6c8ee8":"markdown","cf11908f":"markdown"},"source":{"6e7a2486":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b434490c":"# load data from twosigma\nmarketdf = pd.read_csv('..\/input\/marketdata_sample.csv')\nnewsdf = pd.read_csv('..\/input\/news_sample.csv')","d280c92a":"# import needed libraries \nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, date\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n# data shape\nprint(marketdf.shape, newsdf.shape)","7e908e2a":"# peak into the market data\nmarketdf.head().T","cf0308d3":"#peak into the news data\nnewsdf.head().T","b7752192":"marketdf.describe().round(3)","86877d93":"newsdf.describe().round(3)","6aac934f":"# replace outliers, which are the one with radical changes over one day, with mean values\nmarketdf['close_to_open'] =  np.abs(marketdf['close'] \/ marketdf['open'])\nmarketdf['assetName_mean_open'] = marketdf.groupby('assetName')['open'].transform('mean')\nmarketdf['assetName_mean_close'] = marketdf.groupby('assetName')['close'].transform('mean')\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in marketdf.loc[marketdf['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        marketdf.iloc[i,5] = row['assetName_mean_open']\n    else:\n        marketdf.iloc[i,4] = row['assetName_mean_close']\n\nfor i, row in marketdf.loc[marketdf['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        marketdf.iloc[i,5] = row['assetName_mean_open']\n    else:\n        marketdf.iloc[i,4] = row['assetName_mean_close']\nmarketdf.drop(['close_to_open','assetName_mean_open','assetName_mean_close'], axis=1, inplace=True)\nprint(marketdf.shape)","77ebcec6":"# filter out strange data: platoe data or unknown assetcode\nmarketdf = marketdf[~marketdf.assetCode.isin(marketdf[marketdf.assetName=='Unknown'].assetCode.unique())]\nmarketdf = marketdf[marketdf.assetCode != 'PGN.N']\nprint(marketdf.shape)","426f019c":"marketdf = marketdf[marketdf.assetCode != 'TW.N']","de50197a":"marketdf = marketdf[marketdf.assetCode != 'QRVO.O']","09611e4e":"marketdf = marketdf[marketdf.assetCode != 'TECD.O']","f79e118a":"# dropping EBR.N data in Oct 2016\nmarketdf = marketdf[~((marketdf['assetCode'] == 'EBR.N')& (marketdf['time'] >= '2016-10-01'))]","8c9cd841":"marketdf.describe().round(3)","3649655c":"def prepare_data(marketdf, newsdf):\n    # feature engineering\n    # filter pre-2010 data because of unnormal behavior\n    ttt = pd.to_datetime(marketdf.time, errors='coerce')\n    tt2 = {'time': ttt}\n    tt3 = pd.DataFrame(tt2)\n    marketdf['time'] = tt3.time.dt.strftime(\"%Y%m%d\").astype(int)\n    ttt = pd.to_datetime(newsdf.time, errors='coerce')\n    tt2 = {'time': ttt}\n    tt3 = pd.DataFrame(tt2) \n    newsdf['time'] = tt3.time.dt.strftime(\"%Y%m%d\").astype(int)\n    #marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    #newsdf = newsdf.loc[newsdf['time'] > 20120000]\n    marketdf.sort_values(by=['time'])\n    # market data \n    marketdf['average'] = (marketdf['close'] + marketdf['open'])\/2\n    marketdf['day_return'] = marketdf['close'] \/ marketdf['open']\n    marketdf['prev_day_return'] = marketdf['returnsClosePrevMktres1'] \/ marketdf['returnsOpenPrevMktres1']\n    marketdf['prev_10day_return'] = marketdf['returnsClosePrevMktres10'] \/ marketdf['returnsOpenPrevMktres10']\n    marketdf['price_volume'] = marketdf['volume'] * marketdf['close']\n    marketdf['volume_to_mean'] = marketdf['volume'] \/ marketdf['volume'].mean()\n    marketdf['total_MACD'] = marketdf['close'].rolling(window=12).mean()-marketdf['close'].rolling(window=26).mean()\n    marketdf['total_zscore'] = (marketdf['close']-marketdf['close'].rolling(window=200, min_periods=20).mean())\/marketdf['close'].rolling(window=200, min_periods=20).std()\n    # DIF-MACD\n    ma_12 = lambda x: x.rolling(12).mean()\n    ma_26 = lambda x: x.rolling(26).mean()\n    marketdf['DIF'] = marketdf.groupby('assetCode')['close'].apply(ma_12)-marketdf.groupby('assetCode')['close'].apply(ma_26)\n    marketdf['MACD'] = marketdf['DIF'].rolling(window=9).mean()\n    marketdf['OSC'] = marketdf['DIF']-marketdf['MACD']\n    # Z score: 200, 20\n    zscore_fun_improved = lambda x:(x - x.rolling(window=15, min_periods=7).mean())\/x.rolling(window=15, min_periods=7).std()\n    marketdf['zscore'] = marketdf.groupby('assetCode')['close'].apply(zscore_fun_improved)\n    # time series rolling based features: mean, std, ewm\n    windows = [7,14]\n    f = ['open','close','returnsOpenPrevMktres10', 'returnsClosePrevMktres10']\n    for ff in f:\n        for d in windows:\n            marketdf['%s_%s_mean'%(ff,d)] = marketdf.groupby('assetCode')[ff].apply(lambda x: x.rolling(d).mean())\n            marketdf['%s_%s_std'%(ff,d)] = marketdf.groupby('assetCode')[ff].apply(lambda x: x.rolling(d).std())\n           # marketdf['%s_%s_ewm'%(ff,d)] = marketdf.groupby('assetCode')[ff].transform(lambda x : pd.Series.ewm(x, span=d).mean())\n            \n    # news data\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] \/ newsdf['sentenceCount']\n    newsdf['sentence_word_count'] =  newsdf['sentenceCount'] \/ newsdf['wordCount']\n    # apply tf-idf on  news title\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.feature_extraction.text import TfidfTransformer\n    from nltk.corpus import stopwords\n    #the top hundred words.\n    vectorizer = CountVectorizer(max_features=500, stop_words={\"english\"})\n    #we do this with TF-IDF\n    #print(newsdf['headline'].values)\n    X = vectorizer.fit_transform(newsdf['headline'].values)\n    tf_transformer = TfidfTransformer(use_idf=False).fit(X)\n    X_train_tf = tf_transformer.transform(X)\n    X_train_vals = X_train_tf.mean(axis=1)\n    del vectorizer\n    del X\n    del X_train_tf\n    #mean tf-idf score for news article.\n    d = pd.DataFrame(data=X_train_vals)\n    newsdf['tf_score'] = d\n    # drop extra junk from data\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    # join news reports to market data, note many assets will have many days without news data\n    # encode assetcode\n    lbl = {k: v for v, k in enumerate(marketdf['assetCode'].unique())}\n    marketdf['assetCodeT'] = marketdf['assetCode'].map(lbl)\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])","b3eb12bf":"print('preparing data...')\ncdf = prepare_data(marketdf, newsdf)    \n# add binary target variable\ncdf['binary_returnsOpenNextMktres10'] = (cdf['returnsOpenNextMktres10'] > 0).astype(int)\n#del marketdf, newsdf  # save the memory\nprint(cdf.shape)","fd952f41":"# peak into new combined matrix \ncdf.head(3).T","e34619c2":"# additional feature engineering: remove outlier, scaling\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.preprocessing import PowerTransformer, PolynomialFeatures, MinMaxScaler, RobustScaler\n\ndef remove_outlier(df):\n    low = .01\n    high = .99\n    quant_df = df.quantile([low, high])\n    for name in list(df.columns):\n        if is_numeric_dtype(df[name]) and name in [\"close\", \"open\"]:\n            df = df[(df[name] > quant_df.loc[low, name]) & (df[name] < quant_df.loc[high, name])]\n    return df\n\n# remove outliers\nprint(cdf.shape)\ncdf = remove_outlier(cdf)","5c5b542c":"# train-val split\ndef get_input(cdf, option):\n    # time series slice\n    t = []\n    v = cdf['time'].values\n    #print(cdf['time'])\n    for i in range(len(cdf['time'])):\n        t.append(v[i]+(10*i))\n    #print(t)\n    cdf['time'] = t\n    dates = cdf['time'].unique()\n    train_dates = range(len(dates))[:int(0.85*len(dates))]\n#    print(train_dates)\n    val_dates = range(len(dates))[int(0.85*len(dates)):]\n    # train cols\n    traincols = [col for col in cdf.columns if col not in ['time', 'assetCode','universe','binary_returnsOpenNextMktres10', 'returnsOpenNextMktres10']]\n    #cdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)\n    # build, x, y, t, u, d\n    if option == \"train\":\n        X = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[train_dates])].values\n        Y = cdf['binary_returnsOpenNextMktres10'].fillna(0).loc[cdf['time'].isin(dates[train_dates])].values\n        r = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['time'].isin(dates[train_dates])].values\n        u = cdf['universe'].loc[cdf['time'].isin(dates[train_dates])]\n        d = cdf['time'].loc[cdf['time'].isin(dates[train_dates])]\n    elif option == \"val\":\n        X = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[val_dates])].values\n        Y = cdf['binary_returnsOpenNextMktres10'].fillna(0).loc[cdf['time'].isin(dates[val_dates])].values\n        r = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['time'].isin(dates[val_dates])].values\n        u = cdf['universe'].loc[cdf['time'].isin(dates[val_dates])]\n        d = cdf['time'].loc[cdf['time'].isin(dates[val_dates])]\n    return X,Y,r,u,d","f1dd9a27":"# r, u and d are used to calculate the scoring metric\nprint('building training and validation set...')\nXt,Yt,r_train,u_train,d_train = get_input(cdf, \"train\")\nXv,Yv,r_val,u_val,d_val = get_input(cdf, \"val\")\nprint(Xt.shape, Yt.shape)\nprint(Xv.shape, Yv.shape)","603b1bd7":"# Modeling: LightGBM\nprint ('Training lightgbm')\nevals_result = {}  # to record eval results for plotting\n\n# parameters\nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 600,\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.001,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"verbosity\" : -1 }\n\nlgtrain = lgb.Dataset(Xt, Yt) \nlgval = lgb.Dataset(Xv, Yv)\nlgbmodel = lgb.train(params, lgtrain, 1000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, evals_result=evals_result, verbose_eval=10)","ce1750df":"# plot feature importance of trained model\nimport seaborn as sns\nfeat_importance = pd.DataFrame()\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode','universe','binary_returnsOpenNextMktres10', 'returnsOpenNextMktres10']]\nfeat_importance[\"feature\"] = cdf[traincols].columns\nfeat_importance[\"gain\"] = lgbmodel.feature_importance(importance_type='gain')\nfeat_importance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(8,10))\nax = sns.barplot(y=\"feature\", x=\"gain\", data=feat_importance)","06f40ce8":"# plot feature importance using SHAPE value\nimport shap\npd.set_option(\"display.max_columns\", 96)\npd.set_option(\"display.max_rows\", 96)\n\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\nshap.initjs()\n\n# DF, based on which importance is checked\n# Explain model predictions using shap library:\nexplainer = shap.TreeExplainer(lgbmodel)\nshap_values = explainer.shap_values(Xv)\n\n# Plot summary_plot\nshap.summary_plot(shap_values, Xv)","5704a43b":"# plot loss function\nprint('Plotting metrics recorded during training...')\nax = lgb.plot_metric(evals_result, metric='binary_logloss')\nplt.show()","509b3573":"# plotting tree\nprint('Plotting tree...')  # one tree use categorical feature to split\nax = lgb.plot_tree(lgbmodel, tree_index=1, figsize=(40, 30), show_info=['split_gain'])\nplt.show()","2e76bd6f":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# scaling function\ndef post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)\/ (std * 8)\n    return np.clip(df,-1,1)\n\n# another scaing fuction\ndef rescale(data_in, data_ref):\n    scaler_ref =  StandardScaler()\n    scaler_ref.fit(data_ref.reshape(-1,1))\n    scaler_in = StandardScaler()\n    data_in = scaler_in.fit_transform(data_in.reshape(-1,1))\n    data_in = scaler_ref.inverse_transform(data_in)[:,0]\n    return data_in","7d5c4e78":"# see the distribution of prediction\npredicted_return = lgbmodel.predict(Xv, num_iteration=lgbmodel.best_iteration) * 2 - 1\nscaled_predicted_return = post_scaling(predicted_return)\nscaled_predicted_return2 = rescale(predicted_return, r_train)","7a2c6cfb":"# plot distribution\n# distribution of confidence that will be used as submission\nplt.hist(predicted_return, bins='auto', alpha=0.9, label='Predicted confidence')\nplt.hist(scaled_predicted_return, bins='auto', alpha=0.7, label='scaled predicted confidence')\nplt.hist(scaled_predicted_return2, bins='auto', alpha=0.6, label='scaled predicted confidence2')\nplt.hist(r_val, bins='auto',alpha=0.5, label='Val market return')\nplt.title(\"predicted confidence\")\nplt.legend(loc='best')\nplt.xlim(-1,1)\nplt.show()","0e1acf81":"# calculation of actual metric that is used to calculate final score\nr_val = r_val.clip(-1,1) # get rid of outliers.\n#x_t_i = predicted_return * r_val * u_val\nx_t_i = scaled_predicted_return * r_val * u_val\n#x_t_i = scaled_predicted_return2 * r_val * u_val\ndata = {'day' : d_val, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nprint(mean)\nstd = np.std(x_t)\nprint(std)\nscore_valid = mean \/ std\nprint('Validation score', score_valid)","372373af":"'''\n# generate predictions on testing data\nprint(\"generating predictions...\")\npreddays = env.get_prediction_days()\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe','binary_returnsOpenNextMktres10', 'returnsOpenNextMktres10']]\ni = 0\nfor marketdf, newsdf, predtemplatedf in preddays:\n    print(i)\n    i+=1\n    cdf = prepare_data(marketdf, newsdf)\n    Xp = cdf[traincols].fillna(0).values\n    preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration) * 2 - 1\n    #preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration)\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\nenv.write_submission_file()\n\n# submission file\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])\n'''","9c6a20df":"### Data preparation","f3baf5fb":"### Modeling using lightGBM","0d6c8ee8":"### Data cleansing","cf11908f":"### Data preprocessing and preparation"}}