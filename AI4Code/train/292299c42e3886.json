{"cell_type":{"84288fcd":"code","fa2b99ba":"code","20ebe5c6":"code","18741243":"code","08096fee":"code","4a37cdd3":"code","2d8468bd":"code","fa9289c5":"code","243006b0":"code","3cbffd3e":"code","16bdcf15":"code","1c096e25":"code","1a06bceb":"code","a02009ec":"code","5f87b20b":"code","d88a65bd":"code","9f5ffcff":"code","f695db67":"code","d2c02e71":"code","eb24001d":"code","57da2cf9":"code","1098c763":"code","6a4ae4f2":"code","ac4f41d0":"code","c4ac9ec5":"code","af11e560":"markdown","0dde09b3":"markdown","1cdc9e1f":"markdown","576592a5":"markdown","79c1ac5c":"markdown","dbf34e2e":"markdown","9e3e76a8":"markdown","a5301009":"markdown","f0253d92":"markdown","aa6da0cc":"markdown","5fe3c941":"markdown","5b20bed0":"markdown","e5241345":"markdown","6fcbf0dc":"markdown","e713586b":"markdown","c428f239":"markdown","ee81cbbe":"markdown","0fa70d2b":"markdown","a24ce219":"markdown","aae9010e":"markdown"},"source":{"84288fcd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","fa2b99ba":"df = pd.read_csv('..\/input\/top-1000-kaggle-datasets\/kaggle_-1000.csv')\ndf = df.iloc[:, 1:]\ndf.head()","20ebe5c6":"df['data_age'] = df.last_updated.str.extract(r'([0-9]+.*\\s)')\ndf['data_age']  # This data_age column represents in String dtype. Next, I'll convert to datetime.","18741243":"## Parse data_age to constitute creation date in datetime dtype.\nunits = df.data_age.str.extract(r'(\\W.*)')\nn = df.data_age.str.extract(r'(\\d+)')\n\n## From many units(days, months, years), I'll create Timedelta in 'days' unit to represent data_age.\ndata_timedelta =  list(zip(n.values.ravel(), units.values.ravel()))\n\nfor i, (n_, unit_) in enumerate(data_timedelta):\n    \n    try:\n        n_ = int(data_timedelta[i][0])\n    except ValueError as e:\n        data_timedelta[i] = (np.nan, 'day')\n        continue\n    \n    if 'month' in unit_:\n        n_ *= 30  # assume 1 month = 30 days\n        unit_ = 'day'\n    if 'year' in unit_: \n        n_ *= 365  # assume 1 year = 365 days\n        unit_ = 'day'\n    data_timedelta[i] = (n_,unit_.strip())","08096fee":"## Create Timedelta, and estimate the creation date\nlist_data_timedelta = list(map(lambda x: pd.Timedelta(value=x[0], unit=x[1]) ,data_timedelta))\nlist_data_timedelta = pd.Series(list_data_timedelta)\ndf['data_age_Timedelta'] = list_data_timedelta\n\napprox_data_creation_date = (pd.to_datetime(\"today\") - list_data_timedelta).astype('datetime64[D]')\ndf['data_created_date'] = approx_data_creation_date","4a37cdd3":"df.data_age_Timedelta = df.data_age_Timedelta.astype('timedelta64[D]')\ndf.data_created_date = df.data_created_date.astype('datetime64[D]')\n\ndf[['data_age_Timedelta', 'data_created_date']]","2d8468bd":"## Extract data formats\nfiles_types = df.files.str.extract(r'(\\(.*\\))').values.ravel().tolist()\nfor i, e in enumerate(files_types):\n    try:\n        types = e[1:-1]\n        types = types.split(', ')\n        files_types[i] = types\n    except TypeError as e:\n        files_types[i] = np.nan\n        continue","fa9289c5":"## Summarize data formats\nfiles_types_summary = pd.Series(files_types).explode().reset_index(drop = True)\nfiles_types_summary.value_counts().plot(kind='bar', ylabel='number', title='Data formats distribution')\nplt.xticks(rotation=0);","243006b0":"df['number_files'] = df.files.str.extract(r'(\\d+)')","3cbffd3e":"data_size_kb = df['size'].str.split().values.tolist()\nassert df['size'].isnull().sum() == 0\n\nbigQ_data = dict()\nfor i, size in enumerate(data_size_kb):\n    \n    try:\n        n_, unit_ = size\n    except ValueError as e:\n        bigQ_data[i]=size[0]\n        continue\n        \n    n_ = int(n_)\n    if 'm' in unit_.lower():\n        n_ *= 1024  # 1 MB = 1024 KB\n    elif 'g' in unit_.lower():\n        n_ *= (1024*1024)  # 1 GB = 1024 MB\n    data_size_kb[i] = n_\n    \ndf['data_size_kb'] = data_size_kb","16bdcf15":"df.number_files = pd.to_numeric(df.number_files, downcast='integer', errors='coerce')\ndf.data_size_kb = pd.to_numeric(df.data_size_kb, downcast='integer', errors='coerce')\ndf['avg_size_per_file'] = df.data_size_kb \/ df.number_files","1c096e25":"df.info()","1a06bceb":"fig, axes = plt.subplots(2,3, figsize=(15,8))\nfig.suptitle('Data distributions')\n\ncols_to_plots = ['data_age_Timedelta', 'upvotes', 'number_files', 'usability','data_size_kb', 'avg_size_per_file']\n\nfor i, c in enumerate(cols_to_plots):\n    data_to_plot = df[c]\n    data_to_plot.plot(\n        kind='box', \n        ax=axes[i%2][i%3]\n    )","a02009ec":"to_plot = ['upvotes', 'number_files','data_size_kb', 'avg_size_per_file']\n\nfig, axes = plt.subplots(1,4, figsize=(15,4))\nfor i, c in enumerate(to_plot):\n    data_to_plot = df[c]\n    data_to_plot.plot(\n        kind='box', \n        ax=axes[i],\n        showfliers=False\n    )","5f87b20b":"sns.countplot(data=df, x='badge');","d88a65bd":"print(df[df['badge'] == 'more_horiz'].shape[0])\n\ndf['badge'].replace('more_horiz', 'np.nan', inplace=True)  # marks anormaly as NaN","9f5ffcff":"## Top data uploaders\ndf.uploaded_by.str.upper().str.strip().value_counts()[:10]","f695db67":"col_used = ['usability', 'upvotes', 'data_age_Timedelta', 'number_files', 'data_size_kb']\n## I don't include badge since badge is basically a dependent variable as well.\n\ndf_analyze = df[col_used]\ndf_analyze = df_analyze.fillna(value=df_analyze.median())  ## Impute NaN with Median due to huge skew\n\nX = df_analyze.drop(['upvotes'], axis=1)\nY = df_analyze['upvotes']","d2c02e71":"fig, axes = plt.subplots(1,4, figsize=(15,4), sharey=True)\nfig.suptitle('data distribution')\n\nfor i, col in enumerate(X):\n    X[col].plot(kind='hist', ax=axes[i])\n    axes[i].set_xlabel(col)","eb24001d":"def Normalize(column, isLog=False, isSqrt=False):\n    if isLog:\n        column = np.log(column + 1)\n    elif isSqrt:\n        column = np.sqrt(column)\n    upper = column.max()\n    lower = column.min()\n    y = (column - lower)\/(upper-lower)\n    return y\n\nfig, axes = plt.subplots(1,4, figsize=(15,4), sharey=True)\nfig.suptitle('Log-normalized')\n\nfor i, col in enumerate(X.columns):\n    Normalize(X[col], isLog=True).plot(kind='hist', ax=axes[i])\n    axes[i].set_xlabel(col)","57da2cf9":"import statsmodels.api as sm\n\nX_norm = X.apply(Normalize, axis=0, isSqrt=True)\n\nX_norm = sm.add_constant(X_norm) # adding a constant\nY_norm = Normalize(Y)\nmodel = sm.OLS(Y_norm, X_norm).fit()\nprint_model = model.summary()\nprint(print_model)","1098c763":"from sklearn import linear_model\nclf = linear_model.Lasso(alpha=0.1)\nclf.fit(X_norm, Y_norm)\n\nprint(clf.coef_)\nprint(clf.intercept_)","6a4ae4f2":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nmodel.fit(X, Y)\n\nrank = list(zip(X.columns, np.round(model.feature_importances_, 3)))\nsorted(rank, key = lambda x: x[-1], reverse=True)","ac4f41d0":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\nstop_words = '|'.join(stopwords.words('english'))\ntext = ' '.join(df.title).upper()\ntext = ''.join([e for e in text if e not in string.punctuation])\ntext = re.sub(rf\"\\s(dataset|data|prediction|image|classification|new|{stop_words})\\w*\", \"\", text, flags=re.IGNORECASE)\n\nwordcloud = WordCloud(width = 800, height = 800, background_color ='white').generate(text)\n\nplt.figure( figsize=(7,7) )\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c4ac9ec5":"words = pd.Series(text.split())\nprint('Top 10 words in datasets:')\nprint(words.value_counts()[:10])","af11e560":"R-squared is only 0.016 meaning that, with the linear model, only 1.6% of predictors can actually cover the variation of the dependent variable.","0dde09b3":"### - Derive number of files of each dataset","1cdc9e1f":"Next, we'll see the popular topic among the top 1000 Kaggle datasets.","576592a5":"# 2. Data Analysis","79c1ac5c":"We can detect an anormaly datapoint here. \"more_horiz\" is not in Kaggle's badge.","dbf34e2e":"<div class='alert-info'>\n    <h3>Questions I'm trying to answer<\/h3>\n    <ol>\n        <li>From 1000 top datasets, which data providers are on the list the most?<\/li>\n        <li>What are the mostly related topic of these top datasets?<\/li>\n        <li>Which is the most important factor to gain large upvotes?<\/li>\n    <\/ol>\n<\/div>","9e3e76a8":"## 1.1) Extracts from `last_updated`\n### - Estimate data age (assume last update is the data creation)","a5301009":"Even when being applied Log-transformation, the data still highly skewed. So, we shouldn't use linear model to find the feature importance. However, let's give it a try.","f0253d92":"By using feature_importance from Random forest, we can see that the size of data is the most relevent feature for determining the number of upvotes.","aa6da0cc":"## 2.3) Analysis","5fe3c941":"## 1.3) Extract from `size`\n### - Derive total file size of each datasest (in kB)","5b20bed0":"We can see that most datasets are quite small and consist of just a few files.","e5241345":"We can see from the distributions that:\n- Most of the datasets are approximately 2-5 years old\n- Even datasets having low `usability` (<5) are still on this top 1000 datasets \u2192 I'll explore thses datasets in the later section\n- There are a huge number of outliers in `data_size_kb`, `number_files`, `upvotes`, and `avg_size_per_file` \u2192 Let's clear outliers and see what the box plots look like","6fcbf0dc":"### - Estimate data creation date","e713586b":"## 1.2) Extract from `files`\n### - Data formats distribution","c428f239":"We can see popular topics of top 1000 Kaggle dataset from WordCloud and count values.","ee81cbbe":"Since Lasso gives zero coef for all variables, we are ensured that, according to the dataset we have, we cannot use linear model to find feature importance.","0fa70d2b":"# 1. Explore Data\nIn this section, I'll try to extract useful information from the raw data.","a24ce219":"## 2.1) Continuous data distributions\n> note that `data_age_Timedelta` is in *days* unit","aae9010e":"## 2.2) Discrete data distribution"}}