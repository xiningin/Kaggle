{"cell_type":{"9dc6c3d2":"code","6157c27e":"code","840521b9":"code","bd6e13eb":"code","ce7f734f":"code","ace77c26":"code","d9df0689":"code","7f182bdc":"code","f4042121":"code","0e122bf5":"code","00276df8":"code","52566f10":"code","59b3e8bc":"code","4f911fd9":"code","82ee2aa5":"code","45e1a43e":"code","86cc4979":"code","8162cbdf":"code","3bf07d54":"code","c12a13e1":"code","385a654a":"code","d7feeb3b":"code","0ed28369":"code","8bcd7e09":"code","ec52914a":"code","83acba7b":"code","dd71c4fd":"code","967a619a":"code","2d75c34f":"code","6fe64f8c":"code","b4907e3b":"code","43653ba8":"code","cfed8297":"code","6be05ac1":"code","50ea0dd3":"code","43286edd":"code","aab9941c":"code","c9fa34d8":"code","195ca6e1":"code","d0db23fa":"code","27b638a2":"code","a4ba9fba":"code","d0a7bbeb":"code","f461d062":"code","17045109":"code","303a6933":"code","867e4ef8":"markdown","248914b7":"markdown","68cc860c":"markdown","db296a1e":"markdown","5bb8809a":"markdown","d49d5f80":"markdown","ad0c2ab5":"markdown","dabd374c":"markdown","89a6906f":"markdown","38c1093b":"markdown","8f3e2c0c":"markdown","92ccd48f":"markdown","0e78214a":"markdown","39019443":"markdown","020fd4de":"markdown"},"source":{"9dc6c3d2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","6157c27e":"#Reading the data\n\ndata = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndata.head()","840521b9":"#Removing the columns that are not needed\n\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"body_text\"})","bd6e13eb":"data.describe()","ce7f734f":"data.groupby(\"label\").describe()","ace77c26":"#The shape of the dataset\n\nprint(\"Input data has {} rows and {} columns\".format(len(data), len(data.columns)))","d9df0689":"#How many spam\/ham are there\n\nprint(\"Out of the total {} rows, {} are spam, {} are ham\".format(len(data),\n                                                       len(data[data['label']=='spam']),\n                                                       len(data[data['label']=='ham'])))","7f182bdc":"data.info()","f4042121":"#How much missing data is there\n\nprint(\"Number of null in label: {}\".format(data['label'].isnull().sum()))\nprint(\"Number of null in text: {}\".format(data['body_text'].isnull().sum()))","0e122bf5":"import string\nstring.punctuation","00276df8":"def remove_punct_num(text):\n    text_nopunct= \"\".join([char for char in text if char not in string.punctuation])\n    text_nonum=''.join([i for i in text_nopunct if not i.isdigit()])\n    return text_nonum\n\ndata['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct_num(x))\n\ndata.head()","52566f10":"import re","59b3e8bc":"def tokenize(text):\n    tokens = re.split('\\W+', text)\n    return tokens\n\ndata['body_text_clean'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n\ndata.head()","4f911fd9":"stopword = nltk.corpus.stopwords.words('english')","82ee2aa5":"stopword[0:100:10]","45e1a43e":"def remove_stopwords(tokenized_list):\n    text = [word for word in tokenized_list if word not in stopword]\n    return text\n\ndata['body_text_clean'] = data['body_text_clean'].apply(lambda x: remove_stopwords(x))\n\ndata.head()","86cc4979":"ps = nltk.PorterStemmer()\nps","8162cbdf":"print(ps.stem('grows'))\nprint(ps.stem('growing'))\nprint(ps.stem('grow'))","3bf07d54":"print(ps.stem('run'))\nprint(ps.stem('running'))\nprint(ps.stem('runner'))","c12a13e1":"print(ps.stem(\"fast\"))\nprint(ps.stem(\"fasting\"))\nprint(ps.stem(\"fastest\"))","385a654a":"#Stemming our data\n\ndef stemming(input_text):\n    text = [ps.stem(word) for word in input_text]\n    return text\n\ndata['body_text_stemmed'] = data['body_text_clean'].apply(lambda x: stemming(x))\n\ndata.head(10)","d7feeb3b":"wn = nltk.WordNetLemmatizer()\nwn","0ed28369":"print(ps.stem('meanness'))\nprint(ps.stem('meaning'))","8bcd7e09":"print(wn.lemmatize('meanness'))\nprint(wn.lemmatize('meaning'))","ec52914a":"print(ps.stem('thinking'))\nprint(ps.stem('thinker'))","83acba7b":"print(wn.lemmatize('thinking'))\nprint(wn.lemmatize('thinker'))","dd71c4fd":"def lemmatizing(input_text):\n    text = [wn.lemmatize(word) for word in input_text]\n    return text\n\ndata['body_text_lemmatized'] = data['body_text_clean'].apply(lambda x: lemmatizing(x))\n\ndata.head(10)","967a619a":"data_vector= data[[\"label\",\"body_text_lemmatized\"]]","2d75c34f":"data_vector.head()","6fe64f8c":"len(data_vector)","b4907e3b":"for i in range(0,5572):\n    st=data_vector[\"body_text_lemmatized\"][i]\n    new_st=\" \".join(st)\n    data_vector[\"body_text_lemmatized\"][i]=new_st","43653ba8":"data_vector.head()","cfed8297":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer()\n\nX_counts = count_vect.fit_transform(data_vector[\"body_text_lemmatized\"])","6be05ac1":"print(X_counts.shape)","50ea0dd3":"print(count_vect.get_feature_names())","43286edd":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df","aab9941c":"ngram_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2))","c9fa34d8":"X_counts2 = ngram_vect.fit_transform(data_vector[\"body_text_lemmatized\"])","195ca6e1":"print(X_counts2.shape)","d0db23fa":"print(ngram_vect.get_feature_names())","27b638a2":"X_counts_df2 = pd.DataFrame(X_counts2.toarray())\nX_counts_df2","a4ba9fba":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer()","d0a7bbeb":"X_tfidf = tfidf_vect.fit_transform(data_vector[\"body_text_lemmatized\"])","f461d062":"print(X_tfidf.shape)","17045109":"print(tfidf_vect.get_feature_names())","303a6933":"idf_df=pd.DataFrame(X_tfidf.toarray())\nidf_df","867e4ef8":"For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n\nStemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. \n\nIt comes down to a trade-off of speed. Stemming takes less times, but takes less grammatical approach, hence sometimes gives errors. \nLemmatization takes more time, but is usually more accurate.","248914b7":"# Data Vectorization","68cc860c":"Let us take our lemmatized text for the next step.","db296a1e":"Tokenization is a very common task in NLP, it is basically a task of chopping a character into pieces, called as token, and throwing away the certain characters at the same time, like punctuation. ","5bb8809a":"# Stemming and Lemmatization","d49d5f80":"# TF-IDF (Inverse Document Frequency)","ad0c2ab5":"A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.","dabd374c":"# **Count Vectorization**","89a6906f":"# Porter Stemmer","38c1093b":"# N-Gram Vectorization","8f3e2c0c":"# WordNet lemmatizer","92ccd48f":"# **CLEANING THE TEXT**","0e78214a":"# Tokenization","39019443":"# Removing punctuation and numbers to avoid confusion.","020fd4de":"# Removing Stop Words"}}