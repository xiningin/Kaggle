{"cell_type":{"1e68e9ed":"code","5396efaa":"code","434bd9f9":"code","9bcbae43":"code","93454162":"code","a063ff23":"code","afaf0443":"code","9bf49d93":"code","7ff5e0af":"code","0b06ad36":"code","6cbaa7f0":"code","916db03a":"code","b06a643d":"code","d25198d1":"code","d3f72d4f":"code","88625473":"code","550d1ac1":"code","ac44be17":"code","047d2438":"code","e72137de":"code","3cab4a56":"code","b71ab87b":"code","01eebaca":"code","17a01279":"code","e4075c42":"code","89cb3b35":"code","c702a290":"code","182b44d9":"code","99204b9e":"code","cabd5a21":"code","23c890c1":"code","5dfc2f6b":"code","93a1f7c7":"code","821be943":"code","d549918a":"code","14d59deb":"code","9ebf0b50":"code","641ba194":"code","3ab6d906":"code","9bd251d3":"code","b9329162":"code","6637ec11":"code","9c2d7b33":"code","e2d11a3a":"code","b6b9ef7a":"code","b87632e7":"markdown","649e543b":"markdown","a628c300":"markdown","f42853ed":"markdown","39fc7c03":"markdown","ca174ccc":"markdown","5b028516":"markdown","bc10b82b":"markdown","83d79cfb":"markdown","916dac74":"markdown","f92f39fd":"markdown","effba4d3":"markdown","7aad3d06":"markdown","a72e66f6":"markdown","ac42af29":"markdown","f3238bed":"markdown","39cb9b0b":"markdown","11ffe750":"markdown","8ad662c4":"markdown","ff4547b5":"markdown","948cd4fb":"markdown","14ea1c24":"markdown","033e79ad":"markdown","8450713d":"markdown","8310f3c8":"markdown","c65bdee0":"markdown"},"source":{"1e68e9ed":"import os\nos.listdir('.\/')","5396efaa":"# Extractig the training data\nimport zipfile\nfrom tqdm import tqdm\nwith zipfile.ZipFile(\"..\/input\/dogs-vs-cats\/train.zip\") as zf:\n    for member in tqdm(zf.infolist(), desc='Extracting '):\n        try:\n            zf.extract(member, \".\/\")\n        except zipfile.error as e:\n            pass","434bd9f9":"# Extracting the testing data\nwith zipfile.ZipFile(\"..\/input\/dogs-vs-cats\/test1.zip\") as zf:\n    for member in tqdm(zf.infolist(), desc='Extracting '):\n        try:\n            zf.extract(member, \".\/\")\n        except zipfile.error as e:\n            pass","9bcbae43":"import os\nos.listdir('.\/')","93454162":"# create separate list to store target class (0 or 1) and respective image path\ntrain_filenames=os.listdir('.\/train')\npath=[]\ntarget=[]\nfor i in train_filenames:\n    temp=i.split('.')[0]\n    if temp=='cat':\n        target.append(0)\n    else:\n        target.append(1)\n    path.append('.\/train\/'+i)","a063ff23":"import pandas as pd\ndata={'Image': path,'Target': target}\ndf=pd.DataFrame(data)\ndf.head()\n","afaf0443":"df.describe()","9bf49d93":"import matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport random\nfig = plt.figure(figsize=(10,10))\n\nrandom.seed(15)\nrandom_list=random.sample(range(0,24999),9)\nfor i,index in enumerate(random_list):\n    plt.subplot(330+1+i)\n    file_path=df.loc[index]['Image']\n    img=imread(file_path)\n    plt.imshow(img)\nplt.show()","7ff5e0af":"import seaborn as sns\nax=sns.countplot(data=df,x='Target')","0b06ad36":"import sklearn.model_selection as sk\nimport numpy as np\n\ntrain_x, val1_x, train_y, val1_y = sk.train_test_split(path,target,test_size=0.4, random_state=42)\nval_x,test_x,val_y,test_y=sk.train_test_split(val1_x,val1_y,test_size=0.5, random_state=42)\nprint('Training data:', len(train_x))\nprint('Training Label:', len(train_y))\nprint('Validation Data:',len(val_x))\nprint('Validation Label:', len(val_y))\nprint('Test Data:',len(test_x))\nprint('Test Label:', len(test_y))\nprint('-------------------------------------------')\nprint('Total Data:', len(train_x)+len(val_x)+len(test_x))","6cbaa7f0":"import cv2\nfrom albumentations import (\n    Compose, ToFloat\n)","916db03a":"AUGMENTATIONS_TRAIN = Compose([\n    ToFloat(max_value=255)\n])","b06a643d":"AUGMENTATIONS_TEST = Compose([\n    # CLAHE(p=1.0, clip_limit=2.0),\n    ToFloat(max_value=255)\n])","d25198d1":"from tensorflow.keras.utils import Sequence\nimport cv2","d3f72d4f":"# for training data\nclass Cat_Dog_Sequence(Sequence):\n    def __init__(self, x_set, y_set, batch_size, augmentations, shape):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.augment = augmentations\n        self.shape = shape\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.stack([\n            self.augment(image=cv2.resize(cv2.imread(x, 1), self.shape))[\"image\"] for x in batch_x\n        ], axis=0), np.array(batch_y)","88625473":"#for Testing data\nclass Cat_Dog_Test_Sequence(Sequence):\n    def __init__(self, x_set,batch_size, augmentations, shape):\n        self.x = x_set\n        self.batch_size = batch_size\n        self.augment = augmentations\n        self.shape = shape\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        #batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.stack([\n            self.augment(image=cv2.resize(cv2.imread(x, 1), self.shape))[\"image\"] for x in batch_x\n        ], axis=0)","550d1ac1":"train_y=np.array(train_y).reshape((len(train_y),1))\nval_y=np.array(val_y).reshape((len(val_y),1))","ac44be17":"# Initialise the data generators\ntrain_gen=Cat_Dog_Sequence(train_x,train_y,16, augmentations=AUGMENTATIONS_TRAIN,shape=(312,312))\nval_gen=Cat_Dog_Sequence(val_x,val_y,16, augmentations=AUGMENTATIONS_TEST,shape=(312,312))","047d2438":"from tensorflow.keras.applications import InceptionResNetV2\nmodel=InceptionResNetV2(include_top=False, weights='imagenet',input_shape=[312,312,3])","e72137de":"model.summary()","3cab4a56":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras import Model\nx=model.output\n\n#x=Flatten(name='Flatten')(x)\nx=GlobalAveragePooling2D(name='G_avg_pooling')(x)\n'''x=Dense(1000,activation=('relu'),name=\"top_activation_2\")(x)\nx=Dropout(0.4, name=\"top_dropout2\")(x)\n'''\nx=Dense(256,activation=('relu'), name=\"top_activation_3\")(x)\nx=Dropout(0.3, name=\"top_dropout3\")(x)\n\nx=Dense(1,activation=\"sigmoid\", name=\"pred\")(x)\n\nfinal_model=Model(model.input,x)","b71ab87b":"final_model.summary()","01eebaca":"from tensorflow.keras import losses, optimizers, callbacks\nfile_path='\/kaggle\/working\/inception_resnet_v2.h5'\n\nlr_schedule=callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=2, min_lr=0.0001,verbose=1)\ncheckpoint= callbacks.ModelCheckpoint(file_path, monitor='val_loss',verbose=1,save_best_only=True)\nearlystopping= callbacks.EarlyStopping(monitor='val_loss',patience=3,verbose=1)\nfinal_model.compile(optimizer= optimizers.SGD(learning_rate=0.01),loss='binary_crossentropy', metrics=['accuracy'])","17a01279":"history=final_model.fit(train_gen, epochs=5, callbacks=[lr_schedule,checkpoint,earlystopping],validation_data=val_gen,verbose=1,shuffle=False)","e4075c42":"os.listdir('.\/')","89cb3b35":"import numpy as np\nplt.plot(history.history['loss'], color='b', label=\"Training loss\")\nplt.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nplt.xticks(np.arange(1, 2, 1))\nplt.yticks(np.arange(0, 1, 0.1))\nplt.legend()\nplt.title('Training Loss VS Validation Loss')\nplt.show()","c702a290":"plt.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.xticks(np.arange(1, 2, 1))\nplt.title('Training Accuracy VS Validation Accuracy')\nplt.legend()f\nplt.show()","182b44d9":"#load our model weights\nfinal_model.load_weights('.\/inception_resnet_v2.h5')","99204b9e":"#change the test data to an array\ntest_y=np.array(test_y).reshape((len(test_y),1))","cabd5a21":"#use generators to convert the test data\ntest_gen=Cat_Dog_Sequence(test_x,test_y,16, augmentations=AUGMENTATIONS_TRAIN,shape=(312,312))","23c890c1":"# calculate loss and accuracy for our model\nloss,acc=final_model.evaluate(test_gen,verbose=1)","5dfc2f6b":"print('Accuracy for our test_data:',str(acc*100)+'%')","93a1f7c7":"from sklearn.metrics import classification_report\n\ntest_gen=Cat_Dog_Test_Sequence(test_x, 16, augmentations=AUGMENTATIONS_TEST,shape=(312,312))\noutput=final_model.predict(test_gen,verbose=1)\noutput=output.round().astype(np.int32).reshape(output.shape[0])\ntarget_names=('Cat','Dog')\nprint(classification_report(test_y,output, target_names=target_names))\n\n## According to f1 score, We conclude this as a good model","821be943":"class Cat_Dog_Test_Sequence(Sequence):\n    def __init__(self, x_set,batch_size, augmentations, shape):\n        self.x = x_set\n        self.batch_size = batch_size\n        self.augment = augmentations\n        self.shape = shape\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        #batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.stack([\n            self.augment(image=cv2.resize(cv2.imread(x, 1), self.shape))[\"image\"] for x in batch_x\n        ], axis=0)","d549918a":"# create separate list to store target class (0 or 1) and respective image path\ntest_filenames=os.listdir('.\/test1')\ntest_path=[]\nid=[]\nfor i in test_filenames:\n    temp=i.split('.')[0]\n    id.append(temp)\n    test_path.append('.\/test1\/'+i)","14d59deb":"print('No.of.Test Images:',len(test_path))","9ebf0b50":"test_gen=Cat_Dog_Test_Sequence(test_path, 16, augmentations=AUGMENTATIONS_TEST,shape=(312,312))","641ba194":"output=final_model.predict(test_gen,verbose=1)","3ab6d906":"output.shape","9bd251d3":"output=output.round().astype(np.int32).reshape(output.shape[0])","b9329162":"output.shape","6637ec11":"output_csv={'id':id,'label':output}\no=pd.DataFrame(output_csv)","9c2d7b33":"o.to_csv('submission.csv',index=False)","e2d11a3a":"import os\nos.listdir('.\/')","b6b9ef7a":"!jupyter nbconvert --execute --to pdf __notebook_source__.ipynb","b87632e7":"## **Visualisation**","649e543b":"### **Extracting the train and test data**","a628c300":"### **Test the model and submit the results in submission.csv (Optional)**","f42853ed":"## **Cat vs Dog Classification**\n**Note:** Libraires are imported at the start of each section for my convenience","39fc7c03":"### **Check the class imbalancing**","ca174ccc":"### **Split the dataset to train(60%),validation(20%),test(20%)**","5b028516":"### **Data Generators inherits from keras.sequence class**\n> \n> **Our dataset is too large to fit in memory, we have to load the dataset from the hard disk in batches to our memory.**\n>  \n> * `keras.sequence` class is inherited in `Cat_Dog_Sequence` to split the data to fit into the memory dynamically.\n> * Split the data into batches(for each batch 16 images).\n> * Resize the Image to 312*312 \n","bc10b82b":"#### **Model with 98.92% of accuracy**","83d79cfb":"#### **Add a head to our classification**","916dac74":"### **Workflow**","f92f39fd":"#### **Training Accuracy vs Validation Accuracy**","effba4d3":"### **Preprocessing of the image**","7aad3d06":"#### ***Note: Dataset is Balanced***","a72e66f6":"### **Inference**\n\n> * Due to Imagenet Pretrained weights, our model reach the  training accuracy of 0.9993 at third epochs, Due to the model checkpoint callback we took the epochs with low validation loss as our model weights.\n> * We have large dataset. So, We get this accuracy.\n> * We Need not to consider as a overfitting.","ac42af29":"#### **Test our model with our splitted test data (because csv submission is diabled)**","f3238bed":"### **write the datafame with each image path and respective image class (0 or 1)**","39cb9b0b":"####  ***Note: Not all the images are of same sizes***","11ffe750":"#### **Checking our model using precision and recall other accuracy**","8ad662c4":"### **Intialise the callbacks for our Model**\n\n* `ModelCheckpoint` is used to save the best weights with low validation loss.\n* `EarlyStopping` is used for monitoring validation loss for 2 epochs\n* `ReduceLROnPlateau`(Reduce Learning Rate) is used if validation loss decreases.","ff4547b5":"#### **Fit the inception resnet v2 model for our training data**\n\n* Due to the use pretrained weights, reduce the epochs to 5\n","948cd4fb":"### **Check all the images are of same size using simple visualisation**","14ea1c24":"#### **Training loss vs validation loss**","033e79ad":"### **Inception Resnet v2 model is used from keras.application to get the pretrained weights**\n\n> Pretrained Imagenet weights of InceptionResnetv2 is used.","8450713d":"#### **According to f1 score, We conclude this as a good model.**","8310f3c8":"![WORKFLOW (2).jpg](attachment:1d710f34-06ac-4b63-9037-900619405b3c.jpg)","c65bdee0":"### **Changing all the pixel values to float**\n\n> I preprocessed using albumentation, If data quantity is low we can include augmentation inside `AUGMENTATIONS_TRAIN` and use it for a real time augmentation."}}