{"cell_type":{"3dcf2aa8":"code","93de9de7":"code","dcffb520":"code","13711cfb":"code","562503fc":"code","802efa3d":"code","ea308c15":"code","09bdf9ef":"code","529e6797":"code","f95607c9":"code","4291d0b5":"code","db05ac38":"code","c8011cdf":"code","336b13d6":"code","b4c42241":"code","3498dca0":"code","5f1f2118":"code","31dc5442":"code","0559620a":"markdown","5f10980d":"markdown","aacb726a":"markdown","ab1ed3ee":"markdown","40475446":"markdown","98b12512":"markdown","3dbbeea9":"markdown","d4a7b0f9":"markdown","36d34121":"markdown","9921e338":"markdown","cd27e9a6":"markdown","a7f8aac9":"markdown","a590665d":"markdown","fcbe7053":"markdown","ff1e19a9":"markdown","3e7e1137":"markdown","d892ad8f":"markdown","3cbfef3c":"markdown","a99194b9":"markdown"},"source":{"3dcf2aa8":"# install additional library\n!pip install tldextract -q\n\n# import library\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport gc\nimport random\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.python.util import deprecation\nfrom urllib.parse import urlparse\nimport tldextract\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import models, layers, backend, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# set random seed\nos.environ['PYTHONHASHSEED'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nnp.random.seed(0)\nrandom.seed(0)\ntf.random.set_seed(0)\n\n# other setup\n%config InlineBackend.figure_format = 'retina'\npd.set_option('max_colwidth', 50)\npio.templates.default = \"presentation\"\npd.options.plotting.backend = \"plotly\"\ndeprecation._PRINT_DEPRECATION_WARNINGS = False","93de9de7":"# load data\ndata = pd.read_csv('..\/input\/data.csv')\ndata.head()","dcffb520":"val_size = 0.2\ntrain_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\nfig = go.Figure([go.Pie(labels=['Train Size', 'Validation Size'], values=[train_data.shape[0], val_data.shape[0]])])\nfig.update_layout(title='Train and Validation Size')\nfig.show()","13711cfb":"fig = go.Figure([go.Pie(labels=['Good', 'Bad'], values=data.label.value_counts())])\nfig.update_layout(title='Percentage of Class (Good and Bad)')\nfig.show()","562503fc":"def parsed_url(url):\n    # extract subdomain, domain, and domain suffix from url\n    # if item == '', fill with '<empty>'\n    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n    \n    return [subdomain, domain, domain_suffix]\n\ndef extract_url(data):\n    # parsed url\n    extract_url_data = [parsed_url(url) for url in data['url']]\n    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n    \n    # concat extracted feature with main data\n    data = data.reset_index(drop=True)\n    data = pd.concat([data, extract_url_data], axis=1)\n    \n    return data\n\ndef get_frequent_group(data, n_group):\n    # get the most frequent\n    data = data.value_counts().reset_index(name='values')\n    \n    # scale log base 10\n    data['values'] = np.log10(data['values'])\n    \n    # calculate total values\n    # x_column (subdomain \/ domain \/ domain_suffix)\n    x_column = data.columns[1]\n    data['total_values'] = data[x_column].map(data.groupby(x_column)['values'].sum().to_dict())\n    \n    # get n_group data order by highest values\n    data_group = data.sort_values('total_values', ascending=False).iloc[:, 1].unique()[:n_group]\n    data = data[data.iloc[:, 1].isin(data_group)]\n    data = data.sort_values('total_values', ascending=False)\n    \n    return data\n\ndef plot(data, n_group, title):\n    data = get_frequent_group(data, n_group)\n    fig = px.bar(data, x=data.columns[1], y='values', color='label')\n    fig.update_layout(title=title)\n    fig.show()\n\n# extract url\ndata = extract_url(data)\ntrain_data = extract_url(train_data)\nval_data = extract_url(val_data)","802efa3d":"fig = go.Figure([go.Bar(\n    x=['domain', 'Subdomain', 'Domain Suffix'], \n    y = [data.domain.nunique(), data.subdomain.nunique(), data.domain_suffix.nunique()]\n)])\nfig.show()","ea308c15":"plot(\n    data=data.groupby('label')['domain'], \n    n_group=20, \n    title='Top 20 Domains Grouped By Labels (Logarithmic Scale)'\n)","09bdf9ef":"data[(data['domain'] == 'google') & (data['label'] == 'bad')].head()","529e6797":"plot(\n    data=data.groupby('label')['subdomain'], \n    n_group=20, \n    title='Top 20 Sub Domains Grouped By Labels (Logarithmic Scale)'\n)","f95607c9":"plot(\n    data=data.groupby('label')['domain_suffix'], \n    n_group=20, \n    title='Top 20 Domains Suffix Grouped By Labels (Logarithmic Scale)'\n)   ","4291d0b5":"tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n\n# fit only on training data\ntokenizer.fit_on_texts(train_data['url'])\nn_char = len(tokenizer.word_index.keys())\n\ntrain_seq = tokenizer.texts_to_sequences(train_data['url'])\nval_seq = tokenizer.texts_to_sequences(val_data['url'])\n\nprint('Before tokenization: ')\nprint(train_data.iloc[0]['url'])\nprint('\\nAfter tokenization: ')\nprint(train_seq[0])","db05ac38":"sequence_length = np.array([len(i) for i in train_seq])\nsequence_length = np.percentile(sequence_length, 99).astype(int)\nprint(f'Before padding: \\n {train_seq[0]}')\ntrain_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\nval_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\nprint(f'After padding: \\n {train_seq[0]}')","c8011cdf":"unique_value = {}\nfor feature in ['subdomain', 'domain', 'domain_suffix']:\n    # get unique value\n    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n    \n    # add unknown label in last index\n    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n    \n    # count unique value\n    unique_value[feature] = label_index['<unknown>']\n    \n    # encode\n    train_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in train_data.loc[:, feature]]\n    val_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in val_data.loc[:, feature]]\n    \ntrain_data.head()","336b13d6":"for data in [train_data, val_data]:\n    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]\n    \ntrain_data.head()","b4c42241":"def convolution_block(x):\n    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n    conv_layer = layers.Flatten()(conv_layer)\n    return conv_layer\n\ndef embedding_block(unique_value, size, name):\n    input_layer = layers.Input(shape=(1,), name=name + '_input')\n    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n    return input_layer, embedding_layer\n\ndef create_model(sequence_length, n_char, unique_value):\n    input_layer = []\n    \n    # sequence input layer\n    sequence_input_layer = layers.Input(shape=(sequence_length,), name='url_input')\n    input_layer.append(sequence_input_layer)\n    \n    # convolution block\n    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n    conv_layer = convolution_block(char_embedding)\n    \n    # entity embedding\n    entity_embedding = []\n    for key, n in unique_value.items():\n        size = 4\n        input_l, embedding_l = embedding_block(n + 1, size, key)\n        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n        input_layer.append(input_l)\n        entity_embedding.append(embedding_l)\n        \n    # concat all layer\n    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n    \n    # dense layer\n    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n    \n    # output layer\n    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n    return model\n\n# reset session\nbackend.clear_session()\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(0)\nrandom.seed(0)\ntf.random.set_seed(0)\n\n# create model\nmodel = create_model(sequence_length, n_char, unique_value)\n\n# show model architecture\nplot_model(model, to_file='model.png')\nmodel_image = mpimg.imread('model.png')\nplt.figure(figsize=(75, 75))\nplt.imshow(model_image)\nplt.show()","3498dca0":"# create train data\ntrain_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\ntrain_y = train_data['label'].values\n\n# model training\nearly_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\nhistory = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_split=0.2, shuffle=True, callbacks=early_stopping)\nmodel.save('model.h5')","5f1f2118":"fig = make_subplots(3, 1, subplot_titles=('loss', 'precision', 'recall'))\n\nfor index, key in enumerate(['loss', 'precision', 'recall']):\n    # train score\n    fig.add_trace(go.Scatter(\n        x=list(range(len(history.history[key]))),\n        y=history.history[key],\n        mode='lines+markers',\n        name=key\n    ), index + 1, 1)\n    \n    # val score\n    fig.add_trace(go.Scatter(\n        x=list(range(len(history.history[f'val_{key}']))),\n        y=history.history[f'val_{key}'],\n        mode='lines+markers',\n        name=f'val {key}'\n    ), index + 1, 1)\n\nfig.show()","31dc5442":"val_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\nval_y = val_data['label'].values\n\nval_pred = model.predict(val_x)\nval_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\nprint(f'Validation Data:\\n{val_data.label.value_counts()}')\nprint(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\nprint(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')","0559620a":"# Conclusion\n\nIn conclusion, the model that has been trained has a high precision and recall value but what must be considered is the precision value. The precision value must be high because if it is low then a website that is not malicious has the possibility to be classified as malicious\n\n\nThank you for reading my notebook :)","5f10980d":"# MALICIOUS WEB DETECTION WITH 1D CNN (Convolution Neural Network)\nAuthor: ** Rakha Paleva Kawiswara ** <br>\n\nFeel free to use this notebook for your research. I'm very appreciate if you give credit and upvotes to this notebook :). Suggestion on this notebook is very expected, Thanks!\n\n**nb <br>*\n*psst... do not open the url contained in the data, to avoid opening dangerous websites*\n\n------------------------------------------------------------------------------------------------------------------------------\n\n# What is Malicious Website?\nA malicious website is a site that attempts to install malware (a general term for anything that will disrupt computer operation, gather your personal information or, in a worst-case scenario, gain total access to your machine) onto your device. This usually requires some action on your part, however, in the case of a drive-by download, the website will attempt to install software on your computer without asking for permission first. (source: https:\/\/us.norton.com\/internetsecurity-malware-what-are-malicious-websites.html)\n# Notebook Goals\nThis notebook will create a model that can detect malicious websites. Website url is used as a feature and 1D Convolutional Neural Network (CNN) is used as an algorithm for detection malicious websites. Model will be validated by holdout method","aacb726a":"# Model Training","ab1ed3ee":"As you can see from the plot above, most of them contains bad labels even there are some that aren't\n\nThe next step we need to do tokenization on the url so that it can be used as input to the CNN model","40475446":"Maybe some of there urls contain malware, who knows :)\n\nLets plot other features like sub domain and domain suffix","98b12512":"Each url has a different length, therefore padding is needed to equalize each url length. Next step we will do padding on url that we already have tokenize","3dbbeea9":"Based on the plot above there are interesting things to note, there are some famous domain that don't contain bad labels like linkedin, wikipedia, youtube, amazon, imdb and myspace and there are many famous domain that contains bad labels for example google. Let's take a sample on google domain to see a bad labels","d4a7b0f9":"Lets check how many unique Domain, Subdomain and Domain Suffix that we have extracted","36d34121":"First lets load malicious website data","9921e338":"\nLet me explain to you about the model that has been made. The model received 4 inputs, the first input came from URL that has been done tokenization and padding. Other inputs are subdomains, domains and suffix domains that have been encoded. URL input will pass through embedding layer and convolution layer while other input will pass embedding layer. Then the results from each input will be concatenated.","cd27e9a6":"# Data Analysis and Feature Engineering\nLet's do some data analysis to expand our knowledge of this data and do some feature engineering. First we want to find out whether the data is imbalance","a7f8aac9":"We will also encode subdomain, domain, suffix domains and label into numerical variables","a590665d":"# Model Validation","fcbe7053":"Next, lets plot domain feature","ff1e19a9":"In this notebook the validation method used is the holdout method. The holdout method is a method that separates training and test data by 80% and 20%","3e7e1137":"The next step is to encode the target variable (label) to numeric, for example the bad label becomes 1 and the good label becomes 0","d892ad8f":"# Load Data","3cbfef3c":"# Create CNN Model","a99194b9":"As you can see from the plot above, 82% have good labels while 18% have bad labels. \n\nNext, lets find out the most used suffix domain, domain and sub domain. We need to extract subdomains, domains and domain suffixes to be able to do the analysis"}}