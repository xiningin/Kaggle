{"cell_type":{"f9d64df0":"code","6f691c92":"code","1a91cf69":"code","027e7ce8":"code","da9e977a":"code","b672a039":"code","4e1a33a3":"code","3193bdaa":"code","bdb6d3a5":"code","d0c383c9":"code","7205c07e":"code","77637cdf":"code","6018352f":"code","3eeffb44":"code","5e5d9a25":"code","ba07eadf":"code","1ab1cfb1":"code","5c8d842a":"code","97267541":"code","6a00dd30":"code","628430e9":"code","8bd24d7b":"code","c9ece532":"code","b84d40fd":"code","d0b58beb":"code","7841b7b5":"code","e55dc90a":"code","616f20d5":"code","ecf258ef":"code","92a8d3a7":"code","1168733b":"code","7d335bf3":"code","bc87681e":"code","583bb40a":"code","def306b1":"code","7258bbff":"code","28274f95":"code","ee34a1e8":"code","85eebc89":"code","21fa4acf":"code","3eb0e7d6":"code","d8d7afb9":"markdown","a0dd2697":"markdown","b1d12f8d":"markdown"},"source":{"f9d64df0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nfrom catboost import CatBoostClassifier\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6f691c92":"sc = StandardScaler()\nle = LabelEncoder()\nonehot = OneHotEncoder(sparse=False)","1a91cf69":"# Reading train.csv\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","027e7ce8":"train_df.describe()","da9e977a":"sns.countplot(train_df['Target'])\nplt.show()","b672a039":"# Reading test.csv file\ntest_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","4e1a33a3":"# Combining both train and test file into one. This will help us preprocessing both files simultaneously, and after we are done with that, we can seperate both.\nall_df = train_df.append(test_df, sort=False)\nall_df.shape","3193bdaa":"# Checking fraction of null values in each feature column (ignore the Target variable as its null for the test.csv file)\nmissing_vals = (all_df.isnull().sum() \/ len(all_df)).sort_values(ascending=False)\nmissing_vals = missing_vals[missing_vals > 0]\nmissing_vals = missing_vals.to_frame()\nmissing_vals.columns = ['count']\nmissing_vals.index.names = ['Name']\nmissing_vals['Name'] = missing_vals.index\n\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Name', y = 'count', data=missing_vals)\nplt.xticks(rotation = 90)\nplt.show()","bdb6d3a5":"# Dropping columns having too much null values. Filling null values by interpolating in such feature columns can lead to misleading data, so its better to drop them.\n# Filling feature columns with too little null values with their median values, as the Target classes are imbalanced, its a good idea to replace null values with\n# median values rather than mean values\nall_df.drop(['rez_esc', 'v18q1', 'v2a1'], axis=1, inplace=True)\nall_df.fillna({'SQBmeaned': all_df['SQBmeaned'].median(), 'meaneduc': all_df['meaneduc'].median()}, inplace=True)","d0c383c9":"# dividing feature columns according to their dtypes, so that we can visualize them further\nfloat_cols = [col for col in all_df.columns if all_df[col].dtype=='float64']\nint_cols = [col for col in all_df.columns if all_df[col].dtype=='int64']\nobject_cols = [col for col in all_df.columns if all_df[col].dtype=='object']","7205c07e":"del(float_cols[-1])\nfloat_flat = pd.melt(all_df, value_vars=float_cols)\ng = sns.FacetGrid(float_flat, col='variable', col_wrap=5, sharex=False, sharey=False)\ng = g.map(sns.distplot, 'value')\nplt.show()","77637cdf":"# Log transforming float-type feature columns to remove their skewness using log(1+x)\nlog_meaneduc = np.log1p(all_df['meaneduc'])\nlog_overcrowding = np.log1p(all_df['overcrowding'])\nlog_SQBovercrowding = np.log1p(all_df['SQBovercrowding'])\nlog_SQBdependency = np.log1p(all_df['SQBdependency'])\nlog_SQBmeaned = np.log1p(all_df['SQBmeaned'])\n\ntemp_df = pd.DataFrame({'log_meaneduc': log_meaneduc, 'log_overcrowding': log_overcrowding, 'log_SQBovercrowding': log_SQBovercrowding, 'log_SQBdependency': log_SQBdependency, 'log_SQBmeaned': log_SQBmeaned})\ntemp_df.head()","6018352f":"temp_df.describe()","3eeffb44":"# Converting all log-transformed variables to categorical variables according to their distribution as given in above temp_df.describe() cell\ntemp_df['log_meaneduc'] = pd.cut(temp_df['log_meaneduc'], [0.0, 1.945910, 2.268684, 2.525729, 3.637586], labels=[1, 2, 3, 4], include_lowest=True)\ntemp_df['log_overcrowding'] = pd.cut(temp_df['log_overcrowding'], [0.133531, 0.693147, 0.916291, 1.098612, 2.639057], labels=[1, 2, 3, 4], include_lowest=True)\ntemp_df['log_SQBovercrowding'] = pd.cut(temp_df['log_SQBovercrowding'], [0.020203, 0.693147, 1.178655, 1.609438, 5.135798], labels=[1, 2, 3, 4], include_lowest=True)\ntemp_df['log_SQBdependency'] = pd.cut(temp_df['log_SQBdependency'], [0.0, 0.105361, 0.367725, 1.021651, 4.174387], labels=[1, 2, 3, 4], include_lowest=True)\ntemp_df['log_SQBmeaned'] = pd.cut(temp_df['log_SQBmeaned'], [0.0, 3.610918, 4.332194, 4.892227, 7.222566], labels=[1, 2, 3, 4], include_lowest=True)","5e5d9a25":"# Converting float variables to categorical variables introduces some nan values (don't know the reason), so I replaced them with the max values. Then, I replaced the\n# original feature variables in all_df with these categorical variables. \ntemp_df.fillna({'log_meaneduc': 2, 'log_overcrowding': 2, 'log_SQBovercrowding': 2, 'log_SQBdependency': 1, 'log_SQBmeaned': 4}, inplace=True)\nall_df[['meaneduc', 'overcrowding', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned']] = temp_df[['log_meaneduc', 'log_overcrowding', 'log_SQBovercrowding', 'log_SQBdependency', 'log_SQBmeaned']]","ba07eadf":"# Visualizing integer feature columns\nint_flat = pd.melt(all_df, value_vars=int_cols)\ng = sns.FacetGrid(int_flat, col='variable', col_wrap=6, sharex=False, sharey=False)\ng = g.map(sns.countplot, 'value')\nplt.xticks(rotation=90)\nplt.show()","1ab1cfb1":"# Removing Id feature from list and visualizing object feature columns\ndel(object_cols[0])\nobject_flat = pd.melt(all_df, value_vars=object_cols)\ng = sns.FacetGrid(object_flat, col='variable', col_wrap=4, sharex=False, sharey=False)\ng = g.map(sns.countplot, 'value')\nplt.show()","5c8d842a":"# Encoding feature columns of 'object' dtype\nle = LabelEncoder()\nfor col in object_cols:\n    all_df[col] = le.fit_transform(all_df[col].values)","97267541":"# Removing squared feature columns as these are having same distribution as the original variables\ndup_cols = [col for col in all_df.columns if col[:3] == 'SQB']\nall_df.drop(dup_cols, axis=1, inplace=True)\nall_df.drop('agesq', axis=1, inplace=True)\nall_df.shape","6a00dd30":"# Onehot-encoding feature variables having one prominent category\nall_df['edjefe'] = (all_df['edjefe'] == all_df['edjefe'].max()) * 1\nall_df['edjefa'] = (all_df['edjefa'] == all_df['edjefa'].max()) * 1\n\n# Converting age and idhogar to float variables by scaling them with 0 mean and unit standard deviation\nall_df['age'] = sc.fit_transform(all_df['age'].values.reshape((-1, 1)))\nall_df['idhogar'] = sc.fit_transform(all_df['idhogar'].values.reshape((-1, 1)))","628430e9":"# Updating cat_cols list and then onehot-encoding all columns having more than 2 unique values\n\ncat_cols = [col for col in int_cols if col not in [col for col in int_cols if col[:3]=='SQB']]\ncat_cols.remove('agesq')\ncat_cols.remove('age')\n\nonehot_cols = [col for col in cat_cols if len(all_df[col].unique()) > 2]\nonehot_arr = onehot.fit_transform(all_df[onehot_cols].values)\nonehot_arr.shape","8bd24d7b":"all_df.drop(onehot_cols, axis=1, inplace=True)\nall_df.shape","c9ece532":"# Dividing the whole dataframe into train and test dataframes\ntrain_df = all_df[all_df['Target'].notnull()]\ntest_df = all_df[all_df['Target'].isnull()]\nprint (train_df.shape, test_df.shape)","b84d40fd":"# We have to reduce the target value of each class by 1, otherwise XgBoost thinks its training on 5 classes, since highest class is 4. We will undo this change\n# after prediction\ntrain_df['Target'] = train_df['Target'].apply(lambda x: x-1)","d0b58beb":"# Creating train array and test array from train dataframe and test dataframe and also concatenating onehot array\ntr_cols = [col for col in train_df.columns if col not in ['Id', 'Target']]\nX_train = train_df[tr_cols].values\nX_train = np.concatenate((X_train, onehot_arr[:9557, :]), axis=1)\ny_train = train_df['Target'].values\n\nte_cols = [col for col in test_df.columns if col not in ['Id', 'Target']]\nX_test = test_df[te_cols].values\nX_test = np.concatenate((X_test, onehot_arr[9557:, :]), axis=1)","7841b7b5":"# Reducing dimensionality of complete array to 2-dimensions(for visualizing) using t-distributed Stochastic Neighbor Embedding algorithm, so that we can visualize how\n# different models are predicting on test set.\nall_arr = np.concatenate((X_train, X_test))\ntsne = TSNE(n_components=2)\nall_tsne_arr = tsne.fit_transform(all_arr)\nall_tsne_arr.shape","e55dc90a":"# Declaring class weights as the 4 classes are imbalanced\nclass_weights = compute_class_weight('balanced', np.sort(train_df['Target'].unique()), train_df['Target'].values)","616f20d5":"# Initializing CatBoost classifier, fitting and then predicting\ncat_model = CatBoostClassifier(iterations=500, learning_rate=0.3, depth=5, loss_function='MultiClass', classes_count=4, logging_level='Silent', l2_leaf_reg=2, thread_count=4, class_weights=class_weights)\ncat_model.fit(X_train, y_train)\ncat_preds = cat_model.predict(X_test)\ncat_preds = cat_preds.reshape((-1,)).astype(int)","ecf258ef":"# Initializing Random Forest classifier, fitting and then predicting\nrfc_clf = RandomForestClassifier(n_estimators=70, max_depth=5, max_features=0.8, n_jobs=4, class_weight='balanced')\nrfc_clf.fit(X_train, y_train)\nrfc_preds = rfc_clf.predict(X_test).astype(int)","92a8d3a7":"# Initializing Adam Boost classifier, fitting and then predicting\nada_clf = AdaBoostClassifier(n_estimators=70, learning_rate=0.3)\nada_clf.fit(X_train, y_train, sample_weight=[class_weights[int(y_train[i])] for i in range(y_train.shape[0])])\nada_preds = ada_clf.predict(X_test).astype(int)","1168733b":"# Initializing Bernoulli naive-bayes classifier, fitting and then predicting\nbernoulli_clf = BernoulliNB()\nbernoulli_clf.fit(X_train, y_train, sample_weight=[class_weights[int(y_train[i])] for i in range(y_train.shape[0])])\nbernoulli_preds = bernoulli_clf.predict(X_test).astype(int)","7d335bf3":"# Initializing Gaussian naive-bayes classifier, fitting and then predicting\ngaussian_clf = GaussianNB()\ngaussian_clf.fit(X_train, y_train, sample_weight=[class_weights[int(y_train[i])] for i in range(y_train.shape[0])])\ngaussian_preds = gaussian_clf.predict(X_test).astype(int)","bc87681e":"# Initializing KNN classifier, fitting and then predicting\nknn_clf = KNeighborsClassifier(n_neighbors=8, weights='uniform', n_jobs=4)\nknn_clf.fit(X_train, y_train)\nknn_preds = knn_clf.predict(X_test).astype(int)","583bb40a":"# Initializing Multilayer Perceptron, fitting and then predicting\nmlp_clf = MLPClassifier(hidden_layer_sizes=(50), batch_size=50, learning_rate='constant', learning_rate_init=0.0005, early_stopping=True)\nmlp_clf.fit(X_train, y_train)\nmlp_preds = mlp_clf.predict(X_test).astype(int)","def306b1":"# Visualizing all predictions from above models using a scatter-plot so that we can also see the differences between their predictions\nplt.figure(figsize=(30, 30))\n\nplt.subplot(421)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=cat_preds)\nplt.colorbar()\nplt.title('Catboost predictions')\nplt.grid(True)\n\nplt.subplot(422)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=rfc_preds)\nplt.colorbar()\nplt.title('RandomForest predictions')\nplt.grid(True)\n\nplt.subplot(423)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=ada_preds)\nplt.colorbar()\nplt.title('Adaboost predictions')\nplt.grid(True)\n\nplt.subplot(424)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=bernoulli_preds)\nplt.colorbar()\nplt.title('Bernoulli predicitons')\nplt.grid(True)\n\nplt.subplot(425)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=gaussian_preds)\nplt.colorbar()\nplt.title('Gaussian predictions')\nplt.grid(True)\n\nplt.subplot(426)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=knn_preds)\nplt.colorbar()\nplt.title('KNN predictions')\nplt.grid(True)\n\nplt.subplot(427)\nplt.scatter(all_tsne_arr[9557:, 0], all_tsne_arr[9557:, 1], c=mlp_preds)\nplt.colorbar()\nplt.title('Multi-layer Perceptron predictions')\nplt.grid(True)\n\nplt.show()","7258bbff":"# Combining all predictions and creating a heatplot of their correlations\nall_preds = np.concatenate((cat_preds.reshape((-1, 1)), rfc_preds.reshape((-1, 1))), axis=1)\nall_preds = np.concatenate((all_preds, ada_preds.reshape((-1, 1))), axis=1)\nall_preds = np.concatenate((all_preds, bernoulli_preds.reshape((-1, 1))), axis=1)\nall_preds = np.concatenate((all_preds, gaussian_preds.reshape((-1, 1))), axis=1)\nall_preds = np.concatenate((all_preds, knn_preds.reshape((-1, 1))), axis=1)\nall_preds = np.concatenate((all_preds, mlp_preds.reshape((-1, 1))), axis=1)\n\nall_preds_df = pd.DataFrame(all_preds, columns=['cat_preds', 'rfc_preds', 'ada_preds', 'bernoulli_preds', 'gaussian_preds', 'knn_preds', 'mlp_preds'])\nsns.heatmap(all_preds_df.corr())\nplt.show()","28274f95":"# Visualizing and comparing the freqencies of predicted classes of all models\nall_preds_flat = pd.melt(all_preds_df)\ng = sns.FacetGrid(all_preds_flat, col='variable', col_wrap=4, sharex=False, sharey=False)\ng = g.map(sns.countplot, 'value')\nplt.show()","ee34a1e8":"# Creating level1_train dataset\nlevel1_train = np.zeros((X_train.shape[0], 28))\nskf = StratifiedKFold(n_splits=5)\n\nfor tr_idx, te_idx in skf.split(X_train, y_train):\n    \n    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    X_te, y_te = X_train[te_idx], y_train[te_idx]\n    \n    cat_model.fit(X_tr, y_tr)\n    cat_preds = cat_model.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7] = cat_preds[:, i]\n    \n    rfc_clf.fit(X_tr, y_tr)\n    rfc_preds = rfc_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+1] = rfc_preds[:, i]\n    \n    ada_clf.fit(X_tr, y_tr)\n    ada_preds = ada_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+2] = ada_preds[:, i]\n    \n    bernoulli_clf.fit(X_tr, y_tr)\n    bernoulli_preds = bernoulli_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+3] = bernoulli_preds[:, i]\n    \n    gaussian_clf.fit(X_tr, y_tr)\n    gaussian_preds = gaussian_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+4] = gaussian_preds[:, i]\n    \n    knn_clf.fit(X_tr, y_tr)\n    knn_preds = knn_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+5] = knn_preds[:, i]\n\n    mlp_clf.fit(X_tr, y_tr)\n    mlp_preds = mlp_clf.predict_proba(X_te)\n    for i in range(4):\n        level1_train[te_idx, i*7+6] = mlp_preds[:, i]","85eebc89":"# Creating level1_test dataset\nlevel1_test = np.zeros((X_test.shape[0], 28))\n\ncat_model.fit(X_train, y_train)\ncat_preds = cat_model.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7] = cat_preds[:, i]\n\nrfc_clf.fit(X_train, y_train)\nrfc_preds = rfc_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+1] = rfc_preds[:, i]\n\nada_clf.fit(X_train, y_train)\nada_preds = ada_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+2] = ada_preds[:, i]\n\nbernoulli_clf.fit(X_train, y_train)\nbernoulli_preds = bernoulli_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+3] = bernoulli_preds[:, i]\n    \ngaussian_clf.fit(X_train, y_train)\ngaussian_preds = gaussian_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+4] = gaussian_preds[:, i]\n    \nknn_clf.fit(X_train, y_train)\nknn_preds = knn_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+5] = knn_preds[:, i]\n\nmlp_clf.fit(X_train, y_train)\nmlp_preds = mlp_clf.predict_proba(X_test)\nfor i in range(4):\n    level1_test[:, i*7+6] = mlp_preds[:, i]","21fa4acf":"# Training a meta classifier on level1_train dataset and making predictions on level1_test dataset\nmeta_clf = RidgeClassifier(normalize=True, class_weight='balanced')\nmeta_clf.fit(level1_train, y_train)\nmeta_preds = meta_clf.predict(level1_test).astype(int)\n\nmeta_subm = pd.read_csv('..\/input\/sample_submission.csv')\nmeta_subm['Target'] = meta_preds\n\nsns.countplot(meta_subm['Target'])\nplt.show()","3eb0e7d6":"meta_subm['Target'] = meta_subm['Target'].apply(lambda x: x+1)\nmeta_subm.to_csv('Stack_1.csv', index=False)","d8d7afb9":"This is a starter script where I have tried to explain how **Model Stacking** works using Cross-Validation. The script starts by normal EDA on the dataset, like *filling missing values*, *removing skewness using log-transformation*, *converting few continous variables into categorical variables* and *one-hot encoding* some categorical variables. Following this I have trained few models, which I think would give different results, because of their working and demonstrated their classification results on compressed train dataset having 2 features using *t-Stochastic Neighbours Estimation algorithm*.\n\nFollowing this analysis I will move forward with their stacking using 5-fold cross validation on Training dataset and thus creating a new dataset called **Level-1 Train dataset** using predicted values on left-out fold during each iteration. Then, we create a new dataset called **Level-1 Test dataset** using predictions of all models on all of Testing dataset. Finally, we train our **meta-learner** on Level-1 Train dataset and predict on Level-1 Test dataset to get final predictions. ","a0dd2697":"It is pretty clear from these visualizations the predictions of Gaussian naive-bayes classifier are very opposite from predictions of other models. After that, predictions of KNN classifier are also very less correlated with the predictions of other classifiers. The tree-based classifiers are producing results which are almost similar to each other, showing their similar workings.\n\nNow, we are ready to start with Model Stacking. As discussed earlier, we need to create a level1_train dataset using K-Fold cross-validation(K=5, here), where we fit our model on K-1 folds and make predictions on the one left-out fold, to create the level1_train dataset. Then, we will create level1_test dataset by training models on complete original train dataset and then predicting on complete test dataset. Then finally, we can train a meta-classifier(here, Ridge classifier) on level1_train data and make predictions on level1_test dataset.","b1d12f8d":"In the next three cells I have used visualizations to see how are the predictions from all seven models are different. In Model Stacking (and also Ensembling), it is always a good practice to combine models which are having as different predictions as possible, that will give our stacked model a better power of generalization on test set.\n\nFirstly, I have used the output of TSNE algorithm on complete array and created scatter plot which also shows the predictions of different models. Then, I have created a heatplot to compare correlations of their predictions. And finally I have visualized the frequencies of predicted classes from all models."}}