{"cell_type":{"be27c614":"code","61812162":"code","1f5be46c":"code","e97118cc":"code","82e3f4bc":"code","83dafdf7":"code","418cb9d8":"code","20adf256":"code","817fdb7f":"code","9335a9cb":"code","c050c1af":"code","0e8b5512":"code","80c02e5d":"code","23a628ce":"code","2046dcd6":"code","3c48a39b":"code","68cc5255":"code","375e7f22":"code","c59bc129":"markdown","36dddb15":"markdown","523b6406":"markdown","8f27831b":"markdown","cbe08b83":"markdown"},"source":{"be27c614":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\npd.reset_option('^display.', silent=True)\n\nX_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ny_train = X_train.Survived\nnum_train = len(X_train)\nX_train.drop(['Survived'], axis=1, inplace=True)\n\ndf = pd.concat([X_train, X_test], ignore_index=True)\n\ndf.head()","61812162":"print(\"Total training samples:\", len(X_train), \"\\n\")\nprint(\"Partial data\\n\", X_train.iloc[0:4, 0:6], \"\\n\")\nprint(\"Samples per sex\\n\", X_train.groupby('Sex')['Sex'].count())","1f5be46c":"X_train.describe()","e97118cc":"# Print a survivor as a sample\nsample_index = 25\nprint(X_train.iloc[sample_index])","82e3f4bc":"# Show the column types we are dealing with\n\ndf.dtypes.value_counts()\ncategorical_columns = df.select_dtypes('object').columns\nprint(len(df.columns)-len(df.select_dtypes('object').columns),'numerical columns:')\nprint([i for i in list(df.columns) if i not in list(df.select_dtypes('object').columns)], '\\n')\nprint(len(df.select_dtypes('object').columns),'categorical columns:')\nprint(list(df.select_dtypes('object').columns))","83dafdf7":"pd.set_option('mode.chained_assignment', None)\n\n# Delete unused variable Ticket\ndf = df.drop(['Ticket'],axis=1)\n\n# Use mean variable for missing Embarked\ndf.Embarked[df.Embarked.isnull()] = 'S'\n\n# Remove integer suffix values for Cabin\ndf.Cabin[~df.Cabin.isnull()] = df.Cabin[~df.Cabin.isnull()].map(lambda x: x[0])\n\n# Encode cabins\ndf.Cabin[df.Cabin=='A'] = 1\ndf.Cabin[df.Cabin=='B'] = 2\ndf.Cabin[df.Cabin=='C'] = 3\ndf.Cabin[df.Cabin=='D'] = 3\ndf.Cabin[df.Cabin=='E'] = 3\ndf.Cabin[df.Cabin=='F'] = 4\ndf.Cabin[df.Cabin=='T'] = 5\ndf.Cabin[df.Cabin=='G'] = 6\n\ncabins = df.groupby(['Pclass']).Cabin\nf = lambda x: x.fillna(round(x.median()))\ndf.Cabin = cabins.transform(f)\n\n# Encode the titles of the passengers, some are VIP\n# Thanks https:\/\/www.kaggle.com\/rushikeshdudhat\/accuracy-80-using-xgboost-titanic-ml-challenge\nvip_names = ['Mlle', 'Master', 'Dr', 'Rev', 'Col', 'Major', 'Dona', 'Capt', 'Lady', 'Jonkheer', 'Countess', 'Don', 'Sir']\ndf.Name = df.Name.str.extract('([A-Za-z]+)\\.',expand = False)\ndf.Name = df.Name.replace('Mlle','Miss') \ndf.Name = df.Name.replace('Ms','Mrs') \ndf.Name = df.Name.replace('Mme','Mrs')\ndf.Name = df.Name.replace(vip_names, 'VIP')\n\n# Encode the group that passengers travel in\n#d.sibsp = sibling-spouse, parch=parent\ndf = df.assign(Member=0)\ndf.Member[(df.SibSp == 0) & (df.Parch == 0)] = 1\ndf.Member[(df.SibSp > 0) & (df.Parch == 0)] = 2\ndf.Member[((df.SibSp == 2) & (df.Parch > 0)) | (df.SibSp > 2)] = 3\ndf.Member[((df.SibSp > 0) & (df.Parch > 1)) | ((df.SibSp == 0) & (df .Parch > 1))] = 4\ndf.Member[((df.SibSp < 2) & (df.Parch == 1))] = 5\n\n# Fill missing values for age with median\nages = df.groupby(['Sex','Member']).Age\nf = lambda x: x.fillna(x.median())\ndf.Age = ages.transform(f)\n\n# Fill missing values for fare with median\nfares = df.groupby(['Pclass','Embarked']).Fare\nf = lambda x: x.fillna(x.median())\ndf.Fare = fares.transform(f)\n\n# Make a rank of the passengers's fares\ndf['Farerank'] = df.Fare.rank() \/ len(df.Fare)\n\n# Combine the family features\ndf['Pvar'] = df.Parch+1 * df.SibSp+1\ndf = df.drop(['Parch', 'SibSp'], axis=1)\n\n# Capitalize the Sex\ndf.Sex = df.Sex.str.capitalize()","418cb9d8":"# Check for remaining null values\nprint(df.isnull().values.any())","20adf256":"# Split age into groups and separate by survival rate\n# Thanks to https:\/\/www.kaggle.com\/sid2412\/a-simple-and-effective-approach-to-ml\ndf_surr = pd.concat([df, y_train], axis=1)\ndf_surr['AgeGroup'] = pd.cut(df_surr['Age'],5)\ndf_surr[['AgeGroup', 'Survived']].groupby('AgeGroup', as_index=False).mean().sort_values('Survived', ascending=False)","817fdb7f":"# Encode the age group of passengers based on above tableau\ndf.Age[df.Age <= 16] = 4\ndf.Age[(df.Age > 32) & (df.Age <= 48)] = 3\ndf.Age[(df.Age > 48) & (df.Age <= 64)] = 2\ndf.Age[(df.Age > 16) & (df.Age <= 32)] = 1\ndf.Age[df.Age > 64] = 0","9335a9cb":"from sklearn.preprocessing import OneHotEncoder\n\ndef encode_df(df, object_cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    df_enc = pd.DataFrame(ohe.fit_transform(df[object_cols]))\n    df_enc.columns = ohe.get_feature_names(object_cols)\n    df_enc.index = df.index\n    return df_enc\n\n# Use OH encoder to encode cat cols\nobject_cols = ['Name', 'Embarked']\ndf_enc = encode_df(df, object_cols)\nnum_df = df.drop(object_cols, axis=1)\ndf = pd.concat([num_df, df_enc], axis=1)","c050c1af":"# Split the df into train and test set\n\nX_train = df.iloc[:num_train,:]\nX_test = df.iloc[num_train:,:]","0e8b5512":"X_train.head()","80c02e5d":"# Split training set by male\/female\n\nFmask = (X_train.Sex=='Female')\nFX_train = X_train.loc[Fmask]\nMX_train = X_train.loc[~Fmask]\nFX_train.reset_index(inplace=True)\nMX_train.reset_index(inplace=True)\nFY_train = y_train.loc[Fmask]\nMY_train = y_train.loc[~Fmask]\nFX_train = FX_train.drop(['Sex'], axis=1)\nMX_train = MX_train.drop(['Sex'], axis=1)\n\n# Split test set by male\/female\n\nFmask = (X_test.Sex=='Female')\nFX_test = X_test.loc[Fmask]\nMX_test = X_test.loc[~Fmask]\nFX_test.reset_index(inplace=True)\nMX_test.reset_index(inplace=True)\nFX_test = FX_test.drop(['Sex'], axis=1)\nMX_test = MX_test.drop(['Sex'], axis=1)","23a628ce":"print(f'Females that survived\/died: \\n{FY_train.value_counts()}\\n')\nprint(f'Males that died\/survived: \\n{MY_train.value_counts()}')","2046dcd6":"# Scale train and test data separately\nfrom sklearn.preprocessing import StandardScaler\n\nFsc = StandardScaler()\nFX_train_sc = Fsc.fit_transform(FX_train)\nFX_test_sc = Fsc.transform(FX_test)\n\nMsc = StandardScaler()\nMX_train_sc = Msc.fit_transform(MX_train)\nMX_test_sc = Msc.transform(MX_test)","3c48a39b":"# Feature selection and find best K features\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn import feature_selection\nimport warnings\nwarnings.filterwarnings('ignore')\n\nk=9 \nF_selector = feature_selection.SelectKBest(feature_selection.f_regression, k)\nFX_train_kb = F_selector.fit_transform(FX_train_sc, FY_train)\nFX_test_kb = F_selector.transform(FX_test_sc)\n\nM_selector = feature_selection.SelectKBest(feature_selection.f_regression, k)\nMX_train_kb = M_selector.fit_transform(MX_train_sc, MY_train)\nMX_test_kb = M_selector.transform(MX_test_sc)\n\nf_feature_names = [df.columns[i] for i in F_selector.get_support(indices=True)]\nm_feature_names = [df.columns[i] for i in M_selector.get_support(indices=True)]\n\nprint(f\"Best {k} features for female: {f_feature_names}\")\nprint(f\"Best {k} features for male: {m_feature_names}\")\n\nF_indices = np.argsort(F_selector.scores_)[::-1]\nM_indices = np.argsort(M_selector.scores_)[::-1]\n\nfig, axs = plt.subplots(2, 1, constrained_layout=True, figsize=(10,6))\naxs[0].bar(f_feature_names, F_selector.scores_[F_indices[range(k)]], color='r', align='center')\naxs[0].set_title('Best features for female')\naxs[0].set_ylabel('Scores')\nfig.suptitle(f'Best features by SelectKBest for K={k}', fontsize=16)\n\naxs[1].bar(m_feature_names, M_selector.scores_[M_indices[range(k)]], color='b', align='center')\naxs[1].set_title('Best features for male')\naxs[1].set_ylabel('Scores')\n\nplt.show()","68cc5255":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3], \n              'kernel': ['rbf'],\n              'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]}\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n\nF_model = SVC()\nF_grs = GridSearchCV(F_model, param_grid=param_grid, cv=sss, n_jobs=5)\nF_grs.fit(np.array(FX_train_kb), np.array(FY_train))\nF_predictions = F_grs.predict(FX_test_kb).tolist()\n\nM_model = SVC()\nM_grs = GridSearchCV(M_model, param_grid=param_grid, cv=sss, n_jobs=5)\nM_grs.fit(np.array(MX_train_kb), np.array(MY_train))\nM_predictions = M_grs.predict(MX_test_kb).tolist()\n\nprint(\"Best parameters for F classifier: \" + str(F_grs.best_params_))\nF_gpd = pd.DataFrame(F_grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data: {0:1.4f}\"\n      .format(F_gpd['mean_test_score'][F_grs.best_index_]))\nprint()\nprint(\"Best parameters for M classifier: \" + str(M_grs.best_params_))\nM_gpd = pd.DataFrame(M_grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data: {0:1.4f}\"\n      .format(M_gpd['mean_test_score'][M_grs.best_index_]))\nnum_preds = len(F_predictions) + len(M_predictions)\nprint()\nprint(f\"Total number of predictions: {num_preds}\")","375e7f22":"# Collect female and male predictions and save as CSV\n\npredtot = pd.Series([0]*418)\n\nFinds = [x-891 for x in Fmask[Fmask==True].index.tolist()]\nMinds = [x-891 for x in Fmask[Fmask==False].index.tolist()]\n\npredtot[Finds] = F_predictions\npredtot[Minds] = M_predictions\n\npid = pd.Series(range(892,1310))\npredfinal = pd.DataFrame({'PassengerID': pid, 'Survived': predtot})\n\n# Save to CSV\npredfinal.to_csv('submission.csv', index=False)","c59bc129":"# The dual model (SVC and GridSearchCV)\n\nInspired from previous notebooks, I took to **GridSearchCV** to find an optimal set of parameters for my training data using a CV splitting tecnique.","36dddb15":"# Feature selection (SelectKBest)\n\nI used the **SelectKBest** tool to find the best correlated features by a univariate linear regression model. I found that a subset of the best 9 or 10 features from the original training set produces a good test score.","523b6406":"![](https:\/\/i.imgur.com\/nRh1GdK.jpg)","8f27831b":"# Feature encoding\n\nWhat I've found from various experiments and evaluating other notebooks is that there's quite a few interesting features to be created from the raw data. Recent notebook's suggest that the **Cabin** feature (where the passanger's had checked in on the ship), **Name** (full name of each passanger) and family relations **SibSp**\/**Parch** (if passanger's traveling alone or as a couple) are relevant in predicting survivors.\n\nThe **Pclass** feature is a ticket price class indicating if passanger's travelled first, second or third class (which must be the cargo area I presume).","cbe08b83":"# Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nThis notebook is basically an attempt to up the feature engieering work from my previous [Titanic nootebook](https:\/\/www.kaggle.com\/christianlillelund\/titanic-using-gridsearchcv-10-classifiers) based on some data analysis and inspiration from other people's work. Here, I run the subset selection method SelectKBest on the training set to find the best K features, then split the data into male and female survivors, since there's a significant higher number of survivors amongst the women compared to the men. I train the two models using exhaustive grid search with GridSearchCV and a support vector machine (SVC-RBF) classifer to produce an accuracy of about **80%** on the test set.\n\nKey takeaways:\n\n* Training and test data is split by sex (Male\/female).\n* Used SelectKBest to find best features on either split (K=9 works best).\n* Used GridSearchCV to train two SVC models and then made my predictions.\n"}}