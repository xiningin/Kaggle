{"cell_type":{"631f1f6a":"code","c8d37588":"code","430b5f4a":"code","5a090688":"code","88ed53c1":"code","702f8656":"code","764ba54c":"code","69302205":"code","af1930b4":"code","c65ecd47":"code","3dafbd78":"code","8f1905ed":"code","232e762a":"code","0102306e":"code","0126fb5e":"code","365c5a19":"code","503f1516":"code","f5012be5":"code","16d3a71b":"code","6e08c6be":"code","0d635650":"code","8a6e49de":"code","66b22396":"code","af482281":"code","7f2e00bf":"code","38871483":"code","ef8cefa7":"code","57cb4d87":"code","9b7844a5":"code","dea5da5b":"code","5f31deb7":"code","85afc515":"code","642cf8f3":"code","2cd6e15f":"code","18db3f57":"code","d9033144":"code","0d63409c":"code","32865146":"code","bfae355e":"code","8219f928":"code","22d6f11b":"code","dab751e9":"code","a3c959e1":"code","8b4b5b9c":"code","5b934722":"code","72cf792f":"code","369b4e27":"code","227d0276":"code","95e04428":"code","c9df9f9a":"code","0d0bd8f5":"code","b8517e6d":"code","cf83a8e0":"code","e07a4852":"code","7728c070":"code","7fb0d374":"code","09d62758":"code","2008f69a":"code","7e619bcb":"code","4e268113":"code","cab7c04f":"code","5690721e":"markdown","4ccc70bf":"markdown","4e209c18":"markdown","a99a0d13":"markdown","e0ee11d6":"markdown","bae8bae0":"markdown"},"source":{"631f1f6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c8d37588":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nmin_scaler = MinMaxScaler()\nscaler = StandardScaler()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","430b5f4a":"!pip install jovian","5a090688":"import jovian","88ed53c1":"INPUT_DIR = '..\/input\/m5-forecasting-accuracy'\nclndr = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\ndf_val = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')\nsubmsn = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\nprc = pd.read_csv(f'{INPUT_DIR}\/sell_prices.csv')\ndf = pd.read_csv(f'{INPUT_DIR}\/sales_train_evaluation.csv')","702f8656":"def sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","764ba54c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2 \n    print ('Initial Usage = {:5.2f} Mb'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return None","69302205":"#reduce_mem_usage(df)\nreduce_mem_usage(clndr)\nreduce_mem_usage(prc)","af1930b4":"clndr['date'] = pd.to_datetime(clndr.date)\nclndr['days'] = clndr['date'].dt.day","c65ecd47":"cln = clndr[:1941]","3dafbd78":"dates = cln['date']","8f1905ed":"import time","232e762a":"#da = clndr[0:1913]\nda = clndr.copy()","0102306e":"da['event_name_1'] = da['event_name_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_type_1'] = da['event_type_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_name_2'] = da['event_name_2'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_type_2'] = da['event_type_2'].apply(lambda x: np.where(pd.isnull(x),0,1))","0126fb5e":"da = da.iloc[:,:14]","365c5a19":"da['date'] = pd.to_datetime(da.date)\nda.set_index('date',inplace=True)\nda.drop(['wm_yr_wk','weekday','d'],axis = 1,inplace=True)","503f1516":"l1 = ['wday','month','year']\nfor col in l1:\n    da[col] = da[col].astype('category')\nda = pd.get_dummies(da,columns=l1)\n","f5012be5":"def SMA(data):\n    return data.mean()  \n\ndef std(data):\n    return data.std()\n\ndef EMA(data):\n    dats = data.astype(float)\n    data['EMA'] = dats.ewm(span = 20).mean()\n    return data['EMA']","16d3a71b":"cols = [i for i in df.columns if 'd_' in i ]","6e08c6be":"date_tr = clndr['date']","0d635650":"def get_sale(item):\n    tmp = df.set_index('id').loc[item,cols]\n    tmp = tmp.reset_index().drop('index',axis = 1).rename(columns = {0:item})\n    return pd.merge(date_tr,tmp,left_index = True,right_index=True).set_index('date')\n","8a6e49de":"def dframe(item):\n    tmp = da.copy()\n    item_sale = get_sale(item)\n    \n    #tmp.replace('nan', np.nan).fillna(0)\n    tmp = pd.merge(tmp,item_sale, left_index=True, right_index=True, how = 'left')\n    tmp.rename(columns = {item:'item'},inplace =True)\n    \n    for i in (1,7,14,28,365):\n        tmp['lag_'+str(i)] = tmp['item'].transform(lambda x: x.shift(i))\n    \n    \n    for i in [7,14,28,60,180,365]:\n        tmp['rolling_mean_'+str(i)] = tmp['item'].transform(lambda x: x.shift(28).rolling(i).mean())\n        tmp['rolling_std_'+str(i)]  = tmp['item'].transform(lambda x: x.shift(28).rolling(i).std())\n    \n    \n    tmp = tmp.replace('nan', np.nan).fillna(0)\n    \n    return tmp.to_numpy()\n    \n    \n    ","66b22396":"from torch.utils.data import Dataset,DataLoader","af482281":"scaler = StandardScaler()\ny_scaler = MinMaxScaler()","7f2e00bf":"'''dx = dframe(item)\nxdata = dx.copy()'''","38871483":"'''trains = xdata[:1500]\ntests = xdata[1500:1913]\nval = xdata[1885:1941]\nevl = xdata[1913:]'''","ef8cefa7":"def sliding_windows_mutli_features(data, seq_length):\n    x = []\n    y = []\n    data_x = scaler.fit_transform(data)\n    data_y = data[:,32].reshape(-1,1)\n\n    for i in range((data.shape[0])-seq_length-seq_length+1):\n        #print (data.shape[0])\n        #print (len(data)-seq_length-1)\n        #print (i,(i+seq_length))\n        _x = data_x[i:(i+seq_length),:] ## 16 columns for features  \n        _y = data_y[i+seq_length:i+seq_length+seq_length] ## column 0 contains the labbel\n        #print ('x - ',_x)\n        #print ('y - ',_y)\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y).reshape(-1,28)","57cb4d87":"'''train_x,train_y = sliding_windows_mutli_features(trains, seq_length=28)\ntest_x,test_y = sliding_windows_mutli_features(tests, seq_length=28)\nval_x,val_y = sliding_windows_mutli_features(val, seq_length=28)\nevl_x,evl_y = sliding_windows_mutli_features(evl, seq_length=28)\n\nprint(train_x.shape)\nprint (train_y.shape)\nprint (test_x.shape)\nprint (test_y.shape)\nprint (val_x.shape)\nprint (val_y.shape)\nprint (evl_x.shape)\nprint (evl_y.shape)\n\ntrain_set = FeatureDataset(train_x,train_y)\ntest_set = FeatureDataset(test_x,test_y)\nval_set = FeatureDataset(val_x,val_y)\nevl_set = FeatureDataset(evl_x,evl_y)\n\ntrain_loader = DataLoader(dataset = train_set,\n                         batch_size = 10\n                         )\ntest_loader = DataLoader(dataset = test_set,\n                         batch_size = 10\n                         )\nval_loader = DataLoader(dataset = val_set,\n                         batch_size = 1\n                         )\nevl_loader = DataLoader(dataset = evl_set,\n                         batch_size = 1\n                         )'''","9b7844a5":"class FeatureDataset(Dataset):\n    def __init__(self,feature,target):\n        self.feature = feature\n        self.target = target\n    \n    def __len__(self):\n        return len(self.feature)\n    \n    def __getitem__(self,idx):\n        item = self.feature.reshape(self.feature.shape[0],self.feature.shape[2],self.feature.shape[1])[idx]\n        label = self.target[idx]\n    \n        \n        return item,label","dea5da5b":"def pre_process(xdata,split,seq_length):\n    trains = xdata[:split]\n    tests = xdata[split:1913]\n    val = xdata[1885:1941]\n    evl = xdata[1913:]\n    \n    \n    train_x,train_y = sliding_windows_mutli_features(trains, seq_length)\n    test_x,test_y = sliding_windows_mutli_features(tests, seq_length)\n    val_x,val_y = sliding_windows_mutli_features(val, seq_length=28)\n    evl_x,evl_y = sliding_windows_mutli_features(evl, seq_length=28)\n    \n    train_set = FeatureDataset(train_x,train_y)\n    test_set = FeatureDataset(test_x,test_y)\n    val_set = FeatureDataset(val_x,val_y)\n    evl_set = FeatureDataset(evl_x,evl_y)\n    \n    train_loader = DataLoader(dataset = train_set,\n                         batch_size = 500\n                         )\n    test_loader = DataLoader(dataset = test_set,\n                         batch_size = 300\n                         )\n    val_loader = DataLoader(dataset = val_set,\n                         batch_size = 1\n                         )\n    evl_loader = DataLoader(dataset = evl_set,\n                         batch_size = 1\n                         )\n    \n    return train_loader,test_loader,val_loader,evl_loader","5f31deb7":"'''pre_process(xdata,1500,28)'''","85afc515":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.nn.utils import weight_norm\nimport gc\nimport collections\nimport random","642cf8f3":"class CNN_Forecast(nn.Module):\n    def __init__(self,c_in,c_bw,c_out,ks=3,d=2,s=1):\n        super().__init__()\n        self.conv1 = weight_norm(nn.Conv1d(c_in,c_bw,kernel_size=ks,dilation = d,padding = int((d*(ks-1))\/2),stride = s))\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = weight_norm(nn.Conv1d(c_bw,c_out,kernel_size=ks,dilation = d,padding = int((d*(ks-1))\/2),stride = s))\n        self.dp = nn.Dropout(0.2)\n        self.shortcut = lambda x: x\n        if c_in != c_out:\n            self.shortcut = nn.Conv1d(c_in, c_out,kernel_size = 1,stride=1)\n        \n        y = torch.randn(c_in,28).view(-1,c_in,28)\n        self.to_linear = None\n        self.convs(y)\n        \n        self.fc1 = nn.Linear(self.to_linear,512)\n        self.fc2 = nn.Linear(512,256)\n        self.fc3 = nn.Linear(256,28)\n        \n    \n    def convs(self,x):\n        r = self.shortcut(x)\n        x = self.relu(self.conv1(x))\n        x = self.dp(x)\n        x = F.leaky_relu(self.conv2(x),0.1)\n        x = self.dp(x)\n        #print ('r - ',r.shape)\n        #print ('x - ',x.shape)\n        r = r.view(x.shape)\n        \n        if self.to_linear is None:\n            self.to_linear = x[0].shape[0]*x[0].shape[1]\n        return x.add_(r)\n        \n    def forward(self,x):\n        x = self.convs(x)\n        x = x.view(-1,self.to_linear)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x\n\nmodel = CNN_Forecast(50,64,32)\nprint (model)","2cd6e15f":"def conv1d(c_in,c_out,stride= 1,dilation = 1,ks = 3):\n    pad = int((dilation*(ks-1))\/2)\n    return nn.Conv1d(c_in,c_out,kernel_size = ks, stride = stride, dilation = dilation, padding = pad)\n\ndef reg(max_ks):\n    return nn.Sequential(nn.ReLU(inplace = True),\n                         nn.Dropout(0.2),\n                         nn.MaxPool1d(max_ks))\n\n\ndef size(s_len,ks,d,c_out,pool,max_ks=1,s=1):\n    pad = pad = int((d*(ks-1))\/2)\n    l_out = (s_len+2*pad-d*(ks-1)-1)\/s + 1\n    if not pool:\n        size = c_out*l_out\n    else:\n        l_out = (l_out-(max_ks-1)-1)\/max_ks + 1\n        size = c_out*l_out\n    \n    return int(size)\n\n    \nclass ConvNet(nn.Module):\n    def __init__(self,n_start,c_out,ks,d,max_ks):\n        super().__init__()\n        self.conv0 = conv1d(n_start,c_out[0],ks = ks[0],dilation = d[0])\n        self.conv1 = conv1d(n_start,c_out[1],ks = ks[1],dilation = d[1])\n        self.conv2 = conv1d(n_start,c_out[2],ks = ks[2],dilation = d[2])\n        \n        self.reg0 = reg(max_ks)\n        self.reg1 = reg(max_ks)\n        self.reg2 = reg(max_ks)\n        \n        self.size0 = size(28,ks[0],d[0],c_out[0],max_ks = max_ks,pool = True)\n        self.size1 = size(28,ks[1],d[1],c_out[1],max_ks = max_ks,pool = True)\n        self.size2 = size(28,ks[2],d[2],c_out[2],max_ks = max_ks,pool = True)\n        \n        self.fc1 = nn.Linear((self.size0+self.size1+self.size2),100)\n        self.fc2 = nn.Linear(100,1)\n\n    def forward(self,x):\n        x0 = self.reg0(self.conv0(x))\n        x1 = self.reg1(self.conv1(x))\n        x2 = self.reg2(self.conv2(x))\n        \n        x0 = x0.view(-1,self.size0)\n        x1 = x1.view(-1,self.size1)\n        x2 = x2.view(-1,self.size2)\n        \n        x = torch.cat([x0,x1,x2],1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x\n","18db3f57":"'''device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,2,2],max_ks = 2).double().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\ncriterion = nn.MSELoss()'''","d9033144":"'''train_losses = []\nvalid_losses = []\ndef Train():\n    \n    running_loss = .0\n    \n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        scheduler.step(loss)\n        optimizer.step()\n        running_loss += loss\n        \n    train_loss = running_loss\/len(train_loader)\n    train_losses.append(train_loss.cpu().detach().numpy())\n    \n    print(f'train_loss {train_loss}')\n    \ndef Valid():\n    running_loss = .0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            scheduler.step(loss)\n            running_loss += loss\n            \n        valid_loss = running_loss\/len(test_loader)\n        valid_losses.append(valid_loss.cpu().detach().numpy())\n        print(f'valid_loss {valid_loss}')\n        \nepochs = 50\nfor epoch in range(epochs):\n    print('epochs {}\/{}'.format(epoch+1,epochs))\n    Train()\n    Valid()\n    gc.collect()'''","0d63409c":"'''import matplotlib.pyplot as plt\nplt.plot(train_losses,label='train_loss')\nplt.plot(valid_losses,label='valid_loss')\nplt.title('MSE Loss')\nplt.xlim(0,epochs)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)'''","32865146":"'''prediction = [0]*385\nrunning_loss = 0\nmodel.eval()\nwith torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            prediction[idx] = preds.cpu().detach().numpy()\n            loss = criterion(preds,labels)\n            running_loss += loss\n        test_loss = running_loss\/len(test_loader)\n        print(f'test_loss {test_loss}')'''","bfae355e":"#res = y_scaler.inverse_transform(prediction)\n'''res = pd.DataFrame(prediction)\nres.index = test_ydates\nres.rename({0:'item'},axis =1,inplace=True)\nres.reset_index(inplace=True)\n\n#res1 = y_scaler.inverse_transform(prediction)\nres1 = pd.DataFrame(prediction)\nres1.index = test_ydates\nres1.rename({0:'item'},axis =1,inplace=True)\nres1.reset_index(inplace=True)\n\ny_tests = pd.DataFrame(test_y)\ny_tests.index = test_ydates\ny_tests.rename(columns = {0:'item'},inplace = True)\n\nactual = y_tests.reset_index()'''","8219f928":"'''prediction = [0]*28\nrunning_loss = 0\nmodel.eval()\nwith torch.no_grad():\n        for idx, (inputs, labels) in enumerate(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            prediction[idx] = preds.cpu().detach().numpy()\n            loss = criterion(preds,labels)\n            running_loss += loss\n        test_loss = running_loss\/len(val_loader)\n        print(f'test_loss {test_loss}')'''","22d6f11b":"it = ['HOBBIES_1_002_CA_1_validation','HOBBIES_1_004_CA_1_validation','HOBBIES_1_003_CA_1_validation','HOBBIES_1_001_CA_1_validation']","dab751e9":"def Trainings1(product):\n    running_loss = .0\n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device,non_blocking = True)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        #scheduler.step(loss)\n        optimizer.step()\n        running_loss += loss\n        \n        #print (f\"epoch {epoch}, datapoints {idx*10}\")\n    \n    train_loss = running_loss\/len(train_loader)\n    train_losses[str(product)].append(train_loss.cpu().detach().numpy().item())\n        \n        \ndef Validations1(product):\n    running_loss = .0\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device,non_blocking = True)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            #scheduler.step(loss)\n            running_loss += loss\n            #print (f\"epoch {epoch}, datapoints {idx*10}\")\n            \n        valid_loss = running_loss\/len(test_loader)\n        valid_losses[str(product)].append(valid_loss.cpu().detach().numpy().item())\n            \ndef Predicts1(product):\n    with torch.no_grad():\n            for idx, (inputs,_) in enumerate(val_loader):\n                inputs = inputs.to(device)\n                optimizer.zero_grad()\n                preds = model(inputs)\n                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n\n            \n            ","a3c959e1":"train_cv_loss = pd.DataFrame()\nvalid_cv_loss = pd.DataFrame()","8b4b5b9c":"'''start1 = time.time()\n#start = time.time()\nseq_length = 28\nspilt = 1500\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.MSELoss()\nepochs = 20\n\ntrain_losses = collections.defaultdict(list)\nvalid_losses = collections.defaultdict(list)\nprediction = [0]*28\npred_dict = {}\n#print ('Time Taken for Initialisation = {}s'.format((time.time() - start))*100)\n\nfirst = True\n\nmodel = CNN_Forecast(50,64,32,ks = 11,d=3).double().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n\nfor product in tqdm(df_val['id']):\n    \n    \n    #print (product)\n    #start = time.time()\n    dset = dframe(product)\n    #dset = np.load('.'.join([product,'npy']))\n    #print ('Time Taken for Creating Dataset = {}s'.format((time.time() - start))*100)\n    #print ('-----Dateset Created-----')\n    #start = time.time()\n    train_loader,test_loader,val_loader,_ = pre_process(dset,split,seq_length)\n    #print ('Time Taken for Creating Loaders = {}s'.format((time.time() - start))*100)\n    #print ('Loaders Ready !!!')\n\n    #print ('Training')\n    #start = time.time()\n    for epoch in range(epochs):\n        Trainings1(product)\n        Validations1(product)\n    #print ('Time Taken for Validating Model = {}s'.format((time.time() - start))*100)\n    Predicts1(product)\ntrain_losses = pd.DataFrame(train_losses)\nvalid_losses = pd.DataFrame(valid_losses)\npred_d = pd.DataFrame(pred_dict)\ntrain_cv_loss = pd.concat([train_cv_loss,train_losses],axis = 1)\nvalid_cv_loss = pd.concat([valid_cv_loss,valid_losses],axis = 1)\ntrain_losses = collections.defaultdict(list)\nvalid_losses = collections.defaultdict(list)\n    \n    \nend = time.time()\nprint('Time taken is {}s'.format(end-start1))'''","5b934722":"def Trainings1(product):\n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        #scheduler.step(loss)\n        optimizer.step()\n        \n        \ndef Validations1(product):\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            #scheduler.step(loss)\n\n            \ndef Predicts1(product):\n    with torch.no_grad():\n            for idx, (inputs,_) in enumerate(evl_loader):\n                inputs = inputs.to(device)\n                optimizer.zero_grad()\n                preds = model(inputs)\n                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n\n            \n            ","72cf792f":"idir_torch1 = '..\/input\/torch-model'\nloaded = torch.load(f'{idir_torch1}\/cpoints.pth')","369b4e27":"idir_torch2 = '..\/input\/torch-model-new'\nloaded = torch.load(f'{idir_torch2}\/cpoints_new.pth')","227d0276":"'''start1 = time.time()\n#start = time.time()\nseq_length = 28\nsplit = 1500\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.MSELoss()\nepochs = 20\n\nprediction = [0]*28\npred_dict = {}\n\nfirst = True\n\nmodel = CNN_Forecast(50,64,32,ks = 11,d=3).double()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n\nidir_torch2 = '..\/input\/torch-model-new'\nloaded = torch.load(f'{idir_torch2}\/cpoints_new.pth')\n\nmodel.load_state_dict(loaded['model_state'])\noptimizer.load_state_dict(loaded['optim_state'])\nmodel.to(device)\nfor state in optimizer.state.values():\n    for k, v in state.items():\n         if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n            \nfor product in tqdm(df['id']):\n    \n    \n    dset = dframe(product)\n    #dset = np.load('.'.join([product,'npy']))\n    train_loader,test_loader,val_loader,evl_loader = pre_process(dset,split,seq_length)\n    \n    for epoch in range(epochs):\n        Trainings1(product)\n        Validations1(product)\n   \n    Predicts1(product)\n\npred_d = pd.DataFrame(pred_dict).T\npred_d.to_csv('predictions1.csv')    \n    \nend = time.time()\n\nprint('Time taken is {}s'.format(end-start1))'''","95e04428":"pd.DataFrame(pred_dict).T","c9df9f9a":"checkpoint = {\n        \"model_state\":model.state_dict(),\n        \"optim_state\":optimizer.state_dict()\n    }\n    \ntorch.save(checkpoint,'cpoints_new.pth')","0d0bd8f5":"pred_d.to_csv('predictions_evl.csv')","b8517e6d":"pred_d","cf83a8e0":"#64,32 bs = 500\n#28,28 bs = 500\n#100,200 bs = 500\n#64,32 ks = 20 bs = 500\n#50,64,32,ks = 11,d=4 bs = 1913\n#bs = 200 and 100\n#bs = 500 and 300 64,32 11,3 e = 20\n#e = 15\n#e = 30\n#e = 5\n#e = 15 t = 8hrs for saved else 11 hrs\n#e = 20 t = 13hrs for not saved else 11.25\n#e = 20 t = 10hr no scheduler saved model\n#e= 20 t = 12hrs with scheduler saved model\n#e= 20 t = 10.5hrs with no scheduler non saved model\n\n#ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,2,2],max_ks = 2)\n#CNN_Forecast(50,100,150)\n#ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,3,4],max_ks = 2)\n#ConvNet(50,c_out = [50,64,78],ks = [7,11,13],d = [2,5,7],max_ks = 2)\n#ConvNet(50,c_out = [50,64,78],ks = [7,11,13],d = [2,5,7],max_ks = 2) without batch normalization\n#CNN_Forecast(50,64,32,ks = 11,d=3)","e07a4852":"idir_dset = '..\/input\/predictions'\nloaded_dset1 = pd.read_csv(f'{idir_dset}\/predictions1.csv')\n\nidir_dset2 = '..\/input\/predictions2'\nloaded_dset2 = pd.read_csv(f'{idir_dset2}\/predictions2.csv')\n\nidir_dset3 = '..\/input\/predictions-evl'\nloaded_dset3 = pd.read_csv(f'{idir_dset3}\/predictions_evl.csv')","7728c070":"preds = pd.concat([loaded_dset1,loaded_dset2,loaded_dset3])","7fb0d374":"preds.columns = submsn.columns","09d62758":"preds","2008f69a":"submsn","7e619bcb":"preds.to_csv('submission1.csv')","4e268113":"idir_sb = '..\/input\/m5-first-public-notebook-under-0-50'\nsubmt_dset2 = pd.read_csv(f'{idir_sb}\/submission_2.csv')","cab7c04f":"preds[preds['F25']<0]","5690721e":"Hey Guys!! This is my first Notebook I am submitting here on this platform. There may be many mistakes here but kindly bear with me. Any comments would be highly appreciated.\n  In this notebook i have tried to give a visual overview of the data presented in the competition by means of various graphs in order to gain a better understanding of our objective in this competition.","4ccc70bf":"Data Preparation","4e209c18":"# M5 Forecasting Challenge","a99a0d13":"### *Trying CNNs* ","e0ee11d6":"### *Data Pre-processing*","bae8bae0":"Building Model"}}