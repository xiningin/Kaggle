{"cell_type":{"52022912":"code","532f16f2":"code","0da0d796":"code","437ade2a":"code","d270788d":"code","4a2f52bb":"code","b55465d5":"code","48134e1e":"code","dbb947a8":"code","3b0ca8c1":"code","8fa5277f":"markdown","9907a0e5":"markdown","9645a9f1":"markdown","382ec7c3":"markdown","fd69d5a4":"markdown","84270273":"markdown","f47e2793":"markdown","7e350100":"markdown","73277e45":"markdown","c1b73421":"markdown","1ceccdfa":"markdown"},"source":{"52022912":"from sklearn.datasets import fetch_lfw_people\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA as RandomizedPCA\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings; warnings.simplefilter(\"ignore\")","532f16f2":"faces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)","0da0d796":"fig, ax = plt.subplots(3, 5)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap = 'bone')\n    axi.set(xticks=[], yticks=[],\n           xlabel=faces.target_names[faces.target[i]])","437ade2a":"pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\nsvc = SVC(kernel='rbf', class_weight='balanced')\nmodel = make_pipeline(pca, svc)","d270788d":"X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, random_state=42)","4a2f52bb":"param_grid = {'svc__C': [1, 5, 10, 50],\n             'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(model, param_grid)\n%time grid.fit(X_train, y_train)\nprint(grid.best_params_)","b55465d5":"model = grid.best_estimator_\nyfit = model.predict(X_test)","48134e1e":"fig, ax = plt.subplots(4, 6)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(X_test[i].reshape(62, 47), cmap='bone')\n    axi.set(xticks=[], yticks=[])\n    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n                   color='black' if yfit[i] == y_test[i] else 'red')\nfig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);","dbb947a8":"print(classification_report(y_test, yfit,\n                            target_names=faces.target_names))","3b0ca8c1":"mat = confusion_matrix(y_test, yfit)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=faces.target_names,\n            yticklabels=faces.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","8fa5277f":"# Create Classification Report","9907a0e5":"# Simple guide to confusion matrix terminology\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n\nI wanted to create a \"quick reference guide\" for confusion matrix terminology because I couldn't find an existing resource that suited my requirements: compact in presentation, using numbers instead of arbitrary variables, and explained both in terms of formulas and sentences.\n\nLet's start with an example confusion matrix for a binary classifier (though it can easily be extended to the case of more than two classes):\n\nExample confusion matrix for a binary classifier\n<img src=\"https:\/\/www.dataschool.io\/content\/images\/2015\/01\/confusion_matrix_simple2.png\" alt=\"CFM\">\n\n# What can we learn from this matrix?\n\nThere are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.\nThe classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).\nOut of those 165 cases, the classifier predicted \"yes\" 110 times, and \"no\" 55 times.\nIn reality, 105 patients in the sample have the disease, and 60 patients do not.\nLet's now define the most basic terms, which are whole numbers (not rates):\n\n* true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n* true negatives (TN): We predicted no, and they don't have the disease.\n* false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n* false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")","9645a9f1":"# Import libraries","382ec7c3":"# Split the Datasets","fd69d5a4":"# Support Vector Machines\n\n<b style=\"color:blue\">SVMs<\/b> are a powerful class of supervised learning algorithms for classification and regression problems. In the context of classification, SVMs can be viewed as maximum margin linear classifiers. \n\nThe SVM uses an objective which explicitly encourages low out-of-sample error (good generalization performance). The $D$ dimensional data are divided into classes by maximizing the margin between the hyperplanes for the classes.","84270273":"A Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report ","f47e2793":"# Visulaize the images","7e350100":"# Model Create","73277e45":"# Plot some images","c1b73421":"# Load datasets","1ceccdfa":"# Create Confusion Matrix"}}