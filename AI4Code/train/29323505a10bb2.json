{"cell_type":{"65f9126f":"code","f4827db9":"code","cbe7b1d3":"code","4f8ab162":"code","2046bff7":"code","2499cc35":"code","f65ca402":"code","77192eca":"code","00ea0d9d":"code","6e8a05f3":"code","93c4dfdc":"code","79c0bd99":"code","3c2bcea2":"code","e7722846":"code","0f2baab3":"code","2f37bdc6":"code","d0989a5b":"code","f4819e32":"code","40ac8414":"code","edcf466d":"code","c43c5518":"code","918a7548":"code","80f995af":"code","24525326":"code","60f59d40":"code","c04e3e01":"code","9611e245":"code","e380397e":"code","3ab92b33":"code","b79fd1b9":"code","8126bd3d":"code","c74cd915":"code","143daffa":"code","19987d22":"code","6e2c4eaa":"code","ab780643":"code","542e916b":"code","a3d34f6e":"markdown","cb53db48":"markdown","f8678e7d":"markdown","bfb32dc2":"markdown","229e5cf3":"markdown","3d8bb842":"markdown","df3b8e39":"markdown"},"source":{"65f9126f":"import pandas as pd\nimport numpy as np\nimport cv2\nimport gc\nimport os\nimport re\nfrom tqdm import tqdm\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\n","f4827db9":"df = pd.read_csv('\/kaggle\/input\/face-mask-detection-dataset\/train.csv')\ndf.shape","cbe7b1d3":"len(df.classname.value_counts())","4f8ab162":"df.head()","2046bff7":"### Headers Of CSV File Has Error, So Correcting it\n\ndf.rename(columns = {'x2' : 'y1', 'y1' : 'x2'}, inplace = True)\ndf.head()","2499cc35":"## Converting The Classname to Respective Integer Label using Sklearn's LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nle.fit(df['classname'])\ndf['classname']=(le.transform(df['classname'])+1)\ndf.head()","f65ca402":"## Checking Whether There Is Any NAN or Missing Values In The DataFrame\n\ndf.isnull().sum()","77192eca":"## Splitting The Dataset\n### For Final Training Purpose, Use Whole Dataset For Training Rather Than Splitting It Into Valid Set\n\nimage_ids = df['name'].unique()\nimage_ids.sort()\nvalid_ids=image_ids[:0] \ntrain_ids=image_ids[:]","00ea0d9d":"valid_df = df[df['name'].isin(valid_ids)]\ntrain_df = df[df['name'].isin(train_ids)]","6e8a05f3":"valid_df.shape, train_df.shape","93c4dfdc":"df.classname.unique()","79c0bd99":"class Maskdataset(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = torch.as_tensor(records[['x1', 'y1', 'x2', 'y2']].values, dtype=torch.float32)\n        \n        \n\n        # there are 21 classes\n        labels = torch.as_tensor(records.classname.values,dtype=torch.int64)\n        \n\n        keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0]) ## To Handle NAN LOSS Cases \n        boxes = boxes[keep]\n        labels = labels[keep]\n\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        # target['area'] = area\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n","3c2bcea2":"def get_train_transform():\n    return A.Compose([\n        ToTensor()\n    ])\n        \n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensor()\n    ])","e7722846":"## USING SWISH Activation Rather Than The Regular ReLU\n\nimport torch.nn as nn\n\nclass Swish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n    \n    \ndef convert_relu_to_swish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Swish())\n        else:\n            convert_relu_to_swish(child)","0f2baab3":"## Using Pytorch Faster-RCNN Resnt50 Pretrained Model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","2f37bdc6":"num_classes = 21  # 20 class (masks) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nconvert_relu_to_swish(model) # converting ReLU to SWISH Activation\n\n## Loading The Trained Model Weights\nmodel.load_state_dict(torch.load('..\/input\/face-mask-detection-weights\/model-epoch7.pth'))\n","d0989a5b":"## To Count The Loss During Training\n\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","f4819e32":"## Defining DataLoaders\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n\ntrain_dataset = Maskdataset(train_df,get_train_transform())\n# valid_dataset = Maskdataset(valid_df, get_valid_transform()) ## No need for Final Training Purpose\n\n\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n# valid_data_loader = DataLoader(\n#     valid_dataset,\n#     batch_size=8,\n#     shuffle=False,\n#     num_workers=2,\n#     collate_fn=collate_fn\n# )\n","40ac8414":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","edcf466d":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","c43c5518":"boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()","918a7548":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\nax.imshow(sample)\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","80f995af":"from torch.optim.optimizer import Optimizer\nclass Ralamb(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Ralamb does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        radam_step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    buffered[2] = radam_step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n                else:\n                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm \/ radam_norm\n\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = radam_norm\n                state['trust_ratio'] = trust_ratio\n\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\"\"\" Lookahead Optimizer Wrapper.\nImplementation modified from: https:\/\/github.com\/alphadl\/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https:\/\/arxiv.org\/abs\/1907.08610\n\"\"\"\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\nimport math\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if 'slow_buffer' not in param_state:\n                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n                param_state['slow_buffer'].copy_(fast_p.data)\n            slow = param_state['slow_buffer']\n            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        # print(self.k)\n        # assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group['lookahead_step'] += 1\n            if group['lookahead_step'] % group['lookahead_k'] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict['state']\n        param_groups = fast_state_dict['param_groups']\n        return {\n            'state': fast_state,\n            'slow_state': slow_state,\n            'param_groups': param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            'state': state_dict['state'],\n            'param_groups': state_dict['param_groups'],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n\n        # We want to restore the slow state, but share param_groups reference\n        # with base_optimizer. This is a bit redundant but least code\n        slow_state_new = False\n        if 'slow_state' not in state_dict:\n            print('Loading state_dict from optimizer without Lookahead applied.')\n            state_dict['slow_state'] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            'state': state_dict['slow_state'],\n            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n        if slow_state_new:\n            # reapply defaults to catch missing lookahead specific ones\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)\n\n\ndef LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n    adam = Adam(params, *args, **kwargs)\n    return Lookahead(adam, alpha, k)\n\ndef Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n    ralamb = Ralamb(params, *args, **kwargs)\n    return Lookahead(ralamb, alpha, k)\n\n\nRangerLars = Over9000\n\n","24525326":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer=Over9000(params,lr=0.0001) ##Over9000 Optimizer (LARS + LookAhead + Ralamb)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.00017, div_factor=2 ,steps_per_epoch=len(train_data_loader), epochs=5)\n\nnum_epochs = 25","60f59d40":"##COMMENTED FOR SUBMISSION PURPOSE\n\n# import gc\n# loss_hist = Averager()\n\n\n# for epoch in range(num_epochs):\n    \n#     z=tqdm(train_data_loader)\n\n#     loss_hist.reset()\n\n#     for itr,(images, targets, image_ids) in enumerate(z):\n#         torch.cuda.empty_cache()\n#         gc.collect()\n        \n#         images = list(image.to(device).float() for image in images)\n#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n#         loss_dict = model(images, targets)\n\n#         losses = sum(loss for loss in loss_dict.values())\n#         loss_value = losses.item()\n\n#         loss_hist.send(loss_value)\n#         z.set_description(f'Epoch {epoch+1}\/{num_epochs}, LR: %6f, Loss: %.6f'%(optimizer.state_dict()['param_groups'][0]['lr'],loss_value))\n#         optimizer.zero_grad()\n#         losses.backward()\n#         optimizer.step()\n#         scheduler.step() ## Since We are using 1-Cycle LR Policy, LR update step has to be taken after every batch\n\n\n#     print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n#     torch.save(model.state_dict(), f'\/content\/drive\/My Drive\/internshala round 1\/model-epoch{epoch+1}.pth') \n#     print()\n#     print('Saving Model.......')\n#     # print()","c04e3e01":"class MaskTestDataset(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","9611e245":"def get_test_transform():\n    return A.Compose([\n        ToTensor()\n    ])","e380397e":"test_df=pd.read_csv('..\/input\/face-mask-detection-dataset\/submission.csv')\ntest_df.head()","3ab92b33":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = MaskTestDataset(test_df, get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","b79fd1b9":"torch.cuda.empty_cache()\ngc.collect()","8126bd3d":"%%time\n\ndetection_threshold = 0.60\nresults = []\nmodel.eval()\nfor images, image_ids in test_data_loader:\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        labels = outputs[i]['labels'].data.cpu().numpy()\n\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        \n        result = {\n            'image_id': image_id,\n            'labels': labels,\n            'scores': scores,\n            'boxes': boxes\n        }\n\n        \n        results.append(result)","c74cd915":"## Using Dictionary is Fastest Way to Create SUBMISSION DATASET.\nnew=pd.DataFrame(columns=['image_id', 'boxes', 'label'])\nrows=[]\nfor j in range(len(results)):\n    for i in range(len(results[j]['boxes'])):\n        dict1 = {}\n        dict1={\"image_id\" : results[j]['image_id'],\n                  'x1': results[j]['boxes'][i,0],\n                  'x2': results[j]['boxes'][i,2],\n                  'y1': results[j]['boxes'][i,1],\n                  'y2': results[j]['boxes'][i,3],\n                  'classname':results[j]['labels'][i].item()}\n        rows.append(dict1)\n\n\n    ","143daffa":"sub=pd.DataFrame(rows)\nsub['classname']=le.inverse_transform(sub.classname.values - 1) ## Converting Back Labels To Original Names ","19987d22":"sub.head()","6e2c4eaa":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\nboxes = boxes[scores >= 0.6].astype(np.int32)","ab780643":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)\n\n","542e916b":"sub.to_csv('submission.csv', index = False)","a3d34f6e":"### Plot Some Results Of The SUBMISSION","cb53db48":"# Inference","f8678e7d":"### Lets, Check Some Images","bfb32dc2":"### Finally Create the SUBMISSION File","229e5cf3":"## TRAINING","3d8bb842":"## UTILS And Model ","df3b8e39":"### Over9000 Optimizer"}}