{"cell_type":{"fd10f714":"code","eb531ca6":"code","7694394d":"code","321529d1":"code","521b7f29":"code","1d37172d":"code","169f1276":"code","a84960b4":"code","d417ae21":"code","542667ce":"code","c918f2dc":"code","7eab40c2":"code","5899c704":"code","dd3ad6e3":"code","735d9bed":"code","127dd5db":"markdown","c273c811":"markdown","3782395e":"markdown","3c5d922c":"markdown","76e45d0d":"markdown","24122cc2":"markdown","0259117e":"markdown","d5b2ad94":"markdown","ee30f95c":"markdown","ad9fcf4f":"markdown","88b47ab0":"markdown"},"source":{"fd10f714":"%%capture\n!pip install pytorch-tabnet","eb531ca6":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")","7694394d":"# Normalization\nfor i in range(50):\n    mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n    train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)\/std)","321529d1":"# Train, Test, Validation Split\ntarget = 'target'\nif \"Set\" not in train.columns:\n    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n\ntrain_indices = train[train.Set==\"train\"].index\nvalid_indices = train[train.Set==\"valid\"].index\ntest_indices = train[train.Set==\"test\"].index","521b7f29":"nunique = train.nunique()\ntypes = train.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in train.columns:\n    if types[col] == 'object':\n        print(col, train[col].nunique())\n        l_enc = LabelEncoder()\n        train[col] = l_enc.fit_transform(train[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        train.fillna(train.loc[train_indices, col].mean(), inplace=True)","1d37172d":"# Columns not to use\nunused_feat = ['Set']\n\n# Features to Use\nfeatures = [ col for col in train.columns if col not in unused_feat+[target]] \n\nX_train = train[features].values[train_indices]\ny_train = train[target].values[train_indices]\n\nX_valid = train[features].values[valid_indices]\ny_valid = train[target].values[valid_indices]\n\nX_test = train[features].values[test_indices]\ny_test = train[target].values[test_indices]","169f1276":"# Basic model parameters\nmax_epochs = 30\nbatch_size = 1024\nopt = torch.optim.Adam # Optimizer\nopt_params = dict(lr=1e-3)\nsch = torch.optim.lr_scheduler.StepLR # LR Scheduler\nsch_params = {\"step_size\":10, \"gamma\":0.9}\nmask = 'entmax'\nworkers = 2 # For torch DataLoader\nsample_type = 1 # For automated sampling with inverse class occurrences \nvirtual_batch = 128 # Size of the mini batches used for \"Ghost Batch Normalization\"","a84960b4":"unsupervised_model = TabNetPretrainer(\n    optimizer_fn = opt,\n    optimizer_params = opt_params,\n    mask_type = mask)","d417ae21":"clf = TabNetClassifier(gamma = 1.5,\n                       lambda_sparse = 1e-4,\n                       optimizer_fn = opt,\n                       optimizer_params = opt_params,\n                       scheduler_fn = sch,\n                       scheduler_params = sch_params,\n                       mask_type = mask)","542667ce":"unsupervised_model.fit(\n    X_train=X_train,\n    eval_set=[X_valid],\n    pretraining_ratio=0.8)","c918f2dc":"clf.fit(X_train=X_train, \n    y_train=y_train,\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    eval_name=['train', 'val'],\n    eval_metric=[\"logloss\", 'balanced_accuracy'],\n    max_epochs=max_epochs , patience=15,\n    batch_size=batch_size,\n    virtual_batch_size=virtual_batch,\n    num_workers=workers,\n    weights=sample_type,\n    drop_last=False,\n    from_unsupervised=unsupervised_model)","7eab40c2":"# plot losses\nplt.plot(clf.history['loss'])","5899c704":"# plot auc\nplt.plot(clf.history['train_logloss'])\nplt.plot(clf.history['val_logloss'])","dd3ad6e3":"test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntest_indices = test.index\ntest_ds = test[features].values[test_indices]\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\nsample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = clf.predict_proba(test_ds)\nsample_submission.to_csv('tabnet_submission.csv',index = False)","735d9bed":"sample_submission.head()","127dd5db":"<h1 style=\"background-color:black;color:white;padding:10px; height: 50px;\"> <center>Submission<\/center> <\/h1>","c273c811":"# Table of Content\n\n1. [Packages \ud83d\udce6 and Basic Setup](#basic)\n2. [Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d](#pre)\n3. [The Model \ud83d\udc77\u200d\u2640\ufe0f](#model)\n4. [Training \ud83d\udcaa\ud83c\udffb](#train)","3782395e":"<a id = 'pre'><\/a>\n<h1 style=\"background-color:black;color:white;padding:10px; height: 50px;\"> <center>Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d<\/center> <\/h1>","3c5d922c":"Upon a closer look, we realize that most features are left skewed in this dataset. Thus, Normalization seems ideal.\n\n<center><img src=\"https:\/\/github.com\/SauravMaheshkar\/Tabular-Playground-Series-May-2021\/blob\/main\/assets\/feature_distribution.png?raw=true\"><\/center>\n\n> Image taken from [TPS-May: Categorical EDA](https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda)","76e45d0d":"* Fill NaN values + optional script for `object` columns.","24122cc2":"![](https:\/\/github.com\/SauravMaheshkar\/Tabular-Playground-Series-May-2021\/blob\/main\/assets\/Banner.png?raw=true)","0259117e":"We'll split the dataset into a **80-10-10** split for training, validation and test respectively.","d5b2ad94":"<a id = 'train'><\/a>\n<h1 style=\"background-color:black;color:white;padding:10px; height: 50px;\"> <center>Training \ud83d\udcaa\ud83c\udffb<\/center> <\/h1>","ee30f95c":"<a id = 'basic'><\/a>\n<h1 style=\"background-color:black;color:white;padding:10px; height: 50px;\"> <center>Packages \ud83d\udce6 and Basic Setup<\/center> <\/h1>\n\nInitiially introduced in the paper titled [**\"TabNet: Attentive Interpretable Tabular Learning\"**](https:\/\/arxiv.org\/pdf\/1908.07442.pdf), TabNet is a novel high-performance and interpretable canonical deep tabular data learning architecture. It uses sequential attention to choose which features to reason\nfrom at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features.\n\n![](https:\/\/github.com\/SauravMaheshkar\/Tabular-Playground-Series-May-2021\/blob\/main\/assets\/tabnet.png?raw=true)\n\nA Pytorch Implementation of Tabnet has been made available by the team at [dreamquark-ai](https:\/\/github.com\/dreamquark-ai\/tabnet). We can simply install the package using `pip`.\n\n```\npip install pytorch-tabnet\n```\n\nThis kernel aims to be a starter notebook for you to add your own pre-processing \/ parameters.","ad9fcf4f":"<a id='model'><\/a>\n<h1 style=\"background-color:black;color:white;padding:10px; height: 50px;\"> <center>The Model \ud83d\udc77\u200d\u2640\ufe0f<\/center> <\/h1>","88b47ab0":"The paper highlights a semi-supervised pre-training method which is available via the `TabNetPretrainer` class. We'll use this pretrain this model and use it to boost the tabnet model performance as a unsupervised prior."}}