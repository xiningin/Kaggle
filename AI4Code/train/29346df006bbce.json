{"cell_type":{"bae50ed2":"code","1a8f0776":"code","6e4bfcd7":"code","190cf737":"code","ab65a2d8":"code","efd343d9":"code","fa7a1751":"code","adc137c1":"code","03eda11c":"code","3c57d8c0":"code","ccd55bbb":"code","a108784e":"code","6323ed8f":"code","fc56dfe7":"code","73c641e4":"code","6d96b4bd":"code","d097731c":"code","f8d1bb39":"code","c4b4414a":"code","2a5cb81c":"code","8b7ec835":"code","240b3fe2":"code","a89f1ac5":"code","2132a018":"code","eeb80f44":"code","f2ffde13":"code","0903dcd9":"code","be268ad2":"code","92d6a499":"code","e7864fb4":"code","d75090cb":"code","aa41535f":"code","d952b36a":"code","d3d20d08":"code","4afbee29":"code","9ef34915":"code","05adf8cd":"code","bf65b0c4":"code","b4945815":"code","70c4d0e5":"markdown","1cd28a46":"markdown","c2f13b7e":"markdown","c273ed2d":"markdown","f82a5bcc":"markdown","0a0d1834":"markdown","e156364d":"markdown","a5d43028":"markdown","643f4187":"markdown","60000e63":"markdown","35f3d05f":"markdown","cf036bd5":"markdown","171db376":"markdown","ae7027a9":"markdown","709052f4":"markdown","2e5ff19f":"markdown","07fb480f":"markdown","4c553ab1":"markdown","0d8e3b37":"markdown","0f3fd4b2":"markdown"},"source":{"bae50ed2":"# To be able to use fastai with GPU\n# see here: https:\/\/www.kaggle.com\/product-feedback\/279990\n!pip install -q --user torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0","1a8f0776":"!pip list | grep fastai","6e4bfcd7":"!pip install -q --user unpackai\n!pip install -q --user transformers==4.11.3","190cf737":"from pathlib import Path\n\nimport pandas as pd\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\nfrom unpackai import utils\nfrom unpackai.nlp import *","ab65a2d8":"import torch\ntorch.cuda.set_device(\"cuda:0\")\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on device: {}\".format(device))","efd343d9":"from transformers import (\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    AutoModel,\n    ViTFeatureExtractor,\n    PreTrainedTokenizer,\n    EncoderDecoderModel\n    )\n\nfrom unpackai.nlp import HFTextBlock\nfrom unpackai.utils import ls","fa7a1751":"DATASET_DIR = Path(\"..\/input\")\nCOCO_DIR = DATASET_DIR \/ \"coco2014\"\n\nIMAGE_DATA = COCO_DIR \/ \"val2014\"  # for smaller dataset\n# IMAGE_DATA = COCO_DIR \/ \"train2014\"\nCAPTION_DATA = COCO_DIR \/ \"captions\"","adc137c1":"ls(CAPTION_DATA)","03eda11c":"anno = (CAPTION_DATA \/ \"annotations\/captions_val2014.json\").read_json()\nlen(anno['annotations'])","3c57d8c0":"image_df = pd.DataFrame(anno['images'])\nanno_df = pd.DataFrame(anno['annotations'])","ccd55bbb":"image_df.head()","a108784e":"anno_df.head()","6323ed8f":"file_name_2_id = dict(image_df[[\"file_name\",\"id\",]].values)\nid_2_caption = dict(anno_df[[\"image_id\",\"caption\"]].values)","fc56dfe7":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","73c641e4":"def get_y(path: Path) -> str:\n    \"\"\"Get annotation from a given path\"\"\"\n    # We have built our dictionary with file name and not path\n    file_name = path.name\n    idx = file_name_2_id.get(file_name)\n    return f\": {id_2_caption.get(idx)}\"","6d96b4bd":"INPUT_MEAN = [.5,.5,.5]\nINPUT_STD = [.5,.5,.5]","d097731c":"txt_block = HFTextBlock(tokenizer, max_length=32, padding=\"max_length\", truncation=True)\n\ndblock = DataBlock(\n    blocks=(ImageBlock, txt_block),\n    get_items=get_image_files,\n    item_tfms=Resize(224),\n    batch_tfms=[\n        IntToFloatTensor(),\n        Normalize.from_stats(INPUT_MEAN, INPUT_STD),\n    ],\n    get_y=get_y,\n    splitter=RandomSplitter(),\n)\n","f8d1bb39":"def batch_x_merge_y(batch, b):\n    x, y = batch\n    return (x, y), y\n\ndblock.dls_kwargs['retain'] = batch_x_merge_y","c4b4414a":"dsets = dblock.datasets(IMAGE_DATA)\nx, y = dsets.train[0]","2a5cb81c":"print(f\"text being:\\t{y}\")\nx.resize((224,224))","8b7ec835":"dls = dblock.dataloaders(IMAGE_DATA, bs=16)","240b3fe2":"batch = dls.one_batch()","a89f1ac5":"(batch_img, batch_txt), batch_txt2 = batch\nbatch_img.shape, batch_txt.shape, batch_txt2.shape","2132a018":"# encoder focus on encoding the features from image\n# in thie case, we use a ViT model pretrained on imagenet classification task\nencoder = AutoModel.from_pretrained('google\/vit-base-patch16-224-in21k')\n\n# decoder focus on decoding the image features to text sequence\n# here we use GPT2 as our decoder, a pretrained model good at generating text\ndecoder = AutoModelWithLMHead.from_pretrained(\"gpt2\", add_cross_attention=True)\ndecoder.tokenizer = tokenizer","eeb80f44":"class Caption(nn.Module):\n    \"\"\"\n    The caption model is created by\n        combining decoder and encoder\n    \"\"\"\n    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n        \"\"\"\n        \n        - encoder: The encoder model that can extract image features\n        - decoder: The decoder model that can generate text sequence\n            - you have to set add_cross_attention\n                to True when instantiate the model\n        \"\"\"\n        super().__init__()\n        self.encoder_decoder = EncoderDecoderModel(\n            encoder=encoder,\n            decoder=decoder,)\n        \n        # update generate documentation\n        self.generate.__func__.__doc__ = f\"\"\"\n        Generate text with image:\n        - batch_img: a batch of image tensor\n        - other generate kwargs please see following\n        {self.encoder_decoder.decoder.generate.__doc__}\"\"\"\n        \n    def forward(self, inputs):\n        x, input_ids = inputs\n        # extract image feature with encoder\n        # the extracted feature we call them: encoder_outputs\n        encoder_outputs = self.encoder_decoder.encoder(x)\n        \n        # predict text sequence logits\n        # with the encoder_outputs\n        seq_out = self.encoder_decoder(\n            encoder_outputs=encoder_outputs,\n            # decoder_inputs is to help decoders learn better, \n            # decoder has mask that allow model to see\n            # only the previous text tokens and encoder feature\n            decoder_input_ids=input_ids,\n            labels=input_ids,\n            )\n        return seq_out\n    \n    def generate(self, batch_img, **generate_kwargs):\n        with torch.no_grad():\n            # extract image features first\n            encoder_outputs = self.encoder_decoder.encoder(pixel_values=batch_img)\n            return self.encoder_decoder.decoder.tokenizer.batch_decode(\n                # encoder_decoder has a 'generate' function we can use\n                self.encoder_decoder.generate(\n                    encoder_outputs=encoder_outputs,\n                    **generate_kwargs), )","f2ffde13":"def use_output_loss(output, label):\n    return output.loss","0903dcd9":"def hf_accuracy(output, label):\n    return (output.logits.argmax(-1)==label).float().mean()","be268ad2":"model = Caption(encoder, decoder)","92d6a499":"learn = Learner(dls, model, loss_func=use_output_loss, metrics=[hf_accuracy], )","e7864fb4":"learn.fit(20)","d75090cb":"encoder = encoder.eval()","aa41535f":"from torchvision import transforms as tfm\nfrom PIL import Image\n\ndef image_2_caption(img_path, caption:Caption) -> str:\n    \"\"\"Get caption from an image (need \"model\" and \"tokenizer\")\"\"\"\n    img = Image.open(img_path)\n    image_to_batch = tfm.Compose([\n        tfm.Resize((224,224)),\n        tfm.ToTensor(),\n        tfm.Normalize(INPUT_MEAN, INPUT_STD)\n    ])\n\n    batch = image_to_batch(img)[None, :].to(device)\n    vocab = caption.encoder_decoder.decoder.tokenizer.vocab\n   \n    return caption.generate(\n        batch,\n        top_p=0.6,\n        do_sample=True,  # text generation startegy\n        bos_token_id=vocab[\":\"],\n    )[0].replace(\"<|endoftext|>\", \"\")","d952b36a":"import matplotlib.pyplot as plt\n\nimg_path = Path(\"..\/input\/coco2014\/train2014\/train2014\/COCO_train2014_000000000036.jpg\")\ncaption = image_2_caption(img_path, model)\n\nprint()\nplt.imshow(Image.open(img_path))\nplt.title(caption)\nplt.show()","d3d20d08":"import pickle\n\ndef save_to_pickle(data, path:str):\n    with open(path, 'wb') as f:\n        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)","4afbee29":"model.encoder_decoder.save_pretrained(\"model_encoder_decoder\")\nsave_to_pickle(model.encoder_decoder, \"encoder_decoder.pkl\")","9ef34915":"import pickle\n\ndef load_pickle(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)","05adf8cd":"encoder_decoder = load_pickle(\"encoder_decoder.pkl\")","bf65b0c4":"loaded_caption = Caption(encoder=encoder_decoder.encoder, decoder=encoder_decoder.decoder)","b4945815":"import matplotlib.pyplot as plt\n\nimg_path = Path(\"..\/input\/coco2014\/train2014\/train2014\/COCO_train2014_000000000036.jpg\")\ncaption = image_2_caption(img_path, loaded_caption)\n\nprint()\nplt.imshow(Image.open(img_path))\nplt.title(caption)\nplt.show()","70c4d0e5":"### Exporting","1cd28a46":"# Image Caption","c2f13b7e":"### Inference based on the pre-trained model\n\nThe code for inference from pre-trained model can be seen here:\nhttps:\/\/www.kaggle.com\/jeffthuong\/inference-image-captioning-pre-trained\n\nor, if we want to deploy to a Streamlit app, here:\nhttps:\/\/github.com\/unpackAI\/unpackai\/tree\/main\/examples\/deploy\/CV2NLP\n","c273ed2d":"## Training\n\nCreate the model with the pretrained sub-models: ```encoder``` and ```decoder```","f82a5bcc":"### Pre-Trained Models","0a0d1834":"### Caption Model","e156364d":"### Inference based on pickle","a5d43028":"This (x,y), y format is very unique for encoder-decoder model.\n\nWhere encoder will extract features from x(image)\n\nAnd decoder use the **encoder output**+**prompt from previous tokens** to guess the next token\n\nWhen we train the model, we guess the $n_{th}$ token by\n\n$\\hat{y_n}=EncoderDecoder(x,y_{[1:n]})$\nThen we calculate the loss:\n$L = CrossEntropy(\\hat{y}, y)$,\n\nWithin this encoder_decoder model, this is what happens: $Feature=encoder(x), \\hat{y_{n}}=decoder(Feature, y_{[1:n-1]})$\n\nHence (x,y), y from the dataloaders instead of usual x, y for other deep learning tasks\n\nWhen we generate text after training, for inference, we input only x\n\n","643f4187":"### Dataloaders","60000e63":"Display a line of data","35f3d05f":"### Our thinking\n> About the model structure\n\n* Usually , People attempt this task with CNN + RNN. For example, a Resnet(residual CNN) backbone to read an image, and a LSTM (gated RNN) layer to decipher the feature into words. Most of the models tried this **encoder + decoder** approach.\n\n* Then comes the transformer, many papers have tried to use Transformer's mighty sequence analyze power to replace the RNN part.\n\n* Since ```ViT``` model is slowly creaping up all sorts of computer vision leaderboard, it's a good time to use a full transformer network for the Image Caption job, which sadly has already been studied in the paper [CPTR: Full Transformer Network for Image Caption (Liu et. al)](https:\/\/arxiv.org\/pdf\/2101.10804.pdf), written only 9 months ahead of this notebook. But the model's detail is not the same at all, besides, the basic idea of a purely transformer model.\n\n* Here we use ViT as encoder, GPT2 as decoder, all pretrained.\n\n* And I promise you this is the shortest form of the training notebook I've seen on this topic\n\nRay 2021.10.19","cf036bd5":"We need to make the encoder in `eval` mode for inference","171db376":"For the arguments of **generating text**, you can use the following as default\n\nOr try to understand this very well written [blog post](https:\/\/huggingface.co\/blog\/how-to-generate) explaining what are the arguments exactly","ae7027a9":"## Model","709052f4":"## The fastai datablock","2e5ff19f":"### Loss Function","07fb480f":"Since the output format, the original accuracy(which compares logits with label) won't work, let's redifine one","4c553ab1":"The model output object contains ```logits``` and ```loss```\n\nThe loss result is already in the model output. So our loss function here is only to get the loss number.\n\nBehind the scene, transformer calculated the loss using ```nn.CrossEntropyLoss```","0d8e3b37":"To generate text, we use the transformer decoder for the task, which require the generated target as a part of `x`","0f3fd4b2":"## See the result"}}