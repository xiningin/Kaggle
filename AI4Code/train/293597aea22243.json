{"cell_type":{"ff0e38ad":"code","37bfa772":"code","bb10056f":"code","087e270b":"code","2b2424f3":"code","1f484d97":"code","5d99528a":"code","c0bf69ee":"code","36c93232":"code","31a94a98":"code","7ebc69f8":"code","00450dbd":"code","52d24851":"code","9a29c585":"code","60fa4e34":"code","353b3b38":"code","615a1a50":"code","f82999e9":"code","db9ec12e":"code","fcba34e2":"code","e4552456":"code","7976a396":"code","f9b8b8c0":"code","ecf3bf68":"code","c8bb6996":"markdown","47eb67f6":"markdown","34abc560":"markdown","8d3a765c":"markdown","1ca35746":"markdown","2054547e":"markdown","d8a32802":"markdown","f3583abf":"markdown","bb383b70":"markdown","55bf04b3":"markdown","e4682e85":"markdown","4fdf00aa":"markdown","9e5aa65b":"markdown"},"source":{"ff0e38ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('dark_background')\nimport os\nimport inspect\n\nimport imblearn\nfrom tensorflow import keras\nimport gc # library to free up memory usage\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37bfa772":"# Reading the input data.\n\n# Train Data\ndirectory = r'..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train'\ntrain_ls  = []\nfor folder in os.listdir(directory):\n    if folder == 'NORMAL':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            train_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,0))\n    if folder == 'PNEUMONIA':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            train_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,1))\n        \ntrain_df = pd.DataFrame(train_ls,columns=['Image','Label'])\ntrain_ls.clear()\n\n# Test Data\ndirectory = r'..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test'\ntest_ls  = []\nfor folder in os.listdir(directory):\n    if folder == 'NORMAL':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            test_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,0))\n    if folder == 'PNEUMONIA':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            test_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,1))\n        \ntest_df = pd.DataFrame(test_ls,columns=['Image','Label'])\ntest_ls.clear()\n\n# Validation Data\ndirectory = r'..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val'\nval_ls  = []\nfor folder in os.listdir(directory):\n    if folder == 'NORMAL':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            val_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,0))\n    if folder == 'PNEUMONIA':\n        for i in glob.glob(os.path.join(directory,folder)+'\/\/*'):\n            val_ls.append((cv2.resize(cv2.imread(i,cv2.IMREAD_GRAYSCALE),(224,224))\/255.0,1))\n        \nval_df = pd.DataFrame(val_ls,columns=['Image','Label'])\nval_ls.clear()","bb10056f":"# Lets have a look at our images for both normal and pneumonia\nnormal_sample = train_df[train_df['Label']==0][:5]\npneumonia_sample = train_df[train_df['Label']==1][:5]\n\nfig, ax = plt.subplots(1,5, figsize=(15,15))\nfor i in range(0,5):\n    ax[i].set_title('Normal')\n    ax[i].imshow(normal_sample.iloc[i,0],cmap='gray')\n\nfig, ax = plt.subplots(1,5, figsize=(15,15))\nfor j in range(0,5):\n    ax[j].set_title('Pneumonia')\n    ax[j].imshow(pneumonia_sample.iloc[j,0],cmap='gray')","087e270b":"sns.countplot(train_df['Label'])\nplt.show()\nprint(train_df['Label'].value_counts())","2b2424f3":"# Creating Train and Test data.\nX_train = np.array(train_df.drop('Label',axis=1).iloc[:,0])\nX_train = np.array([x.reshape(224,224,1) for x in X_train])\ny_train = np.array(train_df['Label'])","1f484d97":"# Creating Train and Test data.\nX_test = np.array(test_df.drop('Label',axis=1).iloc[:,0])\nX_test = np.array([x.reshape(224,224,1) for x in X_test])\ny_test = np.array(test_df['Label'])","5d99528a":"# Creating Train and Test data.\nX_val = np.array(val_df.drop('Label',axis=1).iloc[:,0])\nX_val = np.array([x.reshape(224,224,1) for x in X_val])\ny_val = np.array(val_df['Label'])","c0bf69ee":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights","36c93232":"# Oversampling techniques require our data to be in 2D instead of 4D\n# Converting data from 4D to 2D\n# X_train_2D  = np.array(i.reshape(X_train.shape[0],X_train[i].shape[]) for i in X_train)\nX_train.shape","31a94a98":"from imblearn.over_sampling import RandomOverSampler\noversampler = RandomOverSampler(sampling_strategy=1)\nX_train_res,y_train_res = oversampler.fit_sample(X_train.reshape(X_train.shape[0],(X_train.shape[1]*X_train.shape[2])),y_train)","7ebc69f8":"sns.countplot(y_train_res)\nplt.show()\nprint(pd.Series(y_train_res).value_counts())","00450dbd":"from imblearn.under_sampling import RandomUnderSampler\nundersampler = RandomUnderSampler(sampling_strategy=1)\nX_train_rus,y_train_rus = undersampler.fit_sample(X_train.reshape(X_train.shape[0],(X_train.shape[1]*X_train.shape[2])),y_train)","52d24851":"sns.countplot(y_train_rus)\nplt.show()\nprint(pd.Series(y_train_rus).value_counts())","9a29c585":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nX_train_sm, y_train_sm = smote.fit_sample(X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2]),y_train)","60fa4e34":"sns.countplot(y_train_sm)\nplt.show()\nprint(pd.Series(y_train_sm).value_counts())","353b3b38":"# Since RAM usage has gone high let us see which variables are actually using more memory and free up some space\n# import sys\n\n# local_vars = list(locals().items())\n# for var, obj in local_vars:\n#     print(var, sys.getsizeof(obj))\nX_train.shape","615a1a50":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(rotation_range=10,\n                            height_shift_range=0.1,\n                            width_shift_range=0.1,\n                            zoom_range=0.1,\n                            vertical_flip=True,\n                            horizontal_flip=True)","f82999e9":"from kerastuner import HyperModel\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\nfrom kerastuner.tuners import RandomSearch\nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import LeakyReLU","db9ec12e":"model = Sequential()\nmodel.add(Conv2D(input_shape=(224,224,1),filters = 32, kernel_size=(3,3)))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2),strides=2))\nmodel.add(Flatten())\nmodel.add(Dense(units = 32))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.3,trainable=True))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.001) , metrics=['accuracy'])\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=0.0001)","fcba34e2":"history = model.fit(datagen.flow(X_train_res.reshape(-1,224,224,1),y_train_res,batch_size=128),epochs=5,validation_data=(X_val.reshape(-1,224,224,1),y_val),callbacks=[reduce_lr])","e4552456":"plt.plot(history.epoch,history.history['loss'],'r')\nplt.plot(history.epoch,history.history['val_loss'],'b')\nplt.legend()\nplt.show()","7976a396":"from sklearn.metrics import confusion_matrix\ny_pred = model.predict_classes(X_test.reshape(-1,224,224,1))\ncfm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cfm,annot=True,fmt='g')\nplt.show()","f9b8b8c0":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","ecf3bf68":"del history, model\ndel train_ls, test_ls, val_ls\ndel train_df, test_df, val_df\ndel X_train,y_train\ndel X_train_rus, y_train_rus\ndel y_pred\ndel reduce_lr\ngc.collect()","c8bb6996":"Random-Oversampling","47eb67f6":"### WOW !!! With just a simple model, we got an accuracy of over 87%.\n### Will keep updating this in the next version with more detailed explanation and usage of keras-tuner to do hyper-parameter optimization.","34abc560":"* Combining Over and Under Sampling \n* We can also use a combination of both over and undersampling which might give us some better results.\n* However, for now we will  skip this.","8d3a765c":"Weighted Class","1ca35746":"The computed class weight from sklearn has assigned more weight to the Normal class and lesser weight to the Pneumonia class and thus would try to balance out the effect of lesser record count in the train data.","2054547e":"From the above plot we can see that the no. of records in the majority class has been reduced to 1341 as that of minority class.","d8a32802":"So we notice that our dataset is higly imbalanced.\nThis would cause our model to become biased towards more of pneumonia cases and would we would want it to give it a fair result.\nTo over-come such a problem we need to balance the dataset before giving it to our CNN model to train.\nThere are multiple ways to balance out the dataset few of which are listed below.\n1. Class Weight Balancing \n    In this methods we would adjust the weights in such a way that our dominant class has lesser weight than the other class.\n\n2. Re-sampling \n    This method allows us to re-select our dataset in such a way so that we get an overall balanced set of training records from both the classes.\n    These are fast to implement and can be used for both binary and multi-class classification problem.\n    This however, can  be done further in two ways\n        \n        2.1 Oversampling \n                In this method we would randomly select records from the smaller class ( Class 0 in our case ) with replacement ( meaning one record is selected from the                     minority dataset, added to the balanced dataset and replaced back in the original dataset to be selected again, in simple words a random sample can be chosen                 multiple times for the balanced dataset) until we have a balanced set of records in both the classes. \n                This method has a drawback that the samples selected would be in a way just the duplicate copies of the already exsiting records and might cause our algorithm                 to  overfit.\n            \n        2.3 Undersampling \n                In this method we remove random records from the majority class until we have a balanced set of class in our training data to train.\n                This again has a drawback that we might loose on important information while performing undersampling.\n            \n\n3. SMOTE - Synthetic Minority Oversampling\n            In this method, we randomly select a minority point and find it's N near neighbours.\n            Join a line between the minority point and the neighbour found.\n            Create a synthetic point on the line joining between the minority point and the neighbouring point.\n            Repeat the above steps till the dataset is balanced.\n            \n\n4. Data Augmentation using ImageDataGenerator\n            In this method we would create new images from by randomly selecting minority images using the ImageDataGenerator class.\n\n","f3583abf":"Random Undersampling","bb383b70":"From the plot above we can see that we have ramped up the sample records to 3875 by using over - sampling technique.","55bf04b3":"# Building model ","e4682e85":"Lets apply all these methods one by and one and create copies of our training data set. Finally at the end of the section we will train our model on each of these datasets and see what accuracy we get on each of them individually.","4fdf00aa":"Data Augmentation using ImageDataGenerator\n\n\n***WIP***","9e5aa65b":"SMOTE"}}