{"cell_type":{"70eddc36":"code","ea53f692":"code","b8e90cd7":"code","64deb51e":"code","1538b441":"code","c0d9405e":"code","40eef09a":"code","16548229":"code","34f85c78":"code","e485e68b":"code","a3e9f82d":"code","4539e421":"code","d9195b3c":"code","da28e7ab":"code","5d79ac5f":"code","820eadd4":"code","af2535d6":"code","c05e1ee2":"code","0d1a6984":"code","664a05ea":"code","bf2cfd67":"code","8c28907a":"code","b9539ad7":"code","91a5c17b":"code","f4d6ea2c":"code","dbe20ed5":"code","d26605c8":"code","9d16ca33":"code","f1d30af8":"code","d30fb8b5":"code","c1a34533":"code","911c92be":"code","c0749fd6":"code","f5f6fbf4":"code","ce2bdc4a":"code","111d6775":"code","6fb9d6c5":"code","955cd43b":"code","d8fe1460":"code","fb08004a":"code","dfd48582":"code","38ec524f":"code","9b2177fa":"code","e18124d5":"code","9c2ee41b":"markdown","1edb00e4":"markdown","61fb63b5":"markdown","4b0777c6":"markdown","c676e970":"markdown","a9fe284d":"markdown","3f73db7a":"markdown","649bb011":"markdown","3f1a03b3":"markdown","b286ba8f":"markdown","fec76a4d":"markdown","4474c3e7":"markdown","2afab588":"markdown","12d2f868":"markdown","60055320":"markdown","07a3db5c":"markdown","304030dd":"markdown","fca2658a":"markdown","bb460b25":"markdown","c5c99796":"markdown","323aab0e":"markdown","d9451143":"markdown","d16b0b43":"markdown","1e2376bf":"markdown","805ac870":"markdown","5ef1f545":"markdown","b5f27c1d":"markdown","774aaa3e":"markdown","3717e995":"markdown","5543a105":"markdown","3fa0427e":"markdown","63132d4b":"markdown","9969538b":"markdown","90bf1ccd":"markdown","21378c93":"markdown","07c25d12":"markdown","d8108c9e":"markdown","72dd8507":"markdown"},"source":{"70eddc36":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler,RobustScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve, train_test_split, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom IPython.display import display, HTML\n\n# Math\nimport math\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom scipy.stats import boxcox\nfrom scipy.special import boxcox1p\n\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Data Validation\nfrom sklearn import metrics\n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","ea53f692":"df = pd.read_csv('..\/input\/bank-additional-full.csv',sep=';')","b8e90cd7":"df","64deb51e":"df.columns","1538b441":"df.describe()","c0d9405e":"df.info(verbose=True)","40eef09a":"df['y'] = df['y'].map( {'no':0, 'yes':1})","16548229":"df.corr()['y'].sort_values(ascending = False)","34f85c78":"def plot_cat_features(cat_features, nrows, ncols):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 5))\n\n    grid = gridspec.GridSpec(nrows=nrows, ncols=ncols,  figure=fig)\n    row = 0\n    column = 0\n    for cat in cat_features:\n        ax1 = fig.add_subplot(grid[0, column])\n        sns.countplot(df[cat], ax=ax1)\n        plt.xticks(rotation=90)\n\n        column += 1\n        if column == ncols:\n            column = 0\n            row += 1","e485e68b":"plot_cat_features(['job','education', 'marital'], 1, 3)","a3e9f82d":"plot_cat_features(['default', 'housing', 'loan'], 1, 3)","4539e421":"plot_cat_features(['contact', 'month','poutcome'], 1, 3)","d9195b3c":"def plot_numer_features(numer_features, nrows, ncols):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n\n    grid = gridspec.GridSpec(nrows=nrows, ncols=ncols, figure=fig)\n    row = 0\n    column = 0\n    for numer in numer_features:\n        ax1 = fig.add_subplot(grid[row, column])\n        sns.distplot(df[numer], kde=False, ax=ax1)\n        plt.xticks(rotation=90)\n\n        column += 1\n        if column == ncols:\n            column = 0\n            row += 1","da28e7ab":"plot_numer_features(['age','duration', 'campaign','pdays', 'previous'], 2, 3)","5d79ac5f":"def plot_numer_features(numer_features, nrows, ncols):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n\n    grid = gridspec.GridSpec(nrows=nrows, ncols=ncols, figure=fig)\n    row = 0\n    column = 0\n    for numer in numer_features:\n        ax1 = fig.add_subplot(grid[row, column])\n        sns.distplot(df[numer], kde=False, ax=ax1)\n        plt.xticks(rotation=90)\n\n        column += 1\n        if column == ncols:\n            column = 0\n            row += 1","820eadd4":"plot_numer_features(['emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m'], 2, 2)","af2535d6":"def plot_cat_features_withTarget(cat_features, nrows, ncols):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n\n    grid = gridspec.GridSpec(nrows=nrows, ncols=ncols,  figure=fig)\n    row = 0\n    column = 0\n    for cat in cat_features:\n        ax1 = fig.add_subplot(grid[row, column])\n        sns.countplot(df[cat],hue=df['y'], ax=ax1)\n        plt.xticks(rotation=90)\n\n        column += 1\n        if column == ncols:\n            column = 0\n            row += 1","c05e1ee2":"fig = plt.figure()\nsns.countplot(df['y'])","0d1a6984":"df.groupby('contact')['y'].mean().sort_values(ascending=False)","664a05ea":"pd.DataFrame(df.groupby('contact')['y'].mean().sort_values(ascending=False))","bf2cfd67":"plot_cat_features_withTarget(['job', 'education', 'marital', 'contact'], 2, 2)","8c28907a":"# Check our declaration above\ndef check_observation (features):\n    for fea in features:\n        display(pd.DataFrame(df.groupby(fea)['y'].mean().sort_values(ascending=False)))\n\ncheck_observation(['job','education','marital','contact'])","b9539ad7":"def plot_cat_features_withTarget(features):\n    for feat in features:\n        target_0 = df.loc[df['y'] == 0, feat]\n        target_1 = df.loc[df['y'] == 1, feat]\n\n        sns.distplot(target_0,hist=False, rug=True, label='non-converted')\n        sns.distplot(target_1,hist=False, rug=True, label='converted')\n        plt.show()\n    \n        # Source: https:\/\/stackoverflow.com\/questions\/26873127\/show-dataframe-as-table-in-ipython-notebook\/29665452#29665452\n        display(df.groupby('y')[feat].describe())","91a5c17b":"plot_cat_features_withTarget(['age'])","f4d6ea2c":"bins = [-1, 250, 500, 750, np.inf]\nnames = [0, 1, 2, 3]\ndf['new_duration'] = pd.cut(df['duration'], bins, labels=names).astype('int')\n\nbins = [-1, 1, np.inf]\nnames = [0, 1]\ndf['new_previous'] = pd.cut(df['previous'], bins, labels=names).astype('int')\n\nbins = [10, 20, 25, 60, 100]\nnames = ['Youth','Millennials','WorkingClass','RetirementClass']\ndf['new_age_Class'] = pd.cut(df['age'], bins, labels=names)\n\nbins = [-1, 3, 6, 60]\nnames = ['< 3Times','< 6Times','< 100Times']\ndf['new_campaign'] = pd.cut(df['campaign'], bins, labels=names)\n\nbins = [-65, -50, -40, -30, -20]\nnames = ['-65,-50','-50,-40','-40,-30','-30,-20']\ndf['new_cons.conf.idx'] = pd.cut(df['cons.conf.idx'], bins, labels=names)\n\nbins = [91, 93, 94, 96]\nnames = ['<92-93','93-94','94-95']\ndf['new_cons.price.idx'] = pd.cut(df['cons.price.idx'], bins, labels=names)\n\nbins = [-3.5, -1, 1.5]\nnames = ['-3.5to-1','-1to1.4']\ndf['new_emp.var.rate'] = pd.cut(df['emp.var.rate'], bins, labels=names)\n\nbins = [0, 1, 6]\nnames = ['0to1','1to6']\ndf['new_euribor3m'] = pd.cut(df['euribor3m'], bins, labels=names)","dbe20ed5":"\n\"\"\"\n#arr = np.array([1,np.nan])\n#np.unique(df['nr.employed'])\n#df['nr.employed'].unique()\n\nCheck the null value\n    df['cons.conf.idx'].isnull().sum()\n\n\"\"\"","d26605c8":"df['job'].replace('unknown', np.nan, inplace=True)\ndf['education'].replace('unknown', np.nan, inplace=True)\ndf['marital'].replace('unknown', np.nan, inplace=True)\ndf['default'].replace('unknown', np.nan, inplace=True)\ndf['housing'].replace('unknown', np.nan, inplace=True)\ndf['loan'].replace('unknown', np.nan, inplace=True)","9d16ca33":"df.groupby('education')['age'].agg(['mean','median'])","f1d30af8":"#https:\/\/stackoverflow.com\/questions\/44061607\/pandas-lambda-function-with-nan-support\/44061892#44061892\ndef categorical_feature_cleaning(df):\n# Job Cleaning\n    if ((pd.isnull(df['job'])) & ((df['education']=='basic.4y') | (df['education']=='basic.6y') | (df['education']=='basic.9y'))):\n        df['job'] = 'blue-collar'\n    elif ((pd.isnull(df['job'])) & (df['education']=='high.school')):\n        df['job'] = 'services'\n    elif ((pd.isnull(df['job'])) & (df['education']=='professional.course')):\n        df['job'] = 'technician'    \n    elif ((pd.isnull(df['job'])) & (df['education']=='university.degree') & (df['marital']=='single')):\n        df['job'] = 'admin'\n    elif ((pd.isnull(df['job'])) & (df['education']=='university.degree')):\n        df['job'] = 'management'            \n    \n# Education Cleaning    \n    if ((pd.isnull(df['education'])) & (df['job']=='blue-collar') & (df['age']<=39)):\n        df['education'] = 'basic.6y'\n    elif ((pd.isnull(df['education'])) & (df['job']=='blue-collar') & (df['age']<=100)):\n        df['education'] = 'basic.4y'\n    elif ((pd.isnull(df['education'])) & (df['job']=='housemaid')):\n        df['education'] = 'basic.4y'\n    elif ((pd.isnull(df['education'])) & (df['job']=='retired')):\n        df['education'] = 'basic.4y'\n    \n    elif ((pd.isnull(df['education'])) & (df['job']=='admin.')):\n        df['education'] = 'university.degree'\n    elif ((pd.isnull(df['education'])) & (df['job']=='entrepreneur')):\n        df['education'] = 'university.degree'\n    elif ((pd.isnull(df['education'])) & (df['job']=='management')):\n        df['education'] = 'university.degree'\n    elif ((pd.isnull(df['education'])) & (df['job']=='self-employed')):\n        df['education'] = 'university.degree'\n    \n    elif ((pd.isnull(df['education'])) & (df['job']=='services')):\n        df['education'] = 'high.school'\n    elif ((pd.isnull(df['education'])) & (df['job']=='student')):\n        df['education'] = 'high.school' \n    elif ((pd.isnull(df['education'])) & (df['job']=='unemployed')):\n        df['education'] = 'high.school'\n        \n    elif ((pd.isnull(df['education'])) & (df['job']=='technician')):\n        df['education'] = 'professional.course'\n        \n# both Job and Education have missing values\n    if ((pd.isnull(df['job'])) & (pd.isnull(df['education']))):\n        if (df['age'] <= 36):\n            df['job'] = 'student'\n            df['education'] = 'university.degree'\n        elif (df['age'] < 55):\n            df['job'] = 'blue-collar'\n            df['education'] = 'basic.9y'\n        else:\n            df['job'] = 'retired'\n            df['education'] = 'basic.4y'\n\n# # Marital cleaning\n    if pd.isnull(df['marital']):\n        df['marital'] = 'married'\n\n# Default cleaning\n    if pd.isnull(df['default']):\n        df['marital'] = 'no'\n                 \n# Housing cleaning\n    if pd.isnull(df['housing']):\n        df['marital'] = 'yes'\n\n# Loan cleaning\n    if pd.isnull(df['loan']):\n        df['loan'] = 'no'\n# Number Employed\n    if pd.isnull(df['nr.employed']):\n        df['nr.employed'] = 5191  \n    \n    return df","d30fb8b5":"df = df.apply(categorical_feature_cleaning, axis=1)","c1a34533":"df = pd.get_dummies(df, drop_first=True)\ndf1 = df","911c92be":"x = RobustScaler().fit_transform(df.drop('y', axis=1))\ny = df['y']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, stratify=y, random_state=0)","c0749fd6":"logreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\nknn = KNeighborsClassifier(n_neighbors=13)\n\ngnb = GaussianNB()\n\nlinearSVC = LinearSVC()\n\nRbfSVC = SVC()\n\ndt = DecisionTreeClassifier(max_depth=10)\n\nrf = RandomForestClassifier(random_state=0,n_jobs=-1,verbose=0)\n\nadab = AdaBoostClassifier(random_state=0)\n\ngb = GradientBoostingClassifier(random_state=0)\n\nxgb = XGBClassifier(random_state=0)\n\nlgbm = LGBMClassifier(random_state=0)","f5f6fbf4":"def Best_Parameters_model(est, para):\n    model_table = {}\n    \n    MLA_name = est.__class__.__name__\n    model_table['Model Name'] = MLA_name\n\n    pipe = make_pipeline(GridSearchCV(estimator=est,\n                                      param_grid=para,\n                                      scoring='accuracy',\n                                      cv=3,\n                                      n_jobs=-1,\n                                      verbose=0, refit=True))\n    pipe_result = pipe.fit(x_train, y_train)\n\n    model_table['Best Test Accuracy Mean'] = pipe_result[0].best_score_\n    model_table['Best Parameters'] = pipe_result[0].best_params_\n    model_table['Test Dataset Score'] = pipe.score(x_test, y_test)\n    return model_table\n\n# GBoost = GradientBoostingClassifier(random_state=0)\n# GBoost_para = {'learning_rate':[0.1, 0.01],\n#                'n_estimators':[100,500],\n#                'subsample':[0.9, 0.95],\n#                'criterion':['friedman_mse'],\n#                'min_samples_split': [4, 5, 6],\n#                'max_depth':[3, 4, 5],\n#                'max_features':['sqrt']\n#               }\n# Best_Parameters_model(GBoost,GBoost_para)","ce2bdc4a":"cv = StratifiedKFold(10, shuffle=True, random_state=0)\n\n\ndef model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n    \n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        est.fit(x_train, y_train)\n        \n        model_table.loc[row_index, 'Train Accuracy Mean'] = est.score(x_train, y_train)\n        model_table.loc[row_index, 'Test Accuracy Mean'] = est.score(x_test, y_test)\n\n        row_index += 1\n\n        model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","111d6775":"estimators = [logreg,knn,gnb,linearSVC,RbfSVC,dt,rf,gb,xgb,lgbm]","6fb9d6c5":"raw_models = model_check(x, y, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","955cd43b":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"DecisionTree\", dt), (\"RF\", rf), (\"GradientBoosting\", gb), (\"XGB\", xgb)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=df.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","d8fe1460":"df1 = pd.read_csv('..\/input\/bank-additional-full.csv',sep=';')\ndf1['y'] = df1['y'].map( {'no':0, 'yes':1})","fb08004a":"a = df1[(df1['job']=='student') | (df1['job']=='retired') | (df1['job']=='unemployed')]","dfd48582":"sns.countplot(a['job'], hue=a['y'])","38ec524f":"print('Student average age: ', df1[df1['job']=='student']['age'].median())\nprint('Retired average age: ', df1[df1['job']=='retired']['age'].median())\nprint('Unemployed average age: ', df1[df1['job']=='unemployed']['age'].median())","9b2177fa":"df1['campaign_buckets'] = pd.qcut(df1['campaign'], 20, labels=False, duplicates = 'drop')\n\nmean_campaign = df1.groupby(['campaign_buckets'])['y'].mean()\nmean_campaign = mean_campaign.reset_index()\n\nsns.lineplot(x=mean_campaign['campaign_buckets'],y=mean_campaign['y'])\nplt.ylabel('Percentage of Success')\nplt.show()","e18124d5":"bins = [-1, 50, np.inf]\nnames = [0, 1]\ndf1['pdays_bucket'] = pd.cut(df1['pdays'], bins, labels=names).astype('int')\n\nmean_campaign = df1.groupby(['pdays_bucket'])['y'].mean()\nmean_campaign = mean_campaign.reset_index()\n\nsns.lineplot(x=mean_campaign['pdays_bucket'],y=mean_campaign['y'])\nplt.ylabel('Percentage of Success')\nplt.show()","9c2ee41b":"# Credit\nThis work is inspired by multiple great sources done before:\n- https:\/\/www.kaggle.com\/aleksandradeis\/bank-marketing-analysis\n- https:\/\/www.kaggle.com\/janiobachmann\/bank-marketing-campaign-opening-a-term-deposit","1edb00e4":"# FEATURE ENGINEERING","61fb63b5":"#### One-hot Encoding","4b0777c6":"#### Numerical features clearning\nduration, previous, age, campaign, cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m","c676e970":"#### Categorical feature cleaning\njob, education, marital, default, housing, loan","a9fe284d":"# Bank Marketing Prediction of Subscription\n\n### Author: Vu Duong\n\n#### Date: May 28th, 2020","3f73db7a":"### VISUALIZATION\n- dist: numerical\n- count: categorical\n- scatter: numerical + categorical\n- box: numerical\n- bar","649bb011":"##### Observation\n- Non-converted and converted group share similar age distribution in which age average is around 40 ","3f1a03b3":"##### Observation 5: pdays\nAs we have seen pdays diagram below:\n- If a person had been exposed to one of our marketing campaign before, there would be above 60% of success to make a person to convert.","b286ba8f":"##### Observation 1: Duration\nFor those spending great amount of time discussing term deposit are more likely to take subscription.","fec76a4d":"# Approach\nTo optimize the marketing campaign with dataset description, we follow these steps:\n1. Importing essential library for data processing, data visualizing, data modeling, data validation\n2. Importing dataset\n3. Exploring data by reading description and information, seeing the correlation between features, looking for row numbers and missing values\n4. Cleaning the data: remove irrelevant columns, generate new features, deal with missing value based on observation and domain knowledge, turn categorical data into dummy variables.\n5. Building model with different algorithms like Logistic regression, Support Vector Classifier, XGBoost, ... for comparison. \n6. Evaluating model with libraries \n7. Interpreting model by ploting a graph to see the most important features","4474c3e7":"##### Observation\n- Job : Admin, Blue-collar, Technician come as the 1st, 2nd, 3rd largest propotion of all, while unknown and housemaid account for smallest parts.\n- Education: University and high school are 2 largest, whereas unknown and illiterate hold tiny segments.\n- Marital:  married is double number of single and fourfold as divorced. Unknown seems to hit 0.\n\n==> There may be a relationship between job and education and partially impacted by martial\n- As we may predict, having university degree tend to become admin, technician or services. While education at the level of high school or 9 years may end up being blue-collar.\n- For those with lower education level or older may get divorced, as the majority of the young are single or married.","2afab588":"##### Observation\n- Job: Student are most likely to convert, as oppose to blue-collar people.\n- Education: apart from unknown and illterate groups, people with university and professional level are likely to convert, as oppose to those at 4, 6, and 9 year level.\n- Marital: for those being single are more likely to convert, while people with divorced and married label are similar when it comes to conversion.","12d2f868":"### Data Cleaning\n- Preparing dataset before apply machine learning algorithms\n- Group numerical features into bins for the ease of analysis\n- Fill unknown or null values from those features based on the visualization above and our understanding of dataset\n- Convert categorical into dummy variables","60055320":"### Train-Test Split\n- Using Robust Scaler gives better result on prediction as it mitigate the pain of outliers\n- Using stratify as the way to make sure proporation of sample of each label in train and test dataset are the similar.","07a3db5c":"#### Observation\n- Employment Variation Rate: there are some points on the negative side.\n- Consumer Price Index: over 2 years from 2008 to 2010, the figure fuctuate around 92 and 95\n- Consumer Confidence Index: levels of optimism regarding current economic conditions are seriously hopeless.\n\n==> The Great Recession period between 2007 and 2009 marked global declination. This was said as the worst  financial crsis in global history.","304030dd":"##### Observation\n- Age: the distribtion is right skewed. Some people are up to almost 100 years old, which means they are outlilers.\n- Duration: the distribtion is right skewed. Later, we will explore when people spend more time on last contact they are likely to convert.\n- Campaign: the distribtion is right skewed. We will explore if more contacts will lead to conversion or not.\n- Pdays: most people are not contacted by a previous campaign, 999 explains.\n- Previous: the number of contacts before this campaign is range between 0 and 6. Most people were not contacted previously.","fca2658a":"##### Observation\n- Replace unknown value with nan for an ease of filling missing data\n- Using groupby function as below and Tableau Prep Builder to know how to fill in missing data.\n- The feature engineering does not help accuracy improvement as compared to non-feature engineering.","bb460b25":"### Models","c5c99796":"##### Observation 2-3: job and age\nAs we have seen the diagram of job, age below:\n- We should focus on 3 job groups of student, retired, and unemployed.\n- For student, we focus on those below 26 years old. \n- For retired, we focus on those above 60 years old. \n- For unemployed, we focus on those around 40 years old.\n\nThe reasons behind might be that students work as a part-time job and save their money in saving account to earn interest. Retired individuals tend to have term deposits to gain interest payment from the bank until the due date. This group tend not to spend greatly on personal interest rather leading it to the financial institution. \n\nThus, it will be great if the next campaign addressed these 3 categories, increasing the likelihood of more subscriptions.","323aab0e":"##### Observation\n\nAs we can see from 4 diagram below, the most important features are:\n- Duration\n- nr.employed\n- euribor3m\n- pdays\n- campaign\n- consumer confident index\n- age\n\nTherefore the outcomes would be:\n- The more time customer spend with the bank, the higher chance they convert.\n- The higher number of employees, as we extracted this information from the internet, the more likely they convert\n- Pday is important as well. The number of days passing by since the last contact with customer.","d9451143":"### Recommendations","d16b0b43":"##### Observation\n- Default: most people do not default\n- Housing: the number of Yes is sightly higher than that of No, implying people have tendency toward house financing \n\n==> Loan: even some people make a loan or housing but they have not defaulted yet. ","1e2376bf":"# INTRODUCTION\n\nFor most product or any organizations conducting analysis of marketing data is one the most critical skill so as to contribute a huge impact on the financial budget.\n\n \nThis is a marketing data which can be used for many business goals:\n1. Customer segmentation who converted and not converted based on the profile of a customer, thus developing more targeted marketing campaigns.\n2. Derive the marketing campaign result for each customer from multiple factors. This makes sense in the way how to run campaign more efficiently. \n\n\nDetailed description of dataset content is described in the following link: https:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing","805ac870":"#### Categorical features affect Target","5ef1f545":"### Model Evaluation\n- Use Best_Parameters_model below as to choose the best parameter yeilding the best prediction. However it is time comsuming a lot.\n- Use model_check to compare among models and choose the best.","b5f27c1d":"# LIBRARY","774aaa3e":"### Feature Importance","3717e995":"### Analysis of the Target feature (deposit)","5543a105":"### EXLORATORY\n- read_csv\n- head\n- describe\n- info\n- correlation","3fa0427e":"##### Observation 4: campaign\nAs we have seen campaign diagram below:\n- The more campaign display to the same group of people does not guarantee to turn ones into our customers","63132d4b":"#### Numerical features\n\nbalance, day, duration, campaign, pdays, previous","9969538b":"##### Observation\n- Contact: cellular is as double number as telephone\n- Month: people made most contact in may, and least in december\n- Poutcome: failure number is higher than success number is","90bf1ccd":"#### Categorical features\n\njob, marital, education, default, housing, loan, contact, month,poutcome","21378c93":"##### Observation\n- Here I group those features into bins. According to my experiment, this does not improve accuracy but more computationally expensive on traning.\n- We can safely remove those new features. However I decided to leave hear.","07c25d12":"#### Numerical features affect Target","d8108c9e":"Convert outcomes from the result of our customers to 0 as no, and 1 as yes so that we can see the inital correlations between features with the target.","72dd8507":"# DATA MODELING"}}