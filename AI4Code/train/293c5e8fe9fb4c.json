{"cell_type":{"39b6e723":"code","141ac7cc":"code","3f0ea25b":"code","0828a23c":"code","359ea4b4":"code","103cb892":"code","bf01fdae":"code","d8fa520d":"code","3a6f2934":"code","a89be676":"code","e5c4a9b3":"code","1186ce2d":"code","edd1d8a1":"code","6087d90a":"code","8d4705fc":"code","b0874f5c":"code","f36f8625":"markdown","a2db4bd7":"markdown","5abd4450":"markdown","b12c9ad6":"markdown"},"source":{"39b6e723":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(27)","141ac7cc":"# setting up default plotting parameters\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = [20.0, 7.0]\nplt.rcParams.update({'font.size': 22,})\n\nsns.set_palette('viridis')\nsns.set_style('white')\nsns.set_context('talk', font_scale=0.8)","3f0ea25b":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint('Train Shape: ', train.shape)\nprint('Test Shape: ', test.shape)\n\ntrain.head()","0828a23c":"# prepare for modeling\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nX_test = test.drop(['id'], axis=1)\n\n# scaling data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","359ea4b4":"# define models\nridge = linear_model.Ridge()\nlasso = linear_model.Lasso()\nelastic = linear_model.ElasticNet()\nlasso_lars = linear_model.LassoLars()\nbayesian_ridge = linear_model.BayesianRidge()\nlogistic = linear_model.LogisticRegression(solver='liblinear')\nsgd = linear_model.SGDClassifier()","103cb892":"models = [ridge, lasso, elastic, lasso_lars, bayesian_ridge, logistic, sgd]","bf01fdae":"# function to get cross validation scores\ndef get_cv_scores(model):\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n    print('CV Mean: ', np.mean(scores))\n    print('STD: ', np.std(scores))\n    print('\\n')","d8fa520d":"# loop through list of models\nfor model in models:\n    print(model)\n    get_cv_scores(model)","3a6f2934":"penalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalty,\n                  C=C,\n                  class_weight=class_weight,\n                  solver=solver)\n\ngrid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n\nprint('Best Score: ', grid_result.best_score_)\nprint('Best Params: ', grid_result.best_params_)","a89be676":"logistic = linear_model.LogisticRegression(C=1, class_weight={1:0.6, 0:0.4}, penalty='l1', solver='liblinear')\nget_cv_scores(logistic)","e5c4a9b3":"predictions = logistic.fit(X_train, y_train).predict_proba(X_test)\n#### score 0.828 on public leaderboard","1186ce2d":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions\n#submission.to_csv('submission.csv', index=False)\nsubmission.head()","edd1d8a1":"loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\npenalty = ['l1', 'l2', 'elasticnet']\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nlearning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\neta0 = [1, 10, 100]\n\nparam_distributions = dict(loss=loss,\n                           penalty=penalty,\n                           alpha=alpha,\n                           learning_rate=learning_rate,\n                           class_weight=class_weight,\n                           eta0=eta0)\n\nrandom = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, scoring='roc_auc', verbose=1, n_jobs=-1, n_iter=1000)\nrandom_result = random.fit(X_train, y_train)\n\nprint('Best Score: ', random_result.best_score_)\nprint('Best Params: ', random_result.best_params_)","6087d90a":"sgd = linear_model.SGDClassifier(alpha=0.1,\n                                 class_weight={1:0.7, 0:0.3},\n                                 eta0=100,\n                                 learning_rate='optimal',\n                                 loss='log',\n                                 penalty='elasticnet')\nget_cv_scores(sgd)","8d4705fc":"predictions = sgd.fit(X_train, y_train).predict_proba(X_test)\n#### score 0.790 on public leaderboard","b0874f5c":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","f36f8625":"### Stochastic Gradient Descent and Random Search\n\nRandom search is a random (obviously) search over specified parameter values.","a2db4bd7":"### Baseline Models","5abd4450":"From this we can see our best performing models out of the box are logistic regression and stochastic gradient descent.  Let's see if we can optimize these models with hyperparameter tuning.\n\n### Logistic Regression and Grid Search\n\nGrid search is an exhaustive search over specified parameter values.","b12c9ad6":"# Generalized Linear Models and Hyperparameter Tuning\n\nIn this notebook we'll explore modules in sklearn's linear_model module and attempt to optimize the top performing models with hyperparameter tuning.\n"}}