{"cell_type":{"21b20bf9":"code","793d65c7":"code","a061c040":"code","b1540786":"code","64cafc28":"code","79dd2861":"code","f0f75735":"code","2b6ba034":"code","321e96e9":"code","51d8c1d9":"code","c59484f8":"code","3c49379d":"markdown","45b3e65d":"markdown","e09af481":"markdown","a2d2cd69":"markdown","e77babfe":"markdown","a8a84425":"markdown"},"source":{"21b20bf9":"import pandas as pd\nimport numpy as np\nimport random\nimport glob\nimport time\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPRegressor\n\nfrom scipy.optimize import minimize\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_rows = 120\npd.options.display.max_columns = 100\n\nimport warnings\nwarnings.simplefilter('ignore')","793d65c7":"N_SPLITS = 10\nN_ESTIMATORS = 10000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 299792458","a061c040":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","b1540786":"INPUT = \"..\/input\/tabular-playground-series-aug-2021\/\"\nINPUT_PRED = \"..\/input\/tps08-pred\/\"\n\nOUTPUT = \".\/output\/\"\nos.makedirs(OUTPUT, exist_ok=True)\n\ntrain = pd.read_csv(INPUT + \"train.csv\")\ntest = pd.read_csv(INPUT + \"test.csv\")\nsubmission = pd.read_csv(INPUT + \"sample_submission.csv\")\n\noof_files = np.sort(glob.glob(INPUT_PRED + \"oof*.npy\"))\npred_files = glob.glob(INPUT_PRED + \"pred*.npy\")\nadd_features = []\n\nfor i, (oof_file, pred_file) in enumerate(zip(oof_files, pred_files)):\n    train[f'pred{i}'] = np.load(oof_file)\n    test[f'pred{i}'] = np.load(pred_file)\n    add_features.append(f'pred{i}')              \n\nscale_features = [col for col in test.columns if 'f' in col]\nfeatures = scale_features + add_features\n\ntarget = 'loss'","64cafc28":"ss = StandardScaler()\ntrain[scale_features] = ss.fit_transform(train[scale_features])\ntest[scale_features] = ss.transform(test[scale_features])","79dd2861":"train.shape, test.shape","f0f75735":"pre_params = {\n    'objective': 'reg:squarederror',\n    'learning_rate': 5e-3,\n    'seed': SEED,\n    'subsample': 0.8,\n    'colsample_bytree': 0.6,\n    'n_estimators': N_ESTIMATORS,\n    'max_depth': 8,\n    'alpha': 20,\n    'lambda': 9,\n    'min_child_weight': 384,\n    'importance_type': 'total_gain',\n    'tree_method': 'gpu_hist'\n}\n\nparams = {\n    'objective': 'reg:squarederror',\n    'learning_rate': 1e-3,\n    'seed': SEED,\n    'subsample': 0.6,\n    'colsample_bytree': 0.4,\n    'n_estimators': N_ESTIMATORS,\n    'max_depth': 16,\n    'alpha': 20,\n    'lambda': 9,\n    'min_child_weight': 128,\n    'importance_type': 'total_gain',\n    'tree_method': 'gpu_hist'\n}","2b6ba034":"mlp_oof = np.zeros(train.shape[0])\nmlp_pred = np.zeros(test.shape[0])\n\nxgb_oof = np.zeros(train.shape[0])\nxgb_pred = np.zeros(test.shape[0])\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=train[features])):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n    X_valid, y_valid = train[features].iloc[val_idx], train[target].iloc[val_idx]\n    X_test = test[features]\n\n    start = time.time()\n    model = MLPRegressor(hidden_layer_sizes=50,\n                         early_stopping=True,\n                         n_iter_no_change=100,\n                         solver='adam',\n                         shuffle=True,\n                         random_state=SEED)\n    model.fit(X_train,y_train)\n\n    mlp_oof[val_idx] = model.predict(X_valid)\n    mlp_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    \n    rmse = mean_squared_error(y_valid, mlp_oof[val_idx], squared=False)\n    print(f\"fold {fold} - mlp rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n    \n    start = time.time()\n    pre_model = xgb.XGBRegressor(**pre_params)\n    pre_model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        verbose=VERBOSE,\n        callbacks = [xgb.callback.EarlyStopping(\n            rounds=EARLY_STOPPING_ROUNDS,\n            save_best=True)]\n    )\n    \n    model = xgb.XGBRegressor(**params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n        xgb_model=pre_model\n    )\n\n    xgb_oof[val_idx] = model.predict(X_valid)\n    xgb_pred += model.predict(X_test) \/ N_SPLITS\n\n    elapsed = time.time() - start\n    rmse = mean_squared_error(y_valid, xgb_oof[val_idx], squared=False)\n    print(f\"fold {fold} - xgb rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nrmse = mean_squared_error(train[target], mlp_oof, squared=False)\nprint(f\"oof mlp rmse = {rmse:.6f}\")\n\nrmse = mean_squared_error(train[target], xgb_oof, squared=False)\nprint(f\"oof xgb rmse = {rmse:.6f}\")\n\nnp.save(\"mlp_oof.npy\", mlp_oof)\nnp.save(\"mlp_pred.npy\", mlp_pred)\n\nnp.save(\"xgb_oof.npy\", xgb_oof)\nnp.save(\"xgb_pred.npy\", xgb_pred)","321e96e9":"def class_optimizer(X, a0, a1):\n    oof = X[0]*a0 + (1-X[0])*a1\n    return mean_squared_error(train[target], oof, squared=False)\n\nres = minimize(\n    fun=class_optimizer,\n    x0=[0.2],\n    args=tuple([mlp_oof, xgb_oof]),\n    method='Nelder-Mead',\n    options={'maxiter': 300})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {1-res.x[0]}\")","51d8c1d9":"ensemble_oof = res.x[0] * mlp_oof + (1-res.x[0]) * xgb_oof\nensemble_pred = res.x[0] * mlp_pred + (1-res.x[0]) * xgb_pred\n\nprint(mean_squared_error(train[target], ensemble_oof, squared=False))","c59484f8":"submission['loss'] = ensemble_pred\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","3c49379d":"# Parameters","45b3e65d":"# Submission","e09af481":"# Datasets","a2d2cd69":"# Ensemble","e77babfe":"# Libraries","a8a84425":"# XGBoost"}}