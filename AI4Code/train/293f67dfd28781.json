{"cell_type":{"52c2dd1e":"code","8a469c74":"code","9aacbd93":"code","33e4befb":"code","07f6c49e":"code","acb792a4":"code","3d02311d":"code","5eb5fcfe":"code","0c52057d":"code","0a44b956":"code","462ee5c9":"code","273a1c37":"code","5e90db01":"code","48da55b8":"code","cb28fc68":"code","6154b93c":"code","331c44d5":"code","4f26112f":"code","3e089002":"code","bc511229":"code","7bc23346":"code","a48e4314":"code","37d5abb4":"markdown","a6c674d3":"markdown","9c995c81":"markdown","0bbaabfe":"markdown","8d6bc84f":"markdown","0058f9b1":"markdown","fec1acb4":"markdown","253abd5d":"markdown","b98a83b6":"markdown","239b8f7a":"markdown","ae998ca6":"markdown","4f79363f":"markdown","29f1cbf9":"markdown","494051c6":"markdown","488d3439":"markdown","582f7c22":"markdown","ed1d0dad":"markdown","279fe647":"markdown"},"source":{"52c2dd1e":"# necessary import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# import os\n# print(os.listdir(\"..\/input\"))","8a469c74":"# create a data set\nnp.random.seed(42)\nX = np.random.random((4, 2))\nX = np.round_(X, 2)\ny = np.array([1, -1, 1, 1])\nprint(X)\nprint(y)","9aacbd93":"# generate initial value of estimator \nrgen = np.random.RandomState(42)\nw = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\nprint(w)","33e4befb":"# evaluate the net input \nxw = np.dot(X[0], w[1:])\nxw_intercept = xw + w[0]\nprint(xw)\nprint(xw_intercept)","07f6c49e":"# Pridiction from a single iteration\ny_hat = np.where(xw_intercept >= 0.0, 1, -1)\nprint(y_hat)","acb792a4":"eta = 0.01\ntarget = y[0]\nupdate = eta * (y[0] -y_hat)\nprint(update)\nw[1:] += update * X[0]\nprint('With out Intercept:', w)\nw[0] += update\nprint('With Intercept:', w)","3d02311d":"errors_ = [] \nerror =0\nerror += int(update != 0.0) # give out put 1 or 0\nprint(error)\nerrors_.append(error)","5eb5fcfe":"# sample data\nnp.random.seed(42)\nX = np.random.random((4,2))\nX = np.round_(X, 2)\ny = np.array([1, -1, 1, 1])\nprint(X)\nprint(y)","0c52057d":"# enitiate the value of estimator\nrgen = np.random.RandomState(42)\nw = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\nprint(w)","0a44b956":"# function to calculate the net input w_transopseX\ndef net_input(x):\n    return np.dot(x, w[1:]) + w[0]","462ee5c9":"# create the threshlold function for prediction\ndef prediction(x):\n    return np.where(net_input(x) >= 0.0, 1, -1)","273a1c37":"# lets look over one single loop\nresult_list = []\nfor x_i, y_i in zip(X, y):\n    dict_ = {'x': x_i,\n             'y': y_i,\n             'net_input': round(net_input(x_i), 3),\n             'y_hat': prediction(x_i)\n            }\n    result_list.append(dict_)","5e90db01":"result_list","48da55b8":"eta = 0.01\nerror_list = []\nerrors = 0\nfor x_i, y_i in zip(X, y):\n    update = eta * (y_i - prediction(x_i))\n    w[1:] += update * x_i\n    w[0] += update\n    errors += int(update != 0.0)\nerror_list.append(errors)","cb28fc68":"error_list","6154b93c":"# iterate the same process multiple time\neta = 0.01\nerror_list = []\nfor _ in range(10):\n    errors = 0\n    for x_i, y_i in zip(X, y):\n        update = eta * (y_i - prediction(x_i))\n        w[1:] += update * x_i\n        w[0] += update\n        errors += int(update != 0.0)\n    error_list.append(errors)","331c44d5":"error_list","4f26112f":"import numpy as np\n\nclass Perceptron(object):\n    \"\"\"Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    errors_ : list\n      Number of misclassifications (updates) in each epoch.\n\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n          Training vectors, where n_samples is the number of \n          samples and\n          n_features is the number of features.\n        y : array-like, shape = [n_samples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, \n                              size=1 + X.shape[1])\n        self.errors_ = []\n\n        for _ in range(self.n_iter):\n            errors = 0\n            for x_i, y_i in zip(X, y):\n                update = self.eta * (y_i - self.predict(x_i))\n                self.w_[1:] += update * x_i\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, -1)","3e089002":"class LogisticSimulator(object):\n    \"\"\"Logistic data generator with 2 feature.\n\n    Parameters\n    ------------\n    size : sample size (int)\n      Any value between 0 and Infinity.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n    Attributes\n    -----------\n    X_ : nd-array\n      Feature metrics\n    y : 1d-arry\n      Outcome variable \"\"\"\n\n    def __init__(self, size=4000, random_state=13):\n        self.size = size\n        self.random_state = random_state\n\n    def sample_generator(self):\n        np.random.seed(self.random_state)\n        x1_ = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], self.size)\n        x2_ = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], self.size)\n        X_ = np.vstack((x1_, x2_)).astype(np.float32)\n        y_ = np.hstack((np.ones(self.size)*-1, np.ones(self.size)))\n        return X_, y_\n    \n    def viz_generator(self, X, y):\n        plt.figure(figsize=(12,8))\n        plt.scatter(X[:, 0], X[:, 1], c = y, alpha = .3)","bc511229":"# call the class and generate feature metrix and outcome variable\nlgen = LogisticSimulator(size=1000, random_state=13)\nX, y = lgen.sample_generator()\nlgen.viz_generator(X, y)","7bc23346":"ppn = Perceptron(eta=0.01, n_iter=500)\nppn.fit(X, y)","a48e4314":"plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\nplt.xlabel('Epochs')\nplt.ylabel('Number of updates')\nplt.show()","37d5abb4":"Here each row is a sample $X = [x^1, x^2, x^4 , x^3]$ . Each sample has 2 feature. For example sample $x^1 = \\begin{bmatrix}x^1_0\\\\x^1_1\\end{bmatrix} = \\begin{bmatrix}0.73\\\\0.60\\end{bmatrix}$ .  Then we have value to predict $y^t = [y^1, y^2, y^4 , y^3] = [1, 0, 1, 1]$","a6c674d3":"**Tne net input calculation:** \n\nFor a single sample the multiplication is as follows: Here $x_0 = 1$\n\n$w^{t}x = \\begin{bmatrix}w_0&w_1 & w_2\\end{bmatrix} \\begin{bmatrix}x_0\\\\x_1 \\\\ x_2\\end{bmatrix} = \\begin{bmatrix}w_0 +w_1x_1 + w_2x_2\\end{bmatrix} $ \n\nIn case of `numpy` **w** do not need to be Transposed as it is a array and `numpy.dot()` will do the the vector dot product for us. In the below example we can see how it works practically in numpy. I just took first vector from the $X$ array and multiply with $w$ to get the net input for a single sample. As I do not have $x_0w_0$ in my main metrix, so it's added later with $XW$ to get $XW_{intercept}$.","9c995c81":"# Why we should know the math behind?\nYes, we can use the scikit-learn api with out knowing an algorithm. But in that way we won't have good knowledge on what the problem should be if the model is not good or how we can tune the model etc. I am more interested in knowing what's happening behind when we are using an algorithm. \n# Why I wrote that kernal?\nEvery time I start reading a book on ML it goes throung all the math and implement directly with a practical example. But not in detail though it make sence as the books are more focus on general overview. So I decide to break down the examples in to small part so that I or anyone can review when we forget how the whole model works .Before we create our own API let's have a look on the steps it's need to go through to fit and predict from a model.\n# What will be covered?\nThe percentron learning Algorithm will be explained in 3 phases through this kernel.\n* First we will start with a small sample (4 sample and 2 feature) and explain a single iteration of the algorithm\n* Second we run the same algorithm in a loop for multiple iteration\n* Third we write the second step in a object oriented way to use it like scikit-learn API","0bbaabfe":"**Generating Weight:**\n\nTo initiate the learning it's is a classical approach to start with a random weight. So we will have a weight vector $ w = [w_0, w_1, w_2]$ including the intecept\/error term as $w_0$.","8d6bc84f":"Finally we can plot the classificaiton errors to see how the model is working. We can see the model is not working well. But that was not the main point of that karnel. The main focus was to look insede of perceptron learning Algorithm.","0058f9b1":"**Pridicition from a net Input:**\n\nIf we consider a function as:\n\n$\\begin{aligned}\nf(z=w^{t}_{intercept}x) = 1, if  z >= 0.0 \\\\\n= 0, otherwise \n\\end{aligned}$\n\nThen we can develope a `np.where()` method to calculate the predicted output $\\hat{y}$","fec1acb4":"# Phase 1: Working with a Single Sample\nIn that phase we will select a single sample (single row) from the metrics ($x^t = [0.37, 0.95]$) and do a single iteration.","253abd5d":"This time we will take the same sample and run the algorithm in a loop to iterate thorugh all sample for 10 times. In that way our weight will be adjusted everytime and will try to find the best fit based on the data.","b98a83b6":"# A Perceptron Learning Algorithm from Scrach","239b8f7a":"# A Single Sample Case Study:\nLet's generate a small sample with 2 features as $X$ and y as prediction. So we will use the relation between $X$ and $y$ and learn about the relation. Then we will try to predict from what we have learn by building an API.\n","ae998ca6":"# Phase 3: Create the API\nFinally we can put them all together to create a package like structure using the previous knowledge. I will not go to the detail that how exactly to create a package. But I will more focus on creating a perceptron learning algorithm class, which can be used exactly as scikit-learn.  The code I am using here is taken from Python Mechine Learning (S Raschka, V Mirjalili, 2017, packt) with minor modification. Actually the previous part was the break down of that API. If some one is not familier with how to create a package\/API in object oriented approach can read the following blog https:\/\/datapsycho.github.io\/PsychoBlog\/dataparrot-18-01","4f79363f":"# Phase 2: Run the Algorithom in a Batch","29f1cbf9":"**Update the Parameters**:\n\nEach time parameter is updated as follows: $w_j = w_j + \\Delta w_j$\nwhere $\\Delta w_j$ can be expressed as: $\\Delta w_j = \\eta (y^i - y^i_{hat}) x^i_j$, where $\\eta$ is called the learning rate one of a hyperparameter of the model. In the below section we will do the same operation for the sample $x^0$ and $y^0$. First we will update the weight for the parameter of the variables ($x_1$ and $x_2$) then we will udate the parameter for intercept\/error ($x_0$). ","494051c6":"# Conclusion\nWe started from scratch and were able to look inside of perceptron learning algorithm. We were able build an API based on phase 1 and phase 2. Then we create another class to geherate logistic like data and fit our algorithm with the data. There is lot of things we can imporve including model it self. We can implement gradient descent to the algoritm and many other decscend funciton as fruther improvement.","488d3439":"**Classification Error Calculation:**\n\nWe can also record after each weight updated how many of the output classified correctly and how many of them classified incorrectly. As so far we just have worked with single sample and one iteration so there will be just one value. In the next section we are going to run the algorithm in a batch over all samples which will make more sence.","582f7c22":"# A Object Oriented Class to generate Logistic Like Data\nHere I have written a class which can generate logistic like data with 2 feature. We are able to specify the sample and random state to generate such data.","ed1d0dad":"The following loop goes through each sample calculate the small update for the weights and also store the miss-classificaton.","279fe647":"# Sample Data\nLets create a 2d numpy array with the dimention of 4 by 2 for input and 1d array for output\/prediciton."}}