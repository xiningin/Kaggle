{"cell_type":{"4ce9d271":"code","4236b9da":"code","6c254afd":"code","6ebdbb97":"code","8024b763":"code","0eb55970":"code","890862e1":"code","64f12199":"code","f0a92c97":"code","a43341db":"code","fc41378c":"code","54523be9":"code","e2e54b8f":"code","f2fe77f9":"code","7a27ae86":"code","a778ecc5":"code","5bee695e":"code","b386e2cc":"code","d50ac340":"code","4bc8353d":"code","7676851b":"code","1f4827cb":"code","45ffe093":"code","9ae8714c":"code","9a5adfe4":"code","5f3783b3":"code","2b16352b":"markdown","af5dbfa6":"markdown"},"source":{"4ce9d271":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nsub = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","4236b9da":"train.head()","6c254afd":"import operator \nimport re\nimport gensim","6ebdbb97":"# load packages, and embeddings. \n# Due to the memory limit, here we only are using glove, while if you have a better machine, you can also load crawl and other embeddings\n\ndf = pd.concat([train.iloc[:, [0,2]] ,test.iloc[:, :2]])\nglove = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n# crawl =  '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n    \ndef load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    if file == '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec':\n        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(crawl)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index","8024b763":"print(\"Extracting GloVe embedding\")\n#embed_glove = load_embed(glove)\n# print(\"Extracting Crawl embedding\")\n# embed_crawl = load_embed(crawl)","0eb55970":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n# vocab = build_vocab(df['comment_text'])\n\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","890862e1":"print(\"Glove : \")\n#oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_crawl = check_coverage(vocab, embed_crawl)","64f12199":"df['lowered_comment'] = df['comment_text'].apply(lambda x: x.lower())\n# vocab_low = build_vocab(df['lowered_comment'])\nprint(\"Glove : \")\n#oov_glove = check_coverage(vocab_low, embed_glove)\n# print(\"Crawl : \")\n# oov_crawl = check_coverage(vocab_low, embed_crawl)","f0a92c97":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")\n    \nprint(\"Glove : \")\n#add_lower(embed_glove, vocab)\n# oov_glove = check_coverage(vocab_low, embed_glove)\n# print(\"Crawl : \")\n# add_lower(embed_crawl, vocab)\n# oov_crawl = check_coverage(vocab_low, embed_crawl)\n\n# Check Result\n# oov_glove[:10]","a43341db":"contraction_mapping = {\n    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n    'ain;t': 'am not','ain\u00b4t': 'am not','ain\u2019t': 'am not',\"aren't\": 'are not',\n    'aren,t': 'are not','aren;t': 'are not','aren\u00b4t': 'are not','aren\u2019t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n    'can;t': 'cannot','can;t;ve': 'cannot have',\n    'can\u00b4t': 'cannot','can\u00b4t\u00b4ve': 'cannot have','can\u2019t': 'cannot','can\u2019t\u2019ve': 'cannot have',\n    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n    'couldn;t;ve': 'could not have','couldn\u00b4t': 'could not',\n    'couldn\u00b4t\u00b4ve': 'could not have','couldn\u2019t': 'could not','couldn\u2019t\u2019ve': 'could not have','could\u00b4ve': 'could have',\n    'could\u2019ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn\u00b4t': 'did not',\n    'didn\u2019t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn\u00b4t': 'does not',\n    'doesn\u2019t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don\u00b4t': 'do not','don\u2019t': 'do not',\n    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n    'hadn;t;ve': 'had not have','hadn\u00b4t': 'had not','hadn\u00b4t\u00b4ve': 'had not have','hadn\u2019t': 'had not','hadn\u2019t\u2019ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn\u00b4t': 'has not','hasn\u2019t': 'has not',\n    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven\u00b4t': 'have not','haven\u2019t': 'have not',\"he'd\": 'he would',\n    \"he'd've\": 'he would have',\"he'll\": 'he will',\n    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he\u00b4d': 'he would','he\u00b4d\u00b4ve': 'he would have','he\u00b4ll': 'he will',\n    'he\u00b4s': 'he is','he\u2019d': 'he would','he\u2019d\u2019ve': 'he would have','he\u2019ll': 'he will','he\u2019s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n    'how;s': 'how is','how\u00b4d': 'how did','how\u00b4ll': 'how will','how\u00b4s': 'how is','how\u2019d': 'how did','how\u2019ll': 'how will',\n    'how\u2019s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n    'isn,t': 'is not','isn;t': 'is not','isn\u00b4t': 'is not','isn\u2019t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it\u00b4d': 'it would','it\u00b4ll': 'it will','it\u00b4s': 'it is',\n    'it\u2019d': 'it would','it\u2019ll': 'it will','it\u2019s': 'it is',\n    'i\u00b4d': 'i would','i\u00b4ll': 'i will','i\u00b4m': 'i am','i\u00b4ve': 'i have','i\u2019d': 'i would','i\u2019ll': 'i will','i\u2019m': 'i am',\n    'i\u2019ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let\u00b4s': 'let us',\n    'let\u2019s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n    'mayn\u00b4t': 'may not','mayn\u2019t': 'may not','ma\u00b4am': 'madam','ma\u2019am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn\u00b4t': 'might not',\n    'mightn\u2019t': 'might not','might\u00b4ve': 'might have','might\u2019ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn\u00b4t': 'must not','mustn\u2019t': 'must not','must\u00b4ve': 'must have',\n    'must\u2019ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn\u00b4t': 'need not','needn\u2019t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n    'oughtn\u00b4t': 'ought not','oughtn\u2019t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n    'shan,t': 'shall not','shan;t': 'shall not','shan\u00b4t': 'shall not','shan\u2019t': 'shall not','sha\u00b4n\u00b4t': 'shall not','sha\u2019n\u2019t': 'shall not',\n    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she\u00b4d': 'she would','she\u00b4ll': 'she will',\n    'she\u00b4s': 'she is','she\u2019d': 'she would','she\u2019ll': 'she will','she\u2019s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn\u00b4t': 'should not','shouldn\u2019t': 'should not','should\u00b4ve': 'should have',\n    'should\u2019ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n    'that;s': 'that is','that\u00b4d': 'that would','that\u00b4s': 'that is','that\u2019d': 'that would','that\u2019s': 'that is',\"there'd\": 'there had',\n    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n    'there\u00b4d': 'there had','there\u00b4s': 'there is','there\u2019d': 'there had','there\u2019s': 'there is',\n    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n    'they;ve': 'they have','they\u00b4d': 'they would','they\u00b4ll': 'they will','they\u00b4re': 'they are','they\u00b4ve': 'they have','they\u2019d': 'they would','they\u2019ll': 'they will',\n    'they\u2019re': 'they are','they\u2019ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn\u00b4t': 'was not',\n    'wasn\u2019t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren\u00b4t': 'were not','weren\u2019t': 'were not','we\u00b4d': 'we would','we\u00b4ll': 'we will',\n    'we\u00b4re': 'we are','we\u00b4ve': 'we have','we\u2019d': 'we would','we\u2019ll': 'we will','we\u2019re': 'we are','we\u2019ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n    'what;s': 'what is','what;ve': 'what have','what\u00b4ll': 'what will',\n    'what\u00b4re': 'what are','what\u00b4s': 'what is','what\u00b4ve': 'what have','what\u2019ll': 'what will','what\u2019re': 'what are','what\u2019s': 'what is',\n    'what\u2019ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n    'where;s': 'where is','where\u00b4d': 'where did','where\u00b4s': 'where is','where\u2019d': 'where did','where\u2019s': 'where is',\n    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n    'who\u00b4ll': 'who will','who\u00b4s': 'who is','who\u2019ll': 'who will','who\u2019s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n    'won\u00b4t': 'will not','won\u2019t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn\u00b4t': 'would not',\n    'wouldn\u2019t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n    'you;re': 'you are','you\u00b4d': 'you would','you\u00b4ll': 'you will','you\u00b4re': 'you are','you\u2019d': 'you would','you\u2019ll': 'you will','you\u2019re': 'you are',\n    '\u00b4cause': 'because','\u2019cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n    \"havn't\": 'have not',\"here\u2019s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you\u2019ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"\u2018I\":'I',\n    '\u1d00\u0274\u1d05':'and','\u1d1b\u029c\u1d07':'the','\u029c\u1d0f\u1d0d\u1d07':'home','\u1d1c\u1d18':'up','\u0299\u028f':'by','\u1d00\u1d1b':'at','\u2026and':'and','civilbeat':'civil beat',\\\n    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','\u1d04\u029c\u1d07\u1d04\u1d0b':'check','\u0493\u1d0f\u0280':'for','\u1d1b\u029c\u026as':'this','\u1d04\u1d0f\u1d0d\u1d18\u1d1c\u1d1b\u1d07\u0280':'computer',\\\n    '\u1d0d\u1d0f\u0274\u1d1b\u029c':'month','\u1d21\u1d0f\u0280\u1d0b\u026a\u0274\u0262':'working','\u1d0a\u1d0f\u0299':'job','\u0493\u0280\u1d0f\u1d0d':'from','S\u1d1b\u1d00\u0280\u1d1b':'start','gubmit':'submit','CO\u2082':'carbon dioxide','\u0493\u026a\u0280s\u1d1b':'first',\\\n    '\u1d07\u0274\u1d05':'end','\u1d04\u1d00\u0274':'can','\u029c\u1d00\u1d20\u1d07':'have','\u1d1b\u1d0f':'to','\u029f\u026a\u0274\u1d0b':'link','\u1d0f\u0493':'of','\u029c\u1d0f\u1d1c\u0280\u029f\u028f':'hourly','\u1d21\u1d07\u1d07\u1d0b':'week','\u1d07\u0274\u1d05':'end','\u1d07x\u1d1b\u0280\u1d00':'extra',\\\n    'G\u0280\u1d07\u1d00\u1d1b':'great','s\u1d1b\u1d1c\u1d05\u1d07\u0274\u1d1bs':'student','s\u1d1b\u1d00\u028f':'stay','\u1d0d\u1d0f\u1d0ds':'mother','\u1d0f\u0280':'or','\u1d00\u0274\u028f\u1d0f\u0274\u1d07':'anyone','\u0274\u1d07\u1d07\u1d05\u026a\u0274\u0262':'needing','\u1d00\u0274':'an','\u026a\u0274\u1d04\u1d0f\u1d0d\u1d07':'income',\\\n    '\u0280\u1d07\u029f\u026a\u1d00\u0299\u029f\u1d07':'reliable','\u0493\u026a\u0280s\u1d1b':'first','\u028f\u1d0f\u1d1c\u0280':'your','s\u026a\u0262\u0274\u026a\u0274\u0262':'signing','\u0299\u1d0f\u1d1b\u1d1b\u1d0f\u1d0d':'bottom','\u0493\u1d0f\u029f\u029f\u1d0f\u1d21\u026a\u0274\u0262':'following','M\u1d00\u1d0b\u1d07':'make',\\\n    '\u1d04\u1d0f\u0274\u0274\u1d07\u1d04\u1d1b\u026a\u1d0f\u0274':'connection','\u026a\u0274\u1d1b\u1d07\u0280\u0274\u1d07\u1d1b':'internet','financialpost':'financial post', '\u029ca\u1d20\u1d07':' have ', '\u1d04a\u0274':' can ', 'Ma\u1d0b\u1d07':' make ', '\u0280\u1d07\u029f\u026aa\u0299\u029f\u1d07':' reliable ', '\u0274\u1d07\u1d07\u1d05':' need ',\n    '\u1d0f\u0274\u029f\u028f':' only ', '\u1d07x\u1d1b\u0280a':' extra ', 'a\u0274':' an ', 'a\u0274\u028f\u1d0f\u0274\u1d07':' anyone ', 's\u1d1ba\u028f':' stay ', 'S\u1d1ba\u0280\u1d1b':' start', 'SHOPO':'shop',\n    }","fc41378c":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known\n\nprint(\"- Known Contractions -\")\nprint(\"   Glove :\")\n# print(known_contractions(embed_glove))\n# print(\"   Crawl :\")\n# print(known_contractions(embed_crawl))","54523be9":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndf['treated_comment'] = df['lowered_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))\n\n#vocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\n#oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","e2e54b8f":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n\ndef unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown\n\nprint(\"Glove :\")\n#print(unknown_punct(embed_glove, punct))\n# print(\"Crawl :\")\n# print(unknown_punct(embed_crawl, punct))","f2fe77f9":"punct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }\n\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\n\ndf['treated_comment'] = df['treated_comment'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n#vocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\n#oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","7a27ae86":"# oov_glove[:10]","a778ecc5":"mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','G\u0280\u1d07at':'great','\u0299\u1d0f\u1d1bto\u1d0d':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Y\u1d0f\u1d1c':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','\u1d00':'a', '\ud83d\ude09':'wink','\ud83d\ude02':'joy','\ud83d\ude00':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}","5bee695e":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndf['treated_comment'] = df['treated_comment'].apply(lambda x: correct_spelling(x, mispell_dict))\n\n#vocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\n#oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","b386e2cc":"train['comment_text'] = df['treated_comment'][:1804874]\ntest['comment_text'] = df['treated_comment'][1804874:]","d50ac340":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4bc8353d":"print('-' * 80)\nprint('train')\ntrain = reduce_mem_usage(train)\n\nprint('-' * 80)\nprint('test')\ntest = reduce_mem_usage(test)","7676851b":"# train.to_pickle(\"train.pkl\")\n# test.to_pickle(\"test.pkl\")\ntrain.to_csv('train_cleaned.csv', index=None)\ntest.to_csv('test_cleaned.csv', index=None)","1f4827cb":"train.head()","45ffe093":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\n\n","9ae8714c":"EMBEDDING_FILES = [\n    '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec',\n    '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n]\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\n\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n\ndef custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n\n\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n\n    return model\n    \n\ndef preprocess(data):\n    '''\n    Credit goes to https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n    '''\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n\n","9a5adfe4":"\n# train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n# test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_train = preprocess(train['comment_text'])\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) \/ 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) \/ 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\nloss_weight = 1.0 \/ weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\nx_test = preprocess(test['comment_text'])\n\n","5f3783b3":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\nimport pickle\nimport gc\n\nwith open('temporary.pickle', mode='wb') as f:\n    pickle.dump(x_test, f) # use temporary file to reduce memory\n\ndel identity_columns, weights, tokenizer, train, test, x_test\ngc.collect()\n\ncheckpoint_predictions = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        with open('temporary.pickle', mode='rb') as f:\n            x_test = pickle.load(f) # use temporary file to reduce memory\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        del x_test\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n\ndf_submit = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\ndf_submit.prediction = predictions\ndf_submit.to_csv('submission.csv', index=False)","2b16352b":"The following contraction_mapping is borrowed from @Aditya Soni. Credit goes to https:\/\/www.kaggle.com\/adityaecdrid\/public-version-text-cleaning-vocab-65","af5dbfa6":"Text processing applied on Simple LSTM kernel\nhttps:\/\/www.kaggle.com\/tanreinama\/simple-lstm-using-identity-parameters-solution"}}