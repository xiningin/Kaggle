{"cell_type":{"138efd89":"code","c75c6859":"code","75c8dc91":"code","e41318ca":"code","ae7bc02b":"code","cb91e34c":"code","5707a99f":"code","b89e0de3":"code","f8875df8":"code","765e3635":"code","1a6f0dc2":"code","d796bdbc":"code","c404cb2f":"code","c1fa887c":"code","4828aa70":"code","cfe75b85":"markdown","31949f2e":"markdown","08a98417":"markdown","69ce8bf5":"markdown","94df6047":"markdown","9e275695":"markdown","3bb5925d":"markdown"},"source":{"138efd89":"# Import the necessary packages\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.svm import SVC \n\nimport warnings\nwarnings.filterwarnings('ignore')","c75c6859":"# Import and read dataset\n\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","75c8dc91":"data.describe()","e41318ca":"inp_data = data.drop(data[['DEATH_EVENT']], axis=1)\nout_data = data[['DEATH_EVENT']]\n\nscaler = StandardScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","ae7bc02b":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","cb91e34c":"# basic method\nclf = SVC() \nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","5707a99f":"# find best parameters with SVC | Step 1\nkernels = list(['linear', 'rbf', 'poly', 'sigmoid'])\nc = list([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000])\ngammas = list([0.1, 1, 10, 100])\n\nclf = SVC()\nclf.fit(X_train, y_train) \nparam_grid = dict(kernel=kernels, C=c, gamma=gammas)\ngrid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\ngrid.fit(X_train, y_train)\ngrid.best_params_","b89e0de3":"clf = SVC(C=10, gamma=0.1, kernel='linear') \nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","f8875df8":"# find best parameters with SVC | Step 2\nkernels = list(['linear'])\nc = list([5,10,15,20,25,30])\ngammas = list([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n\nclf = SVC()\nclf.fit(X_train, y_train) \nparam_grid = dict(kernel=kernels, C=c, gamma=gammas)\ngrid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\ngrid.fit(X_train, y_train)\ngrid.best_params_","765e3635":"clf = SVC(C=5, gamma=0.05, kernel='linear') \nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","1a6f0dc2":"from imblearn.over_sampling import SMOTE\n\nsms = SMOTE(random_state=12345)\nX_res, y_res = sms.fit_sample(inp_data, out_data)\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","d796bdbc":"# find best parameters with SVC | Step 2\nkernels = list(['linear', 'rbf', 'poly', 'sigmoid'])\nc = list([20,25,30,35,40,45,50])\ngammas = list([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n\nclf = SVC()\nparam_grid = dict(kernel=kernels, C=c, gamma=gammas)\ngrid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\ngrid.fit(X_train, y_train)\ngrid.best_params_","c404cb2f":"clf = SVC(C=30, gamma=0.08, kernel='rbf') \nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","c1fa887c":"y_pred = clf.predict(X_test)\n\ncf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","4828aa70":"scores = [] \nfor i in range(0,500): \n    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2)\n    clf = SVC(kernel='rbf', C=30, gamma=0.08) \n    clf.fit(X_train, y_train)\n    scores.append(accuracy_score(clf.predict(X_test), y_test)) \n\nplt.hist(scores)\nplt.show()","cfe75b85":"## After the SMOTE process","31949f2e":"## Comparison with Other Models\n\n### SVM vs Decision Trees\n- While Random Forest supports multi-class classification, SVM needs more than one model for this.\n- Random Forest may give a probability on the forecast whereas SVM cannot.\n- Random Forest handles categorical data better than SVM.\n\n### SVM vs Naive Bayes\n- Both perform better with little training data and large features.\n- SVM outperforms Naive Bayes if features are mutually dependent.\n- SVM is a distinctive model while NB is a productive model.\n\n### SVM vs Artificial Neural Networks (NN)\n- SVM has a convex optimization function whereas NN can hang at the local minimum.\n- SVM can outperform NN when there is limited training data and many features. NN needs large training data for sufficient accuracy.\n- Multi-class classification requires multiple models for SVM, while NN can do this with a single model.","08a98417":"![svm](https:\/\/i.ibb.co\/T8wFPdH\/svm-kaggle.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3 - Support Vector Machine (SVM)**\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\n\nThe Support Vector machine is a type of ML technique that can be used for both classification and regression. There are two main types to support linear and nonlinear problems. Linear SVM has no kernel and finds linear solution to the problem with minimal margin. Core SVM is used when the solution is not linearly separable.\n\n\n## Basic Theory\nSupport Vector Machine, text classification, image classification, bioinformatics etc. It is a supervised learning technique widely used in the fields.\nA hyperplane is derived by the model that maximizes the classification margin. If N features are present, the hyperplane will be an N-1 dimensional subspace. Boundary nodes in feature space are called support vectors. Based on their relative position, the maximum margin is derived and an optimal hyperplane is drawn at the midpoint.\n\n![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png)\n\nThe value of the margin (m), || w || will be inversely proportional to; where w is the set of weight matrices. To maximize the margin, || w || We'll have to minimize. Optimization problem,\n\n![](https:\/\/i.ibb.co\/TgrvCft\/optimizasyon.jpg)\n\nThe above optimization works well for fully linear separable solutions. To handle outliers, we need a loose term as below. The second term uses hinge loss to get slack variable.\n\n![](https:\/\/i.ibb.co\/ZKmqy58\/hinge.jpg)\n\nC is the regularity parameter that balances the missed penalty and margin width. Since the mathematical explanations are far beyond the scope of this story, I will not explain them in depth.\n\n\nThe basic logic is that in order to minimize the cost function, w is forced to adjust with maximum margin between classes. The value of C will decide the level of regularity applied on the data sets. It decides the level (soft \/ hard) margin to be applied on the data sets. In short, C is the level of ignorance towards outliers.\n\nN linear SVM when the dataset is not linearly separable. A kernel function is used to derive a new subplane for all training data. The distribution of the tags in the new sub plane will be such that the training data is linearly separable. Next, a linear curve will classify the labels on the lower plane. When the classification results are reflected back to the feature space, we get a nonlinear solution.\n\n![](https:\/\/i.ibb.co\/n3NbrxR\/Ek-A-klama-2020-08-29-113927.jpg)\n\nThe only change in the equation here is to describe a new kernel function. The new equation will look like this:\n\n![](https:\/\/i.ibb.co\/X28y1vG\/Ek-A-klama-2020-08-29-114044.jpg)\n\nXi will be replaced by \u03d5 (xi), which will transform the data set into the new hyperplane.","69ce8bf5":"## Loss Function\n\nThe loss function in Eq. 1 can be divided into two parts as follows:\n\n![](https:\/\/i.ibb.co\/xS7bKJ6\/Ek-A-klama-2020-08-29-114318.jpg)\n\nThe first term tries to minimize w parameters and get high margin. The second term refers to loss of hinge. Calculates the slack variable for each data set. If any data set comes between the margin or the wrong side, the hinge loss will be penalized.\n\nThe reduction of the first term causes the broadening of the margin to decrease and widen. Minimizing the second term results in shortening of the margin to reduce hinge loss. Based on the value of C, we finally set a fixed margin. The value of C determines a soft \/ hard margin on the curve.\n\n![](https:\/\/i.ibb.co\/y86PbbY\/Ek-A-klama-2020-08-29-114457.jpg)\n\nIn the above diagram, he is clear about the effects of C in deriving margin.\nGaussian kernel, polynomial kernel, Sigmoid kernel, Laplace RBF kernel etc. in nonlinear kernels. We can use.\n\n\n## Advantages\n- SVM uses kernel number to solve complex solutions.\n- SVM uses a convex optimization function whose global minimum is always achievable.\n- Hinge (hinge) loss provides higher accuracy.\n- Outliers can be handled well using the soft margin constant C.\n\n\n## Disadvantages\n- Loss of hinge (hinge) leads to sparseness.\n- Hyper parameters and cores must be carefully tuned for adequate accuracy.\n- Longer training time for larger data sets.\n\n\n## Hyperparameters\n- **Soft Margin Constant (C):**\n    - It is a hyperparameter that decides the level of penalty on outliers. It is the reverse of the regularization parameter. When C is large, Outliers will be given a high penalty and a stiff margin will be created. When C is small, outliers are neglected and the margin is large.\n- **The degree of the polynomial in the Polynomial Kernel (d):**\n    - When d = 1, it is equivalent to a linear core. When D is higher, the core is flexible enough to distinguish complex patterns by projecting them onto a new hyperplane.\n\n- **Width Parameter (\u03b3) in Gaussian Kernel:**\n    - Gamma decides the width of the Gaussian curve. The width increases with the increase of gamma.","94df6047":"As a sanity check, let\u2019s do 500 rounds of random sampling and assess the stability of our model:","9e275695":"## Coding Time\n\n![](https:\/\/ac-cdn.azureedge.net\/infusionnewssiteimages\/agingcare\/21e637ea-aa74-4ae2-b278-181d2cded7a3.jpg)","3bb5925d":"## Reporting\n\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n**Correctly predicted -> %89.02 (361 of 406 predict are correct)**\n- True Negative -> %42.68 -> Those who were predicted not to die and who did not die\n- True Positive -> %46.34 -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %10.98 (45 of 406 predict are wrong)**\n- False Positive -> %03.66 -> Those who were predicted to die but who did not die\n- False Negative -> %07.32 -> Those who were predicted to not die but who did die"}}