{"cell_type":{"2e5723f5":"code","952a3da9":"code","a62ec202":"code","c1179582":"code","af9c67f1":"code","f62ae100":"code","a90aa860":"code","2d17a35b":"code","f04a0f77":"code","13a64ba5":"code","05c3f48c":"code","fbcea389":"code","03582104":"code","da08cd81":"code","926fe10a":"code","682a23a8":"code","943ece62":"code","a8365dfa":"code","f43843d2":"code","4021ca1f":"code","e8a39672":"code","5b61f1d2":"code","197aa77f":"code","5e795524":"code","56e09dd1":"code","6320a362":"code","70bc698f":"code","6a3e8045":"code","f9afc75a":"code","c98d6387":"code","840fb8ef":"code","4104db83":"code","b3ab228b":"code","985c0a93":"code","bf4a8555":"code","195a1b28":"code","c9af9d0a":"code","adebebdc":"code","8cee4db7":"code","6626efe2":"code","e2d6aa9d":"code","ab58f7b2":"code","767077cf":"code","03dbab3e":"code","901c4e23":"code","c5526256":"code","22218369":"code","d9298bf0":"code","86e131e6":"code","5c2e991d":"code","202f1833":"code","8d21f5bc":"code","49276996":"code","9e608199":"code","50c7aaee":"code","fa20e396":"code","1c6244cf":"code","aa50ad4b":"code","e1c997f0":"code","7ecd7092":"code","09af8935":"code","5f91a1ef":"code","37fb36dd":"markdown","6bf45ba8":"markdown","f9cde28c":"markdown","4d3a6131":"markdown","eebb9589":"markdown","2317ce84":"markdown","5e99a191":"markdown","3e7b7de4":"markdown","c5be0189":"markdown","ac0f4f0f":"markdown","237f4c0e":"markdown","e3f08205":"markdown","5ccc91c5":"markdown","e4008f4b":"markdown","13306ec8":"markdown","0eab3642":"markdown","99eb9895":"markdown","c8750659":"markdown","e5457250":"markdown","7779b4d1":"markdown","b00cf979":"markdown","6162cb2f":"markdown","bd738a0a":"markdown","7b4dc0a2":"markdown","06be2953":"markdown","b7f258f8":"markdown","cf9e0925":"markdown","4023124f":"markdown","ed05fd8c":"markdown","c26057d1":"markdown","f247046e":"markdown","c6603c44":"markdown","6df5d13a":"markdown","cccbb549":"markdown","39cc55c2":"markdown","268cec4e":"markdown","91e2fbce":"markdown","c9f5af9a":"markdown","bccddc66":"markdown","ced18fed":"markdown"},"source":{"2e5723f5":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\nimport os\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nfrom IPython.display import display, clear_output\nimport ipywidgets as widgets\nimport io\nfrom keras import layers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate, BatchNormalization","952a3da9":"labels = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\nX_train = []\ny_train = []\nX_test = []\ny_test = []\nimage_size = 224\nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri\/Training', i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size, image_size))\n        X_train.append(img)\n        y_train.append(i)\n        \nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri\/Testing', i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size,image_size))\n        X_test.append(img)\n        y_test.append(i)\n        \nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\nX_test = np.array(X_test)\ny_test = np.array(y_test)","a62ec202":"k=0\nfig, ax = plt.subplots(1,4,figsize=(20,20))\nfig.text(s='Sample Image From Each Label',size=18,fontweight='bold',\n             fontname='monospace',y=0.62,x=0.4,alpha=0.8)\nfor i in labels:\n    j=0\n    while True :\n        if y_train[j]==i:\n            ax[k].imshow(X_train[j])\n            ax[k].set_title(y_train[j])\n            ax[k].axis('off')\n            k+=1\n            break\n        j+=1","c1179582":"X_train, y_train = shuffle(X_train,y_train, random_state=14)","af9c67f1":"X_train.shape","f62ae100":"X_test.shape","a90aa860":"sns.countplot(y_train)","2d17a35b":"sns.countplot(y_test)","f04a0f77":"y_train_new = []\nfor i in y_train:\n    y_train_new.append(labels.index(i))\ny_train = y_train_new\ny_train = tf.keras.utils.to_categorical(y_train)\n\n\ny_test_new = []\nfor i in y_test:\n    y_test_new.append(labels.index(i))\ny_test = y_test_new\ny_test = tf.keras.utils.to_categorical(y_test)","13a64ba5":"X_train,X_val,y_train,y_val = train_test_split(X_train,y_train, test_size=0.1,random_state=14)","05c3f48c":"X_train = np.array(X_train) \/ 255.\nX_val = np.array(X_val) \/ 255.\nX_test = np.array(X_test) \/ 255.","fbcea389":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range=0.1,  # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","03582104":"tensorboard = TensorBoard(log_dir = 'logs')\ncheckpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n                              mode='auto',verbose=1)","da08cd81":"model_cnn = Sequential()\n\nmodel_cnn.add(Conv2D(64, (3, 3), padding='same',input_shape=(image_size,image_size,3))) \nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(BatchNormalization())\n\nmodel_cnn.add(Conv2D(64, (3, 3))) \nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2))) \nmodel_cnn.add(BatchNormalization())\nmodel_cnn.add(Dropout(0.35))\n\nmodel_cnn.add(Conv2D(64, (3, 3), padding='same'))\nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(BatchNormalization()) \n\nmodel_cnn.add(Conv2D(64, (3, 3)))\nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2))) \nmodel_cnn.add(BatchNormalization())\nmodel_cnn.add(Dropout(0.35)) \n\nmodel_cnn.add(Conv2D(64, (3, 3), padding='same')) \nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(BatchNormalization())\n\nmodel_cnn.add(Flatten()) \nmodel_cnn.add(Dropout(0.5)) \nmodel_cnn.add(Dense(512)) \nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(BatchNormalization())\nmodel_cnn.add(Dense(4)) \nmodel_cnn.add(Activation('softmax'))\n\nmodel_cnn.summary()","926fe10a":"from keras.utils.vis_utils import plot_model\nplot_model(model_cnn, to_file='model_cnn_plot.png', show_shapes=True, show_layer_names=True)","682a23a8":"model_cnn.compile(optimizer = 'adam',\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])","943ece62":"history = model_cnn.fit(X_train, y_train,validation_split=0.1, verbose=1, batch_size = 32, validation_data = (X_val, y_val),\n                     epochs = 20, callbacks =[tensorboard,checkpoint,reduce_lr])","a8365dfa":"model_cnn.save('cnn_model.h5')\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f43843d2":"pred = model_cnn.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","4021ca1f":"accuracy = np.sum(pred==y_test_new)\/len(pred)\nprint(\"Accuracy on testing dataset: {:.2f}%\".format(accuracy*100))","e8a39672":"model_cnn.evaluate(X_train, y_train)","5b61f1d2":"print(classification_report(y_test_new,pred))","197aa77f":"fig,ax=plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True)\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',y=0.92,x=0.28,alpha=0.8)\n\nplt.show()","5e795524":"vgg=VGG16(input_shape=(image_size,image_size,3) , weights='imagenet' , include_top=False)","56e09dd1":"for layers in vgg.layers:\n  layers.trainable=False","6320a362":"model_vgg = vgg.output\nmodel_vgg = GlobalAveragePooling2D()(model_vgg)\nmodel_vgg = Dense(128,activation='relu')(model_vgg)\nmodel_vgg = Dropout(0.15)(model_vgg)\nmodel_vgg = Dense(4,activation='softmax')(model_vgg)\nmodel_vgg = Model(inputs=vgg.input,outputs=model_vgg)\nmodel_vgg.summary()","70bc698f":"from keras.utils.vis_utils import plot_model\nplot_model(model_vgg, to_file='model_vgg_plot.png', show_shapes=True, show_layer_names=True)","6a3e8045":"model_vgg.compile(optimizer = 'adam',\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])","f9afc75a":"history = model_vgg.fit(X_train, y_train,validation_split=0.1, verbose=1, batch_size = 32, validation_data = (X_val, y_val),\n                     epochs = 20, callbacks =[tensorboard,checkpoint,reduce_lr])","c98d6387":"model_vgg.save('vgg16_model.h5')\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","840fb8ef":"pred = model_vgg.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","4104db83":"accuracy = np.sum(pred==y_test_new)\/len(pred)\nprint(\"Accuracy on testing dataset: {:.2f}%\".format(accuracy*100))","b3ab228b":"model_vgg.evaluate(X_train, y_train)","985c0a93":"print(classification_report(y_test_new,pred))","bf4a8555":"fig,ax=plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True)\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',y=0.92,x=0.28,alpha=0.8)\n\nplt.show()","195a1b28":"effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))","c9af9d0a":"model_eff = effnet.output\nmodel_eff = tf.keras.layers.GlobalAveragePooling2D()(model_eff)\nmodel_eff = tf.keras.layers.Dropout(rate=0.5)(model_eff)\nmodel_eff = tf.keras.layers.Dense(4,activation='softmax')(model_eff)\nmodel_eff = tf.keras.models.Model(inputs=effnet.input, outputs = model_eff)\nmodel_eff.summary()","adebebdc":"from keras.utils.vis_utils import plot_model\nplot_model(model_eff, to_file='model_eff_plot.png', show_shapes=True, show_layer_names=True)","8cee4db7":"model_eff.compile(loss='categorical_crossentropy',optimizer = 'Adam', metrics= ['accuracy'])","6626efe2":"history = model_eff.fit(X_train, y_train,validation_split=0.1, verbose=1, batch_size = 32, validation_data = (X_val, y_val),\n                     epochs = 20, callbacks =[tensorboard,checkpoint,reduce_lr])","e2d6aa9d":"model_eff.save('model_eff.h5')\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","ab58f7b2":"pred = model_eff.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","767077cf":"accuracy = np.sum(pred==y_test_new)\/len(pred)\nprint(\"Accuracy on testing dataset: {:.2f}%\".format(accuracy*100))","03dbab3e":"model_eff.evaluate(X_train, y_train)","901c4e23":"print(classification_report(y_test_new,pred))","c5526256":"fig,ax=plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True)\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',y=0.92,x=0.28,alpha=0.8)","22218369":"import tensorflow as tf\nfrom tensorflow import keras\nfrom IPython.display import Image, display\nimport matplotlib.cm as cm\n\nmodel_builder = tf.keras.applications.vgg16.VGG16\nimg_size = (224, 224)\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\ndecode_predictions = tf.keras.applications.vgg16.decode_predictions\n\nlast_conv_layer_name = \"block5_conv3\"\n\n# The local path to our target image\nimg_path =\"..\/input\/brain-tumor-classification-mri\/Training\/meningioma_tumor\/m3 (14).jpg\"\n\ndisplay(Image(img_path))","d9298bf0":"def get_img_array(img_path, size):\n    # `img` is a PIL image of size 224x224\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (224, 224, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 224, 224, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","86e131e6":"# Prepare image\nimg_array = preprocess_input(get_img_array(img_path, size=img_size))\n\n# Make model\nmodel = model_builder(weights=\"imagenet\")\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None\n\n# Print what the top predicted class is\npreds = model.predict(img_array)\nprint(\"Predicted:\", decode_predictions(preds, top=1)[0])\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","5c2e991d":"def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n    display(Image(cam_path))\n\n\nsave_and_display_gradcam(img_path, heatmap)","202f1833":"model_builder = tf.keras.applications.efficientnet.EfficientNetB0\nimg_size = (224, 224)\npreprocess_input = tf.keras.applications.efficientnet.preprocess_input\ndecode_predictions = tf.keras.applications.efficientnet.decode_predictions\n\nlast_conv_layer_name = \"top_conv\"","8d21f5bc":"img_path =\"..\/input\/brain-tumor-classification-mri\/Training\/meningioma_tumor\/m3 (14).jpg\"\n\ndisplay(Image(img_path))\n\n# Prepare image\nimg_array = preprocess_input(get_img_array(img_path, size=img_size))\n\n# Make model\nmodel = model_builder(weights=\"imagenet\")\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None\n\n# Print what the top predicted class is\npreds = model.predict(img_array)\nprint(\"Predicted:\", decode_predictions(preds, top=1)[0])\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()\n\nsave_and_display_gradcam(img_path, heatmap)","49276996":"# Function to convert output labels to its class of tumor.\ndef inverse_classes(num):\n    if num==0:\n        return 'Glioma Tumor'\n    elif num==1:\n        return 'Meningioma Tumor'\n    elif num==2:\n        return 'No Tumor'\n    else:\n        return 'Pituitary Tumor'","9e608199":"train_pred = model_cnn.predict(X_train)\ntrain_pred = np.argmax(train_pred, axis=1)\nY_train_ = np.argmax(y_train, axis=1)\nprint(\"Accuracy on training set: {:.2f}%\".format(np.sum(train_pred==Y_train_)\/len(Y_train_)*100))","50c7aaee":"test_pred = model_cnn.predict(X_test)\ntest_pred = np.argmax(test_pred, axis=1)\nY_test_ = np.argmax(y_test, axis=1)\nprint(\"Accuracy on testing set: {:.2f}%\".format(np.sum(test_pred==Y_test_)\/len(Y_test_)*100))","fa20e396":"# Prediction using CNN model\nplt.figure(figsize=(15,12))\nfor i in range(4):\n    plt.subplot(3,2,(i%12)+1)\n    index=np.random.randint(394)\n    pred_class=inverse_classes(np.argmax(model_cnn.predict(np.reshape(X_test[index],(-1,224,224,3))),axis=1))\n    plt.title('This image is of {0} and is predicted as {1}'.format(inverse_classes(y_test_new[index]),pred_class),\n              fontdict={'size':13})\n    plt.imshow(X_test[index])\n    plt.tight_layout()","1c6244cf":"train_pred = model_vgg.predict(X_train)\ntrain_pred = np.argmax(train_pred, axis=1)\nY_train_ = np.argmax(y_train, axis=1)\nprint(\"Accuracy on training set: {:.2f}%\".format(np.sum(train_pred==Y_train_)\/len(Y_train_)*100))","aa50ad4b":"test_pred = model_vgg.predict(X_test)\ntest_pred = np.argmax(test_pred, axis=1)\nY_test_ = np.argmax(y_test, axis=1)\nprint(\"Accuracy on testing set: {:.2f}%\".format(np.sum(test_pred==Y_test_)\/len(Y_test_)*100))","e1c997f0":"# Prediction using VGG16 model\nplt.figure(figsize=(15,12))\nfor i in range(4):\n    plt.subplot(3,2,(i%12)+1)\n    index=np.random.randint(394)\n    pred_class=inverse_classes(np.argmax(model_vgg.predict(np.reshape(X_test[index],(-1,224,224,3))),axis=1))\n    plt.title('This image is of {0} and is predicted as {1}'.format(inverse_classes(y_test_new[index]),pred_class),\n              fontdict={'size':13})\n    plt.imshow(X_test[index])\n    plt.tight_layout()","7ecd7092":"train_pred = model_eff.predict(X_train)\ntrain_pred = np.argmax(train_pred, axis=1)\nY_train_ = np.argmax(y_train, axis=1)\nprint(\"Accuracy on training set: {:.2f}%\".format(np.sum(train_pred==Y_train_)\/len(Y_train_)*100))","09af8935":"test_pred = model_eff.predict(X_test)\ntest_pred = np.argmax(test_pred, axis=1)\nY_test_ = np.argmax(y_test, axis=1)\nprint(\"Accuracy on testing set: {:.2f}%\".format(np.sum(test_pred==Y_test_)\/len(Y_test_)*100))","5f91a1ef":"# Prediction using EfficientNetB0 model\nplt.figure(figsize=(15,12))\nfor i in range(4):\n    plt.subplot(3,2,(i%12)+1)\n    index=np.random.randint(394)\n    pred_class=inverse_classes(np.argmax(model_eff.predict(np.reshape(X_test[index],(-1,224,224,3))),axis=1))\n    plt.title('This image is of {0} and is predicted as {1}'.format(inverse_classes(y_test_new[index]),pred_class),\n              fontdict={'size':13})\n    plt.imshow(X_test[index])\n    plt.tight_layout()","37fb36dd":"In this,\n\n*   0 - Glioma Tumor\n*   1 - Meningioma Tumor\n*   2 - No Tumor\n*   3 - Pituitary Tumo","6bf45ba8":"**Prediction**","f9cde28c":"**Prediction**\n\nI've used the argmax function as each row from the prediction array contains four values for the respective labels. The maximum value which is in each row depicts the predicted output out of the 4 possible outcomes.\nSo with argmax, I'm able to find out the index associated with the predicted outcome.","4d3a6131":"**Dividing the dataset into Training and Testing sets.**","eebb9589":"# Introduction to Problem of Brain Tumor","2317ce84":"**Previewing the images in each classes**","5e99a191":"**CNN approach**","3e7b7de4":"For the data augmentation, i choosed to :\n1.   Randomly rotate some training images by 10 degrees.\n2.   Randomly Zoom by 10% some training images.\n3.   Randomly shift images horizontally by 10% of the width.\n4.   Randomly shift images vertically by 10% of the height.\n5.   Randomly flip images horizontally.","c5be0189":"**Print out test set shape**","ac0f4f0f":"**Evaluation**","237f4c0e":"# Objective","e3f08205":"**Show the counts of observations in each categorical**","5ccc91c5":"**Prediction using EfficientNetB0**","e4008f4b":"# Description of the Brain tumor Dataset","13306ec8":"**Evaluation**","0eab3642":"**Prediction**","99eb9895":"Appending all the images from the directories into a list and then converting them into numpy arrays after resizing it.","c8750659":"**Print out train set shape**","e5457250":"# Data Preperation","7779b4d1":"**Prediction using CNN model**","b00cf979":"**Normalize the data**","6162cb2f":"# Conclusion\n\nIn this notebook, I performed Image Classification (Brain tumor MRI images) with the help of CNN and Transfer Learning on VGG16 and EfficientNetB0.\n\n>For CNN, the accuracy on training set is 99.38%, but the accuracy on testing set is 71.07%.\n\n>For VGG16, the accuracy on training set is 97.41%, but the accuracy on testing set: 75.89%.\n\n>For EfficientNetB0, the accuracy on training set is 99.85%, but the accuracy on testing set: 78.93%.\n\nAs you can see from the results, the Transfer Learning method gave a better performance of the model over the traditional CNN method. Instead of training a model from scratch, I can use existing architectures that have been trained on a large dataset and then tune them for my task. This reduces the time to train and often results in better overall performance in the Transfer learning process.","bd738a0a":"**VGG16**\n\nThe include_top parameter is set to False so that the network doesn't include the top layer\/ output layer from the pre-built model which allows us to add our own output layer depending upon our use case!","7b4dc0a2":"**Grad-CAM of EfficientNetB0**","06be2953":"**Grad-CAM of VGG16**","b7f258f8":"# Detail of model implementation","cf9e0925":"**Transfer learning approach**\n\nTransfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task. In this project I use pretrained models like VGG-16 and EfficientNetB0. Which is trained on large ImageNet dataset.\n\nThe ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories with a typical category, such as \"balloon\" or \"strawberry\", consisting of several hundred images.","4023124f":"**Shuffle the train set**","ed05fd8c":"**Data Augmentation**\n\n\nTo prevent the problem of overfitting, we can artificially enlarge the dataset. I can increase the size of the current dataset. The idea is to alter the training data with small transformations to reproduce the variations. \u00a0 Data augmentation strategies are methods for changing the array representation while keeping the label the same while altering the training data. Grayscales, horizontal and vertical flips, random crops, color jitters, translations, rotations, and many other augmentations are popular. I can easily double or increase the number of training examples by applying only a couple of these changes to the training data.","c26057d1":"**Performing One Hot Encoding on the labels after converting it into numerical values**","f247046e":"A brain tumor is one of the most dangerous diseases that can affect both children and adults. The majority of primary Central Nervous System (CNS) tumors are brain tumors, which account for 85 to 90% of all cases. Around 11,700 people are diagnosed with a brain tumor each year. People with a cancerous brain or CNS tumor have a 5-year survival rate of approximately 34% for men and 36% for women. Glioma Tumors, Meningioma Tumors, Pituitary Tumors, and other types of brain tumors are classified. To increase the patients' life expectancy, proper treatment, planning, and accurate diagnostics should be implemented. Magnetic Resonance Imaging (MRI) is the most effective method for detecting brain tumors. The scans generate a tremendous amount of image data. The radiologist inspects these photographs. Because of the complexity involved in brain tumors and their properties, a manual examination can be prone to errors.","c6603c44":"**Evaluation**","6df5d13a":"The dataset is organized into 2 folders (Training, Testing) and contains 4 subfolders for each image category. There are 3,264 MRI images (JPEG) and 4 categories (Glioma\/Meningioma\/Pituitary\/No_tumor).\n1. Training set\n* Glioma tumor (826 images)\n* Meningioma tumor (822 images)\n* No tumor (395 images)\n* Pituitary tumor (827 images)\n2. Testing set\n* Glioma tumor (100 images)\n* Meningioma tumor (115 images)\n* No tumor (105 images)\n* Pituitary tumor (74 images)","cccbb549":"**Prediction using VGG16 model**","39cc55c2":"# Importing the necessary libraries","268cec4e":"**Callback function**\n\n\nCallbacks can help fix bugs more quickly, and can help build better models. They can help you visualize how your model\u2019s training is going, and can even help prevent overfitting by implementing early stopping or customizing the learning rate on each iteration.\n\nBy definition, \"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\"\n\nIn this notebook, I'll be using TensorBoard, ModelCheckpoint and ReduceLROnPlateau callback functions","91e2fbce":"# Experimental results","c9f5af9a":"**EfficientNetB0**","bccddc66":"# Grad-CAM visualization\n\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.","ced18fed":"Deep learning is being used to forecast a variety of brain tumor. The major objectives of this notebook are to attain a high level of brain tumor prediction accuracy. Deep learning approaches assist in the early diagnosis of brain tumor. The classification of brain tumor relies heavily on feature extraction. Human labor, such as manual feature extraction and data reconstruction for classification purposes, is reduced when Deep Learning algorithms are used. As a result, proposing a system that uses Deep Learning Algorithms using Convolution Neural Network (CNN) and Transfer Learning (TL) to do classification will be beneficial to doctors all over the world."}}