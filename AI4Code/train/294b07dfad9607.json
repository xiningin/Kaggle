{"cell_type":{"6d0bcfad":"code","a5ec29c6":"code","8e84323e":"code","5d636dfd":"code","fcd5512d":"code","4ee7e764":"code","b0f8d030":"code","08737abb":"code","05053a8d":"code","c737d779":"markdown","8170efbd":"markdown","fc2973d9":"markdown","7cb2bd7f":"markdown","25eaef76":"markdown"},"source":{"6d0bcfad":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\ndf_train = pd.read_csv(\"..\/input\/learn-ai-bbc\/BBC News Train.csv\")\ndf_train['category_id'] = df_train['Category'].factorize()[0]\ndf_train.groupby('Category').category_id.count()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(df_train.Text).toarray()\nlabels = df_train.category_id\ncategory_to_id = {'business':0, 'tech':1, 'politics':2, 'sport':3, 'entertainment':4}\nid_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}\n","a5ec29c6":"from sklearn.model_selection import train_test_split\n\nmodel = RandomForestClassifier()\n\n#Split Data \nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df_train.index, test_size=0.33, random_state=0)\n\n#Train Algorithm\nmodel.fit(X_train, y_train)\n\n# Make Predictions\ny_pred_proba = model.predict_proba(X_test)\ny_pred = model.predict(X_test)\ncategory_id_df = df_train[['Category', 'category_id']].drop_duplicates().sort_values('category_id')","8e84323e":"model2 = RandomForestClassifier()\nmodel2.fit(features, labels)","5d636dfd":"text = 'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (\u00a35.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'","fcd5512d":"text","4ee7e764":"test_features = tfidf.transform([text])\nprediction = model.predict(test_features)\nid_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}\nfor i in range(len(prediction)):\n    print(id_to_category[prediction[i]])","b0f8d030":"from sklearn import preprocessing","08737abb":"import math\nimport spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\nnltk.download('all', halt_on_error=False)\nfrom nltk.corpus import sentiwordnet as swn\nnlp = spacy.load('en', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\ndef remove_special_characters(text):\n    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\ndef normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    for doc in corpus:        \n        if accented_char_removal:\n            doc = remove_accented_chars(doc)            \n        if contraction_expansion:\n            doc = expand_contractions(doc)           \n        if text_lower_case:\n            doc = doc.lower()        \n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)          \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)        \n        if text_lemmatization:\n            doc = lemmatize_text(doc)          \n        if special_char_removal:\n            doc = remove_special_characters(doc)        \n        doc = re.sub(' +', ' ', doc)        \n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)            \n        normalized_corpus.append(doc)        \n    return normalized_corpus\nnptext = np.array([text])\nnorm_articles = normalize_corpus(nptext)\ndef analyze_sentiment_sentiwordnet_lexicon(article):\n    tagged_text = [(token.text, token.tag_) for token in nlp(article)]\n    pos_score = neg_score = token_count = obj_score = 0\n    for word, tag in tagged_text:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n#             print(ss_set)\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n#             ss_set.pos_score()*= 3\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        if ss_set:\n            if 'JJ' in tag:\n                pos_score += ss_set.pos_score()*4\n                neg_score += ss_set.neg_score()*4\n#                 token_count +=3\n            elif 'RB' in tag:\n                pos_score += ss_set.pos_score()*3\n                neg_score += ss_set.neg_score()*3\n#                 token_count +=2\n            elif 'VB' in tag:\n                pos_score += ss_set.pos_score()*2\n                neg_score += ss_set.neg_score()*2\n            else:\n                pos_score += ss_set.pos_score()\n                neg_score +=ss_set.neg_score()\n                token_count += 1\n            obj_score += ss_set.obj_score()\n            token_count +=1\n    final_score = pos_score - neg_score\n#     print(str(token_count)+'words which are being considered as tokens')\n    norm_final_score = round(float(final_score) \/ token_count, 4)\n#     final_sentiment = math.exp(final_sentiment)\n#     final_sentiment = math.log(final_score)\n#     final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n#     norm_final_score = (20 * (norm_final_score)\/2) - 10\n    norm_final_score = math.exp(norm_final_score)\n    norm_final_score = (20 * (norm_final_score)\/2.35) - 10\n    if norm_final_score>=1:\n        final_sentiment = 'positive'\n    elif norm_final_score<=-1:\n        final_sentiment = 'negative'\n    else:\n        final_sentiment = 'neutral'\n    if norm_final_score>10:\n        norm_final_score = 10\n    if norm_final_score<-10:\n        norm_final_score = -10\n    #################  NOTE  ##########################\n    # Please change this if statement to suit the needs because I've only done it using an arbitrary \n    #statement which might be wrong\n    #So please change the if statement\n    #Please please see if we can do it for a specific named entity \n#     (20*math.exp(norm_final_score)\/(2.35)) - 10\n#     exp_sentiment = math.exp(norm_final_score)\n#     minmax_scale = preprocessing.MinMaxScaler(feature_range=(-10, 10))\n#     scaled_feature = minmax_scale.fit_transform(exp_sentiment)\n    return final_sentiment, (norm_final_score), final_score\npredicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(article) for article in norm_articles]\nprint('predicted sentiment score:'+ str(predicted_sentiments[0][1]))\nprint('predicted sentiment:' + str(predicted_sentiments[0][0]))\n# print('predicted sentiment scaled feature:' + str(predicted_sentiments[0][3]))","05053a8d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nfrom collections import defaultdict, OrderedDict\nfrom math import sqrt\nfrom operator import itemgetter\nfrom spacy.tokens import Doc\nimport graphviz\nimport json\nimport logging\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport string\nimport sys\nimport time\nimport unicodedata\n# doc = nlp(text)\n\n######################################################################\n## utility functions\n######################################################################\n\nPAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\nPAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\nPAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n\n\ndef split_grafs (lines):\n    \"\"\"\n    segment raw text, given as a list of lines, into paragraphs\n    \"\"\"\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield \"\\n\".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield \"\\n\".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    \"\"\"\n    filter the quoted text out of a message\n    \"\"\"\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, re.M)\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(\"\\n\"):\n        if line.startswith(\">\"):\n            lines.append(\"\")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\ndef maniacal_scrubber (text):\n    \"\"\"\n    it scrubs the garble from its stream...\n    or it gets the debugger again\n    \"\"\"\n    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n\n    x = x.replace('\u201c', '\"').replace('\u201d', '\"')\n    x = x.replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\").replace(\"`\", \"'\")\n    x = x.replace(\"\u2026\", \"...\").replace(\"\u2013\", \"-\")\n\n    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n\n    # some web content returns \"not string\" ?? ostensibly no longer\n    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n    # have been found in the wild -- YMMV\n\n    try:\n        assert type(x).__name__ == \"str\"\n    except AssertionError:\n        print(\"not a string?\", type(line), line)\n\n    return x\n\n\ndef default_scrubber (text):\n    \"\"\"\n    remove spurious punctuation (for English)\n    \"\"\"\n    return text.lower().replace(\"'\", \"\")\n\n\n######################################################################\n## class definitions\n######################################################################\n\nclass CollectedPhrase:\n    \"\"\"\n    represents one phrase during the collection process\n    \"\"\"\n\n    def __init__ (self, chunk, scrubber):\n        self.sq_sum_rank = 0.0\n        self.non_lemma = 0\n        \n        self.chunk = chunk\n        self.text = scrubber(chunk.text)\n\n\n    def __repr__ (self):\n        return \"{:.4f} ({},{}) {} {}\".format(\n            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n        )\n\n\n    def range (self):\n        \"\"\"\n        generate the index range for the span of tokens in this phrase\n        \"\"\"\n        return range(self.chunk.start, self.chunk.end)\n\n\n    def set_key (self, compound_key):\n        \"\"\"\n        create a unique key for the the phrase based on its lemma components\n        \"\"\"\n        self.key = tuple(sorted(list(compound_key)))\n\n\n    def calc_rank (self):\n        \"\"\"\n        since noun chunking is greedy, we normalize the rank values\n        using a point estimate based on the number of non-lemma\n        tokens within the phrase\n        \"\"\"\n        chunk_len = self.chunk.end - self.chunk.start + 1\n        non_lemma_discount = chunk_len \/ (chunk_len + (2.0 * self.non_lemma) + 1.0)\n\n        # normalize the contributions of all the kept lemma tokens\n        # within the phrase using root mean square (RMS)\n\n        self.rank = sqrt(self.sq_sum_rank \/ (chunk_len + self.non_lemma)) * non_lemma_discount\n\n\nclass Phrase:\n    \"\"\"\n    represents one extracted phrase\n    \"\"\"\n\n    def __init__ (self, text, rank, count, phrase_list):\n        self.text = text\n        self.rank = rank\n        self.count = count\n        self.chunks = [p.chunk for p in phrase_list]\n\n\n    def __repr__ (self):\n        return self.text\n\n\nclass TextRank:\n    \"\"\"\n    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n    used to extract the top-ranked phrases from a text document\n    \"\"\"\n    _EDGE_WEIGHT = 1.0\n    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n    _TOKEN_LOOKBACK = 3\n    \n\n    def __init__ (\n            self,\n            edge_weight=_EDGE_WEIGHT,\n            logger=None,\n            pos_kept=_POS_KEPT,\n            scrubber=default_scrubber,\n            token_lookback=_TOKEN_LOOKBACK\n    ):\n        self.edge_weight = edge_weight\n        self.logger = logger\n        self.pos_kept = pos_kept\n        self.scrubber = scrubber\n        self.stopwords = defaultdict(list)\n        self.token_lookback = token_lookback\n\n        self.doc = None\n        self.reset()\n\n\n    def reset (self):\n        \"\"\"\n        initialize the data structures needed for extracting phrases\n        removing any state\n        \"\"\"\n        self.elapsed_time = 0.0\n        self.lemma_graph = nx.Graph()\n        self.phrases = defaultdict(list)\n        self.ranks = {}\n        self.seen_lemma = OrderedDict()\n\n\n    def load_stopwords (self, path=\"stop.json\"):\n        \"\"\"\n        load a list of \"stop words\" that get ignored when constructing\n        the lemma graph -- NB: be cautious when using this feature\n        \"\"\"\n        stop_path = None\n\n        # check if the path is fully qualified, or if the file is in\n        # the current working directory\n\n        if os.path.isfile(path):\n            stop_path = path\n        else:\n            cwd = os.getcwd()\n            stop_path = os.path.join(cwd, path)\n\n            if not os.path.isfile(stop_path):\n                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n                stop_path = os.path.join(loc, path)\n\n        try:\n            with open(stop_path, \"r\") as f:\n                data = json.load(f)\n\n                for lemma, pos_list in data.items():\n                    self.stopwords[lemma] = pos_list\n        except FileNotFoundError:\n            pass\n\n\n    def increment_edge (self, node0, node1):\n        \"\"\"\n        increment the weight for an edge between the two given nodes,\n        creating the edge first if needed\n        \"\"\"\n        if self.logger:\n            self.logger.debug(\"link {} {}\".format(node0, node1))\n    \n        if self.lemma_graph.has_edge(node0, node1):\n            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n        else:\n            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n\n\n    def link_sentence (self, sent):\n        \"\"\"\n        link nodes and edges into the lemma graph for one parsed sentence\n        \"\"\"\n        visited_tokens = []\n        visited_nodes = []\n\n        for i in range(sent.start, sent.end):\n            token = self.doc[i]\n\n            if token.pos_ in self.pos_kept:\n                # skip any stop words...\n                lemma = token.lemma_.lower().strip()\n\n                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n                    continue\n\n                # ...otherwise proceed\n                key = (token.lemma_, token.pos_)\n\n                if key not in self.seen_lemma:\n                    self.seen_lemma[key] = set([token.i])\n                else:\n                    self.seen_lemma[key].add(token.i)\n\n                node_id = list(self.seen_lemma.keys()).index(key)\n\n                if not node_id in self.lemma_graph:\n                    self.lemma_graph.add_node(node_id)\n\n                if self.logger:\n                    self.logger.debug(\"visit {} {}\".format(\n                        visited_tokens, visited_nodes\n                    ))\n                    self.logger.debug(\"range {}\".format(\n                        list(range(len(visited_tokens) - 1, -1, -1))\n                    ))\n            \n                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                    if self.logger:\n                        self.logger.debug(\"prev_tok {} {}\".format(\n                            prev_token, (token.i - visited_tokens[prev_token])\n                        ))\n                \n                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n                        self.increment_edge(node_id, visited_nodes[prev_token])\n                    else:\n                        break\n\n                if self.logger:\n                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n                    ))\n\n                visited_tokens.append(token.i)\n                visited_nodes.append(node_id)\n\n\n    def collect_phrases (self, chunk):\n        \"\"\"\n        collect instances of phrases from the lemma graph\n        based on the given chunk\n        \"\"\"\n        phrase = CollectedPhrase(chunk, self.scrubber)\n        compound_key = set([])\n\n        for i in phrase.range():\n            token = self.doc[i]\n            key = (token.lemma_, token.pos_)\n        \n            if key in self.seen_lemma:\n                node_id = list(self.seen_lemma.keys()).index(key)\n                rank = self.ranks[node_id]\n                phrase.sq_sum_rank += rank\n                compound_key.add(key)\n        \n                if self.logger:\n                    self.logger.debug(\" {} {} {} {}\".format(\n                        token.lemma_, token.pos_, node_id, rank\n                    ))\n            else:\n                phrase.non_lemma += 1\n    \n        phrase.set_key(compound_key)\n        phrase.calc_rank()\n\n        self.phrases[phrase.key].append(phrase)\n\n        if self.logger:\n            self.logger.debug(phrase)\n\n\n    def calc_textrank (self):\n        \"\"\"\n        iterate through each sentence in the doc, constructing a lemma graph\n        then returning the top-ranked phrases\n        \"\"\"\n        self.reset()\n        t0 = time.time()\n\n        for sent in self.doc.sents:\n            self.link_sentence(sent)\n\n        if self.logger:\n            self.logger.debug(self.seen_lemma)\n\n        # to run the algorithm, we use PageRank \u2013 i.e., approximating\n        # eigenvalue centrality \u2013 to calculate ranks for each of the\n        # nodes in the lemma graph\n\n        self.ranks = nx.pagerank(self.lemma_graph)\n\n        # collect the top-ranked phrases based on both the noun chunks\n        # and the named entities\n\n        for chunk in self.doc.noun_chunks:\n            self.collect_phrases(chunk)\n\n        for ent in self.doc.ents:\n            self.collect_phrases(ent)\n\n        # since noun chunks can be expressed in different ways (e.g., may\n        # have articles or prepositions), we need to find a minimum span\n        # for each phrase based on combinations of lemmas\n\n        min_phrases = {}\n\n        for phrase_key, phrase_list in self.phrases.items():\n            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n            best_phrase = phrase_list[0]\n            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n\n        # yield results\n\n        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n\n        phrase_list = [\n            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n        ]\n\n        t1 = time.time()\n        self.elapsed_time = (t1 - t0) * 1000.0\n\n        return phrase_list\n\n\n    def write_dot (self, path=\"graph.dot\"):\n        \"\"\"\n        output the lemma graph in Dot file format\n        \"\"\"\n        keys = list(self.seen_lemma.keys())\n        dot = graphviz.Digraph()\n\n        for node_id in self.lemma_graph.nodes():\n            text = keys[node_id][0].lower()\n            rank = self.ranks[node_id]\n            label = \"{} ({:.4f})\".format(text, rank)\n            dot.node(str(node_id), label)\n\n        for edge in self.lemma_graph.edges():\n            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n\n        with open(path, \"w\") as f:\n            f.write(dot.source)\n\n\n    def summary (self, limit_phrases=10, limit_sentences=4):\n        \"\"\"\n        run extractive summarization, based on vector distance \n        per sentence from the top-ranked phrases\n        \"\"\"\n        unit_vector = []\n\n        # construct a list of sentence boundaries with a phrase set\n        # for each (initialized to empty)\n\n        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n\n        # iterate through the top-ranked phrases, added them to the\n        # phrase vector for each sentence\n\n        phrase_id = 0\n\n        for p in self.doc._.phrases:\n            unit_vector.append(p.rank)\n\n            if self.logger:\n                self.logger.debug(\n                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n                )\n    \n            for chunk in p.chunks:\n                for sent_start, sent_end, sent_vector in sent_bounds:\n                    if chunk.start >= sent_start and chunk.start <= sent_end:\n                        sent_vector.add(phrase_id)\n\n                        if self.logger:\n                            self.logger.debug(\n                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n                                )\n\n                        break\n\n            phrase_id += 1\n\n            if phrase_id == limit_phrases:\n                break\n\n        # construct a unit_vector for the top-ranked phrases, up to\n        # the requested limit\n\n        sum_ranks = sum(unit_vector)\n        unit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\n        # iterate through each sentence, calculating its euclidean\n        # distance from the unit vector\n\n        sent_rank = {}\n        sent_id = 0\n\n        for sent_start, sent_end, sent_vector in sent_bounds:\n            sum_sq = 0.0\n    \n            for phrase_id in range(len(unit_vector)):\n                if phrase_id not in sent_vector:\n                    sum_sq += unit_vector[phrase_id]**2.0\n\n            sent_rank[sent_id] = sqrt(sum_sq)\n            sent_id += 1\n\n        # extract the sentences with the lowest distance\n\n        sent_text = {}\n        sent_id = 0\n\n        for sent in self.doc.sents:\n            sent_text[sent_id] = sent\n            sent_id += 1\n\n        # yield results, up to the limit requested\n\n        num_sent = 0\n\n        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n            yield sent_text[sent_id]\n            num_sent += 1\n\n            if num_sent == limit_sentences:\n                break\n\n\n    def PipelineComponent (self, doc):\n        \"\"\"\n        define a custom pipeline component for spaCy and extend the\n        Doc class to add TextRank\n        \"\"\"\n        self.doc = doc\n        Doc.set_extension(\"phrases\", force=True, default=[])\n        Doc.set_extension(\"textrank\", force=True, default=self)\n        doc._.phrases = self.calc_textrank()\n\n        return doc\n\nlength=3\ntr = TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\ndoc = nlp(text)\nsent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\nlimit_phrases = 4\n\nphrase_id = 0\nunit_vector = []\n\nfor p in doc._.phrases:\n#     print(phrase_id, p.text, p.rank)\n    \n    unit_vector.append(p.rank)\n    \n    for chunk in p.chunks:\n#         print(\" \", chunk.start, chunk.end)\n        \n        for sent_start, sent_end, sent_vector in sent_bounds:\n            if chunk.start >= sent_start and chunk.start <= sent_end:\n#                 print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n                sent_vector.add(phrase_id)\n                break\n\n    phrase_id += 1\n\n    if phrase_id == limit_phrases:\n        break\nsum_ranks = sum(unit_vector)\nunit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\nfrom math import sqrt\n\nsent_rank = {}\nsent_id = 0\n\nfor sent_start, sent_end, sent_vector in sent_bounds:\n#     print(sent_vector)\n    sum_sq = 0.0\n    \n    for phrase_id in range(len(unit_vector)):\n#         print(phrase_id, unit_vector[phrase_id])\n        \n        if phrase_id not in sent_vector:\n            sum_sq += unit_vector[phrase_id]**2.0\n\n    sent_rank[sent_id] = sqrt(sum_sq)\n    sent_id += 1\nfrom operator import itemgetter\n\nsorted(sent_rank.items(), key=itemgetter(1)) \nlimit_sentences = length\n\nsent_text = {}\nsent_id = 0\n\nfor sent in doc.sents:\n    sent_text[sent_id] = sent.text\n    sent_id += 1\n\nnum_sent = 0\n\nfor sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n    print(sent_text[sent_id])\n    num_sent += 1\n    \n    if num_sent == limit_sentences:\n        break","c737d779":"# Text classification","8170efbd":"# The text cell that has been chosen","fc2973d9":"# Moving onto text summarization","7cb2bd7f":"@Rajarshi check the new conditions that I've added for sentiment analysis scores","25eaef76":"# Moving onto sentiment analysis"}}