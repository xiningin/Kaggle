{"cell_type":{"d7002ba3":"code","44ad2aff":"code","f521b80d":"code","b1514299":"code","31cd4c2c":"code","85579ff9":"code","de4ada78":"code","50d645c3":"code","cacf4f4c":"code","b26b7432":"code","31dc326d":"code","08eda199":"code","96d9999a":"code","0d02a397":"code","686f77cd":"code","403a9e01":"code","f3b304e8":"code","6c5b38af":"code","8461858c":"code","28d82b02":"code","5aa80033":"markdown","817a4bfc":"markdown","cea5910e":"markdown","563ab192":"markdown","0e4a0395":"markdown","24b6fb5a":"markdown","d82fd2ea":"markdown","53fa506d":"markdown","7600cd6e":"markdown","bee72c5d":"markdown","38752caf":"markdown","74a42328":"markdown","bc530148":"markdown","ddc16993":"markdown"},"source":{"d7002ba3":"import pandas as pd\n\niris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","44ad2aff":"iris.head()","f521b80d":"#What is the dataset Size (rows, columns)\niris.shape","b1514299":"# what datatypes we have?\niris.dtypes","31cd4c2c":"# Do we have missing values?\niris.isnull().sum(axis = 0)","85579ff9":"# how many species do we have?\niris['Species'].unique()","de4ada78":"# How many record of each species?\niris.groupby('Species').size()\n","50d645c3":"# Statistical Summary of each numeric variable\niris.describe()","cacf4f4c":"#Categorical Variable stats\niris['Species'].describe()","b26b7432":"from matplotlib import pyplot\niris.hist()\npyplot.show()","31dc326d":"# scatter plot matrix\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(iris)\npyplot.show()","08eda199":"# Creating the X dataset, which includes all variable except target variable\nX= iris.drop(columns=[\"Id\", \"Species\"])\nX","96d9999a":"# Creating the y dataset, which included the target (dependant) variable only\ntarget=  iris.drop([\"Id\", \"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], axis=1)\ntarget","0d02a397":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","686f77cd":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\n\nfor name, model in models:\n    accuracy = cross_val_score(model, X, target.values.ravel(), scoring='accuracy', cv = 10)\n    print(\"Accuracy of %s: is %.2f percent\" % (name ,accuracy.mean()*100))\n    \n      ","403a9e01":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size=0.20, random_state=83)\nprint (\"X_train (rows, columns): \", X_train.shape)\nprint (\"Y_train (rows, columns): \", Y_train.shape)\nprint(\"X_test (rows, columns): \", X_test.shape)\nprint (\"Y_test (rows, columns): \", Y_test.shape)\n","f3b304e8":"# view the datasets\n\nprint (\"X_train: \", X_train)\nprint (\"Y_train: \", Y_train)\nprint(\"X_test: \", X_test)\nprint (\"Y_test: \", Y_test)","6c5b38af":"# Make predictions on testing dataset\nfrom sklearn.svm import SVC\n\nmodel = SVC(gamma='auto')\nmodel.fit(X_train, Y_train.values.ravel())\npredictions = pd.DataFrame(model.predict(X_test), columns=['predictions'])","8461858c":"#Compare predictions to actual values\npd.concat([X_test.reset_index(drop='True'), Y_test.reset_index(drop='True'),predictions],axis=1)","28d82b02":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n#Accuracy: (True Positives + True Negatives)\/(Total observations) (28\/30=93.33%)\nprint(\"Accuracy Score is:\" ,accuracy_score(Y_test, predictions)*100)\n\n# Confusion Matric showing the two observations that we got wrong\nprint(confusion_matrix(Y_test, predictions))\n\n# Precision=TP\/(TP+FP)   Recall=TP\/(TP+FP)\n# Recall for Iris-virginica = 9\/11= 82%\nprint(classification_report(Y_test, predictions))","5aa80033":"<center><h1> Classification of Iris Species\n<HR>","817a4bfc":"Based on the accuracy results shown above, we will use SVM","cea5910e":"![species.jpg](attachment:species.jpg)\nThe Iris Dataset contains four attributes (length and width of sepals) for three different species of Iris (Iris setosa, Iris virginica and Iris versicolor). We will create a model that will predict the Iris specie based on the four attributes.","563ab192":"## 3. Evaluating different ML models\nBefore choosing a model to make predictions, we will check\/evaluate several algorithms to see which one performs best for our dataset. ","0e4a0395":"    We have 50 records of each, dataset is balanced!","24b6fb5a":"    No Nulls","d82fd2ea":"    We have three species: 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'","53fa506d":"## 4. Making Predictions","7600cd6e":"## 2. Splitting the Dataset\nCreating the two datasets (X and target) required for the train_test_split function. We will also use them to evaluate different models.","bee72c5d":"The data is clean and ready to be used in a model. No data manipulation is needed.","38752caf":"Four numeric variables and one categorical","74a42328":"## 1. Data Exploration","bc530148":"As we can see, two prediction for Iris-verginica are wrong - records 12 and 28","ddc16993":"top: most frequently occuring. each variable apears 50 times, not just Iris-virsicolor.\nfreq: number of time the top value occures"}}