{"cell_type":{"903a81ae":"code","1a12c98e":"code","ed8cfb0d":"code","7bc4ea5f":"code","f710b819":"code","c139ec0f":"code","8b61bf0c":"code","98ccc095":"code","bf043c25":"code","6ac5088b":"code","ae6803e4":"code","d88616b5":"code","c9bd2b9a":"code","8722586e":"code","d48c5d5d":"code","d34a44ff":"code","ef69cd4c":"code","2bb0bb67":"code","e7581cc1":"code","add13ac1":"code","0e2bc5a0":"markdown","da8291e5":"markdown","a5a6e3d9":"markdown","c80ab387":"markdown","2ff2b17a":"markdown","d47d82e0":"markdown","df806236":"markdown","faf5b5f4":"markdown","42466a06":"markdown","ffa965ce":"markdown","e0e91525":"markdown","774b2440":"markdown","618e282a":"markdown","99bc51a5":"markdown","7f6e3dcf":"markdown","fbb52921":"markdown","119e8721":"markdown","85cac7cd":"markdown"},"source":{"903a81ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1a12c98e":"bit_data=pd.read_csv(\"..\/input\/bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv\")\nbit_data[\"date\"]=pd.to_datetime(bit_data[\"Timestamp\"],unit=\"s\").dt.date\ngroup=bit_data.groupby(\"date\")\ndata=group[\"Close\"].mean()","ed8cfb0d":"data.shape","7bc4ea5f":"data.isnull().sum()","f710b819":"data.head()","c139ec0f":"close_train=data.iloc[:len(data)-50]\nclose_test=data.iloc[len(close_train):]","8b61bf0c":"#feature scalling (set values between 0-1)\nclose_train=np.array(close_train)\nclose_train=close_train.reshape(close_train.shape[0],1)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\nclose_scaled=scaler.fit_transform(close_train)","98ccc095":"timestep=50\nx_train=[]\ny_train=[]\n\nfor i in range(timestep,close_scaled.shape[0]):\n    x_train.append(close_scaled[i-timestep:i,0])\n    y_train.append(close_scaled[i,0])\n\nx_train,y_train=np.array(x_train),np.array(y_train)\nx_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1) #reshaped for RNN\nprint(\"x_train shape= \",x_train.shape)\nprint(\"y_train shape= \",y_train.shape)","bf043c25":"from keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Dropout,Flatten\n\nregressor=Sequential()\n#first RNN layer\nregressor.add(SimpleRNN(128,activation=\"relu\",return_sequences=True,input_shape=(x_train.shape[1],1)))\nregressor.add(Dropout(0.25))\n#second RNN layer\nregressor.add(SimpleRNN(256,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#third RNN layer\nregressor.add(SimpleRNN(512,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.35))\n#fourth RNN layer\nregressor.add(SimpleRNN(256,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#fifth RNN layer\nregressor.add(SimpleRNN(128,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#convert the matrix to 1-line\nregressor.add(Flatten())\n#output layer\nregressor.add(Dense(1))\n\nregressor.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\nregressor.fit(x_train,y_train,epochs=100,batch_size=64)","6ac5088b":"inputs=data[len(data)-len(close_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","ae6803e4":"x_test=[]\nfor i in range(timestep,inputs.shape[0]):\n    x_test.append(inputs[i-timestep:i,0])\nx_test=np.array(x_test)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)","d88616b5":"predicted_data=regressor.predict(x_test)\npredicted_data=scaler.inverse_transform(predicted_data)","c9bd2b9a":"data_test=np.array(close_test)\ndata_test=data_test.reshape(len(data_test),1)","8722586e":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","d48c5d5d":"from sklearn.metrics import mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout,Flatten\n\nmodel=Sequential()\n\nmodel.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n\nmodel.add(Dense(1))\n\nmodel.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n\nmodel.fit(x_train,y_train,epochs=100,batch_size=32)","d34a44ff":"inputs=data[len(data)-len(close_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","ef69cd4c":"x_test=[]\nfor i in range(timestep,inputs.shape[0]):\n    x_test.append(inputs[i-timestep:i,0])\nx_test=np.array(x_test)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)","2bb0bb67":"predicted_data=model.predict(x_test)\npredicted_data=scaler.inverse_transform(predicted_data)","e7581cc1":"data_test=np.array(close_test)\ndata_test=data_test.reshape(len(data_test),1)","add13ac1":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","0e2bc5a0":"*We will use our 50 data to predict 51th data. Also I will take first 200000 data*","da8291e5":"**Keep going with LSTM method**","a5a6e3d9":"What I did here? I added a colum which is \"date\" and I converted \"Timestamp\" columns to date form.","c80ab387":"I am separating last 50 rows as the test data.","2ff2b17a":"Here we have the results of our prediction. As we can see on the chart that with RNN method we don't have a good result. So let's check LSTM method, then we can compare both results.","d47d82e0":"**What is the differences between RNN & LSTM?**\n\nBefore I tell you what is LSTM let me tell you about the biggest problem with RNNs. So far everything looks good about RNNs until we train it via back-propagation. As the gradient of our training samples gets propagated backward through our network, it gets weaker and weaker, by the time it gets to those neurons that represent older data points in our time-series it has no juice to adjust them properly. This problem is called Vanishing Gradient. A LSTM cell is a type of RNN which stores important information about the past and forgets the unimportant pieces. In this way, when gradient back-propagates, it won\u2019t be consumed by unnecessary information.","df806236":"The goal is making a prediction of daily Close data. So we will predict \"close\" values of bitcoin data","faf5b5f4":"**What did we do???? We learnt our model with train data and then we tried to predict next 50 data (train data=50). Then we compared predictions with our test (real) data.**","42466a06":"Keep going with test data","ffa965ce":"<a href=\"https:\/\/ibb.co\/rv1GT5D\"><img src=\"https:\/\/i.ibb.co\/fQPYBrj\/rnn-lstm.png\" alt=\"rnn-lstm\" border=\"0\"><\/a>","e0e91525":"Let's choose each 50 data as x-train and 51th as y-train","774b2440":"Now we are preparing our test data for prediction","618e282a":"Here I will set our values between 0-1 in order to avoid domination of high values.","99bc51a5":"* ** Conclusion**\n\nWhen we compare the both results (RNN and LSTM) we can see that we have better prediction with LSTM. \n\nUp to here I was trying to show the differences between RNN - LSTM and how to use these methods on time series. \n\nIf you have any question or I have any error please write me dirctly.\n","7f6e3dcf":"I will use the same train and test data","fbb52921":"Time to prepare and run our RNN method","119e8721":"**First I will use RNN to predict our data**","85cac7cd":"it's time to predict"}}