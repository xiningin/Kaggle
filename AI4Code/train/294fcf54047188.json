{"cell_type":{"3790a113":"code","50eabe14":"code","8ed5e63a":"code","953d0e31":"code","91550ef0":"code","f695ce96":"code","6e81fa48":"code","028aede9":"code","e448a2dc":"code","d5eb870d":"code","dc25b067":"code","e274a1f7":"code","ca6c64dd":"code","b582e7ed":"code","b7997a62":"code","883c4a4c":"code","3215f10f":"code","5023bf30":"code","53676a46":"code","eb3a4583":"code","86811596":"code","524bd6d6":"code","3d88d187":"code","ea45eb9b":"code","8d10badf":"code","11ee730b":"code","b1701cd4":"code","060b887d":"code","781ec409":"code","6e54ea7b":"code","4d3fed4b":"code","5788b5fa":"code","f0b0a94c":"code","d4e0306c":"code","5a46d8f9":"code","9ef9e883":"code","3be8f234":"code","124dddc8":"code","060d6dcd":"code","17677b64":"code","7f5b364b":"code","c4bfe2f8":"code","a6c5534c":"code","306a2919":"code","619babca":"code","ec403152":"code","1c395ca6":"code","126572d3":"code","5183932e":"code","27363990":"code","ea295b61":"code","679ef107":"code","2d1ccd61":"code","a597465b":"code","89ecc93d":"code","3a9c013b":"code","85ab6e3f":"code","1b574e8d":"code","5a9a805d":"code","175c6f86":"code","3b0762f8":"code","195328ff":"code","f75ab87b":"code","56e60746":"code","e64c39e5":"code","8e713901":"code","068057c8":"code","90e26c7b":"markdown","ee39eb41":"markdown","a90d57b3":"markdown","393f5be8":"markdown","d6ab18af":"markdown","184a79fa":"markdown","7cf748ff":"markdown","2052f5a3":"markdown","8da7ce2d":"markdown","49d19ebf":"markdown","694179b7":"markdown","6c62380f":"markdown","dbbcda8f":"markdown","9d4c98d3":"markdown","1bfdc4bf":"markdown","62d72ec7":"markdown","01fe82f1":"markdown","5d222915":"markdown","c009c15f":"markdown","d83b568a":"markdown","fd0268c9":"markdown","5c12363f":"markdown","c1e52eb6":"markdown","3a28a650":"markdown","04fef274":"markdown","92f76ceb":"markdown","c1fec14e":"markdown","77c37975":"markdown","e316a82f":"markdown","b2eda042":"markdown","49f55051":"markdown","b5c46531":"markdown","52fd880d":"markdown","25b2f54d":"markdown","a5e9f0f1":"markdown","6967202c":"markdown","a019eaf2":"markdown","c29ac55c":"markdown","d19362cb":"markdown","e0e25832":"markdown","1ff5112e":"markdown","eacf9d3c":"markdown","f316bcaf":"markdown","c8bf26a4":"markdown","f3cef21f":"markdown","db4a55b9":"markdown","c6d5cf13":"markdown","f0e8c1aa":"markdown","6a1b2a87":"markdown","785a9a26":"markdown","3cac9e89":"markdown","03985545":"markdown","4de6fdaf":"markdown","dec0a2d6":"markdown","d6622bb0":"markdown","46d33b38":"markdown","494710c2":"markdown","7c48157c":"markdown","4d30e8a5":"markdown","660fd774":"markdown","d051565a":"markdown","97024505":"markdown","5e156937":"markdown"},"source":{"3790a113":"import os, sys\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","50eabe14":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(2)","8ed5e63a":"df.drop(labels=['customerID'], axis=1, inplace=True)\ndf.head(2)","953d0e31":"df.info()","91550ef0":"## Shorten the Labels\nvalue_mapper = {'Female': 'F', 'Male': 'M', 'Yes': 'Y', 'No': 'N',\n                'No phone service': 'No phone', 'Fiber optic': 'Fiber',\n                'No internet service': 'No internet', 'Month-to-month': 'Monthly',\n                'Bank transfer (automatic)': 'Bank transfer',\n                'Credit card (automatic)': 'Credit card',\n                'One year': '1 yr', 'Two year': '2 yr'}\ndf.replace(to_replace=value_mapper, inplace=True)\n# Another method\n# df = df.applymap(lambda v: value_mapper[v] if v in value_mapper.keys() else v)","f695ce96":"df.columns = [label.lower() for label in df.columns]\ndf.head(10).T","6e81fa48":"df['totalcharges'] = pd.to_numeric(df['totalcharges'], errors='coerce')\ndf['totalcharges'].head()","028aede9":"df.info()","e448a2dc":"df[np.isnan(df['totalcharges'])]","d5eb870d":"df[df['tenure'] == 0].index","dc25b067":"df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)\ndf[df['tenure'] == 0].index","e274a1f7":"df.info()","ca6c64dd":"def summarize_categoricals(df, show_levels=False):\n    \"\"\"\n        Display uniqueness in each column\n    \"\"\"\n    data = [[df[c].unique(), len(df[c].unique()), df[c].isnull().sum()] for c in df.columns]\n    df_temp = pd.DataFrame(data, index=df.columns,\n                           columns=['Levels', 'No. of Levels', 'No. of Missing Values'])\n    return df_temp.iloc[:, 0 if show_levels else 1:]\n\n\ndef find_categorical(df, cutoff=10):\n    \"\"\"\n        Function to find categorical columns in the dataframe.\n    \"\"\"\n    cat_cols = []\n    for col in df.columns:\n        if len(df[col].unique()) <= cutoff:\n            cat_cols.append(col)\n    return cat_cols\n\n\ndef to_categorical(columns, df):\n    \"\"\"\n        Converts the columns passed in `columns` to categorical datatype\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].astype('category')\n    return df","b582e7ed":"summarize_categoricals(df, show_levels=True)","b7997a62":"df = to_categorical(find_categorical(df), df)\ndf.info()","883c4a4c":"new_order = list(df.columns)\nnew_order.insert(16, new_order.pop(4))\ndf = df[new_order]\ndf.head(2)","3215f10f":"df.describe().T","5023bf30":"sns.heatmap(data=df[['tenure', 'monthlycharges', 'totalcharges']].corr(),\n            annot=True, cmap='coolwarm');","53676a46":"sns.lmplot('monthlycharges', 'totalcharges', data=df, hue='churn',\n           scatter_kws={'alpha': 0.1})\nfig = sns.lmplot('tenure', 'totalcharges', data=df, hue='churn',\n                 scatter_kws={'alpha': 0.1})\nfig.set_xlabels('tenure (in months)');","eb3a4583":"def cramers_corrected_stat(contingency_table):\n    \"\"\"\n        Computes corrected Cramer's V statistic for categorial-categorial association\n    \"\"\"\n    chi2 = chi2_contingency(contingency_table)[0]\n    n = contingency_table.sum().sum()\n    phi2 = chi2\/n\n    \n    r, k = contingency_table.shape\n    r_corrected = r - (((r-1)**2)\/(n-1))\n    k_corrected = k - (((k-1)**2)\/(n-1))\n    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    \n    return (phi2_corrected \/ min( (k_corrected-1), (r_corrected-1)))**0.5","86811596":"def categorical_corr_matrix(df):\n    \"\"\"\n        Computes corrected Cramer's V statistic between\n        all the categorical variables in the dataframe\n    \"\"\"\n    df = df.select_dtypes(include='category')\n    cols = df.columns\n    n = len(cols)\n    corr_matrix = pd.DataFrame(np.zeros(shape=(n, n)), index=cols, columns=cols)\n    \n    for col1 in cols:\n        for col2 in cols:\n            if col1 == col2:\n                corr_matrix.loc[col1, col2] = 1\n                break\n            df_crosstab = pd.crosstab(df[col1], df[col2], dropna=False)\n            corr_matrix.loc[col1, col2] = cramers_corrected_stat(df_crosstab)\n    \n    # Flip and add to get full correlation matrix\n    corr_matrix += np.tril(corr_matrix, k=-1).T\n    return corr_matrix","524bd6d6":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(categorical_corr_matrix(df), annot=True, cmap='coolwarm', \n            cbar_kws={'aspect': 50}, square=True, ax=ax)\nplt.xticks(rotation=30, ha='right');\nplt.tight_layout()","3d88d187":"fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\ntitles = ['Gender', 'Senior Citizen', 'Partner', 'Dependents',\n          'Phone Service', 'Multiplelines', 'Internet Service', 'Online Security',\n          'Online Backup', 'Device Protection', 'Techsupport', 'Streaming TV',\n          'Streaming Movies', 'Contract', 'Paperless Billing', 'Churn']\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.countplot(x=''.join(title.lower().split()), data=df, palette='Pastel2', ax=ax)\n    ax.set_title(title)\n    ax.set_ylim(0, 7032)\n\nplt.tight_layout()","ea45eb9b":"df_grouped = df.groupby(by='churn')\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ncols = ['monthlycharges', 'tenure']\nfor i in range(len(cols)):\n    sns.distplot(df_grouped.get_group('N')[cols[i]], bins=20, ax=axes[i], label='N')\n    sns.distplot(df_grouped.get_group('Y')[cols[i]], bins=20, ax=axes[i], label='Y')\n    axes[i].legend(title='Churn')\n\nfig.tight_layout()","8d10badf":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\ncols = ['totalcharges', 'monthlycharges', 'tenure']\nfor i in range(len(cols)):\n    sns.boxplot(x='churn', y=cols[i], data=df, ax=axes[i])\nfig.tight_layout()","11ee730b":"churned_users = df_grouped.get_group('Y')\nbins = [(0, 12), (12, 24), (24, 36), (36, 48), (48, 60), (60, 72)]\nbinned_tenure = pd.cut(list(churned_users['tenure']),\n                       bins=pd.IntervalIndex.from_tuples(bins))\nrevenue_share = churned_users['monthlycharges'] \/ churned_users['monthlycharges'].sum()\ntemp_df = pd.DataFrame(data={'tenure':binned_tenure, \n                             'revenue_share(%)': revenue_share * 100})\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\nsns.barplot(x='tenure', y='revenue_share(%)', data=temp_df,\n            estimator=np.sum, palette='Pastel2', ax=ax)\nax.set_xlabel('Tenure (in months)')\nax.set_ylabel('Percent of Churned Revenue');","b1701cd4":"display(pd.crosstab(df['churn'], df['contract'], dropna=False))\nsns.countplot(x='churn', hue='contract', data=df, palette='Pastel2')\n\n# Put the legend out of the figure\nplt.legend(title='Contract', bbox_to_anchor=(1, 1));","060b887d":"## Adding new column to the dataframe temporarily\n# When the dataframe is grouped later, the estimator in `barplot`\n# will give groupwise revenue share\ndf['share'] = (df['monthlycharges'] * 100) \/ df['monthlycharges'].sum()\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\ntitles = ['Gender', 'Senior Citizen', 'Partner', 'Dependents',\n          'Phone Service', 'Multiplelines', 'Internet Service', 'Online Security',\n          'Online Backup', 'Device Protection', 'Techsupport', 'Streaming TV',\n          'Streaming Movies', 'Contract', 'Paperless Billing', 'Churn']\n\n## axs.flat is an attribute and contains a flattened axs vector\/ list\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.barplot(x=''.join(title.lower().split()), y='share', hue='churn', data=df,\n                estimator=np.sum, palette='Pastel2', ax=ax)\n    ax.set_title(title)\n    ax.set_ylim(0, 100)\n    ax.set_xlabel('')\n    ax.set_ylabel('Monthly Revenue (in %)')\n\nfig.tight_layout()","781ec409":"## Remove the 'share' column\ndf.drop(labels=['share'], axis=1, inplace=True)","6e54ea7b":"def modified_countplot(**kargs):\n    \"\"\"\n        Assumes that columns to be plotted are in of pandas dtype='CategoricalDtype'\n    \"\"\"\n    facet_gen = kargs['facet_generator']    ## Facet generator over facet data\n    curr_facet, facet_data = None, None\n    \n    while True:\n        ## Keep yielding until non-empty dataframe is found\n        curr_facet = next(facet_gen)            ## Yielding facet genenrator\n        df_rows = curr_facet[1].shape[0]\n        \n        ## Skip the current facet if its corresponding dataframe empty\n        if df_rows:\n            facet_data = curr_facet[1]\n            break\n    \n    x_hue = (kargs.get('x'), kargs.get('hue'))\n    cols = [col for col in x_hue if col]\n    col_categories = [facet_data[col].dtype.categories if col else None for col in x_hue]\n    \n    palette = kargs['palette'] if 'palette' in kargs.keys() else 'Pastel2'\n    sns.countplot(x=cols[0], hue=x_hue[1], \n                  order=col_categories[0], hue_order=col_categories[1],\n                  data=facet_data.loc[:, cols], palette=palette)","4d3fed4b":"## phone service, multiple lines, internet service\ndisplay(pd.crosstab(df['churn'], [df['phoneservice'], df['multiplelines'], df['internetservice']], dropna=False))\n\n## PLOT\nfacet = sns.FacetGrid(df, row='multiplelines', col='phoneservice',\n                      sharex=False, sharey=False, margin_titles=True)\nfacet.map(modified_countplot, x='churn', hue='internetservice',\n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('Churn')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='internetservice');","5788b5fa":"## phone service, internet service, seniorcitizen\ncrosstab_cols = [df['phoneservice'], df['seniorcitizen'], df['internetservice']]\ndisplay(pd.crosstab(df['churn'], crosstab_cols, dropna=False))\n\n## PLOT\nfacet = sns.FacetGrid(df, row='phoneservice', col='seniorcitizen', \n                      sharex=False, sharey=False, margin_titles=True)\nfacet.map(modified_countplot, x='churn', hue='internetservice', \n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('Churn')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='internetservice');","f0b0a94c":"## phone service, internet service, gender\ncrosstab_cols = [df['phoneservice'], df['gender'], df['internetservice']]\ndisplay(pd.crosstab(df['churn'], crosstab_cols, dropna=False))\n\n## PLOT\nfacet = sns.FacetGrid(df, row='phoneservice', col='gender', \n                      sharex=False, sharey=False, margin_titles=True)\nfacet.map(modified_countplot, x='churn', hue='internetservice',\n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('Churn')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='internetservice');","d4e0306c":"## internet service, streamingtv, streamingmovies\n# Remap values\n## Having 'DSL' or 'Fiber' internet implies having internet\nvalue_mapper = {'internetservice':  {'Fiber': 'Y', 'DSL': 'Y', \n                                     'No internet': 'N'},\n                'streamingtv': {'No internet': 'N'},\n                'streamingmovies': {'No internet': 'N'}}\ndf_modified = df.replace(to_replace=value_mapper)\ndf_modified = to_categorical(list(value_mapper.keys()), df_modified)\n\ndisplay(pd.crosstab(df_modified['churn'], \n                    [df_modified['internetservice'], \n                     df_modified['streamingtv'], \n                     df_modified['streamingmovies']], \n                    dropna=False))\n\n## PLOT\nfacet = sns.FacetGrid(df_modified, row='internetservice', col='streamingtv', \n                      sharey=False, margin_titles=True)\nfacet_data = facet.facet_data()\nfacet.map(modified_countplot, x='churn', hue='streamingmovies', \n          palette='Pastel2', facet_generator=facet_data)\nfacet.set_xlabels('Churn')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='streamingmovies');","5a46d8f9":"## phone service, multiple lines, internet service\ndisplay(pd.crosstab(df['churn'], [df['contract'], df['internetservice']], dropna=False))\n\n## PLOT\nfacet = sns.FacetGrid(df, col='contract',sharex=False, sharey=False)\nfacet.map(modified_countplot, x='internetservice', hue='churn',\n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('internetservice')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='Churn');","9ef9e883":"x = df.iloc[:, :-1]\ny = df['churn']\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\nnumeric_columns = list(x.select_dtypes(exclude='category').columns)","3be8f234":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\n\n# For CatBoost and Naive Bayes\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train_cat, x_test_cat, y_train_cat, y_test_cat = data_splits\n\n\n# Save the non-scaled version of monthlycharges and totalcharges to compare classifiers\nx_test_charges = np.array(x_test[['monthlycharges', 'totalcharges']], copy=True)\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","124dddc8":"pd.Series(y_test).value_counts()","060d6dcd":"sns.countplot(x=y_test);","17677b64":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\n\n\n## Column Transformer\ntransformers = [('one_hot_encoder',\n                  OneHotEncoder(drop='first',dtype='int'),\n                  categorical_columns),\n                ('standard_scaler', StandardScaler(), numeric_columns)]\nx_trans = ColumnTransformer(transformers, remainder='passthrough')\n\n## Applying Column Transformer\nx_train = x_trans.fit_transform(x_train)\nx_test = x_trans.transform(x_test)\n\n## Label encoding\ny_trans = LabelEncoder()\ny_train = y_trans.fit_transform(y_train)\ny_test = y_trans.transform(y_test)\n\n\n## Save feature names after one-hot encoding for feature importances plots\nfeature_names = list(x_trans.named_transformers_['one_hot_encoder'] \\\n                            .get_feature_names(input_features=categorical_columns))\nfeature_names = feature_names + numeric_columns","7f5b364b":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","c4bfe2f8":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","a6c5534c":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs, drop_intermediate=False)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","306a2919":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","619babca":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","ec403152":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","1c395ca6":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","126572d3":"def report(clf, x_train, y_train, x_test, y_test, sample_weight=None,\n           refit=False, importance_plot=False, confusion_labels=None,\n           feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                                     sample_weight=sample_weight,\n                                                     refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n    \n    \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) \/ 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"------------------------------------------\")\n    print(\" Area Under ROC: \", roc_auc)\n    print(\"------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, train_acc=train_acc, train_time=train_time,\n                train_predictions=train_predictions, test_acc=test_acc,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","5183932e":"def compare_models(y_test=None, clf_reports=[], labels=[]):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train Accuracy', 'Test Accuracy', 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        train_acc = round(clf_reports[i]['train_acc'], 3)\n        test_acc = round(clf_reports[i]['test_acc'], 3)\n        clf_probs = clf_reports[i]['test_probs']\n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = [train_acc, test_acc,\n                               test_acc < train_acc, roc_auc] + true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","27363990":"from sklearn.naive_bayes import CategoricalNB, GaussianNB \nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder\n\nconfusion_lbs = ['No Churn', 'Churn']\n\n## Discretize 'monthlycharges' and 'totalcharges' into 3bins\nkbn = KBinsDiscretizer(n_bins=3, encode='ordinal')\node = OrdinalEncoder(dtype=np.int64)\nnb_trans = [('ordinal', ode, categorical_columns),\n            ('kbn', kbn, numeric_columns[1:])]\nnb_col_trans = ColumnTransformer(nb_trans, remainder='passthrough')\n\n## Applying Column Transformer\nx_train_nb = nb_col_trans.fit_transform(x_train_cat)\nx_test_nb = nb_col_trans.transform(x_test_cat)\n\nnb_clf = CategoricalNB()\n\nnb_clf, nb_report = report(nb_clf, x_train_nb, y_train,\n                           x_test_nb, y_test, refit=True,\n                           confusion_labels=confusion_lbs)","ea295b61":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit_cv = LogisticRegressionCV(class_weight='balanced', cv=5, max_iter=500,\n                                scoring='f1', penalty='l1', solver='liblinear',\n                                n_jobs=-1, random_state=0, refit=True, verbose=0)\n\nlogit_cv, logit_report = report(logit_cv, x_train, y_train,\n                                x_test, y_test, refit=True,\n                                importance_plot=True,\n                                feature_labels=feature_names,\n                                confusion_labels=confusion_lbs)","679ef107":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=91, p=1,\n                           weights='uniform', n_jobs=-1)\n\nknn, knn_report = report(knn, x_train, y_train,\n                         x_test, y_test,\n                         importance_plot=True,\n                         feature_labels=feature_names,\n                         confusion_labels=confusion_lbs)","2d1ccd61":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced',\n                                       criterion='entropy',\n                                       max_depth=3,\n                                       random_state=0)\n\ndecision_tree, decision_tree_report = report(decision_tree, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","a597465b":"from sklearn.ensemble import BaggingClassifier\n\nbagging_dtree = DecisionTreeClassifier(max_depth=2, class_weight='balanced',\n                                       criterion='entropy', random_state=0)\n\nbagging_clf = BaggingClassifier(base_estimator=bagging_dtree,\n                                max_samples=110, n_estimators=80,\n                                max_features=15, n_jobs=-1,\n                                random_state=0)\n\nbagging_clf, bagging_clf_report = report(bagging_clf, x_train, y_train,\n                                         x_test, y_test,\n                                         feature_labels=feature_names,\n                                         confusion_labels=confusion_lbs)","89ecc93d":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                                       max_depth=1, n_estimators=100,\n                                       n_jobs=-1, random_state=0)\n\nrandom_forest, random_forest_report = report(random_forest, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","3a9c013b":"from sklearn.ensemble import AdaBoostClassifier\n\nboosting_dtree = DecisionTreeClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_depth=1, random_state=0)\nadaboot = AdaBoostClassifier(base_estimator=boosting_dtree,\n                             n_estimators=285, learning_rate=0.1,\n                             random_state=0)\n\nadaboot, adaboot_report = report(adaboot, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","85ab6e3f":"from sklearn.svm import SVC\n\nlinear_svc = SVC(kernel='linear', probability=True,\n                 class_weight='balanced', random_state=0)\n\nlinear_svc, linear_svc_report = report(linear_svc, x_train, y_train,\n                                       x_test, y_test,\n                                       importance_plot=True,\n                                       feature_labels=feature_names,\n                                       confusion_labels=confusion_lbs)","1b574e8d":"rbf_svc = SVC(C=0.3, kernel='rbf', probability=True,\n              class_weight='balanced', random_state=0)\n\nrbf_svc, rbf_svc_report = report(rbf_svc, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","5a9a805d":"from xgboost import XGBClassifier\nfrom sklearn.utils import class_weight\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) \/ np.sum(y_train)\n\nxgb_clf = XGBClassifier(learning_rate=0.01, random_state=0,\n                        scale_pos_weight=cls_weight, n_jobs=-1)\nxgb_clf.fit(x_train, y_train);\n\nxgb_clf, xgb_report = report(xgb_clf, x_train, y_train,\n                             x_test, y_test,\n                             importance_plot=True,\n                             feature_labels=feature_names,\n                             confusion_labels=confusion_lbs)","175c6f86":"from catboost import CatBoostClassifier\n\n# Basic working\n\ncatboost_clf = CatBoostClassifier(cat_features=categorical_columns,\n                                  l2_leaf_reg=120, depth=6,\n                                  auto_class_weights='Balanced',\n                                  iterations=200, learning_rate=0.16,\n                                  use_best_model=True,\n                                  early_stopping_rounds=150,\n                                  eval_metric='F1', random_state=0)\n\ncatboost_clf.fit(x_train_cat, y_train, \n                 eval_set=(x_train_cat, y_train),\n                 verbose=False)\n\n\nf_labels = categorical_columns+numeric_columns\ncatboost_clf, catboost_report = report(catboost_clf, x_train_cat, y_train,\n                                       x_test_cat, y_test,\n                                       importance_plot=True,\n                                       feature_labels=f_labels,\n                                       confusion_labels=confusion_lbs)","3b0762f8":"report_list = [nb_report, logit_report, knn_report, decision_tree_report,               \n               bagging_clf_report, random_forest_report, adaboot_report,\n               xgb_report, linear_svc_report, rbf_svc_report, catboost_report]\nclf_labels = [rep['clf'].__class__.__name__ for rep in report_list]\nclf_labels[-3], clf_labels[-2] = 'Linear SVC', 'RBF SVC'","195328ff":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list, labels=clf_labels)\n\ncompare_table.sort_values(by=['Overfitting'])","f75ab87b":"compare_plot","56e60746":"df_charges = list()\nnew_cols = ['RRM\/customer', 'RRM %', 'RRT %']\n\nfor rep in report_list:\n    y_pred = rep['test_predictions']\n    true_positives = (y_test * y_pred).reshape(y_test.shape[0], 1)\n    tp_revenue = (x_test_charges * true_positives).sum(axis=0)\n    tp_revenue = np.insert(tp_revenue, 0, tp_revenue[0] \/ true_positives.sum())\n    df_charges.append(tp_revenue)\n\nrevenue_saved = pd.DataFrame(df_charges, index=clf_labels, \n                             columns=new_cols)\n\ncompare_table_rev = pd.concat([compare_table, revenue_saved], axis=1)\n\n## True Positive Revenue\/ Total Churn Revenue\ntotal_churn_revenue = (x_test_charges * y_test.reshape(y_test.shape[0], 1)).sum(axis=0)\ntemp_cols = (compare_table_rev.iloc[:, -2:] \/ total_churn_revenue) * 100\ncompare_table_rev.iloc[:, -2:] = temp_cols","e64c39e5":"compare_table_rev.sort_values(by=['Overfitting'])","8e713901":"select_cols = ['Overfitting', 'F1-score'] + new_cols\ncompare_table_rev[select_cols].sort_values(by=['Overfitting', 'RRM %'],\n                                           ascending=[True, False])","068057c8":"compare_table_rev[select_cols].sort_values(by=['Overfitting', 'RRM\/customer'],\n                                           ascending=[True, False])","90e26c7b":"<a id=\"logistic-regression\"><\/a>\n## 5.3.  Logistic Regression","ee39eb41":"<a id=\"delete-%60customerid%60-column\"><\/a>\n## 1.1.  Delete `customerid` column\nSince 'customerid' column does not provide any relevant information in predicting the customer churn, we can delete the column.","a90d57b3":"**Highlights:**\n> 1. Correlation analysis using Pearson coefficient for continuous features and Cramer's V for categorical features\n>\n> 2. Data visualization using Seaborn FacetGrid plots\n>\n> 3. Classification of the imbalanced dataset using `class weighted` or `cost sensitive` learning\n>\n> 4. Classifier performance analysis using ***`revenue weighted recall score`*** and ***`revenue retained per customer`*** metrics\n>\n> 5. Rationale for choosing Logistic Regression as an optimal classifier for the churn prediction problem\n>\n> 6. Results for each ML algorithm are presented after performing 5-fold cross validation based on F1-score\n>\nFor a similar analysis using SMOTE data see [here](https:\/\/www.kaggle.com\/para24\/telecom-churn-analysis-and-prediction-smote-data).","393f5be8":"<a id=\"cast-%60totalcharges%60-column-to-%60float%60\"><\/a>\n#### 1.2.3.1.  Cast `TotalCharges` column to `float`","d6ab18af":"<a id=\"churn-among-contract-and-internet-service\"><\/a>\n## 3.11.  Churn among contract and internet service","184a79fa":"<a id=\"churn-among-phone-service%2C-internet-service%2C-and-seniorcitizen\"><\/a>\n## 3.8.  Churn among phone service, internet service, and seniorcitizen","7cf748ff":"<a id=\"further-analysis\"><\/a>\n# 7.  Further Analysis\nWe defined a additional evaluation metrics called `Percentage Monthly Revenue Retained` which is defined as,\n  \n\\begin{align}\n\\text {Revenue Retained (Monthly)} \\%=\\frac{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times y_{\\text {pred}}^{(i)} \\times \\text {monthlycharges}^{(i)}}{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times \\text {monthlycharges}(i)} \\times 100\n\\end{align}","2052f5a3":"<a id=\"data-modeling\"><\/a>\n# 5.  Data Modeling\nSince the dataset is imbalanced we will be using class-weighted\/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","8da7ce2d":"<a id=\"data-visualization\"><\/a>\n# 3.  Data Visualization","49d19ebf":"<a id=\"contribution-to-churned-revenue\"><\/a>\n## 3.4.  Contribution to churned revenue\nFollowing graph represents percent contributed by customers in each tenure bin to the lost\/ churned revenue.","694179b7":"<a id=\"frequency-distribution%3A-categorical-variables\"><\/a>\n## 3.1.  Frequency distribution: categorical variables","6c62380f":"***Inference:*** Within the churned customers, 50% of the churned revenue is contributed by short tenure customers. This also means that the monthly charges of short tenure customers are more than high tenure customers. Therefore, if the ML models are successful in identifying the low tenure customers, then a major chunk of the revenue can be retained.","dbbcda8f":"<a id=\"making-labels-concise\"><\/a>\n### 1.2.2.  Making labels concise\nLet's make the categorical labels more concise. For instance, we will convert the categorical label `'Bank transfer (automatic)'` to `'Bank transfer'` to make it easier to access (and display) during visualization.","9d4c98d3":"<a id=\"train-test-split\"><\/a>\n## 4.1.  Train-Test split\nCatBoost classifier does not require any knd of preprocessing while Naive bayes requires a different kind of preprocesing. Therefore, we will use raw\/ unmodified data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) for CatBoost and preprocessed data (`x_train, x_test, y_train, y_test`) for all other classifiers. For Naive Bayes, we will use the raw data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) and preprocess it as required in the Naive Bayes section.","1bfdc4bf":"<a id=\"naive-bayes\"><\/a>\n## 5.2.  Naive Bayes\nThe fundamental assumption made by Naive Bayes regarding the data is ***class conditional independence of features***. Sklearn provides different variants of Naive Bayes depending on whether the features follow a categorical distribution (CategoricalNB), normal distribution (GaussianNB), bernoulli distribution (BernoulliNB), multinomial distribution (MultinomialNB).\n\nSince majority of the features are categorical and follow a categorical distribution, we will use CategoricalNB. Continuous features will be discretized.","62d72ec7":"<a id=\"checking-for-null-values-in-the-dataset\"><\/a>\n### 1.2.1.  Checking for null values in the dataset","01fe82f1":"<a id=\"churn-among-phone-service%2C-multiple-lines%2C-and-internet-service\"><\/a>\n## 3.7.  Churn among phone service, multiple lines, and internet service\nModifying seaborn countplot make it work with FacetGrid when all 3 arguments (`hue`, `row`, and `col`) are used.","5d222915":"<a id=\"correlation-between-qualitative\/-categorical-variables\"><\/a>\n## 2.2.  Correlation between Qualitative\/ Categorical variables\n`Cramer's V` is more appropriate than Pearson correlation to find correlation between two nominal variables. Here, the `Cramer's V` metric is implemented.","c009c15f":"<a id=\"decision-tree\"><\/a>\n## 5.5.  Decision Tree","d83b568a":"<a id=\"correlation-between-quantitative-variables\"><\/a>\n## 2.1.  Correlation between Quantitative variables","fd0268c9":"<a id=\"one-hot-encoding-and-standardization\"><\/a>\n## 4.2.  One-hot Encoding and Standardization\nWe need to standardize the continuous or quantitative variables\/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. <br\/>\nThere is no need to standardize categorical variables.\n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#columntransformer-for-heterogeneous-data)***","5c12363f":"<a id=\"data-munging\"><\/a>\n## 1.2.  Data Munging","c1e52eb6":"<a id=\"churn-among-different-contract-types\"><\/a>\n## 3.5.  Churn among different contract types","3a28a650":"<a id=\"model-comparison\"><\/a>\n# 6.  Model Comparison\nSince input data format for Naive Bayes and CatBoost are different, we will add them to the comparison manually.","04fef274":"<a id=\"correlations-in-the-data\"><\/a>\n# 2.  Correlations in the data","92f76ceb":"<a id=\"decision-trees-with-adaboost\"><\/a>\n## 5.8.  Decision Trees with AdaBoost\nThe default base estimator for `AdaBoostClassifier` is `DecisionTreeClassifier(max_depth=1)`","c1fec14e":"<a id=\"decision-trees-with-bagging\"><\/a>\n## 5.6.  Decision Trees with Bagging","77c37975":"# Telecom Churn Analysis and Prediction using Cost Sensitive Learning","e316a82f":"<a id=\"which-model-to-choose%3F\"><\/a>\n# 8.  Which model to choose?\nLets consider only the non-overfitting classifiers.\n\nSince Logistic Regression has the best F1-score, it has the best overall performance and hence gives the right tradeoff between precision and recall. ***It can also be seen that Logistic Regression retains a significant portion of the monthly revenue in spite of having lower recall than linear SVC and Random Forests. This shows that most of the customers identified by Logistic Regression have higher monthly expenses.*** In other words, even though Logistic Regression was able to correctly identify fewer churn customers, the churn customers it has identified have higher monthly charges, hence giving Logistic Regression a `RRM %` that is close to Random Forests and linear SVC. Logistic Regression's ability to identify high value customers is also indicated by a high `RRM\/customer` value.\n\nThe reason for the classifiers not achieving higher accuracy scores (like > 95%) is because the churn and no-churn classes are overlapping making it difficult for the classifiers to identify a perfect decision boundary without sacrificing either precision or recall. This can be seen from the lmplots in the correlation section.\n\nIf the telecom company wants to choose a classifier to deploy in the real world, the company will have to make a tradeoff between the amount of revenue it wants to retain and the amount of revenue it is willing to spend on the customer retention programs. This tradeoff is a result of the tradeoff between precision and recall. To retain more revenue, the company needs to choose a classifier that has higher recall and lower precision like Random Forests. However, this will result in higher false positives making the company include no-churn customers as well in the retention programs thereby increasing its spending.\n\nHowever, if the company chooses to retain a decent amount of the revenue while not spending too much on the retention programs, it will have to choose Logistic Regression because it has a higher F1-score and `RRM\/customer` while having `RRM %` that is close to Random Forests.","b2eda042":"<a id=\"random-forests\"><\/a>\n## 5.7.  Random Forests","49f55051":"Similarly, `Revenue Retained (Monthly) per true churn customer` is defined as,\n\n\\begin{align}\n\\text {Revenue Retained (Monthly) per churn customer }=\\frac{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times y_{\\text {pred}}^{(i)} \\times \\text {monthlycharges}^{(i)}}{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times y_{\\text {pred}}^{(i)}} \\times 100\n\\end{align}  \n  \nThis metric indicates how well a classifier is able to identify Most Valued Customers (MVC) or high spending customers. A high value indicates that a classifier was good at identifying more MVCs than a classifier with a lower value of this metric.  \n  \n***Note:*** In the following pandas frames, the following are the abbreviations:  \n> RRM %  => Revenue Retained (Monthly) %  \n> RRT % => Revenue Retained (Total) %  \n> RRM\/customer => Revenue Retained (Monthly) per true churn customer","b5c46531":"<a id=\"catboost\"><\/a>\n## 5.12.  CatBoost\nCat boost performs better without One-hot encoding because it performs an internal categorical encoding that is similar to Leave One Out Encoding (LOOE). So, we can give the dataframe as input to the catboost classifier.","52fd880d":"<a id=\"frequency-distribution%3A-monthlycharges-and-tenure\"><\/a>\n## 3.2.  Frequency distribution: monthlycharges and tenure","25b2f54d":"<a id=\"churn-among-phone-service%2C-internet-service%2C-and-gender\"><\/a>\n## 3.9.  Churn among phone service, internet service, and gender","a5e9f0f1":"<a id=\"utility-functions\"><\/a>\n## 5.1.  Utility Functions","6967202c":"Let's also change column labels from `TitleCase` to `lowercase` to ease access.","a019eaf2":"It can also be noted that the `Tenure` column is 0 for these entries even though the `monthlycharges` column is not empty. Let's see if there are any other 0 values in the `tenure` column.","c29ac55c":"<a id=\"data-preprocessing\"><\/a>\n# 4.  Data Preprocessing\nData needs to be one-hot-encoded before applying machine learning models.","d19362cb":"***Inference:*** We can say that majority of the churned customers are not senior citizens, have dependents, are subscribed to phone service, Fiber internet, and paperless billing, are not subscribed to online security and tech support, and have monthly contracts.","e0e25832":"<a id=\"box-plots\"><\/a>\n## 3.3.  Box Plots","1ff5112e":"***Inference:*** As evident from the correlation matrix and regplots, since ***'totalcharges'*** is the total monthly charges over the tenure of a customer, ***'totalcharges'*** is highly correlated with ***'monthlycharges'*** and ***'tenure'***.","eacf9d3c":"<a id=\"column-type-casting-and-imputation\"><\/a>\n### 1.2.3.  Column Type Casting and Imputation\nPandas couldn't properly cast the data type of several columns. For instance, the `TotalCharges` column is recognized as `object` instead of `float`. Similarly, all the categorical columns were casted as `object` type instead of `pd.Categorical`.","f316bcaf":"***Inference:*** We can see that among the classifiers that do not overfit, Random Forests classifier has the highest recall while Logistic Regression has the highest F1-score. In terms of Revenue Retained, Random Forests are the best. However, Random Forests suffer from low precision.","c8bf26a4":"<a id=\"xgboost\"><\/a>\n## 5.11.  XGBoost","f3cef21f":"Here we see that the `totalcharges` has 11 missing values. Let's see the complete data corresponding to these customers.","db4a55b9":"* [1.  Import and Clean data](#import-and-clean-data)\n    * [1.1.  Delete `customerid` column](#delete-%60customerid%60-column)\n    * [1.2.  Data Munging](#data-munging)\n        * [1.2.1.  Checking for null values in the dataset](#checking-for-null-values-in-the-dataset)\n        * [1.2.2.  Making labels concise](#making-labels-concise)\n        * [1.2.3.  Column Type Casting and Imputation](#column-type-casting-and-imputation)\n            * [1.2.3.1.  Cast `TotalCharges` column to `float`](#cast-%60totalcharges%60-column-to-%60float%60)\n            * [1.2.3.2.  Search for categorical columns and cast them to `pd.Categorical`](#search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60)\n        * [1.2.4.  Reordering Columns](#reordering-columns)\n* [2.  Correlations in the data](#correlations-in-the-data)\n    * [2.1.  Correlation between Quantitative variables](#correlation-between-quantitative-variables)\n    * [2.2.  Correlation between Qualitative\/ Categorical variables](#correlation-between-qualitative\/-categorical-variables)\n* [3.  Data Visualization](#data-visualization)\n    * [3.1.  Frequency distribution: categorical variables](#frequency-distribution%3A-categorical-variables)\n    * [3.2.  Frequency distribution: monthlycharges and tenure](#frequency-distribution%3A-monthlycharges-and-tenure)\n    * [3.3.  Box Plots](#box-plots)\n    * [3.4.  Contribution to churned revenue](#contribution-to-churned-revenue)\n    * [3.5.  Churn among different contract types](#churn-among-different-contract-types)\n    * [3.6.  Churn among various categories and their contribution to (monthly) revenue](#churn-among-various-categories-and-their-contribution-to-%28monthly%29-revenue)\n    * [3.7.  Churn among phone service, multiple lines, and internet service](#churn-among-phone-service%2C-multiple-lines%2C-and-internet-service)\n    * [3.8.  Churn among phone service, internet service, and seniorcitizen](#churn-among-phone-service%2C-internet-service%2C-and-seniorcitizen)\n    * [3.9.  Churn among phone service, internet service, and gender](#churn-among-phone-service%2C-internet-service%2C-and-gender)\n    * [3.10.  Churn among internet service, streamingtv, and streamingmovies](#churn-among-internet-service%2C-streamingtv%2C-and-streamingmovies)\n    * [3.11.  Churn among contract and internet service](#churn-among-contract-and-internet-service)\n* [4.  Data Preprocessing](#data-preprocessing)\n    * [4.1.  Train-Test split](#train-test-split)\n    * [4.2.  One-hot Encoding and Standardization](#one-hot-encoding-and-standardization)\n* [5.  Data Modeling](#data-modeling)\n    * [5.1.  Utility Functions](#utility-functions)\n    * [5.2.  Naive Bayes](#naive-bayes)\n    * [5.3.  Logistic Regression](#logistic-regression)\n    * [5.4.  K-Nearest Neighbors](#k-nearest-neighbors)\n    * [5.5.  Decision Tree](#decision-tree)\n    * [5.6.  Decision Trees with Bagging](#decision-trees-with-bagging)\n    * [5.7.  Random Forests](#random-forests)\n    * [5.8.  Decision Trees with AdaBoost](#decision-trees-with-adaboost)\n    * [5.9.  Linear SVC](#linear-svc)\n    * [5.10.  SVM with RBF kernel](#svm-with-rbf-kernel)\n    * [5.11.  XGBoost](#xgboost)\n    * [5.12.  CatBoost](#catboost)\n* [6.  Model Comparison](#model-comparison)\n    * [6.1.  Evaluation Metrics](#evaluation-metrics)\n    * [6.2.  ROC and PR Curves](#roc-and-pr-curves)\n* [7.  Further Analysis](#further-analysis)\n* [8.  Which model to choose?](#which-model-to-choose%3F)","c6d5cf13":"<a id=\"search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60\"><\/a>\n#### 1.2.3.2.  Search for categorical columns and cast them to `pd.Categorical`\nWe need to manually identify categorical columns in the data before casting them to `pd.Categorical`. Casting categorical columns from the detected *object* type to *categorical* will ease visualization.","f0e8c1aa":"<a id=\"churn-among-various-categories-and-their-contribution-to-%28monthly%29-revenue\"><\/a>\n## 3.6.  Churn among various categories and their contribution to (monthly) revenue","6a1b2a87":"<a id=\"roc-and-pr-curves\"><\/a>\n## 6.2. ROC and PR Curves","785a9a26":"<a id=\"churn-among-internet-service%2C-streamingtv%2C-and-streamingmovies\"><\/a>\n## 3.10.  Churn among internet service, streamingtv, and streamingmovies","3cac9e89":"<a id=\"reordering-columns\"><\/a>\n### 1.2.4.  Reordering Columns","03985545":"**Thank You!!**","4de6fdaf":"<a id=\"svm-with-rbf-kernel\"><\/a>\n## 5.10.  SVM with RBF kernel","dec0a2d6":"As of now we don't see any null values. However, we will find a few in the `TotalCharges` column after casting it to `float64`","d6622bb0":"<a id=\"linear-svc\"><\/a>\n## 5.9.  Linear SVC","46d33b38":"<a id=\"k-nearest-neighbors\"><\/a>\n## 5.4.  K-Nearest Neighbors\nKNN estimator in Scikit-learn does not provide a way to pass class-weights to enable cost-sensitive\/ class-weighted learning.","494710c2":"***Inference:*** The dataset is imbalanced as can be seen from the Churn plot (bottom right corner).","7c48157c":"<a id=\"evaluation-metrics\"><\/a>\n## 6.1.  Evaluation Metrics","4d30e8a5":"and `Percentage Total Revenue Retained` (over the complete tenure of the customer),\n\n\\begin{align}\n\\text {Revenue Retained (Total) } \\%=\\frac{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times y_{\\text {pred}}^{(i)} \\times \\text {totalcharges}^{(i)}}{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times \\text {totalcharges}^{(i)}} \\times 100\n\\end{align}  \n  \nThese are \u201crevenue\u201d weighted recall scores and can be viewed as a business equivalent of Recall. They represent the revenue retained (or saved) by a model as a result of its correct churn predictions i.e., True Positives.","660fd774":"***Inference:*** We can say that most churn customers have monthly charges of around 70 to 110 and a tenure of around 0 to 20 months.","d051565a":"<a id=\"import-and-clean-data\"><\/a>\n# 1.  Import and Clean data","97024505":"There are no additional missing values in the `Tenure` column. Let's delete the rows with missing values in `monthlycharges` and `tenure` columns.","5e156937":"***Inference:*** There is some correlation between *'phone service'* and *'multiple lines'* since those who don't have a phone service cannot have multiple lines. So, knowing that a particular customer is not subscribed to phone service we can infer that the customer doesn't have multiple lines. Similarly, there is also a correlation between *'internet service'* and *'online security', 'online backup', 'device protection', 'streaming tv'* and *'streaming movies'*"}}