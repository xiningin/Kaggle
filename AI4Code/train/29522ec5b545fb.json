{"cell_type":{"7c1dc4e0":"code","46d905fa":"code","6046850e":"code","9f4fba1e":"code","6e1bc3ee":"code","cc6e56da":"code","163f4055":"code","c1903a55":"code","592eeada":"code","f6ccec7f":"code","6de4b5ca":"code","1cf19ccb":"code","673ee7af":"code","c8adf847":"code","366e7562":"code","0f4f2373":"code","414b9200":"code","cf58e758":"code","60ce0b8f":"code","f38d70fd":"code","a5f157ef":"code","ee137d64":"code","81d03b92":"code","2f56ee80":"code","375c9e74":"code","959a15ab":"code","16f68ed3":"code","c99affa2":"code","10a0f53a":"code","5cc29fa9":"code","828b3770":"code","1bd41df7":"code","b4856c74":"code","c1764817":"code","43e86a2b":"code","cbd96603":"code","47476585":"code","7a114475":"code","b5c337c6":"code","2add11ec":"code","f126abdd":"code","81099752":"code","aa1f8afb":"code","951e2b07":"code","9e7c22b8":"code","dce15945":"code","a4c091b6":"markdown","e8592f2b":"markdown","2778eb01":"markdown"},"source":{"7c1dc4e0":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nimport requests\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","46d905fa":"#Import dataset\ndataset = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding = \"latin-1\")","6046850e":"dataset.head()","9f4fba1e":"# Drop unnecessary columns\ndataset = dataset.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])","6e1bc3ee":"dataset.head()","cc6e56da":"# Rename our 2 columns to make them more readable and meaningfull\ndataset = dataset.rename(columns = {\"v1\" : \"target\", \"v2\" : \"sms\"})","163f4055":"dataset.head()","c1903a55":"dataset[\"length\"] = dataset[\"sms\"].str.len()","592eeada":"dataset.head()","f6ccec7f":"# How many spams and how many hams?\nplt.figure(figsize=(10,5))\nsns.countplot(data = dataset, x=\"target\")\nprint(dataset[\"target\"].value_counts())","6de4b5ca":"# Spam mails tend to have more lengthy messages!\nplt.figure(figsize=(15,7))\nplt.xlim(0,200)\nsns.distplot(dataset.loc[dataset[\"target\"] == \"ham\"][\"length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100)\nsns.distplot(dataset.loc[dataset[\"target\"] == \"spam\"][\"length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100)","1cf19ccb":"#Create copy dataset to manipulate\nmanip_dataset = dataset.copy()","673ee7af":"manip_dataset.head()","c8adf847":"import re\nimport nltk\nnltk.download('stopwords') #download non relevant words\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer # Stemming is taking the root of every word (containing what it means)\nsms = [] # will contain all the different sms cleaned\nfor i in range(0, len(dataset)):\n    string = re.sub(\"[^a-zA-Z]\", \" \", manip_dataset[\"sms\"][i]) # replaces anything NOT in a-z or A-Z by a space, in the variable \n    string = string.lower()\n    string = string.split()\n    stemmer = SnowballStemmer(\"english\")\n    all_stopwords = stopwords.words(\"english\")\n    #if the word is not in the stopwords vocabulary then go ahead with the word iter and stem it\n    string = [stemmer.stem(word) for word in string if not word in set(all_stopwords)]\n    string = ' '.join(string) # joins the words again with a space in between them\n    sms.append(string) # add the cleaned sms to our sms list\n","366e7562":"# See the first 5 stemmed messages\nsms[:5]","0f4f2373":"# Iterate through the list and replace the old texts with the new cleaned texts\nfor i in range(0,len(sms)):\n    manip_dataset[\"sms\"][i] = sms[i] ","414b9200":"manip_dataset.head()","cf58e758":"# Creating feature for the length of the \"cleaned\" messages\nmanip_dataset[\"after_length\"] = manip_dataset[\"sms\"].str.len()","60ce0b8f":"manip_dataset.head()","f38d70fd":"# Length distributions more discrete in initial length, so i will not use the after_length attr\nfig, ax =plt.subplots(1,2,figsize=(25,5))\nax[0].set_xlim([0, 200])\nax[1].set_xlim([0, 200])\n\n\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"ham\"][\"after_length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100, ax = ax[0])\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"spam\"][\"after_length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100, ax = ax[0])\n\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"ham\"][\"length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100, ax = ax[1])\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"spam\"][\"length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100, ax = ax[1])","a5f157ef":"manip_dataset = manip_dataset.drop(columns = [\"after_length\"])","ee137d64":"manip_dataset.head()","81d03b92":"# Reindexing our columns\nmanip_dataset = manip_dataset.reindex(columns = [\"sms\", \"length\", \"target\"])","2f56ee80":"manip_dataset.head()","375c9e74":"# Encoding the target values\ntarget_encoder = LabelEncoder()\nmanip_dataset[\"target\"] = target_encoder.fit_transform(manip_dataset[\"target\"])\nmanip_dataset.head()","959a15ab":"# Taking the spam stemmed words to put them in a word cloud\nspam_words = \"\"\nfor val in manip_dataset.loc[manip_dataset[\"target\"] == 1][\"sms\"]: \n    val = str(val)\n    tokens = val.split()\n    spam_words += \" \".join(tokens)+\" \"","16f68ed3":"# Taking the spam stemmed words to put them in a word cloud\nham_words = \"\"\nfor val in manip_dataset.loc[manip_dataset[\"target\"] == 0][\"sms\"]: \n    val = str(val)\n    tokens = val.split()\n    ham_words += \" \".join(tokens)+\" \"","c99affa2":"# Downloading the pic to use and defining our spam_word cloud\npic = np.array(Image.open(requests.get('http:\/\/www.clker.com\/cliparts\/O\/i\/x\/Y\/q\/P\/yellow-house-hi.png',stream=True).raw))\nspam_wordcloud = WordCloud(width = 800, height = 800,\n                      background_color ='white', mask = pic, \n                      min_font_size = 10).generate(spam_words)","10a0f53a":"# Defining our ham_word cloud\nham_wordcloud = WordCloud(width = 800, height = 800,\n                      background_color ='white', mask = pic, \n                      min_font_size = 10).generate(ham_words)","5cc29fa9":"# Displaying the word cloud of most frequent stemmed spam sms messages words.\nplt.figure(figsize = (8, 8), facecolor = 'white', edgecolor='blue') \nplt.imshow(spam_wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","828b3770":"# Displaying the word cloud of most frequent stemmed ham sms messages words.\nplt.figure(figsize = (8, 8), facecolor = 'white', edgecolor='blue') \nplt.imshow(ham_wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","1bd41df7":"X = manip_dataset.drop(columns = [\"target\"])\ny = manip_dataset[\"target\"]","b4856c74":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 4001) # the one at the end is because i'll use the last one for the length feature","c1764817":"# Create our vectors for our bag-of-words model\nX_sms = cv.fit_transform(sms).toarray()","43e86a2b":"# Assign the last value of each vector to the length feature\nfor i in range(0,len(X_sms)):\n    X_sms[i][-1] = X[\"length\"][i]","cbd96603":"# Display first five vectors\nX_sms[:5]","47476585":"# Split our data to train\/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sms, y, test_size=0.20, random_state=42)","7a114475":"# Function to print metrics\ndef print_metrics(y_test,y_pred):\n    print(\"The confusion matrix is : \\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n    print(\"The accuracy score is : \\n\",accuracy_score(y_test, y_pred), \"\\n\")\n    print(\"The precision is : \\n\",precision_score(y_test,y_pred), \"\\n\")\n    print(\"The recall is : \\n\",recall_score(y_test,y_pred), \"\\n\")\n    print(\"The f1 score is : \\n\",f1_score(y_test,y_pred), \"\\n\")","b5c337c6":"# Create function running models and printing scores\ndef run_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print_metrics(y_test,y_pred)","2add11ec":"# Create function running randomized search on model given in the parameters\ndef run_grid_model(model, grid, X_train, y_train, X_test, y_test):\n    rf_random = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, y_train)\n    print(f\"Best params of the randomized CV-2 model is : {rf_random.best_params_} \\n-----------------------------------\")\n    y_pred = rf_random.predict(X_test)\n    print_metrics(y_test,y_pred)","f126abdd":"from sklearn.naive_bayes import GaussianNB\nrun_model(GaussianNB(),X_train, y_train, X_test, y_test)","81099752":"from sklearn.ensemble import AdaBoostClassifier\nrun_model(AdaBoostClassifier(random_state = 42),X_train, y_train, X_test, y_test)","aa1f8afb":"from sklearn.ensemble import GradientBoostingClassifier\nrun_model(GradientBoostingClassifier(random_state = 42),X_train, y_train, X_test, y_test)","951e2b07":"from sklearn.ensemble import RandomForestClassifier\nrun_model(RandomForestClassifier(random_state = 42),X_train, y_train, X_test, y_test)","9e7c22b8":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(200, 2000, num = 7)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [int(x) for x in np.linspace(5, 100, num= 7)],\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}","dce15945":"# Perform RandomizedSearch on our random forest classifier\nrun_grid_model(RandomForestClassifier(random_state = 42),random_grid, X_train, y_train, X_test, y_test)","a4c091b6":"#### Since i'm new and haven't done a word cloud before, I found this implementation to apply here. If you want you can check it out!\nhttps:\/\/medium.com\/@harinisureshla\/wordclouds-basics-of-nlp-5b60be226414","e8592f2b":"#### After RandomizedSearch on our RandomForestClassifier which performed better with default parameters, we managed to reach 98.2% accuracy and increase the recall (86.6%). Precision stayed the same at 100%","2778eb01":"#### As we can see by the above metrics our best model, RandomForest Classifier, detected spam sms messages with accuracy 97,9%.\n#### Also we can see that all the spams we detected, were indeed ACTUALLY spams (precision = 100%) but we missed approximately 15% (recall = 84.66%) of all the spam messages."}}