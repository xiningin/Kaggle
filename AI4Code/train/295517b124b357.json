{"cell_type":{"d50f958d":"code","b04469b7":"code","281d48eb":"code","9decfe1f":"code","06037447":"code","9df138e9":"code","c2547877":"code","374cfbb2":"code","5659e1b9":"code","96b6bc07":"code","48e10e27":"code","09506ccc":"code","23d98942":"code","242f8cb9":"code","3a80255a":"code","0937d8c1":"code","37cb9024":"code","145a0698":"code","2425322a":"code","1865bb7c":"code","f8aa2890":"code","13015306":"code","551b09cc":"code","714efa38":"code","148fc00a":"code","988bec42":"code","8c720391":"code","f79f59bb":"code","bebe5ddf":"code","a3c8bd16":"code","b4a7fb24":"code","e77417b1":"code","b397c0bb":"code","ba6080c7":"code","e7e67a0f":"code","aeb963ae":"code","2fd14301":"code","f0d08a4b":"code","8c23f60b":"code","06511bd5":"code","9615ceed":"code","fe9a1ff1":"markdown","d77fdf20":"markdown","131dfea5":"markdown","2e94b0a5":"markdown","5a0ea806":"markdown","f9bd7a66":"markdown","3f9eeeee":"markdown","945c44fc":"markdown","b0b3f09b":"markdown","d65acc66":"markdown","23577731":"markdown","aee16c65":"markdown","22a22196":"markdown","f13b87fb":"markdown","6fabad1a":"markdown","10c9f243":"markdown","f2646fc9":"markdown","3cd388b0":"markdown","fd5f23f3":"markdown","88e597b5":"markdown","05eab25f":"markdown","1198234a":"markdown","d0323e73":"markdown","41305277":"markdown","7e0a1312":"markdown","7bbc19be":"markdown","c0a3f201":"markdown"},"source":{"d50f958d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b04469b7":"#!pip install keras\n#!pip install --upgrade tensorflow\n#!pip install --upgrade tensorflow-gpu\n!pip install xlrd==1.2.0\n!pip install openpyxl","281d48eb":"import pandas as pd\nfrom pandas import read_excel\nimport numpy as np\nimport re\nfrom re import sub\nimport multiprocessing\nfrom unidecode import unidecode\nimport os\nfrom time import time \nfrom collections import Counter\n\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.python.keras.layers import LSTM,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling2D, Conv1D, MaxPooling1D\nfrom tensorflow.python.keras.models import Sequential\n\nfrom tensorflow.keras.optimizers import SGD,Adam\nfrom tensorflow.keras import regularizers\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n\nimport h5py\nimport csv\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\n\n\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams","9decfe1f":"df_train=pd.read_csv(\"\/kaggle\/input\/textcategorizationbangla\/train_data_stemmer.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/textcategorizationbangla\/test_data_stemmer.csv\")\n","06037447":"display(df_train)\ndisplay(df_test)","9df138e9":"print(\"First rows of TrainSet....\")\ndf_train.head()","c2547877":"print(\"Description of TrainSet...\")\ndf_train.describe()","374cfbb2":"print(\"Dataframe Information on TrainSet...\")\ndf_train.info()","5659e1b9":"print(\"Categories of Trainset:\")\nprint(df_train.category.unique())\ncategory_list=df_train.category.unique()","96b6bc07":"print(\"First rows of TrainSet....\")\ndf_test.head()","48e10e27":"print(\"Description of TrainSe....\")\ndf_test.describe()","09506ccc":"print(\"Information of TrainSet dataframe....\")\ndf_test.info()","23d98942":"print(\"Categories:\")\nprint(df_train.category.unique())\ncategory_list=df_train.category.unique()","242f8cb9":"df_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)","3a80255a":"#counting text length\ndf_train['count'] = df_train['cleanText'].str.split().str.len()\ndf_test['count'] = df_test['cleanText'].str.split().str.len()\n# Remove the text with words less than 5\ndf_train= df_train.loc[df_train['count']>5]\ndf_test= df_test.loc[df_test['count']>5]","0937d8c1":"def text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n                               \n                           \"]+\", flags=re.UNICODE)\n    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n    \n    text=emoji_pattern.sub(r'', text)\n    text=english_pattern.sub(r'', text)\n\n    return text\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b\u00a3|\u00a2|\u0007\u00d1+-*\/=EROero\u09f3\u09e6\u09e7\u09e8\u09e9\u09ea\u09eb\u09ec\u09ed\u09ee\u09ef012\u201334567\u202289\u0964!()-[]{};:'\"\u201c\\\u2019,<>.\/?@#$%^&*_~\u2018\u2014\u0965\u201d\u2030\ud83e\udd23\u26bd\ufe0f\u270c\ufffd\ufff0\u09f7\ufff0'''\n    \n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out","37cb9024":"#df_train['cleanText'] = df_train.cleanText.apply(lambda x: preprocessing(str(x)))\n#df_test['cleanText'] = df_test.cleanText.apply(lambda x: preprocessing(str(x)))","145a0698":"data1 =pd.read_excel('\/kaggle\/input\/bangla-stopwords\/stopwords_bangla.xlsx')\nstop = data1['words'].tolist()","2425322a":"display(data1)","1865bb7c":"def stopwordRemoval(text):    \n    x=str(text)\n    l=x.split()\n\n    stm=[elem for elem in l if elem not in stop]\n    \n    out=' '.join(stm)\n    \n    return str(out)","f8aa2890":"#df_train['cleanText'] = df_train.cleanText.apply(lambda x: stopwordRemoval(str(x)))\n#df_test['cleanText'] = df_test.cleanText.apply(lambda x: stopwordRemoval(str(x)))","13015306":"#make sure to turn on internet on your kernel\n#importing stemmer\n!pip install bangla-stemmer\nfrom bangla_stemmer.stemmer import stemmer\n## stemmer function\ndef stem_text (x):\n  stmr = stemmer.BanglaStemmer()\n  words=x.split(' ')\n  stm = stmr.stem(words)\n  words=(' ').join(stm)\n  return words","551b09cc":"#df_train['cleanText']=df_train['cleanText'].apply(stem_text)\n#df_test['cleanText']=df_test['cleanText'].apply(stem_text)","714efa38":"sns.countplot(df_train['category']);","148fc00a":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.countplot(df_test['category']);","988bec42":"print(\"IN TRAIN SET...\")\ntemp1 = df_train.groupby('category').count()['cleanText'].reset_index().sort_values(by='cleanText',ascending=False)\ntemp1.style.background_gradient(cmap='Purples')","8c720391":"print(\"In Test Set...\")\ntemp2 = df_test.groupby('category').count()['cleanText'].reset_index().sort_values(by='cleanText',ascending=False)\ntemp2.style.background_gradient(cmap='Purples')","f79f59bb":"from plotly import graph_objs as go\nprint(\"On Train Set....\")\nfig = go.Figure(go.Funnelarea(\n    text =temp1.category,\n    values = temp1.cleanText,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Category Distribution on Train Set\"}\n    ))\nfig.show()","bebe5ddf":"print(\"On Test Set...\")\nfig = go.Figure(go.Funnelarea(\n    text =temp2.category,\n    values = temp2.cleanText,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Category Distribution on Test Set\"}\n    ))\nfig.show()","a3c8bd16":"category_list=category_list.tolist()\nfor i in category_list:\n    temp=df_train.loc[df_train['category'] == str(i)]\n    #display(temp)\n    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n    top = Counter([item for sublist in temp['temp_list'] for item in sublist])\n    temp = pd.DataFrame(top.most_common(20))\n    temp.columns = ['Common_words','count']\n    temp.style.background_gradient(cmap='Blues')\n    temp = temp.style.set_caption('Top 20 Words In '+ str(i)+\" Category\")\n    display(temp)\n    ","b4a7fb24":"\ndf_train['temp_list'] = df_train['cleanText'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df_train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","e77417b1":"\ndf_test['temp_list'] = df_test['cleanText'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df_test['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","b397c0bb":"fig = go.Figure(layout=dict(title=dict(text=\"Text Length Histogram of Trainset\")))\nfig.add_trace(go.Histogram(x=df_train['count']))\nfig.show()","ba6080c7":"fig = go.Figure(layout=dict(title=dict(text=\"Text Length Histogram of Testset\")))\nfig.add_trace(go.Histogram(x=df_test['count']))\nfig.show()","e7e67a0f":"df_train=pd.read_csv(\"\/kaggle\/input\/textcategorizationbangla\/train_data_stemmer.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/textcategorizationbangla\/test_data_stemmer.csv\")\n","aeb963ae":"# function to create top 20 n-grams\ndef get_ngrams(data,n):\n    all_words = []\n    for i in range(len(data)):\n        temp = data[\"cleanText\"][i].split()\n        for word in temp:\n            all_words.append(word)\n\n    tokenized = all_words\n    esBigrams = ngrams(tokenized, n)\n\n    esBigram_wordlist = nltk.FreqDist(esBigrams)\n    top20 = esBigram_wordlist.most_common(20)\n    top20 = dict(top20)\n    df_ngrams = pd.DataFrame(sorted(top20.items(), key=lambda x: x[1])[::-1])\n    df_ngrams.columns = ['Ngram','count']\n    return df_ngrams\n\n\n# function to visualize the top 20 n-grams\ndef show(train):\n    display(train.head(20))","2fd14301":"for i in category_list:\n    temp=df_train.loc[df_train['category'] == str(i)]\n    #display(temp)\n    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n    temp.reset_index(drop=True, inplace=True)\n    train_unigrams = get_ngrams(temp,1)\n    print(\"\\t\\t\\t====== Unigrams of \"+str(i)+\"======\")   \n    show(train_unigrams)","f0d08a4b":"for i in category_list:\n    temp=df_train.loc[df_train['category'] == str(i)]\n    #display(temp)\n    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n    temp.reset_index(drop=True, inplace=True)\n    train_bigrams = get_ngrams(temp,2)\n    print(\"\\t\\t\\t====== Bigrams of \"+str(i)+\" ======\")   \n    show(train_bigrams)","8c23f60b":"for i in category_list:\n    temp=df_train.loc[df_train['category'] == str(i)]\n    #display(temp)\n    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n    temp.reset_index(drop=True, inplace=True)\n    train_trigrams = get_ngrams(temp,3)\n    print(\"\\t\\t\\t====== Trigrams of \"+str(i)+\" ======\")   \n    show(train_trigrams)","06511bd5":"# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\nfrom wordcloud import WordCloud\nfrom textwrap import wrap\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager\n\n# Function for generating word clouds\ndef generate_wordcloud(data,title):\n  data = [tuple(x) for x in data.values]\n  wc = WordCloud(font_path=\"\/kaggle\/input\/siyam-rupali-text\/Siyamrupali.ttf\",width=1080, height=720, max_words=150,colormap=\"Dark2\").generate_from_frequencies(dict(data))\n  plt.figure(figsize=(10,8))\n  plt.imshow(wc, interpolation='bilinear')\n  plt.axis(\"off\")\n  plt.title('\\n'.join(wrap(\"Word Cloud of \"+title,60)),fontsize=13)\n  plt.show()","9615ceed":"\nfor i in category_list:\n    temp=df_train.loc[df_train['category'] == str(i)]\n    #display(temp)\n    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n    top = Counter([item for sublist in temp['temp_list'] for item in sublist])\n    temp = pd.DataFrame(top.most_common(500000))\n    temp.columns = ['Common_words','count']\n    generate_wordcloud(temp,str(i))\n    ","fe9a1ff1":"# **Removing Low Length Data**","d77fdf20":"# **Top 20 Words in TestSet Based on Count**","131dfea5":"In both train set and test set,the data have no imbalance problem. We can see that every class have sufficient amount of data.","2e94b0a5":"# **Count Based Top Words in Each Category on Train Set**","5a0ea806":"# **Loading Dataset**","f9bd7a66":"# **Conclusion Of EDA on banglaMCT7 Dataset**\n* **Both TrainSet and TrainSet are similar in size.**\n* **TraintSet and TestSet top word counts are similar,so we can say that TrainSet and TestSet are originated from same distribution of Data.**\n* **The dataset don't have any data imbalance problem,every category have sufficient amount of data.**\n* **Histogram analaysis shows that most of the text are of lenght 1-600.**\n* **Category wise Ngram shows that the category wised created ngrams are relavent to that category.**\n* **Category wise Word Cloud also shows result relavent to the category**","3f9eeeee":"# **Updating\/Installing Libraries**","945c44fc":"# **Stopwords Removal**","b0b3f09b":"From the histogram,we can say that abobut 97% of the text lenght are between 1-600. This information is valuable for model creation and text lenght selection.","d65acc66":"# **Preprocessing\/Cleaning**\nThough the dataset is said to have been cleaned,but i have applied it anyway to remove Unecessary charachters,Emojis,Punctuations with the help of Regex.","23577731":"# **Removing Null Values**","aee16c65":"# **Testset Description**","22a22196":"# **Count of Texts in Each Category**","f13b87fb":"# **Word Cloud Based On Category**","6fabad1a":"# **Top 20 Bigram Count Based On Category**","10c9f243":"# **Top 20 Trigram Count Based On Category**","f2646fc9":"# **Top 20 Unigram Count Based On Category**","3cd388b0":"# **TrainSet Description**","fd5f23f3":"# **Category Wise Data Distribution**","88e597b5":"# **Top 20 Words in TrainSet Based on Count**","05eab25f":"# **N gram Analysis**","1198234a":"# **Funnel Chart of Data Distribution**","d0323e73":"# **Stemming**","41305277":"# **Histogram Analysis Based on Text Length**","7e0a1312":"# **Importing Libraries**","7bbc19be":"I have used a Bangla Stemmer for the dataset. But through experimentation,i have seen than stemming doesn't have that much effect on our accuracy measures. So i have commented out the corresponding code.","c0a3f201":"This is a stopwords list containing about 700+ bangla stopwords. I have manually collected this stopwords."}}