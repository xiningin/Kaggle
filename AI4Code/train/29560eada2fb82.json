{"cell_type":{"1d208a43":"code","2f614e29":"code","1b07e068":"code","f63a24a2":"code","583d73ea":"code","53a74f36":"code","d40df5cf":"code","0dec399a":"code","ed5579cc":"code","ae4f5e23":"code","f6a263e8":"code","471495a4":"code","15dfbbc4":"code","8441442e":"code","04980033":"code","dd83a5b6":"code","d1550ef7":"code","2c2418bb":"code","0b28d712":"code","8a8bfc1a":"code","e55ad099":"code","d0143bee":"code","61e17428":"code","27c6e35b":"code","b1c759fe":"code","b2731ea7":"code","216c7270":"code","a0ea288b":"code","fe2bbded":"code","ca2814ae":"code","1c18ebf6":"code","741424bf":"code","5b0145c6":"code","31a9b118":"code","3db7bfa6":"code","bf0ad119":"code","1610e8ad":"code","80c8dac8":"code","f3cba976":"code","06d5d869":"code","677f0800":"code","2e8a7002":"code","5478125c":"code","4ee5adf1":"markdown","146fa586":"markdown","a1bb24aa":"markdown","4c51768e":"markdown","6a184de3":"markdown","5d280fc9":"markdown","bbc88ca8":"markdown","a81718ef":"markdown","1f5462a7":"markdown","f376a0f0":"markdown","67456157":"markdown","f5d6036c":"markdown","0de9035f":"markdown","72a896cf":"markdown","87a1b6ce":"markdown","03fc232a":"markdown","4fff0acc":"markdown","c66e22c4":"markdown","aaa49f77":"markdown","c4754a71":"markdown","6921d7b0":"markdown","9a315b88":"markdown","1a62a7ad":"markdown","6204b632":"markdown","18f68967":"markdown","a5bfae32":"markdown","bb7d4110":"markdown","7cd7ff0b":"markdown","65e0ff71":"markdown","825160cd":"markdown","58e2ef67":"markdown","8864a99d":"markdown","9527be35":"markdown","dc08a46c":"markdown","31602b2c":"markdown","eed4932d":"markdown","50085fcd":"markdown","0cccf3a2":"markdown"},"source":{"1d208a43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n# Changing the number of characters displayed in pandas \npd.options.display.max_colwidth = 150","2f614e29":"import seaborn as sns\n# preprocessing\nimport regex as re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n# data split\nfrom sklearn.model_selection import train_test_split\n# model\nimport tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout, Input\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n# bert\n!pip install -q tf-models-official==2.3.0\nfrom official.modeling import tf_utils\nfrom official import nlp\nfrom official.nlp import bert\nimport tensorflow_hub as hub\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport official.nlp.bert.tokenization as tokenization","1b07e068":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","f63a24a2":"train.info()\ntest.info()","583d73ea":"train.head(10)","53a74f36":"sns.countplot(data=train, x='target')","d40df5cf":"train['words'] = train['text'].str.split()\ntrain['word_len'] = train['words'].map(lambda x: len(x))\ntrain['word_len'].max()","0dec399a":"test['words'] = test['text'].str.split()\ntest['word_len'] = test['words'].map(lambda x: len(x))\ntest['word_len'].max()","ed5579cc":"# Thanks to https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data - \n# author of this kernel read tweets in training data and figure out that some of them have errors:\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain.loc[train['id'].isin(ids_with_target_error),'target'] = 0\ntrain[train['id'].isin(ids_with_target_error)]","ae4f5e23":"df_all = pd.concat([train,test])\ndf_all.shape","f6a263e8":"df_all['text'] = df_all['text'].str.lower()\ndf_all['text'].head(2)","471495a4":"def remove_breaklines(text):\n    return re.sub('\\n','',text)\ndf_all['text'] = df_all['text'].apply(lambda x: remove_breaklines(x))","15dfbbc4":"def remove_numbers(text):\n    return re.sub('\\w*\\d\\w*', '', text)\ndf_all['text'] = df_all['text'].apply(lambda x: remove_numbers(x))","8441442e":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndf_all['text'] = df_all['text'].apply(lambda x: remove_URL(x))","04980033":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf_all['text']=df_all['text'].apply(lambda x : remove_html(x))","dd83a5b6":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf_all['text']=df_all['text'].apply(lambda x: remove_emoji(x))","d1550ef7":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf_all['text']=df_all['text'].apply(lambda x : remove_punct(x))","2c2418bb":"\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", \n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev(word):\n    return abbreviations[word] if word in abbreviations.keys() else word","0b28d712":"def convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ndf_all['text']=df_all['text'].apply(lambda x : convert_abbrev_in_text(x))","8a8bfc1a":"df_all['text']=df_all['text'].apply(lambda x : word_tokenize(x))","e55ad099":"stop = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stop]\n    return ' '.join(words)\n\ndf_all['text']=df_all['text'].apply(lambda x : remove_stopwords(x))","d0143bee":"cl_ch_len = df_all['text'].apply(lambda x: len(x))\ncl_wd_len = df_all['text'].str.split().map(lambda x: len(x))\nprint('Max words length for cleaned tweets: {}'.format(max(cl_wd_len)))\nprint('Max characters length for cleaned tweets: {}'.format(max(cl_ch_len)))\nMAX_LEN = max(cl_wd_len)","61e17428":"def create_corpus(df):\n    corpus=[]\n    # tqdm show progress bar\n    for tweet in tqdm(df_all['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus = create_corpus(df_all)\ncorpus[0]","27c6e35b":"glove_embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in tqdm(f):\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        glove_embedding_dict[word]=vectors","b1c759fe":"# Define the dimension of word embeddings\nW_E_DIM = 200","b2731ea7":"tokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n# Transforms each text in texts to a sequence of integers.\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n# Padding\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","216c7270":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","a0ea288b":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,W_E_DIM))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=glove_embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec ","fe2bbded":"train_text = tweet_pad[:train.shape[0]]\ntest_text = tweet_pad[train.shape[0]:]\nX_train,X_dev,Y_train,Y_dev=train_test_split(train_text,train['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_dev.shape)","ca2814ae":"model=Sequential()\n# The Embedding layer can be understood as a lookup table that maps from integer indices  to their embeddings. \nembedding=Embedding(num_words,W_E_DIM,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\nmodel.summary()","1c18ebf6":"history=model.fit(X_train,Y_train,batch_size=4,\n                  epochs=10,validation_data=(X_dev,Y_dev),verbose=2)\n","741424bf":"pred = model.predict(test_text)\npred = pred.round().astype('int')","5b0145c6":"df_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_sub['target'] = pred\ndf_sub = df_sub[['id', 'target']]\ndf_sub.to_csv('lstm_submission.csv', index=False, header=True)\ndf_sub.head(10)","31a9b118":"%%time\nbert_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2'\nbert_layer = hub.KerasLayer(bert_url, trainable=True, name='Bert_layer')","3db7bfa6":"def bert_encode(texts, tokenizer, max_len=512):\n    # build three inputs embeddings\n    input_tokens = [] # token embedding \n    input_masks = [] # mask embedding, mask padding as 0\n    input_segments = [] # segment embedding, segment two sentences\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        # segment \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        # padding\n        pad_len = max_len - len(input_sequence)\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        # pad_mask is 0 for padding and 1 for other\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        # single sentence segment_ids are all 0\n        segment_ids = [0] * max_len\n        \n        input_tokens.append(tokens)\n        input_masks.append(pad_masks)\n        input_segments.append(segment_ids)\n    \n    return np.array(input_tokens), np.array(input_masks), np.array(input_segments)","bf0ad119":"def build_model(bert_layer, max_len=512):\n    # build up input layer\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    # define bert layer outputs:[batch_size, 1024] and [batch_size, max_len, 1024]\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    # take [CLS](first token) in the output as classification features\n    cls_output = sequence_output[:,0,:]\n    output = Dense(1, activation='sigmoid')(cls_output)\n    # define model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n    model.compile(Adam(lr=1e-5), loss = 'binary_crossentropy',metrics=['accuracy'])\n    \n    return model\n    ","1610e8ad":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","80c8dac8":"train_text = df_all[:train.shape[0]].text\ntest_text = df_all[train.shape[0]:].text\ntrain_input = bert_encode(train_text, tokenizer, max_len=160)\ntest_input = bert_encode(test_text, tokenizer, max_len = 160)\ntrain_labels = train.target.values","f3cba976":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","06d5d869":"tf.keras.utils.plot_model(model, show_shapes=True)","677f0800":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    callbacks=[checkpoint],\n    batch_size=16\n)","2e8a7002":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","5478125c":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('bert_submission.csv', index=False)","4ee5adf1":"## Build Embedding Dictionary","146fa586":"## Transform to Lowercase","a1bb24aa":"## Read Data","4c51768e":"Prepare data for Bert model.","6a184de3":"Define a functon to build model.","5d280fc9":"## Generate Embedding Matrix","bbc88ca8":"## Remove Emojis","a81718ef":"Resotre vocabulary and configure tokenizer.","1f5462a7":"## Generate Input Padding Sequence","f376a0f0":"## Build Corpus","67456157":"## Remove numbers","f5d6036c":"## Tokenize","0de9035f":"# Import Libaries","72a896cf":"## Number of Words","87a1b6ce":"## Bert Model","03fc232a":"Load Bert model from Tensorflow hub.","4fff0acc":"## Split Data","c66e22c4":"## Deal with Abbreviations","aaa49f77":"Define a function to encode inputs.","c4754a71":"## Remove URLs","6921d7b0":"## Classs Distribution","9a315b88":"## Remove HTML Tags","1a62a7ad":"Here, I create two new features for future data cleaning, 'words' and 'word_len'.","6204b632":"***Observation:***  \nMore non-disaster tweets are appeared than disaster tweets. Classes are almost balanced distributed.","18f68967":"Build model.","a5bfae32":"# Data Cleaning","bb7d4110":"## Data Correction","7cd7ff0b":"# Bulid Word Embeddings","65e0ff71":"## Remove Punctuations","825160cd":"## Remove Breaklines","58e2ef67":"# Exploratory Data Analysis","8864a99d":"# Model Data","9527be35":"## Remove Stop Words","dc08a46c":"# Reference:  \n* https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n* https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n* https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub","31602b2c":"### Glove 100d ","eed4932d":"***Observation:***  \nTraining data contains 3 useful features, keyword and location features contain missing value.  \nTest data contains same features and some missing values in keyword and location as well.","50085fcd":"## Basic LSTM Model","0cccf3a2":"## Combine train and test"}}