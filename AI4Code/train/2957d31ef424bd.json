{"cell_type":{"792b8cb4":"code","b9667c6b":"code","51f9e780":"code","3ca02b1b":"code","da9c169e":"code","20ef765e":"code","0a5a3f22":"code","65514590":"code","47be61a0":"code","f341412d":"code","4ff4ca65":"code","8da04377":"code","acf5bfdd":"code","79e834d5":"code","3d6861b2":"code","8dd313b7":"markdown","5f54938d":"markdown","b20daeb2":"markdown"},"source":{"792b8cb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9667c6b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport string \n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n\n%matplotlib inline\npd.options.display.max_columns = None","51f9e780":"test_raw = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntrain_raw = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","3ca02b1b":"# From the comments I found that tweets are not unique\n# And that sometimes they are mislabeled\nprint(\"Total Train Tweets:\", len(train_raw['text']))\nprint(\"Total Unique Tweets:\", len(train_raw['text'].unique()))\n\nduped_tweets = train_raw[train_raw.duplicated(subset=['text'],keep=False)].sort_values(by=['text'])\nduped_tweets.head()\n\ntrain_raw = train_raw[~train_raw.duplicated(subset=['text','target'],keep=False)]\nprint(\"Total sans dupes: \", len(train_raw))","da9c169e":"# That's all feature engineering we'll do\ntest_df = test_raw.copy()\ntrain_df = train_raw.copy()","20ef765e":"nlp = spacy.load('en_core_web_sm')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\npunctuations = string.punctuation","0a5a3f22":"# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = nlp(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    \n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens\n","65514590":"class predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()","47be61a0":"# We will use this function to get the best model \ndef get_tuned_model(estimator, param_grid, scoring, X, Y):\n    from sklearn.model_selection import GridSearchCV\n\n    grid = GridSearchCV(estimator = estimator, \n                       param_grid = param_grid,\n                       scoring = scoring,\n                       cv=3,\n                       n_jobs= -1\n                      )\n\n    tuned = grid.fit(X, Y)\n\n    print (\"Best score: \", tuned.best_score_) \n    print (\"Best params: \", tuned.best_params_)\n    print (\"IS Score: \", tuned.score(X, Y))\n    \n    return tuned\n","f341412d":"def save_results(model, ids, data):\n    pred_test = model.predict(data)\n\n    test_res = ids.copy()\n    test_res[\"target\"] = pred_test\n    test_res.to_csv(\"\/kaggle\/working\/my_predictions.csv\", index=False)\n    return test_res","4ff4ca65":"bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)","8da04377":"X_train, X_valid, y_train, y_valid = train_test_split(train_df['text'], train_df['target'], test_size=0.3)\n\nX_train = tfidf_vector.fit_transform(X_train)\nX_valid = tfidf_vector.transform(X_valid)","acf5bfdd":"ids = test_df[['id']]\nX_test = tfidf_vector.transform(test_df['text'])","79e834d5":"classifier = LogisticRegression()\n\nparam_grid = {\n    \"C\":  np.logspace(0, 4, 10),\n}\n\n# grd = get_tuned_model(pipe, param_grid, \"accuracy\", train_df['text'], train_df['target'])\ngrd = get_tuned_model(classifier, param_grid, \"accuracy\", X_train, y_train)","3d6861b2":"results = save_results(grd, ids, X_test)\nresults.head()","8dd313b7":"### GridSearchCV with LogisticRegression","5f54938d":"## Data Preprocessing","b20daeb2":"## Modelling"}}