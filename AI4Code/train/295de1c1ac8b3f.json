{"cell_type":{"14c70be4":"code","8ae6cd6c":"code","13bd6e42":"code","108d3979":"code","9df59e2c":"code","e147a08f":"code","8e95699d":"code","b0afe396":"code","23a1b74e":"code","5a36aa60":"markdown","61b37c84":"markdown","08cfa69c":"markdown","fbb24a76":"markdown","55db096c":"markdown","7399fb39":"markdown","82ef79a4":"markdown","ee93be88":"markdown","c4db3dc3":"markdown"},"source":{"14c70be4":"# importing library to handle files\nimport os\n\n# importing libray to handle status bars\nfrom tqdm.notebook import tqdm\n\n# import libray to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing library to process data structures\nimport pandas as pd\n\n# importing library to deal with numeric arrays\nimport numpy as np\n\n# importing deep learning library\nimport tensorflow as tf\n\n# importing library for preprocessing\nfrom sklearn.model_selection import train_test_split","8ae6cd6c":"# initializing lists to store file paths for training and validation\nimg_paths = []\n\n# importing libraries to store label references\nlabels = []\n\n# iterating through directories\nfor dirname, _, filenames in tqdm(os.walk('\/kaggle\/input')):\n    for filename in filenames:\n        \n        path = os.path.join(dirname, filename)\n        \n        if '.jpg' in path:\n        \n            img_paths.append(path)\n            labels.append(path.split(os.path.sep)[-2])","13bd6e42":"# dataframes for training, validation and test datasets\nmain_df = pd.DataFrame({'Path': img_paths, 'Label': labels}).sample(frac = 1,\n                                                                    random_state = 10)\n\noX_train, X_test, oy_train, y_test = train_test_split(main_df['Path'], main_df['Label'], test_size = 0.2,\n                                                      stratify = main_df['Label'], \n                                                      shuffle = True, random_state = 20)\n\nX_train, X_val, y_train, y_val = train_test_split(oX_train, oy_train, test_size = 0.2,\n                                                  stratify = oy_train, \n                                                  shuffle = True, random_state = 40)\n\n# train dataframe\ntrain_df = pd.DataFrame({'Path': X_train, 'Label': y_train})\n\n# validation dataframe\nval_df = pd.DataFrame({'Path': X_val, 'Label': y_val})\n\n# test dataframe\ntest_df = pd.DataFrame({'Path': X_test, 'Label': y_test})","108d3979":"# setting image dimensions\nIMAGE_DIMS = (224, 224, 3)\n\n# loading preprocessing function\nprep_func = tf.keras.applications.vgg16.preprocess_input \n        \n# importing pretrained model\nvgg_model = tf.keras.applications.vgg16.VGG16(input_shape = IMAGE_DIMS,\n                                              include_top = False, weights = 'imagenet')\n        \n# freezing layers in pretrained model\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\n# training generator for augmentation\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func,\n                                                                rotation_range = 10,  \n                                                                zoom_range = 0.1, width_shift_range = 0.1,  \n                                                                height_shift_range = 0.1, \n                                                                horizontal_flip = True, \n                                                                vertical_flip = True)  \n\n# validation\/testing generator for augmentation\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func)\n\n# batch size for training\ntrain_bs = 256\n\n# loading training data in batches\ntrain_generator = train_datagen.flow_from_dataframe(dataframe = train_df, x_col = \"Path\",\n                                                    y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n                                                                                    IMAGE_DIMS[0]),\n                                                    batch_size = train_bs, \n                                                    class_mode = 'binary')\n\n# batch size for validation\nval_bs = 128\n\n# loading validation data in batches\nval_generator = val_datagen.flow_from_dataframe(dataframe = val_df, x_col=\"Path\",\n                                                y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n                                                                                IMAGE_DIMS[0]),\n                                                batch_size = val_bs, \n                                                class_mode = 'binary',\n                                                shuffle = False)\n\n# batch size for testing\ntest_bs = 160\n    \n# loading test data in batches\ntest_generator = val_datagen.flow_from_dataframe(dataframe = test_df, x_col = \"Path\",\n                                                 y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n                                                                                 IMAGE_DIMS[0]),\n                                                 batch_size = test_bs, \n                                                 class_mode = 'binary',\n                                                 shuffle = False)","9df59e2c":"# defining a sequential model to learn \nclf_model = tf.keras.Sequential()\n\n# adding pretrained model\nclf_model.add(vgg_model)\n\n# using global average pooling instead of flatten and global max pooling\nclf_model.add(tf.keras.layers.GlobalAveragePooling2D())\n\nclf_model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\nclf_model.add(tf.keras.layers.Dropout(0.3))\n\nclf_model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\nclf_model.add(tf.keras.layers.Dropout(0.3))\n\nclf_model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\n# model summary\nclf_model.summary()","e147a08f":"# calculating steps for train, validation and testing\nsteps_train = np.ceil(train_df.shape[0]\/train_bs)\nsteps_val = np.ceil(val_df.shape[0]\/val_bs)\nsteps_test = np.ceil(test_df.shape[0]\/test_bs)\n\nprint(\"Steps for training:\", str(steps_train) + ',', \"validation:\", str(steps_val) + ',', \n      \"testing:\", str(steps_test))","8e95699d":"# compiling the model\nclf_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# training\nhistory = clf_model.fit_generator(train_generator, steps_per_epoch = steps_train,\n                                  validation_data = val_generator, epochs = 1,\n                                  validation_steps = steps_val, verbose = 1)","b0afe396":"# evaluation\nclf_eval = clf_model.evaluate_generator(test_generator, steps = steps_test, \n                                        verbose = 1)","23a1b74e":"# getting accuracy from evaluation\nprint(\"Accuracy on Test:\", clf_eval[1])","5a36aa60":"# Preprocessing","61b37c84":"# Model evaluation","08cfa69c":"# Data augmentation","fbb24a76":"# Model training","55db096c":"This notebook provides a quick overview of how to apply transfer learning and flow batch processing of \"relatively large\" datasets for neural networks and deep learning.","7399fb39":"# Importing libraries","82ef79a4":"# Model architecture","ee93be88":"# Model parameters","c4db3dc3":"# Introduction"}}