{"cell_type":{"f2d67210":"code","6f35b3d0":"code","856f0355":"code","22f1078e":"code","5c0e9f26":"code","9885a54a":"code","15f6c959":"code","df78ec2e":"code","e8b172f0":"code","cd15f067":"code","81c81d90":"code","8eb3dfcf":"code","1d829dbb":"code","311587e6":"code","a3fe9ed8":"code","882b70be":"code","ecb9a656":"code","e8529bf7":"code","70d2d1a5":"code","c7c9dc20":"code","86c2cfe0":"code","f7cfea14":"code","8e4fcd2f":"code","69322487":"code","d3ecde49":"code","50bcf726":"code","0c2e8c3f":"code","56943147":"code","0fd636c9":"code","afeda985":"code","98b7202f":"code","bd6862cb":"code","a399cca0":"code","2eb597d1":"code","883618e2":"code","a83a6e13":"code","c4b1bfc6":"code","4b91612e":"code","917aa8a2":"code","1dd29b36":"code","74a4d2cc":"code","be3ae9d7":"code","2e05132f":"code","461c2039":"code","6097f19e":"code","48d10e72":"code","b846b870":"code","0468896c":"code","d34aac8f":"code","16d400f2":"code","7beaf7bc":"code","e1d233ba":"code","b8412156":"code","03b8ffcc":"code","b7d89499":"markdown","fe56e805":"markdown","2f711f52":"markdown","b73ce12c":"markdown","99076f5e":"markdown","e13fc380":"markdown","f6a30117":"markdown","dc3a45ce":"markdown","c8ff0409":"markdown","4124ede9":"markdown","60e40513":"markdown","f0dcdfa5":"markdown","027df5db":"markdown","78a93fe6":"markdown","5f969ce6":"markdown","a83e6080":"markdown","4604d3f2":"markdown","7052ecc7":"markdown","f472ce47":"markdown","0f0d887b":"markdown","9457190f":"markdown","376c62c0":"markdown","ee32dc03":"markdown","20de9e44":"markdown","bb561c28":"markdown","aacb0b82":"markdown","eaa8bc5a":"markdown","50a68c60":"markdown","072ca6ad":"markdown"},"source":{"f2d67210":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f35b3d0":"train = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","856f0355":"train.head()","22f1078e":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns","5c0e9f26":"print(\"Let us see the consistency of data in terms of enrollee id\")\ntrain.sort_values(by=['enrollee_id'])","9885a54a":"print(\"Number of different cities from which data is taken : \" + str(len(np.unique(train.city))))","15f6c959":"data = train.city.value_counts().reset_index().rename(columns={\n    'index':'city',\n    'city' : 'occurences'\n}).head(20)\nfig = plt.figure(figsize=(10,10))\nsns.barplot(x = 'occurences',y = 'city',data=data)\nplt.title('Number of enrollees per city (Top 20 cities only) ')\nplt.xlabel('Count')\nplt.ylabel('City')\nplt.show()","df78ec2e":"fig = plt.figure(figsize=(8,7))\nsns.distplot(train['city_development_index'],hist=False,color='r')\nplt.xlabel('City Development Index')\nplt.ylabel('Probability Density')\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.title(\"Probability Density Function for City Development index\",fontsize=13)\nplt.show()","e8b172f0":"print(\"Percentage of missing values in gender collumn : \" + str(round(train.gender.isna().sum()\/train.shape[0]*100,2)) +\" %\")","cd15f067":"train.gender = train.gender.fillna('Other')\nprint(\"Different categories of gender now : \" + str(np.unique(train.gender)))","81c81d90":"labels1 = ['Male','Other','Female']\nfig = plt.figure(figsize=(9,9))\ncolor = ['c','orange','r']\ntextprop = {'fontsize' : 13}\nplt.pie(train.gender.value_counts(),labels = labels1,autopct = '%1.2f%%',explode=[0,0.1,0.2],\n       shadow=True,textprops = textprop,colors = color)\nplt.title(\"Pie chart depicting percentage values of different employee's genders\",fontsize=14)\nplt.show()","8eb3dfcf":"print(\"Unique values in Experience collumn : \" + str(np.unique(train.relevent_experience)))","1d829dbb":"f,ax = plt.subplots(1,2,figsize=(10,10))\nlabels2 = ['Has relevant Experience','No relevant experience']\ncolor1 = ['blue','orange']\ntextprop = {'fontsize' : 13}\nax[0].pie(train.relevent_experience.value_counts(),labels = labels2,autopct = '%1.2f%%',explode=[0,0.2],\n       shadow=True,textprops = textprop,colors = color1)\nax[0].set_title(\"Pie chart depicting percentage values of Relevant experience\",fontsize=13)\n\n\nsns.countplot(train.relevent_experience,ax=ax[1])\nplt.xticks(fontsize=8)\nplt.tight_layout(pad=4)","311587e6":"sns.distplot(train.training_hours,hist=False)\nplt.title('Probability distribution curve for Training hours',fontsize=13)\nplt.xlabel('Training Hours',fontsize=13)\nplt.ylabel('Probability density',fontsize=13)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()","a3fe9ed8":"fig = plt.figure(figsize=(9,9))\ncolors_1 = ['palegreen','red','magenta','cyan','yellow','lightsteelblue']\nlabels_1 = ['STEM','Humanities','Other','Business Degree','Arts','No Major']\ntextprop = {'fontsize' : 13}\nplt.pie(train.major_discipline.value_counts(),colors=colors_1,labels=labels_1,autopct = '%1.2f%%',explode=[0,0.1,0.2,0.2,0.3,0.3],\n       shadow=True,textprops = textprop)\nplt.title(\"Percentage values of Major Discipline\",fontsize=13)\nplt.show()","882b70be":"fig = plt.figure(figsize=(9,9))\nlabels_2 = [\"Doesn't want to change job\",\"Wants to change job\"]\ncolors = ['cyan','magenta']\ntextprop = {'fontsize' : 13}\nplt.pie(train.target.value_counts(),labels=labels_2,autopct = '%1.2f%%',explode=[0,0.2],\n       shadow=True,textprops = textprop,colors=colors)\nplt.title(\"Percentage values of Outcome variable\",fontsize=13)\nplt.show()","ecb9a656":"print(\"Percentage of missing values in different collumns  is : \\n\" + str(train.isna().sum()\/train.shape[0]*100))","e8529bf7":"train.dtypes","70d2d1a5":"# Concatenate training and testing data only for the purpose of dealing with missing values!\ndata = pd.concat([train,test],axis=0)\ndata.shape","c7c9dc20":"#Create a list of the collumn names to fill the missing values column-wise\ncolumn = train.columns\ncolumn = list(column)\ncolumn.pop()#Remove the target variable and the enrollee_id from the list of collumn names\ncolumn.pop(0)\nprint(column)","86c2cfe0":"#Seperate the collumns which have object and numerical data types respectively!\nobj_val = []\nnum_val = []\nfor i in column:\n    if data[i].dtype == 'object':\n        obj_val.append(i)\n    else:\n        num_val.append(i)","f7cfea14":"num_val,obj_val","8e4fcd2f":"ord_val = ['relevent_experience','education_level','experience']\nfor j in ord_val:\n    print(train[j].unique())","69322487":"data['relevent_experience'] = data['relevent_experience'].map({\n    'Has relevent experience':1,\n    'No relevent experience' : 0\n})\ndata['education_level'] = data['education_level'].map({\n    'Primary School' : 0,\n    'High School' : 1,\n    'Graduate': 2,\n    'Masters' : 3,\n    'Phd':4\n})\ndata['experience'] = data['experience'].map({\n    '<1' : 0,\n    '1':1,\n    '2':2,\n    '3':3,'4':4,'5':5,'6':6,'7':7,'8':8, '9':9,'10':10,'11':11,'12':12,\n    '13':13,'14':14,'15':15,'16':16,'17':17,'18':18,'19':19,'20':20,'>20':21\n})","d3ecde49":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in range(data.shape[1]):\n    if data.iloc[:,i].dtypes == 'object':\n        le.fit(list(data.iloc[:,i].values))\n        data.iloc[:,i] = le.transform(list(data.iloc[:,i].values))","50bcf726":"data[num_val] = data[num_val].fillna(data[num_val].median())\ndata[ord_val] = data[ord_val].fillna(data[ord_val].median())\n\nfor j in data[obj_val]:\n    data[j] = data[j].fillna(data[j].mode())","0c2e8c3f":"data.isna().sum()","56943147":"#Split the data back into test and train respectively\ntesting = data[data.target.isnull() == True]\ntesting = testing.drop(\"target\",axis=1)\ntraining = data.dropna(axis=0)\n","0fd636c9":"X_train = training.iloc[:,:-1]\nY_train = training.iloc[:,-1]\nX_test = testing\n","afeda985":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train.values)\nY_train = Y_train.values\nX_test = scaler.fit_transform(X_test.values)","98b7202f":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nsmote = SMOTE(k_neighbors=5,random_state=9)\nunder = RandomUnderSampler()\nX_train, Y_train = smote.fit_resample(X_train, Y_train)\nX_train, Y_train = under.fit_resample(X_train, Y_train)","bd6862cb":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 80)\nrf.fit(X_train,Y_train)\nranking = np.argsort(-rf.feature_importances_)\n# np.argsort returns indices of features that are of maximum importance\nf, ax = plt.subplots(figsize=(18, 12))\nsns.barplot(x=rf.feature_importances_[ranking], y=training.columns.values[ranking], orient='h')\nax.set_xlabel(\"Feature Importance\")\nplt.tight_layout()\nplt.show()","a399cca0":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_train,Y_train,random_state=60,test_size=0.2)","2eb597d1":"from sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(x_train,y_train)","883618e2":"print(\"Training Score is :\" + str(clf_lr.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_lr.score(x_test,y_test)))","a83a6e13":"#Applying Randomm Forest Regression to our dataset\nclf_rf = RandomForestRegressor(n_estimators=40)\nclf_rf.fit(x_train,y_train)","c4b1bfc6":"print(\"Training Score is :\" + str(clf_rf.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_rf.score(x_test,y_test)))","4b91612e":"#Apply decision Tree to our dataset\nfrom sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier()\nclf_dt.fit(x_train,y_train)","917aa8a2":"print(\"Training Score is :\" + str(clf_dt.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_dt.score(x_test,y_test)))","1dd29b36":"#Apply XGBoost classifier to our dataset\nfrom xgboost import XGBClassifier\nclf_xgb = XGBClassifier()\nclf_xgb.fit(x_train,y_train)","74a4d2cc":"from sklearn.metrics import accuracy_score\nprint(\"Training Score is :\",accuracy_score(y_true=y_train,y_pred = clf_xgb.predict(x_train).round()))\n\nprint(\"Testing Score is :\" ,accuracy_score(y_true=y_test,y_pred = clf_xgb.predict(x_test).round()))","be3ae9d7":"#Apply SVM to our dataset\nfrom sklearn.svm import SVC\nclf_svm = SVC()\nclf_svm.fit(x_train,y_train)","2e05132f":"print(\"Training Score is :\" + str(clf_svm.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_svm.score(x_test,y_test)))","461c2039":"#Apply knn to our dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=5)\nclf_knn.fit(x_train,y_train)","6097f19e":"print(\"Training Score is :\" + str(clf_knn.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_knn.score(x_test,y_test)))","48d10e72":"#Apply bagging technique with knn\nfrom sklearn.ensemble import BaggingClassifier\nclf_bag_knn = BaggingClassifier(KNeighborsClassifier(),max_samples=0.3,max_features=0.3)\nclf_bag_log = BaggingClassifier(LogisticRegression(),max_samples=0.3,max_features=0.3)\nclf_bag_svm = BaggingClassifier(SVC(),max_samples=0.3,max_features=0.3)\nclf_bag_knn.fit(x_train,y_train)\nclf_bag_log.fit(x_train,y_train)\nclf_bag_svm.fit(x_train,y_train)","b846b870":"print(\"Training Score in Knn bagging is :\" + str(clf_bag_knn.score(x_train,y_train)))\n\nprint(\"Testing Score is Knn bagging is:\" + str(clf_bag_knn.score(x_test,y_test)))\n\nprint(\"Training Score in Logistic Regression bagging is :\" + str(clf_bag_log.score(x_train,y_train)))\n\nprint(\"Testing Score is Logistic Regression bagging is:\" + str(clf_bag_log.score(x_test,y_test)))\n\nprint(\"Training Score in SVM bagging is :\" + str(clf_bag_svm.score(x_train,y_train)))\n\nprint(\"Testing Score is SVM bagging is:\" + str(clf_bag_svm.score(x_test,y_test)))","0468896c":"from sklearn.ensemble import VotingClassifier\nclf_voting1 = VotingClassifier(\n    estimators = [('knn',clf_knn),(\"xgb\",clf_xgb),('dt',clf_dt)],\n    voting='hard')\nclf_voting1.fit(x_train,y_train)","d34aac8f":"print(\"Training Score is :\" + str(clf_voting1.score(x_train,y_train)))\n\nprint(\"Testing Score is :\" + str(clf_voting1.score(x_test,y_test)))","16d400f2":"pred = clf_xgb.predict(X_test)","7beaf7bc":"pd.Series(pred).value_counts()","e1d233ba":"training.iloc[:,-1].value_counts()","b8412156":"employee_id = testing.enrollee_id\n","03b8ffcc":"submission = pd.DataFrame({'enrollee_id':employee_id,\n                          'target':pred})\nsubmission.to_csv('hr_analytics.csv',index=False)","b7d89499":"We will be using a library called imblearn(im-balanced-learn) to implement a technique called SMOTE(Synthetic Minority Oversampling Technique). ","fe56e805":"There are a total of 123 different cities that are a part of this data!","2f711f52":"Since the data is imbalanced and biased towards the 0th class. Hence we will have to use oversampling\/undersmapling techniques to balance out the dataset.","b73ce12c":"After applying numerous machine learning models and ensemble methods we were able to achieve a maximum of 85% accuracy.","99076f5e":"**All this E.D.A gives us conclusive evidence that the data is highly imbalanced.**","e13fc380":"We will not go over each collumn variable because this will take up lot of time,space and is not of any fruitful outcome.\nWe will only check the probability distribution of training hours and percentage distribution of our outcome variable and the major discipline.\n","f6a30117":"### Final Step : Applying Machine Learning model on Dataset","dc3a45ce":"**Let us First try to visualise the data to get a better grasp of it**","c8ff0409":"Some variables are categorical whilst other are ordinal in nature. \nOrdinal means that the variables have some particular order like in education level. Whilst categorical variables are the ones that have no particular order for example gender.","4124ede9":"We can safely assume that the missing values in the gender collumns are employees who : \n1. Did not want to disclose their gender\n2. Didn't fit in the conventional male\/female gender collumn\n3. Forgot to fill the gender collumn\n\nNevertheless, we will label all NaN values as \"Others\"","60e40513":"Let us find out the importance of different features in predicting the value of the target variable using Random Forest Regressor","f0dcdfa5":"## Moving on to the gender collumn","027df5db":"First we need to find the percentage of missing values in different collumns, followed by their datatypes.","78a93fe6":"![image.png](attachment:image.png)","5f969ce6":"Let us find out which city harbours the maximum number of enrollees!","a83e6080":"The Above graph shows us that education_level, enrolled_university, gender and relevent_experience play very little role in determining  whether an employee wants to change their job or not","4604d3f2":"## Let us now see the probability distribution curve of the city development index ","7052ecc7":"We will have to deal with the ordinal variables manually only.","f472ce47":"Probability Density Functions are a statistical measure used to gauge the likely outcome of a discrete value. The area under the curve of probability distribution graph represents how likely an event is to occur.","0f0d887b":"### Experience\n","9457190f":"# We will work our way collumn by collumn","376c62c0":"And if it belongs to int\/float data type, then we fill it with median of the remaining values","ee32dc03":"## More the C.D.I, better the living standards of that particular city","20de9e44":"It can be clearly seen that a lot of enrollee_ids'(e.g 3,6,... etc.) are missing. Hence it will be useless for our model.","bb561c28":"We will oversample the '1' class and undersample the '0' class to bring it to a ratio of 50:50.","aacb0b82":"## Dealing with missing values in dataset","eaa8bc5a":"As a basic rule, if the missing value belongs to object data type, we fill it with the mode of the remaining values.","50a68c60":"So far we have tried most of the basic classification machine learning models. Now we will be trying ensemble methods","072ca6ad":"### The maximum number of values are seen to lie between 0.9-1.0"}}