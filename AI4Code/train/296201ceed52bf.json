{"cell_type":{"b402e6a4":"code","ca5daf0e":"code","6a0cf3be":"code","9b79f0ff":"code","8495529e":"code","d56f8d42":"code","34856348":"code","2a44b13f":"code","b3b305d7":"code","5d0ccf76":"code","17ed9b79":"code","e0d65e73":"code","0964dff0":"code","b4c3fbb2":"code","149675b4":"code","ae70cb28":"code","428983e4":"code","421d78e9":"code","26c99810":"code","bd4a23c4":"code","aceee47b":"code","5dcac764":"code","bcedcad4":"code","2f825219":"code","1df94140":"code","e580eed0":"code","d02e054a":"code","278c78d5":"code","7c87ae86":"code","2d377ffd":"code","4421ee0f":"code","30fa3c2a":"code","1b50096b":"code","0387e071":"code","5f6a8622":"code","814f18e1":"code","f70e55bd":"code","34fee0e6":"code","780d572e":"code","f1c276cc":"code","425cb057":"code","e9910f12":"code","d21676bf":"code","b0670989":"code","c4268608":"code","1720d514":"code","a9d7f9a4":"code","daee98bb":"code","7d24ef12":"code","92a20ad0":"markdown","35ad994e":"markdown","5911b6d1":"markdown","8119a492":"markdown","29f605ab":"markdown","c0efa4e6":"markdown","2474b08c":"markdown","3a3d87ac":"markdown","45120fc5":"markdown","3889201b":"markdown","33d93e1d":"markdown","f18446d7":"markdown","0c87a42a":"markdown","dc32b62f":"markdown","228b3019":"markdown","75f3f49d":"markdown","7e7287a4":"markdown","6b161917":"markdown","351ddd3b":"markdown","29fd6caa":"markdown","b5dc826c":"markdown","e12a4569":"markdown","fdf3f771":"markdown","1d710ed1":"markdown","74b32278":"markdown","2454dbe0":"markdown","93adab8f":"markdown","1263f2de":"markdown","6df625fb":"markdown","c8b6aaac":"markdown","575f6186":"markdown","1b41f283":"markdown","8f3047aa":"markdown","19162510":"markdown","909e6798":"markdown","c0297a73":"markdown","72d9a117":"markdown","ad91584d":"markdown","e88ed179":"markdown","c40e09cc":"markdown","5a1d7999":"markdown","968812ba":"markdown","3ba2fefe":"markdown","e1d0e66d":"markdown"},"source":{"b402e6a4":"#main libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","ca5daf0e":"sns.set(rc={'figure.figsize':(10,8)})\nsns.set_style()","6a0cf3be":"train = pd.read_csv('train.csv')","9b79f0ff":"train.head()","8495529e":"train.tail()","d56f8d42":"train.shape","34856348":"train.columns.tolist()","2a44b13f":"train.info()","b3b305d7":"train.isna().sum()","5d0ccf76":"train.count()","17ed9b79":"null_counts = train.isna().sum().tolist()\nnull_counts","e0d65e73":"null_percentages=[]\nfor i in null_counts:\n    perc = (i * 100) \/ train.shape[0]\n    perc = str(np.round(perc,2))+'%'\n    null_percentages.append(perc)\n\nzip_iterator = zip(train.count().index, null_percentages)\ndictionary = dict(zip_iterator)","0964dff0":"dictionary","b4c3fbb2":"sns.heatmap(train.isnull(),cbar=False,cmap='viridis')\nsns.set_style()","149675b4":"nan_records = train[train.isna().any(axis=1)]\nnan_records","ae70cb28":"nan_records.shape","428983e4":"nan_records[nan_records.isnull().sum(axis=1)>3]","421d78e9":"nan_records_shape = nan_records[nan_records.isnull().sum(axis=1)>3].shape\nnan_records_shape","26c99810":"print(str((nan_records_shape[0]*100)\/train.shape[0]) + '%')","bd4a23c4":"nan_records[nan_records.isnull().sum(axis=1)>2]","aceee47b":"nan_records_shape = nan_records[nan_records.isnull().sum(axis=1)>2].shape\nnan_records_shape","5dcac764":"print(str((nan_records_shape[0]*100)\/train.shape[0]) + '%')","bcedcad4":"dropped_indices = train[train.isna().any(axis=1)][train[train.isna().any(axis=1)].isnull().sum(axis=1)>2].index.tolist()\nprint(dropped_indices)","2f825219":"new_train = train.drop(dropped_indices)","1df94140":"len(train) - len(new_train)","e580eed0":"new_train.shape","d02e054a":"new_train.isna().sum()","278c78d5":"null_percentages=[]\nfor i in null_counts:\n    perc = (i * 100) \/ new_train.shape[0]\n    perc = str(np.round(perc,2))+'%'\n    null_percentages.append(perc)\n\nzip_iterator = zip(train.count().index, null_percentages)\ndictionary = dict(zip_iterator)","7c87ae86":"dictionary","2d377ffd":"new_train.dtypes","4421ee0f":"new_train['credit_line_utilization']","30fa3c2a":"new_train['credit_line_utilization'] = new_train['credit_line_utilization'].astype(float)","1b50096b":"new_train[new_train['credit_line_utilization']==\"1,089820359\"]","0387e071":"new_train['credit_line_utilization'] = new_train['credit_line_utilization'].astype(str)","5f6a8622":"new_train['credit_line_utilization'] = new_train['credit_line_utilization'].apply(lambda x: x.replace(',','.'))","814f18e1":"new_train['credit_line_utilization'] = new_train['credit_line_utilization'].astype(float)","f70e55bd":"new_train.dtypes","34fee0e6":"new_train_nn = new_train.apply(lambda x: x.fillna(x.mean()),axis=0)\nnew_train_nn.head(20)\n\n#nn stands for non-null","780d572e":"new_train_nn.describe()","f1c276cc":"new_train_nn.corr()","425cb057":"plt.pcolor(new_train_nn.corr(), cmap = 'Reds')\nplt.colorbar()\nplt.show()","e9910f12":"sns.heatmap(new_train_nn.corr(), annot = True, cmap='Reds')\nsns.set_style()","d21676bf":"sns.pairplot(new_train_nn.sample(500),diag_kind='kde')","b0670989":"for i in new_train_nn:\n    print(f'{i}:',stats.describe(new_train_nn[i]),'\\n')","c4268608":"sns.histplot(new_train_nn['age'],bins=100,kde=True)\nsns.set_style()","1720d514":"sns.histplot(new_train_nn['age'],bins=100,kde=True)\nsns.set_style()","a9d7f9a4":"sns.histplot(new_train_nn['number_dependent_family_members'],bins=100,kde=True)\nsns.set_style()","daee98bb":"new_train_nn['defaulted_on_loan'].value_counts().plot(kind='bar', title='Loan Status (target) of loaners',color='red')\nplt.show()","7d24ef12":"new_train_nn.to_csv('new_train_nn.csv',index=False)","92a20ad0":"<font face='Lora'><i>For this case, we have 2279 rows in which we have at least 3 null columns, let's calculate the percentage this versus dataset also:","35ad994e":"### <font color='navy' face='Lora'> Histograms","5911b6d1":"<font face='Lora'><i><b>Using the pairplot above, we can deduce that almost we don't have any correlation between our variables except the ones which I mentioned above.","8119a492":"<font face='Lora'><i>Having maximum 3 missing values for each row may be considered as normal. But let's check:","29f605ab":"<font face='Lora'><i>We have normal distribution for the age, and the one in the center is the mean which we replaced.","c0efa4e6":"<font face='Lora'><i>We have 308 rows in which we have at least 4 null columns, let's calculate the percentage this versus dataset:","2474b08c":"<font face='Lora'><i>Maybe we have this case several times, let's replace all commas by dots and check if we can proceed:<br>Firstly I convert whole column to string in order to play with characters easily.","3a3d87ac":"<font face='Lora'><i>0.42% is too little, hence we may even look at the number of rows which we have at least 3 null columns:","45120fc5":"<font face='Lora'><b><i>Above is the list of our columns of the dataset.","3889201b":"<font face='Lora'><i>Let's check the shape of our new train dataset:","33d93e1d":"<font face='Lora'><i><b>As we have tidier data, let's proceed!","f18446d7":"<font face='Lora' color='red'><i><b>Ooops, we get an error! Seems like we have a number in which there is <b>,<\/b> (comma) instead of <b>.<\/b> (dot).","0c87a42a":"<font face='Lora'><i>And let's check the state of remained null values:","dc32b62f":"<font face='Lora'><i>Since seaborn pairplot will take too much for our huge dataset, <u>in order to speed up<\/u> the process I will take the sample which is composed of 500 values which represents our whole dataset. And also as diagonal elements, histograms take much more time than kde plots, that's why I will take <b> diag_kind='kde'<\/b>","228b3019":"<font face='Lora'><b><i>\u2191 We have <font color='red'>12<\/font> columns and <font color='red'>72161<\/font> rows.","75f3f49d":"<font face='Lora'><i>And other correlations are not that high to talk about, <u>at least for now<\/u>.","7e7287a4":"<font face='Lora'><i>I think, dropping 3.158% of the dataset will not negatively affect our prediction, even will positively affect since they contain null values. So, let's drop:","6b161917":"<font face='Lora'><b><i>We have some columns with null values.","351ddd3b":"<font face='Lora'><i>We see one object type, let's convert it to the float:","29fd6caa":"<font face='Lora'><i>Let's visualize our correlation matrix in order to see the results clearly:","b5dc826c":"<font face='Lora'><i>We have some high correlations, let's look at the exact values of these correlations:","e12a4569":"<font face='Lora'><i>Let's look at descriptive statistics from SciPy library:","fdf3f771":"<font face='Lora'><i>And from this plot, we can see that the number of loaners who defaulted on loan is greater than the ones didn't.","1d710ed1":"<font face='Lora'><i>Here it is! We have only numeric columns.","74b32278":"<font face='Lora'><i><b>And let's fill all of the null values with the mean of the respective column:","2454dbe0":"<font face='Lora'><i>The rows which have 4 or more than 4 missing values:","93adab8f":"<font face='Lora'><i>As we perform EDA on training dataset, we load only train.","1263f2de":"## <font color='navy' face='Lora'> EDA ","6df625fb":"___","c8b6aaac":"<font face='Lora'><i>Let's check again if we can convert <b>'credit_line_utilization'<\/b> to numeric type:","575f6186":"<font face='Lora'><i>But we may observe that, in absence of a row in one column, we also may have an absence of this row for another column, but since our dataset is too big, it's a bit hard to decide this precisely. Let's display all records having at least one missing among columns:","1b41f283":"<font face='Lora'><i>Let's check if we have done it correctly:","8f3047aa":"<font face='Lora'><i>In some cases, we may drop columns with null values, but may we do in our case? Let's calculate the percentages of null data and decide!","19162510":"<font face='Lora'><i><b>And logically, all our our columns are positively skewed.","909e6798":"<font face='Lora'><i>Then I replace all of the commas with dots:","c0297a73":"<font face='Lora'><i>For each column: we can see the number of values, min & max of column, mean, variance, skewness (positive or negative, <u>note: skewness > 3, positively skewed<\/u>), kurtosis (form of distribution, <u>note: kurtosis > 3, sharp distribution<\/u>).","72d9a117":"<font face='Lora'><i>From the heatmap above, we see that there are correlations between:<br>\n    \n1. <b>'number_of_previous_late_payments_90_days_or_more'<\/b> and <b>'number_of_previous_late_payments_up_to_89_days'<\/b>, <u>which is quite logical<\/u>.\n2.  <b>'number_of_previous_late_payments_90_days_or_more'<\/b> and <b>'number_of_previous_late_payments_up_to_59_days'<\/b>, <u>which is quite logical as well<\/u>.","ad91584d":"<font face='Lora'><i>Yes, we had to drop 2279 rows and that's it.","e88ed179":"<font face='Lora'><i>Above, we see the percentage of the number of null values for each column. As it's not that high, we'll not drop it, but we'll handle with them.","c40e09cc":"### Step Project (Exploratory Data Analysis)","5a1d7999":"<font face='Lora'><i><b> As we are <u>very<\/u> familiar with our dataset, we may proceed to further steps as preprocessing and model training.","968812ba":"<font face='Lora'><i>Let's fill null values with mean, but before we have to check the types:","3ba2fefe":"<font face='Lora'><i><b>In short, we don't have massive amount of nulls compared to total number of rows.","e1d0e66d":"### <font color='navy' face='Lora'> Visual EDA"}}