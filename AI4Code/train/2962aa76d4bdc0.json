{"cell_type":{"7d43035f":"code","014b56c1":"code","3d899852":"code","0d51d1e3":"code","fae0e08e":"code","fc98edc4":"code","8a163fc4":"code","aaf01c2f":"code","b86e2b42":"code","f2966921":"code","e7116788":"code","7a5a2ce4":"code","53028107":"code","f57733d3":"code","ec7a202b":"code","84b88e98":"code","f5457cf6":"code","dd6c87aa":"code","38b0334b":"code","2da0f00e":"code","ada87aae":"code","b85e41ca":"code","20c5938e":"code","f3d6f009":"code","38566741":"code","b61e60a0":"code","406b28ad":"code","4cda32bd":"code","aeb4d3e5":"code","50b58279":"code","7f880481":"code","39de5779":"code","2d5a9719":"code","cf30d8c9":"code","3e72435c":"code","06bc1dc2":"code","2afdf3c3":"code","94e91b26":"code","d5bb396b":"code","7293aa4c":"code","a90f1430":"code","237379c7":"code","610e8163":"code","d642f4ba":"code","2e1cf72d":"code","c114d3b8":"code","3f86f949":"code","2e559c75":"code","41e06a8b":"code","4397093b":"code","acaaf1f0":"code","882fce97":"code","bdbe385c":"code","a7fc6c25":"code","8652ee79":"code","83cb94cb":"code","771d4615":"code","541f129c":"code","99cda3a5":"code","ba339c69":"code","3bd4c14e":"code","2704c8d7":"code","6f62eade":"code","cae1f283":"code","9cd625e5":"code","4a43b4a3":"code","da6b20f5":"markdown","c14fc3aa":"markdown","c9b75d84":"markdown","6020b448":"markdown","7abb97bf":"markdown","de35a87e":"markdown","bf0c975a":"markdown","46845831":"markdown","a87b2afe":"markdown","5cd5667c":"markdown","99ac4a7b":"markdown","c571cbac":"markdown","6a468538":"markdown","34d15afc":"markdown","616fb7bb":"markdown","4168d046":"markdown","82272982":"markdown","37127be3":"markdown","95eef8c8":"markdown","9c703e2e":"markdown","2090a20e":"markdown","380f300c":"markdown","338724b5":"markdown","39d6cdec":"markdown","89c98dc5":"markdown","558b58f5":"markdown","13babf85":"markdown","2a6e3e63":"markdown","53f94b29":"markdown","fae53e03":"markdown","40912622":"markdown","cff9afb9":"markdown","36a8c05c":"markdown","82467719":"markdown","9a0a255e":"markdown","1e1e8d10":"markdown","e959a30b":"markdown","8158ae5e":"markdown","e239a9fd":"markdown","c448975f":"markdown","7bf9b5c3":"markdown","ca446a90":"markdown","30abcd57":"markdown","4af1fa4b":"markdown","b699d5c5":"markdown","0922afc4":"markdown","166fb64b":"markdown","e0fabdbc":"markdown","616c2cd5":"markdown","ffc70410":"markdown","73de0a2f":"markdown","63005dce":"markdown","c2fec6be":"markdown","2460b024":"markdown","8104d2ae":"markdown","0c142dda":"markdown","bc9591da":"markdown","3d6051a2":"markdown","018fbd24":"markdown","448f115d":"markdown","9c46819e":"markdown","6574c92b":"markdown","593a18a9":"markdown","45236433":"markdown","dd091f0d":"markdown","e0e1e39f":"markdown","b08c6e5d":"markdown","e9213006":"markdown","1ad961b9":"markdown","066eafc9":"markdown","c80eedb4":"markdown","c9fed0a3":"markdown","08e10bde":"markdown","c20547d8":"markdown","d6f343e8":"markdown"},"source":{"7d43035f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","014b56c1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3d899852":"dataset = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","0d51d1e3":"dataset","fae0e08e":"dataset.info()","fc98edc4":"dataset.isnull().sum()","8a163fc4":"dataset.bmi.replace(to_replace=np.nan, value=dataset.bmi.mean(), inplace=True)","aaf01c2f":"dataset.isnull().sum()","b86e2b42":"dataset.describe()","f2966921":"dataset.corr()","e7116788":"# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","7a5a2ce4":"print(dataset.gender.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"gender\")\nplt.show()","53028107":"print(dataset.hypertension.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"hypertension\")\nplt.show()","f57733d3":"print(dataset.ever_married.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"ever_married\")\nplt.show()","ec7a202b":"print(dataset.work_type.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"work_type\")\nplt.show()","84b88e98":"print(dataset.Residence_type.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"Residence_type\")\nplt.show()","f5457cf6":"print(dataset.smoking_status.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"smoking_status\")\nax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\nplt.tight_layout()\nplt.show()","dd6c87aa":"print(dataset.stroke.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"stroke\")\nplt.show()","38b0334b":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.avg_glucose_level, color=\"green\", label=\"avg_glucose_level\", kde= True)\nplt.legend()","2da0f00e":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.bmi, color=\"orange\", label=\"bmi\", kde= True)\nplt.legend()","ada87aae":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"bmi\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"bmi\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by BMI', fontsize=15)\nplt.xlim([10,100])\nplt.show()","b85e41ca":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"avg_glucose_level\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"avg_glucose_level\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by Avg. Glucose Level', fontsize=15)\nplt.xlim([30,330])\nplt.show()","20c5938e":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"age\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"age\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by Age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","f3d6f009":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=dataset, x=\"age\", y=\"bmi\", hue='gender')\ngraph.axhline(y= 25, linewidth=4, color='r', linestyle= '--')\nplt.show()","38566741":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=dataset, x=\"age\", y=\"avg_glucose_level\", hue='gender')\ngraph.axhline(y= 150, linewidth=4, color='r', linestyle= '--')\nplt.show()","b61e60a0":"plt.figure(figsize=(13,13))\nsns.set_theme(style=\"darkgrid\")\nplt.subplot(2,3,1)\nsns.violinplot(x = 'gender', y = 'stroke', data = dataset)\nplt.subplot(2,3,2)\nsns.violinplot(x = 'hypertension', y = 'stroke', data = dataset)\nplt.subplot(2,3,3)\nsns.violinplot(x = 'heart_disease', y = 'stroke', data = dataset)\nplt.subplot(2,3,4)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = dataset)\nplt.subplot(2,3,5)\nsns.violinplot(x = 'work_type', y = 'stroke', data = dataset)\nplt.xticks(fontsize=9, rotation=45)\nplt.subplot(2,3,6)\nsns.violinplot(x = 'Residence_type', y = 'stroke', data = dataset)\nplt.show()","406b28ad":"fig = plt.figure(figsize=(10,10))\nsns.pairplot(dataset)\nplt.show()","4cda32bd":"x = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values","aeb4d3e5":"x","50b58279":"y","7f880481":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')\nx = np.array(ct.fit_transform(x))","39de5779":"x","2d5a9719":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:, 15] = le.fit_transform(x[:, 15])\nx[:, 16] = le.fit_transform(x[:, 16])","cf30d8c9":"x","3e72435c":"print('Shape of X: ', x.shape)\nprint('Shape of Y: ', y.shape)","06bc1dc2":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","2afdf3c3":"print(\"Number transactions x_train dataset: \", x_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_test dataset: \", x_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","94e91b26":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","d5bb396b":"from imblearn.over_sampling import SMOTE","7293aa4c":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nx_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","a90f1430":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","237379c7":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","610e8163":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['GaussianNB', GaussianNB()])\nmodels.append(['BernoulliNB', BernoulliNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier(eval_metric= 'error')])\n\nlst_1= []\n\nfor m in range(len(models)):\n    lst_2= []\n    model = models[m][1]\n    model.fit(x_train_res, y_train_res)\n    y_pred = model.predict(x_test)\n    cm = confusion_matrix(y_test, y_pred)  #Confusion Matrix\n    accuracies = cross_val_score(estimator = model, X = x_train_res, y = y_train_res, cv = 10)   #K-Fold Validation\n    roc = roc_auc_score(y_test, y_pred)  #ROC AUC Score\n    precision = precision_score(y_test, y_pred)  #Precision Score\n    recall = recall_score(y_test, y_pred)  #Recall Score\n    f1 = f1_score(y_test, y_pred)  #F1 Score\n    print(models[m][0],':')\n    print(cm)\n    print('Accuracy Score: ',accuracy_score(y_test, y_pred))\n    print('')\n    print(\"K-Fold Validation Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    print('')\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n    print('')\n    print('ROC AUC Score: {:.2f}'.format(roc))\n    print('')\n    print('Precision: {:.2f}'.format(precision))\n    print('')\n    print('Recall: {:.2f}'.format(recall))\n    print('')\n    print('F1: {:.2f}'.format(f1))\n    print('-----------------------------------')\n    print('')\n    lst_2.append(models[m][0])\n    lst_2.append((accuracy_score(y_test, y_pred))*100) \n    lst_2.append(accuracies.mean()*100)\n    lst_2.append(accuracies.std()*100)\n    lst_2.append(roc)\n    lst_2.append(precision)\n    lst_2.append(recall)\n    lst_2.append(f1)\n    lst_1.append(lst_2)","d642f4ba":"df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'K-Fold Mean Accuracy', 'Std. Deviation', 'ROC AUC', 'Precision', 'Recall', 'F1'])","2e1cf72d":"df.sort_values(by= ['Accuracy', 'K-Fold Mean Accuracy'], inplace= True, ascending= False)","c114d3b8":"df","3f86f949":"from sklearn.model_selection import GridSearchCV","2e559c75":"grid_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]), \n               (KNeighborsClassifier(),[{'n_neighbors':[5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}]), \n               (SVC(),[{'C':[0.25,0.5,0.75,1],'kernel':['linear', 'rbf'],'random_state':[0]}]), \n               (GaussianNB(),[{'var_smoothing': [1e-09]}]), \n               (BernoulliNB(), [{'alpha': [0.25, 0.5, 1]}]), \n               (DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]), \n               (RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]), \n              (XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}])]","41e06a8b":"for i,j in grid_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(x_train_res, y_train_res)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print('{}:\\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('Best Parameters : ',best_param)\n    print('')\n    print('----------------')\n    print('')","4397093b":"#Fitting RandomForest Model\nclassifier = RandomForestClassifier(criterion= 'gini', n_estimators= 100, random_state= 0)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","acaaf1f0":"#Fitting XGBClassifier Model\nclassifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","882fce97":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.regularizers import l2","bdbe385c":"# Builing the function\ndef ann_classifier():\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","a7fc6c25":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)","8652ee79":"# We are using 5 fold cross validation here\naccuracies = cross_val_score(estimator = ann, X = x_train_res, y = y_train_res, cv = 5)","83cb94cb":"# Checking the mean and standard deviation of the accuracies obtained\nmean = accuracies.mean()\nstd_deviation = accuracies.std()\nprint(\"Accuracy: {:.2f} %\".format(mean*100))\nprint(\"Standard Deviation: {:.2f} %\".format(std_deviation*100))","771d4615":"# Builing the function\ndef ann_classifier(optimizer = 'adam'):\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= optimizer, loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","541f129c":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)","99cda3a5":"# Using Grid Search CV to getting the best parameters\nparameters = {'batch_size': [25, 32],\n             'epochs': [50, 100, 150],\n             'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator = ann, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\n\ngrid_search.fit(x_train_res, y_train_res)","ba339c69":"best_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","3bd4c14e":"print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","2704c8d7":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\nann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])","6f62eade":"ann_history = ann.fit(x_train_res, y_train_res, batch_size= 25, epochs= 150, validation_split= 0.2)","cae1f283":"loss_train = ann_history.history['loss']\nloss_val = ann_history.history['val_loss']\nepochs = range(1,151)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9cd625e5":"loss_train = ann_history.history['accuracy']\nloss_val = ann_history.history['val_accuracy']\nepochs = range(1,151)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","4a43b4a3":"# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n',cm)\n\n# Calculate the Accuracy\naccuracy = accuracy_score(y_pred,y_test)\nprint('Accuracy: ',accuracy)\n\n#Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","da6b20f5":"*1. BMI below 19 can be seen as under weight. By looking at our graph, not lot of people are underweight.*\n\n*2. BMI between 19-25 can be seen as normal weight. We have relatively good amount of people who have normal weight.*\n\n*3. BMI higher than 25 can be seen as the person is likely overweight or obese. Our graph shows the density is higher around those BMI.*","c14fc3aa":"### Accuracy Graph <a id=\"12.4.2\"><\/a>","c9b75d84":"*Wrapping k-fold cross validation into keras model*","6020b448":"# Encoding <a id=\"5\"><\/a>","7abb97bf":"## Building the ANN <a id=\"12.1\"><\/a>","de35a87e":"### **Age vs Avg. Glucose Level** <a id=\"3.4.2\"><\/a>","bf0c975a":"Therefore, after the multiple visualizations of our and going through all the performance of the models. I tune the hyperparameters with the help of GridSearch to get models. After that, I came to conclusion that ***RandomForestClassifier*** is best model for this dataset.","46845831":"*The ratio can seen from above is around 2:1 for being ever married.*","a87b2afe":"## **Scatter Plot** <a id=\"3.4\"><\/a>","5cd5667c":"*A lot of people works in Private sector.*","99ac4a7b":"We are using **OneHotEncoder()** to encode the categorical columns: '**gender**', '**work_type**' and '**smoking_status**.","c571cbac":"*The **GridSearchCV** is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.*","6a468538":"### **Smoking Status** <a id=\"3.2.6\"><\/a>","34d15afc":"## **Violin Plot** <a id=\"3.5\"><\/a>","616fb7bb":"# Model Selection <a id=\"9\"><\/a>","4168d046":"### **Stroke** <a id=\"3.2.7\"><\/a>","82272982":"# **Data Preprocessing** <a id=\"4\"><\/a>","37127be3":"*Looking at output after **GridSearch**, we can determine that the **RandomForest** and **XGBoost** seems best fit for the model.*","95eef8c8":"### Loss Graph <a id=\"12.4.1\"><\/a>","9c703e2e":"# Stroke Prediction","2090a20e":"## **Categorical Encoding** <a id=\"5.1\"><\/a>","380f300c":"## Label Encoding <a id=\"5.2\"><\/a>","338724b5":"### **No Stroke vs Stroke by Age** <a id=\"3.3.5\"><\/a>","39d6cdec":"## **Heat Map Correlation** <a id=\"3.1\"><\/a>","89c98dc5":"*We only see **RandomForest** and **XGBoost** performance as they have high accuracy.*","558b58f5":"*But, first let's plot more to see how our data does in this state.*","13babf85":"# Conclusion <a id=\"13\"><\/a>","2a6e3e63":"*SMOTE - **Synthetic Minority Oversampling Technique** is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling.*","53f94b29":"## ANN Model after Tuning <a id=\"12.4\"><\/a>","fae53e03":"# **Importing Dataset** <a id=\"2\"><\/a>","40912622":"### **No Stroke vs Stroke by BMI** <a id=\"3.3.3\"><\/a>","cff9afb9":"*The residence type is same for people present in our dataset.*","36a8c05c":"## **Count Plot** <a id=\"3.2\"><\/a>","82467719":"### **BMI** <a id=\"3.3.2\"><\/a>","9a0a255e":"# Models after Tuning Hyperparameters <a id=\"11\"><\/a>","1e1e8d10":"*From above plot, we can see that there are lot of people having BMI above 25 are overweight and obese.*","e959a30b":"*From graph, it shows that the density of people having glucose level less than 100 suffered stroke more.*","8158ae5e":"## **Pair Plot** <a id=\"3.6\"><\/a>","e239a9fd":"### **Work Type** <a id=\"3.2.4\"><\/a>","c448975f":"### **Gender** <a id=\"3.2.1\"><\/a>","7bf9b5c3":"We are using **LabelEncoder()** to encode binary columns: '**ever_married**' and '**residence_type**'","ca446a90":"## **Distribution Plot** <a id=\"3.3\"><\/a>","30abcd57":"## Tuning the ANN <a id=\"12.3\"><\/a>","4af1fa4b":"*We use the Grid Search method for this task*","b699d5c5":"*Above, you can see the Females present in our dataset is higher than males.*","0922afc4":"### **Marriage Status** <a id=\"3.2.3\"><\/a>","166fb64b":"## Evaluating the ANN (Cross Validation) <a id=\"12.2\"><\/a>","e0fabdbc":"**After checking, as you can see there are no null values present in our column.**","616c2cd5":"# Handling Imbalance data using SMOTE <a id=\"8\"><\/a>","ffc70410":"### Let's Start!","73de0a2f":"# Keras ANN <a id=\"12\"><\/a>","63005dce":"### **Residence Type** <a id=\"3.2.5\"><\/a>","c2fec6be":"*From the graph, it shows that the density of overweight people who suffered a stroke is more.*","2460b024":"### Thank You!","8104d2ae":"*From above, it shows that less people are suffering from hypertension.*","0c142dda":"### **Hypertension** <a id=\"3.2.2\"><\/a>","bc9591da":"**There are null values present in 'bmi'.**","3d6051a2":"## RandomForest <a id=\"11.1\"><\/a>","018fbd24":"## XGBoost <a id=\"11.2\"><\/a>","448f115d":"*From above plot, we can see that people having glucose level above 150 are relatively less as compare one below. So, we can say that people above 150 might be suffering from diabetes.*","9c46819e":"*1. The normal glucose levels in adults should be around 80-140. Therefore, the density is higher around that range. So, we can see that the we have lot of people who have normal glucose level, so they are not suffering from diabetes.*\n\n*2. The range 140-200 can considered as pre-diabetes. But, looking at graph we can see that less people are in pre-diabetes zone.*\n\n*3. Anything above 200 can be seen that the person is suffering from diabetes. The density is more as compare to pre-diabetes by looking at the graph.*","6574c92b":"# **Data Visualization** <a id=\"3\"><\/a>","593a18a9":"# Tuning the Models <a id=\"10\"><\/a>","45236433":"# **Importing Libraries** <a id=\"1\"><\/a>","dd091f0d":"*From above dependent variable, we have really less peoples who suffered stroke. But, this also means that our dataset is imbalance. We likely have to use sampling techniques to make the data balance.*","e0e1e39f":"**We replaced null values of 'bmi' with mean in that column.**","b08c6e5d":"*From graph, it can be seen that the density of people having age above 50 suffered stroke more.*","e9213006":"### **No Stroke vs Stroke by Avg. Glucose Level** <a id=\"3.3.4\"><\/a>","1ad961b9":"# Feature Scaling <a id=\"7\"><\/a>","066eafc9":"# Splitting the dataset into the Training set and Test set <a id=\"6\"><\/a>","c80eedb4":"*A lot of people never smoked in their life. But, we also don't know the exact status of Unknowns in our dataset.*","c9fed0a3":"### **Avg. Glucose Level** <a id=\"3.3.1\"><\/a>","08e10bde":"Table of Contents:\n1. [Importing Libraries](#1) <a href= \"1\"><\/a>\n2. [Importing Dataset](#2) <a href= \"2\"><\/a>\n3. [Data Visualization](#3) <a href= \"3\"><\/a> <br> \n    3.1. [Heat Map Correlation](#3.1) <a href= \"3.1\"><\/a> <br>\n    3.2. [Count Plot](#3.2) <a href= \"3.2\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Gender](#3.2.1) <a href= \"3.2.1\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Hypertension](#3.2.2) <a href= \"3.2.2\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [Marriage Status](#3.2.3) <a href= \"3.2.3\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [Work Type](#3.2.4) <a href= \"3.2.4\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [Residence Type](#3.2.5) <a href= \"3.2.5\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; f. [Smoking Status](#3.2.6) <a href= \"3.2,6\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; g. [Stroke](#3.2.7) <a href= \"3.2.7\"><\/a> <br>\n    3.3 [Distribution Plot](#3.3) <a href= \"3.3\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Avg. Glucose Level](#3.3.1) <a href= \"3.3.1\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [BMI](#3.3.2) <a href= \"3.3.2\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [No Stroke vs Stroke by BMI](#3.3.3) <a href= \"3.3.3\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [No Stroke vs Stroke by Avg. Glucose Level](#3.3.4) <a href= \"3.3.4\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [No Stroke vs Stroke by Age](#3.3.5) <a href= \"3.3.5\"><\/a> <br>\n    3.4 [Scatter Plot](#3.4) <a href= \"3.4\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Age vs BMI](#3.4.1) <a href= \"3.4.1\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Age vs Avg. Glucose Level](#3.4.2) <a href= \"3.4.2\"><\/a> <br>\n    3.5 [Cat Plot](#3.5) <a href= \"3.5\"><\/a> <br>\n    3.6 [Pair Plot](#3.6) <a href= \"3.6\"><\/a> <br>\n4. [Data Preprocessing](#4) <a href= \"4\"><\/a> <br>\n5. [Encoding](#5) <a href= \"5\"><\/a> <br>\n    5.1 [Categorical Encoding](#5.1) <a href= \"5.1\"><\/a> <br>\n    5.2 [Label Encoding](#5.2) <a href= \"5.2\"><\/a> <br>\n6. [Splitting the dataset into the Training set and Test set](#6) <a href= \"6\"><\/a> <br> \n7. [Feature Scaling](#7) <a href= \"7\"><\/a> <br>\n8. [Handling Imbalance data using SMOTE](#8) <a href= \"8\"><\/a> <br>\n9. [Model Selection](#9) <a href= \"9\"><\/a> <br>\n10. [Tuning the Models](#10) <a href= \"10\"><\/a> <br>\n11. [Models after Tuning Hyperparameters](#11) <a href= \"11\"><\/a> <br>\n    11.1 [RandomForest](#11.1) <a href= \"11.1\"><\/a> <br>\n    11.2 [XGBoost](#11.2) <a href= \"11.2\"><\/a> <br>\n12. [Keras ANN](#12) <a href= \"12\"><\/a> <br>\n    12.1 [Building the ANN](#12.1) <a href= \"12.1\"><\/a> <br>\n    12.2 [Evaluating the ANN (Cross Validation)](#12.2) <a href= \"12.2\"><\/a> <br>\n    12.3 [Tuning the ANN](#12.3) <a href= \"12.3\"><\/a> <br>\n    12.4 [ANN Model after Tuning](#12.4) <a href= \"12.4\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Loss Graph](#12.4.1) <a href= \"12.4.1\"><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Accuracy Graph](#12.4.2) <a href= \"12.4.2\"><\/a> <br>\n13. [Conclusion](#13) <a href= \"13\"><\/a> <br>","c20547d8":"### **Age vs BMI** <a id=\"3.4.1\"><\/a>","d6f343e8":"*StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1.*"}}