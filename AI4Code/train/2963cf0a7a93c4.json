{"cell_type":{"00d4acad":"code","695164fa":"code","760b06d9":"code","4dd25c44":"code","7d7693c5":"code","b4329ddc":"code","a1f5b13d":"code","3e4e0c8d":"code","3679d608":"code","5de77fd3":"code","08fccabc":"code","692e7c1a":"code","5456c6fb":"code","97135be7":"code","30fc4800":"code","e54666b7":"code","096eed76":"code","78c5b052":"code","028329cf":"code","afe61dc7":"code","f49f8f2f":"code","75e24b68":"code","179c98cd":"code","b284fd35":"code","91361030":"code","d73d5304":"code","77ca239d":"code","0ed431fc":"code","71417a25":"code","145cd11e":"code","55fb01cf":"code","e744917b":"code","abf3c0c3":"code","011fd30f":"code","a95eae2d":"code","cb090f99":"code","da503dec":"code","fc0a5419":"code","6e694f56":"code","31c9bf02":"code","164c2a42":"code","e687c098":"code","b43e6964":"code","77425174":"code","fa9435f3":"code","328ac52c":"code","8b861f31":"code","4a9b6749":"code","10612f0c":"code","8b361622":"code","eb9e8795":"code","7666382e":"code","3709113a":"code","09521a88":"code","faf380d1":"code","2959252a":"code","0b7530dd":"code","9564bb57":"code","eab5ce40":"code","ca48b706":"code","97a92f33":"code","7a18c024":"code","f4a20f12":"markdown","a3ba6b7e":"markdown","b998e381":"markdown","6c0ef2f4":"markdown","36b33d71":"markdown","ee911ddf":"markdown","1dc0f532":"markdown","dda3eeba":"markdown","7ee137e6":"markdown","767ae880":"markdown","5ba1553b":"markdown","8162b4e1":"markdown","164fa938":"markdown","6963af78":"markdown","7bc71080":"markdown","ed995ea7":"markdown","a2a15978":"markdown","19d42cbc":"markdown","469c856d":"markdown","29d482ed":"markdown","f2892472":"markdown","491bd2e1":"markdown","ca9f94dd":"markdown","0eb899df":"markdown","1236e093":"markdown","bb8e408f":"markdown","2c6f202e":"markdown","01f115b8":"markdown","a4144acf":"markdown","05148e4d":"markdown"},"source":{"00d4acad":"# Start of Google Colab Import related codes\n# Keep these lines commented out in Local Drive and in Kaggle\n\n# from google.colab import drive\n# drive.mount('\/content\/drive')\n# import pandas as pd\n# import os\n# # Navigate into Drive where you want to store your Kaggle data\n# os.chdir('\/content\/drive\/MyDrive\/Kaggle_Datasets')\n\n# !pip install kaggle\n\n# The private folder in G-Drive where my Kaggle API Token is saved - kaggle.json\n# os.environ['KAGGLE_CONFIG_DIR']='\/content\/drive\/MyDrive\/Kaggle_Datasets'\n\n# this is the copied API command, the data will download to the current directory\n# !kaggle competitions download -c donorschoose-application-screening\n\n# End of Google Colab Import related codes\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom io import StringIO\nimport requests\nimport pickle\nfrom tqdm import tqdm\nimport os\n# from plotly import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\nfrom collections import Counter\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nfrom collections import Counter\nfrom wordcloud import WordCloud\n","695164fa":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/donorschooseorg-application-screening\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","760b06d9":"# Start of Google Colab related Imports from G-Drive\n# Keep this commented out in Local Drive and in Kaggle\n# train_project_data_original = pd.read_csv(\".\/train.zip\")\n# test_project_data_original = pd.read_csv('.\/test.zip')\n# resource_data = pd.read_csv('.\/resources.zip')\n# End of Google Colab related Imports from G-Drive\n\n# Start of Deepnote related Imports\n# test_project_data_original = pd.read_csv('\/work\/test.zip')\n# train_project_data_original = pd.read_csv ('\/work\/train.zip')\n# resource_data = pd.read_csv('\/work\/resources.zip')\n# End of Deepnote related Imports\n\n# Imports for both Kaggle and Local Machine\ntrain_project_data_original = pd.read_csv(\"..\/input\/donorschooseorg-application-screening\/train.csv\")\ntest_project_data_original = pd.read_csv('..\/input\/donorschooseorg-application-screening\/test.csv')\nresource_data = pd.read_csv('..\/input\/donorschooseorg-application-screening\/resources.csv')\n\n# Reading smaller set of data for doing just experimentation\n# train_project_data_original = pd.read_csv(\"..\/input\/donorschoose-application-screening\/train.zip\", nrows=200)\n# test_project_data_original = pd.read_csv('..\/input\/donorschoose-application-screening\/test.zip', nrows=200)\n# resource_data = pd.read_csv('..\/input\/donorschoose-application-screening\/resources.zip', nrows=200)\n\nprint('Column names from train_project_data_original is : ', train_project_data_original.columns )\ntrain_project_data_original.head()","4dd25c44":"print('Shape of train_project_data_original: ', train_project_data_original.shape)\nprint('Shape of test_project_data_original: ', test_project_data_original.shape)\n# Test data will NOT have any column for 'project_is_approved'\n","7d7693c5":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n","b4329ddc":"train_project_data_original['project_grade_category'].value_counts()","a1f5b13d":"\n# Removing all the spaces, replace the '-' with '_' and convert all the letters to small\n\ntrain_project_data_original['project_grade_category'] = train_project_data_original['project_grade_category'].str.replace(' ','_').str.replace('-', '_')\ntrain_project_data_original['project_grade_category'] = train_project_data_original['project_grade_category'].str.lower()\ntrain_project_data_original['project_grade_category'].value_counts()","3e4e0c8d":"# Same above text-Preprocessing for test dataset -\ntest_project_data_original['project_grade_category'] = test_project_data_original['project_grade_category'].str.replace(' ','_').str.replace('-', '_')\ntest_project_data_original['project_grade_category'] = test_project_data_original['project_grade_category'].str.lower()\ntest_project_data_original['project_grade_category'].value_counts()","3679d608":"train_project_data_original['project_subject_categories'].value_counts()","5de77fd3":"# remove spaces, 'the'\n# replace '&' with '_', and ',' with '_'\ntrain_project_data_original['project_subject_categories'] = train_project_data_original['project_subject_categories'].str.replace(' The ','').str.replace(' ','').str.replace('&','_').str.replace(',','_')\ntrain_project_data_original['project_subject_categories'] = train_project_data_original['project_subject_categories'].str.lower()\ntrain_project_data_original['project_subject_categories'].value_counts()","08fccabc":"# Same above text-Preprocessing for test dataset - project_subject_categories\ntest_project_data_original['project_subject_categories'] = test_project_data_original['project_subject_categories'].str.replace(' The ','').str.replace(' ','').str.replace('&','_').str.replace(',','_')\ntest_project_data_original['project_subject_categories'] = test_project_data_original['project_subject_categories'].str.lower()\ntest_project_data_original['project_subject_categories'].value_counts()\n","692e7c1a":"train_project_data_original['project_subject_subcategories'].value_counts()","5456c6fb":"# Same kind of cleaning as we did in 'project_subject_categories'\n\ntrain_project_data_original['project_subject_subcategories'] = train_project_data_original['project_subject_subcategories'].str.replace(' The ','').str.replace(' ','').str.replace('&','_').str.replace(',','_').str.lower()\n\ntrain_project_data_original['project_subject_subcategories'].value_counts()","97135be7":"# Same above text-Preprocessing for test dataset: project_subject_subcategories\ntest_project_data_original['project_subject_subcategories'] = test_project_data_original['project_subject_subcategories'].str.replace(' The ','').str.replace(' ','').str.replace('&','_').str.replace(',','_').str.lower()\n\ntest_project_data_original['project_subject_subcategories'].value_counts()","30fc4800":"train_project_data_original['teacher_prefix'].value_counts()","e54666b7":"# check if we have any nan values are there\nprint(train_project_data_original['teacher_prefix'].isnull().values.any())\nprint(\"number of nan values\",train_project_data_original['teacher_prefix'].isnull().values.sum())\n\n# If there's indeed any \"NA\" values then fill them up\ntrain_project_data_original['teacher_prefix']=train_project_data_original['teacher_prefix'].fillna('Mrs.')\n\ntrain_project_data_original['teacher_prefix'].value_counts()","096eed76":"# Remove '.'\n# convert all the chars to small\n\ntrain_project_data_original['teacher_prefix'] = train_project_data_original['teacher_prefix'].str.replace('.','')\ntrain_project_data_original['teacher_prefix'] = train_project_data_original['teacher_prefix'].str.lower()\ntrain_project_data_original['teacher_prefix'].value_counts()","78c5b052":"# Same above text-Preprocessing for test dataset: 'teacher_prefix'\ntest_project_data_original['teacher_prefix'] = test_project_data_original['teacher_prefix'].str.replace('.','')\ntest_project_data_original['teacher_prefix'] = test_project_data_original['teacher_prefix'].str.lower()\ntest_project_data_original['teacher_prefix'].value_counts()","028329cf":"train_project_data_original['school_state'].value_counts()\n","afe61dc7":"# convert all of them into small letters\ntrain_project_data_original['school_state'] = train_project_data_original['school_state'].str.lower()\ntrain_project_data_original['school_state'].value_counts()","f49f8f2f":"# Same above text-Preprocessing for test dataset: 'school_state'\ntest_project_data_original['school_state'] = test_project_data_original['school_state'].str.lower()\ntest_project_data_original['school_state'].value_counts()","75e24b68":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef remove_eng_lang_contraction(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# For few other ways to remove contraction check below\n# https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\n","179c98cd":"train_project_data_original['project_title'].head(5)","b284fd35":"print(\"printing some random reviews before Pre-Processing \")\nprint(3, train_project_data_original['project_title'].values[3])\nprint(15, train_project_data_original['project_title'].values[15])\nprint(10, train_project_data_original['project_title'].values[10])","91361030":"# Now applying all the above pre-processing steps and functions\n# Expanding English language contractions with the function we defined above\n# Removing all stopwords\n\nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for current_sentenceence in tqdm(text_data):\n\n        # Expanding English language contractions\n        current_sentence = remove_eng_lang_contraction(current_sentenceence)\n        current_sentence = current_sentence.replace('\\\\r', ' ')\n        current_sentence = current_sentence.replace('\\\\n', ' ')\n        current_sentence = current_sentence.replace('\\\\\"', ' ')\n        current_sentence = re.sub('[^A-Za-z0-9]+', ' ', current_sentence)\n\n        # Removing all stopwords\n        # https:\/\/gist.github.com\/sebleier\/554280\n        current_sentence = ' '.join(e for e in current_sentence.split() if e.lower() not in stopwords)\n        preprocessed_text.append(current_sentence.lower().strip())\n    return preprocessed_text","d73d5304":"preprocessed_titles_train = preprocess_text(train_project_data_original['project_title'].values)\n\ntrain_project_data_original['project_title'] = preprocessed_titles_train\n\nprint(\"printing few random reviews AFTER Pre-Processing \")\nprint(3, train_project_data_original['project_title'].values[3])\nprint(15, train_project_data_original['project_title'].values[15])\nprint(10, train_project_data_original['project_title'].values[10])","77ca239d":"# Same above text-Preprocessing for test dataset: 'project_title'\n\npreprocessed_titles_test = preprocess_text(test_project_data_original['project_title'].values)\n\ntest_project_data_original['project_title'] = preprocessed_titles_test\n\nprint(\"printing few random reviews AFTER Pre-Processing \")\nprint(3, test_project_data_original['project_title'].values[3])\nprint(15, test_project_data_original['project_title'].values[15])\nprint(10, test_project_data_original['project_title'].values[10])","0ed431fc":"# First combine all the 4 essay columns into a single one:\n#  'project_essay_1',  'project_essay_2',   'project_essay_3',   'project_essay_4',\ntrain_project_data_original[\"essay\"] = train_project_data_original[\"project_essay_1\"].map(str) +\\\n                        train_project_data_original[\"project_essay_2\"].map(str) + \\\n                        train_project_data_original[\"project_essay_3\"].map(str) + \\\n                        train_project_data_original[\"project_essay_4\"].map(str)\n\ntest_project_data_original[\"essay\"] = test_project_data_original[\"project_essay_1\"].map(str) +\\\n                        test_project_data_original[\"project_essay_2\"].map(str) + \\\n                        test_project_data_original[\"project_essay_3\"].map(str) + \\\n                        test_project_data_original[\"project_essay_4\"].map(str)\n\n\nprint(\"Checking out few random essay BEFORE Pre-Processing \")\nprint(9, train_project_data_original['essay'].values[9])\nprint('*'*50)\nprint(34, train_project_data_original['essay'].values[34])\nprint('*'*50)\nprint(147, train_project_data_original['essay'].values[147])","71417a25":"preprocessed_essays_train = preprocess_text(train_project_data_original['essay'].values)\n\ntrain_project_data_original['essay'] = preprocessed_essays_train\n\nprint(\"Checking out few random essay AFTER Pre-Processing \")\nprint(9, train_project_data_original['essay'].values[9])\nprint('*'*50)\nprint(34, train_project_data_original['essay'].values[34])\nprint('*'*50)\nprint(147, train_project_data_original['essay'].values[147])\n\nprint('*'*50)","145cd11e":"# Same above text-Preprocessing for test dataset: 'essay'\npreprocessed_essays_test = preprocess_text(test_project_data_original['essay'].values)\ntest_project_data_original['essay'] = preprocessed_essays_test","55fb01cf":"print('Shape of Resource datesset: ', resource_data.shape) #15.42mn rows","e744917b":"price_data_from_resource = resource_data.groupby('id').agg({'price': 'sum', 'quantity': 'sum'}).reset_index()\nprice_data_from_resource.head(2)\n","abf3c0c3":"# Multiply two columns and then create new column with values\nprice_data_from_resource['resource_cost'] = price_data_from_resource['price'] * price_data_from_resource['quantity']\nprice_data_from_resource.head(2)\n","011fd30f":"price_data_from_resource = price_data_from_resource.drop(['price', 'quantity'], axis=1)\nprice_data_from_resource.head(2)","a95eae2d":"train_project_data_original = pd.merge(train_project_data_original, price_data_from_resource, on='id', how='left')\ntest_project_data_original = pd.merge(test_project_data_original, price_data_from_resource, on='id', how='left')\ntrain_project_data_original['resource_cost'].head()\nprint(train_project_data_original.columns)\nprint(test_project_data_original.columns)","cb090f99":"train_project_data_original['resource_cost'].head()\n","da503dec":"test_project_data_original['resource_cost'].head()","fc0a5419":"word_cloud_for_project_resource_summary = ' '.join(train_project_data_original.loc[train_project_data_original['project_is_approved'] == 1, 'project_resource_summary'].values)\n\nwordcloud = WordCloud(max_font_size=None, stopwords=stopwords, background_color='white',\n                      width=1200, height=1000).generate(word_cloud_for_project_resource_summary)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words for approved projects')\nplt.axis(\"off\")\nplt.show()","6e694f56":"word_cloud_for_project_resource_summary = ' '.join(train_project_data_original.loc[train_project_data_original['project_is_approved'] == 0, 'project_resource_summary'].values)\n\nwordcloud = WordCloud(max_font_size=None, stopwords=stopwords, background_color='white',\n                      width=1200, height=1000).generate(word_cloud_for_project_resource_summary)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words for approved projects')\nplt.axis(\"off\")\nplt.show()","31c9bf02":"cols_to_drop_from = [\n    'id',\n    'teacher_id',\n    'project_submitted_datetime',\n    'project_resource_summary'\n]\n\ntrain_df_pre_processed_original = train_project_data_original.drop(cols_to_drop_from, axis=1)\ntest_df_pre_processed_original = test_project_data_original.drop(cols_to_drop_from, axis=1)\n\n# test_project_data_original.drop(cols_to_drop)\nprint('after dropping ', train_df_pre_processed_original.columns)\n# train_project_data_original.to_csv('preprocessed-train.csv')\n# test_project_data_original.to_csv('preprocessed-test.csv')","164c2a42":"train_df_pre_processed_original.head(1)","e687c098":"print('All Features names in preprocessed-train.csv ', train_df_pre_processed_original.columns )\nprint('All Features names in preprocessed-test.csv ', test_df_pre_processed_original.columns )","b43e6964":"y_train_pre_processed_original = train_df_pre_processed_original['project_is_approved'].values\nx_train_pre_processed_original = train_df_pre_processed_original.drop(['project_is_approved'], axis=1)\n\nX_donor_choose_train, X_donor_choose_validation, y_donor_choose_train, y_donor_choose_validation = train_test_split(x_train_pre_processed_original, y_train_pre_processed_original, test_size=0.33, stratify=y_train_pre_processed_original)\nprint('X_donor_choose_train shape ', X_donor_choose_train.shape)\n\nX_donor_choose_train.head(1)","77425174":"print('x_train_pre_processed_original name of all columns values ', x_train_pre_processed_original.columns)","fa9435f3":"vectorizer_clean_categories = CountVectorizer(lowercase=False, binary=True)\n\ntrain_vectorized_ohe_clean_categories = vectorizer_clean_categories.fit_transform(X_donor_choose_train['project_subject_categories'].values)\nprint(train_vectorized_ohe_clean_categories.shape)\n\nvalidation_vectorized_ohe_clean_categories = vectorizer_clean_categories.transform(X_donor_choose_validation['project_subject_categories'].values)\nprint(validation_vectorized_ohe_clean_categories.shape)\n\ntest_df_vectorized_ohe_clean_categories = vectorizer_clean_categories.transform(test_df_pre_processed_original['project_subject_categories'].values)\nprint(test_df_vectorized_ohe_clean_categories.shape)","328ac52c":"vectorizer_clean_subcategories = CountVectorizer(lowercase=False, binary=True)\n\ntrain_vectorized_ohe_clean_subcategories = vectorizer_clean_subcategories.fit_transform(X_donor_choose_train['project_subject_subcategories'].values)\nprint(train_vectorized_ohe_clean_subcategories.shape)\n\nvalidation_vectorized_ohe_clean_subcategories = vectorizer_clean_subcategories.transform(X_donor_choose_validation['project_subject_subcategories'].values)\nprint(validation_vectorized_ohe_clean_subcategories.shape)\n\ntest_df_vectorized_ohe_clean_subcategories = vectorizer_clean_subcategories.transform(test_df_pre_processed_original['project_subject_subcategories'].values)\nprint(test_df_vectorized_ohe_clean_subcategories.shape)\n","8b861f31":"vectorizer_teacher_prefix = CountVectorizer(lowercase=False, binary=True)\n\ntrain_vectorized_ohe_teacher_prefix = vectorizer_teacher_prefix.fit_transform(X_donor_choose_train['teacher_prefix'].values)\nprint(train_vectorized_ohe_teacher_prefix.shape)\n\nvalidation_vectorized_ohe_teacher_prefix = vectorizer_teacher_prefix.transform(X_donor_choose_validation['teacher_prefix'].values)\nprint(validation_vectorized_ohe_teacher_prefix.shape)\n\ntest_df_pre_processed_original['teacher_prefix'] = test_df_pre_processed_original['teacher_prefix'].fillna('None')\n\ntest_df_vectorized_ohe_teacher_prefix = vectorizer_teacher_prefix.transform(test_df_pre_processed_original['teacher_prefix'].values)\nprint(test_df_vectorized_ohe_teacher_prefix.shape)\n","4a9b6749":"vectorizer_project_grade_category = CountVectorizer(lowercase=False, binary=True)\n\ntrain_vectorized_ohe_project_grade_category = vectorizer_project_grade_category.fit_transform(X_donor_choose_train['project_grade_category'].values)\nprint(train_vectorized_ohe_project_grade_category.shape)\n\nvalidation_vectorized_ohe_project_grade_category = vectorizer_project_grade_category.transform(X_donor_choose_validation['project_grade_category'].values)\nprint(validation_vectorized_ohe_project_grade_category.shape)\n\ntest_df_vectorized_ohe_project_grade_category = vectorizer_project_grade_category.transform(test_df_pre_processed_original['project_grade_category'].values)\nprint(test_df_vectorized_ohe_project_grade_category.shape)\n","10612f0c":"vectorizer_school_state = CountVectorizer(lowercase=False, binary=True)\n\ntrain_vectorized_ohe_school_state = vectorizer_school_state.fit_transform(X_donor_choose_train['school_state'].values)\nprint(train_vectorized_ohe_school_state.shape)\n\nvalidation_vectorized_ohe_school_state = vectorizer_school_state.transform(X_donor_choose_validation['school_state'].values)\nprint(validation_vectorized_ohe_school_state.shape)\n\ntest_df_vectorized_ohe_school_state = vectorizer_school_state.transform(test_df_pre_processed_original['school_state'].values)\nprint(test_df_vectorized_ohe_school_state.shape)\n","8b361622":"# ****** THE CODE OF THIS CELL IS ONLY FOR SHOWING HOW TO - Correct way to implement Normalization of Numerical Features. AND CODES IN THIS CELL ARE NOT USED ANYWHERE OUTSIDE OF THIS CELL *****\n\nnormalizer = Normalizer()\n\n\"\"\" printing below just to inspect the shape so I know in which form I need to apply the Normalizer()\n\nPer the above note, we can see that shape_a produces a 2-d column vector, and given Normalizer() normalizes on each sample(row)\nSo shape_a normalized will produce column vector with each value being 1\n\n\"\"\"\n\nresource_cost_arr = X_donor_choose_train['resource_cost'].values\n# print( 'resource_cost_arr ',  resource_cost_arr)\n\nshape_a = X_donor_choose_train['resource_cost'].values.reshape(-1, 1)\nshape_b = X_donor_choose_train['resource_cost'].values.reshape(1, -1)\n# print('shape_a ', shape_a )\n# print('shape_b ', shape_b )\n\n# Below is WRONG reshape and should NOT be done\n# Below 2 variables will produce resource_cost column vectors with each value being 1 and so is worthless\ntrain_normalized_resource_cost_wrong = normalizer.transform(shape_a)\ntrain_normalized_resource_cost_correct = normalizer.transform(shape_b)\n\n\nprint('train_normalized_resource_cost_wrong is : ', train_normalized_resource_cost_wrong[0:10])\n# print('train_normalized_resource_cost_correct is : ', train_normalized_resource_cost_correct)\n# Above line will print like below, i.e. a single column vector\n'''[[1.83906133e-02 3.96946770e-03 1.12896957e-03 3.67034849e-03\n  1.48897540e-01 6.30706441e-04 1.41765263e-02 2.00495074e-03\n  4.53619601e-03 6.88785883e-03 1.37011017e-03 1.73935829e-03\n  5.91764982e-03 1.02206206e-02 4.50035010e-03 3.96785438e-03\n  3.93200848e-03 8.99717107e-03 6.63265212e-03 1.50744385e-03\n  1.91811391e-02 3.99129791e-03 6.66461598e-03 4.00891837e-02\n  ...\n  ]]\n'''","eb9e8795":"# Our first Numerical feature - 'resource_cost'\nnormalizer = Normalizer()\n\n# As explainEd above first I will reshape(1, -1)\nnormalizer.fit(X_donor_choose_train['resource_cost'].values.reshape(1, -1))\n\ntrain_normalized_resource_cost = normalizer.transform(X_donor_choose_train['resource_cost'].values.reshape(1, -1))\n\nvalidation_normalized_resource_cost = normalizer.transform(X_donor_choose_validation['resource_cost'].values.reshape(1, -1))\n\ntest_df_normalized_resource_cost = normalizer.transform(test_df_pre_processed_original['resource_cost'].values.reshape(1, -1))\n\n# After normalization reshape again to (-1, 1) i.e. this time unknown rows (i.e. leaving it to Numpy to decide), and specifying I need 1 column\ntrain_normalized_resource_cost = train_normalized_resource_cost.reshape(-1, 1)\nprint(train_normalized_resource_cost.shape)\n\nvalidation_normalized_resource_cost = validation_normalized_resource_cost.reshape(-1, 1)\nprint(validation_normalized_resource_cost.shape)\n\ntest_df_normalized_resource_cost = test_df_normalized_resource_cost.reshape(-1, 1)\nprint(test_df_normalized_resource_cost.shape)\n","7666382e":"# Second Numerical feature - 'teacher_number_of_previously_posted_projects'\nnormalizer = Normalizer()\n\nnormalizer.fit(X_donor_choose_train['teacher_number_of_previously_posted_projects'].values.reshape(1, -1))\n\ntrain_normalized_teacher_number_of_previously_posted_projects = normalizer.transform(X_donor_choose_train['teacher_number_of_previously_posted_projects'].values.reshape(1, -1))\n\nvalidation_normalized_teacher_number_of_previously_posted_projects = normalizer.transform(X_donor_choose_validation['teacher_number_of_previously_posted_projects'].values.reshape(1, -1))\n\ntest_df_normalized_teacher_number_of_previously_posted_projects = normalizer.transform(test_df_pre_processed_original['teacher_number_of_previously_posted_projects'].values.reshape(1, -1))\n\n# After normalization reshape again to (-1, 1) i.e. this time unknown rows (i.e. leaving it to Numpy to decide), and specifying I need 1 column\ntrain_normalized_teacher_number_of_previously_posted_projects = train_normalized_teacher_number_of_previously_posted_projects.reshape(-1, 1)\nprint(train_normalized_teacher_number_of_previously_posted_projects.shape)\n\nvalidation_normalized_teacher_number_of_previously_posted_projects = validation_normalized_teacher_number_of_previously_posted_projects.reshape(-1, 1)\nprint(validation_normalized_teacher_number_of_previously_posted_projects.shape)\n\ntest_df_normalized_teacher_number_of_previously_posted_projects = test_df_normalized_teacher_number_of_previously_posted_projects.reshape(-1, 1)\nprint(test_df_normalized_teacher_number_of_previously_posted_projects.shape)\n","3709113a":"vectorizer_essay_bow = CountVectorizer(min_df=10)\n\ntrain_vectorized_bow_essay = vectorizer_essay_bow.fit_transform(X_donor_choose_train['essay'])\nprint(train_vectorized_bow_essay.shape)\n\nvalidation_vectorized_bow_essay = vectorizer_essay_bow.transform(X_donor_choose_validation['essay'])\nprint(validation_vectorized_bow_essay.shape)\n\ntest_df_vectorized_bow_essay = vectorizer_essay_bow.transform(test_df_pre_processed_original['essay'])\nprint(test_df_vectorized_bow_essay.shape)\n","09521a88":"X_train_hstacked_all_bow_features_vectorized = hstack((train_vectorized_ohe_clean_categories, train_vectorized_ohe_clean_subcategories, train_vectorized_ohe_teacher_prefix, train_vectorized_ohe_project_grade_category, train_vectorized_ohe_school_state, train_normalized_resource_cost, train_normalized_teacher_number_of_previously_posted_projects, train_vectorized_bow_essay))\n\nprint('X_train_hstacked_all_bow_features_vectorized.shape is ', X_train_hstacked_all_bow_features_vectorized.shape)\n\nX_validation_hstacked_all_bow_features_vectorized = hstack((validation_vectorized_ohe_clean_categories, validation_vectorized_ohe_clean_subcategories, validation_vectorized_ohe_teacher_prefix, validation_vectorized_ohe_project_grade_category, validation_vectorized_ohe_school_state, validation_normalized_resource_cost, validation_normalized_teacher_number_of_previously_posted_projects, validation_vectorized_bow_essay))\n\nprint('X_validation_hstacked_all_bow_features_vectorized.shape is ', X_validation_hstacked_all_bow_features_vectorized.shape)\n\ntest_df_hstacked_all_bow_features_vectorized = hstack((test_df_vectorized_ohe_clean_categories, test_df_vectorized_ohe_clean_subcategories, test_df_vectorized_ohe_teacher_prefix, test_df_vectorized_ohe_project_grade_category, test_df_vectorized_ohe_school_state, test_df_normalized_resource_cost, test_df_normalized_teacher_number_of_previously_posted_projects, test_df_vectorized_bow_essay))\n\nprint('test_df_hstacked_all_bow_features_vectorized.shape is ', test_df_hstacked_all_bow_features_vectorized.shape)","faf380d1":"multinomial_nb_bow = MultinomialNB(class_prior=[0.5, 0.5], fit_prior=False)\n\n'''fit_prior bool, default=True\nWhether to learn class prior probabilities or not. If false, a uniform prior will be used.\nWhenever we initialize the 'class_prior' parameter to any value (other than None), then it is a good practice to initialize fit_prior = False.\n\n'''\n\nparameters = {'alpha':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\n\nclf = GridSearchCV(multinomial_nb_bow, parameters, cv=10, scoring='roc_auc', verbose=1, return_train_score=True)\n\nclf.fit(X_train_hstacked_all_bow_features_vectorized, y_donor_choose_train )\n\n# cv_results_dict\ntrain_auc_bow = clf.cv_results_['mean_train_score']\ntrain_auc_std_bow = clf.cv_results_['std_train_score']\n\ncv_auc_bow = clf.cv_results_['mean_test_score']\ncv_auc_std_bow = clf.cv_results_['std_test_score']\n\nbest_alpha_1_bow = clf.best_params_['alpha']\nbest_score_1_bow = clf.best_score_\n\nprint('Best Alpha BOW: ', best_alpha_1_bow )\nprint('Best Score BOW : ', best_score_1_bow)\n","2959252a":"alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nlog_of_alphas = []\n\nfor alpha in tqdm(alphas):\n  alpha_log = np.log10(alpha)\n  log_of_alphas.append(alpha_log)\n\nplt.figure(figsize=(10, 5))\nplt.plot(log_of_alphas, train_auc_bow, label='Train AUC Curve' )\n\n'''\n[mean - std, mean + std] => It creates a shaded area in between\nTaken from - https:\/\/stackoverflow.com\/a\/48803361\/1902852\n\nThe Axes.fill_between() function in axes module of matplotlib library is used to fill the area between two horizontal curves\n\nAnd pyplot.gca() Get the current Axes instance on the current figure matching the given keyword args, or create one.\n\n\"Current\" here means that it provides a handle to the last active axes. If there is no axes yet, an axes will be created. If you create two subplots, the subplot that is created last is the current one.\n\n'''\nplt.gca().fill_between(log_of_alphas, train_auc_bow - train_auc_std_bow, train_auc_bow + train_auc_std_bow, alpha=0.3, color='darkblue' )\n\n\n# similarly for CV_AUC\nplt.plot(log_of_alphas, cv_auc_bow, label='CV AUC Curve for BOW Set' )\nplt.gca().fill_between(log_of_alphas, cv_auc_bow - cv_auc_std_bow, cv_auc_bow+cv_auc_std_bow + cv_auc_std_bow, alpha=0.3, color='darkorange' )\n\n\nplt.scatter(log_of_alphas, train_auc_bow, label='Train AUC points-BOW' )\nplt.scatter(log_of_alphas, cv_auc_bow, label='CV AUC points-BOW' )\n\nplt.legend()\nplt.xlabel('Hyperparameter: Log of alpha for BOW Set')\nplt.ylabel('AUC-BOW ')\nplt.title('Log of Alpha-Hyperparameter with Train and CV AUC for BOW Set')\nplt.grid()\nplt.show()","0b7530dd":"naive_bayes_results_for_bow_with_best_alpha = MultinomialNB(alpha = best_alpha_1_bow, class_prior=[0.5, 0.5], fit_prior=False )\n\n'''fit_prior bool, default=True\nWhether to learn class prior probabilities or not. If false, a uniform prior will be used.\n\nWhenever we initialize the 'class_prior' parameter to any value (other than None), then it is a good practice to initialize fit_prior = False.\n'''\n\nnaive_bayes_results_for_bow_with_best_alpha.fit(X_train_hstacked_all_bow_features_vectorized, y_donor_choose_train)\n\n# Now instead of .best_param like previous implementation will be using .predict_proba\n# Using .predict_proba directly yields the same results as using .best_param to get the best hyper-parameter through\ny_predicted_for_bow_with_best_alpha_train = naive_bayes_results_for_bow_with_best_alpha.predict_proba(X_train_hstacked_all_bow_features_vectorized)[:, 1]\nprint('y_predicted_for_bow_with_best_alpha_train.shape is ', y_predicted_for_bow_with_best_alpha_train.shape)\n\ny_predicted_for_bow_with_best_alpha_validation = naive_bayes_results_for_bow_with_best_alpha.predict_proba(X_validation_hstacked_all_bow_features_vectorized)[:, 1]\nprint('y_predicted_for_bow_with_best_alpha_validation.shape is ', y_predicted_for_bow_with_best_alpha_validation.shape)\n","9564bb57":"fpr_train_bow, tpr_train_bow, thresholds_train_bow = roc_curve(y_donor_choose_train, y_predicted_for_bow_with_best_alpha_train )\n\nfpr_validation_bow, tpr_validation_bow, thresholds_validation_bow = roc_curve(y_donor_choose_validation, y_predicted_for_bow_with_best_alpha_validation )\n\nprint('fpr_train_bow: ', fpr_train_bow)\n\nax = plt.subplot()\n\nauc_bow_train = auc(fpr_train_bow, tpr_train_bow)\nauc_bow_validation = auc(fpr_validation_bow, tpr_validation_bow)\n\nax.plot(fpr_train_bow, tpr_train_bow, label='Train AUC ='+str(auc(fpr_train_bow, tpr_train_bow)))\nax.plot(fpr_validation_bow, tpr_validation_bow, label='Test AUC ='+str(auc(fpr_validation_bow, tpr_validation_bow)))\n\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('AUC')\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor('white')\nplt.show()\n","eab5ce40":"# using feature_log_prob_ to get the most important features\nprint('naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_ ', naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_)\n# Its basically, just the log probability of each word, given in a 2-d array structure\n# Note, feature_log_prob_ will give me the actual log probabilities of each features,\n# but I need the index position of those probabilities.\n# So, that with that same index position I can get the corresponding feature-names.\n\nprint('LEN of naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_ ', len(naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_[1]))\n\n# So, first, I will sort the result from feature_log_prob_ , and ONLY after that\n# get the corresponding positional-index of the top 20 features.\n# And then I can apply the same positional-index number to get the corresponding features names (which are the labels \/ words )\n# https:\/\/stackoverflow.com\/a\/50530697\/1902852\n\nprobabilities_sorted_of_all_negative_class = naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_[0, :].argsort()\n#class 0\n\npositive_class_sorted_proba = naive_bayes_results_for_bow_with_best_alpha.feature_log_prob_[1, :].argsort()\n#class 1\n\n'''numpy.argsort() function is used to perform an indirect sort along the given axis. It returns an array of indices of the same shape as arr that that would sort the array. Basically it gives me the sorted array indices'''\n\nprint(\"Shape of probabilities_sorted_of_all_negative_class \", probabilities_sorted_of_all_negative_class.shape)\nprint(\"Shape of positive_class_sorted_proba \", positive_class_sorted_proba.shape)\n\n# Now - Lets get all horizontally stacked features which were individually derived from CounterVectorizer earlier\n# Using itertools.chain() method to chain multiple arrays\/lists at once\n# https:\/\/stackoverflow.com\/a\/34665782\/1902852\nfrom itertools import chain\n\nvectorized_all_features_horizontally_stacked = list(chain(vectorizer_clean_categories.get_feature_names(), vectorizer_clean_subcategories.get_feature_names(), vectorizer_teacher_prefix.get_feature_names(),  vectorizer_project_grade_category.get_feature_names(), vectorizer_school_state.get_feature_names(), vectorizer_essay_bow.get_feature_names()))\n\n# After adding all the vectorized features (which includes both categorical and text features)\n# And now I also need to append the two left-over Numerical features which are\n# 'resource_cost' and 'teacher_number_of_previously_posted_projects'\nvectorized_all_features_horizontally_stacked.extend(['resource_cost', 'teacher_number_of_previously_posted_projects'])\n\n# The above extend() is equivalent to below append()\n# vectorized_all_features_horizontally_stacked.append('resource_cost')\n# vectorized_all_features_horizontally_stacked.append('teacher_number_of_previously_posted_projects' )\n\nprint('Length of vectorized_all_features_horizontally_stacked', ' ( should be ', X_train_hstacked_all_bow_features_vectorized.shape[1], ') : ' , len(vectorized_all_features_horizontally_stacked))\n\n# FINALLY THE TOP 20 FEATURES\ntop_20_negative_class_features_labels = np.take(vectorized_all_features_horizontally_stacked, probabilities_sorted_of_all_negative_class[-20: -1])\nprint('top_20_negative_class_features_labels ', top_20_negative_class_features_labels)\n\n\ntop_20_positive_class_features_labels = np.take(vectorized_all_features_horizontally_stacked, positive_class_sorted_proba[-20: -1])\nprint('top_20_positive_class_features_labels ', top_20_positive_class_features_labels)","ca48b706":"# We already have the variable \"naive_bayes_results_for_bow_with_best_alpha\" fitted as below earlier\n# naive_bayes_results_for_bow_with_best_alpha.fit(X_train_hstacked_all_bow_features_vectorized, y_donor_choose_train)\ny_predicted_for_bow_with_best_alpha_test_df = naive_bayes_results_for_bow_with_best_alpha.predict_proba(test_df_hstacked_all_bow_features_vectorized)[:, 1]\n\nprint('y_predicted_for_bow_with_best_alpha_test_df.shape is ', y_predicted_for_bow_with_best_alpha_test_df.shape)","97a92f33":"sample_sub = pd.read_csv(\"..\/input\/donorschooseorg-application-screening\/sample_submission.csv\")\n\n# y_predicted_for_bow_with_best_alpha_test_df","7a18c024":"submission_naive_bayes = pd.concat([sample_sub, pd.Series(y_predicted_for_bow_with_best_alpha_test_df , name='project_is_approved')] , axis=1 ).iloc[:,[0,2]]\nsubmission_naive_bayes.head()\n# submission_naive_bayes.to_csv('submission_naive_bayes.csv', index=False)\n","f4a20f12":"## Preprocessing Categorical Features: project_subject_subcategories","a3ba6b7e":"## 5. Merging (with hstack) all the above vectorized features that we created above\n\n#### You can use the scipy.sparse.hstack to concatenate sparse matrices with the same number of rows (horizontal concatenation):\n\n`hstack((X1, X2))`\n\n### We need to merge all the numerical vectors i.e categorical, text, numerical vectors, once for Naive Bayes on BOW and then for Naive Bayes on TFIDF\n\n## Merging all categorical, text, numerical vectors based on BOW","b998e381":"## From hereon, below all work is completely on the above Pre-Processed dataset (both train and test)\n\n#### Split the train data into Train and Validation Set, so I can do all the experimentation below on these sets itself,\n\nThis is specifically for applying GridSearchCV to get the best alpha to be applied finally on the original test dataset.\n","6c0ef2f4":"## 2. Pre-processing Categorical Features: project_grade_category","36b33d71":"\n## What is `feature_log_prob_` in the naive_bayes MultinomialNB()\n\nFrom Doc\n\nfeature_log_prob_ is and array of shape (n_classes, n_features) => Empirical log probability of features given a class, P(x_i|y).\n\n#### Models like logistic regression, or Naive Bayes algorithm, predict the probabilities of observing some outcomes. In standard binary regression scenario the models give you probability of observing the \"success\" category. In multinomial case, the models return probabilities of observing each of the outcomes. Log probabilities are simply natural logarithms of the predicted probabilities.\n\nLet's take an example feature \"best\" for the purpose of this illustration, the `log` probability of this feature for class `1` is `-2.14006616` (as you pointed out), now if we were to convert it into actual probability score it will be `np.exp(1)**-2.14006616 = 0.11764`. Let's take one more step back to see how and why the probability of \"best\" in class `1` is `0.11764`. As per the documentation of [Multinomial Naive Bayes][2], we see that these probabilities are computed using the formula below:\n\n![img](https:\/\/i.stack.imgur.com\/gyokC.png)\n\nWhere, the numerator roughly corresponds to the number of times feature \"best\" appears in the class `1` (which is of our interest in this example) in the training set, and the denominator corresponds to the total count of all features for class `1`. Also, we add a small smoothing value, `alpha` to prevent from the probabilities going to zero and `n` corresponds to the total number of features i.e. size of vocabulary.\n\n## Predicting Probabilities for the actual test dataset of Kaggle","ee911ddf":"## Bonus Calculation - Derive top 30 Important features - BOW","1dc0f532":"Looks almost there's not real difference, hence I shall include 'project_resource_summary' within deleted-columns\n\n### I will drop following columns from both train and test dataset\n","dda3eeba":"## Fundamental and Intuitive note on Bayes Theorem\n\n![Imgur](https:\/\/imgur.com\/brkFzR6.png)\n\nSame formulae with some notes\n\n![Imgur](https:\/\/imgur.com\/pU0s86X.png)\n\nTo understand why Bayes\u2019 theorem is so important, let\u2019s look at a general form of this problem. Our beliefs describe the world we know, so when we observe something, its conditional probability represents the likelihood of what we\u2019ve seen given what we believe, or:\n\n#### P(observed | belief)\n\n\nFor example, suppose you believe in climate change, and therefore you expect that the area where you live will have more droughts than usual over a 10-year period. Your belief is that climate change is taking place, and our observation is the number of droughts in our area; let\u2019s say there were 5 droughts in the last 10 years. Determining how likely it is that you\u2019d see exactly 5 droughts in the past 10 years if there were climate change during that period may be difficult. One way to do this would be to consult an expert in climate science and ask them the probability of droughts given that their model assumes climate change.\n\nAt this point, all we\u2019ve done is ask, \u201cWhat is the probability of what I\u2019ve observed, given that I believe climate change is true?\u201d But what you want is some way to quantify how strongly you believe climate change is really happening, given what you have observed. Bayes\u2019 theorem allows you to reverse P(observed | belief), which you asked the climate scientist for, and solve for the likelihood of our beliefs given what you\u2019ve observed, or:\n\n\n#### P(belief | observed)\n\n\nIn this example, Bayes\u2019 theorem allows us to transform our observation of five droughts in a decade into a statement about how strongly you believe in climate change after you have observed these droughts. The only other pieces of information you need are the general probability of 5 droughts in 10 years (which could be estimated with historical data) and our initial certainty of our belief in climate change. And while most people would have a different initial probability for climate change, Bayes\u2019 theorem allows you to quantify exactly how much the data changes any belief.\n\nFor example, if the expert says that 5 droughts in 10 years is very likely if we assume that climate change is happening, most people will change their previous beliefs to favor climate change a little, whether they\u2019re skeptical of climate change or they\u2019re Al Gore.\nHowever, suppose that the expert told you that in fact, 5 droughts in 10 years was very unlikely given our assumption that climate change is happening. In that case, our prior belief in climate change would weaken slightly given the evidence. The key takeaway here is that Bayes\u2019 theorem ultimately allows evidence to change the strength of our beliefs.\n\nBayes\u2019 theorem allows us to take our beliefs about the world, combine them with data, and then transform this combination into an estimate of the strength of our beliefs given the evidence we\u2019ve observed. Very often our beliefs are just our initial certainty in an idea; this is the P(A) in Bayes\u2019 theorem. We often debate topics such as whether gun control will reduce violence, whether increased testing increases student performance, or whether public health care will reduce overall health care costs. But we seldom think about how evidence should change our minds or the minds of those we\u2019re debating. Bayes\u2019 theorem allows us to observe evidence about these beliefs and quantify exactly how much this evidence changes our beliefs.\n\n## Simplest Proof of Bayes Theorem\n\nLet\u2019s consider two probabilistic events A and B. We can correlate the marginal probabilities P(A) and P(B) with the conditional probabilities P(A|B) and P(B|A) using the product rule:\n\n![Imgur](https:\/\/imgur.com\/1fGQOXC.png)\n\nFirst of all, let\u2019s consider the marginal probability P(A): this is normally a value that determines how probable a target event is, like P(Spam) or P(Rain). As there are no other elements, this kind of probability is called Apriori, because it\u2019s often determined by mathematical considerations or simply by a frequency count. For example, imagine we want to implement a very simple spam filter and we\u2019ve collected 100 emails. We know that 30 are spam and 70 are regular. So we can say that P(Spam) = 0.3.\n\nHowever, we\u2019d like to evaluate using some criteria (for simplicity, let\u2019s consider a single one), for example, e-mail text is shorter than 50 characters. Therefore, our query becomes:\n\n---\n\n### The time complexity of Naive Bayes\n\nLet, n = no of data points, d = no of features in an input vector x. and c = number of classes.\n\nSo, if we have \u2018d\u2019 features then we calculate \u2018d\u2019 likelihoods for 1 class. Then for \u2018c\u2019 class **d \\* c** likelihood probabilities\n\n#### T (n) = O (ndc)\n\nAll it needs to do is computing the frequency of every feature value di for each class.\n\nIf d is small we can neglect the presence of d and we can tell O(n).\n\n#### For space complexity, we store only the likelihoods for \u2018c\u2019 classes,\n\nS(n) = O(dc)\n\nFor Testing phase : Since all the likelihood probabilities are already calculated in training phase then at\n\nthe time of evaluation we just need to lookup.\n\n#### S(n) = O(dc)\n\n---\n\n### Difference between naive Bayes & multinomial naive Bayes\n\nIn general, to train Naive Bayes for n-dimensional data, and k classes you need to estimate $P(x_i | c_j)$ for each $1 \\leq i \\leq n$, $1 \\leq j \\leq k$ . You can assume any probability distribution for any pair $(i,j)$ (although it's better to not assume discrete distribution for $P(x_i|c_{j_1})$ and continuous for $P(x_i | c_{j_2})$). You can have Gaussian distribution on one variable, Poisson on other and some discrete on yet another variable.\n\n#### Multinomial Naive Bayes simply assumes multinomial distribution for all the pairs, which seem to be a reasonable assumption in some cases, i.e. for word counts in documents.\n\n---\n\nThe general term **Naive Bayes** refers the the strong independence assumptions in the model, rather than the particular distribution of each feature. A Naive Bayes model assumes that each of the features it uses are conditionally independent of one another given some class. More formally, if I want to calculate the probability of observing features $f_1$ through $f_n$, given some class c, under the Naive Bayes assumption the following holds:\n\n$$ p(f_1,..., f_n|c) = \\prod_{i=1}^n p(f_i|c)$$\n\nThis means that when I want to use a Naive Bayes model to classify a new example, the posterior probability is much simpler to work with:\n\n$$ p(c|f_1,...,f_n) \\propto p(c)p(f_1|c)...p(f_n|c) $$\n\nOf course these assumptions of independence are rarely true, which may explain why some have referred to the model as the \"Idiot Bayes\" model, but in practice Naive Bayes models have performed surprisingly well, even on complex tasks where it is clear that the strong independence assumptions are false.\n\nUp to this point we have said nothing about the distribution of each feature. In other words, we have left $p(f_i|c)$ undefined. The term **Multinomial Naive Bayes** simply lets us know that each $p(f_i|c)$ is a multinomial distribution, rather than some other distribution. This works well for data which can easily be turned into counts, such as word counts in text.\n\nIn summary, Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model, while Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which uses a multinomial distribution for each of the features.\n\nReferences:\n\nStuart J. Russell and Peter Norvig. 2003. Artificial Intelligence: A Modern Approach (2 ed.). Pearson Education. _See p. 499 for reference to \"idiot Bayes\" as well as the general definition of the Naive Bayes model and its independence assumptions_\n\n---\n\n## Note on Bag of Words\n\nThe name \u201cbag-of-words\u201d comes from the algorithm simply seeking to know the number of times a given word is present within a body of text. The order or context of the words is not analyzed here. Similarly, if we have a bag filled with six pencils, eight pens, and four notebooks, the algorithm merely cares about recording the number of each of these objects, not the order in which they are found, or their orientation. You typically want to use the bag-of-words feature extraction technique for document classification. Why is this the case? We assume that documents of certain classifications contain certain\nwords. For example, we expect a document referencing political science to perhaps feature jargon such as dialectical materialism or free market capitalism; whereas a document that is referring to classical music will\nhave terms such as crescendo, diminuendo, and so forth. In these instances of document classification, the location of the word itself is not terribly important. It\u2019s important to know what portion of the vocabulary is present in one class of document vs. another.\n\n","7ee137e6":"## Normalizing numerical features (resource_cost, teacher_number_of_previously_posted_projects)\n\n\nHere is how normalizer works and why we should use reshape(1, -1) instead of (-1, 1)\n\n#### Normalizer by default normalizes on each sample(row) while StandardScaler standardises on each feature(column)\n\nHere if I use (-1, 1) it means any number of rows, which is the responsibility of numpy to figure out, while I am specifying that I need to have one column. Remember -1 lets numpy to calculate the unknown dimension for the resultant that will match with the original matrix.\n\nAnd vice versa, if I do reshape(1, -1) means, that I am specifying row to be 1 while leaving column numbers to be calculated by Numpy.\n\nSo for the case, that I use (-1, 1) => i.e. Rows are unknown while columns required is 1\n\n### Note, for normalizing numerical data, we got to use reshape(1, -1) and NOT reshape(-1, 1).\n\nSo now below, I shall derived the vectors for 'resource_cost' and 'teacher_number_of_previously_posted_projects' for both train and validation, and these will be later merged with the other vectorized categorical variables (that I calculated above)  to form the final matrix.\n\nAnd for the reshape, first I will reshape(1, -1) i.e. one row and unknown columns > Then apply Normalization > and then reshape again to (-1, 1) i.e. unknown rows, and 1 column\n\n### 2.1 Normalizing numerical feature: resource_cost\n\n( First I am printing some extra stuff to see the implementation of the above principle, so the below cell's code is only for experimentation and not used outside of the cell )","767ae880":"As we can see there's around 15.42mn rows in the original resource_data.csv. And this is because the same project will require multiple resources.\nSo, now first I want to sum all the Price and Quantity that belongs to the same Project ID.\n\n### How reset_index() function works.\n\nSay, I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: `[1,5,6,10,11]` and I would like to reset it to `[0,1,2,3,4]`. This is where I will use `reset_index()`\n\n`df.reset_index(drop=True)`","5ba1553b":"### Vectorizing Categorical data: teacher_prefix","8162b4e1":"### Normalizing next numerical feature: teacher_number_of_previously_posted_projects","164fa938":"## Preprocessing Categorical Features: school_state","6963af78":"## 1.1 Reading Data","7bc71080":"## Preprocessing Categorical Features: project_title\n\n#### First Expanding English language contractions in Python\n\nThe English language has [a couple of contractions](http:\/\/en.wikipedia.org\/wiki\/Wikipedia%3aList_of_English_contractions). For instance:\n\n    you've -> you have\n    he's -> he is","ed995ea7":"### Vectorizing Categorical data: project_subject_subcategories","a2a15978":"## Deciding which columns to drop - project_resource_summary\n\nLets checkout, which words are used in summaries ('project_resource_summary') of approved and non-approved projects.","19d42cbc":"### Reason of using logathims_of_alphas in the above\n\nOne of the main reason for using log scale is log scales allow a large range to be displayed without small values being compressed down into bottom of the graph.\n\nGenearally, in ML\/DS log transformation is done on many continuous variables very often. Mostly because of skewed distribution. When the variables span several orders of magnitude. Income is a typical example: its distribution is \"power law\", meaning that the vast majority of incomes are small and very few are big. This type of \"fat tailed\" distribution is studied in logarithmic scale because of the mathematical properties of the logarithm:\n\nLogarithm naturally reduces the dynamic range of a variable so the differences are preserved while the scale is not that dramatically skewed. Imagine some people got 100,000,000 loan and some got 10000 and some 0. Any feature scaling will probably put 0 and 10000 so close to each other as the biggest number anyway pushes the boundary. Logarithm solves the issue.\n\nIf you take values 1000,000,000 and 10000 and 0 into account. In many cases, the first one is too big to let others be seen properly by your model. But if you take logarithm you will have 9, 4 and 0 respectively. As you see the dynamic range is reduced while the differences are almost preserved. It comes from any exponential nature in your feature\n\n$$log(x^n)= n log(x)$$\n\nwhich implies\n\n$$log(10^4) = 4 * log(10)$$\n\nand\n\n$$log(10^3) = 3 * log(10)$$\n\n### A note on the alpha parameter\n\nWe add a small smoothing value, alpha to prevent from the probabilities going to zero\n\nIn Multinomial Naive Bayes, the `alpha` parameter is what is known as a [_hyperparameter_](https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization); i.e. a parameter that controls the form of the model itself. In most cases, the best way to determine optimal values for hyperparameters is through a [grid search](http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html) over possible parameter values, using [cross validation](http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html) to evaluate the performance of the model on your data at each value. Read the above links for details on how to do this with scikit-learn.\n\nCheck out the wikipedia page http:\/\/en.wikipedia.org\/wiki\/Additive_smoothing.\n\n![Imgur](https:\/\/imgur.com\/v5Vx17J.png)\n\n### How to choose alpha\n\nBasically the idea is that you want to decrease the effect of rare words: for example if you have one spam email with the word 'multinomialNB' in it, and no nonspam emails with this word, then without additive smoothing, your spam filter will classify every email with this keyword as spam.\n\n---\n\n### class_prior : array-like of shape (n_classes,), default=None\n\nPrior probabilities of the classes. If specified the priors are not adjusted according to the data.\n\nNow that GridSearchCV has completed its run with MultinomialNB, I have the best alpha that I shall now apply in the next step on the test dataset to find its class-probabilities.\n\n## Run MultinomialNB with the best hyperparameter value (categorical + numerical + essay features)\n\nFrom our above GridSearchCV we saw the best_alpha_1_bow was at 0.5\n\nBest Alpha:  0.5\nBest Score :  0.7049241508387795","469c856d":"### Vectorizing Categorical data: project_grade_category","29d482ed":"## Preprocessing Categorical Features: project_subject_categories","f2892472":"## Preprocessing Categorical Features: essay","491bd2e1":"## Stopword Removal\n\nStop words are a set of commonly used words in a language. Examples of stop words in English are \u201ca\u201d, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.\n\nFor example, in the context of a search system, if your search query is \u201cwhat is text preprocessing?\u201d, you want the search system to focus on surfacing documents that talk about text preprocessing over documents that talk about what is. This can be done by preventing all words from your stop word list from being analyzed. Stop words are commonly applied in search systems, text classification applications, topic modeling, topic extraction and others.\nIn my experience, stop word removal, while effective in search and topic extraction systems, showed to be non-critical in classification systems. However, it does help reduce the number of features in consideration which helps keep your models decently sized.","ca9f94dd":"## Creating file for final submission","0eb899df":"## Preprocessing Numerical Values: price\n","1236e093":"## Encoding Essay column using Bag Of Words","bb8e408f":"## A Note on GridSearchCV() function of sklearn\n\n#### when using cross validation with cv=10 the data is split into 10 parts i.e. 10% \/90% then each part is used for training while the rest used for validation. I recommend setting the grid search parameter\n\nThe grid search returns a dictionary (accessible through `.cv_results_`) containing the scores for each fold train\/test scores as well as the time it took to train\/test each fold. Also a summary of that data is included using the mean and the standard deviation. PS. in newer version of pandas you'll need to include return_train_score=True PS.S. when using grid search, splitting the data to train\/test is not necessary for model selection, because the grid search splits the data automatically (cv=10 means that the data is split to 10 folds)\n\nMore on the above point - from this very detailed official [doc](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation) -\n\nA test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k \u201cfolds\u201d:\n\nA model is trained using of the folds as training data;\n\nthe resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n\nThe performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n\n## 6. Applying Multinomial Naive Bayes on BOW\n\n#### Using GridSearchCV with MultinomialNB on BOW based data to find best alpha that I shall apply in the next step on the test dataset to find the probabilities.","2c6f202e":"\n### Summary of the above GridSearchCV Implementation\n\nWe have started with hyperparameter alpha with as low as 0.0001 to 1000.Since it is difficult to plot the given range we have used log alphas on x-axis and Auc on y axis as shown in the plot.\n\nwe observe that as log alpha approaches close to 4, both train AUc and cv AUC lines converge\n\n## Plotting Naive Bayes on BOW - Log of Alpha on X-axis and AUC on Y-axis","01f115b8":"## Vectorization ( convert text to word count vectors with CountVectorizer ) of below Categorical features\n\n- teacher_prefix\n- project_grade_category\n- school_state\n- project_subject_categories\n- project_subject_subcategories\n\nFor eac of the above categorical variable, I have to vectorize from X_donor_choose_train, X_donor_choose_validation and the original test set given in the Kaggle dataset i.e. test_df_pre_processed_original\n\n`CountVectorizer()` will basically create a vector from word count\n\nAs to the workflow in the next part, after vectorizing I will merge  all these ONE HOT features with `hsstack()`\n\nCountVectorizer, an implementation of bag-of-words in which we code text data as a representation of features\/words. The values of each of these features represent the occurrence counts of words across all documents.\n\n### Vectorizing Categorical data: project_subject_categories","a4144acf":"### Vectorizing Categorical data: school_state","05148e4d":"## Preprocessing Categorical Features: teacher_prefix"}}