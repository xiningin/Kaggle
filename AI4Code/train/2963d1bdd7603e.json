{"cell_type":{"bf5184b1":"code","38610891":"code","b15ce843":"code","926b43d8":"code","5db7db25":"code","92e97900":"code","ea3e0c71":"code","d65fceb3":"code","27ff5d25":"code","9a74ec55":"code","e732b417":"markdown","bc39f40f":"markdown","bd7cdd11":"markdown","1c9f081a":"markdown","560b6a95":"markdown","ef8863d7":"markdown","2bb86110":"markdown","f0fad347":"markdown","01723bfa":"markdown","8b033a80":"markdown","df95f0d2":"markdown"},"source":{"bf5184b1":"from __future__ import print_function, division\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nnum_epochs = 100\ntotal_series_length = 50000\ntruncated_backprop_length = 15\nstate_size = 4\nnum_classes = 2\necho_step = 3\nbatch_size = 5\nnum_batches = total_series_length\/\/batch_size\/\/truncated_backprop_length","38610891":"def generateData():\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n    y = np.roll(x, echo_step)\n    y[0:echo_step] = 0\n\n    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n    y = y.reshape((batch_size, -1))\n\n    return (x, y)","b15ce843":"batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size])","926b43d8":"W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\nb = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)","5db7db25":"inputs_series = tf.unpack(batchX_placeholder, axis=1)\nlabels_series = tf.unpack(batchY_placeholder, axis=1)","92e97900":"# Forward pass\ncurrent_state = init_state\nstates_series = []\nfor current_input in inputs_series:\n    current_input = tf.reshape(current_input, [batch_size, 1])\n    input_and_state_concatenated = tf.concat(1, [current_input, current_state])  # Increasing number of columns\n\n    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n    states_series.append(next_state)\n    current_state = next_state","ea3e0c71":"logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\ntotal_loss = tf.reduce_mean(losses)\n\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)","d65fceb3":"def plot(loss_list, predictions_series, batchX, batchY):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batch_series_idx in range(5):\n        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n\n        plt.subplot(2, 3, batch_series_idx + 2)\n        plt.cla()\n        plt.axis([0, truncated_backprop_length, 0, 2])\n        left_offset = range(truncated_backprop_length)\n        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n\n    plt.draw()\n    plt.pause(0.0001)","27ff5d25":"with tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    plt.ion()\n    plt.figure()\n    plt.show()\n    loss_list = []\n\n    for epoch_idx in range(num_epochs):\n        x,y = generateData()\n        _current_state = np.zeros((batch_size, state_size))\n\n        print(\"New data, epoch\", epoch_idx)\n\n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * truncated_backprop_length\n            end_idx = start_idx + truncated_backprop_length\n\n            batchX = x[:,start_idx:end_idx]\n            batchY = y[:,start_idx:end_idx]\n\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                [total_loss, train_step, current_state, predictions_series],\n                feed_dict={\n                    batchX_placeholder:batchX,\n                    batchY_placeholder:batchY,\n                    init_state:_current_state\n                })\n\n            loss_list.append(_total_loss)\n\n            if batch_idx%100 == 0:\n                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n                plot(loss_list, _predictions_series, batchX, batchY)\n\nplt.ioff()\nplt.show()","9a74ec55":"from __future__ import print_function, division\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nnum_epochs = 100\ntotal_series_length = 50000\ntruncated_backprop_length = 15\nstate_size = 4\nnum_classes = 2\necho_step = 3\nbatch_size = 5\nnum_batches = total_series_length\/\/batch_size\/\/truncated_backprop_length\n\ndef generateData():\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n    y = np.roll(x, echo_step)\n    y[0:echo_step] = 0\n\n    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n    y = y.reshape((batch_size, -1))\n\n    return (x, y)\n\nbatchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size])\n\nW = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\nb = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n\n# Unpack columns\ninputs_series = tf.unpack(batchX_placeholder, axis=1)\nlabels_series = tf.unpack(batchY_placeholder, axis=1)\n\n# Forward pass\ncurrent_state = init_state\nstates_series = []\nfor current_input in inputs_series:\n    current_input = tf.reshape(current_input, [batch_size, 1])\n    input_and_state_concatenated = tf.concat(1, [current_input, current_state])  # Increasing number of columns\n\n    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n    states_series.append(next_state)\n    current_state = next_state\n\nlogits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\ntotal_loss = tf.reduce_mean(losses)\n\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n\ndef plot(loss_list, predictions_series, batchX, batchY):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batch_series_idx in range(5):\n        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n\n        plt.subplot(2, 3, batch_series_idx + 2)\n        plt.cla()\n        plt.axis([0, truncated_backprop_length, 0, 2])\n        left_offset = range(truncated_backprop_length)\n        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n\n    plt.draw()\n    plt.pause(0.0001)\n\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    plt.ion()\n    plt.figure()\n    plt.show()\n    loss_list = []\n\n    for epoch_idx in range(num_epochs):\n        x,y = generateData()\n        _current_state = np.zeros((batch_size, state_size))\n\n        print(\"New data, epoch\", epoch_idx)\n\n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * truncated_backprop_length\n            end_idx = start_idx + truncated_backprop_length\n\n            batchX = x[:,start_idx:end_idx]\n            batchY = y[:,start_idx:end_idx]\n\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                [total_loss, train_step, current_state, predictions_series],\n                feed_dict={\n                    batchX_placeholder:batchX,\n                    batchY_placeholder:batchY,\n                    init_state:_current_state\n                })\n\n            loss_list.append(_total_loss)\n\n            if batch_idx%100 == 0:\n                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n                plot(loss_list, _predictions_series, batchX, batchY)\n\nplt.ioff()\nplt.show()","e732b417":"As you can see in the picture below that is done by unpacking the columns (axis = 1) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names \u201cplural\u201d_\u201dseries\u201d is to emphasize that the variable is a list that represent a time-series with multiple entries at each step.\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*f2iL4zOkBUBGOpVE7kyajg.png)\n\nThe fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the init_state placeholder has batch_size rows.\n\n## Forward pass\n\nNext let\u2019s build the part of the graph that does the actual RNN computation.","bc39f40f":"You can see that we are moving truncated_backprop_length steps forward on each iteration (line 15\u201319), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that truncated_backprop_length need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of \u201cmisses\u201d, as you can see on the figure below.\n\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*uKuUKp_m55zAPCzaIemucA.png)\n\nAlso realize that this is just simple example to explain how a RNN works, this functionality could easily be programmed in just a few lines of code. The network will be able to exactly learn the echo behavior so there is no need for testing data.\n\nThe program will update the plot as training progresses, shown in the picture below. Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch.\n\nOur algorithm will fairly quickly learn the task. The graph in the top-left corner shows the output of the loss function, but why are there spikes in the curve? Think of it for a moment, answer is below.\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*ytquMdmGMJo0-3kxMCi1Gg.png)\n\nThe reason for the spikes is that we are starting on a new epoch, and generating new data. Since the matrix is reshaped, the first element on each row is adjacent to the last element in the previous row. The first few elements on all rows (except the first) have dependencies that will not be included in the state, so the net will always perform badly on the first batch.\n\n## Whole program\n \nThis is the whole runnable program, just copy-paste and run. After each part in the article series the whole runnable program will be presented. If a line is referenced by number, these are the line numbers that we mean.","bd7cdd11":"# Related links\n[Getting Started with Deep Learning](https:\/\/www.kdnuggets.com\/2017\/03\/getting-started-deep-learning.html)\n\n[5 Machine Learning Projects You Can No Longer Overlook](https:\/\/www.kdnuggets.com\/2017\/04\/five-machine-learning-projects-cant-overlook-april.html)\n[](http:\/\/)","1c9f081a":"The last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically\u200a\u2014\u200athe computation graph is executed once for each mini-batch and the network-weights are updated incrementally.\n\nNotice the API call to sparse_softmax_cross_entropy_with_logits, it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the \u201cSparse-softmax\u201d, you can read more about it in the API. The usage is to havelogits is of shape [batch_size, num_classes] and labels of shape [batch_size].\n\n## Visualizing the training\n \nThere is a visualization function so we can se what\u2019s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch.","560b6a95":"# Running a training session\n \nIt\u2019s time to wrap up and train the network, in TensorFlow the graph is executed in a session. New data is generated on each epoch (not the usual way to do it, but it works in this case since everything is predictable).\n\n","ef8863d7":"# What is a RNN?\nIt is short for \u201cRecurrent Neural Network\u201d, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. More importantly, this sequence can be of arbitrary length.\n\nThe most straight-forward example is perhaps a time-series of numbers, where the task is to predict the next value given previous values. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has \u201cseen\u201d at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.\n\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*UkI9za9zTR-HL8uM15Wmzw.png)\n\n# Setup\nWe will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. First let\u2019s set some constants we\u2019ll need, what they mean will become clear in a moment.","2bb86110":"Notice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms current_input * Wa + current_state * Wbin the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias b is broadcasted on all samples in the batch.\n\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png)\n\nYou may wonder the variable name truncated_backprop_length is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch.\n\n## Calculating loss\n\nThis is the final part of the graph, a fully connected softmax layer from the state to the output that will make the classes one-hot encoded, and then calculating the loss of the batch.","f0fad347":"# Generate data\nNow generate the training data, the input is basically a random binary vector. The output will be the \u201cecho\u201d of the input, shifted echo_step steps to the right.","01723bfa":"The weights and biases of the network are declared as TensorFlow variables, which makes them persistent across runs and enables them to be updated incrementally for each batch.","8b033a80":"The figure below shows the input data-matrix, and the current batch batchX_placeholder is in the dashed rectangle. As we will see later, this \u201cbatch window\u201d is slided truncated_backprop_length steps to the right at each run, hence the arrow. In our example below batch_size = 3, truncated_backprop_length = 3, and total_series_length = 36. Note that these numbers are just for visualization purposes, the values are different in the code. The series order index is shown as numbers in a few of the data-points.\n\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*n45uYnAfTDrBvG87J-poCA.jpeg)\n\n## Unpacking\nNow it\u2019s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps.","df95f0d2":"Notice the reshaping of the data into a matrix with batch_size rows. Neural networks are trained by approximating the gradient of loss function with respect to the neuron-weights, by looking at only a small subset of the data, also known as a mini-batch. The theoretical reason for doing this is further elaborated in this question. The reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into these mini-batches.\n\n![img](https:\/\/cdn-images-1.medium.com\/max\/800\/1*aFtwuFsboLV8z5PkEzNLXA.png)\n\n\n# Building the computational graph\nTensorFlow works by first building up a computational graph, that specifies what operations will be done. The input and output of this graph is typically multidimensional arrays, also known as tensors. The graph, or parts of it can then be executed iteratively in a session, this can either be done on the CPU, GPU or even a resource on a remote server.\n## Variables and placeholders\nThe two basic TensorFlow data-structures that will be used in this example are placeholders and variables. On each run the batch data is fed to the placeholders, which are \u201cstarting nodes\u201d of the computational graph. Also the RNN-state is supplied in a placeholder, which is saved from the output of the previous run.\n\n"}}