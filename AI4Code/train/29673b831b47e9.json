{"cell_type":{"88bb1898":"code","f4a7339e":"code","d95905c3":"code","1789e834":"code","712baa8d":"code","b07c718e":"code","7f64c091":"code","dc5e0df6":"code","715b5b35":"code","96f6cd90":"code","e2a36300":"code","371a5364":"code","fd9465b3":"code","908faf74":"code","df8de1b8":"code","4a5ce5d9":"code","4f6e2255":"code","3216fccb":"code","6aefd390":"markdown","e12eb422":"markdown","75e8c348":"markdown","a9778e4f":"markdown","8149fd6c":"markdown","a973c0a1":"markdown","e28e4120":"markdown","c83afc8e":"markdown","9df40de7":"markdown","a504f8ea":"markdown","55a3bad5":"markdown","b2a78df5":"markdown","e3f8ce49":"markdown","bb5d8251":"markdown","581e1ca0":"markdown","24ec94ac":"markdown","5b94d93a":"markdown","7ea43a8d":"markdown","a3654d59":"markdown"},"source":{"88bb1898":"import pandas as pd\nimport numpy as np","f4a7339e":"path = '..\/input\/tabular-playground-series-feb-2021\/'\n\ntrain_path = path + 'train.csv'\ntest_path = path + 'test.csv'\nsub_path = path + 'sample_submission.csv'\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsub = pd.read_csv(sub_path)","d95905c3":"train.head()","1789e834":"test.head()","712baa8d":"sub.head()","b07c718e":"train.shape, test.shape, sub.shape","7f64c091":"# tutorial on seaborn and matplotlib on progress...\nimport seaborn as sns\nimport matplotlib.pyplot as plt","dc5e0df6":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb","715b5b35":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score ","96f6cd90":"k_fold = KFold(n_splits = 5, shuffle = True, random_state=0)","e2a36300":"model_dict = {'DT':DecisionTreeRegressor(),\n              'RF':RandomForestRegressor(n_jobs=-1, random_state=0), \n              'LGB':lgb.LGBMRegressor()}","371a5364":"def compare_models(X_train, y_train, model_dict):\n    \n    score = {}\n\n    for model_name in model_dict.keys():\n\n        model = model_dict[model_name]\n\n        score[model_name] = np.mean(cross_val_score(model, X_train.sample(frac=1, random_state=0).head(10000), y_train.sample(frac=1, random_state=0).head(10000), scoring = 'neg_mean_squared_error', cv = k_fold, n_jobs = -1))\n\n        print(f'{model_name} validation completed')\n        \n    return score","fd9465b3":"X_train = train.iloc[:,11:-1]\ny_train = train['target']\nX_test = test[X_train.columns]","908faf74":"score = compare_models(X_train, y_train, model_dict)","df8de1b8":"score","4a5ce5d9":"model = model_dict['RF']\n\nmodel.fit(X_train.sample(frac=1, random_state=0).head(10000), y_train.sample(frac=1, random_state=0).head(10000))","4f6e2255":"sub['target'] = model.predict(X_test)","3216fccb":"sub.to_csv('submission.csv', index=False)","6aefd390":"- By using `pd.DataFrame.sample()` we can randomly shuffle the dataframe and then use `head()` to extract 10k samples. \n\n- In the cell below, we have divided `train`, `test` data into `X_train`, `y_train`, and `X_test` to train the models. ","e12eb422":"- The result comes out in `negative MSE`. So the higher the score is, the better is the performance of the model. \n\n- Since `Random Forest` has highest performance of `-0.784`, we will use `Random Forest` to make inference on the test data. ","75e8c348":"## Data Preprocessing \n\n- *on progress...*","a9778e4f":"- In this section, we will use a `for` loop to train several machine learning models at once. \n\n- In particular, we will train `DecisionTree`, `Random Forest`, and `LightGBM`. \n\n- Like we've stated at the `EDA` section, we will only use 10k samples from 300k total samples of training data due to time constraints. \n\n- However, it is recommended to train on all 300k samples for higher accuracy.\n\n- Let's first import the models.","8149fd6c":"By executing the cell below, it will validate 3 models in model_dict and save the validated score to the `score` variable.","a973c0a1":"- Then, we will import `KFold` and `cross_val_score` functions in order to validate on our training data\n\n- Those two functions are necessary in order to perform `Shuffled K-fold Cross Validation`. \n\n![](http:\/\/ethen8181.github.io\/machine-learning\/model_selection\/img\/kfolds.png)\n- Image Reference: http:\/\/ethen8181.github.io\/machine-learning\/model_selection\/model_selection.html\n\n- As shown on the image above, `Cross Validation` allows you to validate your model on your training data by holding out certain part of the training data for validation.\n\n- If you divide the whole training data into 5 parts and use each part for validating your model, it becomes `5-Fold Cross Validation`.\n\n- When dividing the training data, it is important to shuffle them in order to prevent overfitting. This can be performed with the `KFold` function. ","e28e4120":"- The result should be around `0.87` which is quite low.\n\n- However, do mind that we have only used 10k sample out of 300k training samples for educational purpose. \n\n- Since you are now familiar with the whole machine learning pipeline process, try fiddling the number of training samples and the types of models to increase your score.","c83afc8e":"We will first read the data using `pandas`. ","9df40de7":"- Next, we will define `compare_models` function which will  \n  \n    1. iterate through the models in `model_dict` and perform `5-Fold Cross Validation`\n    2. save the result to `score` variable","a504f8ea":"## Setup","55a3bad5":"An typical machine learning pipeline consists of:\n\n1. data collection\n2. exploratory data analysis (EDA)\n3. data preprocessing\n4. data modeling\n\n  \n- Since `data collection` is done by Kaggle for you, we should focus on step 2 to step 4. \n\n- In this tutorial, we will practice popular packages that are used in those steps. \n\n- Specifically, we will pratice using following packages for each step.\n\nStep 2: `matplotlib`, `seaborn`  \nStep 3: `scikit-learn`  \nStep 4: `scikit-learn` ","b2a78df5":"## Data Modeling","e3f8ce49":"- After training the model, we will make inference on `X_test` and save the result into the `sub` file.","bb5d8251":"## Exploratory Data Analysis","581e1ca0":"- `KFold` function has following parameters.\n\n    - `n_splits`: number of folds (int)\n    - `shuffle`: whether to shuffle the data (boolean)\n    - `random_state`: seed number for reproducibility (int)\n    \n- By setting each parameters 5, True, 0 respectively, we are performing `5-Fold Cross-Validation`.\n\n- We will save the `KFold` object to `k_fold` variable, which will be passed to `cross_val_score` function.","24ec94ac":"# Complete tutorial covering ML pipeline","5b94d93a":"- We have 300k samples for training and 200k samples for testing. \n\n- 300k samples is actually a huge amount. It will take quite some time for the model to train on it all. \n\n- For this tutorial, we will only use 10k samples that have been sampled randomly for training. ","7ea43a8d":"- Let's use `pd.DataFrame.head()` method which will show top 5 rows of each dataframe to verify that the data have been read well.","a3654d59":"- In the cell below, we will define 3 models which will be used for modeling. "}}