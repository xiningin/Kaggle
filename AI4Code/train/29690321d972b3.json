{"cell_type":{"b8dbc9e9":"code","d8713886":"code","985b5468":"code","306c653b":"code","8f45ccea":"code","803a0993":"code","13686990":"code","5686f9d7":"code","80ba5357":"code","fa553911":"code","4508f5a1":"markdown","2e2d3644":"markdown","27f78110":"markdown"},"source":{"b8dbc9e9":"import os, random, math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport torchaudio.transforms as T\nfrom IPython.display import Audio","d8713886":"def get_df(path = '\/kaggle\/input\/themaestrodatasetv2\/maestro-v2.0.0\/'):\n    df = pd.read_csv(f'{path}\/\/maestro-v2.0.0.csv')\n    for col in ['canonical_composer', 'canonical_title', 'split', 'year']:\n        df[col] = df[col].astype('category')\n    df['midi_path'] = path + df.midi_filename\n    df['path'] = path + df.audio_filename.str.split('.').str.get(0) + '.mp3'\n    return df\n\n\nget_df().head()","985b5468":"def plot_features(df):\n    fig = plt.figure(figsize=(15, 5))\n    fig.suptitle('Recording features', fontsize=16)\n    \n    plt.subplot(131)\n    plt.title('Split')\n    df.groupby('split').split.count()\\\n        .map(lambda x: 100 * x \/ len(df))\\\n        .sort_values(ascending=False)\\\n        .plot(kind='bar', ylabel='%', xlabel='', rot=0, grid=True)\n\n    plt.subplot(132)\n    plt.title('Year performed')\n    df.year.hist(\n        bins=df.year.astype('int').max() - df.year.astype('int').min() + 1,\n        xrot=90.0,\n    ).set_ylabel('# of recordings')\n    \n    plt.subplot(133)\n    plt.title('Duration')\n    duration_ax = get_df().duration.map(lambda x: x \/ 60).hist(bins=40)\n    duration_ax.set_xlabel('# of minutes')\n    duration_ax.set_ylabel('# of recordings')\n    plt.show()\n\n    fig = plt.figure(figsize=(15, 2))\n    plt.title('Composers')\n    df.groupby('canonical_composer')\\\n        .size()\\\n        .sort_values(ascending=False)\\\n        .plot(kind='bar', xlabel='', grid=True)\n\n    fig = plt.figure(figsize=(15, 2))\n    plt.title('Top 20 most played titles')\n    title_composers = df.canonical_title.astype('string') + ', ' + \\\n        df.canonical_composer.astype('string').map(lambda x: x.split(' ')[-1])\n    title_composers\\\n        .value_counts()\\\n        .sort_values(ascending=False)[:20]\\\n        .plot(kind='bar', xlabel='', grid=True)\n\n\nplot_features(get_df())","306c653b":"class DataframeDataset(torch.utils.data.Dataset):\n    # df is a dataframe that has a path column (path_col) for each file\n    # features is a list of columns in the dataframe that will be added to items\n    # preload_device can be cpu\/cuda\n    def __init__(self, df, features=[], path_col='path', preload_device=None):\n        self.df = df.copy()\n        for feature in features:\n            if self.df[feature].dtype.name == 'category':\n                # convert categories to indices\n                self.df[feature] = self.df[feature].cat.codes\n\n        self.features = features\n        self.path_col = path_col\n        self.preloaded_data = None\n        if preload_device is not None:\n            self.preloaded_features = []\n            self.preloaded_data = []\n            for i, row in self.df.iterrows():\n                self.preloaded_features.append({\n                    k: torch.tensor(v).to(preload_device)\n                    for k, v in row[features].to_dict().items()\n                })\n                self.preloaded_data.append(\n                    self._get_data(row[self.path_col]).to(preload_device)\n                )\n\n    def __len__(self):\n        return(len(self.df))\n\n    def __getitem__(self, index):\n        if self.preloaded_data is None: \n            row = self.df.iloc[index]\n            features = row[self.features].to_dict()\n            data = self._get_data(row[self.path_col])\n        else:\n            features = self.preloaded_features[index]\n            data = self.preloaded_data[index]\n        return {**features, 'data': data}\n\n    def _get_data(self, path):\n        raise NotImplementedError()\n\n\nclass AudioDataset(DataframeDataset):\n    def __init__(self, df, features=[], path_col='path', transform=None, inv_transform=None, preload_device=None):\n        self.transform = transform\n        self.inv_transform = inv_transform\n        super().__init__(df, features, path_col, preload_device)\n\n    def _get_data(self, path):\n        audio = torchaudio.load(path)[0]\n        if self.transform is not None:\n            audio = self.transform(audio)\n        return audio","8f45ccea":"class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, x):\n        for transform in self.transforms:\n            x = transform(x)\n        return x\n    \n    \nclass RandomCrop1D:\n    def __init__(self, size, pad_if_needed=False, fill=0):\n        self.size = size\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n    def __call__(self, x):\n        x_len = x.shape[-1]\n        if x_len < self.size and not self.pad_if_needed:\n            raise Exception('too small')\n        start = random.randint(0, max(0, x_len - self.size))\n        crop = x[..., start:start+self.size]\n        if pad_if_needed:\n            return F.pad(crop, pad=(0, max(0, self.size - x_len)), value=self.fill)\n        return crop","803a0993":"train_dataset = AudioDataset(\n    get_df()[get_df().split == 'train'],\n    transform=Compose([\n        lambda audio: audio[..., :48000*10], #RandomCrop1D(48000*5),\n        lambda audio: audio[:1],\n        T.Resample(orig_freq=48000, new_freq=16000),\n    ]),\n)\n\ndef display_audio(title, audio):\n    plt.title(title)\n    plt.imshow(torch.log10(T.MelSpectrogram(n_mels=80)(audio) + 1e-6)[0])\n    plt.show()\n    display(Audio(audio, rate=16000))\n\ndisplay_audio('mel spectrogram of original wav recording', train_dataset[0]['data'])","13686990":"# converts midi files to wav files into order to play them\nprint('installing fluidsynth...')\n!apt-get install fluidsynth --assume-yes > \/dev\/null\nprint('done!')\n\n# update the version of music21, used to parse midi files\n!pip install music21==6.3.0","5686f9d7":"from music21 import midi\n\n\ndef read_midi(filepath):\n  mf = midi.MidiFile()\n  mf.open(filepath)\n  mf.read()\n  mf.close()\n  return mf\n\n\ndef write_midi(mf, filename = 'tempmidi.mid'):\n  mf.open(filename, attrib='wb')\n  mf.write()\n  mf.close()\n  return filename\n\n\ndef midi_to_wav(filename):\n  # linux ships with a default midi font\n  !fluidsynth -F $filename\\.wav -r 16000 -i -n -T wav \/usr\/share\/sounds\/sf2\/FluidR3_GM.sf2 $filename > \/dev\/null\n  return filename + '.wav'\n\n\ndef play_midi(mf, secs):\n  data, rate = torchaudio.load(midi_to_wav(write_midi(mf)))\n  display(Audio(data[0, :16000 * secs], rate=rate))\n\n    \nclass MidiToPerformanceConverter:\n    def __init__(self):\n        self.event_to_idx = {}\n        for i in range(128):\n          self.event_to_idx['note-on-' + str(i)] = i\n        for i in range(128):\n          self.event_to_idx['note-off-' + str(i)] = i + 128\n        for i in range(100):\n          self.event_to_idx['time-shift-' + str(i + 1)] = i + 128 + 128\n        for i in range(32):\n          self.event_to_idx['velocity-' + str(i)] = i + 128 + 128 + 100\n        self.idx_to_event = list(self.event_to_idx.keys())\n        self.num_channels = len(self.idx_to_event)\n\n        \n    def midi_to_idxs(self, mf):\n      event_to_idx = self.event_to_idx\n      ticks_per_beat = mf.ticksPerQuarterNote\n      # The maestro dataset uses the first track to store tempo data\n      tempo_data = next(e for e in mf.tracks[0].events if e.type == midi.MetaEvents.SET_TEMPO).data\n      # tempo data is stored at microseconds per beat (beat = quarter note)\n      microsecs_per_beat = int.from_bytes(tempo_data, 'big')\n      millis_per_tick = microsecs_per_beat \/ ticks_per_beat \/ 1e3\n\n      idxs = []\n      started = False\n      previous_t = None\n      is_pedal_down = False\n      notes_to_turn_off = set()\n      notes_on = set()\n\n      # The second track stores the actual performance\n      for e in mf.tracks[1].events:\n        #if started and e.type == 'DeltaTime' and e.time > 0:\n        if e.type == 'DeltaTime' and e.time > 0:\n          # event times are stored as ticks, so convert to milliseconds\n          millis = e.time * millis_per_tick\n\n          # combine repeated delta time events\n          t = millis + (0 if previous_t is None else previous_t)\n\n          # we can only represent a max time of 1 second (1000 ms)\n          # so we must split up times that are larger than that into separate events\n          while t > 0:\n            t_chunk = min(t, 1000)\n            idx = event_to_idx['time-shift-' + str(math.ceil(t_chunk \/ 10))]\n            if previous_t is None:\n              idxs.append(idx)\n            else:\n              idxs[-1] = idx\n              previous_t = None\n            t -= t_chunk\n          previous_t = t_chunk\n\n        elif e.type == midi.ChannelVoiceMessages.NOTE_ON:\n          if e.velocity == 0:\n            if is_pedal_down:\n              notes_to_turn_off.add(e.pitch)\n            elif e.pitch in notes_on:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_on.remove(e.pitch)\n              previous_t = None\n          else:\n            if e.pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_to_turn_off.remove(e.pitch)\n              notes_on.remove(e.pitch)\n\n            # midi supports 128 velocities, but our representation only allows 32\n            idxs.append(event_to_idx['velocity-' + str(e.velocity \/\/ 4)])\n            idxs.append(event_to_idx['note-on-' + str(e.pitch)])\n            notes_on.add(e.pitch)\n            started = True\n            previous_t = None\n\n        elif e.type == midi.ChannelVoiceMessages.CONTROLLER_CHANGE and e.parameter1 == 64: # sustain pedal\n          # pedal values greater than 64 mean the pedal is being held down,\n          # otherwise it's up\n          if is_pedal_down and e.parameter2 < 64:\n            is_pedal_down = False\n            for pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(pitch)])\n              notes_on.remove(pitch)\n            notes_to_turn_off = set()\n            previous_t = None\n          elif not is_pedal_down and e.parameter2 >= 64:\n            is_pedal_down = True\n            previous_t = None\n\n      return idxs\n\n\n    def make_note(self, track, pitch, velocity):\n      e = midi.MidiEvent(track, type=midi.ChannelVoiceMessages.NOTE_ON, channel=1)\n      e.pitch = int(pitch)\n      e.velocity = int(velocity)\n      return e\n\n\n    def idxs_to_midi(self, idxs, verbose=False):\n      if type(idxs) == torch.Tensor:\n        idxs = idxs.detach().numpy()\n\n      mf = midi.MidiFile()\n      mf.ticksPerQuarterNote = 1024\n\n      # The maestro dataset uses the first track to store tempo data, and the second\n      # track to store the actual performance. So follow that convention.\n      tempo_track = midi.MidiTrack(0)\n      track = midi.MidiTrack(1)\n      mf.tracks = [tempo_track, track]\n\n      tempo = midi.MidiEvent(tempo_track, type=midi.MetaEvents.SET_TEMPO)\n      # temp.data is the number of microseconds per beat (per quarter note)\n      # So to set ticks per millis = 1 (easy translation from time-shift values to ticks),\n      # tempo.data must be 1e3 * 1024, since ticksPerQuarterNote is 1024 (see above)\n      tempo.data = int(1e3 * 1024).to_bytes(3, 'big')\n\n      end_of_track = midi.MidiEvent(tempo_track, type=midi.MetaEvents.END_OF_TRACK)\n      end_of_track.data = ''\n      tempo_track.events = [\n        # there must always be a delta time before each event\n        midi.DeltaTime(tempo_track, time=0),\n        tempo,\n        midi.DeltaTime(tempo_track, time=0),\n        end_of_track\n      ]\n\n      track.events = [midi.DeltaTime(track, time=0)]\n\n      # set to 0 initially in case no velocity events occur before the first note\n      current_velocity = 0\n      notes_on = set()\n      errors = {'is_on': 0, 'is_not_on': 0}\n\n      for idx in idxs:\n        if 0 <= idx < 128: # note-on\n          pitch = idx\n          if pitch in notes_on:\n            if verbose:\n              print(pitch, 'is already on')\n            errors['is_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, current_velocity))\n          notes_on.add(pitch)\n\n        elif 128 <= idx < 256: # note-off\n          pitch = idx - 128\n          if pitch not in notes_on:\n            if verbose:\n              print(pitch, 'is not on')\n            errors['is_not_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, 1))\n          notes_on.remove(pitch)\n\n        elif 256 <= idx < 356: # time-shift\n          t = (1 + idx - 256) * 10\n          if track.events[-1].type == 'DeltaTime':\n            # combine repeated delta times\n            track.events[-1].time += t\n          else:\n            track.events.append(midi.DeltaTime(track, time=t))\n\n        else: # velocity\n          current_velocity = (idx - 356) * 4\n\n      if verbose:\n        print('remaining notes left on:', notes_on)\n\n      if track.events[-1].type != 'DeltaTime':\n        track.events.append(midi.DeltaTime(track, time=0))\n      track.events.append(end_of_track)\n\n      return mf, errors","80ba5357":"class MidiDataset(DataframeDataset):\n    def __init__(self, df, features=[], path_col='midi_path', preload_device=None):\n        self.converter = MidiToPerformanceConverter()\n        super().__init__(df, features, path_col, preload_device)\n\n    def _get_data(self, path):\n        return torch.tensor(self.converter.midi_to_idxs(read_midi(path)))","fa553911":"mf = read_midi(train_dataset.df.loc[0, 'midi_path'])\nmidi_filename = write_midi(mf)\nwav_filename = midi_to_wav(midi_filename)\nmidi_to_wav_audio = torchaudio.load(wav_filename)[0]\nmidi_to_wav_audio = midi_to_wav_audio[:1, :16000*10]\n\nmidi_dataset = MidiDataset(get_df()[get_df().split == 'train'])\nidxs = midi_dataset[0]['data'] # these are performance events\nmf2 = midi_dataset.converter.idxs_to_midi(idxs)[0]\nmidi_filename = write_midi(mf2)\nwav_filename = midi_to_wav(midi_filename)\nmidi_to_wav_audio2 = torchaudio.load(wav_filename)\nmidi_to_wav_audio2 = midi_to_wav_audio2[0][:1, :16000*10]\n\ndisplay_audio('midi to wav', midi_to_wav_audio)\ndisplay_audio('midi to perf events and back', midi_to_wav_audio2)\ndisplay_audio('original wav recording', train_dataset[0]['data'])","4508f5a1":"# Midi data as performance events\n\nNow, load midi data as performance events (the representation used by PerformanceRNN). As a test, convert the events back into midi and then render it as wav file.","2e2d3644":"# Metadata","27f78110":"# Audio data\n\nCreate a torch dataset to load wav audio data from the dataset."}}