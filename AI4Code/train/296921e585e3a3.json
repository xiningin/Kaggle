{"cell_type":{"7427114d":"code","ba2eef3f":"code","5487661b":"code","1ebf4c62":"code","3c47d4f1":"code","81f12f9b":"code","ab57247b":"code","a615e4cb":"code","ab0f7f6a":"code","eaab4f3a":"code","6647adc5":"code","d15df613":"code","84999897":"code","a397ea91":"code","04175b37":"code","365ea314":"code","924cd109":"code","406c15fc":"code","310a167c":"code","81fcbcbc":"code","272f39e3":"code","d2ebf86a":"code","2225ddfd":"code","aa2e5e38":"code","de04e461":"code","48b1e509":"code","560a7d48":"code","342c08db":"code","99ee88ea":"code","9ef78771":"code","b3fe0552":"code","5704a0db":"code","80ca5fe0":"code","51b2e9af":"code","2a5a7072":"code","fd3c2dfd":"code","0b767c4c":"code","6a509624":"code","fb385b68":"code","cf09f6dd":"code","cd82dcf5":"code","57dd0a9f":"code","08b6a6b9":"markdown","9181c2f0":"markdown","fb3714cc":"markdown","7eb138d6":"markdown","7f0aad47":"markdown","a6e1e096":"markdown","7015bf4c":"markdown","1f685f01":"markdown","55b475f0":"markdown","fedcc359":"markdown","9de37551":"markdown","9e43e348":"markdown","1c197ee9":"markdown","c83fd1aa":"markdown","c0eab129":"markdown","a120447a":"markdown","fb1aeb63":"markdown"},"source":{"7427114d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Switch on setting to allow all outputs to be displayed\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","ba2eef3f":"# Import the datasets\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","5487661b":"# Perform EDA on the train\ntrain.head()\ntrain.shape\ntrain.dtypes\ntrain.describe(include='all')","1ebf4c62":"# Perform EDA on the test\ntest.head()\ntest.shape\ntest.dtypes\ntest.describe(include='all')","3c47d4f1":"# The histogram provides details on the distribution of the variable. Including the box plot shows key parameter summary values.\n# By using plotly we are able to hover over the values and easily understand how the values compare\nfig = px.histogram(train, x=\"target\",\n                   marginal=\"box\")\nfig.show()","81f12f9b":"# Perform quick analysis to review the distribution of the target variable\n# The key variable is the excerpt, so have to extract as much information from this before running regression analysis\n# AIM 1 : build a simple tokenization algorithm to create new features. then apply the different regression techniques and pipelines to help optimise\n# the model build using sklearn\n\n# Output file has to be called submission.csv","ab57247b":"# Extract insights from the excerpt variable\nimport spacy","a615e4cb":"# Initialise spacy\nnlp = spacy.load('en_core_web_sm')","ab0f7f6a":"# Perform initial test on one excerpt\nsample = train.head(5)\nsample\nsample1 = train.loc[0, 'excerpt']","eaab4f3a":"# Create the spacy doc item for review\ndoc = nlp(sample1)\ndoc","6647adc5":"# Reviewing the token, lemma and stopword for each token (item)\nprint(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\n# Review the first 20 values to test the output\nfor token in doc[:20]:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\\t\\t{len(token)}\")","d15df613":"# A few different options for stopwords, spacy and nltk. Lets compare\nimport nltk\nfrom nltk.corpus import stopwords","84999897":"# Comparison of the stop words available\nprint(f\"NLTK : {len(stopwords.words('english'))} \\n {stopwords.words('english')}\")\nprint(f\"Spacy : {len(nlp.Defaults.stop_words)} \\n {nlp.Defaults.stop_words}\")\n\n# Compare the differences\nnltk_set = set(stopwords.words('english'))\nspacy_set = set(nlp.Defaults.stop_words)\n\n# Union - all values\nunion = nltk_set.union(spacy_set)\n# Intersection - seen in both sets\ninter = nltk_set.intersection(spacy_set)\nprint(f\"Seen in both : {len(inter)} \\n {inter}\")\n# Remainder - differences between sets\nnltk_extra = nltk_set - inter\nspacy_extra = spacy_set - inter\nprint(f\"Extra NLTK : {len(nltk_extra)} \\n {nltk_extra}\")\nprint(f\"Extra Spacy : {len(spacy_extra)} \\n {spacy_extra}\")","a397ea91":"from sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import CountVectorizer","04175b37":"#instantiate CountVectorizer() \ncv=CountVectorizer() \n\n# this steps generates word counts for the words in the sample doc\nword_count_vector=cv.fit_transform(sample.excerpt)\n\nword_count_vector.shape","365ea314":"# Compute the IDF values\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \ntfidf_transformer.fit(word_count_vector)","924cd109":"# print idf values \ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n \n# sort ascending \ndf_idf.sort_values(by=['idf_weights'])\n\ndf_idf.describe()","406c15fc":"# Time to compute the TFIDF\n# count matrix \ncount_vector=cv.transform(sample.excerpt) \n \n# tf-idf scores \ntf_idf_vector=tfidf_transformer.transform(count_vector)","310a167c":"feature_names = cv.get_feature_names() \n \n#get tfidf vector for first document \nfirst_document_vector=tf_idf_vector[0] \n \n#print the scores \ndf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","81fcbcbc":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# settings that you use for count vectorizer will go here \ntfidf_vectorizer=TfidfVectorizer(use_idf=True) \n \n# just send in all your docs here \ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(sample.excerpt)","272f39e3":"# get the first vector out (for the first document) \nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n \n# place tf-idf values in a pandas data frame \ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","d2ebf86a":"tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here\nfitted_vectorizer=tfidf_vectorizer.fit(sample.excerpt)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(sample.excerpt)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=sample['id'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['c12129c31']]\ndf_out.sort_values(by=['c12129c31'], ascending=False)","2225ddfd":"# Test on all training data\n# just send in all your docs here\nfitted_vectorizer=tfidf_vectorizer.fit(train.excerpt)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(train.excerpt)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=train['id'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['c12129c31']]\ndf_out.sort_values(by=['c12129c31'], ascending=False)","aa2e5e38":"# Lets create a dictionary to review the key phrase outputs\nfrom collections import defaultdict, Counter\n\n# Returns integers that map to parts of speech\ncounts_dict = doc.count_by(spacy.attrs.IDS['POS'])\n\n# Print the human readable part of speech tags\nfor pos, count in counts_dict.items():\n    human_readable_tag = doc.vocab[pos].text\n    print(human_readable_tag, count)","de04e461":"pos_counts = defaultdict(Counter)\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n    \nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id], len(doc.vocab.strings[orth_id]))","48b1e509":"# Expanding named entities\nfor entity in doc.ents:\n    print(entity.text, entity.label_)\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\nprint(\"Number of sentences\", len([*doc.sents]))\nprint(\"Sentiment\", doc.sentiment)\n\n# Understand the length of sentences\nfor sent in doc.sents:\n    print(sent.start_char, sent.end_char, (sent.end_char - sent.start_char))","560a7d48":"from spacy import displacy\n\n# Display the entities within a sentence\ndisplacy.render(doc, style='ent', jupyter=True)","342c08db":"# Visualise the dependencies within a sentence\n# displacy.render(doc, style='dep', jupyter=True)","99ee88ea":"# Lets apply the nlp instance to each excerpt\ntrain['excerpt_scy'] = train['excerpt'].apply(nlp)","9ef78771":"# Check the data type for the updated column\ntype(train.loc[0, 'excerpt_scy'])\ntrain.head()","b3fe0552":"# Reviewing a different row\ntrain.loc[1,'excerpt_scy']","5704a0db":"# Create the class methods required to run the analysis\nclass NLPMethods():\n    # Create constructor for the class\n#     def __init__():\n    \n    # Number of sentences\n    def number_sentences(self, nlp_text):\n        return len([*nlp_text.sents])\n    \n    # Average length of sentence\n    def average_sentence_length(self, nlp_text):\n        sent_length = list()\n        for sent in nlp_text.sents:\n            sent_length.append(sent.end_char - sent.start_char)\n        return np.mean(sent_length)\n    \n    # Part of speech tags\n    def part_of_speech_tags(self, nlp_text):\n        counts_dict = nlp_text.count_by(spacy.attrs.IDS['POS'])\n        counts_dict1 = {}\n        # Extract the text that matches to the POS value\n        for k, v in counts_dict.items():\n            counts_dict1[nlp_text.vocab[k].text] = v\n        return counts_dict1\n    \n    # Number of spaces\n    def number_spaces(self, nlp_text):\n        dict_pos = self.part_of_speech_tags(nlp_text)\n        if dict_pos.get('SPACE') != None:\n            space = dict_pos.get('SPACE')\n        else:\n            space = 0\n        return space\n    \n    # Part of speech tags - including the word counts\n    def word_counts(self, nlp_text):\n        pos_counts = defaultdict(Counter)\n        for token in nlp_text:\n            pos_counts[token.pos][token.orth] += 1\n        \n        # Create dictionary for the word counts\n        word_counts_dict = {}\n        for pos_id, counts in sorted(pos_counts.items()):\n            pos = nlp_text.vocab.strings[pos_id]\n            for orth_id, count in counts.most_common():\n                word_counts_dict[nlp_text.vocab.strings[orth_id]] = {'count':count, \n                                                                     'length':len(nlp_text.vocab.strings[orth_id]), \n                                                                     'pos':pos}\n        return word_counts_dict\n    \n    # Number of words\n    def number_words(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        return len(dict_word_counts.items())\n    \n    # Longest word\n    def longest_word(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        df = pd.DataFrame(dict_word_counts).T.reset_index().rename(columns={'index':'variable'})\n        return max(df['length'])","80ca5fe0":"# Add columns for the spacy doc\ntrain['num_sentences'] = train['excerpt_scy'].apply(NLPMethods().number_sentences)\ntrain['avg_sentence_length'] = train['excerpt_scy'].apply(NLPMethods().average_sentence_length)\ntrain['pos_dict'] = train['excerpt_scy'].apply(NLPMethods().part_of_speech_tags)\ntrain['num_space'] = train['excerpt_scy'].apply(NLPMethods().number_spaces)\ntrain['wc_dict'] = train['excerpt_scy'].apply(NLPMethods().word_counts)\ntrain['num_words'] = train['excerpt_scy'].apply(NLPMethods().number_words)\ntrain['longest_word'] = train['excerpt_scy'].apply(NLPMethods().longest_word)","51b2e9af":"train.sample(5)","2a5a7072":"# Review the max value target variable\nmax_val = np.max(train['target'])\ntrain_max = train.loc[(train['target']==max_val), :]\ntrain_max","fd3c2dfd":"# Check the reason for the largest target value\ntype(train.loc[2829, 'excerpt_scy'])\ntrain.loc[2829, 'excerpt_scy']\ntrain.loc[2829, 'excerpt']","0b767c4c":"# Review the min value target variable\nmin_val = np.min(train['target'])\ntrain_min = train.loc[(train['target']==min_val), :]\ntrain_min","6a509624":"# Check the reason for the smallest target value\ntype(train.loc[1705, 'excerpt_scy'])\ntrain.loc[1705, 'excerpt_scy']","fb385b68":"# Import libraries\nimport seaborn as sns","cf09f6dd":"# Correlation analysis\ndf = train.loc[:, ['id', 'target', 'num_sentences', 'avg_sentence_length', 'num_space', 'num_words', 'longest_word']]\ndf.head()\n\ncor = df.corr()\nsns.heatmap(cor, annot=True)\nplt.show()","cd82dcf5":"X = df.drop('id', axis=1)\nX.dtypes","57dd0a9f":"# Review a scatter matrix\nfig = px.scatter_matrix(X)\nfig.show()","08b6a6b9":"## Review stop words","9181c2f0":"Spacy appears to cover a wider range of stopwords. Adding the additional 56 words from the NLTK could help to increase the scope of stopwords available for use.","fb3714cc":"## 3. Create the datasets for training the models","7eb138d6":"Initial thoughts of the test dataset\n***\n* Excerpt is the key variable\n* Seven values mean that the validation of the training set will be key to optimise the model. Developing a hold out sample on this size could help","7f0aad47":"Lower the IDF value the more common the value is","a6e1e096":"# 2. Data discovery","7015bf4c":"# EDA of new variables","1f685f01":"Key challenge is to understand the difficulty of the readability challenge. When reviewing how difficult a text is there are a few key areas of interest:\n* Word difficulty\n> * Vocabulary lists : can be used to highlight the proportion of comman words used. The less common a word is the more difficult it can be perceived and understood to be\n> * Word length : longer words are usually seen as more difficult that short. Therefore a correlation could be constructed between the word length and text difficulty\n* Sentence difficulty\n> * Sentence length : longer sentences lead to more difficult text. Have to be aware that the inclusion of colon and semi-colon can impact sentence length as well as the full stop","55b475f0":"# 1a. Exploratory Data Analysis (EDA)","fedcc359":"Tfidfvectorizer Usage","9de37551":"SPACE value appears to correspond to the new line.","9e43e348":"Initial thoughts\n***\n* Extracting information from the \"Excerpt\" column will be key to this analysis\n> * All of the values are unique\n> * EDA required for text will help with future discoveries\n* Target column shows a broad range of values\n> * Distribution shows a larger proportion of negative values (perform a Histogram and box plot to confirm)\n> * There appears to be a negative skew with the mean value lower than the median (50th percentile)","1c197ee9":"# 1b. Data Visualizations","c83fd1aa":"It appears that the word \"paleontologists\" could be causing the difficulty?","c0eab129":"# CommonLit Readability Challenge\n***\nThe joy of reading to discover new insights. Developing a study that understands how to categorize reading materials can be a challenging process. CommonLit have provided Kaggle with the opportunity to develop algorithms that can help to aid administrators, teachers, parents and students to understand how to assign reading material at the appropriate skill level. In this regard the reading material should provide both enjoyment and challenge to help prevent reading skills from plateauing. The path to discover with this project should encouragement the development of Natural Language Processing techniques that are able to categorize \/ grade which book excerpt should be assigned to each reading level.\n\nLet's begin!!","a120447a":"# 1. Import packages and Data","fb1aeb63":"# Review Tfidftransformer & Tfidfvectorizer\n***\nCredit to https:\/\/kavita-ganesan.com\/tfidftransformer-tfidfvectorizer-usage-differences\/#.YOqsn-hKiCo for writing a great introductory article"}}