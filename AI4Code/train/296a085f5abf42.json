{"cell_type":{"ea7cfe8a":"code","132ae243":"code","776537c1":"code","79c7371d":"code","96f8c3fb":"code","92e40a42":"code","c968c17f":"code","ff9b04de":"code","89bdfbd1":"code","69ab3783":"code","fe5969e2":"markdown","a0705007":"markdown","927314e6":"markdown","4bf4c00a":"markdown","162da943":"markdown","dee41b16":"markdown","09c8cd85":"markdown","f2441256":"markdown","6490e84a":"markdown","a26a3ae3":"markdown"},"source":{"ea7cfe8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom pprint import pprint\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","132ae243":"df = pd.read_csv('..\/input\/bmw_pricing_challenge.csv', delimiter=',')","776537c1":"# Check for any nulls\nprint(df.isnull().sum())\n# Show data types\nprint(df.dtypes)","79c7371d":"#Lets drop unwanted columns\ndf.drop([\"maker_key\",\"sold_at\"], axis=1, inplace=True)\n#Convert string\/text to categorical values\ncar_models = df.model_key.copy()\nmodel_labels = df['model_key'].astype('category').cat.categories.tolist()\nmodel_labels_dict = {k: v for k,v in zip(model_labels,list(range(1,len(model_labels)+1)))}\nfuel_labels = df['fuel'].astype('category').cat.categories.tolist()\nfuel_labels_dict = {k: v for k,v in zip(fuel_labels,list(range(1,len(fuel_labels)+1)))}\npaint_labels = df['paint_color'].astype('category').cat.categories.tolist()\npaint_labels_dict =  {k: v for k,v in zip(paint_labels,list(range(1,len(paint_labels)+1)))}\ntype_labels = df['car_type'].astype('category').cat.categories.tolist()\ntype_labels_dict =  {k: v for k,v in zip(type_labels,list(range(1,len(type_labels)+1)))}\n\n\ndf.replace(model_labels_dict, inplace=True)\ndf.replace(fuel_labels_dict, inplace=True)\ndf.replace(paint_labels_dict, inplace=True)\ndf.replace(type_labels_dict, inplace=True)\n\ndf['model_key'] = df['model_key'].astype('category')\n\n#Convert registration_date to integer\ndf['registration_date'] = df['registration_date'].str.replace(\"-\",\"\").astype(int)\nprint(df.dtypes)\n","96f8c3fb":"# Data visualizations\/Insights\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,15))\nax = fig.gca()\nc = car_models.value_counts()\nc.sort_values(ascending=False).plot.bar(width=0.5,edgecolor='k',align='center',linewidth=1)\nplt.xlabel('Models',fontsize=10)\nplt.ylabel('Counts',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('BMW car models',fontsize=10)\nplt.grid()\nplt.ioff()\nplt.show()\n\nfig = plt.figure(figsize = (10,10))\nax = fig.gca()\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\")\nplt.title(\"Correlation\",fontsize=5)\nplt.show()\n\nfig = plt.figure(figsize = (10,15))\nax = fig.gca()\ndf.plot(ax=ax,kind='density',subplots=True,sharex=False)\nplt.title(\"Density\",fontsize=5)\nplt.show()\n\n# =============================================================================\n# fig = plt.figure(figsize = (20,20))\n# ax = fig.gca()\n# sns.pairplot(data=df[0:100],hue=\"price\") # pair plot a subset. Takes too long for the whole data\n# plt.title(\"Pair plot\",fontsize =10)\n# plt.show()\n# \n# =============================================================================","92e40a42":"def feature_importance_plots(regressor,X_train):\n        feature_importances = pd.DataFrame(regressor.feature_importances_,index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n        feature_importances.plot(kind='bar')\n        plt.show()\n\ndef do_prediction(df, stratify):\n    price = df.price.copy()\n    df.drop(['price'],inplace=True,axis = 1)\n    \n    if stratify and 'model_key' in df:\n         X_train, X_test, y_train, y_test = train_test_split(df,price, test_size=0.25, stratify=df['model_key'], random_state=5811) \n    else:\n        X_train, X_test, y_train, y_test = train_test_split(df, price, test_size=0.25, random_state=5811)\n    \n    \n    gbr = GradientBoostingRegressor(loss ='ls',n_estimators=150, max_depth=7,max_leaf_nodes = 9,random_state=5811)\n    # Look at the parameters\n    print('Parameters currently in use:\\n')\n    pprint(gbr.get_params())\n    # Fit the training data\n    gbr.fit (X_train, y_train)\n    # get the predicted values from the test set\n    \n    predicted_price = gbr.predict(X_test)\n    \n    print('GBR R squared: %.4f' % gbr.score(X_test, y_test))\n    lin_mse = mean_squared_error(predicted_price, y_test)\n    lin_rmse = np.sqrt(lin_mse)\n    print('RMSE: %.4f' % lin_rmse)\n    feature_importance_plots(gbr,X_train)\n    \n    \n    forest_reg = RandomForestRegressor(n_estimators=150,min_samples_split=3,random_state=5811)\n    # Look at the parameters\n    print('Parameters currently in use:\\n')\n    pprint(forest_reg.get_params())\n    forest_reg.fit(X_train, y_train)\n    predicted_price = forest_reg.predict(X_test)\n    \n    print('RFR R squared: %.4f' % forest_reg.score(X_test, y_test))\n    lin_mse = mean_squared_error(predicted_price, y_test)\n    lin_rmse = np.sqrt(lin_mse)\n    print('RMSE: %.4f' % lin_rmse)\n    feature_importance_plots(forest_reg,X_train)","c968c17f":"df_copy = df.copy()\ndo_prediction(df.copy(),False)","ff9b04de":"def replicate_low_count(df_copy):\n    # What if between test\/train splits we dont have the models of cars available?\n    # For that we find the bear minimum count and replicate the data. \n    min_counted = c <= 1\n    # Lets populate the data with replicas for the car models\n    \n    print(\"DataFrame size before append:\",len(df_copy))\n    for item in min_counted.iteritems():\n        if item[1]:\n            v = model_labels_dict[item[0]]\n            v2 = df_copy[df_copy['model_key'] == v].copy()\n            if len(v2) > 1:# Keep one copy- this happens when min_counted > 1\n                mean_price = v2[\"price\"].mean()\n                v2 = v2.iloc[[0]].copy()\n                v2[\"price\"] = mean_price\n            for copy_count in range(0,2):\n                df_copy = df_copy.append(v2,ignore_index=True)\n    \n    print(\"DataFrame after append:\",len(df_copy))\n    return  df_copy","89bdfbd1":"df_copy=replicate_low_count(df_copy)\ndo_prediction(df_copy,True)","69ab3783":"# Now lets use the 8 important features as described in the description file of the dataset\ndf_copy = df.copy()\ndf.drop([\"engine_power\",'mileage','paint_color',\"registration_date\",\"model_key\",\"car_type\",\"fuel\"],inplace=True,axis=1)\n\ndo_prediction(df,False)\ndf_copy = replicate_low_count(df_copy)\ndf_copy.drop([\"engine_power\",'mileage','paint_color',\"registration_date\",\"model_key\",\"car_type\",\"fuel\"],inplace=True,axis=1)\ndo_prediction(df_copy,True)","fe5969e2":"# Have a look at some basics","a0705007":"**We use GradientBoostRegressor and RandomForestRegressor to estimate the price of a BMW car\nThe steps included in the kernel are**\n1. Preprocessing the data. This is mostly related to categorising and transforming the data\n2. Splitting the data into testing and training data\n3. Prediction\n\n**Point of Interest**\nWe have one problem in our data. Splitting the data into test\/train sets will also result into the fact that we may miss some car models in either the test or train sets. To address this we replicate the car models that have a minimum car count. This results in better trained\/tested models as shown along the feature importance plots.\nAnd final run of the code uses the 8 features as mentioned by the description of the dataset. Surprisingly these features are of lesser value and are not good for descrimination.\n\n<a href=\"#pre\"> Preprocessing <\/a>\n\n<a href=\"#graphs\"> Graphs <\/a>\n\n<a href=\"#prediction\">Prediction<\/a>\n\n<a href=\"#replicate\">Replication of low count car models<\/a>\n\n<a href=\"#replicate_prediction\">Prediction<\/a>\n\n<a href=\"#desc_features_run\">8 Features based prediction<\/a>\n","927314e6":"<a id='replicate_prediction'><\/a>\n# Replicate prediction","4bf4c00a":"<a id='pre'><\/a>\n# Data Pre-processing","162da943":"# Predicting the price of BMW cars","dee41b16":"<a id='regression_func'><\/a>\n# Regression ","09c8cd85":"<a id='replicate'><\/a>\n# Replicating car models that have a low count in our data","f2441256":"<a id='prediction'><\/a>\n# Prediction","6490e84a":"<a id=\"desc_features_run\"><\/a>\n# 8 Features based prediction","a26a3ae3":"<a id='graphs'><\/a>\n# A visual peek into DATA"}}