{"cell_type":{"6f2566be":"code","d61cfe14":"code","0112980a":"code","4fd1a587":"code","b4848a31":"code","199348b4":"code","259ec6af":"code","1187a4c9":"code","0f87f9a6":"code","2edcca62":"code","399d4c02":"code","652a7f8c":"code","0d0b0d80":"code","7b74655d":"code","5166c59b":"code","d26b4634":"code","a8b715cd":"code","790352a6":"code","b60a13d9":"code","5cc68a4a":"code","2c5c2f3f":"code","5dfe775f":"code","d3f59faa":"code","f58710ca":"code","b82e89bc":"code","43848880":"code","8330abe8":"code","1372d0cd":"code","0704e3a2":"code","00ddc88e":"code","3220ba2f":"code","9e4aa886":"code","4afdcf1d":"code","f1ee43a7":"code","f892678c":"code","6f3044fe":"code","b7fb2c64":"code","cc221bab":"code","49bd8a9b":"code","6eaef7e2":"code","a185c322":"code","a1bdd01a":"code","b8d1bc4d":"code","4e12f0e1":"code","6f722d1c":"code","d2d342a6":"code","6f394118":"code","e78601bd":"code","fb338adb":"code","a5e28089":"code","dae5d1ba":"code","d26b1de4":"code","757ecdbf":"code","49cb5f2c":"code","d8e06ee9":"code","b32344b6":"code","2f8c0bf0":"code","ae9eaa63":"code","2eba9be0":"code","f3d98cc6":"code","1bb081d5":"code","85f3e735":"code","e3b82fab":"code","aa2cd155":"code","307aa607":"code","7db68e67":"code","9b16c03e":"code","259495ab":"code","728abe92":"code","2e8ac0cf":"code","6b43d58f":"code","4a3e8cff":"code","c352b4d5":"code","e1f2b1d6":"code","8b5cb0de":"code","11ebc298":"code","aa13cf4c":"code","4048190a":"code","b15914e8":"markdown","72cf1d9c":"markdown","930c25a6":"markdown","68dd952d":"markdown","9fd272a3":"markdown","782e8605":"markdown","4c17a1df":"markdown","6e70e310":"markdown","31e96f7a":"markdown","a41fefd0":"markdown","2edde255":"markdown","fb0bbc70":"markdown","2d671855":"markdown","6fb2048c":"markdown","d41956a9":"markdown","eb6d426f":"markdown","06009855":"markdown","500df98e":"markdown","12f93667":"markdown","6ad315b1":"markdown","101eeec1":"markdown","449cacfc":"markdown","ab51cee3":"markdown","960728c6":"markdown","c93215ca":"markdown","f658b460":"markdown","f6867ed2":"markdown","079fd0f6":"markdown","d1c55910":"markdown","4afc18aa":"markdown","1b0148ab":"markdown","3ff30697":"markdown","557a8fb8":"markdown","a5c352f7":"markdown","e1ab5ba1":"markdown","204c2ec4":"markdown","b292251f":"markdown","20151ff4":"markdown","fb66bb2f":"markdown","9be27086":"markdown","53dcab62":"markdown","a833471d":"markdown","92feb1f0":"markdown","c450004a":"markdown","6656332c":"markdown","36869caa":"markdown","dd884ba7":"markdown","c1fc1634":"markdown","67ef5368":"markdown","f29855ae":"markdown","c2cc50e5":"markdown","ad01c4bf":"markdown","5e64cd75":"markdown","1c9e5021":"markdown","01b84836":"markdown","17187dc4":"markdown","1315f803":"markdown","de4648f7":"markdown","dd168386":"markdown","987f508e":"markdown","a97ce5e4":"markdown","6785800d":"markdown","792939c7":"markdown","9cb89d58":"markdown","de9fae27":"markdown","1c357dfd":"markdown","c7f4f0f9":"markdown","3629cd93":"markdown","0334c841":"markdown","3120ef71":"markdown","7ceb8a6d":"markdown"},"source":{"6f2566be":"import pandas as pd\npd.options.display.max_columns = None","d61cfe14":"df = pd.read_csv('.\/molecule_activity.csv',sep=';')\ndf.columns[0:20]","0112980a":"df.Comment.value_counts().head(10)","4fd1a587":"df_inconclusive = df[(df.Comment == 'Inconclusive')]\ndf_act_inac = df[(df.Comment == 'Active') | (df.Comment == 'Not Active')]","b4848a31":"# By counts\ndf_count = df_act_inac.groupby(['Standard Type', 'Comment']).count()[['pChEMBL Value', 'Standard Value']]\n# By mean\ndf_mean = df_act_inac.groupby(['Standard Type', 'Comment']).mean()[['pChEMBL Value', 'Standard Value']]\n# By std\ndf_std = df_act_inac.groupby(['Standard Type', 'Comment']).std()[['pChEMBL Value', 'Standard Value']]","199348b4":"df_count","259ec6af":"df_mean","1187a4c9":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndf_act = df_act_inac[df_act_inac['Comment'] == 'Active']\ndf_inact = df_act_inac[df_act_inac['Comment'] == 'Not Active']\n\ndf_act_counts = df_act.groupby('pChEMBL Value', as_index = False).count()[['pChEMBL Value', 'Smiles']]\ndf_inact_counts = df_inact.groupby('pChEMBL Value', as_index = False).count()[['pChEMBL Value', 'Smiles']]\ndf_act_counts.columns = ['pChEMBL Value', 'Relative Count']\ndf_inact_counts.columns = ['pChEMBL Value', 'Relative Count']\n\n# Standarizing\ndf_act_counts['Relative Count'] = (df_act_counts['Relative Count'] \/ df_act_counts['Relative Count'].sum()) * 100\ndf_inact_counts['Relative Count'] = (df_inact_counts['Relative Count'] \/ df_inact_counts['Relative Count'].sum()) * 100","0f87f9a6":"sns.barplot(x = df_act_counts['pChEMBL Value'], y = df_act_counts['Relative Count'])\nplt.xticks(rotation=90)\nplt.title('Relative count for active molecules')\nplt.show()","2edcca62":"sns.histplot(df_act['pChEMBL Value'], bins=20)\nplt.title('Counts for active molecules')","399d4c02":"sns.barplot(x = df_inact_counts['pChEMBL Value'], y = df_inact_counts['Relative Count'])\nplt.xticks(rotation=90)\nplt.title('Relative count for inactive molecules')\nplt.show()","652a7f8c":"sns.histplot(df_inact['pChEMBL Value'], bins=20)\nplt.title('Counts for inactive molecules')\n","0d0b0d80":"sns.histplot(df_inconclusive['pChEMBL Value'], bins=20)\nplt.title('Counts for inconclusive molecules')","7b74655d":"df_std","5166c59b":"columns = ['Smiles', 'Standard Type', 'Standard Relation', 'Standard Value', 'Standard Units', 'pChEMBL Value', 'BAO Label', 'Target Name']\ncolumns_desired_name = ['Smiles', 'Standard_Type', 'Standard_Relation', 'Standard_Value', 'Standard_Units', 'pChEMBL_Value', 'BAO_Label', 'Target_Name']\n\ndf_act = df_act[columns]\ndf_inact = df_inact[columns]\ndf_inconclusive = df_inconclusive[columns]\n\ndf_act.columns = columns_desired_name\ndf_inact.columns = columns_desired_name\ndf_inconclusive.columns = columns_desired_name","d26b4634":"df_act.to_csv('.\/actives.csv', index=False)\ndf_inact.to_csv('.\/inactives.csv', index=False)\ndf_inconclusive.to_csv('.\/inconclusive.csv', index=False)","a8b715cd":"from rdkit import Chem # RDKit libraries for chemistry functions\nfrom rdkit.Chem import Draw # Drawing chemical structures\nimport pandas as pd # Dealing with data in tables\nfrom rdkit.Chem import PandasTools # Manipulating chemical data\nfrom rdkit.Chem import Descriptors # Calculating molecular descriptors\nfrom rdkit.Chem import rdmolops # Additional molecular properties\nimport seaborn as sns # Making graphs \nimport numpy as np\n\n%matplotlib inline","790352a6":"df_act = df_act[['Smiles', 'pChEMBL_Value']]\ndf_inact = df_inact[['Smiles', 'pChEMBL_Value']]\ndf_inconclusive = df_inconclusive[['Smiles', 'pChEMBL_Value']]","b60a13d9":"def molFromSmiles(smiles):\n    if isinstance(smiles, str):\n        return Chem.MolFromSmiles(smiles)\n    else:\n        return False\n    \ndef wtFromMol(mol):\n    if mol:\n        return Descriptors.MolWt(mol)\n    else:\n        return 0\n    \ndef MolLogP(mol):\n    if mol:\n        return Descriptors.MolLogP(mol)\n    else:\n        return 0\n    \ndef GetFormalCharge(mol):\n    if mol:\n        return rdmolops.GetFormalCharge(mol)\n    else:\n        return 0\n\ndef add_property_columns_to_df(df_in):\n    df_in['MW'] = [wtFromMol(molFromSmiles(smiles)) for smiles in df_in.Smiles]\n    df_in[\"logP\"] = [MolLogP(molFromSmiles(smiles)) for smiles in df_in.Smiles]\n    df_in[\"charge\"] = [GetFormalCharge(molFromSmiles(smiles)) for smiles in df_in.Smiles]\n","5cc68a4a":"add_property_columns_to_df(df_act)\nadd_property_columns_to_df(df_inact)\nadd_property_columns_to_df(df_inconclusive)","2c5c2f3f":"df_act = df_act.assign(status='Active')\ndf_inact = df_inact.assign(status='Inactive')\n\ndf_act_inac = pd.concat([df_act, df_inact]).sample(frac=1).sample(frac=1).sample(frac=1)","5dfe775f":"sns.violinplot(df_act_inac[\"status\"],df_act_inac[\"MW\"])","d3f59faa":"sns.violinplot(df_act_inac[\"status\"],df_act_inac[\"logP\"])","f58710ca":"sns.violinplot(df_act_inac[\"status\"],df_act_inac[\"charge\"])","b82e89bc":"import pandas as pd\nimport numpy as np","43848880":"df_act = pd.read_csv('.\/raw_data\/actives.csv')\ndf_inact = pd.read_csv('.\/raw_data\/inactives.csv')\ndf_inc = pd.read_csv('.\/raw_data\/inconclusive.csv')\ndf_act","8330abe8":"df_act_fil = main_filtering(df_act, 'Actives')\ndf_inact_fil = main_filtering(df_inact, 'Inactives')\ndf_inc_fil = main_filtering(df_inc, 'Inconclusives')\n","1372d0cd":"# TODO: Automatizing rd_filters. There may be problems when converting dataframe into a smi file, making the rd_filters dont parce the file\n\n# import subprocess\n\n# datasets = [df_act_fil, df_inact_fil, df_inc_fil]\n# dataset_name = ['actives', 'inactives', 'inconclusives']\n\n# for name, dataset in zip(dataset_name, datasets):\n#     command_folder = f'mkdir .\/procesed\/{name}'\n#     subprocess.call(command_folder, shell=True)\n#     dataset['Smiles'].to_csv(f'.\/procesed\/{name}\/{name}_for_rd.smi', sep='\\t' , header=False, index=False)\n    \n#     command_rd_filters = \" \".join(['rd_filters', 'filter', '--in', f'.\/procesed\/{name}\/{name}_for_rd.smi', '--prefix', 'rd_filtered_lactamase'])\n#     subprocess.call(command_rd_filters, shell=True)\n","0704e3a2":"import pandas as pd\nimport numpy as np\n\n\ndef basic_filtering(df):\n    target_pref_name = (df['Target_Name'] == 'Beta-lactamase AmpC')\n    bao_label = df['BAO_Label'] == 'assay format'\n    standard_relation = df['Standard_Relation'] == \"'='\"\n    standard_type = (df['Standard_Type'] == 'IC50') | (\n        df['Standard_Type'] == 'Potency') | (df['Standard_Type'] == 'Ki')\n\n    df_filtered = df[target_pref_name & bao_label & standard_relation &\n                     standard_type][['Smiles', 'Standard_Value', 'pChEMBL_Value']]\n\n    # Procedemos ahora a eliminar files con valores perdidos\n    smiles_not_null = df_filtered['Smiles'].notnull()\n    smiles_not_empty = df_filtered['Smiles'] != ''\n\n    pchembl_value_not_nut = df_filtered['pChEMBL_Value'].notnull()\n    pchembl_value_not_empty = df_filtered['pChEMBL_Value'] != ''\n\n    df_without_missing = df_filtered[smiles_not_null &\n                                     smiles_not_empty & pchembl_value_not_nut & pchembl_value_not_empty]\n    return df_without_missing\n\n\ndef main_filtering(df, name):\n    print('= = = = = = = = = = = = = = = =')\n    print(\n        f'There are {len(df)} molecules in the {name} dataset before filtering')\n    df_bf = basic_filtering(df)\n    print(\n        f'There are {len(df_bf)} molecules in the {name} dataset after filtering')\n\n    return df_bf\n\n\n\n\n","00ddc88e":"import pandas as pd\nimport numpy as np\n\n\ndef smiles_standarization(df):\n    from rdkit.Chem.MolStandardize import rdMolStandardize\n\n    def cleaning(df_row):\n        smile = df_row[0]\n        stadarized_smiles = None\n        try:\n            stadarized_smiles = rdMolStandardize.StandardizeSmiles(smile)\n        except:\n            print(f'The molecule {smile} is not stadarizable')\n\n        return stadarized_smiles\n\n    df['Smiles'] = df.apply(cleaning, axis='columns').dropna()\n\n    return df\n\n\ndef duplicate_mean_aggregation(df):\n    #df = df[['Smiles', 'pChEMBL_Value']]\n    df = df.groupby('Smiles', as_index=False).mean()\n    return df\n\n\n# def main_filtering(df, name):\n#     print('= = = = = = = = = = = = = = = =')\n#     print(\n#         f'There are {len(df)} molecules in the {name} dataset before filtering')\n#     df_bf = basic_filtering(df)\n#     print(\n#         f'There are {len(df_bf)} molecules in the {name} dataset after filtering')\n\n#     return df_bf\n\n\ndef standarization_and_aggregation(df, name):\n    print('= = = = = = = = = = = = = = = =')\n    print(\n        f'There are {len(df)} molecules in the {name} dataset before standarization_and_aggregation')\n    df_final = duplicate_mean_aggregation(df)\n    df_final = smiles_standarization(df_final)\n\n    print(\n        f'There are {len(df_final)} molecules in the {name} dataset after standarization_and_aggregation')\n\n    return df_final\n\n\nif __name__ == '__main__':\n    df_act_rd = pd.read_csv('.\/raw_data\/actives.csv')\n    df_inact_rd = pd.read_csv('.\/raw_data\/inactives.csv')\n    df_inc_rd = pd.read_csv('.\/raw_data\/inconclusive.csv')\n\n    datasets = [df_act_rd, df_inact_rd, df_inc_rd]\n    dataset_name = ['actives', 'inactives', 'inconclusives']\n    path = '.\/procesed'\n\n    for name, dataset in zip(dataset_name, datasets):\n        df_smi = pd.read_csv(\n            f'{path}\/{name}\/{name}_filtered_lactamase.smi', header=None, sep=\" \")\n        df_smi.columns = ['Smiles', 'MOL_ID']\n        df_smi = duplicate_mean_aggregation(df_smi)\n\n        df_csv = pd.read_csv(\n            f'{path}\/{name}\/{name}.csv')[['Smiles', 'Standard_Value', 'pChEMBL_Value']]\n        df_csv = duplicate_mean_aggregation(df_csv)\n\n        df_join = pd.merge(df_smi, df_csv, on='Smiles')[\n            ['Smiles', 'pChEMBL_Value']]\n        dataset = smiles_standarization(df_join)\n        dataset.to_csv(f'{path}\/{name}\/{name}_final.csv', index=False)\n","3220ba2f":"from rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem.Draw import rdDepictor\nfrom rdkit.Chem.Draw import rdMolDraw2D\nfrom rdkit.Chem import DataStructs\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nrdDepictor.SetPreferCoordGen(True)","9e4aa886":"df_act = pd.read_csv('.\/procesed\/actives\/actives_final.csv')\ndf_inac = pd.read_csv('.\/procesed\/inactives\/inactives_final.csv')\ndf_inc = pd.read_csv('.\/procesed\/inconclusives\/inconclusives_final.csv')","4afdcf1d":"df_act.head(2)","f1ee43a7":"def smiles2mol(smiles):\n    return Chem.MolFromSmiles(smiles)\n\ndef mol2fparr(mol):\n    arr = np.zeros((0,))\n    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=512)\n    DataStructs.ConvertToNumpyArray(fp, arr)\n    return arr","f892678c":"# ACTIVE\nsmiles_act = df_act.values[:, 0]\nfps_act = np.array([mol2fparr(smiles2mol(smile)) for smile in smiles_act])\n\n# INACTIVE\ndf_inac_2 = df_inac[df_inac['pChEMBL_Value'] <= 4.5]\nsmiles_inac = df_inac_2.values[:, 0] \nfps_inac = np.array([mol2fparr(smiles2mol(smile)) for smile in smiles_inac])\n\n# INCONCLUSIVE\npseudo_actives = df_inc['pChEMBL_Value'] >= 5.5\npseudo_inactives = df_inc['pChEMBL_Value'] <= 4.5\ndf_inc_2 = df_inc[pseudo_actives]\nsmiles_inc = df_inc_2.values[:, 0]\nfps_inc = np.array([mol2fparr(smiles2mol(smile)) for smile in smiles_inc])","6f3044fe":"fps_concat = np.concatenate([fps_act, fps_inac, fps_inc])\n\n# For coloring\n# red: Active\n# blue: Inactive\n# gray: Inconclusive\ndf_act, df_inac_2, df_inc_2 = df_act.assign(kind='red'), df_inac_2.assign(kind='blue'), df_inc_2.assign(kind='gray')\ndf_colors = pd.concat([df_act, df_inac_2, df_inc_2])['kind'].values","b7fb2c64":"pca_concat = PCA(n_components=2)\nchemicalspace_concat = pca_concat.fit_transform(fps_concat)","cc221bab":"from matplotlib.pyplot import figure\nfigure(figsize=(15, 12), dpi=72)\nplt.scatter(x = chemicalspace_concat[:, 0], y = chemicalspace_concat[:, 1], c=df_colors, alpha=0.2, facecolors='none')","49bd8a9b":"np.sum(pca_concat.explained_variance_ratio_)","6eaef7e2":"# TODO: t-SNE \nimport seaborn as sns\n\ndef evaluate_components(fp_list):\n    res = []\n    for n_comp in range(2,120):\n        pca = PCA(n_components=n_comp)\n        crds = pca.fit_transform(fp_list) \n        var = np.sum(pca.explained_variance_ratio_)\n        res.append([n_comp,var])\n    return res","a185c322":"comp_res = evaluate_components(fps_concat)","a1bdd01a":"res_df = pd.DataFrame(comp_res,columns=[\"Components\",\"Variance\"])\nax = sns.lineplot(data=res_df,x=\"Components\",y=\"Variance\")\nplt.hlines(0.57, 0, 80, colors='green', linestyle='dashed')\nplt.vlines(80, 0, 0.57, colors='green', linestyle='dashed')\n\nplt.hlines(0.6, 0, 92, colors='red', linestyle='dashed')\nplt.vlines(92, 0, 0.6, colors='red', linestyle='dashed')","b8d1bc4d":"from sklearn.manifold import TSNE\n\npca_concat = PCA(n_components=92)\nchemicalspace_concat = pca_concat.fit_transform(fps_concat)\n\n%time crds_embedded = TSNE(n_components=2).fit_transform(chemicalspace_concat)","4e12f0e1":"tsne_df = pd.DataFrame(crds_embedded,columns=[\"X\",\"Y\"])\ntsne_df","6f722d1c":"figure(figsize=(15, 12), dpi=72)\nplt.scatter(x = tsne_df[df_colors != 'red'].values[:, 0], y = tsne_df[df_colors != 'red'].values[:, 1], c=df_colors[df_colors != 'red'], alpha=0.1, facecolors='none')\nplt.scatter(x = tsne_df.values[df_colors == 'red'][:, 0], y = tsne_df[df_colors == 'red'].values[:, 1], c=df_colors[df_colors == 'red'], alpha=0.3)","d2d342a6":"# ACTIVE\nsmiles_act = df_act.values[:, 0]\nfps_act = [Chem.RDKFingerprint(smiles2mol(smile)) for smile in smiles_act]\n\n# INACTIVE\ninactives = df_inac['pChEMBL_Value'] <= 4.5\ndf_inac_filt = df_inac[inactives]\nsmiles_inac = df_inac_filt.values[:, 0]\nfps_inac = [Chem.RDKFingerprint(smiles2mol(smile)) for smile in smiles_inac]\n\n# INCONCLUSIVE\npseudo_actives = df_inc['pChEMBL_Value'] >= 5.5\ndf_inc_filt = df_inc[pseudo_actives]\nsmiles_inc = df_inc_filt.values[:, 0]\nfps_inc = [Chem.RDKFingerprint(smiles2mol(smile)) for smile in smiles_inc]","6f394118":"from rdkit import DataStructs\n\nnew_positives_indices = []\nfor i, fp_active in enumerate(fps_act):\n    print(f'Estamos en la molecula activa numero { i + 1 }')\n    candidates_from_pseudoactives = []\n    for e, fp_candidate in enumerate(fps_inc):\n        tn = DataStructs.FingerprintSimilarity(fp_active, fp_candidate)\n        similarity_diversity_criteria = (tn >= 0.7) & (tn <= 0.95)\n        candidate_activity_criteria = df_inc_filt.values[:, 1][e] >= df_act.values[:, 1][i]\n        \n        if similarity_diversity_criteria and candidate_activity_criteria:\n            # print(f'Tenemos un candidato')\n            candidates_from_pseudoactives.append((fp_candidate, e, tn))\n\n    for a, candidate in enumerate(candidates_from_pseudoactives):\n        fp_candidate, e, tn_active = candidate\n        candidate_act_similarity_criteria = True\n\n        for u, fp_inactive in enumerate(fps_inac):\n            tn_inac = DataStructs.FingerprintSimilarity(fp_inactive, fp_candidate)\n            candidate_act_similarity_criteria = tn_active > tn_inac\n\n            if not candidate_act_similarity_criteria:\n                break\n\n        if candidate_act_similarity_criteria:\n            # print('Nueva molecula activa')\n            new_positives_indices.append(e)\n\n","e78601bd":"# TODO","fb338adb":"# Delete duplicate molecules since the same inconclusive molecule can be near to two active molecules. \nvalues = list(set(new_positives_indices))\n\nsmiles_inc_to_postive = df_inc_filt.values[values, 0]\ndf_inc_to_postive = pd.DataFrame({'Smiles':smiles_inc_to_postive}).sample(frac=1).assign(is_active = 1)\ndf_inc_to_postive.to_csv('.\/procesed\/inconclusives\/new_positives.csv', index=False)","a5e28089":"# ACTIVE\nactives_final_df = df_act.sample(frac=1).assign(is_active = 1)[['Smiles', 'is_active']].reset_index(drop=True)\nactives_final_df.to_csv('.\/procesed\/actives\/final_actives.csv', index=False)\n\n# INACTIVE\ninactives_final_df = df_inac.sample(frac=1).assign(is_active = 0)[['Smiles', 'is_active']].reset_index(drop=True)\ninactives_final_df.to_csv('.\/procesed\/inactives\/final_inactives.csv', index=False)","dae5d1ba":"import pandas as pd","d26b1de4":"df_actives = pd.read_csv('.\/procesed\/actives\/final_actives.csv')\ndf_inactives = pd.read_csv('.\/procesed\/inactives\/final_inactives.csv')\ndf_new_positives = pd.read_csv('.\/procesed\/inconclusives\/new_positives.csv')\n\nprint(f'There are {len(df_actives) + len(df_new_positives)} active molecules in the dataset')\nprint(f'There are {len(df_inactives)} inactive molecules in the dataset')\n","757ecdbf":"# .sample(frac = 1) is for shuffle the data\ndf_concat = pd.concat([df_actives, df_inactives, df_new_positives]).sample(frac=1).sample(frac=1).sample(frac=1).sample(frac=1).reset_index(drop=True)\ndf_concat_wo_dup = df_concat.drop_duplicates(subset=['Smiles'])","49cb5f2c":"from sklearn.model_selection import train_test_split","d8e06ee9":"df = df_concat_wo_dup.sample(frac = 1).sample(frac = 1).sample(frac = 1).sample(frac = 1).sample(frac = 1)\n\n# Spliting the data\ndf_train, df_test = train_test_split(df, test_size=0.2)\n\nprint(f'There are {len(df)} molecules in the dataset')\nprint(f'There are {len(df_train)} molecules in training dataset')\nprint(f'There are {len(df_test)} molecules in test dataset')","b32344b6":"df = df_train.copy()\ndf = df.sample(frac=1).sample(frac=1) # Just a casual shuffling\n\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\nprint('Oversampling active ones')\n\n# Actives\ndf_active = df[active].sample(frac=2, replace=True)\n\n# Inactives\ndf_inactive = df[inactive].sample(frac=1)\n\n# Concatening\ndf = pd.concat([df_active, df_inactive]).sample(frac=1).sample(frac=1).sample(frac=1)\n\n# Analizing again the active an inactive molecules\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\ndf_train_labeled = df[['Smiles', 'is_active']]\ndf_train_labeled.to_csv('.\/data_for_training_and_testing\/training_ds.csv', index=False)","2f8c0bf0":"df = df_test.copy()\ndf = df.sample(frac=1) # Just a casual shuffling\n\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\ndf_train_labeled = df[['Smiles', 'is_active']]\ndf_train_labeled.to_csv('.\/data_for_training_and_testing\/test_ds.csv', index=False)","ae9eaa63":"df_actives = df[active]\nnum_actives = len(df_actives)\n\ndf_inactives = df[inactive].sample(num_actives)\n\n# Concatening\ndf_dummy_training = pd.concat([df_actives, df_inactives]).sample(frac=1).sample(frac=1).sample(frac=1)\ndf_dummy_training.to_csv('.\/data_for_training_and_testing\/dummy_training_ds.csv', index=False)","2eba9be0":"df_actives = pd.read_csv('.\/procesed\/actives\/final_actives.csv')\ndf_decoys = pd.read_csv('.\/procesed\/actives\/decoys\/decoy_final.csv')\ndf_inactives = pd.read_csv('.\/procesed\/inactives\/final_inactives.csv')\ndf_new_positives = pd.read_csv('.\/procesed\/inconclusives\/new_positives.csv')\n\nprint(f'There are {len(df_actives) + len(df_new_positives)} active molecules in the dataset')\nprint(f'There are {len(df_inactives) + len(df_decoys)} inactive molecules in the dataset')","f3d98cc6":"df_concat = pd.concat([df_actives, df_decoys, df_inactives, df_new_positives]).sample(frac=1).sample(frac=1).sample(frac=1).sample(frac=1).reset_index(drop=True)\ndf_concat_wo_dup = df_concat.drop_duplicates(subset=['Smiles'])","1bb081d5":"df = df_concat_wo_dup.sample(frac = 1).sample(frac = 1).sample(frac = 1).sample(frac = 1).sample(frac = 1)\n\n# Spliting the data\ndf_train, df_test = train_test_split(df, test_size=0.2)\n\nprint(f'There are {len(df)} molecules in the dataset')\nprint(f'There are {len(df_train)} molecules in training dataset')\nprint(f'There are {len(df_test)} molecules in test dataset')\n","85f3e735":"df = df_train.copy()\ndf = df.sample(frac=1).sample(frac=1) # Just a casual shuffling\n\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\nprint('Oversampling active ones')\n\n# Actives\n# NOT OVERSAMPLING\ndf_active = df[active].sample(frac=1, replace=True)\n\n# Inactives\ndf_inactive = df[inactive].sample(frac=1)\n\n# Concatening\ndf = pd.concat([df_active, df_inactive]).sample(frac=1).sample(frac=1).sample(frac=1)\n\n# Analizing again the active an inactive molecules\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\nprint(' - - - - - - - - - - - - ')\n\ndf_train_labeled = df[['Smiles', 'is_active']]\ndf_train_labeled.to_csv('.\/data_for_training_and_testing\/act_inc_dec\/training_wdec_ds.csv', index=False)","e3b82fab":"df = df_test.copy()\ndf = df.sample(frac=1) # Just a casual shuffling\n\nactive = df['is_active'] == 1\ninactive = df['is_active'] == 0\nprint(f'Ther are {len(df[active])} different active molecules')\nprint(f'Ther are {len(df[inactive])} different inactive molecules')\n\n#df['is_active'] = np.where(df['pChEMBL Value'] >= active_cutoff, 1, 0)\ndf_train_labeled = df[['Smiles', 'is_active']]\ndf_train_labeled.to_csv('.\/data_for_training_and_testing\/act_inc_dec\/test_wdec_ds.csv', index=False)","aa2cd155":"from google.colab import drive\ndrive.mount('\/content\/drive')","307aa607":"!pip install -q torch-scatter -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\n!pip install -q torch-sparse -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\n!pip install -q git+https:\/\/github.com\/pyg-team\/pytorch_geometric.git\n\n!pip install ogb\n!pip install rdkit-pypi","7db68e67":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# For Dataset generation and visualization\nfrom rdkit import Chem\n# from rdkit.Chem.Draw import IPythonConsole\n# from rdkit.Chem import Draw\n# IPythonConsole.ipython_useSVG=True  #< set this to False if you want PNGs instead of SVGs\nfrom ogb.graphproppred.mol_encoder import AtomEncoder\nfrom ogb.utils.features import atom_to_feature_vector, bond_to_feature_vector\n\n# Extras\nimport os.path as osp\nfrom ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import matthews_corrcoef\nfrom math import sqrt\n","9b16c03e":"csv_path = '\/content\/drive\/MyDrive\/GNN\/ampc\/training_ds.csv'\nmolecules = pd.read_csv(csv_path).sample(10).values","259495ab":"# Pytorch geometric modules\nfrom torch_geometric.data import Data, Dataset, InMemoryDataset\nfrom torch_geometric.loader import DataLoader\n\n# Torch\nimport torch\n\nclass moleculesDS(InMemoryDataset):\n  def __init__(self, root, csv_path, transform=None, pre_transform=None):\n    self.csv_path = csv_path\n    super().__init__(root, transform, pre_transform)\n    self.data, self.slices = torch.load(self.processed_paths[0])\n\n  @property\n  def raw_file_names(self):\n    return []\n\n  @property\n  def processed_file_names(self):\n    # After preprocesing usinf comment columns and NOT Decoys\n    files = 'final_v2.pt'\n    return files\n\n\n  def download(self):\n    pass\n\n  def process(self):\n    data_list = []\n    molecules = pd.read_csv(self.csv_path).values\n\n    for smiles, act in molecules:\n        y = torch.tensor(act, dtype=torch.float32).reshape(-1, 1)\n        \n        # Throw molecules in wich molecules can not be obtanined\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n        except:\n            mol = None\n        if mol is None:\n            print('mol is none')\n            continue\n\n        all_node_feats = []\n        for atom in mol.GetAtoms():\n            node_feats = atom_to_feature_vector(atom)\n            all_node_feats.append(node_feats)\n\n        all_node_feats = np.asarray(all_node_feats)\n        x = torch.tensor(all_node_feats, dtype=torch.long).view(-1, 9)\n\n        edge_attr = []\n        edge_index = []\n        for bond in mol.GetBonds():\n\n            bond_feats = bond_to_feature_vector(bond)\n            edge_attr.append([bond_feats, bond_feats])\n\n            i = bond.GetBeginAtomIdx()\n            j = bond.GetEndAtomIdx()\n            edge_index += [[i, j], [j, i]]\n\n\n        edge_attr = torch.tensor(edge_attr)\n        edge_attr = edge_attr.to(torch.long).view(-1, 3)\n\n        edge_index = torch.tensor(edge_index)\n        edge_index = edge_index.t().to(torch.long).view(2, -1)\n\n\n        data = Data(x=x, edge_index=edge_index, edge_attr = edge_attr, y=y.reshape(1, 1), smiles=smiles)\n\n        data_list.append(data)\n\n    data, slices = self.collate(data_list)\n    torch.save((data, slices), self.processed_paths[0])\n","728abe92":"from torch_geometric.nn import GATv2Conv, GCNConv\nfrom torch_geometric.nn import global_mean_pool, BatchNorm\n\nfrom torch.nn import Sequential, ModuleList, ReLU, Linear, Dropout\nimport torch.nn.functional as F\n\nfrom torch_geometric.nn.models import AttentiveFP\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_layers, dropout):\n        super(GCN, self).__init__()\n        torch.manual_seed(12345)\n\n        self.emb = AtomEncoder(in_channels)\n        self.bondemb = BondEncoder(3)\n\n        self.AttentiveFP = AttentiveFP(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=1,\n                     edge_dim=3, num_layers=num_layers, num_timesteps=1, dropout=dropout)\n        \n        \n    def forward(self, x, edge_index, edge_attr, batch_index):\n        x = self.emb(x)\n        edge_attr = self.bondemb(edge_attr)\n        x = self.AttentiveFP(x, edge_index, edge_attr, batch_index)\n        return x","2e8ac0cf":"from sklearn.metrics import precision_score, matthews_corrcoef, accuracy_score\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport rdkit.Chem as Chem\nfrom rdkit.Chem import AllChem\n\nfrom statistics import mean\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device is: {DEVICE}')\n\n\ndef get_metrics(y_true, y_pred):\n    y_pred = np.rint(y_pred)\n\n    precision = precision_score(y_true, y_pred)\n    matthews = matthews_corrcoef(y_true, y_pred)\n    accuracy = accuracy_score(y_true, y_pred)\n\n    return precision, matthews, accuracy\n\n\ndef training_step(model, x, edge_index, edge_attr, batch_index, y_target, criterion, optimizer):\n    model.train()\n    optimizer.zero_grad()\n\n    h = model(x, edge_index, edge_attr, batch_index)\n\n    loss = criterion(h.reshape(-1), y_target.reshape(-1))\n    loss.backward()\n\n    optimizer.step()\n\n    return float(loss), h\n\n\n@torch.no_grad()\ndef test_step(model, x, edge_index, edge_attr, batch_index, y_target, criterion):\n    model.eval()\n\n    h = model(x, edge_index, edge_attr, batch_index)\n\n    loss = criterion(h, y_target)\n\n    return float(loss), h\n\n\ndef epoch(model, dataloader, criterion, optimizer, training=True):\n\n    total_loss = 0\n    total_examples = 0\n\n    y_target_list = []\n    h_list = []\n\n    for data in dataloader:\n        data = data.to(DEVICE)\n        y_target = data.y\n        x, edge_index, edge_attr, batch_index = data.x, data.edge_index, data.edge_attr, data.batch\n\n        y_target = y_target.reshape(-1, 1)\n\n        if training:\n            loss, h = training_step(model, x, edge_index, edge_attr, batch_index, y_target, criterion, optimizer)\n        else:\n            loss, h = test_step(model, x, edge_index, edge_attr, batch_index, y_target, criterion)\n\n        total_loss += loss * len(y_target)\n        total_examples += len(y_target)\n\n        y_target_list.append(y_target)\n        h_list.append(h)\n\n    y_true = torch.cat(y_target_list, dim=0).detach().cpu().numpy()\n    y_pred = torch.sigmoid(torch.cat(h_list, dim=0)).detach().cpu().numpy()\n\n    precision_score, matthews_corrcoef, accuracy = get_metrics(y_true, y_pred)\n\n    return total_loss\/total_examples, precision_score, matthews_corrcoef, accuracy\n\n\ndef training_init(EPOCHS, model, dataloaders, criterion, optimizer):\n    train_metrics = []\n    test_metrics = []\n    test_precision_score_list = []\n\n    train_dataloader, test_dataloader = dataloaders\n\n    for e in range(EPOCHS):\n        train_total_loss, train_precision_score, train_matthews_corrcoef, train_accuracy = epoch(\n            model, train_dataloader, criterion, optimizer)\n        train_metrics.append([train_total_loss, train_precision_score, train_matthews_corrcoef, train_accuracy])\n\n        test_total_loss, test_precision_score, test_matthews_corrcoef, test_accuracy = epoch(\n            model, test_dataloader, criterion, optimizer, training=False)\n        test_metrics.append([test_total_loss, test_precision_score, test_matthews_corrcoef, test_accuracy])\n        test_precision_score_list.append(test_precision_score)\n\n        if e % 20 == 0:\n            print(f'Epoch {e}')\n            print(f'loss {train_total_loss:.4f} | precision_score {train_precision_score:.4f} | matthews_corrcoef {train_matthews_corrcoef:.4f} | accuracy {train_accuracy:.4f}')\n            print(f'loss {test_total_loss:.4f} | precision_score {test_precision_score:.4f} | matthews_corrcoef {test_matthews_corrcoef:.4f} | accuracy {test_accuracy:.4f}')\n            print()\n\n    # For optimization\n    test_precision_score = mean(test_precision_score_list[-10:])\n    return test_precision_score, (train_metrics, test_metrics)\n\ndef get_dataset_and_weight(root, file_name, batch_size, shuffle=True):\n    dataset = moleculesDS(root = root, csv_path = file_name)\n    loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n\n    pos_weight = len(dataset.data.y.reshape(-1)) \/ dataset.data.y.reshape(-1).sum()\n    pos_weight = torch.Tensor([pos_weight])\n    print(f'El n\u00famero de valores en el dataset es de: {len(dataset.data.y.reshape(-1))} y tiene {dataset.data.y.sum()} positivos')\n    return loader, pos_weight\n\ndef get_model_criterion_optimizer(pos_weight, lr, hidden_channels, num_layers, dropout, weight_decay = 1**-6):\n    model = GCN(9, hidden_channels, num_layers, dropout)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n    criterion = torch.nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n    return model, criterion, optimizer\n\n# For Optuna\n# - - - - - - - - - - - - - - - - - - - - - - - - \n\n# Variables with the commented trial.suggest... are the optimum value of first experiment\ndef get_model_criterion_optimizer_for_optuna(trial, weight):\n    lr = 0.0015294022249668856 #trial.suggest_float(\"lr\", 1e-6, 1e-2, log=True, )\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-7, 1e-4, log=True)\n    num_layers = 2 #trial.suggest_int('num_layers', 2, 5, step=1)\n    hidden_channels = 320 # trial.suggest_int('hidden_channels', 20, 520, step=100)\n    dropout = trial.suggest_float(\"dropout\", 0.45, 0.55, step=0.05)\n    return get_model_criterion_optimizer(weight, lr, hidden_channels, num_layers, dropout, weight_decay = weight_decay)\n\n# - - - - - - - - - - - - - - - - - - - - - - - - \ndef get_all_optuna(trial, root, file_name, batch_size, shuffle=True):\n    dataloader, weight = get_dataset_and_weight(root, file_name, batch_size, shuffle=shuffle)\n\n    model, criterion, optimizer = get_model_criterion_optimizer_for_optuna(\n        trial, weight)\n\n    return model, criterion, optimizer, dataloader","6b43d58f":"'''\n\"OPTIMAL PARAMETERS: {\n    'lr': 0.0015294022249668856, \n    'num_layers': 2, \n    'hidden_channels': 320\n}\n# optimizing with wd and dropout\nwd = 2.3820455190468508e-05\ndropout = 0.45\n'''\n\nif __name__ == '__main__':\n    batch_size_train = 500\n    batch_size_test = 150\n\n    root = '\/content\/drive\/MyDrive\/GNN\/training'\n    train_file_name = '\/content\/drive\/MyDrive\/ampc_fp_optuna\/test_ds_mk.csv' \n    train_dataloader, weight = get_dataset_and_weight(\n        root, train_file_name, batch_size_train, shuffle=True)\n\n    root = '\/content\/drive\/MyDrive\/GNN\/testing'\n    test_file_name = '\/content\/drive\/MyDrive\/ampc_fp_optuna\/test_ds_mk.csv' \n    test_dataloader, _ = get_dataset_and_weight(\n        root, test_file_name, batch_size_test, shuffle=False)\n\n    num_layers = 2\n    hidden_channels = 320\n    dropout = 0.45\n    lr = 0.0015294022249668856 # learning was inestable, so small rl will be try\n    weight_decay = 2.3820455190468508e-05\n    model, criterion, optimizer = get_model_criterion_optimizer(\n        weight, lr, hidden_channels, num_layers, dropout, weight_decay=weight_decay)\n    \n    # Generate the model.\n    model = model.to(DEVICE)\n    criterion = criterion.to(DEVICE)\n\n    # Training init\n    EPOCHS = 200\n    _, metrics = training_init(EPOCHS, model, [train_dataloader,\n                  test_dataloader], criterion, optimizer)\n    \n    # Metrics unpacking\n    train_metrics, test_metrics = metrics\n    train_metrics, test_metrics = np.array(train_metrics), np.array(test_metrics)\n\n    train_total_loss, train_precision_score, train_matthews_corrcoef, train_accuracy = train_metrics[:,0], train_metrics[:,1], train_metrics[:,2], train_metrics[:,3]\n    test_total_loss, test_precision_score, test_matthews_corrcoef, test_accuracy = test_metrics[:,0], test_metrics[:,1], test_metrics[:,2], test_metrics[:,3]\n\n    # Taken from matplotlib documentation\n    fig, ax1 = plt.subplots(2, 2, figsize=(18,10))\n    t = range(EPOCHS)\n\n\n    # Loss\n    ax1[0, 0].set_title('Loss vs Epoch')\n    ax1[0, 0].set_xlabel('Epoch')\n    ax1[0, 0].set_ylabel('Loss', color='tab:red')\n    ax1[0, 0].tick_params(axis='y', labelcolor='tab:red')\n    ax1[0, 0].plot(t, train_total_loss, color='tab:red')\n    ax1[0, 0].plot(t, test_total_loss, color='chocolate', linestyle='dashed')\n\n    # Presicion\n    ax1[1, 0].set_title('Presicion vs Epoch')\n    ax1[1, 0].set_xlabel('Epoch')\n    ax1[1, 0].set_ylabel('Precision', color='olivedrab')  \n    ax1[1, 0].tick_params(axis='y', labelcolor='olivedrab')\n\n    ax1[1, 0].plot(t, train_precision_score, color='olivedrab')\n    ax1[1, 0].plot(t, test_precision_score, color='palegreen', linestyle='dashed')\n\n    # matthews_corrcoef\n    ax1[1, 1].set_title('matthews_corrcoef vs Epoch')\n    ax1[1, 1].set_xlabel('Epoch')\n    ax1[1, 1].set_ylabel('matthews_corrcoef', color='tab:blue')  \n    ax1[1, 1].tick_params(axis='y', labelcolor='tab:blue')\n\n    ax1[1, 1].plot(t, train_matthews_corrcoef, color='tab:blue')\n    ax1[1, 1].plot(t, test_matthews_corrcoef, color='skyblue', linestyle='dashed')\n\n    # Accuracy\n    ax1[0, 1].set_title('Accuracy vs Epoch')\n    ax1[0, 1].set_ylabel('Accuracy', color='crimson')  \n    ax1[0, 1].set_xlabel('Epoch')\n    ax1[0, 1].tick_params(axis='y', labelcolor='crimson')\n\n    ax1[0, 1].plot(t, train_accuracy, color='crimson')\n    ax1[0, 1].plot(t, test_accuracy, color='pink', linestyle='dashed')\n\n    fig.tight_layout() \n    plt.show()\n","4a3e8cff":"!pip install optuna","c352b4d5":"import torch\nimport optuna\nimport pandas as pd\nfrom optuna.trial import TrialState","e1f2b1d6":"def objective_fp(trial):\n    print('Test dataset: \\n')\n    batch_size_test = 150\n    root = '\/content\/drive\/MyDrive\/GNN\/testing'\n    test_file_name = '\/content\/drive\/MyDrive\/ampc_fp_optuna\/test_ds_mk.csv' \n    test_dataloader, _ = get_dataset_and_weight(\n        root, test_file_name, batch_size_test, shuffle=False)\n\n    # OPTUNA\n    print('Train dataset: \\n')\n    batch_size_train = 500 #trial.suggest_int('batch_size', 411, 511, step=100)\n    root = '\/content\/drive\/MyDrive\/GNN\/training'\n    train_file_name = '\/content\/drive\/MyDrive\/ampc_fp_optuna\/training_ds_mk.csv' \n    model, criterion, optimizer, train_dataloader = get_all_optuna(trial, root, train_file_name, batch_size_train, shuffle=True)\n    \n    # Generate the model.\n    model = model.to(DEVICE)\n    criterion = criterion.to(DEVICE)\n\n    # Training init\n    EPOCHS = 120\n    test_precision_score_mean, metrics = training_init(EPOCHS, model, [train_dataloader,\n                  test_dataloader], criterion, optimizer)\n\n    # Metrics unpacking\n    train_metrics, test_metrics = metrics\n    train_metrics, test_metrics = np.array(train_metrics), np.array(test_metrics)\n\n    train_total_loss, train_precision_score, train_matthews_corrcoef, train_accuracy = train_metrics[:,0], train_metrics[:,1], train_metrics[:,2], train_metrics[:,3]\n    test_total_loss, test_precision_score, test_matthews_corrcoef, test_accuracy = test_metrics[:,0], test_metrics[:,1], test_metrics[:,2], test_metrics[:,3]\n\n    # Taken from matplotlib documentation\n    # Taken from matplotlib documentation\n    fig, ax1 = plt.subplots(2, 2, figsize=(18,10))\n    fig.suptitle(f'Results of the trial {trial.number}', fontsize=16)\n    t = range(EPOCHS)\n\n\n    # Loss\n    ax1[0, 0].set_title('Loss vs Epoch')\n    ax1[0, 0].set_xlabel('Epoch')\n    ax1[0, 0].set_ylabel('Loss', color='tab:red')\n    ax1[0, 0].tick_params(axis='y', labelcolor='tab:red')\n    ax1[0, 0].plot(t, train_total_loss, color='tab:red')\n    ax1[0, 0].plot(t, test_total_loss, color='chocolate', linestyle='dashed')\n\n    # Presicion\n    ax1[1, 0].set_title('Presicion vs Epoch')\n    ax1[1, 0].set_xlabel('Epoch')\n    ax1[1, 0].set_ylabel('Precision', color='olivedrab')  \n    ax1[1, 0].tick_params(axis='y', labelcolor='olivedrab')\n\n    ax1[1, 0].plot(t, train_precision_score, color='olivedrab')\n    ax1[1, 0].plot(t, test_precision_score, color='palegreen', linestyle='dashed')\n\n    # matthews_corrcoef\n    ax1[1, 1].set_title('matthews_corrcoef vs Epoch')\n    ax1[1, 1].set_xlabel('Epoch')\n    ax1[1, 1].set_ylabel('matthews_corrcoef', color='tab:blue')  \n    ax1[1, 1].tick_params(axis='y', labelcolor='tab:blue')\n\n    ax1[1, 1].plot(t, train_matthews_corrcoef, color='tab:blue')\n    ax1[1, 1].plot(t, test_matthews_corrcoef, color='skyblue', linestyle='dashed')\n\n    # Accuracy\n    ax1[0, 1].set_title('Accuracy vs Epoch')\n    ax1[0, 1].set_ylabel('Accuracy', color='crimson')  \n    ax1[0, 1].set_xlabel('Epoch')\n    ax1[0, 1].tick_params(axis='y', labelcolor='crimson')\n\n    ax1[0, 1].plot(t, train_accuracy, color='crimson')\n    ax1[0, 1].plot(t, test_accuracy, color='pink', linestyle='dashed')\n\n    fig.tight_layout() \n    plt.show()\n\n    # Training of the model.\n    return test_precision_score_mean + (mean(train_total_loss[-5:]) - mean(test_total_loss[-5:]))\/4 # An idea to optimize presicion and avoide overfitting\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective_fp, n_trials=20, timeout=None)\n\n    pruned_trials = study.get_trials(\n        deepcopy=False, states=[TrialState.PRUNED])\n    complete_trials = study.get_trials(\n        deepcopy=False, states=[TrialState.COMPLETE])\n\n    print(\"Study statistics: \")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Number of pruned trials: \", len(pruned_trials))\n    print(\"  Number of complete trials: \", len(complete_trials))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: \", trial.value)\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n","8b5cb0de":"# Optimizaci\u00f3n general de la presici\u00f3n usando todos los par\u00e1metros siguientes:'batch_size', 'lr', 'weight_decay', 'num_layers', 'hidden_channels', 'dropout'\n# Optimizaci\u00f3n del Loss usando dropout y weigth decay, usando los mejores par\u00e1metros encontrados en el paso anterior: batch_size:500, lr:0.0015294022249668856, weight_decay: 3.7540522058417304e-06, num_layers:2, hidden_chanes:320\n# I get the idea to use loss in te obj function to try to optimize both presicion an loss dif between train loss and test loss (to avoide overfitting)\n# After that big wd or dropout make very unstable the training, making all metrics fluctuate very hard. So lower limmit where stablish","11ebc298":"study.best_trials","aa13cf4c":"from optuna.visualization.matplotlib import plot_optimization_history\nfrom optuna.visualization.matplotlib import plot_param_importances\n\nplot_param_importances(study);\nplot_optimization_history(study);","4048190a":"PATH = '\/content\/drive\/MyDrive\/ampc_fp_optuna\/ampc.pth'\ntorch.save(model.state_dict(), PATH)","b15914e8":"## Optuna hyperparameter search","72cf1d9c":"To distinguish from inconclusives from the inactive molecules, there will be only used those truly inactive molecules with a pChEMBL value lower than 4.5.","930c25a6":"Active\/Inactive molecules are only present in the Potency Standard value, which is a general representation of the activity. It would be desirable build a dataset based only in one ST but only potency is available. There are also active molecules for the Ki ST but are only 8. Those will be considered in the dataset. ","68dd952d":"### Saving the new active molecules","9fd272a3":"### Count grouping by 'Standard Type' and 'Comment'","782e8605":"## TODO\nSame aproach to select new inactve molecules from the inconclusive pools. Since there are lot more inactive molecules covering the same places that inconclusives, from this aproach will raise much more inactive molecules.","4c17a1df":"# Clustering the data to get new compounds from the inconclusive datapoints\nIn this section we are goning to cluster the data and try to identify wich of the inconclusive datapoints can be categorizted as actives, inactives or ambiguos (wich at the end will we retired from the dataset).\n\nPart of the code was taken from:\n* https:\/\/nbviewer.org\/gist\/iwatobipen\/ba0f60842f8ff5414ed6e5cea598a58b","6e70e310":"Also the inactives molecules have many molecules with high pChEMBL value.","31e96f7a":"## Main filtering\nThis filtering is going to select:\n1. 'Target_Name' == 'Beta-lactamase AmpC'\n2. 'BAO_Label' == 'assay format'\n3. 'Standard_Relation' == '='\n4. 'Standard_Type' == 'IC50') | 'Potency' | 'Ki')\n\nThen a filtering searching for delete mising values on Smiles and pChEMBL_Value columns","a41fefd0":"## Selection of new active molecules (NOT LINKED TO PCA)\n\nThe selection of actives from the inconclusive ones is going to be carried out using the criteria: \n\n* At least 0.7 Tanimoto similarity \n\n* At maximum of 0.95, to ensure some diversity \n\n* Its activity needs to be bigger than the active molecule compared to \n\n* There cannot be an inactive molecule more similar than the closest active molecule. ","2edde255":"## Data Handling of Graphs","fb0bbc70":"# Raw data analisys \n\nAs part of the preprocessing, a quick check of the raw data downloaded from ChEMBL is going to be carried out. \n\nIn this section we will analize the difference between the molecules label as actives or inactive in the column \"Comment\". ","2d671855":"---","6fb2048c":"### Oversampling the training dataset \n\nAt the end, I decided to not oversample the active molecules and consider the imbalance during the training in the loss function. ","d41956a9":"---","eb6d426f":"## Importing the modules and the data","06009855":"La optimizaci\u00f3n usando costo y presici\u00f3n fall\u00f3. Pienso que se debe a que existe mucha diferencia entre train loss y test loss a comparaci\u00f3n del valor de precisi\u00f3n. As\u00ed que optuna est\u00e1 dando prioridad a el loss.","500df98e":"## Importamos las librerias","12f93667":"### Std grouping by 'Standard Type' and 'Comment'","6ad315b1":"Now we save the dataframes in diferent files","101eeec1":"In the inconclusive values, there is not clear bias in the pChEMBL value, that is ok, since it represents a pool of possible active\/inactive values. \nAs a conclusion, there is not clear difference in pChEMBL value between active and inactive molecules since all distributions looks centered. ","449cacfc":" ## It is recomended to check the [result file in the github repository](https:\/\/github.com\/AlfilAlex\/DataProfesor_community_project\/blob\/main\/Merged_Lactamase_v2.ipynb), since it have the figures of the results.","ab51cee3":"---","960728c6":"# Model","c93215ca":"## Reading of files in different dataframes","f658b460":"# Github repository","f6867ed2":"---","079fd0f6":"As we can see, there is a columns called \"Comment\" which is describes as:\n\n*\"Activity comments may provide the overall activity conclusions from the data depositor (e.g. toxic, non-toxic, active, inactive) after taking into account other factors such as counter screens. This can explain cases where compounds with apparently potent activities are flagged as inactive\/inconclusive.\"*\n\nFrom: https:\/\/chembl.gitbook.io\/chembl-interface-documentation\/frequently-asked-questions\/chembl-data-questions","d1c55910":"## Clustering based on a personal criteria\nThe main idea is to select the molecules from the inconclusives, that are similar to the active molecules but different to molecules marked as inactives. ","4afc18aa":"## Split of the data set into 3 categories: \n\nThe main idea is to subtract from the undefined set, the molecules that are active or inactive not using an arbitrary pChEMBL value cutoff. As I found in some ChEMBL preprocessing pipelines, the column *Comment* is used as a first criteria to split the data, so in this case the comments used were: \n\n* Actives\n* Inactives\n* Inconclusive\n\nPapers using this aproach:\n<div class=\"csl-entry\">Mayr, A., Klambauer, G., Unterthiner, T., Steijaert, M., Wegner, J. K., Ceulemans, H., Clevert, D. A., &#38; Hochreiter, S. (2018). Large-scale comparison of machine learning methods for drug target prediction on ChEMBL. <i>Chemical Science<\/i>, <i>9<\/i>(24), 5441\u20135451. https:\/\/doi.org\/10.1039\/C8SC00148K<\/div>\n\nIn [this paper](https:\/\/f1000researchdata.s3.amazonaws.com\/manuscripts\/15276\/9c9a53a2-9a80-4223-bc8d-67aee14df227_11905_-_sereina_riniker_v2.pdf?doi=10.12688\/f1000research.11905.2&numberOfBrowsableCollections=29&numberOfBrowsableInstitutionalCollections=4&numberOfBrowsableGateways=31) the autors did not use the ambiguos results of the HTS study.\n\nhttps:\/\/f1000researchdata.s3.amazonaws.com\/manuscripts\/15276\/9c9a53a2-9a80-4223-bc8d-67aee14df227_11905_-_sereina_riniker_v2.pdf?doi=10.12688\/f1000research.11905.2&numberOfBrowsableCollections=29&numberOfBrowsableInstitutionalCollections=4&numberOfBrowsableGateways=31\n","1b0148ab":"### Mean grouping by 'Standard Type' and 'Comment'","3ff30697":"Community project to find molecules with activity against B-lactamase which is an enzyme important for bacteria resistance against antibiotics. The project is proposed by Chanin Nantasenamat ([dataprofessor](https:\/\/www.youtube.com\/channel\/UCV8e2g4IWQqK71bbzGDEI4Q)).\n\nIn this analisys I re-downloaded the files from ChEMBL and analyzed them since it was difficult to work with the original data. pChEMBL value by its own was not enough to split the data in active and inactive molecules, so after a search in the web I found a preprocessing pipeline that requires some fields not given in the original csv files.\n\nThe file called **MERGED_lactamase_community_project.ipynb** is the merge of all the files numbered. The file represents the sequential steps used in the preprocessing. It is my first project of this kind of work and basically, I'm new in cheminformatics and ML, so if you find and error in my preprocessing, I would appreciate you to start and issue.\n\n## Data directly download from ChEMBL\n\nAfter several try's, I found impossible to train a model with the given data, so I downloaded directly from the ChEMBL. After that, I tried to do the same as with the files given by the Data Profesor, but I did not success too. By reading in literature i found a pipeline where the first split into actives and inactives was given by a particular column called _Comment_.\n\n## Models\n\nUsing the pipeline mentioned above I was able to train GNN model. The GNN was implemented with Pytorch Geometric using the ChEMBL files. The GNN model was able to learn at certain point, so in the future I'll posting in this readme the results. Hyperparameter tunning was carried out with Optuna and it was done in two steps:\n\n1- A big search for various parameters\n\n2- Using best parameters of step 1 and and finding parameters for WD and dropout to avoid overfitting.\n","557a8fb8":"## Saving the molecules datasets","a5c352f7":"## Using the rd_filters by Pat Walters to \nThis part was done in the bash terminal. An try of code implementation is shown below. \nThe program was executed in the *processed* folder. The final output is the file of signature:\n* .\/procesed\/{actives\/inactives\/inconclusives}\/{actives\/inactives\/inconclusives}_filtered_lactamase.smi","e1ab5ba1":"#### Inactives","204c2ec4":"First, we are going to select the columns that are going to be usefull in the future","b292251f":"# Dataset for training and testing preparation","20151ff4":"As we can see, the most repeated values are Inconclusive, Not active, and Active ones. The rest of the values are going to be deleted from the data set since they are ambiguous. \nFirst, we are going to analysis the values of the actives and inactive in the same data frame. ","fb66bb2f":"---","9be27086":"I will use a humble 60% of variance explanitabilty, wich are 92 PC aprox.","53dcab62":"# Training","a833471d":"## Actives + Inconclusives + Decoys","92feb1f0":"### Algoritm to select molecules with more chances to be active","c450004a":"## Saving the dataframes in different csv files","6656332c":"# Standarizing the molecules and aggregation\nThe standarization is a common procedure used in varios pipelines.","36869caa":"Decoy molecules were generated with DUD-E to enhance the feature extraction from the molecules. As seen in this paper, this approach is used to make benchmark dataset, since this approach test the models in its performance to distinguish between active molecules from similar inactive molecules without bias. \n\n* R\u00e9au M, Langenfeld F, Zagury J-F, Lagarde N and Montes M (2018) Decoys Selection in Benchmarking Datasets: Overview and Perspectives. Front. Pharmacol. 9:11. doi: 10.3389\/fphar.2018.00011\n\nDecoys were generated from the active molecules.","dd884ba7":"### Saving the active and inactive molecules","c1fc1634":"### Explained variance\nAs pointed out by [Pat Walters in this Github repo](https:\/\/github.com\/PatWalters\/workshop\/blob\/master\/predictive_models\/2_visualizing_chemical_space.ipynb), the variance asociated with each PC its important when interpreting how representative is our PCA for representing the chemical space. So after a quick check, I found that the explainded variance of the two PC are 5% approx.","67ef5368":"## Visualizing chemical space","f29855ae":"## Analyzing some molecule properties between molecules datasets\n\nPart of the code was taken from the book: \n* Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande - Deep Learning for the Life Sciences_ Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More-O\u2019Reilly Media (2019)","c2cc50e5":"### Saving the test set","ad01c4bf":"# Basic filtering\nIn this section, a basic filtering, standardization, and aggregation of duplicates by dataset is going to be carried out. After that, a rd_filter is going to be applied. ","5e64cd75":"## Dataset visualization","1c9e5021":"Its important to note that a big quantity of the pChEMBL values for active molecules are low, being almost half of the records between 4.25 and 5.0.","01b84836":"### Preparing the molecules used to compare","17187dc4":"\n* We can see that inconclusive molecules are sparse above the chemical space and do not form any clusters. The same for the inactives (red circles), except for some small clusters inside the mass.\n\n* There are in fact a little small one. For my surprise, there are active molecules remarkably close to inactive molecules, which is something that I did not expect.\n\n* It can be considered new active molecules to those inconclusive molecules near to some active molecule, where there aren't inactive molecules and its pChEMBL value is higher than the mean of the active molecules. Any way, the criteria will be using Tanimoto similarity, so the selection in not going to be based on this plot, but this plot is a reference of what migth be happening in a higger dimention.\n","1315f803":"#### Inconclusive","de4648f7":"Molecule properties will be revised again at the end of the preprocess","dd168386":"# Community project","987f508e":"### Creating and saving a dummy training set","a97ce5e4":"# Supplementary code\nUsed instead of importing","6785800d":"## pChEMBL value in the active and inactive sets\nAs the value of the pChEMBL was proposed as the main distintion for active molecules and inactives, it would be pertinet to actually check if it is a good idea.","792939c7":"### Quick check to dataframe structure","9cb89d58":"## Data downloaded from ChEMBL \n\nAs the data presented in the contest did not have all columns. Direct download from ChEMBL was carried out searching for \"Beta-lactamase AmpC\" as the target. The downloaded file was saved as *molecule_activity* ","de9fae27":"## Reading the files","1c357dfd":"## My interpretation\nThis tell us that this comments are good source to distinguish between actives and inactives with more presicion.","c7f4f0f9":"## Saving the model","3629cd93":"No much of the variance is explained by the selected PCA componentes, so this must not be reliable to see the chemical space. As mentioned in the GH repo mentioned above, and in the [sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html), t-SNE is a very resource consuming procedure, so it is better to first reduce the dimension of the fps with a technic as PCA. To do this, is recomended to use a number of PC that carrry with a considerable amount of variance. \n\nThe following code in taken from [Pat Walters in this Github repo](https:\/\/github.com\/PatWalters\/workshop\/blob\/master\/predictive_models\/2_visualizing_chemical_space.ipynb), to check how many PC are necesary to achive a good dimentionality reduction to the use t-SNE.","0334c841":"This part goes interesting, since the mean value in both datasets have very similar pChEMBL values for the active and inactive molecules, but what is weird is that the mean of inactive molecules is higher than the mean of active ones. That means that pChEMBL value is not that reliable to distinguish between actives and inactives. To make this more visual, there will be presented the histogram for the pChEMBL value for each dataset. ","3120ef71":"#### Actives","7ceb8a6d":"## Spliting the dataset for training and teting with sklearn"}}