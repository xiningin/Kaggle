{"cell_type":{"b37d485f":"code","e797abfd":"code","7968fee7":"code","6fbf6ffb":"code","3d1959ab":"code","ecacd546":"code","32e8d4ee":"code","52e54976":"code","ce4763fb":"code","4c14dca7":"code","f8274e54":"code","4326dd89":"code","83ff4cea":"code","720c65a1":"code","b1833f69":"code","aaf1ea93":"code","8487576b":"code","895099d7":"markdown","b74970de":"markdown","7ee22a68":"markdown","d60834bd":"markdown"},"source":{"b37d485f":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom time import perf_counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import plot_model \nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.utils import to_categorical \nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras import backend, models\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\nimport numpy as np\nimport time\nimport copy\nfrom random import randint\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","e797abfd":"dir= Path('..\/input\/natural-images\/')\nfilepaths = list(dir.glob(r'**\/*.jpg'))\nlabels = os.listdir('..\/input\/natural-images\/data\/natural_images\/')\nprint(labels)","7968fee7":"from IPython.display import Image, display\nnum = []\nfor label in labels:\n    path = '..\/input\/natural-images\/data\/natural_images\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    k = 0\n    print('\\n', label.upper())\n    for image_path in folder_data:\n        if k < 2:\n            display(Image(path+image_path))\n        k = k+1\n    num.append(k)\n    print('there are ', k,' images in ', label, 'class')","6fbf6ffb":"#Displaying the distribution of the classes. The dataset is almost balanced one\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (8,8))\nplt.bar(labels, num)\nplt.title('NUMBER OF IMAGES CONTAINED IN EACH CLASS')\nplt.xlabel('classes')\nplt.ylabel('count')\nplt.show()","3d1959ab":"x_data =[]\ny_data = []\nimport cv2\nfor label in labels:\n    path = '..\/input\/natural-images\/data\/natural_images\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    for image_path in folder_data:\n        image = cv2.imread(path+image_path)\n        image_resized = cv2.resize(image, (32,32)) #resizing the image into (32,32)\n        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY) #Coverting into gray scale\n        x_data.append(np.array(gray))\n        y_data.append(label)","ecacd546":"x_data = np.array(x_data)\ny_data = np.array(y_data)\ny_encoded = LabelEncoder().fit_transform(y_data) # Converting the labels into an numeric format","32e8d4ee":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_encoded, test_size = 0.2)\n# Reshaping the data into a single 1D array to input into the nodes\n# 5519 denotes the number of images and 1024 denotes the number of pixels (32*32) \nx_train=x_train.reshape([5519,1024])\nx_test= x_test.reshape([1380,1024])","52e54976":"def softmax_crossentropy_with_logits(logits, reference_answers):\n    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers                 \n    logits_for_answers = logits[np.arange(len(logits)), reference_answers]    \n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits), axis=-1))    \n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits, reference_answers):\n    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)), reference_answers] = 1    \n    softmax = np.exp(logits) \/ np.exp(logits).sum(axis=-1,keepdims=True)    \n    return (- ones_for_answers + softmax) \/ logits.shape[0]\n\n# A building block. Each layer is capable of performing two things:\n#  - Process input to get output:           output = layer.forward(input)\n#  - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n# Some layers also have learnable parameters which they update during layer.backward.\nclass Layer(object):\n    def __init__(self):        \n        pass\n    \n    def forward(self, input):\n        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        # A dummy layer just returns whatever it gets as input.\n        return input\n    \n    def backward(self, input, grad_output):\n        # Performs a backpropagation step through the layer, with respect to the given input.\n        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n        # d loss \/ d x  = (d loss \/ d layer) * (d layer \/ d x)\n        # Luckily, we already receive d loss \/ d layer as input, so you only need to multiply it by d layer \/ d x.\n        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss \/ d layer\n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly \n        num_units = input.shape[1]\n        d_layer_d_input = np.eye(num_units)\n        return np.dot(grad_output, d_layer_d_input) # chain rule\n\n\nclass ReLU(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        relu_forward = np.maximum(0, input)\n        return relu_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        relu_grad = input > 0\n        return grad_output * relu_grad\nclass Sigmoid(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        sigmoid_forward = 1\/(1+np.exp(-input))\n        return sigmoid_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        sigmoid_grad=1\/(1+np.exp(-input))*(1-1\/(1+np.exp(-input)))\n        return grad_output * sigmoid_grad\nclass Tanh(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        tanh_forward = (np.exp(input) - np.exp(-input)) \/ (np.exp(input) + np.exp(-input))\n        return tanh_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        tanh_grad=1-(np.tanh(input))**2\n        return grad_output * tanh_grad\n\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate = 0.1):\n        # A dense layer is a layer which performs a learned affine transformation: f(x) = <W*x> + b\n        self.learning_rate = learning_rate\n        self.weights = np.random.normal(loc=0.0, scale = np.sqrt(2 \/ (input_units + output_units)), size = (input_units, output_units))\n        self.biases = np.zeros(output_units)\n    \n    def forward(self, input):\n        # Perform an affine transformation: f(x) = <W*x> + b        \n        # input shape: [batch, input_units]\n        # output shape: [batch, output units]        \n        return np.dot(input, self.weights) + self.biases\n    \n    def backward(self, input, grad_output):\n        # compute d f \/ d x = d f \/ d dense * d dense \/ d x where d dense\/ d x = weights transposed\n        grad_input = np.dot(grad_output, self.weights.T)\n        \n        # compute gradient w.r.t. weights and biases\n        grad_weights = np.dot(input.T, grad_output)\n        grad_biases = grad_output.mean(axis=0) * input.shape[0]\n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        \n        # Here we perform a stochastic gradient descent step. \n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n        \n        return grad_input\n\n    \nclass MCP(object):\n    def __init__(self):\n        self.layers = []\n        \n    def add_layer(self, layer):\n        self.layers.append(layer)\n    \n    def forward(self, X):\n        # Compute activations of all network layers by applying them sequentially.\n        # Return a list of activations for each layer. \n        activations = []\n        input = X\n        \n        # Looping through each layer\n        for l in self.layers:\n            activations.append(l.forward(input))\n            # Updating input to last layer output\n            input = activations[-1]\n    \n        assert len(activations) == len(self.layers)\n        return activations\n    \n    \n    def train_batch(self, X, y):\n        # Train our network on a given batch of X and y.\n        # We first need to run forward to get all layer activations.\n        # Then we can run layer.backward going from last to first layer.\n        # After we have called backward for all layers, all Dense layers have already made one gradient step.\n        \n        layer_activations = self.forward(X)\n        layer_inputs = [X] + layer_activations  # layer_input[i] is an input for layer[i]\n        logits = layer_activations[-1]\n        \n        # Compute the loss and the initial gradient    \n        y_argmax =  y.argmax(axis=1)        \n        loss = softmax_crossentropy_with_logits(logits, y_argmax)\n        loss_grad = grad_softmax_crossentropy_with_logits(logits, y_argmax)\n    \n        # Propagate gradients through the network\n        # Reverse propogation as this is backprop\n        for layer_index in range(len(self.layers))[::-1]:\n            layer = self.layers[layer_index]        \n            loss_grad = layer.backward(layer_inputs[layer_index], loss_grad) # grad w.r.t. input, also weight updates\n        \n        return np.mean(loss)\n    \n    def train(self, X_train, y_train, n_epochs = 25, batch_size = 32):\n        train_log = []  \n        loss_graph=[]\n        \n        for epoch in range(n_epochs):        \n            for i in range(0, X_train.shape[0], batch_size):\n                # Get pair of (X, y) of the current minibatch\/chunk\n                x_batch = np.array([x.flatten() for x in X_train[i:i + batch_size]])\n                y_batch = np.array([y for y in y_train[i:i + batch_size]])        \n                loss=self.train_batch(x_batch, y_batch)\n                            \n            loss_graph.append(loss)\n            train_log.append(np.mean(self.predict(X_train) ==  y_train.argmax(axis=-1)))                \n            #print(f\"Epoch: {epoch + 1}, Train accuracy: {train_log[-1]}\")  \n            print(f\"Epoch: {epoch + 1}, loss: {loss}, Train accuracy: {train_log[-1]}\") \n        return train_log,loss_graph\n    \n    def predict(self, X):\n        # Compute network predictions. Returning indices of largest Logit probability\n        logits = self.forward(X)[-1]\n        return logits.argmax(axis=-1)\n","ce4763fb":"def normalize(X):\n    X_normalize = (X - np.min(X)) \/ (np.max(X) - np.min(X))\n    return X_normalize   \n\ndef one_hot(a, num_classes):\n    return np.squeeze(np.eye(num_classes)[a.reshape(-1)]) \n\nx_train = (np.array([np.ravel(x) for x in x_train]))\nx_test = (np.array([np.ravel(x) for x in x_test]))\ny_train = np.array([one_hot(np.array(y, dtype=int), 8) for y in y_train], dtype=int)\ny_test = np.array([one_hot(np.array(y, dtype=int), 8) for y in y_test], dtype=int)\n\nprint('X_train.shape', x_train.shape)\nprint('Y_train.shape', y_train.shape)\ninput_size = x_train.shape[1]\noutput_size = y_train.shape[1]\n\nnetwork = MCP()\nnetwork.add_layer(Dense(input_size, 100, learning_rate = 0.01))\n#network.add_layer(ReLU())\nnetwork.add_layer(Sigmoid())\n#network.add_layer(Tanh())\n\nnetwork.add_layer(Dense(100, 200, learning_rate = 0.01))\n#network.add_layer(ReLU())\nnetwork.add_layer(Sigmoid())\n#network.add_layer(Tanh())\nnetwork.add_layer(Dense(200, output_size))\n\ntrain_log,loss_g = network.train(x_train, y_train, n_epochs = 100, batch_size = 64)\nplt.plot(train_log,label = 'train accuracy')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\nplt.plot(loss_g,label = 'loss error')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\n\n\ntest_corrects = len(list(filter(lambda x: x == True, network.predict(x_test) ==  y_test.argmax(axis=-1))))\ntest_all = len(x_test)\ntest_accuracy = test_corrects\/test_all #np.mean(test_errors)\nprint(f\"Test accuracy = {test_corrects}\/{test_all} = {test_accuracy}\")","4c14dca7":"x_data =[]\ny_data = []\nimport cv2\nfor label in labels:\n    path = '..\/input\/natural-images\/data\/natural_images\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    for image_path in folder_data:\n        image = cv2.imread(path+image_path)\n        image_resized = cv2.resize(image, (32,32))\n        x_data.append(np.array(image_resized))\n        y_data.append(label)\n#Converting into numpy array\nx_data = np.array(x_data)\ny_data = np.array(y_data)\n#converting the y_data into categorical:\n\ny_encoded = LabelEncoder().fit_transform(y_data)\ny_categorical = to_categorical(y_encoded)\nr = np.arange(x_data.shape[0])\nnp.random.seed(42)\nnp.random.shuffle(r)\nX = x_data[r]\nY = y_categorical[r]","f8274e54":"def normalize(X):\n    X_normalize = (X - np.min(X)) \/ (np.max(X) - np.min(X))\n    return X_normalize\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)","4326dd89":"x_train= normalize(x_train)\nx_test= normalize(x_test)","83ff4cea":"cnn = tf.keras.models.Sequential()\n\n# Step 1 - Convolution\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='sigmoid', input_shape=[32, 32, 3]))\n\n\n# Step 2 - Pooling\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n\n# Adding convolutional layer\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='sigmoid'))\n\n\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n\n# Step 3 - Flattening\ncnn.add(tf.keras.layers.Flatten())\n\n# Step 4 - Full Connection\ncnn.add(tf.keras.layers.Dense(units=128, activation='sigmoid'))\n\n# Step 5 - Output Layer\ncnn.add(tf.keras.layers.Dense(units=8, activation='softmax'))\n\n# Part 3 - Training the CNN\n# ca\n# Compiling the CNN\nopt = keras.optimizers.Adam(learning_rate=0.0001)\ncnn.compile(optimizer = opt, \n            loss = 'categorical_crossentropy', \n            metrics = ['accuracy'])","720c65a1":"cnn.summary()","b1833f69":"history = cnn.fit(x_train, y_train, epochs=100, validation_split=0.2)","aaf1ea93":"# Plotting the accuracy and validation accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","8487576b":"score = cnn.evaluate(x_test, y_test, verbose=0)\nprint(f'Test loss: {score[0]} \/ Test accuracy: {score[1]}')","895099d7":"# CNN network","b74970de":"# Section A.1","7ee22a68":"# ANN network","d60834bd":"# Section A.2"}}