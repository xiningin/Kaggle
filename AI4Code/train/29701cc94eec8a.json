{"cell_type":{"95a8dc1f":"code","54c13ccc":"code","86a6eeda":"code","a84919ab":"code","41fe824d":"code","6babd160":"code","ba1e27ea":"code","d3e6557d":"code","4574e195":"code","20f92b01":"code","6c72dff4":"code","cb019d06":"code","0b5474af":"code","465444c0":"code","3b63a19f":"code","ec194b44":"code","42cc4d6b":"code","b416cff9":"code","4cd85840":"code","c862863b":"code","0bf37db0":"code","0e4576f1":"code","7602d937":"code","282f4617":"code","559a8803":"code","f49a6f2e":"code","cd9dfb3c":"code","069af379":"code","c34dd84a":"code","369dfad3":"code","ff6d706d":"code","12815cad":"code","2909203a":"code","68359746":"code","06d93e22":"code","14582a82":"code","8acf150f":"code","af51c3ca":"markdown","251e3ce5":"markdown","b1b00005":"markdown","bb95afd8":"markdown","dd45da3f":"markdown"},"source":{"95a8dc1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54c13ccc":"import bz2\nimport re\n\ntrain =  '\/kaggle\/input\/amazonreviews\/train.ft.txt.bz2'\ntest = '\/kaggle\/input\/amazonreviews\/test.ft.txt.bz2'\n\ntrain_file=bz2.BZ2File(train)\ntest_file=bz2.BZ2File(test)","86a6eeda":"train_file_lines = train_file.readlines()\ntest_file_lines=test_file.readlines()\ntrain_file\n","a84919ab":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]\ntrain_file_lines[:2]","41fe824d":"[0 if x.split(' ')[0]=='__label__1' else 1 for x in train_file_lines[:10]]\n[x.split(' ',1)[1][:-1].lower() for x in train_file_lines[:2]]","6babd160":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n                                                       \nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","ba1e27ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport nltk\nfrom nltk.corpus import stopwords\n#from nltk.classify import SklearnClassifier\n\n#from wordcloud import WordCloud,STOPWORDS\n#import matplotlib.pyplot as plt\n#%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from subprocess import check_output","d3e6557d":"train_data={'Sentence': train_sentences, 'Labels': train_labels}\ntrained_data=pd.DataFrame(train_data)\ntest_data={'Sentence': test_sentences, 'Labels': test_labels}\ntested_data=pd.DataFrame(test_data)\ntrain, test=trained_data.head(900), tested_data.head(100)","4574e195":"sents=[]\nalll=[]\nstopwords_set=set(stopwords.words('english'))\n\ndef word_clean(train):\n    for _,row in train.iterrows():\n        words_filtered=[e.lower() for e in row.Sentence.split() if len(e)>=3]\n        words_filtered=[word for word in words_filtered if 'http' not in word\n                       and not word.startswith('@')\n                       and not word.startswith('#')\n                       and word !='RT']\n        words_filtered=[word for word in words_filtered if not word in stopwords_set]\n        sents.append((words_filtered, row.Labels))\n        alll.extend(words_filtered)\ncleaned_train=word_clean(train)","20f92b01":"def get_word_features(wordlist):\n    wordlists=nltk.FreqDist(wordlist)\n    features=list(wordlists.keys())\n    return features\n\nw_features=get_word_features(alll)\nw_features[:5]","6c72dff4":"def extract_features(doc):\n    doc_u=set(doc)\n    features={}\n    for word in w_features:\n        features['contains(%s)' %word]=(word in doc_u)\n    return features","cb019d06":"training_set=nltk.classify.apply_features(extract_features, sents)\n","0b5474af":"classifier=nltk.NaiveBayesClassifier.train(training_set)\n","465444c0":"train_pos=train[train['Labels']==1]['Sentence']\ntrain_neg=train[train['Labels']==0]['Sentence']\ntest_pos=test[test['Labels']==1]['Sentence']\ntest_neg=test[test['Labels']==0]['Sentence']\n","3b63a19f":"train_pos[1]","ec194b44":"test_neg.head()","42cc4d6b":"neg_cnt=0\npos_cnt=0\nfor obj in test_neg:\n    res=classifier.classify(extract_features(obj.split()))\n    if (res==0):\n        neg_cnt+=1\n        \nfor obj in test_pos:\n    res=classifier.classify(extract_features(obj.split()))\n    if (res==1):\n        pos_cnt+=1\n\nprint('[Negative]: %s\/%s' %(len(test_neg), neg_cnt))\nprint('[Positive]: %s\/%s' %(len(test_pos), pos_cnt))","b416cff9":"acc=((neg_cnt+pos_cnt)\/(len(test_neg)+len(test_pos)))*100\nacc","4cd85840":"classifier.show_most_informative_features(5)\n","c862863b":"contractions= {'you\u2019ve': 'you have', 'you\u2019re': 'you are', 'you\u2019ll\u2019ve': 'you shall have', 'you\u2019ll': 'you will', 'you\u2019d\u2019ve': 'you would have', 'you\u2019d': 'you would', 'y\u2019all\u2019ve': 'you all have', 'y\u2019all\u2019re': 'you all are', 'y\u2019all\u2019d\u2019ve': 'you all would have', 'y\u2019all\u2019d': 'you all would', 'y\u2019all': 'you all', 'wouldn\u2019t\u2019ve': 'would not have', 'wouldn\u2019t': 'would not', 'would\u2019ve': 'would have', 'won\u2019t\u2019ve': 'will not have', 'won\u2019t': 'will not', 'will\u2019ve': 'will have', 'why\u2019ve': 'why have', 'why\u2019s': 'why is', 'who\u2019ve': 'who have', 'who\u2019s': 'who is', 'who\u2019ll\u2019ve': 'who will have', 'who\u2019ll': 'who will', 'where\u2019ve': 'where have', 'where\u2019s': 'where is', 'where\u2019d': 'where did', 'when\u2019ve': 'when have', 'when\u2019s': 'when is', 'what\u2019ve': 'what have', 'what\u2019s': 'what is', 'what\u2019re': 'what are', 'what\u2019ll\u2019ve': 'what will have', 'what\u2019ll': 'what will', 'weren\u2019t': 'were not', 'we\u2019ve': 'we have', 'we\u2019re': 'we are', 'we\u2019ll\u2019ve': 'we will have', 'we\u2019ll': 'we will', 'we\u2019d\u2019ve': 'we would have', 'we\u2019d': 'we would', 'wasn\u2019t': 'was not', 'to\u2019ve': 'to have', 'they\u2019ve': 'they have', 'they\u2019re': 'they are', 'they\u2019ll\u2019ve': 'they will have', 'they\u2019ll': 'they will', 'they\u2019d\u2019ve': 'they would have', 'they\u2019d': 'they would', 'there\u2019s': 'there is', 'there\u2019d\u2019ve': 'there would have', 'there\u2019d': 'there would', 'that\u2019s': 'that is', 'that\u2019d\u2019ve': 'that would have', 'that\u2019d': 'that would', 'so\u2019s': 'so is', 'so\u2019ve': 'so have', 'shouldn\u2019t\u2019ve': 'should not have', 'shouldn\u2019t': 'should not', 'should\u2019ve': 'should have', 'she\u2019s': 'she is', 'she\u2019ll\u2019ve': 'she will have', 'she\u2019ll': 'she will', 'she\u2019d\u2019ve': 'she would have', 'she\u2019d': 'she would', 'shan\u2019t\u2019ve': 'shall not have', 'sha\u2019n\u2019t': 'shall not', 'shan\u2019t': 'shall not', 'oughtn\u2019t\u2019ve': 'ought not have', 'oughtn\u2019t': 'ought not', 'o\u2019clock': 'of the clock', 'needn\u2019t\u2019ve': 'need not have', 'needn\u2019t': 'need not', 'mustn\u2019t\u2019ve': 'must not have', 'mustn\u2019t': 'must not', 'must\u2019ve': 'must have', 'mightn\u2019t\u2019ve': 'might not have', 'mightn\u2019t': 'might not', 'might\u2019ve': 'might have', 'mayn\u2019t': 'may not', 'ma\u2019am': 'madam', 'let\u2019s': 'let us', 'it\u2019s': 'it is', 'it\u2019ll\u2019ve': 'it will have', 'it\u2019ll': 'it will', 'it\u2019d\u2019ve': 'it would have', 'it\u2019d': 'it would', 'isn\u2019t': 'is not', 'I\u2019ve': 'I have', 'I\u2019m': 'I am', 'I\u2019ll\u2019ve': 'I will have', 'I\u2019ll': 'I will', 'I\u2019d\u2019ve': 'I would have', 'I\u2019d': 'I would', 'how\u2019s': 'how is', 'how\u2019ll': 'how will', 'how\u2019d\u2019y': 'how do you', 'how\u2019re': 'how are', 'how\u2019d': 'how did', 'he\u2019s': 'he is', 'he\u2019ll\u2019ve': 'he will have', 'he\u2019ll': 'he will', 'he\u2019d\u2019ve': 'he would have', 'he\u2019d': 'he would', 'haven\u2019t': 'have not', 'hasn\u2019t': 'has not', 'hadn\u2019t\u2019ve': 'had not have', 'hadn\u2019t': 'had not', 'don\u2019t': 'do not', 'doesn\u2019t': 'does not', 'didn\u2019t': 'did not', 'couldn\u2019t\u2019ve': 'could not have', 'couldn\u2019t': 'could not', 'could\u2019ve': 'could have', '\u2019cause': 'because', 'can\u2019t\u2019ve': 'can not have', 'can\u2019t': 'can not', 'aren\u2019t': 'are not', 'ain\u2019t': 'are not', 'dec.': 'december', 'nov.': 'november', 'oct.': 'october', 'sep.': 'september', 'aug.': 'august', 'jul.': 'july', 'jun.': 'june', 'apr.': 'april', 'mar.': 'march', 'feb.': 'february', 'jan.': 'january', \"you've\": 'you have', \"you're\": 'you are', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd've\": 'you would have', \"you'd\": 'you would', \"y'all've\": 'you all have', \"y'all're\": 'you all are', \"y'all'd've\": 'you all would have', \"y'all'd\": 'you all would', \"y'all\": 'you all', \"wouldn't've\": 'would not have', \"wouldn't\": 'would not', \"would've\": 'would have', \"won't've\": 'will not have', \"won't\": 'will not', \"will've\": 'will have', \"why've\": 'why have', \"why's\": 'why is', \"who've\": 'who have', \"who's\": 'who is', \"who'll've\": 'who will have', \"who'll\": 'who will', \"where've\": 'where have', \"where's\": 'where is', \"where'd\": 'where did', \"when've\": 'when have', \"when's\": 'when is', \"what've\": 'what have', \"what's\": 'what is', \"what're\": 'what are', \"what'll've\": 'what will have', \"what'll\": 'what will', \"weren't\": 'were not', \"we've\": 'we have', \"we're\": 'we are', \"we'll've\": 'we will have', \"we'll\": 'we will', \"we'd've\": 'we would have', \"we'd\": 'we would', \"wasn't\": 'was not', \"to've\": 'to have', \"they've\": 'they have', \"they're\": 'they are', \"they'll've\": 'they will have', \"they'll\": 'they will', \"they'd've\": 'they would have', \"they'd\": 'they would', \"there's\": 'there is', \"there'd've\": 'there would have', \"there'd\": 'there would', \"that's\": 'that is', \"that'd've\": 'that would have', \"that'd\": 'that would', \"so's\": 'so is', \"so've\": 'so have', \"shouldn't've\": 'should not have', \"shouldn't\": 'should not', \"should've\": 'should have', \"she's\": 'she is', \"she'll've\": 'she will have', \"she'll\": 'she will', \"she'd've\": 'she would have', \"she'd\": 'she would', \"shan't've\": 'shall not have', \"sha'n't\": 'shall not', \"shan't\": 'shall not', \"oughtn't've\": 'ought not have', \"oughtn't\": 'ought not', \"o'clock\": 'of the clock', \"needn't've\": 'need not have', \"needn't\": 'need not', \"mustn't've\": 'must not have', \"mustn't\": 'must not', \"must've\": 'must have', \"mightn't've\": 'might not have', \"mightn't\": 'might not', \"might've\": 'might have', \"mayn't\": 'may not', \"ma'am\": 'madam', \"let's\": 'let us', \"it's\": 'it is', \"it'll've\": 'it will have', \"it'll\": 'it will', \"it'd've\": 'it would have', \"it'd\": 'it would', \"isn't\": 'is not', \"I've\": 'I have', \"I'm\": 'I am', \"I'll've\": 'I will have', \"I'll\": 'I will', \"I'd've\": 'I would have', \"I'd\": 'I would', \"how's\": 'how is', \"how'll\": 'how will', \"how'd'y\": 'how do you', \"how're\": 'how are', \"how'd\": 'how did', \"he's\": 'he is', \"he'll've\": 'he will have', \"he'll\": 'he will', \"he'd've\": 'he would have', \"he'd\": 'he would', \"hasn't\": 'has not',\"haven't\": 'have not', \"hadn't've\": 'had not have', \"hadn't\": 'had not', \"don't\": 'do not', \"doesn't\": 'does not', \"didn't\": 'did not', \"couldn't've\": 'could not have', \"couldn't\": 'could not', \"could've\": 'could have', \"'cause\": 'because', \"can't've\": 'can not have', \"can't\": 'can not', \"aren't\": 'are not',\"ain't\": 'are not', \"aren't\": 'are not'}\n","0bf37db0":"\nimport nltk\nimport re\nimport numpy as np\nstopwords=nltk.corpus.stopwords.words('english')\nstopwords.remove('no')\nstopwords.remove('not')\nstopwords.remove('but')\n\ndef clean_data(text):\n    text1=text.split()\n    for i, j in enumerate(text1):\n        if j in contractions.keys():\n            text1[i]=contractions[j]\n        else:\n            text1[i]\n    text=\" \".join(text1)\n    text=str(text).lower()\n    text=text.strip()\n    text=re.sub(r'[^a-z\\s]','',text,re.I|re.A)\n    token=nltk.word_tokenize(text)\n    tokens=[word for word in token if word not in stopwords]\n    text=' '.join(tokens)\n    \n    return text\n","0e4576f1":"train_dt=pd.DataFrame()\ntrain_dt['Sentence']=train['Sentence'].apply(lambda x:clean_data(x))\ntrain_dt['Labels']=train['Labels']\ntrain_dt.head(10)\n\ntest_dt=pd.DataFrame()\ntest_dt['Sentence']=test['Sentence'].apply(lambda x:clean_data(x))\ntest_dt['Labels']=test['Labels']\ntest_dt.head(10)","7602d937":"import textblob as tb\ntrain_sentiment_obj=train_dt['Sentence'].apply(lambda x:tb.TextBlob(x).sentiment)\ntrain_sentiment=pd.DataFrame()\ntrain_sentiment['Subjectivity']=[word[0] for word in train_sentiment_obj]\ntrain_sentiment['Polarity'] =[word[1] for word in train_sentiment_obj]\n\ntest_sentiment=pd.DataFrame()\ntest_sentiment_obj=test_dt['Sentence'].apply(lambda x:tb.TextBlob(x).sentiment)\ntest_sentiment['Subjectivity']=[word[0] for word in test_sentiment_obj]\ntest_sentiment['Polarity'] =[word[1] for word in test_sentiment_obj]\ntest_sentiment.head(2)","282f4617":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(stop_words='english')\ntrain_cv=cv.fit_transform(train_dt.Sentence)\ntrain_cv=pd.DataFrame(train_cv.toarray(), columns=cv.get_feature_names())\ntrain_cv.index=train_dt.index\ntrain_cv[:2]","559a8803":"test_cv=cv.transform(test_dt.Sentence)\ntest_cv=pd.DataFrame(test_cv.toarray(), columns=cv.get_feature_names())\ntest_cv.index=test_dt.index\ntest_cv.head(2)\n                     ","f49a6f2e":"from sklearn.linear_model import LogisticRegression\ntrain_combined=pd.concat([train_cv,train_sentiment], axis=1)\ntest_combined=pd.concat([test_cv, test_sentiment], axis=1)\ntrain_combined.head()","cd9dfb3c":"LR=LogisticRegression()\nLR.fit(train_combined,train['Labels'])\npredict=LR.predict(test_combined)","069af379":"predict","c34dd84a":"from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nprint('Confusion Matrix:\\n',confusion_matrix(test['Labels'],predict))\nprint('\\n')\nprint('Accuracy Score:\\n',accuracy_score(test['Labels'], predict))\nprint('\\nClassification Score: \\n', classification_report(test['Labels'], predict))","369dfad3":"from sklearn.naive_bayes import BernoulliNB\nRF=BernoulliNB()\nRF.fit(train_combined, train['Labels'])\npredict_rf_1= RF.predict(test_combined)\n\nprint('Confusion Matrix: \\n', confusion_matrix(test['Labels'], predict_rf_1))\nprint('\\nAccuracy Score: ', accuracy_score(test['Labels'], predict_rf_1))\nprint('\\nClassification Score: \\n', classification_report(test['Labels'], predict_rf_1))","ff6d706d":"import numpy as np\nnormalize_corpus = np.vectorize(clean_data)\nnorm_corpus = normalize_corpus(list(train['Sentence']))","12815cad":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf=TfidfVectorizer(ngram_range=(1,2),min_df=2)\ntfidf_matrix = tf.fit_transform(norm_corpus)\ntrain_tfid = pd.DataFrame(tfidf_matrix.toarray(),columns=tf.get_feature_names())\ntest_tfid_matrix=tf.transform(test['Sentence'])\ntest_tfid = pd.DataFrame(test_tfid_matrix.toarray(), columns=tf.get_feature_names())\ntest_tfid.shape","2909203a":"#assign index of train & test to respective tfid matrix\ntrain_tfid.index=train.index\ntest_tfid.index = test.index","68359746":"#Combined_Data\ntrain_combined_tf = pd.concat([train_sentiment, train_tfid], axis=1)\ntest_combined_tf = pd.concat([test_sentiment, test_tfid], axis=1)","06d93e22":"#model fit amd predict using Logistic Regression\n\nLR.fit(train_combined_tf,train['Labels'] )\npredict_tf = LR.predict(test_combined_tf)","14582a82":"from sklearn.metrics import classification_report\n\nprint('Confusion Matrix: \\n', confusion_matrix(test['Labels'], predict_tf))\nprint('\\nAccuracy Score: ', accuracy_score(test['Labels'], predict_tf))\nprint('\\nClassification Score: \\n', classification_report(test['Labels'], predict_tf))\n\n\n","8acf150f":"from sklearn.naive_bayes import BernoulliNB\nRF=BernoulliNB()\nRF.fit(train_combined_tf, train['Labels'])\npredict_rf= RF.predict(test_combined_tf)\n\nprint('Confusion Matrix: \\n', confusion_matrix(test['Labels'], predict_rf))\nprint('\\nAccuracy Score: ', accuracy_score(test['Labels'], predict_rf))\nprint('\\nClassification Score: \\n', classification_report(test['Labels'], predict_rf))","af51c3ca":"# Method 2(B)","251e3ce5":"import re\ntrain_labels=[0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences=[x.split(' ',1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0', train_sentences[i])\n\ntest_labels=[0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences=[x.split(' ',1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i]=re.sub('\\d','0',test_sentences[i])\n\ntrain_labels[:2]\n","b1b00005":"# ***Approach 1***","bb95afd8":"# ***Approach 2***","dd45da3f":"# Method 2(A)"}}