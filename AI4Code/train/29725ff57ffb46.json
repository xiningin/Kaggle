{"cell_type":{"6dff2940":"code","bc133a2c":"code","baab7609":"code","613d29c5":"code","8c0b6810":"code","9cfef17b":"code","83a3d612":"code","9c92f630":"code","2847efc8":"code","5ee9ee25":"code","1ec9c9b5":"code","ec6aae13":"code","c315a06c":"markdown","b582a948":"markdown","1c1b11d1":"markdown","425c5fd2":"markdown","f39d5b55":"markdown","c597843f":"markdown","8a364c7f":"markdown"},"source":{"6dff2940":"## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\n","bc133a2c":"## defining constants\nPATH_TRAIN = \"\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv\"\nPATH_TEST = \"\/kaggle\/input\/covid19-global-forecasting-week-5\/test.csv\"\nPATH_GOOGLE_MOBILITY = \"\/kaggle\/input\/covid19-mobility-data\/Google_Mobility_Data.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"","baab7609":"## reading data\ndf_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\n\ndf_google = pd.read_csv(PATH_GOOGLE_MOBILITY, low_memory = False)\n","613d29c5":"## preparing data\ndf_train = df_train[df_train.Date < \"2020-04-27\"]\n\ndf_google[\"Date\"] = pd.to_datetime(df_google.date)\n\ndf_google.loc[df_google.country_region == \"United States\", \"country_region\"] = \"US\"\ndf_google.sub_region_2 = df_google.sub_region_2.str.replace(\" County\", \"\")\n\ndf_google[\"geography\"] = df_google.country_region + \"_\" + df_google.sub_region_1 + \"_\" + df_google.sub_region_2\ndf_google.loc[df_google.sub_region_2.isna(), \"geography\"] = df_google[df_google.sub_region_2.isna()].country_region + \"_\" + df_google[df_google.sub_region_2.isna()].sub_region_1\ndf_google.loc[df_google.sub_region_1.isna(), \"geography\"] = df_google[df_google.sub_region_1.isna()].country_region\n\ndf_google[\"Google_Recreation_Index\"] = df_google.retail_and_recreation_percent_change_from_baseline\ndf_google[\"Google_Grocery_Index\"] = df_google.grocery_and_pharmacy_percent_change_from_baseline\ndf_google[\"Google_Parks_Index\"] = df_google.parks_percent_change_from_baseline\ndf_google[\"Google_Transit_Index\"] = df_google.transit_stations_percent_change_from_baseline\ndf_google[\"Google_Workplaces_Index\"] = df_google.workplaces_percent_change_from_baseline\ndf_google[\"Google_Residential_Index\"] = df_google.residential_percent_change_from_baseline\n\ndf_google = df_google[[\n    \"geography\",\n    \"Date\",\n    \"Google_Recreation_Index\",\n    \"Google_Grocery_Index\",\n    \"Google_Parks_Index\",\n    \"Google_Transit_Index\",\n    \"Google_Workplaces_Index\",\n    \"Google_Residential_Index\"\n]].drop_duplicates(subset = [\"geography\", \"Date\"])\n","8c0b6810":"## basic features\ndf = pd.concat([df_train, df_test])\n\ndf.Date = pd.to_datetime(df.Date)\n\ndf[\"geography\"] = df.Country_Region + \"_\" + df.Province_State + \"_\" + df.County\ndf.loc[df.County.isna(), \"geography\"] = df[df.County.isna()].Country_Region + \"_\" + df[df.County.isna()].Province_State\ndf.loc[df.Province_State.isna(), \"geography\"] = df[df.Province_State.isna()].Country_Region\n\nle = LabelEncoder()\ndf.Country_Region = le.fit_transform(df.Country_Region.astype(str))\ndf.Province_State = le.fit_transform(df.Province_State.astype(str))\ndf.County = le.fit_transform(df.County.astype(str))\ndf.Target = le.fit_transform(df.Target.astype(str))\n\ndf = df.merge(df_google, on = [\"geography\", \"Date\"], how = \"left\")\n","9cfef17b":"## lag features\ndf.sort_values([\"geography\", \"Date\", \"Target\"], inplace = True)\n\nfor lag in range(1, 10):\n    df[f\"lag_target_{lag}\"] = df.groupby([\"geography\", \"Target\"])[\"TargetValue\"].shift(lag)\n    df[f\"lag_recreation_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Recreation_Index\"].shift(2 * lag)\n    df[f\"lag_grocery_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Grocery_Index\"].shift(2 * lag)\n    df[f\"lag_parks_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Parks_Index\"].shift(2 * lag)\n    df[f\"lag_transit_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Transit_Index\"].shift(2 * lag)\n    df[f\"lag_workplaces_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Workplaces_Index\"].shift(2 * lag)\n    df[f\"lag_residential_index_{lag}\"] = df.groupby([\"geography\"])[\"Google_Residential_Index\"].shift(2 * lag)\n\n","83a3d612":"df[\"lag_target_1_3\"] = df[[\"lag_target_1\", \"lag_target_2\", \"lag_target_3\"]].mean(axis = 1)\ndf[\"lag_target_1_5\"] = df[[\"lag_target_1\", \"lag_target_2\", \"lag_target_3\", \"lag_target_4\", \"lag_target_5\"]].mean(axis = 1)\ndf[\"lag_target_1_9\"] = df[[\"lag_target_1\", \"lag_target_2\", \"lag_target_3\", \"lag_target_4\", \"lag_target_5\",\n                           \"lag_target_6\", \"lag_target_7\", \"lag_target_8\", \"lag_target_9\"]].mean(axis = 1)\n\ndf[\"lag_recreation_index_1_3\"] = df[[\"lag_recreation_index_1\", \"lag_recreation_index_2\", \"lag_recreation_index_3\"]].mean(axis = 1)\ndf[\"lag_grocery_index_1_3\"] = df[[\"lag_grocery_index_1\", \"lag_grocery_index_2\", \"lag_grocery_index_3\"]].mean(axis = 1)\ndf[\"lag_parks_index_1_3\"] = df[[\"lag_parks_index_1\", \"lag_parks_index_2\", \"lag_parks_index_3\"]].mean(axis = 1)\ndf[\"lag_transit_index_1_3\"] = df[[\"lag_transit_index_1\", \"lag_transit_index_2\", \"lag_transit_index_3\"]].mean(axis = 1)\ndf[\"lag_workplaces_index_1_3\"] = df[[\"lag_workplaces_index_1\", \"lag_workplaces_index_2\", \"lag_workplaces_index_3\"]].mean(axis = 1)\ndf[\"lag_residential_index_1_3\"] = df[[\"lag_residential_index_1\", \"lag_residential_index_2\", \"lag_residential_index_3\"]].mean(axis = 1)\n\ndf[\"lag_recreation_index_1_5\"] = df[[\"lag_recreation_index_1\", \"lag_recreation_index_2\", \"lag_recreation_index_3\",\n                                     \"lag_recreation_index_4\", \"lag_recreation_index_5\"]].mean(axis = 1)\ndf[\"lag_grocery_index_1_5\"] = df[[\"lag_grocery_index_1\", \"lag_grocery_index_2\", \"lag_grocery_index_3\",\n                                  \"lag_grocery_index_4\", \"lag_grocery_index_5\"]].mean(axis = 1)\ndf[\"lag_parks_index_1_5\"] = df[[\"lag_parks_index_1\", \"lag_parks_index_2\", \"lag_parks_index_3\",\n                                \"lag_parks_index_4\", \"lag_parks_index_5\"]].mean(axis = 1)\ndf[\"lag_transit_index_1_5\"] = df[[\"lag_transit_index_1\", \"lag_transit_index_2\", \"lag_transit_index_3\",\n                                  \"lag_transit_index_4\", \"lag_transit_index_5\"]].mean(axis = 1)\ndf[\"lag_workplaces_index_1_5\"] = df[[\"lag_workplaces_index_1\", \"lag_workplaces_index_2\", \"lag_workplaces_index_3\",\n                                     \"lag_workplaces_index_4\", \"lag_workplaces_index_5\"]].mean(axis = 1)\ndf[\"lag_residential_index_1_5\"] = df[[\"lag_residential_index_1\", \"lag_residential_index_2\", \"lag_residential_index_3\",\n                                      \"lag_residential_index_4\", \"lag_residential_index_5\"]].mean(axis = 1)\n\ndf[\"lag_recreation_index_1_9\"] = df[[\"lag_recreation_index_1\", \"lag_recreation_index_2\", \"lag_recreation_index_3\",\n                                     \"lag_recreation_index_4\", \"lag_recreation_index_5\", \"lag_recreation_index_6\",\n                                     \"lag_recreation_index_7\", \"lag_recreation_index_8\", \"lag_recreation_index_9\"]].mean(axis = 1)\ndf[\"lag_grocery_index_1_9\"] = df[[\"lag_grocery_index_1\", \"lag_grocery_index_2\", \"lag_grocery_index_3\",\n                                  \"lag_grocery_index_4\", \"lag_grocery_index_5\", \"lag_grocery_index_6\",\n                                  \"lag_grocery_index_7\", \"lag_grocery_index_8\", \"lag_grocery_index_9\"]].mean(axis = 1)\ndf[\"lag_parks_index_1_9\"] = df[[\"lag_parks_index_1\", \"lag_parks_index_2\", \"lag_parks_index_3\",\n                                \"lag_parks_index_4\", \"lag_parks_index_5\", \"lag_parks_index_6\",\n                                \"lag_parks_index_7\", \"lag_parks_index_8\", \"lag_parks_index_9\"]].mean(axis = 1)\ndf[\"lag_transit_index_1_9\"] = df[[\"lag_transit_index_1\", \"lag_transit_index_2\", \"lag_transit_index_3\",\n                                  \"lag_transit_index_4\", \"lag_transit_index_5\", \"lag_transit_index_6\",\n                                  \"lag_transit_index_7\", \"lag_transit_index_8\", \"lag_transit_index_9\"]].mean(axis = 1)\ndf[\"lag_workplaces_index_1_9\"] = df[[\"lag_workplaces_index_1\", \"lag_workplaces_index_2\", \"lag_workplaces_index_3\",\n                                     \"lag_workplaces_index_4\", \"lag_workplaces_index_5\", \"lag_workplaces_index_6\",\n                                     \"lag_workplaces_index_7\", \"lag_workplaces_index_8\", \"lag_workplaces_index_9\"]].mean(axis = 1)\ndf[\"lag_residential_index_1_9\"] = df[[\"lag_residential_index_1\", \"lag_residential_index_2\", \"lag_residential_index_3\",\n                                      \"lag_residential_index_4\", \"lag_residential_index_5\", \"lag_residential_index_6\",\n                                      \"lag_residential_index_7\", \"lag_residential_index_8\", \"lag_residential_index_9\"]].mean(axis = 1)\n","9c92f630":"## modelling without mobility features\nfeatures = [\n    \"Country_Region\",\n    \"Province_State\",\n    \"County\",\n    \"Population\",\n    \"Target\",\n    \"lag_target_1\",\n    \"lag_target_2\",\n    \"lag_target_3\",\n    \"lag_target_4\",\n    \"lag_target_5\",\n    \"lag_target_6\",\n    \"lag_target_7\",\n    \"lag_target_8\",\n    \"lag_target_9\",\n    \"lag_target_1_3\",\n    \"lag_target_1_5\",\n    \"lag_target_1_9\"\n]\n\ncategorical_features = [\n    \"Country_Region\",\n    \"Province_State\",\n    \"County\",\n    \"Target\"\n]\n\ndf_train = df[~df.TargetValue.isna()]\ndf_build = df_train[df_train.Date < datetime(2020, 4, 20)]\ndf_val = df_train[df_train.Date >= datetime(2020, 4, 20)]\ndf_test = df[df.TargetValue.isna()]\n\ny_train = df_train.TargetValue.values\ny_build = df_build.TargetValue.values\ny_val = df_val.TargetValue.values\n\ntest_ids = df_test.ForecastId.astype(int).astype(str).values\n\ndf_train = df_train[features]\ndf_build = df_build[features]\ndf_val = df_val[features]\ndf_test = df_test[features]\n\nprint(\"Build shape: \", df_build.shape)\nprint(\"Val shape: \", df_val.shape)\nprint(\"Train shape: \", df_train.shape)\nprint(\"Test shape: \", df_test.shape)\n\ndtrain = lgb.Dataset(df_train, label = y_train, categorical_feature = categorical_features)\ndbuild = lgb.Dataset(df_build, label = y_build, categorical_feature = categorical_features)\ndval = lgb.Dataset(df_val, label = y_val, categorical_feature = categorical_features)\n\nparams = {\n    \"objective\": \"regression_l1\",\n    \"num_leaves\": 7,\n    \"learning_rate\": 0.013,\n    \"bagging_fraction\": 0.91,\n    \"feature_fraction\": 0.81,\n    \"reg_alpha\": 0.13,\n    \"reg_lambda\": 0.13,\n    \"metric\": \"mae\",\n    \"seed\": 838861\n}\n\nmodel_lgb_val = lgb.train(params, train_set = dbuild, valid_sets = [dval], num_boost_round = 2000, early_stopping_rounds = 100, verbose_eval = 100)\nmodel_lgb_without_mobility = lgb.train(params, train_set = dtrain, num_boost_round = model_lgb_val.best_iteration)\n    \ny_pred_without_mobility = model_lgb_without_mobility.predict(df_test)\n","2847efc8":"## modelling with mobility features\nfeatures_with_mobility = features + [\n    \"lag_recreation_index_1\",\n    \"lag_recreation_index_2\",\n    \"lag_recreation_index_3\",\n    \"lag_recreation_index_4\",\n    \"lag_recreation_index_5\",\n    \"lag_recreation_index_6\",\n    \"lag_recreation_index_7\",\n    \"lag_recreation_index_8\",\n    \"lag_recreation_index_9\",\n    \"lag_grocery_index_1\",\n    \"lag_grocery_index_2\",\n    \"lag_grocery_index_3\",\n    \"lag_grocery_index_4\",\n    \"lag_grocery_index_5\",\n    \"lag_grocery_index_6\",\n    \"lag_grocery_index_7\",\n    \"lag_grocery_index_8\",\n    \"lag_grocery_index_9\",\n    \"lag_parks_index_1\",\n    \"lag_parks_index_2\",\n    \"lag_parks_index_3\",\n    \"lag_parks_index_4\",\n    \"lag_parks_index_5\",\n    \"lag_parks_index_6\",\n    \"lag_parks_index_7\",\n    \"lag_parks_index_8\",\n    \"lag_parks_index_9\",\n    \"lag_transit_index_1\",\n    \"lag_transit_index_2\",\n    \"lag_transit_index_3\",\n    \"lag_transit_index_4\",\n    \"lag_transit_index_5\",\n    \"lag_transit_index_6\",\n    \"lag_transit_index_7\",\n    \"lag_transit_index_8\",\n    \"lag_transit_index_9\",\n    \"lag_workplaces_index_1\",\n    \"lag_workplaces_index_2\",\n    \"lag_workplaces_index_3\",\n    \"lag_workplaces_index_4\",\n    \"lag_workplaces_index_5\",\n    \"lag_workplaces_index_6\",\n    \"lag_workplaces_index_7\",\n    \"lag_workplaces_index_8\",\n    \"lag_workplaces_index_9\",\n    \"lag_residential_index_1\",\n    \"lag_residential_index_2\",\n    \"lag_residential_index_3\",\n    \"lag_residential_index_4\",\n    \"lag_residential_index_5\",\n    \"lag_residential_index_6\",\n    \"lag_residential_index_7\",\n    \"lag_residential_index_8\",\n    \"lag_residential_index_9\",\n    \"lag_recreation_index_1_3\",\n    \"lag_grocery_index_1_3\",\n    \"lag_parks_index_1_3\",\n    \"lag_transit_index_1_3\",\n    \"lag_workplaces_index_1_3\",\n    \"lag_residential_index_1_3\",\n    \"lag_recreation_index_1_5\",\n    \"lag_grocery_index_1_5\",\n    \"lag_parks_index_1_5\",\n    \"lag_transit_index_1_5\",\n    \"lag_workplaces_index_1_5\",\n    \"lag_residential_index_1_5\",\n    \"lag_recreation_index_1_9\",\n    \"lag_grocery_index_1_9\",\n    \"lag_parks_index_1_9\",\n    \"lag_transit_index_1_9\",\n    \"lag_workplaces_index_1_9\",\n    \"lag_residential_index_1_9\"\n]\n\ndf_train = df[~df.TargetValue.isna()]\ndf_build = df_train[df_train.Date < datetime(2020, 4, 20)]\ndf_val = df_train[df_train.Date >= datetime(2020, 4, 20)]\ndf_test = df[df.TargetValue.isna()]\n\ny_train = df_train.TargetValue.values\ny_build = df_build.TargetValue.values\ny_val = df_val.TargetValue.values\n\ntest_ids = df_test.ForecastId.astype(int).astype(str).values\n\ndf_train = df_train[features_with_mobility]\ndf_build = df_build[features_with_mobility]\ndf_val = df_val[features_with_mobility]\ndf_test = df_test[features_with_mobility]\n\nprint(\"Build shape: \", df_build.shape)\nprint(\"Val shape: \", df_val.shape)\nprint(\"Train shape: \", df_train.shape)\nprint(\"Test shape: \", df_test.shape)\n\ndtrain = lgb.Dataset(df_train, label = y_train, categorical_feature = categorical_features)\ndbuild = lgb.Dataset(df_build, label = y_build, categorical_feature = categorical_features)\ndval = lgb.Dataset(df_val, label = y_val, categorical_feature = categorical_features)\n\nparams = {\n    \"objective\": \"regression_l1\",\n    \"num_leaves\": 9,\n    \"learning_rate\": 0.013,\n    \"bagging_fraction\": 0.81,\n    \"feature_fraction\": 0.81,\n    \"reg_alpha\": 0.13,\n    \"reg_lambda\": 0.13,\n    \"metric\": \"mae\",\n    \"seed\": 838861\n}\n\nmodel_lgb_val = lgb.train(params, train_set = dbuild, valid_sets = [dval], num_boost_round = 2000, early_stopping_rounds = 100, verbose_eval = 100)\nmodel_lgb_with_mobility = lgb.train(params, train_set = dtrain, num_boost_round = model_lgb_val.best_iteration)\n    \ny_pred_with_mobility = model_lgb_with_mobility.predict(df_test)\n","5ee9ee25":"from bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_model_without_mobility = pd.DataFrame({\"feature\": features, \"importance\": model_lgb_without_mobility.feature_importance()})\ndf_model_with_mobility = pd.DataFrame({\"feature\": features_with_mobility, \"importance\": model_lgb_with_mobility.feature_importance()})\n\ndf_model_without_mobility.sort_values(\"importance\", ascending = False, inplace = True)\ndf_model_with_mobility.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_model_without_mobility.feature, title = \"Feature Importance of LGB Model without Mobility features\")\nv1.vbar(x = df_model_without_mobility.feature, top = df_model_without_mobility.importance, width = 0.81)\nv1.xaxis.major_label_orientation = 1.3\n\nv2 = figure(plot_width = 800, plot_height = 400, x_range = df_model_with_mobility.feature, title = \"Feature Importance of LGB Model with Mobility features\")\nv2.vbar(x = df_model_with_mobility.feature, top = df_model_with_mobility.importance, width = 0.81)\nv2.xaxis.major_label_orientation = 1.3\n\nshow(column(v1, v2))\n","1ec9c9b5":"## submission\ny_pred = y_pred_with_mobility\n\ndf_pred_q05 = pd.DataFrame({\"ForecastId_Quantile\": test_ids + \"_0.05\", \"TargetValue\": 0.85 * y_pred})\ndf_pred_q50 = pd.DataFrame({\"ForecastId_Quantile\": test_ids + \"_0.5\", \"TargetValue\": y_pred})\ndf_pred_q95 = pd.DataFrame({\"ForecastId_Quantile\": test_ids + \"_0.95\", \"TargetValue\": 1.15 * y_pred})\n\ndf_submit = pd.concat([df_pred_q05, df_pred_q50, df_pred_q95])\n","ec6aae13":"print(df_submit.shape)\ndf_submit.to_csv(PATH_SUBMISSION, index = False)\n","c315a06c":"## Submission\nChoosing the appropriate final submission. Instead of using a static multiplier, the different quantiles can be optimized to improve score.","b582a948":"## Reading and Preparing Data\nThe datasets will require some basic cleaning to match the countries, states\/cities and counties. I have shown how it can be used for US. Adding more preprocessing for other countries can improve quality of features.","1c1b11d1":"## Modelling without Mobility\nModelling without using any mobility features and data.","425c5fd2":"## Feature Engineering\nWithout going into too much depth, I've created some basic features on the raw data and some lag features with and without mobility. A lot more features can be added to the pipeline to improve the performance.","f39d5b55":"## Feature Importance\nComparing the feature importance of the two models.","c597843f":"## Modelling with Mobility\nModelling with mobility features and data.","8a364c7f":"## Modelling COVID-19 with Mobility Features\nThis notebook aims to provide a basic workflow of how mobility data shared by [Google](https:\/\/www.google.com\/covid19\/mobility\/) and [Apple](https:\/\/www.apple.com\/covid19\/mobility) can potentially be used as features into COVID-19 models.\n\nI will be maintaining a structured version of the dataset here: https:\/\/www.kaggle.com\/rohanrao\/covid19-mobility-data and also likely will use it to some extent for my final submission of this competition.\n\nThe notebook demonstrates how the dataset can be merged and used with the competition data. I've also shared the validation and LB scores with and without using the features.\n\n**P.S.** If you plan to use the data \/ notebook, be careful to use the features with appropriate lagged values since the actual test data duration is 28 days.\n"}}