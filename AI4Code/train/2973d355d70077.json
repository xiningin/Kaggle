{"cell_type":{"efb7c29a":"code","be50018f":"code","71d60d38":"code","0ad204ba":"code","55d6e91b":"code","60b0e427":"code","fe1d798a":"code","653f0e1a":"code","692efe94":"markdown"},"source":{"efb7c29a":"import os\nimport datetime\nimport imageio\nimport skimage\nimport scipy\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom IPython.display import Image\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.__version__","be50018f":"class DataLoader():\n    def __init__(self, dataset_name, img_res=(128, 128)):\n        self.dataset_name = dataset_name\n        self.img_res = img_res\n\n    def load_data(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"test\"\n        path = glob('..\/input\/%s\/%s\/%s\/*' % (self.dataset_name, self.dataset_name, data_type))\n        \n        batch_images = np.random.choice(path, size=batch_size)\n\n        imgs_A = []\n        imgs_B = []\n        for img_path in batch_images:\n            img = self.imread(img_path)\n\n            h, w, _ = img.shape\n            _w = int(w\/2)\n            img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n\n            img_A = scipy.misc.imresize(img_A, self.img_res)\n            img_B = scipy.misc.imresize(img_B, self.img_res)\n\n            # If training => do random flip\n            if not is_testing and np.random.random() < 0.5:\n                img_A = np.fliplr(img_A)\n                img_B = np.fliplr(img_B)\n\n            imgs_A.append(img_A)\n            imgs_B.append(img_B)\n\n        imgs_A = np.array(imgs_A)\/127.5 - 1.\n        imgs_B = np.array(imgs_B)\/127.5 - 1.\n\n        return imgs_A, imgs_B\n\n    def load_batch(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"val\"\n        path = glob('..\/input\/%s\/%s\/%s\/*' % (self.dataset_name, self.dataset_name, data_type))\n\n        self.n_batches = int(len(path) \/ batch_size)\n\n        for i in range(self.n_batches-1):\n            batch = path[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for img in batch:\n                img = self.imread(img)\n                h, w, _ = img.shape\n                half_w = int(w\/2)\n                img_A = img[:, :half_w, :]\n                img_B = img[:, half_w:, :]\n\n                img_A = scipy.misc.imresize(img_A, self.img_res)\n                img_B = scipy.misc.imresize(img_B, self.img_res)\n\n                if not is_testing and np.random.random() > 0.5:\n                        img_A = np.fliplr(img_A)\n                        img_B = np.fliplr(img_B)\n\n                imgs_A.append(img_A)\n                imgs_B.append(img_B)\n\n            imgs_A = np.array(imgs_A)\/127.5 - 1.\n            imgs_B = np.array(imgs_B)\/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n\n    def imread(self, path):\n        return imageio.imread(path).astype(np.float)\n\nclass Pix2Pix():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 256\n        self.img_cols = 256\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Configure data loader\n        self.dataset_name = 'facades'\n        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n                                      img_res=(self.img_rows, self.img_cols))\n\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows \/ 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 64\n        self.df = 64\n\n        optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generator\n        #-------------------------\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Input images and their conditioning images\n        img_A = tf.keras.layers.Input(shape=self.img_shape)\n        img_B = tf.keras.layers.Input(shape=self.img_shape)\n\n        # By conditioning on B generate a fake version of A\n        fake_A = self.generator(img_B)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # Discriminators determines validity of translated images \/ condition pairs\n        valid = self.discriminator([fake_A, img_B])\n\n        self.combined = tf.keras.models.Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n        self.combined.compile(loss=['mse', 'mae'],\n                              loss_weights=[1, 100],\n                              optimizer=optimizer)\n\n    def build_generator(self):\n        \"\"\"U-Net Generator\"\"\"\n\n        def conv2d(layer_input, filters, f_size=4, bn=True):\n            \"\"\"Layers used during downsampling\"\"\"\n            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            \"\"\"Layers used during upsampling\"\"\"\n            u = tf.keras.layers.UpSampling2D(size=2)(layer_input)\n            u = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n            if dropout_rate:\n                u = tf.keras.layers.Dropout(dropout_rate)(u)\n            u = tf.keras.layers.BatchNormalization(momentum=0.8)(u)\n            u = tf.keras.layers.Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = tf.keras.layers.Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf, bn=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u5 = deconv2d(u4, d2, self.gf*2)\n        u6 = deconv2d(u5, d1, self.gf)\n\n        u7 = tf.keras.layers.UpSampling2D(size=2)(u6)\n        output_img = tf.keras.layers.Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n\n        return tf.keras.models.Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, bn=True):\n            \"\"\"Discriminator layer\"\"\"\n            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n            return d\n\n        img_A = tf.keras.layers.Input(shape=self.img_shape)\n        img_B = tf.keras.layers.Input(shape=self.img_shape)\n\n        # Concatenate image and conditioning image by channels to produce input\n        combined_imgs = tf.keras.layers.Concatenate(axis=-1)([img_A, img_B])\n\n        d1 = d_layer(combined_imgs, self.df, bn=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n\n        return tf.keras.models.Model([img_A, img_B], validity)\n\n    def train(self, epochs, batch_size=1, sample_interval=50):\n        start_time = datetime.datetime.now()\n\n        # Adversarial loss ground truths\n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n\n        for epoch in range(epochs):\n            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n                # ---------------------\n                #  Train Discriminator\n                # ---------------------\n\n                # Condition on B and generate a translated version\n                fake_A = self.generator.predict(imgs_B)\n\n                # Train the discriminators (original images = real \/ generated = Fake)\n                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n                # -----------------\n                #  Train Generator\n                # -----------------\n\n                # Train the generators\n                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n\n                elapsed_time = datetime.datetime.now() - start_time\n                # Plot the progress\n                print (\"[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n                                                                        batch_i, self.data_loader.n_batches,\n                                                                        d_loss[0], 100*d_loss[1],\n                                                                        g_loss[0],\n                                                                        elapsed_time))\n\n                # If at save interval => save generated image samples\n                if batch_i % sample_interval == 0:\n                    self.sample_images(epoch, batch_i)\n\n    def sample_images(self, epoch, batch_i):\n        os.makedirs('.\/images\/%s' % self.dataset_name, exist_ok=True)\n        r, c = 3, 3\n\n        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n        fake_A = self.generator.predict(imgs_B)\n\n        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = ['Condition', 'Generated', 'Original']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[i])\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(\".\/images\/%s\/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n        plt.close()","71d60d38":"gan = Pix2Pix()\n# gan.train(epochs=200, batch_size=1, sample_interval=200)","0ad204ba":"gan.train(epochs=10, batch_size=1, sample_interval=200)\n# training logs are hidden in published notebook","55d6e91b":"Image('\/kaggle\/working\/images\/facades\/0_0.png')","60b0e427":"Image('\/kaggle\/working\/images\/facades\/0_200.png')","fe1d798a":"Image('\/kaggle\/working\/images\/facades\/5_200.png')","653f0e1a":"Image('\/kaggle\/working\/images\/facades\/9_200.png')","692efe94":"# Pix-2-Pix with TensorFlow 2.0\n\n## NOTE: Work In Progress\n\nThis is an automatic port of https:\/\/www.kaggle.com\/vikramtiwari\/pix-2-pix-model-using-tensorflow-and-keras using the [upgrade script](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/r2\/guide\/upgrade.md)\n\nNOTE: Upgrade script is not present in stable release yet. You have to install nightly builds to get the script."}}