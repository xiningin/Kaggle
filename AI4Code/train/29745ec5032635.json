{"cell_type":{"8333800e":"code","40a62c2b":"code","5c1f256e":"code","48e19a9f":"code","0878777d":"code","50fd4882":"code","cced1916":"code","af9012fa":"code","c44bcb86":"code","b5b57f7f":"code","1f4e066c":"code","45604276":"code","3a444f99":"code","eb2d30be":"code","80d5b89b":"code","5c5a250b":"code","fdec9cfb":"code","293c2fb1":"code","da7cc31b":"code","6e35d67d":"code","4c498f1c":"code","fbec9670":"code","7cf0c09b":"code","0fc6856d":"code","74fb077b":"code","9199cd49":"code","4c55bd73":"code","eb1c8801":"code","3534d453":"code","5513a28e":"code","7a4892ef":"code","aa037ae5":"code","08708c14":"code","e4b1aeb2":"code","26ac6d4f":"code","a8ca9dd7":"code","ed5ff196":"code","654ecd13":"code","529eba1b":"code","1ea042f0":"code","e4630a20":"code","f14de717":"markdown","35701517":"markdown","8353b3ac":"markdown","ca2a288d":"markdown","18b65e23":"markdown","1496d988":"markdown","79e1d642":"markdown","cc2558b5":"markdown","65680a53":"markdown","88fbed29":"markdown","d597c947":"markdown","9137b4a9":"markdown","c70e27df":"markdown","007e0d92":"markdown","38192fa9":"markdown","8b7b002e":"markdown","e22a6d40":"markdown","327ff3b6":"markdown"},"source":{"8333800e":"import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\n\nimport scipy.cluster.hierarchy as hac\n\nimport math\n\nimport random\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\nfrom fbprophet import Prophet\n\nimport random\n","40a62c2b":"# https:\/\/alpynepyano.github.io\/healthyNumerics\/posts\/time_series_clustering_with_python.html\n# https:\/\/kourentzes.com\/forecasting\/2014\/11\/09\/additive-and-multiplicative-seasonality\/\n# https:\/\/dius.com.au\/2018\/09\/04\/time-series-forecasting-with-fbprophet\/\n# https:\/\/www.kaggle.com\/beebopjones\/bleepblop-prophet-model\n# https:\/\/www.kaggle.com\/viridisquotient\/sarima","5c1f256e":"df = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/train.csv\")\nprint(df.head(10))","48e19a9f":"# Find unique columns\nprint(\"length of database\",len(df))\nprint(\"Unique Stores\",df['store'].unique())\nprint(\"Unique items\",df['item'].unique())\nprint(\"Unique dates\",df['date'].nunique())\n\n","0878777d":"# Include holidays\nholidays = pd.read_csv(\"..\/input\/federal-holidays-usa-19662020\/usholidays.csv\")\n\n\nholidays = holidays.drop(columns=[\"Unnamed: 0\"])\nprint(holidays.head(10))\n","50fd4882":"df['date'] = pd.to_datetime(df['date']) \nholidays['Date'] = pd.to_datetime(holidays['Date']) \nneeded_holidays = holidays[(holidays['Date']>df.iloc[0]['date'])&(holidays['Date']<df.iloc[-1]['date'])]['Date'].to_list()\nprint(len(needed_holidays))\n\n# For later Analysis\nholidays = holidays[(holidays['Date']>df.iloc[0]['date'])&(holidays['Date']<df.iloc[-1]['date'])]\nholidays.rename(columns={\"Date\": \"ds\", \"Holiday\": \"holiday\"},inplace=True)","cced1916":"select_values = needed_holidays\ndf_cut = df[df['date'].isin(select_values)]['date']\nprint(df_cut.nunique())\n\n# Holidays have been included","af9012fa":"# Check for Missing  data\ntotal = df.isnull().sum()\nprint(total)","c44bcb86":"# Getting individual distributions of sales for each item. Taking sum to summate sales for different stores\ndf['date'] = pd.to_datetime(df['date']) \n\ndf_sales_item = df.groupby(['date','item']).sum()  \ndf_sales_item.reset_index(level=0, inplace=True)\ndf_sales_item.reset_index(level=0, inplace=True)\n#print(df_sales_item)\n\ngrid = sns.FacetGrid(df_sales_item, col=\"item\", col_wrap=5)\ngrid = grid.map(plt.scatter, \"date\", \"sales\", marker=\"o\", s=1, alpha=.5)","b5b57f7f":"# Getting individual distributions of sales for each store. Taking sum to summate sales for different items\ndf['date'] = pd.to_datetime(df['date']) \n\ndf_sales_store = df.groupby(['date','store']).sum()  \ndf_sales_store.reset_index(level=0, inplace=True)\ndf_sales_store.reset_index(level=0, inplace=True)\n\n\ngrid = sns.FacetGrid(df_sales_store, col=\"store\", col_wrap=5)\ngris = grid.map(plt.scatter, \"date\", \"sales\", s=1, alpha=.5)","1f4e066c":"# Lets look at sales for a specified store and a item\nplt.figure(figsize=(30,5))\ndf_store_item = df[(df.store==10) & (df.item==5)]\nplt.plot(df_store_item['date'],df_store_item['sales'])\nplt.show()","45604276":"stores = df['store'].unique()\nitems = df['item'].unique()\n\nSales_series = []\ncount = 0\nIndexSale_Series = []\nDates = []\n\n#df['date'] = pd.to_datetime(df['date']) \n\nfor store in stores:\n    for item in items:\n        Sales = df[(df.store==store) & (df.item==item)]['sales'].to_list()\n        dates = df[(df.store==store) & (df.item==item)]['date'].to_list()\n        \n        Dates.append(dates)\n        Sales_series.append(Sales)\n        \n        index = [count,item,store]\n        IndexSale_Series.append(index)\n        \n        count +=1\n\nprint(\"Total Elements: \",count)","3a444f99":"def plot_dendogram(Z):\n    with plt.style.context('fivethirtyeight' ): \n         plt.figure(figsize=(100, 40))\n         plt.title('Dendrogram of time series clustering',fontsize=25, fontweight='bold')\n         plt.xlabel('sample index', fontsize=25, fontweight='bold')\n         plt.ylabel('distance', fontsize=25, fontweight='bold')\n         hac.dendrogram( Z, leaf_rotation=90.,    # rotates the x axis labels\n                            leaf_font_size=15., ) # font size for the x axis labels\n         plt.show()\n        \ndef plot_resultsAndReturnClusters(timeSeries, D, cut_off_level):\n    result = pd.Series(hac.fcluster(D, cut_off_level, criterion='maxclust'))\n    clusters = result.unique()       \n    figX = 100; figY = 20\n    fig = plt.subplots(figsize=(figX, figY))   \n    mimg = math.ceil(cut_off_level\/2.0)\n    gs = gridspec.GridSpec(mimg,2, width_ratios=[1,1])\n    cluster = []\n    for ipic, c in enumerate(clusters):\n        cluster_index = result[result==c].index\n        cluster.append(cluster_index)\n        \n        print(ipic, \"Cluster number %d has %d elements\" % (c, len(cluster_index)))\n        ax1 = plt.subplot(gs[ipic])\n        timeSeries = np.array(timeSeries)\n        ax1.plot(timeSeries.T[:,cluster_index])\n        ax1.set_title(('Cluster number '+str(c)), fontsize=15, fontweight='bold')      \n    \n    plt.show()\n    return cluster","eb2d30be":"D = hac.linkage(Sales_series, method='ward', metric='euclidean')\nplot_dendogram(D)\n\n#---- evaluate the dendogram\ncut_off_level = 2   # level where to cut off the dendogram\nclusters = plot_resultsAndReturnClusters(Sales_series, D, cut_off_level)\n","80d5b89b":"# Random time series in cluster 1\nno = random.randint(0,101)\n\nindex = clusters[0][no]","5c5a250b":"# Check for weekly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=40,ax=ax)\n\n","fdec9cfb":"# Check for monthly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=400,ax=ax)","293c2fb1":"# Check for yearly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=1800,ax=ax)","da7cc31b":"plt.figure(figsize=(30,10))\nprint(\"Image Corresponding to Item:\",IndexSale_Series[index][1],\"And Store:\",IndexSale_Series[index][2])\nplt.plot(Sales_series[index][:100])\nplt.show()\nplt.figure(figsize=(30,10))\nplt.plot(Sales_series[index][1500:])\nplt.show()","6e35d67d":"NonStationary = []\n\nfor saleID in clusters[0]:\n    #print(Sales_series[saleID])\n    result = adfuller(Sales_series[saleID])\n    \n    if result[1] > 0.05:\n        \n        NonStationary.append(saleID)\n    else:\n        pass      \n\nprint(NonStationary)\n","4c498f1c":"SalesSample = pd.DataFrame()\nSalesSample['y'] = Sales_series[index]\nSalesSample['ds'] = Dates[index]","fbec9670":"test_size = 90 # 3 months of data\n\ntrain = SalesSample[:-test_size]\ntest = SalesSample[-test_size:]\n\nplt.subplots(figsize=(20, 5))\n\nplt.plot(train['ds'], train['y'],color='blue', label='Train')\nplt.plot(test['ds'], test['y'], color='red', label='Test')\n","7cf0c09b":"model = Prophet(daily_seasonality=False,\nweekly_seasonality=True,\nyearly_seasonality=True,\nholidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n\n# holiday prior scale to bring down the effect of holidays in the model","0fc6856d":"model.fit(train)\nforecast = model.predict(test)\nmodel.plot_components(forecast)","74fb077b":"plt.figure(figsize=(30, 5))\n\nplt.plot(test['ds'], test['y'], c='r', label='Test')\nplt.plot(forecast['ds'], forecast['yhat'], c='blue', marker='o',label='Forecast')\nplt.show()","9199cd49":"# Calculate SMAPE\ny_true = test['y'].to_list()\ny_true = np.array(y_true)\ny_forecast = forecast['yhat'].to_list()\ny_forecast = np.array(y_forecast)\n\nsmape = (np.absolute(y_true - y_forecast) \/ (np.absolute(y_true) + np.absolute(y_forecast))).mean() * 200\nprint('SMAPE is:', smape)","4c55bd73":"train['y'] = np.log1p(train['y'])","eb1c8801":"model = Prophet(daily_seasonality=False,\nweekly_seasonality=True,\nyearly_seasonality=True,\nholidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n\nmodel.fit(train)\nforecast = model.predict(test)\nmodel.plot_components(forecast)","3534d453":"forecast['yhat'] = np.expm1(forecast['yhat'])\nprint(forecast['yhat'])","5513a28e":"plt.figure(figsize=(30, 5))\nplt.plot(test['ds'], test['y'], c='r', label='Test')\nplt.plot(forecast['ds'], forecast['yhat'], c='blue', marker='o',label='Forecast')\nplt.show()","7a4892ef":"y_true = test['y'].to_list()\ny_true = np.array(y_true)\ny_forecast = forecast['yhat'].to_list()\ny_forecast = np.array(y_forecast)\n\nsmape = (np.absolute(y_true - y_forecast) \/ (np.absolute(y_true) + np.absolute(y_forecast))).mean() * 200\nprint('SMAPE  is:', smape)","aa037ae5":"# Random time series in cluster 2\nno = random.randint(0,101)\n\nindex = clusters[1][no]","08708c14":"# Check for weekly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=24,ax=ax)","e4b1aeb2":"# Check for monthly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=120,ax=ax)","26ac6d4f":"# Check for yearly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=750,ax=ax)","a8ca9dd7":"plt.figure(figsize=(30,10))\nprint(\"Image Corresponding to Item:\",IndexSale_Series[index][1],\"And Store:\",IndexSale_Series[index][2])\nplt.plot(Sales_series[index][:100])\nplt.show()\nplt.figure(figsize=(30,10))\nplt.plot(Sales_series[index][1500:])\nplt.show()","ed5ff196":"NonStationary = []\n\nfor saleID in clusters[1]:\n    #print(Sales_series[saleID])\n    result = adfuller(Sales_series[saleID])\n    \n    if result[1] > 0.05:\n        \n        NonStationary.append(saleID)\n    else:\n        pass      \n\nprint(NonStationary)","654ecd13":"train = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/train.csv\", parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/test.csv\",parse_dates=['date'],index_col=['date'])\n\nresults = test.reset_index()\nresults['sales'] = 0\n\nstores = df['store'].unique()\nitems = df['item'].unique()\n\nfor store in stores :\n    for item in items:\n        \n        to_train = train.loc[(train['store'] == store) & (train['item'] == item)].reset_index()\n        to_train.rename(columns={'date': 'ds', 'sales': 'y'}, inplace=True)\n        \n        to_train['y'] = np.log1p(to_train['y'])\n        \n        model = Prophet(daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        holidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n        \n        model.fit(to_train[['ds', 'y']])\n        \n        future = model.make_future_dataframe(periods=len(test.index.unique()),include_history=False)\n        forecast = model.predict(future)\n        \n        results.loc[(results['store'] == store) & (results['item'] == item),'sales'] = np.expm1(forecast['yhat']).values","529eba1b":"results.drop(['date', 'store', 'item'], axis=1, inplace=True)\nresults.head()","1ea042f0":"results['sales'] = np.round(results['sales']).astype(int)","e4630a20":"results.to_csv('submission.csv', index=False)","f14de717":"- As we can see, there is a pattern of weekly seasonality in the data. This will be helpfull while modelling in FbProphet","35701517":"The patterns in both the clusters seems to be similar. We can use the same parameters for fitting","8353b3ac":"- The variance seems to be constant throughout, so we can use an additive model.","ca2a288d":"- First Cluster","18b65e23":"- The graphs of each combination will be different. Let's look we can cluster graphs with similar patterns","1496d988":"- A pattern of yearly seasonality but a decreasing but still significant one. ","79e1d642":"# References","cc2558b5":"# Implement the model and find 3 months of results","65680a53":"- Lets look at cluster 2","88fbed29":"# Gain an understanding of data features","d597c947":"- Lets try with applying Log Transform on the data and check for SMAPE. It will stabilse variances in the time series","9137b4a9":"- From the above, data looks very seasonal after every passing year. Hence Data is non stationary. Let's see what the Dicky Fuller Test says","c70e27df":"# Perform Data Clustering\nThis will help us understand about the variation among different groups of data and also decide in choosing the machine learning approach","007e0d92":"- From the data clusters and the pentagram, we can conclude that the there is a variation of data in the time series among different stores and items","38192fa9":"# Forecast with FbProphet for a single time series. \n\n","8b7b002e":"- Minor improvement or sometimes none through Log transform. However will include it in final model","e22a6d40":"There is a dip in sales in the middle of year, followed by a rebound in the end of the year. So rather the trend seems to follow similar patterns year on year","327ff3b6":"- Dicky Fuller Test detecting detecting non stationary behaviour only in some of the graphs. However, we know that is not the case as there is a clear seasonality in the data"}}