{"cell_type":{"778dec50":"code","494c955a":"code","aa176d73":"code","29dbc8a8":"code","df67b889":"code","4b93217b":"code","81d2943b":"markdown"},"source":{"778dec50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.datasets import make_classification,make_blobs\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","494c955a":"class Layer:\n    def __init__(self,input_neurons,output_neurons,activation):\n        self.w = np.random.randn(input_neurons,output_neurons)\n        self.b = np.random.randn(output_neurons)\n        self.activation = activation\n        self.A = None\n        self.dZ = None\n        self.dA = None\n    \n    \n    def forward_pass(self,x):\n        wx = np.dot(x,self.w) + self.b\n        h =self.apply_activation(wx)\n        self.A = h\n        return h\n    \n    def apply_activation(self,x,alpha=.01):\n        if self.activation=='relu':\n            return max(0,x)\n        elif self.activation=='leaky_relu':\n            return max(.01*x,x)\n        elif self.activation=='tanh':\n            return np.tanh(x)\n        elif self.activation=='sigmoid':\n            return 1 \/ (1 + np.exp(-x))\n        \n    def derive_activation(self,x):\n        if self.activation=='relu':\n                d_relu = 1*(x>0)\n                return d_relu\n        \n        elif self.activation=='leaky_relu':\n            if x>0:\n                d_leaky_relu = 1\n            else:\n                d_leaky_relu = .01\n                \n            return d_leaky_relu\n        \n        elif self.activation=='tanh':\n            d_tanh = 1-(x**2)\n            return d_tanh\n       \n        elif self.activation=='sigmoid':\n            return x*(1-x)\n    \nclass NeuralNetwork:\n    def __init__(self,learning_rate=.01):\n        self.layers = []\n        self.n = None\n        self.learning_rate = learning_rate\n        self.log = {'train_loss':[],'epoch':[]}\n    \n    def feed_forward(self,x):\n        for layer in self.layers:\n            x = layer.forward_pass(x)\n        return x\n    \n    def add_layer(self,layer):\n        self.layers.append(layer)\n    \n    def mse_loss(self,target,predict):\n        return np.mean(np.square(target - predict))\n    \n    def cross_entropy_loss(self,y,y_hat):\n        cost = -(1\/self.n)*np.sum((y*np.log(y_hat)+(1-y)*np.log(1-y_hat)))\n        return cost\n    \n    def update_weights(self):\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            if i==0:\n                layer.w = layer.w - (1.0\/self.n)*(self.learning_rate*np.dot(X.T,layer.dZ))\n                layer.b = layer.b - (1.0\/self.n)*(self.learning_rate*layer.dZ)\n            else:\n                prev_layer = self.layers[i-1]\n                layer.w = layer.w - (1.0\/self.n)*(self.learning_rate*np.dot(prev_layer.A.T,layer.dZ))\n                layer.b = layer.b - (1.0\/self.n)*(self.learning_rate*layer.dZ)\n            \n    def backpropagation(self,y,y_hat):\n        for i in reversed(range(len(self.layers))):\n            layer = self.layers[i]\n            if layer==self.layers[-1]:\n                layer.dA = -np.divide(y,y_hat) + np.divide(1 - y, 1 - y_hat)\n#                 layer.dA = -(y - y_hat)\n                layer.dZ = layer.dA * layer.derive_activation(layer.A)\n            else:\n                next_layer = self.layers[i+1]\n                layer.dA = np.dot(next_layer.dZ,next_layer.w.T)\n                layer.dZ = layer.dA * layer.derive_activation(layer.A)                \n                \n                \n    def train(self,X,y,max_epochs=100):\n        self.n = X.shape[0]\n        for epoch in range(1,max_epochs+1):\n            y_hat = self.feed_forward(X)\n            self.backpropagation(y,y_hat)\n            self.update_weights()\n            loss = self.cross_entropy_loss(y,y_hat)\n#             loss = self.mse_loss(y,y_hat)\n            self.log['train_loss'].append(loss)\n            self.log['epoch'].append(epoch)\n    \n    def predict(self,X):\n        y_hat = self.feed_forward(X)\n        y_labels = (y_hat>.5)*1.0\n        return y_labels\n    \n    def plot_loss(self):\n        loss_train = self.log['train_loss']\n        epochs = self.log['epoch']\n        plt.plot(epochs, loss_train, 'g', label='Training loss')\n        plt.title('Training loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.show()\n            \n            ","aa176d73":"random_seed = 22\nX, y = make_blobs(n_samples=1000, centers=2)\ny = y[:, np.newaxis]","29dbc8a8":"np.random.seed(100)\nnn = NeuralNetwork()\nnn.add_layer(Layer(2, 20, 'tanh'))\nnn.add_layer(Layer(20, 10, 'tanh'))\nnn.add_layer(Layer(10, 1, 'sigmoid'))\nnn.train(X, y,max_epochs = 1000)\nnn.plot_loss()\ny_hat = nn.predict(X)","df67b889":"accuracy = np.sum(y_hat==y)\/X.shape[0]","4b93217b":"accuracy","81d2943b":"## Neural Network\nIn this notebook we will implement Neural Network from scratch"}}