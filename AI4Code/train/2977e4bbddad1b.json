{"cell_type":{"8b8d4132":"code","12b45b6a":"code","f7a60feb":"code","2b160496":"code","dd7893b8":"code","dc7868f8":"code","ec7c4bd6":"code","c7da2faa":"code","6f943537":"code","1e2cedc3":"code","c08fe602":"code","3c049556":"markdown","fd50d171":"markdown","06025b0a":"markdown","672c31b7":"markdown","490f36b7":"markdown","847614a0":"markdown","a4741c0e":"markdown","e5e88447":"markdown","065064db":"markdown"},"source":{"8b8d4132":"# Import libraries\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","12b45b6a":"#Start a tensorflow session\nsess = tf.compat.v1.Session()\ntf.compat.v1.disable_eager_execution()","f7a60feb":"# Create our sample data from a line space \nY_pred = tf.linspace(-1., 1., 500)\n#Create our target as a zero constant tensor\nY_truth = tf.constant(0.)","2b160496":"#Calculating the L2 loss\nval = tf.square(Y_truth - Y_pred)\nL2_val = sess.run(val)\n\n#ploting the predicted values against the L2 loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, L2_val, 'b-', label='L2 loss' )\nplt.title('L2 loss')\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{true}$', fontsize=15)","dd7893b8":"#Computing L1 loss with the same values \ntemp = tf.abs(Y_truth - Y_pred)\nL1_val = sess.run(temp)\n#ploting the predicted values against the L2 loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, L1_val, 'r-' )\nplt.title('L1 loss')\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{true}$', fontsize=15)","dc7868f8":"#Plot of the Pseudo-Huber loss\ndelta = tf.constant(0.24)\ntemp_ph = tf.multiply(tf.square(delta),tf.sqrt(1. + tf.square((Y_truth - Y_pred) \/ delta)) - 1. )\npseudo_h_vals = sess.run(temp_ph)\n\n#ploting the predicted values against the L2 loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, pseudo_h_vals, 'g-' )\nplt.title('Pseudo Huber loss')\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{true}$', fontsize=15)","ec7c4bd6":"#Redefining our data\nY_pred = tf.linspace(-4., 6., 500)\nY_label = tf.constant(1.)\nY_labels = tf.fill([500,], 1.)\n\n#applying sigmoid\nx_entropy_vals = - tf.multiply(Y_label, tf.math.log(Y_pred)) - tf.multiply((1. - Y_label), tf.math.log(1. - Y_pred))\nx_entropy_loss = sess.run(x_entropy_vals)\n#ploting the predicted values against the cross entropy loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, x_entropy_loss, 'r-' )\nplt.title('Cross entropy loss')\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{label}$', fontsize=15)\nplt.ylim(-2, 5)\nplt.show()","c7da2faa":"x_entropy_sigmoid_vals = tf.nn.sigmoid_cross_entropy_with_logits(labels= Y_labels, logits=Y_pred)\nx_entropy_sigmoid_out = sess.run(x_entropy_sigmoid_vals)\n#ploting the predicted values against the Sigmoid cross entropy loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, x_entropy_sigmoid_out, 'y-' )\nplt.title('Sigmoid cross entropy loss')\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{label}$', fontsize=15)\nplt.ylim(-2, 5)\nplt.show()","6f943537":"y_pred_dist = tf.constant([[1., -3., 10.]])\ntarget_dist = tf.constant([[0.1, 0.02, 0.88]])\nsoftmax_xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=target_dist, logits=y_pred_dist)\nprint(sess.run(softmax_xentropy))","1e2cedc3":"y_pred = tf.constant([[1., -3., 10.]])\nsparse_target_dist = tf.constant([2])  #true value is in the second position of the sparse tensor\nsparse_x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels= sparse_target_dist, logits=y_pred)\nprint(sess.run(sparse_x_entropy))","c08fe602":"#comparing two weights\nweight = tf.constant(1.)\nx_entropy_weighted_vals = tf.nn.weighted_cross_entropy_with_logits(labels=Y_labels, logits=Y_pred, pos_weight=weight)\nx_entropy_weighted_out = sess.run(x_entropy_weighted_vals)\n\nweight2 = tf.constant(0.5)\nx_entropy_weighted_val_2 = tf.nn.weighted_cross_entropy_with_logits(labels=Y_labels, logits=Y_pred, pos_weight=weight2)\nx_entropy_weighted_out_2 = sess.run(x_entropy_weighted_val_2)\n\n#ploting the predicted values against the Sigmoid cross entropy loss\nY_array = sess.run(Y_pred)\nplt.plot(Y_array, x_entropy_weighted_out, 'b-', label=' weight = 1.0' )\nplt.plot(Y_array, x_entropy_weighted_out_2, 'r--', label='weight = 0.5' )\nplt.title('Weighted cross entropy loss')\nplt.legend(loc=4)\nplt.xlabel('$Y_{pred}$', fontsize=15)\nplt.ylabel('$Y_{label}$', fontsize=15)\nplt.ylim(-2, 5)\nplt.show()","3c049556":"# Cross-Entropy Loss\n\nCross entropy loss is also somtimes referred to as the logistic loss function. Cross entropy loss for binary classification is used when we are predicting two classes 0 and 1. Here we wish to measure the distance from the actual class (0 or 1) to the predicted value, which is usually a real number between 0 and 1.\n\n![image.png](attachment:image.png)","fd50d171":"# L1 Norm Loss\/ Absolute Loss Function\n\nThe L1 loss is similar to the L2 loss but instead of taking the square of the distance, we just take the absolute value. The L1 loss is better in detecting outliers than the L2 norm. A major point to note is that L1 loss is not smooth when close to the target\/minimum and this can cause non-convergence for algorithms.\n\n![image.png](attachment:image.png)","06025b0a":"# Pseudo Hubber-Loss\n\nPseudo-huber loss is a variant of the Huber loss function, It takes the best properties of the L1 and L2 loss by being convex close to the target and less steep for extreme values. This loss depends on an extra parameter delta \u03b4 which dictates how steep the function will be.\n\n![image.png](attachment:image.png)","672c31b7":"# Weighted Cross Entropy Loss\n\nThis is a weighted version of the sigmoid cross entropy loss. Here we provide a weight on the positive target. I.e the higher the weight we specify, the higher the peek of the positive values. This can be use to control the outliers for positive predictions.\n\n![image.png](attachment:image.png)","490f36b7":"# Softmax Cross Entropy Loss\n\nThis loss is used to measure a loss when there is only one target category instead of multiple. Because of this, the function first uses a softmax function to transform the outputs into a probability distribution which all sums to 1, and then computes the loss function from the true probability distribution.\n\n![image.png](attachment:image.png)","847614a0":"# L2 Norm Loss\/ Euclidean Loss \n\nL2 Norm Loss is just square of the difference\/distance between the predicted value and the true value. The L2 norm loss is good because it is curved or seem to converge near the target. Implementing this can make algorithms converge more slowly when approaching the target and avoid over-shooting the minimum.\n\n![image.png](attachment:image.png)","a4741c0e":"# Sigmoid Cross Entropy Loss\n\nThis is very similar to the cross entropy loss function, except that we transform the x-values by the sigmoid function before applying the cross entropy loss.\n\n![image.png](attachment:image.png)","e5e88447":"# Assignment (Week 5): Visualize Different Loss Functions","065064db":"# Sparse Softmax Cross Entropy Loss\n\nThis loss is the same as previous one, except instead of the target being a probability distribution, it is an index of which category is true. Instead of a sparse all-zero target vector with one value of one, we just pass in the index of which category is the true value, as follows:\n\n![image.png](attachment:image.png)"}}