{"cell_type":{"ff912472":"code","b82959a1":"code","5c84cc5e":"code","79c26b1d":"code","860080e9":"code","81e265a8":"code","6bd745c4":"code","16c4215e":"code","e853d331":"code","98975cdb":"code","23a50043":"code","b0bd0c0d":"code","7224528e":"code","8885188e":"code","2611a13c":"code","f0f87450":"code","d9402139":"code","a6503859":"code","11d5c147":"code","5c21802e":"code","436bb24c":"code","18e25b67":"code","190b7e65":"code","884c08fa":"code","8a201b48":"code","42a600f9":"code","877a1f3a":"code","c90052b9":"code","f3a4ff5b":"code","593ee4da":"code","bd69ff67":"markdown","12bf8358":"markdown","a4a7f827":"markdown","264224cb":"markdown","22338cf9":"markdown","1ebac15f":"markdown","c8bbd2f6":"markdown"},"source":{"ff912472":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b82959a1":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\ntrain.shape, test.shape","5c84cc5e":"trt = train.copy()\nprint(trt.shape)\ntrt = pd.get_dummies(trt, drop_first=True)\ntrt.shape","79c26b1d":"cols = trt.columns[1:15]    # change indexes to plot different features against SalePrice\nfig, ax = plt.subplots(len(cols), figsize=(8,55))\nfig.subplots_adjust(hspace=1)\nfor i, c in enumerate(cols):\n    ax[i].scatter(trt[c], trt['SalePrice'])\n    ax[i].set_yticks(range(0, trt['SalePrice'].max(), 100_000))\n    ax[i].grid()\n    ax[i].set_title(c)\nplt.show()","860080e9":"outliers = []\n\n# outliers.append(trt[trt['OverallCond']==5][trt['SalePrice']>700_000].index)\n# outliers.append(trt[trt['OverallCond']==2][trt['SalePrice']>300_000].index)\n# outliers.append(trt[trt['OverallCond']==6][trt['SalePrice']>700_000].index)\noutliers.append(trt[trt['OverallQual']==10][trt['SalePrice']<200_000].index)\noutliers.append(trt[trt['LotArea']>100_000].index)\noutliers.append(trt[trt['LotFrontage']>300].index)\noutliers.append(trt[trt['YearBuilt']<1900][trt['SalePrice']>200_000].index)\noutliers.append(trt[trt['YearRemodAdd']<2000][trt['SalePrice']>600_000].index)\noutliers.append(trt[trt['MasVnrArea']==1600].index)\noutliers.append(trt[trt['TotalBsmtSF']>3000][trt['SalePrice']<300_000].index)\noutliers.append(trt[trt['1stFlrSF']>2700][trt['SalePrice']<500_000].index)\noutliers.append(trt[trt['BsmtFullBath']==3.0].index)\noutliers.append(trt[trt['GrLivArea']>3300][trt['SalePrice']<300_000].index)\noutliers.append(trt[trt['FullBath']==0.0][trt['SalePrice']>300_000].index)\noutliers.append(trt[trt['GarageArea']>1200][trt['SalePrice']<200_000].index)\noutliers.append(trt[trt['OpenPorchSF']>500].index)\n\noutliers = [x[0] for x in outliers]\noutliers","81e265a8":"train.drop(outliers, axis=0, inplace=True)\ny = train['SalePrice']\ntrain.head(1)","6bd745c4":"trt.shape, train.shape","16c4215e":"df = pd.concat([train.drop(['SalePrice'], axis=1), test], join='outer')\ndf.drop(['Id'], axis=1)\ndf.shape","e853d331":"empty = [x for x in df if df[x].isna().sum() != 0]\nfor x in empty:\n    if df[x].dtype == 'float64':\n        print(x)\n    if df[x].dtype == 'int64':\n        print(x)","98975cdb":"to_mode = [\n    'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional',\n    'SaleType', 'LotFrontage'\n]\n\nto_none = [\n    'Alley', 'MasVnrType', 'BsmtQual', 'BsmtExposure', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', \n    'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n    'MiscFeature'\n]\n\nto_mean = [\n    'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageCars', 'GarageArea'\n]","23a50043":"for x in to_mod:\n    df[x+'for_na'] = df[x].apply(lambda a:0 if pd.isnull(a)==True else 1)\n    df[x] = df[x].fillna(df[x].mode()[0])\nfor x in to_zero:\n    df[x+'for_na'] = df[x].apply(lambda a:0 if pd.isnull(a)==True else 1)\n    df[x] = df[x].fillna(df[x].mean())\nfor x in to_mean:\n    df[x+'for_na'] = df[x].apply(lambda a:0 if pd.isnull(a)==True else 1)\n    df[x] = df[x].fillna('None')","b0bd0c0d":"df.isna().sum()","7224528e":"train.corr()[-1:]","8885188e":"cols_to_drop = [ 'YrSold', 'MoSold', 'BsmtHalfBath', 'BsmtFinSF2', 'KitchenAbvGr',\n                'LowQualFinSF', 'BedroomAbvGr', '3SsnPorch', \n               ]\ndf.drop(cols_to_drop, axis=1, inplace=True)","2611a13c":"df['MSSubClass'] = df['MSSubClass'].astype('category')","f0f87450":"df = pd.get_dummies(df)\nprint(df.isna().sum().sum())\ndf.head(1)","d9402139":"X = df[:train.shape[0]]\ntrain = df[:train.shape[0]]\ntst = df[train.shape[0]:]","a6503859":"X.shape, tst.shape","11d5c147":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor","5c21802e":"X = StandardScaler().fit_transform(X)\ntst = StandardScaler().fit_transform(tst)\nX.shape","436bb24c":"X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.01)","18e25b67":"#Ridge\nparams = {\n    'alpha': [25, 35],\n    'max_iter': [None, 1000, 5000],\n    'solver': ['svd', 'lsqr', 'sag', 'saga', 'sparse_cg', 'sparse_cg']\n}\n\nM1 = GridSearchCV(\n    Ridge(),\n    scoring='neg_mean_absolute_error',\n    param_grid=params,\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n)\nM1.fit(X, y)\n\nprint(M1.best_estimator_)\n\nmean_absolute_error(y_ts, M1.predict(X_ts))","190b7e65":"# Lasso\nparams = {\n    'alpha': [0.1, 1, 3],\n    'max_iter': [40_000],\n}\n\nM2 = GridSearchCV(\n    Lasso(),\n    scoring='neg_mean_absolute_error',\n    param_grid=params,\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n)\n\nM2.fit(X, y)\nprint(M2.best_estimator_)\n\nmean_absolute_error(y_ts, M2.predict(X_ts))","884c08fa":"# SVC\nparams = {\n    'kernel': ['rbf', 'sigmoid', 'linear'],\n    'C'  : [0,0.5,1,4],\n    'gamma' : [None, 0.01, 0.1, 1, 3]  \n}\n\nM4 = GridSearchCV(\n    SVR(),\n    scoring='neg_mean_absolute_error',\n    param_grid=params,\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n)\n\nM4.fit(X, y)\nprint(M4.best_estimator_)\n\nmean_absolute_error(y_ts, M4.predict(X_ts))","8a201b48":"#Gradient Boost\nparams = {\n    'n_estimators': [500],\n    'learning_rate': [0.01, 0.03, 0.1, 1],\n    'loss': ['ls'],\n}\n\nM5 = GridSearchCV(\n    GradientBoostingRegressor(),\n    scoring='neg_mean_absolute_error',\n    param_grid=params,\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n).fit(X,y)\n\nprint(M5.best_estimator_)\n\nmean_absolute_error(y_ts, M5.predict(X_ts))","42a600f9":"# XG boost\nparams = {\n    'learning_rate': [0.003, 0.01],\n    'n_estimators': [3000, 4000],\n    'max_depth': [2, 3],\n    'min_child_weight': [0, 1],\n    'gamma': [0],\n    'subsample': [0.5, 0.7],\n    'colsample_bytree':[0.5, 0.7],\n    'objective': ['reg:squarederror'],\n    'scale_pos_weight': [1, 2],\n    'reg_alpha': [0.00001, 0.001]\n}\n\nM6 = GridSearchCV(\n    XGBRegressor(),\n    scoring='neg_mean_absolute_error',\n    param_grid=params,\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n).fit(X,y)\n\nprint(M6.best_estimator_)\n\nmean_absolute_error(y_ts, M6.predict(X_ts))","877a1f3a":"# model = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n#                      max_depth=3, min_child_weight=0,\n#                      gamma=0, subsample=0.7,\n#                      colsample_bytree=0.7,\n#                      objective='reg:squarederror', nthread=-1,\n#                      scale_pos_weight=1, seed=27,\n#                      reg_alpha=0.00006)\n# model.fit(X,y)\n# mean_absolute_error(y_ts, model.predict(X_ts))","c90052b9":"preds = M6.predict(tst)\ntst.shape","f3a4ff5b":"submit_file = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\nsubmit_file['SalePrice'] = preds\nsubmit_file.to_csv('submission.csv', index=False)","593ee4da":"submit_file.shape","bd69ff67":"# Predicting and Submitting","12bf8358":"# Dealing with Missing Values","a4a7f827":"# Removing Outliers","264224cb":"I read the description of each feature and accordingly i filled their missing values with either the most repetitive values(mode) for categorical features, average for numerical features and 'None' for some features.\n\nFor example, it would be ridiculous to add Alley feature to the houses whose Alley feature is missing so a new value i.e. 'none' is added for such houses. Similarly every house has a Kitchen so i didnt put 'none' for missing KitchenQual values, rather i filled with most common KitchenQual value. Hope this kinda makes sense.","22338cf9":"# Exploring features","1ebac15f":"# Modelling","c8bbd2f6":"# Dropping Less Important Features"}}