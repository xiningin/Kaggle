{"cell_type":{"e48a7edf":"code","d2ca7e95":"code","cf6e46d2":"code","70f45c45":"code","bd492bc0":"code","a5365e4c":"code","cc58687c":"code","6c2c6363":"code","4b3b7c63":"code","d6da4887":"code","f250f302":"code","6612b6a4":"code","fec546ef":"code","a2d66db9":"code","15aa19ee":"code","72aa25f7":"code","b30791f0":"code","78819fd4":"code","94fac101":"code","8ff70dde":"code","d2ebc720":"code","d7b4c421":"code","f77a4599":"code","ead25365":"code","d5f36117":"code","719936e0":"code","6177cd2b":"code","e2075a7c":"code","a4bcd6ff":"code","182b5bbf":"code","9b3031ec":"code","9b29e116":"code","a915449b":"code","d19fdc29":"code","fe243cd8":"code","b82cc001":"code","82e840d0":"code","8cb825e1":"code","522d8a19":"code","94cd6ce9":"code","44480872":"code","b728dfc8":"code","71c32d81":"code","1c4d0e76":"code","fa6ef6c0":"code","81207cc0":"code","d416ffce":"code","7e26a125":"code","db8865d9":"code","2610fea3":"code","53cd5cf8":"code","94f73e51":"code","797b3464":"code","045ce192":"code","962a4342":"markdown","c667e839":"markdown","83fc2e8e":"markdown","4931ab93":"markdown","83495598":"markdown","cbb7437f":"markdown","370c8234":"markdown","28368eb1":"markdown","121a4088":"markdown","80809f9b":"markdown","87998f13":"markdown","69f815c2":"markdown","1b10f910":"markdown","0d379347":"markdown","cab1fb1e":"markdown","096915a8":"markdown","f41673f6":"markdown","0206bdfc":"markdown","8dd1e018":"markdown","423d7658":"markdown","f0d550c7":"markdown"},"source":{"e48a7edf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","d2ca7e95":"# reading the dataset\ncars = pd.read_csv(\"..\/input\/carprice\/CarPrice_Assignment.csv\")","cf6e46d2":"# summary of the dataset: 205 rows, 26 columns, no null values\nprint(cars.info())","70f45c45":"# head\ncars.head()","bd492bc0":"# symboling: -2 (least risky) to +3 most risky\n# Most cars are 0,1,2\ncars['symboling'].astype('category').value_counts()\n\n","a5365e4c":"# aspiration: An (internal combustion) engine property showing \n# whether the oxygen intake is through standard (atmospheric pressure)\n# or through turbocharging (pressurised oxygen intake)\n\ncars['aspiration'].astype('category').value_counts()","cc58687c":"# drivewheel: frontwheel, rarewheel or four-wheel drive \ncars['drivewheel'].astype('category').value_counts()","6c2c6363":"# wheelbase: distance between centre of front and rarewheels\nsns.distplot(cars['wheelbase'])\nplt.show()","4b3b7c63":"# curbweight: weight of car without occupants or baggage\nsns.distplot(cars['curbweight'])\nplt.show()","d6da4887":"# stroke: volume of the engine (the distance traveled by the \n# piston in each cycle)\nsns.distplot(cars['stroke'])\nplt.show()","f250f302":"# compression ration: ratio of volume of compression chamber \n# at largest capacity to least capacity\nsns.distplot(cars['compressionratio'])\nplt.show()","6612b6a4":"# target variable: price of car\nsns.distplot(cars['price'])\nplt.show()","fec546ef":"# all numeric (float and int) variables in the dataset\ncars_numeric = cars.select_dtypes(include=['float64', 'int64'])\ncars_numeric.head()","a2d66db9":"# dropping symboling and car_ID \ncars_numeric = cars_numeric.drop(['symboling'], axis=1)\ncars_numeric.head()","15aa19ee":"# paiwise scatter plot\n\nplt.figure(figsize=(20, 10))\nsns.pairplot(cars_numeric)\nplt.show()","72aa25f7":"# correlation matrix\ncor = cars_numeric.corr()\ncor","b30791f0":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()\n","78819fd4":"# variable formats\ncars.info()","94fac101":"# converting symboling to categorical\ncars['symboling'] = cars['symboling'].astype('object')\ncars.info()","8ff70dde":"# CarName: first few entries\ncars['CarName'][:30]","d2ebc720":"# Extracting carname\n\n# Method 1: str.split() by space\ncarnames = cars['CarName'].apply(lambda x: x.split(\" \")[0])\ncarnames[:30]","d7b4c421":"# Method 2: Use regular expressions\nimport re\n\n# regex: any alphanumeric sequence before a space, may contain a hyphen\np = re.compile(r'\\w+-?\\w+')\ncarnames = cars['CarName'].apply(lambda x: re.findall(p, x)[0])\nprint(carnames)","f77a4599":"# New column car_company\ncars['car_company'] = cars['CarName'].apply(lambda x: re.findall(p, x)[0])","ead25365":"# look at all values \ncars['car_company'].astype('category').value_counts()","d5f36117":"# replacing misspelled car_company names\n\n# volkswagen\ncars.loc[(cars['car_company'] == \"vw\") | \n         (cars['car_company'] == \"vokswagen\")\n         , 'car_company'] = 'volkswagen'\n\n# porsche\ncars.loc[cars['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n\n# toyota\ncars.loc[cars['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n\n# nissan\ncars.loc[cars['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n\n# mazda\ncars.loc[cars['car_company'] == \"maxda\", 'car_company'] = 'mazda'","719936e0":"cars['car_company'].astype('category').value_counts()","6177cd2b":"# drop carname variable\ncars = cars.drop('CarName', axis=1)","e2075a7c":"cars.info()","a4bcd6ff":"# outliers\ncars.describe()","182b5bbf":"cars.info()","9b3031ec":"# split into X and y\nX = cars.loc[:, ['symboling', 'fueltype', 'aspiration', 'doornumber',\n       'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength',\n       'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber',\n       'enginesize', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio',\n       'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       'car_company']]\n\ny = cars['price']\n","9b29e116":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\ncars_categorical.head()\n","a915449b":"# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\ncars_dummies.head()","d19fdc29":"# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)","fe243cd8":"# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","b82cc001":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","82e840d0":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","8cb825e1":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","522d8a19":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=1000]\ncv_results.head()","94cd6ce9":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","44480872":"alpha = 10\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_","b728dfc8":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","71c32d81":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","1c4d0e76":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","fa6ef6c0":"alpha =100\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","81207cc0":"lasso.coef_","d416ffce":"elasticnet = ElasticNet()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = elasticnet, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","7e26a125":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","db8865d9":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(np.log10(cv_results['param_alpha']), cv_results['mean_train_score'])\nplt.plot(np.log10(cv_results['param_alpha']), cv_results['mean_test_score'])\nplt.xlabel('log10(alpha)')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","2610fea3":"alpha = 0.1\n\nelasticnet = ElasticNet(alpha=alpha)\n        \nelasticnet.fit(X_train, y_train) ","53cd5cf8":"elasticnet.coef_","94f73e51":"def calculate_aic(n, mse, num_params):\n    aic = n * np.log(mse) + 2 * num_params\n    return aic\n\ndef calculate_bic(n, mse, num_params):\n    bic = n * np.log(mse) + np.log(n) * num_params\n    return bic","797b3464":"features = X_train.columns\n\nregression = LinearRegression()\n\nselected_features = []\nmin_aic = np.inf\nfor step in range(0, 10, 1):\n    for feature in features:\n        testing_features = selected_features + [feature]\n        regression.fit(X_train[testing_features], y_train)\n        y_pred = regression.predict(X_test[testing_features])\n        mse = metrics.mean_squared_error(y_test, y_pred)\n        num_params = len(regression.coef_) + 1  # features and intercept\n        n = X_train.shape[0]\n        aic = calculate_aic(n, mse, num_params)\n        bic = calculate_bic(n, mse, num_params)\n        if aic < min_aic:\n            min_aic = aic\n            current_bic = bic\n            best_feature = feature\n    selected_features = selected_features + [best_feature]\n    print(\"Selected features: \", selected_features, \"\\nAIC: \", min_aic, \"\\nBIC: \", current_bic)\n\ny_pred = regression.predict(X_test[selected_features])\nprint(\"\\n\\nFinal r-squared: \", metrics.r2_score(y_test, y_pred))","045ce192":"regression.coef_","962a4342":"#### Data Exploration\n\nTo perform linear regression, the (numeric) target variable should be linearly related to *at least one another numeric variable*. Let's see whether that's true in this case.\n\n\nWe'll first subset the list of all (independent) numeric variables, and then make a **pairwise plot**.","c667e839":"This is quite hard to read, and we can rather plot correlations between variables. Also, a heatmap is pretty useful to visualise multiple correlations in one plot.","83fc2e8e":"Netx, we need to extract the company name from the column ```CarName```. ","4931ab93":"The heatmap shows some useful insights:\n\nCorrelation of price with independent variables:\n- Price is highly (positively) correlated with wheelbase, carlength, carwidth, curbweight, enginesize, horsepower (notice how all of these variables represent the size\/weight\/engine power of the car)\n\n- Price is negatively correlated to ```citympg``` and ```highwaympg``` (-0.70 approximately). This suggest that cars having high mileage may fall in the 'economy' cars category, and are priced lower (think Maruti Alto\/Swift type of cars, which are designed to be affordable by the middle class, who value mileage more than horsepower\/size of car etc.)\n\nCorrelation among independent variables:\n- Many independent variables are highly correlated (look at the top-left part of matrix): wheelbase, carlength, curbweight, enginesize etc. are all measures of 'size\/weight', and are positively correlated \n\n\nThus, while building the model, we'll have to pay attention to multicollinearity (especially linear models, such as linear and logistic regression, suffer more from multicollinearity).","83495598":"Notice that **some car-company names are misspelled** - vw and vokswagen should be volkswagen, porcshce should be porsche, toyouta should be toyota, Nissan should be nissan, maxda should be mazda etc.\n\nThis is a data quality issue, let's solve it.","cbb7437f":"### 1. Data Understanding and Exploration\n\nLet's first have a look at the dataset and understand the size, attribute names etc.","370c8234":"Here, although the variable ```symboling``` is numeric (int), we'd rather treat it as categorical since it has only 6 discrete values. Also, we do not want 'car_ID'.","28368eb1":"Notice that the carname is what occurs before a space, e.g. alfa-romero, audi, chevrolet, dodge, bmx etc.\n\nThus, we need to simply extract the string before a space. There are multiple ways to do that.\n\n\n","121a4088":"## Elastic Net","80809f9b":"The ```car_company``` variable looks okay now. Let's now drop the car name variable.","87998f13":"## Car Price Prediction - Assignment Solution\n\nThe solution is divided into the following sections: \n- Data understanding and exploration\n- Data cleaning\n- Data preparation\n- Model building and evaluation\n","69f815c2":"## Ridge Regression","1b10f910":"## 2. Data Cleaning\n\nLet's now conduct some data cleaning steps. \n\nWe've seen that there are no missing values in the dataset. We've also seen that variables are in the correct format, except ```symboling```, which should rather be a categorical variable (so that dummy variable are created for the categories).\n\nNote that it *can* be used in the model as a numeric variable also. \n\n","0d379347":"## 3. Data Preparation \n\n\n#### Data Preparation\n\nLet's now prepare the data and build the model.","cab1fb1e":"Let's now make a pairwise scatter plot and observe linear relationships.","096915a8":"## Lasso","f41673f6":"## 3. Model Building and Evaluation","0206bdfc":"## Forward Feature Selection","8dd1e018":"Let's create a new column to store the compnay name and check whether it looks okay.","423d7658":"#### Understanding the Data Dictionary\n\nThe data dictionary contains the meaning of various attributes; some non-obvious ones are:","f0d550c7":"## Ridge and Lasso Regression\n\nLet's now try predicting car prices, a dataset used in simple linear regression, to perform ridge and lasso regression."}}