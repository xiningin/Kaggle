{"cell_type":{"46e465c4":"code","6bc6b580":"code","87216ac8":"code","8ae0b350":"code","b25411d4":"code","6e23ae5c":"code","0f63e674":"code","30b3253d":"code","2221faa6":"code","73012e43":"code","dca374d8":"code","a8dc2c78":"code","c4a46bc6":"code","8992fb50":"code","7fd9de9d":"code","ad785211":"code","24558ddc":"code","00e96463":"markdown"},"source":{"46e465c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bc6b580":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.head()","87216ac8":"#df = df.drop(columns = \"PassengerId\")\n#df = df.drop(columns = \"Name\")\ndf.head()","8ae0b350":"df.isnull().sum()","b25411d4":"from sklearn.impute import SimpleImputer\nimport numpy as np\n\n#Takes numpy array #Outputs numpy array\nimr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data ","6e23ae5c":"\n#df = df.drop(columns='num')\ndf","0f63e674":"x = df.values[:,1:]\ny = df.values[:,0]\ny=y.astype('int')\ny","30b3253d":"x.shape","2221faa6":"df.head()","73012e43":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer #takes a list of tuples (string:name of transformation, the transformer, the columns to be transformed)\n\nc_transf = ColumnTransformer([ ('onehot', OneHotEncoder(), [1 , 5 ,7 ,8]),\n                              ('nothing', 'passthrough', [ 0, 2, 3 , 4 , 6 ]) ])\nX_enc = c_transf.fit_transform(x).astype(float).toarray()\nX_enc","dca374d8":"X_enc.shape","a8dc2c78":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nforest = RandomForestClassifier(criterion='gini',\n                                n_estimators= 100 , max_depth = 15 )\n\n\nforest.fit(X_enc,y)\n\n\n\nscores = cross_val_score(estimator= forest ,\n                         X= X_enc ,\n                         y=y ,\n                         cv=7)\n\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","c4a46bc6":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score\n\nscorer = make_scorer(f1_score)\n\nn_estimators_range = [100]\n\nmax_depth_range = [ 15  ]\n\nparam_grid = [ {'n_estimators': n_estimators_range , 'max_depth': max_depth_range } ]\n#param_grid = [ {'max_depth': max_depth_range } ]\n\ngs = GridSearchCV(estimator=forest, \n                  param_grid=param_grid, \n                  scoring= scorer , \n                  refit=True,\n                  cv=5)\n\ngs = gs.fit(X_enc, y)\nprint(gs.best_score_)\nprint(gs.best_params_)","8992fb50":"df.insert(1, 'num', df[\"SibSp\"] + df [\"Parch\"])\n\ndf.head()","7fd9de9d":"df_submit = pd.read_csv('..\/input\/titanic\/test.csv')\n\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n#Takes numpy array #Outputs numpy array\nimr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimr = imr.fit(df_submit.values)\nimputed_data = imr.transform(df_submit.values)\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer #takes a list of tuples (string:name of transformation, the transformer, the columns to be transformed)\n\nc_transf = ColumnTransformer([ ('onehot', OneHotEncoder(), [1 , 5 ,7 ,8]),\n                              ('nothing', 'passthrough', [ 0, 2, 3 , 4 , 6 ]) ])\nX_enc = c_transf.fit_transform(x).astype(float).toarray()\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer #takes a list of tuples (string:name of transformation, the transformer, the columns to be transformed)\n\n\n#df_submit = df_submit.drop(columns = \"PassengerId\")\n#df_submit = df_submit.drop(columns = \"Name\")\n#df_submit.insert(1, 'num', df[\"SibSp\"] + df [\"Parch\"])\n\nX_submit = df_submit.values\n\nprint(\"Evaluation..\")\ny_submit = forest.predict(X_submit)\n\nsubmission = {'Id':id_submit, 'Cover_Type': (y_submit + 1).astype(int)}\ndf_submission = pd.DataFrame(submission)\ndf_submission.to_csv('submission.csv', index=False)\n","ad785211":"df_submit.head()\n","24558ddc":"# Preprocess Test Data\n#df_submit = df_submit.drop(columns = \"PassengerId\")\n#df_submit = df_submit.drop(columns = \"Name\")\n\n## put your feature engineering here but for df_submit ##\n#-------------------------------------------------------#\n#########################################################\n#df.insert(1, 'num', df[\"SibSp\"] + df [\"Parch\"])\n\n\n\n\n","00e96463":"feature engineering "}}