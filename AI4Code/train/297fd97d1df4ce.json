{"cell_type":{"193d21fe":"code","22440004":"code","527e0431":"code","f76229a1":"code","49cd9184":"code","faad5a4d":"code","fc395460":"code","654924b0":"code","9f88f7ef":"code","5067a67d":"code","58e51a01":"code","33da7b1f":"code","2a4b7f1e":"code","f2dbb5f3":"code","d2d2df7f":"code","02b10682":"code","66e12529":"code","3e7cb476":"code","1882b440":"code","aeca5f07":"code","39693d59":"code","c55f4b95":"code","3e9f1c55":"code","22a1d57e":"code","10298702":"code","4fb142e5":"code","0dfc6b50":"code","57e72b3f":"code","4223d775":"code","8c2737be":"code","5f11a093":"code","fe22f8c7":"code","b58d2f1d":"code","8fca8cdd":"code","0cc8a3c9":"code","5c111713":"code","ea4187f4":"code","b99b671b":"markdown","3050312b":"markdown","f2ab1267":"markdown","6ba26a04":"markdown","c484bf08":"markdown"},"source":{"193d21fe":"import xgboost as xgb\nimport lightgbm as lgb","22440004":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/ashrae-energy-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","527e0431":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n \n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","f76229a1":"build_meta = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv')\nbuild_meta = reduce_mem_usage(build_meta)\nweather_train = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/weather_train.csv')\nweather_train = reduce_mem_usage(weather_train)\nweather_test = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/weather_test.csv')\nweather_test = reduce_mem_usage(weather_test)\ntrain_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/train.csv')\ntrain_df = reduce_mem_usage(train_df)\ntest_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/test.csv')\ntest_df = reduce_mem_usage(test_df)","49cd9184":"print(train_df.shape[0])\nprint(test_df.shape[0])","faad5a4d":"gc.collect()","fc395460":"import dask.dataframe as dd","654924b0":"# Join building & weather based on site_id\ntrain_build_df = dd.merge(train_df,build_meta,on='building_id',how='left')\nprint(train_build_df.shape[0])\ntrain_2_df = dd.merge(train_build_df,weather_train,on=['site_id','timestamp'],how='left')\nprint(train_2_df.shape[0])\n\n# Join test with building meta data & weather data\ntest_build_df = dd.merge(test_df,build_meta,on='building_id',how='left')\nprint(test_build_df.shape[0])\ntest_2_df = dd.merge(test_build_df,weather_test,on=['site_id','timestamp'],how='left')\nprint(test_2_df.shape[0])","9f88f7ef":"del train_build_df\ndel test_build_df","5067a67d":"gc.collect()","58e51a01":"print(train_2_df.info())","33da7b1f":"print(test_2_df.info())","2a4b7f1e":"print(train_2_df.isna().sum())","f2dbb5f3":"print(test_2_df.isna().sum())","d2d2df7f":"train_final_df = train_2_df[['building_id','meter','timestamp','site_id','primary_use','square_feet','meter_reading']]","02b10682":"gc.collect()","66e12529":"train_final_df['timestamp'] = pd.to_datetime(train_final_df['timestamp'], format='%Y-%m-%d %H:%M:%S')","3e7cb476":"train_final_df.info()","1882b440":"train_final_df.head()","aeca5f07":"train_final_df['meter'] = train_final_df['meter'].astype('category')","39693d59":"train_final_df['Day_of_Week'] = train_final_df['timestamp'].dt.weekday\ntrain_final_df['Hour_of_Day'] = train_final_df['timestamp'].dt.hour","c55f4b95":"def Weekday_Weekend(df):\n    df['Weekend'] = df['Day_of_Week'].apply(lambda x: 'Y' if x>5 else 'N')\n\ndef Office_Hour(df):\n    df['OfficeTime'] = df['Hour_of_Day'].apply(lambda x: 'Y' if (x>=7 & x<=18) else 'N')","3e9f1c55":"from dask.multiprocessing import get","22a1d57e":"ddata_train = dd.from_pandas(train_final_df,npartitions=20)","10298702":"ddata_train.head()","4fb142e5":"ddout = ddata_train.map_partitions(Weekday_Weekend)\nresult = ddout.compute()\n\nddout = ddata_train.map_partitions(Office_Hour)\nresult = ddout.compute()\n","0dfc6b50":"ddata_train.head()","57e72b3f":"del train_final_df","4223d775":"# COnvert back to pandas dataframe\ntrain_final_1_df = ddata_train.compute()\ntrain_final_1_df.info()","8c2737be":"train_final_1_df = train_final_1_df[['building_id','meter','primary_use','square_feet','Weekend','OfficeTime','Hour_of_Day','meter_reading']]\ntrain_final_1_df = pd.get_dummies(train_final_1_df)","5f11a093":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor","fe22f8c7":"import pickle\n# Split the train to 2 data sets, using 1 for training & other for validation\nnrow = train_final_1_df.shape[0]\ntrain = train_final_1_df[:round(nrow\/2)]\ntest = train_final_1_df[round(nrow\/2)+1:]\n\n\n# Split to train & validation\n#train , test = train_test_split(train_final_1_df, test_size = 0.3)\n\n\nx_train = train.drop('meter_reading', axis=1)\ny_train = train['meter_reading']\nx_test = test.drop('meter_reading', axis = 1)\ny_test = test['meter_reading']\n\n\n# Scale the features\n#scaler = MinMaxScaler(feature_range=(0, 1)\n#x_train_scaled = scaler.fit_transform(x_train)\n#x_train = pd.DataFrame(x_train_scaled)\n#x_test_scaled = scaler.fit_transform(x_test)\n#x_test = pd.DataFrame(x_test_scaled)\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn\n# Set parameters for grid search\nparam_grid = [\n{'n_estimators':[100,150],\n'criterion':['mae'],\n'max_features':['auto'],\n'min_impurity_decrease':[0],\n'n_jobs':[-1],\n'bootstrap':[False]}\n]\n\n# Fit the model\nxgbreg = RandomForestRegressor(random_state=123,n_estimators=200,n_jobs=-1,verbose=2)\nxgbreg.fit(x_train, y_train)\n\n# Predict for test\n#best_model_RF = grid_search.best_estimator_\ny_pred=xgbreg.predict(x_test)\nprint('RMSLE :',np.sqrt(mean_squared_log_error(y_test+1,y_pred+1)))\nmodel_name = 'Model_1.sav'\npickle.dump(xgbreg, open(model_name, 'wb'))\n\n\n\n# 2nd round\ntrain = train_final_1_df[round(nrow\/2)+1:]\ntest = train_final_1_df[:round(nrow\/2)]\n\n\n# Split to train & validation\n#train , test = train_test_split(train_final_1_df, test_size = 0.3)\n\n\nx_train = train.drop('meter_reading', axis=1)\ny_train = train['meter_reading']\nx_test = test.drop('meter_reading', axis = 1)\ny_test = test['meter_reading']\n\n\n# Scale the features\n#scaler = MinMaxScaler(feature_range=(0, 1)\n#x_train_scaled = scaler.fit_transform(x_train)\n#x_train = pd.DataFrame(x_train_scaled)\n#x_test_scaled = scaler.fit_transform(x_test)\n#x_test = pd.DataFrame(x_test_scaled)\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn\n# Set parameters for grid search\nparam_grid = [\n{'n_estimators':[100,150],\n'criterion':['mae'],\n'max_features':['auto'],\n'min_impurity_decrease':[0],\n'n_jobs':[-1],\n'bootstrap':[False]}\n]\n\n# Fit the model\nxgbreg = RandomForestRegressor(random_state=123,n_estimators=200,n_jobs=-1,verbose=2)\nxgbreg.fit(x_train, y_train)\n\n# Predict for test\n#best_model_RF = grid_search.best_estimator_\ny_pred=xgbreg.predict(x_test)\nprint('RMSLE :',np.sqrt(mean_squared_log_error(y_test+1,y_pred+1)))\n\nmodel_name = 'Model_2.sav'\npickle.dump(xgbreg, open(model_name, 'wb'))","b58d2f1d":"gc.collect()","8fca8cdd":"test_final_df = test_2_df[['building_id','meter','timestamp','site_id','primary_use','square_feet']]\ntest_final_df['timestamp'] = pd.to_datetime(test_final_df['timestamp'], format='%Y-%m-%d %H:%M:%S')\ntest_final_df['meter'] = test_final_df['meter'].astype('category')\ntest_final_df['Day_of_Week'] = test_final_df['timestamp'].dt.weekday\ntest_final_df['Hour_of_Day'] = test_final_df['timestamp'].dt.hour\n\nddata_test = dd.from_pandas(test_final_df,npartitions=40)\nddout_test = ddata_test.map_partitions(Weekday_Weekend)\nresult = ddout_test.compute()\n\nddout_test = ddata_test.map_partitions(Office_Hour)\nresult = ddout_test.compute()\n\n\n# COnvert back to pandas dataframe\ntest_final_1_df = ddata_test.compute()\ntest_final_1_df.info()\n\ntest_final_1_df = test_final_1_df[['building_id','meter','primary_use','square_feet','Weekend','OfficeTime','Hour_of_Day']]\ntest_final_1_df = pd.get_dummies(test_final_1_df)","0cc8a3c9":"gc.collect()","5c111713":"# Load the models \nmodel_name1 = 'Model_1.sav'\nmodel_name2 = 'Model_2.sav'\nmodel1 = pickle.load(open(model_name1, 'rb'))\nmodel2 = pickle.load(open(model_name2, 'rb'))\ny_pred_1 = (model1.predict(test_final_1_df))\ny_pred_2 = (model2.predict(test_final_1_df))\nresult = pd.DataFrame({'row_id':test_2_df['row_id'],'pred_1':y_pred_1,'pred_2':y_pred_2})","ea4187f4":"gc.collect()\nresult['meter_reading'] = (result['pred_1']+result['pred_2'])\/2\nfinal = result[['row_id','meter_reading']]\nfinal.to_csv('submission.csv',index=False)","b99b671b":"Read Files","3050312b":"* Missing values in both train & test data sets\n* Ignore those columns for now","f2ab1267":"* ~ 20Million records in train\n* ~ 40 Million records in test","6ba26a04":"<font color=blue> As Air_temp,dew_temp & wind_speed missing values are less in number, replace those missing values with mean <\/font>","c484bf08":"* Convert timestamp column to datetime\n* Extract Day of week, weekday\/weekend, Office hours\/Post Office hours"}}