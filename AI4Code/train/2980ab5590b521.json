{"cell_type":{"156bc55b":"code","2a241fa7":"code","0be3b96e":"code","22d531a5":"code","85c570d7":"code","df7af8ef":"code","1088c09c":"code","8890174a":"code","3f0b098f":"code","2db8b42d":"code","058c88a6":"code","d7477104":"code","18650e97":"code","ec7d2823":"code","02dde2d4":"code","f9b364d9":"code","86287f3a":"code","45c60080":"code","c024feb6":"code","9e245b53":"code","362e1d1a":"code","17f94982":"code","982218ee":"code","14757dc8":"code","c848d6bd":"code","fb92e77b":"code","021d1bd5":"code","8f4b268f":"code","f7edcddd":"code","961c5998":"code","66845ea8":"code","9768730d":"code","c1890b19":"code","cce3f848":"code","851dd83c":"code","29597f3c":"code","3fc3eb66":"code","6ec7069f":"code","54e8419c":"markdown","4308b887":"markdown","788ea78b":"markdown","8ec35db7":"markdown","cf74e5f3":"markdown","9f8d0ad1":"markdown"},"source":{"156bc55b":"! pip install nlplot","2a241fa7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nimport emoji\nimport nlplot\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop=set(stopwords.words('english'))\nfrom collections import defaultdict\nfrom collections import  Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report,accuracy_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,BatchNormalization,TimeDistributed,Dropout,Bidirectional,Flatten,GlobalMaxPool1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth', 300)\npd.options.display.float_format = '{:.3f}'.format","0be3b96e":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","22d531a5":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","85c570d7":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","df7af8ef":"display(train.head(), test.head())","1088c09c":"npt = nlplot.NLPlot(train, target_col='text')","8890174a":"stopwords = npt.get_stopword(top_n=30, min_freq=0)\nstopwords","3f0b098f":"npt.bar_ngram(\n    title='uni-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=1,\n    top_n=30,\n    height=700,\n    stopwords=stopwords,\n)","2db8b42d":"npt.bar_ngram(\n    title='bi-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=2,\n    top_n=30,\n    height=700,\n    stopwords=stopwords,\n)","058c88a6":"npt.bar_ngram(\n    title='tri-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=3,\n    top_n=30,\n    height=700\n)","d7477104":"# \u30d3\u30eb\u30c9\uff08\u30c7\u30fc\u30bf\u4ef6\u6570\u306b\u3088\u3063\u3066\u306f\u51e6\u7406\u306b\u6642\u9593\u3092\u8981\u3057\u307e\u3059\uff09\nnpt.build_graph(stopwords=stopwords, min_edge_frequency=22)","18650e97":"npt.co_network(\n    title='Co-occurrence network',\n    width=1000\n)","ec7d2823":"npt.sunburst(\n    title='sunburst chart',\n    colorscale=True,\n    width=1000\n)","02dde2d4":"df = pd.concat([train,test])\ndf.shape","f9b364d9":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n\ndef remove_number(text):\n    num = re.compile(r'\\w*\\d\\w*')\n    return num.sub(r'',text)\n\n\ndef acronyms(text):\n    # cf. https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#4.-Embeddings-and-Text-Cleaning\n    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n    text = re.sub(r\"m\u00cc\u00bcsica\", \"music\", text)\n    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)\n    text = re.sub(r\"gawx\", \"Georgia Weather\", text)\n    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)\n    text = re.sub(r\"cawx\", \"California Weather\", text)\n    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n    text = re.sub(r\"azwx\", \"Arizona Weather\", text)\n    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)\n    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)\n\n    # Grouping same words without embeddings\n    text = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", text)\n    text = re.sub(r\"SOUDELOR\", \"Soudelor\", text)\n    \n    return text\n\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = acronyms(text)\n    text = text.lower()\n    text = remove_URL(text)\n    text = remove_html(text)\n    text = remove_emoji(text)\n    text = remove_punct(text)\n    text = remove_number(text)\n    \n    return text","86287f3a":"df['text'] = df['text'].progress_apply(lambda x: clean_text(x))","45c60080":"df.head()","c024feb6":"# curpus\u306e\u751f\u6210\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","9e245b53":"corpus = create_corpus(df)","362e1d1a":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","17f94982":"MAX_LEN = 100\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","982218ee":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","14757dc8":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","c848d6bd":"train_tweet = tweet_pad[:train.shape[0]]\ntest_tweet = tweet_pad[train.shape[0]:]","fb92e77b":"X_train,X_test,y_train,y_test=train_test_split(train_tweet, train['target'].values,test_size=0.1)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","021d1bd5":"def build_BLSTM():\n    model = Sequential()\n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=MAX_LEN))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(MAX_LEN, return_sequences = True, recurrent_dropout=0.2)))\n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(MAX_LEN, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(MAX_LEN, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","8f4b268f":"model = build_BLSTM()\nmodel.summary()","f7edcddd":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)","961c5998":"history = model.fit(\n    X_train, \n    y_train, \n    epochs = 15,\n    batch_size = 64,\n    validation_data = [X_test, y_test],\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","66845ea8":"def plot(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n        \n        \ndef metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))","9768730d":"plot(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","c1890b19":"loss, accuracy = model.evaluate(X_test, y_test)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)","cce3f848":"preds = model.predict_classes(X_test)\nmetrics(preds, y_test)","851dd83c":"y_pred = model.predict(test_tweet)\ny_pred = np.round(y_pred).astype(int).reshape(3263)\nsub = pd.DataFrame({'id':submission['id'].values.tolist(),'target':y_pred})","29597f3c":"sub","3fc3eb66":"sub['target'].hist()","6ec7069f":"sub.to_csv('submission.csv',index=False)","54e8419c":"# Data cleaning","4308b887":"## preprosessing","788ea78b":"# data load","8ec35db7":"# Training","cf74e5f3":"# plot using \"nlplot\"\n\nhttps:\/\/github.com\/takapy0210\/nlplot","9f8d0ad1":"## build model"}}