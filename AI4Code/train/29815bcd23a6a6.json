{"cell_type":{"72184a71":"code","bd650319":"code","521188b3":"code","7b7ba700":"code","05baa1fb":"code","df5132d4":"code","27fdd36d":"code","0cfb401f":"code","ca5fe3bf":"code","b49778a4":"code","3f447925":"code","6fe7d70a":"code","d47b98ac":"code","6e49797e":"code","280a0089":"code","e3a41290":"code","c53ef796":"code","b1440d84":"code","a46d3222":"code","11441912":"code","e9e7c0bc":"code","0c1155b1":"code","6e247990":"code","e2318c3b":"code","0b1ba5a1":"code","6b37d481":"code","a61e5df2":"code","42dc80d4":"code","84e2c174":"code","bd6125fb":"code","d74fe12d":"code","a6ccb986":"code","1d202ebf":"code","5066573b":"code","44cf2e5a":"code","4f37df34":"code","d556f7ec":"code","317f3902":"code","17fda36e":"code","483cd54b":"code","e4ffeb33":"code","048092ff":"code","d1a9aa35":"code","f75af15c":"code","9c582f20":"code","8b5adba0":"code","f444f77e":"code","0eb25d81":"code","c7a93a0b":"code","fb8934e0":"code","adeed3bf":"code","528464dd":"code","9e10ee0c":"code","ee9f147e":"code","a61c8498":"code","9a49eaf5":"code","db44e21b":"code","38feb473":"code","6f43b24f":"markdown","8b1602b9":"markdown","96e8b207":"markdown","be6063c2":"markdown","389bbcef":"markdown","428ccac8":"markdown","83cd947c":"markdown","831ba185":"markdown","7b982bf0":"markdown"},"source":{"72184a71":"# import python standard library\nimport re\n\n# import data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# import data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import sklearn model class\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# import sklearn model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# import sklearn model evaluation classification metrics\nfrom sklearn.metrics import accuracy_score, auc, classification_report, confusion_matrix, f1_score, fbeta_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve","bd650319":"# acquiring training and testing data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","521188b3":"# visualize head of the training data\ndf_train.head(n=5)","7b7ba700":"# visualize tail of the testing data\ndf_test.tail(n=5)","05baa1fb":"# combine training and testing dataframe\ndf_train['DataType'], df_test['DataType'] = 'training', 'testing'\ndf_test.insert(1, 'Survived', np.nan)\ndf_data = pd.concat([df_train, df_test], ignore_index=True)","df5132d4":"def countplot(categorical_x: list or str, categorical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a count plot applied for categorical variable in x-axis vs categorical variable in y-axis.\n    \n    Args:\n        categorical_x (list or str): The categorical variable in x-axis.\n        categorical_y (list or str): The categorical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    categorical_x, categorical_y = [categorical_x] if type(categorical_x) == str else categorical_x, [categorical_y] if type(categorical_y) == str else categorical_y\n    if nrows is None: nrows = (len(categorical_x)*len(categorical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.countplot(x=vj, hue=vi, data=data, ax=axes[i*len(categorical_x) + j], rasterized=True) for i, vi in enumerate(categorical_y) for j, vj in enumerate(categorical_x)]\n    return fig","27fdd36d":"def swarmplot(categorical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a swarm plot applied for categorical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        categorical_x (list or str): The categorical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    categorical_x, numerical_y = [categorical_x] if type(categorical_x) == str else categorical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(categorical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.swarmplot(x=vj, y=vi, data=data, ax=axes[i*len(categorical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(categorical_x)]\n    return fig","0cfb401f":"def violinplot(categorical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a violin plot applied for categorical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        categorical_x (list or str): The categorical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    categorical_x, numerical_y = [categorical_x] if type(categorical_x) == str else categorical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(categorical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.violinplot(x=vj, y=vi, data=data, ax=axes[i*len(categorical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(categorical_x)]\n    return fig","ca5fe3bf":"# describe training and testing data\ndf_data.describe(include='all')","b49778a4":"# convert dtypes numeric to object\ncol_convert = ['Survived', 'Pclass', 'SibSp', 'Parch']\ndf_data[col_convert] = df_data[col_convert].astype('object')","3f447925":"# list all features type number\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\nprint('features type number:\\n items %s\\n length %d' %(col_number, len(col_number)))\n\n# list all features type object\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\nprint('features type object:\\n items %s\\n length %d' %(col_object, len(col_object)))","6fe7d70a":"# feature extraction: surname\ndf_data['Surname'] = df_data['Name'].str.extract(r'([A-Za-z]+),', expand=False)","d47b98ac":"# feature extraction: title\ndf_data['Title'] = df_data['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\ndf_data['Title'] = df_data['Title'].replace(['Capt', 'Rev'], 'Crew')\ndf_data['Title'] = df_data['Title'].replace('Ms', 'Miss')\ndf_data['Title'] = df_data['Title'].replace(['Col', 'Countess', 'Don', 'Dona', 'Jonkheer', 'Lady', 'Major', 'Mlle', 'Mme', 'Sir'], 'Royal')\ndf_data['Title'].value_counts()","6e49797e":"# feature exploration: sex\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = countplot(col_object, 'Sex', df_data)","280a0089":"# feature exploration: age\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = swarmplot(col_object, 'Age', df_data)","e3a41290":"# feature extraction: age\ndf_data['Age'] = df_data['Age'].fillna(df_data.groupby(['Title'], as_index=True)['Age'].transform('mean'))","c53ef796":"# feature extraction: family size\ndf_data['FamilySize'] = df_data['SibSp'] + df_data['Parch'] + 1","b1440d84":"# feature extraction: ticket string\ndf_data['TicketString'] = df_data['Ticket'].apply(lambda x: ''.join(re.findall(r'[a-zA-Z]+', x)))\ndf_data['TicketString'] = df_data['TicketString'].replace(['CASOTON', 'SOTONO', 'STONO', 'STONOQ'], 'SOTONOQ')\ndf_data['TicketString'] = df_data['TicketString'].replace(['SC', 'SCParis'], 'SCPARIS')\ndf_data['TicketString'] = df_data['TicketString'].replace('FCC', 'FC')\ndf_data['TicketString'] = df_data['TicketString'].replace(df_data['TicketString'].value_counts()[df_data['TicketString'].value_counts() < 10].index.tolist(), 'OTHER')\ndf_data['TicketString'].value_counts()","a46d3222":"# feature extraction: has ticket string\ndf_data['HasTicketString'] = df_data['TicketString'].apply(lambda x: 1 if x else 0).astype('object')","11441912":"# feature exploration: fare\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = swarmplot(col_object, 'Fare', df_data)","e9e7c0bc":"# feature extraction: fare\ndf_data['Fare'] = df_data['Fare'].fillna(df_data.groupby(['Pclass'], as_index=True)['Fare'].transform('mean'))","0c1155b1":"# feature extraction: cabin\ndf_data['Cabin'] = df_data['Cabin'].fillna(0)","6e247990":"# feature extraction: cabin string\ndf_data['CabinString'] = df_data['Cabin'].str.extract(r'([A-Za-z]+)', expand=False)","e2318c3b":"# feature extraction: has cabin\ndf_data['HasCabin'] = df_data['CabinString'].apply(lambda x: 0 if pd.isnull(x) else 1).astype('object')","0b1ba5a1":"# feature exploration: embarked\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = countplot(col_object, 'Embarked', df_data)","6b37d481":"# feature extraction: embarked\ndf_data['Embarked'] = df_data['Embarked'].fillna(df_data['Embarked'].value_counts().idxmax())","a61e5df2":"# list all features type number\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\nprint('features type number:\\n items %s\\n length %d' %(col_number, len(col_number)))\n\n# list all features type object\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\nprint('features type object:\\n items %s\\n length %d' %(col_object, len(col_object)))","42dc80d4":"# feature exploration: survived\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['PassengerId']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = swarmplot('Survived', col_number, df_data[df_data['DataType'] == 'training'])\n_ = countplot(col_object, 'Survived', df_data[df_data['DataType'] == 'training'])","84e2c174":"# feature exploration: survived where family size equal to 1\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['PassengerId']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = swarmplot('Survived', col_number, df_data[(df_data['DataType'] == 'training') & (df_data['FamilySize'] == 1)])\n_ = countplot(col_object, 'Survived', df_data[(df_data['DataType'] == 'training') & (df_data['FamilySize'] == 1)])","bd6125fb":"# feature exploration: survived where family size more than 1\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['PassengerId']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.drop(['Name', 'Ticket', 'Cabin', 'Surname']).tolist()\n_ = swarmplot('Survived', col_number, df_data[(df_data['DataType'] == 'training') & (df_data['FamilySize'] > 1)])\n_ = countplot(col_object, 'Survived', df_data[(df_data['DataType'] == 'training') & (df_data['FamilySize'] > 1)])","d74fe12d":"# feature extraction: ticket dataframe\ndf_ticket = pd.get_dummies(df_data[df_data['FamilySize'] > 1], columns=['Pclass', 'Sex', 'Embarked', 'DataType', 'Title', 'CabinString', 'HasCabin'], drop_first=False)\ndf_ticket['Survived'] = df_ticket['Survived'].astype(float)\ndf_ticket = df_ticket.groupby(['Ticket'], as_index=False).agg({\n    'Survived': 'mean',\n    'Pclass_1': sum, 'Pclass_2': sum,  'Pclass_3': sum,\n    'Sex_male': sum, 'Sex_female': sum,\n    'Embarked_C': sum, 'Embarked_Q': sum, 'Embarked_S': sum,\n    'DataType_training': sum, 'DataType_testing': sum,\n    'Title_Crew': sum, 'Title_Dr': sum, 'Title_Master': sum, 'Title_Miss': sum, 'Title_Mr': sum, 'Title_Mrs': sum, 'Title_Royal': sum,\n    'CabinString_A': sum, 'CabinString_B': sum, 'CabinString_C': sum, 'CabinString_D': sum, 'CabinString_E': sum, 'CabinString_F': sum, 'CabinString_G': sum,\n    'HasCabin_0': sum, 'HasCabin_1': sum\n})","a6ccb986":"# describe ticket dataframe\ndf_ticket.describe(include='all')","1d202ebf":"# convert dtypes numeric to object\ncol_convert = df_ticket.columns.drop('Ticket').tolist()\ndf_ticket[col_convert] = df_ticket[col_convert].astype('object')","5066573b":"# convert dtypes object to numeric\ncol_convert = ['Survived']\ndf_ticket[col_convert] = df_ticket[col_convert].astype(float)","44cf2e5a":"# feature extraction: together\ndf_ticket['Together'] = df_ticket['Survived'].apply(lambda x: 1 if x == 0 or x == 1 else 0).astype('object')","4f37df34":"# feature exploration: survived\ncol_object = df_ticket.select_dtypes(include=['object']).columns.drop('Ticket').tolist()\n_ = swarmplot(col_object, 'Survived', df_ticket)\n_ = violinplot(col_object, 'Survived', df_ticket)","d556f7ec":"# feature extraction: with sex and title\ndf_data = pd.merge(df_data, df_ticket[['Ticket', 'Sex_male', 'Sex_female', 'Title_Crew', 'Title_Dr', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Royal']], how='left', left_on='Ticket', right_on='Ticket').rename(columns={\n    'Sex_male': 'WithSexMale', 'Sex_female': 'WithSexFemale',\n    'Title_Crew': 'WithTitleCrew', 'Title_Dr': 'WithTitleDr', 'Title_Master': 'WithTitleMaster', 'Title_Miss': 'WithTitleMiss', 'Title_Mr': 'WithTitleMr', 'Title_Mrs': 'WithTitleMrs', 'Title_Royal': 'WithTitleRoyal'\n})\ncol_fillnas = ['WithSexMale', 'WithSexFemale', 'WithTitleCrew', 'WithTitleDr', 'WithTitleMaster', 'WithTitleMiss', 'WithTitleMr', 'WithTitleMrs', 'WithTitleRoyal']\nfor col_fillna in col_fillnas: df_data[col_fillna] = df_data[col_fillna].fillna(0)","317f3902":"# feature extraction: ticket_self dataframe\ndf_temp = df_data.copy(deep=True)\ndf_temp['Survived'] = df_temp['Survived'].astype(float)\ndf_ticket_self = df_temp.groupby(['Ticket'], as_index=True)\n\n# feature extraction: survived peer\ncount = df_ticket_self['Survived'].transform('count')\nmean = df_ticket_self['Survived'].transform('mean')\ndf_data['SurvivedPeer'] = (mean * count - df_data['Survived'].astype(float)) \/ (count - 1)\ndf_data['SurvivedPeer'] = df_data['SurvivedPeer'].astype(float).fillna(-1)","17fda36e":"# feature extraction: ticket_title dataframe\ndf_temp = df_data.copy(deep=True)\ndf_temp['Survived'] = df_temp['Survived'].astype(float)\ncol_revises = ['Crew', 'Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Royal']\nfor col_revise in col_revises:\n    df_temp['Survived' + col_revise] = df_temp['Survived']\n    df_temp.loc[df_temp['Title'] != col_revise, 'Survived' + col_revise] = np.nan\ndf_ticket_title = df_temp.groupby(['Ticket'], as_index=True)\n\n# feature extraction: survived peer title\nfor col_revise in col_revises:\n    count = df_ticket_title['Survived' + col_revise].transform('count')\n    mean = df_ticket_title['Survived' + col_revise].transform('mean')\n    df_data['SurvivedPeer' + col_revise] = (mean * count - df_temp['Survived' + col_revise].astype(float)) \/ (count - 1)\n    df_data['SurvivedPeer' + col_revise] = df_data['SurvivedPeer' + col_revise].astype(float).fillna(-1)","483cd54b":"# feature exploration: survived peer and with sex and title\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['PassengerId']).tolist()\n_ = swarmplot('Survived', col_number, df_data[(df_data['DataType'] == 'training') & (df_data['WithTitleMaster'] >= 1)])\n_ = violinplot('Survived', col_number, df_data[(df_data['DataType'] == 'training') & (df_data['WithTitleMaster'] >= 1)])","e4ffeb33":"# feature extraction: survived\ndf_data['Survived'] = df_data['Survived'].fillna(-1)","048092ff":"# convert category codes for data dataframe\ndf_data = pd.get_dummies(df_data, columns=['Pclass', 'Sex', 'Embarked', 'DataType', 'Title', 'TicketString', 'HasTicketString', 'CabinString', 'HasCabin'], drop_first=True)","d1a9aa35":"# convert dtypes object to numeric for data dataframe\ncol_convert = ['Survived', 'SibSp', 'Parch', 'FamilySize']\ndf_data[col_convert] = df_data[col_convert].astype(int)","f75af15c":"# describe data dataframe\ndf_data.describe(include='all')","9c582f20":"# verify dtypes object\ndf_data.info()","8b5adba0":"# compute pairwise correlation of columns, excluding NA\/null values and present through heat map\ncorr = df_data[df_data['DataType_training'] == 1].corr()\nfig, axes = plt.subplots(figsize=(20, 15))\nheatmap = sns.heatmap(corr, annot=True, cmap=plt.cm.RdBu, fmt='.1f', square=True, vmin=-0.8, vmax=0.8)","f444f77e":"# select all features to evaluate the feature importances\nx = df_data[df_data['DataType_training'] == 1].drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'Surname', 'FamilySize', 'DataType_training', 'SurvivedPeerCrew', 'SurvivedPeerDr', 'SurvivedPeerMaster', 'SurvivedPeerMiss', 'SurvivedPeerMr', 'SurvivedPeerMrs', 'SurvivedPeerRoyal'], axis=1)\ny = df_data.loc[df_data['DataType_training'] == 1, 'Survived']","0eb25d81":"# set up random forest classifier to find the feature importances\nforestclf = RandomForestClassifier(n_estimators=100, random_state=58).fit(x, y)\nfeat = pd.DataFrame(data=forestclf.feature_importances_, index=x.columns, columns=['FeatureImportances']).sort_values(['FeatureImportances'], ascending=False)","c7a93a0b":"# plot the feature importances\nfeat.plot(y='FeatureImportances', figsize=(20, 5), kind='bar', logy=True)\nplt.axhline(0.005, color=\"grey\")","fb8934e0":"# list feature importances\nmodel_feat = feat[feat['FeatureImportances'] > 0.005].index","adeed3bf":"# select the important features\nx = df_data.loc[df_data['DataType_training'] == 1, model_feat]\ny = df_data.loc[df_data['DataType_training'] == 1, 'Survived']","528464dd":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=0.25, random_state=58)","9e10ee0c":"# logistic regression model setup\nmodel_logreg = LogisticRegression(solver='lbfgs', max_iter=1024)\n\n# logistic regression model fit\nmodel_logreg.fit(x_train, y_train)\n\n# logistic regression model prediction\nmodel_logreg_ypredict = model_logreg.predict(x_validate)\n\n# logistic regression model metrics\nmodel_logreg_f1score = f1_score(y_validate, model_logreg_ypredict)\nmodel_logreg_accuracyscore = accuracy_score(y_validate, model_logreg_ypredict)\nmodel_logreg_cvscores = cross_val_score(model_logreg, x, y, cv=20, scoring='accuracy')\nprint('logistic regression\\n  f1 score: %0.4f, accuracy score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_logreg_f1score, model_logreg_accuracyscore, model_logreg_cvscores.mean(), 2 * model_logreg_cvscores.std()))","ee9f147e":"# decision tree classifier model setup\nmodel_treeclf = DecisionTreeClassifier(splitter='best', min_samples_split=5)\n\n# decision tree classifier model fit\nmodel_treeclf.fit(x_train, y_train)\n\n# decision tree classifier model prediction\nmodel_treeclf_ypredict = model_treeclf.predict(x_validate)\n\n# decision tree classifier model metrics\nmodel_treeclf_f1score = f1_score(y_validate, model_treeclf_ypredict)\nmodel_treeclf_accuracyscore = accuracy_score(y_validate, model_treeclf_ypredict)\nmodel_treeclf_cvscores = cross_val_score(model_treeclf, x, y, cv=20, scoring='accuracy')\nprint('decision tree classifier\\n  f1 score: %0.4f, accuracy score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_treeclf_f1score, model_treeclf_accuracyscore, model_treeclf_cvscores.mean(), 2 * model_treeclf_cvscores.std()))","a61c8498":"# random forest classifier model setup\nmodel_forestclf = RandomForestClassifier(n_estimators=100, min_samples_split=5, random_state=58)\n\n# random forest classifier model fit\nmodel_forestclf.fit(x_train, y_train)\n\n# random forest classifier model prediction\nmodel_forestclf_ypredict = model_forestclf.predict(x_validate)\n\n# random forest classifier model metrics\nmodel_forestclf_f1score = f1_score(y_validate, model_forestclf_ypredict)\nmodel_forestclf_accuracyscore = accuracy_score(y_validate, model_forestclf_ypredict)\nmodel_forestclf_cvscores = cross_val_score(model_forestclf, x, y, cv=20, scoring='accuracy')\nprint('random forest classifier\\n  f1 score: %0.4f, accuracy score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestclf_f1score, model_forestclf_accuracyscore, model_forestclf_cvscores.mean(), 2 * model_forestclf_cvscores.std()))","9a49eaf5":"# specify the hyperparameter space\nparams = {'n_estimators': [100],\n          'max_depth': [10, 20, None],\n          'min_samples_split': [3, 5, 7, 9],\n          'random_state': [58],\n}\n\n# random forest classifier grid search model setup\nmodel_forestclf_cv = GridSearchCV(model_forestclf, params, cv=5)\n\n# random forest classifier grid search model fit\nmodel_forestclf_cv.fit(x_train, y_train)\n\n# random forest classifier grid search model prediction\nmodel_forestclf_cv_ypredict = model_forestclf_cv.predict(x_validate)\n\n# random forest classifier grid search model metrics\nmodel_forestclf_cv_f1score = f1_score(y_validate, model_forestclf_cv_ypredict)\nmodel_forestclf_cv_accuracyscore = accuracy_score(y_validate, model_forestclf_cv_ypredict)\nmodel_forestclf_cv_cvscores = cross_val_score(model_forestclf_cv, x, y, cv=20, scoring='accuracy')\nprint('random forest classifier grid search\\n  f1 score: %0.4f, accuracy score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestclf_cv_f1score, model_forestclf_cv_accuracyscore, model_forestclf_cv_cvscores.mean(), 2 * model_forestclf_cv_cvscores.std()))\nprint('  best parameters: %s' %model_forestclf_cv.best_params_)","db44e21b":"# model selection\nfinal_model = model_forestclf\n\n# prepare testing data and compute the observed value\nx_test = df_data.loc[df_data['DataType_training'] == 0, model_feat]\ny_test = pd.DataFrame(final_model.predict(x_test), columns=['Survived'], index=df_data.loc[df_data['DataType_training'] == 0, 'PassengerId'])","38feb473":"# submit the results\nout = pd.DataFrame({'PassengerId': y_test.index, 'Survived': y_test['Survived']})\nout.to_csv('submission.csv', index=False)","6f43b24f":"The exploratory data analysis resulting in,\n* **SurvivedPeer:** The survived peer tend to correlated with the survived status.\n* **SurvivedPeerMaster:** The survived peer tend to correlated with the survived status for the persons who has master title.\n* **SurvivedPeerMiss:** The survived peer tend to correlated with the survived status for the persons who has miss title.\n* **SurvivedPeerMr:** The survived peer tend to correlated with the survived status for the persons who has mr title.","8b1602b9":"The exploratory data analysis resulting in,\n* **Pclass:** The 1st class ticket tend to more survived than 2nd class and 3rd class, respectively.\n* **Sex:** Female tend to more survived than male.\n* **Title:** The master and royal title tend to more survived than other male titles.\n* **FamilySize:** The persons who come with family tend to more survived than single.\n* **CabinString:** The persons assigned cabin A to F except C tend to more survived than cabin C and G, respectively.\n* **HasCabin:** The persons assigned cabin tend to more survived than without assgned cabin.","96e8b207":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome.","be6063c2":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value.\n\nThe pairplots is also useful to observe the distribution of the training data from one feature to the other.\n\nThe pivot table is also another useful method to observe the impact between features.","389bbcef":"> **Problem overview**\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","428ccac8":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution together with exploring some data.","83cd947c":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models.","831ba185":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models.","7b982bf0":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames."}}