{"cell_type":{"3944b101":"code","838bbe43":"code","bad8c7d6":"code","d81b1ebc":"code","59f80d5f":"code","d28203ba":"code","028d687c":"code","e2875c43":"code","c9a15c0c":"code","4ae233ca":"code","a7bd9865":"code","6f1c3342":"code","a7074b4d":"code","a982c875":"code","6df9b51f":"code","c1b92a02":"code","6cf6d3f4":"code","494ef0fa":"code","dc065f2d":"code","697b9f3f":"code","8a669408":"code","a86f462e":"code","691ecb41":"code","55d327c6":"code","e1d5aab5":"code","5bf8123c":"code","79947692":"code","6fe4788c":"code","de3ed2c0":"code","ae9c1ce9":"code","aad05986":"code","d14bed38":"code","8d74a9c7":"code","72ad4fa5":"code","36898e96":"code","619ef9bf":"code","491a2e71":"code","d54d65b5":"code","82abb3a7":"code","a525dda9":"code","aa3a43c2":"code","1597a264":"code","0b4f16e5":"code","d9df0363":"code","453b3127":"code","76b47b8f":"code","95bde26b":"code","6bff9571":"code","35be7b83":"code","ba823f4a":"code","d2b3cf3f":"code","a694a72c":"code","82cca177":"code","3260ff18":"code","19a08ba4":"code","bc497880":"code","80483d1a":"code","fd7dc085":"code","9b85f814":"code","bbec557d":"code","be28db02":"markdown","4401a57f":"markdown","bb11ab3e":"markdown","1884387a":"markdown","95b8281b":"markdown","add87fed":"markdown"},"source":{"3944b101":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport keras\nimport tensorflow as tf\nimport nltk\nimport re\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dropout\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom nltk.stem.porter import PorterStemmer\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","838bbe43":"true=pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake=pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","bad8c7d6":"true.head(10)","d81b1ebc":"fake.head(10)","59f80d5f":"true.shape","d28203ba":"fake.shape","028d687c":"true.isnull().sum()","e2875c43":"fake.isnull().sum()","c9a15c0c":"fake.info()","4ae233ca":"true.info()","a7bd9865":"true['target']=1\nfake['target']=0","6f1c3342":"news_data=pd.concat([true,fake],ignore_index=True,sort=False)","a7074b4d":"news_data.head(5)","a982c875":"news_data.shape","6df9b51f":"plt.figure(figsize=(5,5))\nsns.set(style='darkgrid')\nsns.countplot(news_data.target)\nplt.title('Data distribution of fake and real data')","c1b92a02":"sns.set(style='darkgrid')\nplt.figure(figsize=(15,5))\nplt.title(\"Data distribution of fake and real data\")\nsns.countplot(x='subject',data=news_data,hue='target')\nplt.xlabel('news')\nplt.ylabel('total no count')","6cf6d3f4":"news_data['text']=news_data['title']+''+news_data['text']","494ef0fa":"data=news_data[['text','target']]","dc065f2d":"spacy_tok = spacy.load('en_core_web_sm')\nsample_data=data.text[50]\nsample_data","697b9f3f":"parsed_data=spacy_tok(sample_data)\nparsed_data","8a669408":"!wget https:\/\/raw.githubusercontent.com\/tylerneylon\/explacy\/master\/explacy.py","a86f462e":"import explacy\nexplacy.print_parse_info(spacy_tok, 'Trump urges Congress to pass short-term spending billWASHINGTON (Reuters)')","691ecb41":" explacy.print_parse_info(spacy_tok,data.text[1])","55d327c6":"tokenize=pd.DataFrame()\nfor i,token in enumerate(parsed_data):\n    tokenize.loc[i,'text']=token.text\n    tokenize.loc[i,'lemma']=token.lemma_\n    tokenize.loc[i,'pos']=token.pos_\n    tokenize.loc[i,'dep']=token.dep_\n    tokenize.loc[i, 'is_punctuation'] = token.is_punct\n    tokenize.loc[i, 'shape'] = token.shape_\n    tokenize.loc[i, 'is_alpha'] = token.is_alpha\n    tokenize.loc[i, 'is_stop'] = token.is_stop\ntokenize[:5]","e1d5aab5":"spacy.displacy.render(parsed_data, style='ent')","5bf8123c":"spacy.explain('GPE') # to explain POS tag","79947692":"sentence_spans = list(parsed_data.sents)\nsentence_spans","6fe4788c":"from spacy import displacy\ndisplacy.render(parsed_data, style='dep', jupyter=True,options={'distance': 140})","de3ed2c0":"options = {'compact': True, 'bg': 'violet','distance': 140,\n           'color': 'white', 'font': 'Trebuchet MS'}\ndisplacy.render(parsed_data,jupyter=True, style='dep', options=options)","ae9c1ce9":"data=data[:1000]","aad05986":"## Get the Independent Features\nX=data['text']\ny=data['target']","d14bed38":"X.shape","8d74a9c7":"y.shape","72ad4fa5":"#here we take vocabulry size\nvoc_size=5000","36898e96":"nltk.download('stopwords')","619ef9bf":"X.shape","491a2e71":"ps=PorterStemmer()\ncorpus=[]\nfor i in range(0,len(X)):\n    print(i)\n    news=re.sub('[^a-zA-Z]',' ',data['text'][i])\n    news=news.lower()\n    news=news.split()\n    news=[ps.stem(word) for word in news if not word in stopwords.words('english')]\n    news=' '.join(news)\n    corpus.append(news)","d54d65b5":"corpus","82abb3a7":"onehot_repr=[one_hot(words,voc_size)for words in corpus] \nonehot_repr","a525dda9":"#Embedding the text data\nsent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","aa3a43c2":"embedded_docs[0]","1597a264":"embedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(LSTM(1000))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","0b4f16e5":"len(embedded_docs),y.shape","d9df0363":"import numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","453b3127":"X_final.shape","76b47b8f":"y_final.shape","95bde26b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final,y_final, test_size=0.20, random_state=0)","6bff9571":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)","35be7b83":"y_pred=model.predict_classes(X_test)","ba823f4a":"from sklearn.metrics import confusion_matrix","d2b3cf3f":"confusion_matrix(y_test,y_pred)","a694a72c":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","82cca177":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","3260ff18":"## Creating bidirectional \nembedding_vector_features=40\nmodel1=Sequential()\nmodel1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel1.add(Bidirectional(LSTM(100)))\nmodel1.add(Dropout(0.4))\nmodel1.add(Dense(1,activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","19a08ba4":"model1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)","bc497880":"y_pred1=model1.predict_classes(X_test)","80483d1a":"confusion_matrix(y_test,y_pred1)","fd7dc085":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred1)","9b85f814":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred1)","bbec557d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred1))","be28db02":"# #Here we visualize the data differnent method","4401a57f":"visulization through displacy","bb11ab3e":"After tokenization we can parse tag variety of part of speech to paragraph text.Spacy uses stats models in background to predict which tag will go each word based on the context\nLemmatization:-it is the process of extracting form of the word","1884387a":"Dependency parsing\nSyntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it. As in Subject combined with object makes a sentence. Spacy provides parse tree which can be used to generate this structure.\nSentense Boundry Detection","95b8281b":"# EDA","add87fed":"# Tokenization:-\nTokenization is task splitting text into meaningfull segment called tokens.the input to tokenizer is a unicode text and the output is doc object"}}