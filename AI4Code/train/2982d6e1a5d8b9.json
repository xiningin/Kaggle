{"cell_type":{"6f88a407":"code","953f58df":"code","9ff32877":"code","73c9801a":"code","c208c6c6":"code","603c5c9d":"code","e6895063":"code","63bbf6e3":"code","6b273c12":"code","799a01ab":"markdown","7bdd9ac8":"markdown","ba1de5a3":"markdown","16890d1d":"markdown","c5fdba24":"markdown","fc904d42":"markdown"},"source":{"6f88a407":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","953f58df":"CalendarDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\", header=0)\nSalesDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\", header=0) #June 1st Dataset\nCalendarDF['date'] = pd.to_datetime(CalendarDF.date)\n\nTX_1_Sales = SalesDF[['TX_1' in x for x in SalesDF['store_id'].values]]\nTX_1_Sales = TX_1_Sales.reset_index(drop = True)\n\n# Generate MultiIndex for easier aggregration.\nTX_1_Indexed = pd.DataFrame(TX_1_Sales.groupby(by = ['cat_id','dept_id','item_id']).sum())\n\n# Aggregate total sales per day for each sales category\nFood = pd.DataFrame(TX_1_Indexed.xs('FOODS').sum(axis = 0))\nHobbies = pd.DataFrame(TX_1_Indexed.xs('HOBBIES').sum(axis = 0))\nHousehold = pd.DataFrame(TX_1_Indexed.xs('HOUSEHOLD').sum(axis = 0))\n\n# Merge the aggregated sales data to the calendar dataframe based on date\nCalendarDF = CalendarDF.merge(Food, how = 'left', left_on = 'd', right_on = Food.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Food'})\nCalendarDF = CalendarDF.merge(Hobbies, how = 'left', left_on = 'd', right_on = Hobbies.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Hobbies'})\nCalendarDF = CalendarDF.merge(Household, how = 'left', left_on = 'd', right_on = Household.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Household'})\n\n# Drop dates with null sales data\nCalendarDF = CalendarDF.drop(CalendarDF.index[1941:])\nCalendarDF.reset_index(drop = True)","9ff32877":"# Modify Food data to feed into model\n\nFood.index = CalendarDF.date\nfoodValues = Food.values\n\n# Normalize data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nfoodValues = scaler.fit_transform(foodValues)\n\n# Train and Test datasets\nfoodTrain = foodValues[0:1899]\nfoodTest = foodValues[1899:1941]","73c9801a":"# Create a function to convert array of values into dataset matrix\ndef create_dataset(dataset, look_back = 1):\n    dataX, dataY = [], []\n    for i in range(len(dataset) - look_back - 1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i+look_back, 0])\n    return np.array(dataX), np.array(dataY)","c208c6c6":"# Modify train and test datasets\nlook_back = 1\ntrainX, trainY = create_dataset(foodTrain, look_back)\ntestX, testY = create_dataset(foodTest, look_back)\n\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","603c5c9d":"# Create and fit the LSTM network\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)","e6895063":"# Make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# Invert predictions to scale\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","63bbf6e3":"# Calculate root mean squared error\nimport math\nfrom sklearn.metrics import mean_squared_error\n\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.plot(Food['20160411':'20160522'].values)\nplt.plot(testPredict)\nplt.show()","6b273c12":"# Train and Test datasets\nfoodTrain = Food['20110129':'20160410']\nfoodTest = Food['20160411':'20160522']\n\n# Simple linear smoothing model\nfrom statsmodels.tsa.holtwinters import Holt\n\nmodel = Holt(np.asarray(foodTrain.values))\nmodel.index = pd.to_datetime(foodTrain.index)\n\nfit1 = model.fit(smoothing_level=.3, smoothing_slope=.05)\npred1 = fit1.predict(1899, 1940)\npred1DF = pd.DataFrame(pred1, foodTest.index)\n\n# Estimating the model paramaters by maximizing log values\nfit2 = model.fit(optimized = True)\npred2 = fit2.predict(1899,1940)\npred2DF = pd.DataFrame(pred2, foodTest.index)\n\n# Uses brute force optimizer to search for good starting values\nfit3 = model.fit(use_brute = True)\npred3 = fit3.predict(1899,1940)\npred3DF = pd.DataFrame(pred3,foodTest.index)   \n\nplt.plot(foodTest)\nplt.plot(pred1DF)\nplt.plot(pred2DF)\nplt.plot(pred3DF)\n\nplt.show()","799a01ab":"# Building the Holt Linear Smoothing model","7bdd9ac8":"The Holt-Winters Simple Linear Smoothing does not account for seasonality and mearly predicts linear trend based on training values. The orange line uses the smoothing level of .3 and smoothing slope of .05 to predict the trend (linearly increasing). However, the optimized and brute force parameters automatically create the model parameters and therefore generate a better model of estimating trend. The optimized and brute force parameters returned around the same predicition (shown by the red line which is overlapping another line). This Holt-Winters model does not capture seasonality and predicts that sales will fluctuate around an average near 2100.","ba1de5a3":"# Building the LSTM model","16890d1d":"# Load in Datasets and Prep Datasets","c5fdba24":"# M5 Forecasting Accuracy Research\nThis is a continuation of my work on analyzing the sales data of Walmart's TX_1 store (Version 1 found here:https:\/\/www.kaggle.com\/jimmyliuu\/m5-forecast-accuracy-research-version-1). This week, I used a Long-Short Term Memory network and a simple Holt Linear Smoothing to forecast future food sales values of the store TX_1. \n\nI followed Jason Brownlee's \"Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras\" for the LSTM network. (found here: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/)\n\n* I followed Benjamin Etienne's \"Time Series in Python \u2014 Exponential Smoothing and ARIMA processes\" for Holt Linear Smoothing. (found here: https:\/\/towardsdatascience.com\/time-series-in-python-exponential-smoothing-and-arima-processes-2c67f2a52788)","fc904d42":"The LSTM model is able to accurately capture the trend. However, the predicted values are significantly off from the test values."}}