{"cell_type":{"1e5de400":"code","305447a4":"code","a5c603d5":"code","d4325c79":"code","e2ebd3d1":"code","607f2f97":"code","11d034c1":"code","9093d2f4":"code","abc6b5df":"code","63526ac3":"code","ab917ad6":"code","b8623616":"code","64058fb0":"code","713d97c2":"code","757fbe17":"code","1adc48ec":"code","37691e91":"code","121ed5d0":"code","ac7ed3f0":"code","78daaf5d":"code","6c1b2790":"code","8187f6d7":"code","9a8d3f97":"code","dc8f1b95":"code","6010fb65":"code","ec540a68":"code","162031ca":"code","365d5332":"code","1be0842e":"markdown","e6c5fbda":"markdown"},"source":{"1e5de400":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","305447a4":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import naive_bayes\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV","a5c603d5":"df = pd.read_csv('..\/input\/spam.csv', encoding = \"ISO-8859-1\")\n\ndf.head()","d4325c79":"df.describe()","e2ebd3d1":"df.isnull().sum()","607f2f97":"# Majority of the values in Unnamed: 2, Unnamed: 3 & Unnamed: 4 are null values\n# Dropping the three columns and renaming the columns v1 & v2\n\ndf.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\ndf.rename(columns={\"v1\":\"label\", \"v2\":\"sms\"}, inplace=True)\n\ndf.head()","11d034c1":"df.label.value_counts()","9093d2f4":"# convert label to a numerical variable\ndf.label = pd.Categorical(df.label).codes\n\ndf.head()","abc6b5df":"# Train the classifier if it is spam or ham based on the text\n# TFIDF Vectorizer\nstopset = set(stopwords.words('english'))\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)","63526ac3":"vectorizer.fit(df)","ab917ad6":"y = df.label\n\nX = vectorizer.fit_transform(df.sms)","b8623616":"X","64058fb0":"## Spliting the SMS to separate the text into individual words\nsplt_txt1=df.sms[0].split()\nprint(splt_txt1)","713d97c2":"## Finding the most frequent word appearing in the SMS\nmax(splt_txt1)","757fbe17":"## Count the number of words in the first SMS\nlen(splt_txt1)","1adc48ec":"X[0]","37691e91":"print(X)","121ed5d0":"## Spliting the SMS to separate the text into individual words\nsplt_txt2 = df.sms[1].split()\nprint(splt_txt2)\nprint(max(splt_txt2))","ac7ed3f0":"## The most freaquent word across all the SMSes\nmax(vectorizer.get_feature_names())","78daaf5d":"print (y.shape)\nprint (X.shape)","6c1b2790":"##Split the test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size = 0.2)","8187f6d7":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","9a8d3f97":"## Let us try different models, and check how thye accuracy is for each of the models\n\nclf = naive_bayes.MultinomialNB()\nmodel = clf.fit(X_train, y_train)","dc8f1b95":"prediction = dict()\nprediction['Naive_Bayes'] = model.predict(X_test)\naccuracy_score(y_test, prediction[\"Naive_Bayes\"])","6010fb65":"models = dict()\nmodels['Naive_Bayes'] = naive_bayes.MultinomialNB()\nmodels['SVC'] = SVC()\nmodels['KNC'] = KNeighborsClassifier()\nmodels['RFC'] = RandomForestClassifier()\nmodels['Adaboost'] = AdaBoostClassifier()\nmodels['Bagging'] = BaggingClassifier()\nmodels['ETC'] = ExtraTreesClassifier()\nmodels['GBC'] = GradientBoostingClassifier()","ec540a68":"results = dict()\naccuracies = dict()\n\nfor key, value in models.items():\n    value.fit(X_train, y_train)\n    output = value.predict(X_test)\n    accuracies[key] = accuracy_score(y_test, output)","162031ca":"accuracies","365d5332":"# With the default values, Gradient Boost sems to be performing the best\n# Let's fine tune and make predictions\n\nparamGrid = dict(n_estimators=np.array([50, 100, 200,400,600,800,900]))\n\nmodel = GradientBoostingClassifier(random_state=10)\n\ngrid = GridSearchCV(estimator=model, param_grid=paramGrid)\n\ngrid_result = grid.fit(X_train, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","1be0842e":"It means in the first SMS there are 20 (len(splt_txt1)) words & out of which only 14 elements have been taken, that's why we'll get only 14 tf-idf values for the first the SMS.\n\nLikewise elements or words of all other SMSes are taken into consideration","e6c5fbda":"TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\n\nIDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).\n\ntf-idf score=TF(t)*IDF(t)"}}