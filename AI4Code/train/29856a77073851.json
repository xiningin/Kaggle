{"cell_type":{"5426bee2":"code","0b869e99":"code","ce7617ba":"code","8ba1cf0c":"code","d4372bb4":"code","cbe980e0":"code","fab2082c":"code","7c2edcf4":"code","6e3b993b":"code","79aa9d16":"code","c4f00d5f":"code","4b9033b6":"code","bdbb5289":"code","6e320260":"code","54de92c3":"code","bc50f55c":"code","1db500ed":"code","2ec954ad":"code","2895ecca":"code","240cff5a":"code","d81025a2":"code","ae2204cd":"code","214c2fab":"code","7e562fc2":"markdown","8afbd2a7":"markdown","855fadda":"markdown","28d99a64":"markdown","d972021c":"markdown","2c899e65":"markdown","d0811801":"markdown","f13dcb61":"markdown","6f92b1d2":"markdown","f46f34fa":"markdown","bd7774e5":"markdown","f3bf823f":"markdown","82184812":"markdown","803406e6":"markdown","0bd155a6":"markdown"},"source":{"5426bee2":"import numpy as np #\u6570\u503c\u8ba1\u7b97\u5de5\u5177\nimport pandas as pd #\u6570\u636e\u79d1\u5b66\u8ba1\u7b97\u5de5\u5177\nimport matplotlib.pyplot as plt #\u53ef\u89c6\u5316\nimport seaborn as sns #matplotlib\u7684\u9ad8\u7ea7A\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML, Image\n\ndf = pd.read_csv('..\/input\/diabetes.csv')","0b869e99":"df.head(10) #XX.head()\u662f\u9ed8\u8ba4\u524d5\u884c\uff0cXX.tail()\u662f\u9ed8\u8ba4\u540e5\u884c","ce7617ba":"df.describe() #panda\u7684describe\u63cf\u8ff0\u5c5e\u6027\uff0c\u5c55\u793a\u4e86\u6bcf\u4e00\u4e2a\u5b57\u6bb5\u7684,\u3010count\u6761\u76ee\u7edf\u8ba1\uff0cmean\u5e73\u5747\u503c\uff0cstd\u6807\u51c6\u503c\uff0cmin\u6700\u5c0f\u503c\uff0c25%\uff0c50%\u4e2d\u4f4d\u6570\uff0c75%\uff0cmax\u6700\u5927\u503c\u3011","8ba1cf0c":"f, ax = plt.subplots(1, 2, figsize = (15, 7))\nf.suptitle(\"Diabetes?\", fontsize = 18.)\n_ = df.Outcome.value_counts().plot.bar(ax = ax[0], rot = 0, color = (sns.color_palette()[0], sns.color_palette()[2])).set(xticklabels = [\"No\", \"Yes\"])\n_ = df.Outcome.value_counts().plot.pie(labels = (\"No\", \"Yes\"), autopct = \"%.2f%%\", label = \"\", fontsize = 13., ax = ax[1],\\\ncolors = (sns.color_palette()[0], sns.color_palette()[2]), wedgeprops = {\"linewidth\": 1.5, \"edgecolor\": \"#F7F7F7\"}), ax[1].texts[1].set_color(\"#F7F7F7\"), ax[1].texts[3].set_color(\"#F7F7F7\")","d4372bb4":"fig, ax = plt.subplots(4,2, figsize=(16,16)) #fig, ax = plt.subplots(4,2)\u8868\u793a\u7ed8\u52364*2\u5e45\u56fe\uff0c\u5982\u679c\u662f\uff081\uff0c3\uff09\u8868\u793a\u4f5c\u4e00\u526f\u4e09\u4e2a\u5c0f\u56fe\u6a2a\u7740\u653e\u7684\u56fe\uff0cfigsize\u662f\u6bcf\u4e2a\u5b50\u56fe\u7684\u957f\u5ea6\u548c\u5bbd\u5ea6\nsns.distplot(df.Age, bins = 20, ax=ax[0,0]) #bin\u662f\u7bb1\u5b50\u4e2a\u6570\uff0c\u53c2\u6570ax= \u628a\u56fe\u5f62\u653e\u5728\u54ea\u4e2a\u6846\u91cc\nsns.distplot(df.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(df.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(df.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(df.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(df.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(df.BMI, kde = True,bins = 20, ax=ax[3,1]) #kde = True\u8868\u793a\u9891\u7387\uff0c\u5982\u679ckde = False \u8868\u793a\u7684\u662f\u9891\u7387\uff0c\u4e0d\u518d\u662f\u9891\u6570","cbe980e0":"df.hist(figsize = (16,14)) #\u5bf9\u6bcf\u4e00\u4e2a\u53d8\u91cf\u4f5c\u56fe","fab2082c":"sns.regplot(x='SkinThickness', y= 'Insulin', data=df)  #\u76f8\u5173\u6027\u5206\u6790","7c2edcf4":"sns.pairplot(data=df,hue='Outcome') #pairplot:\u7528\u6765\u5c55\u73b0\u53d8\u91cf\u4e24\u4e24\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7ebf\u6027\u3001\u975e\u7ebf\u6027\u3001\u76f8\u5173\u7b49\u7b49\uff1b#kind:\u7528\u4e8e\u63a7\u5236\u975e\u5bf9\u89d2\u7ebf\u4e0a\u56fe\u7684\u7c7b\u578b\uff0c\u53ef\u9009'scatter'\u4e0e'reg'\n#diag_kind:\u7528\u4e8e\u63a7\u5236\u5bf9\u89d2\u7ebf\u4e0a\u7684\u56fe\u5206\u7c7b\u578b\uff0c\u53ef\u9009'hist'\u4e0e'kde'\uff1bhue\u662f\u8bbe\u7f6e\u6309\u7167\u4ec0\u4e48\u5206\u7ec4\uff0c\u5982\u8fd9\u91cc\u662foutcome\u5206\u7ec4\uff0c\u662f\u5426\u6709\u7cd6\u5c3f\u75c5\u4e24\u79cd\u5206\u7c7b","6e3b993b":"#vars\uff1a\u7814\u7a76\u67d02\u4e2a\u6216\u8005\u591a\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfbvars,\n#x_vars,y_vars\uff1a\u9009\u62e9\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u5b57\u6bb5\uff0c\u4ee5list\u5f62\u5f0f\u4f20\u5165\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cx_vars\u548cy_vars\u8981\u540c\u65f6\u6307\u5b9a\n\nsns.pairplot(data = df,vars=[\"Glucose\",\"Insulin\"])\nsns.pairplot(data = df,x_vars=[\"Glucose\",\"Insulin\"],y_vars=[\"BMI\",\"Age\"]) \n","79aa9d16":"fig,ax = plt.subplots(nrows=4, ncols=2, figsize=(18,18)) #violinplot()\u662f\u753b\u5c0f\u63d0\u7434\u56fe\uff0c\u5c0f\u63d0\u7434\u7684\u4e2d\u70b9\u662fMedian,\u5c0f\u7bb1\u5b50\u662f\u4e0a\u4e0b\u5206\u4f4d\u6570\uff0c\u7a81\u51fa\u90e8\u5206\u662fDensity Plot,Width = Frequency,\u4e2d\u95f4\u7684\u76f4\u7ebf\u662f95% CI\nplt.suptitle('Violin Plots',fontsize=24)\nsns.violinplot(x=\"Pregnancies\", data=df,ax=ax[0,0],palette='Set3')\nsns.violinplot(x=\"Glucose\", data=df,ax=ax[0,1],palette='Set3')\nsns.violinplot (x ='BloodPressure', data=df, ax=ax[1,0], palette='Set3')\nsns.violinplot(x='SkinThickness', data=df, ax=ax[1,1],palette='Set3')\nsns.violinplot(x='Insulin', data=df, ax=ax[2,0], palette='Set3')\nsns.violinplot(x='BMI', data=df, ax=ax[2,1],palette='Set3')\nsns.violinplot(x='DiabetesPedigreeFunction', data=df, ax=ax[3,0],palette='Set3')\nsns.violinplot(x='Age', data=df, ax=ax[3,1],palette='Set3')\nplt.show()\n\n#seaborn.violinplot(x=None, y=None, hue=None, data=None,\n                   order=None, hue_order=None, bw='scott',\n                   cut=2, scale='area', scale_hue=True, gridsize=100, \n                   width=0.8,inner='box', split=False, dodge=True,\n                   orient=None, linewidth=None,color=None, palette=None,\n                   saturation=0.75, ax=None, **kwargs)\n#bw\uff1a{\u2018scott\u2019, \u2018silverman\u2019, float}\n#\u5185\u7f6e\u53d8\u91cf\u503c\u6216\u6d6e\u70b9\u6570\u7684\u6bd4\u4f8b\u56e0\u5b50\u90fd\u7528\u6765\u8ba1\u7b97\u6838\u5bc6\u5ea6\u7684\u5e26\u5bbd\u3002\u5b9e\u9645\u7684\u6838\u5927\u5c0f\u7531\u6bd4\u4f8b\u56e0\u5b50\u4e58\u4ee5\u6bcf\u4e2a\u5206\u7bb1\u5185\u6570\u636e\u7684\u6807\u51c6\u5dee\u786e\u5b9a\u3002\n\n#cut\uff1a{float} \u4ee5\u5e26\u5bbd\u5927\u5c0f\u4e3a\u5355\u4f4d\u7684\u8ddd\u79bb\uff0c\u4ee5\u63a7\u5236\u5c0f\u63d0\u7434\u56fe\u5916\u58f3\u5ef6\u4f38\u8d85\u8fc7\u5185\u90e8\u6781\u7aef\u6570\u636e\u70b9\u7684\u5bc6\u5ea6\u3002\u8bbe\u7f6e\u4e3a 0 \u4ee5\u5c06\u5c0f\u63d0\u7434\u56fe\u8303\u56f4\u9650\u5236\u5728\u89c2\u5bdf\u6570\u636e\u7684\u8303\u56f4\u5185\u3002\uff08\u4f8b\u5982\uff0c\u5728 ggplot \u4e2d\u5177\u6709\u4e0e trim=True \u76f8\u540c\u7684\u6548\u679c\uff09\n\n#scale\uff1a{\u201carea\u201d, \u201ccount\u201d, \u201cwidth\u201d} \u8be5\u65b9\u6cd5\u7528\u4e8e\u7f29\u653e\u6bcf\u5f20\u5c0f\u63d0\u7434\u56fe\u7684\u5bbd\u5ea6\u3002\u82e5\u4e3a area \uff0c\u6bcf\u5f20\u5c0f\u63d0\u7434\u56fe\u5177\u6709\u76f8\u540c\u7684\u9762\u79ef\u3002\u82e5\u4e3a count \uff0c\u5c0f\u63d0\u7434\u7684\u5bbd\u5ea6\u4f1a\u6839\u636e\u5206\u7bb1\u4e2d\u89c2\u5bdf\u70b9\u7684\u6570\u91cf\u8fdb\u884c\u7f29\u653e\u3002\u82e5\u4e3a width \uff0c\u6bcf\u5f20\u5c0f\u63d0\u7434\u56fe\u5177\u6709\u76f8\u540c\u7684\u5bbd\u5ea6\u3002\n\n#scale_hue\uff1a{bool} \u5f53\u4f7f\u7528\u8272\u8c03\u53c2\u6570 hue \u53d8\u91cf\u7ed8\u5236\u5d4c\u5957\u5c0f\u63d0\u7434\u56fe\u65f6\uff0c\u8be5\u53c2\u6570\u51b3\u5b9a\u7f29\u653e\u6bd4\u4f8b\u662f\u5728\u4e3b\u8981\u5206\u7ec4\u53d8\u91cf\uff08scale_hue=True\uff09\u7684\u6bcf\u4e2a\u7ea7\u522b\u5185\u8fd8\u662f\u5728\u56fe\u4e0a\u7684\u6240\u6709\u5c0f\u63d0\u7434\u56fe\uff08scale_hue=False\uff09\u5185\u8ba1\u7b97\u51fa\u6765\u7684\u3002\n\n#gridsize\uff1a{int} \u7528\u4e8e\u8ba1\u7b97\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u79bb\u6563\u7f51\u683c\u4e2d\u7684\u6570\u636e\u70b9\u6570\u76ee\u3002\n\n#width\uff1a{float} \u4e0d\u4f7f\u7528\u8272\u8c03\u5d4c\u5957\u65f6\u7684\u5b8c\u6574\u5143\u7d20\u7684\u5bbd\u5ea6\uff0c\u6216\u4e3b\u8981\u5206\u7ec4\u53d8\u91cf\u7684\u4e00\u4e2a\u7ea7\u522b\u7684\u6240\u6709\u5143\u7d20\u7684\u5bbd\u5ea6\u3002\n\n#inner\uff1a{\u201cbox\u201d, \u201cquartile\u201d, \u201cpoint\u201d, \u201cstick\u201d, None} \u63a7\u5236\u5c0f\u63d0\u7434\u56fe\u5185\u90e8\u6570\u636e\u70b9\u7684\u8868\u793a\u3002\u82e5\u4e3abox\uff0c\u5219\u7ed8\u5236\u4e00\u4e2a\u5fae\u578b\u7bb1\u578b\u56fe\u3002\u82e5\u4e3aquartiles\uff0c\u5219\u663e\u793a\u56db\u5206\u4f4d\u6570\u7ebf\u3002\u82e5\u4e3apoint\u6216stick\uff0c\u5219\u663e\u793a\u5177\u4f53\u6570\u636e\u70b9\u6216\u6570\u636e\u7ebf\u3002\u4f7f\u7528None\u5219\u7ed8\u5236\u4e0d\u52a0\u4fee\u9970\u7684\u5c0f\u63d0\u7434\u56fe\u3002\n\n#split\uff1a{bool} \u5f53\u4f7f\u7528\u5e26\u6709\u4e24\u79cd\u989c\u8272\u7684\u53d8\u91cf\u65f6\uff0c\u5c06split\u8bbe\u7f6e\u4e3a True \u5219\u4f1a\u4e3a\u6bcf\u79cd\u989c\u8272\u7ed8\u5236\u5bf9\u5e94\u534a\u8fb9\u5c0f\u63d0\u7434\u3002\u4ece\u800c\u53ef\u4ee5\u66f4\u5bb9\u6613\u76f4\u63a5\u7684\u6bd4\u8f83\u5206\u5e03\u3002\n","c4f00d5f":"corr=df.corr() #\u8ba1\u7b97\u53d8\u91cf\u7684\u76f8\u5173\u7cfb\u6570\uff0c\u5f97\u5230\u4e00\u4e2aN*N\u7684\u77e9\u9635\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10)) #\u8bbe\u7f6e\u753b\u5e03\u5927\u5c0f\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"black\") #linecolor()\u8bbe\u7f6e\u8fb9\u6846\u7684\u989c\u8272\uff0cheatmap()\u8bbe\u7f6e\u53f3\u8fb9\u5750\u6807\u7684\u8303\u56f4\u548c\u989c\u8272\uff1bsquare()\u8bbe\u7f6e\u683c\u5b50\u662f\u5426\u4e3a\u6b63\u65b9\u5f62\/\u957f\u65b9\u5f62\uff1bannot: \u9ed8\u8ba4\u4e3aFalse\uff0c\u4e3aTrue\u7684\u8bdd\uff0c\u4f1a\u5728\u683c\u5b50\u4e0a\u663e\u793a\u6570\u5b57\uff1bvmax, vmin: \u70ed\u529b\u56fe\u989c\u8272\u53d6\u503c\u7684\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\uff0c\u9ed8\u8ba4\u4f1a\u4ecedata\u4e2d\u63a8\u5bfc\nplt.title('Correlation between features');","4b9033b6":"\n# \u5bfc\u5165\u548c\u7279\u5f81\u9009\u62e9\u76f8\u5173\u7684\u5305\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# SelectKBest() \u53ea\u4fdd\u7559K\u4e2a\u6700\u9ad8\u5206\u7684\u7279\u5f81\n# SelectPercentile() \u53ea\u4fdd\u7559\u7528\u6237\u6307\u5b9a\u767e\u5206\u6bd4\u7684\u6700\u9ad8\u5f97\u5206\u7684\u7279\u5f81\n# \u4f7f\u7528\u5e38\u89c1\u7684\u5355\u53d8\u91cf\u7edf\u8ba1\u68c0\u9a8c\uff1a\u5047\u6b63\u7387SelectFpr\uff0c\u9519\u8bef\u53d1\u73b0\u7387SelectFdr\uff0c\u6216\u8005\u603b\u4f53\u9519\u8bef\u7387SelectFwe\n# GenericUnivariateSelect\u901a\u8fc7\u7ed3\u6784\u5316\u7b56\u7565\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u641c\u7d22\u4f30\u8ba1\u5668\u8fdb\u884c\u7279\u5f81\u9009\u62e9\n\n# SelectKBest()\u548cSelectPercentile()\u80fd\u591f\u8fd4\u56de\u7279\u5f81\u8bc4\u4ef7\u7684\u5f97\u5206\u548cP\u503c\n#\n# sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, percentile=10)\n# sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)\n\n# \u5176\u4e2d\u7684\u53c2\u6570score_func\u6709\u4ee5\u4e0b\u9009\u9879\uff1a\n\n#\u30101\u3011\u56de\u5f52\uff1af_regression:\u76f8\u5173\u7cfb\u6570\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u53d8\u91cf\u4e0e\u76ee\u6807\u53d8\u91cf\u7684\u76f8\u5173\u7cfb\u6570\uff0c\u7136\u540e\u8ba1\u7b97\u51faF\u503c\u548cP\u503c\n#          mutual_info_regression:\u4e92\u4fe1\u606f\uff0c\u4e92\u4fe1\u606f\u5ea6\u91cfX\u548cY\u5171\u4eab\u7684\u4fe1\u606f\uff1a\n#         \u5b83\u5ea6\u91cf\u77e5\u9053\u8fd9\u4e24\u4e2a\u53d8\u91cf\u5176\u4e2d\u4e00\u4e2a\uff0c\u5bf9\u53e6\u4e00\u4e2a\u4e0d\u786e\u5b9a\u5ea6\u51cf\u5c11\u7684\u7a0b\u5ea6\u3002\n#\u30102\u3011\u5206\u7c7b\uff1achi2\uff1a\u5361\u65b9\u68c0\u9a8c\n#          f_classif:\u65b9\u5dee\u5206\u6790\uff0c\u8ba1\u7b97\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u7684F\u503c\uff08\u7ec4\u95f4\u5747\u65b9\/\u7ec4\u5185\u5747\u65b9\uff09\uff1b\n#          mutual_info_classif:\u4e92\u4fe1\u606f\uff0c\u4e92\u4fe1\u606f\u65b9\u6cd5\u53ef\u4ee5\u6355\u6349\u4efb\u4f55\u4e00\u79cd\u7edf\u8ba1\u4f9d\u8d56\uff0c\u4f46\u662f\u4f5c\u4e3a\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\n#                              \u9700\u8981\u66f4\u591a\u7684\u6837\u672c\u8fdb\u884c\u51c6\u786e\u7684\u4f30\u8ba1\u3002","bdbb5289":"X = df.iloc[:, 0:8]  # \u7279\u5f81\u5217 0-7\u5217\uff0c\u4e0d\u542b\u7b2c8\u5217\nY = df.iloc[:, 8]  # \u76ee\u6807\u5217\u4e3a\u7b2c8\u5217\n\nselect_top_4 = SelectKBest(score_func=chi2, k=4)  # \u901a\u8fc7\u5361\u65b9\u68c0\u9a8c\u9009\u62e94\u4e2a\u5f97\u5206\u6700\u9ad8\u7684\u7279\u5f81\uff0c\u770b\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\u6700\u4f73\u7684\u56db\u4e2a\u7279\u5f81\u662f\u8461\u8404\u7cd6\u3001\u80f0\u5c9b\u7d20\u3001BMI\u548c\u5e74\u9f84\n\nfit = select_top_4.fit(X, Y)  # \u83b7\u53d6\u7279\u5f81\u4fe1\u606f\u548c\u76ee\u6807\u503c\u4fe1\u606f\nfeatures = fit.transform(X)  # \u7279\u5f81\u8f6c\u6362\n\n# \u6784\u9020\u65b0\u7279\u5f81DataFrame\nX_features = pd.DataFrame(data = features, columns=['Glucose','Insulin','BMI','Age'])\n\nX_features.head(10)","6e320260":"# \u5b83\u5c06\u5c5e\u6027\u503c\u66f4\u6539\u4e3a \u5747\u503c\u4e3a0\uff0c\u6807\u51c6\u5dee\u4e3a1 \u7684 \u9ad8\u65af\u5206\u5e03.\n# \u5f53\u7b97\u6cd5\u671f\u671b\u8f93\u5165\u7279\u5f81\u5904\u4e8e\u9ad8\u65af\u5206\u5e03\u65f6\uff0c\u5b83\u975e\u5e38\u6709\u7528\n\nfrom sklearn.preprocessing import StandardScaler\n\n# StandardScaler\n# \u4f5c\u7528\uff1a\u53bb\u5747\u503c\u548c\u65b9\u5dee\u5f52\u4e00\u5316\u3002\u4e14\u662f\u9488\u5bf9\u6bcf\u4e00\u4e2a\u7279\u5f81\u7ef4\u5ea6\u6765\u505a\u7684\uff0c\u800c\u4e0d\u662f\u9488\u5bf9\u6837\u672c\u3002\n#StandardScaler\u5bf9\u6bcf\u5217\u5206\u522b\u6807\u51c6\u5316\uff0c\n# \u56e0\u4e3ashape of data: [n_samples, n_features]\n# \u3010\u6ce8\uff1a\u3011 \u5e76\u4e0d\u662f\u6240\u6709\u7684\u6807\u51c6\u5316\u90fd\u80fd\u7ed9estimator\u5e26\u6765\u597d\u5904\u3002","54de92c3":"rescaledX = StandardScaler().fit_transform(\n    X_features)  # \u901a\u8fc7sklearn\u7684preprocessing\u6570\u636e\u9884\u5904\u7406\u4e2dStandardScaler\u7279\u5f81\u7f29\u653e \u6807\u51c6\u5316\u7279\u5f81\u4fe1\u606f\nX = pd.DataFrame(data=rescaledX, columns=X_features.columns)  # \u6784\u5efa\u65b0\u7279\u5f81DataFrame\nX.head()","bc50f55c":"#\u63d0\u53d6\u6570\u636e\u7684\u65b9\u6cd5\uff1a\n#df.loc\u662f\u5229\u7528index\u7684\u540d\u79f0\uff0c\u6765\u83b7\u53d6\u60f3\u8981\u7684\u884c\/\u5217\uff0c\u5982df.loc[:3]\u5b9e\u63d0\u53d6\u884c\u4e2d\u5e263\u7684\u7d22\u5f15\u53ca\u8fd9\u4e4b\u524d\u7684\u4fe1\u606f\uff0c\u5982\u7b2c\u7b2c\u4e00\u6b21\u51fa\u73b03\u8fd9\u4e2a\u6570\u5b57\u7684\u884c\u662f\u7b2c8\u884c\uff0c\u90a3\u6837\u5c31\u4f1a\u63d0\u53d6\u51fa\u524d8\u884c\u7684\u6570\u636e\n#df.iloc\u5229\u7528index\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u6765\u83b7\u53d6\u60f3\u8981\u7684\u884c\/\u5217\uff0c\u5982df.iloc[:3]\u662f\u63d0\u53d6\u524d3\u884c\u7684\u4fe1\u606f\uff0c\n#df_select = df.iloc[:df.index.get_loc('c') + 1, :4] \u8fd9\u662f\u5bf9\u4e00\u4e2adf\u6570\u636e\u6846\uff0c\u63d0\u53d6c\u884c\u53ca\u5176\u4e4b\u524d\u6240\u6709\u7684\u6570\u636e\uff0c\u540c\u65f6\u5c5e\u4e8e\u524d4\u5217\u7684\u6570\u636e\uff1b\n#\u6ce8\u610f\uff0c\u56e0\u4e3ailoc[num_of_row_start : num_of_row_end, num_of_column_start : num_of_column_end]\u4e0d\u5305\u542bnum_of_end\uff0c\u6240\u4ee5\u9700\u8981 +1\u624d\u80fd\u5305\u542bc\u884c\u3002\n#print(df.loc[['b', 'c']]) \u63d0\u53d6\u7b2cb\u884c\u548c\u7b2cc\u884c\uff0c\u5217\u5305\u542b\u6240\u6709\n#print(df.loc[:, ['y', 8]]) \u63d0\u53d6\u7b2cy\u5217\u548c\u7b2c8\u5217\uff0c\u884c\u5305\u542b\u6240\u6709","1db500ed":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX = df.iloc[:, :-1] #\u63d0\u53d6\u6570\u636e\u7684\u884c\u548c\u5217\uff0c-1\u8868\u793a\u7d22\u5f15\u5230\u6700\u540e\u4e00\u4e2a\u5143\u7d20\ny = df.iloc[:, -1]\n\n# \u5207\u5206\u6570\u636e\u96c6\u4e3a\uff1a\u7279\u5f81\u8bad\u7ec3\u96c6\u3001\u7279\u5f81\u6d4b\u8bd5\u96c6\u3001\u76ee\u6807\u8bad\u7ec3\u96c6\u3001\u76ee\u6807\u6d4b\u8bd5\u96c6\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","2ec954ad":"#Model\nLR = LogisticRegression()\n\n#fiting the model\nLR.fit(X_train, y_train)\n\n#prediction\ny_pred = LR.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", LR.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","2895ecca":"#Model\nDT = DecisionTreeClassifier()\n\n#fiting the model\nDT.fit(X_train, y_train)\n\n#prediction\ny_pred = DT.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", DT.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","240cff5a":"#Model\nmodel = GradientBoostingClassifier()\n\n#fiting the model\nmodel.fit(X_train, y_train)\n\n#prediction\ny_pred = model.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", model.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","d81025a2":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# \u5207\u5206\u6570\u636e\u96c6\u4e3a\uff1a\u7279\u5f81\u8bad\u7ec3\u96c6\u3001\u7279\u5f81\u6d4b\u8bd5\u96c6\u3001\u76ee\u6807\u8bad\u7ec3\u96c6\u3001\u76ee\u6807\u6d4b\u8bd5\u96c6\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X, Y, random_state=2019, test_size=0.2)","ae2204cd":"models = []\nmodels.append((\"LR\", LogisticRegression()))  #\u903b\u8f91\u56de\u5f52\nmodels.append((\"NB\", GaussianNB()))  # \u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\nmodels.append((\"KNN\", KNeighborsClassifier()))  #K\u8fd1\u90bb\u5206\u7c7b\nmodels.append((\"DT\", DecisionTreeClassifier()))  #\u51b3\u7b56\u6811\u5206\u7c7b\nmodels.append((\"SVM\", SVC()))  # \u652f\u6301\u5411\u91cf\u673a\u5206\u7c7b","214c2fab":"import warnings\nwarnings.filterwarnings('ignore')  #\u6d88\u9664\u8b66\u544a\n\nresults = []\nnames = []\nfor name, model in models:\n    kflod = KFold(n_splits=10, random_state=2019)\n    cv_result = cross_val_score(\n        model, X_train, Y_train, cv=kflod, scoring='accuracy')\n    names.append(name)\n    results.append(cv_result)\nprint(results,names)\n\nprint('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\nfor i in range(len(names)):\n    print(names[i], results[i].mean)","7e562fc2":"# \u4e00\u3001\u6570\u636e\u7684\u8bfb\u53d6","8afbd2a7":"# \u56db\u3001Feature Extraction \u7279\u5f81\u63d0\u53d6","855fadda":"# \u4e03\u3001\u673a\u5668\u5b66\u4e60-\u4faf\u5efa\u4e8c\u5206\u7c7b\u7b97\u6cd5\u6a21\u578b","28d99a64":"\n# \u4e09\u3001 Correlation between features","d972021c":"Variables within a dataset can be related for lots of reasons. It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation. \n\nA correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.","2c899e65":"## Logistic Regression\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary.  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","d0811801":"## Violin Plots \n\n\n\nA violin plot is a method of plotting numeric data. It is similar to box plot with a rotated kernel density plot on each side. Violin plots are similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram).  \n\nA violin plot is more informative than a plain box plot. In fact while a box plot only shows summary statistics such as mean\/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot clearly shows the presence of different peaks, their position and relative amplitude. This information could not be represented with a simple box plot which only reports summary statistics. The inner part of a violin plot usually shows the mean (or median) and the interquartile range. \n","f13dcb61":"# \u516d\u3001 Predictive Modeling \u9884\u6d4b\u6a21\u578b","6f92b1d2":"## Dist Plot\n\nDist Plot helps us to flexibly plot a univariate distribution of observations.","f46f34fa":"## Pair Plots\n\nPair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our dataset.","bd7774e5":"If You find this notebook useful, **PLEASE UPVOTE **\n\n\n# PIMA Indians Diabetes\n\n\n## Background\n\n**Diabetes**, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period.  Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger.  If left untreated, diabetes can cause many complications.  Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death.  Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis **dataset** is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Objective\n\nWe will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n## Data\n\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n* **Pregnancies**: Number of times pregnant\n* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure**: Diastolic blood pressure (mm Hg)\n* **SkinThickness**: Triceps skin fold thickness (mm)\n* **Insulin**: 2-Hour serum insulin (mu U\/ml)\n* **BMI**: Body mass index (weight in kg\/(height in m)^2)\n* **DiabetesPedigreeFunction**: Diabetes pedigree function\n* **Age**: Age (years)\n* **Outcome**: Class variable (0 or 1)\n\n\n","f3bf823f":"## Gradient Boosting\n\n\n\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","82184812":"## Decision Tree\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. ","803406e6":"# \u4e94\u3001Standardization \u6807\u51c6\u5316","0bd155a6":"# \u4e8c\u3001Data Visualization\u6570\u636e\u53ef\u89c6\u5316"}}