{"cell_type":{"d7b76d89":"code","c3d7e20d":"code","2e282457":"code","aac485bf":"code","8df2f5af":"code","19f0dea5":"code","f80e5260":"code","d5eb8c7e":"code","51952f67":"code","c1aa010b":"code","a4bf99f5":"code","4e89031a":"code","305d499e":"code","69b9ab03":"code","707abddb":"code","9224d8cd":"code","9139da6b":"code","8b64fb6f":"code","50b243ae":"markdown","17ff6e8b":"markdown","7122bd0b":"markdown","bc1d6663":"markdown","45015970":"markdown","f9ed104f":"markdown","5a84c7f6":"markdown","890c70b7":"markdown","86758265":"markdown","2143db67":"markdown","53463852":"markdown","c8ee486e":"markdown","57f7ae28":"markdown","f19831c2":"markdown","aa162fcb":"markdown","6ef8bb86":"markdown","b95cf194":"markdown"},"source":{"d7b76d89":"#importing necessary packages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#algebra, database managing and visualization packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\n\n#Machine Learning packages\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport eli5\n\n#File direction\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c3d7e20d":"df_original = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf = df_original.copy()","2e282457":"df.head()","aac485bf":"df.info()","8df2f5af":"df.describe()","19f0dea5":"df.drop(df.loc[df['TotalCharges'] == \" \"].index, inplace=True)\ndf.TotalCharges = df.TotalCharges.astype(float)","f80e5260":"fig = plt.figure(figsize=(7,7))\ndf.Churn.value_counts().plot(kind=\"pie\", autopct='%1.1f%%', startangle=90, shadow=True, explode=(0,0.05), colors = [\"mediumaquamarine\", \"darksalmon\"])\nplt.title(\"Customers Churn Proportion\",fontdict = {'fontsize' : 20})\nchurners = df.Churn.value_counts()[1]\nnon_c = df.Churn.value_counts()[0]\nprint(f\"Absolute Value of Churners: {churners}\") \nprint(f\"Abbsolute Values of Non-Churners: {non_c}\")","d5eb8c7e":"numerical = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\"]\ndf[numerical].hist(layout=(1,3),figsize=(20,7), color= \"steelblue\", grid=False)\nplt.suptitle(\"Numerical Variables Distribution\", fontsize=20)","51952f67":"plt.figure(figsize=(5,5))\nsns.heatmap(df[numerical].corr(),vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), annot=True)\nplt.title(\"Numerical Variables Correlation Matrix\", fontsize=\"20\")","c1aa010b":"fig, ax = plt.subplots(1, 3, figsize=(15, 8))\nfor variable, subplot in zip(numerical, ax.flatten()):\n    sns.boxplot(x=df[\"Churn\"], y=df[variable], ax=subplot, palette = \"Set2\").set_title(str(variable))","a4bf99f5":"categorical = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\"]\nfig, ax = plt.subplots(3, 5, figsize=(20, 12))\nfor variable, subplot in zip(categorical, ax.flatten()):\n    sns.countplot(df[variable], ax=subplot, palette = \"Set2\")\n\n\nplt.figure(figsize=(3,3))\nchart = sns.countplot(df.PaymentMethod, palette = \"Set2\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)\nplt.show()","4e89031a":"fig, ax = plt.subplots(3, 5, figsize=(20, 12))\nfor variable, subplot in zip(categorical, ax.flatten()):\n    result = pd.crosstab(df['Churn'], df[variable])\n    result_pct = result.div(result.sum(1), axis=0)\n    ax = result_pct.plot(kind='bar', stacked=True, ax=subplot, color = [\"mediumaquamarine\", \"coral\", \"mediumslateblue\"])\n    for p in ax.patches:\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{height:.0%}', (x + width\/2, y + height\/2), ha='center')\n        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.10), ncol=3, fancybox=True, shadow=True)\n        ax.set_title(str(variable), y=-0.10)\nplt.figure(figsize=(3,3))\nresult = pd.crosstab(df['Churn'], df[\"PaymentMethod\"])\nresult_pct = result.div(result.sum(1), axis=0)\nax1 = result_pct.plot(kind='bar', stacked=True, color = [\"mediumaquamarine\", \"coral\", \"mediumslateblue\", \"orchid\"])\n\nfor p in ax1.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax1.annotate(f'{height:.0%}', (x + width\/2, y + height\/2), ha='center')\nax1.legend(bbox_to_anchor=(1.1, 1.05))\nplt.title(\"Payment Method\")\nplt.show()","305d499e":"#Setting the target variable and features\ny = df[\"Churn\"]\nX = df.drop([\"Churn\", \"customerID\"], axis=1)\n\n#Setting the preprocessor for the numerical variables (Scaling) and categorical variables (One Hot Encoding)\npreprocessor = ColumnTransformer(transformers=[(\"num\",StandardScaler(),numerical),(\"cat\", OneHotEncoder(drop=\"if_binary\"), categorical)])\n\n#Calling the different classifiers to try\nclassifiers = [(\"DT_model\", DecisionTreeClassifier(random_state=42)),\n               (\"RF_model\", RandomForestClassifier(random_state=42,n_jobs=-1)),\n               (\"LR_model\", LogisticRegression(random_state=42,n_jobs=-1)),\n               (\"XGB_model\", XGBClassifier(random_state=42, n_jobs=-1))]\n\n#Setting the Cross-Validation method\ncv = KFold(n_splits=4, shuffle=True, random_state=42)\n\n#\"For\" loop to test every classifier and get the Mean Score of each one\nfor name, classifier in classifiers:\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', classifier)])\n    cv_scores = cross_val_score(pipeline, X, y, cv=cv, n_jobs=-1)\n    mean_score = round(np.mean(cv_scores), 4)\n    print(f\"{name} cross validation accuarcy score: {mean_score}\")","69b9ab03":"#Now that we know Logistic Regression is the best one, we will try to fine tune it with Search Grid\nlogistic = LogisticRegression(random_state=42)\npipe = Pipeline(steps=[('preprocessor', preprocessor),('model', logistic)])\n\n#Parameters to try out\nparam_grid = [\n  {'model__penalty': ['l1'], 'model__solver': [ 'liblinear', 'saga'], \"model__C\": np.logspace(-4, 4, 50)},\n  {'model__penalty': ['l2'], 'model__solver': ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga'], \"model__C\": np.logspace(-4, 4, 50)},\n ]\n\nclf = GridSearchCV(pipe, param_grid)\nclf.fit(X, y)\n\n#Getting the best parameters\nprint('Best Params:', clf.best_estimator_.get_params()['model'])\n\n#Getting the scores, mean score and Standard Deviation of a 4 Cross Validation test\nCV_Result = cross_val_score(clf, X, y, cv=4, n_jobs=-1)\nprint(); print(CV_Result)\nprint(); print(CV_Result.mean())\nprint(); print(CV_Result.std())","707abddb":"#We fit the enhanced model with the parameters we got from the Grid Search\ne_model = LogisticRegression(penalty =\"l1\" , C =0.18420699693267145 , solver ='saga' , random_state=42)\ne_clf = Pipeline(steps=[('preprocessor', preprocessor),('model', e_model)])\ne_clf.fit(X,y)\n\n#We get the names of the columns converted with OneHot Encoder from the Transformer of the Pipeline\nonehot_columns = list(e_clf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names(input_features=categorical))\n\n#We add the names to the numerical columns\nfeatures_list = numerical + onehot_columns\n\n#We get the weights from model with the features list as names and print the dataframe\nfeatures_df = eli5.formatters.as_dataframe.explain_weights_df(\n    e_clf.named_steps['model'],\n    feature_names=features_list)\nfeatures_df.head(10)","9224d8cd":"#Plotting the faeture weights in a horizontal bar plot\nfeatures_df.head(10).plot.barh(x=\"feature\", y=\"weight\", figsize=(10,7))\nplt.title(\"10 Most Important Features\", fontsize=20)","9139da6b":"#We drop the \"TotalCharges\" column from the features variables and numerical variables because we use this list in the pipeline\nX = df.drop(\"TotalCharges\", axis=1)\nnumerical.remove(\"TotalCharges\")\n\nCV_Result = cross_val_score(e_clf, X, y, cv=4, n_jobs=-1)\nprint(); print(CV_Result)\nprint(); print(CV_Result.mean())\nprint(); print(CV_Result.std())","8b64fb6f":"#We fit the new model and repeat the process\ne_clf.fit(X,y)\nonehot_columns = list(e_clf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names(input_features=categorical))\n\nfeatures_list = numerical + onehot_columns \nfeatures_df = eli5.formatters.as_dataframe.explain_weights_df(\n    e_clf.named_steps['model'],\n    feature_names=features_list)\n\nfeatures_df.head(10).plot.barh(x=\"feature\", y=\"weight\", figsize=(10,7))\nplt.title(\"10 Most Important Features\", fontsize=20)","50b243ae":"Reading the file and setting a work copy. Then we will take a peek at the data.","17ff6e8b":"## Numerical Variables","7122bd0b":"![customer%20churn.png](attachment:customer%20churn.png)","bc1d6663":"# Cleaning Data","45015970":"## Target Variable","f9ed104f":"### We can see some quick interesting points here, but we got to be careful to remind that we're seeing proportional values. So we also have to take into account the absolute values afterwards\n\n- The churners are relative bigger users of Fiber Optic as Internet Service. Could this be a reason of changing service?\n- Churners  use less complementary services (such as Online Security, Online BackUp, Device Protection, etc) in comparison to non-Churners.\n- It seems that having a Month-to-Month contract and Paperless Billing could be some predictors of Churn.","5a84c7f6":"## In this project we will try to best predict the possibility of customers churning (that means quitting the service) from a telco (telecommunications) company from a dataframe composed of 7000 customers obtained from www.kaggle.com. First we will clean the database if necessary, visualize the variables for a better understanding, try different machine learning models, tune parameters for the one with the best accuracy score, and analyze the feature importances.","890c70b7":"## Categorical Variables","86758265":"### Without TotalCharges we have virtually the same prediction score. The model compensates by giving more weight to the Fiber Optic feature, the others remains quite the same.","2143db67":"### * We can see that, in fact, the most important feature to predict Churn behavior is having the Fiber Optic. This might be telling us about a low quality of service in this regard.\n\n### * Having a contract from month-to-month also appears to influence the tendecy to churn. So an strategy to try to change this contracts into long-terms would be a good advice.\n\n### * Total charges are much lower in Churners than non-Churners because it's just the Monthly Charges * Months of Tenure, I don't really see it as a good predictor. We can try to fit a model without it and see how it goes.\n","53463852":"The dataset is pretty clean already, I only notice some empty values in \"TotalCharges\", so I'll drop them to transform the column to a \"float\" type.","c8ee486e":"## Machine Learning\n### *To churn or not to churn, that is the question*\n### Predicting binary classification","57f7ae28":"- Its pretty clear that TotalCharges would be correlated to the other two variables, being that is the monthly charges multiplied by the months a customer has been a client","f19831c2":"# Visualization","aa162fcb":"- The Monthly plan doesn't seem to be such a big driver of churn, but we can see that 75% of churners pay between 60 and 100 dollars a month\n- One important mark we can see in this plots that 50% of Churners leave the company before the first year goes by.","6ef8bb86":"## Categorical values in respect to Churners","b95cf194":"## Numerical Variables in respect to Churners"}}