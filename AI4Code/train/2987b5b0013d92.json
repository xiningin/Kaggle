{"cell_type":{"c3fd3add":"code","3a199d97":"code","ca17e1ac":"code","c4a23b08":"code","ef510f8a":"code","13bd1860":"code","7b609684":"code","0f69dbe7":"code","ef17078e":"code","5378ec52":"code","69b40fc1":"code","dc497164":"code","6cc7970d":"code","a8ba27bf":"code","b6822cff":"code","b728122f":"code","526429bf":"code","76e2d992":"code","74955db6":"code","771b0d1c":"code","45985160":"code","43f91373":"code","296d9e6c":"markdown","7d5672bf":"markdown","77931db8":"markdown","2f08852f":"markdown","1bc12dc2":"markdown","20676b52":"markdown","312e6498":"markdown","841e1039":"markdown","3e931c0f":"markdown","9fe228d8":"markdown","96f23630":"markdown","c8cf257b":"markdown","cd321c4e":"markdown","d7b472f1":"markdown","8c9cec63":"markdown","eef9d5ad":"markdown"},"source":{"c3fd3add":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re, string\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nbold = \"\\033[1m\"","3a199d97":"episodes = pd.read_csv(\"..\/input\/south-park-scripts-dataset\/SouthPark_Episodes.csv\")\n\nepisodes.head()","ca17e1ac":"lines = pd.read_csv(\"..\/input\/south-park-scripts-dataset\/SouthPark_Lines.csv\")\n\nlines = lines[lines[\"Line\"].notnull()]\nlines","c4a23b08":"episodes_list = episodes.Title.tolist()\n\nscript = {}\n\nfor episode in episodes_list:\n    \n    temp = lines[lines[\"Title\"] == episode]\n    episode_script = \" \".join(temp.Line)\n    script[episode] = episode_script\n    \nfull_script_df = pd.DataFrame(script.items(), columns = [\"Title\", \"Script\"])\nfull_script_df","ef510f8a":"characters = {}\n\nfor episode in episodes_list:\n    temp = lines[lines[\"Title\"] == episode]\n    episode_characters = \" \".join(temp.Character) \n    characters[episode] = episode_characters\n    \nfull_characters_df = pd.DataFrame(characters.items(), columns = [\"Title\", \"Characters\"])\nfull_characters_df","13bd1860":"cast = lines.Character.value_counts()[lines.Character.value_counts() > 50].index.tolist()\n\ncv = CountVectorizer(lowercase = False)\n\ncharacters = cv.fit_transform(full_characters_df[\"Characters\"])\ncharacters_cv = pd.DataFrame(characters.todense(), columns = cv.get_feature_names())\n\ncharacters_cv.set_index(full_characters_df.Title, inplace = True)\n\ncharacters_cv = characters_cv[[x for x in characters_cv.columns if x in cast]]\n\ncharacters_cv","7b609684":"contractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"can not\",\n\"can't've\": \"can not have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\"wanna\": \"want to\",\n\"gonna\": \"going to\",\n\"gotta\": \"have got to\"\n}","0f69dbe7":"all_stopwords = nlp.Defaults.stop_words\n\ndef tokenizer(text):\n    \n    text = text.replace(\"in'\", \"ing\")\n    text = text.replace(\"m'kay\", \"mkay\")\n    tokens = text.split()\n    tokens = [re.sub(token, contractions[token], token) if token in contractions.keys() else token for token in tokens]\n    tokens = [token.strip(string.punctuation) for token in tokens]    \n    tokens = [token.lower() for token in tokens]    \n    tokens = [token for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if len(token) > 1]\n    tokens = [token for token in tokens if token not in all_stopwords]\n\n#     n = lambda pos: pos[:2].startswith(\"N\")\n#     tokens = [word for (word, pos) in nltk.pos_tag(tokens) if n(pos)] \n    \n    lemmatizer = nltk.wordnet.WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(token, \"v\")  if token != \"butters\" else \"butters\" for token in tokens]\n    lemmas = [lemmatizer.lemmatize(token)  if token != \"butters\" else \"butters\" for token in lemmas]\n    \n    return lemmas","ef17078e":"tfidf_script = TfidfVectorizer(min_df = 3, max_df = 0.8, tokenizer = tokenizer, ngram_range = (1, 3), max_features = 5000,\n#                         binary = True, use_idf = False, norm = None\n                       )\n\ntfidf_script_matrix = tfidf_script.fit_transform(full_script_df[\"Script\"])\ntfidf_script_df = pd.DataFrame(tfidf_script_matrix.todense(), columns = tfidf_script.get_feature_names())\n\ntfidf_script_df.set_index(full_script_df.Title, inplace = True)\n\ntfidf_script_df","5378ec52":"tfidf_description = TfidfVectorizer(min_df = 2, max_df = 0.8, tokenizer = tokenizer, ngram_range = (1, 3), max_features = 5000,\n#                         binary = True, use_idf = False, norm = None\n                       )\n\ntfidf_description_matrix = tfidf_description.fit_transform(episodes[\"Description\"])\ntfidf_description_df = pd.DataFrame(tfidf_description_matrix.todense(), columns = tfidf_description.get_feature_names())\n\ntfidf_description_df.set_index(episodes.Title, inplace = True)\n\ntfidf_description_df","69b40fc1":"scaler = MinMaxScaler()\n\nfor col in characters_cv.columns:\n    characters_cv[col] = scaler.fit_transform(characters_cv[col].values.reshape(-1, 1))","dc497164":"train = pd.concat([characters_cv, tfidf_script_df, tfidf_description_df], axis = 1)\ntrain","6cc7970d":"cosine_sim = cosine_similarity(train) #Recommendations for using characters, lines, and description of episodes\ncosine_sim_chars = cosine_similarity(characters_cv) #Recommendations for using characters number of lines of episodes\ncosine_sim_lines = cosine_similarity(tfidf_script_df) #Recommendations for using script of that episode\ncosine_sim_description = cosine_similarity(tfidf_description_df) #Recommendations for using description of that episode\n\nindices = pd.Series(range(0, len(train.index)), index = train.index).drop_duplicates()","a8ba27bf":"def get_recommendations(title, cosine_sim = cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n    \n    recommendations = pd.DataFrame({\"Episodes\": train.iloc[movie_indices].index.tolist(),\n                                    \"Similarity\": [sim[1] for sim in sim_scores]})\n    return recommendations","b6822cff":"get_recommendations(\"Mr. Hankey, the Christmas Poo\", cosine_sim_chars).head(5)","b728122f":"get_recommendations(\"Mr. Hankey, the Christmas Poo\", cosine_sim_lines).head(5)","526429bf":"get_recommendations(\"Mr. Hankey, the Christmas Poo\", cosine_sim_description).head(5)","76e2d992":"get_recommendations(\"Mr. Hankey, the Christmas Poo\").head(5)","74955db6":"print(bold + \"With using all features: \\n\")\ndisplay(get_recommendations(\"Terrance and Phillip: Behind the Blow\").head(5))\n\nprint(bold + \"With using just scripts: \\n\")\ndisplay(get_recommendations(\"Terrance and Phillip: Behind the Blow\", cosine_sim_lines).head(5))","771b0d1c":"print(bold + \"With using all features: \\n\")\ndisplay(get_recommendations(\"Tweek x Craig\").head(5))\n\nprint(bold + \"With using just scripts: \\n\")\ndisplay(get_recommendations(\"Tweek x Craig\", cosine_sim_lines).head(5))","45985160":"print(bold + \"With using all features: \\n\")\ndisplay(get_recommendations(\"Skank Hunt\").head(5))\n\nprint(bold + \"With using just scripts: \\n\")\ndisplay(get_recommendations(\"Skank Hunt\", cosine_sim_lines).head(5))","43f91373":"get_recommendations(\"Butters' Very Own Episode\")","296d9e6c":"**Recommendations for just using characters that have lines:**","7d5672bf":"# 5) Recommend Episodes, or Find Similar Episodes","77931db8":"# 3) Joing Scripts and Characters","2f08852f":"Generally Butters is main character of that episodes.","1bc12dc2":"For similarity, we don't have to use all characters. Setting a minimum line limit helps us for speeding our calculations and getting better results.","20676b52":"It looks like we are fail. We get irrelevant recommendations.","312e6498":"Nice, all recommendations are good for Mr. Hankey lovers.","841e1039":"**Chef's Chocolate Salty Balls** could be a good recommendation, but others... They would better.","3e931c0f":"# 1) Packages","9fe228d8":"If we don't scale, characters_cv frame will be more important. Other features, scripts and descriptions will not affect so much.","96f23630":"# 4) Vectorizing Characters, Scripts and Descriptions","c8cf257b":"Concatenating all features gives good result. However recommendations with using lines could be better.","cd321c4e":"**Recommendations for using characters, lines, and description of episode:**","d7b472f1":"**Recommendations for using script of that episode:**","8c9cec63":"# 2) Load Data","eef9d5ad":"**Recommendations for using description of that episode:**"}}