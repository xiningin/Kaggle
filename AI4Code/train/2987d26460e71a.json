{"cell_type":{"0b5b1ecc":"code","54c8dbb0":"code","035bdb10":"code","0d395b92":"code","003d2922":"code","92775abc":"code","738f1650":"code","61f468a6":"code","18f61324":"code","743bde51":"code","d319525f":"code","bc2173e5":"code","12d11974":"code","985a032d":"code","035a1aa4":"code","5e5dde0e":"code","b3b43482":"code","f8fd3f68":"code","1ecb9b8a":"markdown","692db8cd":"markdown","e9901d2a":"markdown"},"source":{"0b5b1ecc":"# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Imputer, grid, robust, cross\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom scipy.stats import skew\n\n# Machine Learning\nimport lightgbm as lgb\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npath_test = '..\/input\/test.csv'\n\ntrain_data = pd.read_csv('..\/input\/train.csv' , index_col= 0)\ntest_data = pd.read_csv(path_test  , index_col= 0)\n\ntrain_data = train_data[train_data.GrLivArea < 4500]\n\nlabel = train_data[['SalePrice']]\ntrain_data.drop('SalePrice' , axis = 1 , inplace=True)\ntrain_data.head(3)","54c8dbb0":"# Divide in categorical columns and numerical columns\nnumerical_col = []\ncat_col = []\nfor x in train_data.columns:\n    if train_data[x].dtype == 'object':\n        cat_col.append(x)\n        print(x+': ' + str(len(train_data[x].unique())))\n    else:\n        numerical_col.append(x)\n        \nprint('CAT col \\n', cat_col)\nprint('Numerical col\\n')\nprint(numerical_col)","035bdb10":"# Checking skew\nnumerical = train_data.select_dtypes(exclude='object').copy()\n\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(numerical.iloc[:,i].dropna())\n    plt.xlabel(numerical.columns[i])\n\nplt.tight_layout()\nplt.show()","0d395b92":"# Checking corr and choosing columns to be removed\n\ntransformed_corr = train_data.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(transformed_corr)\n\n# Highly-correlated:\n# GarageCars and GarageArea\n# YearBuilt and GarageYrBlt\n# GrLivArea_log1p and TotRmsAbvGrd\n\ncolumns_highCorr_drop = ['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', '2ndFlrSF', '1stFlrSF']\n\n# Drop Highly_correlated\ntrain_data = train_data.drop(columns_highCorr_drop, axis=1)\ntest_data = test_data.drop(columns_highCorr_drop, axis=1)","003d2922":"print(numerical_col)\nnumerical_col.remove('TotRmsAbvGrd')\nnumerical_col.remove('GarageYrBlt')\nnumerical_col.remove('GarageCars')\nnumerical_col.remove('2ndFlrSF')\nnumerical_col.remove('1stFlrSF')\nprint(cat_col)","92775abc":"# Skew data reduce\nlen_train = train_data.shape[0]\n\nhouses=pd.concat([train_data,test_data], sort=False)\nskew=houses.select_dtypes(include=['int','float']).apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskew_df=pd.DataFrame({'Skew':skew})\nskewed_df=skew_df[(skew_df['Skew']>0.5)|(skew_df['Skew']<-0.5)]\n\ntrain_data=houses[:len_train]\ntest_data=houses[len_train:]\n\n# Reduce skew in target - Log\nlabel = np.log(label)","738f1650":"# Cleaning numerical columns from train_data and test_data\nimputer = Imputer(missing_values='NaN' , strategy='mean' , axis = 0)\nimputer = imputer.fit(train_data[numerical_col])\ntrain_num = imputer.transform(train_data[numerical_col])\n\ntest_num = imputer.transform(test_data[numerical_col])\nprint(train_num.shape)\nprint(test_num.shape)","61f468a6":"# Cleaning categorical columns from train_data and test_data\n\ntrain_cat = train_data[cat_col]\ntest_cat = test_data[cat_col]\n\ndropp = ['MiscFeature' , 'PoolQC' , 'Fence' ,'Alley' ]\ntrain_cat.drop(columns=dropp , axis=1, inplace=True)\ntrain_cat = train_cat.astype('category')\ntest_cat.drop(columns=dropp , axis=1, inplace=True)\ntest_cat = test_cat.astype('category')\n\n# Fill null values with the most frequent attribute\nmost_freq = {}\nfor col in train_cat.columns:\n    p = train_cat[col].mode()[0]\n    train_cat[col].fillna(p, inplace=True)\n    most_freq[col] = p\n\nfor col in train_cat.columns:\n    test_cat[col].fillna(most_freq[col], inplace=True)","18f61324":"# Converting to dataframe\n\ntrain_num = pd.DataFrame(train_num)\ntrain_num.head(2)\ntest_num = pd.DataFrame(test_num)\ntest_num.head(2)","743bde51":"# Encode categoricals values\nfor col in train_cat:\n    train_cat[col] = train_cat[col].cat.codes\nfor col in test_cat:\n    test_cat[col] = test_cat[col].cat.codes\ntrain_cat.head(2)","d319525f":"# Same index for numerical and categorical dataframes\ntrain_num.index = train_cat.index\ntest_num.index = test_cat.index\n\n# Get dummies for categorical columns\ntrain_cat = pd.get_dummies(train_cat)\ntest_cat = pd.get_dummies(test_cat)\n\n# Join the 2 datas into one\ntrain_ = train_num.join(train_cat)\ntest_ = test_num.join(test_cat)\n\n# Scaling the data\nscalar = RobustScaler()\ntrain_ = scalar.fit_transform(train_)\ntest_ = scalar.transform(test_)","bc2173e5":"import lightgbm as lgb\nlightgbm = lgb.LGBMRegressor(objective='regression', \n                                       num_leaves=8,\n                                       learning_rate=0.03, \n                                       n_estimators=4000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\nscores = cross_val_score(lightgbm, train_, label, cv=5).mean()\nscores","12d11974":"from sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha =0.001, random_state=1)\nscores = cross_val_score(lasso_model, train_, label, cv=5).mean()\nscores","985a032d":"from sklearn.linear_model import Ridge\nridge_model = Ridge(alpha=0.002, random_state=5)\nscores = cross_val_score(ridge_model, train_, label, cv=5).mean()\nscores","035a1aa4":"# # XGBRegressor - Hyperparameter Tuning\n\n# xgbRegressor = XGBRegressor()\n# n_estimators = range(70, 100)\n# learning_rate = np.arange(0.4, 0.7, 0.1)\n\n# ## Search grid for optimal parameters\n# param_grid = {\"n_estimators\" : n_estimators}\n\n# model_xgb = GridSearchCV(xgbRegressor, param_grid = param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n\n# model_xgb.fit(train_,label)\n\n# # Best score\n# print(model_xgb.best_score_)\n\n# #best estimator\n# model_xgb.best_estimator_","5e5dde0e":"from xgboost import XGBRegressor\nxgb_model = XGBRegressor(n_estimators=4000, learning_rate=0.05)\nscores = cross_val_score(xgb_model, train_, label, cv=5).mean()\nscores","b3b43482":"# remove log from prediction\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\n# Choose model\nlasso_model.fit(train_, label)\npre = lasso_model.predict(test_)","f8fd3f68":"submission = pd.DataFrame({'Id': pd.read_csv(path_test).Id,\n                       'SalePrice': inv_y(pre)})\n\nsubmission.to_csv('submission.csv', index=False)","1ecb9b8a":"Checking and Visualizing data","692db8cd":"Machine Learning and Hyper-Parameters Tuning (TO BE CONTINUED...)","e9901d2a":"Cleaning and Transforming data"}}