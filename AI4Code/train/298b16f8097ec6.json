{"cell_type":{"769ee6af":"code","2e99c0f9":"code","d2945442":"code","84f13558":"code","4772ac2e":"code","ea2cf3af":"code","fec4ae55":"code","a6e5aa7d":"code","8db81727":"code","1fa553cf":"code","22c3f331":"code","0de64cb0":"code","8d194e66":"code","99ff2d3c":"code","015cc8b0":"code","e901bc3f":"code","47e436ea":"code","3b1bf41a":"code","f2beeb39":"markdown","c6afa0eb":"markdown"},"source":{"769ee6af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2e99c0f9":"#Read the dataset into a dataframe\n# df = pd.read_csv('..\/input\/admt-dataset\/file3.csv')\n# df.head()\ndf = pd.read_csv('..\/input\/newdata\/new_adm_pred.csv')\ndf = df.drop('Serial No.', axis=1)\ndf.head()","d2945442":"#Comprehensive description of data\nprint('Rows     :',df.shape[0])\nprint('Columns  :',df.shape[1])\nprint('\\nFeatures :\\n     :',df.columns.tolist())\nprint('\\nMissing values    :',df.isnull().values.sum())\nprint('\\nUnique values :  \\n',df.nunique())","84f13558":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\n\nfig = sns.distplot(df['GRE Score'], kde=False)\nplt.title(\"Distribution of GRE Scores\")\nplt.show()\n\nfig = sns.distplot(df['TOEFL Score'], kde=False)\nplt.title(\"Distribution of TOEFL Scores\")\nplt.show()\n\nfig = sns.distplot(df['University Rating'], kde=False)\nplt.title(\"Distribution of University Rating\")\nplt.show()\n\nfig = sns.distplot(df['CGPA'], kde=False)\nplt.title(\"Distribution of CGPA\")\nplt.show()\n\nplt.show()\n\nprint(\"The distribution plots prove that this university accepts a diverse range of students\")","4772ac2e":"# Gender Ratio Visualization\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nle = preprocessing.LabelEncoder()\nxyz = le.fit_transform(df['Gender'])\nf=0\nm=0\nfor i in xyz:\n    if (i==0):\n        f=f+1\n    else:\n        m=m+1\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.axis('equal')\nlabels = 'Male', 'Female'\nsizes = [m,f]\ncolors = ['#308CCE', '#E27FD3']\nax.pie(sizes, labels=labels, colors=colors, autopct ='% 1.1f %%', shadow = True)\nplt.title('Gender Ratio')\nplt.show()","ea2cf3af":"#Nationality Diversity\ndf['Nation'].value_counts().plot(kind='bar', color = 'orange', title = 'Nationality Distribution')","fec4ae55":"#Research\ndf['Research'].value_counts().plot(kind='bar', color = 'brown', title = 'Research Distribution')","a6e5aa7d":"#Social Work\ndf['Social Work'].value_counts().plot(kind='bar', color = 'Green', title = 'Social Work Distribution')","8db81727":"#Work Exp\ndf['Work Exp'].value_counts().plot(kind='bar', color = 'black', title = 'Work Exp Distribution')","1fa553cf":"#Avg scores\ndf.rename(columns={'GRE Score':'GRE','Avg Living Expense':'ALE','TOEFL Score':'TOEFL','University Rating':'UnivRating','Chance of Admit ':'Chance'},inplace=True)\nprint('Mean CGPA Score is :',int(df[df['CGPA']<=500].CGPA.mean()))\nprint('Mean GRE Score is :',int(df[df['GRE']<=500].GRE.mean()))\nprint('Mean TOEFL Score is :',int(df[df['TOEFL']<=500].TOEFL.mean()))\nprint('Mean University rating is :',int(df[df['UnivRating']<=500].UnivRating.mean()))\nprint(\"\\nTarget of an aspirant would be get more than the mean scores displayed above.\")\nprint('\\nAverage Living Expense (monthly) of students is : $',df['ALE'].mean())","22c3f331":"#minimum scores\ndf_sort=df.sort_values(by=df.columns[-1],ascending=False)\ndf_sort = df_sort.drop(['Research', 'UnivRating', 'Social Work', 'Work Exp', 'ALE'], axis=1)\ndf_sort.head()\ndf_sort[(df_sort['Chance']>0.90)].mean().reset_index()","0de64cb0":"print(\"For having a 90% Chance to get admission one should have GRE=333.61,TOEFL=116.28,CGPA=9.53 .If you get scores more than this then your chances of admission are very good.\")\ndf.head()","8d194e66":"#Now we dont need the columns Gender, Nation, Avg Living Expense so we drop them\ndf.drop(['Gender', 'Nation', 'ALE'], axis=1, inplace=True)\ndf.head()","99ff2d3c":"#Lets split the dataset with training and testing set and prepare the inputs and outputs\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Chance'], axis=1)\ny = df['Chance']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, shuffle=False)","015cc8b0":"#Lets use a bunch of different algorithms to see which model performs better\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodels = [['DecisionTree :',DecisionTreeRegressor()],\n           ['Linear Regression :', LinearRegression()],\n           ['RandomForest :',RandomForestRegressor()],\n           ['KNeighbours :', KNeighborsRegressor(n_neighbors = 2)],\n           ['SVM :', SVR()],\n           ['AdaBoostClassifier :', AdaBoostRegressor()],\n           ['GradientBoostingClassifier: ', GradientBoostingRegressor()],\n           ['Xgboost: ', XGBRegressor()],\n           ['CatBoost: ', CatBoostRegressor(logging_level='Silent')],\n           ['Lasso: ', Lasso()],\n           ['Ridge: ', Ridge()],\n           ['BayesianRidge: ', BayesianRidge()],\n           ['ElasticNet: ', ElasticNet()],\n           ['HuberRegressor: ', HuberRegressor()]]\n\nprint(\"Results...\")\n\nfor name,model in models:\n    model = model\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(name, (np.sqrt(mean_squared_error(y_test, predictions))))\n    \nprint(\"Something as simple as Linear Regression performs the best in this case, which proves that complicated models doesnt always mean better results.\")","e901bc3f":"#Predict random student's chance\nreg=LinearRegression()\nreg.fit(X_train,y_train)\nprint(\"Enter your scores:\")\nlst = []\nfor i in range(7): \n    ele = input()\n    lst.append(ele)\n# Score=['337','118','4','9.65','0','0','1']\nScore=pd.DataFrame(lst).T\nchance=reg.predict(Score)\nprint(chance[0]*100)","47e436ea":"classifier = RandomForestRegressor()\nclassifier.fit(X,y)\nfeature_names = X.columns\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] = X.columns\nimportance_frame['Importance'] = classifier.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)","3b1bf41a":"# df.head()\nimport matplotlib.pyplot as plt\nplt.barh([1,2,3,4,5,6,7], importance_frame['Importance'], align='center', alpha=0.5, color = 'darkgreen')\nplt.yticks([1,2,3,4,5,6,7], importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()\nprint(\"Clearly, CGPA is the most factor for graduate admissions followed by GRE Score. \\nThe other parameters like Research, Social Work, Work experience have less impact on the chance of admission.\")\n","f2beeb39":"* Students have to check their university ratings on the site : [https:\/\/www.topuniversities.com\/university-rankings\/rankings-by-location\/india\/2020](http:\/\/)\n* For Research and Social Work, 1 = Yes and 0 = No.\n* Work Exp is 1 for work experience greater than 2 years and 0 otherwise.","c6afa0eb":"Graduate Admissions Dataset\n\nThis dataset is created by Mohan S Acharya to estimate chances of graduate admission from an Indian perspective. Our analysis will help us in understand what factors are important in graduate admissions and how these factors are interrelated among themselves. It will also help predict one's chances of admission given the rest of the variables.\n\nLets load the dataset and take a look at it"}}