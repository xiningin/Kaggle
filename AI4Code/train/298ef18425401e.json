{"cell_type":{"8638c0a5":"code","7b483c46":"code","f62bd259":"code","ff2cfbde":"code","d94f626b":"code","e0bb5163":"code","176eb277":"code","8730f0b0":"code","939bd624":"code","01f8e649":"code","6f24a7e8":"code","1b689d48":"code","a6c684c2":"code","3921c9a0":"code","524827d0":"code","b2ce5a6f":"code","4abe4b85":"code","f6f3ecba":"code","7db13b6c":"code","1f6d72f5":"code","571289e4":"code","598b9b9f":"code","9d8171a7":"code","24ba28e0":"code","6b4055d1":"code","3b68e0b2":"markdown","e7bd9868":"markdown","b3eccace":"markdown","86ea66ea":"markdown","3c7aa91a":"markdown","6120da88":"markdown","05342a54":"markdown","4b393b82":"markdown","5a2958d6":"markdown","c154b428":"markdown"},"source":{"8638c0a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7b483c46":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedKFold\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\nfrom keras import backend as K","f62bd259":"debug = False\neda_view = True\n\nnum_rows = 2000 if debug else None\ntrain = pd.read_csv('..\/input\/train.csv', nrows = num_rows)\ntest = pd.read_csv('..\/input\/test.csv', nrows = num_rows)","ff2cfbde":"for df in [train, test]:\n    df['sale_yr'] = pd.to_numeric(df.date.str.slice(0, 4))\n    df['sale_month'] = pd.to_numeric(df.date.str.slice(4, 6))\n    df['sale_day'] = pd.to_numeric(df.date.str.slice(6, 8))\n    df.drop(['date'], axis=1, inplace=True)","d94f626b":"if eda_view == True:\n    sns.distplot(train['price'])\n    plt.show()\n    print(train['price'].skew())\n    print(train['price'].kurt())","e0bb5163":"if eda_view == True:\n    for c in train:\n        sns.kdeplot(train[c])\n        plt.show()","176eb277":"features = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']","8730f0b0":"for df in [train, test]:\n    for c in features:\n        df[c] = np.log1p(df[c])\n\ntrain['price'] = np.log1p(train['price'])","939bd624":"for df in [train, test]:\n\tdf['total_rooms'] = df['bathrooms'] + df['bedrooms']\n\tdf['total_sqft'] = df['sqft_living'] + df['sqft_lot']\n\tdf['total_sqft15'] = df['sqft_living15'] + df['sqft_lot15']\n\tdf['grade_multi_cond'] = df['grade'] * df['condition']\n\t# \uc6a9\uc801\ub960 \uc8fc\uac70 \uacf5\uac04 \/ \ub300\uc9c0 \uba74\uc801\n\tdf['far1'] = df['sqft_living'] \/ df['sqft_lot']\n\tdf['far2'] = df['sqft_living15'] \/ df['sqft_lot15']\n    # \uc6a9\uc801\ub960\uc774 \ub298\uc5b4 \ub0ac\ub294\uac00?\n\tdf['is_far_incre'] = df['far2'] - df['far1']\n\tdf['is_far_incre'] = df['is_far_incre'].apply(lambda x : 0 if x < 0 else 1)\n\t\n    #\uac74\ud3d0\ub960 (\ud1a0\uc9c0 \uba74\uc801 \ub300\ube44 1\uce35\uc774 \ucc28\uc9c0\ud558\ub294 \ube44)\n\tdf['far3'] = (df['sqft_above'] \/ df['floors']) \/ df['sqft_lot']\n\t\n    # \uce35\ub2f9 \ud654\uc7a5\uc2e4 \uac1c\uc218\n\tdf['bath_per_floors'] = df['bathrooms'] \/ df['floors']\n    \n    # \uce35\ub2f9 \ubc29 \uc218\n\tdf['bed_per_floors'] = df['bedrooms'] \/ df['floors']\n    \n    # \uc9c0\uc0c1 \uac70\uc8fc \uacf5\uac04 \uc911 1\uce35\uc774 \ucc28\uc9c0\ud558\ub294 \ube44\n\tdf['1st_living'] = df['sqft_above'] \/ df['floors']\n\tdf['is_renovated'] = df['yr_renovated'] - df['yr_built']\n\tdf['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x < 0 else 1)\n\tdf['age_built'] = df['yr_built'] \/ 2015\n\tdf['age_reno'] = df['yr_renovated'] \/ 2015\n\t# \uc9c0\ud558\uac00 \uc788\ub0d0 \uc5c6\ub0d0?\n\tdf['is_basement'] = df['sqft_basement'].apply(lambda x: 0 if x == 0 else 1)\n    #\uc804\uccb4 \ubc29\uc218 + \uc9c0\ud558\uc2e4\n\tdf['total_rooms'] = df['total_rooms'] + df['is_basement']\n    # \ub2e4\ub77d\uc774 \uc788\ub0d0 \uc5c6\ub0d0?\n\tdf['is_top'] = df['floors'].apply(lambda x: 0 if int(x) == x else 1)\n    # 2015\ub144\uc5d0 \uc8fc\uac70 \uacf5\uac04\uc774 \ucee4\uc84c\ub294\uac00?\n\tdf['is_living_wider'] =  df['sqft_living15'] \/ df['sqft_living']\n\tdf['is_living_wider'] = df['is_living_wider'].apply(lambda x:0 if x < 1 else 1)\n    # 2015\ub144 \ud1a0\uc9c0 \uba74\uc801\uc774 \ucee4\ub154\ub294\uac00?\n\tdf['is_lot_wider'] =  df['sqft_lot15'] \/ df['sqft_lot']\n\tdf['is_lot_wider'] = df['is_lot_wider'].apply(lambda x: 0 if x < 1 else 1)\n\tdf['total_grade'] = df['waterfront'] + df['view'] + df['condition'] + df['grade']\n","01f8e649":"train['per_price_living'] = train['price']\/train['sqft_living']\ntrain['per_price_above'] = train['price']\/train['sqft_above']\ntrain['per_price_lot'] = train['price']\/train['sqft_lot']\n\ntrain['per_price_living_grade'] = train['per_price_living'] * train['grade']\ntrain['per_price_above_grade'] = train['per_price_above'] * train['grade']\ntrain['per_price_lot_grade'] = train['per_price_lot'] \/ train['grade']\n\nzipcode_price_living = train.groupby(['zipcode'])['per_price_living'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_living, how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_living, how='left',on='zipcode')\n\nzipcode_price_above = train.groupby(['zipcode'])['per_price_above'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_above,how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_above,how='left',on='zipcode')\n\nzipcode_price_lot = train.groupby(['zipcode'])['per_price_lot_grade'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_lot,how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_lot,how='left',on='zipcode')\n\nzipcode_price_living_grade = train.groupby(['zipcode'])['per_price_living_grade'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_living_grade, how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_living_grade, how='left',on='zipcode')\n\nzipcode_price_above_grade = train.groupby(['zipcode'])['per_price_above_grade'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_above_grade,how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_above_grade,how='left',on='zipcode')\n\nzipcode_price_lot_grade = train.groupby(['zipcode'])['per_price_lot_grade'].agg({'mean','median'}).reset_index()\ntrain = pd.merge(train,zipcode_price_lot_grade,how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price_lot_grade,how='left',on='zipcode')\n\ntrain.drop(['per_price_living', 'per_price_above', 'per_price_lot', 'per_price_living_grade', 'per_price_above_grade', 'per_price_lot_grade'], axis=1, inplace=True)","6f24a7e8":"if eda_view == True:\n    corrmat = train.corr()\n    top_corr_features = corrmat.index[abs(corrmat['price']) >= 0.45]\n    plt.figure(figsize=(12, 12))\n    hm = sns.heatmap(train[top_corr_features].corr(), annot=True)\n    plt.show()","1b689d48":"all_features = True\nif all_features == True:\n\tx_train = train.drop(['price', 'id'], axis=1)\n\tx_test = test.drop(['id'], axis=1)\n\ty_train = train['price']\n\ty_train = y_train.values.reshape(-1,1)\nelse:\n\tx_train = train[top_corr_features]\n\tx_train = x_train.drop(['price'], axis=1)\n\ty_train = train['price']\n\ty_train = y_train.values.reshape(-1,1)\n\tfeatures = [c for c in top_corr_features if c not in ['price']]\n\tx_test = test[features]","a6c684c2":"x_scaler = StandardScaler().fit(x_train)\ny_scaler = StandardScaler().fit(y_train)\nx_train = x_scaler.transform(x_train)\nx_test = x_scaler.transform(x_test)\ny_train = y_scaler.transform(y_train)","3921c9a0":"x_train=pd.DataFrame(x_train)\nx_test=pd.DataFrame(x_test)\n\nx_train = x_train.values.reshape(-1, x_train.shape[1], 1)\nx_test = x_test.values.reshape(-1, x_train.shape[1], 1)","524827d0":"def coeff_determination(y_true, y_pred):\n    SS_res =  K.sum(K.square( y_true-y_pred ))\n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )","b2ce5a6f":"def make_model2():\n    model = models.Sequential()\n    model.add(layers.Dense(8, activation='relu',\tinput_shape=(x_train.shape[1],1)))\n    model.add(layers.Dense(8, activation='relu'))\n    model.add(layers.Flatten())\n    #model.add(layers.Dropout(0.15))\n    model.add(layers.Dense(1))\n\n    optimizer = optimizers.RMSprop(lr=0.001)\n    model.compile(optimizer= optimizer, loss = 'mse', metrics=['mae','mse', coeff_determination])\n    return model","4abe4b85":"patient = 40\ncallbacks2 = [\n    EarlyStopping(monitor='val_mean_squared_error', patience=patient, mode='min', verbose=0),\n \tReduceLROnPlateau(monitor = 'val_mean_squared_error', factor = 0.5, patience = patient \/ 2, min_lr=0.0001, verbose=0, mode='min'),\n    #ModelCheckpoint(filepath=model_path2, monitor='val_mean_squared_error', verbose=0, save_best_only=True, mode='min'),\n    #TensorBoard('.\/log_dir', histogram_freq=0, write_graph=True, write_images=True)\n    ]","f6f3ecba":"# k-fold cross validation \uac80\uc99d\n# fix random seed for reproducibility\nseed = 2019\nnp.random.seed(seed)","7db13b6c":"kfold = RepeatedKFold(n_splits=4, n_repeats=2, random_state=seed)\ncvscores2 = []\nepochs_num = 600\nbatch_num = 32","1f6d72f5":"i = 1\nfor train, test in kfold.split(x_train, y_train):\n    print('\\n Fold : ', i)\n    i = i + 1\n\n    model2 = make_model2()\n        # Fit the model\n    # \uc5ec\uae30\uc11c \uc0ac\uc6a9\ub418\ub294 x_train[train], y_train[train] \uc740 3\uac1c \ud3f4\ub4dc\uc758 \ub9e4 \ud3f4\ub4dc\uc758 train\uacfc target\uc784\n  \n    model2.fit(\n        x_train[train], y_train[train],\n        validation_data=(x_train[test], y_train[test]),\n        epochs=epochs_num, batch_size=batch_num, \n        verbose=0, \n        callbacks=callbacks2)\n\n    # evaluate the model\n    scores2 = model2.evaluate(x_train[test], y_train[test], verbose=0)\n    print(\"\\nmodel2 val_%s: %.2f%%\" % (model2.metrics_names[2], scores2[2]*100))\n    cvscores2.append(scores2[2] * 100)","571289e4":"print(\"\\nmodel2 cvscore %.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores2), np.std(cvscores2)))\n\nmodel2 = make_model2()","598b9b9f":"patient = 30\ncallbacks3 = [\n    EarlyStopping(monitor='mean_squared_error', patience=patient, mode='min', verbose=0),\n \tReduceLROnPlateau(monitor = 'mean_squared_error', factor = 0.5, patience = patient \/ 2, min_lr=0.0001, verbose=0, mode='min'),\n    #ModelCheckpoint(filepath=model_path3, monitor='mean_squared_error', verbose=0, save_best_only=True, mode='min'),\n    #TensorBoard('.\/log_dir', histogram_freq=0, write_graph=True, write_images=True)\n    ]","9d8171a7":"model2.fit(\n    x_train, y_train,\n    epochs=300, batch_size=batch_num, \n    verbose=0,\n    callbacks=callbacks3 \n    )","24ba28e0":"y_preds2 = model2.predict(x_test)\n\ninv_y_preds2 =  np.expm1(y_scaler.inverse_transform(y_preds2))","6b4055d1":"if debug == False:\n    sub = pd.read_csv('..\/input\/sample_submission.csv')\n    sub['price'] = inv_y_preds2\n    sub.to_csv('.\/keras_model2.csv', index=False)","3b68e0b2":"\uc815\uaddc\ud654","e7bd9868":"Feature\ub4e4 \ucd94\uac00","b3eccace":"NN \ubaa8\ub378\uc744 2D \ud615\ud0dc\ub85c \ub3cc\ub9ac\uae30 \uc704\ud574\uc11c  reshape","86ea66ea":"\uacf5\ubd84\uc0b0 \uac12 \ud655\uc778\ud558\uae30 \uc704\ud574\uc11c \ud568\uc218 \ucd94\uac00","3c7aa91a":"\ucca8\ub3c4 \uc65c\ub3c4 \uce58\uc6b0\uce5c feature\ub4e4","6120da88":"nn \ubaa8\ub378 \uad6c\uc131 \n\ud558\uae30 \uad6c\uc131\uc744 \ucc3e\uae30\uae4c\uc9c0 \uc218\ub9ce\uc740 Parameter tunning\uc744 \ud558\uc600\uc2b5\ub2c8\ub2e4.","05342a54":"groupby.agg \uc774\uc6a9\ud558\uc5ec \ubcc0\uc218 \ucd94\uac00","4b393b82":"\uac80\uc99d\uc744 \uc644\ub8cc\ud558\uc600\uc73c\ub2c8 \uc804\uccb4 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \ud6c8\ub828 \uc2dc\ud0a8 \ud6c4\uc5d0 \uc608\uce21.","5a2958d6":"EDA\uc640 Feature Engineering\uc740 \ub2e4\ub978 \ucee4\ub110\ub4e4\uacfc, \ucf00\ub77c\uc2a4 \ucc3d\uc2dc\uc790\uc5d0\uac8c \ubc30\uc6b0\ub294 \ub525\ub7ec\ub2dd \ucc45\uc744 \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4.","c154b428":"4\ud3f4\ub4dc \uac80\uc99d \uc2dc\uc791\n\n\ud1b5\uc0c1 K-Fold \uac80\uc99d\uc740 \ud68c\uadc0, StratifiedKfold\ub294 \ubd84\ub958\uc5d0 \uc801\uc6a9, \n\ud2b9\ud788\ub098 Target class\uac00 \ubd88\uade0\ud615\uc778 \uacbd\uc6b0\uc5d0 Stratifiedkfold\uac00 \uc720\uc694\ud568\n\ub9c8\ucc2c\uac00\uc9c0\ub85c RepeatedKFold \ub294 \ud68c\uadc0, RepeatedStratifiedKFold\ub294 \ubd84\ub958\nRepeated~ \uc774\uac74 \uc790\uc6d0(GPU \uc131\ub2a5)\uc774 \ub530\ub77c\uc918\uc57c \ud569\ub2c8\ub2e4.\n \n4\uac1c\ub85c \ub370\uc774\ud130\ub97c \ubd84\ud65c\ud558\uace0, shuffle\ud558\uc600\uc73c\uba74 random seed\ub294 2019\ub85c \uace0\uc815.\nrandom seed\ub97c \uace0\uc815\ud558\ub294 \uc774\uc720\ub294 \ud6c8\ub828\ud560\ub54c \ub9c8\ub2e4 \ud6c8\ub828\ub370\uc774\ud130\uac00 \ub2ec\ub77c\uc838 \uacb0\uacfc\uac00 \ub2e4\ub978\uba74 \ud53c\uace4\ud558\ub2c8\uae4c...\n\uc774\uac83\uc744 \ud55c\ud140\uc73c\ub85c 2\ubc88 \ubc18\ubcf5\n\uacb0\uad6d \ucd1d 8\ubc88 \uac80\uc99d \ud569\ub2c8\ub2e4.\n"}}