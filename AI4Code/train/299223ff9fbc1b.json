{"cell_type":{"f092be83":"code","02585e1f":"code","f3a40938":"code","02833a7d":"code","74616dd2":"code","965890cb":"code","d9dd1577":"code","0d05d445":"code","9ea6cb1a":"code","8cd0534b":"code","58eb5f55":"code","13ba807a":"code","1bf6d124":"code","96d68a72":"code","b9073159":"code","2a3327a3":"code","8704cf12":"code","45bfe0ea":"code","2b20195e":"code","2479e206":"code","2f18854b":"code","a92bbd38":"code","0f9a10b6":"code","a62df6fb":"code","a7f992d5":"code","0eaf6055":"code","bc09ad7b":"code","e8279336":"code","2389d3a4":"code","96190bbe":"code","c061f6bd":"code","f59199a3":"code","4913a7b2":"code","bb7fb49c":"code","e97cd42a":"code","3749a11c":"code","543cb216":"code","3dde34ba":"code","e3f4e272":"code","04ba326a":"code","26799d53":"code","edcbbc4f":"code","b568ff94":"markdown","4becb683":"markdown","6e300a5a":"markdown","a01d59e3":"markdown","14331630":"markdown","db238ed5":"markdown","d58d5f5d":"markdown","7bb18bc8":"markdown"},"source":{"f092be83":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n\nimport os\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping","02585e1f":"tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)","f3a40938":"des = pd.read_json('\/kaggle\/input\/cassava-leaf-disease-classification\/label_num_to_disease_map.json',\n                  lines=True)\nlabels = des.T\nlabels.columns = [\"labels_description\"]","02833a7d":"labels","74616dd2":"img_train_path = \"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\"\nimg_test_path = \"\/kaggle\/input\/cassava-leaf-disease-classification\/test_images\"","965890cb":"train = pd.read_csv(\"\/kaggle\/input\/cassava-leaf-disease-classification\/train.csv\")","d9dd1577":"# Load a image\npath = \"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/1235188286.jpg\"\n\nsingle_picture = tf.keras.preprocessing.image.load_img(path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\nsingle_picture","0d05d445":"os.listdir(img_train_path)[0]","9ea6cb1a":"picture_path = img_train_path+\"\/\"+\"1235188286.jpg\"","8cd0534b":"picture_path","58eb5f55":"# imread(picture_path)","13ba807a":"train[train['image_id'] == \"1235188286.jpg\"]","1bf6d124":"plt.title(labels.iloc[2].values[0])\nplt.imshow(imread(picture_path))","96d68a72":"len(os.listdir(img_train_path))","b9073159":"sns.countplot(x=train['label'])","2a3327a3":"# train['label'] = train['label'].astype(str)","8704cf12":"class_3_picture = img_train_path+'\/'+ train[train['label']==3]['image_id'][4]","45bfe0ea":"picture_class_3 = imread(class_3_picture)\nplt.imshow(picture_class_3)","2b20195e":"# Test picture\ntest_picture_path = img_test_path+\"\/\"+ os.listdir(img_test_path)[0]\n\nplt.imshow(imread(test_picture_path))","2479e206":"# dim1 = []\n# dim2 = []\n\n#for image_filename in os.listdir(img_train_path):\n    \n   # img = imread(img_train_path+\"\/\"+ image_filename)\n   # d1, d2, colors = img.shape\n   # dim1.append(d1)\n   # dim2.append(d2)","2f18854b":"#print(f\"Size of all pictures is: ({dim1[0]},{dim2[0]})\")","a92bbd38":"SHAPE = 456\nIMAGE_SHAPE = (456, 456, 3)\nBATCH_SIZE = 15","0f9a10b6":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","a62df6fb":"# help(ImageDataGenerator)","a7f992d5":"# imread(picture_path).max()","0eaf6055":"image_gen = ImageDataGenerator(rotation_range=40,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1,\n                               rescale=1\/255, # we need to normalize the data\n                               shear_range=0.2,\n                               zoom_range=0.2,\n                               horizontal_flip=True,\n                               fill_mode=\"nearest\")","bc09ad7b":"plt.imshow(image_gen.random_transform(picture_class_3))","e8279336":"dict_map = {int(i): label for i, label in enumerate(labels['labels_description'].values)}\ntrain[\"class_name\"] = train['label'].map(dict_map)","2389d3a4":"# Split into train\/test set\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(train, test_size=0.05, random_state=45, stratify=train['class_name'])","96190bbe":"train_set = image_gen.flow_from_dataframe(  \n                                         train,\n                                        directory=img_train_path,\n                                        seed=42,\n                                        x_col='image_id',\n                                        y_col='class_name',\n                                        target_size = (SHAPE, SHAPE),\n                                        class_mode='categorical',\n                                        interpolation='nearest',\n                                        shuffle = True,\n                                        batch_size = BATCH_SIZE,\n                                    )","c061f6bd":"# Do the same for our test set\ndatagen_val = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.efficientnet.preprocess_input,\n)\n\ntest_set = datagen_val.flow_from_dataframe(\n    test,\n    directory=img_train_path,\n    seed=42,\n    x_col='image_id',\n    y_col='class_name',\n    target_size = (SHAPE, SHAPE),\n    class_mode='categorical',\n    interpolation='nearest',\n    batch_size=BATCH_SIZE,    \n)","f59199a3":"early_stop = EarlyStopping(monitor='val_loss',mode='min', patience=2, )","4913a7b2":"IMAGE_SHAPE","bb7fb49c":"def create_model():\n    \n    # Instantiate ann model\n    model = Sequential()\n    \n    # Add Convolution layer first\n    model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=IMAGE_SHAPE, activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    # Add Convolution layer first\n    model.add(Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    # Add Convolution layer first\n    model.add(Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    # Flatten out the model\n    model.add(Flatten())\n    \n    # Add Dense layer now\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    # Add output layer\n    model.add(Dense(5, activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","e97cd42a":"ann_model = create_model()","3749a11c":"ann_model.summary()","543cb216":"train_set.image_shape","3dde34ba":"ann_model.fit(train_set, \n              epochs=10, \n              validation_data=test_set, \n              callbacks=[early_stop])","e3f4e272":"ann_model.save(\"First_CNN_model.h5\")","04ba326a":"# imread(img_test_path+\"\/\"+test_images[0])","26799d53":"final_model = tf.keras.models.load_model(\"First_CNN_model.h5\")","edcbbc4f":"test_images = os.listdir(img_test_path)\n\npredictions = []\n\nfor image in test_images:\n    img = Image.open(img_test_path+\"\/\"+ image)\n    img = img.resize((SHAPE, SHAPE))\n    img = np.expand_dims(img, axis=0)\n    predictions.extend(final_model.predict(img).argmax(axis = 1))\n    \n    \nsub = pd.DataFrame({'image_id': test_images, 'label': predictions})\ndisplay(sub)\nsub.to_csv('submission.csv', index = False)","b568ff94":"### Shape of our pictures","4becb683":"This is my first CNN network, so for some of you it can be too simple, but my aim is to explain every step we need to do to create one.","6e300a5a":"### CSV train","a01d59e3":"It is very important to check if the pictures have the same size as CNN won't be able to work with different sizes.","14331630":"### Manipulating images","db238ed5":"### Creating CNN model","d58d5f5d":"## Submission","7bb18bc8":"If the pictures have different size we have to resize it to e.g. mean of those dimentions. Nice way to visualize it is by seaborn jointplot."}}