{"cell_type":{"0b94a74c":"code","af59f002":"code","24f7c23e":"code","772fe61d":"code","e2d7c0b3":"code","2e385efa":"code","d8a50a82":"code","5ba39501":"code","bbabcf91":"code","941876da":"code","3d2868bb":"code","173ede03":"code","bd663ccc":"code","5c1300bb":"code","4bbda227":"code","d7f3eafa":"code","56e4d0f5":"code","36e4bd8d":"code","51046ed0":"code","4f5454f0":"code","2aea8b0a":"code","b5afebbe":"code","dfd98788":"code","e021e570":"code","85926612":"code","91965b1f":"code","ad96a13c":"code","052e8e83":"code","b75e7c90":"code","d862a6c7":"code","54601faf":"code","7eaf997a":"code","093799f1":"code","48c58b88":"code","0b5beb81":"code","585565cd":"code","886d8e43":"code","55c0455a":"code","2cea75ce":"code","68ad5d96":"code","2efb68ec":"code","eccfe026":"code","086cf378":"code","a733cfe4":"code","bd75237f":"code","aee9df65":"markdown","a3a835a6":"markdown","7bd333c7":"markdown","e3460c5e":"markdown","4f606c6f":"markdown","308cf99a":"markdown","40eda0d2":"markdown","76b27128":"markdown","c1e9550f":"markdown","0bd69ca7":"markdown","bb6bfc50":"markdown","b74718f7":"markdown","9c248e98":"markdown","e927f6be":"markdown","0051356b":"markdown","88dceb62":"markdown","08e78c69":"markdown","a15431bf":"markdown","82cbfa24":"markdown","79c868b8":"markdown","da13c05e":"markdown","c7690daf":"markdown","073c9b76":"markdown","89414bb3":"markdown","f00c06c1":"markdown","6f910b26":"markdown"},"source":{"0b94a74c":"import os","af59f002":"# import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display, HTML, display_html\nimport seaborn as sns\nimport datetime\n\n# set formatting\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n# read in CSV file data\ndf = pd.read_csv('..\/input\/reviews.csv')\n\nprint(\"Setup Complete\")","24f7c23e":"# look at data\ndisplay(df.head())","772fe61d":"# shape of data\ndisplay(df.shape)","e2d7c0b3":"# look at data types\ndisplay(df.dtypes)","2e385efa":"# see if there are any null values\ndisplay(df.isnull().any())","d8a50a82":"# display descriptive statistics\ndisplay(df.describe(percentiles = [0.25, 0.5, 0.75, 0.85, 0.95, 0.99]))","5ba39501":"# Rename columns\ndf = df.rename(columns = {'date': 'ds', 'listing_id': 'ts'})\n\n# Group data\ndf_group = df.groupby(by = 'ds').agg({'ts':'count'})\n\n# change index to datetime\ndf_group.index = pd.to_datetime(df_group.index)\n\n# Set frequncy of time series\ndf_group = df_group.asfreq(freq = '1D')\n\n# Sort the values\ndf_group = df_group.sort_index(ascending = True)\n\n# Fill NA values with zero\ndf_group = df_group.fillna(value = 0)\n\n# Show the end of th data\ndisplay(df_group.tail())","bbabcf91":"# Plot time series data\nf, ax = plt.subplots(1,1)\nax.plot(df_group['ts'])\n\n# Add title\nax.set_title('Time-series Graph')\n\n# Rotate x-labels\nax.tick_params(axis= 'x', rotation = 45)\n\n# show graph\nplt.show()\nplt.close()","941876da":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(df, ts):\n    \"\"\"\n    Test Stationarity using moving average statistics and Dickey-Fuller Test\n    \"\"\"\n    \n    # Determining rolling statistics\n    rolmean = df[ts].rolling(window = 12, center = False).mean()\n    rolstd = df[ts].rolling(window = 12, center = False).std()\n    \n    # Plot rolling statistics:\n    orig = plt.plot(df[ts],\n                   color = 'blue',\n                   label = 'Original')\n    mean = plt.plot(rolmean,\n                   color = 'red',\n                   label = 'Rolling Mean')\n    std = plt.plot(rolstd,\n                  color = 'black',\n                  label = 'Rolling Std')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n    \n    print('Dickey-Fuller Test:')\n    print('Null Hypothesis (H_0): time series is not stationary')\n    print('ALternative Hypothesis (H_1): time series is stationary')\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(df[ts],\n                     autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4],\n                        index = ['Test Statistics',\n                                'p-value',\n                                '# Lags Used',\n                                'Number of observation Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n        \n    if (dftest[1] <= 0.05):\n        print('Reject the null hypothesis (H0), the data does not have a unit root and is stationary.')\n    else:\n        print('Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.')\n    \n    print(dfoutput)","3d2868bb":"test_stationarity(df = df_group, ts = 'ts')","173ede03":"def plot_transformed_data(df, ts, ts_transform):\n    \"\"\"\n    Plot transform and original time series data\n    \"\"\"\n    # Plot time series data\n    f, ax = plt.subplots(1,1)\n    ax.plot(df[ts])\n    ax.plot(df[ts_transform], color = 'red')\n    \n    # Add title\n    ax.set_title('%s and %s time-series graph' %(ts, ts_transform))\n    \n    # Rotate x-labels\n    ax.tick_params(axis = 'x', rotation = 45)\n    \n    # Add legend\n    ax.legend([ts, ts_transform])\n    \n    plt.show()\n    plt.close()\n    \n    return","bd663ccc":"# Transformation - log ts\ndf_group['ts_log'] = df_group['ts'].apply(lambda x: np.log(x))","5c1300bb":"# Transformation - 7-day moving average of log ts\ndf_group['ts_log_moving_avg'] = df_group['ts_log'].rolling(window = 7,\n                                                              center = False).mean()","4bbda227":"# Transformation - 7-day moving average of ts\ndf_group['ts_moving_avg'] = df_group['ts'].rolling(window = 7, \n                                                  center = False).mean()","d7f3eafa":"# Transformation - Difference betwen logged ts and first-order difference logged ts\n# df_group['ts_log_diff'] = df_group['ts_log'] - df_group['ts_log'].shift()\ndf_group['ts_log_diff'] = df_group['ts_log'].diff()","56e4d0f5":"# Transformation - Differencing between ts and moving average ts\ndf_group['ts_moving_avg_diff'] = df_group['ts'] - df_group['ts_moving_avg']","36e4bd8d":"# Transformation - Difference between logged ts and logged moving average ts\ndf_group['ts_log_moving_avg_diff'] = df_group['ts_log'] - df_group['ts_log_moving_avg']","51046ed0":"df_group_transform = df_group.dropna()","4f5454f0":"# Transformation - Logged exponentailly weighted moving averages (EWMA) ts\ndf_group_transform['ts_log_ewma'] = df_group_transform['ts_log'].ewm(halflife = 7,\n                                                                    ignore_na = False,\n                                                                    min_periods = 0,\n                                                                    adjust = True).mean()","2aea8b0a":"# Transformation - Difference between logged ts and logged EWMA ts\ndf_group_transform['ts_log_ewma_diff'] = df_group_transform['ts_log'] - df_group_transform['ts_log_ewma']","b5afebbe":"display(df_group_transform.head())","dfd98788":"# Plot Data\nplot_transformed_data(df = df_group,\n                     ts = 'ts',\n                     ts_transform = 'ts_log')\n\nplot_transformed_data(df = df_group,\n                     ts = 'ts_log',\n                     ts_transform = 'ts_log_moving_avg')\n\nplot_transformed_data(df = df_group_transform,\n                     ts = 'ts',\n                     ts_transform = 'ts_moving_avg')\n\nplot_transformed_data(df = df_group_transform,\n                     ts = 'ts_log',\n                     ts_transform = 'ts_log_diff')\n\nplot_transformed_data(df = df_group_transform,\n                   ts = 'ts',\n                   ts_transform = 'ts_moving_avg_diff')\n\nplot_transformed_data(df = df_group_transform,\n                   ts = 'ts_log',\n                   ts_transform = 'ts_log_moving_avg_diff')\n\nplot_transformed_data(df = df_group_transform,\n                   ts = 'ts_log',\n                   ts_transform = 'ts_log_ewma')\n\nplot_transformed_data(df = df_group_transform,\n                   ts = 'ts_log',\n                   ts_transform = 'ts_log_ewma_diff')\n","e021e570":"# Perform Stationarity Test\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_moving_avg')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log_moving_avg')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log_diff')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_moving_avg_diff')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log_moving_avg_diff')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log_ewma')\ntest_stationarity(df = df_group_transform,\n                 ts = 'ts_log_ewma_diff')","85926612":"def plot_decomposition(df, ts, trend, seasonal, residual):\n    \"\"\"\n    Plot Time seris data\n    \"\"\"\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15, 5), sharex = True)\n    \n    ax1.plot(df[ts], label = 'Original')\n    ax1.legend(loc = 'best')\n    ax1.tick_params(axis = 'x', rotation = 45)\n    \n    ax2.plot(df[trend], label = 'Trend')\n    ax2.legend(loc = 'best')\n    ax2.tick_params(axis = 'x', rotation = 45)\n    \n    ax3.plot(df[seasonal], label = 'Seasonality')\n    ax3.legend(loc = 'best')\n    ax3.tick_params(axis = 'x', rotation = 45)\n    \n    ax4.plot(df[residual], label = 'Residuals')\n    ax4.legend(loc = 'best')\n    ax4.tick_params(axis = 'x', rotation = 45)\n    \n    # Show Graph\n    plt.suptitle('Trend, Seasonal, and Residual Dcomposition of %s' %(ts),\n                x = 0.5,\n                y = 1.05,\n                fontsize = 20)\n    plt.show()\n    plt.close()\n    \n    return","91965b1f":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(df_group_transform['ts_log'], period = 365)\n\ndf_group_transform.loc[:, 'trend'] = decomposition.trend\ndf_group_transform.loc[:, 'seasonal'] = decomposition.seasonal\ndf_group_transform.loc[:, 'residual'] = decomposition.resid\n\nplot_decomposition(df = df_group_transform,\n                  ts = 'ts_log',\n                  trend = 'trend',\n                  seasonal = 'seasonal',\n                  residual = 'residual')\n\ntest_stationarity(df = df_group_transform.dropna(), ts = 'residual')","ad96a13c":"def plot_acf_pacf(df,ts):\n    \"\"\"\n    Plot auto-correlation (ACF) and partial auto-crrelation (PACF) plots\n    \"\"\"\n    f, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5))\n    \n    # Plot ACF:\n    \n    ax1.plot(lag_acf)\n    ax1.axhline(y = 0, linestyle = '--', color = 'gray')\n    ax1.axhline(y = -1.96\/np.sqrt(len(df[ts])), linestyle = '--', color = 'gray')\n    ax1.axhline(y = 1.96\/np.sqrt(len(df[ts])), linestyle = '--', color = 'gray')\n    ax1.set_title('Autocorrelation Function for %s' %(ts))\n    \n    # Plot PACF:\n    \n    ax2.plot(lag_pacf)\n    ax2.axhline(y = 0, linestyle = '--', color = 'gray')\n    ax2.axhline(y = -1.96\/np.sqrt(len(df[ts])), linestyle = '--', color = 'gray')\n    ax2.axhline(y = 1.96\/np.sqrt(len(df[ts])), linestyle = '--', color = 'gray')\n    ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n    \n    plt.tight_layout()\n    plt.show()\n    plt.close()\n    \n    return","052e8e83":"# ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\n\n# determine ACF and PACF\nlag_acf = acf(np.array(df_group_transform['ts_log_diff']), nlags = 20, fft = False)\nlag_pacf = pacf(np.array(df_group_transform['ts_log_diff']), nlags = 20)\n\n# plot ACF and PACF\nplot_acf_pacf(df = df_group_transform, ts = 'ts_log_diff')","b75e7c90":"def run_arima_model(df, ts, p, d, q):\n    \"\"\"\n    Run ARIMA model\n    \"\"\"\n    from statsmodels.tsa.arima_model import ARIMA\n    \n    # fit ARIMA model on time series\n    model = ARIMA(df[ts], order = (p, d, q))\n    results = model.fit(disp = -1)\n    \n    # get lenghts correct to calculate RSS\n    len_results = len(results.fittedvalues)\n    ts_modified = df[ts][-len_results:]\n    \n    # calculate root mean square error (RMSE) and residual sun of Squares (RSS)\n    rss = sum((results.fittedvalues - ts_modified)**2)\n    rmse = np.sqrt(rss \/ len(df[ts]))\n    \n    # Plot fit \n    plt.plot(df[ts])\n    plt.plot(results.fittedvalues, color = 'red')\n    plt.title('ARIMA model (%i, %i, %i) for ts %s, RSS:  %.4f, RMSE: %.4f' %(p, d, q, ts, rss, rmse))\n    \n    plt.show()\n    plt.close()\n    \n    return results","d862a6c7":"# Note: We have already done differencing in the transformation of data 'ts_log_diff'\n# AR model with 1st order differencing - ARIMA (1, 0, 0)\nmodel_AR = run_arima_model(df = df_group_transform,\n                          ts = 'ts_log_diff',\n                          p = 1,\n                          d = 0,\n                          q = 0)\n\n# MA model with 1st order differencing - ARIMA (0, 0, 1)\nmodel_MA = run_arima_model(df = df_group_transform,\n                          ts = 'ts_log_diff',\n                          p = 0,\n                          d = 0,\n                          q = 1)\n\n# ARIMA model with 1st order differencing - ARIMA (1, 0, 1)\nmodel_ARIMA = run_arima_model(df = df_group_transform,\n                          ts = 'ts_log_diff',\n                          p = 1,\n                          d = 0,\n                          q = 1)","54601faf":"from fbprophet import Prophet\nimport datetime\nfrom datetime import datetime","7eaf997a":"def days_between(d1, d2):\n    \"\"\"Calculate the number of days between two dates.  D1 is start date (inclusive) and d2 is end date (inclusive)\"\"\"\n    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n    return abs((d2 - d1).days + 1)","093799f1":"df_group","48c58b88":"# Inputs for query\n\ndate_column = 'dt'\nmetric_column = 'ts'\ntable = df_group\nstart_training_date = '2010-07-03'\nend_training_date = '2020-05-08'\nstart_forecasting_date = '2020-05-09'\nend_forecasting_date = '2020-12-31'\nyear_to_estimate = '2020'\n\n# Inputs for forecasting\n\n# future_num_points\n# If doing different time intervals, change future_num_points\nfuture_num_points = days_between(start_forecasting_date, end_forecasting_date)\n\ncap = None # 2e6\n\n# growth: default = 'linear'\n# Can also choose 'logistic'\ngrowth = 'linear'\n\n# n_changepoints: default = 25, uniformly placed in first 80% of time series\nn_changepoints = 25 \n\n# changepoint_prior_scale: default = 0.05\n# Increasing it will make the trend more flexible\nchangepoint_prior_scale = 0.05 \n\n# changpoints: example = ['2016-01-01']\nchangepoints = None \n\n# holidays_prior_scale: default = 10\n# If you find that the holidays are overfitting, you can adjust their prior scale to smooth them\nholidays_prior_scale = 10 \n\n# interval_width: default = 0.8\ninterval_width = 0.8 \n\n# mcmc_samples: default = 0\n# By default Prophet will only return uncertainty in the trend and observation noise.\n# To get uncertainty in seasonality, you must do full Bayesian sampling. \n# Replaces typical MAP estimation with MCMC sampling, and takes MUCH LONGER - e.g., 10 minutes instead of 10 seconds.\n# If you do full sampling, then you will see the uncertainty in seasonal components when you plot:\nmcmc_samples = 0\n\n# holiday: default = None\n# thanksgiving = pd.DataFrame({\n#   'holiday': 'thanksgiving',\n#   'ds': pd.to_datetime(['2014-11-27', '2015-11-26',\n#                         '2016-11-24', '2017-11-23']),\n#   'lower_window': 0,\n#   'upper_window': 4,\n# })\n# christmas = pd.DataFrame({\n#   'holiday': 'christmas',\n#   'ds': pd.to_datetime(['2014-12-25', '2015-12-25', \n#                         '2016-12-25','2017-12-25']),\n#   'lower_window': -1,\n#   'upper_window': 0,\n# })\n# holidays = pd.concat((thanksgiving,christmas))\nholidays = None\n\ndaily_seasonality = True","0b5beb81":"df_group_transform","585565cd":"df_prophet = df_group_transform[['ts']]","886d8e43":"df_prophet","55c0455a":"# get relevant data - note: could also try this with ts_log_diff\ndf_prophet = df_group_transform[['ts']] # can try with ts_log_diff\n\n# reset index\ndf_prophet = df_prophet.reset_index()\n\n# rename columns\ndf_prophet = df_prophet.rename(columns = {'ds': 'ds', 'ts': 'y'}) # can try with ts_log_diff\n\n# Change 'ds' type from datetime to date (necessary for FB Prophet)\ndf_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n\n# Change 'y' type to numeric (necessary for FB Prophet)\ndf_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n\n# Remove any outliers\n# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None","2cea75ce":"df_prophet","68ad5d96":"def create_daily_forecast(df,\n#                           cap,\n                          holidays,\n                          growth,\n                          n_changepoints = 25,\n                          changepoint_prior_scale = 0.05,\n                          changepoints = None,\n                          holidays_prior_scale = 10,\n                          interval_width = 0.8,\n                          mcmc_samples = 1,\n                          future_num_points = 10, \n                          daily_seasonality = True):\n  \"\"\"\n  Create forecast\n  \"\"\"\n  \n  # Create copy of dataframe\n  df_ = df.copy()\n\n  # Add in growth parameter, which can change over time\n  #     df_['cap'] = max(df_['y']) if cap is None else cap\n\n  # Create model object and fit to dataframe\n  m = Prophet(growth = growth,\n              n_changepoints = n_changepoints,\n              changepoint_prior_scale = changepoint_prior_scale,\n              changepoints = changepoints,\n              holidays = holidays,\n              holidays_prior_scale = holidays_prior_scale,\n              interval_width = interval_width,\n              mcmc_samples = mcmc_samples, \n              daily_seasonality = daily_seasonality)\n\n  # Fit model with dataframe\n  m.fit(df_)\n\n  # Create dataframe for predictions\n  future = m.make_future_dataframe(periods = future_num_points)\n  #     future['cap'] = max(df_['y']) if cap is None else cap\n\n  # Create predictions\n  fcst = m.predict(future)\n\n  # Plot\n  m.plot(fcst);\n  m.plot_components(fcst)\n\n  return fcst","2efb68ec":"fcst = create_daily_forecast(df_prophet,\n#                              cap,\n                             holidays,\n                             growth,\n                             n_changepoints,\n                             changepoint_prior_scale,\n                             changepoints, \n                             holidays_prior_scale,\n                             interval_width,\n                             mcmc_samples,\n                             future_num_points, \n                             daily_seasonality)","eccfe026":"def calculate_mape(y_true, y_pred):\n    \"\"\" Calculate mean absolute percentage error (MAPE)\"\"\"\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef calculate_mpe(y_true, y_pred):\n    \"\"\" Calculate mean percentage error (MPE)\"\"\"\n    return np.mean((y_true - y_pred) \/ y_true) * 100\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\" Calculate mean absolute error (MAE)\"\"\"\n    return np.mean(np.abs(y_true - y_pred)) * 100\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\" Calculate root mean square error (RMSE)\"\"\"\n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\ndef print_error_metrics(y_true, y_pred):\n    print('MAPE: %f'%calculate_mape(y_true, y_pred))\n    print('MPE: %f'%calculate_mpe(y_true, y_pred))\n    print('MAE: %f'%calculate_mae(y_true, y_pred))\n    print('RMSE: %f'%calculate_rmse(y_true, y_pred))\n    return","086cf378":"print_error_metrics(y_true = df_prophet['y'], y_pred = fcst['yhat'])","a733cfe4":"\ndef do_lstm_model(df, \n                  ts, \n                  look_back, \n                  epochs, \n                  type_ = None, \n                  train_fraction = 0.67):\n  \"\"\"\n   Create LSTM model\n  \"\"\"\n  # Import packages\n  import numpy\n  import matplotlib.pyplot as plt\n  from pandas import read_csv\n  import math\n  from keras.models import Sequential\n  from keras.layers import Dense\n  from keras.layers import LSTM\n  from sklearn.preprocessing import MinMaxScaler\n  from sklearn.metrics import mean_squared_error\n\n  # Convert an array of values into a dataset matrix\n  def create_dataset(dataset, look_back=1):\n    \"\"\"\n    Create the dataset\n    \"\"\"\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n      a = dataset[i:(i+look_back), 0]\n      dataX.append(a)\n      dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n  # Fix random seed for reproducibility\n  numpy.random.seed(7)\n\n  # Get dataset\n  dataset = df[ts].values\n  dataset = dataset.astype('float32')\n\n  # Normalize the dataset\n  scaler = MinMaxScaler(feature_range=(0, 1))\n  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n  \n  # Split into train and test sets\n  train_size = int(len(dataset) * train_fraction)\n  test_size = len(dataset) - train_size\n  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n  \n  # Reshape into X=t and Y=t+1\n  look_back = look_back\n  trainX, trainY = create_dataset(train, look_back)\n  testX, testY = create_dataset(test, look_back)\n  \n  # Reshape input to be [samples, time steps, features]\n  if type_ == 'regression with time steps':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  elif type_ == 'stacked with memory between batches':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  else:\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n  \n  # Create and fit the LSTM network\n  batch_size = 1\n  model = Sequential()\n  \n  if type_ == 'regression with time steps':\n    model.add(LSTM(4, input_shape=(look_back, 1)))\n  elif type_ == 'memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  elif type_ == 'stacked with memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  else:\n    model.add(LSTM(4, input_shape=(1, look_back)))\n  \n  model.add(Dense(1))\n  model.compile(loss='mean_squared_error', optimizer='adam')\n\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    for i in range(100):\n      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n      model.reset_states()\n  else:\n    model.fit(trainX, \n              trainY, \n              epochs = epochs, \n              batch_size = 1, \n              verbose = 2)\n  \n  # Make predictions\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    trainPredict = model.predict(trainX, batch_size=batch_size)\n    testPredict = model.predict(testX, batch_size=batch_size)\n  else:\n    trainPredict = model.predict(trainX)\n    testPredict = model.predict(testX)\n  \n  # Invert predictions\n  trainPredict = scaler.inverse_transform(trainPredict)\n  trainY = scaler.inverse_transform([trainY])\n  testPredict = scaler.inverse_transform(testPredict)\n  testY = scaler.inverse_transform([testY])\n  \n  # Calculate root mean squared error\n  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n  print('Train Score: %.2f RMSE' % (trainScore))\n  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n  print('Test Score: %.2f RMSE' % (testScore))\n  \n  # Shift train predictions for plotting\n  trainPredictPlot = numpy.empty_like(dataset)\n  trainPredictPlot[:, :] = numpy.nan\n  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n  \n  # Shift test predictions for plotting\n  testPredictPlot = numpy.empty_like(dataset)\n  testPredictPlot[:, :] = numpy.nan\n  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n  \n  # Plot baseline and predictions\n  plt.plot(scaler.inverse_transform(dataset))\n  plt.plot(trainPredictPlot)\n  plt.plot(testPredictPlot)\n  plt.show()\n  plt.close()\n  \n  return","bd75237f":"# LSTM Network for Regression\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 1, \n              epochs = 5)\n\n# LSTM for Regression Using the Window Method\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 3, \n              epochs = 5)\n\n# LSTM for Regression with Time Steps\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 3, \n              epochs = 5, \n              type_ = 'regression with time steps')","aee9df65":"#### Looking at our data:\n* Applying log transformation, weekly moving average smmothing, and differencing made the data more stationary over time.\n* Using **Dickey-Fuller test**, as p=<0.05, we reject the null hypothesis i.e. the time-series is not stationary at the p = 0.05 level, thus we can say that the **time-series is stationary**","a3a835a6":"# 3. Model Building","7bd333c7":"### 3.1.1 ACF and PACF Plots\n\n**How do we determine p, d, and q?** For this, we can use ACF and PACF plots<br>\n\n**Autocorrelation Function (ACF),** correlation between the time series with a lagged version of itself(Ex: Correlation of Y(t) with Y(t-1)).<br>\n\n**Partial Autocorrelation Function (PACF),** Additional correlation explained by each successive lagged terms.\n\n**How to interpret ACF and PACF plots?**\n* p - Lag value where the PACF chart crossess the upper confidence interval for the first time.\n* q - Lag value where the ACF chart crossess the upper confidence interval for the first time.","e3460c5e":"# 1. Time-series data due diligence","4f606c6f":"### 2.5.1 Transformation, Smoothing and Differencing","308cf99a":"### 2.5.2 Decomposition: trend, seasonality, residuals\n\nDecomposition is a statistical task in which the Time Series data is decomposed into several component or extracting seasonality, trend from a series data. These components are defined as follows:\n* **Level:** The average value in the series.\n* **Trend:** The increasing or decreasing value in the series.\n* **Seasonality:** The repeating short-term cycle in the series.\n* **Noise:** The random variation in the series.","40eda0d2":"## 1.2 Looking at the data\n<a id=Data><\/a>\n\n* How many rows are in the dataset?\n* How many columns are in the dataset?\n* Data types of the columns?\n* finding weather the data is complete? Are there nulls?\n* Descriptive Statistics\n* Anything else?","76b27128":"# 2. Let's Jump into Time-series Analysis","c1e9550f":"## 3.1 ARIMA Model\n\nWe can use Arima model when we know there is dependence betn values and we can leverage that information to forecast.<br>\n\n**ARIMA = Auto-Rgression Moving Average**<br>\n**Assumptions =** The time-series is stationary<br>\n**ARIMA depends on following 3 terms:**<br>\n1. **Number of AR (Auto-Regressive) terms (p).**\n2. **Number of I (Integrated or Difference) terms (d).**\n3. **Number of MA (Moving Average) terms (q).**","0bd69ca7":"## 2.1 Data processing","bb6bfc50":"***Here we can see in the data we have Listing Ids and the date on which reviews is been posted in them. We are going to group these by no of reviews on a particular day, changing the index to datetime and setting up its frequency which is the first dtep in time-series analysis and then sort them in ascending order***","b74718f7":"# Time Series Analysis using Statistical modeling and Machine learning ","9c248e98":"## 1.3 What are questions that we can answer with this data?\n\nUnderstanding the limitiations of your data and what types of different question we can answer from a dataset is important. This can help in defining the scope of the project <br>\n\nIf your defined scope for the project is less, more, or somewhat different that can be answered from the dataset, this is the right time to reimagine your scope or get some other data <br>\n\n[Data](#Data): Here the data represent daily count of reviews of listing ids for given dates <br>\n\nQuestions we can try to find anwers to :<br>\n1. Forcast future number of reviews for Los Angeles Area.<br>\n2. Forcast the future numbr of reviews for specific listings in the Los Angeles area. <br>","e927f6be":"## 2.2 Ploting the data","0051356b":"## 1.1 Import Packages","88dceb62":"## 2.5 Eliminating Trend and Seasonality\n* **Transformation**\n    * ***Examples:*** log, square root, etc.\n* **Smoothing**\n    * ***Examples:*** Weekly average, monthly average, rolling averages, etc.\n* **Diffrencing**\n    * ***Examples:*** First-order differencing.\n* **Ploynomial Fitting**\n    * ***Examples:*** Fit a regression model.\n* **Decomposition**","08e78c69":"### Get data from http:\/\/insideairbnb.com\/get-the-data.html `reviews.csv` under \"Los Angeles, California, United States\" section","a15431bf":"* There does appear to be an overall increasing ternd.\n* There appears to be some differences in the variance over time.\n* There may be some seasonality in the data.\n* Not sure about the outliers.","82cbfa24":"## 1.4 Types of Techniques that can be used to answer these Questions\n\n### 1.4.1 Statistical Models:\n* **Ignoring the time-series aspect completely and model using traditional statistical models.**\n    * <i>Examples: Regression based models<\/i>\n* **Univariate statistical time-series modeling.**\n    * <i>Examples: Averaging and smoothing models, ARIMA models<\/i>\n* **Slight modifications to univariate statistical time-series modeling.**\n    * <i>Examples: External Regressors, multi-variate models.<\/i>\n* **Additive or component models.**\n    * <i>Examples: Facebook Prophet Model.<\/i>\n* **Structural time series modeling.**\n    * <i>Examples: Bayesian structural time series modeling, hierarchical time series modeling.<\/i>\n\n### 1.4.2 Machine Learning Models:\n* **Ignore the time-series aspect completely and model using traditional machine learning modeling.**\n    * <i>Examples: Support Vector Machines(SVMs), Random Forest Regression, Gradient-Boosted Decision Trees (GBDTs).<\/i>\n* **Hidden markov models(HMMs)**\n* **Other sequence-based models**\n* **Gaussian Processes(GPs)**\n* **Recurrent neural Network(RNNs)**\n\n### 1.4.3 Additional consideration taken into account before choosing a model\n* Weather or not to incorporate external data\n* Weather or not to keep as univariate or multivariate\n* Outlier detection and handling\n* Missing values imputation|","79c868b8":"We will be doing an example here! Installing the necessary packages might take a couple of minutes.Kaggle already provide prophet package with there notebooks. Talking about Facebook Prophet, a tool that allows folks to forecast using additive or component models relatively easily. It can also include things like:\n\n* Day of week effects\n* Day of year effects\n* Holiday effects\n* Trend trajectory\n* Can do MCMC sampling","da13c05e":"<p> \n    Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.<\/p>\n    \n<p>\n    A powerful type of neural network designed to handle sequence dependence is called recurrent neural networks. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.<\/p>\n    \nAlso, here are some resources on recurrent neural networks (RNN) and Long Short-Term Memory networks (LSTMs):\n* [Link 1](https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/)\n* [Link 2](https:\/\/blog.statsbot.co\/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f)\n* [Link 3](http:\/\/adventuresinmachinelearning.com\/recurrent-neural-networks-lstm-tutorial-tensorflow\/)","c7690daf":"**Looking at the results of our data:**\n* We can see rolling mean and Standard deviation are changing over time\n* There may be some de-trending and removing seasonality involved.\n* Based on **Dicky-Fuller Test**, because p = 0.4 , we fail to reject the null hypothsis (that th time-series is not stationary) at the p = 0.05 level, thus consluding that we fail to reject the null hypothsis that our **time series is not stationary**","073c9b76":"## 3.2 Facebook Prophet","89414bb3":"## 2.3 Looking at Stationarity\n\nMost of the time-series model that we use assume the stationarity of time-series. This assumption gives us some nice statistical properties thereby allowing us to use various models for forcasting.<br>\n\n**Stationary Time-Series** have following characterstics:<br>\n* Constant mean\n* Constant variance\n* Autocovariance doesn't depend on time\n\nTo put it in laymen terms, if we want to predict future using the past data, w should assume that the data will follow the same trends and patterns as in the past. This general statement holds for most training data and modeling tasks.\n\n**Here are some links to know more about stationarity: [here](https:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/complete-tutorial-time-series-modeling\/) and [here](https:\/\/people.duke.edu\/~rnau\/411diff.htm)**\n\nSometimes in order to make time-series stationary we need to transform the data. However, this transformation then calls into questions if this data is truly stationary and is suited to be modeled using these techniques.","f00c06c1":"## LSTM Model","6f910b26":"## 2.4 Handling Stationarity\n\nIt is common for a time-series to have Non stationary behaviour.<br>\nMost common reason behind non- stationary time-series are:\n1. **Trend** - mean is not constant ovr time .<br>\n2. **Seasonality** - variance is not constant over time<br>\n\nThere are ways to correct for trend and seasonality in porder to make times-series stationary.\n\n**What happen if you don't corrct these things**<br>\nMany things can happen:\n* Variance can be mis-specified<br>\n* Model fit can be worse<br>\n* Not leveraging valuable time-dependent nature of data.<br>\n\nHere are some link to show the downfall of using traditional methods for time series analysis:\n* [Link](https:\/\/www.quora.com\/Why-cant-you-use-linear-regression-for-time-series-data)\n* [Link](https:\/\/www.quora.com\/Data-Science-Can-machine-learning-be-used-for-time-series-analysis)\n"}}