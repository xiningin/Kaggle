{"cell_type":{"88258fc6":"code","763291d5":"code","bf1adcb9":"code","834a6309":"code","84a80a91":"code","83c790e0":"code","59974762":"code","999b1867":"code","dafe217d":"code","0cdcb284":"code","e6429432":"code","fe815f96":"code","4a3d0b60":"code","d259fff4":"code","ba32d8fb":"code","5beb5b9e":"code","a86d57ad":"code","7d46781a":"code","bff15497":"code","1b9b53a5":"code","636810e2":"code","0b93efd7":"code","2ea83040":"code","0a0aef50":"code","25445ac1":"code","f9909f99":"code","27579822":"code","0a571e00":"code","9ffbda30":"code","98297cea":"code","d731fe2e":"code","b34112d8":"code","c7d4a9ae":"code","1c2e014f":"code","9c803d9a":"code","12d134b3":"code","fd37ada6":"code","fa2fa89f":"code","3b7be039":"code","60e6543e":"code","ad9d9756":"code","0f9c73d6":"code","f933b636":"code","27ff0974":"markdown","9c28b82a":"markdown","b0cb9681":"markdown","8d65599c":"markdown","94c77812":"markdown"},"source":{"88258fc6":"import datetime as dt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport math","763291d5":"import matplotlib.pyplot as plt\nfrom matplotlib import pyplot","bf1adcb9":"from statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error","834a6309":"# Any results you write to the current directory are saved as output.\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nab\/realTraffic\/realTraffic\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84a80a91":"dirname0 = \"\/kaggle\/input\/nab\/realTraffic\/realTraffic\/\"\nfilename0 = \"TravelTime_387.csv\"\ndataframe = pd.read_csv(dirname0+filename0)#, usecols=[1])#, skipfooter=3)\ndataframe.head()","83c790e0":"def remap(x, in_min, in_max, out_min, out_max):\n    return (x - in_min) * (out_max - out_min) \/ (in_max - in_min) + out_min\n\ndataframe['prop_value'] = remap( dataframe.value.values, np.min (dataframe.value), np.max( dataframe.value), 0.0, 100.0  )\ndataframe.head()","59974762":"!pip install pyod","999b1867":"from pyod.models.ocsvm import OCSVM   #PyOD is a comprehensive and scalable Python toolkit for detecting outlier objects\n\nrandom_state = np.random.RandomState(42)     # A fixed values is assigned, then no matter how many time you execute your code,values generated would be the same\n#Does this mean that later on the code the outliers 5% higher than maximum value of dataset?\noutliers_fraction = 0.05\nclassifiers = {\n        'One Classify SVM (SVM)':OCSVM(kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, contamination=outliers_fraction)\n}","dafe217d":"X = dataframe['value'].values.reshape(-1,1)","0cdcb284":"from scipy import stats\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(X)\n    # predict raw anomaly score\n    scores_pred = clf.decision_function(X) * -1        \n    # prediction of a datapoint category outlier or inlier\n    y_pred = clf.predict(X)\n    n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n    n_outliers = np.count_nonzero(y_pred == 1)\n    \n    # copy of dataframe\n    dfx = dataframe[['value','prop_value']]\n    dfx['outlier'] = y_pred.tolist()\n    IX1 =  np.array(dfx['value'][dfx['outlier'] == 0]).reshape(-1,1)\n    OX1 =  dfx['value'][dfx['outlier'] == 1].values.reshape(-1,1)         \n    print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)        \n    # threshold value to consider a datapoint inlier or outlier\n    threshold = stats.scoreatpercentile(scores_pred,100 * outliers_fraction)\ny = dfx['outlier'].values.reshape(-1,1)","e6429432":"tOut = stats.scoreatpercentile(dfx[dfx['outlier'] == 1]['value'], np.abs(threshold))","fe815f96":"def severity_validation():\n    tOUT10 = tOut+(tOut*0.10)    \n    tOUT23 = tOut+(tOut*0.23)\n    tOUT45 = tOut+(tOut*0.45)\n    dfx['test_severity'] = \"None\"\n    for i, row in dfx.iterrows():\n        if row['outlier']==1:\n            if row['value'] <=tOUT10:\n                dfx['test_severity'][i] = \"Low Severity\" \n            elif row['value'] <=tOUT23:\n                dfx['test_severity'][i] = \"Medium Severity\" \n            elif row['value'] <=tOUT45:\n                dfx['test_severity'][i] = \"High Severity\" \n            else:\n                dfx['test_severity'][i] = \"Ultra High Severity\" \n\nseverity_validation()","4a3d0b60":"dfx.head()","d259fff4":"print(\" inline values proportion mean \",dfx[dfx['outlier']==0]['prop_value'].mean())\nprint(\" outlier values proportion mean\",dfx[dfx['outlier']==1]['prop_value'].mean())\nprint(\" outlier values max \",dfx[dfx['outlier']==1]['prop_value'].min())\n\n    \n","ba32d8fb":"dirname = \"\/kaggle\/input\/nab\/realTraffic\/realTraffic\/\"\nfilename = \"speed_t4013.csv\"\n\ndataframe = pd.read_csv(dirname+filename)#, usecols=[1])#, skipfooter=3)\ndataframe.head()","5beb5b9e":"# Any results you write to the current directory are saved as output.\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nab\/realKnownCause\/realKnownCause\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a86d57ad":"\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nab\/realAdExchange\/realAdExchange\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7d46781a":"# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bff15497":"df8 = pd.read_csv('\/kaggle\/input\/nab\/realAdExchange\/realAdExchange\/exchange-2_cpc_results.csv')\n","1b9b53a5":"df1 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/ec2_cpu_utilization_825cc2.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/ec2_cpu_utilization_ac20cd.csv')\ndf3 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/ec2_cpu_utilization_fe7f93.csv')\ndf4 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/ec2_cpu_utilization_77c1ca.csv')\n\ndf5 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/ec2_network_in_5abac7.csv')\n\ndf6 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/grok_asg_anomaly.csv')\n\ndf7 = pd.read_csv('\/kaggle\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/elb_request_count_8c0756.csv')\n","636810e2":"df6.head()","0b93efd7":"df4.head()","2ea83040":"x = [dt.datetime.strptime(d,\"%Y-%m-%d %H:%M:%S\").date() for d in df4[\"timestamp\"]]\ny = df4[\"value\"]\n\nplt.plot(x,y)\nplt.show()","0a0aef50":"fpath = \"..\/input\/nab\/realAWSCloudwatch\/realAWSCloudwatch\/\/\"\nfname = \"grok_asg_anomaly.csv\"\n\nfullPath = fpath + fname\n\ndef parser(x):\n\treturn dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n \ndata = pd.read_csv(fullPath, header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)","25445ac1":"arimaM = ARIMA(data, order=(5,1,0))\narimaMfit = arimaM.fit(disp=0)\nprint(arimaMfit.summary())","f9909f99":"# plot residual errors\nerrors = pd.DataFrame(arimaMfit.resid)\nerrors.plot()\npyplot.show()\nerrors.plot(kind='kde')\npyplot.show()\nprint(errors.describe())","27579822":"X = data.values\nsize = int(len(X) * 0.70)\nlimitCount = 50\ntrain, test = X[0:size], X[size:size+limitCount]\nhistory = [x for x in train]","0a571e00":"predictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(5,1,0))\n\tmodel_fit = model.fit(disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('pred=%f, exp=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Mean Squared Error: %.3f' % error)\n","9ffbda30":"# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","98297cea":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","d731fe2e":"dataframe = pd.read_csv(dirname+filename, usecols=[1], skipfooter=3)\ndataset = dataframe.values\ndataset = dataset.astype('float32')","b34112d8":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n\tdataX, dataY = [], []\n\tfor i in range(len(dataset)-look_back-1):\n\t\ta = dataset[i:(i+look_back), 0]\n\t\tdataX.append(a)\n\t\tdataY.append(dataset[i + look_back, 0])\n\treturn np.array(dataX), np.array(dataY)","c7d4a9ae":"# fix random seed for reproducibility\nnp.random.seed(7)\n# load the dataset\n# dataframe = pd.read_csv(dirname+filename, usecols=[1], skipfooter=3)\n# dataset = dataframe.values\n# dataset = dataset.astype('float32')\n# dataset = data.values\n# dataset = data.astype('float32')\n\n","1c2e014f":"# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n","9c803d9a":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]","12d134b3":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))","fd37ada6":"# create and fit the LSTM network\nbatch_size = 1\nmodel = Sequential()\nmodel.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nfor i in range(5):\n\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n\tmodel.reset_states()","fa2fa89f":"# make predictions\ntrainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)","3b7be039":"# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","60e6543e":"# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","ad9d9756":"# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict","0f9c73d6":"# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict","f933b636":"# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","27ff0974":"# Approach 2 - timeseries","9c28b82a":"> Values com proportion more than **0.2376237623762376%** are bad.","b0cb9681":"### Mean proportion of outlier values","8d65599c":"# Aproach 1 - Anomaly detection with Severity Level","94c77812":"# Final"}}