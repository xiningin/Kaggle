{"cell_type":{"2389bdb3":"code","0f6f13f3":"code","911cd9b5":"code","dc3e7c63":"code","44819635":"code","c5422834":"code","9e3c50df":"code","9879d0c3":"code","299bedd2":"code","aa5db040":"code","ada0b979":"code","862da851":"code","11745e7e":"code","515aebfe":"code","51042de5":"code","a142f213":"code","41d965d6":"code","7f9b2c11":"code","6033ad98":"code","2c37548a":"code","60a084a3":"code","80fd37e1":"code","c44b3eb9":"code","1018121b":"code","19450f14":"code","95260f31":"code","429438d0":"code","df383e62":"code","4252a95f":"code","70837944":"code","261da0d9":"code","897d0c08":"code","7e6c020d":"code","090f19be":"code","75ce2d7a":"code","a8e69d0b":"code","629c0551":"code","1057558f":"code","c2ba28b9":"code","1a97d19a":"code","6f085878":"code","98c51028":"code","f5dd6efd":"code","95b436c5":"code","b75103c9":"code","2199cc73":"code","2620284c":"code","8d330b21":"code","4ad09445":"code","c01fd5a3":"code","ac9dc5a9":"code","44d7e03f":"code","2e3acda1":"code","0dd131d4":"code","41b8dbbc":"code","1d5793d2":"code","507e73ec":"code","a228fe1d":"code","0b63ff33":"code","7fead36a":"code","0e953afd":"code","e7a1a673":"code","23affda7":"code","922c7392":"code","f4da4f97":"code","0194817d":"code","17dfbb29":"code","d177318c":"code","7d757e58":"code","de560487":"code","8175a7f5":"code","dee4ebb3":"code","6da79b57":"code","8868b4a2":"code","20875ce1":"code","ceccfbe5":"code","7fe01ebb":"code","7d5e697a":"code","1cf8f6ee":"code","e751f788":"code","5e9ae476":"code","546006ba":"code","a7eaae79":"code","05353cfd":"code","84ff7574":"markdown","862d4527":"markdown","963464ba":"markdown","a9e88785":"markdown","550459a4":"markdown","abbf9a9b":"markdown","91348780":"markdown","7d0da9f6":"markdown","95235776":"markdown","11ff02e6":"markdown","29d11c63":"markdown","cc937924":"markdown","71359ca5":"markdown","47595a39":"markdown","0fa27f6f":"markdown","ca020761":"markdown","f9387d28":"markdown","7de6376a":"markdown","125deec9":"markdown","023e6fdb":"markdown","4cf21f54":"markdown","7e2f0dcf":"markdown","f3c5cbbf":"markdown","2798e00a":"markdown","b823cc9b":"markdown","981db479":"markdown","51927efa":"markdown","6f968f35":"markdown","d4812247":"markdown","72fbc78b":"markdown"},"source":{"2389bdb3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,LeakyReLU\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import Ridge,LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","0f6f13f3":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntest_id = test['Id'] # Used during Submission of problem.","911cd9b5":"train['File'] = 'Train' # indicator where the record comes from , helps to get back the data.\ntest['File'] = 'Test' \n\ntest.drop(columns = 'Id', inplace = True) # dropping ID columns \/ not relevant to training\ntrain.drop(columns = 'Id', inplace = True)","dc3e7c63":"test['SalePrice'] = 0 # dummy value for proper concatenation of train and test.","44819635":"train.head()","c5422834":"test.head()","9e3c50df":"train['SalePrice'].describe()","9879d0c3":"#Distribution of Target Variable.\nsns.distplot(train['SalePrice'])\nsns.kdeplot(train['SalePrice'])","299bedd2":"print(f\"Skewness:{train['SalePrice'].skew()}\")\nprint(f\"Kurtosis:{train['SalePrice'].kurt()}\")","aa5db040":"correlation_matrix = train.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(correlation_matrix, vmax=.8, square=True)\nplt.show()","ada0b979":"# Considering only top 10 variables , with highest correlation with Target Variable.\ncols_10 = correlation_matrix.nlargest(10,'SalePrice').index\ncols_10 = list(cols_10)","862da851":"cm10 = correlation_matrix.loc[cols_10,cols_10]\nplt.figure(figsize=(10,8))\nsns.heatmap(cm10,annot=True,fmt = '.2f',square=True)\nplt.show()","11745e7e":"#Cols with less correlation among them\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 1.8)\nplt.show()","515aebfe":"# Combining dataset for combined cleaning and reducing any mismatch during training.\ndata = train.append(test,sort=False)\ndata.shape","51042de5":"total = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.shape[0]).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(34)","a142f213":"# Regarding 'PoolQC', 'MiscFeature', 'Alley'\ndata['PoolQC'].replace(np.nan,\"None\",inplace = True)\ndata['FireplaceQu'].replace(np.nan,\"None\",inplace = True)\ndata['Alley'].replace(np.nan,\"None\",inplace = True)","41d965d6":"sns.boxplot('PoolQC','SalePrice',data = data[:1460])\nplt.show()\nsns.boxplot('FireplaceQu','SalePrice',data = data[:1460])\nplt.show()\nsns.boxplot('Alley','SalePrice',data = data[:1460])\nplt.show()","7f9b2c11":"# Regarding LotFrontage\nprint(data[['LotFrontage','LotArea']].corr())\nsns.lineplot('LotFrontage','LotArea',data=data)\nplt.show()","6033ad98":"# Regarding MSZoning\nsns.boxplot('MSZoning','SalePrice',data = train)\n# used data = train, because, test set has no values for 'SalePrice'","2c37548a":"data['MSZoning'].replace(np.nan,\"RL\",inplace=True)","60a084a3":"# Regarding 'Functional' variable\nsns.boxplot('Functional','SalePrice',data = data[:1460])\nplt.show()\n","80fd37e1":"# Since only 2 values are missing , we'll impute it using MAX -> 'Typ'\ndata['Functional'].replace(np.nan,'Typ',inplace=True)","c44b3eb9":"# Removed Variables till 'MasVnrArea' (Refer the missing list above)\ndata = data.drop((missing_data[missing_data['Total'] > 4]).index,axis=1)","1018121b":"# Dropping the variables that are causing multicollinearity.\nmlc_cols = ['TotRmsAbvGrd','GarageArea','1stFlrSF']\ndata = data.drop(mlc_cols,axis=1)","19450f14":"# Remaining columns having missing values.\ndata.columns[data.isna().any()].tolist()","95260f31":"# Catergorical columns \ndata['Exterior1st'].replace(np.nan,\"VinylSd\",inplace=True) \ndata['Exterior2nd'].replace(np.nan,\"VinylSd\",inplace=True) \ndata['Electrical'].replace(np.nan,\"SBrkr\",inplace=True) \ndata['Utilities'].replace(np.nan,\"AllPub\",inplace=True) \ndata['KitchenQual'].replace(np.nan,'TA',inplace=True) \ndata['SaleType'].replace(np.nan,'WD',inplace=True)","429438d0":"total = data.isnull().sum().sort_values(ascending=False)[:10]\ntotal","df383e62":"# Filling the missing numerical columns, with 0 \ndata.fillna(0,inplace=True)","4252a95f":"# Checking if there are still some missing values remaining.\ndata.isnull().sum().max()","70837944":"# Created a new dataFrame for the resulting df, just in case something went wrong.\ndatax = pd.get_dummies(data,prefix = 'D',drop_first=True)","261da0d9":"datax.head()","897d0c08":"# Getting Train and test back\ntrain = datax[datax['D_Train']==1]\ntrain = train.drop(columns = ['D_Train'])\n\ntest = datax[datax['D_Train']==0]\ntest = test.drop(columns = ['D_Train','SalePrice']) # Remember we put all 0s in SalePrice of Test for concatenation.","7e6c020d":"test.shape","090f19be":"# Splitting our training data into train and test\n X_train, X_test, y_train, y_test = train_test_split(train.drop(['SalePrice'],axis=1), train['SalePrice'], test_size=0.33, random_state=101)","75ce2d7a":"lr = LinearRegression()\nlr.fit(X_train,y_train)","a8e69d0b":"lr.score(X_test,y_test)","629c0551":"rg = Ridge()\nrg.fit(X_train,y_train)","1057558f":"rg.score(X_test,y_test)","c2ba28b9":"sr = SVR(gamma = 'scale',epsilon=0.2)\nsr.fit(X_train,y_train)","1a97d19a":"sr.score(X_test,y_test)","6f085878":"rf = RandomForestRegressor(n_jobs=-1) # n_jobs = -1 , to distribute the computaion on diff cpu cores\nrf.fit(X_train,y_train)","98c51028":"rf.score(X_test,y_test)","f5dd6efd":"reg = Sequential()\nreg.add(Dense(1024, input_dim=X_train.shape[1]))\nreg.add(LeakyReLU(alpha=0.05))\nreg.add(Dropout(0.4))\n\nreg.add(Dense(512))\nreg.add(LeakyReLU(alpha=0.05))\nreg.add(Dropout(0.4))\nreg.add(Dense(256))\nreg.add(LeakyReLU(alpha=0.05))\nreg.add(Dropout(0.2))\nreg.add(Dense(64))\nreg.add(LeakyReLU(alpha=0.05))\nreg.add(Dropout(0.2))\n\nreg.add(Dense(1,activation='linear'))\nreg.summary()","95b436c5":"from keras import backend as K\n\ndef r2_keras(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )","b75103c9":"reg.compile(loss = 'mean_squared_logarithmic_error', optimizer = 'adam',metrics=['mean_squared_logarithmic_error', r2_keras])","2199cc73":"hist = reg.fit(X_train,y_train,128,1000,validation_data=[X_test,y_test])","2620284c":"ypred = reg.predict(X_test)","8d330b21":"r2_score(y_test,ypred)","4ad09445":"plt.plot(hist.history['loss'])\nplt.show","c01fd5a3":"plt.plot(hist.history['r2_keras'])\nplt.show","ac9dc5a9":"predicted_prices =rf.predict(test)","44d7e03f":"my_submission = pd.DataFrame({'Id':test_id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submissionv4.csv', index=False)","2e3acda1":"sc = StandardScaler()","0dd131d4":"datax.info()","41b8dbbc":"numerical_cols = list(datax.select_dtypes(include=['float','int']).columns)\nnumerical_cols.pop()","1d5793d2":"sc.fit(datax[numerical_cols])","507e73ec":"datax[numerical_cols] = sc.transform(datax[numerical_cols])\n","a228fe1d":"train = datax[datax['D_Train']==1]\ntrain = train.drop(columns = ['D_Train'])\n\ntest = datax[datax['D_Train']==0]\ntest = test.drop(columns = ['D_Train','SalePrice'])","0b63ff33":"# Splitting our data into train and test\n X_train, X_test, y_train, y_test = train_test_split(train.drop(['SalePrice'],axis=1), train['SalePrice'], test_size=0.33, random_state=101)","7fead36a":"rg.fit(X_train,y_train)\nrg.score(X_test,y_test)","0e953afd":"rf.fit(X_train,y_train)\nrf.score(X_test,y_test)\n","e7a1a673":"reg.compile(loss = 'mse', optimizer = 'adam',metrics=[r2_keras])","23affda7":"hist = reg.fit(X_train,y_train,128,1000)","922c7392":"ypred = reg.predict(X_test)\nr2_score(y_test,ypred)","f4da4f97":"predicted_prices =reg.predict(test)","0194817d":"predicted_prices = predicted_prices.reshape(-1)\npredicted_prices.shape","17dfbb29":"my_submission = pd.DataFrame({'Id':test_id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('sub_with_scaledXv8.csv', index=False)","d177318c":"tn_rg = rg.predict(X_test)\ntn_reg = reg.predict(X_test)\ntn_rf = reg.predict(X_test)","7d757e58":"tn_reg=tn_reg.reshape(-1)\ntn_rf=tn_rf.reshape(-1)\ntn_rg=tn_rg.reshape(-1)\ntn = np.stack((tn_reg,tn_rg,tn_rf),axis = 1)\ntn.shape","de560487":"ts_rg = rg.predict(test)\nts_reg = reg.predict(test)\nts_rf = reg.predict(test)","8175a7f5":"ts_reg=ts_reg.reshape(-1)\nts_rf=ts_rf.reshape(-1)\nts_rg=ts_rg.reshape(-1)\nts = np.stack((ts_reg,ts_rg,ts_rf),axis = 1)\nts.shape","dee4ebb3":"ranfo = RandomForestRegressor(n_jobs=-1)","6da79b57":"ranfo.fit(tn,y_test)","8868b4a2":"predictions=ranfo.predict(ts)","20875ce1":"predictions.shape","ceccfbe5":"my_submission = pd.DataFrame({'Id':test_id, 'SalePrice': predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('sub_with_scaled_stackedv5.csv', index=False)","7fe01ebb":"import xgboost as xgb","7d5e697a":"xgb_reg = xgb.XGBRegressor() # Default Settings.","1cf8f6ee":"results = xgb_reg.fit(tn,y_test)","e751f788":"xgb_predictions=xgb_reg.predict(ts)\nxgb_predictions=xgb_predictions.reshape(-1)","5e9ae476":"my_submission = pd.DataFrame({'Id':test_id, 'SalePrice': xgb_predictions})\nmy_submission.to_csv('sub_with_scaled_stackxgbv9.csv', index=False)","546006ba":"xgb_reg = xgb.XGBRegressor() # Default Settings.\nxgb_reg.fit(X_train.values,y_train)\ntn_xgb = xgb_reg.predict(X_test.values)\ntn_xgb=tn_xgb.reshape(-1)\ntn = np.stack((tn_reg,tn_rg,tn_rf,tn_xgb),axis = 1)\ntn.shape","a7eaae79":"ts_xgb = xgb_reg.predict(test.values)\nts_xgb=ts_xgb.reshape(-1)\nts = np.stack((ts_reg,ts_rg,ts_rf,ts_xgb),axis = 1)\nts.shape","05353cfd":"xgbr = xgb.XGBRegressor()\nresults = xgbr.fit(tn,y_test)\nxgb_predictions=xgbr.predict(ts)\nxgb_predictions=xgb_predictions.reshape(-1)\nmy_submission = pd.DataFrame({'Id':test_id, 'SalePrice': xgb_predictions})\nmy_submission.to_csv('sub_with_scaled_stackxgbv9.csv', index=False)","84ff7574":"## EDA","862d4527":"Considering the number of Outliers and missing values also, we are discarding these variables","963464ba":"**Getting the data.**","a9e88785":"## Missing Values","550459a4":"## Using Neural Nets","abbf9a9b":"We should impute mising 'MSZoning' with 'RL'","91348780":"# Stacking models.","7d0da9f6":"# References\nThis is my first ever Kaggle competition, I took some help from the following amazing Kernel.\n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n","95235776":"# Using scaled values","11ff02e6":"#### Final Imputation\n","29d11c63":"#### Magnifying Further.","cc937924":"## Trying XGBoost\n**For the first time**  \nHere I am just trying to use XGBoost for fitting the new dataset created using stacking other models predictions.","71359ca5":"### Univariate analysis on Target Variable","47595a39":"## Using XGBoost for stacking also.","0fa27f6f":"### 1. Linear Regression","ca020761":"### 3. Support Vector Regressor","f9387d28":"# Model Training\n","7de6376a":"We find linear and exponential-like relationships between Target Variable and the variables in Cols list","125deec9":"### 4. RandomForestRegressor","023e6fdb":"#### Zooming a little.","4cf21f54":"### 2. Ridge Regression","7e2f0dcf":"## Categorical Values","f3c5cbbf":"# Data Cleaning\n","2798e00a":"**Observations**\n* 'PoolQC', 'MiscFeature', 'Alley' are missing more than 90% values, and not some strong cadidate to affect SalePrice, so we'll drop them but first see their boxplots.\n* 'GarageX' variables are also missing more than 50 % , and most of their significance could be handles using 'GarageCars'\n* Similarly for BsmtX variables with more than 2 % values missing.\n* 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered.","b823cc9b":"**Here we have the following observations.**\n* Highest correlation with OverallQual. \n* The variables 'GarageCars' and 'GarageArea' are highly correlated. So to avoid to avoid Multicollinearity, we must use either one of the feature, but since GarageCars has higher correlation with SalePrice, we're going to use that.\n* GrLivArea is correlated to TotRmsAbvGrnd , which is quite obvious. We can drop the latter.\n","981db479":"### Going with Random Forest Regressor","51927efa":"Positively Skewed  \nShows peakedness :  Leptokurtic","6f968f35":"* TotalBsmtSF and 1stFlrSF are also highy correlated.","d4812247":"Considering number of missing values , and its relationship with LotArea, we'll drop it.","72fbc78b":"### Multivariate analysis"}}