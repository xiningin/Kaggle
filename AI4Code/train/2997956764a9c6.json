{"cell_type":{"490b9dde":"code","15407c0e":"code","b878aa3b":"code","430b2927":"code","f6a7d57f":"code","c2a2f594":"code","e67d4351":"code","dffa6153":"code","f54a21df":"code","5367b4df":"code","0b0ef9d3":"code","bb2cdb45":"code","b0020804":"code","2023876e":"code","afdb5688":"code","d207d878":"code","d5488963":"code","b1f7ae2f":"code","d2d18d45":"code","0c57e3ea":"code","d3b29c0a":"code","42897cf6":"code","4a960239":"code","23cb0498":"code","b54c5877":"code","c7a6c128":"code","42785a17":"code","8cce5c24":"code","0581b5bb":"code","c45179d2":"code","16619091":"code","e16bdc21":"code","4721229e":"code","a5ae18eb":"code","03c1e846":"code","a4844483":"code","34238d55":"code","7562954c":"code","be86a86a":"code","cf5ebfba":"code","c6a661a4":"code","b13118ef":"code","bbeb5bdc":"code","e8021412":"code","b495d65d":"markdown","01e4058b":"markdown","b24cefae":"markdown","188df3f6":"markdown","2d6963cf":"markdown","87166143":"markdown","3a0ea91d":"markdown"},"source":{"490b9dde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; \nfrom sklearn.model_selection import train_test_split\nfrom pylab import rcParams\n\nfrom scipy import stats\nfrom collections import Counter\n\n# for modeling \nimport sklearn\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score,  accuracy_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, classification_report\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n\n# Plotly for interactive graphics \nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15407c0e":"data = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","b878aa3b":"df =data.copy()","430b2927":"df.head()","f6a7d57f":"df.shape","c2a2f594":"nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\nnv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)","e67d4351":"df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)\ndf[['RainToday', 'RainTomorrow']].isnull().sum()","dffa6153":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    display(str.sort_values(by='Nulls', ascending=False))\n\nsummary(df)","f54a21df":"df[['RainToday','RainTomorrow']] = df[['RainToday','RainTomorrow']].replace({'Yes':1, 'No':0})","5367b4df":"df.dropna(inplace=True)","0b0ef9d3":"df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\ndf['Year'] = df['Date'].dt.year.astype('int16')\ndf['Month'] = df['Date'].dt.month.astype('int16')\ndf.head()","bb2cdb45":"# for all variables \nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True, cmap=\"coolwarm\");","b0020804":"df.isnull().sum()","2023876e":"df.drop(columns=\"Date\", axis=1, inplace=True)","afdb5688":"df.sample(10)","d207d878":"for i in df.select_dtypes(include=np.number).columns.tolist():\n    plt.figure()\n    df.boxplot([i])","d5488963":"df = pd.get_dummies(df, drop_first=True, columns = ['Location','WindGustDir','WindDir9am','WindDir3pm'])","b1f7ae2f":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\n\ny = df['RainTomorrow']\nX = df.drop(['RainTomorrow'], axis=1)\n\nclf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.10, random_state=42)\nclf.fit(X)\ny_pred = clf.predict(X)\n\n# the model will predict an inlier with a label of +1 and an outlier with a label of -1\n\noutliers_values = X[clf.predict(X) == -1]\noutliers_values","d2d18d45":"outliers_values = X[clf.predict(X) == -1]\nf\"{len(outliers_values)} rows are outliers\"","0c57e3ea":"df = X.join(y)[clf.predict(X) == 1]\ndf","d3b29c0a":"# separating the dependent and independent data\nX=df.drop([\"RainTomorrow\"], axis=1)\ny=df[\"RainTomorrow\"]\n\n# the function train_test_split creates random data samples (default: 75-25%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state =42)\n\n# getting the shapes\nprint(f\"\"\"shape of X_train: {X_train.shape}\nshape of X_test\\t: {X_test.shape}\nshape of y_train: {y_train.shape}\nshape of y_test\\t: {y_test.shape}\"\"\")","42897cf6":"logreg = LogisticRegression(solver='liblinear', random_state=42)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred","4a960239":"def conf_matrix(model, X_test, y_test, cmap='Blues'):\n    plot_confusion_matrix(model, X_test, y_test, cmap=cmap)\n    plt.grid()\n    plt.show()\n\ndef roc_curve_custom(model, X_test, y_test):\n    plot_roc_curve(model, X_test, y_test)\n    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n    plt.show()\n    \ndef evaluate(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, y_pred=y_pred):\n    # Confusion Matrix\n    print('Confusion Matrix')\n    print('-'*53)\n    conf_matrix(model, X_test, y_test)\n    print('\\n') \n    \n    # Classification Report\n    print('Classification Report') \n    print('-'*53)\n    print(classification_report(y_test, y_pred))\n    print('\\n')\n    \n    # ROC Curve\n    print('ROC Curve')\n    print('-'*53)\n    roc_curve_custom(model, X_test, y_test)\n    print('\\n')\n    \n    # Checking model fitness\n    print('Checking model fitness') \n    print('-'*53)\n    print('Train score:', round(model.score(X_train, y_train), 4))\n    print('Test score: ', round(model.score(X_test, y_test), 4))\n    print('\\n')\n    \nevaluate(logreg)","23cb0498":"X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n\nprint('Original')\nprint('-'*20)\nprint(y_train.value_counts())\nprint('\\n')\nprint('SMOTE')\nprint('-'*20)\nprint(pd.Series(y_train_resampled).value_counts())","b54c5877":"logreg_smote = LogisticRegression(solver='liblinear', random_state=42)\nlogreg_smote.fit(X_train_resampled, y_train_resampled)\ny_pred_smote = logreg_smote.predict(X_test)\ny_pred_smote","c7a6c128":"evaluate(logreg_smote, X_train=X_train_resampled, y_train=y_train_resampled, y_pred=y_pred_smote)","42785a17":"logreg_params = {\n    'C': [1, 1e8, 1e16],\n    'fit_intercept': [True, False],\n    'max_iter': [50, 100, 150],\n    'random_state': [42]\n}\n\nlogreg_tuned = GridSearchCV(logreg, logreg_params, scoring='accuracy', n_jobs=-1, cv=3)\nlogreg_tuned.fit(X_train, y_train)","8cce5c24":"print(\"The best score:\" + str(round(logreg_tuned.best_score_, 4)))\nprint(\"The best parameters: \" + str(logreg_tuned.best_params_))","0581b5bb":"y_pred_logreg_tuned = logreg_tuned.predict(X_test)\ny_pred_logreg_tuned","c45179d2":"evaluate(logreg_tuned, y_pred=y_pred_logreg_tuned)","16619091":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model = nb_model.fit(X_train, y_train)\nnb_model","e16bdc21":"y_pred = nb_model.predict(X_test)","4721229e":"evaluate(nb_model, y_pred=y_pred)","a5ae18eb":"nb_finalscore=cross_val_score(nb_model, X_test, y_test, cv = 10).mean()\nnb_finalscore","03c1e846":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier()\nknn_model = knn_model.fit(X_train, y_train)\n# ?knn_model","a4844483":"y_pred = knn_model.predict(X_test)","34238d55":"accuracy_score(y_test, y_pred)","7562954c":"evaluate(knn_model, y_pred=y_pred)","be86a86a":"knn_params = {\"n_neighbors\": np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(X_train, y_train)","cf5ebfba":"print(\"The best score:\" + str(knn_cv.best_score_))\nprint(\"The best parameters: \" + str(knn_cv.best_params_))","c6a661a4":"knn_tuned =KNeighborsClassifier(n_neighbors = 37)\nknn_tuned = knn_tuned.fit(X_train,y_train)\ny_pred = knn_tuned.predict(X_test)\n\nknn_tuned_score=accuracy_score(y_test,y_pred)\nknn_tuned_score","b13118ef":"evaluate(knn_tuned, y_pred=y_pred)","bbeb5bdc":"svm = SVC(C=5,degree=9,kernel = 'poly')\nsvm.fit(X_train,y_train)\ny_pred = svm.predict(X_test)\n?svm","e8021412":"evaluate(svm, y_pred=y_pred)","b495d65d":"### Tuning","01e4058b":"# 2) Gaussian Naive Bayes ","b24cefae":"## 4)SVM(SUPPORT VECTOR MACHINES)","188df3f6":"1. Logistic  Regression","2d6963cf":"## Creating Models","87166143":"# Outliers","3a0ea91d":"## 3) KNN"}}