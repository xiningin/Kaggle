{"cell_type":{"9200d011":"code","c3a981fd":"code","ca7943de":"code","b5bc1218":"code","f81881f3":"code","2d93e7d8":"code","85260711":"markdown","f0fcb9af":"markdown","7514659b":"markdown","604bfc66":"markdown","77b03227":"markdown"},"source":{"9200d011":"%%bash\npip install allennlp -q --ignore-installed greenlet","c3a981fd":"%%writefile snli_basic.conf\n\/\/ Configuraiton for a textual entailment model based on:\n\/\/  Parikh, Ankur P. et al. \u201cA Decomposable Attention Model for Natural Language Inference.\u201d EMNLP (2016).\n{\n  \"dataset_reader\": {\n    \"type\": \"snli\",\n    \"token_indexers\": {\n      \"tokens\": {\n        \"type\": \"single_id\",\n        \"lowercase_tokens\": true\n      }\n    },\n    \"tokenizer\": {\n      \"end_tokens\": [\"@@NULL@@\"]\n    }\n  },\n  \"train_data_path\": \"..\/input\/snli-jsonl\/snli_1.0_train.jsonl\",\n  \"validation_data_path\": \"..\/input\/snli-jsonl\/snli_1.0_dev.jsonl\",\n  \"model\": {\n    \"type\": \"decomposable_attention\",\n    \"text_field_embedder\": {\n      \"token_embedders\": {\n        \"tokens\": {\n            \"type\": \"embedding\",\n            \"projection_dim\": 200,\n            \"pretrained_file\": \"..\/input\/glove6b\/glove.6B.300d.txt\",\n            \"embedding_dim\": 300,\n            \"trainable\": false\n        }\n      }\n    },\n    \"attend_feedforward\": {\n      \"input_dim\": 200,\n      \"num_layers\": 2,\n      \"hidden_dims\": 200,\n      \"activations\": \"relu\",\n      \"dropout\": 0.2\n    },\n    \"similarity_function\": {\"type\": \"dot_product\"},\n    \"compare_feedforward\": {\n      \"input_dim\": 400,\n      \"num_layers\": 2,\n      \"hidden_dims\": 200,\n      \"activations\": \"relu\",\n      \"dropout\": 0.2\n    },\n    \"aggregate_feedforward\": {\n      \"input_dim\": 400,\n      \"num_layers\": 2,\n      \"hidden_dims\": [200, 3],\n      \"activations\": [\"relu\", \"linear\"],\n      \"dropout\": [0.2, 0.0]\n    },\n     \"initializer\": [\n      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n      [\".*token_embedder_tokens\\\\._projection.*weight\", {\"type\": \"xavier_normal\"}]\n     ]\n   },\n  \"iterator\": {\n    \"type\": \"bucket\",\n    \"sorting_keys\": [[\"premise\", \"num_tokens\"], [\"hypothesis\", \"num_tokens\"]],\n    \"batch_size\": 64\n  },\n  \"trainer\": {\n    \"num_epochs\": 10,\n    \"patience\": 3,\n    \"num_serialized_models_to_keep\": 4,\n    \"cuda_device\": 0,\n    \"grad_clipping\": 5.0,\n    \"validation_metric\": \"+accuracy\",\n    \"optimizer\": {\n      \"type\": \"adagrad\"\n    }\n  }\n}","ca7943de":"%%bash\nallennlp train -s snli_basic snli_basic.conf","b5bc1218":"%%bash\nallennlp evaluate --output-file snli_basic_results.jsonl snli_basic\/model.tar.gz ..\/input\/snli-jsonl\/snli_1.0_test.jsonl","f81881f3":"!head snli_basic_results.jsonl","2d93e7d8":"import pandas as pd\npd.read_csv('snli_basic\/vocabulary\/tokens.txt',sep=' ',header=None,names=['token'])","85260711":"# Evaluate (via CLI)\n\nOnce the training is completed, the model is saved to snli_basic\/model.tar.gz.  The logs and checkpoints are also saved to the SERIALIZATION_DIR.","f0fcb9af":"# Configuration File\n\nIn this example, we'll examine AllenNLP's model configuration file because it displays several different Allen NLP abstractions in one place.   \n\n## DatasetReader\n\nThis example reads in the Standford Natural Language Inference (SNLI) dataset.  This dataset is in a json line format with the following fields:\n\n* sentence1: The premise caption that was supplied to the author of the pair.\n* sentence2: The hypothesis caption that was written by the author of the pair.\n* gold_label: entailment, contridiction, or neutral\n\nThe reader does a few things to prep the data.\n1. Tokenizes the input sentence pairs\n2. Indexes the tokens in both sentences\n3. Indexes the lables\n\n> Note: We're using the default tokenizer from SpaCy.  However, the tokenizer is easily replaced.  Note that the tokens are converted to lower case before being indexed.  \n\n## Model\n\nWe'll configure the decomposible attention model for the training run.  In AllenNLP, the model extends torch.nn.Module with a custom [forward method](https:\/\/allenai.github.io\/allennlp-docs\/api\/allennlp.models.model.html) to plug in to AllenNLP's trainer.\n\n> Note: For a more comprehensive tutorial that includes building your own dataset reader and model, see [An In-Depth Tutorial to AllenNLP (From Basics to ELMo and BERT)](http:\/\/mlexplained.com\/2019\/01\/30\/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert\/)\n\n### text_field_embedder\n\nThe ```text_field_embedder``` converts the premise and hypothesis sentences to embeddings.  In this case, the model usea pretrained glove embeddings.  We can swap out the embeddings by changing this configuration.  \n\n\n### initializer\n\nThis componment initalizes the models parameters (aka weights) based on a regex pattern.  \n\n### Decomposable attention specific modules\n\nThe basic architecture of the model is shown below.  The decomposable attention model has three basic components: attend, compare, and aggregate.  AllenNLP handles these basic components as pytouch modules, which makes the implementation fairly interchangeable.  AllenNLP provides several NLP specific modules, which are documented [here](https:\/\/allenai.github.io\/allennlp-docs\/api\/allennlp.modules.html).\n\n![Decomposable attention](data:image\/png;base64,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)\n\nFor more information see the [models documentation](https:\/\/allenai.github.io\/allennlp-docs\/api\/allennlp.models.decomposable_attention.html#module-allennlp.models.decomposable_attention).\n\n## Iterator\n\nThe iterator feeds the input data to the model.  While this sounds simple, its not becuase the textual data lengths varies.  The iterator handles the padding the input so that the input data fits in the model. \n\nThe example uses a Bucket Iterator: \n\n> An iterator which by default, pads batches with respect to the maximum input lengths per batch. Additionally, you can provide a list of field names and padding keys which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch).\n\n## Trainer\n\nThis component configures the training session.  We'll run the training for at least 10 and stop early if the accuracy hasn't improved for 3 epochs.  The configuration also includes the gradient descent settings, like the gradient descent optizimer.","7514659b":"# Vocabulary \n\nLets look at the vocabulary before moving on.  This file will look different in the next example because the next example will use wordpiece tokens and BERT embeddings. ","604bfc66":"# Train (via CLI)\n\nallennlp train --help\n\n\n```\nusage: allennlp train [-h] -s SERIALIZATION_DIR [-r] [-f] [-o OVERRIDES]\n                      [--file-friendly-logging]\n                      [--include-package INCLUDE_PACKAGE]\n                      param_path\n\nTrain the specified model on the specified dataset.\n\npositional arguments:\n  param_path            path to parameter file describing the model to be\n                        trained\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SERIALIZATION_DIR, --serialization-dir SERIALIZATION_DIR\n                        directory in which to save the model and its logs\n  -r, --recover         recover training from the state in serialization_dir\n  -f, --force           overwrite the output directory if it exists\n  -o OVERRIDES, --overrides OVERRIDES\n                        a JSON structure used to override the experiment\n                        configuration\n  --file-friendly-logging\n                        outputs tqdm status on separate lines and slows tqdm\n                        refresh rate\n  --include-package INCLUDE_PACKAGE\n                        additional packages to include\n```","77b03227":"# AllenNLP Introduction\nThis notebook trains a decomposable attention model on the Stanford Natural Language Inference dataset.  \n\n\n* Install allennlp \n* Configure the model & examine the different abstractions\n* Train the model via the comannd line interface\n* Evaluate the model via the comannd line interface\n"}}