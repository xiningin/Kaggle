{"cell_type":{"ba8c2631":"code","ed2f8a51":"code","d742868c":"code","ac4bba28":"code","5edcefb1":"code","119701e7":"code","22045eba":"code","a0b2bf04":"code","ce074c8e":"code","7d5e059e":"code","d28a189d":"code","0a61ffc7":"code","5c0b9f88":"code","79f23b19":"code","34d314cf":"code","ba0074bb":"code","243dc486":"code","41610375":"code","5c9e05bd":"code","06ead18e":"code","8e264001":"code","bc1afa1d":"code","9f937f33":"code","629b0c54":"code","3076cfbf":"code","04e2f9d4":"code","dc591dcc":"code","edb4a6dc":"code","419b3451":"code","07a7fc7a":"code","ef0e3035":"code","38837352":"code","803feae2":"code","7f41b0df":"code","f8625b4b":"code","e85739d1":"code","87d59917":"code","fb185fc7":"code","24081792":"code","073c757d":"code","a0e9120e":"code","ab522507":"code","82c79196":"code","6d40d302":"code","e9f5ee17":"code","9f88f4f0":"code","3f44ee92":"code","b6fb7bff":"code","52f59fe1":"code","b317a513":"markdown","278d5d09":"markdown","ae5288fe":"markdown","8eb49128":"markdown","bb1ac312":"markdown","1e2b77c7":"markdown"},"source":{"ba8c2631":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ed2f8a51":"!pip uninstall -y torch","d742868c":"!conda install -y pytorch=0.3.1.0 cuda80 -c soumith","ac4bba28":"import torch\ntorch.__version__","5edcefb1":"!pip install fastai==0.7.0\n!pip uninstall -y torchtext\n!pip install torchtext==0.2.3\n\nfrom fastai.text import *\nimport html","119701e7":"train_df=pd.read_csv('..\/input\/train.csv')\ntest_df=pd.read_csv('..\/input\/test.csv')","22045eba":"trn_df,val_df = sklearn.model_selection.train_test_split(train_df, test_size=0.1)","a0b2bf04":"trn_texts = trn_df['question_text']\nval_texts = val_df['question_text']\n\ntrn_labels = trn_df['target']\nval_labels = val_df['target']","ce074c8e":"trn_labels.value_counts()","7d5e059e":"col_names = ['labels','text']","d28a189d":"BOS = 'xbos'  # beginning-of-sentence tag\nFLD = 'xfld'  # data field tag","0a61ffc7":"df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\ndf_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)","5c0b9f88":"!mkdir ..\/updated","79f23b19":"!ls -lart ..\/","34d314cf":"df_trn.to_csv('..\/updated\/train.csv', header=False, index=False)\ndf_val.to_csv('..\/updated\/test.csv', header=False, index=False)","ba0074bb":"chunksize=24000","243dc486":"re1 = re.compile(r'  +')\n\ndef fixup(x):\n    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x))","41610375":"def get_texts(df, n_lbls=1):\n    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n    texts = list(texts.apply(fixup).values)\n\n    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n    return tok, list(labels)","5c9e05bd":"def get_all(df, n_lbls):\n    tok, labels = [], []\n    for i, r in enumerate(df):\n        print(i)\n        tok_, labels_ = get_texts(r, n_lbls)\n        tok += tok_;\n        labels += labels_\n    return tok, labels","06ead18e":"df_trn = pd.read_csv('..\/updated\/train.csv', header=None, chunksize=chunksize)\ndf_val = pd.read_csv('..\/updated\/test.csv', header=None, chunksize=chunksize)","8e264001":"tok_trn, trn_labels = get_all(df_trn, 1)\ntok_val, val_labels = get_all(df_val, 1)","bc1afa1d":"np.save('..\/updated\/tok_trn.npy', tok_trn)\nnp.save('..\/updated\/tok_val.npy', tok_val)","9f937f33":"tok_trn = np.load('..\/updated\/tok_trn.npy')\ntok_val = np.load('..\/updated\/tok_val.npy')","629b0c54":"freq = Counter(p for o in tok_trn for p in o)","3076cfbf":"max_vocab = 60000\nmin_freq = 2","04e2f9d4":"itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\nitos.insert(0, '_pad_')\nitos.insert(0, '_unk_')","dc591dcc":"stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\nlen(itos)","edb4a6dc":"trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\nval_lm = np.array([[stoi[o] for o in p] for p in tok_val])","419b3451":"np.save('..\/updated\/trn_ids.npy', trn_lm)\nnp.save('..\/updated\/val_ids.npy', val_lm)\npickle.dump(itos, open('..\/updated\/itos.pkl', 'wb'))","07a7fc7a":"trn_lm = np.load('..\/updated\/trn_ids.npy')\nval_lm = np.load('..\/updated\/val_ids.npy')\nitos = pickle.load(open('..\/updated\/itos.pkl', 'rb'))","ef0e3035":"vs=len(itos)\nvs,len(trn_lm)","38837352":"!wget -P ..\/updated\/ http:\/\/files.fast.ai\/models\/wt103\/fwd_wt103.h5 ","803feae2":"wgts = torch.load('..\/updated\/fwd_wt103.h5', map_location=lambda storage, loc: storage)","7f41b0df":"enc_wgts = to_np(wgts['0.encoder.weight'])\nrow_m = enc_wgts.mean(0)","f8625b4b":"PATH=Path('..\/updated')","e85739d1":"!wget -P ..\/updated\/ http:\/\/files.fast.ai\/models\/wt103\/itos_wt103.pkl \nitos2 = pickle.load((PATH\/'itos_wt103.pkl').open('rb'))\nstoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})","87d59917":"em_sz,nh,nl = 400,1150,3","fb185fc7":"new_w = np.zeros((vs, em_sz), dtype=np.float32)\nfor i,w in enumerate(itos):\n    r = stoi2[w]\n    new_w[i] = enc_wgts[r] if r>=0 else row_m","24081792":"wgts['0.encoder.weight'] = T(new_w)\nwgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\nwgts['1.decoder.weight'] = T(np.copy(new_w))","073c757d":"import gc\ngc.enable()","a0e9120e":"gc.collect()","ab522507":"wd=1e-7\nbptt=70\nbs=52\nopt_fn = partial(optim.Adam, betas=(0.8, 0.99))","82c79196":"trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\nval_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\nmd = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)","6d40d302":"drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7","e9f5ee17":"learner= md.get_model(opt_fn, em_sz, nh, nl, \n    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n\nlearner.metrics = [accuracy]\nlearner.freeze_to(-1)","9f88f4f0":"learner.model.load_state_dict(wgts)","3f44ee92":"lr=1e-2\nlrs = lr","b6fb7bff":"learner","52f59fe1":"### Taking too long to train as per the kernel requirement I couldn't commit it\n\n#learner.fit(lrs, 1, wds=wd, use_clr=(32,2), cycle_len=1)","b317a513":"**Stay tune to be continued**","278d5d09":"### Language model in action.....","ae5288fe":"#### Set the map with our inputs","8eb49128":"###  ULMFit\n\nThis kernel is the initial version of using universal language model for text classification, which is currently the state of the art method for text classification. To put this thing in this shape, it took a lot of effort in configuration, Finally I figured out the way to use it kaggle kernel.\n\nMajority of the code is taken from FastAI course. I extend my Sincere gratitude  to Jeremey Howard and Sebastian Ruder who are contributing  amazing stuff in NLP.\n\n\n**Understanding ULMFit**\n\nTransfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data.\n\nAt first we train a language model with the given text and use it for classification....\n\n![ULM FIT](http:\/\/nlp.fast.ai\/images\/ulmfit_approach.png)\n\n**Tasks done in this notebook**\n\n1) Training a language model.\n\n2) Using Language model encoder for classification\n\n**TODO**\n\n1) Predictions on test data.\n\n2) Blending both forward and backward trained language model for classification.\n","bb1ac312":"#### Getting the wikipedia language model","1e2b77c7":"#### Speeding up the tokenization:\n\n\n\nFor Speeding up the tokenization process we are breaking frame into chunks"}}