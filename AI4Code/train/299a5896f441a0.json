{"cell_type":{"cbac7396":"code","f49e5a70":"code","f45a89b3":"code","da76e211":"code","6b9a55ea":"code","12ce1163":"code","0a010a2c":"code","5f0ea9b5":"code","50b75bc6":"code","9609c000":"code","e76bd380":"code","d2033551":"code","c35fa648":"code","3187926c":"code","4b178f22":"code","3e9c9ab0":"code","9786c5e5":"code","de1c2e69":"code","d03fbe00":"code","56adab0e":"code","5f37d384":"code","ac2af738":"code","5193a594":"code","ffd8d73c":"code","55f381f0":"code","ec8a935a":"code","5a417ad8":"code","e59b13ee":"code","4682b68e":"code","1fefcc67":"code","dce96c5d":"code","26960039":"code","c5d8e175":"code","5ce88c14":"code","74647b0e":"code","3af15685":"code","a0944def":"code","2f222a22":"code","447aac2f":"code","1650e8e4":"code","3a23795d":"code","8729fda6":"code","d85a1639":"code","2f8ebecf":"code","ff31b681":"code","f5d69eba":"code","441b0ce5":"code","2a70a220":"code","e2c17657":"code","ece238bc":"code","8d30a75d":"code","f7a1fce8":"code","ba659f60":"code","47dd1fd4":"code","4f32e998":"code","166c0f30":"code","be0148c9":"code","924a4b91":"code","36f055e0":"code","d5297597":"code","7f5fa1b6":"code","c08b16dc":"code","56f40851":"code","1094e9e8":"code","c030de0d":"code","fe1d6f91":"code","3799e8a9":"code","9fc92cf6":"code","a8bb2543":"code","b721bcb8":"code","ff0cab66":"code","21854734":"code","1374094e":"code","d09cf6c6":"code","bba4d724":"code","3eeb4a73":"code","59286486":"code","c96a7a3b":"code","f46c4beb":"code","6febad38":"code","c4b4c4fa":"code","ca9e3dec":"code","05ac7dad":"code","3110f78b":"code","3e01b4e6":"code","4f75d02d":"code","54b9e69b":"code","55997c87":"code","b3e177c5":"code","4df02763":"code","18e148a7":"code","c847fc4c":"code","23cf8c23":"code","906a145c":"code","57f5547a":"code","403c9518":"markdown","c2c267a4":"markdown","2bf4261c":"markdown","0aefb093":"markdown","bad10b06":"markdown","4280ead1":"markdown","d3445612":"markdown","f296f8ef":"markdown","dc4dc73f":"markdown","16af80e9":"markdown","f687c40a":"markdown","d931c5a2":"markdown","0576f948":"markdown","d5e1774b":"markdown","e3b278c9":"markdown","e08d2ee8":"markdown","cc2c56bf":"markdown","2817c4e0":"markdown","27349d8d":"markdown","7434e6c5":"markdown","93b94515":"markdown","effd30d1":"markdown","c2df01cf":"markdown","4bd63517":"markdown","f4ff6d96":"markdown","52b84c10":"markdown","169b69a8":"markdown","1826b406":"markdown","e8a75053":"markdown","ebc586e4":"markdown","dee5a356":"markdown","3560348b":"markdown","c29d5527":"markdown","63c2c0f5":"markdown","2bb95431":"markdown","7a741481":"markdown","851eb1c1":"markdown","fa1def81":"markdown","239582e9":"markdown","a5728ed9":"markdown","924f57fe":"markdown","a0f99643":"markdown","7376b115":"markdown","5334865b":"markdown","a750ec3b":"markdown","5c5463a3":"markdown","93f05126":"markdown","b88fa614":"markdown","4e665be3":"markdown","6f727d2f":"markdown","40b7b1ce":"markdown","cdf5f5e0":"markdown","46061a7a":"markdown","79d7efe7":"markdown","b0a8c4c1":"markdown","d73f34d1":"markdown","0dca4a9d":"markdown","3ec0d346":"markdown","0b7d4df9":"markdown","f3854172":"markdown","19315fcd":"markdown","4829e906":"markdown","949e67f0":"markdown","91ef2a7e":"markdown","155e3f84":"markdown","f55f379f":"markdown","a7118263":"markdown","8b50e0c2":"markdown","2a3d98d0":"markdown"},"source":{"cbac7396":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style","f49e5a70":"users_df = pd.read_csv(\"..\/input\/users.csv\")","f45a89b3":"places_df = pd.read_csv(\"..\/input\/places.csv\")","da76e211":"caracteristics_df = pd.read_csv(\"..\/input\/caracteristics.csv\", encoding=\"latin-1\")","6b9a55ea":"users_df.describe()","12ce1163":"users_df.info()","0a010a2c":"places_df.describe()","5f0ea9b5":"caracteristics_df.describe()","50b75bc6":"users_df[\"Num_Acc\"].count()","9609c000":"places_df[\"Num_Acc\"].count()","e76bd380":"caracteristics_df[\"Num_Acc\"].count()","d2033551":"users_df.head()","c35fa648":"users_df = users_df.drop([\"place\", \"catu\", \"sexe\", \"trajet\", \"secu\", \"locp\", \"actp\", \"etatp\", \"an_nais\", \"num_veh\"], axis=1)\n\nusers_df.describe()","3187926c":"users_df.grav.hist()","4b178f22":"users_df = users_df.replace(1,0)\nusers_df = users_df.replace(3,1)\nusers_df = users_df.replace(4,0)\nusers_df = users_df.replace(2,1)\n\nusers_df.grav.hist()","3e9c9ab0":"users_df = users_df.groupby(['Num_Acc'], as_index=False).max()\n\nusers_df.grav.hist()","9786c5e5":"users_df.count()","de1c2e69":"users_df.head(15)","d03fbe00":"places_df.describe()","56adab0e":"places_df = places_df.drop([\"v1\", \"voie\", \"v2\", \"pr\", \"pr1\", \"vosp\", \"prof\", \"plan\", \"lartpc\", \"larrout\", \"situ\", \"env1\"], axis=1)\n\nplaces_df.describe()","5f37d384":"places_df.info()","ac2af738":"places_df[\"catr\"].isna().sum()","5193a594":"places_df[\"catr\"] = places_df[\"catr\"].fillna(9)","ffd8d73c":"places_df[\"catr\"] = places_df[\"catr\"].astype(int)","55f381f0":"places_df[\"circ\"].isna().sum()","ec8a935a":"(places_df[\"circ\"] == 0).sum()","5a417ad8":"places_df[\"circ\"].hist()","e59b13ee":"places_df[\"circ\"] = places_df[\"circ\"].fillna(2)\nplaces_df[\"circ\"] = places_df[\"circ\"].replace(0,2)","4682b68e":"places_df[\"circ\"] = places_df[\"circ\"].astype(int)","1fefcc67":"places_df[\"nbv\"].isna().sum()","dce96c5d":"places_df[\"nbv\"] = places_df[\"nbv\"].where(places_df[\"nbv\"] < 6,0)","26960039":"(places_df[\"nbv\"] == 0).sum()","c5d8e175":"places_df[\"nbv\"].hist()","5ce88c14":"places_df[\"nbv\"] = places_df[\"nbv\"].fillna(2)\nplaces_df[\"nbv\"] = places_df[\"nbv\"].replace(0,2)","74647b0e":"places_df[\"nbv\"] = places_df[\"nbv\"].astype(int)","3af15685":"places_df[\"surf\"].isna().sum()","a0944def":"(places_df[\"surf\"] == 0).sum()","2f222a22":"places_df[\"surf\"].hist()","447aac2f":"(places_df[\"surf\"] == 1).sum()","1650e8e4":"places_df[\"surf\"] = places_df[\"surf\"].fillna(1)\nplaces_df[\"surf\"] = places_df[\"surf\"].replace(0,1)","3a23795d":"places_df[\"surf\"] = places_df[\"surf\"].astype(int)","8729fda6":"(places_df[\"infra\"] == 0).sum()","d85a1639":"places_df = places_df.drop([\"infra\"], axis=1)","2f8ebecf":"places_df.info()","ff31b681":"places_df.describe()","f5d69eba":"caracteristics_df.describe()","441b0ce5":"caracteristics_df = caracteristics_df.drop([\"an\", \"col\", \"com\", \"adr\", \"gps\", \"lat\", \"long\"], axis=1)\n\ncaracteristics_df.describe()","2a70a220":"caracteristics_df.info()","e2c17657":"caracteristics_df[\"atm\"].isna().sum()","ece238bc":"caracteristics_df[\"atm\"] = caracteristics_df[\"atm\"].fillna(1)","8d30a75d":"caracteristics_df[\"atm\"] = caracteristics_df[\"atm\"].astype(int)","f7a1fce8":"(caracteristics_df[\"int\"] == 0).sum()","ba659f60":"caracteristics_df[\"int\"] = caracteristics_df[\"int\"].replace(0,1)","47dd1fd4":"caracteristics_df[\"hrmn\"] = caracteristics_df[\"hrmn\"].div(100).apply(np.floor)","4f32e998":"caracteristics_df[\"hrmn\"] = caracteristics_df[\"hrmn\"].astype(int)","166c0f30":"caracteristics_df[\"hrmn\"].hist()","be0148c9":"caracteristics_df[\"dep\"] = caracteristics_df[\"dep\"].div(10).apply(np.floor)","924a4b91":"caracteristics_df[\"dep\"] = caracteristics_df[\"dep\"].astype(int)","36f055e0":"caracteristics_df.info()","d5297597":"caracteristics_df.describe()","7f5fa1b6":"datamerge = pd.merge(caracteristics_df, places_df, how=\"outer\", on=\"Num_Acc\")","c08b16dc":"datamerge.describe()","56f40851":"data = pd.merge(datamerge, users_df, how=\"outer\", on=\"Num_Acc\")","1094e9e8":"data = data.drop(\"Num_Acc\", axis=1)","c030de0d":"data.info()","fe1d6f91":"data.describe()","3799e8a9":"data.head(15)","9fc92cf6":"#data.to_csv(\"data.csv\")","a8bb2543":"from sklearn.model_selection import train_test_split\n\nX_training, X_test, y_training, y_test = train_test_split(data.drop(\"grav\", axis=1), data[\"grav\"], test_size=0.2, random_state=42)","b721bcb8":"X_train, X_Val, y_train, y_Val = train_test_split(X_training, y_training, test_size=0.2, random_state=42)","ff0cab66":"import time\nfrom sklearn.metrics import accuracy_score","21854734":"from sklearn.ensemble import RandomForestClassifier","1374094e":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=100,random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\nprint('Time taken :' , time.time()-t0)","d09cf6c6":"y_pred = model_rf.predict(X_Val)\nscore_rf = accuracy_score(y_Val,y_pred)\nprint('Accuracy :',score_rf)","bba4d724":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(model_rf.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')","3eeb4a73":"importances","59286486":"X_train = X_train.drop([\"surf\", \"lum\", \"atm\", \"int\", \"nbv\", \"circ\"], axis=1)\nX_Val = X_Val.drop([\"surf\", \"lum\", \"atm\", \"int\", \"nbv\", \"circ\"], axis=1)\nX_test = X_test.drop([\"surf\", \"lum\", \"atm\", \"int\", \"nbv\", \"circ\"], axis=1)","c96a7a3b":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=100,random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\ny_pred = model_rf.predict(X_Val)\nscore = accuracy_score(y_Val,y_pred)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","f46c4beb":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=100, max_depth= 5, max_features= 3, random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\ny_pred = model_rf.predict(X_Val)\nscore = accuracy_score(y_Val,y_pred)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","6febad38":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=50, max_depth= 5, max_features= 3, random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\ny_pred = model_rf.predict(X_Val)\nscore = accuracy_score(y_Val,y_pred)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","c4b4c4fa":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=7, max_depth= 15, max_features= 6, random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\ny_pred = model_rf.predict(X_Val)\nscore = accuracy_score(y_Val,y_pred)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","ca9e3dec":"t0=time.time()\nmodel_rf = RandomForestClassifier(n_estimators=7, max_depth= 15, max_features= 6, random_state=0, n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\ny_pred = model_rf.predict(X_test)\nscore = accuracy_score(y_test,y_pred)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","05ac7dad":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, roc_auc_score","3110f78b":"confusion_matrix(y_test, y_pred)","3e01b4e6":"precision_score(y_test, y_pred)","4f75d02d":"recall_score(y_test, y_pred)","54b9e69b":"y_train.hist()","55997c87":"y_scores = model_rf.predict_proba(X_test)","b3e177c5":"y_scores = y_scores[:,1]","4df02763":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_scores)","18e148a7":"def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","c847fc4c":"r_a_score = roc_auc_score(y_test, y_scores)","23cf8c23":"r_a_score","906a145c":"model_rf.predict(np.array([[12, 31, 18, 1, 13, 3]]))","57f5547a":"model_rf.predict(np.array([[7, 5, 6, 2, 26, 6]]))","403c9518":"## Merging","c2c267a4":"# Import Data","2bf4261c":"For the \"atm\" feature, I fill the missing values by 1","0aefb093":"We can remark that both \"places\" and \"caracteristics\" have the same number of instances : 839985\n\nHowever, in \"users\" we have 1876005 instances. Indeed, \"users\" describe all of the people involved in the accidents, and it is common that there is more than 1 people injured in an accident.","bad10b06":"In the \"circ\" column, there are a few missing values as well as several 0 values, which are not supposed to exist because this is a categorical feature.\n\nThat is why I plot the distribution of this feature and I can conclude that I will replace the missing and null values by 2, which is largely the major category. Indeed, the added instances represent very few in comparison.","4280ead1":"Then, in order to merge the 3 datasets, we will have to have the same number of instances, that is to say 1 line by accident.\n\nSo we have to apply a \"group by\" transformation on the entire database, grouping by the Accident ID.\n\nMoreover, when there are several instances for 1 accident, I specified that we keep the one with the maximum \"grav\" severity. Thus, we keep only the information about the maximum severe injury. keeping in mind the context of the model, this is the information that will be useful to emergency services.","d3445612":"My idea is to transform this feature in a binary form that would differentiate Unscathed and light injury (0) from Serious injury and death (1).","f296f8ef":"## Random Forest","dc4dc73f":"Similarly for the \"caracteristics\" dataset, we keep only the interesting values: \"month\", \"day of month\", \"time\", \"lighting\", \"agglomeration\", \"intersection\", \"atmospheric condition\", \"department\"","16af80e9":"Exporting the cleaned and transformed data","f687c40a":"# Validation","d931c5a2":"## Introduction","0576f948":"### Example 1:\nIf, as an emergency service, are receiving a call on the 31st of December, at 18 pm, calling to report an accident that happened out of a agglomeration, in the department 13 (around Marseille), on a departemental road.\n\nWe enter this data in our model","d5e1774b":"Before starting any preprocessing on the data, it is important to have a first visualization of its size, type, and content","e3b278c9":"The precision here means that on all the \"serious accidents\" predicted, 69% were truly serious","e08d2ee8":"These scores can be explained partially because of the fact that there is more examples of \"light injuries\" than \"serious accidents\" :","cc2c56bf":"To try on some examples what would be the predictions for 2 accidents we have here:","2817c4e0":"We improved the running time but decreased slighlty the accuracy","27349d8d":"# Evaluation","7434e6c5":"Before starting to run any ML algorithm on the data, we have to go through the preprocessing part.\nIn this part, we will keep only interesting features, and then clean the data so there is no missing or unusual value. The goal is that our data is the best possible before applying the algorithm.\n\nWe will do the first part of the preprocessing separately for each dataset.","93b94515":"## Users","effd30d1":"We can conclude, with the accuracy score to take into consideration, that the injuries will be only light.\n\nSimilarly, knowing this information can save some resources that can be more helpful to save other people that need it.","c2df01cf":"In the database \"users\", I will only keep the feature \"grav\" (severity of the accident), which will be the target column.\n\nI can thus drop all the others features.","4bd63517":"For the \"nbv\" feature, we count the missing values as well as the outliers (0 & above 6) and replace them by the value 2, which is the major value from far.","f4ff6d96":"## ROC Curve","52b84c10":"Now we have and clear and nice \"caracteristics\" dataset","169b69a8":"## Feature selection","1826b406":"The \"grav\" features is actually categorical data. 4 categories: \n1: Unscathed\n2: Killed\n3: Serious injury\n4: Light injury","e8a75053":"Then, importing data is the next necessary step.\nI chose to only import the datasets \"users\", \"characteristics\" and \"places\" because I will not use any data contained in the \"holidays\" and \"vehicles\" databases. ","ebc586e4":"## Confusion matrix, Precision & Recall","dee5a356":"### Example 2:\n\nHowever, if we receive a call from someone on the 5th of July, at 6 am, for an accident that happened in an agglomeration, out of an intersection, in the department 26, on a parking lot.\n\nWe enter the data in our model","3560348b":"# Predict Accidents","c29d5527":"### Remark\n\nI know that usually, it is better to split between the Train and the Test set before doing the preprocessing, in order not to bias the model with information from the Test data.\n\nBut here, the only preprocessing that I am applying to the data is filling missing values, cleaning outliers, and calculations to format the data. No information relative to the data is used so the model will not be biased.","63c2c0f5":"Thanks to the description of the data, we can see that there are several missing values and outliers.","2bb95431":"Our goal will now be to improve the accuracy and running time of this model thanks to feature selection and hyperparameter tuning.","7a741481":"We train the Random Forest algorithm with the new dataset.","851eb1c1":"We can predict, knowing the accuracy score, that this accident will involve a user with serious injuries or death. \n\nThis information can be helpful to warn the appropriate services such as the nearest hospital of the incoming of wounded people.","fa1def81":"For the \"hrmn\", we have a bit more transformation to do.\n\nThe data is displayed with 4 numbers, 2 for the hour and 2 for the min.\n\nIn my opinion, we can reduce the precision and create larger categories. I will keep only the information about the hour (2 first numbers).\n\nSo we can compute all the data in the column to keep only the 2 first digits: we divide by 100 and then floor the number.","239582e9":"So we read:\n\nTN: 74188 <br>\nFN : 28377 <br>\nFP : 20127 <br>\nTP : 45305 <br>","a5728ed9":"In the \"infra\" feature, almost 90% of the data is equal to 0. The missing values are too important so I decided to drop the entire column.","924f57fe":"## Caracteristics","a0f99643":"The \"catr\" missing value is filled with a 9 which represents the \"other\" categorie, which seems the most appropriate.","7376b115":"As detailled in the report, my goal is to build a model that could be used in the future to help emergency services be more efficient for road accident. Thus, I want to keep only features that could be used as input data known by this services to predict the severity of the accident.\n\nIt means that the features kept are only the information known thanks to the description of the accident given when calling emergency or other information that can be infered from this.","5334865b":"The recall here means that on all the serious accidents that happened, 61% have been predicted correctly\n\nThat is the measure that really interests us the most because we need to provide the needed services up to the severity of the accidents.","a750ec3b":"We can measure the importance of each feature in the contribution to the prediction","5c5463a3":"# End","93f05126":"Similarly, in the \"surf\" column, we replace missing and null values by the most frequent value, 1.","b88fa614":"# Model","4e665be3":"## Places","6f727d2f":"With our final optimization, we reach a really better running time while increasing a bit the accuracy, from the original model","40b7b1ce":"We have quite the same transformation to make with the departments, which are usually given in 2 digits except for here were they are given in 3 digits to make a difference between the overseas department.\n\nI suggest that we skip on this precision and keep the department on a range 1 to 97, with 2 digits.","cdf5f5e0":"We can now drop this feature that is useless for the prediction","46061a7a":"After viewing all the existing kernels, I wanted to find an interesting thing to predict.\n\nThanks to the EDA kernels, I realized that a very important feature was present. It is the feature called \u201cgrav\u201d in the users.csv dataset, which represents the severity of the injure of each person involved in the accident.\n\nThus, my goal will be to predict the feature** \u201cgrav\u201d**, knowing all the other characteristics of an accident.\n\nThis feature is categorical, and contains 4 categories: 1: Unscathed 2: Killed 3: Hospitalized wounded 4: Light injury\n\nAfter briefly thinking about the algorithms available for this kind of problem, that is to say supervised learning with categorical data, I thought about simplifying the problem to predict binary values. Thus, simpler algorithms can be used to solve the problem.\n\nThat is why this feature will be transformed into Light Injury (1 & 4) or Serious Injury and Death (2 & 3) for the binary values. I will detail this process in the next part.\n\nMoreover, I wanted this predictive model to be used in a specific context, so I thought about a scenario for the cases it is needed. My idea was that this model could be used by emergency services when they receive an emergency call to report an accident. Thanks to the information given by the caller, which is usually the place of the accident and maybe more details, as well as using information that can be inferred from this, they would be able to predict the severity of the accident.\n\nIndeed, knowing this kind of information before sending emergency staff and equipment to the place of the accident could help actually knowing more precisely the need of resources for this accident. Moreover, it would be possible to anticipate by contacting in advance the nearest hospitals to warn them about a serious case coming. By matching the resources sent with the actual need, it also helps letting more resources available for other emergencies that can happen at the same time.\n\nSo that is why for creating this model I will only use the features that can be known by emergency services with a simple call of emergency. Of course, features like time, lighting and climatic conditions, are supposed to be known. I will drop all the features that give information that I think could not be known before going on the place of the accident or after analysis. Of course, except \u201cgrav\u201d, which is the target feature.","79d7efe7":"We import metrics to measure the relevance of each RF Classifier","b0a8c4c1":"## Testing for accuracy","d73f34d1":"# Import Libraries","0dca4a9d":"## Hyperparameter tuning","3ec0d346":"# Predictions","0b7d4df9":"Then we can conclude that we can delete the lowest importance features: \"surf\", \"lum\", \"atm\", \"int\", \"nbv\", \"circ\"","f3854172":"And now we have a clean \"places\" dataset","19315fcd":"In the \"places\" dataset, I will keep only the features: \"road categorie\", \"traffic regime\", \"number of traffic lanes\", \"surface condition\", and \"infrastructure\"\n\nThese are the only ones that seem relevant in the context of the model and its future usage.","4829e906":"# Preprocessing","949e67f0":"Similarly, I replace the null values by 1","91ef2a7e":"The Accuracy means that the model provides true predictions 71% of the times.\n\nHaving almost the same results than with the validation set is reassuring about the fact that our model is not overfitting.\n\nIt is an interesting measure to evaluate our model but in the context, other measures might be useful as well.","155e3f84":"Now we see that there is 839985 instances in the database, the same number as in the others","f55f379f":"# Split Train\/Test\/Validation\n\nIt is now time to split our dataset between Training part and Testing part, before trying to find a model.\n\nI choose to take a ratio of 80\/20 for TRAINING\/TEST.","a7118263":"After all the preprocessing done on each of the 3 datasets, and as they all have the same number of rows, we can merge them into a single dataset, with the feature \"Num_acc\", the Accident ID, as the primary key","8b50e0c2":"We start by importing the libraries that will be needed all along the notebook. Several libraries may be imported later for a specific use.","2a3d98d0":"Similarly, in the Training set I split between the actual Train set and the Validation set, with a 80\/20 ratio as well"}}