{"cell_type":{"5cc34d22":"code","52f02078":"code","8c7d66f7":"code","2c30baa0":"code","a0ddfec9":"code","1c8bda85":"code","9e578950":"code","fd3cd989":"code","a13ba373":"code","38fee9b5":"code","85debfc6":"code","39534519":"code","2f491f79":"code","adac13e4":"code","bedcda62":"code","dc0cee50":"code","7c0b7c4d":"code","679e5aec":"code","8f2259a9":"code","5fd15d82":"code","8bc74a64":"code","548bfee8":"code","807689f4":"code","d775995f":"code","07d7553f":"code","1d8b994b":"code","d9506261":"code","d4b19c4c":"code","c2dc0f5a":"code","617f0e10":"code","9340ad42":"code","d7977a9d":"code","7f236840":"code","25d3964d":"code","559f83c3":"code","a566e2d8":"code","7eebec00":"code","a6df2849":"code","e5b2d4a2":"code","508c3cf6":"code","d61e591a":"code","d039617c":"code","8552cda3":"code","ee72fac9":"code","93e89f95":"code","e87e9f49":"code","28044b29":"code","4d6fba91":"code","753d9170":"code","106877e4":"code","1d97f571":"code","a855cfb7":"code","c2084ea0":"code","69181bb8":"code","ebd22969":"code","80714907":"code","a13da798":"code","f988082e":"code","c959c05c":"code","e03528d4":"code","a9be1c4b":"code","0fd5e9f4":"code","06d8aafb":"code","373344c1":"code","02bcdbe5":"code","61c69644":"code","a7f9b795":"code","4e2dcdc2":"code","eac8829f":"code","671546f8":"code","e801284d":"markdown","69c33e95":"markdown","c7f97825":"markdown","026f11b4":"markdown","6b830b3d":"markdown","9582d22c":"markdown","f92a0b78":"markdown","b08b27c7":"markdown","a9fe0156":"markdown","3181da67":"markdown","28870cb9":"markdown","6448bde8":"markdown","1768ad2e":"markdown","f1f6a33c":"markdown","3a936ba7":"markdown","aa87ce04":"markdown","e1f49872":"markdown","0f2ed78d":"markdown","2986d4d3":"markdown","a2b08798":"markdown","c2500aff":"markdown","fa0691e4":"markdown","49e389cb":"markdown","df1d432a":"markdown"},"source":{"5cc34d22":"# Importing the required libraries\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n\n#visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom datetime import datetime\nfrom scipy.stats import skew\nimport time\nimport copy\nimport math\n\nfrom sklearn.feature_selection import SelectKBest,chi2,f_classif,mutual_info_classif  # Feature Engineering\nfrom sklearn.model_selection import train_test_split                                  # Splitting the dataset into training & testing\nfrom imblearn.over_sampling import SMOTE                                              # For Handling Data Imbalance\nfrom sklearn.preprocessing import OrdinalEncoder                                      # Label Encoding library \nfrom sklearn.utils import resample                                                    # Data Resampling Library\n\n\n# Regression & Classification Models\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom skopt import BayesSearchCV\n\n\n#from tune_sklearn import TuneSearchCV\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n\n# Model Metric libraries\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,recall_score,precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve","52f02078":"# Importing the dataset\n\ndf_train=pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf_test=pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")","8c7d66f7":"df_train.head()","2c30baa0":"print(\"Training Dataset Info:-\")\nprint(\"\\n\")\nprint(df_train.info())","a0ddfec9":"#Checking for missing data\n\nprint(\"Missing Data in the Training Dataset:-\")\nprint(\"\\n\")\nprint(df_train.isnull().sum())\n\nprint(\"\\n\")\n\n#Checking for missing data (In Percentage(%))\n\nprint(\"Missing Data in the Training Dataset (In %):-\")\nprint(\"\\n\")\nprint((df_train.isnull().sum()\/len(df_train))*100)","1c8bda85":"# Let's drop the Missing values.\n\nmissing_value_fields=['gender','enrolled_university','education_level','major_discipline','experience','company_size','company_type',\n                      'last_new_job']\n\ndf_train=df_train.dropna()\n\nprint(\"Training Dataset:-\")\n\nprint(\"\\n\")\nprint(df_train.isnull().sum())","9e578950":"#Gender\n\ndf_train.gender.value_counts()","fd3cd989":"# Dependancy of Gender on Target Variable\n\nGender_crosstab=pd.crosstab(index=df_train['gender'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nGender_crosstab[\"Job Changers' Percentage (%)\"]=round(Gender_crosstab[1]\/(Gender_crosstab[0]+Gender_crosstab[1])*100,2)\nprint(Gender_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Gender_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=12,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=12)\nplt.xlabel('Gender',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Gender',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","a13ba373":"#Revlevant Experience\n\ndf_train.relevent_experience.value_counts()","38fee9b5":"# Dependancy of Relevant Experience on Target Variable\n\nRelevant_Exp_crosstab=pd.crosstab(index=df_train['relevent_experience'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nRelevant_Exp_crosstab[\"Job Changers' Percentage (%)\"]=round(Relevant_Exp_crosstab[1]\/(Relevant_Exp_crosstab[0]+\n                                                                                      Relevant_Exp_crosstab[1])*100,2)\nRelevant_Exp_crosstab","85debfc6":"# Let's plot the result and visualize.\n\ng=Relevant_Exp_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Revelent Experience',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Revelent Experience',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","39534519":"#Enrolled University\n\ndf_train.enrolled_university.value_counts()","2f491f79":"# Dependancy of Enrolled University on Target Variable\n\nUniversity_crosstab=pd.crosstab(index=df_train['enrolled_university'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nUniversity_crosstab[\"Job Changers' Percentage (%)\"]=round(University_crosstab[1]\/(University_crosstab[0]+University_crosstab[1])*100,2)\nUniversity_crosstab","adac13e4":"# Let's plot the result and visualize.\n\ng=University_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('University Enrollment',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t University Enrollment',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","bedcda62":"# Education Level\n\ndf_train.education_level.value_counts()","dc0cee50":"# Dependancy of Education on Target Variable\n\nEducation_crosstab=pd.crosstab(index=df_train['education_level'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nEducation_crosstab[\"Job Changers' Percentage (%)\"]=round(Education_crosstab[1]\/(Education_crosstab[0]+Education_crosstab[1])*100,2)\nprint(Education_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Education_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Education Level',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Education Level',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","7c0b7c4d":"# Major Discipline\n\ndf_train.major_discipline.value_counts()","679e5aec":"# Dependancy of Major Discipline on Target Variable\n\nMajor_crosstab=pd.crosstab(index=df_train['major_discipline'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nMajor_crosstab[\"Job Changers' Percentage (%)\"]=round(Major_crosstab[1]\/(Major_crosstab[0]+Major_crosstab[1])*100,2)\nMajor_crosstab\n\n\n# Let's plot the result and visualize.\n\ng=Major_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Major Discipline',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Major Discipline',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","8f2259a9":"# Experience\n\ndf_train.experience.value_counts()","5fd15d82":"# Dependancy of Experience on Target Variable\n\nExperience_in_years_crosstab=pd.crosstab(index=df_train['experience'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nExperience_in_years_crosstab[\"Job Changers' Percentage (%)\"]=round(Experience_in_years_crosstab[1]\/(Experience_in_years_crosstab[0]+\n                                                                                                    Experience_in_years_crosstab[1])*100,2)\nprint(Experience_in_years_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Experience_in_years_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(19,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 20), \n                   textcoords = 'offset points',\n                   rotation='vertical')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Experience',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Experience',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","8bc74a64":"#Company Size\n\ndf_train.company_size.value_counts()","548bfee8":"# Dependancy of Company Size on Target Variable\n\nCompany_Size_crosstab=pd.crosstab(index=df_train['company_size'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nCompany_Size_crosstab[\"Job Changers' Percentage (%)\"]=round(Company_Size_crosstab[1]\/(Company_Size_crosstab[0]+\n                                                                                      Company_Size_crosstab[1])*100,2)\nprint(Company_Size_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Company_Size_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(19,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Company Size',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Company Size',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","807689f4":"#Last New Job\n\ndf_train.last_new_job.value_counts()","d775995f":"# Dependancy of Last New Job on Target Variable\n\nLast_New_Job_crosstab=pd.crosstab(index=df_train['last_new_job'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nLast_New_Job_crosstab[\"Job Changers' Percentage (%)\"]=round(Last_New_Job_crosstab[1]\/(Last_New_Job_crosstab[0]+\n                                                                                      Last_New_Job_crosstab[1])*100,2)\nprint(Last_New_Job_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Last_New_Job_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Last new Job',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Last new Job',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","07d7553f":"# Let's analyze the No of training hrs of the Employees.\n\nplt.figure(figsize=(18,5))\nsns.histplot(df_train.training_hours,bins=350)\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Training Hours',fontsize=16)\nplt.ylabel('Distribution',fontsize=16)\nplt.title('Histogram plot of Employee Training Hours',fontsize=18)","1d8b994b":"# Checking for outliers.\n\nplt.figure(figsize=(12,5))\nsns.boxplot(x=df_train.training_hours)\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Training Hours',fontsize=16)\nplt.ylabel('Distribution',fontsize=16)\nplt.title('Box Plot of Employee Training Hours',fontsize=18)","d9506261":"x_random = np.random.normal(0, 2, 10000)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('No of Hours',fontsize=14)\nplt.ylabel('Probability Distribution',fontsize=14)\n\nax1.hist(x_random, bins='auto')\nax1.set_title('probability density (Random Data)',fontsize=18)\n\nax2.hist(df_train.training_hours, bins='auto')\nax2.set_title('Training Hours',fontsize=18)","d4b19c4c":"# Let's partition the data into \"Normal\", \"Extensive\" & \"Rigorous\" Training\n\nlabels=['Normal Training','Extensive Training','Rigorous Training']\n\ndf_train['Training_length']=pd.cut(df_train.training_hours,[0,100,200,350],right=False,labels=labels)","c2dc0f5a":"# Dependancy of Training Length on Target Variable\n\nTraining_length_crosstab=pd.crosstab(index=df_train['Training_length'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nTraining_length_crosstab[\"Job Changers' Percentage (%)\"]=round(Training_length_crosstab[1]\/(Training_length_crosstab[0]+\n                                                                                      Training_length_crosstab[1])*100,2)\nprint(Training_length_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=Training_length_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=16)\nplt.yticks(fontsize=16)\nplt.xlabel('Training Length',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t Training Length',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","617f0e10":"# Let's analyze the Probability Distribution of City development index through a histogram.\n\nplt.figure(figsize=(18,6))\nsns.histplot(x=df_train.city_development_index,kde=True)\nplt.xticks(rotation = 0,fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('City Development Index',fontsize=16)\nplt.ylabel('Distribution',fontsize=16)\nplt.title('Histogram plot of City Development Index',fontsize=18)","9340ad42":"# Let's round off the values of CDI to 3 decimals.\n\ndf_train.city_development_index=df_train.city_development_index.apply(lambda x: round(x,3))","d7977a9d":"plt.figure(figsize=(20,6))\nsns.barplot(x='city',y='city_development_index',data=df_train.sort_values(by='city_development_index',ascending=False))\nplt.xticks(rotation = 90,fontsize=10)\nplt.yticks(fontsize=16)\nplt.xlabel('Cities',fontsize=16)\nplt.ylabel('City Development Index',fontsize=16)\nplt.title('Cities w.r.t City Development Index Level',fontsize=18)","7f236840":"def update_CDI(index):\n    if(index >= 0 and index <= 0.5):\n        return \"Less Development\"\n    elif(index > 0.5 and index <= 0.7):\n        return \"Moderate Development\"\n    elif(index > 0.7):\n        return \"High Developement\"","25d3964d":"# Let's categorize the City development level into Less, Moderate & High Development.\n\ndf_train[\"City_development_level\"]=df_train.city_development_index.apply(update_CDI)\ndf_train.City_development_level.value_counts()","559f83c3":"# Dependancy of City Development Level on Target Variable\n\nCDI_crosstab=pd.crosstab(index=df_train['City_development_level'],columns=df_train['target']).sort_values(by=1.0,ascending=False)\nCDI_crosstab[\"Job Changers' Percentage (%)\"]=round(CDI_crosstab[1]\/(CDI_crosstab[0]+CDI_crosstab[1])*100,2)\nprint(CDI_crosstab)\n\n\n# Let's plot the result and visualize.\n\ng=CDI_crosstab.drop([\"Job Changers' Percentage (%)\"], axis=1).plot(kind='bar',stacked=False,figsize=(20,6))\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=14,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=16)\nplt.yticks(fontsize=16)\nplt.xlabel('City Development Index Level',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('HR Analysis w.r.t City Development Index Level',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Job Change Likelihood',title_fontsize=16,prop={\"size\":14})","a566e2d8":"# Label Encoding\n\ndf_train.education_level=df_train.education_level.map({\"Primary School\":0,\"High School\":1,\"Graduate\":2,\"Masters\":3,\"Phd\":4})\n\ndf_train.experience=df_train.experience.map({\"<1\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"10\":10,\"11\":11,\"12\":12,\"13\":13,\n                                             '14':14,\"15\":15,\"16\":16,\"17\":17,\"18\":18,\"19\":19,\"20\":20,\">20\":21})\n\ndf_train.City_development_level=df_train.City_development_level.map({\"Less Development\":0,\"Moderate Development\":1,\"High Developement\":2})\n\ndf_train.gender=df_train.gender.map({'Male':0,'Female':1,'Other':2})\n\ndf_train.enrolled_university=df_train.enrolled_university.map({'Full time course':0,'Part time course':1,'no_enrollment':2})\n\ndf_train.major_discipline=df_train.major_discipline.map({'STEM':0,'Humanities':1,'Business Degree':2,'Arts':3,'Other':4,'No Major':5})\n\ndf_train.company_size=df_train.company_size.map({'10000+':0,'5000-9999':1,'1000-4999':2,'500-999':3,'100-500':4,'50-99':5,'10\/49':6,\n                                                '<10':7})\n\ndf_train.company_type=df_train.company_type.map({'Pvt Ltd':0,'Public Sector':1,'Funded Startup':2,'Early Stage Startup':3,'NGO':4,\n                                                'Other':5})\n\ndf_train.last_new_job=df_train.last_new_job.map({'>4':0,'4':1,'3':2,'2':3,'1':4,'never':5})\n\ndf_train.Training_length=df_train.Training_length.map({'Rigorous Training':0,'Extensive Training':1,'Normal Training':2})\n\n\n\n# Let's apply one-hot Encoding to relevent_experience & city Features.\n\ndf_train=pd.get_dummies(data=df_train,columns=['relevent_experience','city'],drop_first=True)\n\n\n# let's drop the Employee ID field\n\ndf_train.drop(['enrollee_id'],axis=1,inplace=True)","7eebec00":"X_temp=df_train.drop('target',axis=1)\ny_temp=df_train.target\nprint(X_temp.columns)","a6df2849":"X_temp.rename(columns={'company_size_<10':'company_size_lessthan10','last_new_job_>4':'last_new_job_greaterthan4'},inplace=True)","e5b2d4a2":"Best_Params=SelectKBest(score_func=chi2, k='all')\nBest_Params.fit(X_temp,y_temp)","508c3cf6":"df_scores=pd.DataFrame(Best_Params.scores_)\ndf_columns=pd.DataFrame(X_temp.columns)\ndf_scores_evaluation=pd.concat([df_scores,df_columns],axis=1)\ndf_scores_evaluation.columns=['Scores','Features']\ndf_scores_evaluation=df_scores_evaluation.nlargest(131,'Scores')\nprint(df_scores_evaluation)","d61e591a":"X_new=copy.deepcopy(X_temp)\nprint(len(X_temp.columns))\nprint(len(X_new.columns))\nprint(X_new.columns)\n\ny_new=copy.deepcopy(y_temp)","d039617c":"# Let's split the data into Training & Test sets.\n\nX_train,X_test,y_train,y_test=train_test_split(X_new,y_new,test_size=0.20,random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8552cda3":"# Checking Target Class Imbalance\n\nplt.figure(figsize=(8,5))\ng=sns.countplot(x=\"target\",data=df_train)\nfor p in g.patches:\n    g.annotate(format(p.get_height()), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center',\n                   fontsize=12,\n                   xytext = (0, 6), \n                   textcoords = 'offset points',\n                   rotation='horizontal')\nplt.xticks(rotation = 0,fontsize=16)\nplt.yticks(fontsize=18)\nplt.xlabel('Target Classes',fontsize=16)\nplt.ylabel('Count',fontsize=16)\nplt.title('Target Class Imbalance',fontsize=18)\nplt.legend(['Not Looking for a Job','Looking for a Job'],title='Target Class',title_fontsize=15,prop={\"size\":14})","ee72fac9":"# SMOTE\n\nprint(\"Counts of Label 0 before Oversampling: {}\".format(sum(y_train == 0.0)))\n\nprint(\"Counts of Label 1 before Oversampling: {}\".format(sum(y_train == 1.0)))\n\nsmote=SMOTE(random_state=0)\n\nX_train_smote, y_train_smote=smote.fit_resample(X_train,y_train)\n\nprint(\"Shape of X_train after Oversampling: {}\".format(X_train_smote.shape))\n\nprint(\"Shape of y_train after Oversampling: {}\".format(y_train_smote.shape))\n\nprint(\"Counts of Label 0 after Oversampling: {}\".format(sum(y_train_smote==0.0)))\n\nprint(\"Counts of Label 1 after Oversampling: {}\".format(sum(y_train_smote==1.0)))","93e89f95":"# Let's use resample method from sklearn library to Oversample the minority class.\n\nX=pd.concat([X_train,y_train],axis=1)\n\nEmp_NotLeaving=X[X.target == 0]\nEmp_Leaving=X[X.target == 1]\n\nupsampled_Emp_Leaving=resample(Emp_Leaving,\n                       replace=True,\n                       n_samples=len(Emp_NotLeaving),\n                       random_state=42)\n\nupsampled_Data=pd.concat([Emp_NotLeaving,upsampled_Emp_Leaving])\n\n\ny_train_upsampled=upsampled_Data.target\nX_train_upsampled=upsampled_Data.drop('target',axis=1)\n\n\nprint(len(y_train_upsampled == 1))\nprint(len(y_train_upsampled == 0))","e87e9f49":"# Let's convert all the Dataset feature names into list so that we can pass the desired set of features to the models separately for \n#training. \n\ndf_scores_evaluation=df_scores_evaluation.Features.to_list()","28044b29":"# From the SelectKBest method, I have obtained the scores for all features & from trial & error technique, I have determined the \n# Number of features to pass to each model for obtaining the best results.\n# So, I'll be dropping the respective number of features below.\n\nlr_ridge_features=df_scores_evaluation[55:]    # dropping all features except the top 55.\nsvm_features=df_scores_evaluation[90:]\ndt_features=df_scores_evaluation[100:]\nbc_lgbm_features=df_scores_evaluation[30:]\nrf_features=df_scores_evaluation[120:]\n\nprint(len(lr_ridge_features))\nprint(len(svm_features))\nprint(len(dt_features))\nprint(len(bc_lgbm_features))\nprint(len(rf_features))","4d6fba91":"# I'm dropping the above mentioned features explicitely from the dataset & passing the best set of features to each model.\n\nlr_ridge_X_train=copy.deepcopy(X_train_upsampled)\nlr_ridge_X_test=copy.deepcopy(X_test)\nlr_ridge_X_train.drop(lr_ridge_features,axis=1,inplace=True)\nlr_ridge_X_test.drop(lr_ridge_features,axis=1,inplace=True)\n\nsvm_X_train=copy.deepcopy(X_train_smote)\nsvm_X_test=copy.deepcopy(X_test)\nsvm_X_train.drop(svm_features,axis=1,inplace=True)\nsvm_X_test.drop(svm_features,axis=1,inplace=True)\n\ndt_X_train=copy.deepcopy(X_train_upsampled)\ndt_X_test=copy.deepcopy(X_test)\ndt_X_train.drop(dt_features,axis=1,inplace=True)\ndt_X_test.drop(dt_features,axis=1,inplace=True)\n\nbc_lgbm_X_train=copy.deepcopy(X_train_smote)\nbc_lgbm_X_test=copy.deepcopy(X_test)\nbc_lgbm_X_train.drop(bc_lgbm_features,axis=1,inplace=True)\nbc_lgbm_X_test.drop(bc_lgbm_features,axis=1,inplace=True)\n\nrf_X_train=copy.deepcopy(X_train_upsampled)\nrf_X_test=copy.deepcopy(X_test)\nrf_X_train.drop(rf_features,axis=1,inplace=True)\nrf_X_test.drop(rf_features,axis=1,inplace=True)","753d9170":"# LOGISTIC REGRESSION MODEL\n\nlr_model=LogisticRegression()\n\nt0=time.time()\n\n# Hyper-parameter tuning\n#solvers = ['newton-cg', 'lbfgs', 'liblinear', 'saga']\n#penalty = ['l2']\n#lr_c = [100, 10, 1.0, 0.1, 0.01]\n\nsolvers = ['newton-cg']\npenalty=['l2']\nlr_c = [10]\n\nlr_grid = dict(solver=solvers,penalty=penalty,C=lr_c)\n\n# cross-validation using Repeated Stratified K-fold method.\nlr_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Grid Search CV method loops through the different hyper parameters determining the optimal values.\nlr_grid_search = GridSearchCV(estimator=lr_model, param_grid=lr_grid,verbose=1, cv=lr_cv, n_jobs=-1, scoring='accuracy',error_score=0)\n\n# Fitting the Model to the Dataset.\nlr_grid_result=lr_grid_search.fit(lr_ridge_X_train,y_train_upsampled)\n\nprint(\"training time:\", round(time.time()-t0, 3), \"s\")\n\n# returns the best hyper parameters.\n#r_grid_result.best_params_","106877e4":"# Making predictions using our model.\nlr_grid_predictions=lr_grid_result.predict(lr_ridge_X_test)","1d97f571":"print(\"LOGISTIC REGRESSION Model Performance Metrics:\")\nprint(classification_report(y_test,lr_grid_predictions))\n\n#print(classification_report(Test_Label,lr_grid_result_test_data_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,lr_grid_predictions))\n#print(confusion_matrix(Test_Label,lr_grid_result_test_data_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,lr_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\n#fpr, tpr, _ = roc_curve(Test_Label,lr_grid_result_test_data_predictions)\n\nplt.plot(fpr, tpr, marker='.', label='Logistic Regression')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n#ns_probs = [0 for _ in range(len(Test_Label))]\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)\n\n#ns_auc = roc_auc_score(Test_Label, ns_probs)\n\nlr_auc = roc_auc_score(y_test, lr_grid_predictions)\n\n#lr_auc = roc_auc_score(Test_Label, lr_grid_result_test_data_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('LOGISTIC REGRESSION: ROC AUC=%.3f' % (lr_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, lr_grid_predictions)\nlr_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('Precision-Recall AUC Score: %.3f' % lr_auc_score)\nplt.show()","a855cfb7":"rc_model=RidgeClassifier()\n\nt0=time.time()\n\n#alpha = [0.2, 0.4, 0.6, 0.8, 1.0]\n\nalpha=[0.8]\n\nrc_grid = dict(alpha=alpha)\n\nrc_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nrc_grid_search = GridSearchCV(estimator=rc_model, param_grid=rc_grid,verbose=1, cv=rc_cv, scoring='accuracy',error_score=0)\n\nrc_grid_result=rc_grid_search.fit(lr_ridge_X_train,y_train_upsampled)\n\nprint(\"training time:\", round(time.time()-t0, 3), \"s\")\n\n#rc_grid_result.best_params_","c2084ea0":"rc_grid_predictions=rc_grid_result.predict(lr_ridge_X_test)","69181bb8":"print(\"RIDGE CLASSIFIER Model Performance Metrics:\")\nprint(classification_report(y_test,rc_grid_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,rc_grid_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,rc_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\nplt.plot(fpr, tpr, marker='.', label='RIDGE CLASSIFIER')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)             \nrc_auc = roc_auc_score(y_test, rc_grid_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('RIDGE CLASSIFIER: ROC AUC=%.3f' % (rc_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, rc_grid_predictions)\nrc_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('Precision-Recall AUC Score: %.3f' % rc_auc_score)\nplt.show()","ebd22969":"svm_model=LinearSVC()\n\nt0=time.time()\n\n#svm_C=[0.8,1,2,4]\n\nsvm_C=[2]\nsvc_class_weight=['balanced']\ndual=[False]\n\nsvm_grid=dict(C=svm_C,class_weight=svc_class_weight,dual=dual)\n\nsvm_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nsvm_grid_search = GridSearchCV(estimator=svm_model, param_grid=svm_grid, cv=svm_cv, scoring='accuracy',error_score=0)\n\nsvm_grid_result=svm_grid_search.fit(svm_X_train,y_train_smote) \n\nprint(\"training time:\", round(time.time()-t0, 3), \"s\")\n\n#svm_grid_result.best_params_","80714907":"svm_grid_predictions=svm_grid_result.predict(svm_X_test)","a13da798":"print(\"SVM CLASSIFIER Model Performance Metrics:\")\nprint(classification_report(y_test,svm_grid_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,svm_grid_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,svm_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\nplt.plot(fpr, tpr, marker='.', label='SVM CLASSIFIER')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)             \nsvm_auc = roc_auc_score(y_test, svm_grid_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('SVM CLASSIFIER: ROC AUC=%.3f' % (svm_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, svm_grid_predictions)\nsvm_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('Precision-Recall AUC Score: %.3f' % svm_auc_score)\nplt.show()","f988082e":"dtc_model=DecisionTreeClassifier()\n\nt0=time.time()\n\n#dtc_criterion=['gini','entropy']\n#dtc_max_depth=[14,16,18,20,24,28]\n#dtc_max_features = ['sqrt', 'log2']\n#dtc_min_samples_split = [6,8,10,12]\n#dtc_min_samples_leaf = [1, 2, 5, 10]\n\ndtc_criterion=['gini']\ndtc_max_depth=[12]\ndtc_max_features = ['sqrt']\ndtc_min_samples_split = [8]\ndtc_min_samples_leaf = [1]\ndtc_class_weight=['balanced']\n\n\ndtc_grid=dict(criterion=dtc_criterion,max_depth=dtc_max_depth,max_features=dtc_max_features,min_samples_split=dtc_min_samples_split,\n             min_samples_leaf=dtc_min_samples_leaf,class_weight=dtc_class_weight)\n\ndtc_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\ndtc_grid_search = GridSearchCV(estimator=dtc_model, param_grid=dtc_grid, cv=dtc_cv,n_jobs=-1,verbose=1, scoring='accuracy',error_score=0)\n\ndtc_grid_result=dtc_grid_search.fit(dt_X_train,y_train_upsampled) \n\nprint(\"training time:\", round(time.time()-t0, 3), \"s\")\n\n#dtc_grid_result.best_params_","c959c05c":"dtc_grid_predictions=dtc_grid_result.predict(dt_X_test)","e03528d4":"print(\"DECISION TREE CLASSIFIER Model Performance Metrics:\")\nprint(classification_report(y_test,dtc_grid_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,dtc_grid_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,dtc_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\nplt.plot(fpr, tpr, marker='.', label='DECISION TREE CLASSIFIER')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)             \ndtc_auc = roc_auc_score(y_test, dtc_grid_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('DECISION TREE CLASSIFIER: ROC AUC=%.3f' % (dtc_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, dtc_grid_predictions)\ndtc_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('AUC Score: %.3f' % dtc_auc_score)\nplt.show()","a9be1c4b":"#bc_model=BaggingClassifier()\n\nbc_model=BalancedBaggingClassifier()\n\n#print(bc_model.get_params().keys())\n\n#bc_max_samples=[0.8,1.0,1.2,1.4]\n#bc_max_features=[12,14,16,18]\n#bc_base_estimator=KNeighborsClassifier()\n#bc_bootstrap=[True,False]\n#bc_bootstrap_features=[True,False]\n\nt0=time.time()\n\nbc_n_estimators = [1000]\nbc_max_samples=[1.0]\nbc_max_features=[12]\nbc_bootstrap=[False]\nbc_bootstrap_features=[False]\n\nbc_grid = dict(n_estimators=bc_n_estimators,max_samples=bc_max_samples,max_features=bc_max_features,bootstrap=bc_bootstrap,\n              bootstrap_features=bc_bootstrap_features)\n\nbc_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\nbc_grid_search = GridSearchCV(estimator=bc_model, param_grid=bc_grid, n_jobs=-1, verbose=1, cv=bc_cv, scoring='accuracy',error_score=0)\n\nbc_grid_result=bc_grid_search.fit(bc_lgbm_X_train,y_train_smote)\n\nprint(\"training time:\", round(time.time()-t0, 3), \"s\")\n\nbc_grid_result.best_params_","0fd5e9f4":"bc_grid_predictions=bc_grid_result.predict(bc_lgbm_X_test)","06d8aafb":"print(\"BAGGING CLASSIFIER Model Performance Metrics:\")\nprint(classification_report(y_test,bc_grid_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,bc_grid_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,bc_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\nplt.plot(fpr, tpr, marker='.', label='BAGGING CLASSIFIER')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)             \nbc_auc = roc_auc_score(y_test, bc_grid_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('BAGGING CLASSIFIER: ROC AUC=%.3f' % (bc_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, bc_grid_predictions)\nbc_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('AUC Score: %.3f' % bc_auc_score)\nplt.show()","373344c1":"rfc_model=RandomForestClassifier()\n\nt0=time.time()\n\n\n#rfc_max_features = ['sqrt', 'log2']\n#rfc_min_samples_split = [1,2,4,6,8]\n#rfc_min_samples_leaf = [1, 2, 5, 10]\n#rfc_max_depth = [10,12,14,16]\n\n\nrfc_n_estimators = [1000]\nrfc_max_features = ['sqrt']\nrfc_max_depth = [12]\nrfc_min_samples_split = [2]\nrfc_min_samples_leaf = [1]\nrfc_class_weight=['balanced']\n\n\n#rfc_grid = dict(n_estimators=rfc_n_estimators,max_features=rfc_max_features,class_weight=rfc_class_weight,max_depth=rfc_max_depth,\n#               min_samples_split=rfc_min_samples_split,min_samples_leaf=rfc_min_samples_leaf)\n\nrfc_grid = dict(n_estimators=rfc_n_estimators,max_features=rfc_max_features,class_weight=rfc_class_weight,max_depth=rfc_max_depth,\n               min_samples_split=rfc_min_samples_split,min_samples_leaf=rfc_min_samples_leaf)\n\nrfc_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\nrfc_grid_search = GridSearchCV(estimator=rfc_model, param_grid=rfc_grid, n_jobs=-1, verbose=3, cv=rfc_cv, scoring='accuracy',error_score=0)\n\nrfc_grid_result=rfc_grid_search.fit(rf_X_train,y_train_upsampled)\n\nprint(\"training time:\", round(time.time()-t0, 3)\/60)\n\n#rfc_grid_result.best_params_","02bcdbe5":"rfc_grid_predictions=rfc_grid_result.predict(rf_X_test)","61c69644":"print(\"RANDOM FOREST CLASSIFIER Model Performance Metrics:\")\nprint(classification_report(y_test,rfc_grid_predictions))\n\n#print(classification_report(Test_Label,rfc_grid_Test_Data_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,rfc_grid_predictions))\n\n#print(confusion_matrix(Test_Label,rfc_grid_Test_Data_predictions))\n\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,rfc_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\n#fpr, tpr, _ = roc_curve(Test_Label,rfc_grid_Test_Data_predictions)    \n\nplt.plot(fpr, tpr, marker='.', label='RANDOM FOREST CLASSIFIER')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n#ns_probs = [0 for _ in range(len(Test_Label))]\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)\n\n#ns_auc = roc_auc_score(Test_Label, ns_probs)\n\nrfc_auc = roc_auc_score(y_test, rfc_grid_predictions)\n\n#rfc_auc = roc_auc_score(Test_Label, rfc_grid_Test_Data_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('RANDOM FOREST CLASSIFIER: ROC AUC=%.3f' % (rfc_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, rfc_grid_predictions)\nrfc_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('AUC Score: %.3f' % rfc_auc_score)\nplt.show()","a7f9b795":"lgbc_model=LGBMClassifier(is_balance=False)\n\nt0=time.time()\n\n#lgbc_n_estimators=[600,700,800,900]\n#lgbc_max_depth=[10,12,14,16]\n#lgbc_num_leaves=[8,10,12,14]\n#lgbc_learning_rate=[0.0001, 0.001, 0.01, 0.1, 1.0]\n#lgbc_boosting_type=['gbdt', 'dart', 'goss']\n#min_child_samples=[100,200,300,400,500]\n#min_child_weight=[1e-2, 1e-1, 1, 1e1, 1e2]\n\nlgbc_n_estimators=[800]\nlgbc_max_depth=[14]\nlgbc_num_leaves=[14]\nlgbc_learning_rate=[0.1]\nlgbc_boosting_type=['gbdt']\nobjective=['binary']\nmin_child_samples=[100]\nmin_child_weight=[10]\n\n\nlgbc_grid=dict(n_estimators=lgbc_n_estimators,max_depth=lgbc_max_depth,num_leaves=lgbc_num_leaves,learning_rate=lgbc_learning_rate,\n              boosting_type=lgbc_boosting_type,objective=objective,min_child_samples=min_child_samples,min_child_weight=min_child_weight)\n\nlgbc_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n\nlgbc_grid_search = GridSearchCV(estimator=lgbc_model, param_grid=lgbc_grid, n_jobs=-1,verbose=3, cv=lgbc_cv, scoring='accuracy',error_score=0)\n\nlgbc_grid_result=lgbc_grid_search.fit(bc_lgbm_X_train,y_train_smote)\n\nprint(\"training time:\", round(time.time()-t0, 3)\/60)\n\n#lgbc_grid_result.best_params_","4e2dcdc2":"lgbc_grid_predictions=lgbc_grid_result.predict(bc_lgbm_X_test)","eac8829f":"print(\"GRADIENT BOOSTING Model Performance Metrics:\")\nprint(classification_report(y_test,lgbc_grid_predictions))\n\nprint(\"CONFUSION MATRIX :\")\nprint(confusion_matrix(y_test,lgbc_grid_predictions))\nprint(\"\\n\")\n\nplt.figure(figsize=(12,6))\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier') # Let's define a no skill (Dummy) Classifier for reference.\n\nfpr, tpr, _ = roc_curve(y_test,lgbc_grid_predictions)             # passing the target labels & Model Predictions to the roc_curve method. \n\nplt.plot(fpr, tpr, marker='.', label='EXTREME GRADIENT BOOSTING MODEL')  # Plotting the obtained results.\n\nns_probs = [0 for _ in range(len(y_test))]       # generating predictions from the no skill (Dummy) classifier for reference.\n\n# Calculating Area under Curve for the No Skill & the trained Model.\nns_auc = roc_auc_score(y_test, ns_probs)\n          \nlgbc_auc = roc_auc_score(y_test, lgbc_grid_predictions)\n\nprint('NO SKILL CLASSIFIER: ROC AUC=%.3f' % (ns_auc))\nprint('GRADIENT BOOSTING MODEL: ROC AUC=%.3f' % (lgbc_auc))\n\nplt.xlabel('False Positive Rate',fontsize=16)\nplt.ylabel('True Positive Rate',fontsize=16)\nplt.title(\"ROC Curve\",fontsize=18)\nplt.legend(prop={'size':12})\nplt.show()\n\n\n# Let's plot the Precision-Recall Curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, lgbc_grid_predictions)\nlgbc_auc_score = auc(recall, precision)\n\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.title('Precision-Recall Curve',fontsize=18)\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\nplt.plot(recall, precision, marker='.')\nprint('AUC Score: %.3f' % lgbc_auc_score)\nplt.show()","671546f8":"# Model Names\nModel_Names=['LOGISTIC REGRESSION', 'RIDGE CLASSIFIER', 'LINEAR SVM CLASSIFIER', 'DECISION TREE CLASSIFIER', 'BAGGING CLASSIFIER',\n            'RANDOM FOREST CLASSIFIER', 'LIGHT GBM']\n\n# DataFrame Index values\nIndex=['Accuracy (%)','Precision (%)','Recall (%)','F1 Score (%)', 'ROC AUC Score', 'P-R AUC Score']\n\n# Model Prediction values\nModel_Predictions=[lr_grid_predictions,rc_grid_predictions,svm_grid_predictions,dtc_grid_predictions,bc_grid_predictions,\n                   rfc_grid_predictions,lgbc_grid_predictions]\n\nROC_scores=[lr_auc,rc_auc,svm_auc,dtc_auc,bc_auc,rfc_auc,lgbc_auc]\n\nPR_scores=[lr_auc_score,rc_auc_score,svm_auc_score,dtc_auc_score,bc_auc_score,rfc_auc_score,lgbc_auc_score]\n\n# Model Metrics methods\nmodel_metrics=[accuracy_score,precision_score,recall_score,f1_score]\n\n# DataFrame Initialisation\nModel_Metrics_Comparison=pd.DataFrame(columns=Model_Names,index=Index)\n\n# Let's fill the dataframe with the model metrics values of all the trained models above.\nfor index,metric in zip(range(0,4),model_metrics):\n    for model_name,model_prediction in zip(Model_Names,Model_Predictions):\n        Model_Metrics_Comparison[model_name].values[index]=metric(y_test,model_prediction)*100\n        \nfor model_name, rfc_score,pr_score in zip(Model_Names,ROC_scores,PR_scores):\n        Model_Metrics_Comparison[model_name].values[4]=rfc_score\n        Model_Metrics_Comparison[model_name].values[5]=pr_score\n\n# Metric values in Percentage (%).\nModel_Metrics_Comparison","e801284d":"* Most of the Job Changers have not enrolled in a University!.","69c33e95":"**Let's analyze the relationship of each feature with the target variable with the help of crosstab to gain insights.**","c7f97825":"* Most of them trying to search for new Jobs are college graduates and the employees having a primary school level of education are the least Job changers.","026f11b4":"* Best model from the above in terms of accuracy is random Forest followed by bagging & Ridge Classifier.\n\n* Based on Precision & recall (which I tried so much to improve but couldn't :( So someone please help), Random forest is some what   better than the rest followed by Ridge & Bagging Classifier.\n\n* Based on the ROC AUC Scores, again Random forest, Ridge or Logistic Regression could be considered.\n\n* But when dealing with an Imbalanced Dataset such as this, we should actually consider the precision-recall scores which accurately         incicate False +ve & False -ve values predicted by the models. \n\n* Based on the Precision-Recall AUC Scores, Random forest is the best model followed by Ridgr & bagging classifier.","6b830b3d":"# Data Visualization & EDA","9582d22c":"* Most of the Job Changers are having relevent experience.","f92a0b78":"* **Most of the Employees live in cities with CDI greater than 0.9 which indicates High development.**","b08b27c7":"* Let's use SelectKBest function to obtain scores of all the Dataset features.","a9fe0156":"* **As the Target label is highly Imbalanced, we need to upsample the lower class (which is 1.0 in this case).**\n* **If we downsample the Majority class, we might lose significant data.**\n* **We can also use SMOTE (Synthetic Minority Oversampling Technique) which creates synthetic (duplicate) data points taken from the Majority class and can be very effective.**\n* **We can apply both resampling methods and compare the results.**","3181da67":"* **Most of the Job changers live in Moderately developed cities.**","28870cb9":"* Most Employees have undergone about 50 hrs of training and some of them have undergone Extensive training of more than 100 hrs.","6448bde8":"**If you like my Kernel, Please Upvote. Please feel free to provide suggestions in the comments which helps me to improve myself. Thank you :)**","1768ad2e":"* Employees having 20+ years of Experience are the most Job changers and Surprisingly,there are less Job changers with Experience between 10 to 20 years.","f1f6a33c":"* Employees who had chosen STEM as their major descipline during college are the majority job changers.","3a936ba7":"* Most of the Job Changing Employees have undergone training for less than 100 hrs.","aa87ce04":"* **As there are missing values present in both Train & Test datasets, we need to replace the missing values with more suitable & efficient values.**\n* **If we replace missing values with mean or median, it affects the data distribution & may affect the Model performance during model building.**\n* **So, it's best to use \"ffill\" & \"bfill\" methods as the data will be distributed evenly without affecting the distribution.**","e1f49872":"# Feature Selection","0f2ed78d":"* Let's analyze the HR Analytics dataset by performing EDA & find out useful insights & information which would help us to predict that which all employees   might be searching for a new job.\n\n* Later, Let's Train different models & compare their metrics.","2986d4d3":"* Employees having 1 year Experience in the Current Company are the most Job Changers.","a2b08798":"* **Low City development for CDI scores between 0.0 and 0.5.**\n* **Medium City development for CDI scores between 0.5 and 0.8.**\n* **High City development for CDI scores between 0.8 and 1.0.**","c2500aff":"* Most of the job searchers are in the male category followed by the females and others.","fa0691e4":"* **Almost All cities have a CDI above 0.4.**\n* **Let's Categorize the CDI to \"Low Development\", \"Medium Development\" & \"High Development\" for clear understanding.**","49e389cb":"# Data Pre-processing","df1d432a":"* **The Training_hours feature is negatively Skewed.**\n* **But, after applying log & square root transformations, the data was almost positively skewed but, the Score of the feature went down.**\n* **So, it's better to keep it in it's original form**"}}