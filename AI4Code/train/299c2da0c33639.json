{"cell_type":{"d40624d3":"code","52fa7553":"code","0e891272":"code","a0ec8954":"code","3ca2f64e":"code","7a8a2c3d":"code","1e4e2069":"code","44b8c87e":"code","95aaefc9":"code","d7583f81":"markdown","63f027bc":"markdown","c6dcd377":"markdown","6f2a04fc":"markdown"},"source":{"d40624d3":"import numpy as np \nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","52fa7553":"def cal_mi_score(X,y):\n    X = X.copy()\n    X.drop('id', axis=1, inplace=True)\n    for col in X.select_dtypes(['object']):\n        X[col],unique = X[col].factorize()\n    discrete_features = X.dtypes == int\n    mi_scores = mutual_info_regression(X,y,discrete_features=discrete_features, random_state=42)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.figure(figsize=(10,7))\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \ndf_eda = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntarget = df_eda.pop('target')\nscores = cal_mi_score(df_eda,target)\nplot_mi_scores(scores)","0e891272":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nfor n in range(1,6):\n    data = pd.read_csv(f\"..\/input\/custompreddata\/train_pred_{n}.csv\")\n    df = df.merge(data, on=\"id\", how=\"left\")\n    \nall_test_df = []\nfor n in range(1,6):\n    data = pd.read_csv(f'..\/input\/custompreddata\/test_pred_{n}.csv')\n    df_test = df_test.merge(data, on=\"id\", how=\"left\")\n","a0ec8954":"# Meta model \n\nuseful_features = [col for col in df.columns if col.startswith('pred')]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    x_valid = df[df.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_valid.id.values.tolist()\n    \n    y_train = x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    params = {'learning_rate': 0.07803392035787837, \n              'reg_lambda': 1.7549293092194938e-05, \n              'reg_alpha': 20.68267919457715, \n              'subsample': 0.8031450486786944, \n              'colsample_bytree': 0.170759104940733, \n              'max_depth': 3}\n   \n    model = XGBRegressor(random_state = fold,\n#                          tree_method=\"gpu_hist\",\n#                          gpu_id=0,\n#                          predictor=\"gpu_predictor\",\n                         n_estimators=5000,\n                         n_jobs = -1,\n                         **params)\n    model.fit(x_train,\n              y_train,\n              early_stopping_rounds=300,\n              eval_set=[(x_valid, y_valid)],\n              verbose=1000,\n             )\n    \n    preds_valid = model.predict(x_valid)\n    preds_test = model.predict(x_test)\n    final_test_predictions.append(preds_test)\n    final_valid_predictions.update(dict(zip(valid_ids,preds_valid)))\n    rmse = mean_squared_error(y_valid,preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold,rmse)\n    \nprint(np.mean(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\",\"pred_4\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_4.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = ['id', 'pred_4']\nsample_submission.to_csv('level1_test_pred_4.csv', index=False)","3ca2f64e":"# meta model 2 \n\nuseful_features = [col for col in df.columns if col.startswith('pred')]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    x_valid = df[df.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_valid.id.values.tolist()\n    \n    y_train = x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    model = RandomForestRegressor(n_estimators=500, max_depth=3, n_jobs=-1,random_state=42)\n    model.fit(x_train,\n              y_train,)\n    \n    preds_valid = model.predict(x_valid)\n    preds_test = model.predict(x_test)\n    final_test_predictions.append(preds_test)\n    final_valid_predictions.update(dict(zip(valid_ids,preds_valid)))\n    rmse = mean_squared_error(y_valid,preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold,rmse)\n    \nprint(np.mean(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\",\"pred_5\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_5.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = ['id', 'pred_5']\nsample_submission.to_csv('level1_test_pred_5.csv', index=False)","7a8a2c3d":"# meta model 3\n\nuseful_features = [col for col in df.columns if col.startswith('pred')]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    x_valid = df[df.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_valid.id.values.tolist()\n    \n    y_train = x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    params = {\n        'max_depth':3,\n        'colsample_bytree': 0.4,  \n        'learning_rate': 0.1,  \n        'min_child_weight': 1,  \n        'reg_alpha': 10.0,  \n        'reg_lambda': 1.0,  \n        'subsample': 0.7266579209776919,\n        'random_state': 42\n    }\n    model = lgb.LGBMRegressor(**params)\n    model.fit(x_train, y_train,)\n    \n    preds_valid = model.predict(x_valid)\n    preds_test = model.predict(x_test)\n    final_test_predictions.append(preds_test)\n    final_valid_predictions.update(dict(zip(valid_ids,preds_valid)))\n    rmse = mean_squared_error(y_valid,preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold,rmse)\n    \nprint(np.mean(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\",\"pred_6\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_6.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = ['id', 'pred_6']\nsample_submission.to_csv('level1_test_pred_6.csv', index=False)","1e4e2069":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nfor n in range(1,4):\n    d = pd.read_csv(f'..\/input\/level-data\/level1_train_pred_{n}.csv')\n    df = df.merge(d, on=\"id\", how=\"left\")\n\n\nfor n in range(1,4):\n    d = pd.read_csv(f'..\/input\/level-data\/level1_test_pred_{n}.csv')\n    df_test = df_test.merge(d, on=\"id\", how=\"left\")\n    \n    \nfor n in range(4,7):\n    d = pd.read_csv(f'..\/input\/custompreddata\/level1_train_pred_{n}.csv')\n    df = df.merge(d, on=\"id\", how=\"left\")\n\n\nfor n in range(4,7):\n    d = pd.read_csv(f'..\/input\/custompreddata\/level1_test_pred_{n}.csv')\n    df_test = df_test.merge(d, on=\"id\", how=\"left\")  ","44b8c87e":"useful_features = [f'pred_{n}' for n in range(1,7) if n not in [5]]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    enc = preprocessing.StandardScaler()\n    df[useful_features] = enc.fit_transform(df[useful_features])\n    df_test[useful_features] = enc.transform(df_test[useful_features])\n    \n    model = LinearRegression()\n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","95aaefc9":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","d7583f81":"### Why using Mutual Information to find any relation between feature and target ?\nAns: Actually mutual information can predict any kind of relationship while correlation just predict the linear relation \n     between features and target\n     \n **You can see that `cont12` and `cont10` are more related to target than other features**","63f027bc":"### - **I used 3 different models for stacking**\n>     - XGBRegressor\n>     - RandomForestRegressor\n>     - LGBRegressor","c6dcd377":"> **I uploaded my blending + stacking data into 2 datasets**\n> **`level-data` and**\n> **`custompreddata`**","6f2a04fc":"# ***Blending + Stacking***\n### - **I used 5 models with different hyperparameters and features for blending**\n>     - Ordinal encoder + standardization\n>     - Ordinal encoder + standardization (diff hyperparameters)\n>     - target encoding + standardization \n>     - one hot encoding + standardization\n>     - LGBRegressor"}}