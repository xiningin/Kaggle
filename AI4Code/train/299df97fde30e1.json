{"cell_type":{"eba67f2d":"code","e5b79a09":"code","d6bd9bca":"code","999b16bc":"code","1d3cf8fe":"code","e6fd4f1d":"code","f34c2be2":"code","63460fd0":"code","bacd96f9":"code","58aa3e8a":"code","36da59aa":"code","782c6f51":"code","f2ee5ab6":"code","fc6d0864":"code","17796071":"code","9af0cd39":"code","b4091a88":"code","7b2d6a2b":"code","82c00eb6":"code","edc3f226":"code","3930a274":"code","02b93fc9":"code","d8decf64":"code","87b7fe25":"code","b2f923c8":"code","817faa6f":"code","9f15fe8e":"code","0a48f558":"code","55fdd0f3":"code","3d519a16":"code","a3d8b458":"code","4bbcbe77":"code","bbbf438c":"code","c5db6e80":"code","5504fa02":"code","140109a7":"code","17602852":"code","492e5bcd":"code","d75d425e":"code","93ac0989":"code","ad688baf":"code","8b285c3a":"code","27ba913a":"code","b3320c44":"code","88a9fe01":"code","f46f4674":"code","260130ea":"code","6d5f0e79":"code","fdcfbe69":"code","812fe32d":"code","1a38ba46":"code","46e7f419":"code","13b35e80":"code","f8a56451":"code","d90b26ab":"code","9227fbbe":"markdown","3918dfa4":"markdown","8fd7bacb":"markdown","dccff2b4":"markdown","c08aea92":"markdown","a33c13d4":"markdown","2bee1a9d":"markdown","91d6acb0":"markdown","2bef8824":"markdown","a347635c":"markdown","40256c21":"markdown"},"source":{"eba67f2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\n\n#For Data Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\n#For Applying Nlp On Claim Description Column\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5b79a09":"train=pd.read_csv('\/kaggle\/input\/actuarial-loss-estimation\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/actuarial-loss-estimation\/test.csv')","d6bd9bca":"#Entire Dataset\ndf_all = pd.concat([train.assign(role=\"train\"), test.assign(role=\"test\")])","999b16bc":"df_all.isnull().sum()\/len(df_all)*100","1d3cf8fe":"df_all.groupby(df_all['MaritalStatus'])['UltimateIncurredClaimCost'].agg(['mean','median','count'])","e6fd4f1d":"df_all[df_all['MaritalStatus'].isnull()]['UltimateIncurredClaimCost'].mean()","f34c2be2":"#Replacing nan with S\n\ndf_all['MaritalStatus']=df_all['MaritalStatus'].fillna('S')","63460fd0":"#Each Type Of Claim and its freq\n\ndf_all['ClaimDescription'].value_counts()","bacd96f9":"# Claim Cost varies wrt claim descr. \n\ndf_all[df_all.role=='train'].groupby(['ClaimDescription'])['UltimateIncurredClaimCost'].median().sort_values(ascending=False)","58aa3e8a":"# Number Of words in each claim description\n\ndf_all['word_count']=df_all['ClaimDescription'].apply(lambda x:len(x))","36da59aa":"#  word count statistics in each claim description\n\ndf_all['word_count'].describe()","782c6f51":"stops = stopwords.words('english')\n\ndef text_clean(claim):\n    \n    \n    #Converting to Lower Case\n    claim=claim.lower()\n    \n    #Getting List Of Words\n    claim=claim.split()\n    \n    #Removing Stop Words(Words which do not add any information like =is,are,I etc)\n    claim=[word for word in claim if word not in stops]\n    \n    #Stemming the word(words like playing ,played are replaced with play)\n    porter_stemmer = PorterStemmer()\n    claim=[porter_stemmer.stem(word) for word in claim]\n    \n    \n    return claim","f2ee5ab6":"df_all['ClaimDescriptionClean']=df_all['ClaimDescription'].apply(lambda x:' '.join(text_clean(x)))","fc6d0864":"corpus = df_all['ClaimDescriptionClean']\nlst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))","17796071":"#Most common pairs of words that occur together in Claim Description\n\nplt.figsize=((25,30))\ndic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\ndtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n                      columns=[\"Word\",\"Freq\"])\ndtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n                   string for string in x) )\ndtf_bi.set_index(\"Word\").iloc[:30,:].sort_values(by=\"Freq\").plot(kind=\"barh\",figsize=(10,10))\nplt.title('Bigrams')\n                  \nplt.show()","9af0cd39":"# single words that frequently occur in claim descr.\n\nplt.figsize=((25,30))\ndic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 1))\ndtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n                      columns=[\"Word\",\"Freq\"])\ndtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n                   string for string in x) )\ndtf_bi.set_index(\"Word\").iloc[:30,:].sort_values(by=\"Freq\").plot(kind=\"barh\",figsize=(10,10))\nplt.title('Unigrams')\n                  \nplt.show()","b4091a88":"#Creating new features based on claim descr\n\ncd_features=['foreign bodi','lacer','tissu injuri','strain','lower back','knee','wrist','eye','hand','fractur','sprain','knife','slip','finger','tendon','synov','conjunct']\n\nfor feature in cd_features:\n    df_all['CD_'+'_'.join(feature.split())]=df_all['ClaimDescriptionClean'].apply(lambda x:1 if feature in x else 0)\n    \n#Creating Features based on Claim description col.   \ncd_features=['CD_'+'_'.join(feature.split()) for feature in cd_features]\n","7b2d6a2b":"df_all[['DateTimeOfAccident', 'DateReported']].tail()","82c00eb6":"df_all['DateTimeOfAccident']=pd.to_datetime(df_all['DateTimeOfAccident'])\ndf_all['DateReported']=pd.to_datetime(df_all['DateReported'])\n\ndf_all['Rep_Acc_Diff']=df_all['DateReported']-df_all['DateTimeOfAccident']\ndf_all['Rep_Acc_Diff']=df_all['Rep_Acc_Diff'].apply(lambda x:str(x)[:2])\ndf_all['Rep_Acc_Diff']=df_all['Rep_Acc_Diff'].astype('int')\n\ndf_all['Accident_Month']=df_all['DateTimeOfAccident'].dt.month\ndf_all['Accident_Year']=df_all['DateTimeOfAccident'].dt.year\ndf_all['Accident_Day']=df_all['DateTimeOfAccident'].dt.day\ndf_all['Accident_Hour']=df_all['DateTimeOfAccident'].dt.hour\ndf_all['Accident_Time']=df_all['DateTimeOfAccident'].dt.time\n\n\n\ndf_all['Reported_Month']=df_all['DateReported'].dt.month\ndf_all['Reported_Year']=df_all['DateReported'].dt.year\ndf_all['Reported_Day']=df_all['DateReported'].dt.day\n","edc3f226":"train=df_all[df_all.role=='train']","3930a274":"cd_feat_claim_median=pd.DataFrame({feature:train[train[feature]==1]['UltimateIncurredClaimCost'].median() for feature in cd_features},index=['MedianClaim']\n).T.sort_values(by='MedianClaim')\n\ncd_feat_claim_median.plot(kind='barh',figsize=(10,10))\n\nplt.xlabel('Median Claim Amt')\nplt.title('Claim Description & Median Claim Amt')\nplt.show()","02b93fc9":"date_cols=[col for col in train.columns if col.split('_')[-1] in ['Month','Year','Day','Time']]","d8decf64":"fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,8))\nfig.suptitle('Time Year And Median Claim Amt')\nplt.ylabel('Median Claim Amt')\n\ntrain.groupby(train['Accident_Time'])['UltimateIncurredClaimCost'].median().plot(ax=ax[0])\ntrain.groupby(train['Accident_Year'])['UltimateIncurredClaimCost'].median().plot(ax=ax[1])\n\nplt.show()","87b7fe25":"fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,8))\nfig.suptitle('Day Month And Median Claim Amt')\nplt.ylabel('Median Claim Amt')\n\ntrain.groupby(train['Accident_Day'])['UltimateIncurredClaimCost'].median().plot(kind='bar',ax=ax[0])\ntrain.groupby(train['Accident_Month'])['UltimateIncurredClaimCost'].median().plot(kind='bar',ax=ax[1])\n\nplt.show()","b2f923c8":"fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,8))\nfig.suptitle('Day Month And Median Claim Amt')\nplt.ylabel('Median Claim Amt')\n\ntrain.groupby(train['Reported_Day'])['UltimateIncurredClaimCost'].median().plot(kind='bar',ax=ax[0])\ntrain.groupby(train['Reported_Month'])['UltimateIncurredClaimCost'].median().plot(kind='bar',ax=ax[1])\n\nplt.show()","817faa6f":"plt.figure(figsize=(16,8))\ntrain.groupby(train['Rep_Acc_Diff'])['UltimateIncurredClaimCost'].median().plot()\nplt.title('Difference between Reported and Date Of Accident & Claim Amt')\nplt.show()","9f15fe8e":"categoric=['Gender','MaritalStatus','PartTimeFullTime']\nfig, ax = plt.subplots(1, 3, sharey=True, figsize=(16,8))\nfig.suptitle('Count For Each Category')\n\nfor i,col in enumerate(categoric):\n    train[col].value_counts().plot(kind='bar',ax=ax[i])","0a48f558":"categoric=['Gender','MaritalStatus','PartTimeFullTime']\nfig, ax = plt.subplots(1, 3, sharey=True, figsize=(16,8))\nfig.suptitle('Median Claim Amt')\n\nfor i,col in enumerate(categoric):\n    train.groupby(train[col])['UltimateIncurredClaimCost'].median().plot(kind='bar',ax=ax[i])","55fdd0f3":"dep=['DependentChildren','DependentsOther']\nfig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,8))\nfig.suptitle('Dependents And Median Claim Amt')\n\nfor i,col in enumerate(dep):\n    sns.boxplot(x=train[col],y=np.log(train['UltimateIncurredClaimCost']),ax=ax[i])","3d519a16":"#Outlier Analysis of InitialIncurredCalimsCost\n\nsns.boxplot(df_all['InitialIncurredCalimsCost'])\nplt.title('InitialIncurredCalimsCost Before Outlier Treatment')\nplt.show()","a3d8b458":"#Capping Outliers\ndf_all.loc[df_all['InitialIncurredCalimsCost']>=df_all['InitialIncurredCalimsCost'].quantile(.95),'InitialIncurredCalimsCost']=df_all['InitialIncurredCalimsCost'].quantile(.95)","4bbcbe77":"sns.boxplot(df_all['InitialIncurredCalimsCost'])\nplt.title('InitialIncurredCalimsCost After Outlier Treatment')\nplt.show()","bbbf438c":"sns.boxplot(df_all['UltimateIncurredClaimCost'])\nplt.title('Claim Before Outlier Treatment')\nplt.show()","c5db6e80":"# Outlier Treatment\n\ntrain.loc[train['UltimateIncurredClaimCost']>=train['UltimateIncurredClaimCost'].quantile(.95),'UltimateIncurredClaimCost']=train['UltimateIncurredClaimCost'].quantile(.95)","5504fa02":"#After Outlier Treatment\n\nsns.boxplot(train['UltimateIncurredClaimCost'])\nplt.title('Claim After Outlier Treatment')\nplt.show()","140109a7":"train=df_all[df_all.role=='train']\ntest=df_all[df_all.role=='test']","17602852":"train_id=train.ClaimNumber\ntest_id=test.ClaimNumber\n\ntrain.drop(['ClaimNumber','DateTimeOfAccident', 'DateReported','ClaimDescription','role', 'word_count',\n       'ClaimDescriptionClean','Accident_Time'],axis=1,inplace=True)\n\ntest.drop(['ClaimNumber','DateTimeOfAccident', 'DateReported','ClaimDescription','role', 'word_count',\n       'ClaimDescriptionClean','Accident_Time'],axis=1,inplace=True)","492e5bcd":"train=pd.get_dummies(train)\ntest=pd.get_dummies(test)","d75d425e":"train.drop(['Gender_U','MaritalStatus_U','PartTimeFullTime_P'],axis=1,inplace=True)\ntest.drop(['MaritalStatus_U','PartTimeFullTime_P'],axis=1,inplace=True)","93ac0989":"####  Train test Split of train data\nfrom sklearn.model_selection import  train_test_split","ad688baf":"X=train.drop(['UltimateIncurredClaimCost'],axis=1)\nY=train['UltimateIncurredClaimCost']","8b285c3a":"X_train,X_test,y_train,y_test=train_test_split(X,Y,train_size=0.7)","27ba913a":"import xgboost","b3320c44":"#Training on default params\nmodel = xgboost.XGBRegressor() \nmodel.fit(X_train,y_train)","88a9fe01":"model.predict(X_test)","f46f4674":"from sklearn.metrics import mean_absolute_error","260130ea":"mean_absolute_error(y_test,model.predict(X_test))","6d5f0e79":"parameters={'colsample_bytree':[0.4],\n                 'gamma':[0],                 \n                 'learning_rate':[0.07,0.09,0.1],\n                 'max_depth':[2,3,4],\n                 'n_estimators':[1000,2000,3000],                                                                   \n                 'reg_alpha':[0.7,0.75,0.8],\n                 'reg_lambda':[0.45,0.8,1],\n                 'subsample':[0.6],\n                 }","fdcfbe69":"#from sklearn.model_selection import RandomizedSearchCV","812fe32d":"#reg_clf=RandomizedSearchCV(estimator=model,param_distributions=parameters,cv=3,n_jobs=-1,scoring='neg_mean_absolute_error',verbose=100)","1a38ba46":"#mean_absolute_error(y_test,reg_clf.best_estimator_.predict(X_test))","46e7f419":"test=test[X.columns]","13b35e80":"test.shape","f8a56451":"\nsubmission = pd.DataFrame.from_dict({\n    'ClaimNumber': test_id,\n    'UltimateIncurredClaimCost': model.predict(test)})\nsubmission.to_csv(\"submission.csv\", index=False)","d90b26ab":"submission.describe()","9227fbbe":"### ANALYSING TARGET COLUMN","3918dfa4":"#### 2.DATE COLUMNS","8fd7bacb":"### TRAIN DATA AND TEST DATA","dccff2b4":"###  CLAIM DESCRIPTION COLUMN","c08aea92":"## MISSING VALUE IMPUTATION","a33c13d4":"## MY SUBMISSION","2bee1a9d":"reg_clf.fit(X_train,y_train)","91d6acb0":"### DATE TIME COLUMNS","2bef8824":"#### 1.CLAIM DESCRIPTION ","a347635c":" ##  EXPLORATORY DATA ANALYSIS","40256c21":"**Analysing The Marital Status Column which contains null values.**\n1. First we find the mean and median UltimateIncurredClaimCost for various sub categories of Marital Status.\n2. Then compare the median and mean of UltimateIncurredClaimCost that correspond rows where MaritalStatus is missing.\n3. We then observe that Missing Values likely correspond to Singles."}}