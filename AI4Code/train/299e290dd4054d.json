{"cell_type":{"6c02905a":"code","5b2691bf":"code","c6c41ebb":"code","64748378":"code","c5b6c332":"code","38698767":"code","bae9d7a3":"code","91d3186f":"code","db488a4d":"code","6bb84504":"markdown","dc4ead86":"markdown","10bef8af":"markdown"},"source":{"6c02905a":"import numpy as np\nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5b2691bf":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c6c41ebb":"def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n    # Calculate global mean on the train data\n    global_mean = train[target].mean()\n    # Group by the categorical feature and calculate its properties\n    train_groups = train.groupby(categorical)\n    category_sum = train_groups[target].sum()\n    category_size = train_groups.size()\n\n    # Calculate smoothed mean target statistics\n    train_statistics = (category_sum + global_mean * alpha) \/ (category_size + alpha)\n    \n    # Apply statistics to the test data and fill new categories\n    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n    return test_feature.values","64748378":"def train_mean_target_encoding(train, target, categorical, alpha=5):\n    # Create 5-fold cross-validation\n    from sklearn.model_selection import KFold\n    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n    train_feature = pd.Series(index=train.index)\n    \n    # For each folds split\n    for train_index, test_index in kf.split(train):\n        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n      \n        # Calculate out-of-fold statistics and apply to cv_test\n        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n        \n        # Save new feature for this particular fold\n        train_feature.iloc[test_index] = cv_test_feature       \n    return train_feature.values","c5b6c332":"def mean_target_encoding(train, test, target, categorical, alpha=5):\n  \n    # Get the train feature\n    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n  \n    # Get the test feature\n    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n    \n    # Return new features to add to the model\n    return train_feature, test_feature","38698767":"train_pclass_enc, test_pclass_enc = mean_target_encoding(train,test,'Survived','Pclass')","bae9d7a3":"train['Pclass_enc'] = train_pclass_enc\ntest['Pclass_enc'] = test_pclass_enc","91d3186f":"train.drop('Pclass', axis=1,inplace=True)\ntest.drop('Pclass', axis=1,inplace=True)","db488a4d":"display(train.head(3))\ndisplay(test.head(3))","6bb84504":"## Limitations of Target Encoding\n\nTarget encoding is dependent on the distribution of the target which means target encoding requires careful validation as it can be prone to overfitting. This method is also dataset-specific and will only show significant improvements some of the time.","dc4ead86":"## Target Encoding\n\nFor machine learning algorithms, categorical data can be extremely useful. However, in its original form, it is unrecognizable to most models. In order to solve this problem, we can use different \u201cencoding\u201d techniques to make our categorical data legible.","10bef8af":"One encoding technique that kagglers most used is target encoding using mean."}}