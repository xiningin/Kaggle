{"cell_type":{"03f71d25":"code","3b538376":"code","f7f9b3cc":"code","bd6c637c":"code","da218f0f":"code","c3881f31":"code","4f767d73":"code","3dd6d7cd":"code","be02d104":"code","1b4cea89":"code","094ba454":"code","9d578928":"code","081fcba1":"code","95326fa7":"code","15a278ec":"code","58c8b79b":"code","3e578b6e":"code","89130be3":"markdown","685614cf":"markdown","3b4af4fb":"markdown","a02dc738":"markdown"},"source":{"03f71d25":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_moons\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom numpy import linalg\nfrom scipy.spatial import distance\n","3b538376":"#Load the dataset\ndf = pd.read_csv('..\/input\/diabetes.csv')\n\n#Print the first 5 rows of the dataframe.\ndf.head()","f7f9b3cc":"#Let's observe the shape of the dataframe.\ndf.shape","bd6c637c":"#Let's create numpy arrays for features and target\nX = df.drop('Outcome',axis=1).values\ny = df['Outcome'].values","da218f0f":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split","c3881f31":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42, stratify=y)","4f767d73":"class MyKNeighborsClassifier:\n\n    def __init__(self, k,  metric = 'euclidean'):\n        self.k = k\n        self.metric = metric\n\n    def euclidean_dist(self, array1, array2):\n        array1 = np.array(array1)\n\n        array2 = np.array(array2)\n        \n        return linalg.norm(array1 - array2)\n    \n    def manhattan_dist(self, array1, array2):\n    \n        array1 = np.array(array1)\n\n        array2 = np.array(array2)\n        return distance.cityblock(array1, array2)\n\n\n    def k_neighbors(self, test_row):\n        distances = []\n        for i in range(len(self.X_train)):\n            if self.metric == 'euclidean':\n                distance = self.euclidean_dist(test_row, self.X_train[i])\n            else:\n                distance = self.manhattan_dist(test_row, self.X_train[i])\n            distances.append((distance, self.y_train[i]))\n        distances.sort()\n        return distances[:self.k]\n\n\n    def get_nn(self):\n\n        self.X_train = np.array(self.X_train)\n\n        self.X_test = np.array(self.X_test)\n\n        self.y_train = np.array(self.y_train)\n\n        neighbors = []\n\n        for j in range(len(self.X_test)):\n\n            neighbors.append(self.k_neighbors(self.X_test[j]))\n\n        return neighbors\n\n\n    def vote_count(self, lst):\n\n        lst_count = dict()\n\n        for element in lst:\n\n            if element in lst_count:\n\n                lst_count[element] += 1\n\n            else:\n\n                lst_count[element] = 1\n\n        return lst_count\n\n\n    def fit(self, X_train, y_train):\n\n        self.X_train = X_train\n\n        self.y_train = y_train\n\n\n    def predict(self, X_test):\n\n        self.X_test = X_test\n\n        nbrs = self.get_nn()\n\n        predictions = []\n\n        for row in nbrs:\n\n            dist, labels = zip(*row)\n\n            label_dict = self.vote_count(labels)\n\n            predictions.append(max(label_dict, key = label_dict.get))\n        \n        return predictions\n\n    def evaluate(self, y_pred, y_test):\n\n        count = 0\n\n        for act, pred in zip(y_pred, y_test):\n            if act == pred:\n                count += 1\n\n        return count \/ len(y_test)","3dd6d7cd":"k = 4\n\nknn = MyKNeighborsClassifier(k)\n\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn.evaluate(y_pred, y_test)","be02d104":"knn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test) ","1b4cea89":"knn = MyKNeighborsClassifier(k, 'manhattan')\n\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn.evaluate(y_pred, y_test)","094ba454":"knn = KNeighborsClassifier(n_neighbors=k, metric = 'manhattan')\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test) ","9d578928":"\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,11)\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = MyKNeighborsClassifier(k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n\n    #Compute accuracy on the training set\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.evaluate(y_pred, y_test)","081fcba1":"#import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,11)\ntest_accuracy_sklearn = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    \n    #Compute accuracy on the test set\n    test_accuracy_sklearn[i] = knn.score(X_test, y_test) ","95326fa7":"#Generate plot\nplt.title('euclidean k-NN  Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='My Accuracy')\nplt.plot(neighbors, test_accuracy_sklearn, label='Sklearn accuracy')\n\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","15a278ec":"\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,11)\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = MyKNeighborsClassifier(k, 'manhattan')\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n\n    #Compute accuracy on the training set\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.evaluate(y_pred, y_test)","58c8b79b":"#import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,11)\ntest_accuracy_sklearn = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k, metric = 'manhattan')\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    \n    #Compute accuracy on the test set\n    test_accuracy_sklearn[i] = knn.score(X_test, y_test) ","3e578b6e":"#Generate plot\nplt.title('manhattan k-NN  Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='My Accuracy')\nplt.plot(neighbors, test_accuracy_sklearn, label='Sklearn accuracy')\n\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","89130be3":"# \u0412\u044b\u0432\u043e\u0434 \nSklearn \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u044f\u0432\u043d\u043e \u043b\u0443\u0447\u0448\u0435, \u0445\u043e\u0442\u044f \u0432 \u043f\u0440\u0438 metric = 'manhattan' \u0438 \u043f\u0440\u0438 k = 4, 6 \u0438 8 \u043c\u043e\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u043a\u0430\u0437\u0430\u043b\u0430\u0441\u044c \u043b\u0443\u0447\u0448\u0435 \u0438 \u044f \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e, \u043f\u043e\u0447\u0435\u043c\u0443.\n\u0415\u0432\u043a\u043b\u0438\u0434\u043e\u0432\u043e \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043b\u0443\u0447\u0448\u0435 - 0.758, \u0447\u0435\u043c \u0432\u0442\u043e\u0440\u043e\u0435 - 0.74.","685614cf":"It is a best practice to perform our split in such a way that out split reflects the labels in the data. In other words, we want labels to be split in train and test set as they are in the original dataset. So we use the stratify argument.\n\nAlso we create a test set of size of about 40% of the dataset.","3b4af4fb":"### \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0441 kNeighborsClassifier \u0438\u0437 sklearn (4 \u0431\u0430\u043b\u043b\u0430)\n1. (1 \u0431\u0430\u043b\u043b) \u0412\u044b\u0432\u0435\u0434\u0438\u0442\u0435 accuracy_score \u0434\u043b\u044f \u0432\u0430\u0448\u0435\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0438 \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0438\u0437 sklearn, \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 4 \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u044f\u0445 (\u0414\u043e\u043b\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f 8 \u0447\u0438\u0441\u0435\u043b)\n  1. \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\n  2. `weights='distance'`\n  3. `metric='manhattan'`\n  4. `weights='distance'`, `metric='manhattan'`\n2. (2 \u0431\u0430\u043b\u043b\u0430) \u041f\u0435\u0440\u0435\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 k \u043e\u0442 1 \u0434\u043e 10 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u043f\u0443\u043d\u043a\u0442\u0430 \u0432\u044b\u0448\u0435 (\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f 4 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u043f\u043e \u0434\u0432\u0435 \u043b\u0438\u043d\u0438\u0438 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c)\n  1. \u041f\u0435\u0440\u0435\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u0430\u0436\u0434\u044b\u0439 \u0440\u0430\u0437 \u043e\u0431\u0443\u0447\u0430\u0439\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c\n  2. \u0412\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0433\u0440\u0430\u0444\u0438\u043a \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 `accuracy` \u043e\u0442 `k`\n  3. \u041d\u0430 \u044d\u0442\u043e\u043c \u0436\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0435 \u0432\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442\u0438\u0440\u043d\u043e\u0439 \u043b\u0438\u043d\u0438\u0435\u0439 \u0442\u0430\u043a\u0443\u044e \u0436\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 sklearn\n3. (3 \u0431\u0430\u043b\u043b\u0430) \u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432\u044b\u0432\u043e\u0434, \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0433\u043e, \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u2013 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u043b\u0438 \u0443 \u0432\u0430\u0441 \u0434\u043e\u0441\u0442\u0438\u0447\u044c \u0442\u0430\u043a\u0438\u0445 \u0436\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u043a\u0430\u043a \u0432 sklearn, \u043a\u0430\u043a \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u043b\u0438\u044f\u044e\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b, \u043a\u0430\u043a\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0441 \u043a\u0430\u043a\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u043e\u043a\u0430\u0437\u0430\u043b\u0430\u0441\u044c \u043b\u0443\u0447\u0448\u0435\u0439.","a02dc738":"Let's split the data randomly into training and test set. \n\nWe will fit\/train a classifier on the training set and make predictions on the test set. Then we will compare the predictions with the known labels.\n\nScikit-learn provides facility to split data into train and test set using train_test_split method."}}