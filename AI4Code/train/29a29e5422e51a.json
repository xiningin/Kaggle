{"cell_type":{"94965f76":"code","7f760e06":"code","3dca2d17":"code","066c3df4":"code","d898e80c":"code","723e1401":"code","059e644d":"code","44138bdd":"code","b649493c":"code","b63ca9f4":"code","5640cfdf":"code","a6c841b2":"code","7c9167bc":"code","fe801ad5":"code","24abf107":"code","5eb1b4c6":"code","aacc717a":"code","2ae27e76":"code","79c5c2db":"code","0925faf7":"code","1d0f4c90":"code","07922741":"code","21b6c1e5":"code","3c307a59":"code","8748fa1e":"code","cfacf568":"code","d8a53778":"code","933bdcb2":"code","9f4cec19":"code","7abc0a23":"code","5b451285":"code","cf22d474":"code","9af28ee7":"code","9c450ee7":"code","75dc6746":"code","97d46f6a":"code","08a467c3":"code","faea3385":"code","dbdcbb7d":"code","837af4e3":"code","0026cf9f":"code","c3814836":"code","098d8241":"code","889f9e0f":"code","c3419843":"code","66410e9c":"code","7ce9bbc6":"code","ef7300be":"code","8e6cecef":"markdown","423e5944":"markdown","4b36d08e":"markdown","e02e1611":"markdown","0e9fef21":"markdown","fef95c1c":"markdown","5ae03821":"markdown","038d08c1":"markdown","48df181d":"markdown","1117a11d":"markdown","831ef552":"markdown","443a1353":"markdown","628a7f40":"markdown","369a54de":"markdown","3216457d":"markdown","fa4c4eee":"markdown","4634c2af":"markdown","1f052fa6":"markdown","201ab774":"markdown"},"source":{"94965f76":"import os\nimport random\nimport shutil\n\nfrom pathlib import Path\nimport urllib\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom fastai.vision import *","7f760e06":"# Input root directory\ninput_root_dir = \"..\/input\/food-101\/food-101\/food-101\"\ninput_root_path = Path(input_root_dir)\nprint(os.listdir(input_root_dir))\n","3dca2d17":"# Image directory\nimage_dir_path = input_root_path\/'images'\nimage_categories = os.listdir(image_dir_path)\nimage_categories.remove('.DS_Store')\n\n# Peek at the categories and their total number\nprint(image_categories)\nprint(len(image_categories))","066c3df4":"# Peek inside the meta directory\nos.listdir(input_root_path\/'meta')","d898e80c":"# Contruct a pandas DataFrame from the given meta file, storing the filepath, category, train\/validation set\ndef get_df_from_file(file_path, is_valid):   \n    img_df = pd.read_csv(file_path, sep=';', names=['raw_txt'])\n    # Append .jpg extension to all file names\n    img_df['full_file_name'] = img_df['raw_txt'] + '.jpg'\n    \n    regex_pattern = r'(.+)\\\/(.+)'\n    # Format of each line in input file is: category\/filename, so extract according to this pattern\n    extracted_df = img_df['full_file_name'].str.extract(regex_pattern)\n    extracted_df.columns = ['category', 'file_name']\n\n    # Join extracted values and set is_valid\n    img_df = pd.DataFrame.join(img_df, extracted_df)\n    img_df['is_valid'] = is_valid\n    return img_df\n\n# Load data of the train set\ntrain_txt_file_path = (input_root_path.joinpath('meta', 'train.txt'))\ntrain_df = get_df_from_file(train_txt_file_path, False)\nprint(train_df.shape)\n\n# Load data of the validation set\ntest_txt_file_path = (input_root_path.joinpath('meta', 'test.txt'))\ntest_df = get_df_from_file(test_txt_file_path, True)\nprint(test_df.shape)\n\n# Concatenate train and test sets\nimage_df = pd.concat([train_df, test_df])\ndisplay(image_df.sample(10))\nprint(image_df.shape)\n\n# 75750 entries in train and 25250 entries in test","723e1401":"# Visualize transforms separately on a random image\ndef display_transform(img_df, img_dir_path, transform, min_value, max_value, columns, width, height):\n    # Sample an image from the whole dataset\n    sample_file_name = img_df.sample(1)['full_file_name'].iloc[0]\n    img = open_image(img_dir_path\/sample_file_name)\n    img.show(title=sample_file_name + ' - ' + str(transform))\n    \n    fig, axes = plt.subplots(1, columns, figsize=(width,height))\n    \n    # Visualize different levels of variation\n    for value, axis in zip(np.linspace(min_value, max_value, columns), axes):\n        # open a new image, cause old image has been transformed\n        img = open_image(img_dir_path\/sample_file_name)\n        transform(img, value).show(ax=axis, title=f'value={value:.4f}')\n        \ndisplay_transform(image_df, image_dir_path, brightness, \n                  min_value = 0.5 - 0.2, max_value = 0.5 + 0.2, columns = 6, width = 20, height = 14)\ndisplay_transform(image_df, image_dir_path, contrast, \n                  min_value = 1 - 0.4, max_value = 1 + 0.2, columns = 6, width = 20, height = 14)\ndisplay_transform(image_df, image_dir_path, squish,\n                  min_value = 1 - 0.3, max_value = 1 + 0.5, columns = 6, width = 20, height = 14)","059e644d":"# Displaying skew requires a different logic than the other transforms\ndef display_skew_transform(img_df, img_dir_path, rows=2, columns=4, width=16, height=8):\n    sample_file_name = img_df.sample(1)['full_file_name'].iloc[0]\n    img = open_image(img_dir_path\/sample_file_name)\n    img.show(title=sample_file_name)\n\n    fig, axs = plt.subplots(rows, columns, figsize=(width,height))\n    for i, ax in enumerate(axs.flatten()):\n        # open a new image, cause old image has been transformed\n        open_image(img_dir_path\/sample_file_name).skew(i, 0.7).show(ax=ax, title=f'direction={i}')\n\ndisplay_skew_transform(image_df, image_dir_path)","44138bdd":"# Define transforms used for data augmentation\n# By default, only horizontal flip is used, not vertical flip\n# Disable default lighting alteration, and use brightness and constrast for finer control\nimage_transforms = get_transforms(\n    max_rotate=10,\n    max_zoom=1.1,\n    max_lighting=None,\n    max_warp=0.2,\n    xtra_tfms=[\n        brightness(change=(0.5-0.2, 0.5+0.2), p=0.75),\n        contrast(scale=(1-0.4, 1+0.2), p=0.75),\n        squish(scale=(1-0.3, 1+0.5), p=0.75),\n        skew(direction=(0, 7), magnitude=random.randint(0,6)\/10, p=0.75),\n    ]\n)\n","b649493c":"# Display all random transforms combined on the same random image\ndef display_combined_transforms(img_df, img_dir_path, img_transforms, rows, columns, width, height):\n    sample_file_name = img_df.sample(1)['full_file_name'].iloc[0]\n    img = open_image(img_dir_path\/sample_file_name)\n    img.show(title=sample_file_name)\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(width, height))\n    \n    for row in range(rows):\n        for axis in axes[row]:\n            transformed_img = img.apply_tfms(img_transforms[0])\n            transformed_img.show(ax=axis)\n\ndisplay_combined_transforms(image_df, image_dir_path, image_transforms,\n                   rows=3, columns=5, width=24, height=16)","b63ca9f4":"# Loading image to a ImageList from path in DataFrame\nsplit_and_labeled_images = (ImageList.from_df(\n    image_df, \n    image_dir_path,\n    cols='full_file_name',    \n))\n\n# Split image into train and validation set based on the is_valid column\nsplit_and_labeled_images = split_and_labeled_images.split_from_df(col='is_valid')\n\n# Label images based on category column\nsplit_and_labeled_images = split_and_labeled_images.label_from_df(cols='category')\n\nprint(split_and_labeled_images)","5640cfdf":"image_batch_size = 64\ntransform_image_size = (256, 256)\n\n# Set transform, final size and resize method\n# Normalize with ImageNet stats, because the chosen model will be ResNet pre-trained with ImageNet\nimage_data_bunch = (\n    split_and_labeled_images.transform(\n        image_transforms, \n        size=transform_image_size, \n        resize_method=ResizeMethod.SQUISH)\n    .databunch(bs=image_batch_size)\n    .normalize(imagenet_stats)\n)\n\n# Peek inside the DataBunch\n# It contains images of size 256 x 256 with 3 color channels\nprint(image_data_bunch)","a6c841b2":"# Check the categories and their total count\nprint(image_data_bunch.classes)\nprint(len(image_data_bunch.classes))\nprint(image_data_bunch.c)","7c9167bc":"# Visualize some images, with transforms applied randonmly\nimage_data_bunch.show_batch(rows=4, figsize=(24,16))","fe801ad5":"# Start of training","24abf107":"# Leverage transfer learning from a ResNet50 pre-trained with ImageNet\n\nimage_learner = cnn_learner(\n    image_data_bunch,\n    models.resnet50,\n    metrics=accuracy,\n    wd=1e-1,\n)\n\n# Change the output directory to the current working directory\nimage_learner.model_dir = os.getcwd()\nimage_learner.path = Path(os.getcwd())\n\n# Display model architecture\n# image_learner.model","5eb1b4c6":"# Finding a good learning rate\n\nimage_learner.lr_find()\nimage_learner.recorder.plot()","aacc717a":"last_block_lr = 1e-2\n\n# Train with 1 cycle policy\n\nimage_learner.fit_one_cycle(7, slice(last_block_lr))\nimage_learner.save('stage1-resnet50-size256', return_path=True)\n\n# More explanations about the different settings for learning rate in fit one cycle are here:\n# https:\/\/github.com\/hiromis\/notes\/blob\/master\/Lesson5.md","2ae27e76":"# Now unfreeze the pre-trained part.\nimage_learner.unfreeze()","79c5c2db":"# We need another learning rate for the unfrozen model\nimage_learner.lr_find()\nimage_learner.recorder.plot()","0925faf7":"# Still using the 1 cyle policy, but with discriminative learning rate\n\nimage_learner.fit_one_cycle(7, slice(1e-4, last_block_lr\/5))\nimage_learner.export(os.path.join(os.getcwd(), 'stage2-resnet50-size256-export.pkl'))\nimage_learner.save(\"stage2-resnet50-size256\", return_path=True)","1d0f4c90":"# Plot loss history\nimage_learner.recorder.plot_losses()","07922741":"# Plot the cycle of learning rate and momentum\nimage_learner.recorder.plot_lr(show_moms=True)","21b6c1e5":"# Plot metrics history \nimage_learner.recorder.plot_metrics()","3c307a59":"# End of training","8748fa1e":"print(image_learner.metrics)","cfacf568":"# Check the final validation accuracy\nimage_learner.validate(image_learner.data.valid_dl)","d8a53778":"ys = tensor([1], [0], [1], [0], [1])\npreds = tensor([0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.6, 0.4], [0.9, 0.1]) # bs = 5, n = 2\nprint(accuracy(preds, ys))","933bdcb2":"# Visualize a few results\nimage_learner.show_results(ds_type=DatasetType.Valid)","9f4cec19":"# Some useful interpretation\ninterpreter = ClassificationInterpretation.from_learner(image_learner)","7abc0a23":"# Display the most confused categories, min_val = error threshold for display\ninterpreter.most_confused(min_val=10)","5b451285":"# Display the top losses\ninterpreter.plot_top_losses(16, figsize=(24,16), heatmap=True)","cf22d474":"# Start of inference test","9af28ee7":"zip_filepath = '..\/input\/food-101-custom-test-creation-v2\/food101-custom-test-homemade-and-keyword.zip'\n\ncustom_test_image_dir = os.path.join((os.getcwd()),'custom-test')\nshutil.unpack_archive(zip_filepath, custom_test_image_dir)\nprint(os.listdir(os.path.join(custom_test_image_dir,'cheesecake')))","9c450ee7":"# Images from Google often have high contrast and brightness. Therefore, transform should reduce contrast and brightness to create more variation\ncustom_test_image_transforms = get_transforms(\n    max_rotate=10,\n    max_zoom=1.1,\n    max_lighting=None,\n    max_warp=0.2,\n    xtra_tfms=[\n        brightness(change=(0.5-0.4, 0.5), p=0.5),\n        contrast(scale=(1-0.6, 1), p=0.5),\n        squish(scale=(1-0.3, 1+0.5), p=0.5),\n        skew(direction=(0, 7), magnitude=random.randint(0,6)\/10, p=0.5),\n    ]\n)","75dc6746":"# All images are considered as train data for now. They will be switched later to validation\ncustom_test_data = (ImageList.from_folder(custom_test_image_dir)\n                    .split_none()\n                    .label_from_folder()\n                    .transform(custom_test_image_transforms, size=(256, 256), resize_method=ResizeMethod.SQUISH)\n                    .databunch()\n                    .normalize(imagenet_stats)\n                   )\nprint(custom_test_data)","97d46f6a":"custom_test_data.show_batch(rows=4, figsize=(24,16))","08a467c3":"# Keep a reference to the old training & validation data.\n# Swap the old data with the new custom test data\nold_data = image_learner.data\nimage_learner.data = custom_test_data","faea3385":"# Since all images are considered as train data earlier, now we point valididation data to train data\n# This is necessary to use some useful intepretation later\nimage_learner.data.valid_dl = image_learner.data.train_dl","dbdcbb7d":"# Validate with custom test\nimage_learner.validate(image_learner.data.valid_dl)","837af4e3":"# Show a few results from custom test\nimage_learner.show_results(ds_type=DatasetType.Valid)","0026cf9f":"custom_test_interpreter = ClassificationInterpretation.from_learner(image_learner)","c3814836":"custom_test_interpreter.most_confused(min_val=3)","098d8241":"# Top losses from custom test\ncustom_test_interpreter.plot_top_losses(16, figsize=(24,16), heatmap=True)","889f9e0f":"# return top n predicted classes and probabilities in descending order\ndef get_top_predictions(img_learner, img, top_n=5):\n    prediction = img_learner.predict(img)\n    probabilities = prediction[2]\n    partitioned_indexes = np.argpartition(probabilities, -top_n)\n    top_probability_indexes = partitioned_indexes[-top_n:]\n    \n    top_probabilities = probabilities[top_probability_indexes]\n    sorted_meta_indexes = np.argsort(top_probabilities)\n    sorted_top_indexes = top_probability_indexes[sorted_meta_indexes]\n    sorted_top_probabilities = probabilities[sorted_top_indexes].numpy()[::-1]\n    \n    class_nparray = np.array(img_learner.data.classes)\n    sorted_classes = class_nparray[sorted_top_indexes][::-1]\n    \n    for label, probability in zip(sorted_classes, sorted_top_probabilities):\n        print(\"{:f} {:s}\".format(probability, label))\n    return sorted_classes, sorted_top_probabilities","c3419843":"# predict a random image from the custom test directory\ndef display_and_predict_dir(img_learner, img_test_dir):\n    sample_categ = random.choice(os.listdir(img_test_dir))\n    sample_categ_path = os.path.join(img_test_dir, sample_categ)\n    sample_file = random.choice(os.listdir(sample_categ_path))\n    \n    test_image = open_image(os.path.join(sample_categ_path, sample_file))\n    test_image.show(title='\/'.join([sample_categ, sample_file]))\n\n    top_clss, top_proba = get_top_predictions(img_learner, test_image, top_n=5)\n    return top_clss, top_proba\n\ntest_top_classes, test_top_probabilities = display_and_predict_dir(image_learner, custom_test_image_dir)","66410e9c":"# Create a test directory to hold downloaded image\ndownloaded_test_image_dir = Path.cwd().joinpath('download-test-image')\ndownloaded_test_image_dir.mkdir(parents=True, exist_ok=True)\nprint(os.listdir(os.getcwd()))","7ce9bbc6":"# Get an image from a url and return the top predictions in descending order\ndef display_and_predict_url(img_learner, img_url, test_img_dir):\n    urllib.request.urlretrieve(img_url, test_img_dir\/'test1.jpg')\n    test_image = open_image(test_img_dir\/'test1.jpg')\n    test_image.show()\n\n    top_clss, top_proba = get_top_predictions(img_learner, test_image, top_n=5)\n    return top_clss, top_proba\n\ntest_image_url = 'https:\/\/tmbidigitalassetsazure.blob.core.windows.net\/secure\/RMS\/attachments\/37\/1200x1200\/Triple-Chocolate-Mousse-Torte_EXPS_SDAM18_212092_C11_29_2b.jpg'\ntest_top_classes, test_top_probabilities = display_and_predict_url(image_learner, test_image_url, downloaded_test_image_dir)","ef7300be":"# Remove unzipped images due to output file number restriction\nshutil.rmtree(custom_test_image_dir, ignore_errors=True)","8e6cecef":"## How do we choose learning rates?\n\nIf the learning rate is too slow, training will take a lot of time.\nIf the learning rate is too high, we will be jumping around the minimum loss, getting farther and farther from the minimum and never reach it.\n\nSo to start the experiment of finding a learning rate, we train the model while increasing the learning rate. Then we plot the loss against the learning rate, and stop when the loss starts to explode.","423e5944":"This is typical loss evolution shape, due to the 1 cycle policy.\nLoss increases at first, while the learning rate cycle is in the upward phase.\nThen loss decreases in the downward phase of the learning rate cycle, as we are now closer to the optimum range.","4b36d08e":"Then, we divide the training into 2 parts of a cycle.\nIn the 1st part, we gradually increase the learning rate to a maximum value.\nIn the 2nd part, we gradually decrease the learning rate.\nThe learning rate and momentum are plotted below. For more explanations, please see the paragraph following the plot.","e02e1611":"## Current state of the art\n\n![Screenshot%20from%202019-11-03%2014-38-40.png](attachment:Screenshot%20from%202019-11-03%2014-38-40.png)\n(https:\/\/platform.ai\/blog\/page\/3\/new-food-101-sota-with-fastai-and-platform-ais-fast-augmentation-search\/)\n\n<br>\n\n![Screenshot%20from%202019-11-03%2014-45-38.png](attachment:Screenshot%20from%202019-11-03%2014-45-38.png)\n(https:\/\/arxiv.org\/pdf\/1907.11384.pdf)","0e9fef21":"## Further improvements\n\nTry Test Time Augmentation <br><br>\nExperiment with other model architectures <br>\nhttps:\/\/devpost.com\/software\/food101-classification<br>\n\n\nTry other the parameters for image transformations <br><br>\nProgressively resize images while training (1st, train with smaller images (256x256), then train with bigger image (512x512). This might go beyond the kernel running time limit of Kaggle) <br>\n\n\nValidate and inspect with other similar dataset <br>\nhttp:\/\/visiir.lip6.fr\/#demo <br>\nhttps:\/\/www.groundai.com\/project\/foodx-251-a-dataset-for-fine-grained-food-classification\/1 <br>\nhttps:\/\/kuanghuei.github.io\/Food-101N\/ <br>","fef95c1c":"## Visualize image transformation","5ae03821":"Recognizing food is a challenging problem for computer vision. The same dish can appear very differently on image, due to several factors like variation in recipe, lighting, spatial layout. At the same time, different dishes can look very much alike due to their similar color, texture, ingredients.  ","038d08c1":"# Food 101 challenge summary\nFood-101 is a challenging vision problem, but everyone can relate to it.  Recent SoTA is ~80% top-1, 90% top-5.  These approaches rely on lots of TTA, large networks and  even novel architectures.\n\nTrain a decent model >85% accuracy for top-1 for the test set, using a ResNet50 or smaller network with a reasonable set of augmentations.","48df181d":"## Custom test\nThis custom test set contains images from a Google Images search of 101 categories.\nThe images are the 1st 20 results of Google Images search with \"homemade\" followed by the category name as keyword.\n\n(This dataset is the output of this kernel:<br>\nhttps:\/\/www.kaggle.com\/phananhvu\/food-101-custom-test-creation-v2)\n\nThis additional test will verify the result of the validation step earlier.","1117a11d":"There are 75750 images in the train set and 25250 in the validation set.\nThe images have 3 color channels and are of various sizes","831ef552":"In the 1st half of the cycle, by combining a higher learning rate with a lower momentum, we encourage the model to explore a wider area of the whole loss surface. This leads to a better generalization of the model, and reduces the chance of overfitting.\nThen in the 2nd half of the cycle, we are already in a generally good area of the loss surface. Therefore, we decrease the learning rate and increase the momentum to help the model find a local minimum of that area.\n\n(For more information, [this article](https:\/\/sgugger.github.io\/the-1cycle-policy.html) explains the approach suggested in [this paper](https:\/\/arxiv.org\/abs\/1803.09820))\n\n","443a1353":"## How does CNN work?\n\n<img src=\"https:\/\/brohrer.github.io\/images\/cnn3.png\" width=\"600\">\n(https:\/\/brohrer.github.io\/how_convolutional_neural_networks_work.html)\n\nConvolutions look for pieces of features in the image. In the figure above, we can see that the CNN is trying to match the green, orange and violet pieces between 2 pictures. Then, after several examples, the CNN can learn to detect patterns and small pieces of features.\n\nA CNN consists of a chain of several convolution layers. The outputs of lower level layers become the inputs of higher level layers.\n\n<img src=\"https:\/\/brohrer.github.io\/images\/cnn18.png\" width=\"600\">\n(http:\/\/web.eecs.umich.edu\/~honglak\/icml09-ConvolutionalDeepBeliefNetworks.pdf)\n\nLower level convolutions pick up simple features like edges, basic shape. Higher level convolutions take these simple features and combine them to form more and more complex features. In the illustration above, we can see that lines and circles are combined to form eyes, noses, mouths, which then are combined to form human faces.","628a7f40":"Here is the description of the validate function from the [fast.ai documentation of validate](https:\/\/docs.fast.ai\/basic_train.html#Learner.validate)\n> Return the calculated loss and the metrics of the current model on the given data loader dl\n\nOur chosen metric is accuracy.\n\nThe accuracy here is the top 1 accuracy for the test set, as specified in the meta files of the dataset.\nBelow is an explaining example from the [fast.ai documentation of accuracy](https:\/\/docs.fast.ai\/metrics.html#accuracy)","369a54de":"The pre-trained part should have a smaller learning rate, so that the weights only vary a little.\nIf the learning rate for the pre-trained part is too high, we risk losing what the model has learned.\n\nMeanwhile, the not-pre-trained part should have a higher learning rate, so it will converge faster.","3216457d":"Here we have 5 actual labels (ys) and 5 predictions (preds) for a binary classification problems. \nThe actual labels contain the target classes (0 and 1)\nThe predictions contain probabilities for each of the classes in order.\n\nFor example: the 1st prediction shows a probability of 0.4 for class 0 and 0.6 for class 1.\nThe model simply picks the class with the highest probability (argmax function). Class 1 has the highest probability here (0.6).\nThe corresponding actual label for this prediction is 1. Thus, this is counted as a good prediction.\n\nThere are 2 other good predictions in this series: the 3rd prediction [0.2, 0.8] which matches class 1, and the 4th prediction [0.6, 0.4] which matches class 0.\n\nAs a consequence, the accuracy is computed as 3 good predictions over 5 predictions in total, which gives 0.6 (3\/5).","fa4c4eee":"We still have the same train and validation sets. The images still have 3 color channels, but all of them have been resized to 256 x 256","4634c2af":"## Load data","1f052fa6":"## What is ResNet?\n\n<img src=\"http:\/\/cs231n.github.io\/assets\/nn1\/layer_sizes.jpeg\" width=\"600\">\n(http:\/\/cs231n.github.io\/neural-networks-1\/#arch)\n\nThe deeper a neural network is, the more complex patterns it can capture.\nAt the same time, a deeper network is also more prone to overfitting.\n\n\n<img src=\"http:\/\/teleported.in\/post_imgs\/12-deepnet-errors.png\" width=\"600\">\n(https:\/\/arxiv.org\/abs\/1512.03385)\n\nHowever, something strange happens when we compare a deeper network (56 layers) with a shallower network (20 layers) in practice.\nThe deeper network is expected to overfit and have lower training error, but the opposite happens.\n\nThe idea behind ResNet is to create a deeper network that performs at least equally well as a shallower network.\n\n\n<img src=\"http:\/\/teleported.in\/post_imgs\/12-residual-net.png\" width=\"600\">\n(https:\/\/arxiv.org\/abs\/1512.03385)\n\nThe figure above illustrates the core concept of ResNet: Skip Connection. Here, instead of simply using the activation of the layer (F(x)), we add the input (x) to the activation (F(x) + x) to form the new output of the layer.\nIf the weights of this layer are all 0 (F(x) = 0), then this is simply an identity mapping. The input can skip through unchanged.\nThus, the deeper network has a way of adjusting itself to become the equivalent of the shallower network.\n\n\n<img src=\"https:\/\/raw.githubusercontent.com\/hiromis\/notes\/master\/lesson7\/9.png\" width=\"600\">\n(https:\/\/arxiv.org\/pdf\/1712.09913.pdf)\n\nIt turns out that, the Skip Connections are extremely helpful by making the loss surface much smoother, thus much easier to optimize.\n\nSince then, ResNet is widely used. Models built with ResNet usually train fast and perform well.\n\nhttps:\/\/mlperf.org\/training-overview\/#overview\n\nhttps:\/\/mlperf.org\/inference-overview\/#overview","201ab774":"There are some extreme transformation cases. However, during training, these extremes should be attenuated by randomization.\nAt the same time, the dataset also contains extreme examples (e.g. very dark, very low contrast). We should let the model see more extreme cases, so it will have a better chance of performing well in practice."}}