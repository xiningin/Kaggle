{"cell_type":{"7a35f741":"code","25008c55":"code","033d54a0":"code","c04245a7":"code","9666d763":"code","6a987eed":"code","b4e5d5a0":"code","a35184cc":"code","fbb76a58":"code","432bbae2":"code","4ad0c484":"code","acd4a757":"code","fe5519ff":"code","a41f7765":"code","5d54ec01":"code","550ef5ab":"code","f5ae713e":"code","07c406b5":"code","e996c34a":"code","347cc87f":"code","68fe83ea":"code","9c3d3144":"code","3bf3350b":"code","5dcf8e29":"code","ae68a4ab":"code","7d1e2140":"code","32cd2f6d":"code","04498462":"code","ba131610":"code","ea40c099":"code","252481d3":"code","1e700cec":"code","1a954ce2":"code","9e6b7670":"code","095fa6cc":"code","fc44e1e0":"code","f937ee9a":"code","4903d7fe":"code","c7c3dd55":"code","f2051c53":"code","fec6c81a":"code","70035a6d":"code","0fbb933a":"code","f225f624":"code","e6991773":"code","fd5503e1":"code","6c8d4fd6":"code","6b82bbe4":"code","618283d0":"code","7845d37f":"code","3b0c979c":"code","908eaa41":"code","4bee530f":"code","a65402f0":"code","3d82e9b9":"code","de532284":"code","0c54a071":"code","6f675ff6":"code","2224d15c":"code","c8d27a66":"code","39960571":"code","7bbd29df":"code","c56f8bf8":"code","c92f5e4e":"code","d4b3ba04":"code","59baa018":"code","8f2f06a4":"code","2ce9167f":"code","2efb68ca":"code","8b14f5cd":"code","9683f87c":"code","07f97ca8":"code","f689bd4f":"code","e4b9e366":"code","8747c853":"code","086eb944":"code","d58814dd":"code","d4acf3df":"code","eadd0587":"code","29612547":"code","2190c6b5":"code","3a6bb018":"code","6381031f":"code","1d5d8dcc":"code","ce389aa5":"code","938b23c1":"code","b3e95865":"code","5c45453d":"code","3a4cff1c":"markdown","1e6dfe02":"markdown","afca45f1":"markdown","8c901248":"markdown","323d3f83":"markdown","cb826f75":"markdown","9fc9039e":"markdown","8373cd6b":"markdown","73f8411f":"markdown","c4b50ca8":"markdown","cd6dfca3":"markdown","d8cf913f":"markdown","785f4320":"markdown","35b1124b":"markdown","be829944":"markdown","12da67dc":"markdown","1b927593":"markdown","101ee5ec":"markdown","8a2d0689":"markdown","1f3630f7":"markdown","7cd81cc9":"markdown","4a4c79dd":"markdown","c0662428":"markdown","2ec8ad7e":"markdown","f766fd80":"markdown","150acd40":"markdown","36aa14ca":"markdown","7a396023":"markdown"},"source":{"7a35f741":"%pip install -U pip \n%pip install scikit-learn==0.20.3","25008c55":"!pip list |grep scikit-learn","033d54a0":"import sklearn\nprint(sklearn.__version__)","c04245a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9666d763":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport warnings\nimport pandas_profiling as pp","6a987eed":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.express as px\ninit_notebook_mode(connected=True)","b4e5d5a0":"pd.set_option('display.width', 500)\npd.set_option('display.max_columns', 30)","a35184cc":"warnings.filterwarnings(\"ignore\")","fbb76a58":"df_train = pd.read_csv(dirname+'\/train.csv')\ndf_test = pd.read_csv(dirname+'\/test.csv')","432bbae2":"df_train.head()","4ad0c484":"df_test.head()","acd4a757":"pp.ProfileReport(df_train)","fe5519ff":"pp.ProfileReport(df_test)","a41f7765":"df_train.isna().sum()","5d54ec01":"df_train['datetime'] = pd.to_datetime(df_train['datetime'])\ndf_train['hour'] = df_train['datetime'].map(lambda x :x.hour)\ndf_train['day'] = df_train['datetime'].map(lambda x:x.day)\ndf_train['year'] = df_train['datetime'].map(lambda x:x.year)\ndf_train['month'] = df_train['datetime'].map(lambda x:x.month)\ndf_train['day_of_week'] = df_train['datetime'].map(lambda x:x.dayofweek)","550ef5ab":"df_train.head()","f5ae713e":"df_test['datetime'] = pd.to_datetime(df_test['datetime'])\ndf_test['hour'] = df_test['datetime'].map(lambda x :x.hour)\ndf_test['day'] = df_test['datetime'].map(lambda x:x.day)\ndf_test['year'] = df_test['datetime'].map(lambda x:x.year)\ndf_test['month'] = df_test['datetime'].map(lambda x:x.month)\ndf_test['day_of_week'] = df_test['datetime'].map(lambda x:x.dayofweek)","07c406b5":"df_test.head()","e996c34a":"for col in ['casual', 'registered', 'cnt']:\n    df_train['%s_log' % col] = np.log1p(df_train[col])","347cc87f":"df_train['date'] = df_train['datetime'].apply(lambda x:x.date())","68fe83ea":"eda_date = pd.DataFrame(df_train.groupby('date').sum()['cnt']).reset_index()\neda_date.head()","9c3d3144":"fig = px.line(eda_date, x='date', y='cnt')\nfig.show()","3bf3350b":"eda_date = pd.DataFrame(df_train.groupby(['weekday','month','year']).mean()['cnt']).reset_index()\neda_date.head()","5dcf8e29":"fig = px.bar(eda_date, x='weekday', y='cnt', facet_col='month', facet_row='year')\nfig.show()","ae68a4ab":"eda_date = pd.DataFrame(df_train.groupby(['hour','holiday', 'workingday']).mean()['cnt']).reset_index().dropna()\neda_date.head()","7d1e2140":"fig = px.bar(eda_date, x='hour', y='cnt', facet_col='holiday', facet_row='workingday')\nfig.show()","32cd2f6d":"eda_date = pd.DataFrame(df_train.groupby(['hour','weekday','month']).mean()['cnt']).reset_index()\neda_date.head()","04498462":"fig = px.bar(eda_date, x='hour', y='cnt', facet_col='weekday',facet_row='month')\nfig.show()","ba131610":"eda_date = pd.DataFrame(df_train.groupby(['day','month','holiday','year']).sum()['cnt']).reset_index().dropna()\neda_date.head()","ea40c099":"fig = px.bar(eda_date, x='day', y='cnt', facet_col='month', facet_row='year', color='holiday')\nfig.show()","252481d3":"eda_date = pd.DataFrame(df_train.groupby(['hour','weather', 'weekday']).mean()['cnt']).reset_index().dropna()\neda_date.head()","1e700cec":"fig = px.bar(eda_date, x='hour', y='cnt', facet_col='weekday', facet_row='weather')\nfig.show()","1a954ce2":"eda_date = pd.DataFrame(df_train.groupby(['hour','weather', 'workingday']).mean()['cnt']).reset_index().dropna()\neda_date.head()","9e6b7670":"fig = px.bar(eda_date, x='hour', y='cnt', facet_col='weather', facet_row='workingday')\nfig.show()","095fa6cc":"eda_date = pd.DataFrame(df_train.groupby(['hour','weekday','season']).mean()['cnt']).reset_index()\neda_date.head()","fc44e1e0":"fig = px.bar(eda_date, x='hour', y='cnt', facet_col='weekday',facet_row='season')\nfig.show()","f937ee9a":"sns.distplot(df_train['temp'])","4903d7fe":"eda_num = pd.DataFrame(df_train.groupby(['temp']).mean()['cnt']).reset_index().dropna()\neda_num.head()","c7c3dd55":"fig = px.scatter(eda_num,x='temp',y='cnt')\nfig.show()","f2051c53":"sns.distplot(df_train['atemp'])","fec6c81a":"eda_num = pd.DataFrame(df_train.groupby(['atemp']).mean()['cnt']).reset_index().dropna()\neda_num.head()","70035a6d":"fig = px.scatter(eda_num,x='atemp',y='cnt')\nfig.show()","0fbb933a":"eda_num = pd.DataFrame(df_train.groupby(['temp', 'weekday']).mean()['cnt']).reset_index().dropna()\neda_num.head()","f225f624":"fig = px.scatter(eda_num, x='temp', y='cnt', facet_row='weekday')\nfig.show()","e6991773":"sns.distplot(df_train['humidity'])","fd5503e1":"eda_num = pd.DataFrame(df_train.groupby(['humidity']).mean()['cnt']).reset_index().dropna()\neda_num.head()","6c8d4fd6":"fig = px.scatter(eda_num,x='humidity',y='cnt')\nfig.show()","6b82bbe4":"eda_num = pd.DataFrame(df_train.groupby(['humidity','weekday']).mean()['cnt']).reset_index().dropna()\neda_num.head()","618283d0":"fig = px.scatter(eda_num, x='humidity', y='cnt', facet_row='weekday')\nfig.show()","7845d37f":"fig = px.box(df_train,y='cnt',x='hour',color='weekday')\nfig.show()","3b0c979c":"sns.distplot(df_train['windspeed'],hist=False)","908eaa41":"eda_num = pd.DataFrame(df_train.groupby(['windspeed']).mean()['cnt']).reset_index().dropna()\neda_num.head()","4bee530f":"fig = px.scatter(eda_num,x='windspeed',y='cnt')\nfig.show()","a65402f0":"# df_train['working_hour'] = df_train['hour'].apply(lambda x: x >=9 and x <17)\n# df_train['night'] = df_train['hour'].apply(lambda x: x  <=6)\n# df_train['year'] = df_train['year'].apply(lambda x: x  ==2012)\n# df.drop(['date','datetime'])","3d82e9b9":"df_train.head()","de532284":"# Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Model Selection\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Model\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor","0c54a071":"X_train = df_train[['weather', 'temp', 'atemp', 'humidity', 'windspeed', \n                    'holiday', 'workingday', 'season', 'hour', 'day_of_week', 'year']]\ny_train_cas = df_train['casual_log']\ny_train_reg = df_train['registered_log']","6f675ff6":"X_test = df_test[['weather', 'temp', 'atemp', 'humidity', 'windspeed', \n    'holiday', 'workingday', 'season', 'hour', 'day_of_week', 'year']]","2224d15c":"rf_train = df_train[['weather', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday',\n    'hour', 'weekday']]\n\nrf_test = df_test[['weather', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday',\n    'hour', 'weekday']]","c8d27a66":"kf = KFold(5)","39960571":"params = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.1, 'subsample': 0.7, 'loss': 'ls'}\ngb_cas = GradientBoostingRegressor(**params)\ngb_reg = GradientBoostingRegressor(**params)","7bbd29df":"score_cas = cross_val_score(gb_cas,X_train,y_train_cas,cv=5,scoring='neg_mean_squared_error')\nprint(f'Casual CV Score: {score_cas.mean()} \u00b1 {score_cas.std()}')","c56f8bf8":"score_reg = cross_val_score(gb_reg,X_train,y_train_reg,cv=5,scoring='neg_mean_squared_error')\nprint(f'CV Score: {score_reg.mean()} \u00b1 {score_reg.std()}')","c92f5e4e":"rf_cas = RandomForestRegressor(n_estimators = 1000, \n    min_samples_split = 12, \n    n_jobs = -1,\n    random_state = 123, )\nrf_reg = RandomForestRegressor(n_estimators = 1000, \n    min_samples_split = 12, \n    n_jobs = -1,\n    random_state = 123, )","d4b3ba04":"score_cas = cross_val_score(rf_cas,rf_train,y_train_cas,cv=5,scoring='neg_mean_squared_error')\nprint(f'Casual CV Score: {score_cas.mean()} \u00b1 {score_cas.std()}')","59baa018":"score_reg = cross_val_score(rf_reg,rf_train,y_train_reg,cv=5,scoring='neg_mean_squared_error')\nprint(f'CV Score: {score_reg.mean()} \u00b1 {score_reg.std()}')","8f2f06a4":"gb_cas.fit(X_train,y_train_cas)\ngb_reg.fit(X_train,y_train_reg)\n\ny_pred_cas = gb_cas.predict(X_test)\ny_pred_cas = np.exp(y_pred_cas) - 1\n\n\ny_pred_reg = gb_reg.predict(X_test)\ny_pred_reg = np.exp(y_pred_reg) - 1","2ce9167f":"# df_test['count'] = y_pred_cas + y_pred_reg\ny_gb = y_pred_cas + y_pred_reg\n\ny_gb[:20]","2efb68ca":"# params = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.1, 'subsample': 0.7, 'loss': 'ls'}\n# model = GradientBoostingRegressor(**params)\nrf_cas.fit(rf_train,y_train_cas)\nrf_reg.fit(rf_train,y_train_reg)\n\ny_pred_cas = rf_cas.predict(rf_test)\ny_pred_cas = np.exp(y_pred_cas) - 1\n\ny_pred_reg = rf_reg.predict(rf_test)\ny_pred_reg = np.exp(y_pred_reg) - 1\n\n\ny_rf = y_pred_cas + y_pred_reg\n\ny_rf[:20]\n","8b14f5cd":"y_voting = .2*y_rf + .8*y_gb","9683f87c":"y_voting[:20]","07f97ca8":"df_test['count'] = y_voting","f689bd4f":"df_test['count'].head(10)","e4b9e366":"# import shap\n# import lime","8747c853":"# explainerSKGBT_cas = shap.TreeExplainer(gb_cas)\n# shap_values_SKGBT_test_cas = explainerSKGBT_cas.shap_values(X_test)\n# shap_values_SKGBT_train_cas = explainerSKGBT_cas.shap_values(X_train)","086eb944":"# # Scikit GBT\n# df_shap_SKGBT_test_cas = pd.DataFrame(shap_values_SKGBT_test_cas, columns=X_test.columns.values)\n# df_shap_SKGBT_train_cas = pd.DataFrame(shap_values_SKGBT_train_cas, columns=X_train.columns.values)","d58814dd":"# # if a feature has 10 or less unique values then treat it as categorical\n# categorical_features = np.argwhere(np.array([len(set(X_train.values[:,x]))\n# for x in range(X_train.values.shape[1])]) <= 10).flatten()\n \n# # LIME has one explainer for all models\n# explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n# feature_names=X_train.columns.values.tolist(),\n# class_names=['price'],\n# categorical_features=categorical_features,\n# verbose=True,\n#                                                    mode='regression')","d4acf3df":"# shap.initjs()\n# j=30\n# shap.force_plot(explainerSKGBT_cas.expected_value, shap_values_SKGBT_test_cas[j], X_test.iloc[[j]])","eadd0587":"# expSKGBT = explainer.explain_instance(X_test.values[j], gb_cas.predict, num_features=5)\n# expSKGBT.show_in_notebook(show_table=True)","29612547":"# shap.initjs()\n# j=1000\n# shap.force_plot(explainerSKGBT_cas.expected_value, shap_values_SKGBT_test_cas[j], X_test.iloc[[j]])","2190c6b5":"# expSKGBT = explainer.explain_instance(X_test.values[j], gb_cas.predict, num_features=5)\n# expSKGBT.show_in_notebook(show_table=True)","3a6bb018":"# explainerSKGBT_reg = shap.TreeExplainer(gb_reg)\n# shap_values_SKGBT_test_reg = explainerSKGBT_reg.shap_values(X_test)\n# shap_values_SKGBT_train_reg = explainerSKGBT_reg.shap_values(X_train)","6381031f":"# Scikit GBT\n# df_shap_SKGBT_test_reg = pd.DataFrame(shap_values_SKGBT_test_reg, columns=X_test.columns.values)\n# df_shap_SKGBT_train_reg = pd.DataFrame(shap_values_SKGBT_train_reg, columns=X_train.columns.values)","1d5d8dcc":"# shap.initjs()\n# j=30\n# shap.force_plot(explainerSKGBT_reg.expected_value, shap_values_SKGBT_test_reg[j], X_test.iloc[[j]])","ce389aa5":"# expSKGBT = explainer.explain_instance(X_test.values[j], gb_reg.predict, num_features=5)\n# expSKGBT.show_in_notebook(show_table=True)","938b23c1":"# j=1000\n# shap.force_plot(explainerSKGBT_reg.expected_value, shap_values_SKGBT_test_reg[j], X_test.iloc[[j]])","b3e95865":"# expSKGBT = explainer.explain_instance(X_test.values[j], gb_reg.predict, num_features=5)\n# expSKGBT.show_in_notebook(show_table=True)","5c45453d":"\nsubmission = pd.read_csv(dirname+\"\/sampleSubmission.csv\")\n\nsubmission['count'] = df_test[\"count\"]\n\nsubmission.head(10)\n\nsubmission.to_csv('submission35.csv',index=False)","3a4cff1c":"### Registered","1e6dfe02":"## SHAP","afca45f1":"## Modelling","8c901248":"### Date + Categorical Features","323d3f83":"## Feature Engineering","cb826f75":"### Gradient Boosting","9fc9039e":"## EDA\n\nAfter preprocessing and generation let's dive deeper into the features","8373cd6b":"## Read Data","73f8411f":"### Random Forest","c4b50ca8":"We can see a positive trend on count and temp, let's divide for each weekday","cd6dfca3":"We can the same behaviour on every month with workday have sudden increase when it's come home time","d8cf913f":"### Numercial Feature","785f4320":"### Casual","35b1124b":"## What we get from EDA\n\n- There are clear different behaviour between working day and not working day (holiday and weekend)\n    - We can make new feature based on is it working hour or not\n- Weather  have negative corelation with count\n- Year have positive corelation with count\n- Humidity have a weak negative corelation with count\n- Temp have a weak negative corelation with count\n- Season is not have linear relationship with count\n- There some outlier on count\n    - we can drop outlier on hour level","be829944":"Negative trend for humidity and count here","12da67dc":"## Predict","1b927593":"We can clearly see the different between holiday and workday with there are sudden increase on workday in hour 8, 17, 18 i think it's have something to do with the time when people come home from work, let's see whether it's the same behaviour in weekday vs weekend","101ee5ec":"### Numerical Preprocessing\nMost of the numerical value are already normalized so we don't need to normalize it anymore but let's normalize the rest","8a2d0689":"From both of the graph above we can kinda see that there is a good trend happen with 2012 as general is a better year than 2011 with both of them have a trend where it will going up from early to mid year and then slowly going down by the end of year","1f3630f7":"Don't think there any more significance change from Holiday","7cd81cc9":"## Submission\n","4a4c79dd":"## Import Library","c0662428":"The same behaviour as temp,given that both of them are highly correlated","2ec8ad7e":"- After some discussion we will use voting with GradientBoostingRegressor and Random Forest as the base estimator\n- We will also use Bayesian Optimization Hyperparameter tuning for Gradient Boosting and Random Search for Random Forest\n","f766fd80":"### Datetime Preprocessing\n\nGiven datetime, we will divide each of datetime into year,month, and day","150acd40":"## Feature Preprocessing and Generation","36aa14ca":"We can see a decreasing trend on weather but still the same behaviour on working day and holiday","7a396023":"Not so significance on day-level"}}