{"cell_type":{"63251e0b":"code","3ba29595":"code","cdd39d51":"code","467f166e":"code","06c8589c":"code","35311652":"code","02d91c19":"code","e823679f":"code","a02b6ffc":"code","d5d4ce2d":"code","5445af67":"code","f5c5f2da":"code","2a5e3b56":"code","8275686a":"code","224c743d":"code","bf777241":"code","1c7bfee4":"code","d3269397":"code","cde2bf90":"code","e52b6351":"code","f13ee364":"code","e6525e25":"code","3af5abc1":"code","a46fadf7":"code","8e6746f8":"code","45ddb27e":"code","2edc02ff":"code","65b03282":"code","690a3e90":"code","9822bae4":"code","8a35cd12":"code","a770b502":"code","399b6744":"code","2e1e41c7":"code","677e870b":"code","8aec0f57":"code","04f79851":"code","22500726":"code","561d77ba":"code","a6c33be3":"code","7477a913":"code","f7f9abf1":"code","2116a73b":"code","f98f4253":"code","fa246f73":"code","b005d654":"code","14862f67":"code","27a1adbc":"code","1d4f75cd":"code","e7e29466":"code","64d8a192":"code","fc9bb111":"code","5065c9c1":"code","aff07cd8":"code","9deb2da1":"code","9d54b274":"code","fa9c0f17":"code","27de5d06":"code","0bae0a93":"code","4fc3af2b":"code","f01eea15":"code","8b8914a1":"code","e4ee1fe0":"code","88ab3d24":"code","396a7233":"code","2802e96c":"code","8c1af176":"code","174e25d9":"code","bed6f4af":"code","8b6e344a":"markdown","270d8db9":"markdown","44d15196":"markdown","c691d29e":"markdown","b61b9509":"markdown","846b415c":"markdown","c097fe69":"markdown","36f89072":"markdown","d68d36a6":"markdown","06f986aa":"markdown","484caa39":"markdown","cc05329b":"markdown","c5d3542b":"markdown","ec0ab31c":"markdown"},"source":{"63251e0b":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix","3ba29595":"# reading CSV\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')","cdd39d51":"# shape of training data\ntrain_data.shape","467f166e":"train_data.head()","06c8589c":"# getting the type of each feature and number of non-null values\ntrain_data.info()","35311652":"# finding the missing values column\nmissing_values = 100 * train_data.isna().sum() \/ len(train_data)\nmissing_values[missing_values > 0]","02d91c19":"# dropping the feature Cabin\ntrain_data = train_data.drop(['Cabin'], axis=1)","e823679f":"# lets calculate the missing value of column Embarked based on mode\ntrain_data['Embarked'].value_counts()","a02b6ffc":"# replacing the missing value of column Embarked with 'S' as occurence of S is more\ntrain_data['Embarked'] = train_data['Embarked'].fillna('S')","d5d4ce2d":"# finding the mean age based on gender\ntrain_data.groupby('Sex')['Age'].mean()","5445af67":"# for the age column we gonna replace the null values to mean of gender \ntrain_data['Age'] = train_data.groupby('Sex')['Age'].apply(lambda row: row.fillna(row.mean()))","f5c5f2da":"# changing the Age column to int\ntrain_data['Age'] = train_data['Age'].astype(int)","2a5e3b56":"# lets retreive title from name column\ntrain_data['Title'] = train_data['Name'].apply(lambda row: row.split(\", \")[1].split(\".\")[0])","8275686a":"train_data['Title'].value_counts()","224c743d":"# let's combine the title Dr, Rev and other with value count < 10 to others\ntitle = ['Mr', 'Miss', 'Mrs', 'Master']\ndef get_title(row):\n    if row in title:\n        return row\n    else:\n        return 'Others'\n\ntrain_data['Title'] = train_data['Title'].apply(lambda row: get_title(row))\ntrain_data['Title'].value_counts()","bf777241":"# lets see our changed dataframe\ntrain_data.head()","1c7bfee4":"# we can also make the bins for age columns\ndef get_age_binning(row):\n    if row <= 18:\n        return 'Children'\n    elif row > 18 and row <= 26:\n        return 'Youth'\n    elif row > 26 and row <= 60:\n        return 'Adult'\n    elif row > 60:\n        return 'Senior Citizen'\n    else:\n        \"\"\n\ntrain_data['Age_Bins'] = train_data['Age'].apply(lambda row: get_age_binning(row))\ntrain_data['Age_Bins'].value_counts()","d3269397":"# we can identify if the person was alone on ship or not by checking sibsp = 0 and sibsp = Parch\ndef is_alone(sibsp, parch):\n    if parch == 0 and sibsp == parch:\n        return \"Yes\"\n    else:\n        return \"No\"\n\ntrain_data['Alone'] = train_data.apply(lambda row: is_alone(row.SibSp, row.Parch), axis=1)\ntrain_data['Alone'].value_counts()","cde2bf90":"# lets view the distribution of column Fare\nsns.distplot(train_data['Fare'])","e52b6351":"train_data.describe()","f13ee364":"# lets create binning for fare\ndef get_fare_type(row):\n    if row <=100:\n        return \"Low\"\n    elif row > 100 and row <=200:\n        return \"Mid\"\n    else:\n        return \"High\"\n\ntrain_data['Fare_Type'] = train_data['Fare'].apply(lambda row : get_fare_type(row))\ntrain_data['Fare_Type'].value_counts()","e6525e25":"# we can get rid of below columns \n# PassengerId, Name, Age, Ticket, Fare\ntrain_data = train_data.drop(['PassengerId', 'Name', 'Age', 'Fare', 'Ticket'], axis=1)","3af5abc1":"# changing the feature Pclass, SibSp, Parch to category  \ntrain_data['Pclass'] = train_data['Pclass'].astype('category')\ntrain_data['SibSp'] = train_data['SibSp'].astype('category')\ntrain_data['Parch'] = train_data['Parch'].astype('category')","a46fadf7":"# lets view the final features datatype \ntrain_data.info()","8e6746f8":"# lets view our final dataframe \ntrain_data.head()","45ddb27e":"# count of people survived\nsns.countplot(train_data.Survived)","2edc02ff":"# count of males and females\nsns.countplot(train_data.Sex)","65b03282":"# count of people on ship based on fare_type\nsns.countplot(train_data.Fare_Type)","690a3e90":"# count of people on ship based on pclass\nsns.countplot(train_data.Pclass)","9822bae4":"# count of people on ship based on SibSp\nsns.countplot(train_data.SibSp)","8a35cd12":"# count of people on ship based on Parch\nsns.countplot(train_data.Parch)","a770b502":"sns.countplot(train_data.Alone)","399b6744":"sns.countplot(train_data.Age_Bins)","2e1e41c7":"sns.countplot(train_data.Sex, hue=train_data.Survived)","677e870b":"sns.countplot(train_data.Fare_Type, hue=train_data.Survived)","8aec0f57":"sns.countplot(train_data.Age_Bins, hue=train_data.Survived)","04f79851":"sns.countplot(train_data.Alone, hue=train_data.Survived)","22500726":"train_data_copy = train_data.copy()","561d77ba":"train_data_copy['Sex'] = train_data_copy['Sex'].apply(lambda row: 1 if row == 'Male' else 0)\ntrain_data_copy['Alone'] = train_data_copy['Alone'].apply(lambda row: 1 if row == 'Yes' else 0)","a6c33be3":"cols_to_encode = ['Pclass', 'SibSp', 'Parch', 'Title', 'Embarked', 'Age_Bins', 'Fare_Type']\nencoded_data = pd.get_dummies(train_data_copy[cols_to_encode])\nencoded_data.head()","7477a913":"train_data_copy = pd.concat([train_data_copy, encoded_data], axis=1)\ntrain_data_copy.columns","f7f9abf1":"train_data_copy = train_data_copy.drop(['Pclass', 'SibSp', 'Parch', 'Title', 'Embarked', 'Age_Bins', \n                                                  'Fare_Type'], axis=1)","2116a73b":"train_data_copy.head()","f98f4253":"# lets find the correlation matrix \nplt.figure(figsize=(20,10))\nsns.heatmap(train_data_copy.corr(), cmap='YlGnBu', annot=True)\nplt.show()","fa246f73":"X_train = train_data_copy.drop(['Survived'], axis=1)\nY_train = train_data_copy['Survived']","b005d654":"log_reg_model = LogisticRegression()\nrfe = RFE(log_reg_model, 8)\nrfe = rfe.fit(X_train, Y_train)","14862f67":"cols = X_train.columns[rfe.support_]","27a1adbc":"# building the logistic regression model using the 8 best features selected using RFE\nlog_reg = LogisticRegression()\nlog_reg = log_reg.fit(X_train[cols], Y_train)\ny_pred = log_reg.predict(X_train[cols])","1d4f75cd":"accuracy_score(Y_train, y_pred)","e7e29466":"confusion_matrix(Y_train, y_pred)","64d8a192":"# reading the test data\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","fc9bb111":"# getting the shape of test data\ntest_data.shape","5065c9c1":"# finding missing percentge for test data\nmissing_data = 100 * test_data.isna().sum() \/ len(test_data)\nmissing_data[missing_data > 0]","aff07cd8":"# finding mean age in test data\ntest_data.groupby('Sex')['Age'].mean()","9deb2da1":"# replacing the missing with mean value \ntest_data['Age'] = test_data.groupby(['Sex'])['Age'].apply(lambda row: row.fillna(row.mean()))","9d54b274":"test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())\n\n# performing binning for the fare\ntest_data['Fare_Type'] = test_data['Fare'].apply(lambda row : get_fare_type(row))\ntest_data['Fare_Type'].value_counts()","fa9c0f17":"# retrieving the title from the name\ntest_data['Title'] = test_data['Name'].apply(lambda row: row.split(\", \")[1].split(\".\")[0])\ntest_data['Title'] = test_data['Title'].apply(lambda row: get_title(row))\ntest_data['Title'].value_counts()","27de5d06":"# performing the binning for the age column\ntest_data['Age_Bins'] = test_data['Age'].apply(lambda row: get_age_binning(row))\ntest_data['Age_Bins'].value_counts()","0bae0a93":"# finding if the person was alone or not\ntest_data['Alone'] = test_data.apply(lambda row: is_alone(row.SibSp, row.Parch), axis=1)\ntest_data['Alone'].value_counts()","4fc3af2b":"test_data['Pclass'] = test_data['Pclass'].astype('category')\ntest_data['SibSp'] = test_data['SibSp'].astype('category')\ntest_data['Parch'] = test_data['Parch'].astype('category')","f01eea15":"test_data['Sex'] = test_data['Sex'].apply(lambda row: 1 if row == 'Male' else 0)\ntest_data['Alone'] = test_data['Alone'].apply(lambda row: 1 if row == 'Yes' else 0)","8b8914a1":"test_data.head()","e4ee1fe0":"cols_to_encode = ['Pclass', 'SibSp', 'Parch', 'Title', 'Embarked', 'Age_Bins', 'Fare_Type']\ntest_encoded_data = pd.get_dummies(test_data[cols_to_encode])\ntest_encoded_data.head()","88ab3d24":"test_data = pd.concat([test_data, test_encoded_data], axis=1)","396a7233":"# saving the passender id to some dataframe as lateron we need to concat the passenger id and final predictions\npassenger_id = test_data['PassengerId']","2802e96c":"test_data = test_data.drop(['Pclass', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', \n                              'Fare_Type', 'Title', 'Age_Bins', 'PassengerId'], axis=1)","8c1af176":"y_test_pred = log_reg.predict(test_data[cols])","174e25d9":"test_results = pd.concat([passenger_id, pd.DataFrame(y_test_pred)], axis=1)\ntest_results.columns = ['PassengerId','Survived']","bed6f4af":"test_results.to_csv('test_results_titanic.csv', index=None)","8b6e344a":"### lets perform some visualization","270d8db9":"<h2>Columns encoding <\/h2>","44d15196":"number of females survived more as compared to male","c691d29e":"<h3> a) univariate analysis <\/h3>","b61b9509":"mean age of gender male is 31 <br \/>\nmean age of female is 28","846b415c":"most fare value lies between 0-50","c097fe69":"### choosing the best 8 features to create our model using RFE","36f89072":"<h3> b) bivariate analysis<\/h3>","d68d36a6":"people who have taken high or mid fare ticket have more chances of survival","06f986aa":"so very few people were there with high fare ticket","484caa39":"We gonna get rid of Cabin feature as percentage of missing value is more than 70 <br>\nFare missing value will be calculated using the mean of fare column <br\/>\nFor Age column we gonna consider mean age of gender","cc05329b":"### performing the EDA steps on the test data","c5d3542b":"here we have missing values in 3 column <br\/>\nSince missing values in Cabin is more than 70% we will be removing this column from our dataset <br\/>\nFor column Age we will be determining the missing values using the mean based on Gender ie. we group the data based on gender\nand will determing the mean of each gender and fill the null values <br\/> <br\/>\n\nEmbarked column is a categorical column we will be replacing the null value with mode of the column.<br \/>\n","ec0ab31c":"### Encoding the test data"}}