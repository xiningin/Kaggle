{"cell_type":{"14fe81ed":"code","df3edbdf":"code","01f0a53e":"code","354cc21c":"code","28c7f9e7":"code","f29c02e1":"code","64f1699c":"code","59adca37":"code","3daca722":"code","82991022":"code","a937326e":"code","5c63015b":"code","60a25a22":"code","bd722473":"code","6dda1370":"code","3fe6aa09":"code","bf9bfdc9":"code","5c24e557":"code","dec5c7fd":"code","f0704048":"code","4aa82d60":"code","c7371562":"code","75116ea3":"code","741301e1":"code","6ae27b01":"code","e0948acc":"code","1c948268":"code","9a64be1c":"code","5adea245":"code","542638a4":"code","9d3e0b43":"code","86d5008a":"code","82d1cfc8":"code","7d08ce06":"code","be8eab1e":"code","36c18bff":"code","afc24925":"code","f5174032":"code","3b8a45ac":"code","a6e0b44f":"code","239ab42a":"code","e5289ed3":"code","9ca7b1d6":"code","7680362f":"code","9fdf3879":"code","e0421430":"code","63a61477":"code","68a6c0df":"code","0b2efa3e":"code","63979ef4":"code","ea9a2b4e":"code","67e230b7":"code","0bb58bea":"code","be395dc4":"code","c5978b01":"code","05aab610":"code","09ed9d35":"code","aa3cfb64":"code","673f700c":"code","b79e5a8f":"code","a9616366":"code","8def31e7":"code","1ce03e57":"code","f5c86997":"code","d515f413":"code","1e7160c7":"code","cc47076b":"code","8ef4684a":"code","fd28490c":"markdown","98e9e417":"markdown","41d437e0":"markdown","583b0794":"markdown","da34c0ef":"markdown","006d8074":"markdown","b51043bb":"markdown","252e97c3":"markdown","ee63033c":"markdown","2e64b914":"markdown","a4b06774":"markdown","f013dd0c":"markdown","3cb343bc":"markdown","d95e5b11":"markdown","9bf76105":"markdown","aa34132f":"markdown","ead59646":"markdown","59abaaa7":"markdown","d153734f":"markdown","560d3363":"markdown","3e37731f":"markdown","3ab596e2":"markdown","732bf04c":"markdown"},"source":{"14fe81ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df3edbdf":"import matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\nfrom sklearn.utils import shuffle\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport eli5\n\nfrom catboost import CatBoostClassifier\nimport xgboost as xgb\n\n\n\n\nimport riiideducation\n\n%matplotlib inline\n# for heatmap and other plots\ncolorMap1 = sns.color_palette(\"RdBu_r\")\n# for countplot and others plots\ncolorMap2 = 'Blues_r'\n\n","01f0a53e":"sampler = TPESampler(\n    seed=666\n)","354cc21c":"types = {\n        'row_id': 'int64', \n        'timestamp': 'int64', \n        'user_id': 'int32', \n        'content_id': 'int16', \n        'content_type_id': 'int8',\n        'task_container_id': 'int16', \n        'user_answer': 'int8', \n        'answered_correctly': 'int8', \n        'prior_question_elapsed_time': 'float32', \n        'prior_question_had_explanation': 'boolean'\n}","28c7f9e7":"train = pd.read_csv(\n    '\/kaggle\/input\/riiid-test-answer-prediction\/train.csv', \n    low_memory=False, \n    nrows=10**6, \n    dtype=types\n)\nquestions=pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv')\nlectures=pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/lectures.csv')\ntest=pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/example_test.csv')\n\ntrain.head()","f29c02e1":"print(f\"Train shape: {train.shape}\")","64f1699c":"\ntrain.head()","59adca37":"train.describe().style.background_gradient(cmap='Blues')","3daca722":"print(f'Number of unique users: {len(np.unique(train.user_id))}')\n","82991022":"print(train.isnull().sum()\/len(train))","a937326e":"corr_matrix=train.corr()\ncorr_matrix['answered_correctly'].sort_values(ascending=True)","5c63015b":"plt.figure(figsize=(13,10))\nsns.heatmap(corr_matrix,annot=True,\n           linewidths=5,cmap=colorMap1)","60a25a22":"plt.figure(figsize=(15, 10))\nax = sns.countplot(x=\"prior_question_elapsed_time\", \n                   data=train[train['prior_question_elapsed_time'].notnull()],\n                   palette=colorMap2)","bd722473":"freq_answered_tasks = train['task_container_id'].value_counts().reset_index()\nfreq_answered_tasks.columns = [\n    'task_container_id', \n    'freq'\n]\ntrain['freq_task_id'] = ''\ntrain.loc[train['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] < 10000]['task_container_id'].values), 'freq_task_id'] = 'very rare answered'\ntrain.loc[train['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 10000]['task_container_id'].values), 'freq_task_id'] = 'rare answered'\ntrain.loc[train['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 50000]['task_container_id'].values), 'freq_task_id'] = 'normal answered'\ntrain.loc[train['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 200000]['task_container_id'].values), 'freq_task_id'] = 'often answered'\ntrain.loc[train['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 400000]['task_container_id'].values), 'freq_task_id'] = 'very often answered'\n\n","6dda1370":"train.sample(5)","3fe6aa09":"plt.figure(figsize=(15, 10))\nsns.countplot(x='freq_task_id', hue='answered_correctly', data=train, palette=colorMap2)","bf9bfdc9":"WIDTH=800","5c24e557":"ds = train['answered_correctly'].value_counts().reset_index()\n\nds.columns = [\n    'answered_correctly', \n    'percent_of_answers'\n]\n\nds['percent_of_answers'] \/= len(train)\nds = ds.sort_values(['percent_of_answers'])\n\nfig = px.pie(\n    ds, \n    names='answered_correctly', \n    values='percent_of_answers', \n    title='Percent of correct answers', \n    width=WIDTH,\n    height=500 \n)\n\n\nfig.show()","dec5c7fd":"plt.figure(figsize=(15, 11))\nax = sns.countplot(x=\"prior_question_had_explanation\", hue=\"answered_correctly\", data=train[train['prior_question_had_explanation'].notnull()], palette=colorMap2)","f0704048":"N=30\n\nuser_freq = train['user_id'].value_counts().reset_index()\nuser_freq.columns = [\n    'user_id', \n    'count'\n]\n\n# Add ' - ' to convert user_id to str and not sort\nuser_freq['user_id'] = user_freq['user_id'].astype(str) + ' - '\nuser_freq = user_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 15))\nsns.barplot(x='count', y='user_id', data=user_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most active users', fontsize=14)","4aa82d60":"WIDTH=800","c7371562":"ds = train['content_type_id'].value_counts().reset_index()\n\nds.columns = [\n    'content_type_id', \n    'percent'\n]\n\nds['percent'] \/=len(train)\n\nfig = px.pie(\n    ds, \n    names='content_type_id', \n    values='percent', \n    title='Lecures & questions', \n    width=WIDTH,\n    height=500 \n)\n\nfig.show()","75116ea3":"ds=train['user_answer'].value_counts().reset_index()\nds.columns = [\n    'user_answer', \n    'percent_of_answers'\n]\nds['percent_of_answers']\/=len(train)\nds = ds.sort_values(['percent_of_answers'])\nfig = px.bar(\n    ds, \n    x='user_answer', \n    y='percent_of_answers', \n    orientation='v', \n    title='Percent of user answers for every option', \n    width=WIDTH,\n    height=400 \n)\n\nfig.show()","741301e1":"task_ids_freq = train['task_container_id'].value_counts().reset_index()\ntask_ids_freq.columns = ['task_container_id', 'count']\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.pointplot(x='task_container_id', y='count', data=task_ids_freq, palette=colorMap2)\nxticks_range = range(min(task_ids_freq['task_container_id']), \n                     max(task_ids_freq['task_container_id']),\n                     1000)\nplt.xticks(list(xticks_range), list(xticks_range))","6ae27b01":"questions.head()","e0948acc":"questions.describe().style.background_gradient(cmap='Blues')","1c948268":"print(questions.isnull().sum() \/ len(questions))","9a64be1c":"\n\n\n\n\nquestions['tag'] = questions['tags'].str.split(' ')\nquestions = questions.explode('tag')\nquestions = pd.merge(\n    questions, \n    questions.groupby('question_id')['tag'].count().reset_index(), \n    on='question_id'\n    )\n\nquestions = questions.drop(['tag_x'], axis=1)\n\nquestions.columns = [\n    'question_id', \n    'bundle_id', \n    'correct_answer', \n    'part', \n    'tags', \n    'tags_number'\n]\nquestions = questions.drop_duplicates()","5adea245":"ds = questions['correct_answer'].value_counts().reset_index()\n\nds.columns = [\n    'correct_answer', \n    'number_of_answers'\n]\n\nds['correct_answer'] = ds['correct_answer'].astype(str) + '-'\nds = ds.sort_values(['number_of_answers'])\n\nfig = px.bar(\n    ds, \n    x='number_of_answers', \n    y='correct_answer', \n    orientation='h', \n    title='Number of correct answers per group', \n    width=WIDTH,\n    height=300\n)\nfig.show()","542638a4":"ds = questions['part'].value_counts().reset_index()\n\nds.columns = [\n    'part', \n    'count'\n]\n\nds['part'] = ds['part'].astype(str) + '-'\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds, \n    x='count', \n    y='part', \n    orientation='h', \n    title='Parts distribution',\n    width=WIDTH,\n    height=400\n)\nfig.show()","9d3e0b43":"ds = questions['tags_number'].value_counts().reset_index()\n\nds.columns = [\n    'tags_number', \n    'count'\n]\n\nds['tags_number'] = ds['tags_number'].astype(str) + '-'\nds = ds.sort_values(['tags_number'])\n\nfig = px.bar(\n    ds, \n    x='count', \n    y='tags_number', \n    orientation='h', \n    title='Number tags distribution', \n    width=WIDTH,\n    height=400 \n    )\n\nfig.show()","86d5008a":"\n\ncheck = questions['tags'].str.split(' ').explode('tags').reset_index()\ncheck = check['tags'].value_counts().reset_index()\n\ncheck.columns = [\n    'tag', \n    'count'\n]\n\ncheck['tag'] = check['tag'].astype(str) + '-'\ncheck = check.sort_values(['count']).tail(40)\n\nfig = px.bar(\n    check, \n    x='count', \n    y='tag', \n    orientation='h', \n     title='Top 40 most useful tags', \n    width=WIDTH,\n    height=900 \n)\n\nfig.show()","82d1cfc8":"lectures.head(10)","7d08ce06":"print('Part of missing values for every column')\nprint(lectures.isnull().sum() \/ len(lectures))","be8eab1e":"lectures['type_of'].value_counts()","36c18bff":"test.head()","afc24925":"train.head()","f5174032":"features_df = train.iloc[:int(9\/10 * len(train))]\ntrain = train.iloc[int(9\/10 * len(train)):]","3b8a45ac":"train_questions_only_df = features_df[features_df['answered_correctly']!=-1]\ngrouped_by_user_df = train_questions_only_df.groupby('user_id')\n\nuser_answers_df = grouped_by_user_df.agg(\n    {\n        'answered_correctly': [\n            'mean', \n            'count', \n            'std', \n            'median', \n            'skew'\n        ]\n    }\n).copy()\n\nuser_answers_df.columns = [\n    'mean_user_accuracy',\n    'questions_answered',\n    'std_user_accuracy', \n    'median_user_accuracy', \n    'skew_user_accuracy'\n]","a6e0b44f":"grouped_by_content_df = train_questions_only_df.groupby('content_id')\ncontent_answers_df = grouped_by_content_df.agg(\n    {\n        'answered_correctly': [\n            'mean', \n            'count', \n            'std', \n            'median', \n            'skew'\n        ]\n    }\n).copy()\n\ncontent_answers_df.columns = [\n    'mean_accuracy', \n    'question_asked', \n    'std_accuracy', \n    'median_accuracy', \n    'skew_accuracy'\n]\ncontent_answers_df","239ab42a":"del features_df\ndel grouped_by_user_df\ndel grouped_by_content_df","e5289ed3":"features = [\n    'mean_user_accuracy', \n    'questions_answered',\n    'std_user_accuracy', \n    'median_user_accuracy',\n    'skew_user_accuracy',\n    'mean_accuracy', \n    'question_asked',\n    'std_accuracy', \n    'median_accuracy',\n    'prior_question_elapsed_time', \n    'prior_question_had_explanation',\n    'skew_accuracy'\n]\n\ntarget = 'answered_correctly'","9ca7b1d6":"target","7680362f":"train = train[train[target] != -1]","9fdf3879":"train = train.merge(user_answers_df, how='left', on='user_id')\ntrain = train.merge(content_answers_df, how='left', on='content_id')","e0421430":"target","63a61477":"\ntrain['prior_question_had_explanation'] = train['prior_question_had_explanation'].fillna(value=False).astype(bool)\ndf = train.fillna(value=0.5)","68a6c0df":"col_to_drop = set(train.columns.values.tolist()).difference(features + [target])\nfor col in col_to_drop:\n    del df[col]","0b2efa3e":"df = df.replace([np.inf, -np.inf], np.nan)\ndf = df.fillna(0.5)","63979ef4":"train_df, test_df, y_train, y_test = train_test_split(df[features], df[target], random_state=777, test_size=0.2)","ea9a2b4e":"# clf = LGBMClassifier(random_state=777)\n\n# params = {\n#     'n_estimators': [50, 150, 300],\n#     'max_depth': [3, 5, 10],\n#     'num_leaves': [5, 15, 30],\n#     'min_data_in_leaf': [5, 50, 100],\n#     'feature_fraction': [0.1, 0.5, 1.],\n#     'lambda': [0., 0.5, 1.],\n# }\n\n# cv = RandomizedSearchCV(clf, param_distributions=params, cv=5, n_iter=50, verbose=2)\n# cv.fit(df[features], df[target])\n\n# print(cv.best_params_)\n# print(cv.best_score_)","67e230b7":"params = {\n    'num_leaves': 30, \n    'n_estimators': 300, \n    'min_data_in_leaf': 100, \n    'max_depth': 5, \n    'lambda': 0.0, \n    'feature_fraction': 1.0\n}\n","0bb58bea":"model = LGBMClassifier(**params)\nmodel.fit(train_df, y_train)","be395dc4":"print('LGB ROC-AUC score: ', roc_auc_score(y_test.values, model.predict_proba(test_df)[:, 1]))","c5978b01":"params_cat = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' ,\n    'grow_policy': 'Lossguide',\n    'iterations': 2500,\n    'learning_rate': 4e-2,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n}","05aab610":"model_cat =  CatBoostClassifier(**params_cat)\nmodel_cat.fit(train_df, y_train)","09ed9d35":"print('CatBoost ROC-AUC score: ', roc_auc_score(y_test.values, model_cat.predict_proba(test_df)[:, 1]))","aa3cfb64":"from sklearn.ensemble import RandomForestClassifier\nrfclf = RandomForestClassifier()\nrfclf.fit(train_df,y_train)\npred = rfclf.predict(test_df)","673f700c":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test,pred))","b79e5a8f":"from xgboost import XGBClassifier\nxgbclf = XGBClassifier()\nxgbclf.fit(train_df,y_train)\nxgb_pred = xgbclf.predict(test_df)","a9616366":"print(roc_auc_score(y_test,xgb_pred))","8def31e7":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier()\nmlp.fit(train_df,y_train)\nmlp_pred = mlp.predict(test_df)","1ce03e57":"print(roc_auc_score(y_test,mlp_pred))","f5c86997":"eli5.show_weights(model, top=20)","d515f413":"lgb.plot_importance(model)","1e7160c7":"env = riiideducation.make_env()","cc47076b":"iter_test = env.iter_test()","8ef4684a":"for (test_df, sample_prediction_df) in iter_test:\n    # merge\n    test_df = test_df.merge(user_answers_df, on = \"user_id\", how = \"left\")\n    #test_df = test_df.merge(task_container_characteristics, on = \"task_container_id\", how = \"left\")\n    test_df = test_df.merge(content_answers_df, on = \"content_id\", how = \"left\")\n    \n    # type transformation\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n    test_df.fillna(value = 0.5, inplace = True)\n    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n    test_df = test_df.fillna(0.5)\n    \n    # preds\n    test_df['answered_correctly'] = model.predict_proba(test_df[features])[:, 1]\n    cols_to_submission = ['row_id', 'answered_correctly', 'group_num']\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","fd28490c":"Lets look at the missing data","98e9e417":"Metadata for the lectures watched by users as they progress in their education.\n\n**lecture_id**: foreign key for the train\/test content_id column, when the content type is lecture (1).\n\n**part**: top level category code for the lecture.\n\n**tag**: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n\n**type_of**: brief description of the core purpose of the lecture","41d437e0":"**Test.csv**","583b0794":"**row_id**: (int64) ID code for the row.\n\n**timestamp**: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\n**user_id**: (int32) ID code for the user.\n\n**content_id**: (int16) ID code for the user interaction\n\n**content_type_id**: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\n**task_container_id**: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\n**user_answer**: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\n**answered_correctly**: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\n**prior_question_elapsed_time**: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures\nin between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\n**prior_question_had_explanation**: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.","da34c0ef":"**QUESTIONS.CSV**","006d8074":"The answer's showing increases the probability of a successful answering. Let's go further.\nCheck the most active user_id\n\n","b51043bb":"#                  DATA EXPLORATION & EDA","252e97c3":"# Random forest classifier","ee63033c":"\n\n\n\n\n\nLet's check the distribution of **prior_question_elapsed_time** ","2e64b914":"\nLECTURES.CSV","a4b06774":"**TRAIN**","f013dd0c":"# FEATURE ENGINEERING","3cb343bc":"# Riiid! Answer Correctness Prediction","d95e5b11":"#Xgboost classifier","9bf76105":"\nquestion_id: foreign key for the train\/test content_id column, when the content type is question (0).\n\nbundle_id: code for which questions are served together.\n\ncorrect_answer: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n\npart: top level category code for the question.\n\ntags: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.","aa34132f":"Also let's check a correlation matrix to get more information between the columns","ead59646":"prior_question_had_explanation has a medium correlation with the target value. So let's look at his distribution\n\n","59abaaa7":"Submission preparation","d153734f":"And the most useful content_id","560d3363":"\nLet's check any connection between our target value and a frequency of answering questions","3e37731f":"# Modeling","3ab596e2":"Also we need to check distribution in content_type_id: number of video lectures and questions","732bf04c":"The main target of this notebook is giving the base understanding of our data and some useful features.\n\nFirst of all you can find here:\n\nComprehensive description of our data\n\nFeature Engineering\n\nBaseline with LightGBM without serious tuning model's parameters"}}