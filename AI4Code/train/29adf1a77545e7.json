{"cell_type":{"c013db9f":"code","df40c7b6":"code","88d3a7ac":"code","457ae942":"code","4494efc2":"code","963b332f":"code","e145e5fc":"code","de05912a":"code","ecbac940":"code","f5bce611":"code","69469b24":"code","6e0496f8":"markdown","e9761d95":"markdown","97c1b86b":"markdown","506a20a9":"markdown","e31eb35f":"markdown","2775f603":"markdown","d0e84f80":"markdown","8627abaf":"markdown","59da5114":"markdown"},"source":{"c013db9f":"import pandas as pd\nimport os\nfrom pathlib import Path\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib","df40c7b6":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain.head()","88d3a7ac":"train.discourse_type.unique().tolist()","457ae942":"path = Path('..\/input\/feedback-prize-2021\/train')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000'\n         }\n\ndef visualize(example):\n    ents = []\n    for i, row in train[train['id'] == example].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(path\/f'{example}.txt', 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist(), \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)\n","4494efc2":"examples = train['id'].sample(n=5, random_state=42).values.tolist()\n\nfor ex in examples:\n    visualize(ex)\n    print('\\n')","963b332f":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')","e145e5fc":"texts = []\n\nids = train.sample(n=500)['id'].unique().tolist()\nlen(ids)\n\nfor example in ids:\n    with open(path\/f'{example}.txt', 'r') as file: data = file.read()\n    texts.append({\n        'text': data,\n        'n_tokens': len(tokenizer(data)['input_ids'])\n    })\n     \ndf = pd.DataFrame(texts)\n\nprint(len(df[df.n_tokens > 512])\/len(df))\n\ndf.n_tokens.hist();","de05912a":"dist = train.groupby('id')['discourse_type'].apply(lambda x: len(list(x)))\n\nprint(f'Min: {dist.min()}')\nprint(f'Max: {dist.max()}')\n\ndist.hist();","ecbac940":"dist.sort_values()","f5bce611":"visualize('FFFF80B8CC2F')\nvisualize('FC7A3692794B')\nvisualize('149E8C278863')\nvisualize('71259B3EA87F')","69469b24":"sub = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\nsub.head()","6e0496f8":"Ok, so it looks like for test we also get files in a similar format, and we need to predict class and prediction string. I guess it's now time to look at the information provided by the host to understand this better :) \n\n> The word indices are calculated by using Python's .split() function and taking the indices in the resulting list. The two overlaps are calculated by taking the set() of each list of indices in a ground truth \/ prediction pair and calculating the intersection between the two sets divided by the length of each set.\n\nI think that means we need to provide the indices of words that are in our predicted span in the `predictionstring` field.\n\n","e9761d95":"### Training Data\n\nLooks like we have a bunch of text files and a separate `train.csv` with labels. Let's start by looking at the csv. We get reference to the text files, and then multiple lines per text file with spans indicating a specific **discourse type**. Let's also see what are the unique types of discourse below.","97c1b86b":"### Test","506a20a9":"### Visualize with `displacy`\n\nI think it will work best if we can overlay the spans from the csv file onto the text files. Spacy has a great visualizer for this: `displacy`. Let's use it to see what a single example looks like!","e31eb35f":"**Important** More than half texts exceed typical max length of 512 tokens, so we'll likely need to apply some sliding window or other chunking approach to the data!","2775f603":"# Feedback Prize EDA\n\nHey there NLP fans, I didn't expect another NLP competition right now, how about you? Let's take a look to see what this one is about :) We'll start with looking at the data, and then look at all the info from the organizers - this sequence works best for me :) ","d0e84f80":"### Insights\n\nOk - this looks interesting!: \n- For some examples, the entire text is densely split into spans of different categories. In some other examples, the annotators omit some words and the splits look very subjective. It's an indicator that annotations may be noisy. \n- Order seems to be important: start with the lead, mix claims and evidence, finish with concluding statement. We may need to incorporate this into our models. \n- There may be 2 spans of the same class next to each other - it will be important to separate them!\n\n### Text length\n\nAnother important data point is the length of texts - would they fit into a model like `roberta`? Let's check!","8627abaf":"## Baseline\n\nThis is an interesting task! It can be approached in several ways:\n1. Split texts, for example by sentence, and then classify each sentence. \n2. NER: classify individual tokens (words)\n3. Question answering: use discourse types as questions, and predict answer spans\n\nEach approach has some downsides or complexities... I think we have enough data to start building a baseline. I'll go ahead and start to prepare one in a separate notebook, and come back to this EDA later. \n\n### I will smile for every upvote :)","59da5114":"### Number of spans per text\n\nLet's also see how many spans there are per text. Let's also visualize texts with the fewest and the most spans. }"}}