{"cell_type":{"3959abcf":"code","342a1d68":"code","9ab320a0":"code","dae82eb5":"code","31171653":"code","5150177f":"code","d823ca86":"code","4dc13902":"code","3dfab339":"code","4ec03eea":"code","09c73053":"code","6f9a7bc0":"code","d3a42e03":"code","b978e308":"code","ca2312fd":"code","7914ca4b":"code","24d5aa34":"code","1bf14c31":"code","452685e2":"code","7a8e88aa":"code","b7505c5c":"code","76e05dd0":"code","bccef591":"code","067c82c4":"code","d914b79c":"code","59efacda":"code","c65f3806":"code","2b86c6e9":"code","8f53abe1":"code","d6fb6b57":"code","80e320a7":"code","84d527a7":"code","dcfa455b":"code","ef227567":"markdown","9d30aa42":"markdown","5fffc4ee":"markdown","853edb9e":"markdown","01f9141e":"markdown","c68d9972":"markdown","dd7b739a":"markdown","65d07e42":"markdown","f54960c1":"markdown","ad06b150":"markdown","ff6b96fd":"markdown","c7627ee9":"markdown","16e1efcb":"markdown","da88a2d9":"markdown","b80b32ca":"markdown","a35c4a22":"markdown","caf46e99":"markdown","4788f066":"markdown","22e71ef7":"markdown","9ad27b8b":"markdown","a891a8a0":"markdown","00127b0d":"markdown","9b27ed47":"markdown"},"source":{"3959abcf":"import pandas as pd\nimport numpy as np\nimport cv2\nimport sklearn\nimport sklearn.model_selection\nimport tensorflow.keras as keras\n\n%matplotlib inline","342a1d68":"# Download data & unzip if it doesn't already exist\nimport os.path\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile","9ab320a0":"def load_ext_file(data_zip_url, data_path='data\/'):\n    '''Download the zip file from URL and extract it to path (if specified).\n    '''\n    # Check if path already exits\n    if not os.path.exists(data_path):\n        with urlopen(data_zip_url) as zip_resp:\n            with ZipFile(BytesIO(zip_resp.read())) as zfile:\n                # Extract files into the data directory\n                zfile.extractall(path=None)\n       ","dae82eb5":"# Use particular release for data from a simulation run\nload_ext_file('https:\/\/github.com\/MrGeislinger\/clone-driving-behavior\/releases\/download\/v0.14.0\/data.zip')","31171653":"def create_img_meas_dfs(log_csv, data_dir=None, orig_dir=None, skiprows=None):\n    '''Creates DataFrames for the image paths and measurements using CSV path.\n    \n    Returns tuple of two DataFrames.\n    '''\n    data_header = [\n        'image_center',\n        'image_left',\n        'image_right',\n        'steer_angle', # [-1,1]\n        'throttle', # boolen (if accelerating)\n        'break', # boolean (if breaking)\n        'speed' # mph\n    ]\n\n    df = pd.read_csv(\n        log_csv,\n        names=data_header,\n        skiprows=skiprows\n    )\n\n    # Replace the original directory from dataset (if specified)\n    if orig_dir and data_dir:\n        for col in ['image_center','image_left','image_right']:\n            df[col] = df[col].str.replace(orig_dir,data_dir)\n    \n    # Get specifics for each DF\n    df_img_paths = df.iloc[:,:3]\n    df_measurments = df.iloc[:,3:]\n    \n    return df_img_paths,df_measurments, df","5150177f":"df_imgs, df_meas, df_all = create_img_meas_dfs(\n    log_csv='data\/driving_log.csv', \n    skiprows=1)\n\ndisplay(df_imgs.head())\n\nprint('Stats for measurements:')\ndisplay(df_meas.describe())","d823ca86":"ax = df_meas.steer_angle.hist(bins=50)","4dc13902":"def flip_image(image, target):\n    '''Horizontally flip image and target value.\n    '''\n    image_flipped = np.fliplr(image)\n    target_flipped = -target\n    return image_flipped, target_flipped\n    ","3dfab339":"def adjust_offcenter_image(image, target, correction: float = 1e-2,\n                                  img_camera_type: str = 'center'):\n    '''\n    img_camera_type: The type of camera image\n    target: The target value (to be adjusted)\n    '''\n    # TODO: Adjust the target slightly for off-center image\n    if img_camera_type == 'left':\n        new_target = target + correction\n    elif img_camera_type == 'right':\n        new_target = target - correction\n    # Don't make any correction if unknown or centere\n    else: \n        new_target = target\n    return image, new_target","4ec03eea":"def skip_low_steering(steer_value, steer_threshold=0.05, drop_percent=0.2):\n    '''\n    '''\n    # Keep value if greater than threshold or by chance\n    return (steer_value < steer_threshold['left']\n            or steer_value > steer_threshold['right']\n            or np.random.rand() > drop_percent)\n        ","09c73053":"def translate_image (image, target, correction=100, scale_factor=0.2):\n    '''\n    '''\n    # Translate the image randomly about correction factor (then scaled)\n    adjustment = int(correction * np.random.uniform(-1*scale_factor, scale_factor))\n    # Get a new\n    target_new = target + (adjustment \/ correction)\n    n,m,c=image.shape\n    bigsquare=np.zeros((n,m+100,c),image.dtype) \n    if adjustment < 0:\n        bigsquare[:,:m+adjustment]=image[:,-adjustment:m]\n    else:\n        bigsquare[:,adjustment:m+adjustment]=image\n    return bigsquare[:n,:m,:], target_new\n","6f9a7bc0":"def data_generator(X, y ,batch_size=64, center_only=False, data_dir='data\/'):\n    '''\n    Generate a batch of training images and targets from a DataFrame.\n    \n    Inputs:\n        X: array-like of paths to images\n        y: array-like of targerts (in order of X)\n    '''\n    # Loop forever so the generator never terminates\n    while True:\n        # Shuffle the image paths and targets\n        X_final = []\n        y_final = []\n        X_shuffled, y_shuffled = sklearn.utils.shuffle(X, y, n_samples=batch_size)\n        # We grab the first element since there is 1 column\n        for img_path,target in zip(X_shuffled,y_shuffled):\n            fname = data_dir+img_path\n            img = cv2.imread(fname[0])\n            # Skip specifically for the center image (still checks left\/right)\n            steer_thresh = {'left':-0.01, 'right':0.005}\n            drop_ratio = 0.3\n            \n            if skip_low_steering(target[0], steer_thresh, drop_ratio):\n                X_final.append(img)\n                y_final.append(target[0])\n                # Use horizontally flipped images (new target)\n                img_flipped, target_flipped = flip_image(img,target)\n                X_final.append(img_flipped)\n                y_final.append(target_flipped[0])\n            # Check if we should use all images or just center\n            if not center_only:\n                # Translate the image randomly\n                img_trans, target_trans = translate_image(img, target[0], scale_factor=0.5)\n                X_final.append(img_trans)\n                y_final.append(target_trans)\n                \n                # Order: center, left, right\n                # Corret left image target & add image with target to array\n                img_l = cv2.imread(fname[1])\n                img_l, target_l = adjust_offcenter_image(img_l, target, 0.25, 'left')\n                # Corret right image target & add image with target to array                \n                img_r = cv2.imread(fname[2])\n                img_r, target_r = adjust_offcenter_image(img_r, target, 0.25, 'right')\n\n                X_final.append(img_l)\n                y_final.append(target_l[0])\n                # Use horizontally flipped images (new target)\n                img_flipped, target_flipped = flip_image(img_l,target_l)\n                X_final.append(img_flipped)\n                y_final.append(target_flipped[0])\n\n                X_final.append(img_r)\n                y_final.append(target_r[0])\n                # Use horizontally flipped images (new target)\n                img_flipped, target_flipped = flip_image(img_r,target_r)\n                X_final.append(img_flipped)\n                y_final.append(target_flipped[0])\n\n                \n\n              \n        batch_x = np.array(X_final)\n        batch_y = np.array(y_final)\n        # Ensures that targets remain as float32\n        yield (batch_x, batch_y.astype('float32'))","d3a42e03":"# Note the multiple images will keep the proper shape\ntemp_generator = data_generator(\n    df_all[['image_center','image_left','image_right']].values,\n    df_all['steer_angle'].values.reshape(-1,1)\n)","b978e308":"imgs,targets = next(temp_generator)","ca2312fd":"# Test to see if image reading works\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1,4, figsize=(20, 20))\nfig.subplots_adjust(wspace=.2)\naxes = axes.ravel()\n\nfor i in range(4):\n    axes[i].imshow(imgs[i])","7914ca4b":"# Adjust the target for off-center images to be used in training\nX = df_all[['image_center','image_left','image_right']].values\ny = df_all[['steer_angle']].values","24d5aa34":"X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(\n                                        X, y, test_size=0.2, random_state=27)","1bf14c31":"# Using reasobable batch size so GPU can process enough images\ntrain_generator = data_generator(X_train, y_train, batch_size=64)\nvalid_generator = data_generator(X_valid, y_valid, batch_size=64)","452685e2":"# Creating a resuable default convolution\nfrom functools import partial\nDefaultConv2D = partial(keras.layers.Conv2D, kernel_initializer='he_normal',\n                        kernel_size=3, activation='elu', padding='SAME')","7a8e88aa":"input_shape = (160,320,3)","b7505c5c":"# Based on https:\/\/developer.nvidia.com\/blog\/deep-learning-self-driving-cars\/\nmodel_list = [\n    # Normalize the images\n    keras.layers.Lambda(lambda x: (x\/255.0) - 0.5, input_shape=input_shape),\n    DefaultConv2D(filters=24, kernel_size=5),\n    keras.layers.MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=36, kernel_size=5),\n    keras.layers.MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=36, kernel_size=5),\n    keras.layers.MaxPooling2D(pool_size=2),     \n    keras.layers.Dropout(0.4),  # Dropout to regularize\n    DefaultConv2D(filters=48),\n    keras.layers.MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=64),\n    keras.layers.MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=64),\n    keras.layers.MaxPooling2D(pool_size=2),\n    keras.layers.Dropout(0.4),  # Dropout to regularize\n    # Fully connected network\n    keras.layers.Flatten(),\n    keras.layers.Dense(units=1024, activation='relu'),\n    keras.layers.Dropout(0.2),  # Dropout to regularize\n    keras.layers.Dense(units=128, activation='relu'),\n    keras.layers.Dropout(0.2),  # Dropout to regularize\n    keras.layers.Dense(units=64, activation='relu'),\n    keras.layers.Dense(units=16, activation='relu'),\n    keras.layers.Dense(units=1)\n]","76e05dd0":"# Adding in model to crop images first\nmodel_list = (\n    [model_list[0]] +\n    # Crop out \"unnecessary parts of the image\"\n    [keras.layers.Cropping2D(cropping=((60,20), (0,0)))] +\n    model_list[1:]\n)","bccef591":"model = keras.models.Sequential(model_list)","067c82c4":"model.compile(\n    loss='mse', \n    optimizer='nadam'\n)","d914b79c":"model.summary()","59efacda":"# Allow early stopping after not changing\nstop_after_no_change = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True\n)","c65f3806":"history = model.fit(\n    x=train_generator,\n    y=None, # Since using a generator\n    batch_size=None, # Since using a generator\n    epochs=256, # Large since we want to ensure we stop by early stopping\n    steps_per_epoch=64, # Ideal: steps*batch_size = # of images\n    validation_data=valid_generator,\n    validation_steps=32,\n    callbacks=[stop_after_no_change]\n)","2b86c6e9":"import matplotlib.pyplot as plt\n%matplotlib inline \n\ndef eval_model(model, model_history, X, y, show=True):\n    '''\n    '''\n    score = model.evaluate(X, y)\n    print(f'Loss: {score:.2f}')\n\n    if show:\n        plt.plot(model_history.history['loss'], label='Loss (training data)')\n        plt.plot(model_history.history['val_loss'], label='Loss (validation data)')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.legend(loc='upper right')\n        plt.show()","8f53abe1":"test_generator = data_generator(X_valid, y_valid, batch_size=64)\nX_test, y_test = next(test_generator)\neval_model(model, history, X_test, y_test)","d6fb6b57":"# Ignore the first epoch since it's typically very high compared to the rest\nplt.plot(history.history['loss'], label='Loss (training data)')\nplt.plot(history.history['val_loss'], label='Loss (validation data)')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.ylim(\n    top=np.median(history.history['val_loss'])+np.std(history.history['val_loss']), \n    bottom=0.0\n)\nplt.legend(loc='upper right')\nplt.show()","80e320a7":"model.save('model.h5')","84d527a7":"# Clean up the data downloaded (not needed for output)\n!rm -rf data\/","dcfa455b":"!rm -rf __MACOSX\/","ef227567":"We can flip the image and the steering angle to better generalize.","9d30aa42":"Let's checkout how the previous model turned while training.","5fffc4ee":"## Split the data into training and validation","853edb9e":"## Using center images only","01f9141e":"# Model","c68d9972":"## Don't use some small steering values","dd7b739a":"# Creating own generator to read data","65d07e42":"# Data Augmentation","f54960c1":"## Adjust off-center images","ad06b150":"# Load the Data","ff6b96fd":"We can do some data augmentation to the images to have more variety in the training material.","c7627ee9":"## Flip the image","16e1efcb":"We'll try just using center images for training the model. If we simply put in the left and right images for the camera angle, we'd likely have issues with the model learning incorrect behavior. There are some techniques that could allow us to use these other images but for simplicity's sake we'll only use the center images for now.","da88a2d9":"We'll load the log data & also load the images (found in the log file).","b80b32ca":"## Translate Images","a35c4a22":"## Read in images by path from log file","caf46e99":"## Download data (if needed)","4788f066":"Since the data is biased towards to low steering (driving straight), randomly drop some of the data to discourage simply driving straight.","22e71ef7":"We'll use the generator created above to read in images by a batch while training (and validating). But to ensure it works, let's test it out below.","9ad27b8b":"### Generators for both train and validation sets","a891a8a0":"## Read in log file","00127b0d":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Load the Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Download-data-(if-needed)\" data-toc-modified-id=\"Download-data-(if-needed)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Download data (if needed)<\/a><\/span><\/li><li><span><a href=\"#Read-in-log-file\" data-toc-modified-id=\"Read-in-log-file-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Read in log file<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Augmentation\" data-toc-modified-id=\"Data-Augmentation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Augmentation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Flip-the-image\" data-toc-modified-id=\"Flip-the-image-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Flip the image<\/a><\/span><\/li><li><span><a href=\"#Adjust-off-center-images\" data-toc-modified-id=\"Adjust-off-center-images-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Adjust off-center images<\/a><\/span><\/li><li><span><a href=\"#Don't-use-some-small-steering-values\" data-toc-modified-id=\"Don't-use-some-small-steering-values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Don't use some small steering values<\/a><\/span><\/li><li><span><a href=\"#Translate-Images\" data-toc-modified-id=\"Translate-Images-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Translate Images<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Creating-own-generator-to-read-data\" data-toc-modified-id=\"Creating-own-generator-to-read-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Creating own generator to read data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-images-by-path-from-log-file\" data-toc-modified-id=\"Read-in-images-by-path-from-log-file-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Read in images by path from log file<\/a><\/span><\/li><li><span><a href=\"#Split-the-data-into-training-and-validation\" data-toc-modified-id=\"Split-the-data-into-training-and-validation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Split the data into training and validation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Generators-for-both-train-and-validation-sets\" data-toc-modified-id=\"Generators-for-both-train-and-validation-sets-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;<\/span>Generators for both train and validation sets<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Using-center-images-only\" data-toc-modified-id=\"Using-center-images-only-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Using center images only<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","9b27ed47":"### Evaluation"}}