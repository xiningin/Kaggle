{"cell_type":{"fcd361ec":"code","1692f6f3":"code","060b49bb":"code","381fab40":"code","ca28fbb4":"code","366de3a4":"code","dfa8df57":"code","19ce9903":"code","46aa9322":"code","d5ef8352":"code","0a3d64a6":"code","a4b13454":"code","5ec5f11d":"code","6cbf7cff":"code","6df29cce":"code","ce97f5f3":"code","2de511c6":"code","90a3f8d2":"code","c6e96628":"code","aa4aee9f":"code","3eb7bb09":"code","4c1ffa4a":"code","667b1434":"code","5d4c21bc":"code","5b84ea80":"code","f89031b6":"code","b3981f2d":"code","acec5b67":"code","7ed8b161":"code","024404e7":"code","b06f4b0e":"code","212cf3fd":"code","d3b12943":"code","bd8edf5b":"code","8f77098d":"code","ba56fecd":"code","bfb5028b":"code","14fe3b57":"code","075a25ec":"code","87394e15":"code","8a85f8b5":"code","43469bcb":"code","57cc7f1c":"code","c93927c9":"code","d4e91e98":"code","45aaf63f":"code","d71e516e":"code","eb40358f":"code","e2da78f2":"code","0d245fbf":"code","d676df30":"code","e6403e3c":"code","59b1f3e4":"code","f883a8e0":"code","cbacb5d5":"code","bec55548":"code","c6efc3be":"code","dc261339":"markdown","06c84358":"markdown","8562516b":"markdown","70a965cd":"markdown","b1c445a4":"markdown","d89ddee2":"markdown","a5da860d":"markdown","f13f3a56":"markdown","6adb1acb":"markdown","29561cfd":"markdown","b8992f58":"markdown","abe72956":"markdown","17d26ea4":"markdown","21dcdb2d":"markdown","6c9fe673":"markdown","a532cae8":"markdown","3a5703a0":"markdown","3d57f48a":"markdown","d004cd8c":"markdown","606c86ce":"markdown","e04aae73":"markdown","0824316e":"markdown","47328a5e":"markdown","ce9233d8":"markdown","4cc7e341":"markdown","f1761790":"markdown","5f0c41d1":"markdown","df31c2f9":"markdown","c3aad047":"markdown","55cbece2":"markdown","e78e6d9e":"markdown","ab875f9e":"markdown","a8775b9e":"markdown","17213c86":"markdown","b94cdd57":"markdown","c23dfc67":"markdown","83cd0a36":"markdown","9257dee9":"markdown","9b51ba0e":"markdown","2e10460d":"markdown","e2846281":"markdown","571c599b":"markdown","eda7c42d":"markdown","6afcdc65":"markdown","e2de5940":"markdown","3b8c80c4":"markdown","e32b0a4e":"markdown","68406dfd":"markdown","72efdc49":"markdown","8596911d":"markdown","117bbe2b":"markdown","c50e1ae4":"markdown","5143c980":"markdown","b3662dec":"markdown","d3c0dcbd":"markdown","444e753c":"markdown","9acde50c":"markdown","c9c3adeb":"markdown","1ed1eca7":"markdown"},"source":{"fcd361ec":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nfrom functools import partial\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1692f6f3":"df_train = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv')","060b49bb":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","381fab40":"resumetable(df_train)","ca28fbb4":"resumetable(df_train)","366de3a4":"total = len(df_train)\nplt.figure(figsize=(15,19))\n\nplt.subplot(311)\ng = sns.countplot(x=\"City\", data=df_train)\ng.set_title(\"City Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"City Names\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.show()","dfa8df57":"tmp_hour = df_train.groupby(['City', 'Hour'])['RowId'].nunique().reset_index()","19ce9903":"plt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","46aa9322":"plt.figure(figsize=(15,12))\n\ntmp = round(((df_train.groupby(['EntryHeading'])['RowId'].nunique() \/ total) * 100)).reset_index()\n\nplt.subplot(211)\ng = sns.countplot(x=\"EntryHeading\",\n                  data=df_train,\n                  order=list(tmp['EntryHeading'].values),\n                  hue='ExitHeading', dodge=True)\ng.set_title(\"Entry Heading by Exit Heading\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Entry Heading Region\", fontsize=17)\ngt = g.twinx()\ngt = sns.pointplot(x='EntryHeading', y='RowId', \n                   data=tmp, order=list(tmp['EntryHeading'].values),\n                   color='black', legend=False)\ngt.set_ylim(0, tmp['RowId'].max()*1.1)\ngt.set_ylabel(\"% of Total(Black Line)\", fontsize=16)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"EntryHeading\", order=list(tmp['EntryHeading'].values), \n                   data=df_train, hue='City')\ng1.set_title(\"Entry Heading Distribution By Cities\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Entry Heading Region\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","d5ef8352":"plt.figure(figsize=(15,6))\ndf_train.IntersectionId.value_counts()[:45].plot(kind='bar')\nplt.xlabel(\"Intersection Number\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 45 most commmon IntersectionID's \", fontsize=22)\n\nplt.show()","0a3d64a6":"df_train.groupby(['IntersectionId', 'EntryHeading', 'ExitHeading'])['RowId'].count().reset_index().head()","a4b13454":"t_stopped = ['TotalTimeStopped_p20',\n             'TotalTimeStopped_p50', \n             'TotalTimeStopped_p80']\nt_first_stopped = ['TimeFromFirstStop_p20',\n                   'TimeFromFirstStop_p50',\n                   'TimeFromFirstStop_p80']\nd_first_stopped = ['DistanceToFirstStop_p20',\n                   'DistanceToFirstStop_p50',\n                   'DistanceToFirstStop_p80']","5ec5f11d":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set', fontsize=22)\nsns.heatmap(df_train[t_stopped + \n                     #t_first_stopped + \n                     d_first_stopped].astype(float).corr(),\n            vmax=1.0,  annot=True)\nplt.show()","6cbf7cff":"from sklearn.preprocessing import minmax_scale\n\ntarget_cols = t_stopped + d_first_stopped","6df29cce":"for col in target_cols:\n    df_train[col+str(\"_minmax\")] = (minmax_scale(df_train[col], feature_range=(0,1)))\n    \nmin_max_cols = ['TotalTimeStopped_p20_minmax', 'TotalTimeStopped_p50_minmax',\n                'TotalTimeStopped_p80_minmax', 'DistanceToFirstStop_p20_minmax',\n                'DistanceToFirstStop_p50_minmax', 'DistanceToFirstStop_p80_minmax']","ce97f5f3":"pca = PCA(n_components=3, random_state=5)\n\nprincipalComponents = pca.fit_transform(df_train[min_max_cols])\n\nprincipalDf = pd.DataFrame(principalComponents)\n\n# df.drop(cols, axis=1, inplace=True)\nprefix='Target_PCA'\nprincipalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\ndf_train = pd.concat([df_train, principalDf], axis=1)","2de511c6":"pca.explained_variance_ratio_[:2].sum()","90a3f8d2":"g = sns.FacetGrid(df_train.sample(50000), col=\"City\", \n                  col_wrap=2, height=5, aspect=1.5, hue='Weekend')\n\ng.map(sns.scatterplot, \"Target_PCA0\", \"Target_PCA1\", alpha=.5 ).add_legend();\ng.set_titles('{col_name}', fontsize=17)\nplt.show()","c6e96628":"#sum of squared distances\nssd = []\n\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=4)\n    km = km.fit(df_train[min_max_cols])\n    ssd.append(km.inertia_)\n    \nplt.plot(K, ssd, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method For Optimal k')\n\nplt.show()","aa4aee9f":"km = KMeans(n_clusters=4, random_state=4)\nkm = km.fit(df_train[min_max_cols])\ndf_train['clusters_T'] = km.predict(df_train[min_max_cols])","3eb7bb09":"tmp = pd.crosstab(df_train['City'], df_train['clusters_T'], \n                  normalize='columns').unstack('City').reset_index().rename(columns={0:\"perc\"})\n\ntotal = len(df_train)\nplt.figure(figsize=(15,16))\n\nplt.subplot(311)\ng = sns.countplot(x=\"clusters_T\", data=df_train)\ng.set_title(\"Cluster Target Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(312)\ng1 = sns.countplot(x=\"clusters_T\", data=df_train, hue='City')\ng1.set_title(\"CITIES - Cluster Target Distribution\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=10)\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(313)\ng1 = sns.boxplot(x=\"clusters_T\", y='Target_PCA0', \n                 data=df_train, hue='City')\ng1.set_title(\"PCA Feature - Distribution of PCA by Clusters and Cities\", \n             fontsize=20)\ng1.set_ylabel(\"PCA 0 Values\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.5)\n\nplt.show()","4c1ffa4a":"plt.figure(figsize=(15,6))\n\nsns.scatterplot(x='Target_PCA0', y='Target_PCA1',\n                hue='clusters_T', data=df_train,\n                palette='Set1')\nplt.title(\"PCA 0 and PCA 1 by Clusters\", fontsize=22)\nplt.ylabel(\"Target PCA 1 values\", fontsize=18)\nplt.xlabel(\"Target PCA 0 values\", fontsize=18)\n\nplt.show()","667b1434":"g = sns.FacetGrid(df_train.sample(500000), col=\"City\", \n                  col_wrap=2, height=4, aspect=1.5, \n                  hue='clusters_T')\n\ng.map(sns.scatterplot, \"Target_PCA0\", \"Target_PCA1\", \n      alpha=.5).add_legend();\ng.set_titles('{col_name}', fontsize=50)\n\nplt.suptitle(\"CITIES \\nPrincipal Component Analysis Dispersion by Cluster\", fontsize=22)\n\nplt.subplots_adjust(hspace = 0.3, top=.85)\n\nplt.show()","5d4c21bc":"g = sns.FacetGrid(df_train.sample(500000), col=\"City\", \n                  col_wrap=2, height=4, aspect=1.5, \n                  hue='clusters_T')\n\ng.map(sns.scatterplot, \"Hour\", \"Target_PCA0\", \n      alpha=.5).add_legend();\ng.set_titles('{col_name}', fontsize=50)\n\nplt.suptitle(\"CITIES \\nPrincipal Component Analysis Dispersion by HOURS AND CLUSTERS\", fontsize=22)\n\nplt.subplots_adjust(hspace = 0.3, top=.85)\n\nplt.show()","5b84ea80":"round(pd.crosstab([df_train['clusters_T'], df_train['Weekend']], df_train['City'],\n            normalize='index' ) * 100,0)","f89031b6":"df_train = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv')","b3981f2d":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df\n\ndf_train = date_cyc_enc(df_train, 'Hour', 24)\ndf_test = date_cyc_enc(df_test, 'Hour', 24) ","acec5b67":"df_train['is_day'] = df_train['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\ndf_test['is_day'] = df_test['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n\ndf_train['is_morning'] = df_train['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\ndf_test['is_morning'] = df_test['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n\ndf_train['is_night'] = df_train['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\ndf_test['is_night'] = df_test['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n\ndf_train['is_day_weekend'] = np.where((df_train['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_day_weekend'] = np.where((df_test['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_mor_weekend'] = np.where((df_train['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_mor_weekend'] = np.where((df_test['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_nig_weekend'] = np.where((df_train['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_nig_weekend'] = np.where((df_test['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)","7ed8b161":"df_train[\"Intersec\"] = df_train[\"IntersectionId\"].astype(str) + df_train[\"City\"]\ndf_test[\"Intersec\"] = df_test[\"IntersectionId\"].astype(str) + df_test[\"City\"]\n\nprint(df_train[\"Intersec\"].sample(6).values)","024404e7":"le = LabelEncoder()\n\nle.fit(pd.concat([df_train[\"Intersec\"],df_test[\"Intersec\"]]).drop_duplicates().values)\ndf_train[\"Intersec\"] = le.transform(df_train[\"Intersec\"])\ndf_test[\"Intersec\"] = le.transform(df_test[\"Intersec\"])","b06f4b0e":"road_encoding = {\n    'Road': 1,\n    'Street': 2,\n    'Avenue': 2,\n    'Drive': 3,\n    'Broad': 3,\n    'Boulevard': 4\n}","212cf3fd":"def encode(x):\n    if pd.isna(x):\n        return 0\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n    return 0","d3b12943":"df_train['EntryType'] = df_train['EntryStreetName'].apply(encode)\ndf_train['ExitType'] = df_train['ExitStreetName'].apply(encode)\ndf_test['EntryType'] = df_test['EntryStreetName'].apply(encode)\ndf_test['ExitType'] = df_test['ExitStreetName'].apply(encode)","bd8edf5b":"directions = {\n    'N': 0,\n    'NE': 1\/4,\n    'E': 1\/2,\n    'SE': 3\/4,\n    'S': 1,\n    'SW': 5\/4,\n    'W': 3\/2,\n    'NW': 7\/4\n}","8f77098d":"df_train['EntryHeading'] = df_train['EntryHeading'].map(directions)\ndf_train['ExitHeading'] = df_train['ExitHeading'].map(directions)\n\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(directions)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(directions)","ba56fecd":"df_train['diffHeading'] = df_train['EntryHeading']-df_train['ExitHeading']  \ndf_test['diffHeading'] = df_test['EntryHeading']-df_test['ExitHeading'] ","bfb5028b":"df_train[\"same_str\"] = (df_train[\"EntryStreetName\"] ==  df_train[\"ExitStreetName\"]).astype(int)\ndf_test[\"same_str\"] = (df_test[\"EntryStreetName\"] ==  df_test[\"ExitStreetName\"]).astype(int)","14fe3b57":"# Concatenating the city and month into one variable\ndf_train['city_month'] = df_train[\"City\"] + df_train[\"Month\"].astype(str)\ndf_test['city_month'] = df_test[\"City\"] + df_test[\"Month\"].astype(str)","075a25ec":"monthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, \n                    'Atlanta8': 3.67, 'Atlanta9': 4.09,'Atlanta10': 3.11, 'Atlanta11': 4.10, \n                    'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n                    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,\n                    'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38,\n                    'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n                    'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, \n                    'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n                    'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 ,\n                    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n\n# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\ndf_train[\"average_rainfall\"] = df_train['city_month'].map(monthly_rainfall)\ndf_test[\"average_rainfall\"] = df_test['city_month'].map(monthly_rainfall)","87394e15":"print(f'Shape before dummy transformation: {df_train.shape}')\ndf_train = pd.get_dummies(df_train, columns=['City' ],\\\n                          prefix=['City'], drop_first=False)\n\nprint(f'Shape after dummy transformation: {df_train.shape}')\n\ndf_test = pd.get_dummies(df_test, columns=['City' ],\\\n                          prefix=['City'], drop_first=False)\n","8a85f8b5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfor col in ['Latitude','Longitude']:\n    scaler.fit(df_train[col].values.reshape(-1, 1))\n    df_train[col] = scaler.transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","43469bcb":"df_train.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'\n              ],axis=1, inplace=True)\ndf_test.drop(['RowId', 'Path',\n              'EntryStreetName','ExitStreetName'],axis=1, inplace=True)","57cc7f1c":"interesting_feat = ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading',\n                    'ExitHeading', 'Hour', 'Weekend', 'Month',\n                    'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend',\n                    'is_nig_weekend', #  'Hour_sin',\n                    'Hour', 'same_str', 'Intersec', 'EntryType',\n                    'ExitType', 'diffHeading', 'average_rainfall', 'is_day',\n                    'City_Boston', 'City_Chicago', 'City_Philadelphia', \n                    'City_Atlanta']\n\ntotal_time = ['TotalTimeStopped_p20',\n              'TotalTimeStopped_p50', \n              'TotalTimeStopped_p80']\n\ntarget_stopped = ['DistanceToFirstStop_p20',\n                  'DistanceToFirstStop_p50',\n                  'DistanceToFirstStop_p80']\n","c93927c9":"X = df_train[interesting_feat]\ny = df_train[total_time + target_stopped]\n\nX_test = df_test[interesting_feat]","d4e91e98":"print(f'Shape of X: {X.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","45aaf63f":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10,\n                                                  random_state=42)","d71e516e":"# Define searched space\nhyper_space = {'objective': 'regression',\n               'metric':'rmse',\n               'boosting':'gbdt', 'gpu_device_id': 0,\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\n               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\n               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\n               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\n               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n               #'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n               #'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","eb40358f":"cat_feat = ['IntersectionId','Hour', 'Weekend','Month', \n            'is_day', 'is_morning', 'is_night', \n            'same_str', 'Intersec', 'City_Atlanta', 'City_Boston',\n            'City_Chicago', 'City_Philadelphia', 'EntryType', 'ExitType']","e2da78f2":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\ndef evaluate_metric(params):\n    \n    all_preds_test ={0:[],1:[],2:[],3:[],4:[],5:[]}\n    \n    print(f'Params: {params}')\n    FOLDS = 4    \n    \n    count=1\n    \n    for i in range(len(all_preds_test)):\n        \n        score_mean = 0\n        \n        kf = KFold(n_splits=FOLDS, shuffle=False, \n                   random_state=42)\n        \n       \n        for tr_idx, val_idx in kf.split(X, y):\n            \n            X_tr, X_vl = X.iloc[tr_idx, :], X.iloc[val_idx, :]\n            y_tr, y_vl = y.iloc[tr_idx], y.iloc[val_idx]\n\n            lgtrain = lgb.Dataset(X_tr, label=y_tr.iloc[:,i])\n            lgval = lgb.Dataset(X_vl, label=y_vl.iloc[:,i])\n\n            lgbm_reg = lgb.train(params, lgtrain, 2000, valid_sets = [lgval],\n                                 categorical_feature=cat_feat,\n                                 verbose_eval=0, \n                                 early_stopping_rounds = 300)\n                        \n        pred_lgb = lgbm_reg.predict(X_val, num_iteration=lgbm_reg.best_iteration)\n        all_preds_test[i] = pred_lgb\n        score_uni = np.sqrt(mean_squared_error(pred_lgb, y_val.iloc[:,i]))\n        print(f'Score Validation : {score_uni}')\n\n\n    pred = pd.DataFrame(all_preds_test).stack()\n    pred = pd.DataFrame(pred)\n    \n    y_val_sc = pd.DataFrame(y_val).stack()\n    y_val_sc = pd.DataFrame(y_val_sc)    \n    \n    count = count +1\n    \n    score = np.sqrt(mean_squared_error(pred[0].values, y_val_sc[0].values ))\n    #score = metric(df_val, pred)\n    \n    print(f'Full Score Run: {score}')\n \n    return {\n        'loss': score,\n        'status': STATUS_OK\n    }","0d245fbf":"# Seting the number of Evals\nMAX_EVALS= 15\n\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, \n                 space=hyper_space,\n                 verbose=-1,\n                 algo=tpe.suggest, \n                 max_evals=MAX_EVALS)\n\n# Print best parameters\nbest_params = space_eval(hyper_space, best_vals)","d676df30":"best_params","e6403e3c":"all_preds ={0:[],1:[],2:[],3:[],4:[],5:[]}","59b1f3e4":"%%time\nimport lightgbm as lgb\n\nfor i in range(len(all_preds)):\n    print(f'## {i+1} Run')\n    X_tr,X_val,y_tr,y_val=train_test_split(X, y.iloc[:,i],\n                                           test_size=0.10, random_state=31)\n\n    xg_train = lgb.Dataset(X_tr, label = y_tr)\n    xg_valid = lgb.Dataset(X_val, label = y_val )\n    \n    lgbm_reg = lgb.train(best_params, xg_train, 10000,\n                      valid_sets = [xg_valid],\n                      verbose_eval=500, \n                      early_stopping_rounds = 250)\n    \n    all_preds[i] = lgbm_reg.predict(X_test, num_iteration=lgbm_reg.best_iteration)\n    \n    print(f\"{i+1} running done.\" )","f883a8e0":"sub  = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")","cbacb5d5":"dt = pd.DataFrame(all_preds).stack()\ndt = pd.DataFrame(dt)\nsub['Target'] = dt[0].values","bec55548":"sub.head()","c6efc3be":"sub.to_csv(\"lgbm_pred_hyperopt_test.csv\", index = False)","dc261339":"## Month rainfall ratio by city and seasons","06c84358":"## Applying the transformation in Entry and Exit Heading Columns","8562516b":"## Hour Feature \n- Let's encode the Hour Features","70a965cd":"# Exploring numerical features\nIf you readed the competition description, you know that these are the target features;\n\nThe targets are: \n- TotalTimeStopped_p20\n- TotalTimeStopped_p50\n- TotalTimeStopped_p80\n- DistanceToFirstStop_p20\n- DistanceToFirstStop_p50\n- DistanceToFirstStop_p80\n\nAnd the as the TimeFromFirstStop is an optional data, I will use it to see the correlations.\n","b1c445a4":"# PCA Dispersion by clusters and by Each City\n- To better understand the patterns, let's plot by Cities","d89ddee2":"# Getting Dummies ","a5da860d":"Nice, now we have the PCA features... Let's see the ratio of explanation of the first two Principal Components","f13f3a56":"## Dropping not used features","6adb1acb":"# EntryHeading and Exit Heading","29561cfd":"### Importing datasets","b8992f58":"# Street Feature\n- Extracting informations from street features","abe72956":"Nice. <br>\nIn Entry and Exit Heading chart:\n- We can note that in general the Entry and Exit Region is exactly the same. \n\nIn Entry by Cities chart:\n- We can note the difference patterns on the cities. It's a very interesting and could give us many interesting insights. ","17d26ea4":"Most part of the first modeling try I got from @danofer<br>\nPlase, visit the kernel with all work here: https:\/\/www.kaggle.com\/danofer\/baseline-feature-engineering-geotab-69-5-lb\n<br>\nThe Catboost model I got from @rohitpatil kernel, Link: https:\/\/www.kaggle.com\/rohitpatil\/geotab-catboost<br>\nSome ideas of modelling I saw on: https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm\n","21dcdb2d":"# Encoding the Regions","6c9fe673":"Nice, this func give us a lot of cool and useful informations;\n- We have only two features with missing values. Entry and Exit StreetName","a532cae8":"### Summary of the data","3a5703a0":"## Ploting Clusters\n- Understanding the cluster distribution\n- Exploring by Cities","3d57f48a":"Cool! We can have a best intuition about the data and how it posible clustered the data. ","d004cd8c":"## Getting the binary if the entry and exit was in the same street","606c86ce":"# Modeling \n- As I was getting problems with my model, I decided to implement the solution of the public kernels\n- I will import the datasets again \n\nMany parts of this implementation I got on @dcaichara Kernel. <br>\nYou can see the kernel here: https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm","e04aae73":"# Date Features\n- Hour Distribution\n- Month Distribution\n","0824316e":"## Creating the new feature","47328a5e":"With the 2 first components we have almost 84% of the data explained. It's a very way to easiest visualize the differences between the patterns.","ce9233d8":"Cool. It gives us a good understand of the boundaries of Clusters. <br>\nI suspect that the cluster 2 is about traffic;\n\nLet's plot it by each city and try to find any pattern in the PCA dispersion.","4cc7e341":"Cool! We can see that Atlanta and Philadelphia have similar pattern of the Cluster 2;<Br>\nThe other cluster seens very similar ","f1761790":"We can note that:\n- The most common value is Philadelphia and it have 45.29% of the total entries.\n- The other categories don't have a so discrepant difference between them. \n?\nLet's \n","5f0c41d1":"### Importing the Main Libraries to work with data","df31c2f9":"Cool. <br>\n\nIn the hours chart:\n- We can see that cities can have different hours patterns.\n- Philadelphia is by far the most common in all hours. Only on 5 a.m that is almost lose to Boston in total entries.\n- Atlanta is the city with less entries in all day, but after 17 p.m to 4a.m it's the second city with more rides \n\nIn the month chart:\n- We can note that the data is about only 6 months (with few values in January and May)\n- Also, the pattern of the Boston City improved througout the time and the others seem very unchanged. \n\nNow, let's explore the Entry and Exit features.\n","c3aad047":"## Running the hyperopt Function","55cbece2":"## Reduce memory usage","e78e6d9e":"## Building Hyperopt Function to be optimized","ab875f9e":"## Importing submission file\n- stacking all results in the same file","a8775b9e":"# Hyperopt Space\n- Here we will set all range of our hyperparameters\n","17213c86":"# PCA\n- To better see the distribution of our metrics, lets apply PCA to reduce the dimensionality of the data","b94cdd57":"## Setting X and y","c23dfc67":"## Welcome to my EDA Kernel\n\n### Description:\nThe dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n<img src=\"https:\/\/cdn.citylab.com\/media\/img\/citylab\/2018\/02\/AP_17153592466989\/facebook.jpg\" alt=\"Italian Trulli\">\n","83cd0a36":"Cool. We can see differet patterns by the Cities and their weekend patterns. ","9257dee9":"Cool!<br>\nWe can see that the best correlation between the metrics are:\n- Distance to First Stop p20 and Total Time Stopped p20 have a high correlation.","9b51ba0e":"## Flag - is day?\nTesting some features about the data","2e10460d":"# NOTE: This Kernel is not finished. \n# Please stay tuned and votes up the kernel, please!","e2846281":"# MinMax Scaling the lat and long","571c599b":"# Intersec - Concatenating IntersectionId and City","eda7c42d":"# Scaling the target\n- Geting the min_max transformation to get clusterization and PCA features","6afcdc65":"# Difference between the regions","e2de5940":"# PCA values by CLUSTERS \n- Let's see in another way how the algorithmn have decided by the clusterization","3b8c80c4":"# City's\n- I will start exploring the distribution of City's because it is a categorical with only a few categorys inside.\n","e32b0a4e":"Nice. <br>\nBased on Elbow Method the best number of cluster is 4. So, let's apply the K means on data.","68406dfd":"# Scatter plot of cities by the PCA ","72efdc49":"### Util functions ","8596911d":"## Spliting data into train and validation","117bbe2b":"# Clusters by the Hours\nI was wondering and I had an insight that I will try to implement here. \n- I think that make a lot of sense explore the hours by the clusters\n- Let's see the distribution of PCA0 and the Clusters by the Hours","c50e1ae4":"## IntersectionID ","5143c980":"# Objective:\nIt's a first contact with the data, so I want to explore it and understand how the data is. \n\nSome important things that is standard to analyze:\n- what are the data types of the features?\n- We have missing values?\n- How many unique values we have in each feature;\n- The shape of full dataset.\n- The entropy of each feature (that show us the level of disorder on this column, it's like a \"messy metric\")\n\nAfter this first analyze we can think in other questions to explore:\n- Which distribution we have in our columns? \n- Which are the most common cities?\n- Which are the distribution of the stops, time, distances?\n- How long is our date range? \n- What are the distribution of the regions?\n\nAnd many more questions;\n\n## <font color=\"red\"> I'm near of grandmaster tier, so, if you find this kernel useful or interesting, please don't forget to upvote the kernel =)<\/font>","b3662dec":"## Concatenating City and Month","d3c0dcbd":"# Label Encoder of Intersecion + City","444e753c":"# KMeans Clusterization\n- First, I will apply the elbow method to find the correct number of cluster we have in our data\n- After it, we will implement the kmeans with the best quantity","9acde50c":"X = reduce_mem_usage(X)\nX_test = reduce_mem_usage(X_test)","c9c3adeb":"# Heatmap Target Features","1ed1eca7":"Nice. <br>\n### In the first chart:\n- We can note that the most common cluster is the 1 that have 73% of all data.\n\n### Second chart: \n- Philadelphia is the most common in the first 3 clusters. \n- Boston is the second most common in 0,1 and the most common on Cluster 3;\n- In the second cluster, Atlanta is the second most common city.\n\n### Third Chart:\n- Is clear to understand how the algorithmn divided the data in PCA values\n\n## NOTE: EVERY TIME I RUN IT, THE VALUES CHANGES, SO SORRY BY THE WRONG \n"}}