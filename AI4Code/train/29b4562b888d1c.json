{"cell_type":{"551e9dd6":"code","e19e6aeb":"code","749536c1":"code","6ceee49f":"code","b64aaf76":"code","b8f54318":"code","3ea6c162":"code","9462ac13":"code","f5462e3e":"code","7c6c9194":"code","27ea45c5":"markdown","4cf2e23b":"markdown","cf3b7e7e":"markdown","0dde1c9e":"markdown","f4e720b6":"markdown","89df2684":"markdown","70e51bef":"markdown","72246409":"markdown","92af332c":"markdown","fe362ae8":"markdown","701447ed":"markdown","9890cb67":"markdown","385f5dd6":"markdown","d4ec4830":"markdown","fcf663ce":"markdown","74c0cb66":"markdown"},"source":{"551e9dd6":"import numpy as np\nimport pandas as pd\ndf = pd.read_excel('..\/input\/salaries\/salary.xlsx')\ndf","e19e6aeb":"df.describe(include = 'object')","749536c1":"df.info()","6ceee49f":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf['company'] = lb.fit_transform(df['company'])\ndf['job'] = lb.fit_transform(df['job'])\ndf['degree'] = lb.fit_transform(df['degree'])\ndf","b64aaf76":"x = df.iloc[:, :3]\ny = df.iloc[:, -1]\nprint(x, y)","b8f54318":"import matplotlib.pyplot as plt\nfig,(ax1, ax2, ax3) =  plt.subplots(1,3)\n\nax1.scatter(df['company'], y)\nax1.set_title('company V\/S salary')\nax1.set_ylabel('Salary')\nax1.set_xlabel('company')\n\nax2.scatter(df['job'], y)\nax2.set_title('job V\/S salary')\nax2.set_ylabel('Salary')\nax2.set_xlabel('job')\n\nax3.scatter(df['degree'], y)\nax3.set_title('degree V\/S salary')\nax3.set_ylabel('Salary')\nax3.set_xlabel('degree')","3ea6c162":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(x, y)","9462ac13":"clf.predict([[2,2,0]])","f5462e3e":"clf.predict([[1,1,1]])","7c6c9194":"clf.score(x, y)","27ea45c5":"The Bayesian's rule is given as follows:\n![](https:\/\/miro.medium.com\/max\/1994\/1*CnoTGGO7XeUpUMeXDrIfvA.png)","4cf2e23b":"**Baysian Rule:** It is described by [Thomas Baysian](https:\/\/en.wikipedia.org\/wiki\/Thomas_Bayes). It describe the probability of an event, based on the prior knowledge of the condition. \n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d4\/Thomas_Bayes.gif)","cf3b7e7e":"Check the score of the model. I think, this model won't be much accurate, because we have only 16 example for training. As we will increase the training data, the accuracy will increase.","0dde1c9e":"So, our model is 68.75% accurate. We can increase accuracy by increasing the number of examples of training data.","f4e720b6":"Check the count and the number of unique values in the categorical data. For that use the following code.","89df2684":"**The proof of the Bayesian theorem is as follows. It is very simple. For that we have some knowledge of the conditional probability. We can see more about conditional probability [here](https:\/\/en.wikipedia.org\/wiki\/Conditional_probability).**\n\n![](https:\/\/techshayari.files.wordpress.com\/2019\/02\/bayes-theorem-proof-1.jpg)","70e51bef":"It is based on the Bayesian rule. First we will see the Bayesian rule and then we will see the multinomial naive Bayes.","72246409":"There are three types of naive Bayes:\n1. Bernoulli Naive Bayes\n2. Multinomial Naive Bayes\n3. Guassian Naive Bayes\n\nThere are more also. To know more about it [click here!](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)","92af332c":"Yes, It has returned 0. That is he\/she will not get salary more than 100k.\n\nLet's see another example.","fe362ae8":"**Conclusion:** We have build a model of classification using Multinomial Naive Bayes.\n\n\n\n\n\n\n<center style=  \"margin-top: 80px;\">Thank You.<\/center>","701447ed":"<h1 style = 'color:purple; font-size:50px; font-weight:bolder; text-align:center'>Classification using Multinomial Naive Bayes<\/h1>","9890cb67":"Now, our model is trained. Now let's classify the user with values 2,2,0.","385f5dd6":"<p style = 'color:red' >Classification Using Multinomial <\/p>\n\nI will use the salaries dataset to classification problem. I will build a model that will make classification. So, first I will read a excel file using pandas library.","d4ec4830":"Let's preprocess the data. Convert the categorical data into numeric. So, that we can build a model. I had covered all techniques of convetring categorical data into numeric in [this](https:\/\/www.kaggle.com\/omkarsantoshraut\/data-pre-processing) notebook. I will use LabelEncoder for this example.","fcf663ce":"Now, store input and output data into two seperate variables.","74c0cb66":"**Let's build a model of classification using multinomial naive Bayes. For that I will use MultinomialNB from sklearn.naive_bayes library. I will train the model by using fit() method.**"}}