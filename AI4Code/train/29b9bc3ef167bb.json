{"cell_type":{"a92f3805":"code","a246af49":"code","e2e3a4ca":"code","9809dc70":"code","8f386004":"code","8cbe8ac3":"code","de64aa4a":"code","52e178b7":"code","6293be91":"code","45d187a5":"code","45e35569":"code","016e88cc":"code","ace765ed":"code","abbb2545":"code","38d6cca1":"code","c2fa045a":"code","7292ca2d":"code","0ef83cd4":"code","106f8461":"code","cdef2c9b":"code","9354ef0c":"markdown","23715a9a":"markdown","429ba46d":"markdown"},"source":{"a92f3805":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a246af49":"data = pd.read_csv('..\/input\/data.csv')","e2e3a4ca":"data.info()","9809dc70":"data.head(5)","8f386004":"data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\ndata.info()","8cbe8ac3":"diagnosis_all = list(data.shape)[0]\ndiagnosis_categories = list(data['diagnosis'].value_counts())\n\nprint(\"The data has {} diagnosis, {} malignant and {} benign.\".format(diagnosis_all,\n                                                                      diagnosis_categories[0], \n                                                                      diagnosis_categories[1]))","de64aa4a":"features_mean= list(data.columns[1:11])","52e178b7":"plt.figure(figsize=(9,9))\nsns.heatmap(data[features_mean].corr(), annot=True, square = True,cmap='coolwarm')\nplt.show()","6293be91":"bins = 12\nplt.figure(figsize=(9,9))\nfor i, feature in enumerate(features_mean):\n    rows = int(len(features_mean)\/2)\n    \n    plt.subplot(rows, 2, i+1)\n    \n    sns.distplot(data[data['diagnosis']=='M'][feature], bins=bins, color='red', label='M');\n    sns.distplot(data[data['diagnosis']=='B'][feature], bins=bins, color='blue', label='B');\n    \n    plt.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()","45d187a5":"data_shuffle=data.iloc[np.random.permutation(len(data))]\ndata_y=data_shuffle.reset_index(drop = True)","45e35569":"data.head(5)","016e88cc":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nimport time\ndata_y['diagnosis'] =data_y['diagnosis'].map({'M':1, 'B':0})\ndata_class=data_y['diagnosis'].values","ace765ed":"training_indices,validation_indices = training_indices, testing_indices = train_test_split(data_y.index, \n                                                                                          stratify= data_class, train_size = 0.75, test_size = 0.25)","abbb2545":"!pip install genepy","38d6cca1":"from operator import add, sub,mul\nfrom genepy.core import EvolutionaryAlgorithm","c2fa045a":"def compute_fitness(tree, features, data):\n    \"\"\"\n    Computes a normalized MAE on the predictions made by one tree.\n    \"\"\"\n    predicted = [tree.predict(feat) for feat in features]\n    difference = [abs(predicted[i] - data[i]) for i in range(len(data))]\n    mae = reduce(lambda a,b: a+b, difference) \/ len(data)\n    fitness = 1 \/ mae if mae != 0 else 1.0\n    fitness \/= len(tree.nodes)\n    return fitness","7292ca2d":"parameters = {\n  'min_depth':        3,\n  'max_depth':        5,\n  'nb_trees':         50,\n  'max_const':        100,\n  'func_ratio':       0.5,\n  'var_ratio':        0.5,\n  'crossover_prob':   0.8,\n  'mutation_prob':    0.2,\n  'iterations':       1000,\n  'functions':        [add,sub,mul],\n  'fitness_function': compute_fitness\n}","0ef83cd4":"\nfrom functools import reduce\nea = EvolutionaryAlgorithm(**parameters)\nea.fit(data_y.drop('diagnosis',axis=1).loc[training_indices].values,data_y.loc[training_indices,'diagnosis'].values)\n","106f8461":"predicted = ea.predict(testing_indices)","cdef2c9b":"print('The Prediction accuracy is :', predicted* 0.1, '%')","9354ef0c":"**Preprocessing**","23715a9a":"**Importing Libraries **","429ba46d":"**Reading Dataset**"}}