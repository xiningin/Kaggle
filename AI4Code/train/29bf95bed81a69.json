{"cell_type":{"dbf1b3b6":"code","a489f6e4":"code","675c2b1b":"code","4024627a":"code","5798faac":"code","726b82e1":"code","6cdf27c5":"code","21dfc754":"code","ae9faf38":"code","45adda5b":"code","2d5c6c93":"code","d507f8cb":"code","8e4bc5ff":"code","0a973b09":"code","2ad8cf02":"code","b6a03b75":"code","6e68f38e":"code","2483a043":"code","96c316dd":"code","5422ad25":"code","dc00fdcc":"code","1681922a":"code","dcd86bcb":"code","17a97b19":"code","3b084e97":"code","220e5bce":"code","99320213":"code","8982adf9":"code","ffffade5":"code","7ede6936":"code","8495ccd3":"code","48aa8914":"code","1921221e":"code","06da57e3":"code","5e72d87c":"code","d1093d3a":"code","49d995e1":"code","00ce6093":"code","ddbb6d4c":"code","ff7c95a9":"code","c8bbe3ec":"code","b6f7092c":"code","f0b2612f":"code","8f5047cc":"code","a529f370":"code","cb195500":"code","7bbf7c2d":"code","6a1f5481":"code","3e7647ed":"code","2d228829":"code","50d84f15":"code","3edb7221":"code","679ecf6a":"code","82b967cd":"markdown","9ce32e50":"markdown","5f717a2f":"markdown","2e26ca65":"markdown","dbb3ee8f":"markdown","d57bcc5b":"markdown","0d4d63d8":"markdown","ccbf08a3":"markdown","8a3051f0":"markdown","7dedc252":"markdown","72d29466":"markdown","7257e9a1":"markdown","f1a67cb5":"markdown","0217889b":"markdown","ba325e79":"markdown"},"source":{"dbf1b3b6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nimport logging\nimport nltk\nimport string\nimport collections\nfrom collections import Counter\nimport wordcloud\nfrom wordcloud import WordCloud","a489f6e4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","675c2b1b":"pd.set_option('display.max_colwidth',200)","4024627a":"logging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s :: %(name)s :: %(levelname)s :: %(message)s',datefmt='%d-%b-%y %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.info('Logger initialised...')","5798faac":"train_df=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')","726b82e1":"train_df.shape","6cdf27c5":"train_df['target'].value_counts()","21dfc754":"sns.countplot(train_df['target'])","ae9faf38":"logger.info(\"% of samples where keyword column is 0\")\nlen(train_df[train_df['keyword'].isna()])*100\/len(train_df)","45adda5b":"logger.info(\"% of samples where location column is 0\")\nlen(train_df[train_df['location'].isna()])*100\/len(train_df)","2d5c6c93":"sns.barplot(train_df['keyword'].value_counts()[:20].values,train_df['keyword'].value_counts()[:20].index,orient=\"H\")","d507f8cb":"## Keyword chart when target is 1\nsns.barplot(train_df[train_df['target']==1]['keyword'].value_counts()[:20].values,train_df[train_df['target']==1]['keyword'].value_counts()[:20].index,orient=\"H\")","8e4bc5ff":"sns.barplot(train_df['location'].value_counts()[:20].values,\\\n            train_df['location'].value_counts()[:20].index,orient=\"H\")","0a973b09":"## Location chart when target is 1\nsns.barplot(train_df[train_df['target']==1]['location'].value_counts()[:20].values,\\\n            train_df[train_df['target']==1]['location'].value_counts()[:20].index,orient=\"H\")","2ad8cf02":"def to_lowercases(x):\n    return x.lower()\n\ntrain_df['text']=train_df['text'].apply(to_lowercases)","b6a03b75":"train_df.head(1)","6e68f38e":"def clean_text(x):\n    text = re.sub('(\\d+)','',x)    \n    return text\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\n\ntrain_df.head(2)","2483a043":"def remove_url(x):\n    text = re.sub('(https?:\\\/\\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\\\/([a-zA-Z0-9_]+]*)',' ',x)\n    return text\n\ntrain_df['text'] = train_df['text'].apply(remove_url)","96c316dd":"def remove_punct(x):\n    text_without_puct = [t for t in x if t not in string.punctuation]\n    text_without_puct = ''.join(text_without_puct)\n    return text_without_puct\n\ntrain_df['text'] = train_df['text'].apply(remove_punct)","5422ad25":"def get_tokens(x):\n    tokens = nltk.word_tokenize(x)\n    return tokens\n\ntrain_df['text'] = train_df['text'].apply(get_tokens)","dc00fdcc":"stop_words = nltk.corpus.stopwords.words('english')\n\ndef remove_stop_words(x):\n    text_without_stopwords = [t for t in x if t not in stop_words]\n    \n    return text_without_stopwords\n\ntrain_df['text'] = train_df['text'].apply(remove_stop_words)","1681922a":"from nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()\n\ndef lemmatization(x):\n    try:\n        lemmatized = np.vectorize(lemma.lemmatize)(x)\n        return lemmatized\n    except ValueError:\n        return []\n    \ntrain_df['text_lemm'] = train_df['text'].apply(lemmatization)","dcd86bcb":"train_df.head(2)","17a97b19":"from nltk import FreqDist","3b084e97":"fdist = FreqDist()\ndef freq_dist(x):\n    for word in x:\n        fdist[word]+=1\n    \n    return fdist","220e5bce":"train_df['text_lemm'].apply(freq_dist)[1]","99320213":"fdist = FreqDist()\ndef freq_dist(x):\n    for word in x:\n        fdist[word]+=1\n    \n    return fdist","8982adf9":"most_common = Counter(train_df['text_lemm'].apply(freq_dist)[1]).most_common(50)\nl=[]\nfor k,v in most_common:\n    l.append(k.replace(\"\\'\",''))","ffffade5":"wordcloud = WordCloud(background_color='white',\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1).generate(str(l))\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud)","7ede6936":"fdist = nltk.FreqDist()\ndef bigrams(x):\n    y = list(nltk.bigrams(x))\n    for word in y:\n        fdist[word]+= 1\n    \n    return fdist","8495ccd3":"bigrams = train_df['text_lemm'].apply(bigrams)","48aa8914":"Counter(fdist).most_common(20)","1921221e":"fdist = nltk.FreqDist()\ndef trigrams(x):\n    y = list(nltk.trigrams(x))\n    for word in y:\n        fdist[word]+= 1\n    \n    return fdist","06da57e3":"trigrams = train_df['text_lemm'].apply(trigrams)","5e72d87c":"Counter(fdist).most_common(20)","d1093d3a":"l = []\nfor i in range(len(train_df)):\n    l.append(' '.join(train_df.loc[i,'text_lemm']))","49d995e1":"from sklearn.feature_extraction.text import CountVectorizer\n\ncountvect = CountVectorizer()\n\ncountvect_text = countvect.fit_transform(l)\n\ncountvect_text.get_shape()","00ce6093":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf = TfidfVectorizer()\ntf_text = tf.fit_transform(l)\ntf_text.shape","ddbb6d4c":"f = open('..\/input\/glove-100d-word-embeddings\/glove.6B.100d.txt')\nembeddings = {}\nfor line in f:\n    word = line.split(' ')\n    embeddings[word[0]] = np.asarray(word[1:])\nf.close()","ff7c95a9":"labels = train_df.target.values","c8bbe3ec":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model","b6f7092c":"tokenizer = Tokenizer(num_words=10000)\n\ntokenizer.fit_on_texts(l)\n\nsequences = tokenizer.texts_to_sequences(l)\n\nword_index = tokenizer.word_index","f0b2612f":"print(\"Unique Tokens %s\"%len(tokenizer.word_index))","8f5047cc":"data = pad_sequences(list(sequences),maxlen=20,truncating='post',padding='post')","a529f370":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\n\ndata = data[indices]\nlabels_y = labels[indices]\n\nnb_validation_sample = int(.15*data.shape[0])","cb195500":"x_train = data[:-nb_validation_sample]\ny_train = labels_y[:-nb_validation_sample].reshape(-1,1)\nx_test = data[-nb_validation_sample:]\ny_test = labels_y[-nb_validation_sample:].reshape(-1,1)","7bbf7c2d":"embedding_matrix = np.zeros((len(word_index) + 1, 100))","6a1f5481":"for word,i in word_index.items():\n    try:\n        vector = embeddings[word]\n        if vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = vector\n    except KeyError:\n        continue","3e7647ed":"from tensorflow.keras import backend\n\nfrom tensorflow.keras.layers import Input,Dense,Activation,Embedding,Flatten,LSTM,Dropout,SpatialDropout1D\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint","2d228829":"inp = Input(shape=(x_train.shape[1],))\nemb = Embedding(len(embedding_matrix),\n                100,\n                weights=[embedding_matrix],\n                trainable=False,\n               input_length=20)(inp)\n\n#out = Flatten()(emb)\nout = SpatialDropout1D(rate=0.2)(emb)\nout = LSTM(100)(out)\nout = Dropout(rate=0.2)(out)\n#out = Dense(20,activation='relu')(out)\nout = Dense(1,activation='sigmoid')(out)\nadam = Adam(learning_rate=.001)\n\nmodel = Model(inputs=[inp],outputs=[out])","50d84f15":"model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])","3edb7221":"model.fit(x_train,y_train,verbose=1,\n          batch_size=4,\n          epochs=10,\n          validation_data=[x_test,y_test])","679ecf6a":"model.summary()","82b967cd":"#### Lemmatization","9ce32e50":"#### Remove Stop Words","5f717a2f":"#### Bigrams and Trigrams","2e26ca65":"#### TFIDF","dbb3ee8f":"#### Remove URL","d57bcc5b":"#### Remove Punctuations","0d4d63d8":"#### 1. Make text lowercase","ccbf08a3":"#### Bag of Words","8a3051f0":"## EDA","7dedc252":"#### Clean text","72d29466":"#### Glove Embeddings","7257e9a1":"### Text Analysis","f1a67cb5":"#### Tokenization","0217889b":"## Data Preprocessing","ba325e79":"### Cleaning text\n* Converting Text Lowercase\n* Tokenization\n* Removing Punctuatons\n* Stop Words removal\n* Stemmning\n* Lemmatization\n* POS Tagging"}}