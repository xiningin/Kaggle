{"cell_type":{"0de60b7f":"code","b26fe5e5":"code","4359717c":"code","62d57de9":"code","f4d00e1b":"code","9a295d3f":"code","357488b4":"code","80bdd0d1":"code","ea4627f2":"code","17371417":"code","896c2217":"code","e19014c4":"code","afdb625c":"markdown","3c9faf72":"markdown","e2c2ed86":"markdown","c829fa1d":"markdown","5b7460ae":"markdown","41d9b6b8":"markdown","9a6604bc":"markdown","0e740ea7":"markdown","81942683":"markdown"},"source":{"0de60b7f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nsns.set()\n","b26fe5e5":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain = pd.concat([train, test],axis=0)\nprint(train.describe())","4359717c":"null_cols = train.columns[train.isnull().any()]\nquantity_null_col = train.isnull().any().sum()\nquantity_per_col  = train[null_cols].isnull().sum()\n\nprint(quantity_per_col)\nprint('')\nprint(\"number of features with null columns\")\nprint(quantity_null_col)","62d57de9":"#---------------\n#LotFrontage Imputation\nimputer = SimpleImputer()\nimputer_mode = SimpleImputer(strategy='most_frequent')\n\n#imputer = SimpleImputer(Strategy = 'median')\nimputed_data = imputer.fit_transform(train['LotFrontage'].values.reshape(2919, 1))\ntrain['LotFrontage'] = imputed_data\n\n#Alley\n# train['Alley'] = train['Alley'].replace({'NA':1, 'Pave':2, 'Grvl':3})\ntrain['Alley'].describe()\ntrain['Alley'] = train['Alley'].fillna(1)\ntrain['Alley'] = train['Alley'].replace({'Grvl':2, 'Pave':3})\n\n#MasVnrType\n# lb = LabelBinarizer()\nimputed_data = imputer_mode.fit_transform(train['MasVnrType'].values.reshape(2919,1))\ntrain['MasVnrType'] = imputed_data\n\n#-------Street-----------\ntrain['Street'] = train['Street'].replace({'Grvl':1, 'Pave':0})\n\n#---------LotShape---------\ntrain['LotShape'] = train['LotShape'].replace({'Reg':4, 'IR1':3, 'IR2':2, 'IR3':1})\n\n#------LandContour\ntrain['LandContour'] = train['LandContour'].replace({'Lvl':4, 'Bnk':3, 'HLS':2, 'Low':1})\n\n#----------Utilities---------\ntrain['Utilities'] = train['Utilities'].fillna('AllPub')\ntrain['Utilities'] = train['Utilities'].replace({'AllPub':4, 'NoSewr':3, 'NoSeWa':2, 'ELO':1})\n\n#---------LandSlope\ntrain['LandSlope'] = train['LandSlope'].replace({'Gtl':3, 'Mod':2, 'Sev':1})\n\n#------MasVnrArea---\ntrain['MasVnrArea'] = imputer.fit_transform(train['MasVnrArea'].values.reshape(2919,1))\n\n#------------Exterqual-----------\ntrain['ExterQual'] = train['ExterQual'].replace({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1})\n\n#------ExternCond-------\ntrain['ExterCond'] = train['ExterCond'].replace({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1})\n\n#-----BsmtQual------\ntrain['BsmtQual'] = train['BsmtQual'].fillna(1)\ntrain['BsmtQual'] = train['BsmtQual'].replace({'Ex':6, 'Gd':5, 'TA':4, 'Fa':3, 'Po':2})\n\n#-----BsmtCond\ntrain['BsmtCond'] = train['BsmtCond'].fillna(1)\ntrain['BsmtCond'] = train['BsmtCond'].replace({'Ex':6, 'Gd':5, 'TA':4, 'Fa':3, 'Po':2})\n\n#--------BsmtExposure\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna(1)\ntrain['BsmtExposure'] = train['BsmtExposure'].replace({ 'Gd':5, 'Av':4, 'Mn':3, 'No':2})\n\n#BsmtFinType1\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna(1)\ntrain['BsmtFinType1'] = train['BsmtFinType1'].replace({'Unf':2, 'LwQ':3, 'Rec':4, 'BLQ':5, 'ALQ':6, 'GLQ':7})\n\n#BsmtFinType2\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna(1)\ntrain['BsmtFinType2'] = train['BsmtFinType2'].replace({'Unf':2, 'LwQ':3, 'Rec':4, 'BLQ':5, 'ALQ':6, 'GLQ':7})\n\n#Heating\ntrain['HeatingQC'] = train['HeatingQC'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n#CentralAir\ntrain['CentralAir'] = train['CentralAir'].replace({'Y':1, 'N':0})\n\n#Electrical\ntrain['Electrical'] = train['Electrical'].fillna(2)\ntrain['Electrical'] = train['Electrical'].replace({'Mix':1, 'FuseP':2, 'FuseF':3, 'FuseA':4, 'SBrkr':5})\n\n#kitchenQual\ntrain['KitchenQual'] = train['KitchenQual'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n#FireplceQu\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna(1)\ntrain['FireplaceQu'] = train['FireplaceQu'].replace({'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n#GarageType\nimputed_data_GYB = imputer_mode.fit_transform(train['GarageType'].values.reshape(2919,1))\ntrain['GarageType'] = imputed_data_GYB\n\n#GarageYrBlt\nimputed_data_GYB = imputer_mode.fit_transform(train['GarageYrBlt'].values.reshape(2919,1))\ntrain['GarageYrBlt'] = imputed_data_GYB\n\n#GarageFinish\ntrain['GarageFinish'] = train['GarageFinish'].fillna(1)\ntrain['GarageFinish'] = train['GarageFinish'].replace({'Unf':2, 'RFn':3, 'Fin':4})\n\n#GarageQual\ntrain['GarageQual'] = train['GarageQual'].fillna(1)\ntrain['GarageQual'] = train['GarageQual'].replace({'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n#GarageCond\ntrain['GarageCond'] = train['GarageCond'].fillna(1)\ntrain['GarageCond'] = train['GarageCond'].replace({'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n#PavedDrive\ntrain['PavedDrive'] = train['PavedDrive'].replace({'N':1, 'P':2, 'Y':3})\n\n#PoolQC\ntrain['PoolQC'] = train['PoolQC'].fillna(1)\ntrain['PoolQC'] = train['PoolQC'].replace({'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n#Fence\ntrain['Fence'] = train['Fence'].fillna('None')\n\n#BsmtFinSF1         1\nimputed_data = imputer.fit_transform(train['BsmtFinSF1'].values.reshape(2919, 1))\ntrain['BsmtFinSF1'] = imputed_data\n# BsmtFinSF2         1\ntrain['BsmtFinSF2'] = train['BsmtFinSF2'].fillna(0)\n# BsmtFullBath       2\nimputed_data = imputer_mode.fit_transform(train['BsmtFullBath'].values.reshape(2919,1))\ntrain['BsmtFullBath'] = imputed_data\n# BsmtHalfBath       2\nimputed_data = imputer_mode.fit_transform(train['BsmtHalfBath'].values.reshape(2919,1))\ntrain['BsmtHalfBath'] = imputed_data\n# BsmtUnfSF          1\nimputed_data = imputer.fit_transform(train['BsmtUnfSF'].values.reshape(2919, 1))\ntrain['BsmtUnfSF'] = imputed_data\n# GarageArea         1\nimputed_data = imputer.fit_transform(train['GarageArea'].values.reshape(2919, 1))\ntrain['GarageArea'] = imputed_data\n# GarageCars         1\nimputed_data = imputer_mode.fit_transform(train['GarageCars'].values.reshape(2919,1))\ntrain['GarageCars'] = imputed_data\n# KitchenQual        1\nimputed_data = imputer_mode.fit_transform(train['KitchenQual'].values.reshape(2919,1))\ntrain['KitchenQual'] = imputed_data\n# TotalBsmtSF        1\nimputed_data = imputer.fit_transform(train['TotalBsmtSF'].values.reshape(2919, 1))\ntrain['TotalBsmtSF'] = imputed_data\n# Utilities          2\n","f4d00e1b":"lis_one_hot = ['MSZoning', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n                'BldgType', 'HouseStyle', 'RoofMatl','RoofStyle', 'Exterior1st', 'Exterior2nd',\n                'MasVnrType', 'Foundation', 'Heating', 'Functional', 'GarageType', 'Fence',\n                'MiscFeature', 'SaleType', 'SaleCondition']\n\none_hot = pd.get_dummies(train, columns=lis_one_hot)","9a295d3f":"Ids = one_hot['Id']\ntemp_train = one_hot.iloc[:1460]\ntemp_test = one_hot.iloc[1459:]\n# id_train = one_hot['Id'].iloc[:1461]\nid_test = one_hot['Id'].iloc[1460:]\n# prnt(id_test)\n# print(temp_train)","357488b4":"Y = temp_train['SalePrice']\ntemp_train = temp_train.drop(['Id', 'SalePrice'], axis=1)\ntrain_x, val_x, train_y, val_y = train_test_split(temp_train, Y, random_state = 42)\nprint(train_x.shape, val_x.shape, train_y.shape, val_y.shape)","80bdd0d1":"number_trees = []\nmse_train = []\nmse_val = []\nfor i in range(100,1100,100):\n    print(\"starting training for: \", i)\n    number_trees.append(i)\n    rr = RandomForestRegressor(n_estimators=i, max_depth=25,max_features=60)\n    rr.fit(train_x, train_y)\n    print(\"---predicting---\")\n    predict_train = rr.predict(train_x)\n    predict_val = rr.predict(val_x)\n    print(\"---mse---\")\n    mse_train.append(mean_squared_error(train_y, predict_train))\n    mse_val.append(mean_squared_error(val_y, predict_val))\n    \nfig = plt.figure()\nax = plt.axes()\nplt.plot(number_trees, mse_train,color='r')\nplt.plot(number_trees, mse_val, color='g')\nplt.show()","ea4627f2":"number_trees = []\nmse_train = []\nmse_val = []\n\nfor i in range(2,100,5):\n    print(\"starting training for: \", i)\n    number_trees.append(i)\n    rr = RandomForestRegressor(n_estimators=200, max_depth=25,max_features=i)\n    rr.fit(train_x, train_y)\n    print(\"---predicting---\")\n    predict_train = rr.predict(train_x)\n    predict_val = rr.predict(val_x)\n    print(\"---mse---\")\n    mse_train.append(mean_squared_error(train_y, predict_train))\n    mse_val.append(mean_squared_error(val_y, predict_val))\n    \nfig = plt.figure()\nax = plt.axes()\nplt.plot(number_trees, mse_train,color='r')\nplt.plot(number_trees, mse_val, color='g')\nplt.show()","17371417":"number_trees = []\nmse_train = []\nmse_val = []\n\nfor i in range(2,100,10):\n    print(\"starting training for: \", i)\n    number_trees.append(i)\n    rr = RandomForestRegressor(n_estimators=200, max_depth=i,max_features=60)\n    rr.fit(train_x, train_y)\n    print(\"---predicting---\")\n    predict_train = rr.predict(train_x)\n    predict_val = rr.predict(val_x)\n    print(\"---mse---\")\n    mse_train.append(mean_squared_error(train_y, predict_train))\n    mse_val.append(mean_squared_error(val_y, predict_val))\n    \nfig = plt.figure()\nax = plt.axes()\nplt.plot(number_trees, mse_train,color='r')\nplt.plot(number_trees, mse_val, color='g')\nplt.show()","896c2217":"model = RandomForestRegressor(n_estimators=200, max_depth = 20, max_features=60)\nmodel.fit(temp_train, Y)","e19014c4":"temp_test = temp_test.drop(['SalePrice', 'Id'], axis=1)\nsubmission = model.predict(temp_test)","afdb625c":"Now we'll use one hot encoding for the features where the order of the categories don't matter. One hot encoding gives equal weightage to all the categories. It increases the number of features though, so you have to take steps to ensure that the model will not overfit.","3c9faf72":"![download%282%29.png](attachment:download%282%29.png)\nBeautiful, Its just like Mr.Andre Ng had said. The mse_val and mse_train will converge more and more to a specific value as the complexity of the model increases. Excellent , if you observe the graph, you'll see that the error value is almost constant after 60-65ish. So I'll set max_features to 65. Now let's take a look at the 'max_depth' parametre.","e2c2ed86":"![download%281%29.png](attachment:download%281%29.png)\nThe graph looks something like the image above.The 'n_estimator ' parameter has no effect on the overall MSE of the training or the evaluation data. So I'll be setting 'n_estimators' to around 150. Now lets take a look at  'max_features' parametre.","c829fa1d":"Before applying models, Its wise to massage the given data a little bit.  Let's make a collection of incomplete features in the given data. We can also think about using techniques like imputation to fill the missing spaces in the training data.\n","5b7460ae":"I'll b using a simple Random Forest regressor. Let's draw up some graphs to tune some of the hyper parameters.   Go ahead hit that blue play button for the cell below.","41d9b6b8":"Dropping the target column from the training data and dividing the training data further into evaluation and training sets! ","9a6604bc":"![download%283%29.png](attachment:download%283%29.png)\nThe error is almost constant after 15ish value. nice! So max depth can be set to 15.","0e740ea7":"Aah!! so many darn features to take care of. Let's go step by step. \nIn the cell bellow I've taken care of features where the order of the categories matter. Lots of donkey work.","81942683":"Now it's time gain some insights on the missing data in the training set. The detailed information about the data can be found in the data_description.txt. We'll refer that in the later steps for feature engineering."}}