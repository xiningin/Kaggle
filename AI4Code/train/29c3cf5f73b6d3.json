{"cell_type":{"e97dd55e":"code","238f80eb":"code","60d00099":"code","ecd3c404":"code","56a6c1f3":"code","df640da9":"code","94c203a4":"code","f2111c75":"code","2674c6c3":"code","9c7b8a14":"code","889fd4f4":"code","cb1c287c":"code","d4d041bd":"code","ea183ccf":"code","f1061bad":"code","e156d951":"code","fceea063":"code","06c0c0b7":"code","6aa1f6fb":"code","b15fd95a":"markdown","29266080":"markdown","6ae4593d":"markdown","7333b00f":"markdown","2c8f684f":"markdown","d481f760":"markdown","0c76ee3e":"markdown","5199325a":"markdown","c074ad7b":"markdown","87b6ede8":"markdown","7f6149ac":"markdown","f2ec3f62":"markdown","35877677":"markdown","5c189bd7":"markdown"},"source":{"e97dd55e":"# Let's install tensorflow 2.2 first\n!pip install tensorflow==2.2rc2","238f80eb":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras import optimizers, callbacks\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom tensorflow.keras.applications import vgg19\n\nfrom albumentations import (\n                        PadIfNeeded,\n                        HorizontalFlip,\n                        VerticalFlip,    \n                        CenterCrop,    \n                        Crop,\n                        Compose,\n                        Transpose,\n                        RandomRotate90,\n                        Rotate,\n                        RandomSizedCrop,\n                        OneOf,\n                        CLAHE,\n                        RandomBrightnessContrast,    \n                        RandomGamma    \n                    )\n\n# always set the seed\nseed=1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nsns.set()\n%matplotlib inline","60d00099":"# path to the original files\nfiles_path = Path(\"..\/input\/digitally-reconstructed-radiographs-drr-bones\")\n\n# get all the file names as a list of strings\nfiles = list(map(str, list(files_path.glob(\"**\/*.png\"))))\nprint(\"Total number of files found: \", len(files))\n\n\n# store the above info in a pandas dataframe\nbone_drr = pd.DataFrame([(x, x.replace('.png','_mask.png')) for x in files if not x.endswith('_mask.png')])\nbone_drr.columns = ['image','bones']\nprint(f'Total instances: {bone_drr.shape[0]}')\nbone_drr.head()","ecd3c404":"def plot_random_images(nb_images, df, idx=None, figsize=(15,8)):\n    \"\"\"Plots random images from the data\n    Args:\n        nb_images: Number of images to plot\n        df: dataframe object\n        idx: list of the indices to plot\n        figsize: size of the plot\n    \"\"\"\n    \n    if idx is not None:\n        idx = idx\n        if nb_images != len(idx):\n            raise ValueError(\"\"\"Number of indices and the \n            number of images to plot should be same\"\"\")\n    else:\n        idx = np.random.choice(len(df), size=nb_images)\n        \n    ncols = 2\n    nrows = nb_images\n        \n    f, ax = plt.subplots(nrows, ncols, figsize=figsize)\n    \n    for i, index in enumerate(idx):\n        img = cv2.imread(df['image'][index], 0)\n        bone = cv2.imread(df['bones'][index], 0)\n        ax[i, 0].imshow(img, cmap='gray')\n        ax[i, 1].imshow(bone, cmap='gray')\n        ax[i, 0].axis(\"off\")\n        ax[i, 1].axis(\"off\")\n    plt.show()","56a6c1f3":"plot_random_images(nb_images=4, df=bone_drr, figsize=(15, 20))","df640da9":"IMG_SHAPE = (512, 512, 3)\nSPLIT_IDX = 160\n\n# Shuffle rows in dataframe\nbone_drr = bone_drr.sample(frac=1, random_state=seed)\ndf_train = bone_drr[:SPLIT_IDX].reset_index(drop=True)\ndf_val = bone_drr[SPLIT_IDX:].reset_index(drop=True)","94c203a4":"def load_data_as_numpy_array(df, im_shape):\n    X, y = [], []\n    for i in range(len(df)):\n        img = cv2.imread(df['image'][i], 0)\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        img = cv2.resize(img, im_shape[:2])\n        \n        mask = cv2.imread(df['bones'][i])\n        mask = cv2.resize(mask, im_shape[:2])\n        \n        X.append(img)\n        y.append(mask)\n    \n    X = np.array(X)\n    y = np.array(y)\n    return X, y","f2111c75":"# Load training and validation data\nX_train, y_train = load_data_as_numpy_array(df_train, IMG_SHAPE)\nX_val, y_val = load_data_as_numpy_array(df_val, IMG_SHAPE)","2674c6c3":"class DataGenerator(Sequence):\n    \"\"\"Performs augmentation using albumentations\"\"\"\n    def __init__(self, \n                 data, \n                 labels,\n                 img_dim=IMG_SHAPE, \n                 batch_size=32,  \n                 shuffle=True,\n                 augment=True,\n                ):\n        \"\"\"\n        Args:\n            data: numpy array containing images\n            labels: numpy array containing corresponding masks\n            img_dim: fixed image shape that is to be used\n            batch_size: batch size for one step\n            shuffle: (bool) whether to shuffle the data or not\n            augment: (bool) whether to augment the data or not\n        \n        Returns:\n            A batch of images and corresponding masks\n        \"\"\"\n        \n        self.data = data\n        self.labels = labels\n        self.img_dim = img_dim\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.indices = np.arange(len(self.data))\n        self.augmentations()\n        self.on_epoch_end()\n        \n        \n    def augmentations(self):\n        self.aug = OneOf([VerticalFlip(p=0.2),\n                        HorizontalFlip(p=1.0),\n                        RandomBrightnessContrast(p=0.5),\n                        Rotate(p=1.0, limit=20, border_mode=0)])\n        \n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n    \n    def augment_data(self, img, label):\n        augmenetd = self.aug(image=img, mask=label)\n        return augmenetd['image'], augmenetd['mask']\n\n\n    def __len__(self):\n        return int(np.ceil(len(self.data) \/ self.batch_size))\n    \n    \n    def __getitem__(self, idx):\n        curr_batch = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n        # print(curr_batch)\n        batch_len = len(curr_batch)  \n        X = np.zeros((batch_len, *self.img_dim), dtype=np.float32)\n        y = np.zeros((batch_len, *self.img_dim), dtype=np.float32)\n        \n        for i, index in enumerate(curr_batch):\n            img = self.data[index]\n            label = self.labels[index]\n            if self.augment:\n                img, label = self.augment_data(img, label)\n            img = img.astype(np.float32)\n            img -= img.mean()\n            img \/= img.std()\n            label = label.astype(np.float32) \/ 127.5 -1.\n            \n            X[i], y[i] = img, label\n        return X, y","9c7b8a14":"def conv_block(inputs,\n               filters, \n               kernel_size, \n               dilation_rate=1, \n               padding=\"same\", \n               activation=\"relu\",\n               kernel_initializer=\"he_normal\"):\n    \n    x = layers.Conv2D(filters,\n                      kernel_size=kernel_size,\n                      dilation_rate=dilation_rate,\n                      kernel_initializer=kernel_initializer,\n                      padding=padding,\n                      activation=activation\n                     )(inputs)\n    return x\n\ndef pool_block(inputs, pool=\"max\", pool_size=((2,2)), strides=(2,2)):\n    return layers.MaxPooling2D(strides=strides, pool_size=pool_size)(inputs)","889fd4f4":"def dilated_unet(im_shape, addition=1, dilate=1, dilate_rate=1):\n    x = inputs = layers.Input(im_shape)\n    \n    down1 = conv_block(x, 44, 3)\n    down1 = conv_block(x, 44, 3, dilation_rate=dilate_rate)\n    down1pool = pool_block(down1)\n    \n    down2 = conv_block(down1pool, 88, 3)\n    down2 = conv_block(down1pool, 88, 3, dilation_rate=dilate_rate)\n    down2pool = pool_block(down2)\n    \n    down3 = conv_block(down2pool, 176, 3)\n    down3 = conv_block(down3, 176, 3, dilation_rate=dilate_rate)\n    down3pool = pool_block(down3)\n    \n    if dilate == 1:\n        dilate1 = conv_block(down3pool, 176, 3, dilation_rate=1)\n        dilate2 = conv_block(dilate1, 176, 3, dilation_rate=2)\n        \n        if addition == 1:\n            dilate_all_added = layers.add([dilate1, dilate2])\n            up3 = layers.UpSampling2D((2, 2))(dilate_all_added)\n        else:\n            up3 = layers.UpSampling2D((2, 2))(dilate2)\n            \n    up3 = conv_block(up3, 88, 3)\n    up3 = layers.concatenate([down3, up3])\n    up3 = conv_block(up3, 88, 3)\n    up3 = conv_block(up3, 88, 3)\n    \n    up2 = layers.UpSampling2D((2, 2))(up3)\n    up2 = conv_block(up2, 44, 3)\n    up2 = layers.concatenate([down2, up2])\n    up2 = conv_block(up2, 44, 3)\n    up2 = conv_block(up2, 44, 3)\n    \n    up1 = layers.UpSampling2D((2, 2))(up2)\n    up1 = conv_block(up1, 22, 3)\n    up1 = layers.concatenate([down1, up1])\n    up1 = conv_block(up1, 22, 3)\n    up1 = conv_block(up1, 22, 3)\n    \n    out = layers.Conv2D(1, 1, activation=\"tanh\")(up1)\n    model = models.Model(inputs=inputs, outputs=out)\n    return model","cb1c287c":"dunet = dilated_unet(IMG_SHAPE)\ndunet.summary()","d4d041bd":"vgg = vgg19.VGG19(include_top=False, weights=\"imagenet\", input_shape=(512, 512, 3))\nloss_model = models.Model(inputs=vgg.input, outputs=vgg.get_layer(\"block3_conv3\").output)\nloss_model.trainable = False\nloss_model.summary()","ea183ccf":"def perceptual_loss_vgg19(y_true, y_pred):\n    y_pred = tf.image.grayscale_to_rgb(y_pred, name=None)\n    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))","f1061bad":"class BoneUNet(tf.keras.Model):\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model = base_model\n        \n    def compile(self, base_model_opt, loss_fn):\n        super().compile()\n        self.base_model_optimizer = base_model_opt\n        self.loss_fn = loss_fn\n        \n    def train_step(self, inputs):\n        images, labels =  inputs\n        \n        with tf.GradientTape() as tape:\n            preds = self.base_model(images, training=True)\n            loss = self.loss_fn(labels, preds)\n        \n        grads = tape.gradient(loss, self.base_model.trainable_weights)\n        self.base_model_optimizer.apply_gradients(\n                    zip(grads, self.base_model.trainable_weights)) \n        \n        return {'loss':loss}\n    \n    def call(self, images):\n        preds = self.base_model(images, training=False)    \n        return preds\n        \n    def test_step(self, inputs):\n        images, labels = inputs\n        preds = self.call(images)\n        loss = self.loss_fn(labels, preds)\n        return {'loss': loss}","e156d951":"bone_model = BoneUNet(base_model=dunet)\nbone_model.compile(optimizers.Adam(), loss_fn=perceptual_loss_vgg19)\n\nbatch_size = 8\nepochs = 100\nes = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n\nnb_train_steps = int(np.ceil(len(X_train) \/ batch_size))\nnb_valid_steps = int(np.ceil(len(X_val)) \/ batch_size)\n\ntrain_data_gen = DataGenerator(data=X_train,\n                               labels=y_train,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               augment=False)\n\nvalid_data_gen = DataGenerator(data=X_val,\n                              labels=y_val,\n                              batch_size=batch_size,\n                              shuffle=False,\n                              augment=False)","fceea063":"bone_model.fit(train_data_gen,\n                validation_data=valid_data_gen,\n                epochs=epochs,\n                steps_per_epoch=nb_train_steps,\n                validation_steps=nb_valid_steps,\n                callbacks=[es])","06c0c0b7":"def plot_prediction(filepath, figsize=(16, 8)):\n    orig_img = cv2.imread(filepath, 0)\n    img = orig_img.copy()\n    \n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    img = cv2.resize(img, (512, 512))\n    \n    img = img.astype(np.float32)\n    img -= img.mean()\n    img \/= img.std()\n    \n    pred = bone_model.predict(np.expand_dims(np.array(img), 0))[0, :, :, 0]\n    pred = ((pred + 1)*127.5).astype(np.uint8)\n    pred = cv2.resize(pred, (img.shape[1], img.shape[0]))\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n    axes[0].imshow(orig_img, 'gray')\n    axes[1].imshow(pred, 'gray')\n    plt.show()","6aa1f6fb":"sample_img_path = '..\/input\/padchest-chest-xrays-sample\/sample\/216840111366964012819207061112010316094555679_04-017-068.png'\nplot_prediction(sample_img_path)","b15fd95a":"### Loss function\n\nBefore start training something, let's look at the proposed loss function to use during training.\nAs the training sample is quite small, standard loss functions like MSE or RMSE on pixel level not necessarily translates to good looking bone mask predictions - this was also observed in the paper. Perception loss was introduced - which is a very interesting concept.\n\nThe idea of perception loss is to compare feature maps of proxy model, providing original input (bone mask) and predicted output (predicted bone mask by UNet) - and calculating difference between them. The smaller overall mean difference - the more conceptually similar original and predicted images are! \n\nMoreover, perceptual loss helps us somewhat deal with data being synthetic, as model focuses less on overfitting to the blurry synthetic data we have. This is important for our task - transfer the model to real X-rays.\n\nLet's use good old VGG19 model and take one of its layers as a perception vector.","29266080":"The chest X-ray image looks too blurry to be considered a standard X-ray. Other than that, it is a nice looking image with no artifacts. As for bone mask layer - it appears only bone tissue is visualized - this is what we want to predict on a regular chest X-ray!\n\nSo, we have almost 200 images to work with. It seems like not much, but the images are heavily standardized, thus it may be possible for a network to learn something here. Fingers crossed!","6ae4593d":"### Import all the libraries we need","7333b00f":"# UNet to predict bone layer on chest X-rays\n\nWe are going to use **synthetically** generated X-rays in the form of CT DRR to train UNet model with perception loss. The [dataset](https:\/\/www.kaggle.com\/raddar\/digitally-reconstructed-radiographs-drr-bones) will be used in further experiments.\n\nDigitally Reconstructed Radiograph (DRR) is a novel approach to generate synthetic chest X-rays (CXR) from real CT scan data. Although real data was used, I am using term **synthetic**, as the DRR images were generated in a process described with parametric model (more on that: https:\/\/arxiv.org\/pdf\/2003.10839.pdf). The parameters were selected to pass the human eye test - if the generated image resembles appearance of a real chest X-ray. Moreover, the synthetic images would be considered too blurry and of very low quality compared to X-rays taken with standard X-ray devices.\n\nAlthough DRR has obvious drawbacks, it can make use of most of the good stuff coming from CT scans, like bones, soft tissue, organs, etc. can be easily separated.\n\nThis notebook was an interesting weekend project for me to explore, if chest CT scan data can yield a model suitable for chest X-ray modality. It could also solve some tasks such as segmenting the ribs and other bones - which is a very tedious task to hand label on chest X-rays.","2c8f684f":"### Sample predictions","d481f760":"### Training\nHere are the biggest changes. Although in this case, this isn't necessary, this is to show that how can you override the `train` and `test` steps. Why though? Two reasons:\n\n1. When writing custom training loops, you often lose some good things like callbacks, checkpointing, etc and you have to write these as well with the custom logic. \n2. If you override train and test steps, all you have to change the code at two place only and rest everything would work as with a native keras model. Then, you won't have to write functionlities like custom callbacks and all ","0c76ee3e":"### Dataset prepration\nIf possible, I always like to store the information in a pandas dataframe. Makes life much easier!","5199325a":"### Split dataset into train and validation sets","c074ad7b":"### Model definition\n**UNet + dilated convolutions + perception loss**: The analyzed paper (https:\/\/arxiv.org\/pdf\/2003.10839.pdf) used standard UNet model with dilated convolutions. Let's take one, shamelessly taken from https:\/\/github.com\/GlastonburyC\/Dilated-U-net\/blob\/master\/dilatedUnet.py.\n\n**Note:** Here are the biggest changes in the code. I refactored it to make it less cluttered. Apart from refactoring, the model is same.","87b6ede8":"### Sanity check\nLet's load some images and the corresponding bone images","7f6149ac":"The dataset is small. It's much better to store the images into numpy arrays and work with them afterwards.\n\n**Note**: raddar normalized the image and masks here only. I won't do it here because while doing augmentation, it is better to work with raw pixel integer values.","f2ec3f62":"There is a lot more that can be done in this model to improve the quality of preedictions. I will leave it here as I just wanted to showcase the other things, especially the `train` and `test` steps. Feedback is always welcome!","35877677":"### Augmentation pipeline\nWe want to do augmentation on the fly. I will be using the [albumentations](https:\/\/github.com\/albumentations-team\/albumentations) library for it as it's **the best** library for doing augmentation.\n\n**Note:** raddar used the customized `ImageDataGenerator` but IMO this is a much simpler and better way to do it.","5c189bd7":"Hello Kagglers! I am back to Kaggle after a long time and I thought that it would be good to start with a notebook. \n\n### Disclaimer\nThis is a copy of [raddar's](https:\/\/www.kaggle.com\/raddar) work. The original notebook is [here](https:\/\/www.kaggle.com\/raddar\/bone-drr-unet\/notebook). In this notebook, I will show you the nice things that you will get to use in new Tensorflow version. Also, I refactored the code a lot, so I might have missed something(though dobutful!)."}}