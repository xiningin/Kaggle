{"cell_type":{"e2b61673":"code","8a21e406":"code","de3fbf7c":"code","083cfb0c":"code","77d4b503":"code","d86eec2d":"code","78e8e9a3":"code","6dec831a":"code","39006582":"code","a06d0f9a":"code","8d389286":"code","7c1b650a":"code","38a477b9":"code","ffba0b18":"code","4f1135e1":"code","38c21183":"code","25f55e01":"code","49196576":"code","1277bc14":"code","b61f5776":"code","8e1e4d38":"code","87e1fbc6":"code","27c3c54f":"code","02bbfcb9":"code","0bc0ac4c":"code","420468c0":"code","6615014a":"code","ef81d4d2":"code","8da443c0":"code","c0466f4f":"code","96f98d48":"code","97441f76":"code","1145baf9":"code","b365631f":"code","45e852ec":"code","7bdde7ce":"code","fac76884":"code","cda50622":"code","dc2a520c":"code","b68fc200":"code","9e4d1f5a":"markdown","4961bfa0":"markdown","ec7e8d37":"markdown","ab827454":"markdown"},"source":{"e2b61673":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a21e406":"#data analysis libraries\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.stats import norm, skew\nfrom scipy import stats\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n","de3fbf7c":"#import train and test CSV files\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#take a look at the training data\ndf_train.describe(include='all')","083cfb0c":"#check for duplicate IDs\nidsUni = len(set(df_train.Id))\nidsTotal = len(df_train.Id)\nidsDupli = idsUni - idsTotal\nprint('There are {} duplicate Ids in the training data'.format(idsDupli))\n\n#save the 'Id' column, then drop them\ndf_train_Id = df_train['Id']\ndf_test_Id = df_test['Id']\ndf_train.drop('Id', axis = 1, inplace = True)\ndf_test.drop('Id', axis = 1, inplace = True)\n\n#now we check the size of df_train and df_test\nprint('The train data size: {}'.format(df_train.shape))\nprint('The test data size: {}'.format(df_test.shape))","77d4b503":"#get a list of the features within the dataset\nprint(df_train.columns)","d86eec2d":"#see the first 5 of the train dataset to get an idea of the variables\ndf_train.head(5)","78e8e9a3":"#see the first 5 of the test dataset\ndf_test.head(5)","6dec831a":"#correlation map\ncorrmat = df_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.8, square=True)","39006582":"#saleprice correlation\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","a06d0f9a":"data = pd.concat({\"GrLivArea\":df_train, \"SalePrice\":df_train})\ndata.plot(x = \"GrLivArea\", y = \"SalePrice\", kind = \"scatter\")\nplt.ylabel('SalePrice', fontsize=12)\nplt.xlabel('GrLivArea', fontsize=12)\nplt.show()","8d389286":"data = pd.concat({\"TotalBsmtSF\":df_train, \"SalePrice\":df_train})\ndata.plot(x = \"TotalBsmtSF\", y = \"SalePrice\", kind = \"scatter\")\nplt.ylabel('SalePrice', fontsize=12)\nplt.xlabel('TotalBsmtSF', fontsize=12)\nplt.show()","7c1b650a":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\ndf_train = df_train.drop(df_train[(df_train['TotalBsmtSF']>4000)].index)","38a477b9":"#check the size of df_train after deleting outliers\nprint('The train data size: {}'.format(df_train.shape))","ffba0b18":"# Skew and kurt\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\n\n#check histogrom of saleprice\nsns.distplot(df_train['SalePrice'], fit = norm)\nplt.ylabel('Frequency')\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","4f1135e1":"#we use log1p function for 'SalePrice' column\ndf_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n\n#then plot the target again\nsns.distplot(df_train['SalePrice'], fit = norm)\nplt.ylabel('Frequency', fontsize=12)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","38c21183":"n_train = df_train.shape[0]\nn_test = df_test.shape[0]\ny_train = df_train['SalePrice'].values\ndf_train.drop('SalePrice', axis=1, inplace=True)\n\n#before handling missing data, we connect test and training data\ndf_all = pd.concat((df_train, df_test)).reset_index(drop = True)\nprint('df_all siza: {}'.format(df_all.shape))","25f55e01":"#missing data\ndf_all_NA = pd.isnull(df_all).sum().sort_values(ascending=False)\nNA_percent = ((df_all_NA \/ len(df_all))*100).sort_values(ascending=False)\ndf_all_NA = pd.concat([df_all_NA, NA_percent], axis=1, keys=['Number', 'Percent'])\n#df_all_NA = df_all_NA.drop(df_all_NA[df_all_NA==0].index)\nprint('df_all_NA siza: {}'.format(df_all_NA.shape))","49196576":"df_all_NA.head(35)","1277bc14":"#drop some feature\ndf_all = df_all.drop((df_all_NA[df_all_NA['Percent'] > 40].index), 1)\nprint('df_all size: {}'.format(df_all.shape))","b61f5776":"#input missing value\n\n\ndf_all['LotFrontage'] = df_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nfor c in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath', 'MasVnrArea'):\n    df_all[c] = df_all[c].fillna(0)\n    \nfor c in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    df_all[c] = df_all[c].fillna('None')\n\ndf_all['MSZoning'] = df_all['MSZoning'].fillna(df_all['MSZoning'].mode()[0])\n\ndf_all = df_all.drop(['Utilities'], axis=1)\n\ndf_all[\"Functional\"] = df_all[\"Functional\"].fillna(\"Typ\")\n\ndf_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\n\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])\n\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\n\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\n\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])","8e1e4d38":"#check remains missing value\nprint(pd.isnull(df_all).sum().sort_values(ascending=False))","87e1fbc6":"#convert some numerical feature to categorical\n\ndf_all['OverallCond'] = df_all['OverallCond'].astype(str)\ndf_all['MSSubClass'] = df_all['MSSubClass'].astype(str)\ndf_all['YrSold'] = df_all['YrSold'].astype(str)\ndf_all['MoSold'] = df_all['MoSold'].astype(str)","27c3c54f":"#encode some categorical feature as ordered number\nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_all[c].values)) \n    df_all[c] = lbl.transform(list(df_all[c].values))\n     \nprint('df_all size: {}'.format(df_all.shape))","02bbfcb9":"#skew from the feature\nnumeric_feature = df_all.dtypes[df_all.dtypes != 'object'].index\nskewed_feature = df_all[numeric_feature].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'skew': skewed_feature})\nskewness.head(10)","0bc0ac4c":"#Box Cox\nskewness = skewness[abs(skewness) > 0.5]\nprint('{} skewed numerical features for Box Cox transform'.format(len(skewness)))\n\nfrom scipy.special import boxcox1p\nlam = 0.15\nfor f in skewness.index:\n    df_all[f] = boxcox1p(df_all[f], lam)","420468c0":"#create dummy feature for categorical\ndf_all = pd.get_dummies(df_all)\nprint('df_all size: {}'.format(df_all.shape))","6615014a":"#partition the dataset into train and test \nX_train = df_all[:n_train]\nX_test = df_all[n_train:]","ef81d4d2":"#cross validation\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    return rmse","8da443c0":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import mean_squared_error","c0466f4f":"#LASSO Regression\nlasso = Lasso(alpha =0.0005, random_state=1)\nscore_lasso = rmse_cv(lasso)","96f98d48":"#Elastic Net Regression\nENet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nscore_ENet = rmse_cv(ENet)","97441f76":"#Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore_KRR = rmse_cv(KRR)\n","1145baf9":"#Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore_GBoost = rmse_cv(GBoost)\n","b365631f":"#XGBoost\n#import xgboost as xgb\n#model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n#                             learning_rate=0.05, max_depth=3, \n#                             min_child_weight=1.7817, n_estimators=2200,\n#                             reg_alpha=0.4640, reg_lambda=0.8571,\n#                             subsample=0.5213, silent=1,\n#                             random_state =7, nthread = -1)\n#score_xgb = rmse_cv(model_xgb)\n","45e852ec":"#LightGBM\n#import lightgbm as lgb\n#model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n#                              learning_rate=0.05, n_estimators=720,\n#                              max_bin = 55, bagging_fraction = 0.8,\n#                             bagging_freq = 5, feature_fraction = 0.2319,\n#                              feature_fraction_seed=9, bagging_seed=9,\n#                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n#score_lgb = rmse_cv(model_lgb)\n","7bdde7ce":"models = pd.DataFrame({\n    'Model': ['LASSO Regression', 'Elastic Net Regression', 'Kernel Ridge Regression', \n              'Gradient Boosting Regression'],\n    'Score': [score_lasso.mean(), score_ENet.mean(), score_KRR.mean(), score_GBoost.mean()]})\nmodels.sort_values(by = 'Score', ascending=True)","fac76884":"def get_stacking():\n    #define the base models\n    level0 = list()\n    level0.append(('ENet', ENet))\n    level0.append(('GBoost', GBoost))\n    level0.append(('KRR', KRR))\n    level1 = lasso\n    \n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model","cda50622":"stacking_model = get_stacking()\nscore_stack = rmse_cv(stacking_model)\nprint('Staching Model score: {}'.format(score_stack.mean()))","dc2a520c":"stacking_model.fit(X_train.values, y_train)\nstacking_y_pred = stacking_model.predict(X_train.values)\nstacking_pred = np.expm1(stacking_model.predict(X_test.values))\nprint(np.sqrt(mean_squared_error(y_train, stacking_y_pred)))","b68fc200":"#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({'Id': df_test_Id, 'SalePrice': stacking_pred})\noutput.to_csv('submission.csv', index = False)","9e4d1f5a":"# Data Processing","4961bfa0":"# Modelling","ec7e8d37":"Some of the notebooks I read:\n* [Comprehensive Data Exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\/notebook)\n* [A Study on Regression Applied to the Ames Dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n* [Stacked Regressions](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook)\n* [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)","ab827454":"# Read in and Explore the Data"}}