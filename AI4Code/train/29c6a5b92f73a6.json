{"cell_type":{"92f3eae5":"code","2e51a325":"code","78c0189e":"code","c4fdf0a0":"code","c5256d6d":"code","3d30395a":"code","fa12b1db":"code","7e21c4d7":"code","ab0a5bd4":"code","4011d73f":"code","cd16a290":"code","9e1a8eda":"code","7ff6c130":"code","24962977":"code","d1366e59":"code","991ff983":"code","dd1ec596":"code","3537739e":"code","7d28af39":"code","fbdf7911":"code","70239cc8":"code","7e776d44":"code","f3be5b6f":"code","0556f53f":"code","8b61fb02":"code","aeb87899":"code","bd52568d":"code","9dd8b239":"code","fc6e431d":"code","a4eb3ca9":"code","80cd2f73":"code","b25d74b1":"code","b9b8ac50":"code","9ae40f47":"code","3affc32e":"code","1af85d16":"code","83b70d1e":"code","81734c82":"code","32fb2d34":"code","e8ea8db8":"code","70fa53c1":"code","e24bbda8":"code","674aa25f":"code","52dd0b19":"code","5e6f82ad":"code","baa80c6e":"code","b63f40ef":"code","3851acec":"code","dd9531ad":"code","9e5929b8":"code","51b36a9b":"code","74db2846":"code","1d4ba6f0":"code","1bb897ef":"code","0bc425ef":"code","c8775e51":"code","1586892e":"code","120e9e0e":"code","05e61b9b":"code","6563a879":"code","8f36b40c":"code","ae510ca3":"code","2248f633":"code","7e0b6b06":"code","299d0abe":"code","71afbb3c":"markdown","f96763fe":"markdown","2b712c1d":"markdown","d14094bd":"markdown","2daa928b":"markdown","0ae226be":"markdown","a1fa3883":"markdown","168af6d3":"markdown","3b6576b8":"markdown","27bff142":"markdown","275591d4":"markdown","51b6890c":"markdown","8be83f21":"markdown","ed9762f5":"markdown","fa02723c":"markdown","6049e3b6":"markdown","a276d97f":"markdown","6b13eb23":"markdown","dc4ecc22":"markdown","b6041ef5":"markdown","21643da6":"markdown","03ca9bb0":"markdown","58c65821":"markdown","09a4f23d":"markdown","c82f0b91":"markdown","d7c8f992":"markdown","27a354a7":"markdown","02f1bf63":"markdown","68f38d79":"markdown","469b5bf4":"markdown","98ef9087":"markdown","95201e37":"markdown","6c797fd0":"markdown","a2e74b4c":"markdown","df22490c":"markdown","47757a6f":"markdown","1711dbfa":"markdown","ce93c26b":"markdown","daa44885":"markdown","7ef910b4":"markdown","4cc3482a":"markdown","0f040996":"markdown","d0419e70":"markdown","acb479b7":"markdown","f4f52335":"markdown"},"source":{"92f3eae5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e51a325":"# Plotting Libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\n%matplotlib inline\n\n# Metrics for Classification technique\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# Scaler\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\n# Cross Validation\n\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n\n# Linear Models\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Ensemble Technique\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n\n# Other model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Model Stacking \n\nfrom mlxtend.classifier import StackingCVClassifier\n\n# Other libraries\n\nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.impute import SimpleImputer\nfrom numpy import nan\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","78c0189e":"# Importing Data\n\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata = df.copy()\ndf.head(6) # Mention no of rows to be displayed from the top in the argument","c4fdf0a0":"#Size of the dataset\n\nn = df.shape[0]\nm = df.shape[1]\n\nprint(\"No of rows - \",n,\"| No of columns - \",m)","c5256d6d":"df.info()","3d30395a":"df.describe().transpose()","fa12b1db":"plt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 1.3)\nsns.heatmap(data.corr(),annot=True,cmap='coolwarm',linewidth = 4,linecolor='black')\nplt.tight_layout()","7e21c4d7":"# plt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 2.3)\ndf.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(20, 10), \n                                                   title=\"Correlation with the target feature\")\nplt.tight_layout()","ab0a5bd4":"# Let's check 10 ages and their count\n\nplt.figure(figsize=(25,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.barplot(x=data.age.value_counts()[:10].index,y=data.age.value_counts()[:10].values)\nplt.tight_layout()","4011d73f":"minAge=min(data.age)\nmaxAge=max(data.age)\nmeanAge=data.age.mean()\nprint('Min Age :',minAge)\nprint('Max Age :',maxAge)\nprint('Mean Age :',meanAge)","cd16a290":"Young = data[(data.age>=29)&(data.age<40)]\nMiddle = data[(data.age>=40)&(data.age<55)]\nElder = data[(data.age>55)]","9e1a8eda":"plt.figure(figsize=(23,10))\nsns.set_context('notebook',font_scale = 1.5)\nsns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(Young),len(Middle),len(Elder)])\nplt.tight_layout()","7ff6c130":"data['AgeRange']=0\nyoungAge_index=data[(data.age>=29)&(data.age<40)].index\nmiddleAge_index=data[(data.age>=40)&(data.age<55)].index\nelderlyAge_index=data[(data.age>55)].index","24962977":"for index in elderlyAge_index:\n    data.loc[index,'AgeRange']=2\n    \nfor index in middleAge_index:\n    data.loc[index,'AgeRange']=1\n\nfor index in youngAge_index:\n    data.loc[index,'AgeRange']=0","d1366e59":"plt.figure(figsize=(23,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.violinplot(x=\"AgeRange\",y=\"age\",data=data,palette=[\"r\", \"c\", \"y\"],hue=\"sex\")\nplt.tight_layout()","991ff983":"plt.figure(figsize=(23,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.violinplot(x=\"AgeRange\",y=\"age\",data=data,palette=[\"r\", \"c\", \"y\"],hue=\"sex\")\nplt.tight_layout()","dd1ec596":"colors = ['blue','green','yellow']\nexplode = [0,0,0.1]\nplt.figure(figsize=(10,10))\nsns.set_context('notebook',font_scale = 1.2)\nplt.pie([len(Young),len(Middle),len(Elder)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')\nplt.tight_layout()","3537739e":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(df['sex'])\nplt.tight_layout()","7d28af39":"# Let's plot the relation between sex and slope.\n\nplt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(df['sex'],hue=df[\"slope\"])\nplt.tight_layout()","fbdf7911":"# Let's plot the relation between sex and Age Group.\n\nplt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['sex'],hue=data[\"AgeRange\"])\nplt.tight_layout()","70239cc8":"# Let's plot the relation between sex and target.\n\nplt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['sex'],hue=data[\"target\"])\nplt.tight_layout()","7e776d44":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'])\nplt.tight_layout()","f3be5b6f":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'],hue=data[\"sex\"])\nplt.tight_layout()","0556f53f":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'],hue=data[\"target\"])\nplt.tight_layout()","8b61fb02":"plt.figure(figsize=(23,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.violinplot(x=\"AgeRange\",y=\"age\",data=data,palette=[\"r\", \"c\", \"y\"],hue=\"cp\")\nplt.tight_layout()","aeb87899":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['thal'])\nplt.tight_layout()","bd52568d":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['thal'],hue=data[\"target\"])\nplt.tight_layout()","9dd8b239":"plt.figure(figsize=(23,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.violinplot(x=\"AgeRange\",y=\"age\",data=data,palette=[\"r\", \"c\", \"y\"],hue=\"thal\")\nplt.tight_layout()","fc6e431d":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['target'])\nplt.tight_layout()","a4eb3ca9":"\ntarget_0_agerang_0=len(data[(data.target==0)&(data.AgeRange==0)])\ntarget_1_agerang_0=len(data[(data.target==1)&(data.AgeRange==0)])","80cd2f73":"colors = ['green','red']\nexplode = [0,0.1]\nplt.figure(figsize = (10,10))\nplt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0','Target 1'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Young Age ',color = 'blue',fontsize = 15)\nplt.tight_layout()","b25d74b1":"target_0_agerang_1=len(data[(data.target==0)&(data.AgeRange==1)])\ntarget_1_agerang_1=len(data[(data.target==1)&(data.AgeRange==1)])","b9b8ac50":"colors = ['green','red']\nexplode = [0.1,0]\nplt.figure(figsize = (10,10))\nplt.pie([target_0_agerang_1,target_1_agerang_1], explode=explode, labels=['Target 0','Target 1'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Middle Age',color = 'blue',fontsize = 15)\nplt.show()","9ae40f47":"target_0_agerang_2=len(data[(data.target==0)&(data.AgeRange==2)])\ntarget_1_agerang_2=len(data[(data.target==1)&(data.AgeRange==2)])","3affc32e":"colors = ['green','red']\nexplode = [0,0.1]\nplt.figure(figsize = (10,10))\nplt.pie([target_0_agerang_2,target_1_agerang_2], explode=explode, labels=['Target 0','Target 1'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Elderly Age ',color = 'blue',fontsize = 15)\nplt.show()","1af85d16":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n    print(\"#######\")\n    print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","83b70d1e":"categorical_val.remove('target')\ndfs = pd.get_dummies(df, columns = categorical_val)","81734c82":"dfs.head(6)","32fb2d34":"sc = StandardScaler()\ncol_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndfs[col_to_scale] = sc.fit_transform(dfs[col_to_scale])","e8ea8db8":"dfs.head(6)","70fa53c1":"X = dfs.drop('target', axis=1)\ny = dfs.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e24bbda8":"knn = KNeighborsClassifier(n_neighbors = 10)","674aa25f":"knn.fit(X_train,y_train)","52dd0b19":"y_pred1 = knn.predict(X_test)","5e6f82ad":"print(accuracy_score(y_test,y_pred1))","baa80c6e":"# Hyperparameter Optimization\n\ntest_score = []\nneighbors = range(1, 25)\n\nfor k in neighbors:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    test_score.append(accuracy_score(y_test, model.predict(X_test)))","b63f40ef":"plt.figure(figsize=(18, 8))\nplt.plot(neighbors, test_score, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\nplt.tight_layout()","3851acec":"knn = KNeighborsClassifier(n_neighbors = 19)","dd9531ad":"knn.fit(X_train,y_train)","9e5929b8":"y_pred1 = knn.predict(X_test)","51b36a9b":"print(accuracy_score(y_test,y_pred1))","74db2846":"rfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\ny_pred2 = rfc.predict(X_test)","1d4ba6f0":"print(accuracy_score(y_test,y_pred2))","1bb897ef":"## Hyperparameter Optimization\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n\nparams2 ={\n    \n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)], \n    'max_features': ['auto', 'sqrt'],\n    'max_depth': max_depth, \n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4], \n    'bootstrap': [True, False]\n}","0bc425ef":"rfc = RandomForestClassifier(random_state=42)\n\nrfcs = RandomizedSearchCV(estimator=rfc, param_distributions=params2, n_iter=100, cv=5, \n                               verbose=2, random_state=42, n_jobs=-1)","c8775e51":"rfcs.fit(X_train,y_train)","1586892e":"rfcs.best_estimator_","120e9e0e":"model2 = RandomForestClassifier(max_depth=70, min_samples_leaf=4, min_samples_split=10,\n                       n_estimators=400, random_state=42)","05e61b9b":"model2.fit(X_train,y_train)\ny_pred2 = model2.predict(X_test)","6563a879":"print(accuracy_score(y_test,y_pred2))","8f36b40c":"xgb = XGBClassifier(random_state = 42)\nxgb.fit(X_train,y_train)\ny_pred3 = xgb.predict(X_test)","ae510ca3":"print(accuracy_score(y_test,y_pred3))","2248f633":"model4 = CatBoostClassifier(random_state=42)","7e0b6b06":"model4.fit(X_train,y_train)\ny_pred4 = model4.predict(X_test)","299d0abe":"print(accuracy_score(y_test,y_pred4))","71afbb3c":"# Thal Analysis","f96763fe":"**We achieved accuracy 81% approx with CatBoost Classifier Model.**","2b712c1d":"## Random Forest Classifier","d14094bd":"**Individual Feature Analysis**","2daa928b":"# Thank You!!","0ae226be":"## Importing Necessary Libraries","a1fa3883":"# Modelling","168af6d3":"**Let's plot the relation between Age Group and Target feature.**","3b6576b8":"**Elderly people are more likely to have chest pain.**","27bff142":"* Out of 14 features, we have 13 int type and only one with float data type.\n* Woah! We have no missing values in our dataset.","275591d4":"**Males are more likely to have heart disease than Female.**","51b6890c":"**Let's check the correlation of various features with the target feature.**","8be83f21":"* Four feature( \"cp\", \"restecg\", \"thalach\", \"slope\" ) are positively correlated with the target feature.\n* Other features are negatively correlated with the target feature.","ed9762f5":"**We achieved accuracy 82% approx with XGBoost Classifier Model.**","fa02723c":"**The ratio between 1 and 0 is much less than 1.5 which indicates that target feature is not imbalanced. So for a balanced dataset, we can use accuracy_score as evaluation metrics for our model.**","6049e3b6":"**Let's check the range of age in the dataset.**","a276d97f":"**We achieved accuracy 86% approx with Random Forest Classifier Model. There is no improvement after Hyperparameter Optimization.**","6b13eb23":"**This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis.**","dc4ecc22":"## Target ","b6041ef5":"We will work on following algo - \n\n* KNN\n* Random Forest Classifier\n* XGBoost\n* CatBoost","21643da6":"**Ratio of Male to Female is approx 2:1**","03ca9bb0":"## XGBoost","58c65821":"**People with fixed defect are more likely to have heart disease.**","09a4f23d":"**We should divide the Age feature into three parts - \"Young\", \"Middle\" and \"Elder\"**","c82f0b91":"**Let's check correleation between various features.**","d7c8f992":"**At K=19, we are getting highest test accuracy.**","27a354a7":"**As seen, there are 4 types of chest pain**\n\n1. status at least\n2. condition slightly distressed\n3. condition medium problem\n4. condition too bad","02f1bf63":"**A large proportion of dataset contains Elder people.**","68f38d79":"**Let's plot the relation between Age Group and Sex feature.**","469b5bf4":"1. 3 = normal\n2. 6 = fixed defect\n3. 7 = reversable defect","98ef9087":"**Splitting our dataset**","95201e37":"## Chest Pain Type(\"cp\") Analysis","6c797fd0":"# Heart Disease Prediction using Classification Algorithm\n![image.png](attachment:image.png)\n\n* A brief work is done on EDA and Modelling.\n* I have achieved accuracy of 89% using Hyperparameter Optimization.\n* If you find my work interesting, do upvote it.\n\n\n\n## Problem statement\n\nWe are given with various clinical parameters of a patient and we have to predict whether or not she is suffereing from heart disease.\n\n## Features\n\n* Age (age in years)\n* Sex (1 = male; 0 = female)\n* CP (chest pain type)\n* TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))\n* CHOL (serum cholestoral in mg\/dl)\n* FPS (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* RESTECH (resting electrocardiographic results)\n* THALACH (maximum heart rate achieved)\n* EXANG (exercise induced angina (1 = yes; 0 = no))\n* OLDPEAK (ST depression induced by exercise relative to rest)\n* SLOPE (the slope of the peak exercise ST segment)\n* CA (number of major vessels (0-3) colored by flourosopy)\n* THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* TARGET (1 or 0)\n\n## Introduction\n\nThe three major portion of this notebook are:-\n\n1. Exploratory Data Analysis\n2. Feature Engineering\n3. Modeling with Hyperparameter Optimization","a2e74b4c":"**Elderly people are more likely to suffer from heart disease.**","df22490c":"## Age(\"age\") Analysis","47757a6f":"# Exploratory Data Analysis ","1711dbfa":"## Sex(\"sex\") Feature Analysis","ce93c26b":"**So far the best accuracy is achieved through KNN Model which is 89%.**","daa44885":"# Feature Enginnering","7ef910b4":"* People having least chest pain are not likely to heart disease.\n* People having severe chest pain are  likely to heart disease.","4cc3482a":"## Data Loading\n\nOur first step is to extract train and test data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them.","0f040996":"**We have a dataset with 303 rows which indicates a smaller set of data.**","d0419e70":"**We achieved accuracy 89% with KNN Model after Hyperparameter Optimization.**","acb479b7":"## CatBoost","f4f52335":"## KNN"}}