{"cell_type":{"c7b12868":"code","097bab2b":"code","9d41a912":"code","7b52037a":"code","ef23d26a":"code","4f22a632":"code","867a7a21":"code","da413e11":"code","cc5a4c56":"code","f44e523c":"code","d5547370":"code","45e46779":"code","eaaed092":"code","42166671":"code","6d84acb9":"code","aa2c1904":"code","60d69f7d":"code","97fa5ea1":"code","73205d97":"code","3dd45561":"code","c31cbc4e":"code","c0cc2459":"code","63c7aaa5":"code","3c78b1fd":"code","a71d45e7":"code","78c42e72":"code","14f5392a":"code","ada99c3c":"code","14710026":"code","ce443f5f":"code","3f07aeba":"code","b72bf866":"code","da1740ef":"code","1dde6154":"code","c5ff1b7c":"code","e9121695":"code","61093638":"markdown","5c97cdd1":"markdown","a3dd1f21":"markdown","4f116bdb":"markdown","5b8f1f8d":"markdown","62b69ef8":"markdown","c8843be9":"markdown"},"source":{"c7b12868":"from IPython.display import Image\nImage(url='https:\/\/raw.githubusercontent.com\/MaksimEkin\/COVID19-Literature-Clustering\/master\/cover\/bokeh_plot.png', width=800, height=800)","097bab2b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","9d41a912":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","7b52037a":"meta_df.info()","ef23d26a":"all_json = ! ls $root_path\/document_parses\/pdf_json\nlen(all_json)","4f22a632":"\nall_json = [root_path + \"document_parses\/pdf_json\/\" + s for s in all_json]\nall_json[:5]","867a7a21":"len(all_json)","da413e11":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","cc5a4c56":"from tqdm import tqdm\n\ndef clean_json(all_json):\n    all_json_clean = list()\n    for idx, entry in tqdm(enumerate(all_json), total=len(all_json)):\n\n        try:\n            content = FileReader(entry)\n        except Exception as e:\n            continue  # invalid paper format, skip\n\n        if len(content.body_text) == 0:\n            continue\n\n        all_json_clean.append(all_json[idx])\n    return all_json_clean\n\n    \nall_json = clean_json(all_json)\nlen(all_json)","f44e523c":"import random\n\nrandom.seed(42)\n\nall_json = random.sample(all_json, 50000)","d5547370":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","45e46779":"from tqdm import tqdm\ndict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in tqdm(enumerate(all_json), total = len(all_json)):\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    if len(content.body_text) == 0:\n        continue\n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","eaaed092":"df_covid.info()","42166671":"df_covid.head()","6d84acb9":"df = df_covid\ndf.dropna(inplace=True)\ndf.info()","aa2c1904":"pip install langdetect\n","60d69f7d":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","97fa5ea1":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","73205d97":"df['language'] = languages\ndf = df[df['language'] == 'en'] \ndf.info()","3dd45561":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz ","c31cbc4e":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg","c0cc2459":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","63c7aaa5":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","3c78b1fd":"\n# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","a71d45e7":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)\n","78c42e72":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X","14f5392a":"text = df['processed_text'].values\nmax_features = 2**12\n\nX = vectorize(text, max_features)","ada99c3c":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42)\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","14710026":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans","ce443f5f":"from sklearn import metrics\nfrom scipy.spatial.distance import cdist\n\n# run kmeans with many different k\ndistortions = []\nK = range(2, 30)\nfor k in K:\n    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n    k_means.fit(X_reduced)\n    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n    #print('Found distortion for {} clusters'.format(k))","3f07aeba":"X_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","b72bf866":"k = 20\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['y'] = y_pred","da1740ef":"from sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\nX_embedded = tsne.fit_transform(X.toarray())","1dde6154":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\nplt.title('t-SNE with no Labels')\nplt.savefig(\"t-sne_covid19.png\")\nplt.show()","c5ff1b7c":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(13,9)})\n\n# colors\npalette = sns.hls_palette(20, l=.4, s=.9)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"improved_cluster_tsne.png\")\nplt.show()","e9121695":"y_pred.shape","61093638":"## Loading data","5c97cdd1":"-> L\u1ef1a conhj k=20","a3dd1f21":"L\u1ea5y path t\u1ea5t c\u1ea3 c\u00e1c file json trong th\u01b0 m\u1ee5c document_parses","4f116bdb":"## Data Processing","5b8f1f8d":"## Vectorization","62b69ef8":"## PCA & Clustering","c8843be9":"# COVID-19 Literature Clustering"}}