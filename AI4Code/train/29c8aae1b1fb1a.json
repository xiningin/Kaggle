{"cell_type":{"e5b1ae90":"code","97f9aa4d":"code","c5e800cb":"code","ae6d3793":"code","22ff04e5":"code","b74dd033":"code","293cc45c":"code","a0436166":"code","19b3e699":"code","2ab3de09":"code","daee345d":"code","84823a36":"code","4ee2c896":"code","6fd3384c":"code","4a8c8cc8":"code","43552944":"code","83e50634":"code","5e02037f":"code","69327c6c":"code","fc3357a0":"code","6b0fe1fa":"code","7ab40315":"code","e19191f3":"code","9d2fed88":"code","1e9562e1":"code","5372a88d":"code","9fc3a079":"code","21669da7":"code","569a86c0":"code","02357915":"code","d071514c":"code","25cac9f6":"code","d3a5efd6":"code","188e8136":"code","1db4e25e":"markdown","9e6d80fa":"markdown","015a8df3":"markdown","5b331b92":"markdown","12bfbeb8":"markdown","79cb8c0a":"markdown","c3778ef1":"markdown","549d3b52":"markdown","589415c9":"markdown","661378ef":"markdown","3f06334f":"markdown","46a8e18e":"markdown","0f1ffbc9":"markdown","43b4095d":"markdown"},"source":{"e5b1ae90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97f9aa4d":"# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c5e800cb":"#Load the dataset\ndata = pd.read_csv(\"..\/input\/updated-resume-dataset\/UpdatedResumeDataSet.csv\")","ae6d3793":"#print top 5 rows of dataset.\ndata.head()","22ff04e5":"# Print the last 5 rows.\ndata.tail()","b74dd033":"# total size of dataset.\ndata.size","293cc45c":"#Dataset has 962 rows with 2 columns.\ndata.shape","a0436166":"#Information about columns of dataset.\ndata.info()","19b3e699":"#Check for null values.\ndata.isnull().sum()","2ab3de09":"# Create count plot for categoris columns.\n\nplt.figure(figsize=(22,8))\nax = sns.countplot(x = 'Category', data= data,palette = 'mako')\nax.set_title(\"Count of Categories.\",fontweight = 'bold',size=32)\nplt.xticks(rotation=80)\nax.set_ylabel('Count',fontweight='bold',size=34)\nax.set_xlabel(\"Categories\",fontweight = 'bold',size=34)\nplt.show()","daee345d":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom wordcloud import ImageColorGenerator\ntext = \" \".join(i for i in data.Resume)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\nplt.figure( figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","84823a36":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, len(data)):\n  review = re.sub('[^a-zA-Z]', ' ', data['Resume'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","4ee2c896":"corpus[1]","6fd3384c":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500,ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()\ny = data['Category'].values","4a8c8cc8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","43552944":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","83e50634":"y_pred = classifier.predict(X_test)","5e02037f":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)","69327c6c":"print(acc)","fc3357a0":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))\nX=tfidf_v.fit_transform(corpus).toarray()","6b0fe1fa":"y=data['Category'].values","7ab40315":"from sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","e19191f3":"print(X1_train.shape)\nprint(X1_test.shape)\nprint(y1_train.shape)\nprint(y1_test.shape)","9d2fed88":"count_df = pd.DataFrame(X1_train, columns=tfidf_v.get_feature_names())","1e9562e1":"count_df.head()","5372a88d":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X1_train, y1_train)","9fc3a079":"y1_pred = classifier.predict(X1_test)","21669da7":"cm = confusion_matrix(y1_test, y1_pred)\nacc1 = accuracy_score(y1_test, y1_pred)","569a86c0":"print(acc1)","02357915":"from sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()","d071514c":"classifier.fit(X1_train, y1_train)\npred = classifier.predict(X1_test)\npred = classifier.predict(X1_test)\nscore = accuracy_score(y1_test, pred)\nscore","25cac9f6":"classifier=MultinomialNB(alpha=0.1)","d3a5efd6":"previous_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X1_train,y1_train)\n    y_pred=sub_classifier.predict(X1_test)\n    score = accuracy_score(y1_test, pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {}, Score : {}\".format(alpha,score))","188e8136":"mylist=[]\nmylist2=[]\nmylist.append(score)\nmylist2.append(\"MultinomialNB\")\nmylist.append(acc)\nmylist2.append(\"Naive Bayes(Bag of Words)\")\nmylist.append(acc1)\nmylist2.append(\"Naive Bayes(TF - IDF Vectorizer)\")\nplt.rcParams['figure.figsize']=8,6\nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"ch:s=.25,rot=-.25\", saturation =1.5)\nplt.xlabel(\"Classification Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classification Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","1db4e25e":"# MultinomialNB Algorithm","9e6d80fa":"# Training the Naive Bayes model on the Training set","015a8df3":"# Most common words","5b331b92":"# Multinomial Classifier with Hyperparameter","12bfbeb8":"# Splitting the dataset into the Training set and Test set","79cb8c0a":"# Splitting the dataset into the Training set and Test set","c3778ef1":"# Creating the Bag of Words model","549d3b52":"# Cleaning the Resume texts","589415c9":"# Create visualization for all model with their Accuracy","661378ef":"![](https:\/\/media.istockphoto.com\/illustrations\/thank-you-translation-illustration-id1275501527?k=20&m=1275501527&s=612x612&w=0&h=urN5Ns8Xk1-Ywg97zjOuSv-L9Q5SQdSRnVwap0ZKTq0=)","3f06334f":"# Confusion matrix","46a8e18e":"![](https:\/\/d3kqdc25i4tl0t.cloudfront.net\/articles\/content\/_340594_resumeanatomy.hero.jpg)","0f1ffbc9":"# Naive Bayes","43b4095d":"# Creating the TF - IDF Vectorizer model"}}