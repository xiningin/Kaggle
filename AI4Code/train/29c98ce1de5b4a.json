{"cell_type":{"34523902":"code","ef8643ca":"code","31bc0dc9":"code","63f7d08f":"code","a7fb6b04":"code","706a0e93":"code","2b1abb44":"code","7a033b12":"code","a226b252":"code","95cc92c3":"code","90fa7261":"code","6dd6fca3":"code","b5ef0100":"code","3ee2ee40":"code","3fd1e55d":"code","c02c5dfe":"code","1b310d13":"code","57b5c634":"code","8050bc20":"code","6b9116c9":"code","68e26a7a":"code","d2e9e4fb":"code","05b632ca":"code","29f9adcc":"code","0d5a6060":"code","727241d1":"code","60b94acc":"code","49db7098":"code","4e26f10d":"code","cb4cb314":"code","7872cdf7":"code","091c00ad":"code","905bb67f":"code","3688139d":"code","831bb19b":"code","14bb33b5":"code","1d9eb3df":"code","816375a8":"code","7b8b08bf":"code","3e6a8fe1":"code","a280e416":"code","5e3172fb":"code","0cf7574b":"code","024e1951":"code","9a638fd2":"code","bbf1e33c":"code","92f1e099":"code","5da2920d":"markdown","01762010":"markdown","b74cee31":"markdown","200653c9":"markdown","6c4597db":"markdown","a2556f28":"markdown","aa4a8925":"markdown","0a03aa24":"markdown","958392d7":"markdown","2d79bc99":"markdown","fff52b4d":"markdown","9c5509ef":"markdown","7e84f1ea":"markdown","e3647682":"markdown","e761ca2b":"markdown","0d2f3d44":"markdown","a9c45dde":"markdown","591d22ab":"markdown"},"source":{"34523902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef8643ca":"df = pd.read_csv('..\/input\/medical-insurance-dataset\/Train_Data.csv')\ndf","31bc0dc9":"df.isna().sum()","63f7d08f":"# from sklearn.preprocessing import OneHotEncoder","a7fb6b04":"def preprocess_inputs(df):\n    \n    df = df.copy()\n    #Wrapper for gender\n    sex_wrapper = {'male':0, 'female':1}\n    df.sex = df.sex.replace(sex_wrapper)\n\n    #Wrapper for smoker\n    df.smoker.value_counts()\n    smoker_wrapper = {'no':0, 'yes':1}\n    df.smoker = df.smoker.replace(smoker_wrapper)\n\n    #Dealing with region - OHE\n#     ohe = OneHotEncoder()\n#     feat_array = ohe.fit_transform(df[['region']]).toarray()\n#     df_ohe = pd.DataFrame(feat_array, columns=ohe.categories_)\n#     df = pd.concat([df, df_ohe], axis = 1)\n#     df = df.drop('region', axis=1)\n    df = pd.get_dummies(df, columns=['region'])\n    \n    return df\n","706a0e93":"df = preprocess_inputs(df)","2b1abb44":"df.isna().sum()","7a033b12":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler","a226b252":"y = df.charges\nX = df.drop('charges', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state=1)\n\nscaler = StandardScaler()\n\ns_X_train = scaler.fit_transform(X_train)\ns_X_test = scaler.transform(X_test)\n\n#Linear Regression\n\nLR = LinearRegression()\nLR.fit(s_X_train, y_train)\ny_pred = LR.predict(s_X_test)","95cc92c3":"import math\nfrom sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, r2_score as r2","90fa7261":"#XGBoost Regressor\nxgbr = XGBRegressor(verbosity=0)\nxgbr.fit(X_train, y_train)\n\ny_pred_XGBR = xgbr.predict(X_test)","6dd6fca3":"def get_metrics(preds, actual, model_type):\n    r2s = r2(preds, actual)\n    maes = mae(preds, actual)\n    mses = mse(preds, actual)\n    rmses = math.sqrt(mses)\n#     return rmse, mse, mae, r2\n    df = pd.DataFrame([r2s, maes, mses, rmses, model_type]).transpose()\n    df.columns = ['r2 coeff.', 'MAE', 'MSE', 'RMSE', 'Type of model']\n    return df","b5ef0100":"df_metrics = get_metrics(y_pred, y_test, 'Linear Regression')\ndf_metrics_X = get_metrics(y_pred_XGBR, y_test, 'XGBR')\n\ndf_metrics = pd.concat([df_metrics, df_metrics_X], axis=0)\ndf_metrics","3ee2ee40":"import matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport seaborn as sns","3fd1e55d":"df = pd.read_csv('..\/input\/medical-insurance-dataset\/Train_Data.csv')\ndf","c02c5dfe":"df.isna().sum()","1b310d13":"numerical_columns = ['age', 'bmi', 'charges']\nfor column in numerical_columns:\n    df.boxplot(column)\n    plt.show()","57b5c634":"def gen_zscores(df, col):\n    df['Z-score_'+col] = zscore(df[col])\n\n[gen_zscores(df, nc) for nc in numerical_columns]","8050bc20":"df.describe()","6b9116c9":"df = df[df['Z-score_bmi'] < 4] # Less than 4 because we want all the BMI's where it's less than 4 SD away","68e26a7a":"#Examining 'charges'\ndf[df['Z-score_charges'] > 3]","d2e9e4fb":"df = df.drop(['Z-score_age', 'Z-score_bmi', 'Z-score_charges'], axis=1)\ndf","05b632ca":"df.isna().sum()","29f9adcc":"sns.pairplot(df);","0d5a6060":"# sns.histplot(x='charges', data=df);\ndef gen_histplots(column, df):\n    sns.histplot(x=column, data=df);\n    plt.show();\n    \n[gen_histplots(column, df) for column in numerical_columns]","727241d1":"categorical_columns = df.select_dtypes(include='object').columns\ncategorical_columns","60b94acc":"def gen_countplots(column, df=df):\n    sns.countplot(x=column, data=df);\n    plt.show();\n\n[gen_countplots(column) for column in categorical_columns]","49db7098":"fig, ax = plt.subplots(1, 3, figsize=(15, 10))\nfor var, subplot in zip(categorical_columns, ax.flatten()):\n    sns.boxplot(x=var, y='charges', hue = 'sex', data=df, ax=subplot)","4e26f10d":"fig, ax = plt.subplots(1, 3, figsize=(15, 10))\nfor var, subplot in zip(categorical_columns, ax.flatten()):\n    sns.boxplot(x=var, y='charges', hue = 'smoker', data=df, ax=subplot)","cb4cb314":"fig, ax = plt.subplots(1, 3, figsize=(15, 10))\nfor var, subplot in zip(categorical_columns, ax.flatten()):\n    sns.boxplot(x=var, y='charges', hue = 'region', data=df, ax=subplot)","7872cdf7":"fig, ax = plt.subplots(1, 3, figsize=(15, 10))\nfor var, subplot in zip(categorical_columns, ax.flatten()):\n    sns.boxplot(x=var, y='charges', hue = 'children', data=df, ax=subplot)","091c00ad":"df = preprocess_inputs(df)\ndf","905bb67f":"df.isna().sum()","3688139d":"y = df.charges\nX = df.drop('charges', axis=1)\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state=1)\n\n# scaler = StandardScaler()\n\n# s_X_train = scaler.fit_transform(X_train)\n# s_X_test = scaler.transform(X_test)","831bb19b":"from sklearn.linear_model import Lasso, Ridge,LassoCV, RidgeCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold","14bb33b5":"scaler = StandardScaler()","1d9eb3df":"def test(models, data, iterations):\n    results = {}\n    for i in models:\n        r2_train = []\n        r2_test = []\n        rmse_train = []\n        rmse_test = []\n#         mae_train = []\n#         mae_test = []\n#         mse_train = []\n#         mse_test= []\n        for j in range(iterations):\n            X_train, X_test, y_train, y_test = train_test_split(X, \n                                                                y, \n                                                                test_size= 0.2)\n            \n            s_X_train = scaler.fit_transform(X_train)\n            s_X_test = scaler.transform(X_test)\n            \n            #R2 score\n            r2_train.append(r2(y_train, models[i].fit(s_X_train, y_train).predict(s_X_train)))\n            r2_test.append(r2(y_test, models[i].fit(s_X_train, y_train).predict(s_X_test)))\n            \n            #RMSE\n            rmse_train.append(mse(y_train, models[i].fit(s_X_train, y_train).predict(s_X_train), squared=False))\n            rmse_test.append(mse(y_test, models[i].fit(s_X_train, y_train).predict(s_X_test), squared=False))\n            \n#             #MAE\n#             mae_train.append(mae(y_train, models[i].fit(s_X_train, y_train).predict(s_X_train)))\n#             mae_test.append(mae(y_test, models[i].fit(s_X_train, y_train).predict(s_X_test)))\n            \n#             #MSE\n#             mse_train.append(mse(y_train, models[i].fit(s_X_train, y_train).predict(s_X_train)))\n#             mse_test.append(mse(y_test, models[i].fit(s_X_train, y_train).predict(s_X_test)))\n            \n            \n        \n        results[i] = [np.mean(r2_train), np.mean(r2_test), np.mean(rmse_train), np.mean(rmse_train)]\n#                      np.mean(mae_train), np.mean(mae_test), np.mean(mse_train), np.mean(mse_test)]\n        \n    return pd.DataFrame(results, index=['Train_r2', 'Test_r2', 'Train_RMSE', 'Test_RMSE'])\n\npd.options.display.float_format = '{:.5f}'.format","816375a8":"models = {'OLS': LinearRegression(),\n         'Lasso': Lasso(),\n         'Ridge': Ridge()}","7b8b08bf":"test(models, df, iterations = 10)","3e6a8fe1":"test(models, df, iterations = 30)","a280e416":"lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\nridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}\n\nmodels2 = {'OLS': LinearRegression(),\n           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params).fit(X, y).best_estimator_,\n           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params).fit(X, y).best_estimator_,}","5e3172fb":"test(models2, df, iterations = 30)","0cf7574b":"from numpy import arange","024e1951":"m = {'LassoCV': LassoCV(alphas=arange(0.01,1,0.01), cv=5, normalize=True).fit(X,y),\n     'RidgeCV': RidgeCV(alphas=[0.001,0.01,1,10], cv=5, normalize=True).fit(X,y),\n     'OLS': LinearRegression()}","9a638fd2":"df_metrics","bbf1e33c":"test(m, df, iterations=30)","92f1e099":"#Re-testing XGBR\nxgbr = XGBRegressor(verbosity=0)\nxgbr.fit(X_train, y_train)\n\ny_pred_XGBR = xgbr.predict(X_test)\n\n\nprint(f'the r2 score for XGBR is: {r2(y_test,y_pred_XGBR)}, and the RMSE is: {mse(y_test,y_pred_XGBR, squared=False)}')\n# print(f'The r2 score for XGBR_train is: {r2(y_train, y_pred_XGBR)} and for XGBR_test its: {r2(y_test, y_pred_XGBR)}')\n      \n# print(f'The rmse score for XGBR_train is: {mse(y_train, y_pred_XGBR, Squared=False)} and for XGBR_test its: {mse(y_test, y_pred_XGBR, Squared=False)}')","5da2920d":"A. Creating box plots to visualize and see if there are any outliers","01762010":"# Regression analysis on Medical Insurance dataset\n# By Anas Puthawala","b74cee31":"'charges' column is alright, no extreme outliers","200653c9":"### Original:","6c4597db":"Let's improve on this a bit\n# Back to the drawing-board\n### Let's perform some feature engineering and outlier analysis to try and fix up our model\n1. Outlier Analysis and EDA will be performed to better understand the dataset\n2. We'll work to implement different regression models (i.e. Lasso, Ridge, OLS) and see how they perform\n3. We'll evaluate and compare the results to the original Linear Regression Model","a2556f28":"Insights:\n1. Right off the bat, if you're a smoker you will usually be incurring higher charges\n2. Northeast region also (sometimes) tends to incur higher charges\n3. Men also tend to incur higher charges","aa4a8925":"# Splitting and training model","0a03aa24":"# Pre-processing","958392d7":"### We're trying to predict 'charges' based off the other features","2d79bc99":"### Revised:","fff52b4d":"Pre-processing","9c5509ef":"### 1. Outlier Analysis & EDA","7e84f1ea":"### 3. Evaluation and comparison to original Linear Regression model and XGBRegessor model","e3647682":"Notes:\nThe initial fit was with an r2 coefficient of .624 and a RMSE of ~5900. The new revised fit deems that the best (non-treebased model) has been improved a decent amount due to the outlier analysis and hyperparameter optimizations","e761ca2b":"BMI might be a bit concerning, only the two or so that might be above 50. We can look a bit further into this to see just how much of an outlier these values are using z-score analysis","0d2f3d44":"Comparing the new XGBR r2 and RMSE scores from the previous model we can see improvements here as well.","a9c45dde":"#### Using LassoCV and RidgeCV to find optimal hyperparameters","591d22ab":"### 2. Implementing different regression models"}}