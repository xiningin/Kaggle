{"cell_type":{"011b73fb":"code","00a01a00":"code","505facc8":"code","18a276f2":"code","4901f6d2":"code","3e1ce82d":"code","81e90b81":"code","aa7971b0":"code","fefee953":"code","5b2a0669":"code","862761f9":"code","e6d3d8d2":"code","4147216a":"code","401d25e4":"code","032e694b":"code","e5262ff5":"code","765db2ee":"code","b8e940f6":"code","9ddf2f78":"code","18d8c350":"code","c1c1ed75":"code","e714d34f":"code","92b7a950":"code","a82b10ab":"code","2f03effb":"code","29414b21":"code","af0f74f0":"code","0a390141":"code","932273bc":"code","4aaa5dda":"code","005001b5":"code","8cef44d7":"code","243adf1f":"markdown","a1566865":"markdown","d29d3e58":"markdown","da0b5c83":"markdown","f20061f1":"markdown","07be0562":"markdown","9c948b63":"markdown","17efc7ad":"markdown","2fda5db0":"markdown","caaa4098":"markdown","4504413e":"markdown","18ff37d1":"markdown","acf78932":"markdown","59c55078":"markdown","91cb85c8":"markdown","e61e686d":"markdown","67dcffe0":"markdown","5604f0c3":"markdown","42577993":"markdown","4710a5dc":"markdown","ad19113e":"markdown","a4b74063":"markdown","4a45733a":"markdown","31e3be9b":"markdown","144965d7":"markdown","dca58aec":"markdown","2cd298de":"markdown","659a8917":"markdown","f58451fe":"markdown","07fe7746":"markdown","74e3fb47":"markdown","7d9c1a7b":"markdown","259f259e":"markdown","e6511cf8":"markdown","e18dc272":"markdown","7f8679cb":"markdown","8abdfcb7":"markdown","778b8d40":"markdown","62911000":"markdown","0f1d5e11":"markdown","adbc4f77":"markdown","23e190da":"markdown","2cd03d26":"markdown","883c7ff4":"markdown","995b6bbe":"markdown","8a5d4b99":"markdown"},"source":{"011b73fb":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom geopy.geocoders import Nominatim\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","00a01a00":"def format_spines(ax, right_border=True):\n    \"\"\"\n    This function sets up borders from an axis and personalize colors\n    \n    Input:\n        Axis and a flag for deciding or not to plot the right border\n    Returns:\n        Plot configuration\n    \"\"\"    \n    # Setting up colors\n    ax.spines['bottom'].set_color('#CCCCCC')\n    ax.spines['left'].set_color('#CCCCCC')\n    ax.spines['top'].set_visible(False)\n    if right_border:\n        ax.spines['right'].set_color('#CCCCCC')\n    else:\n        ax.spines['right'].set_color('#FFFFFF')\n    ax.patch.set_facecolor('#FFFFFF')\n    \ndef count_plot(feature, df, colors='Blues_d', hue=False, ax=None, title=''):\n    \"\"\"\n    This function plots data setting up frequency and percentage in a count plot;\n    This also sets up borders and personalization.\n    \n    Input:\n        The feature to be counted and the dataframe. Other args are optional.\n    Returns:\n        Count plot.\n    \"\"\"    \n    # Preparing variables\n    ncount = len(df)\n    if hue != False:\n        ax = sns.countplot(x=feature, data=df, palette=colors, hue=hue, ax=ax)\n    else:\n        ax = sns.countplot(x=feature, data=df, palette=colors, ax=ax)\n\n    # Make twin axis\n    ax2=ax.twinx()\n\n    # Switch so count axis is on right, frequency on left\n    ax2.yaxis.tick_left()\n    ax.yaxis.tick_right()\n\n    # Also switch the labels over\n    ax.yaxis.set_label_position('right')\n    ax2.yaxis.set_label_position('left')\n    ax2.set_ylabel('Frequency [%]')\n\n    # Setting up borders\n    format_spines(ax)\n    format_spines(ax2)\n\n    # Setting percentage\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text\n    \n    # Final configuration\n    if not hue:\n        ax.set_title(df[feature].describe().name + ' Counting plot', size=13, pad=15)\n    else:\n        ax.set_title(df[feature].describe().name + ' Counting plot by ' + hue, size=13, pad=15)  \n    if title != '':\n        ax.set_title(title)       \n    plt.tight_layout()","505facc8":"metadata = pd.read_csv('..\/input\/global-terrorism-metadata\/global_terrorism_metadata_us.txt', sep=';', index_col='attribute')\nmetadata.head(10)","18a276f2":"terr = pd.read_csv('..\/input\/gtd\/globalterrorismdb_0718dist.csv', encoding='ISO-8859-1')\nterr.head()","4901f6d2":"high_importance = metadata.query('importance == \"high\"')\nhigh_importance","3e1ce82d":"print(f'We have {len(high_importance)} attributes of \"high\" importance')\nterr_data = terr.loc[:, high_importance.index]\nterr_data.head()","81e90b81":"terr_data.dtypes","aa7971b0":"terr_data.isnull().sum()","fefee953":"null_city = terr_data[terr_data.loc[:, ['city']].isnull().values]\nnull_city.head()","5b2a0669":"null_city['region_txt'].value_counts()","862761f9":"null_city['country_txt'].value_counts()","e6d3d8d2":"geolocator = Nominatim(user_agent=\"Y_BzShFZceZ_rj_t-cI13w\")\nlocation = geolocator.reverse(\"52.509669, 13.376294\")\nprint(location.address)","4147216a":"lat_sample = null_city['latitude'].iloc[0]\nlong_sample = null_city['longitude'].iloc[0]\nnull_city.iloc[0, [5, 6, 7]]","401d25e4":"location = geolocator.reverse(lat_sample, long_sample)\nlocation.address","032e694b":"location = geolocator.reverse(str(lat_sample) + ',' + str(long_sample))\nlocation.address","e5262ff5":"lat_10_samples = null_city['latitude'].iloc[:10].values\nlong_10_samples = null_city['longitude'].iloc[:10].values\ncoord_address = []\nfor lat, long in zip(lat_10_samples, long_10_samples):\n    location = geolocator.reverse(str(lat) + ',' + str(long))\n    coord_address.append(location.address)\ncoord_address[:5]","765db2ee":"all_lat = null_city['latitude'].values\nall_long = null_city['longitude'].values\ncoord_address = []\nfor lat, long in zip(all_lat, all_long):\n    try:\n        location = geolocator.reverse(str(lat) + ',' + str(long))\n        coord_address.append(location.address)\n    except:\n        coord_address.append('Unknown')\n        pass\ncoord_address[:10]","b8e940f6":"terr_data.loc[:, 'city'].fillna('Unknown', inplace=True)","9ddf2f78":"terr_data.loc[:, 'natlty1_txt'].fillna('Unknown', inplace=True)\nterr_data.isnull().sum()[np.r_[7, 18]]","18d8c350":"null_analysis = metadata.loc[terr_data.columns[terr_data.isnull().any().values], :]\nattribs_null = ['summary', 'corp1', 'motive', 'nperps', 'nkillter', 'nwoundte']\nnull_analysis = null_analysis.loc[attribs_null, 'null_percent']\nlabels = ['Not Null Entries', 'Null Entries']\ncolors = ['skyblue', 'crimson']","c1c1ed75":"circles = []\nfor i in range(6):\n    circles.append(plt.Circle((0,0), 0.75, color='white'))\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\ni = 0\nj = 0\nk = 0\nfor title, size in zip(null_analysis.index, null_analysis.values):\n    axs[i, j].pie((1-size, size), labels=labels, colors=colors, autopct='%1.1f%%')\n    axs[i, j].set_title(title, size=18)\n    p = plt.gcf()\n    axs[i, j].add_artist(circles[k])\n    j += 1\n    k += 1\n    if j == 3:\n        j = 0\n        i += 1\n    plt.tight_layout()\nplt.show()","e714d34f":"metadata.loc[terr_data.columns].query('data_type == \"qualitative\"')","92b7a950":"fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(13, 7))\ncount_plot('extended', terr_data, ax=axs[0,0])\ncount_plot('success', terr_data, ax=axs[0,1])\ncount_plot('suicide', terr_data, ax=axs[0,2])\ncount_plot('specificity', terr_data, ax=axs[1,0])\ncount_plot('ishostkid', terr_data, ax=axs[1,1])\ncount_plot('region_txt', terr_data, ax=axs[1,2])\naxs[1,2].set_xticklabels(axs[1,2].get_xticklabels(), rotation=90)\nplt.show()","a82b10ab":"terr_data['eventid'] = terr_data['eventid'].astype(str)\nterr_data['event_date'] = terr_data['eventid'].apply(lambda x: x[:4] + '\/' + x[4:6] + '\/' + x[6:8])\ntry: \n    terr_data['event_date'] = pd.to_datetime(terr_data['event_date'])\nexcept ValueError as error:\n    print(f'ValueError: {error}')","2f03effb":"terr_data['event_date'][:5]","29414b21":"terr_data.iloc[:5, [0, 1, 2, 3, -1]]","af0f74f0":"# Days higher than 31 are not correct too\nlen(terr_data.query('iday > 31'))","0a390141":"# months higher than 12?\nlen(terr_data.query('imonth > 12'))","932273bc":"# Applying transformations\nterr_data['iday'] = terr_data['iday'].apply(lambda day: day + 1 if day == 0 else day)\nterr_data['imonth'] = terr_data['imonth'].apply(lambda month: month + 1 if month == 0 else month)\nprint((terr_data['iday'] == 0).any())\nprint((terr_data['imonth'] == 0).any())","4aaa5dda":"year = terr_data['iyear'].astype(str)\nmonth = terr_data['imonth'].astype(str)\nday = terr_data['iday'].astype(str)\nterr_data['event_date'] = year + \"\/\" + month + \"\/\" + day\nterr_data['event_date'] = pd.to_datetime(terr_data['event_date'])\nterr_data.iloc[:5, np.r_[:3, -1]]","005001b5":"terr_data['event_date'].dtype","8cef44d7":"terr_data['day_of_week'] = terr_data['event_date'].apply(lambda x: x.dayofweek)\nterr_data['day_of_week_name'] = terr_data['event_date'].dt.day_name()\nterr_data.iloc[:10, np.r_[:3, -3, -2, -1]]","243adf1f":"This is our first step to clearly understand the data we have. The characteristics raised on each of the attributes will be user as filter criteria to facilitate our analysis. Let's keep going and read our original data.","a1566865":"Excellent!","d29d3e58":"Let's investigate whether there are qualitative attributes with some incoherent categories.","da0b5c83":"Now we went trough a rough process of understanding the data, we have enough information for moving forward to apply certain transformations in the data. The idea is to analyze our dataset, already filtered by the importance we judged in the metadata, on:\n\n* Data types;\n* Null data threatment;\n* Categorical attributes;\n* Aditional transformations (datetime columns).","f20061f1":"Before reading the `.csv` file, I performed a hard individual analysis on each attribute of the dataset, collecting information already provided by Kaggle and working on a kind of subjective analysis to raise specific information that might be very usefull from now on.\n\nThe data has 135 attributes and it is difficult enough to keep all of them for analytical purposes. So, I created a second dataset with metadata of the original one with information about null data, categorical entries, numerical entries and feature importance of each attribute. This is the result:","07be0562":"The location API was created for free (up to 250k requests per month) on [HERE](https:\/\/www.here.com\/products\/mapping) - a platform found in research. An account was created, a new projet on the platform was started and afinally a new JavaScript \/ REST API key was instantiated and inserted into the `user_agent` parameter. I decided to keep the API key in this project to be used in this kernel. Now we can use the coordinates of null city dataset to found the cities missing.","9c948b63":"See that we've got an exception. Apparently some data could not be converted to the datetime type. Let's investigate.","17efc7ad":"Let's apply the first one","2fda5db0":"So now we have both original and metadata sets. Let's use this information to make decisions about filtering data ans selecting only attributes that would generate value for a future graphical analysis. In a first approach, let's filter the original data with high importance attributes.","caaa4098":"## Null Data Threatment","4504413e":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Libraries<\/a><\/span><\/li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Functions<\/a><\/span><\/li><li><span><a href=\"#Understanding-the-Data\" data-toc-modified-id=\"Understanding-the-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Understanding the Data<\/a><\/span><\/li><li><span><a href=\"#Preparing-the-Data\" data-toc-modified-id=\"Preparing-the-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Preparing the Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Types\" data-toc-modified-id=\"Data-Types-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Data Types<\/a><\/span><\/li><li><span><a href=\"#Null-Data-Threatment\" data-toc-modified-id=\"Null-Data-Threatment-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Null Data Threatment<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#GeoPy-and-Location-API\" data-toc-modified-id=\"GeoPy-and-Location-API-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;<\/span>GeoPy and Location API<\/a><\/span><\/li><li><span><a href=\"#Other-Patterns\" data-toc-modified-id=\"Other-Patterns-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;<\/span>Other Patterns<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Categorical-Entries\" data-toc-modified-id=\"Categorical-Entries-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Categorical Entries<\/a><\/span><\/li><li><span><a href=\"#Aditional-Transformations\" data-toc-modified-id=\"Aditional-Transformations-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Aditional Transformations<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Event-Date-Column\" data-toc-modified-id=\"Event-Date-Column-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;<\/span>Event Date Column<\/a><\/span><\/li><li><span><a href=\"#Day-of-Week-Column\" data-toc-modified-id=\"Day-of-Week-Column-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;<\/span>Day of Week Column<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","18ff37d1":"The greatest amount of null cities are from Southest Asia and South Asia. Let's go deeper.","acf78932":"# Functions","59c55078":"Actually, even in the `iday` and `imonth` columns we have entries identifying day `0`. The appropriate way (perhaps not the most correct) to solve this situation is to add a day in days with input 0. ","91cb85c8":"# Conclusion","e61e686d":"### GeoPy and Location API","67dcffe0":"Let's see what we've got.","5604f0c3":"## Aditional Transformations","42577993":"Something strange has been identified: is it possible that a date has `0` values? It this a problem in the applied function for converting data or is it intrinsic to the dataset? Let's check.","4710a5dc":"In a future implementation, we will go through:\n* Counting of incidents by year;\n* Incidentes with more than 24 hours of duration (extended);\n* Counting incidentes by country, region, city or location (country_txt, region_txt, city, lat, long);\n* Cross informations of lat\/long with other attributes (specificity, success, suicide, attacktype1_txt and others);\n* Worst terrorists group responsible for attacks (gname);\n* Locations that received attacks with large number of terrorists (nperps);\n* Weapons most used in attacks (weaptype1_txt);\n* Cross informations of tools, location and terrorists groups with number of victims (nkill);\n* Cross information of terrorists who died in at","ad19113e":"The addresses we've got (or at least those linked to Thailand) are unreadable. The probably action, in this case, will be to fill null entries in `city` attribute with something like 'Unknown'. Just to be sure, we will perform the search with all available coordinates (this might be take a little longer).","a4b74063":"# Understanding the Data","4a45733a":"We arrived at a crucial part of the preparation: the moment when we evaluate applications of significant transformations in the data. Here we will investigate the following topics:\n\n* Collection of data information through the `eventid` attribute or `iday`, `imonth` and `iyear`;\n* Creation of a new attribute containing information of day of the week of the incident.","31e3be9b":"Something went wrong. This result makes no sense. We expecteded to see some city in `Thailand` and not in `Ghana`. Searching in GeoPy documentation, it was possible to identify the right way to insert the coordinates: they have to be inserted in a string format and not like a tuple (like we did). Let's try again.","144965d7":"Apparently, it is safer to perform this date transformation using the `iyear`, `imonth` and `iday` columns, given that no inconsistency (other than days and months = 0) has been found.","dca58aec":"In this notebook, we will work on a dataset containing information about terrorist attacks around the world between 1970 and 2017 (except 1993). Between this period, more than 180,000 records were collected and made available by researches at the National Consortium for the Study of Terrorism and Responses to Terrorism (START) at the University of Maryland. This us the **Part 1** of a combination of kernels responsible for analysis on the Global Terrorism data. In the second one, we will go through data analysis and visualizations to gather insights.\n\nThe dataset was provided by [Kaggle](https:\/\/www.kaggle.com) and can be found on [this link](https:\/\/www.kaggle.com\/START-UMD\/gtd), as well as information about context, content, discussions and other users analysis about the data. In order to improve learning and to withdray insights from raw data, this implementation will apply exploratory analysis techniques in Python to provide explanatory results on future conclusions. Here you will find concepts of **Data Preparation** for assembling a dataset capable of being explored graphically in a future analysis.\n\nEnglish is not my mother language, sorry for any mistake. I hope you all enjoy this notebook and, if you do so, please **upvote**!","2cd298de":"Let's test one approach:","659a8917":"## Data Types","f58451fe":"It seems like we have [everything in its right place](https:\/\/www.youtube.com\/watch?v=sKZN115n6MI). We have just to give special attention to some attributes classified as numerical (float64) but with categorical meaning (extended, success, suicide, and others).","07fe7746":"After some research, I find a Python library called [GeoPy](https:\/\/geopy.readthedocs.io\/en\/stable\/) with some useful functionality within it was proposed for threat null data in `city` attribute. Basically, this library allows us to perform addres ookups from latitude and longitude coordinates. For this to be possible, the `Nominatim` module nees to be used defining the `user_agent` parameter wich requires an API Key.","74e3fb47":"The research and analysis applied to the dataset Global Terrorism served as inspiration for the implementation of future analyzes already mentioned in the introduction of this notebook. The transformations carried out allow us to conclude that the initial objective was reached and the data were prepared to be worked on for the generation of insights through visualizations.\n\nIn a second notebook, these same data will be used in a different way: generating visualizations and graphical analysis.","7d9c1a7b":"Maybe we can get insights from how to fill null data in cities by looking for other related attributes. For example, let's see the main locations that generated these null data.","259f259e":"As we suspected, the vast majority of null entries in `city` attribute exist because they are encoded in 'special' alphabets, such as Arabic, Thai, Egyptian and other illegible characters. Thus, it is possible to raise two applicable actions in this case:\n\n* 1. Fill null data with the string 'Unknown';\n* 2. Search for new forms of encoding when opening file.","e6511cf8":"## Categorical Entries","e18dc272":"Wow! As we expected, we have a huge amount of null data in different attributes. How to handle it? Given the context and the amount of null data, I think that filling this data with some statistical measure is not applicable. Unfortunately they are historical data within and extremely delicate context. For example, how do we fill null data on attributes that describe the number of vitims or the number of children held hostage in a terrorist attack? It's a difficult decision.\n\nOn the other hand, one thing that can be done is to full null data in some categortical attributes with a string like 'Unknow' or something like that. Maybe `city` and `natlty1_txt` are examples of this. But, before we take any action, let's check if there is any particularity in null data in these two attributes.","7f8679cb":"### Day of Week Column","8abdfcb7":"# Preparing the Data","778b8d40":"By observing the attributes with missing data and taking into consideration our future analysis (end of the second topic), we will apply filling techniques only in some categorical attributes using the string 'Unknown'.\n\nWe do not intend to train a Machine Learning model (at least for now), so some attributes will not have their null data filled or deleted. In othere words, the null data in this case does not change the data analysis, but in the course of the project, it we feel that it interferes in the results or plots, data indexing techniques will be applied to return only the data that is filled in the attribute we are working on.","62911000":"# Libraries","0f1d5e11":"### Event Date Column","adbc4f77":"With the creation of the `event_date` column of datetime type, the path to creating a new `day_of_week` column is quite simple. Remembering that in this Python approach, Monday = 0 and Sunday = 6.","23e190da":"Interesting! Now we have a plausible result, or at least it is the same that we've got with a manual search done on [google maps](https:\/\/www.google.com.br\/maps\/place\/6%C2%B011'24.3%22N+101%C2%B047'52.7%22E\/@6.1901859,101.2376058,9z\/data=!4m5!3m4!1s0x0:0x0!8m2!3d6.190088!4d101.797961) itself. With this result we can understant why there are null entris in the `city` attribute. We will apply this methodology in some other samples in order to verify if this pattern appears again.","2cd03d26":"Thailand and Afghanistan are the countries with most null data amount. How to use this information in our favor? One good idea: search cities in some location API using `latitude` and `longitude` coordinates.","883c7ff4":"Let's take a look at percentage of null data in some attributes","995b6bbe":"The third point of attention at this stage of data transformation concerns the number of categorial inputs for each attribute. This analysis is only valid if we are looking at qualitative attributes in our set. This way, we will visualize information in the metadata by filtering by attributes of the qualitative type.","8a5d4b99":"### Other Patterns"}}