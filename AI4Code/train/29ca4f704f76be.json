{"cell_type":{"87b862ef":"code","5a6f2d1c":"code","f6c5c94e":"code","1c5bde39":"code","66512094":"code","74a77f33":"code","830e8037":"code","83bf3791":"code","c32e0e4c":"code","ecb021af":"code","0402cca7":"code","e50484f3":"code","db5ebecf":"code","ec62c283":"code","03e0539f":"code","f687b86b":"code","1036ece3":"code","1edc2c80":"code","eda89e9d":"code","2f0e512b":"code","d3a0c6ce":"code","ad63943a":"code","b72a5cd2":"code","bea3e0d7":"code","80a00739":"code","3fec9fc3":"code","3e333ce5":"code","3b450137":"code","b06b469e":"code","a490b202":"code","42e0cdda":"code","146d81ad":"code","7e14604d":"code","30cfafc7":"code","746eb356":"code","6a2c53d8":"code","dd12348c":"code","9ab7216a":"code","aba08cd1":"code","e500fe6f":"code","f79b055f":"code","44930e41":"code","d09536ab":"code","708a2d0a":"code","215518c9":"code","1904f7d8":"code","7bc22fec":"code","5d808ee4":"code","3821a8ec":"code","a3753d45":"code","48a47b08":"code","04b630e9":"code","688e244a":"code","40fc1c7e":"code","cfc5982d":"code","8b6768aa":"code","9c40005a":"code","89f14467":"code","7c81b487":"code","508cb6b2":"code","5eb65467":"code","cedf2c91":"code","137c8084":"code","82c46d3a":"code","b1d3a7e4":"code","7d8b784b":"code","da20cd04":"code","9cca543d":"code","3a20f7e9":"code","81b68862":"code","6c29df4c":"code","0aa8b99c":"code","448b9658":"code","aafcc722":"code","b3d99484":"code","532d57b7":"code","74059332":"code","cb0b1726":"markdown","7861ba5a":"markdown","90ad4e3d":"markdown","bfd6e120":"markdown","007fa1cb":"markdown","adaebbd1":"markdown","e76e693c":"markdown","87cbb2c7":"markdown","20dbe884":"markdown","1fcd7937":"markdown","8196aa82":"markdown","a5a690af":"markdown","278c143d":"markdown","b9c01489":"markdown","bf5398d4":"markdown","7f10dbbc":"markdown","95f9ff81":"markdown","d902336f":"markdown","a2ce8230":"markdown","595d0571":"markdown","1cdfa152":"markdown","222c5eda":"markdown","ddd05695":"markdown","83b7de43":"markdown","4afd3ee6":"markdown","4ba3d002":"markdown","d3e822c4":"markdown","ba3a2616":"markdown","c498eb57":"markdown","fb11ed01":"markdown","3be2c82f":"markdown","e4505fd6":"markdown","5b851f8d":"markdown","97362186":"markdown","feca662c":"markdown","3f0b47b8":"markdown","45f8c3b8":"markdown","139696fd":"markdown","2277da20":"markdown"},"source":{"87b862ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a6f2d1c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f6c5c94e":"train=pd.read_csv('..\/input\/diabetes-classification\/train.csv')\ntrain.head()","1c5bde39":"test=pd.read_csv('..\/input\/diabetes-classification\/test.csv')\ntest.head()","66512094":"train.shape, test.shape","74a77f33":"train.isna().sum()","830e8037":"train.info()","83bf3791":"test.info()","c32e0e4c":"sns.pairplot(train)","ecb021af":"train.describe()","0402cca7":"# Generate and visualize the correlation matrix\ncorr = train.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","e50484f3":"plt.figure(figsize=(20,20))\nsns.heatmap(train.corr(), cmap='Blues', annot=True)\nplt.tight_layout()","db5ebecf":"plt.figure(figsize=(10,10))\nsns.kdeplot(train['age'], shade=True, Label=\"Age\")\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.show()","ec62c283":"train['age'].describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1])","03e0539f":"train['diabetes'].value_counts()","f687b86b":"plt.figure(figsize=(10,5))\nsns.countplot(x='diabetes', data=train)\nplt.show()","1036ece3":"X=train.drop(['diabetes'], axis=1)\ny=train['diabetes']","1edc2c80":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=7)","eda89e9d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix","2f0e512b":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","d3a0c6ce":"lr=LogisticRegression()\nlr.fit(X_train, y_train)","ad63943a":"y_pred=lr.predict(X_train)","b72a5cd2":"from sklearn import metrics","bea3e0d7":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","80a00739":"# visualize the difference between the actual and predicted price \nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Dibates\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Diabtes\")\nplt.show()","3fec9fc3":"sns.distplot(y_train-y_pred)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of residula\")\nplt.show()","3e333ce5":"# Predicting the Test data with model \ny_test_pred=lr.predict(X_test)","3b450137":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","b06b469e":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]","a490b202":"print(\"Confusion Matrix\", cfm)","42e0cdda":"print(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","146d81ad":"print(\"correct prediction\", \n      round((trueNegative+truePositive)\/len(y_test_pred)*100, 1),'%')","7e14604d":"cfm_df=pd.DataFrame(cfm, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(cfm_df, cmap='Reds', annot=True)\nplt.show()","30cfafc7":"pd.crosstab(y_test, y_test_pred, rownames=['True'], colnames=['Predicted'], margins=True)","746eb356":"from sklearn.metrics import classification_report","6a2c53d8":"print(classification_report(y_test, y_test_pred))","dd12348c":"y_test_pred_prob=lr.predict_proba(X_test)[:,1]","9ab7216a":"from sklearn.metrics import roc_curve","aba08cd1":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)","e500fe6f":"plt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","f79b055f":"from xgboost import XGBClassifier\nxgb=XGBClassifier()\nxgb.fit(X_train, y_train)","44930e41":"y_pred=xgb.predict(X_train)","d09536ab":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","708a2d0a":"# Predicting the Test data with model \ny_test_pred=xgb.predict(X_test)","215518c9":"xgb_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",xgb_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","1904f7d8":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]","7bc22fec":"print(\"Confusion Matrix\", cfm)","5d808ee4":"print(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","3821a8ec":"print(\"correct prediction\", \n      round((trueNegative+truePositive)\/len(y_test_pred)*100, 1),'%')","a3753d45":"plt.imshow(cfm, cmap='binary')","48a47b08":"z=cfm","04b630e9":"print(z)","688e244a":"\nz_df=pd.DataFrame(z, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(z_df, cmap='Reds', annot=True)","40fc1c7e":"print(classification_report(y_test, y_test_pred))","cfc5982d":"y_test_pred_prob=xgb.predict_proba(X_test)[:,1]","8b6768aa":"from sklearn.metrics import roc_curve\nfpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='XGBoost')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC \")\nplt.show()","9c40005a":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(X_train, y_train)","89f14467":"y_pred=rfc.predict(X_train)","7c81b487":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","508cb6b2":"# Predicting the Test data with model \ny_test_pred=rfc.predict(X_test)","5eb65467":"rfc_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",rfc_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","cedf2c91":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]","137c8084":"print(\"Confusion Matrix\", cfm)","82c46d3a":"print(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","b1d3a7e4":"print(\"correct prediction\", \n      round((trueNegative+truePositive)\/len(y_test_pred)*100, 1),'%')","7d8b784b":"\ncfm_df=pd.DataFrame(cfm, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(cfm_df, cmap='Reds', annot=True)","da20cd04":"print(classification_report(y_test, y_test_pred))","9cca543d":"y_test_pred_prob=xgb.predict_proba(X_test)[:,1]","3a20f7e9":"from sklearn.metrics import roc_curve\nfpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Random Forest')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC \")\nplt.show()","81b68862":"print(\"Logistic Regression\", lr_acc*100)\nprint(\"XGBosst\", xgb_acc*100)\nprint(\"Random Forest\", rfc_acc*100)","6c29df4c":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()","0aa8b99c":"model.fit(X_train, y_train)","448b9658":"prediction=model.predict(X_train)","aafcc722":"test_x=test","b3d99484":"predicted=model.predict(test_x)","532d57b7":"my_submission=pd.DataFrame({'p_id':test.p_id, 'diabetes':predicted})\n","74059332":"my_submission.to_csv('submission2.csv', index=False)\n","cb0b1726":"# Age prone to dibetes","7861ba5a":"# Confusion Matrix Visualization","90ad4e3d":"# Model Evaluation of Test datase","bfd6e120":"# Statistical summary ","007fa1cb":"# True Positive, True Negative, False Positive and False Negative","adaebbd1":"# Claculation ","e76e693c":"# Fitting The model","87cbb2c7":"# Diabatic Counts","20dbe884":"# ROC-AUC","1fcd7937":"# Visualization of the Confusion Matrix\n*  a confusion matrix, also known as an error matrix\n*  specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one\n*  Each row of the matrix represents predicted class, while each column represents actual class\n# USE\n* used for evaluating the performance of a classification model,\n* matrix compares the actual target values with  predicted by the machine learning model\n","8196aa82":"# Mode Evaluation of Test dataset","a5a690af":"# Density plot of Age","278c143d":"# Inference\nMaximum age is within the range of 20to 30 years. From the plot it is clear that max age is 25 year who are having diabties","b9c01489":"# Evaluation Comparisoon of the all model","bf5398d4":"# Random Forest Classifier","7f10dbbc":"# ROC-AUC ","95f9ff81":"# Summary table of Precision, Recall, F1-Score, Support","d902336f":"# Crross table for Confusion Matrix","a2ce8230":"# XGBoost classification","595d0571":"# Claculation","1cdfa152":"# Fitting the model","222c5eda":"# Accuracy of the model(XGBoost Classifier)","ddd05695":"# Summary table of Precision, Recall, F1-Score, Support","83b7de43":"# Heatmap","4afd3ee6":"# Histogram of total DIBATIC and NON-DIBATIC ","4ba3d002":"# ROC -AUC","d3e822c4":"# FInding the NULL Value","ba3a2616":"# Model Evaluation","c498eb57":"# Accuracy of Random Forest\n","fb11ed01":"# Logistic Regresssion","3be2c82f":"# Mode Evaluation","e4505fd6":"# Correlation Matrix","5b851f8d":"# Visualization","97362186":"# Model Evaluation for Test dataset ","feca662c":"# FItting the model","3f0b47b8":"# Model Evaluation","45f8c3b8":"# Submission Code","139696fd":"# Conclusion \nLogistic Regression is working very good \n* The submission will be done using Logistic Model","2277da20":"# End"}}