{"cell_type":{"ba085fb6":"code","a39d9f1f":"code","8df07351":"code","e4132d4b":"code","d190e2ae":"code","045a310e":"code","d06c24e0":"code","559e0831":"code","e2f667b5":"code","71876a5e":"code","d554ae7e":"code","243e7f51":"code","4e04650b":"code","7e7e03a8":"code","749542fc":"code","9c2047a2":"code","a092ec10":"code","a319ccba":"code","f3547f76":"code","fb64ded6":"code","b53d1df4":"code","4f86fb42":"code","2a7bc7db":"code","34e90a2c":"code","f045cdea":"code","9d8664c4":"code","b1faadcd":"markdown","e0ca29fc":"markdown","41d0c570":"markdown","9e6d876a":"markdown","a853eaf2":"markdown","39dcc6f7":"markdown","8302e8ff":"markdown"},"source":{"ba085fb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# ML tools\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a39d9f1f":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadsgermancsv\/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'german.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","8df07351":"# target\nprint(df.Creditability.value_counts())\n\n# plot\ndf.Creditability.value_counts().plot(kind='bar')\nplt.title('Target \"Creditability\"')\nplt.grid()\nplt.show()","e4132d4b":"# numerical features\nfeatures_num = ['Credit_Amount', 'Duration_of_Credit_monthly', 'Purpose', \n                'Account_Balance', 'Age_years', 'Payment_Status_of_Previous_Credit',\n                'Occupation', 'Instalment_per_cent', 'No_of_Credits_at_this_Bank', 'Foreign_Worker',\n                'Guarantors']","d190e2ae":"# basic stats\ndf[features_num].describe(include='all')","045a310e":"# pairwise scatter plot and histograms [this takes a few minutes!!!]\nt1 = time.time()\nsns.pairplot(df[features_num],kind='reg', \n             plot_kws={'line_kws':{'color':'magenta'},\n                       'scatter_kws': {'alpha': 0.1}})\nplt.show()\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2))","d06c24e0":"# correlations\ncorr_pearson = df[features_num].corr(method='pearson')\ncorr_spearman = df[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","559e0831":"# plot target vs features for all features\nfor f in features_num:\n    plt.figure(figsize=(10,5))\n    plt.scatter(df[f], df.Creditability, alpha=0.15)\n    corr_target = np.round(df[f].corr(df.Creditability),4)\n    my_title = 'Target vs ' + f + ', corr=' + str(corr_target)\n    plt.title(my_title)\n    plt.grid()","e2f667b5":"#Alternative visualization - Plot feature distribution by target level\n\nfor f in features_num:\n    plt.figure(figsize=(10,5))\n    sns.violinplot(data=df, y='Creditability', x=f, orient='h')\n    plt.title(f)\n    plt.grid()\n    plt.show()","71876a5e":"# select predictors\npredictors = features_num\nprint('Number of predictors: ', len(predictors))\nprint(predictors)\n\n# define target\ntarget='Creditability'","d554ae7e":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","243e7f51":"# upload data frame in H2O environment\ndf_hex = h2o.H2OFrame(df)\n\n# train \/ test split (80\/20)\ntrain_hex, test_hex = df_hex.split_frame(ratios=[0.8], seed=999)","4e04650b":"# define Gradient Boosting model\nn_cv = 5\nfit_1 = H2OGradientBoostingEstimator(ntrees = 50,\n                                     max_depth=6,\n                                     min_rows=5,\n                                     sample_rate=1,\n                                     col_sample_rate=0.5,\n                                     nfolds=n_cv,\n                                     seed=999)","7e7e03a8":"# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y=target,\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","749542fc":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","9c2047a2":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [RMSE]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","a092ec10":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","a319ccba":"# predict on training data\npred_train = fit_1.predict(train_hex)\ny_train_act = train_hex.as_data_frame()[target].values # actuals\ny_train_pred = pred_train.as_data_frame().predict.values # predictions\n# wrap results in data frame\ndf_train_eval = pd.DataFrame({'Actual' : y_train_act,\n                              'PredNum' : y_train_pred})","f3547f76":"# plot predictions vs actual\np=sns.jointplot(data=df_train_eval,\n                x='Actual', y='PredNum',\n                joint_kws={'alpha' : 0.15})\np.fig.suptitle('Prediction vs Actual - Training Data')\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.show()","fb64ded6":"# we have to map the continuous values from our regression exercise to the classes now\ny_train_pred_class = np.round(y_train_pred,0).astype(int)\n# also add to data frame\ndf_train_eval['PredClass'] = y_train_pred_class\ndf_train_eval.head()","b53d1df4":"# confusion matrix; rows ~ actual observations, cols ~ predictions\nconf_train = pd.crosstab(df_train_eval.Actual, df_train_eval.PredClass)\n# visualize\nsns.heatmap(conf_train, cmap='Blues', annot=True, \n            cbar=False, fmt='d',\n            linecolor='black',\n            linewidths=0.1)\nplt.show()","4f86fb42":"# predict on test data\npred_test = fit_1.predict(test_hex)\ny_test_act = test_hex.as_data_frame()[target].values # actual values\ny_test_pred = pred_test.as_data_frame().predict.values # predictions\n# wrap results in data frame\ndf_test_eval = pd.DataFrame({'Actual' : y_test_act,\n                             'PredNum' : y_test_pred})","2a7bc7db":"# plot predictions vs actuals\np=sns.jointplot(data=df_test_eval,\n                x='Actual', y='PredNum',\n                joint_kws={'alpha' : 0.15})\np.fig.suptitle('Prediction vs Actual - Test Data')\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.show()","34e90a2c":"# map the continuous values to classes again\ny_test_pred_class = np.round(y_test_pred,0).astype(int)\n# also add to data frame\ndf_test_eval['PredClass'] = y_test_pred_class\ndf_test_eval.head()","f045cdea":"# confusion matrix; rows ~ actual observations, cols ~ predictions\nconf_test = pd.crosstab(df_test_eval.Actual, df_test_eval.PredClass)\n# visualize\nsns.heatmap(conf_test, cmap='Blues', annot=True, \n            cbar=False, fmt='d',\n            linecolor='black',\n            linewidths=0.1)\nplt.show()","9d8664c4":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks Chris X, @docxian for all the script' )","b1faadcd":"#Regression => Classification:","e0ca29fc":"#All script by Chris X  https:\/\/www.kaggle.com\/docxian\/portugal-wine-quality","41d0c570":"In supervised learning, the dataset is labeled with the answer that algorithm should come up with. Supervised learning takes input variables (x) along with an output variable (y). The output variable represents the column that you want to predict on.\n\nThe algorithm then uses these variables to learn and approximate the mapping function from the input to the output. Supervised learning algorithms support classification and regression problems.\n\nhttps:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science.html","9e6d876a":"#Regression => Classification:","a853eaf2":"![](https:\/\/5b378f06dc82cad32808-dde2f412afa97a75335b0e97fc82422c.ssl.cf2.rackcdn.com\/h2o_eb874f02b7923924e633b4db54b4f6fe.png)training.h2o.ai","39dcc6f7":"#SHAP Summary\n\nSHAP summary plot shows the contribution of the features for each instance (row of data). The sum of the feature contributions and the bias term is equal to the raw prediction of the model, i.e., prediction before applying inverse link function.\n\n\n\nSHAP Local Explanation\n\nSHAP explanation shows contribution of features for a given instance. The sum of the feature contributions and the bias term is equal to the raw prediction of the model, i.e., prediction before applying inverse link function. H2O implements TreeSHAP which when the features are correlated, can increase contribution of a feature that had no influence on the prediction.\n\nhttps:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/explain.html","8302e8ff":"#Evaluate on test set"}}