{"cell_type":{"42cf9b55":"code","9245b39f":"code","76ca9e83":"code","d51900af":"code","646faed1":"code","3b850722":"code","a344c804":"code","f87481f1":"code","5e1a9c99":"code","50c473d6":"code","89f37097":"code","9501f96e":"code","fcf8e6b5":"code","4624aea4":"code","6a565a3a":"code","b7728ca4":"code","4b38fd2f":"code","061bf4d4":"code","9631c506":"code","b4e6edef":"code","c1696364":"code","f014cfeb":"code","4eab0b43":"code","0efabd38":"code","35185f76":"code","ac650396":"code","ed64b175":"code","389b4723":"code","eeea7da6":"code","5da2fc1d":"code","e84e0bf7":"code","4a4e272a":"code","4c85d3c7":"code","eb30a49e":"code","ee7867fa":"code","23bd0e28":"code","00adee66":"code","d12bb29a":"code","40f410f6":"code","0b7a4b1f":"code","2834366d":"code","9cd5cdaa":"code","99fcda7c":"code","7e07ea0a":"code","488ded45":"markdown","da710f43":"markdown"},"source":{"42cf9b55":"!pip -q install torchsummary","9245b39f":"# imports\nimport string\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import make_grid\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom torchsummary import summary\nfrom tqdm import tqdm\n\n# some settings\n# set background color to white\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'\n\n# set default figure size\nmatplotlib.rcParams['figure.figsize'] = (15, 7)","76ca9e83":"# read data\ntrain_df = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","d51900af":"# checkout data\ntrain_df.head()","646faed1":"train_df.describe()","3b850722":"train_df.info()","a344c804":"test_df.info()","f87481f1":"# create a dictionary for mapping numbers to letters\nalpha_dict = {idx:letter for idx, letter in enumerate(string.ascii_lowercase)}\nalpha_dict","5e1a9c99":"# check class distribution\n# convert to actual letters using dict\nalpha_labels = train_df.label.apply(lambda x: alpha_dict[x])\nsns.countplot(x=alpha_labels)\nplt.show()","50c473d6":"# create custom pytorch dataset class\nclass SignDataset(Dataset) :\n    def __init__(self, img, label) :\n        self.classes = np.array(label)\n        img = img \/ 255.0\n        self.img = np.array(img).reshape(-1, 28, 28, 1)\n        print(self.img.shape)\n        self.transform = T.Compose([\n            T.ToTensor()\n        ])\n        \n    def __len__(self) :\n        return len(self.img)\n    \n    def __getitem__(self, index) :\n        label = self.classes[index]\n        img = self.img[index]\n        img = self.transform(img)\n        \n        label = torch.LongTensor([label])\n        img = img.float()\n        \n        return img, label","89f37097":"# create datasets\ntrain_set = SignDataset(train_df.drop('label', axis=1), train_df['label'])\ntest_set = SignDataset(test_df.drop('label', axis=1), test_df['label'])","9501f96e":"# show a single image\ndef show_image(img, label, dataset):\n    plt.imshow(img.permute(1, 2, 0))\n    print(img.shape)\n    plt.axis('off')\n    plt.title(f\"Label: {dataset.classes[label]}\\nAlpha Label: {alpha_dict[dataset.classes[label]]}\")","fcf8e6b5":"import cv2\nimport os\ntimg = cv2.resize(cv2.imread('..\/input\/test-fist\/test_fist.JPG', 0), (28,28), interpolation = cv2.INTER_AREA)\nplt.imshow(timg)","4624aea4":"timg.reshape(-1,28, 28, 1).shape","6a565a3a":"show_image(*test_set[4], test_set)","b7728ca4":"show_image(*train_set[45], train_set)","4b38fd2f":"batch_size = 128\ntrain_dl = DataLoader(train_set, batch_size=batch_size)\ntest_dl = DataLoader(test_set, batch_size=batch_size)","061bf4d4":"# visualize a batch of images\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(20, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","9631c506":"# show a batch of images (128 images)\nshow_batch(train_dl)","b4e6edef":"# convlutional block with batchnorm, max pooling and dropout\ndef conv_block(in_channels, out_channels, pool=False, drop=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    if drop: layers.append(nn.Dropout())\n    return nn.Sequential(*layers)","c1696364":"# network architecture\nclass SignConvNet(nn.Module):\n    def __init__(self, in_channels, out_classes):\n        super().__init__()\n        self.conv1 = conv_block(in_channels, 16)\n        self.conv2 = conv_block(16, 32, pool=True)\n        self.conv3 = conv_block(32, 64, pool=True, drop=True)\n        self.fc =  nn.Sequential(*[\n                        nn.Flatten(),\n                        nn.Linear(7 * 7 * 64, out_classes)\n                    ])\n        \n    def forward(self, img):\n        img = self.conv1(img)\n        img = self.conv2(img)\n        img = self.conv3(img)\n        return self.fc(img)","f014cfeb":"# get number of classes\nnum_classes = len(alpha_dict)\n\n# set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# create model, optim  and loss\nmodel = SignConvNet(1, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# checkout model layer output shapes, and memory usage\nsummary(model, (1, 28, 28))","4eab0b43":"epochs = 10\nlosses = []\nfor epoch in range(epochs):\n    # for custom progress bar\n    with tqdm(train_dl, unit=\"batch\") as tepoch:\n        epoch_loss = 0\n        epoch_acc = 0\n        for data, target in tepoch:\n            tepoch.set_description(f\"Epoch {epoch + 1}\")\n            data, target = data.to(device), target.to(device) # move input to GPU\n            out = model(data)\n            loss = criterion(out, target.squeeze())\n            epoch_loss += loss.item()\n            loss.backward()\n            optim.step()\n            optim.zero_grad()\n            tepoch.set_postfix(loss = loss.item()) # show loss and per batch of data\n    losses.append(epoch_loss)","0efabd38":"# plot losses\nsns.set_style(\"dark\")\nsns.lineplot(data=losses).set(title=\"loss change during training\", xlabel=\"epoch\", ylabel=\"loss\")\nplt.show()","35185f76":"# predict on testing data samples (the accuracy here is batch accuracy)\ny_pred_list = []\ny_true_list = []\nwith torch.no_grad():\n    with tqdm(test_dl, unit=\"batch\") as tepoch:\n        for inp, labels in tepoch:\n            inp, labels = inp.to(device), labels.to(device)\n            print(inp.shape, type(inp))\n            y_test_pred = model(inp)\n            _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n            y_pred_list.append(y_pred_tag.cpu().numpy())\n            y_true_list.append(labels.cpu().numpy())","ac650396":"# show a single image\ndef show_image(img, label, dataset):\n    print(type(img))\n    print(type(label))\n    plt.imshow(img.permute(1, 2, 0))\n    print(img.shape)\n    plt.axis('off')\n    plt.title(f\"Label: {dataset.classes[label]}\\nAlpha Label: {alpha_dict[dataset.classes[label]]}\")","ed64b175":"#plt.imshow(train_set[4][0].permute(1,2,0))\n#show_image(*train_set[45], train_set)\ntrain_set[4][0].unsqueeze(0).shape","389b4723":"t = train_set[4][0].unsqueeze(0)\nt.to('cpu')\nmodel.to('cpu')\ny_pred = model(t)\ntorch.max(y_pred, dim = 1)","eeea7da6":"# flatten prediction and true lists\nflat_pred = []\nflat_true = []\nfor i in range(len(y_pred_list)):\n    for j in range(len(y_pred_list[i])):\n        flat_pred.append(y_pred_list[i][j])\n        flat_true.append(y_true_list[i][j])\n        \nprint(f\"number of testing samples results: {len(flat_pred)}\")","5da2fc1d":"# calculate total testing accuracy\nprint(f\"Testing accuracy is: {accuracy_score(flat_true, flat_pred) * 100:.2f}%\")","e84e0bf7":"# Display 15 random picture of the dataset with their labels\ninds = np.random.randint(len(test_set), size=15)\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in zip(inds, axes.flat):\n    img, label = test_set[i]\n    ax.imshow(img.permute(1, 2, 0))\n    dict_real = alpha_dict[test_set.classes[label]]\n    dict_pred = alpha_dict[test_set.classes[flat_pred[i]]]\n    ax.set_title(f\"True: {test_set.classes[label]}, {dict_real}\\nPredicted: {test_set.classes[flat_pred[i]]}, {dict_pred}\")\nplt.tight_layout()\nplt.show()","4a4e272a":"# classification report\nprint(classification_report(flat_true, flat_pred))","4c85d3c7":"# plot confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(flat_true, flat_pred)).rename(columns=alpha_dict, index=alpha_dict)\nplt.figure(figsize=(20, 10))\nsns.heatmap(confusion_matrix_df, annot=True, fmt='').set(title=\"confusion matrix\", xlabel=\"Predicted Label\", ylabel=\"True Label\")\nplt.show()","eb30a49e":"nimg = torch.tensor(timg.reshape(-1,1,28,28))\nnimg = nimg.float()\nprint(nimg.shape)\nplt.imshow(nimg.reshape(28,28,1))","ee7867fa":"pred = model(nimg)","23bd0e28":"model = model.cpu()","00adee66":"nimg = nimg.cpu()","d12bb29a":"pred\n_, answer = torch.max(pred, dim = 1)\nprint(answer.numpy())","40f410f6":"c = 0\nfor batch, label  in test_dl:\n    print(batch.shape, label.shape)\n    batch, label = batch.to('cpu'), label.to('cpu')\n    y_pred = model(batch)\n    _, y_pred_tag = torch.max(y_pred, dim = 1)\n    print(y_pred_tag)\n    print(\"**\")\n    break\n    ","0b7a4b1f":"y_pred_tag[:20]","2834366d":"label[:20]","9cd5cdaa":"index = 4\nplt.imshow(batch[index].reshape(28, 28))\nprint(label[index])","99fcda7c":"y_pred_tag[0].numpy()","7e07ea0a":"fig = plt.figure(figsize=(30,30))\nlimit = 32\nfor i in range(limit):\n    plt.subplot(8,4,i+1)\n    plt.title(\"actual: \"+str(label.numpy()[i][0])+\" prediction: \"+str(y_pred_tag.numpy()[i]))\n    plt.imshow(batch[i].reshape(28,28))\n    #plt.plot([1,2,3], [2,4,6])\nplt.show()","488ded45":"# Sign Language Classification using Pytorch CNN\n![image](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/3258\/5337\/amer_sign2.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210413%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210413T174110Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=b69ad067b5ff1071d329cd7922d620bda295ba78680512b24c4284bc23d71b785a40e83cf1ddb08e3370b23174393b0b76f7122507ded87d340a5e38432995f3ddde3bbc41ac3ff5267f123b870bdc6c9d9a214c7ffd564b04935d42386100d8fa1a0ca4564e5ecb51cfa4dfb7f973edc71b77276abe409768d4f21705fe440cae4b4b83a21155a60057d50b1508bf68c1576a50579298c5bb435ac574a28ee0f88a4ed5fd3a4a5f03a4cd2a211716c9161d517d3af76a8f8922cc2718b8f3f94bd06266df257361908ab698c9f6e9fdaaa2d91189c2c9e404d0fe1590a93f0eeca9120598e79e2adeca7cb665101d54ca8ca0f20692d2cac5d4532406bce603)\n### Data Info\nthe dataset is saved as a csv containing pixel values for 784 pixels resulting in images of size 28 * 28 * 1 with one color channel.\n","da710f43":"each row in the data represents an image with the first column being the label for the image"}}