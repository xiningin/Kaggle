{"cell_type":{"09c0a671":"code","dfdccbbe":"code","c6430d8b":"code","563802c6":"code","5e628769":"code","c9b31e7b":"code","753db3a7":"code","409fae65":"code","f7df57ce":"code","4bde9f97":"code","5b5396f9":"code","7618eb1f":"code","5246b9d3":"code","17f7a0ca":"code","2b811e6e":"markdown","c4c59d1b":"markdown","800923ff":"markdown","28c2331e":"markdown","6163b245":"markdown","63df24f8":"markdown","80f5d8b3":"markdown","cc9c5f04":"markdown"},"source":{"09c0a671":"import os\nimport sys \nimport json\nimport glob\nimport random\nimport collections\nimport time\nimport re\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, log_loss","dfdccbbe":"# Load and process images\n\nimport os\nimport glob\nimport re\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom base64 import b64encode\nimport matplotlib.animation as animation\n\n# Settings\n\nif os.path.exists(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"):\n    data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    working_dir = \"\/tmp\/rsna\"\n    modelpath = \"..\/input\/brain-tumor-models\"\nelse:\n    data_directory = '\/media\/roland\/data\/kaggle\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    working_dir = \"\/tmp\/rsna\"\n\nif not os.path.exists(working_dir):\n    os.mkdir(working_dir)\n\nmri_types = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\nSIZE = 256\nNUM_IMAGES = 32\nUSE_IMAGE_CACHE = False\nUSE_LUT_CONTRAST = None  # {\"window_width\": 2000, \"window_level\": 2000}\nUSE_VOI_LUT = True\n\n\ndef find_crop_area(imgfiles):\n    x1 = 1000\n    x2 = 0\n    y1 = 1000\n    y2 = 0\n\n    for f in imgfiles:\n        dicom = pydicom.read_file(f)\n        data = dicom.pixel_array\n        # bb = None\n        if np.max(data) > np.min(data):\n            data = data - np.min(data)\n            data = data \/ np.max(data)\n            data = (data * 255).astype(np.uint8)\n            bb = cv2.boundingRect(data)\n            if (bb[2] > 0) and (bb[3] > 0):\n                if bb[0] < x1:\n                    x1 = bb[0]\n                if bb[1] < y1:\n                    y1 = bb[1]\n                if bb[0] + bb[2] > x2:\n                    x2 = bb[0] + bb[2]\n                if bb[1] + bb[3] > y2:\n                    y2 = bb[1] + bb[3]\n\n        # print(bb, x1, x2, y1, y2)\n\n    if (x2 > x1) and (y2 > y2):\n        return x1, y1, x2 - x1, y2 - y1\n    else:\n        return 0, 0, data.shape[0], data.shape[1]\n\n\ndef load_dicom_image(path, img_size=SIZE, crop_area=None, voi_lut=USE_VOI_LUT, rotate=0):\n    dicom = pydicom.read_file(path)\n    # print(path[-10:], dicom.InstanceNumber, np.min(data), np.max(data), end=\" \")\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n        # print(\"lut\", np.min(data), np.max(data))\n    else:\n        data = dicom.pixel_array\n        # print()\n    if crop_area:\n        cropped = data[crop_area[1]:crop_area[1] + crop_area[3], crop_area[0]:crop_area[0] + crop_area[2]]\n        data = cropped\n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        data = cv2.rotate(data, rot_choices[rotate])\n\n    try:\n        data = cv2.resize(data, (img_size, img_size))\n    except Exception as exc:\n        print(exc)\n        print(path)\n        print(crop_area)\n        raise (exc)\n    return data\n\n\ndef get_image_plane(img_path):\n    # Ref:\n    # https:\/\/www.kaggle.com\/davidbroberts\/determining-mr-image-planes\n\n    dicom = pydicom.read_file(img_path)\n    loc = dicom.ImageOrientationPatient\n\n    row_x = round(loc[0])\n    row_y = round(loc[1])\n    row_z = round(loc[2])\n    col_x = round(loc[3])\n    col_y = round(loc[4])\n    col_z = round(loc[5])\n\n    if row_x == 1 and row_y == 0 and col_x == 0 and col_y == 0:\n        return 0, \"Coronal\"\n\n    if row_x == 0 and row_y == 1 and col_x == 0 and col_y == 0:\n        return 1, \"Sagittal\"\n\n    if row_x == 1 and row_y == 0 and col_x == 0 and col_y == 1:\n        return 2, \"Axial\"\n\n    return \"Unknown\"\n\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\",\n                         split=\"train\", lut_contrast=USE_LUT_CONTRAST,  # {\"window_width\": 1000, \"window_level\": 2000},\n                         offset=0, voi_lut=USE_VOI_LUT, rotate=0, use_cache=USE_IMAGE_CACHE,\n                         threshold=0):\n    cfn = f\"{working_dir}\/{scan_id}_{mri_type}_{SIZE}_{num_imgs}_{offset}_{rotate}{'L' if voi_lut else ''}{'C' if lut_contrast else ''}.pkl\"\n    if use_cache and os.path.exists(cfn):\n        img3d = pickle.load(open(cfn, \"rb\"))\n    else:\n        files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"),\n                       key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n        assert len(files) > 0, f\"no image files for {data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"\n        middle = len(files) \/\/ 2 + offset\n        # print(\"A \",scan_id, \"n\", len(files), offset, \"m\", middle)\n        if (middle <= 5) or (middle >= (len(files) - 5)):\n            middle = len(files) \/\/ 2\n        num_imgs2 = num_imgs \/\/ 2\n        p1 = max(0, middle - num_imgs2)\n        p2 = min(len(files), middle + num_imgs2)\n        # print(\"B \",\"n\",len(files), offset, \"m\", middle, \"s\",p1, \"e\", p2)\n\n        crop_area = find_crop_area(files[p1:p2])\n        img3d = np.stack([load_dicom_image(f, SIZE, crop_area, voi_lut, rotate) for f in files[p1:p2]]).T\n        # print(np.min(img3d), np.max(img3d), np.mean(img3d), np.median(img3d))\n        if img3d.shape[-1] < num_imgs:\n            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n            img3d = np.concatenate((img3d, n_zero), axis=-1)\n\n        # print(np.min(img3d), np.max(img3d), np.mean(img3d), np.median(img3d))\n        if lut_contrast:\n            img3d = img3d - np.min(img3d)\n            img3d = img3d \/ np.max(img3d)\n            img3d = lut_contrast[\"window_level\"] * img3d\n            # print(np.min(img3d), np.max(img3d), np.mean(img3d), np.median(img3d))\n            lut = make_lut(img3d, windowWidth=lut_contrast[\"window_width\"], windowLevel=lut_contrast[\"window_level\"])\n            img3d = np.reshape(apply_lut(img3d, lut), (img3d.shape[0], img3d.shape[1], img3d.shape[2]))\n            # print(np.min(img3d), np.max(img3d), np.mean(img3d), np.median(img3d))\n        if np.min(img3d) < np.max(img3d):\n            img3d = img3d - np.min(img3d)\n            img3d = img3d \/ np.max(img3d)\n            #img3d = 2*img3d - 1\n\n        if threshold > 0:\n            idx = img3d < threshold\n            img3d[idx] = 0\n\n        if use_cache:\n            pickle.dump(img3d, open(cfn, \"wb\"))\n\n    # print(np.min(img3d), np.max(img3d), np.mean(img3d), np.median(img3d))\n    return np.expand_dims(img3d, 0)\n\n\n# Adjusting Contrast on MR Images\n# https:\/\/www.kaggle.com\/davidbroberts\/adjusting-contrast-on-mr-images\n# Make a simple linear VOI LUT from the raw (stored) pixel data\ndef make_lut(storedPixels, windowWidth, windowLevel, p_i=\"MONOCHROME2\"):\n    # Slope and Intercept set to 1 and 0 for MR. Get these from DICOM tags instead if using\n    # on a modality that requires them (CT, PT etc)\n    slope = 1.0\n    intercept = 0.0\n    minPixel = int(np.amin(storedPixels))\n    maxPixel = int(np.amax(storedPixels))\n\n    # Make an empty array for the LUT the size of the pixel 'width' in the raw pixel data\n    lut = [0] * (maxPixel + 1)\n\n    # Invert pixels and windowLevel for MONOCHROME1. We invert the specified windowLevel so that \n    # increasing the level value makes the images brighter regardless of photometric intrepretation\n    invert = False\n    if p_i == \"MONOCHROME1\":\n        invert = True\n    else:\n        windowLevel = (maxPixel - minPixel) - windowLevel\n\n    # Loop through the pixels and calculate each LUT value\n    for storedValue in range(minPixel, maxPixel):\n        modalityLutValue = storedValue * slope + intercept\n        voiLutValue = (((modalityLutValue - windowLevel) \/ windowWidth + 0.5) * 255.0)\n        clampedValue = min(max(voiLutValue, 0), 255)\n        if invert:\n            lut[storedValue] = round(255 - clampedValue)\n        else:\n            lut[storedValue] = round(clampedValue)\n\n    return lut\n\n\n# Apply the LUT to a pixel array\ndef apply_lut(pixels_in, lut):\n    pixels_in = pixels_in.flatten()\n    pixels_out = [0] * len(pixels_in)\n\n    for i in range(0, len(pixels_in)):\n        pixel = int(pixels_in[i])\n        pixels_out[i] = int(lut[pixel])\n\n    return pixels_out\n\n\n# Save images as video\ndef play(filename):\n    html = ''\n    video = open(filename, 'rb').read()\n    src = 'data:video\/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=500 controls autoplay loop><source src=\"%s\" type=\"video\/mp4\"><\/video>' % src\n    return HTML(html)\n\n\ndef create_video(imgs, output=f'{working_dir}\/vis_video.mp4', frame_delay=200):\n    fig, ax = plt.subplots(figsize=(15, 10))\n    ims = []\n    n_imgs = imgs.shape[-1]\n    for i in range(n_imgs):\n        # print(i,np.min(imgs[0,:,:,i]), np.max(imgs[0,:,:,i]))\n        im = ax.imshow(imgs[0, :, :, i], animated=True, cmap='gray')\n        ims.append([im])\n    plt.close(fig)\n    # print(len(ims))\n    ani = animation.ArtistAnimation(fig, ims, interval=frame_delay, blit=True, repeat_delay=1000)\n\n    ani.save(output)\n    return output\n\n\nif __name__ == \"__main__\":\n    a = load_dicom_images_3d(\"00000\")\n    print(a.shape)\n    print(np.min(a), np.max(a), np.mean(a), np.median(a))\n\nTHRESHOLD = 0.5\n\n! rm {working_dir}\/*.pkl\na = load_dicom_images_3d(\"00144\", offset=-2, threshold=THRESHOLD)\nprint(a.shape)\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))\n\nplay(create_video(a))","c6430d8b":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(42)","563802c6":"class Dataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\", augment=False):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n        self.augment = augment\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.split==\"test\":\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], \n                                        split=self.split, threshold=THRESHOLD)\n        elif self.split==\"valid\":\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], \n                                        split=\"train\", threshold=THRESHOLD)\n        else:\n            offset = 0\n            if self.augment:\n                # offset = np.random.randint(-10,10)\n                rotation = np.random.randint(0,4)\n            else:\n                rotation = 0\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], \n                                        split=\"train\", offset=offset, rotate=rotation,\n                                        threshold=THRESHOLD)\n            #if self.augment:\n            #    if np.random.random() > 0.5:\n            #        data = np.transpose(data, [0, 2,1,3])\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n\n\n","5e628769":"class Model(nn.Module):\n    \n    def __init__(self, num_classes=1, num_channels=1):\n        super(Model, self).__init__()\n        \n        self.conv_layers = nn.ModuleList([self._conv_layer_set(num_channels, 8)])\n        d1 = int((SIZE-2)\/2)\n        d2 = int((NUM_IMAGES-2)\/2)\n        #print(d1,d1,d2)\n        self.conv_layers.append(self._conv_layer_set(8, 16))\n        d1 = int((d1-2)\/2)\n        d2 = int((d2-2)\/2)\n        #print(d1,d1,d2)\n        self.conv_layers.append(self._conv_layer_set(16, 32))\n        d1 = int((d1-2)\/2)\n        d2 = int((d2-2)\/2)\n        #print(d1,d1,d2)\n        #self.conv_layer4 = self._conv_layer_set(32, 16)\n        #print(d1,d1,d2)\n        self.fc1 = nn.Linear(32*d1*d1*d2, 128)\n        #self.fc1 = nn.Linear(16*14*14*2, 128)\n        self.fc_final = nn.Linear(128, num_classes)\n        self.activation1 = nn.LeakyReLU()\n        self.batchnorm1 = nn.BatchNorm1d(128)\n        #self.drop1 = nn.Dropout(p=0.15)        \n        \n    def _conv_layer_set(self, in_c, out_c):\n        conv_layer = nn.Sequential(\n        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n        nn.LeakyReLU(),\n        nn.MaxPool3d((2, 2, 2)),\n        )\n        return conv_layer\n    \n\n    def forward(self, x):\n        #print(x.shape)\n        out = self.conv_layers[0](x)\n        #print(out.shape)\n        for l in self.conv_layers[1:]:\n            out = l(out)\n            #print(out.shape)\n        out = out.view(out.size(0), -1)\n        #print(out.shape)\n        out = self.fc1(out)\n        #print(out.shape)\n        out = self.activation1(out)\n        #print(out.shape)\n        out = self.batchnorm1(out)\n        #out = self.drop1(out)\n        out = self.fc_final(out)\n        #print(out.shape)\n        \n        return out","c9b31e7b":"class Trainer:\n    def __init__(\n        self, \n        model, \n        device, \n        optimizer, \n        criterion\n    ):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n                n_epoch, train_loss, train_time\n            )\n            \n            self.info_message(\n                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n                n_epoch, valid_loss, valid_auc, valid_time\n            )\n\n            # if True:\n            #if self.best_valid_score < valid_auc:\n            if self.best_valid_score > valid_loss:\n                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n                self.info_message(\n                     \"loss improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n                    self.best_valid_score, valid_loss, self.lastmodel\n                )\n                self.best_valid_score = valid_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(\"\\nValid score didn't improve last {} epochs.\", patience)\n                break\n            \n    def train_epoch(self, train_loader):\n        self.model.train()\n        t = time.time()\n        sum_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            sum_loss += loss.detach().item()\n\n            self.optimizer.step()\n            \n            message = 'Train Step {}\/{}, train_loss: {:.4f}'\n            self.info_message(message, step, len(train_loader), sum_loss\/step, end=\"\\r\")\n        \n        return sum_loss\/len(train_loader), int(time.time() - t)\n    \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        sum_loss = 0\n        y_all = []\n        outputs_all = []\n\n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n\n                sum_loss += loss.detach().item()\n                y_all.extend(batch[\"y\"].tolist())\n                outputs_all.extend(torch.sigmoid(outputs).tolist())\n\n            message = 'Valid Step {}\/{}, valid_loss: {:.4f}'\n            self.info_message(message, step, len(valid_loader), sum_loss\/step, end=\"\\r\")\n            \n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        auc = roc_auc_score(y_all, outputs_all)\n        \n        return sum_loss\/len(valid_loader), auc, int(time.time() - t)\n    \n    def save_model(self, n_epoch, save_path, loss, auc):\n        self.lastmodel = f\"{working_dir}\/{save_path}-e{n_epoch}-loss{loss:.3f}-auc{auc:.3f}.pth\"\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            self.lastmodel,\n        )\n    \n    @staticmethod\n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)","753db3a7":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_mri_type(trn, val, mri_type, start_model=None, fold=None):\n    if mri_type==\"all\":\n        train_list = []\n        valid_list = []\n        for mri_type in mri_types:\n            trn.loc[:,\"MRI_Type\"] = mri_type\n            train_list.append(trn.copy())\n            val.loc[:,\"MRI_Type\"] = mri_type\n            valid_list.append(val.copy())\n\n        trn = pd.concat(train_list)\n        val = pd.concat(valid_list)\n    else:\n        trn.loc[:,\"MRI_Type\"] = mri_type\n        val.loc[:,\"MRI_Type\"] = mri_type\n\n    print(fold, mri_type, trn.shape, val.shape)\n    display(trn.head())\n    \n    train_data_retriever = Dataset(\n        trn[\"BraTS21ID\"].values, \n        trn[\"MGMT_value\"].values, \n        trn[\"MRI_Type\"].values,\n        augment=False\n    )\n\n    valid_data_retriever = Dataset(\n        val[\"BraTS21ID\"].values, \n        val[\"MGMT_value\"].values,\n        val[\"MRI_Type\"].values,\n        split=\"valid\"\n    )\n\n    train_loader = torch_data.DataLoader(\n        train_data_retriever,\n        batch_size=2,\n        shuffle=True,\n        num_workers=4, drop_last=True, pin_memory = True\n    )\n\n    valid_loader = torch_data.DataLoader(\n        valid_data_retriever, \n        batch_size=2,\n        shuffle=False,\n        num_workers=4, pin_memory = True\n    )\n\n    model = Model()\n    model.to(device)\n\n    if start_model:\n        print(\"Loading checkpoint:\", start_model)\n        checkpoint = torch.load(start_model)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    print(model)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    criterion = torch_functional.binary_cross_entropy_with_logits\n\n    trainer = Trainer(\n        model, \n        device, \n        optimizer, \n        criterion\n    )\n    save_path = f\"{mri_type}\"\n    if fold:\n        save_path += f\"_fold{fold}\"\n    history = trainer.fit(\n        20, \n        train_loader, \n        valid_loader, \n        save_path, \n        10,\n    )\n    \n    return trainer.lastmodel","409fae65":"start_modelfiles = None \nuseCV = True\n\ntrain_df = pd.read_csv(f\"{data_directory}\/train_labels.csv\")\nprint(train_df.shape)\ntrain_df = train_df[~train_df[\"BraTS21ID\"].isin([109,123,709])]\nprint(train_df.shape)\n\nfor idx, row in train_df.iterrows():\n    for mri_type in mri_types:\n        fpath = os.path.join(data_directory,\"train\", str(row[\"BraTS21ID\"]).zfill(5), mri_type)\n        imgfiles = glob.glob(os.path.join(fpath, \"*\"))\n        train_df.loc[idx,f\"{mri_type}_img_cnt\"] = len(imgfiles)\n\ndisplay(train_df)\n\n\nif useCV:\n    modelfiles = []\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n    fold=0\n    for trn_index, val_index in skf.split(train_df, train_df[\"MGMT_value\"]):\n        fold+=1\n        trn_df = train_df.iloc[trn_index]\n        val_df = train_df.iloc[val_index]\n        for mt in mri_types:\n            modelfiles.append(train_mri_type(trn_df, val_df, mt, fold=fold))\nelse:\n    df_train, df_valid = sk_model_selection.train_test_split(\n        train_df, \n        test_size=0.2, \n        random_state=42, \n        stratify=train_df[\"MGMT_value\"],\n    )\n\n    df_train.head()\n    modelfiles = [train_mri_type(df_train, df_valid, mt) for mt in mri_types]\n\nprint(modelfiles)        ","f7df57ce":"def predict(modelfile, df, mri_type, split):\n    print(\"Predict:\", modelfile, mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    data_retriever = Dataset(\n        df.index.values, \n        mri_type=df[\"MRI_Type\"].values,\n        split=split\n    )\n\n    data_loader = torch_data.DataLoader(\n        data_retriever,\n        batch_size=4,\n        shuffle=False,\n        num_workers=4, pin_memory = True\n    )\n   \n    model = Model()\n    model.to(device)\n    \n    checkpoint = torch.load(modelfile)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        print(f\"{e}\/{len(data_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(batch[\"id\"].numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","4bde9f97":"if useCV:\n    splits = {}\n    fold = 0\n    for trn_index, val_index in skf.split(train_df, train_df[\"MGMT_value\"]):\n        fold+=1\n        trn_idx = trn_index\n        val_idx = val_index\n        splits[fold] = (trn_idx, val_idx)\n    print(splits.keys())\n    train_df[\"MGMT_pred\"] = 0\n    for m in modelfiles:\n        mtype = m.split(\"_\")[0].split(\"\/\")[-1]\n        fold = int(m.split(\"-\")[0][-1])\n        val_df0 = train_df.iloc[splits[fold][1]]\n        val_df = val_df0.set_index(\"BraTS21ID\")\n        #print(m, mtype, fold, val_df.shape)\n        pred = predict(m, val_df, mtype, \"valid\")\n        tmp = train_df.loc[val_df0.index,\"MGMT_pred\"] + pred[\"MGMT_value\"].values\n        train_df.loc[val_df0.index,\"MGMT_pred\"] = tmp\n    train_df[\"MGMT_pred\"] \/= 4\n    auc = roc_auc_score(train_df[\"MGMT_value\"], train_df[\"MGMT_pred\"])\n    loss = log_loss(train_df[\"MGMT_value\"], train_df[\"MGMT_pred\"])\nelse:\n    df_valid = df_valid.set_index(\"BraTS21ID\")\n    df_valid[\"MGMT_pred\"] = 0\n    for m, mtype in zip(modelfiles,  mri_types):\n        pred = predict(m, df_valid, mtype, \"valid\")\n        df_valid[\"MGMT_pred\"] += pred[\"MGMT_value\"]\n    df_valid[\"MGMT_pred\"] \/= len(modelfiles)\n    auc = roc_auc_score(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\n    loss = log_loss(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\nprint(f\"Validation ensemble loss {loss:.4f}, AUC: {auc:.4f}\")","5b5396f9":"if useCV:\n    sns.displot(train_df[\"MGMT_pred\"])\n    plt.title(f'lut_contrast={USE_LUT_CONTRAST}, voi_lut={USE_VOI_LUT}, 5 fold CV, val AUC {auc:.3f}')\nelse:\n    sns.displot(df_valid[\"MGMT_pred\"])\n    plt.title(f'lut_contrast={USE_LUT_CONTRAST}, voi_lut={USE_VOI_LUT} val AUC {auc:.3f}')","7618eb1f":"submission = pd.read_csv(f\"{data_directory}\/sample_submission.csv\", index_col=\"BraTS21ID\")\n\nsubmission[\"MGMT_value\"] = 0\nfor m in modelfiles:\n    if useCV:\n        mtype = m.split(\"_\")[0].split(\"\/\")[-1]\n    else:\n        mtype = m.split(\"-\")[0].split(\"\/\")[-1]\n    print(m, mtype)\n    pred = predict(m, submission, mtype, split=\"test\")\n    submission[\"MGMT_value\"] += pred[\"MGMT_value\"]\n\nsubmission[\"MGMT_value\"] \/= len(modelfiles)\nsubmission[\"MGMT_value\"].to_csv(\"submission.csv\")","5246b9d3":"submission","17f7a0ca":"sns.displot(submission[\"MGMT_value\"])","2b811e6e":"## Dataset class","c4c59d1b":"## train models","800923ff":"## Predict function","28c2331e":"## Ensemble for validation","6163b245":"## Functions to load images","63df24f8":"## Ensemble for submission","80f5d8b3":"## Use stacked images (3D) and simple 3D CNN model\n\nThis is a variation of https:\/\/www.kaggle.com\/rluethy\/efficientnet3d-with-one-mri-type using a simple 3D CNN and 5-fold cross validation. Also, a threshold is applied to the images.\n\nAcknowledgements:\n\n- https:\/\/www.kaggle.com\/ihelon\/brain-tumor-eda-with-animations-and-modeling\n- https:\/\/www.kaggle.com\/furcifer\/torch-efficientnet3d-for-mri-no-train\n- https:\/\/www.kaggle.com\/davidbroberts\/adjusting-contrast-on-mr-images\n \n    \nUse models with only one MRI type, then ensemble the 4 models \n\nThe resulting models were used in the 26th ranked submission on the private LB: https:\/\/www.kaggle.com\/rluethy\/predict-only-simple-3d-cnn-with-one-mri-type  ","cc9c5f04":"## Model class"}}