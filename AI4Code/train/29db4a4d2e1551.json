{"cell_type":{"75021f9c":"code","2a19b780":"code","7ac85fe5":"code","6d8d8826":"code","135835e7":"code","986f895c":"code","26eb54bb":"code","f6053eaa":"code","0db0c89e":"code","a52221c0":"code","e219f34b":"code","951dc6f4":"code","9097a83b":"code","59f76c2b":"code","f84308eb":"code","5feb61fd":"code","6cfdc61e":"code","b19af370":"code","1ddbf1e6":"code","b2c8784b":"code","9b6017bf":"code","458dca79":"code","3a32f929":"code","baa7bfbd":"code","fd0bdeaf":"code","4dc8319b":"code","52cbcd39":"code","14938682":"code","c624bd22":"code","5adb0e1a":"code","e0437a54":"code","7ece7afb":"code","db41becf":"code","d15382b2":"code","de2e5979":"code","1aa30bf7":"code","cb44ec9f":"code","37329e5e":"code","9e7e9f1a":"code","98770ac2":"code","8d970827":"code","7e5f37a4":"code","008390ce":"code","7cc31d58":"code","47211ce3":"code","076bc52b":"code","eca7e612":"code","592aef6e":"code","00c5f99e":"code","ba827cb5":"code","7a8887e6":"code","d542d6af":"code","a9a091a3":"code","9cafdb47":"code","24ca8044":"code","5497acbf":"code","f951aa19":"code","111a25ad":"code","f383cb9b":"code","d99e423a":"code","310dc2f6":"code","53c6c068":"code","9450bb7d":"code","93afc48e":"code","61b5fab3":"code","24a32d77":"code","fbab25b5":"code","0b117ac0":"code","1c6efb7b":"code","0b9a86d9":"code","0e2b259d":"code","e5775a14":"code","b946af39":"code","a884b53b":"code","152f838e":"code","eabdca7b":"code","c1e48b7b":"code","f69234de":"code","2d4d8b33":"code","cf17ae36":"code","e017647f":"code","973becae":"code","dd21adb7":"code","1851b03d":"code","b39f92c8":"code","2cd0d504":"code","e8d53ba9":"code","677802b3":"markdown","05e8f998":"markdown","db8faa59":"markdown","4243500e":"markdown","ea7778c7":"markdown","97bc17c1":"markdown","48ebbea1":"markdown","0ec1fdc9":"markdown","42faf935":"markdown","44fd57d2":"markdown","b64b1909":"markdown","15e61615":"markdown","13ca03c2":"markdown","27f82178":"markdown","6ae108e1":"markdown","a8630343":"markdown","24bd3979":"markdown","b5d3ab2f":"markdown","9fa32e0f":"markdown","bd920356":"markdown","16855201":"markdown","1f4e6b0f":"markdown","689142c2":"markdown","cfcd5b89":"markdown","5d3c2ae0":"markdown","3429868f":"markdown","5deca0ce":"markdown","e5940ca2":"markdown","95ac13ed":"markdown","82ca94f5":"markdown","a0f06c35":"markdown","0d732aff":"markdown","cf0a03ce":"markdown","57f846c2":"markdown","62005f96":"markdown","8a3cd485":"markdown","b02dd6d8":"markdown","30933f01":"markdown","d7bc6944":"markdown","77a93de3":"markdown","bd20f6e2":"markdown","8506452f":"markdown","a65c4a50":"markdown","dd367c01":"markdown","435bf7aa":"markdown","8fb81c3f":"markdown","51bd8b3f":"markdown","ed49aaeb":"markdown","4b4cc944":"markdown","c1c832e0":"markdown","5321cc6d":"markdown"},"source":{"75021f9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\n\n#for visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import r2_score\n\n\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\nfrom sklearn.model_selection import GridSearchCV,cross_validate\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom scipy.stats import normaltest, norm, skew\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2a19b780":"df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf1 = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n# this is not to skip some columns when showing the dataframe\npd.set_option('display.max_columns', df.shape[-1])","7ac85fe5":"print(\"The shape of train is {}\\nThe shape of test is {}\".format(df.shape,df1.shape))","6d8d8826":"df.describe()","135835e7":"df1.describe()","986f895c":"# For train.csv\nplt.figure(figsize=(15, 4))\nsns.heatmap(df.isna(),cbar=False)","26eb54bb":"#For test.csv\nplt.figure(figsize=(15, 4))\nsns.heatmap(df1.isna(),cbar=False)","f6053eaa":"# getting the correlation table\ncorr = df.corr()\n# selecting only correlations that have a strength above 0.5 both ways\ntop_corr = corr.index[abs(corr['SalePrice'])>0.5]\n\n# plot it in a heatmap\nplt.figure(figsize = (20,20))\nsns.heatmap(df[top_corr].corr(),cbar=True, annot=True)","0db0c89e":"#WARNING: This will take very long to execute\nsns.set()\nsns.pairplot(df[top_corr], height = 2.5)","a52221c0":"fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e219f34b":"# removing some outliers\ndf = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']>100000)].index)","951dc6f4":"fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9097a83b":"fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","59f76c2b":"df = df.drop(df[(df['1stFlrSF']>2700) & (df['SalePrice']>100000)].index)","f84308eb":"fig, ax = plt.subplots()\nax.scatter(x = df['1stFlrSF'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()","5feb61fd":"fig, ax = plt.subplots()\nax.scatter(x = df['GarageArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","6cfdc61e":"df = df.drop(df[(df['GarageArea']>1240) & (df['SalePrice']<300000)].index)","b19af370":"fig, ax = plt.subplots()\nax.scatter(x = df['GarageArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","1ddbf1e6":"fig, ax = plt.subplots()\nax.scatter(x = df['TotalBsmtSF'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","b2c8784b":"df = df.drop(df[(df['TotalBsmtSF']>3000) & (df['SalePrice']>200000)].index)","9b6017bf":"fig, ax = plt.subplots()\nax.scatter(x = df['TotalBsmtSF'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","458dca79":"fig, ax = plt.subplots()\nax.scatter(x = df['YearRemodAdd'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('YearRemoAdd', fontsize=13)\nplt.show()\n\ndf = df.drop(df[(df['YearRemodAdd']<1970) & (df['SalePrice']>300000)].index)\ndf = df.drop(df[(df['YearRemodAdd']<2000) & (df['SalePrice']>600000)].index)\n","3a32f929":"fig, ax = plt.subplots()\nax.scatter(x = df['YearRemodAdd'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('YearRemoAdd', fontsize=13)\nplt.show()","baa7bfbd":"fig, ax = plt.subplots()\nax.scatter(x = df['YearBuilt'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('YearBulit', fontsize=13)\nplt.show()\ndf = df.drop(df[(df['YearBuilt']<1900) & (df['SalePrice']>400000)].index)","fd0bdeaf":"\nfig, ax = plt.subplots()\nax.scatter(x = df['YearBuilt'], y = df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('YearBulit', fontsize=13)\nplt.show()","4dc8319b":"target = df.pop('SalePrice')","52cbcd39":"#concatinating the two dataframes\nwhole_df = pd.concat([df,df1],keys=[0,1])\n# Sanity check to ensure the total length matches \nwhole_df.shape[0] == df.shape[0]+df1.shape[0]","14938682":"whole_IDs = whole_df.pop('Id')","c624bd22":"non_num_cols = whole_df.select_dtypes(include=object).columns.tolist()\nnum_cols = whole_df.select_dtypes(include=np.number).columns.tolist()\n#checking column discription and adding each column containing NA as a value to the list\ncheck_list = ['Alley','BsmtQual',\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\"GarageType\",'GarageFinish','GarageQual',\"GarageCond\",'PoolQC','Fence','MiscFeature']","5adb0e1a":"def unique_values(df,cols,n_unique_vals=None, return_cols=False):\n    '''\n    Given the dataframe and feature names, this function will print out or return\n    all the unique values in each column.\n    \n    In case n_unique_vals is set to an integer, any column which has the number\n    of unique values over the value passed as an argument will only be printed\n    '''\n    output = []  #to accumulate the column names (only usefull when return_cols is True)\n    \n    for col in cols:\n        # if user didnt specify a threshold\n        if n_unique_vals == None: \n            print(\"{}: {}\".format(col,df[col].unique()))\n            output.append(col)\n        # if number of unique values in the feature is more than specified by user\n        elif n_unique_vals >= len(df[col].unique()):\n            print(\"{}: {}\".format(col,df[col].unique()))\n            output.append(col)\n    # if user wants the name of the columns to be returned\n    if return_cols:\n        return output\n    \n    \ndef make_category_numeric(df,col):\n    '''\n    This function helps convert non numeric categorical features to numeric categorical features\n    it is used to find the distribution\n    '''\n    # count how many unique values in the column\n    # for each unique value, mape all similar values to a number starting from 0\n    return df[col].dropna().map({value:idx for idx,value in enumerate(df[col].unique())})\n\n\n\n\ndef missing_percentage(df, cols, verbose = 1, threshold = 0.00, return_output=False):\n    '''\n    This function calculates the percentage of missing enteries in each column in cols\n    Threshold is a percentage of which the feature will only be shown if the percentage\n    of missing data in the feature is more than the percentage specified\n    '''\n    result = []\n    for col in cols:\n        \n        # calculate the percentage of missing data in the column\n        percentage = df[col].isna().sum() \/ df[col].shape[0]\n        \n        # user wants to see the percentage of every column\n        if verbose == 2:\n            print(\"Missing data in {} is {:.2%}\".format(col,percentage))\n            \n        # user only wants to see for columns which actually has missing data\n        elif verbose == 1 and percentage > float(threshold):\n            print(\"Missing data in {} is {:.2%}\".format(col,percentage))\n            \n        # this is used with return output to return columns which the missing\n        # data percentage is above the specified threshold\n        if percentage > float(threshold):\n            result.append(col)\n            \n    if return_output:\n        return result\n\n\n\ndef classify_distribution(df,columns,numeric=True):\n    '''This function runs a normality test using the p-value. The threshold is set to 0.05'''\n    result = []\n    for col in columns:\n        num_cat_col = make_category_numeric(df,col) if not numeric else df[col].dropna()\n\n            # run normality test >> reference:#http:\/\/mathforum.org\/library\/drmath\/view\/72065.html\n        if normaltest(num_cat_col)[-1] > 0.05: \n            # if test succeeds it means it is normal\n            result.append((col,0))\n        else:\n            # if not normal then return the skew\n            result.append((col,num_cat_col.skew()))\n\n    return result\n\n\n\n# show only categorical columns that have missing data        \nmissing_percentage(whole_df,non_num_cols,verbose=1)","e0437a54":"unique_values(whole_df,check_list)","7ece7afb":"whole_df[check_list] = whole_df[check_list].fillna(value='NA',axis=1)","db41becf":"# rechecking the unique values after filling nun with NA\nunique_values(whole_df,check_list)","d15382b2":"# checking if they still have a nan values\nmissing_percentage(whole_df,check_list,verbose=1)\n#nothing is returned which means no nan values, feel free to change verbose to 2","de2e5979":"missing_categorical = missing_percentage(whole_df,non_num_cols,verbose=1,return_output=True)","1aa30bf7":"# getting all the columns that have less than 25 unique values in them\nnumeric_category = unique_values(whole_df,num_cols,25, return_cols=True)","cb44ec9f":"# getting the names of those columns\nnumeric_cats = unique_values(whole_df,num_cols,25, return_cols=True)","37329e5e":"#change it to type string to dummify it later\nwhole_df[numeric_cats] = whole_df[numeric_cats].astype(str)\nwhole_df[numeric_cats].info()","9e7e9f1a":"missing_categorical = missing_categorical+numeric_cats\n\n# plotting a heatmap to see whether there are any missing values\nplt.figure(figsize=(15, 4))\nsns.heatmap(whole_df[missing_categorical].isna(),cbar=False)","98770ac2":"whole_df['MasVnrType'].fillna('None',inplace=True)","8d970827":"whole_df['MasVnrArea'].fillna(0,inplace=True)","7e5f37a4":"plt.figure(figsize=(15, 4))\nsns.heatmap(whole_df[missing_categorical].isna(),cbar=False)","008390ce":"# this is because some columns in num_cols is categorical is seen above\nactual_numeric = [col for col in num_cols if col not in numeric_cats]","7cc31d58":"# checking how where the nan is in our continuos features\nplt.figure(figsize=(15, 4))\nsns.heatmap(whole_df[actual_numeric].isna(),cbar=False)","47211ce3":"whole_df['GarageYrBlt'].fillna(whole_df['YearBuilt'], inplace=True)","076bc52b":"# checking for changes\nplt.figure(figsize=(15, 4))\nsns.heatmap(whole_df[actual_numeric].isna(),cbar=False)","eca7e612":"# grouping per neighborhood\n# replacing nan values with the median of the neighborhood group\n\nwhole_df[\"LotFrontage\"] = whole_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","592aef6e":"#checking for changes\nplt.figure(figsize=(15, 4))\nsns.heatmap(whole_df[num_cols].isna(),cbar=False)","00c5f99e":"#checking for missing enteries\nmissing_percentage(whole_df,whole_df.columns, verbose = 1)","ba827cb5":"# All of these will be filled with 0 as the nan values are present when there is no basement or no garage or NA somewhere\nwhole_df.fillna(0,inplace=True)","7a8887e6":"#checking again\nmissing_percentage(whole_df,whole_df.columns, verbose = 1)","d542d6af":"# Check the skew of all numerical features\nnumeric_skew = classify_distribution(whole_df,actual_numeric)\ncol_names = [col for col, val in numeric_skew]\nskewness = [val for col, val in numeric_skew]\n\nsns.barplot(x=skewness, y=col_names).set_title('Skewness of features')","a9a091a3":"# this will contain all the numerical features\nto_log = actual_numeric.copy()\n\n# this contains some features that we do not want to transform\n# transforming them will increase their skewness in the other direction\nto_remove = ['GarageYrBlt','TotalBsmtSF','GarageArea','BsmtUnfSF']\n\n# removing the columns in to_remove from to_log\nfor x in to_remove:\n    to_log.remove(x)\n\n# Apply log transformation to the remaining column sin to_log\nwhole_df[to_log] = np.log1p(whole_df[to_log])\n\n\n# This is for plotting-----------------------------------\nnumeric_skew = classify_distribution(whole_df,actual_numeric)\ncol_names = [col for col, val in numeric_skew]\nskewness = [val for col, val in numeric_skew]\n\nsns.barplot(x=skewness, y=col_names).set_xlim((-2,23))","9cafdb47":"# non_num_cols contains all categorical columns from the start\n# numeric_cats contains numeric columns that we converted to string\nall_category_cols = non_num_cols + numeric_cats\n\n\nfor col in all_category_cols:\n    # get the dummies of that column\n    dummy = pd.get_dummies(whole_df[col],prefix=col)\n    # concatenate it with the dataframe \n    whole_df = pd.concat([whole_df,dummy], axis= 1)\n\n    # remove the old categorical columns\nwhole_df.drop(labels=all_category_cols, axis=1,inplace=True)\n\n","24ca8044":"# log transforming the target \ntarget = np.log1p(target)\n\n# readding the IDs back\nwhole_df['Id'] = whole_IDs\n\n# separating the two dataframes (train.csv and test.csv)\ndf,df1 = whole_df.xs(0),whole_df.xs(1)\n\n# readding the target to train.csv\ndf['SalePrice'] = target","5497acbf":"# Making X and y data to train the model\n\nX = df.drop(['SalePrice','Id'],axis=1)\ny = df['SalePrice']\n\n","f951aa19":"# this is used for experimentation only\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","111a25ad":"# making a linear regression\nlnr = LinearRegression()\n# train it on train data from test_train split\nlnr.fit(x_train, y_train)\ny_hat = lnr.predict(x_test)\n# measure the RMSE\nlinear_regression_rmse = np.sqrt(mean_squared_error(y_test,y_hat))\nlinear_regression_rmse","f383cb9b":"\n# making folds to be 5 (it will be used for all the CV below)\nkf = KFold(n_splits=5, shuffle=True)\n\n\nlnr_cv = LinearRegression()\n\n# getting a score after training and validating the model on 5 folds\ncv_results = cross_val_score(lnr_cv, X, y, cv=kf,scoring='neg_root_mean_squared_error')\nnp.mean(cv_results)\n","d99e423a":"# doing the same process but for lasso\n\nlasso = Lasso(random_state=100)\nlasso.fit(x_train,y_train)\n\ny_hat = lasso.predict(x_test)\nlasso_rmse = np.sqrt(mean_squared_error(y_test,y_hat))\nlasso_rmse","310dc2f6":"# Using cross-validation with lasso\n\nlasso_cv = Lasso(random_state=100)\ncv_results = cross_val_score(lasso_cv, X, y, cv=kf,scoring='neg_root_mean_squared_error')\nlasso_cv_rmse = np.mean(cv_results)\nlasso_cv_rmse","53c6c068":"rigde = Ridge(random_state=100)\nrigde.fit(x_train,y_train)\n\ny_hat = rigde.predict(x_test)\nrigde_rmse = np.sqrt(mean_squared_error(y_test,y_hat))\nrigde_rmse","9450bb7d":"ridge_cv = Ridge(random_state=100)\ncv_results = cross_val_score(ridge_cv, X, y, cv=kf,scoring='neg_root_mean_squared_error')\nridge_cv_rmse = np.mean(cv_results)\nridge_cv_rmse","93afc48e":"elastic = ElasticNet(random_state=100)\nelastic.fit(x_train,y_train)\n\ny_hat = elastic.predict(x_test)\nelastic_rmse = np.sqrt(mean_squared_error(y_test,y_hat))\nelastic_rmse","61b5fab3":"dct = DecisionTreeRegressor(random_state=100)\ndct.fit(x_train,y_train)\n\ny_hat = dct.predict(x_test)\ndct_rmse = np.sqrt(mean_squared_error(y_test,y_hat))\ndct_rmse","24a32d77":"dct_cv = DecisionTreeRegressor(random_state=100)\n\ncv_results = cross_val_score(dct_cv, X, y, cv=kf,scoring='neg_root_mean_squared_error')\ndct_cv_rmse = np.mean(cv_results)\ndct_cv_rmse","fbab25b5":"lasso_gc = Lasso(random_state=100)\nparams = {'alpha':[0.0008,0.0007,0.0009,0.001,0.002,0.003,0.6,0.7,0.8],'max_iter':[10000,1000]}\nlasso_gc = GridSearchCV(lasso_gc, params,cv=kf,scoring='neg_root_mean_squared_error')\n\nlasso_gc.fit(X,y)\nlasso_gc.best_score_","0b117ac0":"lasso_gc.best_params_","1c6efb7b":"ridge_gc = Ridge(random_state=100)\nparams = {'alpha':[0.8,0.9,1,1.1],'max_iter':[10000,1000]}\nridge_gc = GridSearchCV(ridge_gc, params,cv=kf,scoring='neg_root_mean_squared_error')\n\nridge_gc.fit(X,y)\nridge_gc.best_score_","0b9a86d9":"ridge_gc.best_params_","0e2b259d":"elastic_gc = ElasticNet()\nparams = {'l1_ratio':[0.9,0.8,0.7,0.1,0.2,0.3,0.4],'max_iter':[10000,1000]}\nelastic_gc = GridSearchCV(elastic_gc, params,cv=kf,scoring='neg_root_mean_squared_error')\n\nelastic_gc.fit(X,y)\nelastic_gc.best_score_","e5775a14":"elastic_gc.best_params_","b946af39":"dct_select = DecisionTreeRegressor(random_state=24)\n\nparams = {'max_depth':[2,5,8,10,12,15],'min_samples_split':[20,25,30,40,50,60],'min_samples_leaf':[1,4,10,20,25,30],\"max_features\":['auto']}\ndct_select_gc = GridSearchCV(dct_select, params,cv=kf,scoring='neg_root_mean_squared_error')\n\ndct_select_gc.fit(X,y)\ndct_select_gc.best_score_","a884b53b":"dct_select_gc.best_params_","152f838e":"lasso_model = Lasso(alpha=0.0007, max_iter=10000)\nlasso_model.fit(X,y)\ncoefs = [idx for idx, col in enumerate(lasso_model.coef_) if abs(col) > 0.00005]\nprint(\"{} features selected\".format(len(coefs)))\n\nselected_cols = [X.columns[col_index] for col_index in coefs]","eabdca7b":"select_lnr = LinearRegression()\ncv_results = cross_val_score(select_lnr, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nnp.mean(cv_results)\n# improved ","c1e48b7b":"ridge_cv_select = Ridge(alpha= 9.5, max_iter= 10000)\ncv_results = cross_val_score(ridge_cv_select, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nnp.mean(cv_results)\n","f69234de":"elastic_select = ElasticNet(l1_ratio=0.1)\ncv_results = cross_val_score(elastic_select, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nnp.mean(cv_results)","2d4d8b33":"dct_select = DecisionTreeRegressor(max_depth= 12,max_features= 'auto',min_samples_leaf= 10,min_samples_split=20)\ncv_results = cross_val_score(dct_select, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nnp.mean(cv_results)","cf17ae36":"# ridge on feature selection\nridge_cv = Ridge(alpha=0.99, max_iter=10000)\ncv_results = cross_val_score(ridge_cv, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nridge_cv_rmse = np.mean(cv_results)\nridge_cv_rmse","e017647f":"# ridge without feature selection\nridge_cv = Ridge(alpha=0.99, max_iter=10000)\ncv_results = cross_val_score(ridge_cv, X, y, cv=kf,scoring='neg_root_mean_squared_error')\nridge_cv_rmse = np.mean(cv_results)\nridge_cv_rmse","973becae":"# linear reg with feature selection\nl = LinearRegression()\ncv_results = cross_val_score(l, X[selected_cols], y, cv=kf,scoring='neg_root_mean_squared_error')\nl_cv_rmse = np.mean(cv_results)\nl_cv_rmse","dd21adb7":"# linear reg without feature selection\nl = LinearRegression()\ncv_results = cross_val_score(l, X, y, cv=kf,scoring='neg_root_mean_squared_error')\nl_cv_rmse = np.mean(cv_results)\nl_cv_rmse","1851b03d":"ridge = Ridge(alpha=0.99, max_iter=10000)\nridge.fit(X[selected_cols],y)","b39f92c8":"# IDs is not used in prediction\nids = df1.pop('Id')","2cd0d504":"X_test = df1\n\n# using the model to predict the house prices\nyhat = ridge.predict(X_test[selected_cols])","e8d53ba9":"# inverse the log transforem and store it in the dataframe\noutdf = pd.DataFrame({'Id':ids,'SalePrice':np.expm1(yhat)})\n#save it as a csv file\noutdf.to_csv('\/kaggle\/working\/submissionv6.csv', index=False)","677802b3":"Replotting a nan heatmap to double check that our categorical data is free of nan","05e8f998":"Converting numerical values which are actually categorical to be categorical","db8faa59":"##### Missing categorical has all the newly converted categorical columns + the old remaining missing categorical columns","4243500e":"## Prep for model","ea7778c7":"##### Checking the unique values of categorical columns that has 'NA' in the description","97bc17c1":"### Ridge","48ebbea1":"According to above, some of our continuos features are very positively skewed. Hence, i will attempt to apply log transformation to all the continous features","0ec1fdc9":"## Final Model","42faf935":"##### preserving the IDs to ensure they dont get modified by mistake","44fd57d2":"## Check the skewness of numeric values","b64b1909":"### Elastic Net","15e61615":"## Concatenating both Dataframes","13ca03c2":"### Gridsearch Elastic net","27f82178":"##### replacing empty values with NA for only categories that contain NA as a value in discription","6ae108e1":"### Plotting a scatterplot for each moderately correlated feature and the target and eleminating outliers","a8630343":"And since there is a relationship between the vaneer type and its area (None will mean 0 area as vaneer doesnt exist)\n\n we can substitute empty rows in MasVnrArea with 0 ","24bd3979":" According to the description, LotFrontage is the linear feet of street connected to property\n \n Here, im asuming that multiple houses in the same neighborhood would have similar lot frontage","b5d3ab2f":"### Ridge CV","9fa32e0f":"## completeness of the Datasets","bd920356":"### Linear regression with cross validation","16855201":"Above we can see that all the features in y-axis have correlation with the target of strength above 0.5","1f4e6b0f":"### Gridsearch Ridge","689142c2":"### Decision tree","cfcd5b89":"### Decision tree on selected features","5d3c2ae0":"### Ridge opt CV on selected features","3429868f":"### Checking the entire dataset","5deca0ce":"The missing column in test is the target","e5940ca2":"## Converting categorical to dummies","95ac13ed":"### Decision tree CV","82ca94f5":"Now, we dealt with the null values of categorical features which contain NA in their description.\n\nGetting the remaining categorical columns which contains nan values ","a0f06c35":"## Models Testing","0d732aff":"### Lasso","cf0a03ce":"### Elastic net CV on selected features","57f846c2":"## top 2 models with and without feature selection","62005f96":"## Importing datasets","8a3cd485":"### Gridsearch Lasso","b02dd6d8":"## Finding the features that have high correlation with target","30933f01":"From the pairplot above, we can see that some of the features are skewed and in some scatterplots, we can see a slight correlation. However, it is difficult to see everything in a pairplot","d7bc6944":"We can se some missing values in BasVnrType\n\n BasVnrType is the Masonry veneer type which contains None as a value according to the data discription. hence will convert nan to 'None'","77a93de3":"## Classifying the features \n\nNon_num_cols will have all the columns names that are not numeric\n\nnun_cols will have all the columns names that are numeric\n\ncheck_list has the names of columns that contain \"NA\" based on the data discription","bd20f6e2":"### Plotting a pairplot for every feature with high correlation","8506452f":"### Linear Regression on selected features","a65c4a50":"#### First we need to take out the label as test.csv does not have a label","dd367c01":"##### It seems that some numerical columns are actually classes (categorical)","435bf7aa":"### GridSearch  Decision Tree","8fb81c3f":"### Lasso CV","51bd8b3f":"## filling numeric columns that are actually numeric","ed49aaeb":"### Linear regression","4b4cc944":"Since year built and garage year built matches, we can substitute ","c1c832e0":"### Using lasso model for feature selection\n\nwe have over 300 features in our dataset, what if we reduce them and try to fit them again","5321cc6d":"We can observe that log transformation helped drastically in decreasing the skewness in the continuos features in our dataset"}}