{"cell_type":{"794e5e79":"code","10810c66":"code","35c99ff7":"code","fa3ce437":"code","f98336e6":"code","1de9bb3a":"code","3c1da169":"code","a09f5376":"code","25f15d8d":"code","70e03333":"code","70fcd932":"code","d8d62fcc":"code","83433e38":"code","0ee451a0":"code","bc227ed4":"code","5bc86ecb":"code","57b15698":"code","6abb4c66":"code","2827d65d":"code","a9d4565d":"code","39f25c2d":"code","7a892cfc":"code","75801401":"code","33316368":"code","d373d6ea":"code","fa735966":"code","d62d288b":"code","249ee630":"code","4051d34e":"code","73300ddc":"markdown","613c295f":"markdown","e84bb3d3":"markdown","6c24fee5":"markdown","be0da213":"markdown","0aaf1846":"markdown","359134e2":"markdown","0b2a15cc":"markdown","791eaa7c":"markdown","5e6862da":"markdown","d21a8eb2":"markdown","5eaa633c":"markdown","fe3219e0":"markdown","5cec9e9f":"markdown"},"source":{"794e5e79":"#Installation of Libraries","10810c66":"# Common imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport warnings\nimport os\nimport csv\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input,Dense,Embedding,Bidirectional,LSTM, RepeatVector, TimeDistributed\nfrom tensorflow.keras.models import Model\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nSTOPWORDS = set(stopwords.words('english'))\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n#Disabling Warnings\nwarnings.filterwarnings('ignore')\n\n#Time\/CPU Profiling\noverall_start_time= time.time()\n\n#TPU Related Setting\nfrom kaggle_datasets import KaggleDatasets\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","35c99ff7":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","fa3ce437":"# Accessing Data thorugh Google Cloud Storage\nGCS_DS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"","f98336e6":"#Verifying pathname of dataset before loading - for Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n        print(os.listdir(\"..\/input\"))","1de9bb3a":"# Load Datasets\ndef loadDataset(file_name):\n    df = pd.read_csv(file_name,engine = 'python')\n    return df\n\nstart_time= time.time()\ndf = loadDataset(\"\/kaggle\/input\/webjavascripttext-size50\/WebContent_for_UnsupervisedLearning.csv\")\n#Removing Unwanted Columns \ndf = df[['text']]\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","3c1da169":"#Selection lower numbers as of now for fast testing from 361934 rows\n#df= df.iloc[:500000,]\nprint(len(df), 'train examples')","a09f5376":"start_time= time.time()\ndf['text'] = df['text'].str.lower()\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n#Looking for NaN, if any\nprint(df.isnull().sum())","25f15d8d":"# Removing Stopwords in the text\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\ndf","70e03333":"#Converting the dataframes into numpy arrays \ntext = df['text'].to_numpy()\n# text [0] #For checking","70fcd932":"#Setting of values of Hyperparameters\nvocab_size = 2000\nembedding_dim = 50\nmax_length = 50\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\ntraining_portion = .8","d8d62fcc":"# Code to Check the appropriate Batch Size for TPU (divisible by 128) \n\n# Function to find the largest number smaller \n# than or equal to N that is divisible by k \ndef findNum(N, K): \n    rem = N % K \n    if(rem == 0): \n        return N \n    else: \n        return N - rem \n\nN = 400000\nK = 128\nprint(\"Largest number smaller than or equal to\" + str(N) + \"that is divisible by\" + str(K) +\"is\", findNum(N, K))\nN = 100000\nK = 128\nprint(\"Largest number smaller than or equal to\" + str(N) + \"that is divisible by\" + str(K) +\"is\", findNum(N, K))","83433e38":"# Segregating Training and Validation Data\ntrain_size = int(len(text) * training_portion)\ntrain_text = text[0: 400000] # Size Made compatible to TPU\nvalidation_text = text[400000:499968] \nvalidation_text = validation_text[:198016] # Size Made compatible to TPU\nprint(train_size)\nprint(len(train_text))\nprint(len(validation_text))","0ee451a0":"# Tokenizing Words: Training Text\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_text)\nword_index = tokenizer.word_index\ndict(list(word_index.items())[0:10]) #Checking the first ten in array","bc227ed4":"train_sequences = tokenizer.texts_to_sequences(train_text)\nprint(train_sequences[10])","5bc86ecb":"train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n#Checking \nprint(len(train_sequences[0]))\nprint(len(train_padded[0]))\nprint(len(train_sequences[1]))\nprint(len(train_padded[1]))","57b15698":"#Checking\nprint(train_padded[10])","6abb4c66":"# Tokenizing Word: Validation Text\nvalidation_sequences = tokenizer.texts_to_sequences(validation_text)\nvalidation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(len(validation_sequences))\nprint(validation_padded.shape)","2827d65d":"#Checking Encoding and Decoding of Tokenizer\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_text(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\nprint(decode_text(train_padded[10]))\nprint('---')\nprint(train_text[10])","a9d4565d":"# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","39f25c2d":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    #Encoder\n    encoder_inputs = Input(shape=(max_length,), name='Encoder-Input')\n    emb_layer = Embedding(vocab_size, embedding_dim,input_length = max_length, name='Body-Word-Embedding', mask_zero=False)\n    x = emb_layer(encoder_inputs)\n    x1 = LSTM(50, return_sequences=True, activation='relu', name='Encoder-LSTM1')(x)\n    x2 = LSTM(30, return_sequences=True, activation='relu', name='Encoder-LSTM2')(x1)\n    state_h = LSTM(20,activation='relu', name='Encoder-LSTM3')(x2)\n    encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n    seq2seq_encoder_out = encoder_model(encoder_inputs)\n    encoder_model.summary()\n    #Decoder\n    decoded = RepeatVector(max_length)(seq2seq_encoder_out)\n    decoder_lstm1 = LSTM(20, return_sequences=True, name='Decoder-LSTM-1')\n    decoder_lstm1_output = decoder_lstm1(decoded)\n    decoder_lstm2 = LSTM(30, return_sequences=True, name='Decoder-LSTM-2')\n    decoder_lstm2_output = decoder_lstm2(decoder_lstm1_output)\n    decoder_lstm3 = LSTM(50, return_sequences=True, name='Decoder-LSTM-3')\n    decoder_lstm3_output = decoder_lstm3(decoder_lstm2_output)\n    decoder_dense = Dense(vocab_size, activation='softmax', name='Final-Output-Dense-before')\n    decoder_outputs = decoder_dense(decoder_lstm3_output)\n    #Combining the Model for training\n    seq2seq_Model = Model(encoder_inputs, decoder_outputs)\n    seq2seq_Model.summary()\n    seq2seq_Model.compile(optimizer=keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',metrics=['accuracy'])    ","7a892cfc":"# Computing the batch size for max utilisation of TPU\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nBATCH_SIZE","75801401":"# Training of the Auto Encoder on Combined Model\nseq2seq_Model.compile(optimizer=keras.optimizers.Adam(), loss='sparse_categorical_crossentropy')\nhistory = seq2seq_Model.fit(train_padded, np.expand_dims(train_padded, -1),\n         batch_size=BATCH_SIZE,\n          epochs=100)","33316368":"#Plotting the Accuracy and Loss Achieved in the Unsupervised Model\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  #plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string])\n  plt.show()\n  \nplot_graphs(history, \"loss\")","d373d6ea":"#Testing with a random text\nsentence = 'hello you'\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(sentence)\nseq = tokenizer.texts_to_sequences([sentence])\npad_seq = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n#sentence_vec = encoder_model.predict(pad_seq)\n#print(sentence_vec)","fa735966":"# Save the entire model as a SavedModel.\n#!mkdir -p saved_model\n#encoder_model.save('PretrainedTFModel\/1') ","d62d288b":"#Re-loading the Saved Model for Checking\n#model = tf.keras.models.load_model('PretrainedTFModel\/1')\n\n# Check its architecture\n#model.summary()","249ee630":"#Testing the Embedding from Saved Model\nsentence = 'hello'\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(sentence)\nseq = tokenizer.texts_to_sequences([sentence])\npad_seq = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n#sentence_vec = model.predict(pad_seq)\n#print(sentence_vec)","4051d34e":"# Total Runtime of this Notebook\nprint(\"***Total Time taken --- %s mins ---***\" % ((time.time() - overall_start_time)\/60))","73300ddc":"## <font color=blue> Basic Initialisation <\/font>","613c295f":"## <font color=blue> Saving the Model for Stage-II of Hybrid Deep Learning Moded (Supervised Training) <\/font>","e84bb3d3":"## <font color=blue> Checking the Text Encoding <\/font>","6c24fee5":"#### Cleaning the Dataset","be0da213":" ## <font color=blue> Preprocessing the Dataset <\/font>","0aaf1846":"#### Setting of Hyperparameter values for Text Embedding and Encoding","359134e2":"#### Segregating Training and Validation Data","0b2a15cc":"#### Tokenizing and Padding","791eaa7c":"TPU Hardware Detection","5e6862da":"# <font style=\"color:red;\"><center>Unsupervised Learning of <br> Web Content and JavaScript Embedding <br> (Stage-I of Hybrid Deep Learning Model)<\/center><\/font>","d21a8eb2":"## <font color=blue> Re-checking of Stored Model <\/font>","5eaa633c":"## <font color=blue> Run Time Profiling Statistics of this Notebook <\/font>","fe3219e0":"## <font color=blue> LSTM AutoEncoder Based Text Coding\/Summarization <\/font>","5cec9e9f":"### <font color=blue>Loading Dataset <\/font>"}}