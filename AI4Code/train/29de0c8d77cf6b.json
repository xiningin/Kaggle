{"cell_type":{"5da83f66":"code","e82c8845":"code","5fc38540":"code","7aec887d":"code","8e6e7c74":"code","165aa02b":"code","0eeb555a":"code","7975d6ac":"code","0956945d":"code","50fa366c":"code","1721226a":"code","e853e9aa":"code","96d02a99":"code","5862f595":"code","989f3646":"code","8f1a224d":"code","f35a8762":"markdown"},"source":{"5da83f66":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model","e82c8845":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding='ISO-8859-1')","5fc38540":"df.head()","7aec887d":"# drop unnecessary columns\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)","8e6e7c74":"df.head()","165aa02b":"# rename columns to something better\ndf.columns = ['labels', 'data']","0eeb555a":"# create binary labels\ndf['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})\nY = df['b_labels'].values","7975d6ac":"# split up the data\ndf_train, df_test, Ytrain, Ytest = train_test_split(df['data'], Y, test_size=0.33)","0956945d":"# Convert sentences to sequences\nMAX_VOCAB_SIZE = 20000\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(df_train)\nsequences_train = tokenizer.texts_to_sequences(df_train)\nsequences_test = tokenizer.texts_to_sequences(df_test)","50fa366c":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nV = len(word2idx)\nprint('Found %s unique tokens.' % V)","1721226a":"# pad sequences so that we get a N x T matrix\ndata_train = pad_sequences(sequences_train)\nprint('Shape of data train tensor:', data_train.shape)\n\n# get sequence length\nT = data_train.shape[1]","e853e9aa":"data_test = pad_sequences(sequences_test, maxlen=T)\nprint('Shape of data test tensor:', data_test.shape)","96d02a99":"\nD = 20\n\n# Note: we actually want to the size of the embedding to (V + 1) x D,\n# because the first index starts from 1 and not 0.\n# Thus, if the final index of the embedding matrix is V,\n# then it actually must have size V + 1.\n\ni = Input(shape=(T,))\nx = Embedding(V + 1, D)(i)\nx = Conv1D(32, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(64, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)","5862f595":"# Compile and fit\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)\n\n\nprint('Training model...')\nr = model.fit(\n  data_train,\n  Ytrain,\n  epochs=5,\n  validation_data=(data_test, Ytest)\n)","989f3646":"# Plot loss per iteration\nimport matplotlib.pyplot as plt\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","8f1a224d":"# Plot accuracy per iteration\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()","f35a8762":"**NAME-YASH DIXIT**\n\n***SPAM DETECTION***"}}