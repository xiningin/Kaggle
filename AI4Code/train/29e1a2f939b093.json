{"cell_type":{"9a53680c":"code","60aff54b":"code","be634087":"code","70363911":"code","df0b9881":"code","5432fc2f":"code","4c412c74":"code","8a940140":"code","2d740f2e":"code","adf71323":"code","f2c8b880":"code","098282be":"code","731b6172":"code","7733e95b":"code","db0bcd3a":"code","465026c7":"code","730fcacf":"code","c62203d3":"code","8ca88d23":"code","758a89a7":"code","20abfcc7":"code","e004710b":"code","bcf9651c":"code","2cc4daf2":"code","8f6eae0f":"code","6ae5639e":"code","bdbb851f":"code","b529b516":"code","b768a55a":"code","de13f506":"code","da59b87e":"code","2db041dd":"code","53783398":"code","5428e9a2":"code","3a7f8a06":"code","6cc9a10f":"code","afd19a41":"code","5ab2acfe":"code","053bc306":"code","a28e23b6":"code","943851a5":"code","87a57474":"code","9c5bbffe":"code","5d612d8e":"code","837fed74":"code","672be665":"code","1591090c":"code","5c0ddd7d":"markdown","ddab282f":"markdown","319eec33":"markdown","88413651":"markdown","a9775f41":"markdown","199e0876":"markdown","5f088799":"markdown","7c81cdad":"markdown","071e5a37":"markdown","f5dc5ca9":"markdown","28265cf7":"markdown","5abf5061":"markdown","4060b52e":"markdown","11f1424d":"markdown","563ea55f":"markdown","01b951ee":"markdown","9a780f63":"markdown","c578e76f":"markdown","d88e77bb":"markdown","84888250":"markdown","a3660577":"markdown","a2a063a7":"markdown","73587971":"markdown","00e95e64":"markdown","418af24a":"markdown","84600a4f":"markdown","15289ea1":"markdown","8a45b9ec":"markdown","1161c519":"markdown","0f663ae0":"markdown","c57529a5":"markdown","380e38f0":"markdown","933ec800":"markdown","3f61e019":"markdown","02ddbb16":"markdown","900fa7dd":"markdown","29680bba":"markdown","c7db65f5":"markdown","ae0a8a76":"markdown","897b61ec":"markdown","022a16c4":"markdown","3178956b":"markdown","0e5fad01":"markdown","a558266b":"markdown","cf8ad2cb":"markdown","084ca75e":"markdown","98870e81":"markdown","d9ff7c3e":"markdown","1a85dff0":"markdown"},"source":{"9a53680c":"##########################Load Libraries  ####################################\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom ipywidgets import widgets, interactive\nimport gc\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime, timedelta \nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom itertools import cycle\nimport datetime as dt\nfrom torch.autograd import Variable\nimport random \nimport os\nfrom matplotlib.pyplot import figure\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport time \nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import mean_squared_error\nimport torch \n\n%matplotlib inline\n\n#from gensim.models import Word2Vec\n#import gensim.downloader as api\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","60aff54b":"device = 'cuda:0'\n#device = 'cpu'","be634087":"INPUT_DIR_PATH = '..\/input\/m5-forecasting-accuracy\/'","70363911":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\ndef read_data():\n    sell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\n    sell_prices_df = reduce_mem_usage(sell_prices_df)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n\n    calendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\n    calendar_df = reduce_mem_usage(calendar_df)\n    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n\n    sales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n\n    submission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')\n    return sell_prices_df, calendar_df, sales_train_validation_df, submission_df    ","df0b9881":"_,  calendar_df, sales_train_validation_df, _ = read_data()","5432fc2f":"#Create date index\ndate_index = calendar_df['date']\ndates = date_index[0:1913]\ndates_list = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in dates]","4c412c74":"# Create a data frame for items sales per day with item ids (with Store Id) as columns names  and dates as the index \nsales_train_validation_df['item_store_id'] = sales_train_validation_df.apply(lambda x: x['item_id']+'_'+x['store_id'],axis=1)\nDF_Sales = sales_train_validation_df.loc[:,'d_1':'d_1913'].T\nDF_Sales.columns = sales_train_validation_df['item_store_id'].values\n\n#Set Dates as index \nDF_Sales = pd.DataFrame(DF_Sales).set_index([dates_list])\nDF_Sales.index = pd.to_datetime(DF_Sales.index)\nDF_Sales.head()","8a940140":"#Select arbitrary index and plot the time series\nindex = 6780\ny = pd.DataFrame(DF_Sales.iloc[:,index])\ny = pd.DataFrame(y).set_index([dates_list])\nTS_selected = y \ny.index = pd.to_datetime(y.index)\nax = y.plot(figsize=(30, 9),color='red')\nax.set_facecolor('lightgrey')\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.legend(fontsize=20)\nplt.title(label = 'Sales Demand Selected Time Series Over Time',fontsize = 23)\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","2d740f2e":"#del calendar_df, sales_train_validation_df,DF_Sales\n#gc.collect()","adf71323":"SEED = 1345\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(SEED)","f2c8b880":"data = np.array(y)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_data_normalized = scaler.fit_transform(data.reshape(-1, 1))","098282be":"print(train_data_normalized[:5])\nprint(train_data_normalized[-5:])","731b6172":"fig, axs = plt.subplots(2)\n \nfig.suptitle('Data Distribution Before and After Normalization ',fontsize = 19)\npd.DataFrame(data).plot(kind='hist',ax = axs[0] , alpha=.4 , figsize=[12,6], legend = False,title = ' Before Normalization',color ='red') \npd.DataFrame(train_data_normalized).plot(kind='hist', ax = axs[1] ,figsize=[12,6], alpha=.4 , legend = False,title = ' After Normalization'\\\n                                         ,color = 'blue')   ","7733e95b":"###  This function creates a sliding window or sequences of 28 days and one day label ####\ndef sliding_windows(data, seq_length):\n    x = []\n    y = []\n\n    for i in range(len(data)-seq_length-1):\n        _x = data[i:(i+seq_length)]\n        _y = data[i+seq_length]\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y)","db0bcd3a":"#train_inout_seq = create_inout_sequences(train_data_normalized, train_window)\nseq_length = 28\nx, y = sliding_windows(train_data_normalized, seq_length)\nprint(x.shape)\nprint(y.shape)","465026c7":"train_size = int(len(y) * 0.67)\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\ntrainX = Variable(torch.Tensor(np.array(x[0:train_size])))\ntrainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\ntestX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\ntestY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))","730fcacf":"print(\"train shape is:\",trainX.size())\nprint(\"train label shape is:\",trainY.size())\nprint(\"test shape is:\",testX.size())\nprint(\"test label shape is:\",testY.size())","c62203d3":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        #self.seq_length = seq_length\n        self.dropout = nn.Dropout(p=0.2)\n        \n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = 0.25)\n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        c_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc(h_out)\n        out = self.dropout(out)\n       \n        return out","8ca88d23":"# create a nn class (just-for-fun choice :-) \nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))","758a89a7":"#####  Parameters  ######################\nnum_epochs = 500\nlearning_rate = 1e-3\ninput_size = 1\nhidden_size = 512\nnum_layers = 1\nnum_classes = 1\n\n#####Init the Model #######################\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\n\n##### Set Criterion Optimzer and scheduler ####################\ncriterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n# Train the model\n\nfor epoch in progress_bar(range(num_epochs)): \n    lstm.train()\n    outputs = lstm(trainX.to(device))\n    optimizer.zero_grad()\n    \n    # obtain the loss function\n    loss = criterion(outputs, trainY.to(device))\n    \n    loss.backward()\n    \n    \n    optimizer.step()\n    \n    #Evaluate on test     \n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n    scheduler.step(vall_loss)\n    \n    if epoch % 50 == 0:\n      print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))","20abfcc7":"######Prediction###############\nlstm.eval()\ntrain_predict = lstm(dataX.to(device))\ndata_predict = train_predict.cpu().data.numpy()\ndataY_plot = dataY.data.numpy()\n\n## Inverse Normalize \ndata_predict = scaler.inverse_transform(data_predict)\ndataY_plot = scaler.inverse_transform(dataY_plot)\n\n## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[:-29]])\ndf_labels = pd.DataFrame(dataY_plot)\ndf_labels = df_labels.set_index([dates_list[:-29]])\n\n# Plot \nfigure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","e004710b":"#######Plot the test set ##########################\nfigure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(df_labels.iloc[-testX.size()[0]:][0])\nplt.plot(df_predict.iloc[-testX.size()[0]:][0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Test',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","bcf9651c":"np.sqrt(((dataY_plot[-testX.size()[0]:] - data_predict[-testX.size()[0]:] ) ** 2).mean())","2cc4daf2":"class LSTM2(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM2, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.batch_size = 1\n        #self.seq_length = seq_length\n        \n        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True,dropout = 0.25)\n       \n        \n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(p=0.2)\n    def forward(self, x):\n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n         \n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n       \n        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n     \n        #print(\"hidden state shpe is:\",hn.size())\n        y = hn.view(-1, self.hidden_size)\n        \n        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n        #print(\"final state shape is:\",final_state.shape)\n        out = self.fc(final_state)\n        #out = self.dropout(out)\n        #print(out.size())\n        return out        ","8f6eae0f":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)","6ae5639e":"num_epochs = 700\nlearning_rate = 1e-3\ninput_size = 1\nhidden_size = 512\nnum_layers = 2\n\nnum_classes = 1\n\nlstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\n\n\nlstm.apply(init_weights)\n\ncriterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=100, factor =0.5 ,min_lr=1e-7, eps=1e-08)\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n# Train the model\n\nfor epoch in progress_bar(range(num_epochs)): \n    lstm.train()\n    outputs = lstm(trainX.to(device))\n    optimizer.zero_grad()\n    torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n    # obtain the loss function\n    loss = criterion(outputs, trainY.to(device))\n    \n    loss.backward()\n    \n    scheduler.step(loss)\n    optimizer.step()\n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n    scheduler.step(vall_loss)\n    \n    if epoch % 50 == 0:\n      print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))","bdbb851f":"######Prediction###############\nlstm.eval()\ntrain_predict = lstm(dataX.to(device))\ndata_predict = train_predict.cpu().data.numpy()\ndataY_plot = dataY.data.numpy()\n\n## Inverse Normalize \ndata_predict = scaler.inverse_transform(data_predict)\ndataY_plot = scaler.inverse_transform(dataY_plot)\n\n## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[:-29]])\ndf_labels = pd.DataFrame(dataY_plot)\ndf_labels = df_labels.set_index([dates_list[:-29]])\n\n# Plot \nfigure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","b529b516":"#######Plot the test set ##########################\nfigure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(df_labels.iloc[-testX.size()[0]:][0])\nplt.plot(df_predict.iloc[-testX.size()[0]:][0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Test',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","b768a55a":"np.sqrt(((dataY_plot[-testX.size()[0]:] - data_predict[-testX.size()[0]:] ) ** 2).mean())","de13f506":"# Re-Use the Time Series we have selected earlier\nDF = TS_selected\ncolnames = DF.columns\nDF = DF.rename(columns={colnames[0]:'sales'})\nDF.tail()","da59b87e":"start_time = time.time()\nfor i in (1,7,14,28,365):\n    print('Shifting:', i)\n    DF['lag_'+str(i)] = DF['sales'].transform(lambda x: x.shift(i))\nprint('%0.2f min: Time for bulk shift' % ((time.time() - start_time) \/ 60))","2db041dd":"DF = DF.set_index([dates_list])\nProduct = \"Time Series\"\n\n################Create Plot ##############################################\nfig, axs = plt.subplots(6, 1, figsize=(33, 16))\naxs = axs.flatten()\nax_idx = 0\n\nfor i in (0,1,7,14,28,365):\n    if i == 0:\n        ax = DF['sales'].plot(fontsize = 21,\n                     legend =False,\n                     color=next(color_cycle),\n                     ax=axs[ax_idx])\n        ax.set_ylabel(\"Sales Demand\",fontsize = 21)\n        ax.set_xlabel(\"Date\",fontsize = 21)\n        ax.set_title(fontsize = 21,label = Product)\n\n        ax_idx += 1\n    else : \n        ax = DF[f'lag_{i}'].plot(fontsize = 21,\n                     legend =False,\n                     color=next(color_cycle),\n                     ax=axs[ax_idx])\n        ax.set_ylabel(\"Sales Demand\",fontsize = 21)\n        ax.set_xlabel(\"Date\",fontsize = 21)\n        ax.set_title(fontsize = 21,label = Product+f'  Lag {i}')\n\n        ax_idx += 1    \n   \n   \nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\n\nplt.tight_layout()\nplt.show()","53783398":"for i in [7,14,28,60,180,365]:\n    print('Rolling period:', i)\n    DF['rolling_mean_'+str(i)] = DF['sales'].transform(lambda x: x.shift(28).rolling(i).mean())\n    DF['rolling_std_'+str(i)]  = DF['sales'].transform(lambda x: x.shift(28).rolling(i).std())\n\n\nprint('%0.2f min: Time for loop' % ((time.time() - start_time) \/ 60))\nDF.head()","5428e9a2":"DF = DF.replace('nan', np.nan).fillna(0)\nDF.head()","3a7f8a06":"DF_normlized = DF.copy(deep=True)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ny_scaler = MinMaxScaler(feature_range=(-1, 1))\nscaled_data = scaler.fit_transform(DF) \ny_scaler.fit_transform(DF['sales'].values.reshape(-1, 1))\nDF_normlized.iloc[:,:] =  scaled_data\nDF_normlized.head()","6cc9a10f":"DF_normlized = DF_normlized.reset_index()\nDF_normlized = DF_normlized.rename(columns={'index':'date'})\nDF_normlized.head()\nDF_normlized['date'] = DF_normlized['date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\nDF_normlized = DF_normlized.merge(calendar_df[['date','weekday']],on='date')\nDF_normlized.head()","afd19a41":"## Adding the embedded vectors \nDF_normlized['wd1'] =0\nDF_normlized['wd2'] =0\nDF_normlized['wd3'] =0\nDF_normlized['wd4'] =0\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Sunday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Sunday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Sunday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Sunday']= 0.4 ,-0.3 ,0.6,0.1\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Monday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Monday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Monday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Monday']= 0.2 ,0.2 ,0.5,-0.3\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Tuesday'] ,DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Tuesday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Tuesday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Tuesday']= 0.1,-1.0,1.3,0.9\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Wednesday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Wednesday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Wednesday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Wednesday']= -0.6,0.5,1.2,0.7\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Thursday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Thursday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Thursday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Thursday']= 0.9,0.2,-0.1,0.6\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Friday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Friday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Friday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Friday']= 0.4,1.1,0.3,-1.5\n\nDF_normlized.loc[:,'wd1'][DF_normlized['weekday'] =='Saturday'] , DF_normlized.loc[:,'wd2'][DF_normlized['weekday'] =='Saturday'],\\\nDF_normlized.loc[:,'wd3'][DF_normlized['weekday'] =='Saturday'] , DF_normlized.loc[:,'wd4'][DF_normlized['weekday'] =='Saturday']= 0.3,-0.2,0.6,0.0","5ab2acfe":"fig, axs = plt.subplots(2)\n \nfig.suptitle('rolling_mean_14 - Data Distribution Before and After Normalization ',fontsize = 19)\npd.DataFrame(DF['rolling_mean_14']).plot(kind='hist',ax = axs[0] , alpha=.4 , figsize=[12,6], legend = False,title = ' Before Normalization',color ='red') \npd.DataFrame(DF_normlized['rolling_mean_14']).plot(kind='hist', ax = axs[1] ,figsize=[12,6], alpha=.4 , legend = False,title = ' After Normalization'\\\n                                         ,color = 'blue')","053bc306":"###  This function creates a sliding window or sequences of 28 days and one day label ####\n###  For Multiple features                                                            ####\ndef sliding_windows_mutli_features(data, seq_length):\n    x = []\n    y = []\n\n    for i in range((data.shape[0])-seq_length-1):\n        _x = data[i:(i+seq_length),:] ## 16 columns for features  \n        _y = data[i+seq_length,0] ## column 0 contains the labbel\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y).reshape(-1,1)","a28e23b6":"# Select only the features and the target for prediction  \ndata_with_features = DF_normlized[[\"sales\",\"lag_7\",\"lag_1\",\"lag_28\",\"lag_365\",\"rolling_mean_7\",\\\n\"rolling_std_7\",\"rolling_mean_14\",\"rolling_std_14\",\"rolling_mean_28\",\"rolling_std_28\",\"rolling_mean_60\",\"rolling_std_60\",'lag_28','wd1','wd2','wd3','wd4']].to_numpy()             \n\n#data_with_features = DF_normlized['sales'].to_numpy().reshape(-1,1)\ndata_with_features.shape","943851a5":"x , y = sliding_windows_mutli_features(data_with_features,seq_length=28)\nprint(\"X_data shape is\",x.shape)\nprint(\"y_data shape is\",y.shape)","87a57474":"train_size = int(len(y) * 0.67)\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\ntrainX = Variable(torch.Tensor(np.array(x[0:train_size])))\ntrainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\ntestX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\ntestY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n\n\nprint(\"train shape is:\",trainX.size())\nprint(\"train label shape is:\",trainY.size())\nprint(\"test shape is:\",testX.size())\nprint(\"test label shape is:\",testY.size())","9c5bbffe":"class LSTM2(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM2, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.batch_size = 1\n        #self.seq_length = seq_length\n        \n        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True,dropout = 0.2)\n       \n        self.fc1 = nn.Linear(hidden_size,256)\n        self.bn1 = nn.BatchNorm1d(256,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.dp1 = nn.Dropout(0.25)\n        \n        self.fc2 = nn.Linear(256, 128)\n            \n        self.bn2 = nn.BatchNorm1d(128,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.dp2 = nn.Dropout(0.2)\n        self.fc3= nn.Linear(128, 1)\n        self.relu = nn.ReLU()\n       \n    def forward(self, x):\n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n       \n        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n     \n        #print(\"hidden state shpe is:\",hn.size())\n        y = hn.view(-1, self.hidden_size)\n        \n        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n        #print(\"final state shape is:\",final_state.shape)\n        \n        x0 = self.fc1(final_state)\n        x0 = self.bn1(x0)\n        x0 = self.dp1(x0)\n        x0 = self.relu(x0)\n        \n        x0 = self.fc2(x0)\n        x0 = self.bn2(x0)\n        x0 = self.dp2(x0)\n        \n        x0 = self.relu(x0)\n        \n        out = self.fc3(x0)\n        #print(out.size())\n        return out   ","5d612d8e":"num_epochs = 500\nlearning_rate = 1e-3\ninput_size = 18\nhidden_size = 512\nnum_layers = 4\nnum_classes = 1\n\nbest_val_loss = 100\n\nlstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\n\n\nlstm.apply(init_weights)\n\ncriterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=0.01, momentum=0.9,weight_decay=1e-5)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=50, factor =0.5 ,min_lr=1e-7, eps=1e-08)\n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n# Train the model\n\nfor epoch in progress_bar(range(num_epochs)): \n    lstm.train()\n    outputs = lstm(trainX.to(device))\n    optimizer.zero_grad()\n    torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n    # obtain the loss function\n    loss = criterion(outputs, trainY.to(device))\n    \n    loss.backward()\n    #torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n    \n    optimizer.step()\n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n    scheduler.step(vall_loss)\n    #scheduler.step()\n    \n    if vall_loss.cpu().item() < best_val_loss:\n         torch.save(lstm.state_dict(), 'best_model.pt')\n         print(\"saved best model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n         best_val_loss = vall_loss.cpu().item()\n        \n    \n    if epoch % 50 == 0:\n      print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))","837fed74":"######Prediction###############\nlstm.load_state_dict(torch.load('best_model.pt'))\n\nlstm.eval()\ntrain_predict = lstm(dataX.to(device))\ndata_predict = train_predict.cpu().data.numpy()\ndataY_plot = dataY.data.numpy()\nprint(data_predict.shape)\nprint(dataY_plot.shape)\n\n## Inverse Normalize \ndata_predict = y_scaler.inverse_transform(data_predict)\ndataY_plot = y_scaler.inverse_transform(dataY_plot.reshape(-1, 1))\n\n## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[:-29]])\ndf_labels = pd.DataFrame(dataY_plot)\ndf_labels = df_labels.set_index([dates_list[:-29]])\n\n# Plot \nfigure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","672be665":"#######Plot the test set ##########################\nfigure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(df_labels.iloc[-testX.size()[0]:][0])\nplt.plot(df_predict.iloc[-testX.size()[0]:][0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Test',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","1591090c":"np.sqrt(((dataY_plot[-testX.size()[0]:] - data_predict[-testX.size()[0]:] ) ** 2).mean())","5c0ddd7d":"If we print some of the examples, we can see that the values are now between -1 and 1 ","ddab282f":"So we got :\n* 1262 sets of 28 samples each as the features (X) and 1262 labels as our target(y) in the training set \n* 622 sets of 28 samples with 622 labels in our tests set \n\nYou can see that in Pytorch the tensor dimensions are opposite to the NumPy dimensions ","319eec33":"# Simple LSTM model <a id=\"6\"><\/a>\nIn this section, we create the Pytorch LSTM model.\nThe Model has one LST layer and one dense input layer.\nYou can read all the details about the LSTM \n(Versus RNN) - at the link, I provided above \nThe picture of one cell is taken from that blog.\n\n![LSTM.JPG](attachment:LSTM.JPG)\n\nNote: that Pytorch has the nn.LSTM  function and nn.LSTMCell.\nFrom what I have read you should use the nn.LSTM . \nIt is the layer that will automatically create multiple LSTM layer,\nand it seems that it uses more efficiently the Cuda drivers \nyou can see more in this discussion \nhttps:\/\/discuss.pytorch.org\/t\/nn-lstmcell-inside-nn-lstm\/51189","88413651":"The following parameters are provided to the net\n* Num-classes - is the number of output in this case 1\n* Input size - we don't use batch, so we have one input (of 28 samples)\n* Hidden layers, number of hidden layer in each cell, the more is better, but also will slow down the training\n* Num layers - we have one layer of LSTM (layer we will increase it)","a9775f41":"## LSTM model \nThe model is similar to the previous one with  some enhancement at the output layers ","199e0876":"### Test RMSE","5f088799":"## Predict","7c81cdad":"## Plot Lags\nLet's Plot our lags \nit is a bit hard to see the small lags (as the Time Series containing few years), but for the longer lags \nsuch as 365, we can see the shift ...","071e5a37":"## Pytorch Tensors \nPytorch use tensors as the input to the model \nVariable is a wrapper to the tensor \nThis kernel is only a preliminary starter, \n\nSo I use the Variable wrapper\nA more common way is to train with batches and use the dataset class\nBut this is for later.\n\nIf you want to learn more about Tensors \nRead this tutorial \n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/tensor_tutorial.html\n\nWe also split the data to train and testing or validation sets.\nThere are multiple methods to do this (hold one-off, Cross-Validation),\nbut for Time Series, this needs careful and gentile planning.\nFor simplicity, at this stage, We will split the data for the training set and a test set","f5dc5ca9":"## Predicting Time Series with LSTM Deep Learning Network\n\n### Background \nSince this is my first Time Series competition in Kaggle, I am mainly using it for learning.\nThere are great kernel here, mostly using the boosting models (the most popular is LightGBM \nAnd I have learned how to prepare the data and use it with this popular model.\nMoving forwards, I have decided to learn a bit more about the use of deep learning for Time Series prediction.\nI do have a background from other competition with deep learning but for image vision, working mostly with Pytorch.\nTo go deeper and learn the topic, I have decided to build a learning kernel, that at least at the beginning will explain the topic and the concepts, the definition and the basics, From my experience when you try to explain to others, you learn the most.\n\nSo the first kernel is only trying to explain the basic idea using an arbitrary series from the M5 data.\nI hope that the next versions will go deeper and I can provide a full submission with deep learning. \n## If you like my kernel - Please Vote \n\nIf You like this kernel \nHere a similar one with Seq2Seq model : \n\nhttps:\/\/www.kaggle.com\/omershect\/learning-pytorch-seq2seq-with-m5-data-set\n\n","28265cf7":"# Add Features <a id=\"8\"><\/a> \nSo far we add only one feature - The Sales demand \nNow  let's add more features \n\n![Features.JPG](attachment:Features.JPG)","5abf5061":"Also  I am happy to get comments or things that I need to fix.","4060b52e":"### Load Libraries","11f1424d":"## Versions\n* Version 1-2 First Draft. \n* Version 3 Add Table Of Content.\n* Version 4-7   Add Model with Multiple features\n* Version 8 - Add more epochs to the Multiple features model","563ea55f":"## GPU use \nSince this is a deep learning model, The use of GPU will accelerate the training. \nThe first models are not so demanding so you can still use CPU training (but it will be slower).","01b951ee":"The lags and rolling windows created Nan values \nWhen I tried to train with Nan values, the loss was also Nan \nNeed further understanding, but for now, I will replace the Nan by zero ","9a780f63":"Create the sliding window data-set ","c578e76f":"# Load Data <a id=\"2\"><\/a>","d88e77bb":"# Resources \nHere are some useful resources that provide some background about deep learning, Time series, and various Reuicurent Networks :\n\n* Coursera has a course about Sequences, Time series prediction. It is a basic course that provides a preliminary overview of the concept \n  And building models to Univariate Time series. The course use Keras and TensorFlow,   but it provides a good starting point for this topic\n   \n   Link : https:\/\/www.coursera.org\/learn\/tensorflow-sequences-time-series-and-prediction\/home\/welcome\n* LSTM networks - Great post\/Blog that explains  the concept beyond  LSTM networks \n  \n  Link : https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n* Post about building Time Series deep learning  - The post cover the Keras\/TensorFlow framework, but it also gives a great overview of the main concepts and how to prepare the data.\n  \n  Link : https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/\n* Pytorch Basic example - This is a simple example of how to build a Pytorch LSTM network for simple Univariate Time Series.\n  \n  Link https:\/\/github.com\/pytorch\/examples\/tree\/master\/time_sequence_prediction\n","84888250":"Rolling window is some calculation over a window (example mean )\n\n![rollwindow.JPG](attachment:rollwindow.JPG)","a3660577":"Now we can start again with the selected Time Series, Add some features and modify a bit our model and training.","a2a063a7":"# Multiple LSTM layers <a id=\"7\"><\/a> \nWe can Enhance the net by using multiple LSTM layers. \n![LSTMnet2.JPG](attachment:LSTMnet2.JPG)","73587971":"## Date List\nHere we create dates list, that will help later on to display the Time Series, with the right dates ","00e95e64":"### Test RMSE","418af24a":"## Pytorch Tensor","84600a4f":"Features can be lags or rolling windows. It is easier to implement using data-frame Lag is just shifting the sales demand.\n\n![Lag1.JPG](attachment:Lag1.JPG)","15289ea1":"# Table Of Contents\n\n* [<font size=4>Main Steps<\/font>](#1)\n* [<font size=4>Load Data<\/font>](#2)\n* [<font size=4>One Time Series<\/font>](#3)\n* [<font size=4>Normlize Data<\/font>](#4)\n* [<font size=4>Create Sequences<\/font>](#5)\n* [<font size=4>Simple LSTM model<\/font>](#6)\n* [<font size=4>Multiple LSTM layers<\/font>](#7)\n* [<font size=4>Add Features <\/font>](#8)\n\n\n\n\n\n","8a45b9ec":"The Basic idea of Time Series prediction and RNN (Recurrent Neural Network) is to re-arrange the data \ninto windows of sequences, and labels. \nFor this example, I will use a window or a sequence of 28 samples (28 days ) \nSo the data should look like this series.\n\n![TS3.JPG](attachment:TS3.JPG)\n\nWe create a sliding window which builds sequences and labels.\nIn our case, we have sequences of 28 days, that will use to predict the next day.","1161c519":"To use the day of the week - we will merge data from the calendar DF ","0f663ae0":"# Main Steps <a id=\"1\"><\/a>\n* These are the main steps for building a Time Series Prediction Model : \n![image.png](attachment:image.png)","c57529a5":"## Plot The distribution \nPlot the distribution before and after the Normalization. \nAs you can see, we kept the distribution of the data, but we change its scales.","380e38f0":"## Multi-Dimensional Sliding Window","933ec800":"## Predict on Entire Data Set ","3f61e019":"Let's Compare one example again to verify that the normalization was done properly","02ddbb16":"## Plot The TS","900fa7dd":"## Rolling windows \nFor rolling windows, we will use mean and std (standard deviation)","29680bba":"# Create Sequences <a id=\"5\"><\/a>\nIn this part we align the data to input features and labels with techniques which adapt for Time Series processing\n\n![TS2.JPG](attachment:TS2.JPG)\n\nOr in a more schematic ilustriation \n\n![TS1.JPG](attachment:TS1.JPG)","c7db65f5":"## Normlize Data <a id=\"4\"><\/a>\nNormalization is a technique often applied as part of data preparation for machine learning.\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values.\nFor machine learning, every dataset does not require normalization.\nIt is required only when features have different rangesor scales.\n\n\n\nNormalizes our data using the min\/max scaler with minimum and maximum values of -1 and 1, respectively","ae0a8a76":"# Select One Time Series as an Example <a id=\"3\"><\/a>\nSelecting one arbitrary Time Series ","897b61ec":"The illustration below shows a schematic of our simple LSTM net :\n![LSTMnet1.JPG](attachment:LSTMnet1.JPG)","022a16c4":"Lets see the shape of our data","3178956b":"## Day Of the Week \nThis article : \n\nhttps:\/\/medium.com\/@davidheffernan_99410\/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n\nI have decided to try it as-is on the day of the week - So I will add four-vectors which describe the days of the week \n\n![daysweek.JPG](attachment:daysweek.JPG)","0e5fad01":"### What Next \n* Add more features \n* Calculate WRMSE \n* Change Loss function \n* Train MultiVarient series \n* Use batches \n* Add Convolotional Network \n* Use Cross Validiation \n* Create Submission \n\n\n## If you like the kernel, please vote, and this will encourage me to post more \n","a558266b":"## Credits\nThe Basic data loading and the reduce memory function were taken from this great kernel \n\nhttps:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n\nAlso I have learned alot from this Pytorch LSTM kernel \nhttps:\/\/www.kaggle.com\/gopidurgaprasad\/m5-forecasting-eda-lstm-pytorch-modeling\/notebook?scriptVersionId=31373530","cf8ad2cb":"## RMSE - Test","084ca75e":"## Training \nSome enhancement  we save the best model (based on the lowest validation loss)","98870e81":"## Normalize\nDue to the multi-dimension, we need to adjust \nWe can normalize the full Data-Frame. However, we need to do \ndummy normalize to our target (the sales) as our prediction will be 1D \n\n","d9ff7c3e":"## Lag features ","1a85dff0":"## SEED all "}}