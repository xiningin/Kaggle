{"cell_type":{"daff1098":"code","ae91f78d":"code","86276251":"code","18f3e492":"code","a7c5c67a":"code","bb1a0b07":"code","d54e4a24":"code","b5a927e8":"code","d6418fa3":"code","a5e1d6df":"code","b7a87489":"code","6a18f8fa":"code","8d80784f":"code","8e396858":"code","39a43e01":"code","006cb249":"code","5bd751ff":"code","6e629c59":"code","afb48f9d":"code","09142b94":"code","e9a6fb03":"code","64d65b8a":"code","135138fc":"code","249ada29":"code","b8864de7":"code","cb4a0938":"code","34895ac0":"code","55a97d09":"code","e4299c43":"code","c977b13d":"code","52a51fae":"code","60b08cf6":"code","715a924b":"code","94c0cabd":"code","93de6945":"code","7a8eb714":"code","6a1ff148":"code","024e27a0":"code","9fbafcd8":"code","b387b7c8":"code","aa545058":"code","3388a6cb":"code","1bcee855":"code","6ce5d854":"code","716ec0c5":"code","d44916de":"code","e573d0fe":"code","2a07cf53":"code","8b1ed63a":"code","a0df67e5":"code","81b599ce":"code","d7e1cc9d":"code","471f2965":"code","4871b8ff":"code","d0062f9d":"code","272d8590":"code","5744dd0a":"code","35c86929":"code","7040ca7a":"code","7c73b6ec":"code","f4917f4c":"code","ca573a7f":"code","0aa8ecfc":"code","547ce15b":"code","08efa802":"code","1bb51c8a":"code","90b22523":"code","c216d43a":"code","1213b00e":"code","ac08d3de":"code","5c3a9f43":"code","37edd590":"markdown","be688b23":"markdown","b98da344":"markdown","24772846":"markdown","38604b8c":"markdown","6cabc2a3":"markdown","d119be62":"markdown","371a738e":"markdown","12880a28":"markdown","a6ec0428":"markdown","a860e5a4":"markdown","326c89e8":"markdown","515383db":"markdown","bab57754":"markdown","b5a38815":"markdown","0784f306":"markdown","224dcd58":"markdown","cef22696":"markdown","01f0b4c2":"markdown"},"source":{"daff1098":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae91f78d":"import seaborn as sns\nimport matplotlib.pyplot as plt","86276251":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","18f3e492":"from sklearn.ensemble import  ExtraTreesClassifier","a7c5c67a":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [df, test_df]","bb1a0b07":"df.isnull().sum()","d54e4a24":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\",\n    \"Dona\" : \"Mrs\"\n}\n\ndef status(feature):\n    print('Processing', feature, ': ok')\n\ndef get_titles(combined):\n    # we extract the title from each name\n    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # a map of more aggregated title\n    # we map each title\n    combined['Title'] = combined.Title.map(Title_Dictionary)\n    status('Title')\n    return combined\n\ndf = get_titles(df)\ntest_df = get_titles(test_df)\n\ncombine=[df,test_df]","b5a927e8":"def process_names(combined):\n    # we clean the Name variable\n    combined.drop('Name', axis=1, inplace=True)\n    \n    # encoding in dummy variable\n    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n    combined = pd.concat([combined, titles_dummies], axis=1)\n    \n    # removing the title variable\n    combined.drop('Title', axis=1, inplace=True)\n    \n    status('names')\n    return combined\n\ndf = process_names(df)\ntest_df = process_names(test_df)\n\ndf = df.drop('Title_Royalty',axis = 1)\ncombine=[df,test_df]","d6418fa3":"test_df.head()","a5e1d6df":"df.head()","b7a87489":"test_df.info()","6a18f8fa":"freq_port = df.Embarked.dropna().mode()[0]\nfreq_port","8d80784f":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ndf[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8e396858":"for dataset in combine:\n        dataset[\"Sex\"] = dataset[\"Sex\"].map({'male': 1,'female':0})\n        dataset[\"Embarked\"] = dataset[\"Embarked\"].map({'S': 1,'C':2,'Q':3})\n    \nemb = pd.get_dummies(df.Embarked)\ndf = df.join(emb)\ndf = df.rename(columns={1.0: \"S\", 2.0: \"C\", 3.0 : \"Q\"})\n\nemb = pd.get_dummies(test_df.Embarked)\ntest_df = test_df.join(emb)\ntest_df = test_df.rename(columns={1.0: \"S\", 2.0: \"C\", 3.0 : \"Q\"})\n\n\ndf = df.drop([\"Embarked\",\"PassengerId\"],axis = 1)\ntest_df = test_df.drop([\"Embarked\"],axis = 1)\n\ncombine = [df, test_df]         #updating combine","39a43e01":"def status(feature):\n    print('Processing', feature, ': ok')\n","006cb249":"def process_cabin(combined):    \n    # replacing missing cabins with U (for Uknown)\n    combined.Cabin.fillna('U', inplace=True)\n    \n    # mapping each Cabin value with the cabin letter\n    combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])\n    \n    # dummy encoding ...\n    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    \n    combined = pd.concat([combined, cabin_dummies], axis=1)\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    status('cabin')\n    return combined","5bd751ff":"df = process_cabin(df)\ntest_df = process_cabin(test_df)\n","6e629c59":"combine = [df,test_df]","afb48f9d":"def cleanTicket(ticket):\n    ticket = ticket.replace('.','')\n    ticket = ticket.replace('\/','')\n    ticket = ticket.split()\n    ticket = map(lambda t : t.strip(), ticket)\n    #ticket = filter(lambda t : not t.isdigit(), ticket)\n    ticket = [t for t in ticket if not t.isdigit()]\n    if len(ticket) > 0:\n        return 'YYY'\n    else: \n        return 'XXX'\n\n\nfor dataset in combine:\n    dataset['Ticket'] = dataset['Ticket'].map(cleanTicket)\n    dataset['Ticket'] = dataset['Ticket'].map({'XXX': 0, 'YYY':1})\n\n","09142b94":"df.loc[df['Ticket'] == 1]","e9a6fb03":"combine = [df,test_df]","64d65b8a":"df.head()","135138fc":"df.info()","249ada29":"test_df.info()","b8864de7":"df = df.drop('Cabin_T', axis=1)\ndf.head()","cb4a0938":"df.head()","34895ac0":"combine = [df,test_df]","55a97d09":"grid = sns.FacetGrid(df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","e4299c43":"guess_ages = np.zeros((2,3))\nguess_ages\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ndf.head()","c977b13d":"g = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","52a51fae":"df.info()","60b08cf6":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ndf.head()","715a924b":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass","94c0cabd":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ndf[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","93de6945":"df.info()","7a8eb714":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","6a1ff148":"df = df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)     #dropiing the useless coloumns\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [df, test_df]\n\ndf.head()","024e27a0":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)","9fbafcd8":"test_df.isnull().sum()","b387b7c8":"df['FareBand'] = pd.qcut(df['Fare'], 4)\ndf[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","aa545058":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ndf = df.drop(['FareBand'], axis=1)\ncombine = [df, test_df]\n    \ndf.head(10)","3388a6cb":"for dataset in combine: \n    dataset['Ability'] = df['Fare'] \/ df['Pclass'].astype(np.int8)  \n    \ncombine = [df , test_df]","1bcee855":"corrMatrix = df.corr()\nsns.heatmap(corrMatrix)\nplt.show()","6ce5d854":"test_df.head(10)","716ec0c5":"df.head()","d44916de":"df.info(10)","e573d0fe":"test_df.info()","2a07cf53":"X_train = df.drop(\"Survived\", axis=1)\nY_train = df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","8b1ed63a":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","a0df67e5":"coeff_df = pd.DataFrame(df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","81b599ce":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_predn = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","d7e1cc9d":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","471f2965":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","4871b8ff":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","d0062f9d":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","272d8590":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","5744dd0a":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_predt = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","35c86929":"# Extremely randomized trees\nex = ExtraTreesClassifier(random_state = 6, bootstrap=True, oob_score=True)\nex.fit(X_train, Y_train)\ny_predERT = ex.predict(X_test)\nex.score(X_train, Y_train)\nscore = round(ex.score(X_train, Y_train) * 100, 2)\nprint('Extremely Randomized Trees', score)","7040ca7a":"from sklearn.model_selection import GridSearchCV","7c73b6ec":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [2,4]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the param grid\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n#print(random_grid)\n\nrf_Model = RandomForestClassifier()\n\nrf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)\n\nrf_Grid.fit(X_train, Y_train)\n\nrf_Grid.best_params_ = {'bootstrap': True,\n 'max_depth': 4,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 1,\n 'min_samples_split': 5,\n 'n_estimators': 10}\n","f4917f4c":"\nY_pred = rf_Grid.predict(X_test)\nrf_Grid.score(X_train, Y_train)\nacc_random_forest = round(rf_Grid.score(X_train, Y_train) * 100, 2)\nacc_random_forest","ca573a7f":"# Support Vector Machines\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(SVC(),param_grid=hyper,verbose=True)\ngd.fit(X_train, Y_train)\nY_preds = gd.predict(X_test)\nacc_svc = round(gd.score(X_train, Y_train) * 100, 2)\nacc_svc","0aa8ecfc":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nsorted_model=models.sort_values(by='Score', ascending=False)\nsorted_model","547ce15b":"from tensorflow import keras\nfrom tensorflow.keras import layers, callbacks","08efa802":"early_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=55, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","1bb51c8a":"model = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[24]),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1 , activation='sigmoid'),\n])","90b22523":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)","c216d43a":"history = model.fit(\n    X_train, Y_train,\n    batch_size=256,\n    epochs=3000,\n    callbacks=[early_stopping],\n    verbose = 0 # put your callbacks in a list \n)\n\n","1213b00e":"\n# evaluate the keras model\naccuracy = model.evaluate(X_train, Y_train)","ac08d3de":"y_predN = model.predict(X_test)\nprobas = np.array(y_predN)\ny_predN = (probas < 0.8).astype(np.int)\ny_predN = y_predN.flatten()\ny_predN","5c3a9f43":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_predN\n    })\n\nsubmission.to_csv('submission2.csv', index=False)","37edd590":"Loading the data","be688b23":"Checking for null values. as we can see age has quite some null values along with cabin","b98da344":"I used one hot encoding for Embarked as it was catogorical but not a ordinal value","24772846":"References\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\nhttps:\/\/www.kaggle.com\/soham1024\/titanic-data-science-eda-with-meme-solution\/data\n\nhttps:\/\/www.kaggle.com\/serorjb\/top-3-extremely-randomized-trees\n\nhttps:\/\/www.kaggle.com\/digenessilva\/top-7-titanic-gridsearch#One-Hot-Encoding---Pclass","38604b8c":"Hello Everyone this is my first kaggle notebook ever","6cabc2a3":"Seeing the distribution age in pclass","d119be62":"making a new coloumn (age* class) as we saw that each age of each class are varing slightly in surviblilty","371a738e":"here i tried to get the name title out and make it a catogorical data","12880a28":"Trying desion tree","a6ec0428":"filling the null embarked with more frequent port that is 'S'","a860e5a4":"Filling null values in age as the median of each pclass ","326c89e8":"as titles are catogorical but not ordinal i one hot encoded them","515383db":"Making age a catogorical value as it is more useful","bab57754":"Making a new coloum familySize as a cobination of both parched and sibsize","b5a38815":"Trsining and testing diffrent models","0784f306":"survival groped by age","224dcd58":"There is 1 missing value in test filling it with the median of the fare","cef22696":"making a binanry variable isAlone ","01f0b4c2":"Making Fare a catogorical\/ordinal value too"}}