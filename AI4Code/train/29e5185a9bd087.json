{"cell_type":{"3c005bbd":"code","8fb4bee4":"code","d29c290b":"code","a45585e1":"code","19f8e235":"code","783ce17b":"code","2ee48938":"code","02f85ba7":"code","a91fa46f":"code","10850602":"code","15125868":"code","14ce41bb":"code","e7f1698f":"code","4d602a1e":"code","84f3c683":"code","31e0ba01":"code","0684d9c4":"code","9829a829":"code","ce5bc9bb":"code","cb76fcd5":"code","af9383ca":"code","cf451618":"code","087ec187":"code","2cb3878b":"code","7e057346":"code","d2b5690e":"code","6a64b56f":"markdown","5d9a0262":"markdown","f3dfbb58":"markdown","d2bd3d9e":"markdown","a64398c0":"markdown","2bae285b":"markdown","bb554cb1":"markdown","4e302c9b":"markdown","42cecc44":"markdown","58107b26":"markdown","887582dc":"markdown","5934ce01":"markdown","61542c7a":"markdown","e03ae370":"markdown","373b5113":"markdown","8042a2c7":"markdown","e5a3e8b6":"markdown","ce444d03":"markdown"},"source":{"3c005bbd":"#STEP 1: IMPORTING LIBRARIES AND DATASET\n\n# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate, GridSearchCV\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier, XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\n\n# Importing the dataset from Kaggle\ntraindf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntestdf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\noutcome = 'SalePrice' # Within quotes\n","8fb4bee4":"traindf[outcome].describe()","d29c290b":"# Plotting the curve to understand data distribution\nsns.distplot(traindf[outcome], fit=norm);\nfig = plt.figure()\nres = stats.probplot(traindf[outcome], plot=plt)","a45585e1":"# Applying log transformation to resolve skewness\ntraindf[outcome] = np.log(traindf[outcome])\nsns.distplot(traindf[outcome], fit=norm);\nfig = plt.figure()\nres = stats.probplot(traindf[outcome], plot=plt)","19f8e235":"#correlation matrix for all numerical features\ncor = traindf.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(cor, vmax=.8, square=True, cmap=\"YlGnBu\");","783ce17b":"# top 10 correlated numerical features\nk = 10 #number of variables for heatmap\ncols = cor.nlargest(k, outcome)[outcome].index\ncm = np.corrcoef(traindf[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(cm, vmax=.8, cbar=True, cmap=\"YlGnBu\", annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2ee48938":"# Identifying outliers through scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']\nsns.pairplot(traindf[cols], height = 2.5)\nplt.show();","02f85ba7":"#deleting outliers points by index --> GrLivArea\nvar = 'GrLivArea'\ntemp = pd.concat([traindf[var], traindf[outcome]], axis=1)\ntemp.plot.scatter(x=var, y=outcome)\ntemp.sort_values(by = var, ascending = True)\ntraindf = traindf.drop(traindf[traindf[var] == 4676].index, axis=0)\ntraindf = traindf.drop(traindf[traindf[var] == 5642].index, axis=0)\n\n\n# Identifying outliers through scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']\nsns.pairplot(traindf[cols], height = 2.5)\nplt.show();","a91fa46f":"#STEP 3: DATA PRE-PROCESSING AND FEATURE ENGINEERING ON COMBINED DATASET\n#finding number of missing data\ndf = pd.concat([traindf, testdf], axis=0, sort=False).reset_index(drop=True) #combining the datasets\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(50)","10850602":"#Systematic approach to missing data in each feature\n\n#Columns to fill with 'None'\ncols_to_fill = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageQual', 'GarageFinish', 'GarageCond', 'GarageType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\nfor col in cols_to_fill:\n    df[col] = df[col].fillna('None')\n\n#Columns to fill with 0\ncols_to_fill = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\nfor col in cols_to_fill:\n    df[col] = df[col].fillna(0)\n\n#Columns to fill with mean\ncols_to_fill = []\nfor col in cols_to_fill:\n    df[col] = df[col].fillna(df[col].mean()[0])\n    \n#Columns to fill with mode\ncols_to_fill = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd','SaleType']\nfor col in cols_to_fill:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\n#Miscelleneous replacements\ndf['Functional'] = df['Functional'].fillna('Typ')\n\n#Columns to drop\ncols_to_drop = ['LotFrontage', 'Utilities', '1stFlrSF', 'GarageArea', 'GarageYrBlt']\nfor col in cols_to_drop:\n    df = df.drop([col], axis=1)\n\n#Check for missing data again\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(50)","15125868":"# Analysing and normalising target variable\nvar = 'GrLivArea'\nsns.distplot(df[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df[var], plot=plt)\n\n# Applying log transformation to resolve skewness\ndf[var] = np.log(df[var])\nsns.distplot(df[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'TotalBsmtSF'\nsns.distplot(df[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df[var], plot=plt)\n\n# Creating a new variable column for 'HasBsmt'\ndf['HasBsmt'] = 0\ndf.loc[df['TotalBsmtSF']>0, 'HasBsmt'] = 1\n\n# Applying log transformation to resolve skewness\ndf.loc[df['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(df['TotalBsmtSF'])\nsns.distplot(df[df[var]>0][var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df[df[var]>0][var], plot=plt)\ndf = df.drop(['HasBsmt'],axis=1)","14ce41bb":"#recasting numerical data that are actually categorical\ncols_to_cast = ['MSSubClass']\nfor col in cols_to_cast:\n    df[col] = df[col].astype(str)\n\n#Label encoding for ordinal values\ncols_to_label = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'OverallCond', \n        'YrSold', 'MoSold']\n\nfor col in cols_to_label:\n    lbl = LabelEncoder()\n    lbl.fit(list(df[col].values))\n    df[col] = lbl.transform(list(df[col].values))\n\n#OneHotEncoder\/get_dummies for remaining categorical features\ndf = pd.get_dummies(df)","e7f1698f":"#STEP 4: XGBOOST MODELING WITH PARAMETER TUNING\n\n#Creating train_test_split for cross validation\nX = df.loc[df[outcome]>0]\nX = X.drop([outcome], axis=1)\ny = df[[outcome]]\ny = y.drop(y.loc[y[outcome].isnull()].index, axis=0)\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2, random_state=8)\n\n#Creating DMatrices for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n","4d602a1e":"#Setting initial parameters\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':5,\n    'min_child_weight': 1,\n    'eta':0.3,\n    'subsample': 0.80,\n    'colsample_bytree': 0.80,\n    'reg_alpha': 0,\n    'reg_lambda': 0,\n    # Other parameters\n    'objective':'reg:squarederror',\n}\n\n\n#Setting evaluation metrics - MAE from sklearn.metrics\nparams['eval_metric'] = \"mae\"\n\nnum_boost_round = 5000\n\n#Begin training of XGB model\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1\n\n#Establishing baseline MAE\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=8,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=50\n)\ncv_results\ncv_results['test-mae-mean'].min()\n","84f3c683":"#Parameter-tuning for max_depth & min_child_weight (First round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(0,12,2)\n    for min_child_weight in range(0,12,2)\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n\n#Parameter-tuning for max_depth & min_child_weight (Second round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in [3,4,5]\n    for min_child_weight in [3,4,5]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n\n#Parameter-tuning for max_depth & min_child_weight (Third round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in [3]\n    for min_child_weight in [i\/10. for i in range(30,50,2)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","31e0ba01":"#Updating max_depth and mind_child_weight parameters\nparams['max_depth'] = 3\nparams['min_child_weight'] = 3.2","0684d9c4":"#Recalibrating num_boost_round after parameter updates\nnum_boost_round = 5000\n\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1\n\n","9829a829":"#Parameter-tuning for subsample & colsample (First round)\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(3,11)]\n    for colsample in [i\/10. for i in range(3,11)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n#Parameter-tuning for subsample & colsample (Second round)\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/100. for i in range(80,100)]\n    for colsample in [i\/100. for i in range(70,90)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","ce5bc9bb":"#Updating subsample and colsample parameters\nparams['subsample'] = 0.84\nparams['colsample'] = 0.71","cb76fcd5":"#Recalibrating num_boost_round after parameter updates\nnum_boost_round = 5000\n\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1\n","af9383ca":"#Parameter-tuning for reg_alpha & reg_lambda\ngridsearch_params = [\n    (reg_alpha, reg_lambda)\n    for reg_alpha in [1e-5, 1e-4, 1e-3, 1e-2, 0.1]\n    for reg_lambda in [1e-5, 1e-4, 1e-3, 1e-2, 0.1]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n\nfor reg_alpha, reg_lambda in gridsearch_params:\n    print(\"CV with reg_alpha={}, reg_lambda={}\".format(\n                             reg_alpha,\n                             reg_lambda))\n    # We update our parameters\n    params['reg_alpha'] = reg_alpha\n    params['reg_lambda'] = reg_lambda\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (reg_alpha,reg_lambda)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","cf451618":"#Updating reg_alpha and reg_lambda parameters\nparams['reg_alpha'] = 1e-05\nparams['reg_lambda'] = 0.001","087ec187":"#Resetting num_boost_round to 5000\nnum_boost_round = 5000\n\n#Parameter-tuning for eta\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [0.3, 0.2, 0.1, 0.05, 0.01, 0.005]:\n    print(\"CV with eta={}\".format(eta))\n\n    params['eta'] = eta\n    cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=8,\n            nfold=5,\n            metrics=['mae'],\n            early_stopping_rounds=50\n          )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","2cb3878b":"params['eta'] = 0.005","7e057346":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=5000,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nnum_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)\n\nmean_absolute_error(best_model.predict(dtest), y_test)\n","d2b5690e":"testdf = df.loc[df[outcome].isnull()]\ntestdf = testdf.drop([outcome],axis=1)\nsub = pd.DataFrame()\nsub['Id'] = testdf['Id']\ntestdf = xgb.DMatrix(testdf)\n\ny_pred = np.expm1(best_model.predict(testdf))\nsub['SalePrice'] = y_pred\n\nsub.to_csv('submission.csv', index=False)","6a64b56f":"Now that all ideal parameters are found, let us recap:\n\nparams = {\n    'max_depth':3,\n    'min_child_weight': 3.2,\n    'eta':0.005,\n    'subsample': 0.84,\n    'colsample_bytree': 0.71,\n    'reg_alpha': 1e-05,\n    'reg_lambda': 0.001,\n}\n","5d9a0262":"And with this model, we'll now fit the actual test data to the model. Don't forget to reverse the logarhithm we applied on 'SalePrice' during normalisation.","f3dfbb58":"So let's plot a second seaborn map, but this time we'll only focus on the top 10 correlated numerical features.","d2bd3d9e":"Next we'll set the initial parameters for the model. I have followed the logics laid out in this guide for the setting of original parameters: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nAnd with the initial parameters, we'll determine the ideal 'num_boost_round' and set a baseline MAE to beat. This way, we'll know whether the parameters we're tuning will indeed be resulting in a lower MAE.","a64398c0":"Now that all missing data have been handled, we'll take a look at the distribution pattern for some of our critical features. As mentioned at the beginning of this journal, most machine learning techniques work better with normalised data. I'll be using the same technique to identify and correct for skewness for 'GrLivArea' and 'TotalBsmtSF' features.\n\nThe technique for normalising 'TotalBsmtSF' can be found in Pedro's notebook. An extra step is needed as data consist of several '0' values that cannot be logarithmized. However, unlike Pedro, I have decided to drop the additional 'HasBsmt' column after normalisation. ","2bae285b":"**#STEP 1: IMPORTING LIBRARIES AND DATASET**\n\nI believe this step begins like all other kaggle submission, importing the relevant libraries for data science challenges, and of course importing of train and test data set. It is also at this point I have chosen to assign the target variable 'SalePrice' to a variable 'outcome'. This is because I wanted to build a set of codes\/templates that can easily be reused for other challenges. ","bb554cb1":"There! Now that we have a normally distributed target variable, the next step would be to explore the remaining variables. Let's begin with numerical features.\n\nAs our dataset has a plethora of independent variable, feature selection is more critical than feature engineering in this particular problem. Thankfully, we can use seaborn to plot a correlation matrix. Seaborn not only helps us to identify 2 important things:\n1. Correlation between numerical features and our target variable\n2. Correlation between numerical features and other key features\n\nYou can choose to plot the entire features map, but personally I find it overwhelming (see below). ","4e302c9b":"And.... done! I welcome all feedbacks, please feel free to point out any areas I can do better.\nThank you!","42cecc44":"In the final step of data pre-processing, we need to ensure that all features are holding numerical values, so that we can run them through the XGBRegressor model. We need tho do the following:\n* Recast any numerical features that are actually categorical\n* Conduct Label Encoding for ordinal features\n* Conduct OneHotEncoder for remaining categorical features","58107b26":"Beautiful. At first it might be overwhelming, but that's precisely why we chose top 5 features instead of all 10 features for this step. That would be confusing and frankly unnecessary. From these scatterplots, we can already identify some data points that are clearly outliers. So let's weed them out, we can do so by zooming into SalePrice\/GrLivArea to identify the specific index of two outlier points. Then we'll re-do the scatterplot again to verify.","887582dc":"Much better! And we can see from the get-go that 'GarageCars' and 'GarageArea' is strongly correlated. And intuitively, that should not come as a surprise. They're basically measuring the same thing, just in different units (number of cars vs number of squarefeet). Now that we have narrowed down on the critical features to focus on. The next step would be to catch outliers that may not be representative of the data.\n\nAs these are the most strongly correlated features, any outliers should show up immediately through a scatterplot of just our top 5 features. I have skipped 'GarageArea', as we have concluded that it should not present us with any new information that 'GarageCars' isn't. ","5934ce01":"**About Me**\nI publish kaggle notebooks to outline my learning journey in Data Science. This is the very first notebook that I have pusblished, and it contains the step-by-step approach I have adopted to solve the \"House Prices\" problem.\n\nI am fairly new to Data Science and Kaggle Challenges, and have depended heavily on the guides made public by other Kagglers. Namely I have adopted much of my data exploration techniques from both Pedro Marcelino's guide (https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) as well as Serigne's guide (https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard). However I have also processed data using my own understanding, and at certain points have chosen to make different decisions from the two guides published above.\n\n**Disclaimers**\nAs this is my first time  publishing a notebook on Kaggle, I hope you find the guide helpful in some ways. If you find any areas for improvement, please feel free to suggest new approaches I may adopt. I welcome all feedbacks. \n\n**Content Outline**\nMy approach can be largely categorised into 4 major steps:\nSTEP 1: IMPORTING LIBRARIES AND DATASET\nSTEP 2: EXPLORATORY DATA ANALYSIS ON TRAINING SET\nSTEP 3: DATA PRE-PROCESSING AND FEATURE ENGINEERING ON COMBINED DATASET\nSTEP 4: XGBOOST MODELING WITH PARAMETER TUNING\n\nLet's dive in!","61542c7a":"Some may argure that the SalePrice\/GrLivArea plot shows 2 more outlier points. I have decided to keep them even though they may be further from other points, but they seem to still be following the same trend.\n\nOkay! So far we have normalised the target variable, identified key numerical features and weeded out outlier points. I think we're done with data exploration with the training set, so let's move on!\n\n\n**#STEP 3: DATA PRE-PROCESSING AND FEATURE ENGINEERING ON COMBINED DATASET**\n\nFirstly, we'll combine the train and dataset, so that any data transformation will be applied to all data uniformly. Once done, we'll need to find out what exactly are the missing data we need to handle.","e03ae370":"We can see that the baseline MAE to beat is ~0.1004. And initial num_boost_round will be set as 29. From here on, we will begin parameter-tuning in 3 phases:\n* Tuning max_depth & min_child_weight (Tree-specific parameter)\n* Tuning subsample & colsample (Tree-specific parameter)\n* Tuning reg_alpha & reg_lambda (Regularisation parameter)\n* Tuning eta (Learning Rate)\n\nBy using gridsearch technique, we can narrow down on various values we want to test for each phase. Once the ideal value is determined, we need to update the parameters and recalibrate num_boost_round.","373b5113":"And now, finally, our data is cleaned up and ready for modelling. That brings us to the next step.\n\n**#STEP 4: XGBOOST MODELING WITH PARAMETER TUNING**\n\nFor this journal, I have chosen a single model of XGBRegressor. The approach I have adopted for parameter-tuning can be found here: https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n\nI have gone with the common 'Mean Absolute Error' as the measuring metric, and will be applying a 5-fold cross-validation technique for training. Before we begin, we'll do a train_test_split, and load them into DMatrix (data format required for XGB models).","8042a2c7":"There are multiple methods to handle missing data:\n* Fill missing data with or 0 (Common for Numerical Features)\n* Fill missing data with 'None' (Common for Categorical Features)\n* Fill missing data with Mean (Common for Numerical Features)\n* Fill missing data with Mode (Common for Categorical Features)\n* Drop the Column (Common for features with large percentage of missing data)\n* Drop the Row (Common for rare occurances among data)\n* Replace with any other value you deem logical\n\nIn our case, many of the \"missing data\" actually meant that the house does not have that particular feature (e.g. absence of pool results in empty field for 'PoolQC' etc).\n","e5a3e8b6":"**#STEP 2: EXPLORATORY DATA ANALYSIS ON TRAIN DATASET**\n\nPersonally, I find that it is important to understand the problem that we're trying to solve right from the start. In this particular problem, we are to predict an independent variable 'SalePrice' based on a long list of dependent variables. The first step for me, would then be to understand more about the 'SalePrice' variable - We're interested in what's the count of data and how are they distributed?","ce444d03":"So based on initial analysis, we can see a total count of 1460 data count in the labelled train set, centred around the mean of ~180,921. However we will not say that the data is normally distributed, and have demonstrated postive skewness. The probability plot is a technique picked up from Pedro Marcelino's guide - normally distributed data should be following the diagonal line closely.\n\nSo next step would be to resolve the skewness and normalise our target variable. This is important as most machine learning techniques are either built on, or simply works better with normally distributed data. A simple technique would be to apply log transformation to resolve the skewness."}}