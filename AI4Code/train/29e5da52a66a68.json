{"cell_type":{"07c990b0":"code","120ad1df":"code","be528ae8":"code","01d57b75":"code","d43d1a84":"code","b942c91c":"code","4a9b40ba":"code","cb6af2c1":"code","5b45d1e5":"code","8f7fcb36":"code","af20caf7":"code","0c68b1f4":"code","7635fe8c":"code","23706227":"code","ef600521":"code","9b1c8865":"code","cc203716":"code","60e2ffe4":"code","c640d52c":"code","3f5ca34d":"code","eaeb1557":"code","a5ba06a8":"code","131e80be":"code","bef3da51":"code","a2ee1664":"code","e747ef17":"code","b7fb1c62":"code","d6fedb98":"code","2e92a58a":"markdown","009e7bf9":"markdown","c558c390":"markdown","d2ba6830":"markdown","ba1b9d68":"markdown","cb1ac1b2":"markdown","690aaabd":"markdown","5f2cdc54":"markdown","056a77cb":"markdown","6fe9ab6b":"markdown"},"source":{"07c990b0":"import datetime\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler","120ad1df":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","be528ae8":"# data type\uc744 \ubcf4\uace0, \uadf8\uc5d0 \ub9de\ucdb0\uc11c \uadf8\ub798\ud504 \ubd84\uc11d\uc744 \uc2dc\ud589\ud560 \uac81\ub2c8\ub2e4.\ndf_train.info()","01d57b75":"sns.boxplot(df_train[\"Age\"])","d43d1a84":"sns.boxplot(df_train[\"Fare\"])","b942c91c":"def outlier_replace(data, col_name, q1=0.25, q3=0.75):\n    quartile1 = data[col_name].quantile(q1)\n    quartile3 = data[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    data.loc[(data[col_name] < low_limit), col_name] = low_limit\n    data.loc[(data[col_name] > up_limit), col_name] = up_limit","4a9b40ba":"outlier_replace(df_train, 'Age')\noutlier_replace(df_train, 'Fare')\noutlier_replace(df_test, 'Age')\noutlier_replace(df_test, 'Fare')","cb6af2c1":"def missing_values_table(dataframe, na_name=False):\n    # null\uac12\uc774 1\uac1c \uc774\uc0c1 \uc788\ub294 column \ucd94\ucd9c\n    # The columns name that contains missing value\n    null_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[null_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[null_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return null_columns\n    \nmissing_values_table(df_train, True)","5b45d1e5":"df_train[\"Age\"].fillna(df_train[\"Age\"].median())\ndf_test[\"Age\"].fillna(df_test[\"Age\"].median())\n\ndf_train = df_train.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\ndf_test = df_test.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode()[0])\n\ndf_train.isnull().sum().sort_values(ascending=False)","8f7fcb36":"df_train=df_train.drop(['Name','Ticket'],axis=1)\ndf_test=df_test.drop(['Name','Ticket'],axis=1)","af20caf7":"cat_cols = [col for col in df_train.columns if df_train[col].dtype not in [\"float16\",\"float32\",\"float64\",\"int\"]]\ny = df_train[\"Survived\"].copy()\ny_nm = 'Survived'\ntrain_y = pd.DataFrame(df_train[y_nm])\n\ntrain_x = df_train.drop(y_nm, axis = 1)\ntrain_y = pd.DataFrame(df_train[y_nm])\n\ntest_x = df_test\n\nfor cols in cat_cols:\n    enc = TargetEncoder(cols=[cols])\n    train_x = enc.fit_transform(train_x, y)\n    test_x = enc.transform(test_x)\n\ndf_train = pd.concat([train_x, train_y], axis=1) ","0c68b1f4":"# Scaling the dataset\nscaler = MinMaxScaler()\ndf_train = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\ndf_test = pd.DataFrame(scaler.fit_transform(test_x), columns=test_x.columns)\ndf_train.head()","7635fe8c":"def new_col(df):\n    df[\"NEW_FAMILY_SIZE\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n    df[\"NEW_AGExPCLASS\"] = df[\"Age\"] * df[\"Pclass\"]\n    df.loc[((df['SibSp'] + df['Parch']) > 0), \"NEW_IS_ALONE\"] = \"NO\"\n    df.loc[((df['SibSp'] + df['Parch']) == 0), \"NEW_IS_ALONE\"] = \"YES\"\n    \nnew_col(df_train)\nnew_col(df_test)\n\ndf_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)","23706227":"random_state_val =777\ntest_size_val =0.3\n\ndf_train, df_val = train_test_split(df_train, test_size = test_size_val, random_state = random_state_val)\n\ndrop_col = ['Survived', 'PassengerId']\ny_nm = 'Survived'\n\ndf_train_x = df_train.drop(drop_col, axis = 1)\ndf_train_y = pd.DataFrame(df_train[y_nm])\n\ndf_val_x = df_val.drop(drop_col, axis = 1)\ndf_val_y = pd.DataFrame(df_val[y_nm])","ef600521":"LGBClassifier = lgb.LGBMClassifier(objective='binary',\n                                   max_depth = 8,\n                                   learning_rate = 0.01,\n                                   n_estimators = 9000,\n                                   max_bin = 200,\n                                   bagging_freq = 4,\n                                   bagging_seed = 8,\n                                   feature_fraction = 0.2,\n                                   feature_fraction_seed = 8,\n                                   min_sum_hessian_in_leaf = 11,\n                                   verbose = -1,\n                                   random_state = 42)","9b1c8865":"start = datetime.datetime.now()\nlgbm = LGBClassifier.fit(df_train_x.values,\n                       df_train_y.values.ravel(),\n                       eval_set = [(df_train_x.values, df_train_y), (df_val_x.values, df_val_y)],\n                       eval_metric ='logloss',\n                       early_stopping_rounds = 20,\n                       verbose =False)\nend = datetime.datetime.now()\nend-start","cc203716":"feature_imp= pd.DataFrame(sorted(zip(lgbm.feature_importances_, df_train_x.columns), reverse = True), columns = ['Value', 'Feature'])\n# feature_imp.to_excel(\"feature_imp.xlsx\")\n\nplt.figure(figsize=(7,5))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.tight_layout()\nplt.show()\n# plt.savefig('lightGBM_ Importances.png')","60e2ffe4":"result_lst =[]\nmax_accuracy =0.\nopt_threshold =0.\nval_y_prob = lgbm.predict_proba(df_val_x.values)[:, 1]\n\nfor n in range(0,60):\n    threshold = round(((n+1)*0.01),2)\n    pred_yn = val_y_prob.copy()\n    pred_yn = np.where(pred_yn > threshold, 1., 0.)\n    \n    result_dict = {}\n    precision, recall, f1_score, support = precision_recall_fscore_support(df_val_y.values.ravel(), pred_yn, average='binary')\n    accuracy = accuracy_score(df_val_y.values.ravel(), pred_yn)\n    kappa = cohen_kappa_score(df_val_y.values.ravel(), pred_yn)\n    \n    result_dict ={'Threshold': threshold, 'Accuracy': round(accuracy,4), 'Precision': round(precision,4), 'Recall': round(recall,4), 'F1_Score': round(f1_score,4), 'Kappa': round(kappa,4)}\n    result_lst.append(result_dict)\n    \n    if max_accuracy <= accuracy:\n        max_accuracy = accuracy\n        opt_threshold = threshold\n        \n    confMat = confusion_matrix(df_val_y.values.ravel(), pred_yn, labels=[1,0])\n    \nmatric_df = pd.DataFrame(result_lst, columns=['Threshold','Accuracy', 'Precision', 'Recall', 'F1_Score', 'Kappa'])\nmatric_df.to_csv('REC_scores.csv',sep=',', header=True, index=False, encoding='UTF-8')\n\nprint('\ucd5c\uace0 Accuracy-SCORE =%f, \uc784\uacc4\uce58=%f'%(max_accuracy, opt_threshold))\nprint('Threshold \uc124\uc815 \uc644\ub8cc')","c640d52c":"predict_lgbm = lgbm.predict_proba(df_train_x.values)[:,1]\npred_train = np.where(predict_lgbm > opt_threshold, 1., 0.)\n\ntp, fn, fp, tn = confusion_matrix(df_train_y.values.ravel(), pred_train, labels=[1,0]).ravel()","3f5ca34d":"conf_matrix = pd.DataFrame(\n    confusion_matrix(df_train_y.values.ravel(), pred_train),\n    columns=['Predicted Value 0', 'Predicted Value 1'],\n    index=['True Value 0', 'True Value 1']\n)\n\nprint(\"1. Counfusion Matrix\")\nprint(conf_matrix.T)\nprint(\"\")\n\nprint(\"2. Classification Report\")\nprint(classification_report(df_train_y.values.ravel(), pred_train))","eaeb1557":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(df_train_y.values.ravel(), predict_lgbm)\n\nimport matplotlib.pyplot as plt\nroc_auc = auc(fpr, tpr)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","a5ba06a8":"Accuracy_Rate = (tp + tn) \/ (tp + tn + fp + fn)\nRecall_Rate = tp \/ (tp + fn)\nPrecision_Rate = tp \/ (tp + fp)\nSpecificity_Rate = tn \/ (tn + fp)\nF1_Score = (Precision_Rate * Recall_Rate) \/ (Precision_Rate + Recall_Rate) * 2\n\nprint(\"3. Model Metric Sumamry\")\nprint(\" - Accuracy Rate    : {:2.3f} %\".format(Accuracy_Rate*100))\nprint(\" - Recall Rate      : {:2.3f} %\".format(Recall_Rate*100))\nprint(\" - Precision Rate   : {:2.3f} %\".format(Precision_Rate*100))\nprint(\" - Specificity Rate : {:2.3f} %\".format(Specificity_Rate*100))\nprint(\" - F1 Score         : {:2.3f} \".format(F1_Score*100))\nprint(\" - ROC AUC          : {:2.3f} \".format(roc_auc*100))","131e80be":"df_test = df_test.drop(['PassengerId'], axis = 1)\n\npredict_lgbm = lgbm.predict_proba(df_test.values)[:,1]\npred_test = np.where(predict_lgbm > opt_threshold, 1., 0.)\n\n# test_result= pd.DataFrame(pred_test)\n# test_result.columns = ['Survived']\n# predict = test_result['Survived']\n# df_test = pd.read_csv('..\/input\/titanic\/test.csv')\n# Id_No = df_test['PassengerId']\n# submission = pd.DataFrame({'PassengerId': Id_No, \"Survived\": predict})\n# submission['Survived'] = submission['Survived'].astype('Int64')\n# submission.to_csv('submission.csv', index=False)\n\nmodel_score = lgbm.score(df_train_x.values, df_train_y.values.ravel())\nprint(\"Model Score : {:.2f} %\".format(model_score * 100))","bef3da51":"from sklearn.ensemble import RandomForestClassifier\n\ndrop_col = ['Survived', 'PassengerId']\ny_nm = 'Survived'\n\ndf_train_x = df_train.drop(drop_col, axis = 1)\ndf_train_y = pd.DataFrame(df_train[y_nm])\n# df_test = df_test.drop(['PassengerId'], axis = 1)\n\nrf_model = RandomForestClassifier(max_depth=5, n_estimators= 9000, random_state=42)\nrf_model.fit(df_train_x.values, df_train_y.values.ravel())\nmodel_score = rf_model.score(df_train_x.values, df_train_y.values.ravel())\nprint(\"Model Score : {:.2f} %\".format(model_score * 100))","a2ee1664":"from sklearn.svm import SVC\n\nSVM_model = SVC(C = 100, gamma= 0.01, kernel='rbf')\nSVM_model.fit(df_train_x.values, df_train_y.values.ravel())\nmodel_score = SVM_model.score(df_train_x.values, df_train_y.values.ravel())\nprint(\"Model Score : {:.2f} %\".format(model_score * 100))","e747ef17":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(learning_rate= 0.2, max_depth=4, n_estimators=75, reg_lambda=0.5, subsample= 0.5, use_label_encoder=False, eval_metric='logloss') #0,843\nxgb_model.fit(df_train_x.values, df_train_y.values.ravel())\nmodel_score = xgb_model.score(df_train_x.values, df_train_y.values.ravel())\nprint(\"Model Score : {:.2f} %\".format(model_score * 100))","b7fb1c62":"df_test2 = pd.read_csv('..\/input\/titanic\/test.csv')\nId_No = df_test2['PassengerId']\n\nlgbm_predict = lgbm.predict(df_test.values)\nxgb_predict = xgb_model.predict(df_test.values) \nrf_predict = rf_model.predict(df_test.values) \nsvm_predict = SVM_model.predict(df_test.values) \n\ndef vote(votes):\n    weight_dict = {'LGBM':1,'XGB':1,'RF':1,\"SVM\":1}\n    weights = np.array(list(weight_dict.values()))\n    sw = weights.sum()\n    v = [v * weights[i] for i,v in enumerate(votes)]\n    return sum(v)\/ sw\n\nall_predict = pd.DataFrame({'PassengerId': Id_No,\n                            'LGBM': lgbm_predict, 'XGB': xgb_predict, 'RF':rf_predict, \"SVM\":svm_predict})\nclfs = ['LGBM','XGB','RF',\"SVM\"]\nall_predict['Vote'] = all_predict[clfs].apply(lambda row: vote(row), axis = 1)\nall_predict['Predict'] = all_predict.Vote.apply(lambda row: int(np.rint(row)))\nvc_predictions = all_predict.Predict\nall_predict.head(10)","d6fedb98":"submission = pd.DataFrame({'PassengerId': Id_No, 'Survived': vc_predictions})\nsubmission['Survived'] = submission['Survived'].astype('Int64')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","2e92a58a":"\uc2a4\ucf00\uc77c\ub9c1 \uc885\ub958 (Type of feature scaling): \n - StandardScaler: SND\ub97c \ub530\ub984. \ud3c9\uade0\uc740 0\/ \uc2dd\uc740 z = (x - u) \/ s\n - RobustScaler: 1\/3\ubd84\uc704\uc218\ub97c \ud65c\uc6a9\ud55c \uac15\ub825\ud55c \ud1b5\uacc4\ubc29\uc2dd\/ value = (value \u2013 median)\/ (p75(3\ubd84\uc704) \u2013 p25(1\ubd84\uc704))\n - MinMaxScaler: X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0)), X_scaled = X_std * (max - min) + min\n - Logaritmic Scaler: \ub85c\uadf8\ub97c \uc0ac\uc6a9\/ \uc74c\uc218 \uc0ac\uc6a9 \ubd88\uac00 \uac10\uc548 \uc0ac\uc6a9\uc2dc \uc8fc\uc758\uac00 \ud544\uc694\n(Taking the log of the value. But, if we have a negative values we couldn't take the log. So we need to be careful abaout it.)","009e7bf9":"1\ubd84\uc704\uc218\uc640 3\ubd84\uc704\uc218\ub97c \uae30\uc900 \uac12\uc744 \uad6c\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e4\uace0, \uc774\uac83\uc744 outlier\uc5d0 \uc801\uc6a9\ud558\ub294 \ub85c\uc9c1\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \n(Create a function to find the reference values for the first and third quartiles, and create a logic that applies them to outliers.)","c558c390":"# 2. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30(Read Dataset) \ubc0f EDA\n \n\uc544\ub798\uc640 \uac19\uc774 \ub370\uc774\ud130 \ubd84\uc11d \ubc0f Data \uc804\ucc98\ub9ac \uc608\uc815\uc785\ub2c8\ub2e4.\n(Data analysis and data preprocessing will be conducted as below.)\n - Outliers (Missing\uac12\uc744 \ub300\uccb4 \ud560\ub54c \ud3c9\uade0\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc5d0 \uba3c\uc800 \uc2dc\ud589\ud569\ub2c8\ub2e4.)\n - Encoding(x,y\uac12\uc744 \ub098\ub220\uc57c \ud558\uae30\uc5d0 \ub4a4\uc5d0\uc11c \ud560 \uc218 \uc788\uc74c) + Missing Values(encoding \ub41c \uac12\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc5b4\uc11c \ub458\uc774 \ud568\uaed8 \uc791\uc5c5\ud568)\n - Feature Scaling\n - Feature Extraction\n - Feature Interactions\n \n\uc0c1\uae30 \ubd84\uc11d\uc740 \ud558\ub2e8\uc758 \uc2f8\uc774\ud2b8\ub97c \ucc38\uc870\ud558\uc5ec \ubd84\uc11d \ud558\uc600\uc2b5\ub2c8\ub2e4.\n(The above analysis was conducted by referring to the site at the bottom.)\nhttps:\/\/www.kaggle.com\/seneralkan\/advanced-feature-engineering","d2ba6830":"LightGBM\ub9cc \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8 0.78\uc758 Best Score\ub97c \ub2ec\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4. \n\ud558\uc9c0\ub9cc, \uc774\uac83\uc774 \uacfc\uc801\ud569\uc774\ub418\uc5b4 \ud574\ub2f9 \uac12\uc774 \ub098\uc628 \uac83\uc778\uc9c0 \uc54c\uc218 \uc5c6\uace0, \ub370\uc774\ud130 \uc790\uccb4\uac00 \ub108\ubb34 \uc801\uae30 \ub54c\ubb38\uc5d0 \ub2e4\ub978 \ucc38\uac00\uc790\ub4e4\uc758 \ucf54\ub4dc\ub97c \ucc38\uace0\ud558\uc5ec \ubaa8\ub378\ub9c1 \ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n(I have already achieved a best score of 0.78 using only LightGBM.\nHowever, I don't know if this was overfitting and the value came out, and because the data itself was too small, I modeled it with reference to other participants' codes.)","ba1b9d68":"\ud558\ub2e8\uc740 https:\/\/www.kaggle.com\/lovroselic\/titanic-ls#Predictions \uc744 \ucc38\uc870\ud558\uc5ec \ubaa8\ub378\ub9c1 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n(The bottom was modeled by referring to https:\/\/www.kaggle.com\/lovroselic\/titanic-ls#Predictions.)","cb1ac1b2":"# 3.\ubaa8\ub378\ub9c1(Modeling)","690aaabd":"\ucd08\uae30 \ubaa8\ub378\uc740 LightGBM\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\ub9c1 \uc9c4\ud589\ud558\uc600\uace0, \uc774\uac83\uc5d0 \ub300\ud55c \uc124\uba85\uc740 \ud558\ub2e8\uc758 \uc2f8\uc774\ud2b8\uc5d0 \uc790\uc138\ud788 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \n(The initial model was modeled using LightGBM, and the description of this is detailed on the site below.)\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\n\n\ud30c\ub77c\uba54\ud130 \uac80\uc0c9\ubc29\ubc95\uc740 \u2460 \uc0c1\ub2e8\uc758 \uc2f8\uc774\ud2b8 \ub4e4\uc5b4\uac00\uc11c, \u2461 \uc67c\ucabd \uba54\ub274\uc911 \"Parameters\" \uc120\ud0dd , \u2462 Core Parameters\ub97c \ubcf4\uc2dc\uba74 \nHyper Parameter Tuning\uc5d0 \ub300\ud55c \ud301\uc744 \uc5bb\uc73c\uc2e4\uc218 \uc788\uc2b5\ub2c8\ub2e4. \n(For parameter search methods, go to the site, select \"Parameters\" from the left menu, and see Core Parameters to get tips on Hyper Parameter Tuning.)","5f2cdc54":"\uae30\ubcf8\uc801\uc73c\ub85c boxplot\uc744 \uadf8\ub824 Outlier\uac00 \uc5bc\ub9c8\ub098 \uc788\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uace0, \uc774\ubd80\ubd84\uc5d0 \ub300\ud558\uc5ec \uc804\ucc98\ub9ac \uc2dc\ud589\ud569\ub2c8\ub2e4.\n(Basically, draw a boxplot to check how many outliers there are, and preprocess this part.)","056a77cb":"![image.png](attachment:02c465c2-0ee4-47ea-86d3-ab1f7de5abff.png)","6fe9ab6b":"# 1. \uac1c\uc694 (Introduction)\n\n\ubd84\ub958 \ubaa8\ub378\uc758 \uacbd\uc6b0 \uc5b4\ub5a0\ud55c Data\uac00 \ub4e4\uc5b4\uc640\ub3c4 \uc88b\uc740 \uacb0\uacfc\ub97c \ub0bc\uc218 \uc788\uc73c\uba74\uc11c Simple\ud55c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc704\ud558\uc5ec \n\ub370\uc774\ud130\uac00 \uc801\uc5b4 \uacb0\uacfc\ub97c \ube68\ub9ac \ubcfc\uc218 \uc788\ub294 \ud0c0\uc774\ud0c0\ub2c9 Data\ub97c \ud65c\uc6a9\ud558\uc5ec \ubaa8\ub378\ub9c1 \uc791\uc5c5\uc744 \uc2e4\ud589\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n(In the case of classification models, \nin order to create a simple model while producing good results no matter what data comes in, \nI will use Titanic Data, which has little data, so we can quickly see the results)"}}