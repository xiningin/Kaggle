{"cell_type":{"1fcaf4b3":"code","6830de91":"code","1614b620":"code","27831b99":"code","5d4983f6":"code","9e3d4d0d":"code","cee9149b":"code","59f9bb3e":"code","beb6a3bb":"code","f2c6c97d":"code","a5536438":"code","9f66d5e8":"code","396f2bb8":"code","a5a7826d":"code","483eb02a":"code","45dd4345":"code","179b9ded":"code","7682fd51":"code","f014149d":"code","70e67b1c":"code","5c220830":"code","f0d0ad59":"markdown","d8ec1c13":"markdown","8714791a":"markdown","a123f8db":"markdown","0621c6c0":"markdown","2bfa37f4":"markdown","7d20fe2a":"markdown","15f110c6":"markdown","055523f1":"markdown","d4f977b0":"markdown","cb4c3747":"markdown","1d2d6905":"markdown","7e40b4b0":"markdown"},"source":{"1fcaf4b3":"##importing required libraries\nimport math\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nget_ipython().magic('matplotlib inline')\nnp.random.seed(1)\nimport cv2\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n##printing version of tensorflow\nprint(tf.__version__)","6830de91":"train_path=\"..\/input\/mnist-in-csv\/mnist_train.csv\"\ntest_path=\"..\/input\/mnist-in-csv\/mnist_test.csv\"\ntrain = pd.read_csv(train_path)\nprint(train.shape)\nprint(train.head())\ntest= pd.read_csv(test_path)\nprint(test.shape)\nprint(test.head())\n","1614b620":"# put labels into y_train variable\ny_orig1 = train[\"label\"]\ny_orig1_test = test[\"label\"]\n# Drop 'label' column\nX_orig2 = train.drop(labels = [\"label\"],axis = 1,inplace=False)\nX_orig2_test = test.drop(labels = [\"label\"],axis = 1,inplace=False)\n","27831b99":"# visualize number of digits classes\nimport seaborn as sns\nplt.figure(figsize=(15,7))\ng = sns.countplot(y_orig1, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\ny_orig1.value_counts()\n\n# visualize number of digits classes\nimport seaborn as sns\nplt.figure(figsize=(15,7))\ng = sns.countplot(y_orig1_test, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\ny_orig1_test.value_counts()\n","5d4983f6":"# Example of a picture\nindex = 1000\nimg = X_orig2.iloc[index].values\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(y_orig1[index])\nplt.axis(\"off\")\nplt.show()\n\n# Example of a picture\nindex = 1000\nimg = X_orig2_test.iloc[index].values\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(y_orig1_test[index])\nplt.axis(\"off\")\nplt.show()\n","9e3d4d0d":"# Normalize the data\nX_orig1 = X_orig2 \/ 255.0\nX_orig1_test = X_orig2_test \/ 255.0\nprint(\"x_orig1 shape: \",X_orig1.shape)\nprint(\"x_orig1_test shape: \",X_orig1_test.shape)\n\n# Reshape\nX_orig = X_orig1.values.reshape(-1,28,28,1)\nX_orig_test = X_orig1_test.values.reshape(-1,28,28,1)\nprint(\"x_orig shape: \",X_orig.shape)\nprint(\"x_orig_test shape: \",X_orig_test.shape)\n\n# Label Encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nprint(\"old y_orig1 shape: \", y_orig1.shape)\ny_orig = to_categorical(y_orig1, num_classes = 10)\nprint(\"y_orig shape: \",y_orig.shape)\n\nprint(\"old y_orig1_test shape: \", y_orig1_test.shape)\ny_orig_test = to_categorical(y_orig1_test, num_classes = 10)\nprint(\"y_orig_test shape: \",y_orig_test.shape)\n","cee9149b":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_orig, y_orig, test_size = 0.2, random_state=42)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_val shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_val shape\",Y_val.shape)\n","59f9bb3e":"# Some examples\nindex=1\nprint(Y_train[index])\nplt.imshow(X_train[index][:,:,0],cmap='gray')\nplt.show()\n","beb6a3bb":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))","f2c6c97d":"# Define the optimizer\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)","a5536438":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","9f66d5e8":"epochs = 10  # for better result increase the epochs\nbatch_size = 200","396f2bb8":"%%time\nhistory = model.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=epochs\n                    ,shuffle=True,steps_per_epoch=X_train.shape[0] \/\/ batch_size)","a5a7826d":"# Plot the loss and accuracy curves for training and validation \nplt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\nplt.title(\"Val Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","483eb02a":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","45dd4345":"### Printing classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9']\nprint(classification_report(Y_true, Y_pred_classes, target_names=target_names))","179b9ded":"##Accuracy Score\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy on val sample is: %f\" %(accuracy_score(Y_true, Y_pred_classes)))","7682fd51":"Y_pred_test = model.predict(X_orig_test)\nY_pred_classes_test = np.argmax(Y_pred_test,axis = 1)\n\nimage = X_orig_test[0].reshape( 28, 28)\nplt.imshow(image)\nprint(Y_pred_classes_test[0])\nY_true_test = np.argmax(y_orig_test,axis = 1)\nprint(y_orig_test[0])\n","f014149d":"i = 120\nimage = X_orig_test[i].reshape( 28, 28)\nplt.imshow(image)\nprint(Y_pred_classes_test[i])\nprint(y_orig_test[i])","70e67b1c":"##Accuracy Score\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy on test sample is: %f\" %(accuracy_score(Y_true_test, Y_pred_classes_test)))","5c220830":"### Printing classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9']\nprint(classification_report(Y_true_test, Y_pred_classes_test, target_names=target_names))","f0d0ad59":"## Model Evaluation\n- val Loss visualization\n- Confusion matrix","d8ec1c13":"## Read data and converting pandas dataframe into images array\nSplitting labeled data into train and val","8714791a":"## Define Optimizer\n- Adam Optimizer","a123f8db":"### Convoluation Operation\n- We have some image and feature detector(3*3)\n- Feature detector does not need to be 3 by 3 matrix. It can be 5 by 5 or 7 by 7.\n- Feature detector = kernel = filter\n- Feauture detector detects features like edges or convex shapes. Example, if out input is dog, feature detector can detect features like ear or tail of the dog.\n- feature map = conv(input image, feature detector). Element wise multiplication of matrices.\n- feature map = convolved feature\n- Stride = navigating in input image.\n- We reduce the size of image by setting the value of stride more than 1. This is important because code runs faster but we loose information.\n- We create multiple feature maps because we use multiple feature detectors(filters).\n- After having convolution layer we use ReLU to break up linearity.\n- **tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W$, this function convolves $W$'s filters on X. The third parameter ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). Normally, you'll choose a stride of 1 for the number of examples (the first value) and for the channels (the fourth value), which is why we wrote the value as `[1,s,s,1]`. You can read the full documentation on [conv2d](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D).\n\n![picture](https:\/\/miro.medium.com\/max\/1462\/1*ROh_38pysewuh6fVPQpxFQ.png)\n\n\n### Same Padding\n- As we keep applying conv layers, the size of the volume will decrease faster than we would like. In the early layers of our network, we want to preserve as much information about the original input volume so that we can extract those low level features.\n- input size and output size are same.\n\n![picture](https:\/\/miro.medium.com\/max\/666\/1*noYcUAa_P8nRilg3Lt_nuA.png)\n\n\n### Max Pooling\n- It makes down-sampling or sub-sampling (Reduces the number of parameters)\n- It makes the detection of features invariant to scale or orientation changes.\n- It reduce the amount of parameters and computation in the network, and hence to also control overfitting.\n- **tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=None,padding='valid', data_format=None)** given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, we usually operate on a single example at a time and a single channel at a time.  So the first and fourth value in `[1,f,f,1]` are both 1.  You can read the full documentation on [max_pool](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool3D).\n\n![picture](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20190721025744\/Screenshot-2019-07-21-at-2.57.13-AM.png)\n\n\n### Flattening\n- For example, given a tensor with dimensions [100,2,3,4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [flatten](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Flatten).\n\n![picture](https:\/\/sds-platform-private.s3-us-east-2.amazonaws.com\/uploads\/73_blog_image_2.png)\n\n\n### Fully Connected(FC)\/Dense\n- Neurons in a fully connected layer have connections to all activations in the previous layer\n- Artificial Neural Network\n- **tf.keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None, bias_regularizer=None,activity_regularizer=None,kernel_constraint=None, bias_constraint=None:** \n- given the flattened input,F, it returns the output computed using a fully connected layer. You can read the full documentation on [fully_connected](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense).\n\n![picture](https:\/\/static.packt-cdn.com\/products\/9781788996242\/graphics\/451a22fa-7568-4602-a522-e2dd3826e53e.png)\n\n\n### Using Keras\n- Implement the `forward_propagation` function below to build the following model: `CONV2D -> MAXPOOL -> CONV2D -> MAXPOOL -> FLATTEN -> FULLYCONNECTED -> FULLYCONNECTED`. You should use the functions above.","0621c6c0":"**Introduction to CNN**\n- ConvNet\n- Max Pooling\n- Fully Connected Layer\n- Softmax\n\n### > **Do upvote if you find this notebook helpful.****","2bfa37f4":"## Test the model\nWe will now test the model on some random test data set. We will first construct the image of the number from the test data and then run our classifier to verify if it could identify it correctly.\n","7d20fe2a":"## Fit the model\n\n","15f110c6":"As we can see in the above examples we are getting the right prediction on test data.\nWe are able to get almost **99% accuracy** on both **val** and **test** dataset.","055523f1":"## Compile Model\n\n- optimizer - Adam\n- loss - categorical cross entropy\n- metrics - accuracy\n\n![picture](https:\/\/gombru.github.io\/assets\/cross_entropy_loss\/softmax_CE_pipeline.png)","d4f977b0":"## Train Test split\n- We split the data into train and test sets.\n- test size is 20%.\n- train size is 80%.\n","cb4c3747":"## Normaliation, Reshape and Label Encoding\n- Normalization\n  - We perform a grayscale normalization to reduce the effect of illumination's differences.\n  - If we perform normalization, CNN works faster.\n- Reshape\n  - Train and test images (28 x 28)\n  - We reshape all data to 28x28x1 3D matrices.\n  - Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel.\n- Label Encoding\n  - Encode labels to one hot vectors\n    - 2 => [0,0,1,0,0,0,0,0,0,0]\n    - 4 => [0,0,0,0,1,0,0,0,0,0]","1d2d6905":"## Model -  Convolutional Neural Network","7e40b4b0":"## Epochs and Batch Size\n- Say you have a dataset of 10 examples (or samples). You have a **batch size** of 2, and you've specified you want the algorithm to run for 3 **epochs**. Therefore, in each epoch, you have 5 **batches** (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations **per epoch**."}}