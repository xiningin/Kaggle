{"cell_type":{"ef89b58d":"code","b32af0c7":"code","e8af5ed3":"code","1c61e491":"code","2ce8170e":"code","089b397a":"code","0f02bcf6":"code","678ca98f":"code","02f528dc":"code","8c60efe5":"code","90edadc1":"code","20016621":"code","ebbaae3d":"code","82aef61a":"code","9831a149":"code","aac729c3":"code","5de0838f":"code","4cb82680":"code","4fe34558":"code","7ffa4e40":"code","87bc3db8":"code","e92e4c62":"code","ec04ade6":"markdown","780aeda3":"markdown","0b53b9db":"markdown","86885e03":"markdown","4fae6897":"markdown","25d62361":"markdown","d7d3c8dc":"markdown","156bc7cc":"markdown","af86c16a":"markdown","14805f14":"markdown","4eae6ac9":"markdown","1144e888":"markdown","05b1c662":"markdown","512d091f":"markdown","a55324f3":"markdown","4991e725":"markdown","49b5b36b":"markdown","264de063":"markdown","aeec658a":"markdown","a8591bd8":"markdown","1aece807":"markdown","a29b71fb":"markdown","dc6bbcd0":"markdown","fdd03214":"markdown","09518189":"markdown"},"source":{"ef89b58d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\nimport string\nimport matplotlib.pyplot as plt;\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b32af0c7":"df = pd.read_csv('..\/input\/Tweets.csv')\ndf.describe()","e8af5ed3":"stop_words = stopwords.words('english') ","1c61e491":"def clean_text(txt):\n    \n    \"\"\"\n    removing all hashtags , punctuations, stop_words  and links, also stemming words \n    \"\"\"\n    txt = txt.lower()\n    txt = re.sub(r\"(@\\S+)\", \"\", txt)  # remove hashtags\n    txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations \n    txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n    txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stop_words ]) # stem & remove stop words\n    txt = ''.join([i for i in txt if not i.isdigit()]).strip() # remove digits ()\n    return txt","2ce8170e":"print('Original Text : ',df['text'][3])  \nprint('Processed Text : ',clean_text(df['text'][3]))","089b397a":"    df['sent_encoded'] = df['airline_sentiment'].apply(lambda x:0 if x =='negative' else 1)\n    df['cleaned_text'] = df['text'].apply(clean_text)  ","0f02bcf6":"def train_test_data():   \n    y = df['sent_encoded']   # define target and feature column\n    X = df['cleaned_text']\n     \n    text_train, text_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)    # do the split\n    vect = CountVectorizer(min_df=5, ngram_range=(1, 4)) # create Count vectorizer.\n    X_train = vect.fit(text_train).transform(text_train) # transform text_train  into a vector \n    X_test = vect.transform(text_test) \n    feature_names = vect.get_feature_names() # to return all words used in vectorizer\n  \n    return X_train, X_test, y_train, y_test, feature_names","678ca98f":" X_train, X_test, y_train, y_test, feature_names = train_test_data()","02f528dc":"print(X_train.shape)\nprint(X_test.shape)","8c60efe5":"lgstc = LogisticRegressionCV(class_weight={1:0.515,0:0.485})\nlgstc.fit(X_train, y_train)","90edadc1":"def print_model_performance(model,X_train,X_test,y_train,y_test):\n    training_sample = model.predict(X_train)\n    testing_sample = model.predict(X_test)\n    print('training ')\n    #print(classification_report(training_sample, y_train))  #uncomment if you want to see full report \n    print('train accuracy ',accuracy_score(training_sample, y_train))\n    print('train precision_score ',precision_score(training_sample, y_train)) \n    print('train recall score',recall_score(training_sample, y_train)) \n    \n    print('\\n testing  ')\n    print(classification_report(testing_sample, y_test))   #uncomment if you want to see full report \n    print('test average accuracy ',accuracy_score(testing_sample, y_test))\n    print('test average precision_score ',precision_score(testing_sample, y_test)) \n    print('test average recall score',recall_score(testing_sample, y_test)) \n    print('test AUC ',roc_auc_score(testing_sample, y_test))\n    \n    print(confusion_matrix(testing_sample, y_test))","20016621":" print_model_performance(lgstc, X_train, X_test, y_train, y_test)","ebbaae3d":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split","82aef61a":"max_fatures = 2000 # maximum number of words to use\nembed_dim = 120 # embidding dimention\nlstm_out = 190 # lstm size\nbatch_size = 32 # batch size\nvalidation_size = 1500 # validation set size","9831a149":"def get_X_y(feature, target):\n    data = df.copy() # create a copy so we dont mess up our old dataframe\n    \n    data = data[[feature,target]] # cut down all dataframe to only features and target variables \n    \n    data = data.dropna(subset=[feature]) # make sure there is no (NA) values as it will not help predictions \n    \n    data[feature] = data[feature].apply(lambda x: x.lower()) # convert text to lower case\n    data[feature] = data[feature].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove all digits\n    \n    tokenizer = Tokenizer(num_words=max_fatures, split=' ') # create our tokenizer here we used NLTK as it's a preferable package for developers using deep learning \n    tokenizer.fit_on_texts(data[feature].values) \n    \n    X = tokenizer.texts_to_sequences(data[feature].values) # here is the main trick! we convert:  'hi im x' to something like [2, 12, 53] where 2 , 12 , 34 ()\n    X = pad_sequences(X)    \n    Y = pd.get_dummies(data[target]).values\n    return X, Y","aac729c3":" X, Y = get_X_y('text', 'sent_encoded')","5de0838f":"print(df['text'][3]) # third tweet\nprint(X[[3]]) # third tweet to sequence","4cb82680":"def get_trained_model(X, Y):\n    model = Sequential()\n    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n    model.add(SpatialDropout1D(rate=0.9))\n    model.add(LSTM(lstm_out))\n    model.add(Dense(2,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n    \n    model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)\n\n    X_validate = X_test[-validation_size:]\n    Y_validate = Y_test[-validation_size:]\n    X_test = X_test[:-validation_size]\n    Y_test = Y_test[:-validation_size]\n    score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n    print(\"score: %.2f\" % (score))\n    print(\"acc: %.2f\" % (acc))\n    return model, X_validate, Y_validate, X_test, Y_test","4fe34558":"model, X_validate, Y_validate, X_test, Y_test = get_trained_model(X,Y)","7ffa4e40":"def validate_model(model, X_validate, Y_validate, X_test, Y_test):\n    pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n    all_predictions = []\n    for x in range(len(X_validate)):\n    \n        result = model.predict(X_validate[x].reshape(1,X_test.shape[1]), batch_size=1, verbose=2)[0]\n        all_predictions.append(result)\n        if np.argmax(result) == np.argmax(Y_validate[x]):\n            if np.argmax(Y_validate[x]) == 0:\n                neg_correct += 1\n            else:\n                pos_correct += 1\n    \n        if np.argmax(Y_validate[x]) == 0:\n            neg_cnt += 1\n        else:\n            pos_cnt += 1\n\n    print(\"pos_acc\", pos_correct\/pos_cnt*100, \"%\")\n    print(\"neg_acc\", neg_correct\/neg_cnt*100, \"%\")\n    \n    all_currects = neg_correct + pos_correct\n    all_samples = neg_cnt + pos_cnt\n    \n   \n    y_predict = np.asarray(all_predictions)\n    y_actual = np.asarray(Y_validate)\n    \n    y_actual = np.argmax(y_actual, axis=1)\n    y_predict = np.argmax(y_predict, axis=1)\n    \n  \n    cm = confusion_matrix(y_predict, y_actual)\t\n    print('AUC : ',roc_auc_score(y_predict, y_actual))\n    #print(\"Average Accuracy : \", all_currects\/all_samples*100, \"%\")\n    return cm.ravel()","87bc3db8":"tn, fp, fn, tp = validate_model(model, X_validate, Y_validate, X_test, Y_test)","e92e4c62":"print('average accuracy : ', (tp + tn) \/ (tn+fp+fn+tp))\nprint('average precision : ', (tp) \/ (fp+tp))\nprint('average Recall : ', (tp) \/ (tp+fn))","ec04ade6":"Tryout our newly defined function ","780aeda3":"to test the model performance , we create a function to print out all results, to test overfitting we must compare test performance to train performance.\n","0b53b9db":"lets try it out : ","86885e03":"### **3- get X and y**","4fae6897":"lets try to \"complicate things\" a bit. \nnow we will create a deep learning model ( RNN-lstm )  to increase our 82% accuracy.","25d62361":"from the confusion matrix bellow , we see that model is better at detecting negative tweets, reason is because we have more negative samples than positive. \n","d7d3c8dc":"### **3- Test model performance **","156bc7cc":"### **2- Train The Model**","af86c16a":" here is the main trick! we convert:  'hi im x' to something like [2, 12, 53] by using text_to_sequences functions  ( where 2, 12, 53) are indexes of words hi, m , x in **order** . \n here we sense the power of LSTM , as order of words can mean different thing \n ","14805f14":"Second model is better, and there is a big room for improvement. maybe try adding more hidden layers. or even do more text preprocessing. ","4eae6ac9":"I came out with these values by intuition :) also trial and error. ","1144e888":"Create Train text splits.\nfor a model to run we need to:\n\n1- tokenize Text\n\n2- encode every word as a feature\n\n3- represent word accurencies in a text as a count.( done by **CountVectorizer** )","05b1c662":"Now apply our function to the dataset, \nalso we need to encode target variable (airline_sentiment) to be 0 whenever a tweet is negative and 1 otherwise.","512d091f":"We have a model with 85% accuracy , which seems fine ( at least we improved a bit !). and all thats left is to validate model to see it's actual performance: \n","a55324f3":"# first: Logistic Regression Model","4991e725":"### **2- define the global variables: **","49b5b36b":"# Deep Learning (LSTM ) : ","264de063":"lets try the newly created function ","aeec658a":"### 4- create the model : \nwe will create an LSTM model with a sigmoid activation. you might want to use softmax instead of sigmoid . but from my personal experience sigmoid performs better in binary situations. \nbut you are free try out on your own and see what fits you more :)","a8591bd8":"although this function might look scary . but actually not so much is happeing , all im doing is checking negative and positive accuracy. and return confusion matrix to compare it with the model above. \n\nnote: no need to praise me for this function it's a straight up copy and past from keras tutorials. ( with some of my touches) ","1aece807":"as you can see I used dropout rate of 90% , how did i reach that number?? answer is trial and error :).","a29b71fb":"### **1- Text Clean Up**","dc6bbcd0":"**In this kernel we will try to predict sentiment with Logistic Regression and again with LSTM to compare results.\n**I hope you enjoy it:) ","fdd03214":"### **5- model validation **","09518189":"our model of selection is Logistic Regression reason is \"simplicity\". as it's always a good idea to start with the simplest model possible then see if you need to complicate it. \n<< Start small , think big >>"}}