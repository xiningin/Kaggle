{"cell_type":{"38f8a1f1":"code","a479767b":"code","1c467678":"code","6439fb7d":"code","04a11766":"code","91b398ba":"code","5a49f504":"code","48a3c434":"code","2f4005f8":"code","9451b591":"code","2bb1bcd3":"code","d85be89b":"code","c8cb1ab3":"code","530cf7f8":"code","d0b68650":"code","3a2a37e8":"code","4436c617":"code","4a5c6cd0":"code","e179acc1":"code","eccfa33e":"code","fe2e8f85":"code","fa8e356d":"code","2626cc43":"code","4ef4c3ea":"code","1411e31c":"code","b6f63760":"markdown","503c31a2":"markdown","37f37a43":"markdown","d34ab9fd":"markdown","d9a2da60":"markdown","e280701d":"markdown","c93b5db1":"markdown","9e39b8aa":"markdown","7b7f5275":"markdown","5063fb38":"markdown","123f3f3a":"markdown","aa6a59de":"markdown","b3ea4901":"markdown","f750b918":"markdown","ef2641de":"markdown","b7ccdb00":"markdown","4579ca43":"markdown","26c14d59":"markdown","d3bf6335":"markdown","a5ce5c3d":"markdown","c428c120":"markdown","b9226b33":"markdown","3fcd24ed":"markdown","a690fd99":"markdown","b800d5fb":"markdown","c3ffa64c":"markdown","a2526699":"markdown","07af5431":"markdown","864b9e9a":"markdown","8a04897a":"markdown","8ec27c9b":"markdown","08d957e4":"markdown","c11d0f74":"markdown","6b9cfbea":"markdown","0e7c43cb":"markdown","f99da82d":"markdown","f63efbd0":"markdown","e6edcd29":"markdown","650208f6":"markdown","fcd31d4b":"markdown","afc1fc9e":"markdown","b4cc9efc":"markdown","d24c5075":"markdown","3c36c8c3":"markdown","045a73ec":"markdown","5a8ca60e":"markdown","9d968d46":"markdown","3a3be610":"markdown"},"source":{"38f8a1f1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nplt.rcParams['figure.figsize'] = 10,5\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score,classification_report\n# for preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# base model\nfrom sklearn.linear_model import LogisticRegression\n\n# hyperparameter tuning \nfrom sklearn.model_selection import GridSearchCV","a479767b":"from warnings import filterwarnings\nfilterwarnings('ignore')\n%matplotlib inline","1c467678":"# train_X = pd.read_csv('.\/data\/train_values.csv')\n# train_y = pd.read_csv('.\/data\/train_labels.csv')\n# test_X = pd.read_csv('.\/data\/test_values.csv')\npath = '..\/input\/warm-up-machine-learning-with-a-heart\/'\ntrain_X = pd.read_csv(path+'train_values.csv',index_col='patient_id')\ntrain_y = pd.read_csv(path+'train_labels.csv')\ntest_X = pd.read_csv(path+'test_values.csv',index_col='patient_id')\n\nn=20\nprint(\"Train\".center(n,'='))\nprint(train_X.shape)\ntrain_X.info()\n\nprint()\n\nprint(\"Test\".center(n,'='))\nprint(test_X.shape)\ntest_X.info()","6439fb7d":"print(f'Nulls in Train: {train_X.isna().sum().sum()} & Test:{test_X.isna().sum().sum()}')","04a11766":"target = 'heart_disease_present'\nX = train_X.iloc[:,1:]\ny = train_y[target]\n\nX_t = test_X.iloc[:,1:]\n# test_ids = test_X['patient_id']\n\nprint(\"Train X & y:\",X.shape, y.shape)\nprint(\"Test X:\",X_t.shape)","91b398ba":"data = pd.merge(train_X,train_y,left_on='patient_id',right_on='patient_id')\ndata.shape","5a49f504":"data.set_index('patient_id',inplace=True)\ndata.head()","48a3c434":"print(y.value_counts())\nax = y.value_counts().plot.barh(grid=True)","2f4005f8":"print('No. of features:',len(X.columns),'\\n',X.nunique().sort_values())","9451b591":"num_cols = \"oldpeak_eq_st_depression resting_blood_pressure age max_heart_rate_achieved serum_cholesterol_mg_per_dl\".split()\nprint(len(num_cols),num_cols,sep='\\n')","2bb1bcd3":"for i,col in enumerate(num_cols,1):\n    sns.FacetGrid(data,hue=target,height=4,aspect=3,palette='cool')\\\n    .map(sns.distplot,col,rug=True,bins=15)\\\n    .add_legend()\nplt.show()","d85be89b":"for i,col in enumerate(num_cols,1):\n    print(col,'skewness :',round(X[col].skew(),2))","c8cb1ab3":"# Local figsize\nplt.figure(figsize=(18,10))\nax = sns.boxplot(data=X[num_cols],orient='h')","530cf7f8":"cat_cols = list( set(X.columns) - set(num_cols) )\nprint(len(cat_cols),cat_cols,sep='\\n')","d0b68650":"plt.figure(figsize=(16,16))\nfor i,col in enumerate(cat_cols,1):\n    plt.subplot(4,2,i)\n    X[col].value_counts(normalize=True).plot.bar(alpha=0.4,cmap='cool',rot=45)\n    plt.legend()","3a2a37e8":"for col in num_cols:\n    mean_per_cat = pd.pivot_table(data,values=col,index=target,aggfunc=\"mean\")\n    ax = mean_per_cat.plot.barh(alpha=0.4)\nplt.show()","4436c617":"n_cat_cols = len(cat_cols)\nfig,ax = plt.subplots(n_cat_cols,1,figsize=(10,6*n_cat_cols))\nfor i,col in enumerate(cat_cols):\n    # Create a cross table for stacked graph\n    ct = pd.crosstab(data[col],data[target],normalize=\"index\")\n    ct.plot.barh(stacked=True,ax=ax[i],alpha=0.85)\n    \nplt.show()","4a5c6cd0":"sns.pairplot(data,hue=target,vars=num_cols,diag_kind='hist')\nplt.show()","e179acc1":"# Correlation of numerical features and target\nplt.figure(figsize=(10,7))\nwith sns.axes_style(\"white\"):\n    cor = data.corr()\n    sns.heatmap(cor,annot=True,cbar=False,fmt='.2f',vmax=.7,cmap=\"YlGnBu\",mask=np.triu(cor,1))\nplt.show()","eccfa33e":"plt.subplot(211)\nax = sns.boxplot(data=X,x='oldpeak_eq_st_depression')\nplt.subplot(212)\nax1 = sns.distplot(X['oldpeak_eq_st_depression'],rug=True)","fe2e8f85":"plt.subplot(211)\nax = sns.boxplot(data=X,x='resting_blood_pressure')\nplt.subplot(212)\nax1 = sns.distplot(X['resting_blood_pressure'],rug=True)","fa8e356d":"plt.subplot(211)\nax = sns.boxplot(data=X,x='serum_cholesterol_mg_per_dl')\nplt.subplot(212)\nax1 = sns.distplot(X['serum_cholesterol_mg_per_dl'],rug=True)","2626cc43":"drop = ['thal']\nX.drop(columns=drop,inplace=True)\nX_t.drop(columns=drop,inplace=True)","4ef4c3ea":"# for combining the preprocess with model training\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline(steps=[('scale', StandardScaler()), \n                       ('logistic', LogisticRegression())])\npipe","1411e31c":"param_grid = {'logistic__C': [0.0001, 0.001, 0.01, 1, 10], \n              'logistic__penalty': ['l1', 'l2']}\ngs = GridSearchCV(estimator=pipe, \n                  param_grid=param_grid, \n                  cv=3)\ngs.fit(X, y)\ngs.best_params_","b6f63760":"## Looking at our features","503c31a2":"## Dependencies ","37f37a43":"Even though most of the columns have datatype int\/float, the ones with lesser number of unique values can be analyzed similar to categorical features.","d34ab9fd":"## Heart disease present","d9a2da60":"There are no null values in the dataset so we can skip the `null value treatment`.\n\nIf there were null values then we would have either removed those values or imputed with another values(depending upon the number of null values in the column).","e280701d":"Skewness can be seen in `oldpeak_eq_st_depression` and `serum_cholesterol_mg_per_dl`","c93b5db1":"<center><img src=\"https:\/\/guardian.ng\/wp-content\/uploads\/2016\/08\/Heart-diseases.jpg\" width=\"600px\"><\/center>","9e39b8aa":"Considering the mean value of numerical columns with respect to target","7b7f5275":"## Bivariate analysis","5063fb38":"For categorical variables, frequency table can be used to understand distribution of each category. \n\nWe will measure Count and Count% against each category. \n\n>Bar chart, Count plot can be used for visualization.","123f3f3a":"### Categorical & Categorical","aa6a59de":"### Numerical columns","b3ea4901":"The dependent variable is well balanced so we won't take any steps here to equalize the classes.\n\nHowever, the ideal case would have been same number of dara points for each class","f750b918":"# Machine learning with a Heart","ef2641de":"**resting_blood_pressure**\n\nThe outlier in the data could be because of abnormal heart conditon so we will not remove this one.","b7ccdb00":"### Continuous & Categorical","4579ca43":"Load the data and try to understand what each column signifies.","26c14d59":"Key takeaways from the above chart are :\n\n- Columns which show positive relationship with heart disease\n    - `sex`: Males are more likely.\n    - `exercise_induced_angina`: If they are having angina\n    - `slope_of_peak_exercise_st_segment`: If the quality of blood flow is of type 2 or 3\n    - `thal`: people with reversible defect and fixed defect\n    - `resting_ekg_results`: 100% of people of type 1 show issues.\n    - `chest_pain_type`: Type 4 is most concering\n    - `num_major_vessels` Direct proportionality with the target (as the number increase chance also increase)\n- *No or not much significance column*: fasting_blood_sugar_gt_120_mg_per_dl\n\n","d3bf6335":"**Outlier treatment** - Replace, Remove, Reduce\n\n1. Replace with NaN and fill with another value.\n2. Remove values\n3. Reduce the scale","a5ce5c3d":"## Data understanding","c428c120":"**Method 2**: Drop them\n```python\nfor col_name in num_cols:\n    q1 = X[col_name].quantile(q=0.25)\n    q3 = X[col_name].quantile(q=0.75)\n\n    lower_limit = q1 - (q3-q1)*1.5\n    upper_limit = q3 + (q3-q1)*1.5\n    mask = ( X[col_name] >= lower_limit) & (X[col_name] <= upper_limit)\n    X = X.loc[mask]\n    y = y.loc[mask]\n    \nprint(X.shape)\nprint(X[num_cols].describe())\n```","b9226b33":"**serum_cholesterol_mg_per_dl**","3fcd24ed":"From the above diagram, we can see that people having heart disease is\n- Variably more for columns `oldpeak_eq_st_depression` and `age`.\n- Less for people with smaller `max_heart_rate_achieved`.\n- Indifferent for other columns.","a690fd99":"## Evaluation metric","b800d5fb":"Search for the best hyperparameters using GridSearchCV","c3ffa64c":"In the United States, the Centers for Disease Control and Prevention is a good resource for information about heart disease. According to their website:\n\n- About 610,000 people die of heart disease in the United States every year\u2013that\u2019s **1 in every 4 deaths**.\n- Heart disease is the leading cause of death for both men and women. More than half of the deaths due to heart disease in 2009 were in men.\n- Coronary heart disease (CHD) is the most common type of heart disease, killing over 370,000 people annually.\n- Every year about 735,000 Americans have a heart attack. Of these, 525,000 are a first heart attack and 210,000 happen in people who have already had a heart attack.\n- Heart disease is the leading cause of death for people of most ethnicities in the United States, including African Americans, Hispanics, and whites. For American Indians or Alaska Natives and Asians or Pacific Islanders, heart disease is second only to cancer.\nFor more information, you can look at the website of the Centers for Disease Control and Prevention: [preventing heart disease](https:\/\/www.cdc.gov\/heartdisease\/prevention.htm)","a2526699":">Log Loss","07af5431":"### Categorical columns","864b9e9a":"Note the double underscore, it is to specify that the C and penalty value belongs to Logistic regressions and not StandardScaler.","8a04897a":"Univariate analysis\n- In this stage, we check features one by one to using various statistical measures and\/or visualizations.\n- As part of the analysis, we can check  \n    - Missing values\n    - Outlier values\n    - Normally distribution (skewness and kurtosis)\n    - Scales ","8ec27c9b":"## Objective","08d957e4":"Let us now prepare the dataset for analysis. Train_X and train_y both are having patient_ids for ","c11d0f74":"## Data cleaning\/prep followed by Baseline model","6b9cfbea":"Let's now check for the outliers.","0e7c43cb":"Outliers found in `oldpeak_eq_st_depression`, `resting_blood_pressure` and `serum_cholesterol_mg_per_dl`. `oldpeak_eq_st_depression` and `resting_blood_pressure`.","f99da82d":"Your goal is to predict the binary class `heart_disease_present`, which represents whether or not a patient has heart disease","f63efbd0":"**oldpeak_eq_st_depression**\n\nST depression induced by exercise relative to rest, a measure of abnormality in electrocardiograms - 3 outliers","e6edcd29":"### Pair plot","650208f6":"## About ","fcd31d4b":"## Multivariate analysis","afc1fc9e":"In case of continuous variables, we need to understand the distribution (`central tendency` and `spread`).\n\n> Density plot, histogram, box plot,etc can be used for visualization.","b4cc9efc":"Also, the range of various numerical features are different so depending upon the model we will have to decide whether we need to scale them.","d24c5075":"### Heat map","3c36c8c3":"The above plots do not convey whether any of the feature will be a strong contributor of heart disease. But, of the all, age and max heart rate achieved looks like good features.","045a73ec":"Kernel status: Complete\n\n- [X] Analysis\n- [X] Cleaning\n- [X] Pipeline\n- [X] Model\n- [X] Tuning","5a8ca60e":"**Method 1**: Replace with NaN and fill another value\n\n```python\nfor col_name in X.columns:\n    q1 = X[col_name].quantile(q=0.25)\n    q3 = X[col_name].quantile(q=0.75)\n\n    lower_limit = q1 - (q3-q1)*1.5\n    upper_limit = q3 + (q3-q1)*1.5\n    X[col_name].loc[(X[col_name] < lower_limit) | (X[col_name] > upper_limit)] = np.NaN\n\n    X[col_name].fillna(method=\"ffill\",inplace=True)\n    X[col_name].fillna(method=\"bfill\",inplace=True)\n    \n    \nplt.subplot(211)\nax = sns.boxplot(data=X,x='oldpeak_eq_st_depression')\nplt.subplot(212)\nax1 = sns.distplot(X['oldpeak_eq_st_depression'],rug=True)\n```","9d968d46":"Heatmap shows heart disease having some correlation with - exercise_induced_anigma, num_major_vessels,chest_pain_type.","3a3be610":"**Method 3**: Scaling\n(Transformations are used for bringing down the skewness )\n\n**Normalization** is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks\n\n**Standardization** assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis."}}