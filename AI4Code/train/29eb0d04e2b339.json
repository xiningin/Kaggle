{"cell_type":{"ac07edb2":"code","1e1a9f37":"code","114c9bcd":"code","d912dd38":"code","abdd4c57":"code","4901220a":"code","738367a4":"code","89d5340a":"code","bfc41b36":"code","3f9ef209":"code","087531e6":"code","d4a70a71":"code","6745da1c":"code","9cfee9da":"code","ba1a76fc":"code","00323d61":"code","2d284a1a":"code","d9a3d4bd":"code","f838e744":"code","21dbfce1":"code","4303de53":"code","5bc0b4a1":"code","df0dbb6f":"code","9d29f63f":"code","b5d9e146":"code","81bd4523":"code","728a4c7e":"code","2e57c87f":"code","ee80f380":"code","e9296bf2":"code","34a78cc9":"code","690224d2":"code","72f98f6d":"markdown","3764f30e":"markdown","b41e2785":"markdown","556b4833":"markdown","85316aac":"markdown","b6eb6985":"markdown","ce49cece":"markdown","e0ec0100":"markdown","98dd01eb":"markdown","5da44940":"markdown","5298e28b":"markdown","92ee708f":"markdown","c81a0d9a":"markdown","ba66769f":"markdown","5631ca06":"markdown","550127a8":"markdown","8e9d0bbb":"markdown","e60946e7":"markdown","28b20a71":"markdown","2e93c3ee":"markdown"},"source":{"ac07edb2":"# Data Manipulation libraries\nimport pandas as pd\nimport numpy as np\n# Data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# for normalization\nfrom sklearn.preprocessing import StandardScaler\n# for encoding\nfrom sklearn.preprocessing import LabelEncoder\n# for feature selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\n\n# for model selection and training\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB \nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\n# for model evaluation\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Seed for random state\nSEED = 42","1e1a9f37":"df = pd.read_csv(\"..\/input\/network-intrusion-detection\/Train_data.csv\")","114c9bcd":"# Let's view the data.\nprint(\"Training data has {} rows & {} columns\".format(df.shape[0],df.shape[1]))\ndf.head()","d912dd38":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop(\"class\", axis=1), df[\"class\"], test_size=0.3, random_state=SEED)","abdd4c57":"X_train.info()","4901220a":"# Descriptive analysis of the data\nX_train.describe()","738367a4":"print(X_train['is_host_login'].value_counts())\nprint(X_train['num_outbound_cmds'].value_counts())","89d5340a":"# 'num_outbound_cmds' and 'is_host_login' are redundant column so remove it from data.\nX_train.drop(['num_outbound_cmds','is_host_login'], axis=1, inplace=True)\nX_test.drop(['num_outbound_cmds','is_host_login'], axis=1, inplace=True)","bfc41b36":"# Target Class Distribution\nsns.countplot(y_train)","3f9ef209":"sns.countplot(X_train['protocol_type'], hue=y_train)","087531e6":"plt.figure(figsize=(12,6))\nsns.countplot(X_train['flag'], hue=y_train)","d4a70a71":"plt.figure(figsize=(16,6))\nsns.distplot(X_train['count'], kde=False)","6745da1c":"sns.distplot(X_train.dst_host_srv_count)","9cfee9da":"# Encoding target class to 0 and 1\ny_train = y_train.apply(lambda x: 1 if x==\"anomaly\" else 0)\ny_test = y_test.apply(lambda x: 1 if x==\"anomaly\" else 0)","ba1a76fc":"# Correlation Heatmap\nplt.figure(figsize=(16,10))\nsns.heatmap(X_train.corr().apply(abs))","00323d61":"corr_with_target = X_train.corrwith(y_train).apply(abs)\ncorr_with_target[corr_with_target>0.7]","2d284a1a":"# Custom Label Encoder for handling unknown values\nclass LabelEncoderExt(object):\n    def __init__(self):\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, data):\n        self.label_encoder = self.label_encoder.fit(list(data) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n        return self\n\n    def transform(self, data):\n        new_data = list(data)\n        for unique_item in np.unique(data):\n            if unique_item not in self.label_encoder.classes_:\n                new_data = ['Unknown' if x==unique_item else x for x in new_data]\n        return self.label_encoder.transform(new_data)","d9a3d4bd":"le = LabelEncoderExt()\n\n# encode the selected columns\nfor col in X_train.select_dtypes(\"object\"):\n  le.fit(X_train[col])\n  X_train[col] = le.transform(X_train[col])\n  X_test[col] = le.transform(X_test[col])","f838e744":"scaler = StandardScaler()\n# store the columns\ncols = X_train.columns\n\n# transform the data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train_scaled = pd.DataFrame(X_train_scaled, columns = cols)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns = cols)","21dbfce1":"rfc = RandomForestClassifier();\n\nrfc.fit(X_train_scaled, y_train)\n\nfeat_imp = pd.DataFrame({'feature':X_train.columns,'importance':rfc.feature_importances_})\nfeat_imp = feat_imp.sort_values('importance',ascending=False).set_index('feature')","4303de53":"# plot feat_imp\nplt.figure(figsize = (10, 5))\nplt.title(\"Feature Importance\")\nplt.ylabel(\"Importances\")\nplt.xlabel(\"Features\")\nplt.xticks(rotation=90)\nplt.plot(feat_imp)","5bc0b4a1":"estimator = RandomForestClassifier()\nselector = RFE(estimator)\nselector.fit(X_train_scaled, y_train)\n\nX_train_scaled = selector.transform(X_train_scaled)\nX_test_scaled = selector.transform(X_test_scaled)","df0dbb6f":"# SVC Model\nsvc = SVC(random_state=SEED)\n\n# LogisticRegression Model\nlr = LogisticRegression()\n\n# Gaussian Naive Bayes Model\nbnb = BernoulliNB()","9d29f63f":"# Train XGBoost Classifier\nxgbc = XGBClassifier(eval_metric=\"logloss\", random_state=SEED)\n\n# Train LightGBM Classifier\nlgbmc = LGBMClassifier(random_state=SEED)","b5d9e146":"models = {}\nmodels['SVC']= svc\nmodels['LogisticRegression']= lr\nmodels['Naive Bayes Classifier']= bnb\nmodels['XGBoost Classifier']= xgbc\nmodels['LightGBM Classifier']= lgbmc\nscores = {}\nfor name in models:\n  scores[name]={}\n  for scorer in ['precision','recall']:\n    scores[name][scorer] = cross_val_score(models[name], X_train_scaled, y_train, cv=10, scoring=scorer)","81bd4523":"def line(name):\n  return '*'*(25-len(name)\/\/2)\n\nfor name in models:\n  print(line(name), name, 'Model Validation', line(name))\n\n  for scorer in ['precision','recall']:\n    mean = round(np.mean(scores[name][scorer])*100,2)\n    stdev = round(np.std(scores[name][scorer])*100,2)\n    print (\"Mean {}:\".format(scorer),\"\\n\", mean,\"%\", \"+-\",stdev)\n    print()","728a4c7e":"for name in models:\n    for scorer in ['precision','recall']:\n        scores[name][scorer] = scores[name][scorer].mean()\nscores=pd.DataFrame(scores).swapaxes(\"index\", \"columns\")*100","2e57c87f":"scores.plot(kind = \"bar\",  ylim=[80,100], figsize=(10,6), rot=0)","ee80f380":"models = {}\nmodels['SVC']= svc\nmodels['LogisticRegression']= lr\nmodels['Naive Bayes Classifier']= bnb\nmodels['XGBoost Classifier']= xgbc\nmodels['LightGBM Classifier']= lgbmc\npreds={}\nfor name in models:\n    models[name].fit(X_train_scaled, y_train)\n    preds[name] = models[name].predict(X_test_scaled)\nprint(\"Predictions complete.\")","e9296bf2":"def line(name,sym=\"*\"):\n    return sym*(25-len(name)\/\/2)\ntarget_names=[\"normal\",\"anamoly\"]\nfor name in models:\n    print(line(name), name, 'Model Testing', line(name))\n    print(confusion_matrix(y_test, preds[name]))\n    print(line(name,'-'))\n    print(classification_report(y_test, preds[name], target_names=target_names))","34a78cc9":"f1s = {}\nfor name in models:\n    f1s[name]=f1_score(y_test, preds[name])\nf1s=pd.DataFrame(f1s.values(),index=f1s.keys(),columns=[\"F1-score\"])*100","690224d2":"f1s.plot(kind = \"bar\",  ylim=[80,100], figsize=(10,6), rot=0)","72f98f6d":"#### Exploratory Data Analysis","3764f30e":"### Model Selection","b41e2785":"**We are `encoding` the target class to 0s and 1s, so that it can be used for further analysis and training.**","556b4833":"**Visualization of the Feature Importances**","85316aac":"##### Observations from Data Analysis\n- We identified a slight imbalance in the target column \"class\" of our dataset. But it is not significant, otherwise we could go for oversampling.\n- 80% of traffic belongs `TCP` while 12% belongs to `UDP` and rest to `ICMP`.\n- Most of the `ICMP` traffic had `anomaly`; most of the `UDP` traffic was `normal`; while the distribution was almost equal in case of `TCP`.\n- The traffic distribution on the basis of flags was also uneven where most of it had `SF(Sign Flag)`.\n- Most of the traffic with `SF` was `normal`, while that had `S0` flag had `anomaly`.\n- Most of the traffic recorded was `unique`.\n- Count of most of the connections having the same destination host and using the same service was either very low or very high.","b6eb6985":"#### Feature Selection","ce49cece":"#### Normalizing the numerical data.","e0ec0100":"- From the above correlation heatmap, we can see that most of the data has very low correlation. This is a good characterstic for our Machine Learning Process.\n- Few features had high correlation with our target class namely,`same_srv_rate`, `dst_host_srv_count`, which will be helpful for our model.","98dd01eb":"## Machine Learning based Intrusion Detection System.\n\nPerformed the Machine Learning workflow which included the following steps:\n\n1. **Data Collection:** \n  - This included finding and selecting data that was processed and used to train our machine learning model. \n   - The source of the dataset used is from [Kaggle](https:\/\/www.kaggle.com\/sampadab17\/network-intrusion-detection). It is a huge repository of community published data.\n  - Short description: The dataset consists of a wide variety of intrusions simulated in a military network environment. It has 25192 rows & 42 columns.\n  - *Note: Just after loading the data, the data was split into 7:3 training and testing data to prevent data leakage.*\n\n2. **Data Analysis and Preparation:**\n  - Here, we analysed the data to find any discrepancies, interesting patterns, coorrelation in data, etc. This step is also popularly known as *Exploratory Data Analysis*.\n  - After analysing, we performed some standard data preprocessing techniques. It was done wherever we felt that it would affect our process.\n  - Most of our time was consumed during this process.\n  - Some methods used are:\n    - *Data cleaning* - handling missing values by mean imputation, etc.\n    - *Data Scaling and Normalisation* - Scaling or Normalisation is common preprocessing technique used in machine learning where the data is ususally normalised to a scale of 0 to 1.\n    - *Data Encoding* - Most of the models cannot process strings\/objects. So the data needs to be transformed to numerical data. This process is known as data encoding(also data transformation).\n    - *Feature Selection* - Removing redundant features or selecting the most \"useful\" features. We used `recursive feature elimination` for feature selection.\n3. **Model Selection\/Building:**\n  - Here, we choose the right models that can be used for the required task.\n  - Our task required us to use a *Classification model*.\n  - We selected 2 State of the art models - *LightGBM* and *XGBoost*.\n  - We also selected a few standard models to compare the results namely Logistic Regression, SVC, Naive Bayes.\n\n4. **Model Training:**\n  - The model was trained on the training data which took a few minutes for each model.\n\n5. **Model Evaluation**\n  - After the model is trained, we evaluate the performance of the model.\n  - The evaluation metrics used are: *Precision* and *Recall*.\n\n6. **Parameter Tuning**\n  - A model needs to be \"tuned\" for each particular scenario\/ usecase based on the dataset.\n  - This includes changing various parameters and evaluating the results simulaneously.\n\n7. **Making Predictions**\n  - After the model parameters are finalized and it is trained, it can be saved and used for making predictions.","5da44940":"*We plot various graphs to identify distributions, relationships or any pattern that is not visible by seeing raw data.*","5298e28b":"### Import Relevant Libraries","92ee708f":"**Using `recursive feature elimination` for Feature Selection**","c81a0d9a":"Though SVC classifier was close, but from the above results, we can observe that our model XGBoost Classifier and LightGBM Classifier perform the best on the validation data.\n\nThe evaluation metrics used are:\n- Precision: also called positive predictive value, is the fraction of correct positive predictions among all the positive predictions.\n\n- Recall: also known as sensitivity, is the fraction of correct positive predictions that were correct positives.\n\nPrecision and Recall can be calculated by:\n\n![Precision and Recall](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/d37e557b5bfc8de22afa8aad1c187a357ac81bdb)\nReference: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall)\n","ba66769f":"#### Encoding Categorical Data","5631ca06":"### Load the dataset","550127a8":"### Data Analysis and Preprocessing","8e9d0bbb":"- We identified various features with binary values and a few with redundancy.\n- A few features have object data type that need to be encoded into numerical values.\n- Also, few features have high scale difference and need to normalised.","e60946e7":"#### Data Splitting","28b20a71":"#### Model Testing on Validation Data","2e93c3ee":"Here, we found that 'is_host_login' and 'num_outbound_cmds' have only one unique value i.e., 0. This introduces redundancy, as a feature with only 1 value won't affect our model. We can remove it and reduce the size of the data and hence improve the training process."}}