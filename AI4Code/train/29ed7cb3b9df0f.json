{"cell_type":{"793c0d70":"code","6081d47e":"code","777342d5":"code","3e7ebfea":"code","8085969d":"code","8f681fa3":"code","d4e4c1b9":"code","6915db94":"code","c74257be":"code","58926283":"code","2be96187":"code","94d26397":"code","1511d028":"code","a74684af":"code","1a8d58c1":"code","cd210cd4":"code","ee2c0690":"code","ee439e68":"code","b7db8fb5":"code","2baf052e":"code","c4586f0c":"code","961cb65a":"code","d5006334":"code","2a74a78e":"code","35f01aa0":"code","f3810b30":"code","be066bbe":"code","3c63de30":"code","f157f707":"code","57ce181d":"code","aa19a8af":"code","bf6ec77f":"code","ced5423a":"code","7c9758df":"code","145e3f51":"code","356045fd":"code","204067b3":"code","d9afdead":"code","f574da35":"code","54daa4e1":"code","227ae121":"code","485d69b9":"code","d6331640":"code","95c1f22a":"code","06861821":"code","bbaffb93":"code","66211587":"code","2194deed":"code","850ca1dc":"code","865ac7eb":"code","7c7b783a":"code","f40f4fbd":"code","c5791a7a":"code","f88fbb59":"code","5faecb3d":"code","840d96b9":"code","57a24470":"code","f92603be":"code","56d5c084":"code","ce707207":"code","581cd679":"code","308455f4":"code","8b8d72ed":"code","6ba09e6f":"code","aa897df9":"code","847ef840":"code","da52622f":"code","5e791a82":"code","6e51eefc":"code","8aa1dc4a":"code","281ab366":"code","997dcffa":"code","686a6c69":"code","cd9d4258":"code","2b57b244":"code","7d7cf1ee":"code","5ef77ba0":"code","3f959b86":"code","8ccb0d5a":"code","6c03a662":"code","becb3141":"code","6fb5fa1d":"code","8c67db95":"code","225160fb":"code","e5dac3f9":"code","6d887045":"code","39439592":"markdown","9101b452":"markdown","9aca2a54":"markdown","c24ed194":"markdown","18dab258":"markdown","2bdfea1e":"markdown","da190fd8":"markdown","47d36573":"markdown","89f9e54e":"markdown","0d14dbf9":"markdown","abe7ed29":"markdown","ddb8eda9":"markdown","fa759318":"markdown","b2bae0ae":"markdown","acc65db7":"markdown","f186eb3c":"markdown","ac2665aa":"markdown","013fe61a":"markdown","cc940bc6":"markdown","81ee6592":"markdown","d641210d":"markdown","744b9d86":"markdown","fbff27d4":"markdown","07856e72":"markdown","6a610f8c":"markdown","5300718e":"markdown","a6da0058":"markdown","1df3faba":"markdown","8857b42d":"markdown","858d78bb":"markdown","ea7970aa":"markdown","b9f01ab3":"markdown","79f27947":"markdown","4f507a96":"markdown","ea91b7fc":"markdown","aee25b24":"markdown","f07e7c01":"markdown","69387127":"markdown","83b28210":"markdown","95654049":"markdown","fe8c8472":"markdown","175c4abe":"markdown","bfed5698":"markdown","e71f9bd4":"markdown","943be97b":"markdown","8cb96aa9":"markdown","74a5252b":"markdown","594be105":"markdown","e327d6af":"markdown","e8fa268a":"markdown","befa2e8f":"markdown","3e7cf037":"markdown","73e8a0ac":"markdown","0a00a5df":"markdown","ead48299":"markdown","e3e252e3":"markdown","9369637c":"markdown","1699230b":"markdown","7ba59326":"markdown","2a7836ee":"markdown","4216991e":"markdown","008a7f93":"markdown","98d95921":"markdown","7d1e5a9a":"markdown","3ea5bb7f":"markdown","2ea8cbf4":"markdown","80e855aa":"markdown","7deddb12":"markdown","0dc44693":"markdown","61c313f0":"markdown","49e68569":"markdown","61a7c857":"markdown","e0de87f0":"markdown"},"source":{"793c0d70":"# import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\n\n# data visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style  \n        \n# ML Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom vecstack import stacking\nfrom sklearn import tree\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMModel,LGBMClassifier\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport tensorflow as tf\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n        \n# Results written to current directory are saved as output","6081d47e":"# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# create data handles\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","777342d5":"# here we create a useful function to display all important confusion matrix metrics\ndef display_confusion_matrix(target, prediction, score=None):\n    cm = metrics.confusion_matrix(target, prediction)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    if score:\n        score_title = 'Accuracy Score: {0}'.format(round(score, 5))\n        plt.title(score_title, size = 14)\n    classification_report = pd.DataFrame.from_dict(metrics.classification_report(target, prediction, output_dict=True))\n    display(classification_report.round(2))","3e7ebfea":"train_data.head(10)","8085969d":"\ntest_data.head(5)","8f681fa3":"train_data.info()","d4e4c1b9":"## another way to see the percentage of missing vals in each category\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","6915db94":"# we see there are missing values\ntrain_data.describe()","c74257be":"%time\n#train_data.profile_report()","58926283":"ht = sns.heatmap(train_data.iloc[:, 1:].corr(),\n                annot=True, \n                fmt = \".2f\", \n                cmap = \"cool\")\nht","2be96187":"# do preliminary analysis of features to see which ones to use\n\nsurvived = 'survived'\nnot_survived = 'not survived'\n\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n\nwomen = train_data[train_data['Sex']=='female']\nmen = train_data[train_data['Sex']=='male']\n\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n\nax.legend()\nax.set_title('Female')\n\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n\nax.legend()\n_ = ax.set_title('Male')\n","94d26397":"# women on port Q and S have a higher chance of survival\nFacetGrid = sns.FacetGrid(train_data, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","1511d028":"sns.barplot(x='Pclass', y='Survived', data=train_data)\n#the higher the class the better probability of survival","a74684af":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","1a8d58c1":"test_data['Fare'].isnull().values.any()","cd210cd4":"# SIBSP PARCH - create RELATIVES feature\ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain_data['not_alone'].value_counts()","ee2c0690":"axes = sns.factorplot('relatives','Survived', \n                      data=train_data, aspect = 2.5, )\n# higher probability of survival with 1 to 3 relatives (and in some cases with 6 relatives)","ee439e68":"# remove PASSENGERID from train set since it is irrelevant to predicted survival probability\ntrain_data = train_data.drop(['PassengerId'], axis=1)\n\n# but do not remove it from test set, since it is required for submission","b7db8fb5":"# Transform CABIN feature - create a number feature \"DECK\" from it\n\nimport re\ndata = [train_data, test_data]\n\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    \n# we can now drop the cabin feature\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)","2baf052e":"train_data.head()\n# cabin is now deck","c4586f0c":"#train_data[\"Pclass\"].values","961cb65a":"# Pclass value distribution\ng = sns.countplot(train_data[\"Pclass\"],order=[1,2,3])","d5006334":"def stacked_bars(feature):\n    deck_1 = train_data[train_data['Deck']==1][feature].value_counts()\n    deck_2 = train_data[train_data['Deck']==2][feature].value_counts()\n    deck_3 = train_data[train_data['Deck']==3][feature].value_counts()\n    deck_4 = train_data[train_data['Deck']==4][feature].value_counts()\n    deck_5 = train_data[train_data['Deck']==5][feature].value_counts()\n    deck_6 = train_data[train_data['Deck']==6][feature].value_counts()\n    deck_7 = train_data[train_data['Deck']==7][feature].value_counts()\n    deck_8 = train_data[train_data['Deck']==8][feature].value_counts()\n    \n    df = pd.DataFrame([deck_1, deck_2, deck_3, deck_4, deck_5, deck_6, deck_7, deck_8])\n    df.index=['1','2','3','4','5','6','7','8']\n    ax = df.plot(kind='bar',stacked=True,title='feature distribution')\n    ax.set_xlabel(\"Decks\")\n    ax.set_ylabel(feature)","2a74a78e":"stacked_bars('Pclass')\n# decks 1,2,3 are 100% first class\n# so we see that most of 1st class passengers are on Decks 2 & 3","35f01aa0":"stacked_bars('Survived')\n# 1 or blue means survived","f3810b30":"# Missing AGE values: create an array of random numbers \n# computed based on the mean age in regards to the std dev and is_null\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    mean = train_data[\"Age\"].mean()\n    std = test_data[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    \n    # compute random numbers between the mean, std and is_null\n    # low, high, size\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    \n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_data[\"Age\"].astype(int)\n\ntrain_data[\"Age\"].isnull().sum()","be066bbe":"# EMBARKED feature has only two missing values - we fill these with the most common one\n# which is 'S'\n\ntrain_data['Embarked'].describe()","3c63de30":"# EMBARKED fill in NaNs\ncommon_value = 'S'\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","f157f707":"train_data.info()\n# now we have 4 categorical(object) features: Name, Sex, Ticket and Embarked","57ce181d":"# EMBARKED feature values to numeric\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_data, test_data]\n\nfor dataset in data: \n    dataset['Embarked'] = dataset['Embarked'].map(ports)","aa19a8af":"# AGE feature values transform from float to int categories\ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","bf6ec77f":"# let's see how AGE is distributed \ntrain_data['Age'].value_counts()","ced5423a":"# FARE transform from float to int\ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","7c9758df":"# FARE \ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","145e3f51":"# from NAME extract TITLES to test a new feature\ndata = [train_data, test_data]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # select Name column from each dataset & extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with Rare or with one of 5 titles\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev',\n                                                'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles to numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # fill NaN with 0\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ntrain_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)","356045fd":"# SEX convert to numeric\ngenders = {\"male\": 0, \"female\": 1}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","204067b3":"# TICKET\ntrain_data['Ticket'].describe()\n\n#since there are 681 unique values, cannot convert them to useful categories","d9afdead":"# so we drop TICKET\ntrain_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)","f574da35":"train_data.describe()\n\n# 38% have survived according to the train data set\n# now there are no more missing values","54daa4e1":"# AGE CLASS feature\n\ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['Age_Class'] = dataset['Age'] * dataset['Pclass']","227ae121":"# FARE \/ person feature\n\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","485d69b9":"# Except for \"Survived\" column all the other columns possess predictive values\ntrain_data.head(10)","d6331640":"# for train data we drop the Survived column\nX_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\n\n# PassengerId is necessary for submission so we make a copy and delete PassengerId in this copy\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()","95c1f22a":"decision_tree = DecisionTreeClassifier()\n\n# fit\ndecision_tree.fit(X_train, Y_train)\n\n# predict\nY_pred_dt = decision_tree.predict(X_test)\n\n# score\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\nacc_decision_tree","06861821":"%time\ntree.plot_tree(decision_tree);","bbaffb93":"random_forest = RandomForestClassifier(n_estimators=100)\n\n# fit\nrandom_forest.fit(X_train, Y_train)\n\n# predict\nY_pred_rf = random_forest.predict(X_test)\n\n# score\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\nacc_random_forest","66211587":"linear_svc = LinearSVC()\n\n# fit\nlinear_svc.fit(X_train, Y_train)\n\n# predict\nY_pred_lsvm = linear_svc.predict(X_test)\n\n# score\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n\nacc_linear_svc","2194deed":"svm = SVC(C=5, random_state=42)\n\n# fit\nsvm.fit(X_train, Y_train)\n\n# predict\nY_pred_svm = svm.predict(X_test)\n\n# score\nacc_svm = round(svm.score(X_train, Y_train) * 100, 2)\n\nacc_svm","850ca1dc":"logreg = LogisticRegression(C=5, penalty='l2',random_state=42)\n\n# fit\nlogreg.fit(X_train, Y_train)\n\n# predict\nY_pred_lr = logreg.predict(X_test)\n\n# score\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n\nacc_log","865ac7eb":"knn = KNeighborsClassifier(n_neighbors = 3)\n\n# fit\nknn.fit(X_train, Y_train)\n\n# predict\nY_pred_knn = knn.predict(X_test)\n\n# score\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n\nacc_knn","7c7b783a":"gaussian = GaussianNB()\n\n# fit\ngaussian.fit(X_train, Y_train)\n\n# predict\nY_pred_gnb = gaussian.predict(X_test)\n\n# score\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n\nacc_gaussian","f40f4fbd":"perceptron = Perceptron(max_iter = 5)\n\n# fit\nperceptron.fit(X_train, Y_train)\n\n# predict\nY_pred_p = perceptron.predict(X_test)\n\n# score\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n\nacc_perceptron","c5791a7a":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\n\n#fit\nsgd.fit(X_train, Y_train)\n\n#predict\nY_pred_sgd = sgd.predict(X_test)\n\n#score\nsgd.score(X_train, Y_train)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\nacc_sgd","f88fbb59":"def build_ann(optimizer='adam'):\n    \n    # Initializing our ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of our ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(13,)))\n    \n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann","5faecb3d":"# we divide training set(for which we have both x & y values) into X&Y subsets for Keras neural network\nX_train_4nn, X_test_4nn, Y_train_4nn, Y_test_4nn = train_test_split(X_train, Y_train, \n                                                                    test_size=0.2, random_state=42)","840d96b9":"opt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\n\n# Fit ANN\nhistory = ann.fit(X_train_4nn, Y_train_4nn, batch_size=16, epochs=30, \n                  validation_data=(X_test_4nn, Y_test_4nn))","57a24470":"# Predict\nann_prediction = ann.predict(X_test_4nn)\nann_prediction = (ann_prediction > 0.5) # convert probabilities to binary output\n\n# Score - compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_test_4nn, ann_prediction)\ndisplay_confusion_matrix(Y_test_4nn, ann_prediction, score=score)","f92603be":"# we divide training set(for which we have both x & y values) into X&Y subsets for TensorFlow NN\nX_train_4tf, X_test_4tf, Y_train_4tf, Y_test_4tf = train_test_split(X_train, Y_train, \n                                                                    test_size=0.2, random_state=42)","56d5c084":"train_data.head()","ce707207":"Pclass = tf.feature_column.numeric_column(\"Pclass\")\nSex = tf.feature_column.numeric_column(\"Sex\")\nAge = tf.feature_column.numeric_column(\"Age\")\nSibSp = tf.feature_column.numeric_column(\"SibSp\")\nParch = tf.feature_column.numeric_column(\"Parch\")\nFare = tf.feature_column.numeric_column(\"Fare\")\nEmbarked = tf.feature_column.numeric_column(\"Embarked\")\nrelatives = tf.feature_column.numeric_column(\"relatives\")\nnot_alone = tf.feature_column.numeric_column(\"not_alone\")\nDeck = tf.feature_column.numeric_column(\"Deck\")\nTitle = tf.feature_column.numeric_column(\"Title\")\nAge_Class = tf.feature_column.numeric_column(\"Age_Class\")\nFare_Per_Person = tf.feature_column.numeric_column(\"Fare_Per_Person\")\n\nfeat_cols = [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, relatives, not_alone, Deck,\n             Title, Age_Class, Fare_Per_Person]","581cd679":"input_func=tf.compat.v1.estimator.inputs.pandas_input_fn(x=X_train_4tf,\n                                               y=Y_train_4tf,\n                                               batch_size=100,\n                                               num_epochs=None,\n                                               shuffle=True)\n\nmodel = tf.estimator.LinearClassifier(feature_columns = feat_cols)","308455f4":"%%time\n# 10000 steps so that it doesn't run forever\nmodel.train(input_fn=input_func, max_steps=10000)\n\npred_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=X_test_4tf,batch_size=len(X_test_4tf),shuffle=False)\n\npredictions = list(model.predict(input_fn=pred_fn))\nfinal_preds = []\nfor pred in predictions:\n    final_preds.append(pred['class_ids'][0])\n    \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test_4tf,final_preds))","8b8d72ed":"gb = GradientBoostingClassifier(\n            #loss='exponential',\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.5,\n            random_state=0).fit(X_train, Y_train)\n\n# fit\ngb.fit(X_train, Y_train)\n\n# predict\nY_pred_gb = gb.predict(X_test)\n\n# score\ngb_acc = round(gb.score(X_train, Y_train) * 100, 2)\n\ngb_acc","6ba09e6f":"xgb = XGBClassifier(max_depth=5, learning_rate=0.1, n_jobs=-1, nthread=-1, \n                    gamma=0.06, min_child_weight=5, \n                    subsample=1, colsample_bytree=0.9, \n                    reg_alpha=0, reg_lambda=0.5, \n                    random_state=42)\n\n# fit\nxgb.fit(X_train, Y_train)\n\n# predict\nY_pred_xgb = xgb.predict(X_test)\n\n# score\nxgb_acc = round(xgb.score(X_train, Y_train) * 100, 2)\n\nxgb_acc","aa897df9":"lgbm = LGBMClassifier(num_leaves=31, learning_rate=0.1, \n                      n_estimators=64, random_state=42, n_jobs=-1)\n\n# fit\nlgbm.fit(X_train, Y_train)\n\n# predict\nY_pred_lgbm = lgbm.predict(X_test)\n    \n# score\nlgbm_acc = round(lgbm.score(X_train, Y_train) * 100, 2)\n\nlgbm_acc ","847ef840":"# We pick some of the best performing models to be the first layer of the stack \n# XGB is set at layer 2 to make the final prediction\n\n# we divide our training set for which we have both x & y values into subsets - X&Y for stacking\nX_train_4s, X_test_4s, Y_train_4s, Y_test_4s = train_test_split(X_train, Y_train, test_size=0.2)","da52622f":"# select several models\nmodels = [random_forest, knn, lgbm]\n\n# perform stacking\nS_train, S_test = stacking(models,\n                           X_train_4s, Y_train_4s, X_test_4s,\n                           regression=False,\n                           mode='oof_pred_bag',\n                           n_folds=5,\n                           save_dir=None,\n                           needs_proba=False,\n                           random_state=42,\n                           stratified=True,\n                           shuffle=True,\n                           verbose=2\n                          )","5e791a82":"# Fit the 2nd level model on the output of level 1\nxgb.fit(S_train, Y_train_4s)","6e51eefc":"# Make predictions on the localized test set\nstacked_pred = xgb.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(Y_test_4s, stacked_pred))","8aa1dc4a":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_data = results.sort_values(by='Score', ascending=False)\nresult_data = result_data.set_index('Score')\nresult_data.head(9)","281ab366":"# training data is split into K subsets (folds)\n# model is trained and evaluated K times\n\n# the following code performs cross-validation 10 times\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring='accuracy')\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","997dcffa":"# measure feature importance for the best model \n# tune features to further improve best model score\n# more features - more likely model will be overfit\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","686a6c69":"importances.plot.bar()","cd9d4258":"# not_alone and Parch are not important in random forest predictions.\n# we drop them and see updated results\ntrain_data  = train_data.drop(\"not_alone\", axis=1)\ntest_data  = test_data.drop(\"not_alone\", axis=1)\n\ntrain_data  = train_data.drop(\"Parch\", axis=1)\ntest_data  = test_data.drop(\"Parch\", axis=1)","2b57b244":"# this time set out of bag samples to true\nrandom_forest2 = RandomForestClassifier(n_estimators=100, oob_score = True)\n\n# fit\nrandom_forest2.fit(X_train, Y_train)\n\n# predict\nY_pred_rf2 = random_forest2.predict(X_test)\n\n# score\nrandom_forest2.score(X_train, Y_train)\nacc_random_forest2 = round(random_forest2.score(X_train, Y_train) * 100, 2)\n\nacc_random_forest2","7d7cf1ee":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","5ef77ba0":"# takes a long time to tune","3f959b86":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n              \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n              \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n\nclf.fit(X_train, Y_train)\n\n#now it is called best_params\nclf.best_params_","8ccb0d5a":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","6c03a662":"# Evaluation of a classification model is more complicated than eval of a regression model\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","becb3141":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))\n\n# Our model predicts 81% of the time, a passengers survival correctly (precision). \n# The recall tells us that it predicted the survival of 72 % of the people who actually survived.","6fb5fa1d":"# You can combine precision and recall into one score, which is called the F-score. \n# The F-score is computed with the harmonic mean of precision and recall. \n# Note that it assigns much more weight to low values. \n# As a result of that, the classifier will only get a high F-score, if both recall and precision are high.\n\nfrom sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","8c67db95":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()\n","225160fb":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","e5dac3f9":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","6d887045":"## output set up to use predictions_rfc\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_pred_rf2})\n\n## output to be saved in csv\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","39439592":"<div id=\"lr\"><\/div>\n### Logistic Regression","9101b452":"<div id=\"7.14.\"><\/div>\n### 7.14. Precision Recall Curve\nFor each person the Random Forest algorithm has to classify, it computes a probability based on a function and it classifies the person as survived (when the score is bigger the than threshold) or as not survived (when the score is smaller than the threshold). That\u2019s why the threshold plays an important part.","9aca2a54":"### Embarked","c24ed194":"<div id=\"7.11.\"><\/div>\n### 7.11. Evaluate model with Confusion Matrix","18dab258":"<div class=\"alert alert-block alert-warning\">  \n- ***TO DO's*** to improve predictions are highlighted in ***yellow***","2bdfea1e":"<div id=\"7.5.\"><\/div>\n### 7.5. K-Fold Cross Validation","da190fd8":"<div id=\"7.16.\"><\/div>\n### 7.16. Potential next steps to improve score\n<div class=\"alert alert-block alert-warning\"> \n- feature engineering. removing unimportant features\n- hyperparameter tuning\n- ensemble learning","47d36573":"<div id=\"tf\"><\/div>\n### TensorFlow","89f9e54e":"<div id=\"nb\"><\/div>\n### Gaussian Naive Bayes","0d14dbf9":"<div id=\"2\"><\/div>\n# 2. Get descriptive info on raw data","abe7ed29":"<div id=\"10\"><\/div>\n# 10. References:\n<div class=\"alert alert-block alert-success\">  \n#### Feature engineering\n- https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n- https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n- https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n- https:\/\/www.kaggle.com\/abhinand05\/machine-learning-for-everyone-lb-top-5\n\n#### Efficient use of pandas, sklearn libries\n- https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis\n- https:\/\/www.kaggle.com\/python10pm\/pandas-100-tricks\n- https:\/\/www.kaggle.com\/python10pm\/sklearn-27-best-tips-and-tricks\n\n#### Hyper-Parameter Optimization\n- https:\/\/medium.com\/@cjl2fv\/an-intro-to-hyper-parameter-optimization-using-grid-search-and-random-search-d73b9834ca0a\n\n\n#### Ensemble models\n- https:\/\/www.kaggle.com\/pliptor\/how-am-i-doing-with-my-score\n- https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting\n- https:\/\/www.kaggle.com\/nhlr21\/complete-titanic-tutorial-with-ml-nn-ensembling\n- https:\/\/www.kaggle.com\/vanshjatana\/applied-machine-learning\n- https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n\n#### ML\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n- https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/\n\n#### Neural networks\n- https:\/\/elitedatascience.com\/keras-tutorial-deep-learning-in-python\n- https:\/\/www.tensorflow.org\/tutorials\/keras\/classification\n- https:\/\/keras.io\/","ddb8eda9":"<div id=\"7.4.\"><\/div>\n### 7.4. Score the models","fa759318":"### Correlation matrix","b2bae0ae":"<div id=\"7.15.\"><\/div>\n### 7.15. ROC AUC Curve\nThis curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.","acc65db7":"<div id=\"svm\"><\/div>\n### Support Vector Machine Classifier","f186eb3c":"![image.png](attachment:image.png)","ac2665aa":"<div id=\"xgb\"><\/div>\n### XGBoost - Extreme Gradient Boosting","013fe61a":"<div id=\"9\"><\/div>\n# 9. Important non-obvious lessons\n<div class=\"alert alert-block alert-info\"> \n#### Key Takeaways on feature engineering: \n- We need to consider every single feature in terms of values & correlations w oth features. For example the Master is a unique title because it is given only to male passengers below 26 with the highest survival rate\n- Raw data EDA is the fastest way to generate first feature hypotheses\n- Use as many graphs as necessary to evaluate features & their correlations from all angles\n- Use google to gain better understanding of the underlying structure of features that have a lot of missing values\n- We need to consider every single feature in terms of values & correlations w oth features","cc940bc6":"<div id=\"gb\"><\/div>\n### Gradient Boosting","81ee6592":"<div id=\"7.8.\"><\/div>\n### 7.8. Out of bag samples (instead of K-fold CV) to eval RF classifier","d641210d":"### Cabin -> Deck\nWe have 687 missing cabin values in the training data only\n\nBy googling we find that only T cabin is present in the dataset, but there were rooms labeled as T, U, W, X, Y, Z, which are FEATURE VALUES ABSENT in the dataset provided except for T!\n\n<div class=\"alert alert-block alert-info\">\nTakeaway: Use google to gain better understanding of the underlying structure of features that have a lot of missing values","744b9d86":"<div id=\"7.7.\"><\/div>\n### 7.7. Random Forest 2nd test (after removing unimportant features)","fbff27d4":"<div id=\"dt\"><\/div>\n### Decision Tree","07856e72":"<div id=\"1.1.\"><\/div>\n### 1.1. Useful functions ","6a610f8c":"<div id=\"rf\"><\/div>\n### Random Forest","5300718e":"### Fare","a6da0058":"<div id=\"2ppr\"><\/div>\n### 2.1. Pandas Profiling Report","1df3faba":"<div id=\"k\"><\/div>\n### Keras Artificial Neural Network","8857b42d":"<div id=\"8\"><\/div>\n# 8. Export output to csv & submit","858d78bb":"<div id=\"dl\"><\/div>\n### DEEP LEARNING: TF, Keras, PyTorch","ea7970aa":"<div id=\"1\"><\/div>\n# 1. Import libraries & files ","b9f01ab3":"### Pclass to Survived","79f27947":"<div id=\"7.12.\"><\/div>\n### 7.12. Precision and Recall","4f507a96":"<div id=\"7.1.\"><\/div>\n### 7.1. Look at preprocessed training data info","ea91b7fc":"### PassengerID","aee25b24":"### NEW Age_Class","f07e7c01":"### Embarked\nfill EMBARKED missing values - fill in most common values.\n<div class=\"alert alert-block alert-warning\"> \n***TO DO: google info about passengers & get embarked values from results***","69387127":"### Name -> Titles\n<div class=\"alert alert-block alert-info\"> \nTakeaway: We need to consider every single feature in terms of values & correlations w oth features.","83b28210":"<div id=\"7.9.\"><\/div>\n### 7.9. Hyperparameter Tuning","95654049":"<div id=\"7.2.\"><\/div>\n### 7.2. Prepare train & test datasets for model training","fe8c8472":"<div id=\"sgd\"><\/div>\n### Stockastic Gradient Descent (SGD)","175c4abe":"![image.png](attachment:image.png)\nThe first row is about the not-survived-predictions: \n- 492 passengers were correctly classified as not survived (called true negatives) \n- 57 where wrongly classified as not survived (false positives).\n\n\nThe second row is about the survived-predictions: \n- 96 passengers where wrongly classified as survived (false negatives) \n- 246 where correctly classified as survived (true positives).","bfed5698":"### Ticket","e71f9bd4":"<div id=\"7.10.\"><\/div>\n### 7.10. Test model with best_params_ from GridSearchCV","943be97b":"<div id=\"4\"><\/div>\n# 4. Clean data: eliminate NaN & transform non-classifiable features\n<div class=\"alert alert-block alert-info\">\nTakeaway: Use as many graphs as necessary to evaluate features & their correlations from all angles","8cb96aa9":"### ROC AUC (True Positive Rate Area Under Curve) Score\nA classifiers that is 100% correct would have a ROC AUC Score of 1 \nand a completely random classiffier would have a score of 0.5.","74a5252b":"<div id=\"lsvm\"><\/div>\n### Linear Support Vector Machine","594be105":"<div class=\"alert alert-block alert-warning\"> \n- ***TO DO: Master is a unique title because it is given only to male passengers below 26 with the highest survival rate. We can predict that all males titled Master whose entire family lives will live***\n- ***TO DO: Predict die for all females whose entire family dies***\n- ***TO DO: Is_Married is a potential NEW FEATURE since Mrs has the highest survival rate among females***","e327d6af":"### Pclass distribution in Decks","e8fa268a":"<div id=\"7.13.\"><\/div>\n### 7.13. F-Score","befa2e8f":"<div class=\"alert alert-block alert-info\">\n<b>Takeaway:<\/b> Raw data EDA is the fastest way to generate first feature hypotheses","3e7cf037":"# ML framework collection applied to \"Titanic\" Kaggle dataset\n\n### Table of contents: \n1. [Import libraries & files](#1)\n    - [1.1. Useful functions](#1.1.)\n2. [Get descriptive info on raw data](#2)\n    - [2.1. Pandas Profiling Report](#2ppr)\n3. [Raw data exploratory data analysis & visualization](#3)\n4. [Clean data: eliminate NaN & transform non-classifiable features](#4)\n5. [Transform classifiable feature values to ints](#5)\n6. [Engineer new features](#6)\n7. [Train ML models, tune hyperparams & evaluate](#7)\n    - [7.1. Look at preprocessed training data info](#7.1.)\n    - [7.2. Prepare train & test datasets for model training](#7.2.)\n    - [7.3 Train the models](#7.3.)\n    - [7.4. Score the models](#7.4.)\n    - [7.5. K-Fold Cross Validation](#7.5.)\n    - [7.6. Feature importance](#7.6.)\n    - [7.7. Random Forest 2nd test (after removing unimportant features)](#7.7.)\n    - [7.8. Out of bag samples (instead of K-fold CV) to eval RF classifier](#7.8.)\n    - [7.9. Hyperparameter Tuning](#7.9.)\n    - [7.10. Test model with best_params_ from GridSearchCV](#7.10.)\n    - [7.11. Evaluate model with Confusion Matrix](#7.11.)\n    - [7.12. Precision and Recall](#7.12.)\n    - [7.13. F-Score](#7.13.)\n    - [7.14. Precision Recall Curve](#7.14.)\n    - [7.15. ROC AUC Curve](#7.15.)\n    - [7.16. Potential next steps to improve score](#7.16.)\n8. [Export output to csv & submit](#8)\n8. [Important non-obvious lessons](#9)\n10. [References](#10)\n\n### ML frameworks featured in 7.3: \n- [Decision Tree](#dt)\n- [Random Forest Classifier](#rf)\n- [Linear Support Vector Machine](#lsvm)\n- [Support Vector Machine Classifier](#svm)\n- [Logistic Regression](#lr)\n- [K Nearest Neighbors](#knn)\n- [Gaussian Naive Bayes](#nb)\n- [Perceptron](#p)\n- [Stockastic Gradient Descent (SGD)](#sgd)\n- [DEEP LEARNING: TF, Keras](#dl)\n    - [TensorFlow](#tf)\n    - [Keras](#k)\n- [Gradient Boosting](#gb)\n- [XGBoost](#xgb)\n- [LightGBM](#lgbm)\n- [ENSEMBLE LEARNING: Bagging, Boosting, Stacking](#ensemble)\n    - [Stacking](#stacking)\n\n\n**Researcher: Vladislav Semin**","73e8a0ac":"<div id=\"3\"><\/div>\n# 3. Raw data exploratory data analysis & visualization","0a00a5df":"<div id=\"knn\"><\/div>\n### K Nearest Neighbors","ead48299":"Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n\nmean_fare = test_data.groupby(['Pclass', 'Parch', 'SibSp']).['Fare'].mean()[3][0][0]\n\ntest_data['Fare'] = test_data['Fare'].fillna(mean_fare)","e3e252e3":"<div id=\"7.3.\"><\/div>\n### 7.3 Train the models","9369637c":"### NEW Fare_Per_Person","1699230b":"### Age\nDeal with missing AGE values is to create an array of random numbers within +- 1 std dev from the mean of all values.\n<div class=\"alert alert-block alert-warning\"> \n***TO DO: take median values of AGE according to Pclass groups, this would be an improvement***\n\n***data.groupby(['Sex', 'Pclass']).median()['Age']***","7ba59326":"### Sex to Survived","2a7836ee":"### Sex","4216991e":"<div id=\"6\"><\/div>\n# 6. Engineer new features","008a7f93":"### SibSp, Parch -> Relatives","98d95921":"<div id=\"7.6.\"><\/div>\n### 7.6. Feature importance","7d1e5a9a":"<div id=\"lgbm\"><\/div>\n### LightGBM","3ea5bb7f":"### Subheader formatting:\n<div class=\"alert alert-block alert-info\">\n<b>Takeaways<\/b> are highlighted in ***blue***","2ea8cbf4":"<div id=\"p\"><\/div>\n### Perceptron","80e855aa":"### Fare\nFare is contingent on family size (Parch and SibSp) and Pclass features, since there are fares for multiple people\n\nWe have only one missing fare value in the test set, we will not touch the test set now","7deddb12":"<div class=\"alert alert-block alert-warning\">  \n- ***TO DO:*** fill-in MISSING VALUES for age, cabin, embarked features - DONE below\n- ***TO DO:*** convert sex, cabin, embarked TO CLASSIFIABLE values - DONE below\n- ***TO DO:*** GROUP VALUES of certain features - DONE below","0dc44693":"<div id=\"7\"><\/div>\n# 7. Train ML models, tune hyperparams & evaluate","61c313f0":"<div id=\"5\"><\/div>\n# 5. Transform classifiable feature values to ints","49e68569":"<div id=\"ensemble\"><\/div>\n### ENSEMBLE LEARNING: Bagging, Boosting, Stacking\nBagging will mainly focus at getting an ensemble model with less variance than its components \n\nboosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).\n<div id=\"stacking\"><\/div>\n### Model Stacking","61a7c857":"### Embarked to Survived","e0de87f0":"### Age"}}