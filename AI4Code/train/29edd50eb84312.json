{"cell_type":{"84d83cd9":"code","6b156082":"code","eb1764bc":"code","c0e48966":"code","f5013a21":"code","4e5ea307":"code","68464cc9":"code","83b4f348":"code","4a9da196":"code","44661cfc":"code","664357b9":"code","e5755514":"code","58e68d21":"code","728b126c":"code","1a297fe1":"code","95a798f1":"code","52690312":"code","24f73170":"code","c2be4964":"code","645ac685":"code","394d32a1":"code","12bd110e":"code","982ceb9f":"code","89a92040":"code","e5c65d9c":"code","2c46c3c3":"code","7ff71a84":"code","588b9eea":"code","eb14dbe9":"code","10eb813c":"code","986bf0b0":"code","8c5b80e0":"code","f4d4d32c":"code","268bab9c":"code","3001b463":"code","6b25efb3":"code","0b76fe65":"code","f78327b5":"code","b9d3a32e":"code","61ee4956":"code","13680910":"code","32843b23":"code","1a5fd1d6":"code","1d938d70":"code","55900ac9":"code","f5afaab0":"code","9d1ed7e5":"code","419fc5b2":"code","e4bc1dc8":"code","ea50133b":"code","1ebb3968":"code","86b5ac43":"code","53289653":"code","7f2021e4":"code","9bfeb03b":"code","1c93ac5c":"code","a0ca8a47":"code","e740aa01":"code","d3ef43fa":"code","a46d4d2b":"code","7f43f98b":"code","f836cf3c":"code","601876b9":"code","76488aa0":"code","f6c211f7":"code","8f8ac07b":"code","b37b535f":"code","4c044a27":"code","0b6a751e":"code","9654dba6":"code","f9be489f":"code","e9275f1c":"code","b85cedd7":"code","785ffe4c":"code","869ee2cc":"code","2cebf60a":"code","439ed20b":"code","c4c8b8d5":"code","082a766d":"code","2a21982b":"code","77de19de":"code","b55760ab":"code","700e9ac5":"code","39be30e7":"code","8a26135c":"code","acaa8832":"code","50aba732":"code","862670c3":"code","af1d9af9":"code","80189c34":"code","3b5c9073":"code","2fc0a18c":"code","7907bf4a":"code","ce6d5145":"markdown","1d336ab6":"markdown","57ac3c41":"markdown","7538e933":"markdown","f66d1310":"markdown","12fbd1ae":"markdown","752998ee":"markdown","fb578ba8":"markdown","557f24f6":"markdown","a9a5d301":"markdown","75acc70a":"markdown","11d488e3":"markdown","d893e4c7":"markdown"},"source":{"84d83cd9":"!git clone https:\/\/github.com\/dsc-iem\/AI-Hacktoberfest.git","6b156082":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('.\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb1764bc":"!pip install plotly","c0e48966":"!pip install cufflinks","f5013a21":"!pip install chart-studio","4e5ea307":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n\nfrom plotly.offline import iplot\nimport plotly as py\nimport plotly.tools as tls\nimport cufflinks as cf\n","68464cc9":"pd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)","83b4f348":"cm = sns.light_palette(\"green\", as_cmap=True)","4a9da196":"py.offline.init_notebook_mode(connected = True)\ncf.go_offline()\n# cf.set_config_file(theme='solar')\nplt.style.use('dark_background')","44661cfc":"df = pd.read_csv('..\/input\/forest-fire-prediction\/forestfires.csv')\ndf.head().style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen',\n                           'border-color': 'white'})","664357b9":"df.info()","e5755514":"numerical_features = [features for features in df.columns if df[features].dtypes != 'O']\nprint('Number of Numerical variables are: ', len(numerical_features))\nprint('Numerical features are: ', numerical_features)\ndf[numerical_features].head().style.background_gradient(cmap=cm)","58e68d21":"discrete_feature = [features for features in numerical_features if len(df[features].unique())< 20]\nprint(f\"length of discrete numerical variables are: {len(discrete_feature)}\")\nprint(f\"And the discreate features are: {discrete_feature}\")\n# lets see the head of the data frame consists of discrete numerical values\ndf[discrete_feature].head().style.background_gradient(cm).highlight_null('green')","728b126c":"df['X'].value_counts()","1a297fe1":"df['Y'].value_counts()","95a798f1":"df['rain'].value_counts()","52690312":"# lets see the different values in each discreate variables\nprint(df['X'].value_counts())\nprint('\\n')\nprint(df['Y'].value_counts())\nprint('\\n')\nprint(df['rain'].value_counts())","24f73170":"#  les search for year feature\nyear_feature = [features for features in numerical_features if 'Yr' in features or 'Year' in features or 'yr' in features or 'year' in features]\nprint(f\"year features are : {year_feature}\")","c2be4964":"continuous_feature=[features for features in numerical_features if features not in discrete_feature]\nprint(f\"Continuous feature Count {len(continuous_feature)}\")\nprint(f\"Continuous feature are: {continuous_feature}\")\n\n# lets see the head\ndf[continuous_feature].head().style.background_gradient(cmap=cm)","645ac685":"categorical_features = [features for features in df.columns if df[features].dtypes =='O']\nprint(f\"Now categorical variables are: {categorical_features}\")\nprint(f\"number of categorical variables are: {categorical_features}\")\n\n# see the head\n# CANT COLOR A CATEGORICAL VARIABLE\ndf[categorical_features].head()","394d32a1":"df['month'].describe()","12bd110e":"df['day'].describe()","982ceb9f":"# lets see the different values in each categorical variables\nprint(df['month'].value_counts())\nprint('\\n')\nprint(df['day'].value_counts())\nprint('\\n')","89a92040":"df.describe().style.background_gradient(cmap='Reds')","e5c65d9c":"df['month'].value_counts().iplot(kind='bar',color='magenta')","2c46c3c3":"df['day'].value_counts().iplot(kind='bar',color='red')","7ff71a84":"df['rain'].value_counts().iplot(kind='bar',color='green')","588b9eea":"df['X'].value_counts().iplot(kind = 'bar',color='yellow')","eb14dbe9":"df['Y'].value_counts().iplot(kind = 'bar',color='blue')","10eb813c":"fig = tls.make_subplots(rows=2, cols=1)\n\n# fig1 = df1.iplot(kind='bar',barmode='stack',x='catagory',\n#                        y=['dogs','cats','birds'],asFigure=True)\nfig1 = pd.DataFrame(df[\"FFMC\"]).iplot(kind=\"histogram\", \n                bins=40, \n                theme=\"pearl\",\n                title=\"Histogram of FFMC\",\n                xTitle='FFMC', \n                yTitle='Count',\n                asFigure=True)\n\nfig.append_trace(fig1['data'][0], 1, 1)\n\n# fig2 = pd.DataFrame(df[\"DMC\"]).iplot(kind=\"histogram\", \n#                 bins=40, \n#                 theme=\"solar\",\n#                 title=\"Histogram of DMC\",\n#                 xTitle='DMC', \n#                 yTitle='Count',\n#                 asFigure=True)\nfig2 = df['FFMC'].iplot(kind = 'scatter',theme=\"solar\" , mode = 'markers',asFigure=True)\n\n\nfig.append_trace(fig2['data'][0], 2, 1)\n\n\n# x = df['FFMC'].values.tolist()\n# cumsum = np.cumsum(x)\n# fig3 = pd.DataFrame(cumsum).iplot(asFigure=True)\n\n# fig.append_trace(fig3['data'][0], 1, 2)\n\n# fig4 = df['FFMC'].iplot(kind = 'box',asFigure=True)\n\n# fig.append_trace(fig4['data'][0], 2, 2)\n\n\niplot(fig)\n","986bf0b0":"# df[['FFMC']] = df[['FFMC']].astype('float64', copy=False)\ndf['FFMC'].iplot(kind = 'scatter' , mode = 'markers')\n","8c5b80e0":"df['DMC'].iplot(kind = 'scatter' , mode = 'markers')\n\n","f4d4d32c":"import plotly.figure_factory as ff\nimport numpy as np\nnp.random.seed(1)\n\n# x = np.random.randn(1000)\n# hist_data = [x]\nx = np.array(df['DMC'])\nhist_data = [x]\ngroup_labels = ['DMC'] # name of the dataset\n\nfig = ff.create_distplot(hist_data, group_labels)\nfig.show()","268bab9c":"x = df['DMC'].values.tolist()\n\ncumsum = np.cumsum(x)\n\n\npd.DataFrame(cumsum).iplot()\n","3001b463":"df['area'].iplot()","6b25efb3":"import plotly.figure_factory as ff\nimport numpy as np\nnp.random.seed(1)\n\n# x = np.random.randn(1000)\n# hist_data = [x]\nx = np.array(df['area'])\nhist_data = [x]\ngroup_labels = ['area'] # name of the dataset\n\nfig = ff.create_distplot(hist_data, group_labels)\nfig.show()","0b76fe65":"df['area'].iplot(kind = 'scatter' , mode = 'markers')\n\n","f78327b5":"x = df['area'].values.tolist()\n\ncumsum = np.cumsum(x)\n\n\npd.DataFrame(cumsum).iplot()","b9d3a32e":"pd.DataFrame(df[\"area\"]).iplot(kind=\"histogram\", \n                bins=40, \n                theme=\"pearl\",\n                title=\"Histogram of FFMC\",\n                xTitle='FFMC', \n                yTitle='Count',\n                asFigure=True)","61ee4956":"import plotly.express as px\n\n# df = px.data.tips()\nfig = px.violin(df, y=\"area\", box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","13680910":"df","32843b23":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor i in categorical_features:\n    df[i]=label_encoder.fit_transform(df[i])","1a5fd1d6":"df.head()","1d938d70":"SEED = 42\n\ndata = df.copy()\ny = data['area']\nx = data.drop(['area'],axis=1)\n\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size = 0.2,random_state = SEED)","55900ac9":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n# from sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=5, random_state=SEED)\nmodel = regr.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nscore =  mean_squared_error(y_val, y_pred)\n# y_val.size\nprint(score)\nprint(np.sqrt(score))","f5afaab0":"print(r2_score(y_val,y_pred))","9d1ed7e5":"print(model.score(x_val, y_val))","419fc5b2":"x_train.dtypes","e4bc1dc8":"y_val_rf = pd.DataFrame(y_val)\ny_pred_rf = pd.DataFrame(y_pred)\nrf_pred_anlys = y_val_rf.join(y_pred_rf)\nrf_pred_anlys1 = pd.concat([y_val_rf,y_pred_rf])\nrf_pred_anlys1.head()","ea50133b":"type(y_val_rf)","1ebb3968":"rf_pred_anlys","86b5ac43":"rf_pred_anlys.iplot(kind='scatter',x='y_val_rf',y='y_pred_rf',mode='markers')","53289653":"reg = RandomForestRegressor(\n    n_estimators=50,\n    criterion='gini',\n    max_depth=2,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=-1,\n    random_state=SEED,\n    verbose=0,\n    warm_start=False\n)","7f2021e4":"params={'min_child_weight':[4,5], 'gamma':[i\/10.0 for i in range(3,6)],  'subsample':[i\/10.0 for i in range(6,11)],\n'colsample_bytree':[i\/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}","9bfeb03b":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","1c93ac5c":"reg_rf_rscv = RandomForestRegressor()","a0ca8a47":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nrandom_search_rf = RandomizedSearchCV(reg_rf_rscv, random_grid,n_iter=5, n_jobs=1, cv=5,verbose=2)","e740aa01":"random_search_rf.fit(x_train,y_train)","d3ef43fa":"random_search_rf.best_params_","a46d4d2b":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\nbase_model = RandomForestRegressor(n_estimators= 1400,\n                                     min_samples_split= 10,\n                                     min_samples_leaf= 4,\n                                     max_features= 'sqrt',\n                                     max_depth= 60,\n                                     bootstrap= True,\n                                    random_state = SEED)\nbase_model.fit(x_train, y_train)\nbase_accuracy = evaluate(base_model, x_val,y_val)","7f43f98b":"base_accuracy","f836cf3c":"y_pred_rf_rscv = base_model.predict(x_val)\nprint(np.sqrt(mean_squared_error(y_pred_rf_rscv,y_val)))\nprint(r2_score(y_val,y_pred_rf_rscv))","601876b9":"def compute_mse(model,x_test,y_test):\n    prediction = model.predict(x_test)\n    mse = mean_squared_error(prediction,y_test)\n    return mse\n\ndef compute_r2(model,x_test,y_test):\n    prediction = model.predict(x_test)\n    r2 = r2_score(prediction,y_test)\n    return r2\n\ndef compute_rmse(model,x_test,y_test):\n    prediction = model.predict(x_test)\n    mse = mean_squared_error(prediction,y_test)\n    return np.sqrt(mse)","76488aa0":"from sklearn.model_selection import KFold\n\ncv = KFold(n_splits=3, shuffle=False,random_state=SEED)\n# results = pd.DataFrame(columns=['training_score', 'test_score'])\nrmse_train, mse_train, r2_scores_train = [], [], []\nrmse_test, mse_test, r2_scores_test = [], [], []\n    \nfor (train, test), i in zip(cv.split(data), range(10)):\n    reg_rf_rscv.fit(x.iloc[train], y.iloc[train])\n    \n    mse_data_train = compute_mse(reg_rf_rscv,x.iloc[train],y.iloc[train])\n    rmse_data_train = compute_rmse(reg_rf_rscv,x.iloc[train],y.iloc[train])\n    r2_data_train= compute_r2(reg_rf_rscv,x.iloc[train],y.iloc[train])\n    \n    \n    mse_data_test = compute_mse(reg_rf_rscv,x.iloc[test],y.iloc[test])\n    rmse_data_test = compute_rmse(reg_rf_rscv,x.iloc[test],y.iloc[test])\n    r2_data_test = compute_r2(reg_rf_rscv,x.iloc[test],y.iloc[test])\n    \n    \n    r2_scores_train.append(r2_data_train)\n    r2_scores_test.append(r2_data_test)\n    \n    mse_train.append(mse_data_train)\n    mse_train.append(mse_data_test)\n    \n    rmse_test.append(rmse_data_train)\n    rmse_test.append(rmse_data_test)\n    print('done')\n\n# plot_roc_curve(fprs, tprs);\n# pd.DataFrame(scores, columns=['AUC Train', 'AUC Test'])","f6c211f7":"# rmse_train\n# mse_train\nr2_scores_train\n# rmse_test\n# mse_test\n# r2_scores_test ","8f8ac07b":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","b37b535f":"# clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(reg_rf_rscv, x_train, y_train, cv=k_fold, n_jobs=1, scoring='r2')\nprint(score)","4c044a27":"print('r2 %2f' %(1 * score.mean()))\n","0b6a751e":"import sklearn\nsklearn.metrics.SCORERS.keys()","9654dba6":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit(data)\n# scaler.transform(data)","f9be489f":"# SEED = 42\n\n# # data = df.copy()\n# Y = data['area']\n# X = data.drop(['area'],axis=1)\n\n\n# from sklearn.model_selection import train_test_split\n# X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size = 0.2,random_state = SEED)","e9275f1c":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport xgboost ","b85cedd7":"params={'min_child_weight':[4,5], 'gamma':[i\/10.0 for i in range(3,6)],  'subsample':[i\/10.0 for i in range(6,11)],\n'colsample_bytree':[i\/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}","785ffe4c":"regressor = xgboost.XGBRegressor()\n","869ee2cc":"random_search = RandomizedSearchCV(regressor, params,n_iter=5, n_jobs=1, cv=5)","2cebf60a":"random_search.fit(x_train,y_train)","439ed20b":"random_search.best_estimator_","c4c8b8d5":"xgbo = xgboost.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=0.3, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n             min_child_weight=5,monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n             tree_method='exact', validate_parameters=1, verbosity=None)","082a766d":"xgbo.fit(x_train,y_train)\ny_pred_xg = xgbo.predict(x_val)\nmse_xg = mean_squared_error(y_val,y_pred_xg)\nprint(mse_xg)\nprint(r2_score(y_val,y_pred_xg))","2a21982b":"print(np.sqrt(mse_xg))\n","77de19de":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\n\nbase_learners = [\n                 ('rf_1', RandomForestRegressor(n_estimators=100, random_state=SEED)),\n                 ('adb',AdaBoostRegressor(n_estimators=100, random_state=SEED)),\n                 ('ext',ExtraTreesRegressor(n_estimators=100, random_state=SEED)),\n                 ('gbc',GradientBoostingRegressor(n_estimators=100,random_state=SEED)),\n                 ('svc', SVR())\n    \n    \n                ]\n\n# Initialize Stacking regression with the Meta Learner\nstk_reg = StackingRegressor(estimators=base_learners, final_estimator=LinearRegression(),cv=10)\n\nstk_reg.fit(x_train, y_train)","b55760ab":"y_pred_stack = stk_reg.predict(x_val)\nmse_stack = mean_squared_error(y_val,y_pred_stack)\nprint(mse_stack)","700e9ac5":"print(np.sqrt(mse_stack))","39be30e7":"print(model.score(x_train, y_train))","8a26135c":"# y_pred_cascaded = y_pred*0.0001 + y_pred_stack*0.9999 + y_pred_xg*0.0\n# mse_cascaded = mean_squared_error(y_val,y_pred_cascaded)\n# print(mse_cascaded)\n# print(np.sqrt(mse_cascaded))","acaa8832":"x_val_merge = x_val\n# y_val_merge = y_val\ny_val_merge1 = pd.DataFrame(y_val,columns =  ['area'])\n\n# test_df = pd.concat([x_val_merge,y_val_merge1])\n# test_df.head()\n# type(y_val_merge1)\nx_val_merge.shape\n# pd.merge(x_val_merge,y_val_merge)","50aba732":"y_val_merge1.head()","862670c3":"df_row = pd.concat([x_val_merge,y_val_merge1])\n\ndf_row","af1d9af9":"val_df = x_val_merge.join(y_val_merge1)","80189c34":"val_df.head()","3b5c9073":"L = 15\nfeat = ['X','Y','month','day','FFMC','DMC','DC','ISI','temp','RH','wind','rain','area']\n\nM = df.area.mean()\nte = df.groupby(feat)['area'].agg(['mean','count']).reset_index()\nte['ll'] = ((te['mean']*te['count'])+(M*L))\/(te['count']+L)\ndel te['mean'], te['count']\n\nval_df = val_df.merge( te, on=feat, how='left' )\n# val_df['ll'] = test['ll'].fillna(M)\n\nval_df.head()","2fc0a18c":"print(r2_score(y_val,val_df['ll']))","7907bf4a":"# y_pred_rf_rscv = base_model.predict(x_val)\nprint(np.sqrt(mean_squared_error(val_df['ll'],y_val)))","ce6d5145":"## Rf with Kfold CV:","1d336ab6":"# Model Building:","57ac3c41":"### Uni-variate analysis of Area:","7538e933":"|Type of variable|Column name|\n|--|--|\n|Numerical Variables|'X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area'|\n|Year variable\/features|No year variables|\n|Discrete Variables|'X', 'Y', 'rain'|\n|Continuous Variables|'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'area'|\n|Categorical Variables|'month', 'day'|\n","f66d1310":"### Stacking:","12fbd1ae":"## Lets bring some color and draw some graphs","752998ee":"## Train test split:","fb578ba8":"'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'area'","557f24f6":"## Lable Encoding:","a9a5d301":"### Statistical Modeling:","75acc70a":"### Xgboost with RandomizedSearchCV:","11d488e3":"### KFold CV:","d893e4c7":"### Random Forest:"}}