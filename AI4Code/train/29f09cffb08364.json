{"cell_type":{"8b6212b0":"code","e9438ab5":"code","1244469d":"code","ce51cc6e":"code","bdf2d125":"code","0ac7c0c0":"code","90d07e95":"code","fb15c7bc":"code","35d37515":"code","4fe11094":"code","9bd01cc3":"code","972bcc52":"code","5fb9efbf":"code","c2a5d7ce":"code","080cf893":"code","bda858b1":"code","f14fff6b":"code","0ca37efb":"code","2b50d530":"code","9b6bcd51":"code","eff8748c":"code","0c9d6a94":"code","4f4fd351":"code","a936b450":"code","df15a651":"code","b413daa2":"code","01ab3996":"code","b35207e9":"code","629977f3":"code","c87e1971":"code","b24f4d2d":"code","c783b830":"code","3724d79a":"code","ad8552c4":"code","29fdcb77":"code","4a6b2c6d":"code","f5e6363d":"code","de2e92c9":"code","36055e58":"code","cce9b363":"code","94a3876b":"code","35fc9877":"code","33eaa043":"code","04e803e7":"code","845cf72a":"code","e0da9924":"code","042f9a12":"code","efbba778":"code","265f2b42":"code","7cfe825c":"code","4a984056":"code","7dfbb043":"code","8fcf0898":"code","e9c91ebe":"code","e791b16f":"code","59738626":"code","4d726deb":"code","27733777":"code","4323172c":"code","b44f09db":"code","f8dc40ea":"code","d92c3691":"markdown","91ded997":"markdown","a2f57b16":"markdown","a881f337":"markdown","ef497949":"markdown","4ae83fd4":"markdown","b7df88d5":"markdown","1b124231":"markdown","a0a4f5f8":"markdown","23a55647":"markdown","54c026d0":"markdown","2b4067ad":"markdown","93b3331f":"markdown","88ba003d":"markdown","349ed56b":"markdown","f1dd2831":"markdown","fe15e38d":"markdown","e9f632b8":"markdown","2f11ee18":"markdown","22ed9aaa":"markdown","68a6f65d":"markdown","787352a4":"markdown","ebbcecb4":"markdown","dba76de1":"markdown","e92ee77d":"markdown","4de91549":"markdown","bf0eb146":"markdown","b1ce102e":"markdown","dbb8a7df":"markdown","8d86932a":"markdown"},"source":{"8b6212b0":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","e9438ab5":"#import data from kaggle store\ndata=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n#data.head()","1244469d":"# nice resume table to describe the data\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    summary['Fourth Value'] = df.loc[3].values\n    summary['Fifth Value'] = df.loc[4].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=10),4) \n\n    return summary","ce51cc6e":"# resumetable(data)","bdf2d125":"#also handy built\/in function for data description\n# data.describe()","0ac7c0c0":"# and one more\n# data.info()","90d07e95":"# simple categorical plolt for target variable 'Churn'\n# ax = sns.catplot(y=\"Churn\", kind=\"count\", data=data, height=3, aspect=2, orient='h')","fb15c7bc":"# retyping TotalCharges to numeric\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n\n# for the sake of ease while plotting\ndf_y = data[data[\"Churn\"]==\"Yes\"] \ndf_n = data[data[\"Churn\"]==\"No\"]","35d37515":"# tenure histograms \nsns.histplot(df_n['tenure'],  kde=False, label='Churn: No', color = \"#22ff57\")\nsns.histplot(df_y['tenure'],  kde=False, label='Churn: Yes',  color= \"#FF5722\")\n\nplt.legend(prop={'size': 12})\nplt.title('Tenure distributions by churn')\nplt.xlabel('Tenure [month]')\nplt.ylabel('Count')","4fe11094":"# histogram for MonthlyCharges\nsns.histplot(df_n['MonthlyCharges'],  kde=False, label='Churn: No',color = \"#22ff57\")\nsns.histplot(df_y['MonthlyCharges'],  kde=False, label='Churn: Yes',  color= \"#FF5722\")\n\nplt.legend(prop={'size': 12})\nplt.title('MonthlyCharges distributions by churn')\nplt.xlabel('MonthlyCharges [$]')\nplt.ylabel('Count')","9bd01cc3":"#print(data.dtypes)\n\nsns.histplot(df_n['TotalCharges'],  kde=False, label='Churn: No',color = \"#22ff57\")\nsns.histplot(df_y['TotalCharges'],  kde=False, label='Churn: Yes', color= \"#FF5722\")\n\nplt.legend(prop={'size': 12})\nplt.title('TotalCharges distributions by churn')\nplt.xlabel('TotalCharges')\nplt.ylabel('Count')","972bcc52":"# KDE ... Kernel Density Estimators for better view of probability of each possibilities...\n# KDE being a nice feature for comparing probabilities of variables with different counts\ndef kde_plot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(data[data['Churn'] == 'No'][feature].dropna(), color = \"#22ff57\", label= 'Churn: No')\n    ax1 = sns.kdeplot(data[data['Churn'] == 'Yes'][feature].dropna(), color= \"#FF5722\", label= 'Churn: Yes')\n    plt.legend(prop={'size': 12})\n#kde_plot('tenure')\n#kde_plot('MonthlyCharges')\n#kde_plot('TotalCharges')","5fb9efbf":"# Calculate differene between Totalcharge and Tenure*MonthlyCharges\ndata['TotalCharge_diff'] = (data['tenure'] * data['MonthlyCharges']) - data['TotalCharges']\ndata['TotalCharge_diff_abs'] = data['TotalCharge_diff'].abs()\n# leaving both as a possible good features, from logic of the thing, I suppose only TotalCharges_diff will be of any use","c2a5d7ce":"# plot\nplt.figure(figsize=(14, 4))\nplt.title(\"KDE for {}\".format('TotalCharge_diff'))\nax0 = sns.histplot(data[data['Churn'] == 'No']['TotalCharge_diff'].dropna(), color = \"#22ff57\", label= 'Churn: No')\nax1 = sns.histplot(data[data['Churn'] == 'Yes']['TotalCharge_diff'].dropna(), color= \"#FF5722\", label= 'Churn: Yes')\nplt.legend(prop={'size': 12})\n","080cf893":"#kde_plot('TotalCharge_diff')\n#kde_plot('TotalCharge_diff_abs')","bda858b1":"# create copy for thingama-jigging with categorical vars\ndf=data","f14fff6b":"# borrowed fcn for plotting nice barplots\ndef barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n    g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]\/len(df)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()","0ca37efb":"# borrowed fcn for plotting pie plots with percentages of each category based on rule\ndef plot_var_percentages (df, var_list):\n\n    n_rows = math.ceil(len(var_list)\/3)\n    mapper = []\n    count_c = 0\n    count_r = 0\n    for n in range(len(var_list)):\n        if count_c <= 2:\n            mapper.append((count_r,count_c))\n            count_c += 1\n        else:\n            count_r += 1\n            count_c = 0\n            \n    #fig, axes = plt.subplots(nrows = n_rows,ncols = 3,figsize = (15,12))\n    for i,var in enumerate(var_list):\n        \n        labels = list(df[var].value_counts().index)\n        counts = list(df[var].value_counts())\n        \n        plt.figure(i)\n        plt.pie(counts, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n        plt.title(var)\n    plt.show ","2b50d530":"# Brief look at data distribution across categories\nvar_list = data.columns[1:-5].drop(['tenure'])\n\n#plot_var_percentages(data, var_list)","9b6bcd51":"#print(var_list)\n#print(data.columns)","eff8748c":"# Lets start with our seniors\n#barplot_percentages(\"SeniorCitizen\")","0c9d6a94":"df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n#g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=.9)\n#ax = g.map(sns.barplot, \"gender\", \"churn_rate\", palette = \"Blues_d\", order= ['Female', 'Male'])","4f4fd351":"#churn rates across customers w\/ partners and dependents\n\nfig, axis = plt.subplots(1, 2, figsize=(12,4))\naxis[0].set_title(\"Has partner\")\naxis[1].set_title(\"Has dependents\")\naxis_y = \"percentage of customers\"\n# Plot Partner column\ngp_partner = df.groupby('Partner')[\"Churn\"].value_counts()\/len(df)\ngp_partner = gp_partner.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\nax = sns.barplot(x='Partner', y= axis_y, hue='Churn', data=gp_partner, ax=axis[0])\n# Plot Dependents column\ngp_dep = df.groupby('Dependents')[\"Churn\"].value_counts()\/len(df)\ngp_dep = gp_dep.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\nax = sns.barplot(x='Dependents', y= axis_y, hue='Churn', data=gp_dep, ax=axis[1])","a936b450":"# Categorical vars - MultipleLines\n#plt.figure(figsize=(9, 4.5))\n#barplot_percentages(\"MultipleLines\", orient='h')","df15a651":"# What internet service the customer has? \n#plt.figure(figsize=(9, 4.5))\n#barplot_percentages(\"InternetService\", orient=\"h\")","b413daa2":"plt.figure(figsize=(15, 15))\nsome_vars = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling']\ni=1\nfor var in some_vars:\n    plt.subplot(3, 2, i)\n    sns.countplot(x=var,data=data, hue='Churn')\n    i+=1","01ab3996":"# Now, how about other services that customers use? \nother_services = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n\n#for var in other_services:\n#    ax1 = sns.catplot(x=var, kind=\"count\", hue=\"Churn\", data=data)","b35207e9":"# And how about contract information? \ncontract_info = ['Contract','PaymentMethod','PaperlessBilling']\n#for var in contract_info:\n#    ax1 = sns.catplot(x=var, kind=\"count\", hue=\"Churn\", data=data)","629977f3":"# Show what we get here. Again.\nresumetable(df)","c87e1971":"non_dummy_cols = ['customerID','tenure','MonthlyCharges','TotalCharges','Churn','churn_rate','TotalCharge_diff','TotalCharge_diff_abs'] \ndummy_cols = list(set(df.columns) - set(non_dummy_cols))\ndf_test = pd.get_dummies(df, columns=dummy_cols)\n\n# non_dummy_cols = ['A','B','C'] \n# Takes all 47 other columns\n# dummy_cols = list(set(df.columns) - set(non_dummy_cols))\n# df = pd.get_dummies(df, columns=dummy_cols)\n","b24f4d2d":"resumetable(df_test)","c783b830":"df_dummie = pd.concat([df['Contract'], df['PaperlessBilling'], df['PaymentMethod'], df['OnlineSecurity'], df['OnlineBackup'], df['DeviceProtection'], df['TechSupport'], \n                       df['StreamingTV'], df['StreamingMovies'], df['MultipleLines'], df['InternetService']] ,axis=1,\n                      keys=['Contract', 'PaperlessBilling','PaymentMethod','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n                            'StreamingTV', 'StreamingMovies','MultipleLines','InternetService'])\n\n#'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n#pd.concat([df1['c'], df2['c']], axis=1, keys=['df1', 'df2'])\ndf_dummie.head()\n\ndf_dummies = pd.get_dummies(df_dummie)\n#print(df_dummies.head())","3724d79a":"#drop 1-0 variant, rest will be dropped later\ndf_dummies.drop(['PaperlessBilling_No'],axis=1,inplace=True)\n\n#resumetable(df)\n\ndf_old = df\ndf.drop(['Contract', 'PaperlessBilling','PaymentMethod','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n         'TechSupport', 'StreamingTV', 'StreamingMovies','MultipleLines','InternetService'],axis=1,inplace=True)\n","ad8552c4":"# tenure - create two more categories\ndf['tenure_short'] = np.where(df['tenure']<18, 1, 0)\ndf['tenure_long'] = np.where(df['tenure']>54, 1, 0)\n#df.head()","29fdcb77":"# create cat var for high monthly charges\n#df.loc[:,'high_payer'] = np.where(df['MonthlyCharges'] > 60, 1,0)","4a6b2c6d":"frames = [df, df_dummies]\n\ndf_ready = pd.concat(frames,axis=1)\n#print(df_ready.head())","f5e6363d":"#resumetable(df_ready)","de2e92c9":"# drop NaNs in TotalCharges\ndf_ready = df_ready.dropna()\n\n# drop customerID, as would not be of any help\ndf_ready.drop(['customerID'],axis=1,inplace=True)\n\nresumetable(df_ready)","36055e58":"# Further usage of just \"df\"\ndf = df_ready","cce9b363":"# encode variables (those that need, apart from int and float ones)\nle=LabelEncoder()\n\ncols=df.columns\ncat_cols=df.select_dtypes(exclude=['int','float']).columns\n#print(df.dtypes)\nenc_data=list(cat_cols)\n\n#inspect data for encoding\n#print(enc_data)\n\ndf[enc_data]=df[enc_data].apply(lambda col:le.fit_transform(col))\n#print(df[enc_data].head())\nprint(df.shape)","94a3876b":"df.dtypes","35fc9877":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression","33eaa043":"# drop \"No internet service\" items and others with high correlation. It was nto clear to me what is the meaning, from the correlation it is clear there is no information added by multiple columns\n# this was actually added after looking at Correrlation matrix, but I left it here for the sake of simplicity\n# df_ready.drop(['TechSupport_No internet service','StreamingTV_No internet service','DeviceProtection_No internet service','OnlineBackup_No internet service'],\n#              'OnlineSecurity_No internet service', 'StreamingMovies_No internet service', 'MultipleLines_No phone service'],axis=1,inplace=True)\n              # ,'MultipleLines_No',\n              # 'OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],\n#               axis=1,inplace=True)\n        \n# leaving all for now","04e803e7":"df_CM = df.drop(['Churn','churn_rate'], axis=1)\n\ncorrMatrix = df_CM.corr()\nfig, ax = plt.subplots(figsize=(30,25))\nsns.heatmap(corrMatrix,annot=True, annot_kws={'size':12},cmap=\"GnBu\")\nplt.show();","845cf72a":"#Correlation of \"Churn\" with other variables in 1D:\ndf_CMB = df.drop(['Churn'], axis=1)\nplt.figure(figsize=(15,8))\ndf_CMB.corr()['churn_rate'].sort_values(ascending = False).plot(kind='bar')","e0da9924":"target0 = df['churn_rate'] # for y\nfeatures0 = df.drop(['Churn','churn_rate'], axis=1) # for X","042f9a12":"# To preserve the shape of the dataset (no distortion), data will be min max scaled to values between (0, 1) \n# instead of standard scaled. I tried also StandardScaler, but results were worse since the distribution of data is not gaussian. RobustScaler was similar in performance\nscaler0=MinMaxScaler()\n\nf_scale0 = scaler0.fit_transform(features0)","efbba778":"# create train and test split on scaled data\nX_train0, X_test0, y_train0, y_test0 = train_test_split (f_scale0,target0,test_size=0.2, random_state=252)","265f2b42":"from sklearn.metrics import roc_curve, auc\n\n#Initial Model\n\n# increased the number of iterations here, as the lbfgs solver was not converging in 100 steps\nlogreg0 = LogisticRegression(max_iter=500)\n\n#Probability scores for test set\ny_score0 = logreg0.fit(X_train0, y_train0).decision_function(X_test0)\n#False positive Rate and true positive rate\nfpr0, tpr0, thresholds0 = roc_curve(y_test0, y_score0)\n\n#Visualization for ROC curve\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n\nprint('AUC: {}'.format(auc(fpr0, tpr0)))\nplt.figure(figsize=(10,8))\nlw = 2\nplt.plot(fpr0, tpr0, color='darkorange',\n         lw=lw, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i\/20.0 for i in range(21)])\nplt.xticks([i\/20.0 for i in range(21)])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n#print(y_score0)","7cfe825c":"from sklearn.metrics import confusion_matrix\ny_hat_test0 = logreg0.predict(X_test0)\ncm0=confusion_matrix(y_test0,y_hat_test0)\nconf_matrix0=pd.DataFrame(data=cm0,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix0, annot=True,fmt='d',cmap=\"YlGnBu\")\nprint(y_hat_test0)","4a984056":"from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n#print(precision_recall_fscore_support(y_hat_test0, y_test0))\nprint(classification_report(y_hat_test0, y_test0))\nprint(\"Accuracy: \")\nprint (accuracy_score(y_test0, y_hat_test0))","7dfbb043":"# checking what is the accuracy on Train set. lbfgs solver contains L2 regularization by default\n# print(\"Train Accuracy:\",logreg0.score(X_train0, y_train0))","8fcf0898":"features0.columns.values","e9c91ebe":"# To get the weights of all the variables\nweights = pd.Series(logreg0.coef_[0],\n                 index=features0.columns.values)\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\n# show features negatively affecting churn - based on LR coefficients\nweights.sort_values(ascending = False)[:10].plot(kind='bar')\nplt.subplot(1, 2, 2)\n# show features positively affecting churn - based on LR coefficients\nweights.sort_values(ascending = False)[-10:].plot(kind='bar')\n","e791b16f":"churn_prob = logreg0.predict_proba(X_test0[:10])\nprint(churn_prob)\nprint(y_test0[:10])","59738626":"from sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n\nxgb_params = {'colsample_bytree': 0.7, 'gamma': 0.36, 'learning_rate': 0.07, 'max_depth': 2, 'n_estimators': 195, 'subsample': 0.8}\ngbc_params = {'learning_rate': 0.25, 'max_depth': 17, 'max_features': 23, 'max_leaf_nodes': 86, 'min_samples_leaf': 185, 'n_estimators': 30, 'n_iter_no_change': 5}\n#rf_params = {'max_depth': 14, 'max_features': 11, 'max_leaf_nodes': 57, 'max_samples': 0.6, 'min_samples_leaf': 20, 'n_estimators': 66}\n\nclassifiers = {\n  \n    \"Naive Bayes\"  : GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=7), \n    \"Random Forest\": RandomForestClassifier(max_depth=14, max_features=11, max_leaf_nodes=57, max_samples=0.6, min_samples_leaf=20, n_estimators=66),\n    \"GBC\"          : GradientBoostingClassifier(learning_rate=0.25, max_depth=17, max_features=23, max_leaf_nodes=86, min_samples_leaf=185, n_estimators=30, n_iter_no_change=5),\n    \"ETC\"          : ExtraTreesClassifier(),\n    \"AdaBoost\"     : AdaBoostClassifier(),      \n    \"xgboost\"      : xgb.XGBClassifier(random_state=42,use_label_encoder=False,eval_metric=\"auc\", parameters = xgb_params)\n}","4d726deb":"from time import time\n\nhead = list(classifiers.items())\n\nfor name, classifier in head:\n    start = time()\n    classifier.fit(X_train0, y_train0)\n    train_time = time() - start\n    start = time()\n    score = roc_auc_score(y_test0, classifier.predict_proba(X_test0)[:,1])\n    score_time = time()-start\n    acc = accuracy_score(y_test0, classifier.predict(X_test0))\n    \n    print(\"{:<15}| ROC-AUC score = {:.3f} | time = {:,.3f}s\/{:,.3f}s | Accuracy = {:.3f}\".format(name, score, train_time, score_time, acc))","27733777":"# from scipy.stats import uniform, randint\n# from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n\n# X = X_train0\n# y = y_train0\n\n# xgb_model = xgb.XGBClassifier(use_label_encoder=False,eval_metric=\"auc\")\n# rf_model = RandomForestClassifier()\n\n# params = {\n#     \"colsample_bytree\": uniform(0.3, 0.6),\n#     \"gamma\": uniform(0, 0.5),\n#     \"learning_rate\": uniform(0.01, 0.6), # default 0.1 \n#     \"max_depth\": randint(2, 9), # default 3\n#     \"n_estimators\": randint(100, 200), # default 100\n#     \"subsample\": uniform(0.6, 0.4)\n# }\n\n# params_rf = {\n#     'max_depth': randint(2,50),\n#     'max_leaf_nodes': randint(1,200),\n#     'min_samples_leaf':randint(1,1000),\n#     'n_estimators': randint(2,200),\n#     'max_samples': uniform(0,0.95),\n#     'max_features': randint(5,25)  \n# }\n\n# search = RandomizedSearchCV(rf_model, \n#                             param_distributions=params_rf, \n#                             random_state=42, \n#                             n_iter=100, \n#                             cv=3, \n#                             verbose=1, \n#                             n_jobs=1, \n#                             return_train_score=True)\n\n# search.fit(X, y)","4323172c":"# from scipy.stats import uniform, randint\n# from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n\n# X = X_train0\n# y = y_train0\n\n# params_gbc = {\n#     'max_depth':         randint(2,50),     #\n#     'max_leaf_nodes':    randint(1,200),    #\n#     'min_samples_leaf':  randint(1,200),   #\n#     'n_estimators':      randint(2,200),    #\n#     'max_features':      randint(5,25),     #\n#     'learning_rate':     uniform(0.01,0.95),#\n#     'n_iter_no_change':  randint(5,6),     # \n# }\n\n# gbc_model = GradientBoostingClassifier()\n\n\n# search = RandomizedSearchCV(gbc_model, \n#                             param_distributions=params_gbc, \n#                             random_state=42, \n#                             n_iter=100, \n#                             cv=3, \n#                             verbose=1, \n#                             n_jobs=1, \n#                             return_train_score=True)\n\n# search.fit(X, y)","b44f09db":"# def report_best_scores(results, n_top=3):\n#     for i in range(1, n_top + 1):\n#         candidates = np.flatnonzero(results['rank_test_score'] == i)\n#         for candidate in candidates:\n#             print(\"Model with rank: {0}\".format(i))\n#             print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n#                   results['mean_test_score'][candidate],\n#                   results['std_test_score'][candidate]))\n#             print(\"Parameters: {0}\".format(results['params'][candidate]))\n#             print(\"\")","f8dc40ea":"# report_best_scores(search.cv_results_, 1)","d92c3691":"# Tree based methods comparison","91ded997":"# 3. TotalCharge vs Tenure x MonthlyCharges - Discounts effect\n* TotalCharge should equal MonthlyCharges x Tenure. If not, it is a sign of a given discount or price inrease, that the customer got. \n* That might be a big factor for churning, lets see further","a2f57b16":"On first sight, Yes churns on Tenure and Totalcharges look similar, Tenure x MonthlyCharge should equal TotalCharge, which is not the case (precisely) here. \n* Why is there a difference? \n* Will be explored further.","a881f337":"* so TotalCharge_diff is not correlated with churn, interesting! Gender, as expected, is also not. \n\n* Seems we can leave all features for now, none are so heavily correlated that they should be omited atm. ","ef497949":"# **2. Lets look at histogram of numerical variables: tenure, MonthlyCharge and TotalCharge**\n\nKDE was omitted intentionally for the first few plots, in order not to pollute visually the histograms","4ae83fd4":"# How to apply findings?\n* Accuracy is certainly not high, although giving some hint and maybe better than nothing (in 75\/25 data split having accuracy over 82% is just 7% better.. which is not much). But still increase of roughly 30% compared to not using any model.\n* Since it is easy to get to probabilities outputted by Logistic Regression, we might consider using these probabilities of churn, and maybe combine information about probability of churning with MonthlyCharges and try not to loose most valuable customers\n","b7df88d5":"# ***Telco customer churn predictions*** \nrecommended music for exploring this notebook: \nhttps:\/\/www.youtube.com\/watch?v=t3217H8JppI&ab_channel=AnAmericanComposer\n\nWas used while creating.\n\nSome parts (EDA, hyperparameters tuning) are now commented out, to speed up the execution of notebook. Uncomment with selecting lines and pressinf 'Ctrl' + '\/'\n****","1b124231":"# Next possible steps\n* add better features! even though present features, even while using a simple logistic regression model, ensure accuracy over 81%.\n* try out different models e.g. tree=based, SVM or NNs\n* with different models try grid search for best hyperparameters. For LR it does not make much sense though...\n* with otber models, cross-validation and AUC might be good to compare\n* and the obvious step ... get more data (or synthesize?)\n\n\n* prettify the syntax, add more comments and use more looping for the sake of simplicity. This solutions is just a get-dirty-hands try, and I know it doesnt read as easy as I would liek to... ","a0a4f5f8":"* MultipleLines var seems to be not of much use","23a55647":"* No missing data\n* One line ... one customer\n* Objects should be retyped, TotalCharges checked\n","54c026d0":"* It seems senior citizens like to change operators more often. Might be bored at home and be the only one to answer the cold calls. Or they might just have more time to calculate what services do they need for what price. Or stg else :-)\n* One way or the other, it seems that being a senior citizen goes with significantly higher probability of churn.","2b4067ad":"* Seems we do not have many senior citizen customers (even though we clearly see how are they behaing differently concernign churn)\n* PhoneService - also not many user not using PhoneService. Lets see, if this inbalance will cause trouble further. ","93b3331f":"#  **4. Now lets explore the categorical variables**","88ba003d":"* Churn rate of women\/men is similar accross ages (but is highr for senior citizens)","349ed56b":"Customers w\/o partners and dependents are more likely tu churn. Feeling free. Interesting.","f1dd2831":"# Exploratory data analysis\n\n# 1. Firstly lets look at target variable","fe15e38d":"# Show correlation matrix","e9f632b8":"* This might lead us to giving some vouchers, packages to customers with churn probability e.g >40% or stg like that","2f11ee18":"# Create encoding","22ed9aaa":"# Open questions\n\n* Does a circa 75\/25% churn split match the real life situation of a telco cpy? \n* What metrics would fit best the needs of service provider - is recall more important? Or false negatives? Probably FN connected to revenue generated by customer?","68a6f65d":"* The customers are probarly more satisfied with fiber optic connection than with DSL.\n* This mighe be a good feature!","787352a4":"* Interestingly, not churning customers seems to be following \"fatter\" distribution ends while looking at difference of payment. \n* It seems that customers that were not exposed to change of payment are more prone to churn, while customers that were exposed to change in payment are less prone to churn. \n* **Interestingly the distribution is kind of symetrical on both sides, but, as expected, the fatter end is on the left side, i.e. customers who got a discount compred to their previous pay were less likely to churn**\n* **BUT** we might be exposed to Law of Small Numbers, as the sample nor the effect are as big.","ebbcecb4":"Looking good now. ","dba76de1":"Model with rank: 1\nMean validation score: 0.803 (std: 0.002)\nParameters: {'learning_rate': 0.2453444400260996, 'max_depth': 17, 'max_features': 23, 'max_leaf_nodes': 86, 'min_samples_leaf': 185, 'n_estimators': 30, 'n_iter_no_change': 5}","e92ee77d":"We have circa 75\/25 split between churn and no churn customers. This might or might not match the reality of a telco cpy? It seem to me the real churn rate might be yearly around this value.","4de91549":"# 5. Preprocessing: Data preps, feature adding based on EDA","bf0eb146":"# Drop the Gun (customerID), Johnny!","b1ce102e":"# Import data and explore basic properties","dbb8a7df":"# Logistic Regression model training and feature eval","8d86932a":"# Import libraries, for starters"}}