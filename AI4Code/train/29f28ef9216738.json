{"cell_type":{"359d780e":"code","86ab0c82":"code","d68f5f13":"code","ce459ffc":"code","56cf8814":"code","ba0e52c1":"code","85e41873":"code","4f30c2c2":"code","f8710bb0":"code","110cb958":"code","4890b1dc":"code","4a3d19e5":"code","16d9ea56":"code","0cedfcb7":"code","7b9025ba":"code","fe20d974":"code","8b9bbf19":"code","37f62801":"code","fa8cf6bd":"code","18b467d3":"code","9b573865":"code","ce7c72b4":"code","dda421b4":"code","ae8a7007":"code","d9b0c460":"code","7363b25e":"code","ca359eff":"code","92ad8963":"code","267fd576":"code","ebbebfe3":"code","010f9436":"code","327ab444":"code","646e0d06":"code","04c946f6":"code","29db1e1e":"code","7b123c90":"code","64e55c80":"code","59b8a45c":"code","2d6f1ef4":"code","3879d6b1":"markdown","90320d52":"markdown","fed6409c":"markdown","4c00cb2a":"markdown","ece095a8":"markdown","1528e415":"markdown","0dbceb14":"markdown","882f0974":"markdown","3e12a776":"markdown","cf32caa0":"markdown","586793d7":"markdown","be58db39":"markdown","42ca5ffc":"markdown","96c707a1":"markdown","813f51ee":"markdown","1d530f69":"markdown","bcba4db5":"markdown","4d923bfa":"markdown","90e7ca98":"markdown","1005aff0":"markdown","a08dbadf":"markdown","43f39752":"markdown","e23a2fe7":"markdown","927d1c17":"markdown","3492c1e0":"markdown","f35a1c55":"markdown","195e88e4":"markdown","9c6e24e6":"markdown","914874f3":"markdown","13c3ee63":"markdown","98e9795a":"markdown","00606b87":"markdown","2801b108":"markdown","63cc1195":"markdown","3af34e0e":"markdown","1fcf2bef":"markdown","467cf43f":"markdown","3f927365":"markdown","f2ef7954":"markdown","84a9df98":"markdown","9ec70dce":"markdown","ec0641be":"markdown","6a6de123":"markdown","52e787dc":"markdown","2bb8f52f":"markdown","000148c8":"markdown","6a94fac2":"markdown","3d9edb22":"markdown","6e0a8c45":"markdown","0b60fe58":"markdown"},"source":{"359d780e":"!pip install ..\/input\/sacremoses\/sacremoses-master \n!pip install ..\/input\/transformers\/transformers-master ","86ab0c82":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# classification metric\nfrom scipy.stats import spearmanr\n\n# cross validation\nfrom sklearn.model_selection import KFold\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","d68f5f13":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)","ce459ffc":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","56cf8814":"seed=42\nseed_all(seed)","ba0e52c1":"DATA_ROOT = Path(\"..\/input\/google-quest-challenge\/\")\nMODEL_ROOT = Path(\"..\/input\/distilbertbaseuncased\/\")\n#DATA_ROOT = Path.cwd() \/ 'data'\n#MODEL_ROOT = Path.cwd() \/ 'distilbert'\ntrain = pd.read_csv(DATA_ROOT \/ 'train.csv')\ntest = pd.read_csv(DATA_ROOT \/ 'test.csv')\nsample_sub = pd.read_csv(DATA_ROOT \/ 'sample_submission.csv')\nprint(train.shape,test.shape)","85e41873":"train.head()","4f30c2c2":"labels = list(sample_sub.columns[1:].values)\nfor label in labels: print(label)","f8710bb0":"MODEL_CLASSES = {\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}\nmodel_type = 'distilbert'\nmodel_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","110cb958":"model_class.pretrained_model_archive_map.keys()","4890b1dc":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n        return [CLS] + tokens + [SEP]","4a3d19e5":"transformer_tokenizer = tokenizer_class.from_pretrained(MODEL_ROOT)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","16d9ea56":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)","0cedfcb7":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","7b9025ba":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","fe20d974":"bs = 14","8b9bbf19":"databunch = (TextList.from_df(train, cols=['question_title','question_body','answer'], processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols=labels)\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","37f62801":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","fa8cf6bd":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","18b467d3":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        attention_mask = (input_ids!=0).type(input_ids.type()) # attention_mask for distilBERT\n            \n        logits = self.transformer(input_ids,\n                                attention_mask = attention_mask)[0]   \n        return logits","9b573865":"use_fp16 = False\n\nconfig = config_class.from_pretrained(MODEL_ROOT)\nconfig.num_labels = 30\nconfig.use_bfloat16 = use_fp16","ce7c72b4":"transformer_model = model_class.from_pretrained(str(MODEL_ROOT), config = config)\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","dda421b4":"class AvgSpearman(Callback):\n    \n    def on_epoch_begin(self, **kwargs):\n        self.preds = np.empty( shape=(0, 30) )\n        self.target = np.empty( shape=(0, 30) )\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        self.preds = np.append(self.preds,last_output,axis=0)\n        self.target = np.append(self.target,last_target,axis=0)\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        spearsum = 0\n        for col in range(self.preds.shape[1]):\n            spearsum += spearmanr(self.preds[:,col],self.target[:,col]).correlation\n        res = spearsum \/ (self.preds.shape[1] + 1)\n        return add_metrics(last_metrics, res)","ae8a7007":"from fastai.callbacks import *\nfrom transformers import AdamW\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = lambda input: AdamW(input,correct_bias=False), \n                  metrics=[AvgSpearman()])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Not working in the tutorial\nif use_fp16: learner = learner.to_fp16()\n    \nlearner.save(\"untrained\")","d9b0c460":"print(learner.model);","7363b25e":"num_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')","ca359eff":"list_layers = [learner.model.transformer.distilbert.embeddings,\n               learner.model.transformer.distilbert.transformer.layer[0],\n               learner.model.transformer.distilbert.transformer.layer[1],\n               learner.model.transformer.distilbert.transformer.layer[2],\n               learner.model.transformer.distilbert.transformer.layer[3],\n               learner.model.transformer.distilbert.transformer.layer[4],\n               learner.model.transformer.distilbert.transformer.layer[5],\n               learner.model.transformer.pre_classifier]\n\nlearner.split(list_layers);","92ad8963":"num_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')","267fd576":"seed_all(seed)\nlearner.freeze_to(-1)","ebbebfe3":"learner.lr_find()","010f9436":"learner.recorder.plot(suggestion=True)","327ab444":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","646e0d06":"unfreeze_layers = [-1,-3,-5]\nlearning_rates = [1e-4, 1e-5, 1e-6]\nepochs = [5, 2, 1]\nall_predictions = []\ntrain['is_valid'] = False\n\n# prepare to get a five fold split\nkf = KFold(n_splits=5, random_state=42, shuffle=True)","04c946f6":"for ind, (tr, val) in enumerate(kf.split(train)):\n    \n    # designate the new training and validation set\n    train['is_valid'][tr] = False\n    train['is_valid'][val] = True\n    \n    # create the databunch\n    databunch = (TextList.from_df(train, cols=['question_title','question_body','answer'], \n                                  processor=transformer_processor)\n             .split_from_df(col='is_valid')\n             .label_from_df(cols=labels)\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))\n    \n    # create the learner\n    learner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = lambda input: AdamW(input,correct_bias=False), \n                  metrics=[AvgSpearman()])\n    learner.callbacks.append(ShowGraph(learner))\n    if use_fp16: learner = learner.to_fp16()\n        \n    # load empty weights and split the model into layers\n    learner.load(\"untrained\")\n    learner.split(list_layers);\n    \n    # progressively unfreeze the layers and train\n    for layer in range(0,len(unfreeze_layers)):\n        learner.freeze_to(unfreeze_layers[layer])\n        print('fold:',ind,'freezing to:',unfreeze_layers[layer],' - ',epochs[layer],'epochs')\n        learner.fit_one_cycle(epochs[layer], \n                              max_lr=slice(learning_rates[layer]*0.95**num_groups, learning_rates[layer]),\n                              moms=(0.85, 0.75))\n\n    test_preds = get_preds_as_nparray(DatasetType.Test)\n    all_predictions.append(test_preds)","29db1e1e":"avg_preds = np.mean(all_predictions, axis=0)","7b123c90":"avg_preds.shape","64e55c80":"sample_submission = pd.read_csv(DATA_ROOT \/ 'sample_submission.csv')\nsample_submission[labels] = avg_preds\nsample_submission.to_csv(\"submission.csv\", index=False)","59b8a45c":"test.head()","2d6f1ef4":"sample_submission.head()","3879d6b1":"This statement prints all of the directories in the \/kaggle\/input\/ directory. This can be useful when trying to determine the path of the external datasets. If you are running into weird errors where you can't reach one of you dataset, even though you have attached it, check the output of this cell to double check the path that kaggle has assigned it.","90320d52":"Breifly, this notebook uses a transfer learning approach to aid in the question-answer classification task. The first **transfer learning** method applied to Natural Language Processing (NLP) was [Universal Language Model Fine-tuning for Text Classification](https:\/\/medium.com\/r\/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf).(ULMFiT) method. This method uses language models trained to predict the next word in a sequence, which is then used in a classification taks.[Viedo here.](https:\/\/course.fast.ai\/videos\/?lesson=4).\n\nRecently, a new architecture called the **Transformer** (cf. [Attention is all you need](https:\/\/arxiv.org\/abs\/1706.03762)) has been shown to be powerful. Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) all have pre-trained their own models with architectures based on the transformer. These pretrained models are availiable through the [HuggingFace](https:\/\/huggingface.co\/) \ud83e\udd17 [transformers library](https:\/\/github.com\/huggingface\/transformers). Formerly known as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library has both pre-trained NLP models and additional utilities like tokenizers, optimizers and schedulers. \n\nThis kernel uses the ``transformers`` library within the ``fastai`` framework. Specifically, I am using the [distilBERT model](https:\/\/towardsdatascience.com\/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8). I've broken the process down into different parts:\n\n1. Set Up and Data Loading\n1. Specifying Data Preprocessing\n1. Loading and Processing Data\n1. Creating the Model\n1. Training the Model\n    - With cross validation (V3)\n1. Predictions and Submission","fed6409c":"Now that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer.","4c00cb2a":"A tokenizer takes the text and transforms it into tokens. The ``fastai`` documentation notes that: \n1. The [``TokenizeProcessor`` object](https:\/\/docs.fast.ai\/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) \u2192 List[str]`` that take a text ``t`` and returns the list of its tokens.\n\nTo use the distilBERT tokenizer, we create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function. DistilBERT does not require a space to start the input string, so ``add_prefix_space`` is not used.","ece095a8":"**Kernel Technicality:**\nTo make this run completely within the Kaggle kernels, we need to upload the pretrained configuration, vocabulary and model as a dataset (i.e., MODEL_ROOT below). To create the dataset, I had to create a separate notebook, download the files from the internet, and use `.save_pretrained` to save them in the format HuggingFace was expecting.For example:\n\n    MODEL_ROOT = Path(\"\/wherever\/distilbert-base-uncased\")\n    transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n    transformer_tokenizer.save_pretrained(MODEL_ROOT)\n\n    config = config_class.from_pretrained(pretrained_model_name)\n    config.num_labels = 30\n    config.use_bfloat16 = use_fp16\n    config.save_pretrained(MODEL_ROOT)\n\n    transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n    transformer_model.save_pretrained(MODEL_ROOT)","1528e415":"When using pretrained models, the current data needs to be preprocessed in the same way as the data that trained the model. In ``transformers``, each model architecture is associated with 3 main types of classes:\n* A **model class** to load\/store a particular pre-train model.\n* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n* A **configuration class** to load\/store the configuration of a particular model.\n\nAll these classes share a common class method ``from_pretrained(pretrained_model_name,\u00a0...)`` with all the other huggingface models. The parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model\/tokenizer\/configuration to load, e.g ``'bert-base-uncased'``. For a kaggle competition, we have to load\/save the model in another step. We can find all the shortcut names in the transformers documentation [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html#pretrained-models). Check out the [original tutorial](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta) for other MODEL_CLASSES names. ","0dbceb14":"#### Find an appropriate initial learning rate","882f0974":"# Specifying Data Preprocessing ","3e12a776":"An important part of training is being able to see how the model is performing. In this competition,\n>Submissions are evaluated on the mean column-wise Spearman's correlation coefficient. The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.\n\nAlthough scipy provides an implementation of Spearman's R, we also need to take the average across all of the target columns. Therefore, we need to create our own custom metric. [Others have noted](https:\/\/www.kaggle.com\/c\/google-quest-challenge\/discussion\/120072) that Scipy's implementation Spearman's Rho is kind of slow, and suggest a custom alternative. \n\n- `on_epoch_begin`: create empty numpy arrays to hold the predictions and targets\n- `on_batch_end`: after each block, append the most recent output (predictions) and targets\n- `on_epoch_end`: when the epoch is finished, compute Spearman's R on the columns, and then take the average","cf32caa0":"Thanks for looking through this kernel! I hope that it helps you understand transformers, and how to integrate Huggingface with fastai. \n\nCheck out the original for some other cool architectures:\n[Fastai with HuggingFace \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta)","586793d7":"In fastai, the `Learner` holds the data, model and other parameters, like the optimizer and evaluation metric(s). Since we're using transformers, we want to use an optimizer designed for them: the AdamW optimizer. This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``. To reproduce BertAdam specific behavior, you have to set ``correct_bias = False``.","be58db39":"### Custom evaluation metric","42ca5ffc":"One group won't allow us to unfreeze parts of the model. The tutorial kernel suggested to divide the distilBERT model in 8 blocks:\n* 1 Embedding\n* 6 transformer layers\n* 1 classifier","96c707a1":"### Model Training","813f51ee":"### Fastai Learner\u00a0with Custom Optimizer","1d530f69":"The numericalizer takes the the tokens, and turns them into numbers. The ``fastai`` documentation notes that:\n1. The [``NumericalizeProcessor``  object](https:\/\/docs.fast.ai\/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https:\/\/docs.fast.ai\/text.transform.html#Vocab)\n\nTo use the distilBERT numericalizer, we create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.","bcba4db5":"First, a utility function to making sure the output data us in the right order. As [noted in other tutorials](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) the function ``get_preds`` does not return the test\/validation elements in order by default. Therefore, we will have to resort the test elements into their correct order.","4d923bfa":"A utility function to set the seed for generating random numbers","90e7ca98":"Now that we've created the Learner, we can train the model. During training, we are going to use techniques known to help in other classification tasks: **discriminative layer training**, **gradual unfreezing** and **slanted triangular learning rates**. The kernel tutorial author noted that he didn't find any documentation about influence of these techniques with transformers. I've used them because I hope these techniques are domain general. Anecdotally, I find a benefit to gradual unfreezing with respect to time. Training with more layers frozen takes less time because there are fewer parameters. \n\nTo implement unfreezing, our model needs to be specified into different layer groups. ``fastai`` allows us to \"split\" the structure model into groups, [described here](https:\/\/docs.fast.ai\/basic_train.html#Discriminative-layer-training).\n\nTo see the structure of the distilBERT model, look at the output of the following:","1005aff0":"# Set Up and Data Loading","a08dbadf":"This notebook is based off of [this great tutorial kernel](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta) and accompanying [article](https:\/\/medium.com\/p\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376) Check out the original kernel (and give an upvote!) for lots of other transformer models that can be easily implemented in these notebooks. \n\n\nIf you've seen my [roBERTa kernel](https:\/\/www.kaggle.com\/melissarajaram\/roberta-fastai-huggingface-transformers) this one is going to be quite similar. Here I've chosen to use the distilBERT architecture because it's smaller, and I'll be able to train more of the paramerters of the model within the confines of the kernel. I've kept a lot of the original tutorial's text to explain what the code is doing.\n\n**A major difference in this notebook and the RoBERTa (and previous versions) notebooks is the implementation of cross validation. Instead of training one model, I train 5 different models, and average the output. Due to time contraints, I will not be able to train each model for very long, but I'm hoping the average across all is better. **\n\n## Google Quest Q&A Overview\n\nThis challenge is about questions and answers. \n\nIn [question answering (QA)](https:\/\/en.wikipedia.org\/wiki\/Question_answering) systems are built that automatically answer questions posed by humans in a natural language. These computer systems excel at answering questions with single, verifiable answers. In contrast, humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context.  \n\nFor the [Google QUEST Q&A Labeling competition](https:\/\/www.kaggle.com\/c\/google-quest-challenge\/overview), we're tasked with predicting different subjective aspects of question-answering. The data for this competition includes questions and answers, and the task is to predict target values of 30 labels for each question-answer pair.Target labels with the prefix question_ relate to the question_title and\/or question_body features in the data, and target labels with the prefix answer_ relate to the answer feature.\n\nThis is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Submissions are evaluated with the mean [Spearman's rank correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Spearman%27s_rank_correlation_coefficient).\n","43f39752":"As mentioned [here](https:\/\/github.com\/huggingface\/transformers#models-always-output-tuples), the distilBERT model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits. If we check out the [distilbert classification source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/modeling_distilbert.html#DistilBertForSequenceClassification), we find that the model is only outputting the logits, and so the information we want is in the [0] location. To access the logits, we create a custom model.","e23a2fe7":"The training data. In this kernel, I'll use the `question_title`, `question_body` and `answer` columns.","927d1c17":"# Loading and Processing Data","3492c1e0":"We check the order","f35a1c55":"Check batch and tokenizer. Because the distilBERT tokenizer was used, there are a some special tokens prefaced with `##`.","195e88e4":"# Predictions and Submission","9c6e24e6":"The predicted lables are in the columns of the sample submission. Note that some labels are with respect to the question, and some are with respect to the answer.","914874f3":"# Training Cross Validated Models","13c3ee63":"### Custom processor","98e9795a":"## References\n* [Fastai with HuggingFace \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta)\n* Hugging Face, Transformers GitHub (Nov 2019), [https:\/\/github.com\/huggingface\/transformers](https:\/\/github.com\/huggingface\/transformers)\n* Fast.ai, Fastai documentation (Nov 2019), [https:\/\/docs.fast.ai\/text.html](https:\/\/docs.fast.ai\/text.html)\n* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https:\/\/arxiv.org\/abs\/1801.06146](https:\/\/arxiv.org\/abs\/1801.06146)\n* Keita Kurita's article\u00a0: [A Tutorial to Fine-Tuning BERT with Fast AI](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/)\u00a0(May 2019)\n* Dev Sharma's article\u00a0: [Using RoBERTa with Fastai for NLP](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)","00606b87":"# Implementing distilBERT with fastai and HuggingFace \ud83e\udd17Transformers","2801b108":"### Custom Tokenizer","63cc1195":"Now that we have a custom processor, which contains the custom tokenizer and numericalizer, we can create the `DataBunch`. During the DataBunch creation, we have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding. For RoBERTa, it's usually advised to pad the inputs on the right rather than the left.","3af34e0e":"This kernel uses [the data block API](https:\/\/docs.fast.ai\/data_block.html#The-data-block-API), to create the `DataBunch`. \n\nIn the `DataBunch` creation, I have specified to use the 'question_title','question_body', and 'answer' columns as the training data.","1fcf2bef":"To make the transformer adapted to multiclass classification, we need to specify the number of labels before loading the pre-trained model.","467cf43f":"Check batch and numericalizer :","3f927365":"#### Cross Validated Training with progressive unfreezing","f2ef7954":"Text data is preprocessed through tokenization and numericalization. To match the pretrained models, we need to use the same tokenization and numericalization as the model. Fortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model. In ``fastai``, data pre-processing is performed during the creation of the ``DataBunch``. When creating a `DataBunch`, the tokenizer and numericalizer are passed in the 'processor' argument.\n\nTherefore, the first step is to create a customized tokenize and numericalizer that use the distilBERT transformer tokenizer classes, and then create a custom processor which we can use to create the `DataBunch`. ","84a9df98":"Here, the layers to unfreeze, epochs and learning rates are all specified. I create a new column on the training dataframe to hold which items are in the validation set.","9ec70dce":"This statement tells us the distilBERT models we could select. Here, I'm using the 'distilbert-base-uncased'. I've tried using the 'distilbert-base-uncased-distilled-squad', and had a lot of difficulty getting it to accurately classify. I'm not sure why.","ec0641be":"## Notebook Contents","6a6de123":"# Creating the Model","52e787dc":"Let's check how many layer groups we currently have:","2bb8f52f":"To train the model we will:\n- Find an appropriate initial learning rate\n- Use 5 fold cross validation\n- Progressively unfreeze the layers while training\n\nDuring all training, we use the **Slanted Triangular Learning Rates** with the `.fit_one_cycle` command, described [here](https:\/\/docs.fast.ai\/callbacks.one_cycle.html). Because we have initially created the `DataBunch` with a small batch size, we can use the same `DataBunch` throughout the training process.","000148c8":"Due to randomness, there can be little differences in the learning rate. Based on a few runs on my computer, I've chosen 1e-4 for the starting learning rate. ","6a94fac2":"This kernel uses fastai and Huggingface transformser. fastai is already installed on Kaggle, and [here](https:\/\/www.kaggle.com\/c\/tensorflow2-question-answering\/discussion\/117716) is a discussion post that shows how to get Huggingface installled. In total, we'll need three datasets: `sacremoses`, `transformers`, and `distilbert`. See the note below in the 'kernel technicality' cell on setting up the distilbert dataset.","3d9edb22":"### Custom Numericalizer","6e0a8c45":"Let's check that we now have 8 layer groups:","0b60fe58":"## Using the distilBERT tokenizer and numericalizer within fastai"}}