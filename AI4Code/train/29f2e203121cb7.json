{"cell_type":{"050f6e28":"code","7d886dd5":"code","e50d01da":"code","619659a1":"code","696a9dc2":"code","dfb900e6":"code","cd7ea148":"code","8111d080":"code","2cc45940":"code","a117f4e2":"code","a20062df":"code","b43e5e59":"code","42fac010":"code","9676c94e":"code","7d72f324":"code","bf9d7bfd":"code","de6468bc":"code","e9aa873e":"code","b2560898":"code","dd921dcb":"code","17068a23":"code","d3f0f84d":"code","63375798":"code","2ae6fe43":"code","fef95fa2":"code","9c0db91c":"code","99f63db3":"code","1ac0b125":"code","a4f2cf6e":"code","7f6a9f7b":"code","a0cbcbbb":"code","bcaf020e":"code","9f1380dc":"code","af827775":"code","a6d3da95":"code","d43daa8a":"code","2d79d6d6":"code","5d29bf1c":"code","a52203ad":"code","9d764104":"code","d9e313f7":"code","0f5bdde9":"markdown","da9186c0":"markdown","3b3ab92d":"markdown","3e5343ae":"markdown","e7c691f3":"markdown","2e77dbae":"markdown"},"source":{"050f6e28":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\n\nimport cv2\nimport sys\nimport math\n\nfrom timeit import default_timer as timer\nfrom datetime import datetime\nfrom numba import cuda","7d886dd5":"sys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","e50d01da":"_det_model_path = \"\/kaggle\/input\/collect-submit-model\/det\/\"\n_classify_model_path = \"\/kaggle\/input\/covidmodels\/Archive\/classify-ep12\/\"\n\n_test_files_path = \"\/kaggle\/input\/covid19512\/test\/\"\n_data_dir = \"\/kaggle\/input\/covid19512\/\"\n\nmeta_df = pd.read_csv(_data_dir + \"meta.csv\")","619659a1":"IMG_SIZE = 512","696a9dc2":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import *\n\nimport collections\nfrom collections import defaultdict\n\nimport timm\nfrom timm.models.efficientnet import *\n\nimport torch.cuda.amp as amp","dfb900e6":"data_dir = _data_dir\nimage_size = IMG_SIZE\n\nstudy_name_to_predict_string = {\n    'Negative for Pneumonia'  :'negative',\n    'Typical Appearance'      :'typical',\n    'Indeterminate Appearance':'indeterminate',\n    'Atypical Appearance'     :'atypical',\n}\n\nstudy_name_to_label = {\n    'Negative for Pneumonia'  :0,\n    'Typical Appearance'      :1,\n    'Indeterminate Appearance':2,\n    'Atypical Appearance'     :3,\n}\nstudy_label_to_name = { v:k for k,v in study_name_to_label.items()}\nnum_study_label = len(study_name_to_label)","cd7ea148":"def make_fold(mode='train-0'):\n    if 'test' in mode:\n        df_meta  = pd.read_csv(data_dir+'meta.csv')\n        df_valid = df_meta[df_meta['split']=='test'].copy()\n\n        for l in study_name_to_label.keys():\n            df_valid.loc[:,l]=0\n        df_valid = df_valid.reset_index(drop=True)\n        return df_valid","8111d080":"class SiimDataset(Dataset):\n    def __init__(self, df, augment=None):\n        super().__init__()\n        self.df = df\n        self.augment = augment\n        self.length = len(df)\n\n    def __str__(self):\n        string  = ''\n        string += '\\tlen = %d\\n'%len(self)\n        string += '\\tdf  = %s\\n'%str(self.df.shape)\n\n        string += '\\tlabel distribution\\n'\n        for i in range(num_study_label):\n            n = self.df[study_label_to_name[i]].sum()\n            string += '\\t\\t %d %26s: %5d (%0.4f)\\n'%(i, study_label_to_name[i], n, n\/len(self.df) )\n        return string\n\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n\n        image_file = data_dir + '\/test\/%s.png' % (d.image_id)\n        image = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n        onehot = d[study_name_to_label.keys()].values\n\n        mask = np.zeros_like(image)\n\n\n        r = {\n            'index' : index,\n            'd' : d,\n            'image' : image,\n            'mask' : mask,\n            'onehot' : onehot,\n        }\n        if self.augment is not None: r = self.augment(r)\n        return r\n\ndef null_collate(batch):\n    collate = defaultdict(list)\n\n    for r in batch:\n        for k, v in r.items():\n            collate[k].append(v)\n\n    # ---\n    batch_size = len(batch)\n    onehot = np.ascontiguousarray(np.stack(collate['onehot'])).astype(np.float32)\n    collate['onehot'] = torch.from_numpy(onehot)\n\n    image = np.stack(collate['image'])\n    image = image.reshape(batch_size, 1, image_size,image_size).repeat(3,1)\n    image = np.ascontiguousarray(image)\n    image = image.astype(np.float32) \/ 255\n    collate['image'] = torch.from_numpy(image)\n\n\n    mask = np.stack(collate['mask'])\n    mask = mask.reshape(batch_size, 1, image_size,image_size)\n    mask = np.ascontiguousarray(mask)\n    mask = mask.astype(np.float32) \/ 255\n    collate['mask'] = torch.from_numpy(mask)\n\n    return collate","2cc45940":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        e = efficientnet_b3a(pretrained=False, drop_rate=0.3, drop_path_rate=0.2)\n        self.b0 = nn.Sequential(\n            e.conv_stem,\n            e.bn1,\n            e.act1,\n        )\n        self.b1 = e.blocks[0]\n        self.b2 = e.blocks[1]\n        self.b3 = e.blocks[2]\n        self.b4 = e.blocks[3]\n        self.b5 = e.blocks[4]\n        self.b6 = e.blocks[5]\n        self.b7 = e.blocks[6]\n        self.b8 = nn.Sequential(\n            e.conv_head, #384, 1536\n            e.bn2,\n            e.act2,\n        )\n\n        self.logit = nn.Linear(1536,num_study_label)\n        self.mask = nn.Sequential(\n            nn.Conv2d(136, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )\n\n\n    # @torch.cuda.amp.autocast()\n    def forward(self, image):\n        batch_size = len(image)\n        x = 2*image-1     # ; print('input ',   x.shape)\n\n        x = self.b0(x) #; print (x.shape)  # torch.Size([2, 40, 256, 256])\n        x = self.b1(x) #; print (x.shape)  # torch.Size([2, 24, 256, 256])\n        x = self.b2(x) #; print (x.shape)  # torch.Size([2, 32, 128, 128])\n        x = self.b3(x) #; print (x.shape)  # torch.Size([2, 48, 64, 64])\n        x = self.b4(x) #; print (x.shape)  # torch.Size([2, 96, 32, 32])\n        x = self.b5(x) #; print (x.shape)  # torch.Size([2, 136, 32, 32])\n        #------------\n        mask = self.mask(x)\n        #-------------\n        x = self.b6(x) #; print (x.shape)  # torch.Size([2, 232, 16, 16])\n        x = self.b7(x) #; print (x.shape)  # torch.Size([2, 384, 16, 16])\n        x = self.b8(x) #; print (x.shape)  # torch.Size([2, 1536, 16, 16])\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        #x = F.dropout(x, 0.5, training=self.training)\n        logit = self.logit(x)\n        return logit, mask","a117f4e2":"def probability_to_df_study(df_valid, probability):\n    df_study = pd.DataFrame()\n    df_image = df_valid.copy()\n    df_study.loc[:,'id'] = df_valid.study + '_study'\n    for i in range(num_study_label):\n        df_study.loc[:,study_name_to_predict_string[study_label_to_name[i]]]=probability[:,i]\n        df_image.loc[:,study_name_to_predict_string[study_label_to_name[i]]]=probability[:,i]\n    \n    \n    df_study = df_study.groupby('id', as_index=False).mean()\n    df_study.loc[:, 'PredictionString'] = \\\n           'negative '      + df_study.negative.apply(lambda x: '%0.6f'%x)      + ' 0 0 1 1' \\\n        + ' typical '       + df_study.typical.apply(lambda x: '%0.6f'%x)       + ' 0 0 1 1' \\\n        + ' indeterminate ' + df_study.indeterminate.apply(lambda x: '%0.6f'%x) + ' 0 0 1 1' \\\n        + ' atypical '      + df_study.atypical.apply(lambda x: '%0.6f'%x)      + ' 0 0 1 1'\n\n    df_study = df_study[['id','PredictionString']]\n    return df_study, df_image","a20062df":"def do_predict(net, valid_loader, tta=['flip','scale']): #flip\n\n    valid_probability = []\n    valid_num = 0\n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        batch_size = len(batch['index'])\n        image  = batch['image'].cuda()\n        onehot = batch['onehot']\n        label  = onehot.argmax(-1)\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(F.softmax(logit,-1))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(F.softmax(logit,-1))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                logit, mask = net(F.interpolate(image, scale_factor=1.33, mode='bilinear', align_corners=False))\n                probability.append(F.softmax(logit,-1))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        valid_num += batch_size\n        valid_probability.append(probability.data.cpu().numpy())\n        print('\\r %8d \/ %d  %s' % (valid_num, len(valid_loader.dataset), time_to_str(timer() - start_timer, 'sec')),\n              end='', flush=True)\n\n    assert(valid_num == len(valid_loader.dataset))\n    print('')\n\n    probability = np.concatenate(valid_probability)\n    return probability","b43e5e59":"class Logger(object):\n    def __init__(self):\n        self.terminal = sys.stdout  #stdout\n        self.file = None\n\n    def open(self, file, mode=None):\n        if mode is None: mode ='w'\n        self.file = open(file, mode)\n\n    def write(self, message, is_terminal=1, is_file=1 ):\n        if '\\r' in message: is_file=0\n\n        if is_terminal == 1:\n            self.terminal.write(message)\n            self.terminal.flush()\n            #time.sleep(1)\n\n        if is_file == 1:\n            self.file.write(message)\n            self.file.flush()\n\n    def flush(self):\n        # this flush method is needed for python 3 compatibility.\n        # this handles the flush command by doing nothing.\n        # you might want to specify some extra behavior here.\n        pass","42fac010":"def time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)\/60\n        hr = t\/\/60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n\n    elif mode=='sec':\n        t   = int(t)\n        min = t\/\/60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n\n    else:\n        raise NotImplementedError","9676c94e":"def run_submit():\n    for fold in [0 ,1 ,2, 3, 4]:\n        out_dir = '.\/study_predict\/'\n        initial_checkpoint = \\\n            _classify_model_path + 'f' + str(fold) + '.pth'\n        ## setup  ----------------------------------------\n        #mode = 'local'\n        mode = 'remote'\n\n        submit_dir = out_dir + '%s-fold%d'%(mode, fold)\n        os.makedirs(submit_dir, exist_ok=True)\n\n        log = Logger()\n        log.open(out_dir + 'log.submit.txt', mode='a')\n        log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n        #log.write('\\t%s\\n' % COMMON_STRING)\n        log.write('\\n')\n\n        #\n        ## dataset ------------------------------------\n        \n        df_valid = make_fold('test')\n\n        valid_dataset = SiimDataset(df_valid)\n        valid_loader  = DataLoader(\n            valid_dataset,\n            sampler = SequentialSampler(valid_dataset),\n            batch_size  = 32,#128, #\n            drop_last   = False,\n            num_workers = 8,\n            pin_memory  = True,\n            collate_fn  = null_collate,\n        )\n        log.write('mode : %s\\n'%(mode))\n        log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n\n        ## net ----------------------------------------\n        if 1:\n            net = Net().cuda()\n            net.load_state_dict(torch.load(initial_checkpoint)['state_dict'], strict=True)\n\n            #---\n            start_timer = timer()\n            probability = do_predict(net, valid_loader)\n            log.write('time %s \\n' % time_to_str(timer() - start_timer, 'min'))\n            log.write('probability %s \\n' % str(probability.shape))\n\n            np.save(submit_dir + '\/probability.npy',probability)\n            df_valid.to_csv(submit_dir + '\/df_valid.csv', index=False)\n\n        else:\n            probability = np.load(submit_dir + '\/probability.npy')\n\n        #----\n        df_study, df_image = probability_to_df_study(df_valid, probability)\n        \n        #df_image = probability_to_df_image(df_valid, None, None)\n        #df_submit = pd.concat([df_study,df_image])\n        df_submit = pd.concat([df_study])\n        df_submit.to_csv(submit_dir + '\/submit.csv', index=False)\n\n        log.write('submit_dir : %s\\n' % (submit_dir))\n        log.write('initial_checkpoint : %s\\n' % (initial_checkpoint))\n        log.write('df_submit : %s\\n' % str(df_submit.shape))\n        log.write('%s\\n' % str(df_submit))\n        log.write('\\n')\n        ","7d72f324":"IDENTIFIER   = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\nrun_submit()","bf9d7bfd":"#df_valid = make_fold('test')\n#df_valid","de6468bc":"def run_remote_ensemble():\n    out_dir = '.\/study_predict\/'\n    log = Logger()\n    log.open(out_dir + 'log.submit.txt', mode='a')\n    log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n    #log.write('\\t%s\\n' % COMMON_STRING)\n    log.write('\\n')\n\n\n    submit_dir=[\n        out_dir+'remote-fold0',\n        out_dir+'remote-fold1',\n        out_dir+'remote-fold2',\n        out_dir+'remote-fold3',\n        out_dir+'remote-fold4',\n    ]\n\n    probability=0\n    for d in submit_dir:\n        p = np.load(d + '\/probability.npy')\n        probability += p**0.5\n    probability = probability\/len(submit_dir)\n\n\n    #----\n    df_valid = pd.read_csv(submit_dir[1] + '\/df_valid.csv')\n\n    df_study, df_image  = probability_to_df_study(df_valid, probability)\n    #df_image  = probability_to_df_image(df_valid, None, None)\n    #df_submit = pd.concat([df_study, df_image])\n    df_submit = pd.concat([df_study])\n    \n    #df_submit.to_csv(out_dir + '\/effb3-full-512-mask-submit-ensemble1.csv', index=False)\n\n    log.write('submit_dir : %s\\n' % (submit_dir))\n    log.write('df_submit : %s\\n' % str(df_submit.shape))\n    log.write('%s\\n' % str(df_submit))\n    log.write('\\n')\n    \n    return df_submit, df_image","e9aa873e":"IDENTIFIER   = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\npredict_study, df_image = run_remote_ensemble()","b2560898":"cuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","dd921dcb":"#!pip install \/kaggle\/input\/kerasapplications -q","17068a23":"#!pip install \/kaggle\/input\/efficientnet-keras-source-code\/ -q --no-deps","d3f0f84d":"#import efficientnet.tfkeras as efn","63375798":"\"\"\"\nimport tensorflow as tf\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n\n    return strategy\n\n\ndef build_decoder(with_labels=True, target_size=(300, 300), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n\n    def decode_with_labels(path, label):\n        return decode(path), label\n\n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        return img\n\n    def augment_with_labels(img, label):\n        return augment(img), label\n\n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n\n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n\n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n\n    return dset\n\n#COMPETITION_NAME = \"siim-cov19-test-img512-study-600\"\nstrategy = auto_select_accelerator()\nBATCH_SIZE = strategy.num_replicas_in_sync * 16\n\nIMSIZE = (224, 240, 260, 300, 380, 456, 528, 600, 512)\n\"\"\"","2ae6fe43":"submit_df = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')","fef95fa2":"df_2class = meta_df.loc[meta_df.split == \"test\"].copy()","9c0db91c":"#test_paths = f'\/kaggle\/input\/covid19512\/test\/' + df_2class['image_id'] +'.png'","99f63db3":"df_2class['none'] = 0","1ac0b125":"#label_cols = df_2class.columns[5]\n#label_cols","a4f2cf6e":"# \u6ce8\u610f\uff01\uff01\uff01\uff01\uff01\uff01 \u8fd9\u4e2a\u6a21\u578b\u8bad\u7ec3\u65f6 \u6ca1\u6709 drop duplicate\n\"\"\"\ntest_decoder = build_decoder(with_labels=False, target_size=(IMG_SIZE, IMG_SIZE), ext='png')\ndtest = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=False, cache=False,\n    decode_fn=test_decoder\n)\n\nwith strategy.scope():\n    \n    models = []\n    \n    models0 = tf.keras.models.load_model(\n        '..\/input\/siim-covid19-efnb7-train-fold0-5-2class\/model0.h5'\n    )\n    models1 = tf.keras.models.load_model(\n        '..\/input\/siim-covid19-efnb7-train-fold0-5-2class\/model1.h5'\n    )\n    models2 = tf.keras.models.load_model(\n        '..\/input\/siim-covid19-efnb7-train-fold0-5-2class\/model2.h5'\n    )\n    models3 = tf.keras.models.load_model(\n        '..\/input\/siim-covid19-efnb7-train-fold0-5-2class\/model3.h5'\n    )\n    models4 = tf.keras.models.load_model(\n        '..\/input\/siim-covid19-efnb7-train-fold0-5-2class\/model4.h5'\n    )\n    \n    models.append(models0)\n    models.append(models1)\n    models.append(models2)\n    models.append(models3)\n    models.append(models4)\n\n       \ndf_2class[label_cols] = sum([model.predict(dtest, verbose=1) for model in models]) \/ len(models)\ndf_2class = df_2class.reset_index(drop=True)\n\"\"\"","7f6a9f7b":"df_2class['none'] = df_image['negative'].values\ndf_2class.head()","a0cbcbbb":"#cuda.select_device(0)\n#cuda.close()\n#cuda.select_device(0)","bcaf020e":"shutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5')","9f1380dc":"MODEL_PATH = _det_model_path + \"yolom-f0.pt\" + \" \" + _det_model_path + \"yolom-f1.pt\" + \" \" + _det_model_path + \"yolom-f2.pt\" + \" \" + _det_model_path + \"yolom-f3.pt\" + \" \" + _det_model_path + \"yolom-f4.pt\"","af827775":"!python detect.py --weights {MODEL_PATH} \\\n                  --source {_test_files_path} \\\n                  --img {IMG_SIZE} \\\n                  --conf 0.001 \\\n                  --iou-thres 0.5 \\\n                  --save-txt \\\n                  --save-conf\n","a6d3da95":"def yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\n\nimage_ids = []\nPredictionStrings = []\n\n#for file_path in tqdm(glob('runs\/detect\/exp\/labels\/*.txt')):\nfor dir_path, _, filenames in os.walk(_test_files_path):\n        print(len(filenames))\nfor file in filenames:\n    file_path = 'runs\/detect\/exp\/labels\/' + file.replace(\".png\", '.txt')\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = meta_df.loc[meta_df.image_id == image_id,['dim1', 'dim0']].values[0]\n    if not os.path.exists(file_path):\n        bboxes = \"none 1 0 0 1 1\"\n    else:\n        f = open(file_path, 'r')\n        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n        data = data[:, [0, 5, 1, 2, 3, 4]]\n        bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n        for idx in range(len(bboxes)):\n            bboxes[idx] = str(int(float(bboxes[idx]))) if idx%6!=1 else bboxes[idx]\n        bboxes = ' '.join(bboxes)\n    image_id += \"_image\"\n    image_ids.append(image_id)\n    PredictionStrings.append(bboxes)\n\npredict_image = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})","d43daa8a":"for i in range(predict_image.shape[0]):\n    if predict_image.loc[i,'PredictionString'] == \"none 1 0 0 1 1\":\n        continue\n    sub_df_split = predict_image.loc[i,'PredictionString'].split()\n    sub_df_list = []\n    for j in range(int(len(sub_df_split) \/ 6)):\n        sub_df_list.append('opacity')\n        sub_df_list.append(sub_df_split[6 * j + 1])\n        sub_df_list.append(sub_df_split[6 * j + 2])\n        sub_df_list.append(sub_df_split[6 * j + 3])\n        sub_df_list.append(sub_df_split[6 * j + 4])\n        sub_df_list.append(sub_df_split[6 * j + 5])\n    predict_image.loc[i,'PredictionString'] = ' '.join(sub_df_list)","2d79d6d6":"for i in range(predict_image.shape[0]):\n    if predict_image.loc[i,'PredictionString'] != 'none 1 0 0 1 1':\n        _none = str(df_2class.loc[df_2class.image_id + \"_image\" == predict_image.iloc[i].id]['none'].item())\n        predict_image.loc[i,'PredictionString'] = predict_image.loc[i,'PredictionString'] + ' none ' + _none + ' 0 0 1 1'#","5d29bf1c":"for index, row in predict_study.iterrows():\n    submit_df.loc[submit_df.id == row.id, \"PredictionString\"] = row.PredictionString\n    \nfor index, row in predict_image.iterrows():\n    submit_df.loc[submit_df.id == row.id, \"PredictionString\"] = row.PredictionString","a52203ad":"submit_df.to_csv('\/kaggle\/working\/submission.csv',index = False)  ","9d764104":"\"\"\"\nsample = submit_df.iloc[-1]\nprint(sample.id, sample.PredictionString)\n\n_study = meta_df.loc[meta_df.image_id + \"_image\" == sample.id].study.item()\nprint(_study)\n_study_item = submit_df.loc[submit_df.id == _study + \"_study\"]\n_study_item\n\"\"\"","d9e313f7":"shutil.rmtree('\/kaggle\/working\/yolov5')\nshutil.rmtree('\/kaggle\/working\/study_predict')","0f5bdde9":"# Warrning \n1. This notebook only include public testset for faster testif you submit it you will get 0 score\n2. Change wandb API KEY in training notebook to yours\n3. colab free GPU is about 3x faster than kaggle","da9186c0":"# Detect","3b3ab92d":"# Approach and Refferences\n\n> efficientnetb3a with aux loss for study + efficientnetb5 for 2class + yolov5m for image \n\n## Refferences\n\n1. henhttps:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/246586gk's aux loss https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/240233\n2. alien's 2 class tricks https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/246586\n3. darian's duplicate analysis https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/240878\n\n## Training notebook\n\n1. drop duplicate and create mask https:\/\/www.kaggle.com\/drzhuzhe\/siiim-covid-stratified-k-fold-and-create-mask\n\n2. training study level https:\/\/www.kaggle.com\/drzhuzhe\/covid19-classify efficientnetb3a 1e-3 10 ep + 1e-4 5ep CV map*0.66 score (3.76 + 3.92 + 3.85 + 3.7 + 3.6)\/5 avg 3.766\n\n3. training yolo https:\/\/www.kaggle.com\/drzhuzhe\/covid19-det?scriptVersionId=67605495 efficientnetb5 only 10 epoch  CV map score (0.4947 + 0.5103 + 0.4848 +0.4692 +0.5198)\/5 avg 0.49575 \n\n4. 2 class https:\/\/www.kaggle.com\/drzhuzhe\/siim-covid19-efnb7-train-fold0-5-2class  yolov5m 15 epoch (0.869 + 0.860 + 0.882 + 0.878 + 0.876)\/5 avg 0.872\n\n## Experiments\n\n1. study level with 6 class is doable single fold LB 0.586 + \n2. efficientnetV2-m got low result https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/248442 according to this disscus efficientnetV2 may need larger batchsize\n3. aux CNN head attach to block 4 get mediocre result\n","3e5343ae":"# Clssification","e7c691f3":"# Submit","2e77dbae":"# Predict 2class"}}