{"cell_type":{"d6fd0b81":"code","510b9d67":"code","8a0b40cb":"code","f404edaf":"code","f1657be8":"code","f120912e":"code","457d74e0":"code","058b5937":"code","f1e0c68f":"code","aee332b0":"code","fda8c0a5":"code","1e7e344e":"code","b8b7c54e":"code","2a17eaf6":"code","8588f556":"code","69f01542":"code","b16b7e5c":"code","3517d52c":"code","83a45158":"code","2515a206":"code","71a470c8":"code","45ee73e2":"code","37191463":"code","22c8284d":"code","eff3bf4d":"code","b2a1c35f":"code","c24102f4":"code","8ce297d9":"code","4d997207":"code","583abd8c":"code","105fafde":"code","763f4a5d":"code","82412a31":"code","30126ea2":"code","2ad25246":"code","8fdc59df":"code","171018c4":"code","313fe64d":"code","e7754e2a":"code","d4a59acd":"code","cce04720":"code","4d818f83":"code","9dfff2a7":"code","aad97041":"code","4a9f9248":"code","43b2337e":"code","979296c4":"code","4929f2e1":"code","3a0e8ffb":"code","9ef06642":"code","f602fdcd":"code","21ef304c":"code","b1d94591":"code","b2ceccd7":"code","bef3b1d3":"code","b0b30035":"code","5c9f91d3":"code","56da3630":"code","51e16eb0":"code","fd6c8ed0":"code","8439a701":"code","8f7b13d8":"code","e36e5383":"code","338bc6c6":"code","53d15b8f":"code","9f1d2f08":"code","8d08e2bb":"code","d9a0d78b":"code","2c9129eb":"code","474a251f":"code","32ffc26f":"code","c661b085":"code","fd3bacd1":"code","98f00c50":"code","3b7e504c":"code","779ef86c":"code","5d167181":"code","de8c5bae":"code","b85245cb":"code","cf4a0a1e":"code","0468aa08":"code","e3e328a2":"code","00fee10d":"code","4b69d428":"code","122a1e09":"code","cc3c36a1":"code","cde1f82b":"code","0f95ea72":"code","27a646b8":"code","0de70f34":"code","d1fa40a5":"code","4228d2f5":"code","440b4eb7":"markdown","2c9c9fab":"markdown","36d4f6ad":"markdown","de8b26a9":"markdown","3de2ef29":"markdown","88cff25c":"markdown","f08d9977":"markdown","64078f27":"markdown","3a75258a":"markdown","310f5313":"markdown","4dca0e6f":"markdown","e9993c08":"markdown","b8174a67":"markdown","7e7fa900":"markdown","c1bcef07":"markdown","41d6d270":"markdown","6c5be5ed":"markdown","713aff7f":"markdown","d2353af5":"markdown","bda987da":"markdown","da98488a":"markdown","bdce3455":"markdown","7b8ea34a":"markdown","46546025":"markdown","26a7a128":"markdown","cace94ac":"markdown","a810e44d":"markdown","70aa8731":"markdown","a8220ba2":"markdown","73ba378d":"markdown","7c1af55e":"markdown","216f5f82":"markdown","abe7fb2a":"markdown"},"source":{"d6fd0b81":"import cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras","510b9d67":"import os","8a0b40cb":"path = '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/'","f404edaf":"from keras.preprocessing.image import img_to_array","f1657be8":"def num (image) :\n    \n    val = 0\n    \n    for i in range(len(image)) :\n        if image[i] == '(' :\n            while True :\n                i += 1\n                if image[i] == ')' :\n                    break\n                val = (val*10) + int(image[i])\n            break\n    \n    return val","f120912e":"X_b, y_b = np.zeros((437, 128, 128, 1)), np.zeros((437, 128, 128, 1))\nX_n, y_n = np.zeros((133, 128, 128, 1)), np.zeros((133, 128, 128, 1))\nX_m, y_m = np.zeros((210, 128, 128, 1)), np.zeros((210, 128, 128, 1))","457d74e0":"for i, tumor_type in enumerate(os.listdir(path)) :\n    for image in os.listdir(path+tumor_type+'\/') :\n        p = os.path.join(path+tumor_type, image)\n        img = cv2.imread(p,cv2.IMREAD_GRAYSCALE)           # read image as  grayscale\n        \n        if image[-5] == ')' :\n            \n            img = cv2.resize(img,(128,128))\n            pil_img = Image.fromarray (img)\n            \n            if image[0] == 'b' :\n                X_b[num(image)-1]+= img_to_array(pil_img)  # If image is real add it\n            if image[0] == 'n' :                           # to X as benign , normal\n                X_n[num(image)-1]+= img_to_array(pil_img)  # or malignant.\n            if image[0] == 'm' :\n                X_m[num(image)-1]+= img_to_array(pil_img)\n        else :\n            img = cv2.resize(img,(128,128))\n            pil_img = Image.fromarray (img)\n            \n            if image[0] == 'b' :\n                y_b[num(image)-1]+= img_to_array(pil_img)  # Similarly add the target\n            if image[0] == 'n' :                           # mask to y.\n                y_n[num(image)-1]+= img_to_array(pil_img)\n            if image[0] == 'm' :\n                y_m[num(image)-1]+= img_to_array(pil_img)","058b5937":"plt.figure(figsize = (20,10))\n\nfor i in range(5) :\n    plt.subplot(2,5,i+1)\n    plt.imshow(X_b[i+1], 'gray')\n    plt.title('Real Image')\n    plt.axis('off')\n\nfor i in range(5) :\n    plt.subplot(2,5,i+6)\n    plt.imshow(y_b[i+1], 'gray')\n    plt.title('Mask Image')\n    plt.axis('off')\nplt.show()","f1e0c68f":"X = np.concatenate((X_b, X_n, X_m), axis = 0)\ny = np.concatenate((y_b, y_n, y_m), axis = 0)","aee332b0":"X \/= 255.0\ny \/= 255.0","fda8c0a5":"print(X.shape)\nprint(y.shape)","1e7e344e":"print(X.max())\nprint(X.min())","b8b7c54e":"print(y.max())\nprint(y.min())","2a17eaf6":"y[y > 1.0] = 1.0","8588f556":"print(y.max())\nprint(y.min())","69f01542":"plt.figure(figsize = (10,30))\ni = 0\nwhile i < 16 :\n    \n    x = np.random.randint(0,780)\n    \n    plt.subplot(8,2,i+1)\n    plt.imshow(X[x],'gray')\n    plt.title('Real Image')\n    plt.axis('off')\n    \n    plt.subplot(8,2,i+2)\n    plt.imshow(y[x],'gray')\n    plt.title('Mask Image')\n    plt.axis('off')\n    \n    i += 2\nplt.show()","b16b7e5c":"from sklearn.model_selection import train_test_split","3517d52c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 1)","83a45158":"print(X_train.shape)\nprint(y_train.shape)","2515a206":"print(X_test.shape)\nprint(y_test.shape)","71a470c8":"from keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Dropout\nfrom keras.layers import Concatenate\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Conv2DTranspose\n\nfrom keras import Model","45ee73e2":"inply = Input((128, 128, 1,))\n\nconv1 = Conv2D(2**6, (3,3), activation = 'relu', padding = 'same')(inply)\nconv1 = Conv2D(2**6, (3,3), activation = 'relu', padding = 'same')(conv1)\npool1 = MaxPooling2D((2,2), strides = 2, padding = 'same')(conv1)\ndrop1 = Dropout(0.2)(pool1)\n\nconv2 = Conv2D(2**7, (3,3), activation = 'relu', padding = 'same')(drop1)\nconv2 = Conv2D(2**7, (3,3), activation = 'relu', padding = 'same')(conv2)\npool2 = MaxPooling2D((2,2), strides = 2, padding = 'same')(conv2)\ndrop2 = Dropout(0.2)(pool2)\n\nconv3 = Conv2D(2**8, (3,3), activation = 'relu', padding = 'same')(drop2)\nconv3 = Conv2D(2**8, (3,3), activation = 'relu', padding = 'same')(conv3)\npool3 = MaxPooling2D((2,2), strides = 2, padding = 'same')(conv3)\ndrop3 = Dropout(0.2)(pool3)\n\nconv4 = Conv2D(2**9, (3,3), activation = 'relu', padding = 'same')(drop3)\nconv4 = Conv2D(2**9, (3,3), activation = 'relu', padding = 'same')(conv4)\npool4 = MaxPooling2D((2,2), strides = 2, padding = 'same')(conv4)\ndrop4 = Dropout(0.2)(pool4)","37191463":"convm = Conv2D(2**10, (3,3), activation = 'relu', padding = 'same')(drop4)\nconvm = Conv2D(2**10, (3,3), activation = 'relu', padding = 'same')(convm)","22c8284d":"tran5 = Conv2DTranspose(2**9, (2,2), strides = 2, padding = 'valid', activation = 'relu')(convm)\nconc5 = Concatenate()([tran5, conv4])\nconv5 = Conv2D(2**9, (3,3), activation = 'relu', padding = 'same')(conc5)\nconv5 = Conv2D(2**9, (3,3), activation = 'relu', padding = 'same')(conv5)\ndrop5 = Dropout(0.1)(conv5)\n\ntran6 = Conv2DTranspose(2**8, (2,2), strides = 2, padding = 'valid', activation = 'relu')(drop5)\nconc6 = Concatenate()([tran6, conv3])\nconv6 = Conv2D(2**8, (3,3), activation = 'relu', padding = 'same')(conc6)\nconv6 = Conv2D(2**8, (3,3), activation = 'relu', padding = 'same')(conv6)\ndrop6 = Dropout(0.1)(conv6)\n\ntran7 = Conv2DTranspose(2**7, (2,2), strides = 2, padding = 'valid', activation = 'relu')(drop6)\nconc7 = Concatenate()([tran7, conv2])\nconv7 = Conv2D(2**7, (3,3), activation = 'relu', padding = 'same')(conc7)\nconv7 = Conv2D(2**7, (3,3), activation = 'relu', padding = 'same')(conv7)\ndrop7 = Dropout(0.1)(conv7)\n\ntran8 = Conv2DTranspose(2**6, (2,2), strides = 2, padding = 'valid', activation = 'relu')(drop7)\nconc8 = Concatenate()([tran8, conv1])\nconv8 = Conv2D(2**6, (3,3), activation = 'relu', padding = 'same')(conc8)\nconv8 = Conv2D(2**6, (3,3), activation = 'relu', padding = 'same')(conv8)\ndrop8 = Dropout(0.1)(conv8)","eff3bf4d":"outly = Conv2D(2**0, (1,1), activation = 'relu', padding = 'same')(drop8)\nmodel = Model(inputs = inply, outputs = outly, name = 'U-net')","b2a1c35f":"keras.utils.plot_model(model, '.\/model_plot.png', show_shapes = True)","c24102f4":"from keras.metrics import MeanIoU","8ce297d9":"model.compile(loss = 'mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate = 0.00005))\nprint(model.summary())","4d997207":"from keras.callbacks import ModelCheckpoint","583abd8c":"checkp = ModelCheckpoint('.\/cancer_image_model.h5', monitor = 'val_loss', save_best_only = True, verbose = 1)","105fafde":"history = model.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data = (X_test, y_test), callbacks = [checkp])","763f4a5d":"plt.figure(figsize = (20,7))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training loss', 'validation loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Losses')\nplt.title('Losses vs Epochs', fontsize = 15)","82412a31":"from keras.models import load_model\nmodel = load_model('.\/cancer_image_model.h5')","30126ea2":"y_pred = model.predict(X_test)","2ad25246":"print(y_pred.shape)","8fdc59df":"plt.figure(figsize = (20,80))\n\ni = 0\nx = 0\nwhile i < 45 :\n    \n    plt.subplot(15,3,i+1)\n    plt.imshow(X_test[x], 'gray')\n    plt.title('Real medic Image')\n    plt.axis('off')\n    \n    plt.subplot(15,3,i+2)\n    plt.imshow(y_test[x], 'gray')\n    plt.title('Ground Truth Img')\n    plt.axis('off')\n    \n    plt.subplot(15,3,i+3)\n    plt.imshow(y_pred[x], 'gray')\n    plt.title('Predicited Image')\n    plt.axis('off')\n    \n    x += 1\n    i += 3\nplt.show()","171018c4":"info = [\n    'benign'   ,  # 0\n    'normal'   ,  # 1\n    'malignant',  # 2\n]","313fe64d":"path = '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/'","e7754e2a":"X = []\ny = []\n\nlabel_num = -1\n\nfor label_class in os.listdir(path) :\n    \n    new_path   = path + label_class\n    label_num += 1\n    \n    for img in os.listdir(new_path) :\n        if 'mask' not in img :\n            \n            y.append(label_num)\n            x = cv2.imread(path + label_class +'\/'+img, cv2.IMREAD_GRAYSCALE)\n            X.append(img_to_array(Image.fromarray(cv2.resize(x, (128,128)))))","d4a59acd":"X = np.array(X)\ny = np.array(y)","cce04720":"X\/= 255.0","4d818f83":"from keras.utils import to_categorical","9dfff2a7":"y = to_categorical(y)","aad97041":"print(X.shape)\nprint(y.shape)","4a9f9248":"print(X.min())\nprint(X.max())","43b2337e":"plt.imshow(X[0], 'gray')\nplt.axis('off')","979296c4":"from keras.models import load_model","4929f2e1":"localize = load_model('.\/cancer_image_model.h5')","3a0e8ffb":"M = localize.predict(X)","9ef06642":"print(M.min())\nprint(M.max())\n\nplt.imshow(M[0], 'gray')\nplt.axis('off')","f602fdcd":"import pandas\nimport seaborn","21ef304c":"seaborn.histplot(data = pandas.DataFrame({'id' : [info[p] for p in np.argmax(y, axis = 1)]}), x = 'id')\nplt.title('Distribution of classes accross the entire dataset', fontsize = 15)","b1d94591":"X_train, X_test, y_train, y_test = train_test_split(M, y, test_size = 0.1, shuffle = True, random_state = 1)","b2ceccd7":"print(X_train.shape)\nprint(y_train.shape)","bef3b1d3":"print(X_test.shape)\nprint(y_test.shape)","b0b30035":"from numpy.random import randint","5c9f91d3":"plt.figure(figsize = (20,20))\ni = 0\nSIZE = 702\nwhile i < 25 :\n    \n    x = randint(0, SIZE)\n    plt.subplot(5,5,i+1)\n    plt.imshow(X_train[x], 'gray')\n    plt.title(f'{info[np.argmax(y_train[x])]}', fontsize = 15)\n    plt.axis('off')\n    \n    i += 1\nplt.show()","56da3630":"from keras.preprocessing.image import ImageDataGenerator","51e16eb0":"train_gen = ImageDataGenerator(horizontal_flip = True, rotation_range = 15, width_shift_range = [-10, 10], height_shift_range = [-10, 10], zoom_range = [0.80, 1.00])","fd6c8ed0":"train_gen.fit(X_train)","8439a701":"pointer = train_gen.flow(X_train, y_train)","8f7b13d8":"trainX, trainy = pointer.next()","e36e5383":"plt.figure(figsize = (20,20))\n\ni = 0\n\nwhile i < 25 :\n    \n    plt.subplot(5, 5, i+1)\n    plt.imshow(trainX[i], 'gray')\n    plt.title(f'{info[np.argmax(trainy[i])]}', fontsize = 15)\n    plt.axis('off')\n    \n    i += 1\nplt.show()","338bc6c6":"from keras.layers import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import Adam\nfrom keras.layers import Flatten\nfrom keras.layers import Dense","53d15b8f":"def conv_block (filterx) :\n    \n    model = Sequential()\n    \n    model.add(Conv2D(filterx, (3,3), strides = 1, padding = 'same', kernel_regularizer = 'l2'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.2))\n    model.add(LeakyReLU())\n    \n    model.add(MaxPooling2D())\n    \n    return model\n\ndef dens_block (hiddenx) :\n    \n    model = Sequential()\n    \n    model.add(Dense(hiddenx, kernel_regularizer = 'l2'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.2))\n    model.add(LeakyReLU())\n    \n    return model","9f1d2f08":"def cnn (filter1, filter2, filter3, filter4, hidden1) :\n    \n    model = Sequential([\n        \n        Input((128,128,1,)),\n        conv_block(filter1),\n        conv_block(filter2),\n        conv_block(filter3),\n        conv_block(filter4),\n        Flatten(),\n        dens_block(hidden1),\n        Dense(3, activation = 'softmax')\n    ])\n    \n    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0005), metrics = ['accuracy'])\n    \n    return model","8d08e2bb":"model = cnn(32, 64, 128, 256, 32)\nmodel.summary()","d9a0d78b":"from keras.utils import plot_model","2c9129eb":"plot_model(model, 'cancer_classify.png', show_shapes = True)","474a251f":"checkp = ModelCheckpoint('.\/valid_classifier.h5', monitor = 'val_loss', save_best_only = True, verbose = 1)","32ffc26f":"history = model.fit(train_gen.flow(X_train, y_train, batch_size = 64), epochs = 400, validation_data = (X_test, y_test), callbacks = [checkp])","c661b085":"plt.figure(figsize = (20,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training_loss', 'validation_loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Losses')\nplt.title('Loss val wrt. Epochs', fontsize = 15)","fd3bacd1":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","98f00c50":"model = keras.models.load_model('.\/valid_classifier.h5')","3b7e504c":"y_pred = model.predict(X_test)","779ef86c":"y_pred = np.argmax(y_pred, axis = 1)\ny_test = np.argmax(y_test, axis = 1)","5d167181":"print('Accuracy : ' + str(accuracy_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred, target_names = info))","de8c5bae":"cm = confusion_matrix(y_test,y_pred)","b85245cb":"plt.figure(figsize = (12,12))\nax = seaborn.heatmap(cm, cmap=plt.cm.Greens, annot=True, square=True, xticklabels = info, yticklabels = info)\nax.set_ylabel('Actual', fontsize=40)\nax.set_xlabel('Predicted', fontsize=40)","cf4a0a1e":"image_path = [\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/benign\/benign (110).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/benign\/benign (100).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/benign\/benign (101).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/benign\/benign (107).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/normal\/normal (101).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/normal\/normal (111).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/normal\/normal (106).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/malignant\/malignant (115).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/malignant\/malignant (111).png',\n    '..\/input\/breast-ultrasound-images-dataset\/Dataset_BUSI_with_GT\/malignant\/malignant (110).png',\n]","0468aa08":"from keras.models import load_model","e3e328a2":"classifier = load_model('.\/valid_classifier.h5')\nlocalize = load_model('.\/cancer_image_model.h5')","00fee10d":"testX = []\nfor img in image_path :\n    testX.append(img_to_array(Image.fromarray(cv2.resize(cv2.imread(img, cv2.IMREAD_GRAYSCALE), (128,128)))))","4b69d428":"testX = np.array(testX)\ntestX\/= 255.0","122a1e09":"print(testX.shape)\nprint(f'Minimum : {testX.min()}')\nprint(f'Maximum : {testX.max()}')","cc3c36a1":"plt.imshow(testX[0], 'gray')\nplt.axis('off')","cde1f82b":"predY = localize.predict(testX)","0f95ea72":"print(predY.shape)","27a646b8":"plt.imshow(predY[0], 'gray')\nplt.axis('off')","0de70f34":"print(predY.min())\nprint(predY.max())","d1fa40a5":"pred_label = classifier.predict(predY)","4228d2f5":"print(np.argmax(pred_label, axis = 1))\nplt.figure(figsize = (10,40))\n\ni = 0\nj = 0\nwhile i < 20 :\n    \n    plt.subplot(10,2,i+1)\n    plt.imshow (testX[j], 'gray')\n    plt.title('Original Image', fontsize = 15)\n    plt.axis('off')\n    \n    plt.subplot(10,2,i+2)\n    plt.imshow (predY[j], 'gray')\n    plt.title(f'{info[np.argmax(pred_label[j])]}', fontsize = 15)\n    plt.axis('off')\n    \n    j += 1\n    i += 2\nplt.show()","440b4eb7":"# predict mask and label","2c9c9fab":"Visualize the results to verify the above method","36d4f6ad":"# Data augmentation","de8b26a9":"# Model Performance","3de2ef29":"# Predictions","88cff25c":"### Expanding layer","f08d9977":"### Contracting path","64078f27":"# Data distribution","3a75258a":"# make CNN model","310f5313":"# AIM\n<div style = \"text-align: justify\">Using U-net, localize the area which contains tumor growth <b>(which cannot be easily determined by looking at the actual medical images)<\/b> and compare it against the mask images. Then by looking at the generated mask image, classify whether the tumor growth is <b>malignant, benign or normal.<\/b> Then we must also classify the masks.<\/div>\n\n# Note\n<div style = \"text-align: justify\">Later in the notebook, I have mentioned images taken from medical imaging as <b>real image<\/b> !!!<\/div>\n\n# Dataset [Link](https:\/\/www.kaggle.com\/aryashah2k\/breast-ultrasound-images-dataset)\n\n# Please checkout the [U-net paper](https:\/\/arxiv.org\/pdf\/1505.04597.pdf)","4dca0e6f":"### Why did I take these pixelated masks instead of taking original size ?\n<div style = \"text-align: justify\">I did try to take large image sizes, <b>but due to GPU and RAM constraints<\/b>, my kernel kept on crashing. So I went with smaller sizes. I encourage the reader to try some different sizes where masks are more accurate.<\/div>","e9993c08":"# load images","b8174a67":"### Bottleneck layer","7e7fa900":"# Create datasets for model training and validation","c1bcef07":"# Train test split","41d6d270":"# Model Creation [U-net](https:\/\/towardsdatascience.com\/unet-line-by-line-explanation-9b191c76baf5)\n<img src = \"https:\/\/miro.medium.com\/max\/3600\/1*f7YOaE4TWubwaFF7Z1fzNw.png\"\/>","6c5be5ed":"# Loss function\n\n<div style = \"text-align: justify\">The loss for evaluating the performance of model in semantic segmentation will be <b>IoU (Intersection over Union)<\/b>. It is the ratio of intersection of pixels between predicted and target image over their union. The MeanIoU() method in tf.keras.metrics package can be used.<\/div>","713aff7f":"# load models","d2353af5":"# fit()","bda987da":"# Confusion matrix","da98488a":"# Visualization","bdce3455":"### Basic Architecture\n<div style = \"text-align: justify\">U-net architecture can localize the area of interest. It was first used in Biomedical imaging. The reason it is able to <b>distinguish and localize<\/b> the area is by classifying every pixel in the input image. <b>So the size of input and output images is the same<\/b>. It comprises of two paths - <b>Contracting path and Expanding path<\/b>.<\/div>\n\n### Contract Path\nThe Contracting path has two Convolutional layers and a Maxpooling layer.\n\n### Expansive Path\n<div style = \"text-align: justify\">The Expanding path consists of both transpose Convolutional layer and two Convolutional layers. The corresponding image from contracting path is fed to this layer for precise predictions.<\/div>","7b8ea34a":"**Helper function** to get the index for real image and mask.","46546025":"<div style = \"text-align: justify\"> <b>Take a good look at image 2 and 8<\/b> and think if the masks were not provided, then would it have been easy to know the location tumor. NO !!! This is the aim of U-net model, localize the abnormalities in the image itself. Let's see the implementation.<\/div>","26a7a128":"### Modifications\n<div style = \"text-align: justify\">I kept the padding same so that I can get the mask of exact same dimensions as the actual image. The adam gradient descent was used with a small <b>learning rate of 0.00005<\/b>. Also I am planning to add BatchNormalization which was discovered after U-net. <\/div>","cace94ac":"# train-test split","a810e44d":"# Import images","70aa8731":"# Predictions","a8220ba2":"<div style = \"text-align: justify\">Although this is a imbalanced distribution, a model can easily be developed that does well in classification task. This is beause these images have clear distinctions among them.<\/div>","73ba378d":"# Overall task\n<div style = \"text-align: justify\">Now that the models are complete, we first get the mask for input image and then classify the tumor type <b>benign, malignant or normal<\/b> based on mask shape.<\/div>","7c1af55e":"# Training","216f5f82":"<div style = \"text-align: justify\">Initialize the arrays for benign, normal and malignant tumors, both real and mask. As already given the number of samples in benign, normal & malignant are <b>437, 133 and 210<\/b> respectively.<\/div>","abe7fb2a":"# Classifier"}}