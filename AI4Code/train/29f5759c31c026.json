{"cell_type":{"6c2692f7":"code","9330c667":"code","806223d3":"code","7409fed9":"code","23c08bc4":"code","bf1c9cb3":"code","85e575cd":"code","a5b2f10f":"code","3dfa7921":"code","b4362c1d":"code","2c6c77bc":"code","bfeb0dfb":"code","cf0120aa":"code","23808fd1":"code","96322518":"code","812a12ed":"code","e37e835f":"code","60957453":"code","35d07e6f":"code","6ba718e5":"code","1d692a70":"code","5e5e2e7c":"code","b2346425":"code","4f87d38c":"code","e57eec3b":"code","ff38ac6d":"code","6d0224e4":"code","cf1ee0f3":"code","a1f54458":"code","51428f0f":"code","2b68ee27":"code","aef09f37":"code","c6d62e58":"code","6c5eb21a":"code","0438eeeb":"code","4476a676":"code","0d653be5":"code","0b39e1bb":"code","58b27e6b":"code","d3e92f0c":"code","16d2cf3d":"code","f405c1d6":"code","872fbbe9":"code","d87d2efa":"code","462d84eb":"code","59e2d312":"code","b4097457":"code","0fb88717":"code","13b11016":"code","36da2459":"code","98c86f18":"code","1e5bde3a":"code","9fa6825f":"code","9e120c8e":"code","469c91e4":"code","62b686fd":"code","65f2fff7":"code","a5435e57":"code","f14f292f":"code","d44319dd":"code","605b50c6":"code","34bc8087":"code","47363250":"markdown","e51944bf":"markdown","500528da":"markdown","fd2d55b3":"markdown","6c5e1108":"markdown","cf690cef":"markdown","374d5eda":"markdown","a7b25336":"markdown","ea31a054":"markdown","326da930":"markdown","8332af8b":"markdown","8457f906":"markdown","41d47020":"markdown","4cfab808":"markdown","d536592e":"markdown","67cbe8b1":"markdown","f158b281":"markdown","34e43650":"markdown","dc6c50d2":"markdown","6c73a752":"markdown","b4cc9f09":"markdown","58ffad11":"markdown","823ac285":"markdown","3d531adc":"markdown","b002fb8c":"markdown","72d06377":"markdown","863c5351":"markdown","94e78b96":"markdown","83bb6e11":"markdown","33d596cb":"markdown","37648c0f":"markdown","dca14911":"markdown","1585e411":"markdown","bb780ee4":"markdown","dc3dbbe3":"markdown","8fc0456f":"markdown","d4c1e036":"markdown","d2a03dbc":"markdown","7a5ba257":"markdown","79b3586d":"markdown","65017bd1":"markdown","8ca2657a":"markdown","dc1c4968":"markdown","c6ab42e9":"markdown","3867b20d":"markdown","70349d13":"markdown","12643b1c":"markdown","336be9a6":"markdown","b7876f22":"markdown","140d328f":"markdown","384ea36a":"markdown","56e5f4f8":"markdown","96b6e57d":"markdown","5fae521e":"markdown","843e65e5":"markdown","a19320a3":"markdown","57ba96f5":"markdown"},"source":{"6c2692f7":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier,Pool\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import NuSVR\nfrom scipy.stats import norm\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport glob\nimport sys\nimport os\nimport gc","9330c667":"# for get better result chage fold_n to 5\nfold_n=5\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","806223d3":"print('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","7409fed9":"from sklearn.metrics import roc_auc_score, roc_curve","23c08bc4":"print(os.listdir(\"..\/input\/\"))","bf1c9cb3":"# import Dataset to play with it\ntrain= pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv('..\/input\/test.csv')","85e575cd":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission.head()","a5b2f10f":"train.shape, test.shape, sample_submission.shape","3dfa7921":"train.head(5)","b4362c1d":"#Based on this great kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",df[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",df[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df, NAlist","2c6c77bc":"train, NAlist = reduce_mem_usage(train)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","bfeb0dfb":"test, NAlist = reduce_mem_usage(test)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","cf0120aa":"train.columns","23808fd1":"print(len(train.columns))","96322518":"print(train.info())","812a12ed":"train.describe()","e37e835f":"train['target'].value_counts().plot.bar();","60957453":"f,ax=plt.subplots(1,2,figsize=(20,10))\ntrain[train['target']==0].var_0.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('target= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntrain[train['target']==1].var_0.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('target= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","35d07e6f":"train[train.columns[2:]].mean().plot('hist');plt.title('Mean Frequency');","6ba718e5":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('target')\nax[0].set_ylabel('')\nsns.countplot('target',data=train,ax=ax[1])\nax[1].set_title('target')\nplt.show()","1d692a70":"train[\"var_0\"].hist();","5e5e2e7c":"train[\"var_81\"].hist();","b2346425":"train[\"var_2\"].hist();","4f87d38c":"sns.set(rc={'figure.figsize':(9,7)})\nsns.distplot(train['target']);","e57eec3b":"sns.violinplot(data=train,x=\"target\", y=\"var_0\")","ff38ac6d":"sns.violinplot(data=train,x=\"target\", y=\"var_81\")","6d0224e4":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","cf1ee0f3":"check_missing_data(train)","a1f54458":"check_missing_data(test)","51428f0f":"train['target'].unique()","2b68ee27":"train['target'].value_counts()","aef09f37":"def check_balance(df,target):\n    check=[]\n    # written by MJ Bahmani for binary target\n    print('size of data is:',df.shape[0] )\n    for i in [0,1]:\n        print('for target  {} ='.format(i))\n        print(df[target].value_counts()[i]\/df.shape[0]*100,'%')\n    ","c6d62e58":"check_balance(train,'target')","6c5eb21a":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['target'].skew())\nprint(\"Kurtosis: %f\" % train['target'].kurt())","0438eeeb":"cols=[\"target\",\"ID_code\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]\n","4476a676":"X_test  = test.drop(\"ID_code\",axis=1)","0d653be5":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nrfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","0b39e1bb":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)","58b27e6b":"eli5.show_weights(perm, feature_names = val_X.columns.tolist(), top=150)","d3e92f0c":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","16d2cf3d":"features = [c for c in train.columns if c not in ['ID_code', 'target']]","f405c1d6":"from sklearn import tree\nimport graphviz\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)","872fbbe9":"graphviz.Source(tree_graph)","d87d2efa":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_81')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'var_81')\nplt.show()","462d84eb":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_82')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'var_82')\nplt.show()","59e2d312":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_139')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'var_139')\nplt.show()","b4097457":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_110')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'var_110')\nplt.show()","0fb88717":"row_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nrfc_model.predict_proba(data_for_prediction_array);","13b11016":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rfc_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","36da2459":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","98c86f18":"# params is based on following kernel https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works\nparams = {'objective' : \"binary\", \n               'boost':\"gbdt\",\n               'metric':\"auc\",\n               'boost_from_average':\"false\",\n               'num_threads':8,\n               'learning_rate' : 0.01,\n               'num_leaves' : 13,\n               'max_depth':-1,\n               'tree_learner' : \"serial\",\n               'feature_fraction' : 0.05,\n               'bagging_freq' : 5,\n               'bagging_fraction' : 0.4,\n               'min_data_in_leaf' : 80,\n               'min_sum_hessian_in_leaf' : 10.0,\n               'verbosity' : 1}","1e5bde3a":"%%time\ny_pred_lgb = np.zeros(len(X_test))\nnum_round = 1000000\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_round,#change 20 to 2000\n                    valid_sets = [train_data, valid_data],verbose_eval=1000,early_stopping_rounds = 3500)##change 10 to 200\n            \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\/5","9fa6825f":"y_pred_rfc = rfc_model.predict(X_test)","9e120c8e":"y_pred_tree = tree_model.predict(X_test)","469c91e4":"train_pool = Pool(train_X,train_y)\ncat_model = CatBoostClassifier(\n                               iterations=3000,# change 25 to 3000 to get best performance \n                               learning_rate=0.03,\n                               objective=\"Logloss\",\n                               eval_metric='AUC',\n                              )\ncat_model.fit(train_X,train_y,silent=True)\ny_pred_cat = cat_model.predict(X_test)","62b686fd":"submission_rfc = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": y_pred_rfc\n    })\nsubmission_rfc.to_csv('submission_rfc.csv', index=False)","65f2fff7":"submission_tree = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": y_pred_tree\n    })\nsubmission_tree.to_csv('submission_tree.csv', index=False)","a5435e57":"submission_cat = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": y_pred_cat\n    })\nsubmission_cat.to_csv('submission_cat.csv', index=False)","f14f292f":"# good for submit\nsubmission_lgb = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)","d44319dd":"submission_rfc_cat = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": (y_pred_rfc +y_pred_cat)\/2\n    })\nsubmission_rfc_cat.to_csv('submission_rfc_cat.csv', index=False)","605b50c6":"submission_lgb_cat = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": (y_pred_lgb +y_pred_cat)\/2\n    })\nsubmission_lgb_cat.to_csv('submission_lgb_cat.csv', index=False)","34bc8087":"submission_rfc_lgb = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": (y_pred_rfc +y_pred_lgb)\/2\n    })\nsubmission_rfc_lgb.to_csv('submission_rfc_lgb.csv', index=False)","47363250":" <a id=\"7\"><\/a> <br>\n# 7- References & credits\nThanks fo following kernels that help me to create this kernel.","e51944bf":"Now you can change your model and submit the results of other models.","500528da":" <a id=\"41\"><\/a> <br>\n##   4-1-1Data set fields","fd2d55b3":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n1. [Load packages](#2)\n    1. [import](21)\n    1. [Setup](22)\n    1. [Version](23)\n1. [Problem Definition](#3)\n    1. [Problem Feature](#31)\n    1. [Aim](#32)\n    1. [Variables](#33)\n    1. [Evaluation](#34)\n1. [Exploratory Data Analysis(EDA)](#4)\n    1. [Data Collection](#41)\n    1. [Visualization](#42)\n    1. [Data Preprocessing](#43)\n1. [Machine Learning Explainability for Santander](#5)\n    1. [Permutation Importance](#51)\n    1. [How to calculate and show importances?](#52)\n    1. [What can be inferred from the above?](#53)\n    1. [Partial Dependence Plots](#54)\n1. [Model Development](#6)\n    1. [lightgbm](#61)\n    1. [RandomForestClassifier](#62)\n    1. [DecisionTreeClassifier](#63)\n    1. [CatBoostClassifier](#64)\n    1. [Funny Combine](#65)\n1. [References](#7)","6c5e1108":"### Here is how to calculate and show importances with the [eli5](https:\/\/eli5.readthedocs.io\/en\/latest\/) library:","cf690cef":"## 6-1 lightgbm","374d5eda":"For the sake of explanation, I use a Decision Tree which you can see below.","a7b25336":"### Prepare our data for our model","ea31a054":"<a id=\"57\"><\/a> <br>\n## 5-7 SHAP Values\n**SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of **any machine learning model**. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the SHAP NIPS paper for details).\n\n<img src='https:\/\/raw.githubusercontent.com\/slundberg\/shap\/master\/docs\/artwork\/shap_diagram.png' width=400 height=400>\n[image credits](https:\/\/github.com\/slundberg\/shap)\n><font color=\"red\"><b>Note: <\/b><\/font>\nShap can answer to this qeustion : **how the model works for an individual prediction?**","326da930":" <a id=\"41\"><\/a> <br>\n## 4-1 Data Collection","8332af8b":" <a id=\"43\"><\/a> <br>\n## 4-3 Data Preprocessing\nBefore we start this section let me intrduce you, some other compitation that they were similar to this:\n\n1. https:\/\/www.kaggle.com\/artgor\/how-to-not-overfit\n1. https:\/\/www.kaggle.com\/c\/home-credit-default-risk\n1. https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction","8457f906":" <a id=\"422\"><\/a> <br>\n## 4-2-2 Mean Frequency","41d47020":"1. **Imbalanced dataset** is relevant primarily in the context of supervised machine learning involving two or more classes. \n\n1. **Imbalance** means that the number of data points available for different the classes is different\n\n<img src='https:\/\/www.datascience.com\/hs-fs\/hubfs\/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>\n[Image source](http:\/\/api.ning.com\/files\/vvHEZw33BGqEUW8aBYm4epYJWOfSeUBPVQAsgz7aWaNe0pmDBsjgggBxsyq*8VU1FdBshuTDdL2-bp2ALs0E-0kpCV5kVdwu\/imbdata.png)","4cfab808":" <a id=\"62\"><\/a> <br>\n## 6-2 RandomForestClassifier","d536592e":"If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in  **shap.TreeExplainer(my_model)**. But the SHAP package has explainers for every type of model.\n\n1. shap.DeepExplainer works with Deep Learning models.\n1. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.","67cbe8b1":" <a id=\"22\"><\/a> <br>\n##  2-2 Setup","f158b281":" <a id=\"433\"><\/a> <br>\n## 4-3-3 Is data set imbalance?","34e43650":"# Reducing  memory size by ~50%\nBecause we make a lot of calculations in this kernel, we'd better reduce the size of the data.\n1. 300 MB before Reducing\n1. 150 MB after Reducing","dc6c50d2":"<a id=\"55\"><\/a> <br>\n## 5-5  Partial Dependence Plot\nIn this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https:\/\/pdpbox.readthedocs.io\/en\/latest\/).","6c73a752":"<a id=\"424\"><\/a> \n## 4-2-4 hist\nIf you check histogram for all feature, you will find that most of them are so similar","b4cc9f09":"As guidance to read the tree:\n\n1. Leaves with children show their splitting criterion on the top\n1. The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.\n><font color=\"red\"><b>Note: <\/b><\/font>\nYes **Var_81** are more effective on our model.","58ffad11":" <a id=\"51\"><\/a> <br>\n## 5-1 Permutation Importance\n In this section we will answer following question:\n 1. What features have the biggest impact on predictions?\n 1. how to extract insights from models?","823ac285":"### Create  a sample model to calculate which feature are more important.","3d531adc":" <a id=\"23\"><\/a> <br>\n## 2-3 Version\n","b002fb8c":" <a id=\"65\"><\/a> <br>\n## 6-5 Funny Combine ","72d06377":"Reducing for train data set","863c5351":" <a id=\"63\"><\/a> <br>\n## 6-3 DecisionTreeClassifier","94e78b96":"<a id=\"32\"><\/a> \n### 3-2 Aim\nIn this competition, The task is to predict the value of **target** column in the test set.","83bb6e11":"## 4-3-4 skewness and kurtosis","33d596cb":"<a id=\"33\"><\/a> \n### 3-3 Variables\n\nWe are provided with an **anonymized dataset containing numeric feature variables**, the binary **target** column, and a string **ID_code** column.\n\nThe task is to predict the value of **target column** in the test set.","37648c0f":" <a id=\"1\"><\/a> <br>\n## 1- Introduction\nAt [Santander](https:\/\/www.santanderbank.com) their mission is to help people and businesses prosper. they are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n<img src='https:\/\/www.smava.de\/kredit\/wp-content\/uploads\/2015\/12\/santander-bank.png' width=400 height=400>\n\nIn this kernel we are going to create a **Machine Learning Explainability** for **Santander** based this perfect [course](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) in kaggle.\n><font color=\"red\"><b>Note: <\/b><\/font>\nhow to extract **insights** from models?","dca14911":"A large part of the data is unbalanced, but **how can we  solve it?**","1585e411":"<a id=\"34\"><\/a> \n## 3-4 evaluation\n**Submissions** are evaluated on area under the [ROC curve](http:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed target.\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6b\/Roccurves.png' width=300 height=300>","bb780ee4":" <a id=\"432\"><\/a> <br>\n## 4-3-2 Binary Classification","dc3dbbe3":"<a id=\"4\"><\/a> \n## 4- Exploratory Data Analysis(EDA)\n In this section, we'll analysis how to use graphical and numerical techniques to begin uncovering the structure of your data. \n*  Data Collection\n*  Visualization\n*  Data Preprocessing\n*  Data Cleaning\n<img src=\"http:\/\/s9.picofile.com\/file\/8338476134\/EDA.png\" width=400 height=400>","8fc0456f":"<a id=\"3\"><\/a> \n<br>\n## 3- Problem Definition\nIn this **challenge**, we should help this **bank**  identify which **customers** will make a **specific transaction** in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this **problem**.\n","d4c1e036":"<a id=\"2\"><\/a> <br>\n## 2- A Data Science Workflow for Santander \nOf course, the same solution can not be provided for all problems, so the best way is to create a **general framework** and adapt it to new problem.\n\n**You can see my workflow in the below image** :\n\n <img src=\"http:\/\/s8.picofile.com\/file\/8342707700\/workflow2.png\"  \/>\n\n**You should feel free\tto\tadjust \tthis\tchecklist \tto\tyour needs**\n###### [Go to top](#top)","d2a03dbc":" <a id=\"52\"><\/a> <br>\n## 5-2 How to calculate and show importances?","7a5ba257":" <a id=\"6\"><\/a> <br>\n# 6- Model Development\nSo far, we have used two  models, and at this point we add another model and we'll be expanding it soon.\nin this section you will see following model:\n1. lightgbm\n1. RandomForestClassifier\n1. DecisionTreeClassifier\n1. CatBoostClassifier","79b3586d":" <a id=\"431\"><\/a> <br>\n## 4-3-1 Check missing data for test & train","65017bd1":" <a id=\"422\"><\/a> <br>\n## 4-2-2 numerical values Describe","8ca2657a":"<a id=\"31\"><\/a> \n### 3-1 Problem Feature\n\n1. train.csv - the training set.\n1. test.csv - the test set. The test set contains some rows which are not included in scoring.\n1. sample_submission.csv - a sample submission file in the correct format.\n","dc1c4968":"you can follow me on:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/)\n> ###### [Kaggle](https:\/\/www.kaggle.com\/mjbahmani\/)\n\n <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES<\/font> would be very much appreciated.<b\/>\n ","c6ab42e9":"<a id=\"421\"><\/a> \n## 4-2-1 hist","3867b20d":"<a id=\"427\"><\/a> \n## 4-2-7 violinplot","70349d13":" <a id=\"2\"><\/a> <br>\n ## 2- Load packages\n  <a id=\"21\"><\/a> <br>\n## 2-1 Import","12643b1c":"<a id=\"426\"><\/a> \n## 4-2-6 distplot\n The target in data set is **imbalance**","336be9a6":" <a id=\"42\"><\/a> <br>\n## 4-2 Visualization","b7876f22":" #  <div style=\"text-align: center\">  Santander ML Explainability  <\/div> \n###  <div style=\"text-align: center\">CLEAR DATA. MADE MODEL. <\/div> \n<img src='https:\/\/galeria.bankier.pl\/p\/b\/5\/215103d7ace468-645-387-261-168-1786-1072.jpg' width=600 height=600>\n<div style=\"text-align:center\"> last update: <b> 10\/03\/2019<\/b><\/div>\n\n\n\nYou can Fork code  and  Follow me on:\n\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n> ###### [Kaggle](https:\/\/www.kaggle.com\/mjbahmani\/)\n-------------------------------------------------------------------------------------------------------------\n <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES<\/font> would be very much appreciated.<\/b>\n    \n -----------","140d328f":"<a id=\"53\"><\/a> <br>\n## 5-3 What can be inferred from the above?\n1. As you move down the top of the graph, the importance of the feature decreases.\n1. The features that are shown in green indicate that they have a positive impact on our prediction\n1. The features that are shown in white indicate that they have no effect on our prediction\n1. The features shown in red indicate that they have a negative impact on our prediction\n1.  The most important feature was **Var_110**.","384ea36a":"<a id=\"423\"><\/a> \n## 4-2-3 countplot","56e5f4f8":" <a id=\"64\"><\/a> <br>\n## 6-4 CatBoostClassifier","96b6e57d":"1. [https:\/\/www.kaggle.com\/dansbecker\/permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [https:\/\/www.kaggle.com\/dansbecker\/partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n1. [https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv](https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv)\n1. [https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb](https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb)\n1. [https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction)\n1. [https:\/\/www.kaggle.com\/dansbecker\/permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [https:\/\/www.kaggle.com\/dansbecker\/partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n1. [https:\/\/www.kaggle.com\/dansbecker\/shap-values](https:\/\/www.kaggle.com\/dansbecker\/shap-values)\n1. [https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/algorithm-choice](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/algorithm-choice)\n1. [kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65](kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65)\n1. [https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works](https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works)","5fae521e":"Reducing for test data set","843e65e5":" <a id=\"5\"><\/a> <br>\n# 5- Machine Learning Explainability for Santander\nIn this section, I want to try extract **insights** from models with the help of this excellent [**Course**](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) in Kaggle.\nThe Goal behind of ML Explainability for Santander is:\n1. All features are senseless named.(var_1, var2,...) but certainly the importance of each one is different!\n1. Extract insights from models.\n1. Find the most inmortant feature in models.\n1. Affect of each feature on the model's predictions.\n<img src='http:\/\/s8.picofile.com\/file\/8353215168\/ML_Explain.png'>\n\nAs you can see from the above, we will refer to three important and practical concepts in this section and try to explain each of them in detail.","a19320a3":"<a id=\"56\"><\/a> <br>\n## 5-6 Chart analysis\n1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n1. A blue shaded area indicates level of confidence","57ba96f5":"<a id=\"54\"><\/a> <br>\n## 5-4 Partial Dependence Plots\nWhile **feature importance** shows what **variables** most affect predictions, **partial dependence** plots show how a feature affects predictions.[6][7]\nand partial dependence plots are calculated after a model has been fit. [partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)"}}