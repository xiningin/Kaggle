{"cell_type":{"f385525e":"code","ee01b2f2":"code","4e0fe4c0":"code","a05d87ed":"code","13932185":"code","33bb56e0":"code","e9612106":"code","584a246e":"code","d67b14a8":"code","377d79d7":"code","4fe7c753":"code","2edb573c":"code","e39646de":"code","4e96814f":"code","c7913291":"code","ba510b8d":"code","d484358d":"code","10452a78":"markdown","63a2aa94":"markdown","4559d9bc":"markdown","11941bc1":"markdown","468b1a24":"markdown","40acbc8a":"markdown","d4a9ce05":"markdown","c1e7ea24":"markdown","2cf74503":"markdown"},"source":{"f385525e":"\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport gensim\nimport spacy\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"german\")\n\n\n\ndata_test = pd.read_csv('..\/input\/10k-german-news-articles\/Articles.csv')\ndata_test.head()","ee01b2f2":"\nbody_new = [re.sub('<[^<]+?>|&amp', '', word) for word in data_test['Body'].values]\ndata_test['Body'] = body_new\n#! python -m spacy download de_core_news_sm\n","4e0fe4c0":"! python -m spacy download de_core_news_sm\nimport spacy\nimport de_core_news_sm\nnlp = de_core_news_sm.load()\n\n\n#nlp = spacy.load('de_core_news_sm')\n\n","a05d87ed":"## Drop Duplicates and NA values\n\ndata_test_clean = data_test.dropna()\ndata_test_clean = data_test_clean.drop_duplicates()","13932185":"article_data = data_test_clean[['ID_Article','Title','Body']].drop_duplicates()\narticle_data.shape","33bb56e0":"\n## Convert the body text into a series of words to be fed into the model\n\ndef text_clean_tokenize(article_data):\n    \n    review_lines = list()\n\n    lines = article_data['Body'].values.astype(str).tolist()\n\n    for line in lines:\n        tokens = word_tokenize(line)\n        tokens = [w.lower() for w in tokens]\n        table = str.maketrans('','',string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove remaining tokens that are not alphabetic\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words('german'))\n        words = [w for w in words if not w in stop_words]\n        words = [stemmer.stem(w) for w in words]\n\n        review_lines.append(words)\n    return(review_lines)\n    \n    \nreview_lines = text_clean_tokenize(article_data)","e9612106":"## Building the word2vec model\n\nmodel =  gensim.models.Word2Vec(sentences = review_lines,\n                               size=100,\n                               window=2,\n                               workers=4,\n                               min_count=2,\n                               seed=42,\n                               iter= 50)\n\nmodel.save(\"word2vec.model\")\n","584a246e":"\n\n\nword_list = list(model.wv.vocab)\n\nfor words in word_list[1:10]:\n    print('Similar Words for :',words)\n    \n    print(model.wv.similar_by_word(words))\n    print('--------------------------\\n')\n\n","d67b14a8":"\n\n\n\n#print(word_list)\n\n# Convert each article lines to word2vec representation\nimport spacy\n\ndef tokenize(sent):\n    doc = nlp.tokenizer(sent)\n    return [token.lower_ for token in doc if not token.is_punct]\n\nnew_df = (article_data['Body'].apply(tokenize).apply(pd.Series))\n\nnew_df = new_df.stack()\nnew_df = (new_df.reset_index(level=0)\n                .set_index('level_0')\n                .rename(columns={0: 'word'}))\n\nnew_df = new_df.join(article_data.drop('Body', 1), how='left')\n\n","377d79d7":"new_df = new_df[['word','ID_Article']]\nvectors = model.wv[word_list]\nvectors_df = pd.DataFrame(vectors)\nvectors_df['word'] = word_list\nmerged_frame = pd.merge(vectors_df, new_df, on='word')\nmerged_frame_rolled_up = merged_frame.drop('word',axis=1).groupby('ID_Article').mean().reset_index()\ndel merged_frame\ndel new_df\ndel vectors\n","4fe7c753":"merged_frame_rolled_up.head()","2edb573c":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_matrix = pd.DataFrame(cosine_similarity(merged_frame_rolled_up))\ncosine_matrix.columns = list(merged_frame_rolled_up['ID_Article'])\n","e39646de":"cosine_matrix.head()","4e96814f":"\nreco_articles = {}\ni = 0\nfor col_name in cosine_matrix.columns:\n    tmp = cosine_matrix[[col_name]].sort_values(by=col_name,ascending=False)\n    tmp = tmp.iloc[1:]\n    tmp = tmp.head(20)\n    recommended_articles = list(article_data[article_data['ID_Article'].isin(tmp.index)]['Title'].values)\n    chosen_article = list(article_data[article_data['ID_Article']==col_name]['Title'].values)\n    tmp = {'Chosen-Articles': len(recommended_articles)* chosen_article,'Recommended-Articles':recommended_articles}\n    reco_articles[i] = tmp\n    i = i+1\n    del tmp\nprint('Ended')\n    \n    \n    \n    \n    ","c7913291":"## Convert Dictionary Object to a data frame\n\ndf_reco = pd.concat([pd.DataFrame(v) for k, v in reco_articles.items()])\ndf_reco.head()\n\n","ba510b8d":"## Making sure that the same articles do not get recommended\n\ndf_reco = df_reco[df_reco['Chosen-Articles']!=df_reco['Recommended-Articles']]","d484358d":"import random\n\nlist_of_articles = df_reco['Chosen-Articles'].values\nrandom.shuffle(list_of_articles)\nlist_of_articles = list_of_articles[:9]\n\nfor article in list_of_articles:\n    tmp = df_reco[df_reco['Chosen-Articles']==article]\n    print('--------------------------------------- \\n')\n    print('Recommendation for ',article,' is :')\n    print('Recommended Articles')\n    print(tmp['Recommended-Articles'].values)\n    \n","10452a78":"## Getting Recommendations\n\nThe cosine similarity matrix gives a pair wise relationship between articles. This relationship\/similarity ranges from 0 to 1. With 0 being least similar and with 1 being most similar.","63a2aa94":"## Dropping NA values and Duplicate Articles\n\nFrom the pulled data, rows that contain NA values are dropped. Duplicate rows are also dropped. After this, the columns \"Headline\" & \"Body\" are chosen. Each article is indexed with the row number it belongs in.","4559d9bc":"The model is built on the existing article data we have. Let's have a look at word similarities for a few words. The function `model.wv.similar_by_word(<word>)`, displays top words that are similar to the `<word>` as per cosine similarility","11941bc1":"## Numeric Vectors for each article\n\nA glimpse of the numeric vectors for each article is displayed below. Each article is represented by a 100 long numeric vector. These numeric vectors are then used to compute pair wise cosine simlilarities amongst articles.","468b1a24":"# Clustering German Articles using Word2Vec\n\n## Word2Vec\n\nWord2Vec is a method to represent words in a numerical - vector format such that words that are closely related to each other are close to each other in numeric vector space. This method was developed by Thomas Mikolov in 2013 at Google.\n\nEach word in the corpus is modeled against surrounding words, in such a way that the surrounding words get maximum probabilities. The mapping that allows this to happen , becomes the word2vec representation of the word. The number of surrounding words can be chosen through a model parameter called \"window size\". The length of the vector representation is chosen using the parameter 'size'.\n\nIn this notebook, the library gensim is used to construct the word2vec models\n\n## Loading the library and getting the data\n","40acbc8a":"After dropping rows that contain NA values and rows that have duplicated values, redundant words such as punctuation marks and stopwords are removed. As the texts are in German, the stopword corpus for German is used from the `spacy` library. After this preprocessing, the words are converted to a list of words to be fed into the model.","d4a9ce05":"To tackle stopwords in German, the spaCy module is used. The spaCy module consists of information regarding stopwords for different languages. The command `! python -m spacy download de_core_news_sm` enables the download of German- related module","c1e7ea24":"## Converting articles into numeric vectors\n\nThe word2vec model converts each word into a numeric vector. Each article is converted to a numeric vector by taking the constituent words' numeric vectors and averaging them.\n\nTo do that the following steps are taken\n* Tokenize each article into its constituent words\n* Map each word to numeric vector representation.\n* Take average for each article.\n","2cf74503":"## Recommendation for 10 randomly chosen articles"}}