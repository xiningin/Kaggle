{"cell_type":{"13e753fd":"code","7aa4d684":"code","772815cd":"code","24183568":"code","e365dfc9":"code","aeebe75d":"code","2202dcfc":"code","1255362d":"code","869b8a2e":"code","92b25823":"code","9c12d808":"code","84ebee36":"code","1f49d691":"code","4d69eab9":"code","a02a2335":"code","881f18ab":"code","a1d86936":"code","bceef647":"code","5a77999c":"code","2966b8b8":"code","deeb74c1":"code","6db580c3":"code","d1d61940":"code","154164c4":"code","5dd0028e":"code","ae63b34f":"code","b63dda65":"code","325b5d61":"code","442b4113":"code","bf3b903e":"code","7d5e6a56":"code","8461a7b5":"code","71a9ae38":"code","3f2b8827":"code","041f4401":"code","6da00b90":"code","1c2e8988":"code","6bd25cc9":"markdown","5f57010b":"markdown","5e8328dc":"markdown","7ecacc7b":"markdown","75151244":"markdown","87134048":"markdown","53f9eb5b":"markdown"},"source":{"13e753fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","7aa4d684":"#Use data load function -> Load data into a Data Frame. \n#Df is a table that contains the contents for the data\nusa_2016_presidential_election_by_county = pd.read_csv('\/kaggle\/input\/us-elections-dataset\/usa-2016-presidential-election-by-county.csv', sep=';')","772815cd":"print(f\"There are {len(usa_2016_presidential_election_by_county)} Records in total\")\nusa_2016_presidential_election_by_county.head()\n#Print out how many data records\/rows in the dataset \n#.head() function takes the first five rows in the dataset\n#The dataset indicates indiviaul county of the U.S ","24183568":"for k in usa_2016_presidential_election_by_county.keys():\n    print(k)\n#prints out: all the attributes for each county","e365dfc9":"usa_2016_presidential_election_by_county[[\"County\", \"Republicans 2016\"]]\n#Look at the attributes\n#[A1]: one attribute, [[county, A1]]: prints out 2 attributes e.g. county and the percentage that the citizens voted for republicans","aeebe75d":"\"\"\"\nDrop the existing records that we are interested in\nLike: how many votes HC got votes in 2016, republicans performance in etc...\nwhenever a row contains a missing value, the record will be dropped\n\"\"\"\ndf = usa_2016_presidential_election_by_county.dropna(subset=[\n    \"Votes16 Clintonh\", \"Votes16 Trumpd\",\n    \"Republicans 2016\", \"Democrats 2016\",\n    \"Republicans 2012\", \"Republicans 2008\",\n    \"Democrats 2012\", \"Democrats 2008\", \"Votes\"])\n","2202dcfc":"#df[\"Votes16 Clintonh\"].sum()","1255362d":"#[Ref initial entropy1]\nn_dem = df[\"Votes16 Clintonh\"].sum() #df[]: How many votes H.C received each county -> Sum\nn_rep = df[\"Votes16 Trumpd\"].sum()\np_dem = n_dem \/ (n_dem + n_rep) # Respective number\/total votes -> to get the probability each D.T or H.C\np_rep = n_rep \/ (n_dem + n_rep) #Rep:D.T\nprint(f\"Votes for DEM {n_dem}, probability {p_dem:.4f}\")\nprint(f\"Votes for REP {n_rep}, probability {p_rep:.4f}\")\n\n#Nearly have half\/half chance of voting for each of the candidate\n# -> that means almost have maximum uncertainty of his\/her vote\n","869b8a2e":"#[Ref initial entropy2]\nent = - (p_dem * np.log2(p_dem) + p_rep * np.log2(p_rep)).sum()\nprint(f\"Entropy: {ent:.4f}\")\n#Since the entropy is very high, we need some attributes\/information to recude the uncertainty ","92b25823":"\"\"\"\nInstead of looking at the entire population, select sub-population and see if things get more formative\n\"\"\"\ndf[df[\"State\"] == \"California\"] #if the record is within California, -> df","9c12d808":"#since the computation is repeated, make a def \ndef exam_votes(df_i):\n    n_dem = df_i[\"Votes16 Clintonh\"].sum() \n    n_rep = df_i[\"Votes16 Trumpd\"].sum()\n    p_dem = n_dem \/ (n_dem + n_rep)\n    p_rep = n_rep \/ (n_dem + n_rep)\n    print(f\"2016 Vote Statistics {n_dem + n_rep} votes in {len(df_i)} counties\")\n    print(f\"Votes for DEM {n_dem}, probability {p_dem:.4f}\")\n    print(f\"Votes for REP {n_rep}, probability {p_rep:.4f}\")\n    ent = - (p_dem * np.log2(p_dem) + p_rep * np.log2(p_rep)).sum()\n    print(f\"Entropy: {ent:.4f}\")\n    return ent, p_dem, p_rep, n_dem, n_rep","84ebee36":"#input = df[CA]\nent, p_dem, p_rep, n_dem, n_rep = exam_votes(df[df[\"State\"] == \"California\"])\n","1f49d691":"import plotly.express as px\nfig = px.scatter_geo(df, lat=\"lat\", lon=\"lon\", color=\"Republicans 2016\", hover_name=\"County\", size=\"Votes\")#, \n \nfig.show()","4d69eab9":"px.scatter(df, x=\"Republicans 2016\", y=\"Democrats 2016\", hover_name=\"County\")","a02a2335":"# Get the record of the county \"District of Columbia, District of Columbia\"\ndf[df[\"County\"] == \"District of Columbia, District of Columbia\"] # only one record","881f18ab":"# Let's to the entropy computation\n_ = exam_votes(df[df[\"County\"] == \"District of Columbia, District of Columbia\"])","a1d86936":"df[\"Republicans Won 2016\"] = df[\"Democrats 2016\"] < df[\"Republicans 2016\"]","bceef647":"# Check the 2016 results\ndf[\"Republicans Won 2016\"].value_counts(normalize=True)  #First one: T or F, second: %\n                                                         #True: Republican won","5a77999c":"#[Ref draft3]\nprob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\nprob = np.array(prob)\nprint(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\nent = - (prob * np.log2(prob)).sum()\nprint(f\"Entropy is {ent:.4f}\")","2966b8b8":"#State\n\ndef exam_counties(df, verbose=True):\n    prob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\n    prob = np.array(prob)\n    ent = - (prob * np.log2(np.maximum(prob, 1e-6))).sum()\n    if verbose:\n        print(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\n        print(f\"Entropy is {ent:.4f}\")\n    return ent","deeb74c1":"#[Ref draft1], split the data respect to county, the no. of individual county in each state\nstates = df[\"State\"].value_counts()\nstates","6db580c3":"#exam_counties(df[df[\"State\"]==\"Texas\"], verbose=False)","d1d61940":"#[Ref draft2]\ntotal_ent = 0\nnum_counties = 0\nfor k, v in states.iteritems():\n    ent = exam_counties(df[df[\"State\"]==k], verbose=False) # in this particular state take sub-population in a certain county\n    print(f\"State {k} has {v} counties, result entropy {ent:.3f}\")\n    total_ent += v * ent\n    num_counties += v\n    \nprint(f\"Weighted sum of entropies {total_ent\/num_counties :.3f}\")\nent0 = - (prob * np.log2(prob)).sum()\nprint(f\"Entropy is {ent0:.3f}\")\nprint(f\"Info Gain: {ent0 - total_ent\/num_counties:.3f}\")\n#compare to the original entropy 0.6252, it got reduced","154164c4":"#Education\n# Examine the education information.\ndf[[\"Less Than High School Diploma\", \"At Least High School Diploma\",\n    \"At Least Bachelors's Degree\",\"Graduate Degree\"]]","5dd0028e":"fig = px.scatter(df, x=\"At Least Bachelors's Degree\", y=\"Democrats 2016\", \n                 color=\"Republicans Won 2016\", color_discrete_sequence=['red','blue'])\nfig.show()","ae63b34f":"df[\"More Than 30p Bachelors\"] = df[\"At Least Bachelors's Degree\"] > 30\n#whether more than 30% of residents received Bachelor's degree","b63dda65":"#[Ref Edu]\ntotal_ent = 0\nnum_counties = 0\nattr = \"More Than 30p Bachelors\"\nfor k, v in df[attr].value_counts().iteritems():\n    ent = exam_counties(df[df[attr]==k], verbose=False) # in this particular state\n    print(f\"there are {v} counties where {attr} is {k}, result entropy {ent:.3f}\")\n    total_ent += v * ent\n    num_counties += v\n    \nprint(f\"Weighted sum of entropies {total_ent\/num_counties :.3f}\")\n\n# recall that the original entropy is ... (copied from above)\nprob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\nprob = np.array(prob)\nprint(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\nent0 = - (prob * np.log2(prob)).sum()\nprint(f\"Entropy is {ent0:.4f}\")\nprint(f\"Info Gain: {ent0 - total_ent\/num_counties:.4f}\")","325b5d61":"#Population\nfig = px.scatter(df, x=\"White (Not Latino) Population\", y=\"Democrats 2016\", color=\"Republicans Won 2016\",\n                color_discrete_sequence=['red','blue'])\nfig.show()\n#cut at 60","442b4113":"df[\"White (Not Latino) Population Is Greater Than 60p\"] = df[\"White (Not Latino) Population\"] > 60","bf3b903e":"def compute_weighted_sub_entropy(df, attr, verbose=True):\n    total_ent = 0\n    num_counties = 0\n    for k, v in df[attr].value_counts().iteritems():\n        ent = exam_counties(df[df[attr]==k], verbose=False) # in this particular sub-population\n        if verbose:\n            print(f\"there are {v} counties where {attr} is {k}, result entropy {ent:.3f}\")\n        total_ent += v * ent\n        num_counties += v\n    \n    weighted_ent = total_ent\/num_counties\n    if verbose:\n        print(f\"Weighted sum of entropies {weighted_ent:.3f}\")\n    return weighted_ent\n\nweighted_ent = compute_weighted_sub_entropy(df, \"White (Not Latino) Population Is Greater Than 60p\")\nprint(f\"Info Gain: {ent0 - weighted_ent:.4f}\")","7d5e6a56":"#[Ref attributes in C]\nattributes = [\"White (Not Latino) Population\", \n    \"African American Population\",\n    \"Native American Population\",\n    \"Asian American Population\", \n    \"Latino Population\",\n    \"Less Than High School Diploma\",\n    \"At Least High School Diploma\",\n    \"At Least Bachelors's Degree\",\n    \"Graduate Degree\",\n    \"School Enrollment\",\n    \"Median Earnings 2010\",\n    \"Children Under 6 Living in Poverty\",\n    \"Adults 65 and Older Living in Poverty\",\n    \"Preschool.Enrollment.Ratio.enrolled.ages.3.and.4\",\n    \"Poverty.Rate.below.federal.poverty.threshold\",\n    \"Gini.Coefficient\",\n    \"Child.Poverty.living.in.families.below.the.poverty.line\",\n    \"Management.professional.and.related.occupations\",\n    \"Service.occupations\",\n    \"Sales.and.office.occupations\",\n    \"Farming.fishing.and.forestry.occupations\",\n    \"Construction.extraction.maintenance.and.repair.occupations\",\n    \"Production.transportation.and.material.moving.occupations\",\n    \"Median Age\",\n    \"Poor.physical.health.days\",\n    \"Poor.mental.health.days\",\n    \"Low.birthweight\",\n    \"Teen.births\",\n    \"Children.in.single.parent.households\",\n    \"Adult.smoking\",\n    \"Adult.obesity\",\n    \"Diabetes\",\n    \"Sexually.transmitted.infections\",\n    \"HIV.prevalence.rate\",\n    \"Uninsured\",\n    \"Unemployment\",\n    \"Violent.crime\",\n    \"Homicide.rate\",\n    \"Injury.deaths\",\n    \"Infant.mortality\"]\nnew_attributes = []\nfor a in attributes:\n    new_a = \"Quant4.\" + a\n    df[new_a] = pd.qcut(df[a], q=4, labels=[\"q1\", \"q2\", \"q3\", \"q4\"])\n    new_attributes.append(new_a)","8461a7b5":"#pd.qcut(df[ \"At Least Bachelors's Degree\"], q=4, labels=[\"q1\", \"q2\", \"q3\", \"q4\"])","71a9ae38":"#[REF_TreeBuilding]\ndef compute_entropy(y):\n    \"\"\"\n    :param y: The data samples of a discrete distribution\n    \"\"\"\n    if len(y) < 2: #  a trivial case\n        return 0\n    freq = np.array( y.value_counts(normalize=True) )\n    return -(freq * np.log2(freq + 1e-6)).sum() # the small eps for \n    # safe numerical computation \n    \ndef compute_info_gain(samples, attr, target):\n    values = samples[attr].value_counts(normalize=True)\n    split_ent = 0\n    for v, fr in values.iteritems():\n        index = samples[attr]==v\n        sub_ent = compute_entropy(target[index])\n        split_ent += fr * sub_ent\n    \n    ent = compute_entropy(target)\n    return ent - split_ent\n\nclass TreeNode:\n    \"\"\"\n    A recursively defined data structure to store a tree.\n    Each node can contain other nodes as its children\n    \"\"\"\n    def __init__(self, node_name=\"\", min_sample_num=10, default_decision=None):\n        self.children = {} # Sub nodes -- starts from here, and make it growing, chldrn=collection of more tree nodes\n        # recursive, those elements of the same type (TreeNode)\n        self.decision = None # Undecided\n        self.split_feat_name = None # Splitting feature\n        self.name = node_name\n        self.default_decision = default_decision\n        self.min_sample_num = min_sample_num\n\n    def pretty_print(self, prefix=''):\n        if self.split_feat_name is not None:\n            for k, v in self.children.items():\n                v.pretty_print(f\"{prefix}:When {self.split_feat_name} is {k}\")\n                #v.pretty_print(f\"{prefix}:{k}:\")\n        else:\n            print(f\"{prefix}:{self.decision}\")\n\n    def predict(self, sample):\n        if self.decision is not None:\n            # uncomment to get log information of code execution\n            print(\"Decision:\", self.decision)\n            return self.decision\n        else: \n            # this node is an internal one, further queries about an attribute \n            # of the data is needed.\n            attr_val = sample[self.split_feat_name]\n            child = self.children[attr_val]\n            # uncomment to get log information of code execution\n            print(\"Testing \", self.split_feat_name, \"->\", attr_val)\n\n            # [Exercise]\n            # Insert your code here\n            return child.predict(sample)\n \n#below: \n    def fit(self, X, y):\n        if self.default_decision is None:\n            self.default_decision = y.mode()[0]\n            \n            \n        print(self.name, \"received\", len(X), \"samples\")\n        if len(X) < self.min_sample_num:\n            # If the data is empty when this node is arrived, \n            # we just make an arbitrary decision\n            \"\"\"\n            if the sub-population is small enough, it's better not to split anymore ^^, \n            **OR**\n            we have received unanimous conclusion that all counties in the sub-population\n            will give one same decision\n            \"\"\"\n            if len(X) == 0:\n                self.decision = self.default_decision\n                print(\"DECESION\", self.decision)\n            else:\n                self.decision = y.mode()[0]\n                print(\"DECESION\", self.decision)\n            return\n        else: \n            unique_values = y.unique()\n            if len(unique_values) == 1:\n                self.decision = unique_values[0]\n                print(\"DECESION\", self.decision)\n                return\n            else:\n                info_gain_max = 0    #From here, important: divide data using different keys like Qs\n                for a in X.keys(): # Examine each attribute\n                    aig = compute_info_gain(X, a, y)\n                    if aig > info_gain_max:\n                        info_gain_max = aig\n                        self.split_feat_name = a\n                print(f\"Split by {self.split_feat_name}, IG: {info_gain_max:.2f}\")\n                self.children = {}\n                for v in X[self.split_feat_name].unique():\n                    index = X[self.split_feat_name] == v\n                    self.children[v] = TreeNode( #Create Treenode #creat sub population using children\n                        node_name=self.name + \":\" + self.split_feat_name + \"==\" + str(v),\n                        min_sample_num=self.min_sample_num,\n                        default_decision=self.default_decision)\n                    self.children[v].fit(X[index], y[index]) #split and fit the children of one child by another using sub-population\n# Test tree building\ndata = df[new_attributes].dropna('columns', 'any')\ntarget = df[\"Republicans Won 2016\"]\n\nt = TreeNode(min_sample_num=50)\nt.fit(data, target)\n        ","3f2b8827":"#[Ref Show]\ndata = df[new_attributes].dropna('columns', 'any')\ndata.keys()\ndata[['Quant4.White (Not Latino) Population',\n      'Quant4.African American Population']]","041f4401":"corr = 0\nerr_fp = 0 #false positive: actually false, but predicted true\nerr_fn = 0 #false negative: actually true, predicted false\nfor (i, ct), tgt in zip(data.iterrows(), target):\n    a = t.predict(ct)\n    if a and not tgt:\n        err_fp += 1\n    elif not a and tgt:\n        err_fn += 1\n    else:\n        corr += 1","6da00b90":"corr, err_fp, err_fn ","1c2e8988":"\"\"\"dataset = usa_2016_presidential_election_by_county\nX, y = dataset[\"data\"], dataset[\"target\"]\nind = (y == 0) + (y == 1)\nX = X[ind]\ny = y[ind] # take two classes\"\"\"","6bd25cc9":"select attribute","5f57010b":"use collective data for each county, not individual","5e8328dc":"Look at the data visually ","7ecacc7b":"TREE BUILDING","75151244":"Predictive task - setting up and primitive attempt","87134048":"#Entropy of the vote distributions\n\nHow much unceratinty & How can we remove the uncertainty as much as possible","53f9eb5b":"Population\nWhite (Not Latino) Population\nAfrican American Population\nNative American Population\nAsian American Population\nOther Race or Races\nLatino Population\nChildren Under 6 Living in Poverty\nAdults 65 and Older Living in Poverty\nTotal Population"}}