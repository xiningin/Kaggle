{"cell_type":{"ad9edfb4":"code","af6bb216":"code","41b6f7d1":"code","c7f5f1a6":"code","273a180b":"code","b8c6873b":"code","80b11dab":"code","a143fe13":"code","533def49":"markdown","7d110365":"markdown","9bd92ae9":"markdown","c7e20f35":"markdown","539a5648":"markdown","2f64db80":"markdown","ca421bac":"markdown","a76421e0":"markdown","a3161919":"markdown","7742adcf":"markdown","b5765c36":"markdown","aa160020":"markdown","a39c3a0a":"markdown","71070272":"markdown"},"source":{"ad9edfb4":"# Installing latest version of flax and other dependencies\n!pip install --upgrade jax\n!pip install --upgrade flax\n!pip install --upgrade jaxlib","af6bb216":"import jax\nimport flax\nimport numpy as onp\nimport jax.numpy as jnp\nimport tensorflow as tf\nimport tensorflow_datasets as tfds","41b6f7d1":"class CNN(flax.nn.Module):\n    \n  def apply(self, x):\n    x = flax.nn.Conv(x, features=32, kernel_size=(3, 3))\n    x = flax.nn.relu(x)\n    x = flax.nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = flax.nn.Conv(x, features=64, kernel_size=(3, 3))\n    x = flax.nn.relu(x)\n    x = flax.nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))\n    x = flax.nn.Dense(x, features=256)\n    x = flax.nn.relu(x)\n    x = flax.nn.Dense(x, features=10)\n    x = flax.nn.log_softmax(x)\n    return x","c7f5f1a6":"@jax.vmap\ndef cross_entropy_loss(logits, label):\n  return -logits[label]","273a180b":"def compute_metrics(logits, labels):\n  loss = jnp.mean(cross_entropy_loss(logits, labels))\n  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n  return {'loss': loss, 'accuracy': accuracy}","b8c6873b":"@jax.jit\ndef train_step(optimizer, batch):\n  def loss_fn(model):\n    logits = model(batch['image'])\n    loss = jnp.mean(cross_entropy_loss(\n        logits, batch['label']))\n    return loss\n  grad = jax.grad(loss_fn)(optimizer.target)\n  optimizer = optimizer.apply_gradient(grad)\n  return optimizer","80b11dab":"@jax.jit\ndef eval(model, eval_ds):\n  logits = model(eval_ds['image'] \/ 255.0)\n  return compute_metrics(logits, eval_ds['label'])","a143fe13":"def train():\n  train_ds = tfds.load('cifar10', split=tfds.Split.TRAIN)\n  train_ds = train_ds.map(lambda x: {'image': tf.cast(x['image'], tf.float32),\n                                     'label': tf.cast(x['label'], tf.int32)})\n  train_ds = train_ds.cache().shuffle(1000).batch(128)\n  test_ds = tfds.as_numpy(tfds.load(\n      'cifar10', split=tfds.Split.TEST, batch_size=-1))\n  test_ds = {'image': test_ds['image'].astype(jnp.float32),\n             'label': test_ds['label'].astype(jnp.int32)}\n  _, initial_params = CNN.init_by_shape(\n  jax.random.PRNGKey(0),\n   [((1, 32, 32, 3), jnp.float32)])\n  model = flax.nn.Model(CNN, initial_params)\n  optimizer = flax.optim.Momentum(\n      learning_rate=0.1, beta=0.9).create(model)\n  for epoch in range(10):\n    for batch in tfds.as_numpy(train_ds):\n      batch['image'] = batch['image'] \/ 255.0\n      optimizer = train_step(optimizer, batch)\n      metrics = eval(optimizer.target, test_ds)\n    print('eval epoch: %d, loss: %.4f, accuracy: %.2f'\n         % (epoch+1,\n          metrics['loss'], metrics['accuracy'] * 100))\n    \ntrain()","533def49":"# \ud83d\udc4e\ud83c\udffb \ud83d\udc4d\ud83c\udffb The Loss Function","7d110365":"## **Flax: A neural network library for JAX designed for flexibility**","9bd92ae9":"Flax is being actively improved and has a growing community of researchers and engineers at Google who happily use Flax for their daily research. It's especially designed for JAX and offers a lot of flexibility because of it's Abstraction based build","c7e20f35":"# \ud83d\udcc8 Evaluation Metrics","539a5648":"# \u27a1\ufe0f Propagation","2f64db80":"Here, we create a simple `eval()` function which returns the metrics from the `compute_metrics` function.","ca421bac":"Here, we create a simple `compute_metrics` function where, we take the mean of the output of the cross entropy loss and then calculate the accuracy of our model.  ","a76421e0":"Here, we create a  `train` function which imports the Cifar10 dataset using `tensorflow_datasets` after applying basic transformations such as splitting into test and train, typecast image into `float32` format, cache, shuffle and batching.\n\n---\nWe need to initialize our model weights using the `init_by_shape` function of `flax.nn.Module`. \n\n\n---\n\nWe then define the Momentum optimizer using `flax.optim.Momentum` by specifying our learning rate as `0.1` and beta as `0.9`.\n\n---\n\nThen, we just pass the inputs in batches using the `tfds.as_numpy` and print the metrics after each epoch","a3161919":"The core of Flax is the Module abstraction. Modules allow us to write parameterized functions just like writing a normal numpy function with JAX. The Module api allows us to declare parameters and use them directly with the JAX api\u2019s.\n\nHere, we create a simple CNN module for our classification model. A Module is created by defining a subclass of `flax.nn.Module` and implementing the apply method.\n\nOur custom CNN module has the following layers:\n\n* 2 Convolutional Blocks followed by ReLU non-linearity and average pooling.\n\n\n* We then Flatten the output of the last convolutional block\n\n* We then add a simple Dense layer with 256 units followed by ReLU non-linear activation\n\n* Cifar10 has 10 classes, therefore we create a last dense layer with 10 units followed by the log softmax activation function.\n","7742adcf":"# \ud83d\udc77\ud83c\udffb\u200d\u2642\ufe0fTraining the Model ","b5765c36":"Here, we implement the Cross Entropy Loss function with two parameters, namely `logits` and `label`. You can notice that this function is meant to process single samples rather than matrices as compared to other deep learning frameworks. This way we don't have to think about much about dimensionality while writing custom loss function.\n\n\nThe `@jax.vmap` decorator is a inbuilt JAX decorator which automatically vectorizes our loss function to work with batch processing.","aa160020":"# \ud83d\udce6 Importing Packages","a39c3a0a":"# \ud83d\udea7 Defining Model Architecture","71070272":"Here, we essentially create the propagation part of our model: \n\n\n* We run our image through the model by passing it as `batch['image']`\n* Calculate the loss by passing the output through the `cross_entropy_loss` function\n* Calculate the gradient using the `jax.grad` function\n* \"Apply\" this gradient to our optimizer using the `apply_gradient` function\n\n---\n\nThe `@jax.jit` decorator compiles our function into fused device operations which can then run efficiently on GPUs or TPUs. \n\n> This is the great thing about Flax, we don't need to explicitly specify TPU configurations like we have to done in Keras\/Tensorflow."}}