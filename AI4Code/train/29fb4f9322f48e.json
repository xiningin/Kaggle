{"cell_type":{"fea9ee0d":"code","b1c48a36":"code","ce8da3f4":"code","cf88837e":"code","75f82a5b":"code","96f8b3f5":"code","212b572d":"code","74547fad":"code","5514a7b4":"code","13860b52":"code","08562ff2":"code","eb8243c3":"code","80348931":"code","c5a9e97e":"code","0f2de840":"code","9ef5807c":"code","0389f5ee":"code","3d669032":"code","a818ca3f":"code","70097bc0":"code","f7667e71":"code","247c257f":"code","18b5ef52":"code","34d108eb":"code","d45ee999":"code","d2c8847b":"code","7a9281d2":"code","5f17e2ba":"code","9da2d306":"code","697ca478":"code","649f33f9":"code","f195c192":"code","3a28d8df":"code","2b68b119":"code","1d90fc0e":"code","58e948fd":"code","6d0a396f":"code","ad683ffa":"code","ed945705":"code","b894ffe6":"code","85895b8b":"code","b8ce3985":"code","7be7ce08":"code","b0955d8b":"code","e19e7197":"code","0a0f5dc5":"code","8ee75f36":"code","ee4195df":"code","fcde6d71":"code","dea59677":"code","f9a4cb50":"code","1a633ba0":"code","b1de1351":"code","c6b32b21":"code","cb611bc9":"code","328f001d":"code","34ae7ef2":"code","d5c70728":"code","34815b14":"code","8b0180d4":"code","73e4e362":"markdown","5c850a87":"markdown","207c4c79":"markdown","ce19204c":"markdown","352a1bc9":"markdown","bb12501f":"markdown","69309ba6":"markdown","92f26650":"markdown","1b17f68d":"markdown","804bfe4b":"markdown","b9fe34c0":"markdown","af15e0e9":"markdown","b9e7ebbe":"markdown","b231226e":"markdown","46110ef4":"markdown","a62c48d4":"markdown","f45d5fb1":"markdown","901ace8f":"markdown","79b09354":"markdown","50b96213":"markdown","51ab36ee":"markdown","edfd1888":"markdown","8cdb81a4":"markdown","a8e9954f":"markdown","b4a29868":"markdown","379e739a":"markdown","4650051b":"markdown","46587f5b":"markdown","6b7cdbd6":"markdown","c971ee9c":"markdown","140f6660":"markdown","34a5e6b9":"markdown","3a63fcfb":"markdown","acb99ca6":"markdown","d2a14f62":"markdown","d2c4c86a":"markdown","5d1cbd69":"markdown","f3ab9c32":"markdown","e6f99359":"markdown","6a44bf66":"markdown","ea8252b5":"markdown","1833aff3":"markdown","fffaf93c":"markdown","1804799d":"markdown","61d648e0":"markdown","86c48e85":"markdown","d3c0ac08":"markdown","d92a5678":"markdown","c139c943":"markdown","32db86cc":"markdown","0e0c3b71":"markdown","b5e496e4":"markdown","d399a7c5":"markdown","9156c272":"markdown"},"source":{"fea9ee0d":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b1c48a36":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","ce8da3f4":"df.head()","cf88837e":"df.info()","75f82a5b":"df.shape","96f8b3f5":"df.columns","212b572d":"df.isna().sum()","74547fad":"df.describe()","5514a7b4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot('Class', data=df)\nplt.title('Distribui\u00e7\u00e3o da vari\u00e1vel target')\nplt.xlabel('Class')\nplt.ylabel('Frequencia')","13860b52":"df['Class'].value_counts()","08562ff2":"plt.figure(figsize=(14,9))\nsns.heatmap(df.corr(), linewidths=0.5)","eb8243c3":"features = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n\nfeatures_all = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n\nlabel = ['Class']","80348931":"from sklearn.model_selection import train_test_split\nX, y = df[features], df[label]\nX_train, X_test, y_train, y_test =\\\n     train_test_split(X, y,\n                      test_size=0.94,\n                      random_state=0,\n                      stratify=y)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","c5a9e97e":"from matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","0f2de840":"num_folds = 10\nseed = 7\nscoring = 'accuracy'","9ef5807c":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RF', RandomForestClassifier()))\nresults = []\nnames = []\n# fiz um pipeline\n# aplica\u00e7\u00e3o do Kfold e dos modelos\n# olhar o \"model\"\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","0389f5ee":"fig = pyplot.figure()\nfig.suptitle('Compara\u00e7\u00e3o entre as acur\u00e1cias')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","3d669032":"pipelines = []\npipelines.append(('LR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(solver='liblinear'))])))\npipelines.append(('LDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('KNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\npipelines.append(('CART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\npipelines.append(('NB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\npipelines.append(('RF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))\npipelines.append(('SVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(gamma='auto'))])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","a818ca3f":"fig = pyplot.figure()\nfig.suptitle('Escala de Algoritmos - Compara\u00e7\u00e3o')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","70097bc0":"# Random Forest Classifier\nfrom yellowbrick.classifier import ClassificationReport\nmodel_RF =  RandomForestClassifier()\nvisualizer = ClassificationReport(model, size=(800, 533))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()","f7667e71":"# Regress\u00e3o Log\u00edstica \nmodel_LR =  LogisticRegression(solver='liblinear')\nvisualizer = ClassificationReport(model, size=(800, 533))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()","247c257f":"# LDA\nmodel_LDA =  LinearDiscriminantAnalysis()\nvisualizer = ClassificationReport(model, size=(800, 533))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()","18b5ef52":"# Come\u00e7ar por Regress\u00e3o log\u00edstica\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\n\nC = [0.2, 0.4, 0.6, 0.8, 1.0]\nfit_intercept = ['True','False']\nclass_weight = ['dic', 'balanced']\nsolver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n\nparam_grid = dict(C=C, fit_intercept=fit_intercept, class_weight=class_weight, solver=solver)\n\nmodel = LogisticRegression()\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n\ngrid_result = grid.fit(rescaledX, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","34d108eb":"# Random Forest Classifier\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\n\ncriterion = ['gini','entropy']\nmin_samples_split = [2,3,4]\nmax_features = ['auto','sqrt','log2']\noob_score = ['True','False']\n\nparam_grid = dict(criterion=criterion, min_samples_split=min_samples_split, max_features=max_features, oob_score=oob_score)\n\nmodel = RandomForestClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n\ngrid_result = grid.fit(rescaledX, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d45ee999":"# LinearDiscriminantAnalysis\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\n\nsolver = ['svd', 'lsqr', 'eigen']\nstore_covariance = ['True','False']\ntol = [0.0001,0.0002, 0.0003, 0.0004]\n\nparam_grid = dict(solver=solver, store_covariance=store_covariance, tol=tol)\n\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n\ngrid_result = grid.fit(rescaledX, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d2c8847b":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel_LR = LogisticRegression(C= 1.0, class_weight= 'balanced', fit_intercept= True, solver= 'liblinear')\nmodel_LR.fit(rescaledX, y_train)","7a9281d2":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel_RF = RandomForestClassifier(criterion ='entropy', max_features = 'auto', min_samples_split = 4, oob_score = True)\nmodel_RF.fit(rescaledX, y_train)","5f17e2ba":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel_LDA = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nmodel_LDA.fit(rescaledX, y_train)","9da2d306":"# Regress\u00e3o Logistica\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_LR.predict(rescaledValidationX)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","697ca478":"# Random Forest\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_RF.predict(rescaledValidationX)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","649f33f9":"# LDA\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_LDA.predict(rescaledValidationX)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","f195c192":"from imblearn.over_sampling import (RandomOverSampler,ADASYN,BorderlineSMOTE,\n                                    KMeansSMOTE,SMOTE,SVMSMOTE)\n\nfrom imblearn.under_sampling import (RandomUnderSampler,CondensedNearestNeighbour,\n                                     EditedNearestNeighbours,\n                                    RepeatedEditedNearestNeighbours,\n                                    NeighbourhoodCleaningRule,AllKNN,TomekLinks)\nfrom imblearn.pipeline import Pipeline","3a28d8df":"rus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_rus, y_rus)\npred_test = lda.predict(X_test)","2b68b119":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","1d90fc0e":"cnn = CondensedNearestNeighbour()\nX_cnn, y_cnn = cnn.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_cnn, y_cnn)\npred_test = lda.predict(X_test)","58e948fd":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","6d0a396f":"enn = EditedNearestNeighbours()\nX_enn, y_enn = enn.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_enn, y_enn)\npred_test = lda.predict(X_test)","ad683ffa":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","ed945705":"renn = RepeatedEditedNearestNeighbours()\nX_renn, y_renn = renn.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_renn, y_renn)\npred_test = lda.predict(X_test)","b894ffe6":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","85895b8b":"ncr = NeighbourhoodCleaningRule()\nX_ncr, y_ncr = ncr.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_ncr, y_ncr)\npred_test = lda.predict(X_test)","b8ce3985":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","7be7ce08":"akn = AllKNN()\nX_akn, y_akn = akn.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_akn, y_akn)\npred_test = lda.predict(X_test)","b0955d8b":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","e19e7197":"tl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_tl, y_tl)\npred_test = lda.predict(X_test)","0a0f5dc5":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","8ee75f36":"ros = RandomOverSampler()\nX_ros, y_ros = ros.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_ros, y_ros)\npred_test = lda.predict(X_test)","ee4195df":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","fcde6d71":"adn = ADASYN()\nX_adn, y_adn = adn.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_adn, y_adn)\npred_test = lda.predict(X_test)","dea59677":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","f9a4cb50":"bsm = BorderlineSMOTE()\nX_bsm, y_bsm = bsm.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_bsm, y_bsm)\npred_test = lda.predict(X_test)","1a633ba0":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","b1de1351":"sm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X_train,y_train)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_sm, y_sm)\npred_test = lda.predict(X_test)","c6b32b21":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","cb611bc9":"smo = SVMSMOTE()\nX_smo, y_smo = smo.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_smo, y_smo.values.ravel())\npred_test = lda.predict(X_test)","328f001d":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","34ae7ef2":"kms = KMeansSMOTE()\nX_kms, y_kms = kms.fit_resample(X,y)\nlda = LinearDiscriminantAnalysis(solver = 'svd', store_covariance = True, tol = 0.0001)\nlda.fit(X_kms, y_kms)\npred_test = lda.predict(X_test)","d5c70728":"print(accuracy_score(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\nprint(confusion_matrix(y_test, pred_test))","34815b14":"from xgboost import XGBClassifier","8b0180d4":"tl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(X,y)\nxgb = XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.94,random_state=0, stratify=y)\nxgb.fit(X_train, y_train.values.ravel())\npred = xgb.predict(X_test)\nprint('Acur\u00e1cia: ', accuracy_score(y_test,pred))\nprint('Classification report:\\n', classification_report(y_test, pred))\nprint('Confusion matrix:\\n', confusion_matrix(y_test, pred))","73e4e362":"### Percebemos h\u00e1 desbalanceamento entre as classes. Muitos valores para o caso '0' de n\u00e3o fraude, e poucos para o caso '1' de fraude.\n### Ou seja, uma maior ocorrencia de a\u00e7\u00f5es n\u00e3o fraudulentas.\n### Temos um caso de dataframe desbalanceado. Se usarmos ele na forma original teremos problemas como o Overfitting, nosso modelo assumiria que na maioria dos casos n\u00e3o h\u00e1 fraudes.","5c850a87":"## ADASYN","207c4c79":"### Definir as minhas features e a label.","ce19204c":"### Plotar um boxplot com a acur\u00e1cia dos modelos que foram definidos para compara\u00e7\u00e3o.","352a1bc9":"### Qual modelo foi melhor usando o StandardScaler.\n### Observar o boxplot abaixo e escolher tres t\u00e9cnicas para comparar.","bb12501f":"### Aplicar uma padroniza\u00e7\u00e3o nos dados.","69309ba6":"* ### Esse \u00e9 um dataset de detec\u00e7\u00e3o de fraude em cart\u00e3o de cr\u00e9dito que est\u00e1 dispon\u00edvel para uso no Kaggle.\n* ### O objetivo \u00e9 identificar transa\u00e7\u00f5es fraudulentas em cat\u00f5es de cr\u00e9dito.\n* ### Este dataset contem informa\u00e7\u00f5es sobre as transa\u00e7\u00f5es feitas por cart\u00e3o de cr\u00e9dito em setembro de 2013 por titulares de cart\u00e3o europeu e essas transa\u00e7\u00f5es ocorreram em dois dias.\n* ### O dataset possue uma vari\u00e1vel que est\u00e1 muito desbalanceada, 492 casos de fraudes e 284316 casos de n\u00e3o fraude, com isso percebemos que se trata de um dataset desbalanceado.\n* ### E qual seria o algoritmo mais adequado para esse conjunto de dados? Para isso vamos utilizar alguns algoritmos de classifica\u00e7\u00e3o para saber qual o melhor para esse caso.\n* ### Como o conjunto de dados est\u00e1 desbalanceado, se utilizarmos qualquer algoritmo de classifica\u00e7\u00e3o sem antes balancear a vari\u00e1vel veremos que o classificador tende a prever a classe com maior n\u00famero de observa\u00e7\u00f5es.","92f26650":"### O c\u00e1lculo da amostra foi realizado no site: https:\/\/www.solvis.com.br\/calculos-de-amostragem\/\n### Este par\u00e2metro stratify faz uma divis\u00e3o para que a propor\u00e7\u00e3o de valores na amostra produzida seja a mesma que a propor\u00e7\u00e3o de valores fornecidos para estratificar o par\u00e2metro.","1b17f68d":"## CondensedNearestNeighbour","804bfe4b":"### Para mais informa\u00e7\u00f5es sobre os parametros do K-Fold acesse o link: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html","b9fe34c0":"### Primeiramente eu vou importar o dataset para ele ser lido e depois come\u00e7ar as opera\u00e7\u00f5es dentro dele.","af15e0e9":"### Criar um gr\u00e1fico com a vari\u00e1vel 'Class' para observar a distribui\u00e7\u00e3o da classe.","b9e7ebbe":"## TomekLinks","b231226e":"# An\u00e1lise explorat\u00f3ria dos dados","46110ef4":"### Sobre o cross-validation: O sklearn tem o train_test_split que serve para dividir o dataset rapidamente em treino e testel. Ao avaliar os parametros para o estimador podemos correr o risco de dar overffiting no conjunto de teste, pois os parametros podem ser ajustados at\u00e9 que ele funcione de forma \u00f3tima. Com isso, o conhecimento sobre os dados de teste podem vazar para modelo e isso pode causar problemas nas m\u00e9tricas de avalia\u00e7\u00e3o. Para resolver este problema criammos os dados de valida\u00e7\u00e3o que procede nos dados de treinamento.\n### Particionar os dados em 3 conjuntos reduzimos o n\u00famero de amostras que podem ser usadas no modelo.\n### Uma solu\u00e7\u00e3o para o problema \u00e9 a valida\u00e7\u00e3o cruzada, ou cross-validation. Nele o conjunto de valida\u00e7\u00e3o n\u00e3o \u00e9 mais necess\u00e1rio. Na abordagem b\u00e1sica, chamada de k-fold CV, o conjunto de treinamento \u00e9 dividido em k sets menores.","a62c48d4":"### Explicar o dataset\n### Explicar a dificuldade de se tratar datasets desbalanceados e explicar o algoritmo adequado para esse dataset\n### qualquer classificador tende a prever a classe majorit\u00e1ria","f45d5fb1":"### O \"num_folds\" diz o quanto eu vou dividir a dataframe.\n### O \"seed\" \u00e9 a vari\u00e1vel para o random_state.\n### O \"scoring\" \u00e9 para dizer qual m\u00e9todo vou usar no modelo.","901ace8f":"### O LinearDiscriminantAnalysis \u00e9 um classificador com um limite de decis\u00e3o linear gerado pela adapta\u00e7\u00e3o das densidades condicionais de classe aos dados usando a regra de Bayes.\n### O Teorema de Bayes \u00e9 \u00e9 uma f\u00f3rmula matem\u00e1tica usada para o c\u00e1lculo da probabilidade de um evento dado que outro evento j\u00e1 ocorreu, o que \u00e9 chamado de probabilidade condicional. Ou seja, eu preciso ter alguma informa\u00e7\u00e3o anterior do evento.\n\n\n\n### O LDA faz algumas suposi\u00e7\u00f5es  sobre os seu dados: que seus dados s\u00e3o Guassianos e que cada vari\u00e1vel tem a forma de uma curva de sino quando plotada e que cada atributo tem a mesma variancia. Com essas suposi\u00e7\u00f5es o modelo LDA estima a m\u00e9dia e variancia dos seus dados.\n\n\n### Leia mais em: https:\/\/www.voitto.com.br\/blog\/artigo\/teorema-de-bayes \n### e em: https:\/\/machinelearningmastery.com\/linear-discriminant-analysis-for-machine-learning\/","79b09354":"### Acima o melhor tipo de balanceamento foi utilizando o UnderSampling j\u00e1 com o melhor modelo.","50b96213":"### Visualizar a quantidade de linhas e colunas no nosso dataframe.","51ab36ee":"## EditedNearestNeighbours","edfd1888":"### Fazer um gr\u00e1fico de calor com a correla\u00e7\u00e3o das vari\u00e1veis.\n### O gr\u00e1fico mostra quais variaveis est\u00e3o correlacionadas uma com a outra numa escala de 1 sendo a mais correlacionada e -1 sendo a menos correlacionada.","8cdb81a4":"### Importar o que vou utilizar nas pr\u00f3ximas c\u00e9lulas.","a8e9954f":"### Temos 284807 linhas e 31 colunas.","b4a29868":"### A ideia do StandardScaler \u00e9 que ele transformar\u00e1 seus dados de forma que sua distribui\u00e7\u00e3o tenha um valor m\u00e9dio 0 e desvio padr\u00e3o de 1. No caso de dados multivariados, isto \u00e9 feito independente para cada coluna dos dados. Dada a distribui\u00e7\u00e3o dos dados, cada valor no conjunto de dados ter\u00e1 o valor m\u00e9dio subtra\u00eddo, e ent\u00e3o dividido pelo desvio padr\u00e3o de todo o conjunto de dados.\n\n### Voce pode procurar mais sobre o StandardScaler no link: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html","379e739a":"### para lembrar: setar parametros do kfold","4650051b":"### Mas primeiramente vamos comparar alguns modelos para saber qual deles \u00e9 o melhor com o nosso dataframe.","46587f5b":"## KMeansSMOTE","6b7cdbd6":"### Otimiza\u00e7\u00e3o dos parametros via GridSearch.","c971ee9c":"### A partir do gr\u00e1fico de boxplot acima eu escolhi tr\u00eas modelos para comparar e saber qual deles \u00e9 o melhor com os nossos dados. Escolhi o RandomForestClassifier, LogisticRegression e o LDA.","140f6660":"## NeighbourhoodCleaningRule","34a5e6b9":"## RandomUnderSampler","3a63fcfb":"### Criar um Pipeline, usando o StandardScaler, com os nossos modelos.","acb99ca6":"## SVMSMOTE","d2a14f62":"### Dividir o dataframe utilizando o train_test_split e verificar o seu tamanho.","d2c4c86a":"### Visualizar as vari\u00e1veis do dataframe com suas informa\u00e7\u00f5es sobre qual tipo de dado ela \u00e9.","5d1cbd69":"## SMOTE","f3ab9c32":"## RepeatedEditedNearestNeighbours","e6f99359":"### Criar uma lista vazia e ir adicionando os modelos de classifica\u00e7\u00e3o para fins de compara\u00e7\u00e3o, ou seja, fazer um pipeline.\n### Agora \u00e9 hora de aplicar o kfold e o cross validation score nos modelos.","6a44bf66":"### Checar se h\u00e1 valores nulos.","ea8252b5":"## RandomOverSampler","1833aff3":"# Agora com Over Sampling","fffaf93c":"## O melhor modelo foi LDA.","1804799d":"### Como foi visto o dataframe cont\u00e9m apenas vari\u00e1veis num\u00e9ricas de entrada que s\u00e3o o resultado de uma transforma\u00e7\u00e3o da An\u00e1lise de Componentes Principais. Infelizmente, devido a quest\u00f5es de confidencialidade, n\u00e3o podemos fornecer as caracter\u00edsticas originais e mais informa\u00e7\u00f5es de fundo sobre os dados.","61d648e0":"## BorderlineSMOTE","86c48e85":"# Come\u00e7ar com o Under Sampling","d3c0ac08":"### Para lidar com as classes desbalanceadas podemos usar uma biblioteca chamada imbalanced-learn. Para isso basta importa-la.\n### Agora devemos pensar se vamos usar Undersampling ou Oversampling. Undersampling visa balancear a classe a partir da classe majorit\u00e1ria, j\u00e1 o Oversampling replica os exemplos da classe minorit\u00e1ria para que tenhamos uma classe balanceada.","d92a5678":"### Olhando com est\u00e1 o dataset sem modifica\u00e7\u00f5es","c139c943":"### N\u00e3o existe valores nulos no dataframe.","32db86cc":"### Yelowbrick \u00e9 um conjunto de ferramentas de an\u00e1lise visual e diagn\u00f3stico. Todas as visuliza\u00e7\u00f5es s\u00e3o geradas em Matplotlib.","0e0c3b71":"# Tratar o desbalanceamento.","b5e496e4":"## Para m\u00e9todos de compara\u00e7\u00e3o vamos usar o modelo de XGBOOST com o melhor sampler.","d399a7c5":"## Colunas do dataset\n* ### Time(Tempo): N\u00famero de segundos transcorridos entre esta transa\u00e7\u00e3o e a primeira transa\u00e7\u00e3o no conjunto de dados.\n* ### V1 a V28: pode ser resultado de uma redu\u00e7\u00e3o da dimens\u00e3o da An\u00e1lise de Componentes Principais(PCA) para proteger as identidades dos usu\u00e1rios e caracter\u00edsticas sens\u00edveis. Est\u00e3o no site: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n* ### Amount(Valor): valor da transa\u00e7\u00e3o\n* ### Class: 1 para transa\u00e7\u00f5es fraudulentas e 0 para n\u00e3o fraudulentas.","9156c272":"## AllKNN"}}