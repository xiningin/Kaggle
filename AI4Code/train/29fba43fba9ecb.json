{"cell_type":{"566fbf68":"code","457fe4fb":"code","a9ee10ac":"code","23d256f5":"code","f7e8a4b4":"code","33493abb":"code","638cfd49":"code","bd4aea70":"code","96e77fa6":"code","5c45b61d":"code","f2005c07":"code","1cd21ed0":"code","2141042b":"code","87d60b01":"code","d49b4e42":"code","d4878951":"code","8602dc93":"code","f8479f0f":"code","9f2053cb":"code","97bdce52":"code","fc787407":"code","da7ba803":"code","09cec1ce":"markdown","64a366fe":"markdown","4f9552c0":"markdown","093fa3a4":"markdown","cc3acf7b":"markdown","21162b46":"markdown","90344224":"markdown","ce21f1b0":"markdown","a383e326":"markdown","a26af676":"markdown","d7196437":"markdown","dd6a82a1":"markdown","529dc2ff":"markdown","299fc617":"markdown","16def402":"markdown"},"source":{"566fbf68":"import seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb","457fe4fb":"\ndf=pd.read_csv('\/kaggle\/input\/did-it-rain-in-seattle-19482017\/seattleWeather_1948-2017.csv')\ndf.head()","a9ee10ac":"df.info()","23d256f5":"df.describe()","f7e8a4b4":"df['rain'] = df['RAIN'].map({True:1 ,False:0}) \ndel df['RAIN']\ndf.head()","33493abb":"sns.countplot(x=df['rain'].values)","638cfd49":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(14,6))\ndf['TMAX'].plot()","bd4aea70":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='rain',y='TMAX',data=df,palette='winter')","96e77fa6":"df.isna().sum()","5c45b61d":"import matplotlib as plt\n\nsns.distplot(a = df['PRCP'], kde=True,color='blue',bins=1)","f2005c07":"df['PRCP']=df['PRCP'].fillna(df['PRCP'].median())","1cd21ed0":"df['rain']=df['rain'].fillna(df['rain'].value_counts().index[0])","2141042b":"df.isnull().sum()","87d60b01":"df['DATE'] = pd.to_datetime(df['DATE'])","d49b4e42":"df['month']=pd.DatetimeIndex(df['DATE']).month\ndel df['DATE']","d4878951":"a=df.pivot_table(index=['rain'], columns='month', aggfunc='size', fill_value=0)\nkey=a.loc[1.0].index\nvalue=a.loc[1.0].values\n\nimport seaborn as sns\nax = sns.barplot(x=key, y=value)\n","8602dc93":"df = pd.get_dummies(df, columns = ['month'])","f8479f0f":"X=df.iloc[:,:-1]\ndel X['rain']\ny=df.loc[:,'rain']","9f2053cb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)","97bdce52":"xgb_m = xgb.XGBClassifier()\nparams = {\n'learning_rate': [.1,.4, .45, .5, .55, .6],\n'colsample_bytree': [.6, .7, .8, .9, 1],\n'booster':[\"gbtree\"],\n 'min_child_weight': [0.001,0.003,0.01],\n}\nxgb_cv = GridSearchCV(xgb_m, params, scoring = \"accuracy\", verbose = 0, cv = 5)\nxgb_cv.fit(X_train, y_train)\nbest_params = xgb_cv.best_params_\nprint(f\"Best parameters: {best_params}\")    \nxgb_m=xgb.XGBClassifier()   \nxgb_m.fit(X_train, y_train)","fc787407":"pred = xgb_m.predict(X_test)","da7ba803":"from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix,mean_squared_error,r2_score\n\nprint(round(f1_score(y_test, pred,average='binary'), 5))\nconfusion_matrix(y_test, pred)","09cec1ce":"<h2> LET'S HANDLE MISSING VALUES NOW.<\/h2>","64a366fe":"<h4> We can either drop the missing data to Fill it with Mean\/Median\/Mode. Ideally we have only 3 rows with missing data in PRCP column, so we can drop them..However, I don't prefer that. Instead we will impute the data i.e fill it with Mean\/Median\/Mode. However, let's find out how to figure out which one to use<\/h4>","4f9552c0":"<h1> PERFECTO. PLEASE UPVOTE IF YOU ENJOYED THE NOTEBOOK <\/h1> ","093fa3a4":"<p> Now let's see model F1 score and confusion Matrix <\/p>","cc3acf7b":"<h2> Now we have a column named DATE.<\/h2>\n<ul>\n<li>We will first convert it into date Format<\/li>\n <li>Then we will extract the month field from date<\/li>\n <li>We will then try to see if there is a relation between the month and rainfall<\/li>\n <li>If we see there is a relation, we shall keep the month feature in our model<\/li>\n<\/ul>\n    <\/h2>","21162b46":"<h3> Now let's fill the nulls in the target variable <\/h3>","90344224":"<h4> Let's visualize PRCP <\/h4>","ce21f1b0":"<p> Box Plot below clearly indicates there is a difference in TMAX when it rains and when it doesn't <\/p>","a383e326":"<h3> Clearly we can see, data has quite some outliers. Let's impute the missing values in this column with Median <\/h3>","a26af676":"**Now Enough with the theory.. Let's get our hands dirty. Let's extract the data and look at it and try to visualize it**","d7196437":"<h2> Let's begin with model building. I am running XGBoost here .First let's do a train test split <\/h2>","dd6a82a1":"<h1 style='text-align:center'> Boosting for the Win: Will it rain in Seattle?<\/h1>\n    \n<p> About the dataset <\/p>\n<ul>\n    <li>DATE: This field specifies the date correposnding to the row. <\/li>\n    <li>PRCP: This column specifies the amount of precipitation <\/li>\n    <li>TMAX: This column specifies the Maximum temperature of the day <\/li>\n    <li>TMIN: This column specifies the Minimum temperature of the day <\/li>\n    <li>RAIN: This column is the categorical target variable containing value-'True' if it rained or False if it didn't rain<\/li>\n<\/ul>\n\n<h1 style='text-align:center'> Introduction to Ensemble Learning <\/h1>\n\n<h3> What is ensemble learning?<\/h3>\n<p>Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. <\/p>\n<p> Let us take decision tree as an example. A Decision Tree determines the predictive value based on series of questions and conditions. For instance, this simple Decision Tree determining on whether an individual should play outside or not. The tree takes several weather factors into account, and given each factor either makes a decision or asks another question.However, if it is raining, we must ask if it is windy or not? If windy, we will not play. But given no wind, tie those shoelaces tight because were going outside to play.<\/p>\n\n<img src='https:\/\/miro.medium.com\/max\/750\/1*ML5ABmp7pxnZuhIokXlPCw.png'>\n<br>\n<p> When making Decision Trees, there are several factors we must take into consideration: On what features do we make our decisions on? What is the threshold for classifying each question into a yes or no answer? In the first Decision Tree, what if we wanted to ask ourselves if we had friends to play with or not. If we have friends, we will play every time. If not, we might continue to ask ourselves questions about the weather. By adding an additional question, we hope to greater define the Yes and No classes.<\/p>\n\n<h1> Why Ensemble Learning?<\/h1>\n\n<p>This is where Ensemble Methods come in handy! Rather than just relying on one Decision Tree and hoping we made the right decision at each split, Ensemble Methods allow us to take a sample of Decision Trees into account, calculate which features to use or questions to ask at each split, and make a final predictor based on the aggregated results of the sampled Decision Trees.<\/p>\n\n<h1> Types of Ensemble Learning<\/h1>\nThere are many ensemble techniques available but we will discuss about the below two most widely used methods:\n<ul>\n    <li> Bagging <\/li>\n    <li> Boosting<\/li>\n<\/ul>\n<p> To read more about their difference please read this awesome article <a herf=\"https:\/\/towardsdatascience.com\/types-of-ensemble-methods-in-machine-learning-4ddaf73879db\"> Here<\/a><\/p>\n ","529dc2ff":"<h3> Clearly we can see that there is a season when the days when it rained is much more. Hence we shall now consider this feature<\/h3>\n<p> We shal consider this as a categorical variable and one hot encode it <\/p>","299fc617":"<h2> I am personally not a huge fan of Boolean data type. So let's convert it to 1's and 0's <\/h2>","16def402":"<h3> Now let's do some data visualization to understand the data more. <\/h3>"}}