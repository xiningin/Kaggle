{"cell_type":{"9fc4c421":"code","e7fd59d3":"code","ead0d6a8":"code","6e41f19d":"code","483cf20e":"code","5d7bb1a9":"code","e66fd589":"code","665c337f":"code","755494df":"code","9bac369f":"code","68858cb9":"code","11700087":"code","48bfce1d":"code","fa68b557":"code","92a05824":"code","7577ae16":"code","ad68e561":"code","93cfe22d":"code","851e4c87":"code","c0cf4283":"code","650d2124":"code","ba52526f":"code","7367ffcd":"code","4b6167a3":"code","f54c9669":"code","a0b6b8c0":"markdown","4bc3ef9b":"markdown","0584dcf0":"markdown","20e0ef14":"markdown","fbede5eb":"markdown","d47506ec":"markdown","095b3095":"markdown","194d2954":"markdown","b26e2dd3":"markdown","9213ab3b":"markdown","15e2f75b":"markdown","d7676a49":"markdown","ca4dcb50":"markdown","7b61dcfd":"markdown","13947ab4":"markdown","66092ea1":"markdown","d525b61e":"markdown","04d24c2a":"markdown","e557179e":"markdown","f312a663":"markdown"},"source":{"9fc4c421":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_style(style='darkgrid')","e7fd59d3":"path = '\/kaggle\/input\/cern-electron-collision-data\/dielectron.csv'\ndf = pd.read_csv(path)\ndf.head()","ead0d6a8":"df.isnull().sum()","6e41f19d":"df.dropna(inplace=True)","483cf20e":"df.drop(columns=['Run', 'Event'], inplace=True)","5d7bb1a9":"corr = df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True, fmt='0.1f');","e66fd589":"plt.figure(figsize=(10,10), tight_layout=True)\nr,c = 3, 3\nplt.subplot(r,c,1)\nsns.scatterplot(x=df['E1'], y=df['pt1']);\nplt.subplot(r,c,2)\nsns.scatterplot(x=df['px1 '], y=df['px2']);\nplt.subplot(r,c,3)\nsns.scatterplot(x=df['phi1'], y=df['py1']);\nplt.subplot(r,c,4)\nsns.scatterplot(x=df['py1'], y=df['py2']);\nplt.subplot(r,c,5)\nsns.scatterplot(x=df['pz1'], y=df['pz2']);\nplt.subplot(r,c,6)\nsns.scatterplot(x=df['pt1'], y=df['pz2']);\nplt.subplot(r,c,7)\nsns.scatterplot(x=df['E2'], y=df['pt2']);\nplt.subplot(r,c,8)\nsns.scatterplot(x=df['pz2'], y=df['eta2']);\nplt.subplot(r,c,9)\nsns.scatterplot(x=df['phi2'], y=df['py2']);","665c337f":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","755494df":"X = df.drop(columns='M')\ny = df[['M']].squeeze()","9bac369f":"X.describe()","68858cb9":"xTr, xTs, yTr, yTs = train_test_split(X, y, test_size = 0.25)","11700087":"scaler = MinMaxScaler()\nxTr = scaler.fit_transform(xTr)\nxTs = scaler.transform(xTs)","48bfce1d":"reg = LinearRegression()\nreg.fit(xTr, yTr)\nyPred = reg.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","fa68b557":"plt.figure(figsize=(10,8))\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred);","92a05824":"from sklearn.ensemble import RandomForestRegressor","7577ae16":"reg = RandomForestRegressor()\nreg.fit(xTr, yTr)\nyPred = reg.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","ad68e561":"plt.figure(figsize=(10,8))\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred);","93cfe22d":"plt.figure(figsize=(10,8))\nfeature_importance = reg.feature_importances_\nidx = np.argsort(-feature_importance, )\nsns.barplot(x=df.columns[idx] ,y=feature_importance[idx]);","851e4c87":"import tensorflow as tf","c0cf4283":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n            tf.keras.layers.Dense(units=32, input_shape=(xTr.shape[1],), activation='relu', name='FC1'), \n            tf.keras.layers.Dense(units=8, activation='relu', name='FC2'), \n            tf.keras.layers.Dense(units=1, name='Output')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss=tf.keras.losses.mean_squared_error)\ninit = 0\nmodel.summary()","650d2124":"class LogMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 10 == 9:\n            val_loss = mean_squared_error(yTs, self.model.predict(xTs))\n            print(f'Epoch: {epoch+1}\/{self.params[\"epochs\"]} - val_loss: {val_loss:0.4f} ')","ba52526f":"xtr, xval, ytr, yval = train_test_split(xTr, yTr, test_size=0.1)\nepochs = 500\nhist = model.fit(xtr, ytr, validation_data=(xval, yval), epochs=epochs, verbose=0, batch_size=128, \n                 callbacks=[LogMetrics(), tf.keras.callbacks.EarlyStopping(min_delta=0.1, patience=5)])","7367ffcd":"plt.figure(figsize=(8,4))\nsplot = sns.lineplot(data=hist.history);\nsplot.set(xscale=\"log\");","4b6167a3":"yPred = model.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","f54c9669":"plt.figure(figsize=(10,8))\nsns.set_style(style='darkgrid')\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred.squeeze());","a0b6b8c0":"Using random forest, the model performs better on test data with relatively small MSE and higher R2. Since the model performed well, let us understand which features are important in predicting the target variable and their ranking.","4bc3ef9b":"Since only 0.085% of the data is missing, let us drop those rows.","0584dcf0":"Applying minmax scaling","20e0ef14":"A correlation matrix helps in understanding which features directly affect the target variable.","fbede5eb":"A graph of actual vs predicted gives how well the model fits the data","d47506ec":"## Random forest\n\nSince the data is not linear, to design a more complex model, random forest of decision trees can be applied.","095b3095":"From the heatmap, it can be observed that target variable, `M` is dependent directly on `[E1, pt1, E2, pt2]`. \\\\\nFrom the correlation matrix, it can be observed that some features correlated to each other. Let us visualize some of the correlation.","194d2954":"Passing early stopping callback to avoid overfitting.","b26e2dd3":"Fit a linear model and check the MSE and R2 for test data","9213ab3b":"Scaling the data helps in increasing the performance of the regression model. Let us split the data into train and test set.","15e2f75b":"The given dataset is not linear on the target variable, since MSE is a large number, and R2 is small.","d7676a49":"Before applying linear regression, let us check the data distribution","ca4dcb50":"Building a FC NN model with 2 hidden layers. One with 32 units another with 8 units. The model has 817 trainable parameters.","7b61dcfd":"`[pt2, pt1, E2, E1, pz2, eta1, eta2, px2, py1, py2, px1, Event, phi2, phi1, Run, Q2, Q1]` is the order of importance of the features in predicting the target `M`.","13947ab4":"### Exploratory Data Analysis\n\nCheck for missing values.","66092ea1":"Writing a custom callback function to log MSE of validation data every 10 epochs.","d525b61e":"A graph of traing loss vs valiation loss gives insight on model overfitting\/ underfitting","04d24c2a":"## Fully Connected Neural Network\n\nEventhough with random forest MSE reduced sigificantly, and model predicts with relatively high accuracy, neural network can be built for the model since 100k data is available with only 16 fetures.","e557179e":"Features `[Run, Event]` are the run number and event numbers which does not contribute to the target variable and can be dropped.","f312a663":"## Linear Regression\n\nSince the target is floating value, linear regression can be applied on the dataset."}}