{"cell_type":{"768abc69":"code","2100890b":"code","7d592623":"code","561c41ad":"code","95edd060":"code","ed590123":"code","81339c68":"code","f5ee54cd":"code","f19e8411":"code","5c8614b8":"code","a63894cd":"code","3248c758":"code","c05dda1b":"code","d83bf695":"code","901613f3":"code","dec4b4d4":"code","7520fea3":"code","e9a5fd5e":"code","15d0f262":"code","d15b389b":"code","56519756":"code","27ef94a5":"code","bee1bde3":"code","f037a2b4":"code","8301a8a0":"code","3972f399":"code","415f0c25":"code","86a54451":"code","5454bdac":"code","fd69404d":"code","af524516":"code","ca93bb62":"code","f9642d90":"code","4bec3f8c":"code","9f2e681c":"code","9cc20bf9":"code","d13d0926":"code","e8a0a27a":"code","b9dfcd80":"code","0ee1e452":"code","70d7e1df":"markdown","2f1cc6a6":"markdown","7f761103":"markdown","8f35f01a":"markdown","717974a0":"markdown","280a14a9":"markdown","8c77d09f":"markdown","c6562dd0":"markdown","9455a9c7":"markdown","101cc9f4":"markdown","7f26f3d0":"markdown","2929e0b3":"markdown","948fc011":"markdown","da5b922e":"markdown","1bf33b48":"markdown"},"source":{"768abc69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Any results you write to the current directory are saved as output.","2100890b":"# Importar datos \ntrain= pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\np=pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","7d592623":"train.isnull().sum()","561c41ad":"test.isnull().sum()","95edd060":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df2 = pd.DataFrame([survived,dead])\n    df2.index = ['Survived','Dead']\n    df2.plot(kind='bar',stacked=True, figsize=(10,5))","ed590123":"#separar de los nombres el titulo que posee cada persona y agregarlo como un nuevo dato a los sets\ncombinado = [train,test]\nfor dataset in combinado:\n    dataset ['Titulo']= dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntrain['Titulo'].value_counts()","81339c68":"#Agrupar los Titulos en categorias mas generales \nfor dataset in combinado:\n    dataset['Titulo'] = dataset['Titulo'].replace([ 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Otros')\n    dataset['Titulo'] = dataset['Titulo'].replace(['Countess', 'Lady', 'Sir'], 'Otros')\n    dataset['Titulo'] = dataset['Titulo'].replace('Mlle', 'Miss')\n    dataset['Titulo'] = dataset['Titulo'].replace('Ms', 'Miss')\n    dataset['Titulo'] = dataset['Titulo'].replace('Mme', 'Mrs')\n    dataset['Titulo'] = dataset['Titulo'].fillna('Otros')\n    \ntrain[['Titulo', 'Survived']].groupby(['Titulo'], as_index=False).mean()","f5ee54cd":"bar_chart('Titulo')","f19e8411":"#One Hot Encoding por cada Titulo  \ntrain = pd.concat([train, pd.get_dummies(train['Titulo'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['Titulo'])], axis=1)\n\n\ntrain=train.drop(labels=['Name','Titulo'], axis=1) \ntest=test.drop(labels=['Name','Titulo'], axis=1)\n\n","5c8614b8":"#agregar columna de tiene Cabina para one hot encoding \ntest['Tiene_Cabina'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntrain['Tiene_Cabina'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\ntrain[['Tiene_Cabina', 'Survived']].groupby(['Tiene_Cabina'], as_index=False).mean()","a63894cd":"bar_chart('Tiene_Cabina')","3248c758":"train=train.drop(labels=['Cabin'], axis=1)\ntest=test.drop(labels=['Cabin'], axis=1) ","c05dda1b":"#One Hot Encoding  para cada sexo   \ntrain = pd.concat([train, pd.get_dummies(train['Sex'])], axis=1) \ntest = pd.concat([test, pd.get_dummies(test['Sex'])], axis=1)\n\n","d83bf695":"bar_chart('Sex')","901613f3":"train=train.drop(labels=['Sex'], axis=1)\ntest=test.drop(labels=['Sex'], axis=1)","dec4b4d4":"#LLenar valores de enbarked faltantes con s:\ntrain = train.fillna({\"Embarked\": \"S\"})\n\n","7520fea3":"bar_chart('Embarked')","e9a5fd5e":"#One Hot Encoding  para cada sitio de embarque \ntrain = pd.concat([train, pd.get_dummies(train['Embarked'])], axis=1) \ntest = pd.concat([test, pd.get_dummies(test['Embarked'])], axis=1)\n\ntrain=train.drop(labels=['Embarked'], axis=1)\ntest=test.drop(labels=['Embarked'], axis=1)","15d0f262":"#Quitar valores null de el dato edad\ncombinado = [train,test]\nfor dataset in combinado:\n    media=dataset['Age'].mean()\n    std=dataset['Age'].std()\n    cantidad=dataset['Age'].isnull().sum()\n    #Seleccionar una edad aleatoria con los datos disponibles del dataset entero\n    edad= np.random.randint(media - std, media + std, size=cantidad) \n    #De los datos null se asigna a uno aleatorio la edad seleccionada\n    dataset['Age'][np.isnan(dataset['Age'])] = edad\n    dataset['Age'] = dataset['Age'].astype(int)\n\n","d15b389b":"train=train.drop(labels=['Ticket'], axis=1)\ntest=test.drop(labels=['Ticket'], axis=1)","56519756":"combinado = [train,test]\nfor dataset in combinado:\n    dataset['Familia']=dataset['Parch']+dataset['SibSp']+1\ntrain=train.drop(labels=['Parch','SibSp'], axis=1)\ntest=test.drop(labels=['Parch','SibSp'], axis=1)","27ef94a5":"train = pd.concat([train, pd.get_dummies(train['Pclass'])], axis=1) \ntest = pd.concat([test, pd.get_dummies(test['Pclass'])], axis=1)\ntrain=train.drop(labels=['Pclass'], axis=1)\ntest=test.drop(labels=['Pclass'], axis=1)\ntrain.head()","bee1bde3":"train=train.drop(labels=['PassengerId'], axis=1)\ntest.head()","f037a2b4":"y_train= train['Survived']\nX_train= train.drop(labels='Survived', axis=1)\nX_test= test.drop(labels='PassengerId',axis=1)","8301a8a0":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test= sc.fit_transform(X_test)","3972f399":"X_train,val_dfX,y_train, val_dfY = train_test_split(X_train,y_train , test_size=0.10, stratify=y_train)\nprint(\"Entrenamiento: \",X_train.shape)\nprint(\"Validacion : \",val_dfX.shape)\n","415f0c25":"from keras.layers import Input, Dense, BatchNormalization, Add, GaussianNoise, Dropout\nfrom keras.models import Model\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom sklearn.metrics import roc_auc_score","86a54451":"precisiones_globales=[]\n\ndef graf_model(train_history):\n    f = plt.figure(figsize=(15,10))\n    ax = f.add_subplot(121)\n    ax2 = f.add_subplot(122)\n    # summarize history for accuracy\n    ax.plot(train_history.history['binary_accuracy'])\n    ax.plot(train_history.history['val_binary_accuracy'])\n    ax.set_title('model accuracy')\n    ax.set_ylabel('accuracy')\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(train_history.history['loss'])\n    ax2.plot(train_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \ndef precision(model, registrar=False):\n    y_pred = model.predict(X_train)\n    train_auc = roc_auc_score(y_train, y_pred)\n    y_pred = model.predict(val_dfX)\n    val_auc = roc_auc_score(val_dfY, y_pred)\n    print('Train AUC: ', train_auc)\n    print('Vali AUC: ', val_auc)\n    if registrar:\n        precisiones_globales.append([train_auc,val_auc])","5454bdac":"arquitectura=[16,18,16,14,12]     \nfirst =True\ninp = Input(shape=(17,))\nfor capa in arquitectura:        \n    if first:\n        x=Dense(capa, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='zeros')(inp)            \n        first = False\n    else:\n        x=Dense(capa, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='zeros')(x)  \nx=Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='zeros')(x)  \nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n\nprint(model.summary())\n\n","fd69404d":"train_history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(val_dfX, val_dfY),verbose=2)","af524516":"graf_model(train_history)","ca93bb62":"precision(model, True)","f9642d90":"print(\"Entrenamiento: \",X_train.shape)\nprint(\"Validacion : \",val_dfX.shape)","4bec3f8c":"def create_model_R():   \n    inp = Input(shape=(17,))\n    x=Dropout(0.1)(inp)\n    x=Dense(20, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(inp)\n    x=Dropout(0.2)(x)\n    x=Dense(18, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    x=Dropout(0.2)(x)\n    x=Dense(16, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    x=Dropout(0.1)(x)\n    x=Dense(14, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    x=Dropout(0.1)(x)\n    x=Dense(12, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    x=Dropout(0.1)(x)\n    x=Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n    return model\nmodel_R = create_model_R()\nprint(model_R.summary())","9f2e681c":"train_history_R = model_R.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(val_dfX, val_dfY),verbose=2)","9cc20bf9":"graf_model(train_history_R)","d13d0926":"precision(model_R, True)","e8a0a27a":"y_pred = model_R.predict(X_test)\n\ny_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': y_final})\n","b9dfcd80":"output.to_csv('prediction.csv', index=False)\npred = pd.read_csv('prediction.csv')\npred.head()","0ee1e452":"pred.head(418)","70d7e1df":"## 7- Familia","2f1cc6a6":"# Normalizar los Datos","7f761103":"# Modelo de la Red","8f35f01a":"## 4- Enbarque","717974a0":"## 1-Nombre","280a14a9":"# Prediccion con el Modelo","8c77d09f":"# Limpieza de Datos","c6562dd0":"## 3- Sexo","9455a9c7":"La variable Ticket se descidio ignorar debido a que esta no aporta ningun significado de valor para el modelo ya que hay muchos formatos del dato ticket ","101cc9f4":"## Arquitectura sin Regularizaci\u00f3n","7f26f3d0":"## Arquitectura no Regularizada","2929e0b3":"## 6- Ticket","948fc011":"## 8- Clase","da5b922e":"## 2-Cabina\n\nPara el dato de cabina al tener muchos datos null se asume que puede ser gente que notiene cabina asignada","1bf33b48":"## 5- Edad"}}