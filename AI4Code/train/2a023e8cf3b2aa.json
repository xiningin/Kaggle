{"cell_type":{"aba4fb7a":"code","557d47f6":"code","523e439a":"code","2f919040":"code","2c9edc72":"code","9afe86ed":"code","aa3d8442":"code","54f135bf":"code","f3a3c195":"code","6b081522":"code","d2a32dbf":"code","0995f400":"code","809f108c":"code","4cfba9d0":"code","69ce4477":"code","34c4884f":"code","b0517b1a":"code","8bb9304d":"code","66c1a2d3":"code","dcced6e6":"code","b3ab2d70":"markdown","54249b20":"markdown","e4a70ce9":"markdown","5dd8a6ce":"markdown","8cfedb1f":"markdown","74bf3d19":"markdown","69b9c273":"markdown"},"source":{"aba4fb7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","557d47f6":"#import the data\ntrain_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","523e439a":"from sklearn.model_selection import KFold\ntrain_df['kfold'] = -1\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=train_df)):\n  train_df.loc[valid_indicies, 'kfold'] = fold","2f919040":"train_df.kfold.value_counts()","2c9edc72":"for i in range(0,5):\n  train_df[train_df.kfold == i].song_popularity.hist()\n  plt.show()","9afe86ed":"train_df.hist(figsize=(30,15))","aa3d8442":"train_df.isna().sum()","54f135bf":"null_columns = ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'key']","f3a3c195":"numerical_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n       'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo',\n       'audio_valence']","6b081522":"!pip install impyute","d2a32dbf":"from impyute.imputation.cs import mice\n# handling the missing data using impyute \n\ntrain_df[null_columns] = mice(train_df[null_columns].values)\ntest_df[null_columns] = mice(test_df[null_columns].values)","0995f400":"features = [f for f in train_df.columns if f not in('id','kfold','song_popularity')]\ntest_df = test_df[features]","809f108c":"from catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import RobustScaler\nprediction = []\nscore = []\ncategorical_features_indices = [5, 8, 11]\nfor fold in range (5):\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    X_test = test_df.copy()\n  \n    # dependent variables \n    y_train = X_train.song_popularity\n    y_val = X_val.song_popularity\n\n    # independent variables\n    X_train = X_train[features]\n    X_val = X_val[features]\n\n    # preprocessing the data \n    # handling the misssing values \n    # X_train[null_columns] = mice(X_train[null_columns].values)\n    # X_val[null_columns] = mice(X_val[null_columns].values)\n    # X_test[null_columns] = mice(X_test[null_columns].values)\n\n    scaler = RobustScaler()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n    X_train[['key', 'audio_mode', 'time_signature']] = X_train[['key', 'audio_mode', 'time_signature']].astype(int)\n    X_val[['key', 'audio_mode', 'time_signature']] = X_val[['key', 'audio_mode', 'time_signature']].astype(int)\n    X_test[['key', 'audio_mode', 'time_signature']] = X_test[['key', 'audio_mode', 'time_signature']].astype(int)\n\n\n    # catboost modelling \n    # model=CatBoostRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01, early_stopping_rounds=5)\n    # model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_val, y_val),plot=True)\n\n    # XGBRegressor moddelling \n    model = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    test_predict = model.predict(X_test)\n    prediction.append(test_predict)\n    roc1= roc_auc_score(y_val,preds_valid)\n\n    #Score \n    score.append(roc1)\n    print(f\"fold:{fold},roc:{roc1}\")\n\nprint(np.mean(score),np.std(score))\n\n\n\n","4cfba9d0":"final_predict = np.mean(np.column_stack(prediction),axis=1)","69ce4477":"df_test = pd.DataFrame(final_predict, columns=['song_popularity'])\ndf_test","34c4884f":"import optuna \n\ndef hyp_optimizer(trial):\n    categorical_features_indices = [5, 8, 11]\n    fold = 0\n    # hyperparameters for XGBoost\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1,7)\n\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    # X_test = test_df.copy()\n\n    # dependent variables \n    y_train = X_train.song_popularity\n    y_val = X_val.song_popularity\n\n    # independent variables\n    X_train = X_train[features]\n    X_val = X_val[features]\n\n    scaler = RobustScaler()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n    # X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n    X_train[['key', 'audio_mode', 'time_signature']] = X_train[['key', 'audio_mode', 'time_signature']].astype(int)\n    X_val[['key', 'audio_mode', 'time_signature']] = X_val[['key', 'audio_mode', 'time_signature']].astype(int)\n    # X_test[['key', 'audio_mode', 'time_signature']] = X_test[['key', 'audio_mode', 'time_signature']].astype(int)\n\n\n    # catboost modelling \n    # model=CatBoostRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01, early_stopping_rounds=5)\n    # model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_val, y_val),plot=True)\n\n    # XGBRegressor moddelling \n    model = XGBRegressor(\n      tree_method='gpu_hist', \n      gpu_id=0, predictor='gpu_predictor',\n      learning_rate=learning_rate, \n      reg_lambda=reg_lambda,\n      reg_alpha=reg_alpha,\n      subsample=subsample,\n      colsample_bytree=colsample_bytree,\n      max_depth=max_depth,\n      )\n\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    # test_predict = model.predict(X_test)\n    # prediction.append(test_predict)\n    roc1= roc_auc_score(y_val,preds_valid)\n\n    #Score \n    # score.append(roc1)\n    # print(f\"fold:{fold},roc:{roc1}\")\n\n    return roc1","b0517b1a":"study = optuna.create_study(direction='maximize')\nstudy.optimize(hyp_optimizer, n_trials=100)","8bb9304d":"print(study.best_params)\nprint(study.best_trial)","66c1a2d3":"prediction = []\nscore = []\ncategorical_features_indices = [5, 8, 11]\nfor fold in range (5):\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    X_test = test_df.copy()\n\n    # dependent variables \n    y_train = X_train.song_popularity\n    y_val = X_val.song_popularity\n\n    # independent variables\n    X_train = X_train[features]\n    X_val = X_val[features]\n\n    # preprocessing the data \n    # handling the misssing values \n    # X_train[null_columns] = mice(X_train[null_columns].values)\n    # X_val[null_columns] = mice(X_val[null_columns].values)\n    # X_test[null_columns] = mice(X_test[null_columns].values)\n\n    scaler = RobustScaler()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n    X_train[['key', 'audio_mode', 'time_signature']] = X_train[['key', 'audio_mode', 'time_signature']].astype(int)\n    X_val[['key', 'audio_mode', 'time_signature']] = X_val[['key', 'audio_mode', 'time_signature']].astype(int)\n    X_test[['key', 'audio_mode', 'time_signature']] = X_test[['key', 'audio_mode', 'time_signature']].astype(int)\n\n\n    # catboost modelling \n    # model=CatBoostRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01, early_stopping_rounds=5)\n    # model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_val, y_val),plot=True)\n\n    # XGBRegressor moddelling \n    model = XGBRegressor(tree_method='gpu_hist',\n                       gpu_id=0, \n                       predictor='gpu_predictor',\n                       learning_rate=0.09007597481402824, \n                       reg_lambda=91.16435406373603,\n                       reg_alpha=18.42310392373454,\n                       subsample=0.26279292699733914,\n                       colsample_bytree=0.713216394431597,\n                       max_depth=5)\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    test_predict = model.predict(X_test)\n    prediction.append(test_predict)\n    roc1= roc_auc_score(y_val,preds_valid)\n\n    #Score \n    score.append(roc1)\n    print(f\"fold:{fold},roc:{roc1}\")\nprint(np.mean(score),np.std(score))\n","dcced6e6":"final_predict = np.mean(np.column_stack(prediction),axis=1)\ndf_test = pd.DataFrame(final_predict, columns=['song_popularity'])\ndf_test","b3ab2d70":"Handling Missing Data ","54249b20":"Reading Dataset","e4a70ce9":"Visualizing the data distribution and density ","5dd8a6ce":"Please Upvote\ud83d\udd3c","8cfedb1f":"Hyperparamter tuning using Optuna ","74bf3d19":"Model development using XGBoost","69b9c273":"Train and Validation split using Kfold "}}