{"cell_type":{"d779ce81":"code","4879aebc":"code","f29139ec":"code","194ffbf2":"code","65a7c212":"code","83ebc2aa":"code","5e1c611a":"code","961add5b":"code","066c240a":"code","8d709954":"code","5580a7f6":"code","5752382b":"code","99086d28":"code","ae9ca325":"code","f43b683a":"code","8821f41b":"code","0d8787fc":"code","62130a75":"code","69221b07":"code","1c600de0":"code","5602a3e0":"code","ccbc9f2b":"code","f4b714ca":"code","0d67244a":"code","cd8ff0fc":"code","d929dc5c":"code","2f9eb393":"code","fadf2640":"code","e0adea12":"code","3312690d":"code","9dbad171":"code","b9f33797":"code","c3d34913":"code","ea98136c":"markdown","ab6a73e4":"markdown","d7978c32":"markdown","cfbfc396":"markdown","5c512e6b":"markdown","feff3248":"markdown","7d66d56e":"markdown","83afa96c":"markdown","2f4e6e0e":"markdown","4af123b1":"markdown"},"source":{"d779ce81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4879aebc":"# data = pd.read_csv('gdrive\/My Drive\/hw1-regression\/train.csv', header = None, encoding = 'big5')\ndata = pd.read_csv('\/kaggle\/input\/ml2020spring-hw1\/train.csv', encoding = 'big5')","f29139ec":"# data.iloc[9]\n# data.iloc[:17]","194ffbf2":"# \u53bb\u9664\u524d\u4e09\u5217(\u65e5\u671f\/\u6d4b\u7ad9\/\u6d4b\u9879)\ndata = data.iloc[:, 3:]\ndata[data == 'NR'] = 0\nraw_data = data.to_numpy()\n#pandas to_numpy\uff08\uff09\u4ea7\u751f\u4e00\u4e2a\u5217\u8868\u6570\u7ec4","65a7c212":"# print(raw_data)","83ebc2aa":"month_data = {}\nfor month in range(12):\n    sample = np.empty([18, 480])\n    for day in range(20):\n        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]\n        #\u53d6raw_data\u7684\u884c,\u5bf9sample\u7684\u5217\u8fdb\u884c\u62fc\u63a5\n    month_data[month] = sample","5e1c611a":"# print(month_data[0][0])","961add5b":"x = np.empty([12 * 471, 18 * 9], dtype = float)\ny = np.empty([12 * 471, 1], dtype = float)\nfor month in range(12):\n    for day in range(20):\n        for hour in range(24):\n            if day == 19 and hour > 14:#\u6700\u540e\u4e00\u5929\u7684\u6700\u540e10h\u662f\u6700\u540e\u4e00\u7ec4\u6837\u672c\u4e86,\u518d\u5f80\u540e\u65e0\u6cd5\u4f5c\u6837\u672c\u4e86\n                continue\n            #-1\u5728\u8fd9\u91cc\u5e94\u8be5\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u6b63\u6574\u6570\u901a\u914d\u7b26\uff0c\u5b83\u4ee3\u66ff\u4efb\u4f55\u6574\u6570. reshape(1, -1)\u5316\u4e3a\u4e00\u884c\n            # print(month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1))\n            #x\u5206\u7247\u65f6\u662f\u6240\u6709\u884c(\u5c5e\u6027), \u5982\u679c\u4e0d\u8fdb\u884creshape, \u5219\u6bcf\u4e2a\u5c5e\u6027\u76849\u5c0f\u65f6\u5355\u72ec\u62101\u884c\n            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value\u7b2c10\u4e2a\u5c5e\u6027\u662fPM2.5\nprint(x)\nprint(y)","066c240a":"mean_x = np.mean(x, axis = 0) #18 * 9 \nstd_x = np.std(x, axis = 0) #18 * 9 \u6807\u51c6\u5dee\nfor i in range(len(x)): #12 * 471\n    for j in range(len(x[0])): #18 * 9 \n        if std_x[j] != 0:\n            x[i][j] = (x[i][j] - mean_x[j]) \/ std_x[j]\nx","8d709954":"import math\nx_train_set = x[: math.floor(len(x) * 0.8), :]#\u53d680%\u7684\u6837\u672c, floor\u5411\u4e0b\u53d6\u6574. 0~0.8\ny_train_set = y[: math.floor(len(y) * 0.8), :]\nx_validation = x[math.floor(len(x) * 0.8): , :]#0.8~1\ny_validation = y[math.floor(len(y) * 0.8): , :]\nprint(x_train_set)\nprint(y_train_set)\nprint(x_validation)\nprint(y_validation)\nprint(len(x_train_set))\nprint(len(y_train_set))\nprint(len(x_validation))\nprint(len(y_validation))","5580a7f6":"# # \u9898\u76ee4-\u6570\u636e\u5212\u5206-\u53ea\u53d6\u540e5h-cell\n# #\u53ea\u80fd\u8fd0\u884c\u4e00\u6b21\n# x_5h = x[:, (4*18):]\n# #test\u6d4b\u8bd5\u96c6\u6570\u636e\u9884\u5904\u7406\n# testdata = pd.read_csv('\/kaggle\/input\/ml2020spring-hw1\/test.csv', header = None, encoding = 'big5')\n# test_data = testdata.iloc[:, 2:]#\u53ea\u80fd\u8fd0\u884c\u4e00\u6b21\n# test_data[test_data == 'NR'] = 0\n# test_data = test_data.to_numpy()","5752382b":"# # \u9898\u76ee4-\u6d4b\u8bd5\u6570\u636e\u5f62\u72b6\u6539\u53d8-\u53ea\u53d6\u540e5h-cell\n# test_x = np.empty([240, 18*9], dtype = float)\n# # test_x_5h = np.empty([240, 18*5], dtype = float)\n# # 10 \u5c0f\u6642\u70ba\u4e00\u7b46\uff0c\u524d\u4e5d\u5c0f\u6642\u4f5cx\u7684feature. \u4e00\u5171\u53d6\u51fa 240 \u7b46\u4e0d\u91cd\u8907\u7684 test data\uff0c\n# for i in range(240):#\u6bcf\u4e00\u7b14\u5206\u522b\u5b58, \u4e00\u7b14\u4e2d\u670918\u4e2afeature\n#     test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)#\u539f\u6570\u636e\u7684\u5c0f\u65f6\u5728\u5217\u4e2d\u4f53\u73b0, \u628a\u6240\u6709\u524d9\u5c0f\u65f6\u90fd\u5b58\u5165\u4e86\n# #     test_x_5h[i, :] = test_data[18 * i: 18* (i + 1), 4:].reshape(1, -1)#\u53ea\u53d6\u540e5h\n# test_x_5h = test_x[:, (4*18):]","99086d28":"# # \u9898\u76ee4-\u6a21\u578b\u6539\u8fdb-\u53ea\u53d6\u540e5h-cell\n# dim_5h = 18 * 5 + 1\n# w_5h = np.zeros([dim_5h, 1])\n# n_train_set = math.floor(12 * 471)\n# #train\n# learning_rate = 2\n# iter_time = 1000\n# eps = 0.0000000001\n# #np.ones\u5168\u90e8\u8d4b\u4e3a1, concatenate\u6570\u7ec4\u62fc\u63a5\n# x_5h = np.concatenate((np.ones([n_train_set, 1]), x_5h), axis = 1).astype(float)\n# #train\n# adagrad_5h = np.zeros([dim_5h, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss = np.sqrt(np.sum(np.power(np.dot(x_5h, w_5h) - y, 2))\/n_train_set)#rmse\u6807\u51c6\u5dee\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n#     if(t%100==0):\n#         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient_5h = 2 * np.dot(x_5h.transpose(), np.dot(x_5h, w_5h) - y) #dim*1\n#     adagrad_5h += gradient_5h ** 2\n#     w_5h = w_5h - learning_rate * gradient_5h \/ np.sqrt(adagrad_5h + eps)\n# np.save('weight_5h.npy', w_5h)\n# w_5h","ae9ca325":"# # \u9898\u76ee4-\u6d4b\u8bd5\u96c6\u9884\u6d4b-\u53ea\u53d6\u540e5h-cell\n# #\u6d4b\u8bd5\u96c6\u8f93\u5165\u6570\u636e\u6807\u51c6\u5316\n# mean_x_5h = np.mean(test_x_5h, axis = 0)\n# std_x_5h = np.std(test_x_5h, axis = 0) #\u6807\u51c6\u5dee\n# for i in range(len(test_x_5h)):\n#     for j in range(len(test_x_5h[0])):\n#         if std_x_5h[j] != 0:\n#             test_x_5h[i][j] = (test_x_5h[i][j] - mean_x_5h[j]) \/ std_x_5h[j]\n# test_x_5h = np.concatenate((np.ones([240, 1]), test_x_5h), axis = 1).astype(float)","f43b683a":"# # \u9898\u76ee4-\u6d4b\u8bd5\u96c6\u9884\u6d4b-\u53ea\u53d6\u540e5h-cell\n# #\u9884\u6d4b\n# w_5h = np.load('weight_5h.npy')\n# ans_y_5h = np.dot(test_x_5h, w_5h)\n# # ans_y_5h\n# #\u4fdd\u5b58\u9884\u6d4b\u6570\u636e\n# import csv\n# with open('submit.csv', mode='w', newline='') as submit_file:\n#     csv_writer = csv.writer(submit_file)\n#     header = ['id', 'value']\n#     print(header)\n#     csv_writer.writerow(header)\n#     for i in range(240):\n#         row = ['id_' + str(i), ans_y_5h[i][0]]\n#         csv_writer.writerow(row)\n#         print(row)","8821f41b":"# # \u9898\u76ee3-PM2.5\u8f93\u5165\u7279\u5f81\u5212\u5206-cell\n# n_train_set = math.floor(12 * 471 * 0.8)\n# n_validation_set = math.ceil(12 * 471 * 0.2)\n\n# # x = np.empty([12 * 471, 18 * 9], dtype = float)\n# x_train_set_PM = np.empty([n_train_set, 1 * 9], dtype = float)\n# x_validation_PM = np.empty([n_validation_set, 1 * 9], dtype = float)\n# for hour in range(9):#\u6bcf\u7b14\u8f93\u5165\u6570\u636e\u4e2d\u7684\u5c0f\u65f69, \u548c\u63d0\u53d6PM2.5\u6240\u5728\u7684\u884c\u65709, \u5728\u7406\u89e3\u7684\u65f6\u5019\u4e0d\u8981\u641e\u6df7\n#     x_train_set_PM[:,hour] = x_train_set[:, 18*hour + 9]\n#     x_validation_PM[:,hour] = x_validation[:, 18*hour + 9]\n# x_train_set_PM, x_validation_PM","0d8787fc":"# # \u9898\u76ee3-\u5168\u5c5e\u6027\u5bf9\u6bd4\u53ea\u8f93\u5165PM2.5\u6570\u636e-cell\n# # \u8bad\u7ec3\n# dim = 18 * 9 + 1\n# w = np.zeros([dim, 1])\n# x_train_set = np.concatenate((np.ones([n_train_set, 1]), x_train_set), axis = 1).astype(float)\n# # x_train_set\n# #train\n# learning_rate = 100\n# iter_time = 1000\n# adagrad = np.zeros([dim, 1])\n# eps = 0.0000000001\n# for t in range(iter_time):\n#     loss = np.sqrt(np.sum(np.power(np.dot(x_train_set, w) - y_train_set, 2))\/n_train_set)#rmse\u6807\u51c6\u5dee\u4e5f\u8981\u6839\u636e\u5212\u5206\u540e\u7684\u6570\u636e\u7b14\u6570\u4fee\u6539\u6240\u9664\u7684n\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n# #     if(t%100==0):\n# #         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set) #dim*1\n#     adagrad += gradient ** 2\n#     w = w - learning_rate * gradient \/ np.sqrt(adagrad + eps)","62130a75":"# # \u9898\u76ee3-cell\n# # PM2.5\u8bad\u7ec3\n# dim_PM = 1 * 9 + 1\n# w_PM = np.zeros([dim_PM, 1])\n# #np.ones\u5168\u90e8\u8d4b\u4e3a1, concatenate\u6570\u7ec4\u62fc\u63a5\n# x_train_set_PM = np.concatenate((np.ones([n_train_set, 1]), x_train_set_PM), axis = 1).astype(float)\n# #train\n# adagrad_PM = np.zeros([dim_PM, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss = np.sqrt(np.sum(np.power(np.dot(x_train_set_PM, w_PM) - y_train_set, 2))\/n_train_set)#rmse\u6807\u51c6\u5dee\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n# #     if(t%100==0):\n# #         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient_PM = 2 * np.dot(x_train_set_PM.transpose(), np.dot(x_train_set_PM, w_PM) - y_train_set) #dim*1\n#     adagrad_PM += gradient_PM ** 2\n#     w_PM = w_PM - learning_rate * gradient_PM \/ np.sqrt(adagrad_PM + eps)\n    \n    \n# # \u6d4b\u8bd5\n# x_validation = np.concatenate((np.ones([n_validation_set, 1]), x_validation), axis = 1).astype(float)\n# ans_y = np.dot(x_validation, w)\n# # PM2.5\n# x_validation_PM = np.concatenate((np.ones([n_validation_set, 1]), x_validation_PM), axis = 1).astype(float)\n# ans_y_PM = np.dot(x_validation_PM, w_PM)\n\n\n# # \u5206\u522b\u8ba1\u7b97\u5e73\u5747\u8bef\u5dee\n# n_vali=len(ans_y)\n# average_error=0\n# for i in range(0, n_vali):  \n#     average_error+=abs(y_validation[i][0]-ans_y[i][0])\n# average_error\/=n_vali\n# # PM2.5\n# average_error_PM=0\n# for i in range(0, n_vali):  \n#     average_error_PM+=abs(y_validation[i][0]-ans_y_PM[i][0])\n# average_error_PM\/=n_vali\n\n# print(average_error)\n# print(average_error_PM)","69221b07":"# # \u9898\u76ee2-5\u5c0f\u65f6\u8f93\u5165\u5212\u5206-cell\n# x_train_set_5h = x_train_set[:, (4*18):]\n# x_validation_5h = x_validation[:, (4*18):]","1c600de0":"# #\u9898\u76ee2-5h,9h\u5bf9\u6bd4\u6570\u636e\u8bad\u7ec3-cell\n# dim = 18 * 9 + 1\n# n_train_set = math.floor(12 * 471 * 0.8)\n# w = np.zeros([dim, 1])\n# x_train_set = np.concatenate((np.ones([n_train_set, 1]), x_train_set), axis = 1).astype(float)\n# # x_train_set\n# #train\n# learning_rate = 100\n# iter_time = 1000\n# adagrad = np.zeros([dim, 1])\n# eps = 0.0000000001\n# for t in range(iter_time):\n#     loss = np.sqrt(np.sum(np.power(np.dot(x_train_set, w) - y_train_set, 2))\/n_train_set)#rmse\u6807\u51c6\u5dee\u4e5f\u8981\u6839\u636e\u5212\u5206\u540e\u7684\u6570\u636e\u7b14\u6570\u4fee\u6539\u6240\u9664\u7684n\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n#     if(t%100==0):\n#         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set) #dim*1\n#     adagrad += gradient ** 2\n#     w = w - learning_rate * gradient \/ np.sqrt(adagrad + eps)\n# # 5h\n# dim_5h = 18 * 5 + 1\n# w_5h = np.zeros([dim_5h, 1])\n# #np.ones\u5168\u90e8\u8d4b\u4e3a1, concatenate\u6570\u7ec4\u62fc\u63a5\n# x_train_set_5h = np.concatenate((np.ones([n_train_set, 1]), x_train_set_5h), axis = 1).astype(float)\n# #train\n# adagrad_5h = np.zeros([dim_5h, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss = np.sqrt(np.sum(np.power(np.dot(x_train_set_5h, w_5h) - y_train_set, 2))\/n_train_set)#rmse\u6807\u51c6\u5dee\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n# #     if(t%100==0):\n# #         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient_5h = 2 * np.dot(x_train_set_5h.transpose(), np.dot(x_train_set_5h, w_5h) - y_train_set) #dim*1\n#     adagrad_5h += gradient_5h ** 2\n#     w_5h = w_5h - learning_rate * gradient_5h \/ np.sqrt(adagrad_5h + eps)","5602a3e0":"# #\u9898\u76ee2-5h,9h\u5bf9\u6bd4\u6570\u636e\u6d4b\u8bd5-cell\n# #test\n# x_validation = np.concatenate((np.ones([math.ceil(12 * 471 * 0.2), 1]), x_validation), axis = 1).astype(float)\n# # test_x\n# # w = np.load('weight-by-train-set.npy')\n# ans_y = np.dot(x_validation, w)\n# # ans_y\n# # 5h\n# x_validation_5h = np.concatenate((np.ones([math.ceil(12 * 471 * 0.2), 1]), x_validation_5h), axis = 1).astype(float)\n# ans_y_5h = np.dot(x_validation_5h, w_5h)\n# ans_y_5h","ccbc9f2b":"# #\u9898\u76ee2-\u5206\u522b\u8ba1\u7b97\u5e73\u5747\u8bef\u5dee-cell\n# n_vali=len(ans_y)\n# average_error=0\n# for i in range(0, n_vali):  \n#     average_error+=abs(y_validation[i][0]-ans_y[i][0])\n# average_error\/=n_vali\n# # 5h\n# average_error_5h=0\n# for i in range(0, n_vali):  \n#     average_error_5h+=abs(y_validation[i][0]-ans_y_5h[i][0])\n# average_error_5h\/=n_vali\n\n# print(average_error)\n# print(average_error_5h)","f4b714ca":"# baseline-cell\ndim = 18 * 9 + 1\nw = np.zeros([dim, 1])\n#np.ones\u5168\u90e8\u8d4b\u4e3a1, concatenate\u6570\u7ec4\u62fc\u63a5\nx = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\nx","0d67244a":"# #\u9898\u76ee1-\u4f5c\u56fecell\n# learning_rate = [0.5, 2, 50, 100]\n# iter_time = 1000\n# adagrad = np.zeros([dim, 1])\n# eps = 0.0000000001\n# model_data = {\n#     't': [ ],\n#     'loss1': [ ],\n#     'loss2': [ ],\n#     'loss3': [ ],\n#     'loss4': [ ]\n# }\n# w1=w#\u56e0\u4e3a\u4f5c\u56fe\u65f6\u4e0d\u540c\u5b66\u4e60\u7387\u90fd\u8981\u7528\u5230w,adagrad,\u9700\u8981\u7559\u5e95\u6216\u91cd\u7f6e\n# w2=w\n# w3=w\n# w4=w\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss1 = np.sqrt(np.sum(np.power(np.dot(x, w1) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x.transpose(), np.dot(x, w1) - y) #dim*1\n#     adagrad += gradient ** 2\n#     w1 = w1 - learning_rate[0] * gradient \/ np.sqrt(adagrad + eps)\n#     model_data['t'].append(t)\n#     model_data['loss1'].append(loss1)\n# adagrad = np.zeros([dim, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss2 = np.sqrt(np.sum(np.power(np.dot(x, w2) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x.transpose(), np.dot(x, w2) - y) #dim*1\n#     adagrad += gradient ** 2\n#     w2 = w2 - learning_rate[1] * gradient \/ np.sqrt(adagrad + eps)\n#     model_data['loss2'].append(loss2)\n# adagrad = np.zeros([dim, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss3 = np.sqrt(np.sum(np.power(np.dot(x, w3) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x.transpose(), np.dot(x, w3) - y) #dim*1\n#     adagrad += gradient ** 2\n#     w3 = w3 - learning_rate[2] * gradient \/ np.sqrt(adagrad + eps)\n#     model_data['loss3'].append(loss3)\n# adagrad = np.zeros([dim, 1])\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss4 = np.sqrt(np.sum(np.power(np.dot(x, w4) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x.transpose(), np.dot(x, w4) - y) #dim*1\n#     adagrad += gradient ** 2\n#     w4 = w4 - learning_rate[3] * gradient \/ np.sqrt(adagrad + eps)\n#     model_data['loss4'].append(loss4)\n# # model_data","cd8ff0fc":"# #\u9898\u76ee1-\u4f5c\u56fecell\n# import matplotlib.pyplot as plt\n \n# #\u8bfb\u53d6\u6570\u636e\n# # model_data = {\n# #     't': [1,2,3],\n# #     'loss': [7.1,23.2,11.8]\n# # }\n# plt.figure(figsize=(10,5))#\u8bbe\u7f6e\u753b\u5e03\u7684\u5c3a\u5bf8\n# plt.title('Loss-Iteration Line Chart',fontsize=20)#\u6807\u9898\uff0c\u5e76\u8bbe\u5b9a\u5b57\u53f7\u5927\u5c0f\n# plt.xlabel(u'x-iteration times',fontsize=14)#\u8bbe\u7f6ex\u8f74\uff0c\u5e76\u8bbe\u5b9a\u5b57\u53f7\u5927\u5c0f\n# plt.ylabel(u'y-loss',fontsize=14)#\u8bbe\u7f6ey\u8f74\uff0c\u5e76\u8bbe\u5b9a\u5b57\u53f7\u5927\u5c0f\n# plt.xlim((0, 1000))\n# plt.ylim((5, 15))\n# #color\uff1a\u989c\u8272\uff0clinewidth\uff1a\u7ebf\u5bbd\uff0clinestyle\uff1a\u7ebf\u6761\u7c7b\u578b\uff0clabel\uff1a\u56fe\u4f8b\uff0cmarker\uff1a\u6570\u636e\u70b9\u7684\u7c7b\u578b\n# plt.plot(model_data['t'],model_data['loss1'],color=\"darkblue\",linewidth=1,linestyle='--',label='learning_rate=0.5', marker='+')\n# plt.plot(model_data['t'],model_data['loss2'],color=\"deeppink\",linewidth=2,linestyle=':',label='learning_rate=2', marker='o')\n# plt.plot(model_data['t'],model_data['loss3'],color=\"goldenrod\",linewidth=1.5,linestyle='-',label='learning_rate=50', marker='*')\n# plt.plot(model_data['t'],model_data['loss4'],color=\"green\",linewidth=1.5,linestyle='-',label='learning_rate=100', marker='*')\n\n \n# plt.legend(loc=1)#\u56fe\u4f8b\u5c55\u793a\u4f4d\u7f6e\uff0c\u6570\u5b57\u4ee3\u8868\u7b2c\u51e0\u8c61\u9650\n# plt.show()#\u663e\u793a\u56fe\u50cf","d929dc5c":"# # baseline-cell\n# learning_rate = 100\n# iter_time = 1000\n# adagrad = np.zeros([dim, 1])\n# eps = 0.0000000001\n# for t in range(iter_time):\n#     #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n#     loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n#     #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n#     if(t%100==0):\n#         print(str(t) + \":\" + str(loss))\n#     #transpose\u77e9\u9635\u8f6c\u7f6e\n#     gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1\n#     adagrad += gradient ** 2\n#     w = w - learning_rate * gradient \/ np.sqrt(adagrad + eps)\n# np.save('weight.npy', w)\n# w","2f9eb393":"# beat-baseline-cell\nlearning_rate = 1\niter_time = 1200\nadagrad = np.zeros([dim, 1])\neps = 0.0000000001\nfor t in range(iter_time):\n    #np.dot\u77e9\u9635\u70b9\u79ef, power:x^y\n    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))\/471\/12)#rmse\u6807\u51c6\u5dee\n    #\u6bcf\u8fdb\u884c100\u6b21\u8fed\u4ee3, \u8f93\u51faloss\u503c\n    if(t%100==0):\n        print(str(t) + \":\" + str(loss))\n    #transpose\u77e9\u9635\u8f6c\u7f6e\n    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1\n    adagrad += gradient ** 2\n    w = w - learning_rate * gradient \/ np.sqrt(adagrad + eps)\nnp.save('weight.npy', w)\nw","fadf2640":"# baseline-cell\n# testdata = pd.read_csv('gdrive\/My Drive\/hw1-regression\/test.csv', header = None, encoding = 'big5')\ntestdata = pd.read_csv('\/kaggle\/input\/ml2020spring-hw1\/test.csv', header = None, encoding = 'big5')\ntest_data = testdata.iloc[:, 2:]\ntest_data[test_data == 'NR'] = 0\ntest_data = test_data.to_numpy()","e0adea12":"# baseline-cell\ntest_x = np.empty([240, 18*9], dtype = float)\nfor i in range(240):\n    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)","3312690d":"# baseline-cell\nfor i in range(len(test_x)):\n    for j in range(len(test_x[0])):\n        if std_x[j] != 0:\n            test_x[i][j] = (test_x[i][j] - mean_x[j]) \/ std_x[j]\ntest_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\ntest_x","9dbad171":"# # beat-baseline-cell\n# mean_test_x = np.mean(test_x, axis = 0) #18 * 9 \n# std_test_x = np.std(test_x, axis = 0) #18 * 9 \u6807\u51c6\u5dee\n# for i in range(len(test_x)):\n#     for j in range(len(test_x[0])):\n#         if std_test_x[j] != 0:\n#             test_x[i][j] = (test_x[i][j] - mean_test_x[j]) \/ std_test_x[j]\n# test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n# test_x","b9f33797":"# baseline-cell\nw = np.load('weight.npy')\nans_y = np.dot(test_x, w)\nans_y","c3d34913":"# baseline-cell\nimport csv\nwith open('submit.csv', mode='w', newline='') as submit_file:\n    csv_writer = csv.writer(submit_file)\n    header = ['id', 'value']\n    print(header)\n    csv_writer.writerow(header)\n    for i in range(240):\n        row = ['id_' + str(i), ans_y[i][0]]\n        csv_writer.writerow(row)\n        print(row)","ea98136c":"# **Normalize (1)**\n\u9884\u5904\u7406\u6570\u636e\u6709\u4e00\u4e2a\u5f88\u5173\u952e\u7684\u6b65\u9aa4\u5c31\u662f\u6570\u636e\u7684\u6807\u51c6\u5316\n\u8bb8\u591a\u5b66\u4e60\u7b97\u6cd5\u4e2d\u76ee\u6807\u51fd\u6570\u7684\u57fa\u7840\u90fd\u662f\u5047\u8bbe\u6240\u6709\u7684\u7279\u5f81\u90fd\u662f\u96f6\u5747\u503c\u5e76\u4e14\u5177\u6709\u540c\u4e00\u9636\u6570\u4e0a\u7684\u65b9\u5dee\u3002\u5982\u679c\u67d0\u4e2a\u7279\u5f81\u7684\u65b9\u5dee\u6bd4\u5176\u4ed6\u7279\u5f81\u5927\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u90a3\u4e48\u5b83\u5c31\u4f1a\u5728\u5b66\u4e60\u7b97\u6cd5\u4e2d\u5360\u636e\u4e3b\u5bfc\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u5b66\u4e60\u5668\u5e76\u4e0d\u80fd\u50cf\u6211\u4eec\u8bf4\u671f\u671b\u7684\u90a3\u6837\uff0c\u4ece\u5176\u4ed6\u7279\u5f81\u4e2d\u5b66\u4e60\u3002\n## 1 StandardScaler\n\u6807\u51c6\u5316\u6570\u636e\u901a\u8fc7\u51cf\u53bb\u5747\u503c\u7136\u540e\u9664\u4ee5\u65b9\u5dee\uff08\u6216\u6807\u51c6\u5dee\uff09\uff0c\u8fd9\u79cd\u6570\u636e\u6807\u51c6\u5316\u65b9\u6cd5\u7ecf\u8fc7\u5904\u7406\u540e\u6570\u636e\u7b26\u5408\u6807\u51c6\u6b63\u6001\u5206\u5e03\uff0c\u5373\u5747\u503c\u4e3a0\uff0c\u6807\u51c6\u5dee\u4e3a1\n## 2 MinMaxScaler\n\u8fd9\u79cd\u65b9\u6cd5\u662f\u5bf9\u539f\u59cb\u6570\u636e\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u5c06\u6570\u636e\u5f52\u4e00\u5230[0,1]\u4e2d\u95f4\n## 3 RobustScaler\n\u5982\u679c\u4f60\u7684\u6570\u636e\u5305\u542b\u8bb8\u591a\u5f02\u5e38\u503c\uff0c\u4f7f\u7528\u5747\u503c\u548c\u65b9\u5dee\u7f29\u653e\u53ef\u80fd\u5e76\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\u3002\nThis Scaler removes the median\uff08\u4e2d\u4f4d\u6570\uff09 and scales the data according to the quantile range(\u56db\u5206\u4f4d\u8ddd\u79bb\uff0c\u4e5f\u5c31\u662f\u8bf4\u6392\u9664\u4e86outliers)","ab6a73e4":"# **Load 'train.csv'**\ntrain.csv \u7684\u8cc7\u6599\u70ba 12 \u500b\u6708\u4e2d\uff0c\u6bcf\u500b\u6708\u53d6 20 \u5929\uff0c\u6bcf\u5929 24 \u5c0f\u6642\u7684\u8cc7\u6599(\u6bcf\u5c0f\u6642\u8cc7\u6599\u6709 18 \u500b features)\u3002","d7978c32":"# **Preprocessing** \n\u53d6\u9700\u8981\u7684\u6578\u503c\u90e8\u5206\uff0c\u5c07 'RAINFALL' \u6b04\u4f4d\u5168\u90e8\u88dc 0\u3002Rainfall\u5c5e\u6027\u7684\u65e0\u964d\u96e8\u503c\u4e3aNR\uff0c\u964d\u96e8\u503c\u4e3a1. \u5c06NR\u66ff\u6362\u4e3a\u6570\u503c0\n\u53e6\u5916\uff0c\u5982\u679c\u8981\u5728 colab \u91cd\u8986\u9019\u6bb5\u7a0b\u5f0f\u78bc\u7684\u57f7\u884c\uff0c\u8acb\u5f9e\u982d\u958b\u59cb\u57f7\u884c(\u628a\u4e0a\u9762\u7684\u90fd\u91cd\u65b0\u8dd1\u4e00\u6b21)\uff0c\u4ee5\u907f\u514d\u8dd1\u51fa\u4e0d\u662f\u81ea\u5df1\u8981\u7684\u7d50\u679c\uff08\u82e5\u81ea\u5df1\u5beb\u7a0b\u5f0f\u4e0d\u6703\u9047\u5230\uff0c\u4f46 colab \u91cd\u8907\u8dd1\u9019\u6bb5\u6703\u4e00\u76f4\u5f80\u4e0b\u53d6\u8cc7\u6599\u3002\u610f\u5373\u7b2c\u4e00\u6b21\u53d6\u539f\u672c\u8cc7\u6599\u7684\u7b2c\u4e09\u6b04\u4e4b\u5f8c\u7684\u8cc7\u6599\uff0c\u7b2c\u4e8c\u6b21\u53d6\u7b2c\u4e00\u6b21\u53d6\u7684\u8cc7\u6599\u6389\u4e09\u6b04\u4e4b\u5f8c\u7684\u8cc7\u6599\uff0c...\uff09\u3002","cfbfc396":"# **Training**\n\n\u4e0b\u9762\u7684 code \u63a1\u7528 Root Mean Square Error\n\n\u56e0\u70ba\u5e38\u6578\u9805\u7684\u5b58\u5728\uff0c\u6240\u4ee5 dimension (dim) \u9700\u8981\u591a\u52a0\u4e00\u6b04\uff1beps \u9805\u662f\u907f\u514d adagrad \u7684\u5206\u6bcd\u70ba 0 \u800c\u52a0\u7684\u6975\u5c0f\u6578\u503c\u3002\n\n\u6bcf\u4e00\u500b dimension (dim) \u6703\u5c0d\u61c9\u5230\u5404\u81ea\u7684 gradient, weight (w)\uff0c\u900f\u904e\u4e00\u6b21\u6b21\u7684 iteration (iter_time) \u5b78\u7fd2\u3002","5c512e6b":"# **Extract Features (1)**\n\u5c5e\u6027\u7ec4\u621018\u7ef4\u5411\u91cf\n\u5c07\u539f\u59cb 4320 * 18 \u7684\u8cc7\u6599\u4f9d\u7167\u6bcf\u500b\u6708\u5206\u91cd\u7d44\u6210 12 \u500b\u6708( \u6bcf\u4e2a\u670820day*24h=480h\u6a2a\u5411\u5c0f\u65f6 ), 18 (features) * 480 (hours) \u7684\u8cc7\u6599\u3002\n\u8fd9\u6837,\u6bcf\u4e2a\u6708\u7684\u8d44\u6599\u5728\u6a2a\u5411\u4e0a\u662f\u8fde\u7eed\u7684\u65f6\u95f4","feff3248":"# **Prediction**\nweight \u548c\u6e2c\u8a66\u8cc7\u6599\u9810\u6e2c target\u3002","7d66d56e":"# **Save Prediction to CSV File**","83afa96c":"# **Testing**\n\u8f09\u5165 test data\uff0c\u4e26\u4e14\u4ee5\u76f8\u4f3c\u65bc\u8a13\u7df4\u8cc7\u6599\u9810\u5148\u8655\u7406\u548c\u7279\u5fb5\u8403\u53d6\u7684\u65b9\u5f0f\u8655\u7406\uff0c\u4f7f test data \u5f62\u6210 240 \u500b\u7dad\u5ea6\u70ba 18 * 9 + 1 \u7684\u8cc7\u6599\u3002\n\n\u8bfb\u53d6csv\u6587\u4ef6,\u8f93\u5165x\u6bcf18\u884c\u52a0\u4e00\u4e2a\u5e38\u6570\u9879\u7ef4\u5ea6, \u52a0\u4e0a\u524d9\u5c0f\u65f6\u6570\u636e\n\n\u8f93\u51fa\u77e9\u9635weight vector\u4e0ex\u7684\u70b9\u4e58","2f4e6e0e":"# **Split Training Data Into \"train_set\" and \"validation_set\"**\n\u9019\u90e8\u5206\u662f\u91dd\u5c0d\u4f5c\u696d\u4e2d\u7b2c\u4e8c\u984c\u3001\u7b2c\u4e09\u984c\uff0c\u751f\u6210\u6bd4\u8f03\u4e2d\u7528\u4f86\u8a13\u7df4\u7684 train_set \u548c\u4e0d\u6703\u88ab\u653e\u5165\u8a13\u7df4\u3001\u53ea\u662f\u7528\u4f86\u9a57\u8b49\u7684 validation_set\u3002","4af123b1":"# **Extract Features (2)**\n\u6bcf\u4e2aarray\u662f\u4e00\u4e2a\u6708, print(month_data[0][0])\u53ef\u67e5\u770b\u7b2c\u4e00\u4e2a\u6708\u7b2c\u4e00\u5929\n\u6bcf\u500b\u6708\u6703\u6709 480hrs\uff0c\u6bcf 9 \u5c0f\u6642\u5f62\u6210\u4e00\u500b data\uff0c\u6bcf\u500b\u6708\u6703\u6709 471 \u500b data\uff0c\u6545\u7e3d\u8cc7\u6599\u6578\u70ba 471 * 12 \u7b46\uff0c\u800c\u6bcf\u7b46 data \u6709 9 * 18 \u7684 features (\u4e00\u5c0f\u6642 18 \u500b features * 9 \u5c0f\u6642)\u3002\n\n\u5c0d\u61c9\u7684 target \u5247\u6709 471 * 12 \u500b(\u7b2c 10 \u500b\u5c0f\u6642\u7684 PM2.5)\n\n\u524d9h\u6570\u636e\u4f5c\u4e3atrain_x, \u7b2c10h\u7684pm2.5\u4f5c\u4e3atrain_y, \u5236\u4f5cxy\u6837\u672c"}}