{"cell_type":{"6dec1cae":"code","00f3abe0":"code","7bcf67ec":"code","7a35fc3e":"code","450a2a1f":"code","9dc18a5b":"code","d3b2d022":"code","6c3daa7b":"code","1ec9e4ce":"code","4a0e43db":"code","d6f69581":"code","a3bf941e":"code","3a02daf6":"code","82651a37":"code","ca65db88":"code","3a4d617e":"code","09c5e45e":"code","819be520":"code","2edb140e":"code","14d082a5":"code","1db8f0e7":"code","c44c983c":"code","8e0ea27b":"code","0a598b88":"code","c128f466":"code","57a37781":"code","95af6c3d":"code","f1271f45":"code","33056288":"code","592637b1":"code","7ad8b099":"code","81fe6744":"code","760648f3":"code","77606526":"code","1fa3e588":"code","fb2f820d":"code","8bc6d968":"code","9492f504":"code","3e504462":"code","f0f894b6":"code","d475cdd8":"code","e76f8368":"code","90537abd":"code","74f61ef0":"code","adb07168":"code","08fc73d6":"code","1e4f4efc":"code","5b3c32f3":"code","e53ce047":"code","45a6da77":"code","d6d6590d":"code","b37bca9c":"code","d3619217":"code","574344fd":"code","88dbfd26":"code","bea4792c":"code","e8dd6171":"code","d4419a0c":"code","2ec966d0":"code","1a05105e":"code","30ded058":"code","e961c826":"code","917be6dc":"code","c924d49e":"code","15123590":"code","b3efefe9":"code","21acc2c8":"code","591adc9a":"code","474d05e2":"code","1f2afb71":"code","3bd7b900":"code","6111ecf6":"code","c8db6b3d":"code","b5a2f594":"code","4c5598df":"code","2bd09ed5":"code","61f6b8ae":"code","8d4e2276":"code","4fe7d130":"code","c03aaa56":"code","c2ff2c1d":"code","0649e046":"code","942b0c5e":"code","4dec5533":"code","af3f84c3":"code","95575956":"code","f49eadc8":"code","7bbe6b7e":"code","421ed55d":"code","f3fb315c":"code","c143efe6":"code","7f35308c":"code","79985491":"code","b53952ef":"code","67cfe12f":"code","f733d9fa":"code","303b6a12":"code","5f868a98":"code","3193bfde":"code","0c1ea966":"code","3bf719d1":"code","abfa7e79":"code","4bb81e52":"code","2780be33":"code","bd6404f9":"code","915336aa":"code","fcbdd76a":"code","19338337":"code","da93c947":"code","2731603b":"code","9cd77941":"code","efb2b594":"code","0476c99f":"code","b9e21d94":"code","d2f5d576":"code","f02abea6":"code","73e7afed":"code","fccf0088":"code","75f71b1c":"code","39f4ffcb":"code","ef0c7bd5":"code","821fc15b":"code","06b5860a":"code","28aebc8c":"code","f6b82449":"code","6f241f16":"code","11c55537":"code","59a3657f":"code","c30cb72b":"code","5fa88e11":"code","701dd4a1":"code","8ad1283b":"code","e98770f1":"code","002a3e17":"code","29113825":"code","611d143d":"code","48bd6972":"code","13c48d38":"code","836a9ffc":"code","db953221":"code","a9edb676":"code","5f4abfee":"code","3dced7d9":"code","39e25f82":"code","dc34ba48":"code","53d9a9c5":"code","6ea49c9e":"code","1ba014b8":"code","a2f7ec8b":"code","00a49884":"code","5e96443b":"code","da291a1e":"code","22d2491a":"code","020919ee":"code","d0997435":"code","e9412907":"code","008d7962":"code","06c22d2c":"code","54bcff2f":"code","e71c96da":"code","d6b07a66":"code","96656e97":"code","beb4841f":"code","4dc4100d":"code","0140ae0f":"code","91584730":"code","b06b0d1f":"code","49252ce6":"code","76dab7f8":"code","94871f57":"code","67f817e7":"code","d5e9044a":"markdown","8ca16673":"markdown","e40dc31e":"markdown","6cd861b2":"markdown","64e14299":"markdown","231d0c88":"markdown","1d1a4b4d":"markdown","a6d3a6ed":"markdown","42fdf9f3":"markdown","249e9d64":"markdown","6f61d0ac":"markdown","a7786834":"markdown","6415cca5":"markdown","f006545b":"markdown","82099609":"markdown","f2ce7454":"markdown","ee7cf802":"markdown","97b88388":"markdown","ec20e8df":"markdown","a92d9f27":"markdown","c7651b71":"markdown","cdfd6958":"markdown","41382596":"markdown","258f6812":"markdown","14a33975":"markdown","a993e852":"markdown","9495db83":"markdown","d94db1e8":"markdown","73dbf887":"markdown","087ecc0a":"markdown","6aac3019":"markdown","a4305951":"markdown","2bfcdde3":"markdown","1bf81782":"markdown","61e57208":"markdown","3f60660d":"markdown","1dda6bc0":"markdown","1be04391":"markdown","52224f5f":"markdown","c9cd210f":"markdown","38ace865":"markdown","9bd3224e":"markdown","219e10e3":"markdown","6f74cf23":"markdown","ab0f5f69":"markdown","cb039cd9":"markdown","b3d38102":"markdown","de47fe86":"markdown","3ef1c042":"markdown","76e95f5a":"markdown","d6c92aa6":"markdown","89bae267":"markdown","9f2c8f8c":"markdown","05071fbc":"markdown","03fa3b34":"markdown","db423748":"markdown","cb7b2c27":"markdown","ddee5049":"markdown","1f335e89":"markdown","1f262e20":"markdown","047c88cf":"markdown","0891fd4d":"markdown","3060fc73":"markdown","42cc2cb3":"markdown","4c7a2543":"markdown","9c895df3":"markdown","f8ead5fb":"markdown","1a051409":"markdown","dccbfa17":"markdown","722ab57d":"markdown","e2ea2929":"markdown","ee3e5fa9":"markdown","0d46dd84":"markdown","ba02c838":"markdown","34300717":"markdown","e44d638a":"markdown","eead086a":"markdown","046241ec":"markdown","860df867":"markdown","60381ed4":"markdown","43305364":"markdown","31c17408":"markdown","0e0e409e":"markdown","5f9f9ac0":"markdown","7de8b573":"markdown","c85a2b7f":"markdown","615dd204":"markdown","744ba824":"markdown","bd1d796a":"markdown","7547f0a5":"markdown","9485036a":"markdown","880076f0":"markdown","715ca0f4":"markdown"},"source":{"6dec1cae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","00f3abe0":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Imputer\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_recall_curve, roc_curve","7bcf67ec":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTE","7a35fc3e":"import warnings\nwarnings.filterwarnings('ignore')","450a2a1f":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.columns = ['PassengerId', \"Survi'ved\", 'P class', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',  # \u308f\u3056\u3068 \"'\" \u3068 ' ' \u3092\u4ed8\u3051\u308b\n                    'Ticket', 'Fare', 'Cabin', 'Embarked']\ntrain_df.head()\n","9dc18a5b":"train_df.columns = [c.lower().replace(\"'\", \"\").replace(' ', '') for c in train_df.columns]\ntrain_df.columns","d3b2d022":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","6c3daa7b":"test_df.columns = [c.lower().replace(\"'\", \"\").replace(' ', '') for c in test_df.columns]\ntest_df.columns","1ec9e4ce":"combine = [train_df, test_df]","4a0e43db":"fig, axes = plt.subplots(figsize=(3,3))\nprint(f\"{train_df.groupby(['survived']).size()}\")\nprint(pd.DataFrame(train_df.groupby(['survived']).size()).apply(lambda x: x \/ sum(x))) # \ntrain_df.groupby(['survived']).size().plot.pie(autopct='%.2f%%')","d6f69581":"train_df.describe()","a3bf941e":"num_cols = [col for col in train_df.columns if train_df[col].dtypes not in ['object']]\nnum_cols","3a02daf6":"train_df.describe(exclude='number')","82651a37":"null_cols = [col for col in train_df.columns if train_df[col].isnull().any()]\nnull_cols","ca65db88":"ctg_cols = [c for c in train_df.columns if train_df[c].dtypes in ['object']]\nctg_cols","3a4d617e":"cols_combine = num_cols + ctg_cols\nprint([col for col in cols_combine if col not in train_df.columns])\nprint([col for col in train_df.columns if col not in cols_combine])\nset(cols_combine) == set(train_df.columns)","09c5e45e":"fig, axes = plt.subplots(1, 6, figsize=(20, 3))\nfor i, col in enumerate(['survived', 'pclass', 'sex', 'sibsp', 'parch', 'embarked']):\n    ax = axes.ravel()[i]\n    df_i = pd.DataFrame(train_df[col])\n#     print(f\"{i}, {df_i.groupby([col]).size()}\")  # \u30ab\u30c6\u30b4\u30ea\u5225\u306e\u983b\u5ea6\u3068\u69cb\u6210\u6bd4\n#     print(f\"\\n{pd.DataFrame(df_i.groupby([col]).size()).apply(lambda x: x \/ sum(x))}\\n\")\n    df_i.groupby([col]).size().plot.pie(ax=ax, legend=False,\n                                        colors=['lightpink', 'lightgrey', 'lavender'])\n    ax.set_title(col)","819be520":"(0 * 445 + 1 * 233) \/ (445 + 233) # parch 0","2edb140e":"print(f\"{train_df[['parch', 'survived']].groupby(['parch', 'survived']).size()}\\n\")\nprint(f\"\u4e0a\u8a18\u306e\u5e73\u5747\u3092 mean() \u3067\u53d6\u5f97\\n{train_df[['parch', 'survived']].groupby(['parch']).mean()}\")","14d082a5":"fig, axes = plt.subplots(2, 3, figsize=(14,3))\nfor i, col in enumerate(['pclass', 'sex', 'sibsp', 'parch', 'embarked']):\n    ax = axes.ravel()[i]\n    gby_mean = train_df[[col, 'survived']].groupby([col]).mean()\n    print(f\"{gby_mean}\\n\")\n    gby_mean.plot.bar(ax=ax, legend=False)\n    ax.set_xlabel(''); ax.set_title(col.upper())\n    ax.set_xticklabels(gby_mean.index, rotation=0)\n\nfig.tight_layout()","1db8f0e7":"# pvt_parch = pd.pivot_table(train_df,\n#                index=['parch'],\n#                columns=['survived'],\n#                values=['passengerid'],\n#                aggfunc={'passengerid': [len]},)\n# print(f\"{pvt_parch}\") # NaN \u304c\u3042\u308b\u306e\u3067\n# pvt_parch = pvt_parch.fillna(0) # .fillna(0) \u3067\u7f6e\u63db\n# print(f\"{pvt_parch}\")","c44c983c":"# pvt_parch_2 = pd.pivot_table(train_df,\n#                index=['parch'],\n#                columns=['survived'],\n#                values=['name'],                      # \u3053\u3053\u3068\n#                aggfunc={'name': [len]}).fillna(0)    # \u3053\u3053\u3092\n# print(f\"{pvt_parch_2}\")                              # 'name'\u306b\u5909\u3048\u3066\u3082\u540c\u3058\u7d50\u679c\u306b\u306a\u308b","8e0ea27b":"# # def get_ratio(x):\n# #     return x \/ sum(x)\n# # pvt_ratio = pvt_parch.apply(get_ratio, axis=1)\n# # print(f\"{(pvt_ratio)}\")\n# pvt_ratio = pvt_parch.apply(lambda x: x \/ sum(x), axis=1) # \u4e0a\u8a18\u95a2\u6570\u3092 lambda \u306b\u3057\u305f\n# print(f\"{pvt_ratio}\")\n# pvt_ratio.iloc[:, 1]","0a598b88":"fig, axes = plt.subplots(2, 3, figsize=(14,3))\nfor i, col in enumerate(['pclass', 'sex', 'sibsp', 'parch', 'embarked']):\n    ax = axes.ravel()[i]\n    pivot_col = pd.pivot_table(train_df,\n                               index=[col],\n                               columns=['survived'],\n                               values=['name'],\n                               aggfunc={'name': [len]}).fillna(0)\n    pivot_ratio = pivot_col.apply(lambda x: x \/ sum(x), axis=1)\n    survived_ratio = pivot_ratio.iloc[:, 1]\n    survived_ratio.plot.bar(ax=ax, color='silver')\n    ax.set_xlabel(''); ax.set_title(col.upper())\n    ax.set_xticklabels(survived_ratio.index, rotation=0)\n    print(f\"{(survived_ratio)}\")\nfig.tight_layout()","c128f466":"print(f\"{[c for c in num_cols if c != 'survived']}, {len([c for c in num_cols if c != 'survived'])}\")\nprint(f\"{train_df['survived'].unique()}\")\nfig, axes = plt.subplots(2, 3, figsize=(12,4))\nfor i, col in enumerate([c for c in num_cols if c != 'survived']):\n    ax = axes.ravel()[i]\n    df_i = train_df[[col, 'survived']]\n    ax.set_ylabel('')\n    ax.set_title(col.upper())\n    colors = ['red', 'blue']\n    for i_s in set(train_df['survived']):\n        df_is = df_i[df_i['survived'] == i_s][col]\n        df_is.plot.hist(ax=ax, alpha=.3, legend=False, color=colors[i_s], bins=20)\nfig.tight_layout()","57a37781":"for i, col in enumerate([c for c in num_cols if c != 'survived']):  # \u3053\u308c\u3067 for \u304c 6\u56de\u307e\u308f\u308b\n    df_i = train_df[[col, 'survived']]\n#     print(df_i)\n    fig, axes = plt.subplots(1, 2, figsize=(10,1))\n#     hist, bins = np.histogram(df_i) # ax.hist \u306f\u6b20\u640d\u5024(age)\u3067\u30a8\u30e9\u30fc\n    colors = ['red', 'blue']\n    for i_s in set(train_df['survived']):\n        ax = axes.ravel()[i_s]\n        df_is = df_i[df_i['survived'] == i_s][col]\n        df_is.plot.hist(ax=ax, alpha=.3, legend=False, sharex=True, color=colors[i_s], bins=20)\n#         print(f\"{df_is.max()}\")\n        ax.set_ylabel('')\n        ax.set_title(\"{} - survived:{}\".format(col.upper(), i_s))\n        ax.set_xlabel(\"max: {}\".format(df_is.max()))\n# fig.tight_layout()","95af6c3d":"pvt_age = pd.pivot_table(train_df\n               , index=['age']\n               , columns=['survived']\n#                , values=['passengerid']\n               , aggfunc={'passengerid': {len}}).fillna(0)","f1271f45":"fig, axes = plt.subplots(figsize=(20,2))\npvt_age.plot.bar(ax=axes, legend=False, color=['pink', 'blue'])\n# pvt_age.columns.shape","33056288":"[int(c) for c in np.linspace(0, 80, 17)]","592637b1":"bin_list = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]\nage_cut = pd.cut(train_df['age'], bin_list)","7ad8b099":"train_df['agebin'] = age_cut\ntrain_df.head()","81fe6744":"pvt_bin = pd.pivot_table(train_df\n                        , index=['agebin']\n                        , columns=['survived']\n                        , values=['passengerid']\n                        , aggfunc={'passengerid': [len]})\npvt_bin","760648f3":"fig, axes = plt.subplots(figsize=(12,2))\npvt_bin.fillna(0).plot.bar(ax=axes, width=.8)\n","77606526":"fig, axes = plt.subplots(figsize=(12,2))\ndf_age_srv = train_df[['age', 'survived']]\nxticks_range = np.linspace(0, 80, 17)\nfor i in set(df_age_srv['survived']): # {0, 1}\n    df_i = df_age_srv[df_age_srv['survived'] == i]['age']\n    df_i.plot.hist(ax=axes, alpha=.3, bins=40, label=\"survived: {}\".format(i))\n    axes.set_xticks(xticks_range); axes.set_ylabel('')\n    axes.set_title(\"histogram of age in survived\")\n    axes.grid(); axes.legend()\n# xticks_range\n","1fa3e588":"df_ts = train_df[['pclass', 'age', 'survived']] \nprint(f\"set(df_ts['pclass']): {set(df_ts['pclass'])}\")\n# print(f\"{df_ts.head()}\\n\")\nfig, axes = plt.subplots(1, 3, figsize=(14,2))\nfor i, cls in enumerate(set(df_ts['pclass'])):\n    df_class = df_ts[df_ts['pclass'] == cls]\n    colors = ['red', 'blue']\n    for i_suv in set(df_class['survived']):\n        ax = axes.ravel()[i]\n        df_class_suv = df_class[df_class['survived'] == i_suv]\n        df_class_suv['age'].plot.hist(ax=ax, bins=20, color=colors[i_suv], alpha=.3,\n                                      label=\"survived: {}\".format(i_suv), legend=True)\n        ax.set_ylabel(''); ax.set_title(\"pclass: {}\".format(cls))","fb2f820d":"df_i = train_df[['pclass', 'age', 'survived']]\nprint(f\"{set(df_i['pclass'])}\")\nfor i, cls in enumerate(set(df_i['pclass'])):  # {1, 2, 3} \u30a4\u30c6\u30ec\u30fc\u30c83\u56de\n    df_cls = df_i[df_i['pclass'] == cls]\n#     hist, bins = np.histogram(df_cls['age'].dropna(), bins=20)\n#     print(f\"{i}: {hist}\\n{bins}\\n{hist.max()}\")\n#     print(f\"{np.arange(max(hist)+1)}\")\n    fig, axes = plt.subplots(1, 2, figsize=(10,1.5))\n    hist_max = 0\n    bins = 20\n    for isv in set(df_i['survived']): # y_ticks \u53d6\u5f97\u306e\u305f\u3081\u306e\u30a4\u30c6\u30ec\u30fc\u30c8\n#         ax = axes.ravel()[isv]\n        df_sv = df_cls[df_cls['survived'] == isv]\n        hist, bins = np.histogram(df_sv['age'].dropna(), bins=bins) # np.histogram \u3067 hist \u3092\u53d6\u5f97\n#         print(f\"{i}:{isv}, {hist}, {max(hist)}\")\n        if max(hist) > hist_max:\n            hist_max = max(hist)    # hist \u306e max \u3092\u53d6\u5f97\u3057\u3066\n        y_ticks = np.arange(0, hist_max+1+4, 5) # \u305d\u308c\u3092\u3082\u3068\u306b y_ticks \u8a2d\u5b9a\u3002\u3044\u3063\u305f\u3093\u30eb\u30fc\u30d7\u7d42\u4e86\n#         print(f\"y_ticks: {y_ticks}\")\n    \n    colors = ['pink', 'lightblue']\n    for i_fin in set(df_i['survived']): # \u4e0a\u8a18\u3067\u53d6\u5f97\u3057\u305fy_ticks\u3092\u4f7f\u3063\u3066\u518d\u5ea6\u30a4\u30c6\u30ec\u30fc\u30c8\u3057\u3066\u6a2a\u4e00\u5217\u306b\u30b0\u30e9\u30d5\u63cf\u753b\n#         print(f\"i_fin: {i}, {i_fin}, {y_ticks}\")\n        ax = axes.ravel()[i_fin]\n        df_sv = df_cls[df_cls['survived'] == i_fin]\n        df_sv['age'].plot.hist(ax=ax, bins=bins, yticks=y_ticks, color=colors[i_fin]) # yticks=y_ticks \u3067\u5de6\u53f3\u306ey\u8ef8\u304c\u4e00\u81f4\n        ax.set_ylabel(''); ax.set_title(f\"pclass: {cls} - survived={i_fin}\")","8bc6d968":"grid = sns.FacetGrid(train_df, row='pclass', col='survived', size=2, aspect=1.6)\ngrid.map(plt.hist, 'age', bins=20)\ngrid.add_legend()","9492f504":"print(f\"{set(train_df['embarked'])}\")\n# grid = sns.FacetGrid(train_df, row='embarked', height=2.2, aspect=1.6)\ngrid = sns.FacetGrid(train_df, row='embarked', aspect=1.6)\ngrid.map(sns.pointplot, 'pclass', 'survived', 'sex', palette='deep')\n# grid.map(sns.pointplot(x='pclass', y='survived', data='sex', palette='deep') )","3e504462":"# grid = sns.FacetGrid(train_df, row='embarked', col='survived', height=2, aspect=1.6)\ngrid = sns.FacetGrid(train_df, row='embarked', col='survived', aspect=1.6)\ngrid.map(sns.barplot, 'sex', 'fare', alpha=.5, ci=None)\n# grid.map(sns.barplot, 'fare', 'sex', alpha=.5, ci=None)","f0f894b6":"train_df.head()","d475cdd8":"# \u30a8\u30e9\u30fc\u9632\u6b62\u306e\u305f\u3081\u3053\u3053\u3067\u6539\u3081\u3066\u8aad\u307f\u8fbc\u307f\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ncombine = [train_df, test_df]\n\nfor df in combine:\n    df.columns = [c.replace('_', '').replace(' ', '').lower() for c in df.columns]\ncombine = [train_df, test_df]\n\nprint(f\"Before: {train_df.shape}, {test_df.shape}, {combine[0].shape}, {combine[1].shape}\")\n\ntrain_df = train_df.drop(['ticket', 'cabin'], axis=1)\ntest_df = test_df.drop(['ticket', 'cabin'], axis=1)\n\ncombine = [train_df, test_df]\n\nprint(f\"After: {train_df.shape}, {test_df.shape}, {combine[0].shape}, {combine[1].shape}\")\n\ncombine[1].head()","e76f8368":"print(f\"{type(train_df[['name']])}\\n{type(train_df['name'])}\\n{type(train_df.name)}\\n\")\nprint(f\"train_df['name'].head(): ### \u307e\u305a\u306f\u540d\u524d\u3092\u8868\u793a\\n{train_df['name'].head()}\\n\")\nprint(\"\u3053\u308c\u3060\u3068\u5148\u982d\u306e\u4e00\u6587\u5b57\u306e\u307f\u3092\u53d6\u5f97\")\nprint(\"train_df['name'].str.extract('([A-Za-z])', expand=False).head()\")\nprint(f\"{train_df['name'].str.extract('([A-Za-z])', expand=False).head()}\\n\")\nprint(\"+\uff08\u30d7\u30e9\u30b9\uff09\u8ffd\u52a0\u3067\u82f1\u5b57\u306e1\u56de\u4ee5\u4e0a\u306e\u7e70\u308a\u8fd4\u3057\u3002\u3059\u306a\u308f\u3061 , \u306e\u76f4\u524d\u307e\u3067\")\nprint(\"train_df['name'].str.extract('([A-Za-z]+)', expand=False).head()\")\nprint(f\"{train_df['name'].str.extract('([A-Za-z]+)', expand=False).head()}\\n\")\nprint(\"\u30e1\u30bf\u6587\u5b57\u306e.\uff08\u30c9\u30c3\u30c8\uff09\u306f\u4efb\u610f\u306e\u4e00\u6587\u5b57\u3092\u8868\u3059\u306e\u3067\\\u3067\u30a8\u30b9\u30b1\u30fc\u30d7\u3057\u3066name\u5185\u306e.\u3092\u8868\u73fe\u3002\u82f1\u5b57\u30d7\u30e9\u30b9\u30c9\u30c3\u30c8\u3092\u62bd\u51fa\")\nprint(\"train_df['name'].str.extract('([A-Za-z]+)\\.', expand=False).head()\")\nprint(\"{}\".format(train_df['name'].str.extract('([A-Za-z]+)\\.', expand=False).head()))","90537abd":"for dataset in combine:\n    dataset['title'] = dataset['name'].str.extract('([A-Za-z]+)\\.', expand=False)\n\ntrain_df.head()","74f61ef0":"pd.concat([train_df['title'], test_df['title']], axis=0).value_counts()","adb07168":"pd.crosstab(train_df['sex'], train_df['title'])","08fc73d6":"pd.pivot_table(train_df\n              , index=['sex']\n              , columns=['title']\n              , aggfunc={'name': [len]}\n              , fill_value=0)\n","1e4f4efc":"# train_df[['sex', 'title']].groupby(['sex', 'title']).size()","5b3c32f3":"_, axes = plt.subplots(figsize=(8,2))\npd.crosstab(train_df['title'], train_df['sex']).plot.bar(ax=axes)","e53ce047":"pv_t = pd.pivot_table(train_df\n              , index=['sex']\n              , columns=['title']\n              , values=['age']\n              , aggfunc={'age': [len]}\n              , fill_value=0).T\nfig, axes = plt.subplots(1, 2, figsize=(12,2))\nex_col = [('age', 'len', 'Mr'), ('age', 'len', 'Miss'), ('age', 'len', 'Master'), ('age', 'len', 'Mrs')] # \u53f3\u306e\u30b0\u30e9\u30d5\u3067\u9664\u304f\u30ab\u30e9\u30e0\nnew_col = [indx for indx in pv_t.index if indx not in ex_col]\ncols, titles = [pv_t.index, new_col], ['all titles', 'titles with a few elements']\nfor i, ax in enumerate(axes.ravel()): # \u30ab\u30e9\u30e0\u3054\u3068\u306e\u4ef6\u6570\u306b\u30d0\u30e9\u30c4\u30ad\u304c\u3042\u308b\u306e\u3067\u53f3\u5074\u306f\u4e0a\u4f4d\u3092\u9664\u3044\u305f\u30b0\u30e9\u30d5\u3092\u8868\u793a\n    pv_t.loc[cols[i], :].plot.bar(ax=ax)\n    ax.set_xticklabels(pv_t.loc[cols[i], :].index.levels[2])\n    ax.set_xlabel('')\n    ax.set_title(titles[i])","45a6da77":"grid = sns.FacetGrid(train_df, size=1.5, aspect=1.5, row='title', col='survived')\ngrid.map(plt.hist, 'age')","d6d6590d":"print(f\"{train_df.groupby(['title']).size().shape}\")\nfig, axes = plt.subplots(3, 6, figsize=(14,5))\ncolors = ['red', 'blue']\nfor i, idx in enumerate(train_df.groupby(['title']).size().index):\n    ax = axes.ravel()[i]\n    df_t = train_df[train_df['title'] == idx][['age', 'survived']].dropna()\n#     print(f\"{idx}:\\n{df_t}\")\n#     print(f\"\\n{idx}:\")\n    for i_s in set(train_df['survived']):\n        df_hist = df_t[df_t['survived'] == i_s]\n#         print(f\"{i_s}:\\n{df_hist}, {df_hist.shape}\")\n        if df_hist.shape[0] == 0:  # Empty DataFrame \u306e\u6642\u306f\n            continue              # continue\u6587\u3067\u30eb\u30fc\u30d7\u5185\u306e\u51e6\u7406\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b\n        df_hist['age'].plot.hist(ax=ax, bins=20, label=\"{}\".format(i_s), color=colors[i_s], alpha=.3)\n        ax.set_ylabel(''); ax.set_title(idx); ax.legend()\nfig.tight_layout()\n# print(f\"{train_df[train_df['title'] == 'Master']}\")","b37bca9c":"train_df[train_df['title'] == 'Master']","d3619217":"pd.pivot_table(train_df[train_df['title'] == 'Master']\n              , index=['survived']\n              , columns=['sibsp']\n              , values=['passengerid']\n              , aggfunc={'passengerid': [len]}\n              , fill_value=0\n              , )","574344fd":"print(f\"{set(train_df['title']) - set(test_df['title'])}\")\nprint(f\"{set(test_df['title']) - set(train_df['title'])}\")\nn_train = [n for n in set(train_df['title'])]\nn_test = [n for n in set(test_df['title'])]\n# print(f\"{n_train}\\n{n_test}\")\n# print(f\"{n_train in n_test}\\n{n_test in n_train}\")\nprint(set(n_train + n_test))","88dbfd26":"print(f\"{train_df.shape}, {test_df.shape}\")\ntitle_all = pd.concat([train_df['title'], test_df['title']])\nprint(f\"{title_all.shape}\")\nprint(f\"{pd.DataFrame(title_all).groupby(['title']).size()}\\n\")\nprint(pd.DataFrame(title_all).groupby(['title']).size().index)","bea4792c":"for dataset in combine:\n    dataset['title'] = dataset['title'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dona', 'Dr', 'Jonkheer'\\\n                                                  , 'Lady', 'Major', 'Rev', 'Sir'], 'Rare')\n    dataset['title'] = dataset['title'].replace('Mlle', 'Miss')\n    dataset['title'] = dataset['title'].replace('Mme', 'Mrs')\n    dataset['title'] = dataset['title'].replace('Ms', 'Miss')\n    \ntrain_df[['title', 'survived']].groupby(['title']).mean()","e8dd6171":"print(f\"{train_df.groupby(['title']).size().shape}\")\nfig, axes = plt.subplots(figsize=(6,1.5))\ntrain_df.groupby(['title']).size().plot.bar(color='silver', rot=False); axes.set_xlabel('')\nfig, axes = plt.subplots(2, 3, figsize=(10,4))\ncolors = ['red', 'blue']\nfor i, id in enumerate(train_df.groupby(['title']).size().index):\n    ax = axes.ravel()[i]\n    df_i = train_df[train_df['title'] == id][['age', 'survived']].dropna()\n#     print(f\"{i}: {id}\")\n    for isv in set(train_df['survived']):\n#         print(f\"{isv}::\")\n        df_hist = df_i[df_i['survived'] == isv]\n#         print(f\"{df_hist}\")\n        df_hist['age'].plot.hist(ax=ax, bins=20, label=\"{}\".format(isv), color=colors[isv], alpha=.3)\n        ax.set_ylabel(''); ax.set_title(\"{}\".format(id))\n        ax.legend()\nfig.tight_layout()\n{k: v for k, v in zip(train_df.groupby(['title']).size().index, train_df.groupby(['title']).size().values)}","d4419a0c":"train_df.groupby(['title']).size().sort_values(ascending=False).index","2ec966d0":"train_df.groupby(['title']).size().index.isin(['Master', 'Miss', 'Mr', 'Mrs', 'Rare']).any()","1a05105e":"title_mapping = {c: i + 1 for i, c in enumerate(train_df.groupby(['title'])\\\n                                                .size().sort_values(ascending=False).index)}\nprint(f\"{title_mapping}\")\n\nif train_df.groupby(['title']).size().index.isin(['Master', 'Miss', 'Mr', 'Mrs', 'Rare']).any():   # \u30a8\u30e9\u30fc\u9632\u6b62\u306e\u305f\u3081\n    for dataset in combine:\n        dataset['title'] = dataset['title'].map(title_mapping)\n        dataset['title'] = dataset['title'].fillna(0)\n    \ntrain_df.head()","30ded058":"train_df['title'].value_counts()","e961c826":"for i, dataset in enumerate(combine):\n    print(f\"{[col for col in dataset.columns.values]}, {dataset.shape}\")","917be6dc":"train_df.columns.isin(['passengerid', 'name']).any()","c924d49e":"if train_df.columns.isin(['passengerid', 'name']).any():\n    train_df = train_df.drop(['passengerid', 'name'], axis=1)\n    test_df = test_df.drop(['name'], axis=1)\ncombine = [train_df, test_df]\nfor dataset in combine:\n    print(f\"{[n for n in dataset.columns]}, {dataset.shape}\")","15123590":"set(train_df['sex'])","b3efefe9":"if set(train_df['sex']) == {'female', 'male'}:\n    for dataset in combine:\n        dataset['sex'] = dataset['sex'].map({'male': 0, 'female': 1})\ntrain_df.head()","21acc2c8":"np.corrcoef(train_df['pclass'], train_df['age'].fillna(0))","591adc9a":"col_short = []\nfig, axes = plt.subplots(2, 4, figsize=(12, 3))\ni_hist = 0\nfor i, col in enumerate(train_df.columns):\n    if train_df[col].nunique() < 10:\n        col_short.append(col)\n        print(f\"{i}:{col} \\n{train_df[col].nunique()}: {train_df[col].unique()}\\ni_hist: {i_hist}\\n\")\n        ax = axes.ravel()[i_hist]\n        train_df.groupby(col).size().plot.bar(ax=ax, color='silver')\n        ax.set_xlabel(''); ax.set_title(col)\n        i_hist += 1\nfig.tight_layout()\nprint(f\"col_short: {col_short}, {len(col_short)}\")\nprint(f\"\u4e0a\u8a18\u4ee5\u5916\u306e\u30ab\u30e9\u30e0: {train_df.columns[~train_df.columns.isin(col_short)]}\\n\")\nprint(f\"train_df.head(): \\n{train_df.head()}\")","474d05e2":"list_corr_coef = []; name_corr_coef = [c for c in col_short if c != 'embarked']\nfor col in name_corr_coef:\n    corr_coef = np.corrcoef(train_df[col], train_df['age'].fillna(0))\n    print(f\"{corr_coef[0, 1]}\")\n    list_corr_coef.append(corr_coef[0, 1])\nfig, axes = plt.subplots(figsize=(10,2))\nxticks = np.arange(len(name_corr_coef))\naxes.bar(xticks, list_corr_coef)\naxes.set_xticks(xticks)\naxes.set_xticklabels(name_corr_coef); axes.grid()","1f2afb71":"# grid = sns.FacetGrid(train_df, row='pclass', col='sex', height=2, aspect=1.6)\ngrid = sns.FacetGrid(train_df, row='pclass', col='sex', aspect=1.6)\ngrid.map(plt.hist, 'age', alpha=.5, bins=20)","3bd7b900":"train_df_copy = train_df.copy()\ntest_df_copy = test_df.copy()\nprint(f\"{[c for c in train_df_copy.columns]}, {train_df_copy.columns.shape}\")\nprint(f\"{[c for c in test_df_copy.columns]}, {test_df_copy.columns.shape}\")\nprint(f\"{[c for c in train_df_copy.columns if c not in [c for c in test_df_copy.columns]]}: column(s) only in train_df_copy\")","6111ecf6":"train_df_copy.head(3)","c8db6b3d":"print(f\"{set(train_df_copy['pclass'])}\\n{set(train_df_copy['sex'])}\\n\")\nfor i in set(train_df_copy['pclass']):\n    df_i = train_df_copy[train_df_copy['pclass'] == i]\n    for j in set(train_df_copy['sex']):\n        df_j = df_i[df_i['sex'] == j]\n#         print(f\"{i}, {j}\\n{df_j.head(3)}\")\n        age_target = df_j['age'].median()\n        print(f\"{i}, {j}: {age_target}\")  # i, j \u305d\u308c\u305e\u308c\u3067 df \u3092\u4f5c\u3063\u3066\u62bd\u51fa\u53ca\u3073 age_target \u306e\u7b97\u51fa\u306f\u3067\u304d\u305f\u3051\u3069\u3001\n        print(f\"{df_j[df_j['age'].isnull()].head()}\\n\")\n#         print(f\"{train_df_copy.loc[(train_df_copy['pclass'] == i) & (train_df_copy['sex'] == j) & \\\n#               (train_df_copy['age'].isnull()), 'age']}\")  # \u6b20\u640d\u5024\u88dc\u5b8c\u306f train_df_copy \u306e\u64cd\u4f5c\u306a\u306e\u3067 loc \u884c\u8a2d\u5b9a\u30673\u7a2e\u306e bool \u8a2d\u5b9a\n#         print((train_df_copy['pclass'] == i) & (train_df_copy['sex'] == j) & (train_df_copy['age'].isnull())) # bool \u884c\u62bd\u51fa","b5a2f594":"train_df_copy.head(3)","4c5598df":"nullrow_age = train_df_copy['age'].isnull()\nprint(f\"{train_df_copy[nullrow_age].head(10)}\\n\")","2bd09ed5":"combine_copy =[train_df_copy, test_df_copy]\n\nfor dataset in combine_copy:\n    print(f\"{dataset.columns[0]}\")\n    for i in set(dataset['pclass']):\n        df_i = dataset[dataset['pclass'] == i]\n        for j in set(dataset['sex']):\n            df_j =df_i[df_i['sex'] == j]\n            age_target = df_j['age'].median() ## .dropna() \u304c\u629c\u3051\u3066\u308b\n            print(f\"{i}, {j}:\\n{df_j[df_j['age'].isnull()].head(3)}\\n{age_target} <--\")\n            \n            print(f\"{dataset.loc[(dataset['pclass'] == i) & (dataset['sex'] == j) & (dataset['age'].isnull()), 'age'].head(3)}\\n\")\n            dataset.loc[(dataset['pclass'] == i) & (dataset['sex'] == j) & \n                        (dataset['age'].isnull()), 'age'] = age_target\n    \n    dataset['age'] = dataset['age'].astype(int)","61f6b8ae":"print(f\"{train_df_copy[nullrow_age].head(10)}\")","8d4e2276":"print(f\"{set(train_df['pclass'])}\")\nprint(f\"{set(train_df['sex'])}\")\n\nbins = 20\ncolors = ['r', 'b']\nfig, axes = plt.subplots(1, 3, figsize=(14,2))\nfor i in set(train_df['pclass']):\n#     print(f\"\\n{i}:\")\n    df_i = train_df[train_df['pclass'] == i]\n    \n    hist_max = 0\n    for j in set(train_df['sex']):  # hist_max \u53d6\u5f97\u306e\u305f\u3081\u3060\u3051\u306e\u30a4\u30c6\u30ec\u30fc\u30c8\n        df_j = df_i[df_i['sex'] == j]\n        hist, h_bins = np.histogram(df_j['age'].dropna(), bins=bins)\n        print(f\"{i}, {j}\\n{hist}, {hist.max()}\\n{h_bins}\")\n        if hist.max() > hist_max:\n            hist_max = hist.max()\n        print(f\"hist_max: {hist_max}\") # hist_max \u3092\u53d6\u5f97\u3057\u3066\u4e00\u65e6\u30eb\u30fc\u30d7\u7d42\u4e86\n\n#     print(f\"{df_i.head(3)}\")\n    for j in set(train_df['sex']):\n        ax = axes.ravel()[i-1]\n#         print(f\"{j}:\")\n        df_j = df_i[df_i['sex'] == j]\n#         print(f\"{df_j.head(2)}\")\n        df_j['age'].dropna().hist(ax=ax, color=colors[j], alpha=.3, bins=bins, label=\"sex: {}\".format(j))\n        ax.vlines(x=df_j['age'].median(), ymin=0, ymax=hist_max, color=colors[j])\n        ax.vlines(x=df_j['age'].mean(), ymin=0, ymax=hist_max, color=colors[j], alpha=.7)\n        ax.set_title(\"pclass: {}\".format(i))\n        ax.set_xlabel('age')\n        ax.legend()","4fe7d130":"guess_ages = np.zeros([2, 3])\nguess_ages","c03aaa56":"print(f\"train_df['age'].isnull().sum(): {train_df['age'].isnull().sum()}\")\nrow_null_age = train_df['age'].isnull()\ntrain_df.loc[row_null_age, :].head()","c2ff2c1d":"for id, dataset in enumerate(combine):\n    print(f\"dateset: {id}\\n\")\n    for i in range(0, 2):\n        print(f\"{i}:\")\n        for j in range(0, 3):\n            print(f\" {j}:\")\n            guess_df_all = dataset[(dataset['sex'] == i) & (dataset['pclass'] == j+1)]\n#             print(f\"{guess_df_all.head()}\")\n            guess_df = guess_df_all['age'].dropna()\n#             print(f\"{guess_df.head()}, {type(guess_df)}\")\n            age_guess = guess_df.median()\n#             print(f\"{age_guess}\")\n            \n            # Convert random age float to nearest .5 age\n            print(f\"{int( age_guess\/0.5 + 0.5 ) * 0.5}\\n\")\n            guess_ages[i, j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n\n    print(f\"{guess_ages}\\n\")\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n#             print(f\"{i}: {j}\\n{dataset.loc[(dataset['age'].isnull()) & (dataset['sex'] == i) & (dataset['pclass'] == j+1),\\\n#             'age'].head()}\\n\")\n            dataset.loc[(dataset['age'].isnull()) & (dataset['sex'] == i) & (dataset['pclass'] == j+1), 'age'] = guess_ages[i, j]\n    \n    dataset['age'] = dataset['age'].astype(int)\n\ntrain_df.loc[row_null_age, :].head()","0649e046":" train_df.age.isnull().sum()","942b0c5e":"train_df.loc[row_null_age, :].head(10)","4dec5533":"(train_df['age'] != train_df_copy['age']).all()","af3f84c3":"print(f\"{(train_df['age'] != train_df_copy['age']).sum()}\")\ntrain_df[train_df['age'] != train_df_copy['age']]","95575956":"print(f\"{train_df.groupby(['sibsp']).size()}\")\nprint(f\"{train_df.groupby(['parch']).size()}\")","f49eadc8":"train_df['familysize'] = train_df['sibsp'] + train_df['parch'] + 1 # \u30bc\u30ed\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b 1 \u3092\u8db3\u3059\ntrain_df.head()","7bbe6b7e":"for dataset in combine:\n    dataset['familysize'] = dataset['sibsp'] + dataset['parch'] + 1\ntrain_df[['sibsp', 'parch', 'familysize']].head()","421ed55d":"train_df.groupby(['familysize']).mean()['survived'].sort_values(ascending=False)","f3fb315c":"combine = [train_df, test_df]\nfor dataset in combine:\n    dataset['isalone'] = 0\n    dataset.loc[dataset['familysize'] == 1, 'isalone'] = 1 # \u4e8c\u3064\u4e0a\u3067\u5144\u5f1f\u306e\u6570\u3082\u89aa\u5b50\u306e\u6570\u3082\u30bc\u30ed\u306b1\u3092\u8db3\u3057\u305f\u306e\u30671\u304c\u5144\u5f1f\u89aa\u5b50\u304c\u30bc\u30ed\n    print(f\"{dataset.head()}\")\ntrain_df[['sibsp', 'parch', 'familysize', 'isalone']].head()","c143efe6":"train_df[['isalone', 'survived']].groupby(['isalone']).mean()","7f35308c":"print(f\"Before: {train_df.columns.values}\")\nif train_df.columns.isin(['sibsp', 'parch', 'familysize']).any():\n    train_df = train_df.drop(['sibsp', 'parch', 'familysize'], axis=1)\nif test_df.columns.isin(['sibsp', 'parch', 'familysize']).any():\n    test_df = test_df.drop(['sibsp', 'parch', 'familysize'], axis=1)\n    \nprint(f\"{train_df.columns.values}\")\nprint(f\"{test_df.columns.values}\")\ntrain_df.head()","79985491":"print(f\"{train_df.embarked.isnull().sum()}\")\ntrain_df.groupby(['embarked']).size().sort_values(ascending=False)","b53952ef":"freq_port = train_df['embarked'].dropna().mode()[0]\nfreq_port","67cfe12f":"combine = [train_df, test_df]\nfor dataset in combine:\n    dataset['embarked'] = dataset['embarked'].fillna(freq_port)\nprint(f\"{train_df.embarked.isnull().sum()}\")\ntrain_df.groupby(['embarked']).size().sort_values(ascending=False)","f733d9fa":"train_df[['embarked', 'survived']].groupby(['embarked']).mean().sort_values('survived', ascending=False)","303b6a12":"print(\"{}\".format({v: i for i, v in enumerate(['S', 'C', 'Q'])}))\ncombine = [train_df, test_df]\nif dataset.groupby(['embarked']).size().index.isin(['C', 'Q', 'S']).any():\n    for dataset in combine:\n        dataset['embarked'] = dataset['embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\ntrain_df.groupby(['embarked']).size()\nprint(f\"After: {train_df.groupby(['embarked']).size().index.values}\")\ntrain_df.head()","5f868a98":"null_fare_row = test_df['fare'].isnull()\nprint(f\"Before: \\n{test_df.loc[null_fare_row, :]}\")\n# print(f\"{}\")\ntest_df['fare'].fillna(test_df['fare'].dropna().median(), inplace=True)\nprint(f\"After: \\n{test_df.loc[null_fare_row, :]}\")","3193bfde":"h_bins = 50\nfig, axes = plt.subplots(figsize=(12,2))\ntrain_df['age'].plot.hist(bins=h_bins, alpha=.3)\nhist, n_bins = np.histogram(train_df['age'], bins=h_bins)\nprint(f\"{hist.min()}, {hist.max()}\")\nprint(\"\")\n\nbins=5\ncut, cbins = pd.cut(train_df['age'], bins=bins, retbins=True)\nprint(\"cut: {}\".format(pd.DataFrame(cut).groupby(['age']).size()))\nprint(f\"cbins: {cbins}\\n\")\naxes.vlines(x=cbins, ymin=hist.min(), ymax=hist.max(), color='y', alpha=.8, label='cut_bins')\n\ncut, cbins = pd.cut(train_df['age'], bins=bins, retbins=True, labels=False)\nprint(\"cut( , labels=False): {}\".format(pd.DataFrame(cut).groupby(['age']).size()))\nprint(f\"cbins: {cbins}\\n\")\n\ncut, cbins = pd.qcut(train_df['age'], q=bins, retbins=True)\nprint(\"qcut: {}\".format(pd.DataFrame(cut).groupby(['age']).size()))\nprint(f\"cbins: {cbins}\")\n\naxes.vlines(x=cbins, ymin=hist.min(), ymax=hist.max(), color='r', alpha=.8, label='qcut_bins')\n\naxes.legend()","0c1ea966":"# train_df['ageband'] = pd.cut(train_df['age'], 5)\n# train_df","3bf719d1":"# train_df[['ageband', 'survived']].groupby(['ageband']).mean()","abfa7e79":"# train_df.groupby(['ageband']).size()","4bb81e52":"train_df.head(3)","2780be33":"pipe_rf = Pipeline([('scl', StandardScaler()), ('est', RandomForestClassifier(n_estimators=100, random_state=0))])\npipe_gb = Pipeline([('scl', StandardScaler()), ('est', GradientBoostingClassifier(random_state=0))])","bd6404f9":"import warnings\nwarnings.filterwarnings('ignore')\n\nage_max, fare_max = 12, 10\nage_bins, fare_bins = np.arange(2, age_max + 1), np.arange(2, fare_max + 1) # \u30d3\u30cb\u30f3\u30b0\u306e\u30ea\u30b9\u30c8\u4f5c\u6210\nprint(f\"{age_bins}\\n{fare_bins}\\n\")\n\ntrain_df_bins = train_df.copy() # train_df_bins \u3068\u3044\u3046\u4f5c\u696d\u7528\u30b3\u30d4\u30fc\u4e0a\u3067\u30d3\u30cb\u30f3\u30b0\u3092\u8a66\u3059\n\nage_list, fare_list, rf_acc_list, rf_f1_list, gb_acc_list, gb_f1_list = [], [], [], [], [], []\nscore_name = ['rf_acc', 'rf_f1', 'gb_acc', 'gb_f1']\nbest_score = 0\nfor age_bin in age_bins:\n    bins = age_bin\n    for fare_bin in fare_bins:\n        q = fare_bin\n        cut, cbins = pd.cut(train_df['age'], bins=bins, retbins=True, labels=False)\n        train_df_bins['age'] = cut\n        qcut, qbins = pd.qcut(train_df['fare'], q=q, retbins=True, labels=False)\n        train_df_bins['fare'] = qcut\n\n        train_df_bins['age_class'] = train_df_bins['age'] * train_df_bins['pclass']\n\n        X_bin = train_df_bins.iloc[:, 1:]\n        y_bin = train_df_bins.iloc[:, [0]]\n        \n        indices = np.arange(X_bin.shape[0])\n        X_bin_train, X_bin_valid, y_bin_train, y_bin_valid, indices_train, indices_valid = train_test_split(\n            X_bin, y_bin, indices, random_state=0)\n        \n        pipe_rf.fit(X_bin_train, y_bin_train.values.ravel())\n        pipe_gb.fit(X_bin_train, y_bin_train.values.ravel())\n        \n        rf_acc = accuracy_score(y_bin_valid.values.ravel(), pipe_rf.predict(X_bin_valid))\n        rf_f1 = f1_score(y_bin_valid.values.ravel(), pipe_rf.predict(X_bin_valid))\n        \n        gb_acc = accuracy_score(y_bin_valid.values.ravel(), pipe_gb.predict(X_bin_valid))\n        gb_f1 = f1_score(y_bin_valid.values.ravel(), pipe_gb.predict(X_bin_valid))\n        \n        age_list.append(age_bin); fare_list.append(fare_bin); rf_acc_list.append(rf_acc); rf_f1_list.append(rf_f1)\n        gb_acc_list.append(gb_acc); gb_f1_list.append(gb_f1)\n        \n        if max([rf_acc, rf_f1, gb_acc, gb_f1]) > best_score:\n            best_score = max([rf_acc, rf_f1, gb_acc, gb_f1])\n            best_indx = [rf_acc, rf_f1, gb_acc, gb_f1].index(max([rf_acc, rf_f1, gb_acc, gb_f1]))\n            best_age_bin, best_fare_bin = age_bin, fare_bin\n        \n#         print(f\"{bins}: {q}: \\n{train_df.columns.values}\\n{train_df_bins.columns.values}\")\n#         print(f\"{train_df_bins.groupby(['age']).size()}\")\n#         print(f\"{train_df_bins.groupby(['fare']).size()}\")\n#         print(f\"{train_df_bins.groupby(['age_class']).size()}\")\n\n#         print(f\"X_bin.head(5):\\n{X_bin.head(5)}\")\n#         print(f\"y_bin.head(5):\\n{y_bin.head(5)}\")\n#         print(f\"{X_bin.shape}, {y_bin.shape}\\n{X_bin_train.shape}, {X_bin_valid.shape}, {y_bin_train.shape}, {y_bin_valid.shape}\")\n        \n#         print(f\"{rf_acc}\\n{rf_f1}\\n{gb_acc}\\n{gb_f1}\")\n        print(f\"{bins}: {q}: {[rf_acc, rf_f1, gb_acc, gb_f1]}\")\n#         print(f\"{max([rf_acc, rf_f1, gb_acc, gb_f1])}, {[rf_acc, rf_f1, gb_acc, gb_f1].index(max([rf_acc, rf_f1, gb_acc, gb_f1]))}\")\n        \nprint(f\"{best_age_bin}, {best_fare_bin}, {best_score}, {best_indx}, {score_name[best_indx]}\")\n# X_bin_train\n# y_bin_train\n# X_bin_valid\n# y_bin_valid","915336aa":"len(age_list)\nlen(fare_list)\nv_index = [(a, f) for a, f in zip(age_list, fare_list)]\nprint(f\"{v_index[rf_acc_list.index(max(rf_acc_list))]}: {max(rf_acc_list)}\\n{v_index[rf_f1_list.index(max(rf_f1_list))]}: \\\n{max(rf_f1_list)}\\n{v_index[gb_acc_list.index(max(gb_acc_list))]}: {max(gb_acc_list)}\\n\\\n{v_index[gb_f1_list.index(max(gb_f1_list))]}: {max(gb_f1_list)}\")","fcbdd76a":"fig, axes = plt.subplots(4, 1, figsize=(12,4))\nfor i, data in enumerate([rf_acc_list, rf_f1_list, gb_acc_list, gb_f1_list]):\n    data = np.array(data)\n    dindx = list(data).index(data.max())\n    dmax = data.max() - data.min()\n#     print(dindx)\n#     print(list(data).index(dmax))\n    ax=axes.ravel()[i]\n    pd.DataFrame(data-data.min()).plot.bar(ax=ax, alpha=.7)\n    ax.set_xticklabels('')\n    ax.set_title(score_name[i])\n    ax.plot(dindx, dmax, 'o', color='r', alpha=.5)\n    print(f\"{score_name[i]:<6}: {data.max():.4f}, {v_index[dindx]}\")\naxes.ravel()[3].set_xticklabels(v_index); fig.tight_layout()","19338337":"print(f\"best_age_bin: {best_age_bin}\\nbest_fare_bin: {best_fare_bin}\")","da93c947":"combine = [train_df, test_df]\nprint(f\"{train_df.groupby(['age']).size().shape[0]}\")\nif train_df.groupby(['age']).size().shape[0] == 71: # \u5909\u66f4\u524d\u306e\u5e74\u9f62\u30b0\u30eb\u30fc\u30d7\u6570\n    for i, dataset in enumerate(combine):\n        cut, cbins = pd.cut(dataset['age'], bins=best_age_bin, retbins=True, labels=False)\n        dataset['age'] = cut\n        print(f\"{i}: {dataset.groupby(['age']).size().shape[0]}\")\n        print(\"\")","2731603b":"for i, dataset in enumerate(combine):\n    print(f\"{i}:\\n{dataset.groupby(['age']).size()}\")","9cd77941":"for i, dataset in enumerate(combine):\n    print(f\"{i}:\\n{dataset.head()}\\n\")","efb2b594":"# if 'ageband' in train_df.columns:\n#     train_df = train_df.drop(['ageband'], axis=1)\n# train_df.head()","0476c99f":"gby = train_df[['age', 'pclass', 'survived']].groupby(['age', 'pclass']).mean()\ngby","b9e21d94":"pivot_mean = pd.pivot_table(train_df\n              , index=['age']\n              , columns=['pclass']\n#               , values=['survived']\n              , aggfunc={'survived': [np.mean, len]}\n              , )\npivot_mean","d2f5d576":"combine = [train_df, test_df]\nfor dataset in combine:\n    dataset['age*class'] = dataset['age'] * dataset['pclass']\n    print(dataset[['age*class', 'age', 'pclass']].head())\ntrain_df.loc[:, ['pclass', 'age', 'age*class']].head(10)\ntrain_df.groupby(['pclass', 'age', 'age*class']).size()","f02abea6":"print(f\"{train_df['fare'].describe()}\")\nprint(f\"median: {train_df['fare'].median()}\")\nbins = 20\nhist, bins = np.histogram(train_df['fare'], bins=bins)\n# print(f\"{hist}, {hist.min()}, {hist.max()}, {hist.mean()}\\n{bins}, {bins.min()}, {bins.max()}\")\nfig, axes = plt.subplots(figsize=(8,2))\ntrain_df.fare.plot.hist(bins=bins, alpha=.3)\n# axes.plot(train_df['fare'].median(), 400, 'o')\n# axes.hlines(y=hist.max(), xmin=bins.min(), xmax=bins.max(), colors='r', alpha=.5)\naxes.vlines(x=train_df['fare'].median(), ymin=hist.min(), ymax=hist.max(), colors='b', label='median')\naxes.legend()","73e7afed":"fig, axes = plt.subplots(figsize=(8,2))\nbins = 20\ntrain_df['fare'].plot.hist(bins=bins, color='grey', alpha=.3, label='')\nhist, hbins = np.histogram(train_df['fare'], bins=bins)\n\nbins = best_fare_bin\ncut, cbins = pd.cut(train_df['fare'], bins=bins, retbins=True, labels=False) # \u6700\u5927\u5024\u3068\u6700\u5c0f\u5024\u306e\u9593\u3092\u7b49\u9593\u9694\u3067\u5206\u5272\u3059\u308b\u3002\nprint(f\"cut.head(3):\\n{cut.head(3)}\\ncbins: {cbins}\\n\") # \u5f15\u6570retbins=True\u3067\u3001\u30d3\u30f3\u5206\u5272\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3068\u5883\u754c\u5024\u306e\u30ea\u30b9\u30c8\u3092\u540c\u6642\u306b\u53d6\u5f97\u3067\u304d\u308b\u3002\nprint(f\"{pd.DataFrame(cut).groupby(['fare']).size()}\\n\")\naxes.vlines(x=cbins, ymin=hist.min(), ymax=hist.max()\/3, colors='r', alpha=.5, label='cut_bins')\n\ncut, cbins = pd.qcut(train_df['fare'], q=bins, retbins=True, labels=False) # \u5404\u30d3\u30f3\u306b\u542b\u307e\u308c\u308b\u500b\u6570\uff08\u8981\u7d20\u6570\uff09\u304c\u7b49\u3057\u304f\u306a\u308b\u3088\u3046\u306b\u30d3\u30cb\u30f3\u30b0\nprint(f\"qcut.head(10): # \u78ba\u8a8d\u7528\\n{cut.head(10)}\\ncbins: {cbins}\\n\")\nprint(f\"{pd.DataFrame(cut).groupby(['fare']).size()}\\n\")\naxes.vlines(x=cbins, ymin=hist.min(), ymax=hist.max()\/3*2, colors='b', alpha=.5, label='qcut_bins')\naxes.legend()","fccf0088":"# train_df[['fareband', 'survived']].groupby(['fareband']).mean().sort_values('survived', ascending=False)","75f71b1c":"train_df.head()","39f4ffcb":"best_fare_bin","ef0c7bd5":"combine = [train_df, test_df] \nq = best_fare_bin\ntrain_df['fare'].isin([n for n in np.arange(q)]).all()\nif not train_df['fare'].isin([n for n in np.arange(q)]).all():\n    for i, dataset in enumerate(combine):\n        qcut, qbins = pd.qcut(dataset['fare'], q=q, retbins=True, labels=False)\n        print(f\"{i}:\\n{pd.DataFrame(qcut).groupby(['fare']).size()}\\n\")\n        dataset['fare'] = qcut\n        dataset['fare'] = dataset['fare'].astype(int)\n    \n# if 'fareband' in train_df.columns:\n#     train_df = train_df.drop(['fareband'], axis=1)","821fc15b":"for i, dataset in enumerate(combine):\n    print(f\"{i}:\\n{dataset.groupby(['fare']).size()}\\n\")","06b5860a":"train_df.head(10)","28aebc8c":"test_df.head()","f6b82449":"train_df.corr()","6f241f16":"sns.heatmap(train_df.corr(), cmap='PuBu', annot=True, fmt='.2f')","11c55537":"corr_id = [id for id in train_df.corr()['survived'].index if id != 'survived'] # 'survived' \u4ee5\u5916\u306e\u30a4\u30f3\u30c7\u30af\u30b9\u3092\u53d6\u5f97\ncorr_id","59a3657f":"print(f\"{train_df.corr().loc['survived', corr_id]}\")\nfig, axes = plt.subplots(figsize=(7,3))\ntrain_df.corr().loc['survived', corr_id].plot.bar(ax=axes)\n# axes.axhline(y=0, color='silver', linestyle='-', linewidth=1)\naxes.set_title('corrcoef to survived')\naxes.set_xticklabels([id.upper() for id in corr_id])\n# axes.grid()\nfig.tight_layout()","c30cb72b":"X_train = train_df.iloc[:, 1:]\ny_train = train_df.iloc[:, [0]]\nX_test = test_df.drop('passengerid', axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","5fa88e11":"X_train.head(3)\n# y_train","701dd4a1":"X_test.head(3)","8ad1283b":"from sklearn.metrics import precision_score, recall_score\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train.as_matrix().ravel())\ny_pred = logreg.predict(X_test)\nprint(f\"{logreg.score(X_train, y_train):.4f}\")\nprint(f\"{accuracy_score(y_train.as_matrix().ravel(), logreg.predict(X_train)):.4f}\") # \u540c\u3058\nprint(f\"{f1_score(y_train.as_matrix().ravel(), logreg.predict(X_train)):.4f}\")\nprint(f\"{confusion_matrix(y_train.as_matrix().ravel(), logreg.predict(X_train))}\")\nprint(f\"{precision_score(y_train.as_matrix().ravel(), logreg.predict(X_train)):.4f}\")\nprint(f\"{recall_score(y_train.as_matrix().ravel(), logreg.predict(X_train)):.4f}\")","e98770f1":"print(f\"{logreg.coef_}, {logreg.coef_.shape}\\n{X_train.columns}, {X_train.columns.shape}\")\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['feature']\ncoeff_df['correlation'] = logreg.coef_.ravel()\ncoeff_df.sort_values('correlation', ascending=False)","002a3e17":"coeff_df = pd.DataFrame([X_train.columns, logreg.coef_.ravel()], index=['feature', 'correlation']).T\n\n# # \u4ee5\u4e0b\u3067\u3082\u53ef\n# coeff_df = pd.DataFrame({\n#       'feature': X_train.columns\n#     , 'correlation': logreg.coef_.ravel()}\n#     , columns=['feature', 'correlation'] )\n\nfig, axes = plt.subplots(figsize=(8,2))\ncoeff_df.plot.bar(ax=axes)\naxes.set_xticklabels(coeff_df['feature'], rotation=0)\n\ncoeff_df.T","29113825":"[c for c in X_train.columns.values], [c for c in X_test.columns.values]","611d143d":"logreg = LogisticRegression()\nsvc = SVC()\nknn = KNeighborsClassifier()\ngaussian = GaussianNB()\nperceptron = Perceptron(max_iter=5)\nlinear_svc = LinearSVC()\nsgd = SGDClassifier()\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\ngb = GradientBoostingClassifier()\n\nestimators = [logreg, svc, knn, gaussian, perceptron, linear_svc, sgd, decision_tree, random_forest, gb]\nest_names = ['LogisticRegression', 'SVC', 'KNeighborsClassifier', 'GaussianNB', 'Perceptron', 'LinearSVC',\n             'SGDClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier']","48bd6972":"acc_scores, f1_scores = [], []\nfor i, est in enumerate(estimators):\n    est.fit(X_train, y_train.values.ravel())\n    score_acc = accuracy_score(y_train.as_matrix().ravel(), est.predict(X_train))\n    score_f1 = f1_score(y_train.as_matrix().ravel(), est.predict(X_train))\n    acc_scores.append(score_acc)\n    f1_scores.append(score_f1)\n        \n    print(f\"{est_names[i]}:\")\n#     print(f\"score: {est.score(X_train, y_train):>10.4f}\")\n    print(f\"acc_score: {score_acc:.4f}\")\n    print(f\"f1_score:  {score_f1:.4f}\")\n    print(f\"{confusion_matrix(y_train.as_matrix().ravel(), est.predict(X_train))}\")\n    print(f\"pre_score: {precision_score(y_train.as_matrix().ravel(), est.predict(X_train)):.4f}\")\n    print(f\"rec_score: {recall_score(y_train.as_matrix().ravel(), est.predict(X_train)):.4f}\\n\")    ","13c48d38":"df_score = pd.DataFrame([est_names, acc_scores, f1_scores], index=['est_name', 'accuracy', 'f1_score'], ).T # \u8ee2\u7f6e\ndf_score # \u3068\u308a\u3042\u3048\u305a\u30ea\u30b9\u30c8\u304b\u3089\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u3063\u3066\u307f\u308b","836a9ffc":"df_best_acc = df_score[df_score['accuracy'] == df_score['accuracy'].max()] # df_score['accuracy'].max() \u3068\u4e00\u81f4\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u4f5c\u6210\nbest_acc_name = df_best_acc.iloc[0, 0] # \nprint(f\"best_acc_name ({best_acc_name}): {df_best_acc.iloc[0, 1]}\")\nbest_acc_index = df_best_acc.index.values[0]; print(f\"best_acc_index: {best_acc_index}\") # \u30a4\u30f3\u30c7\u30af\u30b9\u3092\u53d6\u5f97\u3057\u3066\nbest_acc_estimator = estimators[best_acc_index]; print(f\"best_acc_estimator: \\n{best_acc_estimator}\") # estimators[\u30a4\u30f3\u30c7\u30af\u30b9]\ndf_best_acc","db953221":"print(f\"{acc_scores.index(max(acc_scores))}, {est_names[acc_scores.index(max(acc_scores))]}: {max(acc_scores)}\")\nbest_acc_est = estimators[acc_scores.index(max(acc_scores))]  # acc_scores \u306e max \u306e index \u3092\u53d6\u5f97\u3057\u3066 estimators \u306b\u3076\u3064\u3051\u308b\n\nbest_acc_est","a9edb676":"best_acc_est.fit(X_train, y_train.as_matrix().ravel())","5f4abfee":"fig, axes = plt.subplots(3, 1, figsize=(10,3)) # \u4e09\u5217\u4f5c\u3063\u3066\nh_col = [n for n in df_score.columns if n != 'est_name'] # 'est_name' \u3092\u9664\u3044\u305f\u6570\u5024\u5217\u306e ['accuracy', 'f1_score'] \nprint(f\"{h_col}\")\nfor i, col in enumerate(h_col): # 1\u5217\u30682\u5217\u3092\u305d\u308c\u305e\u308c\u63cf\u753b\n    ax = axes.ravel()[i]\n#     print(df_score[col])\n    df_score[col].plot.bar(ax=ax, color='b')\n    ax.set_title(col)\n    ax.set_xticklabels('')\ndf_score.plot.bar(ax=axes.ravel()[2]) # 3\u5217\u76ee\u306f\u4e00\u7dd2\u306b\u63cf\u753b\naxes.ravel()[2].set_xticklabels([n[:11] for n in df_score['est_name']], rotation=0)\nfig.tight_layout()","3dced7d9":"y_pred = best_acc_est.predict(X_test)\ny_pred.shape","39e25f82":"test_df['passengerid'].shape","dc34ba48":"submission = pd.DataFrame({\n                    'PassengerId': test_df['passengerid'],\n                    'Survived': y_pred,\n                })\nsubmission.to_csv('submission.csv', index=False)\nsubmission","53d9a9c5":"print(f\"X_train.shape: {X_train.shape}\\ny_train.shape: {y_train.shape}\\nX_test.shape:  {X_test.shape}\\n\")\nprint(f\"{test_df[['passengerid']].head(3)}\\n\")\nprint(\"{}\\n{}\".format([c for c in X_train.columns], [c for c in X_test.columns]))\nX_train.head()","6ea49c9e":"# X_train.isnull().sum()","1ba014b8":"# X_test.isnull().sum()","a2f7ec8b":"# X_train.dtypes","00a49884":"# X_test.dtypes","5e96443b":"print(f\"X_train.shape: {X_train.shape}\\ny_train.shape: {y_train.shape}\\nX_test.shape:  {X_test.shape}\")\nindices = np.arange(X_train.shape[0])\nprint(f\"{indices.shape}\")\nX_fin_train, X_fin_valid, y_fin_train, y_fin_valid, indices_train, indices_valid = train_test_split(\n    X_train, y_train, indices, random_state=0)\nprint(f\"shapes of: X_fin_train, X_fin_valid, y_fin_train, y_fin_valid, indices_train, indices_valid; \\n\\\n{X_fin_train.shape}, {X_fin_valid.shape}, {y_fin_train.shape}, {y_fin_valid.shape}, {indices_train.shape}, {indices_valid.shape}\")","da291a1e":"X_train","22d2491a":"pipe_logreg = Pipeline([('scl', StandardScaler()), ('est', LogisticRegression(random_state=0))])\npipe_svc = Pipeline([('scl', StandardScaler()), ('est', SVC(random_state=0))])\npipe_knn = Pipeline([('scl', StandardScaler()), ('est', KNeighborsClassifier())])\npipe_gaussian = Pipeline([('scl', StandardScaler()), ('est', GaussianNB())])\npipe_perceptron = Pipeline([('scl', StandardScaler()), ('est', Perceptron(max_iter=5, random_state=0))])\npipe_linear_svc = Pipeline([('scl', StandardScaler()), ('est', LinearSVC(random_state=0))])\npipe_sgd = Pipeline([('scl', StandardScaler()), ('est', SGDClassifier(max_iter=1000, tol=1e-3, random_state=0))])\npipe_decision_tree = Pipeline([('scl', StandardScaler()), ('est', DecisionTreeClassifier(random_state=0))])\npipe_rf = Pipeline([('scl', StandardScaler()), ('est', RandomForestClassifier(random_state=0))])\npipe_gb = Pipeline([('scl', StandardScaler()), ('est', GradientBoostingClassifier(random_state=0))])\npipe_xgb = Pipeline([('scl', StandardScaler()), ('est', XGBClassifier(random_state=0))])\n\npipe_xgb.named_steps","020919ee":"pipes =[pipe_logreg, pipe_svc, pipe_knn, pipe_gaussian, pipe_perceptron, pipe_linear_svc, pipe_sgd, \n        pipe_decision_tree, pipe_rf, pipe_gb, pipe_xgb]","d0997435":"pipe_xgb.named_steps['est']","e9412907":"param_grid_logreg = {'est__C': [0.05, 0.1, 1.0, 10.0, 100.0],\n                    'est__penalty': ['l1', 'l2']}\nparam_grid_svc = {\n#                 'est__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                'est__C': [1, 10, 100, 1000],\n                'est__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\nparam_grid_knn = {'est__n_neighbors': range(1,101),\n                  'est__weights': ['uniform', 'distance']}\n\nparam_grid_gaussian = {}\nparam_grid_perceptron = {'est__penalty': [None, 'l2', 'l1', 'elasticnet'],\n                'est__alpha': [0.00001, 0.0001, 0.001, 0.01],}\n\nparam_grid_linear_svc = {\n#                 'est__penalty': ['l1', 'l2'],\n#                 'est__loss': ['hinge', 'squared_hinge'],\n                'est__dual': [True, False],\n                'est__tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n                'est__C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]}\n\nparam_grid_sgd = {'est__loss': ['hinge', 'log', 'modified_huber'],\n                'est__penalty': ['none', 'l2', 'l1', 'elasticnet'],\n}\n\nparam_grid_decision_tree = {'est__criterion': ['gini', 'entropy'],\n                    'est__max_depth': range(1, 11),\n                    'est__min_samples_split': range(2, 21),\n                    'est__min_samples_leaf': range(1, 21),}\n\nparam_grid_rf = {\n                'est__n_estimators': [10, 50, 100, 150, 200],\n                'est__criterion': ['gini', 'entropy'],\n                'est__max_features': np.arange(0.05, 1.01, 0.05),\n}\n\nparam_grid_gb = {\n                'est__n_estimators': [50, 100, 150],\n                'est__learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n                'est__max_depth': range(1, 11),\n#                 '': [],\n}\n\nparam_grid_xgb = {\n#       'est__n_estimators': [100]\n#     , 'est__learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.] \n     'est__learning_rate': [1e-1, 0.5, 1.] \n#     , 'est__max_depth': range(1, 11) \n    , 'est__max_depth': range(1, 11, 2) \n#     , 'est__min_child_weight': range(1, 21) \n    , 'est__min_child_weight': range(1, 11) \n    , 'est__subsample': np.arange(0.05, 1.01, 0.05) \n#     , 'est__nthread': [1] \n}","008d7962":"param_grids = [param_grid_logreg, param_grid_svc, param_grid_knn, param_grid_gaussian, param_grid_perceptron, param_grid_linear_svc, \n              param_grid_sgd, param_grid_decision_tree, param_grid_rf, param_grid_gb, param_grid_xgb]","06c22d2c":"import time\nstart_time = time.time()\n\nbest_score, best_acc_score, best_f1_score, best_params, best_estimator = [], [], [], [], []\nfor i, pipe in enumerate(pipes):\n    i_time = time.time()\n    param_grid = param_grids[i]\n    gs = GridSearchCV(pipe, param_grid, cv=3)\n    gs.fit(X_fin_train, y_fin_train.as_matrix().ravel())\n    best_score.append(gs.best_score_)\n    acc_score = accuracy_score(y_fin_valid.as_matrix().ravel(), gs.predict(X_fin_valid))\n    best_acc_score.append(acc_score)\n    f_score = f1_score(y_fin_valid, gs.predict(X_fin_valid))\n    best_f1_score.append(f_score)\n    best_params.append(gs.best_params_)\n    best_estimator.append(gs.best_estimator_)\n    print(f\"{i}:\\n{pipe.named_steps['est']}\")\n    print(f\"gs.best_score_: {gs.best_score_:.4f}\\naccuracy_score: {acc_score:.4f}\\nf1_score_valid: {f_score:.4f}\")\n    print(f\"{gs.best_params_}\")\n    print(f\"{gs.best_estimator_.named_steps['est']}\")\n    \n    print(f\"{time.time() - i_time:.2f} sec.\\n{time.time() - start_time:.2f} sec.\\n\")\n","54bcff2f":"est_names = ['LogisticRegression', 'SVC', 'KNeighbors', 'GaussianNB', 'Perceptron', 'LinearSVC', \n             'SGDClassifier', 'DecisionTree', 'RandomForest', 'GradientBoosting', 'XGBClassifier']","e71c96da":"scores_df = pd.DataFrame([best_score, best_acc_score, best_f1_score]\n            , index=['best_score', 'best_acc_score', 'best_f1_score']\n            , columns=[c[:10] for c in est_names]).T\nscores_df.to_csv('scores_df.csv')\nscores_df.sort_values(by=['best_score'], ascending=False)","d6b07a66":"scores_df","96656e97":"print(f\"max(best_score): {max(best_score)}\")\nprint(f\"best_score.index(max(best_score)): {best_score.index(max(best_score))}\")\nprint(f\"{best_params[best_score.index(max(best_score))]}\")\nprint(f\"est_names[best_score.index(max(best_score))]: {est_names[best_score.index(max(best_score))]}\")\nfinal_pipe = best_estimator[best_score.index(max(best_score))]\nfinal_pipe.named_steps['est']","beb4841f":"# \u305d\u306e\u524d\u306b joblib \u3067\u30c7\u30a3\u30b9\u30af\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f\nfrom sklearn.externals import joblib\nfor i, pipe in enumerate(best_estimator):\n#     print(f\"{i}: {est_names[i]}\\n{pipe}\\n\")\n    joblib.dump(pipe, est_names[i] + '.pkl')","4dc4100d":"# \u30c7\u30a3\u30b9\u30af\u304b\u3089\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u547c\u3073\u51fa\u3057\u3066\u3082\u30aa\u30fc\u30b1\u30fc\nbest_name = est_names[best_score.index(max(best_score))]\n# joblib_load_best = joblib.load('..\/input\/' + best_name + '.pkl')\njoblib_load_best = joblib.load(best_name + '.pkl')\n\njoblib_load_best.named_steps['est']","0140ae0f":"print(f\"X_test.shape: {X_test.shape}\")\nfinal_pred = final_pipe.predict(X_test)\nprint(f\"final_pred.shape: {final_pred.shape}\")\nprint(f\"test_df['passengerid'].shape: {test_df['passengerid'].shape}\")","91584730":"submission_final = pd.DataFrame({\n    'PassengerId': test_df['passengerid'], \n    'Survived': final_pred\n    })\nsubmission_final.to_csv('submission_final_0010_020.csv', index=False)\nsubmission_final","b06b0d1f":"print(f\"{best_estimator[9].named_steps['est']}\")\n\npd.DataFrame({\n    'PassengerId': test_df['passengerid'], \n    'Survived': best_estimator[9].predict(X_test)\n}).to_csv('submission_final_0010_020_GB.csv', index=False)","49252ce6":"n_to_get = 4\nfig, axes = plt.subplots(1, n_to_get, figsize=(16,3))\n\ne_list = np.arange(len(best_estimator))[-n_to_get:]\nprint(f\"{e_list}\")\n\nfor h, i in enumerate(e_list):\n    ax = axes.ravel()[h]\n    df_imp = pd.DataFrame(\n        best_estimator[i].named_steps['est'].feature_importances_,\n        index = X_train.columns ,\n        columns=['f_imp'])\n    df_imp.iloc[::-1, :].plot.barh(ax=ax); ax.set_title(est_names[i])\n#     df_imp.plot.barh(ax=ax); ax.set_title(est_names[i])\n    ax.set_yticklabels([c.upper()[-8:] for c in X_train.columns[::-1]])\n#     print(f\"{i}: {est_names[i]}\\n{df_imp.iloc[::-1, :]}\\n\")\nfig.tight_layout()","76dab7f8":"pdp_pipe = best_estimator[9] # GradientBoostingClassifier\ndf_imp = pd.DataFrame(pdp_pipe.named_steps['est'].feature_importances_, index=X_fin_train.columns, columns=['importance'])\nfig, axes = plt.subplots(figsize=(8, 2))\ndf_imp.plot.bar(ax=axes); axes.set_xticklabels([i.upper()[:6] for i in df_imp.index])\nfig.tight_layout()","94871f57":"from sklearn.ensemble.partial_dependence import plot_partial_dependence\ndf_sort = df_imp.reset_index().sort_values(by=['importance'], ascending=False)\nfig, axes = plt.subplots(figsize=(12,6))\nplot_partial_dependence(pdp_pipe.named_steps['est'], X_fin_train, features=df_sort.index, feature_names=df_sort['index'], ax=axes)\nprint(f\"{df_sort}\")\nfig.tight_layout()","67f817e7":"# pd.DataFrame(gs.cv_results_)","d5e9044a":"Let us create Age bands and determine correlations with Survived.","8ca16673":"### Partial Dependence Plots","e40dc31e":"\u4ee5\u4e0b\u3092 train_df \u306b test_df \u306b\u9069\u7528\u3059\u308b\u3002","6cd861b2":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).","64e14299":"\u3053\u308c\u3089\u306e\u533a\u9593\u306b\u57fa\u3065\u3044\u3066\u5e74\u9f62\u3092\u5e8f\u6570\u306b\u7f6e\u304d\u63db\u3048\u307e\u3057\u3087\u3046\u3002","231d0c88":"\u4e0a\u8a18\u3092 pd.pivot_table \u3067\u63cf\u753b\uff08\u7df4\u7fd2 14-Aug-2018, Tue\uff09","1d1a4b4d":"\u89b3\u5bdf\n\n    \u3088\u308a\u9ad8\u3044\u904b\u8cc3\u3092\u652f\u6255\u3046\u4e57\u5ba2\u306f\u3088\u308a\u826f\u3044\u751f\u5b58\u7387\u3092\u793a\u3057\u305f\u3002 \u4eee\u8aac\u3010Fare\u306e\u7bc4\u56f2\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f5c\u6210\u3011\u3002\n    \u4e57\u8239\u6e2f\u306f\u751f\u5b58\u7387\u3068\u76f8\u95a2\u3059\u308b\u3002\n\n\u7d50\u8ad6\n\n    Fare\u7279\u5fb4\u91cf\u306e\u30d0\u30f3\u30c7\u30a3\u30f3\u30b0(\u4e00\u5b9a\u306e\u533a\u9593\u3067\u533a\u5207\u3063\u3066\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u306b\u3059\u308b)\u3092\u691c\u8a0e\u3059\u308b\u3002","a6d3a6ed":"\u30c6\u30b9\u30c8\u30bb\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3002","42fdf9f3":"We can also create an artificial feature combining Pclass and Age.<br>\nPclass\u3068Age\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u4eba\u5de5\u7684\u306a\u7279\u5fb4\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002","249e9d64":"\u6b20\u640d\u5024\u88dc\u5b8c\u6e08\u307f\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u6570\u5024\u3078\u306e\u5909\u63db\u6e08\u307f\u3002X_train \u3068 y_train \u3067 holdout \u3092\u884c\u3046\u3002 Pipeline \u3092\u4f5c\u6210\u3057\u3001GridSearch \u3092\u884c\u3046\u3002RFE \u306f\u304a\u305d\u3089\u304f\u4e0d\u8981\u3002","6f61d0ac":"\u4e26\u3079\u3066\u8868\u793a","a7786834":"\u9023\u7d9a\u7684\u6570\u5024\u306e\u7279\u5fb4\u91cf\u3092\u88dc\u5b8c\u3059\u308b\n\n\u4eca\u5ea6\u306f\u3001\u6b20\u640d\u5024\u307e\u305f\u306fnull\u5024\u3092\u6301\u3064\u7279\u5fb4\u91cf\u3092\u3001\u305d\u306e\u6b20\u640d\u5024\u3092\u63a8\u5b9a\u3057\u3066\u88dc\u5b8c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u307e\u305a\u3001age\u7279\u5fb4\u91cf\u3067\u3053\u308c\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u6570\u5024\u9023\u7d9a\u7684\u7279\u5fb4\u91cf\u3092\u88dc\u5b8c\u3059\u308b\u306e\u306b\u3001\u3053\u3053\u3067\u306f3\u3064\u306e\u65b9\u6cd5\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002\n\n    \u7c21\u5358\u306a\u65b9\u6cd5\u306f\u3001\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u306e\u9593\u306e\u4e71\u6570\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\n    \u6b20\u640d\u5024\u3092\u63a8\u6e2c\u3059\u308b\u3088\u308a\u6b63\u78ba\u306a\u65b9\u6cd5\u306f\u3001\u4ed6\u306e\u76f8\u95a2\u3059\u308b\u7279\u5fb4\u91cf\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u4eca\u56de\u306e\u30b1\u30fc\u30b9\u3067\u306f\u3001\u5e74\u9f62\u3001\u6027\u5225\u3001\u304a\u3088\u3073Pclass\u306e\u9593\u306e\u76f8\u95a2\u3092\u8a18\u9332\u3059\u308b\u3002 Pclass\u3068Gender\u306e\u7279\u5fb4\u91cf\u306e\u7d44\u307f\u5408\u308f\u305b\u306e\u30bb\u30c3\u30c8\u5168\u4f53\u3067age\u306e\u4e2d\u592e\u5024\u3092\u4f7f\u7528\u3057\u3066Age\u306e\u5024\u3092\u63a8\u6e2c\u3057\u307e\u3059\u3002Pclass = 1\u3001Gender = 0\u3001Pclass = 1\u3001Gender = 1\u306a\u3069\u306e\u4e2d\u9593\u306e\u5e74\u9f62\u306a\u3069\u306a\u3069\u3002\n\n    \u65b9\u6cd51\u30682\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3002\u4e2d\u592e\u5024\u306b\u57fa\u3065\u3044\u3066\u5e74\u9f62\u5024\u3092\u63a8\u6e2c\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001Pclass\u3068Gender\u306e\u7d44\u307f\u5408\u308f\u305b\u306e\u30bb\u30c3\u30c8\u306b\u57fa\u3065\u3044\u3066\u3001\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u306e\u9593\u306b\u306a\u308b\u4e71\u6570\u3092\u4f7f\u7528\u3059\u308b\u3002\n\n\u65b9\u6cd51\u30683\u306f\u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u3092\u30e2\u30c7\u30eb\u306b\u5c0e\u5165\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u3001\u8907\u6570\u56de\u306e\u5b9f\u884c\u7d50\u679c\u304c\u7570\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3088\u3063\u3066\u65b9\u6cd52\u3092\u512a\u5148\u3057\u307e\u3059\u3002","6415cca5":"\u76f8\u95a2\u3059\u308b\u7279\u5fb4\u91cf\u3068\u3057\u3066 pclass \u3068 sex \u3092\u9078\u3093\u3060\u306e\u306f\u305d\u308c\u304c age \u3068\u5f37\u304f\u76f8\u95a2\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u3088\u308a\u3082\u3001\u7279\u5fb4\u91cf\u5185\u306e\u30ab\u30c6\u30b4\u30ea\u6570\u304c\u5c11\u306a\u3044\uff08\u8a08\u7b97\u304c\u5bb9\u6613\uff09\u304b\u3089\u3068\u3044\u3046\u7406\u7531\u306e\u65b9\u304c\u5927\u304d\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\uff08\u4ee5\u4e0b\u3092\u53c2\u7167\uff09\u3002\u305f\u3060\u30ab\u30c6\u30b4\u30ea\u3054\u3068\u306e\u3070\u3089\u3064\u304d\u304c\u5c11\u306a\u3044\u3068\u3044\u3046\u306e\u306f\u3042\u308b\u3002","f006545b":"\u4e0a\u8a18\u306e estimator \u53d6\u5f97\u304c\u683c\u597d\u60aa\u3044\u3002\u4ee5\u4e0b\u306a\u3089\u4e00\u767a\u3002","82099609":"\u4e0a\u8a18 final_pipe \u3067 X_test \u304b\u3089 predict","f2ce7454":"['parch'] \u3092\u4f8b\u306b\u3068\u3063\u3066 .groupby(['']).mean() \u306e\u304a\u3055\u3089\u3044\u3002survived \u304c 01 \u306a\u306e\u3067\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u548c\u3092\u8981\u7d20\u6570\u3067\u5272\u3063\u305f\u5e73\u5747 mean() \u306f\u305d\u306e\u30ab\u30c6\u30b4\u30ea\u5185\u306e1\uff08\u3053\u306e\u5834\u5408\u306f\u751f\u304d\u6b8b\u3063\u305f\u4eba\uff09\u306e\u69cb\u6210\u6bd4\u306b\u306a\u308b","ee7cf802":"train_df 'pclass', 'sex', 'age' \u3092\u53ef\u8996\u5316","97b88388":"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference Wikipedia https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine .\n\nNote that the model generates a confidence score which is higher than Logistics Regression model.","ec20e8df":"\u884c(passengerid)\u6bce\u306b sibsip \u3068 parch \u306e\u5024\uff08\u5144\u5f1f\u306e\u6570\u3068\u89aa\u5b50\u306e\u6570\uff09\u3092\u8db3\u3057\u305f\u3082\u306e\u3092\u65b0\u3057\u3044\u7279\u5fb4\u3068\u3057\u3066\u63a1\u7528\u3059\u308b","a92d9f27":"\u4e0a\u8a18\u30d4\u30dc\u30c3\u30c8\u30c6\u30fc\u30d6\u30eb\u3088\u308a\u3082\u4ee5\u4e0b\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e bins \u8abf\u7bc0\u306e\u307b\u3046\u304c\u304d\u308c\u3044","c7651b71":"https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","cdfd6958":"seaborn \u3060\u3068 yticks \u304c\u5909\u66f4\u3067\u304d\u306a\u3044\u306e\u3067\u518d\u63cf\u753b","41382596":"\u81ea\u529b\u3067 age \u306e\u6b20\u640d\u5024\u51e6\u7406\u3092\u3059\u308b\u3002\u30c6\u30ad\u30b9\u30c8\u3068\u540c\u69d8\u306b 'pclass' \u3068 'sex' \u306e\u7d44\u307f\u5408\u308f\u305b\u304b\u3089 'age' \u3092\u7b97\u51fa\u3057\u3066\u88dc\u5b8c\u3059\u308b\u3002","258f6812":"\u307e\u305a parch \u3067\u30c6\u30b9\u30c8","14a33975":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","a993e852":"\u6700\u3082\u30b9\u30b3\u30a2\u306e\u9ad8\u3044 age \u3068 fare \u306e\u30d3\u30cb\u30f3\u30b0\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u63a2\u3059","9495db83":"FareBand\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002","d94db1e8":"\u4eca\u5ea6\u306f\u3001Sex\uff080\u307e\u305f\u306f1\uff09\u3068Pclass\uff081,2,3\uff09\u3092\u7e70\u308a\u8fd4\u3057\u30016\u3064\u306e\u7d44\u307f\u5408\u308f\u305b\u306eAge\u306e\u63a8\u6e2c\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002","73dbf887":"GradientBoosting.predict()","087ecc0a":"\u91cf\u7684\u7279\u5fb4\u91cf\u3054\u3068\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3002\u91cd\u306d\u3066\u8868\u793a","6aac3019":"\u307e\u305a\u306f\u305d\u306e\u305f\u3081\u306b\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210","a4305951":"\uff08FareBand\u306b\u57fa\u3065\u3044\u3066Fare\u7279\u5fb4\u91cf\u3092\u5e8f\u6570\u306b\u5909\u63db\u3057\u307e\u3059\u3002\uff09","2bfcdde3":"pclass \u5225\u306e age \u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u4f5c\u3063\u3066\u307f\u308b","1bf81782":"\u3053\u3053\u3067\u5b9f\u884c","61e57208":"\u3048\u30fc\u4f55\u3053\u308c\u3069\u3046\u3044\u3046\u3053\u3068\uff1f\u5144\u5f1f\u304c\u5c11\u306a\u3044\u5b50\u3092\u512a\u5148\u7684\u306b\u52a9\u3051\u305f\u3063\u3066\u3053\u3068\uff1f","3f60660d":"\u5148\u751f\u306f pd.cut \u3092\u4f7f\u3063\u3066\u3044\u308b","1dda6bc0":"\u5217\u65b9\u5411\u306e\u6bd4\u7387\u306b\u5909\u63db","1be04391":"\u4e0a\u8a18\u3092 pd.pivot_table \u3067\u8868\u73fe\u3057\u3066\u307f\u308b","52224f5f":"\u4e00\u81f4\u306e\u78ba\u8a8d","c9cd210f":"Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n    A simple way is to generate random numbers between mean and standard deviation.\n\n    More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n    Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.\n","38ace865":"We can create FareBand.","9bd3224e":"Analyze by pivoting features \u7279\u5fb4\u91cf\u306e\u76f8\u95a2\u3092\u89e3\u6790\u3059\u308b","219e10e3":"Pclass x Gender\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u57fa\u3065\u3044\u3066\u63a8\u6e2c\u3055\u308c\u305fAge\u5024\u3092\u683c\u7d0d\u3059\u308b\u70ba\u306e\u3001\u7a7a\u306e\u914d\u5217\u3092\u6e96\u5099\u3059\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u307e\u3057\u3087\u3046\u3002","6f74cf23":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.","ab0f5f69":"We can replace many titles with a more common name or classify them as Rare.","cb039cd9":"\u30b0\u30e9\u30d5\u306f\u9006\u306b\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u65b9\u304c\u4fbf\u5229\u3002\u7279\u306b .bar()","b3d38102":"Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n\nInversely as Pclass increases, probability of Survived=1 decreases the most.\n\nThis way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n\nSo is Title as second highest positive correlation.","de47fe86":"pd.pivot_table \u3092 for \u3067\u56de\u3057\u3066\u53ef\u8996\u5316\uff08\u30ab\u30e9\u30e0\u5185\u306e\u30ab\u30c6\u30b4\u30ea\u5225\u306e suevived \u306e\u6bd4\u7387\uff09","3ef1c042":"train_df \u3068 test_df \u3067 pd.cut( , retbins=True, labels=False) \u3067\u53d6\u5f97\u3057\u305f\u6570\u5b57\uff08indicators of the bins\uff09\u3092\u5145\u3066\u308b\u3002","76e95f5a":"\u6b63\u898f\u8868\u73fe\u306e\u7df4\u7fd2","d6c92aa6":"\u6a2a\u4e26\u3073\u306b\u3057\u3066y\u8ef8\u306e\u4e00\u81f4\u304c\u3084\u3063\u3068\u51fa\u6765\u305f\u30fc \uff3c(^o^)\uff0f ","89bae267":"\nModel, predict and solve\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n    Logistic Regression\n    KNN or k-Nearest Neighbors\n    Support Vector Machines\n    Naive Bayes classifier\n    Decision Tree\n    Random Forrest\n    Perceptron\n    Artificial neural network\n    RVM or Relevance Vector Machine\n\n","9f2c8f8c":"\u4e0a\u8a18\u3092 for \u3067\u56de\u3057\u3066\u53ef\u8996\u5316","05071fbc":"pd.cut \u3068 pd.qcut \u306e\u6bd4\u8f03","03fa3b34":"\u5e74\u9f62\u304c\u4f4e\u304f\u3066 survived \u304c\u62ee\u6297\u3057\u3066\u3044\u308b master \u3092\u898b\u3066\u307f\u308b","db423748":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.<br>\nIsAlone\u304c\u826f\u3055\u3052\u306a\u306e\u3067\u3001Parch\u3001SibSp\u3001\u304a\u3088\u3073FamilySize\u7279\u5fb4\u91cf\u3092\u524a\u9664\u3057\u307e\u3059\u3002","cb7b2c27":"\u4ee5\u4e0b\u306f\u5b9f\u884c\u3057\u306a\u3044\u3002train_df \u3068 test_df \u3067 pd.cut( , retbins=True, labels=False) \u3067\u53d6\u5f97\u3057\u305f\u6570\u5b57\uff08indicators of the bins\uff09\u3092\u5145\u3066\u308b\u3002","ddee5049":"\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u4f5c\u3063\u3066\u307f\u308b","1f335e89":"Variable\tDefinition\tKey<br>\nsurvival \tSurvival \t0 = No, 1 = Yes<br>\npclass \tTicket class \t1 = 1st, 2 = 2nd, 3 = 3rd<br>\nsex \tSex \t<br>\nAge \tAge in years<br> \t\nsibsp \t# of siblings \/ spouses aboard the Titanic \t<br>\nparch \t# of parents \/ children aboard the Titanic \t<br>\nticket \tTicket number \t<br>\nfare \tPassenger fare \t<br>\ncabin \tCabin number \t<br>\nembarked \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton<br>\n\nVariable Notes<br>\npclass: A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n\nsibsp: The dataset defines family relations in this way...<br>\nSibling = brother, sister, stepbrother, stepsister<br>\nSpouse = husband, wife (mistresses and fiances were ignored)<br>\n\nparch: The dataset defines family relations in this way...<br>\nParent = mother, father<br>\nChild = daughter, son, stepdaughter, stepson<br>\nSome children travelled only with a nanny, therefore parch=0 for them.<br>","1f262e20":"AgeBand\u7279\u5fb4\u91cf\u3092\u524a\u9664\u3057\u307e\u3059\u3002","047c88cf":"\n\nLogistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference Wikipedia.\n\nNote the confidence score generated by the model based on our training dataset.\n","0891fd4d":"Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).","3060fc73":"\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u306a\u30bf\u30a4\u30c8\u30eb\u3092\u5e8f\u6570\u306b\u5909\u63db\u3059\u308b","42cc2cb3":"Age\u306e\u533a\u9593(Band)\u3092\u4f5c\u6210\u3057\u3001Survived\u3068\u306e\u76f8\u95a2\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","4c7a2543":"We can convert the categorical titles to ordinal.","9c895df3":"\u5225\u306b\u76f8\u95a2\u304c\u5f37\u3044\u3063\u3066\u308f\u3051\u3067\u306f\u306a\u3044\u3002pclass \u306b\u306f\u8ca0\u306e\u76f8\u95a2\u304c\u3042\u308b\u304c\u3002","f8ead5fb":"Age \u3068 Cabin \u306b\u6b20\u640d\u5024\u3042\u308a","1a051409":"And the test dataset.","dccbfa17":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","722ab57d":"Let us replace Age with ordinals based on these bands.","e2ea2929":"\u65e2\u5b58\u306e\u3082\u306e\u304b\u3089\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b\n\nName\u3068PassengerId\u3092\u524a\u9664\u3059\u308b\u524d\u306b\u3001Name\u304b\u3089Title(\u80a9\u66f8)\u3092\u62bd\u51fa\u3057\u3001Title\u3068Survived\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u8abf\u3079\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u6b21\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001\u6b63\u898f\u8868\u73fe\u3092\u4f7f\u7528\u3057\u3066Title\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 \u6b63\u898f\u8868\u73fe\u30d1\u30bf\u30fc\u30f3 (\\w+\\.)\u306f\u3001Name\u7279\u5fb4\u91cf\u5185\u306e\u30c9\u30c3\u30c8\u6587\u5b57\u3067\u7d42\u308f\u308b\u6700\u521d\u306e\u5358\u8a9e\u3068\u4e00\u81f4\u3057\u307e\u3059\u3002 expand = False\u30d5\u30e9\u30b0\u306fDataFrame\u3092\u8fd4\u3057\u307e\u3059\u3002","ee3e5fa9":"Decisions.\n\n    Add Sex feature to model training.\n    Complete and add Embarked feature to model training.\n    \n\u7d50\u8ad6\n\n    \u30e2\u30c7\u30eb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306bSex\u7279\u5fb4\u91cf\u3092\u8ffd\u52a0\u3059\u308b\u3002\n    \u30e2\u30c7\u30eb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306bEmbarked\u7279\u5fb4\u91cf\u3092\u88dc\u5b8c\u3057\u3066\u8ffd\u52a0\u3059\u308b\u3002\n","0d46dd84":"train_df, test_df \u4e21\u65b9\u3067\u306e\u64cd\u4f5c\u306e\u305f\u3081\u306b\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3059\u308b","ba02c838":"\u5fa9\u7fd2\u3092\u517c\u306d\u30665\u6b73\u523b\u307f\u3067 pd.cut \u3067\u30d3\u30cb\u30f3\u30b0\u51e6\u7406\u3092\u3059\u308b","34300717":"\nCompleting a categorical feature\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.<br>\n\n\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u7279\u5fb4\u91cf\u3092\u88dc\u5b8c\u3059\u308b\n\nEmbarked\u7279\u5fb4\u91cf\u306f\u3001\u4e57\u8239\u6e2f\u306b\u57fa\u3065\u3044\u3066S\u3001Q\u3001C\u306e\u5024\u3092\u53d6\u308a\u307e\u3059\u3002 \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f2\u3064\u306e\u6b20\u640d\u5024\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u6700\u3082\u4e00\u822c\u7684\u306a\u51fa\u73fe\u3067\u57cb\u3081\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002","e44d638a":"predict","eead086a":"Converting categorical feature to numeric<br>\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.<br>\n\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u7279\u5fb4\u91cf\u3092\u6570\u5024\u306b\u5909\u63db\u3059\u308b<br>\n\u6b20\u640d\u5024\u3092\u88dc\u5b8c\u3057\u305f\u306e\u3067\u3001Embarked\u7279\u5fb4\u91cf\u3092\u6570\u5024\u306b\u5909\u63db\u51fa\u6765\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002","046241ec":"\u4e0a\u8a18\u3067\u5b66\u7fd2\u3067\u304d\u308b\u3002\u3053\u3063\u3061\u306e\u65b9\u304c\u7c21\u6f54\u3002","860df867":"We can create another feature called IsAlone.\n<br>IsAlone\u3068\u3044\u3046\u5225\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002","60381ed4":"Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.\n","43305364":"### \u30c6\u30ad\u30b9\u30c8\u901a\u308a\u306f\u3053\u3053\u307e\u3067\u3002\u3053\u3053\u304b\u3089\u306f Pipeline, GridSearch, StandardScaler \u7b49\u3092\u4f7f\u3063\u3066\u3088\u308a\u826f\u3044\u30b9\u30b3\u30a2\u3092\u76ee\u6307\u3059\u3002","31c17408":"pd.cut \u3068 pd.qcut \u306e\u6bd4\u8f03","0e0e409e":"Quick completing and converting a numeric feature\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\n Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.<br>\n\n\u30af\u30a4\u30c3\u30af\u88dc\u5b8c\u3068\u6570\u5024\u7279\u5fb4\u91cf\u306e\u5909\u63db\n\n\u3053\u306e\u7279\u5fb4\u91cf\u306e\u6700\u983b\u5024\uff08\u6700\u3082\u983b\u7e41\u306b\u767b\u5834\u3059\u308b\u5024\uff09\u3092\u4f7f\u7528\u3057\u3066\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5024\u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u5834\u5408\u306b\u3001Fare\u7279\u5fb4\u91cf\u3092\u88dc\u5b8c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u308c\u306f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u884c\u3048\u307e\u3059\u3002\n\n    \u88dc\u5b8c\u3059\u308b\u76ee\u7684\u306f\u3001\u6b20\u640d\u5024\u3092\u57cb\u3081\u3066\u3001\u30e2\u30c7\u30eb\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u3042\u308b\u7a0b\u5ea6\u671b\u307e\u3057\u3044\u72b6\u614b\u3067\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u5fc5\u8981\u4ee5\u4e0a\u306b\u6b20\u640d\u5024\u306e\u63a8\u6e2c\u306b\u6642\u9593\u3092\u639b\u3051\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u901a\u8ca8\u3092\u8868\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u904b\u8cc3\u306e\u5c0f\u6570\u70b9\u7b2c\u4e8c\u4f4d\u4ee5\u4e0b\u3092\u56db\u6368\u4e94\u5165\u3059\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002","5f9f9ac0":"\u9762\u5012\u306a\u306e\u3067\u6700\u521d\u306e LogisticRegression \u304b\u3089\u30a4\u30c6\u30ec\u30fc\u30c8\u3059\u308b","7de8b573":"### groupby([col]).size() \u3067\u30ab\u30c6\u30b4\u30ea\u5225\u306e\u983b\u5ea6\u3001\u3055\u3089\u306b\u53ef\u8996\u5316\uff08pandas plot.pie() \u3067\u5186\u30b0\u30e9\u30d5\uff09","c85a2b7f":"\u65e2\u5b58\u306e\u7279\u5fb4\u91cf\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b\n\nParch\u3068SibSp\u3092\u7d44\u307f\u5408\u308f\u305b\u3066FamilySize\u3068\u3057\u3066\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089Parch\u3068SibSp\u3092\u524a\u9664\u3067\u304d\u307e\u3059\u3002","615dd204":"Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","744ba824":"train_df_copy, test_df_copy \u4e0a\u3067\u6b20\u640d\u5024\u88dc\u5b8c","bd1d796a":"\u307e\u305a\u306f train_df_copy \u3067\u30d7\u30ea\u30f3\u30c8\u3057\u3066\u307f\u308b","7547f0a5":"\u4e0a\u8a18\u3088\u308a\u3082\u3053\u3063\u3061\u306e\u65b9\u304c\u7d20\u6575","9485036a":"Convert the Fare feature to ordinal values based on the FareBand.","880076f0":"Creating new feature extracting from existing\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.","715ca0f4":"\nWrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\nCorrecting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.\n"}}