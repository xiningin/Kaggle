{"cell_type":{"119199ab":"code","da3f5d86":"code","38aac977":"code","8a05f5b2":"code","1b1a5921":"code","c9161f47":"code","050fec70":"code","52113228":"code","78690018":"code","31b250f9":"code","5dd6d33a":"code","01df40c9":"code","47596d7e":"code","d70c076c":"code","8cd7d117":"code","495f9360":"code","e8a9d17d":"markdown","5ca9946e":"markdown","4e205e81":"markdown"},"source":{"119199ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# from google.colab import drive\n# drive.mount('\/content\/drive\/')\n\nimport re","da3f5d86":"train_df  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","38aac977":"train_df.head()","8a05f5b2":"# A negative and a positive text for a disaster\ntrain_df = train_df.sample(frac=1)\nprint(train_df[train_df['target']==0]['text'].values[0])\nprint(train_df[train_df['target']==1]['text'].values[0])","1b1a5921":"df = pd.concat([train_df, test_df])\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x : remove_URL(x))","c9161f47":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x : remove_html(x))","050fec70":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))","52113228":"import string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","78690018":"\"\"\"Preprocessing and \ncreating a dictionary that has an index for each of the unique words\"\"\"\n\ndf['text'] = df['text'].str.lower()\n\n# # removing all the words starting with http and @ \n# df['text'] = df['text'].map(lambda x: (' '.join(word for word in x.split(' ') if not word.startswith(('http','@')))))\n\n# removing any non-alphanumeric characters\ndf['text']= df['text'].str.replace('[^a-z A-Z]', '')\n\n# separating the training, validation and testing set\n\ntrain_df = df.iloc[:7613]\ntest_df = df.iloc[7613:]\n\n\nwords = list(train_df['text'].str.split(' ', expand=True).stack().unique()) # getting the list of unique words\nvocabulary_size = len(words)\n\nprint('The number of unique words is:%d '%(vocabulary_size))\nprint('The total_number of words is : %d'%(len(list(train_df['text'].str.split(' ', expand=True).stack()))))\n\n# creating a dictionary for indexing the words\nwords_idx = { word:i for i, word in enumerate(words) }\ntrain_df.head()\n# words_idx\n\n","31b250f9":"\"\"\"Converting a single training example into the index retrieved from the dictionary \"\"\"\nexample = train_df['text'].str.split().values[1]\ninputs = [words_idx[i] for i in example]\ntargets = train_df['target'].values[1]\nprint(example)\nprint(inputs)\nprint(targets)","5dd6d33a":"# hyperparameters\nlearning_rate = 0.005\nn_h = hidden_size = 100\nn_x = vocabulary_size\nn_y = 2\n\n# model_parameters \nWhh = np.random.randn(hidden_size, hidden_size)*0.1\nWhx = np.random.randn(hidden_size, vocabulary_size)*0.1\nWyh = np.random.randn(2, hidden_size) *0.1\nby  = np.zeros((n_y,1))\nbh  = np.zeros((n_h,1))\n\n# \"\"\"loading the saved model\"\"\"\n# import pickle\n# filename = '\/kaggle\/input\/pkl-model\/rnn_model_v2.pkl'\n# with open(filename, \"rb\") as f:\n#     Whh, Whx, bh, by, Wyh  = pickle.load(f)\n\ncopy_df = train_df\ntrain_df = train_df.iloc[:7000]\nvalidation_df = copy_df.iloc[7000:]\n\n\nprint('The training set examples: %d' %(len(train_df)))\nprint('The validation set examples: %d' %(len(validation_df)))","01df40c9":"def feedforward(inputs):  # takes in the index of words in a example tweet and return the prediction \n    \n    xs,hs = [], np.zeros((n_h,1))\n    for t in range(len(inputs)):\n        \n        xs.append(np.zeros((n_x,1)))  # encode in 1-of-k representation\n        xs[t][inputs[t]] = 1\n        \n        hs = np.tanh(np.dot(Whh, hs) + np.dot(Whx, xs[t]) + bh)  # hidden state\n        \n    ys = np.dot(Wyh,hs) +by   # unnormalized log probabilities for next chars\n    ps = np.exp(ys)\/ np.sum(np.exp(ys),axis = 0)  # softmax probabiltity of non -disaster \/ disaster tweets\n    \n    prediction = np.argmax(ps)\n    \n    return prediction","47596d7e":"\"\"\"Generating a function that takes in one training example, feedforward into the network, calculate the cost which is the function of \npredicted y vs actual y. Then we perform a backward propagation and find the gradient of all the parameters and return it\"\"\"\n\ndef loss_func(inputs, targets):\n    \n    # input is the list of index in a training example (shape= (1,T_x))\n    # targets (0 or 1) for a training example[0,1]\n    \n    xs, hs =[],[]   # creating a cache of xs, hs for each unit of propagation\n    hs.append(np.zeros((n_h,1)))\n    loss = 0\n# feedforward propagation\n    \n    for t in range(len(inputs)):\n        \n        xs.append(np.zeros((n_x,1)))\n        xs[t][inputs[t]] = 1\n        \n        hs.append(np.tanh(np.dot(Whh, hs[t]) + np.dot(Whx, xs[t]) + bh))\n        \n    ys = np.dot(Wyh,hs[-1]) + by\n    ps = np.exp(ys)\/ np.sum(np.exp(ys),axis = 0)\n    \n# cost \n    y = np.zeros((2,1))\n    y[targets] =  1\n    loss = -np.log(np.sum(ps * y,axis = 0)) # cross_entropy loss\n    \n# backward_propagation through time \n    \"\"\"gradient of cost with respect to model parameters \"\"\"\n    dWhh, dWyh, dWhx = np.zeros_like(Whh), np.zeros_like(Wyh), np.zeros_like(Whx)\n    dby, dbh = np.zeros_like(by), np.zeros_like(bh)\n    \n    dy = ps-y\n    dWyh = np.dot(dy,hs[-1].transpose())\n    dby = np.copy(dy)\n    \n    dh = np.dot(Wyh.transpose(),dy)  \n    dh_raw = (1- hs[-1]*hs[-1]) * dh\n    \n    dWhx = np.dot(dh_raw, xs[-1].transpose())\n    dWhh = np.dot(dh_raw, hs[-2].transpose())\n    dbh  = np.copy(dh_raw)\n    dh_next = np.dot(Whh.transpose(),dh_raw)\n\n    for t in reversed(range(len(inputs)-2)):\n        \n        dh = np.copy(dh_next)\n        dh_raw = (1- hs[t+1]*hs[t+1]) * dh\n        \n        dWhx += np.dot(dh_raw, xs[t].transpose())\n        dWhh += np.dot(dh_raw, hs[t].transpose())\n        dbh  += np.copy(dh_raw)\n        dh_next = np.dot(Whh.transpose(),dh_raw)\n\n    for dparams in [dWhh, dWhx, dbh, dby, dWyh]: # clipping to avoid exploding gradients\n        np.clip(dparams, -5, 5 , out = dparams) \n\n    return loss, dWhh, dWhx, dbh, dby, dWyh ","d70c076c":"\"\"\"Feeding into the network to retrive the gradient and using Adagrad optimizer to perform the gradient descent.\nThen we repeat this for all the training examples and for n epochs.\"\"\"\n\nnum_iterations = 21000\n\nmWhh, mWyh, mWhx = np.zeros_like(Whh), np.zeros_like(Wyh), np.zeros_like(Whx)\nmby, mbh = np.zeros_like(by), np.zeros_like(bh)                                   # memory variables for Adagrad\n\nfor j in range(num_iterations):\n    \n    idx = j% len(train_df)\n    example = train_df['text'].str.split().values[idx]\n    inputs = [words_idx[i] for i in example]\n    targets = int(train_df['target'].values[idx])\n    \n    loss, dWhh, dWhx, dbh, dby, dWyh = loss_func(inputs, targets)\n    \n    \n    # Adagrad optimizer  \n    # perform parameter update with Adagrad\n    for param, dparam, mem in zip([Whx, Whh, Wyh, bh, by], \n                                [dWhx, dWhh, dWyh, dbh, dby], \n                                [mWhx, mWhh, mWyh, mbh, mby]):\n        mem += dparam * dparam\n        param += -learning_rate * dparam \/ np.sqrt(mem + 1e-8) # adagrad update\n\n    # validation accuracy\n    # using for loop instead of vectorization\n    if j % 700 == 0:\n      predictions = []\n      count=0\n      actual_targets= validation_df['target'].tolist()\n      for i in range(len(validation_df)):\n          example = validation_df['text'].str.split().values[i]\n          inputs = [words_idx[l] for l in example]\n          predictions.append(feedforward(inputs))\n          \n      for y, y_hat in zip(actual_targets, predictions):\n          if y==y_hat:\n              count+=1\n      print('The validation_accuracy after iterations:%d is %d'%(j,(count\/len(validation_df))*100))\n\n    #  training accuracy\n      \n      # predictions = []\n      # count = 0\n      # actual_targets = train_df['target'].tolist()\n\n      \n      # for i in range(len(train_df)):\n          \n      #     example = train_df['text'].str.split().values[i]\n      #     inputs = [words_idx[l] for l in example]\n      #     predictions.append(feedforward(inputs))\n          \n      # for y, y_hat in zip(actual_targets, predictions):\n      #     if y==y_hat:\n      #         count+=1\n              \n      # print('The training_accuracy after iterations:%d is %d'%(j,(count\/len(train_df))*100))\n   ","8cd7d117":"# predictions in the test set \ntest_predictions  = []\nfor i in range(len(test_df)):\n    example = test_df['text'].str.split().values[i]\n    inputs = []\n    for l in example:\n        if l in words_idx:\n            inputs.append(words_idx[l])\n            \n    test_predictions.append(feedforward(inputs))\n    \ntest_df['target'] = test_predictions\ntest_df = test_df[['id','target']].set_index('id')\ntest_df.to_csv('submission.csv')\n","495f9360":"# saving the model\nimport pickle\nfilename = 'rnn_model_v2.pkl'\n\nwith open(filename, \"wb\") as f:\n    pickle.dump((Whh, Whx, bh, by, Wyh ), f)\n\n\n","e8a9d17d":"The testing accuracy arrives at 76% (better than random guessing). I have not experimented with any hyperparameters or high level data-cleaning. Still, it seems that our RNN is learning association between words that helps it to classify the tweets.\n\nWe see there is a high bias. We may try \n1. different network architectures \n2. train for longer iteration","5ca9946e":"The approach can broadly classified in following step:\n1. Creating a dictionary of words and indexing to be later used in encoding each words into a vector using 1-of-k encoding \n2. Initialize the RNN model parameters\n3. Feedforward the training tweet (vectorized form) into the network and calculate loss for that training example\n4. Backpropagate through time and obtain the gradient of the parameters \n5. Clip the gradients to avoid exploding gradient problem\n6. Choose a learning rate and calculate the new model parameters\n7. Repeat 3-6 for some number of iterations for all the training examples","4e205e81":"The idea is to create a minimal RNN in Python\/numpy that will provide a baseline model for more complex algorithms,to gain a low level understanding of the working of RNN.\n\nThis kernel was inspired by\n1. Andrej Karpathy https:\/\/gist.github.com\/karpathy\/d4dee566867f8291f086: Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python\/numpy. And the blog http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/.\n2. The deep learning book by Michael Nielsen particularly  http:\/\/neuralnetworksanddeeplearning.com\/chap6.html\n3. Andrew ng Deep learning course (Course 5) on Coursera\n"}}