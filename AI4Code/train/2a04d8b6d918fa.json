{"cell_type":{"bf64dd63":"code","b624d774":"code","c5a2859d":"code","49a03125":"code","2ebe9539":"code","d9c0f6c4":"code","9846e690":"code","846fbb5d":"code","859d25a1":"code","db3f58d6":"code","a530fc79":"markdown","386be0e1":"markdown","11c13d84":"markdown","7e2dd9b2":"markdown","f951f682":"markdown","f61ccd6e":"markdown","71b4ad17":"markdown"},"source":{"bf64dd63":"#B\u00e1sicos\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport itertools\nimport datetime\n\nfrom textblob import TextBlob\n\n# NLTK\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport unidecode\nimport string\n\nfrom nltk.probability import FreqDist","b624d774":"train_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df = train_df[train_df['text'].notna()]\ntrain_df = train_df.reset_index()\ntrain_df.head(10)","c5a2859d":"train_df.info()","49a03125":"stop_words = set(stopwords.words('english'))\n\nappos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"i would\",\n\"i'd\" : \"i had\",\n\"i'll\" : \"i will\",\n\"i'm\" : \"i am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"i have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}","2ebe9539":"def text_preprocess(text):\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    \n    text = str(text)\n    \n    #removing mentions and hashtags\n\n    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", text).split())\n    \n    #remove http links from tweets\n    \n    \n    link_regex    = re.compile('((https?):((\/\/)|(\\\\\\\\))+([\\w\\d:#@%\/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], '')  \n    \n    text_pattern = re.sub(\"`\", \"'\", text)\n    \n    #fix misspelled words\n    #Para esto solo se comprueba que no hayan palabras con dos letras seguidas iguales.'\n\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    \n    \n   # print(text_pattern)\n    \n    #Convert to lower and negation handling\n    \n    text_lr = text_pattern.lower()\n    \n   # print(text_lr)\n    \n    words = text_lr.split()\n    text_neg = [appos[word] if word in appos else word for word in words]\n    text_neg = \" \".join(text_neg) \n   # print(text_neg)\n    \n    #remove stopwords\n    tokens = word_tokenize(text_neg)\n    text_nsw = [i for i in tokens if i not in stop_words]\n    text_nsw = \" \".join(text_nsw) \n   # print(text_nsw)\n    \n    \n    #remove tags\n    text_tags=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text_nsw)\n\n    # remove special characters and digits\n    text_alpha=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_tags)\n    \n    #Remove accented characters\n    text = unidecode.unidecode(text_alpha)\n    \n    '''#Remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = [w.translate(table) for w in text.split()]'''\n    \n    sent = TextBlob(text)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n   \n    return \" \".join(lemmatized_list)","d9c0f6c4":"train_df['processed_text'] = None\n\nfor i in range(len(train_df)):\n    train_df.processed_text[i] = text_preprocess(train_df.text[i])","9846e690":"train_df.head(10)","846fbb5d":"train_df.tail(10)","859d25a1":"import matplotlib.pyplot as plt\nax = train_df['sentiment'].value_counts(sort=False).plot(kind='barh')\nax.set_xlabel('N\u00famero de muestras')\nax.set_ylabel('Etiqueta')","db3f58d6":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Polarity ==  negative\ntrain_s0 = train_df[train_df.sentiment == 'negative']\nall_text = ' '.join(word for word in train_s0.processed_text)\nwordcloud_neg = WordCloud(colormap='Reds', width=1000, height=1000, background_color='white').generate(all_text) #mode='RGBA'\nplt.figure(figsize=(20,10))\nplt.title('Negative sentiment - Wordcloud')\nplt.imshow(wordcloud_neg, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neg.to_file('negative_senti_wordcloud.jpg')\n\n# Polarity ==  neutral\ntrain_s1 = train_df[train_df.sentiment == 'neutral']\nall_text = ' '.join(word for word in train_s1.processed_text)\nwordcloud_neu = WordCloud(width=1000, height=1000, colormap='Blues', background_color='white').generate(all_text)\nplt.figure( figsize=(20,10))\nplt.title('Neutral sentiment - Wordcloud')\nplt.imshow(wordcloud_neu, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neu.to_file('neutral_senti_wordcloud.jpg')\n\n# Polarity ==  positive\ntrain_s2 = train_df[train_df.sentiment  == 'positive']\nall_text = ' '.join(word for word in train_s2.processed_text)\nwordcloud_pos = WordCloud(width=1000, height=1000, colormap='Wistia',background_color='white').generate(all_text)\nplt.figure(figsize=(20,10))\nplt.title('Positive sentiment - Wordcloud')\nplt.imshow(wordcloud_pos, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_pos.to_file('positive_senti_wordcloud.jpg')","a530fc79":"2. Cargamos los datos","386be0e1":"4. Funci\u00f3n principal de pre-procesado de datos","11c13d84":"5. Vemos si el DataSet esta debalanceado","7e2dd9b2":"1. Preparaci\u00f3n del Entorno","f951f682":"# ******* Grupo Laura-Borja-Nehomar *******","f61ccd6e":"6. Wordcloud","71b4ad17":"3. StopWords"}}