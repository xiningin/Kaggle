{"cell_type":{"2804ffbe":"code","f76e56b4":"code","320623df":"code","180c78ec":"code","970e16c3":"code","df578d8c":"code","d1f2ce96":"code","8a9a193c":"code","9fc00f65":"code","53210c2a":"code","e346cefd":"code","eca95969":"code","8a935391":"code","c8e11729":"code","21cf3a4a":"code","795d58ee":"code","d4983a7f":"code","4816abdb":"code","63010d3e":"code","78595899":"code","7b8746d6":"code","66271568":"code","86920af3":"code","07a90db5":"code","c0b8f5a8":"code","91365776":"code","33efed2d":"code","7f68c334":"code","27cc6b10":"code","d31771a1":"code","7aa7064e":"code","549f345c":"code","c6626185":"code","701a062d":"code","a211b73e":"code","3139cbb5":"code","8d8cd495":"code","e8d6c1f5":"code","67618416":"code","435f3981":"code","641cecc6":"code","8a6a6fd4":"code","cc6a70d8":"code","a309c58a":"code","7c7f683f":"code","0853c5d3":"code","40103ac2":"code","d9bb818f":"code","0074c0a3":"code","62a22acf":"code","b276581b":"code","436d6a84":"code","3b731a8d":"code","db2f64ea":"code","16faae6a":"code","f0a2b9a1":"code","34a62548":"code","1c2a6b38":"code","b7cc5195":"code","b987cf18":"code","1e29c20c":"markdown","92206ea0":"markdown","9e2b727e":"markdown","4730b7f9":"markdown","780c1245":"markdown","d65710b2":"markdown","cc72529a":"markdown","f729ab51":"markdown","df1de474":"markdown","afbc950a":"markdown","496fc2aa":"markdown","70620803":"markdown","5955f57d":"markdown","837d088c":"markdown","a368b9b2":"markdown","6a34d663":"markdown","0284b4c1":"markdown","a93c5611":"markdown"},"source":{"2804ffbe":"import pandas as pd # collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nimport numpy as np  # foundational package for scientific computing\nimport re           # Regular expression operations\nimport matplotlib.pyplot as plt # Collection of functions for scientific and publication-ready visualization\n%matplotlib inline\nimport plotly.offline as py     # Open source library for composing, editing, and sharing interactive data visualization \nfrom matplotlib import pyplot as pp\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport plotly.tools as tls\nimport seaborn as sns  # Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n\nimport random \nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\nfrom sklearn.cluster import KMeans \nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\nnp.random.seed(123)\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","f76e56b4":"# Import dataset\ndf = pd.read_csv('..\/input\/AH-Data - Data.csv')","320623df":"df.drop({'Timestamp', 'EMAIL'}, axis=1, inplace=True) \ndf.head(2)","180c78ec":"df6 = df.pivot_table(values='TIME_FOR_PASSION', index=['AGE'], columns=['GENDER'], )\ndf6.head()","970e16c3":"colomn = df.columns\ncolomn","df578d8c":"# Convert categorical features into numerical \ndf['GENDER'] = df['GENDER'].map( {'Female': 0, 'Male': 1} ).astype(int)\ndf['AGE'] = df['AGE'].map( {'Less than 20': 0, '21 to 35': 1, '36 to 50': 2, '51 or more': 3} ).astype(int)\ndfcopy= df.copy()","d1f2ce96":"plt.figure(figsize=(16,16))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap='coolwarm_r', linecolor='white', annot=True)","8a9a193c":"factor = 0.2\naverage = df['TIME_FOR_PASSION'].mean()\ndf.loc[df['TIME_FOR_PASSION']  <= average*(1-factor), 'TIME_FOR_PASSION']=0\ndf.loc[df['TIME_FOR_PASSION']  >  average*(1+factor), 'TIME_FOR_PASSION']=1\ndf.loc[(df['TIME_FOR_PASSION'] >  average*(1-factor)) &\n       (df['TIME_FOR_PASSION'] <= average*(1+factor)), 'TIME_FOR_PASSION']=None","9fc00f65":"average","53210c2a":"dfcopy.head()","e346cefd":"df.dropna(inplace=True)","eca95969":"# Studying first our ability to find Time for our Passions\ndf_feat = df.copy()\ndf_feat.drop('TIME_FOR_PASSION', axis=1, inplace=True) \ndf_feat.head(2)","8a935391":"# Split test from train data\nfrom sklearn.model_selection import train_test_split\nX = df_feat\ny = df['TIME_FOR_PASSION']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","c8e11729":"# Fit and predict\nknn= KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nacc_predictions = knn.predict(X_test)\nacc_knn = knn.score(X_test, y_test) # Return the mean accuracy\nacc_knn","21cf3a4a":"# Evaluate Model: classification and confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Classification Report \\n',classification_report(y_test, acc_predictions))\nprint('\\n Confusion Matrix')\ncm = pd.DataFrame(confusion_matrix(y_test, acc_predictions), ['Actual: 0', 'Actual: 1'], ['Predicted: 0', 'Predicted: 1'])\nprint(cm)","795d58ee":"# Fit and predict\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\nrfc_predictions = rfc.predict(X_test)\nacc_rfc = rfc.score(X_test, y_test) # Return the mean accuracy\nacc_rfc","d4983a7f":"# Evaluate Model: classification and confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Classification Report \\n',classification_report(y_test, rfc_predictions))\nprint('\\n Confusion Matrix')\ncm = pd.DataFrame(confusion_matrix(y_test, rfc_predictions), ['Actual: 0', 'Actual: 1'], ['Predicted: 0', 'Predicted: 1'])\nprint(cm)","4816abdb":"feature_importances = pd.DataFrame(rfc.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances[:5]","63010d3e":"# Plot most important features\nfeature_importances[:7].plot(kind='bar', color=('cornflowerblue'), figsize=(14,3), alpha=0.9)\nplt.tick_params(axis='x', rotation=0)\nplt.title('Feature Importance') # : {}'.format(Category_labels[j]))","78595899":"colomn","7b8746d6":"keyfeature = {'SLEEP_HOURS', 'DAILY_STRESS', 'ACHIEVEMENT', 'TIME_FOR_PASSION',  'SUPPORTING_OTHERS'}\nfactor = 0.1\n\nfor name in keyfeature:\n    df = dfcopy.copy()    \n    average = df[name].mean()\n    df.loc[df[name]  <= average*(1-factor), name]=0\n    df.loc[df[name]  >  average*(1+factor), name]=1\n    df.loc[(df[name] >  average*(1-factor)) &\n       (df[name] <= average*(1+factor)), name]=None\n\n    df.dropna(inplace=True)\n    df_feat = df.copy()\n    df_feat.drop(name, axis=1, inplace=True) \n    \n    # Split test from train data\n    X = df_feat\n    y = df[name]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n    rfc.fit(X_train, y_train)\n    rfc_predictions = rfc.predict(X_test)\n    acc_rfc = rfc.score(X_test, y_test) # Return the mean accuracy\n    \n    # Plot most important features\n    feature_importances = pd.DataFrame(rfc.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n    colors = tuple(np.where(feature_importances[:7]>(0.9*feature_importances[:7].max()), 'cornflowerblue', 'yellow'))\n    feature_importances[:7].plot(kind='bar', color=colors, figsize=(14,3), alpha=0.8)\n    plt.tick_params(axis='x', rotation=0)\n    plt.title('Most important features predicting {}'.format(name))","66271568":"df = dfcopy.copy()    \ndf[  (df['DAILY_SHOUTING']>1)\n   & (df['DAILY_MEDITATION']<3)\n   & (df['TIME_FOR_PASSION']<3)\n   & (df['LOST_VACATION']>2)]['AGE'].count()","86920af3":"df[  (df['DAILY_SHOUTING']>1)\n   & (df['DAILY_MEDITATION']<3)\n   & (df['TIME_FOR_PASSION']<3)\n   & (df['LOST_VACATION']>2)]['DAILY_STRESS'].mean()","07a90db5":"df = dfcopy.copy()    \ndf[  (df['PERSONAL_AWARDS']>3)\n   & (df['FLOW']>3)\n   & (df['SUPPORTING_OTHERS']>3)\n   & (df['TIME_FOR_PASSION']>3)\n   & (df['LIVE_VISION']>3)]['AGE'].count()","c0b8f5a8":"df[  (df['PERSONAL_AWARDS']>3)\n   & (df['FLOW']>3)\n   & (df['SUPPORTING_OTHERS']>3)\n   & (df['TIME_FOR_PASSION']>3)\n   & (df['LIVE_VISION']>3)]['ACHIEVEMENT'].mean()","91365776":"df['ACHIEVEMENT'].mean()","33efed2d":"df[  (df['PERSONAL_AWARDS']>3)\n   & (df['FLOW']>3)\n   & (df['SUPPORTING_OTHERS']>3)\n   & (df['TIME_FOR_PASSION']>3)\n   & (df['LIVE_VISION']>3)\n   & (df['GENDER']==0)]['AGE'].count()\/1116","7f68c334":"df[  (df['PERSONAL_AWARDS']>3)\n   & (df['FLOW']>3)\n   & (df['SUPPORTING_OTHERS']>3)\n   & (df['TIME_FOR_PASSION']>3)\n   & (df['LIVE_VISION']>3)\n   & ((df['AGE']==2) | (df['AGE']==3))]['AGE'].count()\/1116","27cc6b10":"df[((df['AGE']==2) | (df['AGE']==3))]['AGE'].count()\/10004","d31771a1":"df = dfcopy.copy()    ","7aa7064e":"Feature = 'DAILY_STRESS'\n\n# Save labels as string\nLabels = df[Feature]\n#Data = Data.drop(['rn', 'activity'], axis = 1)\nLabels_keys = Labels.unique().tolist()\nLabels = np.array(Labels)\nprint('Original labels: ' + str(Labels_keys))\n\n# Normalize dataset\nscaler = StandardScaler()\ndf = scaler.fit_transform(df)","549f345c":"# Find optimal k value\nks = range(1, 10)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(df)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","c6626185":"n_clust=2\nk_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\nk_means.fit(df)\nc_labels = k_means.labels_\ndf2 = pd.DataFrame({'clust_label': c_labels, 'orig_label': Labels.tolist()})\nct = pd.crosstab(df2['clust_label'], df2['orig_label'])\ny_clust = k_means.predict(df)\ndisplay(ct)","701a062d":"# Clustering evaluation\nlist = [k_means.inertia_,homogeneity_score(Labels, y_clust),\n                      completeness_score(Labels, y_clust), v_measure_score(Labels, y_clust),\n                      adjusted_rand_score(Labels, y_clust), adjusted_mutual_info_score(Labels, y_clust),\n                      silhouette_score(df, y_clust, metric='euclidean')]\nlist = [\"%.3f\" % member for member in list]\nscore = pd.DataFrame(list,\n                     {'inertia', 'homogeneity', 'completeness', 'v_measure', 'adjusted_rand_score',\n                      'adjusted_mutual_info', 'silhouette'}, {'Value'})\nscore","a211b73e":"#check for optimal number of features\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=123)\npca.fit(df)\nfeatures = range(pca.n_components_)\n\nplt.figure(figsize=(8,4))\nplt.bar(features[:15], pca.explained_variance_[:15], color='lightskyblue')\nplt.xlabel('PCA feature \\n (note: new components are a combination of the original features)')\nplt.ylabel('Variance')\nplt.xticks(features[:15])\nplt.show()","3139cbb5":"pca = PCA(n_components=2, random_state=123)\nglobal df_reduced\ndf_reduced = pca.fit_transform(df)\nprint('Shape of the new Data df: ' + str(df_reduced.shape))","8d8cd495":"# Re-clustering after PCA\nn_clust=2\nk_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\nk_means.fit(df_reduced)\nc_labels = k_means.labels_\ndf2 = pd.DataFrame({'clust_label': c_labels, 'orig_label': Labels.tolist()})\nct = pd.crosstab(df2['clust_label'], df2['orig_label'])\ny_clust = k_means.predict(df_reduced)\ndisplay(ct)","e8d6c1f5":"df = dfcopy.copy()    \ndf.head(2)","67618416":"# Save labels as string\n#Labels = df[Feature]\n#Data = Data.drop(['rn', 'activity'], axis = 1)\n#Labels_keys = {'test'}#Labels.unique().tolist()\n#Labels = np.array(Labels)\n#print('Original labels: ' + str(Labels_keys))\n\nn_clust=3\nk_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\nk_means.fit(df)\ndisplay(k_means.labels_)\n#df = pd.DataFrame({'clust_label': c_labels, 'orig_label': Labels.tolist()})\n#ct = pd.crosstab(df['clust_label'], df['orig_label'])\n#y_clust = k_means.predict(df)\n#display(ct)","435f3981":"import numpy as np\nunique, counts = np.unique(k_means.labels_, return_counts=True)\nCluster_size = np.asarray((unique, counts)).T\nCluster_size","641cecc6":"# Initializing KMeans\nkmeans = KMeans(n_clusters=n_clust)\n# Fitting with inputs\nkmeans = kmeans.fit(df)\n# Predicting the clusters\nlabels = kmeans.predict(df)\n# Getting the cluster centers\nC = kmeans.cluster_centers_\nprint(C)","8a6a6fd4":"Centers = pd.DataFrame(C)\nCenters","cc6a70d8":"column_dic = {0:'ACHIEVEMENT', 1:'AGE', 2:'BMI_RANGE', 3:'CORE_CIRCLE', 4:'DAILY_MEDITATION',\n                        5:'DAILY_SHOUTING', 6:'DAILY_STEPS', 7:'DAILY_STRESS', 8:'DONATION', 9:'FLOW',\n                        10:'FRUITS_VEGGIES', 11: 'GENDER', 12: 'LIVE_VISION', 13: 'LOST_VACATION',\n                        14:'PERSONAL_AWARDS', 15: 'PLACES_VISITED', 16: 'SLEEP_HOURS', 17: 'SOCIAL_NETWORK',\n                        18: 'SUFFICIENT_INCOME', 19: 'SUPPORTING_OTHERS', 20: 'TIME_FOR_PASSION',\n                        21: 'TODO_COMPLETED'}\nCenters.rename(columns=column_dic, inplace=True)\nselected_column = ['DAILY_STRESS', 'AGE', 'GENDER', 'DAILY_SHOUTING', 'DAILY_MEDITATION','TIME_FOR_PASSION', 'LOST_VACATION']\nCenters = Centers[selected_column]\nCenters","a309c58a":"Relative_Centers=Centers.copy()\nRelative_Centers.loc[1].idxmin(axis=0, skipna=True)","7c7f683f":"selected_column.index(Relative_Centers.loc[1].idxmin(axis=0, skipna=True))","0853c5d3":"Centers.iloc[1][selected_column.index(Relative_Centers.loc[1].idxmin(axis=0, skipna=True))]","40103ac2":"colomn = Centers.columns\ncolomn","d9bb818f":"for names in colomn:\n    average = Centers[names].mean()\n    for i in range(0, n_clust):\n        Relative_Centers[names][i]=Centers[names][i]\/average*100\nRelative_Centers","0074c0a3":"#Centers.loc[0].max()\nCenters.iloc[:,selected_column.index(Relative_Centers.loc[1].idxmax(axis=0, skipna=True))].max()","62a22acf":"colomn = Centers.columns\ncolomn","b276581b":"Centers.iloc[:,selected_column.index(Relative_Centers.loc[0].idxmax(axis=0, skipna=True))].max()","436d6a84":"# Visualize Cluster Centers (Centroids)\nRelative_Centers.plot(kind='barh', color=('darksalmon', 'cornflowerblue', 'brown', 'lightblue', 'seagreen', 'navy', 'tomato')\n             ,figsize=(20,10), alpha=0.7).tick_params(axis='x', rotation=0)\nplt.title('Cluster Center Description (Selected Features)', fontsize=20)\nplt.xlabel('Key Features (Relative Scale)', fontsize=16)\nplt.ylabel('Clusters', fontsize=16)\nplt.legend(ncol=3, fontsize=12, loc='lower right')\nfor j in range(0, n_clust):\n    averagemax = round(Centers.iloc[:,selected_column.index(Relative_Centers.loc[j].idxmax(axis=0, skipna=True))].max(), 2)\n    averagemin = round(Centers.iloc[j][selected_column.index(Relative_Centers.loc[j].idxmin(axis=0, skipna=True))], 2)\n    plt.annotate('Cluster '+str(j) + ': '\n                 + str(Cluster_size[j][1])+' respondents'+'\\n'\n                 + 'Strongest feature: ' + Relative_Centers.loc[j].idxmax(axis=0, skipna=True)\n                 + ' with a mean of ' + str(averagemax)+'\\n'\n                 + 'Weakest feature: ' + Relative_Centers.loc[j].idxmin(axis=0, skipna=True)\n                 + ' with a mean of ' + str(averagemin)\n                 , xy=(90, j+0.26), xytext=(95, j+0.29),\n             arrowprops=dict(facecolor='black'),fontsize=13)\nplt.rc('font', size=15)","3b731a8d":"fig = plt.figure(figsize=(16,6))\nsns.set(style=\"white\", color_codes=True)\nsns.stripplot(x=df['TIME_FOR_PASSION'], y=df['LOST_VACATION'],\n              hue=df['GENDER'],size=10, marker=\"D\", palette=\"Set2\", dodge=True, data=df, jitter=0.2, alpha=.1)\nsns.despine()","db2f64ea":"#Import libraries:\nfrom sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\nfrom sklearn import metrics   #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV   #Perforing grid search\nfrom sklearn.model_selection import cross_val_score\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ntrain = df.copy()\nIDcol = 'ID'","16faae6a":"target = 'TIME_FOR_PASSION'\n\ndef modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['TIME_FOR_PASSION'])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_val_score(alg, dtrain[predictors], dtrain['TIME_FOR_PASSION'],\n                                                    cv=cv_folds)#, scoring='roc_auc')\n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', figsize=(8,2), title='Predictors of TIME_FOR_PASSION\\nAccuracy : %.3g'\n                      % metrics.accuracy_score(dtrain['TIME_FOR_PASSION'].values, dtrain_predictions))\n        plt.ylabel('Feature Importance Score')\n        \n#Choose all predictors except target & IDcols\npredictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm0 = GradientBoostingClassifier(random_state=10)\nmodelfit(gbm0, train, predictors)","f0a2b9a1":"accuracies_data=np.arange(22.0)\nfig,axes = plt.subplots(nrows=round(round((len(df.columns)\/2), 0)), ncols=2, figsize=(16,40))\n\ndef modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[df.columns[k]])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_val_score(alg, dtrain[predictors], dtrain[df.columns[k]],\n                                                    cv=cv_folds)#, scoring='roc_auc')\n    # Feature Importance Graph\n    if k<11: c=0\n    else: c=1\n    feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Predictors for '+df.columns[k]+'\\nAccuracy : %.3g'\n                  % metrics.accuracy_score(dtrain[df.columns[k]].values, dtrain_predictions), \n                  ax=axes[k-c*11,c])\n    plt.ylabel('Feature Importance Score')\n    return metrics.accuracy_score(dtrain[df.columns[k]].values, dtrain_predictions)\n        \n#Choose all predictors except target & IDcols\n\nfor k in range(0,len(df.columns)):\n    target = df.columns[k]\n    predictors = [x for x in train.columns if x not in [target, IDcol]]\n    gbm0 = GradientBoostingClassifier(random_state=10)\n    accuracies_data[k]=modelfit(gbm0, train, predictors)\n    \nfig.tight_layout()","34a62548":"# Display results\naccuracies_data = accuracies_data.round(3) # Round all values to 3 digits\nnp.sort(accuracies_data, axis=0)\ny_pos = np.arange(len(df.columns))   \nplt.figure(figsize=(15,12))\nplt.barh(y_pos, accuracies_data, left = 0, align='center', alpha=0.5,\n         color=['green', 'grey', 'purple', 'orange', 'red'], tick_label= accuracies_data)\nplt.yticks(y_pos, df.columns, rotation='horizontal')\nplt.xticks(np.arange(0.35, 0.85, step=0.05))\n#plt.axes([0.2,0.1,0.9,0.9])\nplt.xlabel('Accuracy')\nfor i, v in enumerate(accuracies_data):\n    plt.text(1, i, str(v), color='black', bbox=dict(facecolor='white', alpha=0.5))\nplt.title('Classifier Accuracies to Predict Each Features')\nplt.show()","1c2a6b38":"accuracies_data","b7cc5195":"accuracy_outcome = pd.DataFrame(df.columns)# ['Features'])\naccuracies_data = accuracies_data.round(3) # Round all values to 3 digits\nnp.sort(accuracies_data, axis=0)\naccuracy_outcome['Values']=accuracies_data\naccuracy_outcome.set_index(accuracy_outcome.columns[0], inplace=True)\naccuracy_outcome.sort_values(by=['Values'], inplace=True)\naccuracy_outcome","b987cf18":"# Display results\nax=accuracy_outcome.plot(kind='barh', left = 0, align='center', alpha=0.7,\n         color=['gbymc'], figsize=(15,8))\nplt.yticks(y_pos, accuracy_outcome.index.values.tolist(), rotation='horizontal')\nplt.xticks(np.arange(0.35, 0.85, step=0.05))\n#plt.axes([0.2,0.1,0.9,0.9])\nplt.xlabel('Accuracy')\nplt.ylabel('Features')\n\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.01, i.get_y(), \\\n            str(round((i.get_width()), 3)), fontsize=11, color='k')\n\nplt.title('Accuracy of Classifier by Feature \\n i.e. Ability to Predict Feature Based on Other Features in Dataset')\nplt.show()","1e29c20c":"# 2. Pearson Correlation Heatmap\nThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product-moment correlation coefficient (PPMCC) correlation between features. Pearson is bivariate correlation, measuring the linear correlation between two features.\n\nThe Pearson correlation measures the linear association between continuous variables. In other words, this coefficient quantifies the degree to which a relationship between two variables can be described by a line.\n","92206ea0":"**Notes and observations about the RFC predictive model outcome**\n\n* How do the most important features differ from the Pearson correlation coefficient?\nCorrelations are more \"simplistic\"as they only explain **linear relationships** between variables. I.e. they provide little value in assessing slopes or nonlinear relationships. But more importantly, Pearson Correlations only explain patterns in your underlying data, not what your machine learning model actually learns. The standard mantra \u201ccorrelations do not imply causation\u201d applies.\n\n* In contrast, a classifier like Random Forest (RFC) consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. The measure based on which the optimal condition is chosen is called impurity.\nFor classification, it is typically either Gini impurity or for regression trees it is variance. Thus when training a tree, it can be computed how much each feature decreases the weighted impurity in a tree. For a forest, **the impurity decrease from each feature can be averaged and the features are ranked according to this measure.**\n\n* In simple words, feature importances from RFC are unexcelled in accuracy among current algorithms.\nThey simply give the best possible estimates of what variables are important in the classification.\n\n* Note: a relationship may appear to be causal through a higher correlation. But this may be the result of other   unobserved features that were not included in the survey. In the above example, features such as country or education levels.","9e2b727e":"## 3.2 DecisionTreeClassifier to predict Time_for_Passion","4730b7f9":"# 6. Light Gradient Boosting (LGBM) algorithm to predict the value of a selected feature\n\n## 6.1 Predicting TIME_FOR_PASSION","780c1245":"**Observations**\n* Inertia, or the within-cluster sum of squares criterion, is like a measure of how internally coherent clusters are.\n* The \"elbow\" point where the curve starts to flatten, in this case k=2","d65710b2":"# 3. Classification Algorithms","cc72529a":"# 5. Clustering","f729ab51":"## 3.3 DecisionTreeClassifier to predict key features","df1de474":"## 3.4 So what?\nThere are multiple ways to apply this predictive algorithm:\n\n### 1. Stress Risk Assessment\nWhile employers have a legal duty to protect employees from stress at work by doing a risk assessment and acting on it, the above stress predictor model can be used to assess the risk of high stress affecting an individual based on their lifestyle.\n\nFor instance, using the features DAILY_SHOUTING, DAILY_MEDITATION, TIME_FOR_PASSION, LOST_VACATION, we can single out 310 respondents with an average level of stress of nearly 4 on a scale of 1 to 5 (a lot of stress).\nThere are most certainly other stress factors affecting them, but statistically-speaking, this group would benefit greatly of working on these five factors to lower their daily stress levels.\n\nCalculations below.","afbc950a":"df6 = df.pivot_table(values='TIME_FOR_PASSION', index=['AGE'], columns=['GENDER'], )\ndf6.head()","496fc2aa":"## 3.1 K Nearest Neighbors (KNN)","70620803":"### 2. Best practices from high-achievers\n\nWe all want to achieve more and strengthen both our sense of accomplishment and self-esteem.\nThe above model gives clear guidance of the behaviors and lifestyle of high achievers:\n* Lot's of PERSONAL_AWARDS and recognitions\n* Scheduling their days and weeks around FLOW sessions\n* Helping and SUPPORTING_OTHERS\n* Carving out personal TIME_FOR_PASSION or successfully turning their passion into a source of income\n* Picturing their LIVE_VISION a couple of years ahead, for instance with tools like a Vision Board.\n\nSo, who are the high achievers? \n* 1,116 individuals with 6.5 average achievements (versus 4 for all)\n* 60% are women\n* 56% are 36 or older (versus 49% for the entire population)\n\nCalculations below.","5955f57d":"** Observations from the Pearson analysis:**\n\nThe strongest correlations are found between (in descending order):\n* Time for Passion with Flow (0.48), Achievement (0.38) and Supporting Others (0.32)\n* Supporting Others with Donation (0.4), Achievements (0.36), Core Circle (0.34), Personal Awards (0.33) and Social network (0.31)\n* Achievement with Flow (0.4), Personal Award (0.4), Time for Passion (0.38) and Supporting Others (0.36)\n* Supporting Others with Core Circle (0.34) and Personal Awards (0.33)\n* Todo completed with Achievement (0.31) and Flow (0.30)\n* Life Vision with Achievement (0.32) and Flow (0.3)","837d088c":"# Work-Life Balance survey, a Machine-Learning analysis of global best practices to re-balance our lives\n\n\n# Introduction\n### Dataset\nThe dataset analyzed in this kernel contains10,000+ responses to Authentic-Happiness.com global work-life survey.\nThis [online survey](http:\/\/www.authentic-happiness.com\/your-life-satisfaction-score) includes 23 questions about the way we design our lifestyle and achieve work-life balance.\n\n### Objectives\nThis is the continuation of the EDA described in the \"Work-Life Balance survey, an Exploratory Data Analysis of global best practices to re-balance our lives\".\nThe objective of this notebook is to apply ML model to:\n* Find out the predictors of our work-life balance and \n* Cluster respondents into logical groups with similar lifestyles\n\n### Table of content TBU\nThere are four main sections:\n1. Data extraction from Google Sheet and preparation\n2. Pearson Correlation Heatmap\n3. Classification Algorithms\n4. Clustering\n\n\n### In summary DRAFT\nThe key take-away for each of the five areas are the following:\n1. Our **BMI** is most influenced by a quality nutrition (fruits\/Vegetables) and physical activity (daily steps)\n2. Key influencers of our **stress level** are our ability to concentrate on our work during flow sessions, daily meditation and the sufficiency of our income.\n3. Those of us who **achieve the most remarkable things**, have also maximized our productivity (completing our daily todo list), focus on our activities (flow sessions) and have earnt multiple personal awards and recognition.\n4. Having a **core circle of family members and close friends** influences the amount of new places we visit (discovery), reduces our daily stress and improve our social connections outside of this core circle.\n5. We find **more time for passion** when we complete well our daily todo list (personal productivity), flow through the day and have obtained many personal awards and recognition.\n\n### Check other Kaggle notebooks from [Yvon Dalat](https:\/\/www.kaggle.com\/ydalat):\n* [Titanic, a step-by-step intro to Machine Learning](https:\/\/www.kaggle.com\/ydalat\/titanic-a-step-by-step-intro-to-machine-learning): **a practice run ar EDA and ML-classification**\n* [HappyDB, a step-by-step application of Natural Language Processing](https:\/\/www.kaggle.com\/ydalat\/happydb-what-100-000-happy-moments-are-telling-us): **find out what 100,000 happy moments are telling us**\n* [Work-Life Balance survey, an Exploratory Data Analysis of lifestyle best practices](https:\/\/www.kaggle.com\/ydalat\/work-life-balance-best-practices-eda): **key insights into the factors affecting our work-life balance**\n*  [Work-Life Balance survey, a Machine-Learning analysis of best practices to rebalance our lives](https:\/\/www.kaggle.com\/ydalat\/work-life-balance-predictors-and-clustering): **discover the strongest predictors of work-life balance**\n\n**Interested in more facts and data to balance your life, check the [360 Living guide](https:\/\/amzn.to\/2MFO6Iy) ![360 Living: Practical guidance for a balanced life](https:\/\/images-na.ssl-images-amazon.com\/images\/I\/61EhntLIyBL.jpg)**\n\n# 1. Data import and preparation\n","a368b9b2":"## 4.3. Principal Component Analysis (PCA)\nPrincipal Component Analysis is a dimension-reduction tool\nthat can be used to reduce a large set of variables to a small set\nthat still contains most of the information in the large set.","6a34d663":"## 6.2 Applying predictor to all features in dataset","0284b4c1":"## 4.2 Elbow Value","a93c5611":"# 4. Clustering\n## 4.1 Clustering labels from a selected feature"}}