{"cell_type":{"b2422ef4":"code","1eebaea6":"code","0b8342e8":"code","c334b02a":"code","a6a65db7":"code","a0dc5997":"code","3d99de4a":"code","a410ba59":"code","ccdc3b94":"code","f9dece6e":"code","2986bdb6":"code","64a54197":"code","7773cfca":"code","aa42a1b7":"code","4393f639":"code","ba765791":"code","9b41424e":"code","16ed9eb8":"code","a981ef91":"code","ba806586":"code","b76f9cb9":"code","bfb80463":"code","c4a7922b":"code","2759378a":"code","e181c62e":"code","b24f4f4f":"code","a8bca3c6":"code","523bde40":"code","59dad95f":"code","6d03d2ba":"code","38d14ee6":"markdown","ecdd525a":"markdown","4eee230b":"markdown","f0861a8a":"markdown","87a59fc8":"markdown","e944de11":"markdown","ab82d305":"markdown","4bef5577":"markdown","3313b412":"markdown","0d5fac01":"markdown","b1bcd5e6":"markdown","84c06032":"markdown","a893b420":"markdown","9da588eb":"markdown","186b1d0c":"markdown","ef31bba3":"markdown","8a5e8fcd":"markdown","135679dc":"markdown"},"source":{"b2422ef4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","1eebaea6":"train = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","0b8342e8":"train.shape, test.shape, sub.shape","c334b02a":"train.head()","a6a65db7":"train.drop('id',axis=1,inplace=True)  # dropping id from both train and test data\ntest.drop('id',axis=1,inplace=True)","a0dc5997":"print('train: ')\ntrain.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","3d99de4a":"print('test: ')\ntest.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","a410ba59":"plt.figure(figsize=(14,5))\ntarget_values = train['loss'].value_counts()\nsns.barplot(x=target_values.index, y=target_values.values,linewidth=1.5, facecolor=(1, 1, 1, 0),\n                 errcolor=\".2\", edgecolor=\".2\")\nplt.title(\"Target unique values\", fontdict={'fontsize':20})\nplt.show()","ccdc3b94":"# plot the boxplot of area distribution\nplt.figure(figsize=(14,5))\nsns.boxplot(train.loss,color = 'white',linewidth=2.5)\nplt.title('loss Distribution')\nplt.xlabel('loss')\nplt.show()","f9dece6e":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[:100][i]], shade = True, alpha = 0.9, linewidth = 1.5, facecolor=(1, 1, 1, 0), edgecolor=\".2\")\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","2986bdb6":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.boxplot(train[train.columns.tolist()[:100][i]], linewidth = 2.5,color = 'white')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","64a54197":"y = train['loss']\ntrain.drop('loss',axis=1,inplace=True)","7773cfca":"features = []\nfor feature in train.columns:\n    features.append(feature)\nprint(features)","aa42a1b7":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\ntrain[features] = mm.fit_transform(train[features])\ntest[features] = mm.transform(test[features])\nX = train","4393f639":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    \n    model = LGBMRegressor(**params,device = 'gpu', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","ba765791":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","9b41424e":"lgb_params = {'reg_alpha': 0.4972562469417825, 'reg_lambda': 0.3273637203281044, \n          'num_leaves': 50, 'learning_rate': 0.032108486615557354, \n          'max_depth': 40, 'n_estimators': 4060, \n          'min_child_weight': 0.0173353329222102,\n          'subsample': 0.9493343850444064, \n          'colsample_bytree': 0.5328221263825876, 'min_child_samples': 80,'device':'gpu'}","16ed9eb8":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.values[train_idx], y.values[train_idx]\n        x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","a981ef91":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","ba806586":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\",200,2000,100),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\",0.6,1,0.1),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\",0.6,1,0.1),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-3,0.1),\n        \"reg_alpha\": trial.suggest_int(\"reg_alpha\",1,50),\n        \"reg_lambda\": trial.suggest_int(\"reg_lambda\",5,100),\n        \"max_depth\": trial.suggest_int(\"max_depth\",5,20),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,20),\n    }\n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","b76f9cb9":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","bfb80463":"xgb_params = {'n_estimators': 800,\n 'learning_rate': 0.03007864183766922,\n 'subsample': 0.9,\n 'colsample_bytree': 0.6,\n 'eta': 0.0011496873778752918,\n 'reg_alpha': 11,\n 'reg_lambda': 62,\n 'max_depth': 5,\n 'min_child_weight': 11,'tree_method':'gpu_hist'}","c4a7922b":"xgb_model = cross_val(X, y, XGBRegressor, xgb_params)","2759378a":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.03 , 0.04),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.32 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    \n    \n    model = CatBoostRegressor(**params, random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","e181c62e":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","b24f4f4f":"cat_params = {'iterations': 1224,\n 'od_wait': 1243,\n 'learning_rate': 0.03632022350716054,\n 'reg_lambda': 0.3257139588327784,\n 'subsample': 0.9741256425198503,\n 'random_strength': 41.06792107841663,\n 'depth': 12,\n 'min_data_in_leaf': 27,\n 'leaf_estimation_iterations': 10,'task_type':'GPU'}","a8bca3c6":"cat_model = cross_val(X, y, CatBoostRegressor, cat_params)","523bde40":"cat = CatBoostRegressor(**cat_params)\nlgb = LGBMRegressor(**lgb_params)\nxgb = XGBRegressor(**xgb_params)","59dad95f":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n                ('xgb', xgb)\n            ],\n            weights = [0.15, 0.65]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits ","6d03d2ba":"sub['loss'] = lgb_model.predict(test)\nsub.to_csv(f'lgb.csv',index = False)\n\nsub['loss'] = xgb_model.predict(test)\nsub.to_csv(f'xgb.csv',index = False)\n\nsub['loss'] = cat_model.predict(test)\nsub.to_csv(f'cat.csv',index = False)\n\nsub['loss'] = predictions\nsub.to_csv(f'vote.csv',index = False)","38d14ee6":"Reference:[https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm](https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm)","ecdd525a":"Scaling the data!","4eee230b":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Preprocessing<\/center><\/h1>\n<\/div>","f0861a8a":"* these are the best params recovered from **Optuna**.","87a59fc8":"* these are the best params recovered from **Optuna**.","e944de11":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Importing Libraries<\/center><\/h1>\n<\/div>","ab82d305":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Model Building+Optuna<\/center><\/h1>\n<\/div>","4bef5577":"<div class=\"alert alert-warning\">\n<h4>If you like this notebook, please upvote it! \n     Thank you! :)<\/h4>\n<\/div>","3313b412":"* these are the best params recovered from **Optuna**.","0d5fac01":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>xgboost<\/center><\/h2>\n<\/div>","b1bcd5e6":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>catboost<\/center><\/h2>\n<\/div>","84c06032":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>Prediction and submission<\/center><\/h2>\n<\/div>","a893b420":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>lightgbm<\/center><\/h2>\n<\/div>","9da588eb":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Exploration<\/center><\/h1>\n<\/div>","186b1d0c":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Visualization<\/center><\/h1>\n<\/div>","ef31bba3":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>Final Voting<\/center><\/h2>\n<\/div>","8a5e8fcd":"### **Feature Distribution:**","135679dc":"### **Target Distribution:**"}}