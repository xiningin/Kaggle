{"cell_type":{"dea0efde":"code","5f6571bd":"code","f7a536f3":"code","8e7fe75b":"code","08aa4a26":"code","387c8287":"code","e974f13d":"code","d3ed3788":"code","ae775757":"code","726fa713":"code","19191a63":"code","aa77cd8f":"code","92d0df60":"code","13b71fad":"code","93619cec":"code","911d650c":"code","3638cac2":"code","536fa157":"code","aeb06061":"code","cde445f3":"code","1109ddc9":"code","05494d82":"code","22e3110a":"code","19ff4076":"code","0544b274":"code","77944335":"code","41e98443":"code","478f2088":"code","208487a9":"code","3546e7bb":"code","e4b59204":"code","7963449e":"code","555bacb0":"code","8110e99d":"code","5db9b7e0":"code","1795f07b":"code","6e083404":"code","53a03ead":"code","d854a815":"code","095ae804":"code","b3e4e7ba":"code","d3446e67":"code","973e8d0c":"code","37da140f":"code","60a6d79c":"code","ca9035ac":"code","0965f6ef":"code","fb8d0893":"code","27a42e2f":"code","865f52ce":"code","c9c93751":"code","9ed116db":"code","d743d276":"code","d5a2893e":"code","87eeff6d":"code","eaad4eb6":"code","52d7015c":"markdown","8cf8d11d":"markdown","1bde77d0":"markdown","74d23e4a":"markdown","3d6f9b5a":"markdown","efeb7f12":"markdown","3f4bb46d":"markdown","076cefb1":"markdown","7d97ba36":"markdown","b2f4486c":"markdown","ae865afc":"markdown","b0285a60":"markdown","d2ca8695":"markdown","c96ae9ba":"markdown","d5745559":"markdown","cb95fd9f":"markdown","c7732ebe":"markdown","fc9ff1f3":"markdown","4d91112a":"markdown"},"source":{"dea0efde":"from IPython.display import Image\nimport os\nImage(\"..\/input\/mypics\/IMG_20190104_103954-01.jpeg\", width = 500, height = 500)\n# print(os.listdir(\"..\/input\/hututu\/IMG_20190104_103954.jpg\"))","5f6571bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport json\nimport gc\nimport glob\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\n\nimport scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\nimport time\nimport datetime\nimport os\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\"))\n\n# Any results you write to the current directory are saved as output.","f7a536f3":"breeds_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\ncolors_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nstates_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')\n\ntrain_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\ntarget_df = train_df[['PetID','AdoptionSpeed']]\ndef label(ID,target_df=target_df):\n    return target_df['AdoptionSpeed'].loc[target_df['PetID']==ID].values[0]","8e7fe75b":"print(\"train:\",len(train_df))\nprint(\"test:\",len(test_df))","08aa4a26":"train_df.head(2)","387c8287":"train_df.info()","e974f13d":"from wordcloud import WordCloud\nfig, ax = plt.subplots(figsize = (12, 8))\ntext_cat = ' '.join(train_df['Description'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text_cat)\nplt.imshow(wordcloud)\nplt.title('Top words in description');\nplt.axis(\"off\");","d3ed3788":"train_df['AdoptionSpeed'].value_counts().sort_index().plot('barh', color='teal');\nplt.title('Adoption speed classes counts');","ae775757":"from sklearn.metrics import cohen_kappa_score\ndef kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","726fa713":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","19191a63":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'..\/input\/petfinder-adoption-prediction\/{mode}_sentiment\/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'..\/input\/petfinder-adoption-prediction\/{mode}_metadata\/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","aa77cd8f":"debug = False\ntrain_pet_ids = train_df.PetID.unique()\ntest_pet_ids = test_df.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","92d0df60":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","13b71fad":"train_sentiment_gr.head(2)","93619cec":"train_metadata_desc.head(2)","911d650c":"# Train merges:\ntrain_1 = train_df.copy()\ntrain_1 = train_1.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_1 = train_1.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_1 = train_1.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_1 = train_1.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_1 = test_df.copy()\ntest_1 = test_1.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_1 = test_1.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_1 = test_1.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_1 = test_1.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nprint(train_1.shape, test_1.shape)\nassert train_1.shape[0] == train_df.shape[0]\nassert test_1.shape[0] == test_df.shape[0]","3638cac2":"train_1.head(2)","536fa157":"sentiment_dict = {}\nfor filename in os.listdir('..\/input\/petfinder-adoption-prediction\/train_sentiment\/'):\n    with open('..\/input\/petfinder-adoption-prediction\/train_sentiment\/' + filename, 'r') as f:\n        sentiment = json.load(f);\n        max_emotion = max(sentiment['sentences'], key=lambda sen : sen['sentiment']['magnitude']*sen['sentiment']['score'])\n        min_emotion = min(sentiment['sentences'], key=lambda sen : sen['sentiment']['magnitude']*sen['sentiment']['score'])\n        pet_id = filename.split('.')[0]\n        sentiment_dict[pet_id] = {}\n        sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\n        sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n        sentiment_dict[pet_id]['language'] = sentiment['language']\n        sentiment_dict[pet_id]['max_score'] = max_emotion['sentiment']['score']\n        sentiment_dict[pet_id]['min_score'] = min_emotion['sentiment']['score']\n        sentiment_dict[pet_id]['max_magnitude'] = max_emotion['sentiment']['magnitude']\n        sentiment_dict[pet_id]['min_magnitude'] = min_emotion['sentiment']['magnitude']\n    \nfor filename in os.listdir('..\/input\/petfinder-adoption-prediction\/test_sentiment\/'):\n    with open('..\/input\/petfinder-adoption-prediction\/test_sentiment\/' + filename, 'r') as f:\n        sentiment = json.load(f);\n        max_emotion = max(sentiment['sentences'], key=lambda sen : sen['sentiment']['magnitude']*sen['sentiment']['score'])\n        min_emotion = min(sentiment['sentences'], key=lambda sen : sen['sentiment']['magnitude']*sen['sentiment']['score'])\n        pet_id = filename.split('.')[0]\n        sentiment_dict[pet_id] = {}\n        sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\n        sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n        sentiment_dict[pet_id]['language'] = sentiment['language']\n        sentiment_dict[pet_id]['max_score'] = max_emotion['sentiment']['score']\n        sentiment_dict[pet_id]['min_score'] = min_emotion['sentiment']['score']\n        sentiment_dict[pet_id]['max_magnitude'] = max_emotion['sentiment']['magnitude']\n        sentiment_dict[pet_id]['min_magnitude'] = min_emotion['sentiment']['magnitude']\n        \n\n    ","aeb06061":"train_1['language'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else None)\ntrain_1['magnitude'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntrain_1['score'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\ntrain_1['max_score'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['max_score'] if x in sentiment_dict else 0)\ntrain_1['min_score'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['min_score'] if x in sentiment_dict else 0)\ntrain_1['max_magnitude'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['max_magnitude'] if x in sentiment_dict else 0)\ntrain_1['min_magnitude'] = train_1['PetID'].apply(lambda x: sentiment_dict[x]['min_magnitude'] if x in sentiment_dict else 0)\n\ntest_1['language'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else None)\ntest_1['magnitude'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntest_1['score'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\ntest_1['max_score'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['max_score'] if x in sentiment_dict else 0)\ntest_1['min_score'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['min_score'] if x in sentiment_dict else 0)\ntest_1['max_magnitude'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['max_magnitude'] if x in sentiment_dict else 0)\ntest_1['min_magnitude'] = test_1['PetID'].apply(lambda x: sentiment_dict[x]['min_magnitude'] if x in sentiment_dict else 0)\n","cde445f3":"text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\nfor col in text_columns:\n    train_1[col] = train_1[col].fillna('None')\n    test_1[col] = test_1[col].fillna('None')\ntrain_1['Length_Description'] = train_1['Description'].map(len)\ntrain_1['Length_metadata_annots_top_desc'] = train_1['metadata_annots_top_desc'].map(len)\ntrain_1['Lengths_sentiment_entities'] = train_1['sentiment_entities'].map(len)\ntest_1['Length_Description'] = test_1['Description'].map(len)\ntest_1['Length_metadata_annots_top_desc'] = test_1['metadata_annots_top_desc'].map(len)\ntest_1['Lengths_sentiment_entities'] = test_1['sentiment_entities'].map(len)","1109ddc9":"#train\nn_components = 32\ntext_features = []\n\n# Generate text features:\nfor txtcol in text_columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {txtcol}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(train_1[txtcol].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(txtcol))\n    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\ntrain_2 = pd.concat([train_1, text_features], axis=1)\n\nfor txtcol in text_columns:\n    train_2.drop(txtcol, axis = 1, inplace = True) \n    \n    \n#test\nn_components = 32\ntext_features = []\n\n# Generate text features:\nfor txtcol in text_columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {txtcol}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(test_1[txtcol].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(txtcol))\n    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\ntest_2 = pd.concat([test_1, text_features], axis=1)\n\nfor txtcol in text_columns:\n    test_2.drop(txtcol, axis = 1, inplace = True) ","05494d82":"train_2.head(2)","22e3110a":"def Namelength(x):\n    x = str(x)\n    if 'No' in x.split() or 'None' in x.split():\n        return 0\n    else:\n        return len(x)","19ff4076":"train_2['Lengths_Name'] = train_2['Name'].map(Namelength)\ntrain_2.drop('Name', axis = 1, inplace = True)\ntest_2['Lengths_Name'] = test_2['Name'].map(Namelength)\ntest_2.drop('Name', axis = 1, inplace = True)","0544b274":"train_2.head(2)","77944335":"split_char = '\/'\nfrom PIL import Image\ntrain_df_ids = train_df[['PetID']]\ntest_df_ids = test_df[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var'],\n    'width': ['sum', 'mean', 'var'],\n    'height': ['sum', 'mean', 'var'],\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\n# agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)\ntrain_3 = train_2.merge(agg_train_imgs, how='left', on='PetID')\ntest_3 = test_2.merge(agg_test_imgs, how='left', on='PetID')","41e98443":"import cv2\nimport os\nfrom keras.applications.densenet import preprocess_input, DenseNet121","478f2088":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id, pic_num):\n    image = cv2.imread(f'{path}{pet_id}-{pic_num}.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image\n","208487a9":"img_size = 256\nbatch_size = 32","3546e7bb":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Flatten, Lambda, AveragePooling2D, MaxPooling2D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = MaxPooling2D(pool_size=4, strides=4)(x)\nx = AveragePooling2D(2)(x)\nout = Flatten()(x)\n\nm = Model(inp,out)","e4b59204":"m.summary()","7963449e":"def gen_image_feature(df, batch_size = 32, img_size = 256, mode = 'train',pic_num = '1'):\n    pet_ids = df['PetID'].values\n    n_batches = len(pet_ids) \/\/ batch_size + 1\n\n    features = {}\n    for b in tqdm(range(n_batches)):\n        start = b*batch_size\n        end = (b+1)*batch_size\n        batch_pets = pet_ids[start:end]\n        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n        for i,pet_id in enumerate(batch_pets):\n            try:\n                batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/{mode}_images\/\", pet_id, pic_num)\n            except:\n                pass\n        batch_preds = m.predict(batch_images)\n        for i,pet_id in enumerate(batch_pets):\n            features[pet_id] = batch_preds[i]\n    return features","555bacb0":"train_imagefeature1 = gen_image_feature(train_df, batch_size = 32, img_size = 256, mode = 'train',pic_num = '1')\ntrain_imagefeature2 = gen_image_feature(train_df, batch_size = 32, img_size = 256, mode = 'train',pic_num = '2')\ntrain_imagefeature3 = gen_image_feature(train_df, batch_size = 32, img_size = 256, mode = 'train',pic_num = '3')\ntest_imagefeature1 = gen_image_feature(test_df, batch_size = 32, img_size = 256, mode = 'test',pic_num = '1')\ntest_imagefeature2 = gen_image_feature(test_df, batch_size = 32, img_size = 256, mode = 'test',pic_num = '2')\ntest_imagefeature3 = gen_image_feature(test_df, batch_size = 32, img_size = 256, mode = 'test',pic_num = '3')","8110e99d":"train_feats1 = pd.DataFrame.from_dict(train_imagefeature1, orient='index')\ntrain_feats2 = pd.DataFrame.from_dict(train_imagefeature2, orient='index')\ntrain_feats3 = pd.DataFrame.from_dict(train_imagefeature3, orient='index')\ntrain_feats = pd.concat([train_feats1,train_feats2,train_feats3],axis = 1)\ntrain_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]\ntest_feats1 = pd.DataFrame.from_dict(test_imagefeature1, orient='index')\ntest_feats2 = pd.DataFrame.from_dict(test_imagefeature2, orient='index')\ntest_feats3 = pd.DataFrame.from_dict(test_imagefeature3, orient='index')\ntest_feats = pd.concat([test_feats1,test_feats2,test_feats3], axis = 1)\ntest_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","5db9b7e0":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=2017)\n\n# features_df = pd.concat([train_feats, test_feats], axis=0)\ntrain_features = train_feats[[f'pic_{i}' for i in range(256*3)]].values\n\nsvd_col = svd_.fit_transform(train_features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\ntrain_img_features = pd.concat([train_df['PetID'], svd_col], axis=1)\n\nn_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=2017)\n\n# features_df = pd.concat([train_feats, test_feats], axis=0)\ntest_features = test_feats[[f'pic_{i}' for i in range(256*3)]].values\n\nsvd_col = svd_.fit_transform(test_features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\ntest_img_features = pd.concat([test_df['PetID'], svd_col], axis=1)","1795f07b":"train_4 = train_3.merge(train_img_features, how='left', on='PetID')\ntest_4 = test_3.merge(test_img_features, how='left', on='PetID')","6e083404":"alldata = pd.concat([train_4, test_4], ignore_index=True, sort=False)","53a03ead":"#num to catergory\ncol_num2cat = [ 'Breed1', 'Breed2', 'Color1', 'Color2', 'Color3','State']\nalldata[col_num2cat] = alldata[col_num2cat].astype(str)","d854a815":"#category cols\ncat_cols = col_num2cat + ['language','RescuerID']\n#fill nan in language using mode of the same state group\nalldata['language'] = alldata.groupby('State')['language'].transform(lambda x: x.fillna(x.mode()[0]))\nalldata.fillna(0)","095ae804":"def cat2count(df,var):\n    count = df.groupby([var])['PetID'].count().reset_index()\n    count.columns = [var, var+'_COUNT']\n    return count","b3e4e7ba":"for var in cat_cols:\n    count = cat2count(alldata,var)\n    alldata = alldata.merge(count, how='left', on=var)","d3446e67":"objects = []\nfor i in alldata.columns:\n    if alldata[i].dtype == object:\n        objects.append(i)\n# features.update(features[objects].fillna('None'))\n# drop column PetID\nalldata.drop(objects, axis = 1, inplace = True)","973e8d0c":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in alldata.columns:\n    if alldata[i].dtype in numeric_dtypes:\n        numerics.append(i)\n# numerics\n# features.update(features[numerics].fillna(0))","37da140f":"from scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","60a6d79c":"skew_features = alldata[numerics].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\nskew_features","ca9035ac":"# for i in skew_index:\n#     alldata[i] = boxcox1p(alldata[i], boxcox_normmax(alldata[i] + 1))","0965f6ef":"X_train = alldata.loc[np.isfinite(alldata.AdoptionSpeed), :]\nX_test = alldata.loc[~np.isfinite(alldata.AdoptionSpeed), :]\nlabel = X_train['AdoptionSpeed']\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train_df.shape[0]\nassert X_test.shape[0] == test_df.shape[0]\n\n","fb8d0893":"import xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\n\nparam = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 15,\n          'max_depth': 10,\n          'learning_rate': 0.0075,\n          'bagging_fraction': 0.8,\n          'feature_fraction': 0.7,\n          'min_split_gain': 0.02,\n          'min_child_samples': 55,\n          'min_child_weight': 0.02,\n          'lambda_l2': 0.022,\n          'verbosity': 1,\n          'data_random_seed': 2017,\n#           'early_stop': 500,\n#           'verbose_eval': 5000,\n          'num_rounds': 1000000\n}","27a42e2f":"def run_lgb(params, X_train, X_test, random_state):\n    n_splits = 8\n    \n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0]))\n    feature_importance_df = pd.DataFrame()\n    \n    \n\n    for fold_,(train_idx, valid_idx) in enumerate(kf.split(X_train, X_train['AdoptionSpeed'].values)):\n        print(\"Fold {}\".format(fold_))\n        \n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n        \n        trn_data = lgb.Dataset(X_tr, label=y_tr)\n        val_data = lgb.Dataset(X_val, label=y_val)\n\n        \n        clf = lgb.train(param, trn_data,  valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n        \n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"Feature\"] = X_val.columns\n        fold_importance_df[\"importance\"] = clf.feature_importance()\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        valid_pred = clf.predict(X_val, num_iteration=clf.best_iteration)\n        test_pred = clf.predict(X_test, num_iteration=clf.best_iteration) \n\n        oof_train[valid_idx] = valid_pred\n        oof_test += test_pred\/n_splits\n\n        \n    return clf, oof_train, oof_test, feature_importance_df","865f52ce":"n_run = 7\nprediction = np.zeros((X_test.shape[0]))\ntrain_pred = np.zeros((X_train.shape[0]))\nfor i in range(n_run):\n    model, oof_train, oof_test, feature_importance_df = run_lgb(param, X_train, X_test, i*2019)\n    prediction += oof_test\/n_run\n    train_pred += oof_train\/n_run","c9c93751":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","9ed116db":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","d743d276":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(train_pred, coefficients)\nqwk = kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"train kappa = \", qwk)","d5a2893e":"train_predictions = optR.predict(train_pred, coefficients).astype(np.int8)\ntest_predictions = optR.predict(prediction, coefficients).astype(np.int8)","87eeff6d":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(X_train['AdoptionSpeed'].values, train_predictions)","eaad4eb6":"submission = pd.DataFrame({'PetID': test_df['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","52d7015c":"## Group extracted features by PetID: one PetID may have several images and many  entries extracted from the json files","8cf8d11d":"## Acknowledgements: Many functions and ideas are forked from these great kernels and many many kernels therein. Thanks for sharing\nhttps:\/\/www.kaggle.com\/artgor\/exploration-of-data-step-by-step\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/extract-image-features-from-pretrained-nn\n\nhttps:\/\/www.kaggle.com\/skooch\/petfinder-simple-lgbm-baseline","1bde77d0":"### OptimizeRounder from [OptimizedRounder() - Improved](https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved)","74d23e4a":"## LGBM Model","3d6f9b5a":"# PetFinder.my Adoption Prediction\nAnimal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos.\n\nIn this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization.\n\n## For HUTU","efeb7f12":"## Let's have a quick look at the  descriptions","3f4bb46d":"## Parse Data from Sentiment and Metadata json files\n\n## Image Metadata\nWe have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n\n## Sentiment Data\nWe have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.","076cefb1":"## split back train and test","7d97ba36":"## Use pretrained models to extract more feature","b2f4486c":"## Metric: Quadratic_weighted_kappa","ae865afc":"## A bit more information(maybe):\n\nI also attach the language and the overall score and magnitude plus its max and min to the dataframe.","b0285a60":"## Add image_size features","d2ca8695":"# Data Overview\nHere we use a pretrained model and transfer learning to try and identify the different types of proteins present in the image.\n\nrefer to https:\/\/www.kaggle.com\/artgor\/exploration-of-data-step-by-step","c96ae9ba":"## Combine train and test together to do category-to-num transformation and analysis","d5745559":"### Merge sentiment and metadata ","cb95fd9f":"### Treat \"Name\": there are different expressions such as \"None\" or \"No Name Yet\"  in the Name column all meaning that the pet has no name. I map such names to 0 and to length otherwise","c7732ebe":"## We use TfidfVectorizer function to transform the text columns into numerical vector.","fc9ff1f3":"## Target: Adoption speed\n\n* 0 - Pet was adopted on the same day as it was listed.\n* 1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\n* 2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\n* 3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n* 4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days). ","4d91112a":"## We use TruncatedSVD to reduce the dimentsion"}}