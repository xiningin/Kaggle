{"cell_type":{"daa0d9e2":"code","918c2214":"code","efebd96f":"code","7ca33e67":"code","3d21a5e3":"code","36f3f934":"code","dc81beca":"code","e0114c70":"code","d5064364":"code","eb61f4d8":"code","6ecefaac":"code","2ea6ed0e":"code","c874aa70":"code","ea09d997":"code","3a388cb8":"code","e78179b3":"code","7ec2f7c5":"code","aaf459c2":"code","03f55a9e":"code","98b09567":"code","a72030f5":"code","81108fba":"code","1abbdf24":"markdown","b5ac9946":"markdown","edb7f0d0":"markdown","585ea64f":"markdown","096f4fee":"markdown","b568bd12":"markdown","8df9a2e2":"markdown","f613212b":"markdown","5476af76":"markdown","ec59b800":"markdown","5f9ce2ce":"markdown","972a78a2":"markdown"},"source":{"daa0d9e2":"import pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, Conv1D, BatchNormalization, Dropout, multiply, MaxPooling1D, Flatten\nfrom keras.utils.vis_utils import plot_model\n\n# from sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import RepeatedKFold\n# from sklearn.datasets import make_regression\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error","918c2214":"train = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","efebd96f":"def measure_mse(actual, predicted):\n    return mean_squared_error(actual, predicted)","7ca33e67":"def series_to_supervised(data, n_in = 10, n_out=1):\n    df = pd.DataFrame(data)\n    cols = list()\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n\n    agg = pd.concat(cols, axis=1)\n\n    agg.dropna(inplace=True)\n    return agg.values","3d21a5e3":"def train_model_cnn_lstm(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,1))\n    layer_regr = Conv1D(filters=500, kernel_size=3, activation='relu', padding='same', kernel_initializer='truncated_normal')(layer_in)\n    \n    layer_class = LSTM(128, activation='softmax', kernel_initializer='truncated_normal')(layer_regr)\n    \n    layer_regr = LSTM(128, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = BatchNormalization()(layer_regr)\n    \n    layer_regr = multiply([layer_regr, layer_class])\n    \n    layer_regr = Dropout(0.3)(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.2)(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, batch_size=len(train_x), epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","36f3f934":"def train_model_lstm(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,1))\n    layer_regr = LSTM(128, activation='relu', kernel_initializer='truncated_normal', return_sequences=True)(layer_in)\n    layer_regr = BatchNormalization()(layer_regr)\n    layer_regr = Dropout(0.3)(layer_regr)\n    layer_regr = LSTM(n_nodes, activation='relu', kernel_initializer='truncated_normal', return_sequences=True)(layer_regr)\n    layer_regr = LSTM(128, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.3)(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.2)(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, batch_size=len(train_x), epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","dc81beca":"def train_model_cnn(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,1))\n    layer_regr = Conv1D(filters=250, kernel_size=3, activation='relu', padding='same', kernel_initializer='truncated_normal')(layer_in)\n    layer_regr = BatchNormalization()(layer_regr)\n    layer_regr = Conv1D(125, kernel_size=3, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = MaxPooling1D(pool_size=2)(layer_regr)\n    layer_regr = Flatten()(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.2)(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, batch_size=len(train_x), epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","e0114c70":"def train_model_cnn_filter(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,1))\n    layer_regr = Conv1D(filters=250, kernel_size=3, activation='relu', padding='same', kernel_initializer='truncated_normal')(layer_in)\n    layer_regr = BatchNormalization()(layer_regr)\n    layer_class = Conv1D(125, kernel_size=3, activation='softmax', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Conv1D(125, kernel_size=3, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = multiply([layer_regr, layer_class])\n    layer_regr = MaxPooling1D(pool_size=2)(layer_regr)\n    layer_regr = Flatten()(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.2)(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, batch_size=len(train_x), epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","d5064364":"def train_model_dense(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,))\n    layer_regr = Dense(64, activation='relu', kernel_initializer='truncated_normal')(layer_in)\n    layer_regr = BatchNormalization()(layer_regr)\n    layer_class = Dense(32, activation='softmax', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = multiply([layer_regr, layer_class])\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","eb61f4d8":"def train_model_dense_enc(train_x, train_y, n_nodes):\n    layer_in = Input(shape=(n_nodes,))\n    layer_regr = Dense(64, activation='relu', kernel_initializer='truncated_normal')(layer_in)\n    layer_regr = BatchNormalization()(layer_regr)\n    layer_regr = Dense(n_nodes, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n\n    model = Model(inputs=layer_in, outputs=layer_out)\n    model.compile(loss='mse', optimizer='adam')\n    \n    for i in range(300):\n        model.fit(train_x, train_y, epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","6ecefaac":"n_nodes = 21","2ea6ed0e":"train_set = train.iloc[1][6:-200].values\ntrain_set = series_to_supervised(train_set, n_in = n_nodes, n_out=1)\ntrain_x, train_y = train_set[:, :-1], train_set[:, -1]","c874aa70":"model_dense = train_model_dense(train_x, train_y, n_nodes)\nmodel_dense_enc = train_model_dense_enc(train_x, train_y, n_nodes)\n\ntrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n\nmodel_cnn_lstm = train_model_cnn_lstm(train_x, train_y, n_nodes)\nmodel_lstm = train_model_lstm(train_x, train_y, n_nodes)\nmodel_cnn = train_model_cnn(train_x, train_y, n_nodes)\nmodel_cnn_filter = train_model_cnn_filter(train_x, train_y, n_nodes)","ea09d997":"series = train.iloc[1][6:-200].values\nactual_set = series_to_supervised(series, n_in = (n_nodes-1), n_out=1)\n\npred_d = model_dense.predict(actual_set)\npred_d_enc = model_dense_enc.predict(actual_set)\n\nactual_set = actual_set.reshape((actual_set.shape[0], actual_set.shape[1], 1))\n\npred_cnn_lstm= model_cnn_lstm.predict(actual_set)\npred_lstm = model_lstm.predict(actual_set)\npred_cnn = model_cnn.predict(actual_set)\npred_cnn_f = model_cnn_filter.predict(actual_set)","3a388cb8":"test = train.iloc[1][-200:].values\ntest_set = series_to_supervised(test, n_in = (n_nodes-1), n_out=1)\n\ntest_d = model_dense.predict(test_set)\ntest_d_enc = model_dense_enc.predict(test_set)\n\ntest_set = test_set.reshape((test_set.shape[0], test_set.shape[1], 1))\n\ntest_cnn_lstm= model_cnn_lstm.predict(test_set)\ntest_lstm = model_lstm.predict(test_set)\ntest_cnn = model_cnn.predict(test_set)\ntest_cnn_f = model_cnn_filter.predict(test_set)\n\nprint('cnn + lstm score: ', measure_mse(test[20:], test_cnn_lstm))\nprint('lstm score: ', measure_mse(test[20:], test_lstm))\nprint('cnn score: ', measure_mse(test[20:], test_cnn))\nprint('cnn with filter cells score: ', measure_mse(test[20:], test_cnn_f))\nprint('dense with filter cells score: ', measure_mse(test[20:], test_d))\nprint('dens autoencoder score: ', measure_mse(test[20:], test_d_enc))","e78179b3":"series = train.iloc[1][6:].values\n\nfig, ax = plt.subplots(6, figsize=(15,30))\n\nax[0].plot( series, 'tab:red')\nax[0].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_cnn_lstm.reshape(-1,1), test_cnn_lstm.reshape(-1,1))), 'tab:green')\nax[0].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[0].set_title('CNN + LSTM plot')\nax[0].set(xlabel='days', ylabel='sales')\n\nax[1].plot( series, 'tab:red')\nax[1].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_lstm.reshape(-1,1), test_lstm.reshape(-1,1))), 'tab:green')\nax[1].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[1].set_title('LSTM plot')\nax[1].set(xlabel='days', ylabel='sales')\n\nax[2].plot( series, 'tab:red')\nax[2].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_cnn.reshape(-1,1), test_cnn.reshape(-1,1))), 'tab:green')\nax[2].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[2].set_title('CNN plot')\nax[2].set(xlabel='days', ylabel='sales')\n\nax[3].plot( series, 'tab:red')\nax[3].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_cnn_f.reshape(-1,1), test_cnn_f.reshape(-1,1))), 'tab:green')\nax[3].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[3].set_title('CNN with filter cells plot')\nax[3].set(xlabel='days', ylabel='sales')\n\nax[4].plot( series, 'tab:red')\nax[4].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_d.reshape(-1,1), test_d.reshape(-1,1))), 'tab:green')\nax[4].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[4].set_title('Dense with filter cells plot')\nax[4].set(xlabel='days', ylabel='sales')\n\nax[5].plot( series, 'tab:red')\nax[5].plot( np.vstack((np.array([0 for x in range(n_nodes)]).reshape(-1,1),pred_d_enc.reshape(-1,1), test_d_enc.reshape(-1,1))), 'tab:green')\nax[5].axvspan(len(series)-200, len(series), color='red', alpha=0.2)\nax[5].set_title('Dense encoder - decoder plot')\nax[5].set(xlabel='days', ylabel='sales')\n\nfig.show()","7ec2f7c5":"print('Dense model with filter cells')\nplot_model(model_dense, show_shapes=True, show_layer_names=True)","aaf459c2":"print('Dense model autoencoder')\nplot_model(model_dense_enc, show_shapes=True, show_layer_names=True)","03f55a9e":"print('CNN + LSTM model')\nplot_model(model_cnn_lstm, show_shapes=True, show_layer_names=True)","98b09567":"print('CNN filter model')\nplot_model(model_cnn_filter, show_shapes=True, show_layer_names=True)","a72030f5":"print('CNN model')\nplot_model(model_cnn, show_shapes=True, show_layer_names=True)","81108fba":"print('LSTM model')\nplot_model(model_lstm, show_shapes=True, show_layer_names=True)","1abbdf24":"# CNN model with filter cells","b5ac9946":"<h1 style='color: red'> Please upvote this notebook if you found it helpful <\/h1>","edb7f0d0":"# CNN + LSTM model","585ea64f":"# Vanilla LSTM","096f4fee":"# Vanilla CNN","b568bd12":"# Dense model with filter cells","8df9a2e2":"# Models","f613212b":"<div style=\"text-align:center\">\n<hr>\n    <h1> Content of this notebook<\/h1>\n    <h3>1. Models to be evaluated<\/h3>\n    <h3>2. Plots and scores<\/h3>\n    <h3>3. Model diagrams<\/h3>\n<hr>\n<\/div>","5476af76":"# Encoder -  Decoder Dense model","ec59b800":"<h1 style='text-align:center'>Neural Networks Models<\/h1>","5f9ce2ce":"Top models in the leadboard right now ( 7 April 2020 )\nout of 15 models\n1. LightGBM - 12 models ( 80 % )\n2. NN\n    * Dense (MLP) - 2 models( 13.33 % )\n3. Catboost - 1 model ( 6.66 % )","972a78a2":"<div style=\"text-align:center\">\n<hr>\n    <h1> Types of Models for evaluation<\/h1>\n    <h3>1. CNN + LSTM model with filter cells<\/h3>\n    <h3>2. Vanilla LSTM model<\/h3>\n    <h3>3. Vanilla CNN model<\/h3>\n    <h3>4. CNN model with filter cells<\/h3>\n    <h3>5. Dense model with filter cells<\/h3>\n    <h3>6. Encoder - Decoder Dense Model<\/h3>\n<hr>\n<\/div>"}}