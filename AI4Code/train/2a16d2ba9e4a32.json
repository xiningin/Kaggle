{"cell_type":{"a2e60fa9":"code","09e308e6":"code","ff5d0c05":"code","1be9a91c":"code","9bf721d0":"code","7e955fa6":"code","f87c081f":"code","475a6cb2":"code","a8ef3258":"code","1023529e":"code","8310d865":"code","a49784c3":"code","51e292ab":"code","bd4c8dd7":"code","dca5810e":"code","e1244c1e":"code","a5f78476":"code","c1a33544":"code","5f5d73ae":"code","8a2aa965":"code","62776ab6":"code","5f756023":"code","8b389dc4":"code","c3f10425":"code","cfdd70be":"code","75095a6c":"code","2d7caa66":"code","5049793a":"code","959a9312":"code","e2393e93":"code","de145652":"code","a019175c":"code","da0e12e4":"code","ef328dc2":"code","61d95391":"code","f789bf50":"code","1bacb2e0":"code","a6df9022":"code","4ccf147b":"code","7d905fb8":"code","99a6b327":"markdown","5d8727f3":"markdown","7f7e33e2":"markdown","b4bb2826":"markdown","ed9813ae":"markdown","198d60dd":"markdown","d64d77dc":"markdown","3a320cbb":"markdown","68c9f92c":"markdown","13c0849e":"markdown","45a1452c":"markdown","d4ceb053":"markdown","fbb842da":"markdown","5608b00b":"markdown","6b5b4c63":"markdown","bf1c8ea0":"markdown","da04eefd":"markdown","25ec44ce":"markdown","7b66363f":"markdown","b446547f":"markdown","8d341be3":"markdown","0dcfb7f5":"markdown","8c5ef603":"markdown","2597feb1":"markdown","3be4057f":"markdown","aeb2af35":"markdown","58600878":"markdown","5534db1a":"markdown","fe1b7fc5":"markdown","91e971b4":"markdown","b35a73f3":"markdown","be0a5151":"markdown"},"source":{"a2e60fa9":"!pip install -q textstat","09e308e6":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm_notebook().pandas()\n\nfrom matplotlib_venn import venn2, venn3, venn2_circles, venn3_circles \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly.offline as plty\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom scipy import stats\nimport textstat\n\nfrom nltk import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nfrom wordcloud import WordCloud, STOPWORDS\n","ff5d0c05":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1be9a91c":"PATH = '\/kaggle\/input\/google-quest-challenge\/'\n\ntrain_df = pd.read_csv(f'{PATH}train.csv')\ntest_df = pd.read_csv(f'{PATH}test.csv')","9bf721d0":"def DataDesc(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n    \n    return summary","7e955fa6":"DataDesc(train_df)[:10]","f87c081f":"DataDesc(test_df)[:10]","475a6cb2":"sub_df = pd.read_csv(f'{PATH}sample_submission.csv')\ntarget_cols = train_df[train_df.columns[train_df.columns.isin(sub_df.columns[1:])]].columns","a8ef3258":"target_cols","1023529e":"train_df.host[:5]","8310d865":"train_df.category.value_counts()","a49784c3":"train_df['host_cat'] = train_df['host'].apply(lambda x : x.split('.')[0])\ntrain_df.drop(['host'], axis=1, inplace=True)","51e292ab":"host = train_df.groupby(['host_cat'])['url'].nunique().sort_values(ascending=False)\ncategory = train_df.groupby(['category'])['url'].nunique().sort_values(ascending=False)\n\nplt.figure(figsize=(16,12))\nplt.suptitle('Unique URL by Host and Categories', size=22)\n\nplt.subplot(211)\ng0 = sns.barplot(x=category.index, y=category.values, color='blue')\ng0.set_title(\"Unique Answers by category\", fontsize=22)\ng0.set_xlabel(\"Category Name\", fontsize=19)\ng0.set_ylabel(\"Total Count\", fontsize=19)\n#g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g0.patches:\n    height = p.get_height()\n    g0.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(height\/category.sum()*100),\n            ha=\"center\",fontsize=11) \n\nplt.subplot(212)\ng1 = sns.barplot(x=host[:20].index, y=host[:20].values, color='blue')\ng1.set_title(\"TOP 20 HOSTS with more UNIQUE questions\", fontsize=22)\ng1.set_xlabel(\"Host Name\", fontsize=19)\ng1.set_ylabel(\"Total Count\", fontsize=19)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(height\/host.sum()*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.3, top = 0.90)\n\nplt.show()","bd4c8dd7":"plt.figure(figsize=(12,8))\n\nvenn2([set(train_df['question_user_name'].values), \n       set(train_df['answer_user_name'].values)],\n      set_labels=('Question Users', 'Answer Users'), alpha=.5)\n\nplt.title('Comparison of Question and Answer Users Intersection\\n', fontsize=20)\n\nplt.show()","dca5810e":"import matplotlib.gridspec as gridspec # to do the grid of plots\n\ngrid = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(16,3*4))\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different CATEGORIES', size=20)\n\nfor n, col in enumerate(set(train_df['category'].values)):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.category == col]['question_user_name'].values), \n           set(train_df[train_df.category == col]['answer_user_name'].values)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","e1244c1e":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,5*4))\n\ntop_cat = train_df['host_cat'].value_counts(ascending=False).index[:15]\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different Host Categories', size=20)\n\nfor n, col in enumerate(top_cat):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.host_cat == col]['question_user_name'].values), \n           set(train_df[train_df.host_cat == col]['answer_user_name'].values)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","a5f78476":"question_body_tokens = [word_tokenize(question) for question in train_df.question_body.values]\n\nlen_words = []\n\nfor i in range(len(question_body_tokens)):\n    len_words.append(len(question_body_tokens[i]))\n    \n# Create a new feature for the lengh of each review\ntrain_df['question_n_words'] = len_words","c1a33544":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Title and Question Lenghts by Different Categories \\nThe Mean in RED - Also 5% and 95% lines', size=20)\ncount=0\ntop_cats=train_df['category'].value_counts().index\nfor n, col in enumerate(top_cats):\n    for i, q_t in enumerate(['question_title', 'question_body', 'question_n_words']):\n        ax = plt.subplot(grid[count])\n        if q_t == 'question_n_words':\n            sns.distplot(train_df[train_df['category'] == col][q_t], bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\nQuestion #Total Words Distribution\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].mean()\n            ax.axvline(mean_val, color='red' )\n            ax.set_xlabel('')            \n        else:\n            sns.distplot(train_df[train_df['category'] == col][q_t].str.len(), bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\n{str(q_t)}\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].str.len().mean()\n            ax.axvline(mean_val, color='red' )\n            #ax.text(x=mean_val*1.1, y=.02, s='Holiday in US', alpha=0.7, color='#334f8d')\n            ax.set_xlabel('')\n        count+=1\n        \nplt.subplots_adjust(top = 0.90, hspace=.4, wspace=.15)\nplt.show()","5f5d73ae":"grid = gridspec.GridSpec(10, 3)\n\nplt.figure(figsize=(16,8*4))\ncount=0\nplt.suptitle('Distribution of QA metrics (Target Features)', size=20)\n# top_host = df_train['host_cat'].value_counts()[:15].index\nfor n, col in enumerate(target_cols):\n    #if df_train[target_cols].std()[col] > .15:\n    ax = plt.subplot(grid[count])\n    sns.boxplot(x='category', y=col, data=train_df)\n    ax.set_title(str(col), fontsize=13)\n    ax.set_xlabel('')\n    ax.set_ylabel(' ')\n    count+=1\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.subplots_adjust(top = 0.95, hspace=.9, wspace=.2)\n\nplt.show()","8a2aa965":"plt.figure(figsize=(16,10))\nsns.heatmap(train_df[target_cols].corr(),vmin=-1,cmap='YlGnBu')","62776ab6":"stopwords = set(STOPWORDS)\nnewStopWords = ['amp', 'gt', 'lt', 'div', 'id',\n                'fi', 'will', 'use', 'one', 'nbsp', 'need']\nstopwords.update(newStopWords)\n\ngrid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Word Cloud OF CATEGORY FEATURE', size=20)\n\nfor n, col in enumerate(train_df['category'].value_counts().index):\n    ax = plt.subplot(grid[n])  \n    \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['category'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Category: {col}\",fontsize=18)\n    plt.axis('off')\nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()","5f756023":"grid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Answers Word Cloud \\nTOP 10 hosts with more questions', size=20)\n\nfor n, col in enumerate(train_df['host_cat'].value_counts()[:10].index):\n    ax = plt.subplot(grid[n])   \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['host_cat'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Host: {col}\",fontsize=18)\n    plt.axis('off')\n    \nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()","8b389dc4":"def polarity(text):\n    if type(text) == str:\n        return SIA.polarity_scores(text)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\ntrain_df[\"ans_polarity\"] = train_df[\"answer\"].progress_apply(polarity)","c3f10425":"# Calculate mean values of Sentiments\nneg_vals, pos_vals, neutral_vals = [], [], []\n\nfor x_dict in train_df[\"ans_polarity\"].values:\n    neg_vals.append(x_dict['neg'])\n    pos_vals.append(x_dict['pos'])\n    neutral_vals.append(x_dict['neu'])\n\nneg_vals = np.asarray(neg_vals)\npos_vals = np.asarray(pos_vals)\nneutral_vals = np.asarray(neutral_vals)\n\nmean_neg = neg_vals.mean()\nmean_pos = pos_vals.mean()\nmean_neu = neutral_vals.mean()","cfdd70be":"neg_pol = [pols['neg'] for pols in train_df[\"ans_polarity\"] if type(pols) is dict]\nneg_pol = list(filter((0.0).__ne__, neg_pol))\n\n\n\ntrace = go.Histogram(x=neg_pol, marker=dict(\n            color='lightseagreen')\n    )\ndata = [trace]\n\nlayout = go.Layout(\n\n    shapes= [{'line': {'color': '#FF0000', 'dash': 'solid', 'width': 1},\n        'type': 'line',\n        'x0': mean_neg,\n        'x1': mean_neg,\n        'xref': 'x',\n        'y0': -0.1,\n        'y1': 1,\n        'yref': 'paper'}],\n\n    # Annotations\n    annotations=[dict(x=mean_neg,\n                y=1,\n                xref='x',\n                yref='paper',\n                text=f\"Mean a = {np.round(mean_neg,3)}\",\n                showarrow=True,\n                arrowhead=7,\n                ax=1,\n                ay=1,\n            )]\n)\nfig = go.Figure(data=data, layout=layout)\n\n\n\nfig.update_layout(xaxis_title=\"Negativity sentiment\", \n                  title_text=\"Negativity sentiment\", template=\"simple_white\")\nfig.show()","75095a6c":"pos_pol = [pols['pos'] for pols in train_df[\"ans_polarity\"] if type(pols) is dict]\npos_pol = list(filter((0.0).__ne__, pos_pol))\n\n\n\ntrace = go.Histogram(x=pos_pol, marker=dict(\n            color='lightsalmon')\n    )\ndata = [trace]\n\nlayout = go.Layout(\n\n    shapes= [{'line': {'color': 'seagreen', 'dash': 'solid', 'width': 1},\n        'type': 'line',\n        'x0': mean_pos,\n        'x1': mean_pos,\n        'xref': 'x',\n        'y0': -0.1,\n        'y1': 1,\n        'yref': 'paper'}],\n\n    # Annotations\n    annotations=[dict(x=mean_pos,\n                y=1,\n                xref='x',\n                yref='paper',\n                text=f\"Mean a = {np.round(mean_pos,3)}\",\n                showarrow=True,\n                arrowhead=7,\n                ax=1,\n                ay=1,\n            )]\n)\nfig = go.Figure(data=data, layout=layout)\n\n\n\nfig.update_layout(xaxis_title=\"Positivity sentiment\", \n                  title_text=\"Positivity sentiment\", template=\"simple_white\")\nfig.show()","2d7caa66":"neu_pol = [pols['neu'] for pols in train_df[\"ans_polarity\"] if type(pols) is dict]\nneu_pol = list(filter((0.0).__ne__, neu_pol))\n\n\n\ntrace = go.Histogram(x=neu_pol, marker=dict(\n            color='lightskyblue')\n    )\ndata = [trace]\n\nlayout = go.Layout(\n\n    shapes= [{'line': {'color': 'lightsalmon', 'dash': 'solid', 'width': 1},\n        'type': 'line',\n        'x0': mean_neu,\n        'x1': mean_neu,\n        'xref': 'x',\n        'y0': -0.1,\n        'y1': 1,\n        'yref': 'paper'}],\n\n    # Annotations\n    annotations=[dict(x=mean_neu,\n                y=1,\n                xref='x',\n                yref='paper',\n                text=f\"Mean a = {np.round(mean_neu,3)}\",\n                showarrow=True,\n                arrowhead=7,\n                ax=1,\n                ay=1,\n            )]\n)\nfig = go.Figure(data=data, layout=layout)\n\n\n\nfig.update_layout(xaxis_title=\"Neutral sentiment\", \n                  title_text=\"Neutral sentiment\", template=\"simple_white\")\nfig.show()","5049793a":"train_df[\"flesch_reading_ease\"] = train_df[\"answer\"].progress_apply(textstat.flesch_reading_ease)\n#train_df[\"automated_readability\"] = train_df[\"answer\"].progress_apply(textstat.automated_readability_index)\n#train_df[\"dale_chall_readability\"] = train_df[\"answer\"].progress_apply(textstat.dale_chall_readability_score)","959a9312":"fig = go.Figure(go.Histogram(x=train_df.query(\"flesch_reading_ease > 0\")[\"flesch_reading_ease\"], marker=dict(\n            color='darkorange')\n    ))\n\nfig.update_layout(xaxis_title=\"Flesch reading ease\", title_text=\"Flesch reading ease\", \n                  template=\"simple_white\")\nfig.show()","e2393e93":"fig = go.Figure()\n\nfor category, color in zip(set(train_df['category'].values), ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']):\n    fig.add_trace(go.Histogram(name = category, x=train_df.query(f\"flesch_reading_ease > 0 and category == '{category}'\")[\"flesch_reading_ease\"], marker=dict(\n                color=color)))\n    \n\nfig.update_layout(xaxis_title=\"Flesch reading ease\", title_text=\"Flesch reading ease\", \n                  template=\"simple_white\", barmode='overlay')\n\nfig.update_traces(opacity=0.6)\nfig.show()","de145652":"train_df.columns","a019175c":"extra_cols = ['host_cat', 'question_n_words', 'ans_polarity', 'flesch_reading_ease']\n\ny_train = train_df[target_cols].copy()\nX_train = train_df.drop(list(extra_cols) + list(target_cols), axis=1)\ndel train_df\n\nX_test = test_df.copy()\ndel test_df\n\ngc.collect()","da0e12e4":"from transformers import BertTokenizer, TFBertModel\nfrom tokenizers import BertWordPieceTokenizer\n\n\nnp.set_printoptions(suppress=True)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\nsave_path = '\/kaggle\/working\/bert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n'''fast_tokenizer = BertWordPieceTokenizer('bert_base_uncased\/vocab.txt', \n                                        lowercase=True)'''\n\nfast_tokenizer = BertTokenizer('bert_base_uncased\/vocab.txt', \n                                        lowercase=True)\n","ef328dc2":"MAX_SEQUENCE_LENGTH = 512\n\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\n# Computing the inputs\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    \n    for _, instance in tqdm(df[columns].iterrows()):\n        \n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","61d95391":"## Computing the error metric to the model optimization\ndef compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    #config = BertConfig() # print(config) to see settings\n    #config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    \n    \n    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, \n                             token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, \n                             token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","f789bf50":"outputs = compute_output_arrays(y_train, y_train.columns)\n\ninputs = compute_input_arrays(X_train, X_train.columns, \n                              fast_tokenizer, MAX_SEQUENCE_LENGTH)\n\ntest_inputs = compute_input_arrays(X_test, X_test.columns, \n                                   fast_tokenizer, MAX_SEQUENCE_LENGTH)","1bacb2e0":"## Creating Kfold with 10 splits \ngkf = GroupKFold(n_splits=10).split(X=X_train.question_body, groups=X_train.question_body)\n\n## to receive predictions\nvalid_preds = []\ntest_preds = []\n\n## Looping throught the folds\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # Train for particular folds(used for training for less folds for testing)\n    if fold in np.arange(10):  #[0, 1, 2]\n        \n        ## Train index from Kfold \n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n        ## Valid index from Kfold \n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        K.clear_session()\n        \n        ## Instantiating the Bert Model\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        \n        ## Fiting the model\n        model.fit(train_inputs, train_outputs, epochs=2, batch_size=6)\n        \n        # model.save_weights(f'bert-{fold}.h5')\n        valid_preds.append(model.predict(valid_inputs))\n        # predicting the test set and appending to test_preds\n        test_preds.append(model.predict(test_inputs))\n        \n        # Calculating the error in the valid set\n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)","a6df9022":"columns = list(y_train.columns)\ny = valid_outputs[:10]\ny_pred = model.predict(valid_inputs)","4ccf147b":"print('Actual output')\npd.DataFrame(data=y, columns=columns)","7d905fb8":"print('Predicted output')\npd.DataFrame(data=y_pred[:10], columns=columns)","99a6b327":"Calculate mean Sentiment values","5d8727f3":"<h3><center>3. Exploring Data<\/center><\/h3>","7f7e33e2":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nWe could see high correlation between Answer(Instructions,procedure,explanation) & Question(Instructions,procedure,explanation) respectively\n    <\/div>","b4bb2826":"The data looks pretty clean with no missing values.","ed9813ae":"<h3>Distribution of Target Features<\/h3>","198d60dd":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nAll the different categories show similar distribution indicating that the answers given do not have fixed complexity based on category.\n    <\/div>","d64d77dc":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    We have a Gaussian Distribution of Scores ranging from very simple to complex readablity, Let's have a deeper look into the reading scores of different categories to understand more clearly\n    <\/div>","3a320cbb":"<h3>Preprocessing Data to Bert Architecture<\/h3>","68c9f92c":"<h3><center>4. Bert Tokenizer<\/center><\/h3>","13c0849e":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nAfter comparing with Negativity, the answers have slightly greater positivity\n    <\/div>","45a1452c":"Positive Sentiment","d4ceb053":"<h3><center>1. Importing Libraries<\/center><\/h3>","fbb842da":"<h3><center>2. Reading Files<\/center><\/h3>","5608b00b":"<h3>Readability<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nReadability is an indication of how \"easy\" it is to read some text. There are several metrics that can be used to measure the readability of a piece of text, including Flesch reading ease, automated readability, and Dale-Chall readability.\n    <\/div>","6b5b4c63":"<h3><center>7. Predicting Data<\/center><\/h3>","bf1c8ea0":"<h3><center>5. Creating Model<\/center><\/h3>","da04eefd":"<h3><center>6. Training Model<\/center><\/h3>","25ec44ce":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nFrom the above plot, we can see that negative sentiment has a strong rightward (positive) skew, indicating that negativity is usually on the lower side. This suggests that most comments are not toxic or negative. In fact, the most common negativity value is around <b>0.036<\/b>. Virtually no comments have a negativity greater than 0.35\n    <\/div>","7b66363f":"![image.png](attachment:image.png)","b446547f":"<h3>Sentiment Analysis<\/h3>","8d341be3":"<h3>Preprocessing <\/h3>","0dcfb7f5":"<h3>Correlation Between Features<\/h3>","8c5ef603":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    It is clear that as positivity & Negativity are low we have high neutral answers with mean value 0.88\n    <\/div>","2597feb1":"* We can see that 405 users have played role in both questioning & answering sessions.\n* We have more users who have answered than the questions asked as multiple users might have responded","3be4057f":"Negative Sentiment","aeb2af35":"<h3>No of Words in body<\/h3>","58600878":"<h3>Question & Answer Texts<\/h3>","5534db1a":"Splitting and fetching first string to indentfiy the category","fe1b7fc5":"<h3><center>Introduction<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    The CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of quality scoring aspects.\n<br><br>\nIn this Kernel, we will use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion.<br><br>\n    <\/div>\n\n![image.png](attachment:image.png)","91e971b4":"![image.png](attachment:image.png)","b35a73f3":"<h3>Target Columns<\/h3>","be0a5151":"![image.png](attachment:image.png)"}}