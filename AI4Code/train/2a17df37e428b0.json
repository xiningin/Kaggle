{"cell_type":{"999755f2":"code","6e158407":"code","bbf539da":"code","48713110":"code","89ac9b42":"code","d42aebc9":"code","d3ac4b82":"code","b650fb99":"code","325ac9ac":"markdown","9a965895":"markdown","78055d9e":"markdown","da81e475":"markdown","a9c9d8a2":"markdown","b0ab9647":"markdown","1bdedaf3":"markdown","07319a41":"markdown","01f030c8":"markdown","603f6b6b":"markdown","464531e6":"markdown","e2fa72ba":"markdown"},"source":{"999755f2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import normalize\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression","6e158407":"# Data from https:\/\/archive.ics.uci.edu\/ml\/datasets\/adult\nteste = pd.read_csv('..\/input\/uci-in-txt\/adult_test.txt',\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\").dropna()\n\ntreino_i = pd.read_csv('..\/input\/uci-in-txt\/adult.txt',\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\").dropna()","bbf539da":"treino = treino_i.drop(['fnlwgt',\"Education-Num\", 'Capital Gain', 'Capital Loss'], axis = 1) # Eliminacao de categorias desnecessarias\nmais50 = treino[treino['Target'] == '>50K']\nmenos50 = treino[treino['Target'] == '<=50K']\nfor label in treino.columns[:-1]:\n    dist_total = treino[label].value_counts()\n    dist_pobres = menos50[label].value_counts()\n    \n    prop_pobres = (dist_pobres\/dist_total).sort_values()\n    prop_ricos = 1-prop_pobres\n\n    plt.bar(prop_pobres.index, prop_pobres, label = '<=50K')\n    plt.bar(prop_ricos.index, prop_ricos, bottom = prop_pobres, label = '>50K')\n    plt.title(label)\n    plt.xticks(rotation = 90)\n    plt.legend(loc = 4)\n    plt.show()","48713110":"fatores = ['Age', 'Education', 'Marital Status', 'Country', 'Sex', 'Relationship']\n\nXtreino = treino[fatores]\nXtreino = normalize(OrdinalEncoder().fit_transform(Xtreino))\nYtreino = treino.Target\n\nXteste = teste[fatores]\nXteste = normalize(OrdinalEncoder().fit_transform(Xteste))\nYteste = pd.Series([x[:-1] for x in teste['Target']])","89ac9b42":"acuracia_max = 0\nfor n_ests in range(10,211,50): # Testa-se o n\u00famero de estimadores ideal\n    ada = AdaBoostClassifier(n_estimators = n_ests)\n    ada.fit(Xtreino,Ytreino)\n    ada_acuracia = np.mean(cross_val_score(ada, Xtreino, Ytreino, cv=5))\n    if ada_acuracia > acuracia_max:\n        acuracia_max = ada_acuracia\n        n_f = n_ests\n        ada_f = ada\n    print('Accuracy with n_estimators = {}: '.format(n_ests), ada_acuracia)\nprint('Best accuracy, with n_estimators = {}, : '.format(n_f), acuracia_max)\nprevisao_ada = ada_f.predict(Xteste)\nada_acuracia_f = accuracy_score(Yteste,previsao_ada)\nprint('Test accuracy = ', ada_acuracia_f)","d42aebc9":"acuracia_max_nb = 0\nfor correct in [0,1]:     # Testa fatores de corre\u00e7\u00e3o\n    multi_nb = MultinomialNB(alpha = correct)\n    multi_nb.fit(Xtreino,Ytreino)\n    nb_acuracia = np.mean(cross_val_score(multi_nb, Xtreino, Ytreino, cv=5))\n    if nb_acuracia > acuracia_max_nb:\n        acuracia_max_nb = nb_acuracia\n        correct_f = correct\n        nb_f = multi_nb\n    print('Accuracy with \u03b1 = {}: '.format(correct), nb_acuracia)\nprint('Best accuracy, with \u03b1 = {}, : '.format(correct_f), acuracia_max_nb)\nprevisao_nb = nb_f.predict(Xteste)\nnb_acuracia_f = accuracy_score(Yteste,previsao_nb)\nprint('Test accuracy = ', nb_acuracia_f)","d3ac4b82":"svm_acuracia_max = 0\nfor c_svm in np.linspace(.25,1,4):     # Testa fatores de corre\u00e7\u00e3o\n    clf_svm = svm.SVC(C = c_svm)\n    clf_svm.fit(Xtreino, Ytreino)\n    svm_acuracia = np.mean(cross_val_score(clf_svm, Xtreino, Ytreino, cv=5))\n    if svm_acuracia > svm_acuracia_max:\n        svm_acuracia_max = svm_acuracia\n        c_svm_f = c_svm\n        clf_svm_f = clf_svm\n    print('Accuracy with c = {}: '.format(c_svm), svm_acuracia)\nprint('Best accuracy, with c = {}, : '.format(c_svm_f), svm_acuracia_max)\nprevisao_svm = clf_svm_f.predict(Xteste)\nsvm_acuracia_f = accuracy_score(Yteste,previsao_svm)\nprint('Test accuracy = ', svm_acuracia_f)","b650fb99":"logit_acuracia_max = 0\nfor c_logit in np.linspace(.1,1,10):     # Testa for\u00e7a de regulariza\u00e7\u00e3o\n    logit = LogisticRegression(C= c_logit)\n    logit.fit(Xtreino, Ytreino)\n    logit_acuracia = np.mean(cross_val_score(logit, Xtreino, Ytreino, cv=5))\n    if logit_acuracia > logit_acuracia_max:\n        logit_acuracia_max = logit_acuracia\n        c_logit_f = c_logit\n        logit_f = logit\n    print('Accuracy with c = {}: '.format(c_logit), logit_acuracia)\nprint('Best accuracy, with c = {}, : '.format(c_logit_f), logit_acuracia_max)\nprevisao_logit = logit_f.predict(Xteste)\nlogit_acuracia_f = accuracy_score(Yteste,previsao_logit)\nprint('Test accuracy = ', logit_acuracia_f)","325ac9ac":"Para o AdaBoost:","9a965895":"De forma que o erro evidencia a necessidade de smoothing. Para o SVM:","78055d9e":"Com os dados em m\u00e3os, percebeu-se a nececssidade de verificar as propor\u00e7\u00f5es do par\u00e2metro alvo nas varias segmenta\u00e7\u00f5es e categorias da amostra. No caso, esperava-se obter quais categorias teriam diferen\u00e7as mais significativas entre suas parcelas para o par\u00e2metro-alvo, e julgou-se que tal tarefa seria facilitada com uma representa\u00e7\u00e3o gr\u00e1fica.\n\nAssim, preparou-se um loop para produzir gr\u00e1ficos de barra com as propor\u00e7\u00f5es de indiv\u00edduos com rendas >50k e <=50k de cada parcela das categorias:","da81e475":"Evidentemente, o melhor classificador neste caso \u00e9 o de AdaBoosting, com n\u00famero de estimadores de 210 e acur\u00e1cia de 0.79.","a9c9d8a2":"De forma que, para economizar poder computacional, deve-se cortar cedo. Para o Naive Bayes Multinomial:","b0ab9647":"# 2. Aplica\u00e7\u00e3o do m\u00e9todo","1bdedaf3":"Finalmente, definidos os vetores de treino, prosseguiu-se para aplica\u00e7\u00e3o dos classificadores. Escolheu-se especificamente utilizar os m\u00e9todos AdaBoost, Naive Bayes Multinomial, Support Vector Machine e Regress\u00e3o Log\u00edstica. Para cada, aplicou-se loops para a sele\u00e7\u00e3o de hyperpar\u00e2metros, com valida\u00e7\u00e3o cruzada five-fold. ","07319a41":"Primeiramente, decidiu-se por preparar uma visualiza\u00e7\u00e3o inicial dos dados e como eles se relacionam ao par\u00e2metro-alvo. No c\u00f3digo, come\u00e7ou-se com a importa\u00e7\u00e3o das bibliotecas utilizadas em todo o percurso:","01f030c8":"Dentre as categorias dispon\u00edveis, algumas imagens mostram padr\u00f5es que evidenciam o contraste entre as propor\u00e7\u00f5es nas parcelas. Principalmente, os contrastes s\u00e3o mais acentuados nas categorias \"Marital Status\", \"Sex\" e \"Relationship\", enquanto h\u00e1 um padr\u00e3o claro entre categorias com \"Age\", \"Education\" e \"Country\". Filtraram-se os conjuntos de treino e teste para incluir somente tais categorias - transformando-os em categorias num\u00e9ricas normalizadas - e separaram-se as colunas do par\u00e2metro-alvo:","603f6b6b":"Para, em seguida, importar a base de dados UCI (eliminando linhas com elementos desconhecidos):","464531e6":"# 1. An\u00e1lise preliminar","e2fa72ba":"De forma que a melhor op\u00e7\u00e3o \u00e9 com c = 1. Para a Regress\u00e3o Log\u00edstica:"}}