{"cell_type":{"ce972cb1":"code","8a909e2a":"code","27ae5bc1":"code","1557d06d":"code","eda2f078":"code","727f4498":"code","eac4d06d":"code","1653e67a":"code","78757bd4":"code","4113fa5b":"code","e5c4afeb":"code","9810d56f":"code","564b7bd5":"code","c8bbf32c":"code","9c1db075":"code","7fde507b":"code","e0c3a3ef":"code","8fcfd177":"code","5ef78969":"code","fd4ef468":"code","29d9ca00":"code","09c6c438":"code","10b94c0a":"code","e4e1d439":"code","03854696":"code","65c2e469":"code","3a697d7a":"code","d4b1e8d9":"code","4b0d1562":"code","f1a4ab33":"code","b707f2a7":"code","8754e401":"code","06bbf3d7":"code","9fab71a2":"code","11dd0a13":"code","bc57d5c0":"code","24ce4f7f":"code","c81f39aa":"code","a48cfbbb":"code","9f014127":"code","d68be437":"code","38795edf":"code","65d1ec9e":"code","2e9c5275":"code","79f0abfa":"markdown","10aec6d1":"markdown","0913eb46":"markdown","cf28fd31":"markdown","f585b7f2":"markdown","ab35a3b6":"markdown","f3e6cb96":"markdown"},"source":{"ce972cb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a909e2a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import base\nfrom sklearn import svm\n\nfrom sklearn import pipeline\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\nfrom skopt import BayesSearchCV\n\nimport optuna","27ae5bc1":"df = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')","1557d06d":"df.head()","eda2f078":"df.info()","727f4498":"df.nunique()","eac4d06d":"df.isnull().sum()","1653e67a":"def percent_missing(df):\n    \n    precent_nan = 100*df.isnull().sum() \/ len(df)\n    precent_nan = precent_nan[precent_nan>0].sort_values()\n    \n    return precent_nan","78757bd4":"percent_missing(df)\u0438","4113fa5b":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","e5c4afeb":"imputer = IterativeImputer(estimator=ensemble.RandomForestRegressor(), imputation_order='ascending')","9810d56f":"X = df.drop('Potability',axis=1)\ny = df['Potability']","564b7bd5":"imputer.fit(X)","c8bbf32c":"Xtrans = imputer.transform(X)","9c1db075":"dframe = pd.DataFrame(Xtrans,columns=df.columns[0:-1])","7fde507b":"dframe['Potability'] = y","e0c3a3ef":"percent_missing(dframe)","8fcfd177":"sns.pairplot(dframe,hue='Potability')","5ef78969":"X = dframe.drop('Potability',axis=1)\ny = dframe['Potability']","fd4ef468":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","29d9ca00":"scaler = preprocessing.StandardScaler()","09c6c438":"scaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","10b94c0a":"classifier = ensemble.RandomForestClassifier(n_jobs=-1)","e4e1d439":"classifier.fit(scaled_X_train,y_train)","03854696":"y_preds = classifier.predict(scaled_X_test)","65c2e469":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,y_preds))\nconfusion_matrix(y_test,y_preds)","3a697d7a":"plt.figure(figsize=(12,8),dpi=150)\nfeat_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh')","d4b1e8d9":"def optimize(params,param_names,x,y):\n    params = dict(zip(param_names,params))\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x,y=y):\n        train_idx,test_idx = idx[0],idx[1]\n        \n        xtrain = x.iloc[train_idx]\n        ytrain = y.iloc[train_idx]\n        \n        xtest=x.iloc[test_idx]\n        ytest=y.iloc[test_idx]\n        \n        model.fit(xtrain,ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest,preds)\n        accuracies.append(fold_acc)\n    return -1.0 * np.mean(accuracies)","4b0d1562":"param_space = [\n    space.Integer(2,30,name='max_depth'),\n    space.Integer(100,1500,name='n_estimators'),\n    space.Integer(1,20,name='min_samples_leaf'),\n    space.Categorical(['gini','entropy'],name='criterion'),\n    space.Real(0.01,1,prior='uniform',name='max_features')\n    \n]","f1a4ab33":"param_names= [\n    'max_depth',\n    'n_estimators',\n    'min_samples_leaf',\n    'criterion',\n    'max_features'\n]","b707f2a7":"optimization_function = partial(\noptimize,\nparam_names=param_names,\nx=X,\ny=y)","8754e401":"optimization_function = partial(optimize,param_names=param_names,x=X,y=y)","06bbf3d7":"result = gp_minimize(\noptimization_function,\ndimensions=param_space,\nn_calls=30,\nn_random_starts=10,\nverbose=0)\n\nprint(\ndict(zip(param_names,result.x))\n)","9fab71a2":"from sklearn.metrics import accuracy_score","11dd0a13":"def rf_objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\", [7, 8, 9, 10, 11, 12, None]),\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0]),\n        \"n_jobs\": trial.suggest_categorical('n_jobs', [-1]),\n    }\n    model = ensemble.RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    return -accuracy_score(y_test, model.predict(X_test))","bc57d5c0":"study = optuna.create_study()","24ce4f7f":"study.optimize(rf_objective, n_trials=200, timeout=3600 * 2)\nprint(f\"Best RandomForest accuracy: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")","c81f39aa":"svc = svm.SVC()","a48cfbbb":"svc.fit(scaled_X_train,y_train)","9f014127":"svc_preds = svc.predict(scaled_X_test)","d68be437":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,svc_preds))\nconfusion_matrix(y_test,svc_preds)","38795edf":"def svc_objective(trial):\n    params = {\n        \"C\": trial.suggest_float('C', 1e-10, 1e10, log=True),\n        \"kernel\": trial.suggest_categorical('kernel',['linear', 'poly', 'rbf', 'sigmoid']),\n        \"degree\": trial.suggest_int('degree',1,20,step=1 ),\n        \"gamma\": trial.suggest_categorical(\"gamma\",['scale', 'auto']),\n        \"max_iter\": trial.suggest_int('max_iter',1,10000,step=10),\n        \"decision_function_shape\": trial.suggest_categorical('decision_function_shape', ['ovo', 'ovr']),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0]),\n    }\n    model = svm.SVC(**params)\n    model.fit(scaled_X_train, y_train)\n    return -accuracy_score(y_test, model.predict(scaled_X_test))","65d1ec9e":"other_study = optuna.create_study()","2e9c5275":"other_study.optimize(svc_objective, n_trials=500, timeout=3600 * 2)\nprint(f\"Best svc accuracy: {-round(other_study.best_value, 4)} with parameters {other_study.best_params}\\n\\n\")","79f0abfa":"Feature engineering","10aec6d1":"SVC","0913eb46":"RF + Optuna","cf28fd31":"Standart RF","f585b7f2":"RF + Bayesian Optimization with Gaussian Process","ab35a3b6":"SVC + Optuna","f3e6cb96":"Imports"}}