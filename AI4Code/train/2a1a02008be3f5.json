{"cell_type":{"2dd977b3":"code","efb06ec9":"code","418afa67":"code","2eedfbf8":"code","4affb6e5":"code","cde8a786":"code","79844617":"code","b86c8fad":"code","85f1be58":"code","2218f736":"code","45336321":"code","b982b118":"code","71da28f1":"code","0fa9130d":"code","d24d0103":"code","ffd11fd0":"code","e7b937bb":"code","e464ba1d":"code","2d33c298":"code","3d2f6a5b":"code","5687404f":"code","c61b48df":"code","17dc5bce":"code","314025fe":"code","8ce63c13":"code","b896d05b":"code","20e56424":"code","525b9eda":"code","b703d94d":"code","9516c05d":"code","bd72ea40":"code","584971dc":"markdown","15ee3883":"markdown","0b1a7f32":"markdown","f5774b89":"markdown","8ba9d85d":"markdown","9448dac7":"markdown","e64fa5da":"markdown","bbb1f6a1":"markdown","68231e4f":"markdown","6f16a1f9":"markdown","f05ae0f6":"markdown","6c6ebd8a":"markdown","020132b9":"markdown","07d4d0d1":"markdown","16ed640c":"markdown","cd2a1dae":"markdown","31e1bfe4":"markdown","40c08120":"markdown","87ca1c55":"markdown","8ebfbcfb":"markdown","e7cc3af5":"markdown","6e8b9c2c":"markdown","f8acc8d3":"markdown","67f9f00e":"markdown","d719998e":"markdown","b2efbd7f":"markdown","fa4d1504":"markdown","5c36e23f":"markdown","11712aae":"markdown","e431bff6":"markdown","fb58a629":"markdown","7cc059ef":"markdown","528594dc":"markdown","7c33d9f1":"markdown","93493bb4":"markdown","e267e122":"markdown","37c49eee":"markdown","f67a9d5d":"markdown","c729bc57":"markdown","b58c1ddc":"markdown","3333a4d2":"markdown"},"source":{"2dd977b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\ndata_raw=data.copy()","efb06ec9":"data.dtypes","418afa67":"pd.isnull(data).sum()","2eedfbf8":"missing_index=data[data['CREDIT_LIMIT'].isnull()].index.to_list()\ndata=data.drop(index=missing_index[0])","4affb6e5":"sns.kdeplot(data['MINIMUM_PAYMENTS'], shade=True) \nplt.title('Kernel Density Estimation Plot') \ndata['MINIMUM_PAYMENTS']=data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())","cde8a786":"pd.isnull(data).sum()","79844617":"data_dist=data.iloc[:,1:17]\ndata_columns=data_dist.columns\n\nr,c=0,0\nfig, axes=plt.subplots(4,4, figsize=(20,16))\n#plt.tight_layout()\nfor i in data_columns:\n    sns.distplot(data[i], ax=axes[r,c])\n    c += 1\n    if c == 4: \n        r += 1\n        c=0\n    if r == 4: break\nplt.suptitle('Kernel Density Estimation Plot', fontsize=15)","b86c8fad":"from scipy import stats\ndata1=data.drop(columns=['CUST_ID', 'TENURE']) # drop string feature and features with meaningful range\nz_score=pd.DataFrame(np.abs(stats.zscore(data1)), columns=data1.columns) # calculate z-score\n\n# Find out features with more than 2% outliers (absolute z-score >3)\nz_score3=[]\nover3_index=[] \nfor i in z_score.columns:\n    indexs=z_score.index[z_score[i] > 3].tolist()\n    ans=i, \"{:.3f}\".format(len(indexs)\/len(z_score)), indexs\n    z_score3.append(ans) \n    if len(indexs)\/len(z_score) > 0.02:\n        over3_index.append(i)  \n\n# remove 'BALANCE' and 'CASH_ADVANCE' since thay are regarded as high discriminative features\ndel over3_index[0]\ndel over3_index[1]\n\n# replace 'BALANCE_FREQUENCY','CASH_ADVANCE_FREQUENCY', and 'PURCHASES_TRX' with their square root value\nfor i in over3_index:\n    data1['sqrt_%s' % i]=data1[i].apply(np.sqrt)","85f1be58":"print('feature: ', list(data1.columns))\nprint('data shape: ', data1.shape)","2218f736":"corr_coef=data[1:].corr()\n\n# Heatmap\nplt.figure(figsize=(25, 25))\nsns.heatmap(corr_coef, cmap='Greens', annot=True, annot_kws={'size':14},\n            xticklabels=corr_coef.columns,\n            yticklabels=corr_coef.columns)\nplt.title('Correlation Matrix')\n\n# Find out feature pairs whose coefficient >= 0.7\ncorr_cols=corr_coef.columns.to_list() \nsignif_corr=[]\nfor i in range(len(corr_cols)):\n    col=corr_cols[i]\n    signif_corr.append(abs(corr_coef[col])[abs(corr_coef[col]) >= 0.7])\nsignif_corr_df=pd.DataFrame(signif_corr)\n#signif_corr_df['PURCHASES']['ONEOFF_PURCHASES'] ","45336321":"sns.kdeplot(data1['INSTALLMENTS_PURCHASES'], shade=True)\nsns.kdeplot(data1['ONEOFF_PURCHASES'], shade=True)\nsns.kdeplot(data1['PURCHASES'], shade=True)\nplt.title('Kernel Density Estimation Plot')","b982b118":"sns.kdeplot(data1['PURCHASES_INSTALLMENTS_FREQUENCY'], shade=True)\nsns.kdeplot(data1['ONEOFF_PURCHASES_FREQUENCY'], shade=True)\nsns.kdeplot(data1['PURCHASES_FREQUENCY'], shade=True)\nplt.title('Kernel Density Estimation Plot')","71da28f1":"data1['avg_oneoff_purchases']=data1['ONEOFF_PURCHASES']\/data1['ONEOFF_PURCHASES_FREQUENCY']\ndata1['avg_oneoff_purchases']=data1['avg_oneoff_purchases'].fillna(0)\n\ndata1['avg_installment_purchases']=data1['INSTALLMENTS_PURCHASES']\/data1['PURCHASES_INSTALLMENTS_FREQUENCY']\ndata1['avg_installment_purchases']=data1['avg_installment_purchases'].fillna(0)\n\ndata1['avg_cash_advance']=data1['CASH_ADVANCE']\/data1['CASH_ADVANCE_TRX']\ndata1['avg_cash_advance']=data1['avg_cash_advance'].fillna(0)","0fa9130d":"import math\ndigit_index=list(data1.columns)\n\nfor i in digit_index:\n    max_v=math.ceil(data1[i].describe()['max'])\n    min_v=math.floor(data1[i].describe()['min'])\n    bins_range=np.arange(min_v, max_v, (max_v-min_v)\/8)    \n    data1['digit_%s' % i]=np.digitize(data1[i], bins=bins_range)\n    #print(np.unique(data1['digit_%s' % i], return_counts=True))\n\ndata1['CUST_ID']=data['CUST_ID']\ndata1['TENURE']=data['TENURE']","d24d0103":"print('data shape: ', data1.shape)","ffd11fd0":"from sklearn.preprocessing import Normalizer, RobustScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.model_selection import GridSearchCV","e7b937bb":"from sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names] ","e464ba1d":"categ=list(data1.columns)[22:44]\ncateg1=np.delete(categ,[1, 9, 11]).tolist()\nnumer=['TENURE']\n\nprint('Categorical Features: ', categ1)\nprint('Numerical Feature: ', numer)","2d33c298":"numerical_pipeline=Pipeline([('selector', DataFrameSelector(numer)),\n                             ('RobustScaler', RobustScaler())])\n\ncategorical_pipeline=Pipeline([('selector', DataFrameSelector(categ1)),\n                             ('OneHotEncoder', OneHotEncoder())])\n\nselector_pipeline=FeatureUnion([('numerical_pipeline', numerical_pipeline),\n                                ('categorical_pipeline', categorical_pipeline)])","3d2f6a5b":"def silhouette_score_cal(estimator,data):       \n    preprocess=FeatureUnion([('selector_pipeline', selector_pipeline), \n                             ('Normalizer', Normalizer(norm='l2')),\n                             ('pca', PCA(n_components=15))])            \n    trans_results=preprocess.fit_transform(data)          \n    clusters=estimator.fit_predict(data)\n    score=silhouette_score(trans_results, clusters)\n    return score","5687404f":"categ_copy=categ1.copy()\ncateg_copy.append('TENURE')\ndata_model=data1[categ_copy]\n\npreprocess=FeatureUnion([('selector_pipeline', selector_pipeline), \n                             ('Normalizer', Normalizer(norm='l2')),\n                             ('pca', PCA(n_components=15))])\n\ntrans_results=preprocess.fit_transform(data_model)  # for visualization    \nkmeans=Pipeline([('preprocess', preprocess), ('kmeans', KMeans())])     \nsearch_space=[{'kmeans__n_clusters':np.arange(3,10)}] # test various(3-9) n_clusters\ncv = [(slice(None), slice(None))]\n\ngs=GridSearchCV(estimator=kmeans,param_grid=search_space, \n                scoring=silhouette_score_cal,cv=cv, n_jobs=-1)\n\nbest_model=gs.fit(data_model)","c61b48df":"print('best model - number of cluster: ', best_model.best_params_)\nprint('best model - Silhouette Score: ', best_model.best_score_)\ngrid_predict=best_model.predict(data_model)\ndata1['cluster']=grid_predict\ngrid_results=best_model.cv_results_\nprint('number of observations in each cluster: ', list(np.unique(grid_predict, return_counts=True)[1])) ","17dc5bce":"grid_scores=grid_results['mean_test_score']\nplt.plot(range(3,10), grid_results['mean_test_score'])\nplt.title('Silhouette Score under Various Number of Cluster')","314025fe":"df_visual=pd.DataFrame(TruncatedSVD(n_components=2).fit_transform(trans_results), columns=['p1','p2'])\n\nplt.figure(figsize=(10,8))\nplt.scatter(df_visual['p1'], df_visual['p2'], c=grid_predict, cmap=plt.cm.summer)\nplt.title('Clustering Results Visualization')","8ce63c13":"for feature in list(data.columns[1:]):\n    g=sns.FacetGrid(data1, col='cluster')\n    g=g.map(plt.hist, feature)","b896d05b":"data1.insert(0, 'TENURE', data1['TENURE'], allow_duplicates=True) # just for convience\neach_cluster=data1.groupby('cluster').mean().iloc[:, :23]\neach_cluster","20e56424":"high=['BALANCE', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n      'MINIMUM_PAYMENTS']\nlow=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', \n     'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', \n     'PURCHASES_TRX','PRC_FULL_PAYMENT', 'TENURE']\ntop=['CREDIT_LIMIT', 'PURCHASES', 'PAYMENTS']\n\neach_cluster_high=each_cluster[high].T\neach_cluster_low=each_cluster[low].T\neach_cluster_top=each_cluster[top].T","525b9eda":"def render_plot(data, dataLenth, labels, color, facecolor):    \n    angles = np.linspace(0, 2*np.pi, dataLenth, endpoint=False)\n    data, angles = np.concatenate((data, [data[0]])), np.concatenate((angles, [angles[0]])) # for visualize circle\n        \n    ax = fig.add_subplot(121, polar=True)# polar: drawing circle\n    ax.plot(angles, data, color, linewidth=1)\n    ax.fill(angles, data, facecolor=facecolor, alpha=0.1)# fill color\n    ax.set_thetagrids(angles * 180\/np.pi, labels, fontproperties=\"SimHei\")\n    ax.set_title(\"Feature Value of Each Cluster\", va='baseline', fontproperties=\"SimHei\")\n    ax.grid(True)","b703d94d":"fig = plt.figure(figsize=(15,15))\nlabels = np.array(list(each_cluster_top.index))\nrender_plot(each_cluster_top.iloc[:,0], len(each_cluster_top.iloc[:,0]), labels,'go-', 'g')\nrender_plot(each_cluster_top.iloc[:,1], len(each_cluster_top.iloc[:,0]), labels,'bo-', 'b')\nrender_plot(each_cluster_top.iloc[:,2], len(each_cluster_top.iloc[:,0]), labels,'ro-', 'r')","9516c05d":"fig=plt.figure()\nfig = plt.figure(figsize=(15,15))\nlabels = np.array(list(each_cluster_high.index))\nrender_plot(each_cluster_high.iloc[:,0], len(each_cluster_high.iloc[:,0]), labels,'go-', 'g')\nrender_plot(each_cluster_high.iloc[:,1], len(each_cluster_high.iloc[:,0]), labels,'bo-', 'b')\nrender_plot(each_cluster_high.iloc[:,2], len(each_cluster_high.iloc[:,0]), labels,'ro-', 'r')","bd72ea40":"fig = plt.figure(figsize=(15,15))\nlabels = np.array(list(each_cluster_low.index))\nrender_plot(each_cluster_low.iloc[:,0], len(each_cluster_low.iloc[:,0]), labels,'go-', 'g')\nrender_plot(each_cluster_low.iloc[:,1], len(each_cluster_low.iloc[:,0]), labels,'bo-', 'b')\nrender_plot(each_cluster_low.iloc[:,2], len(each_cluster_low.iloc[:,0]), labels,'ro-', 'r')","584971dc":"<a id=\"Model---KMeans\"><\/a>","15ee3883":"# **Objective**\nDevelop a credit card customer segmentation to define marketing strategy.","0b1a7f32":"**Outliers Treatment**<a><\/a>\n\nThere are 2 ways:\n1. Remove: If the observations are wrongly recorded (e.g., credit limit is not in accordance with bank's regulation), we directly remove the observations. \n2. Keep: Replace that features with their square root value or log value to downsize outliers impact.","f5774b89":"**Model - KMeans**<a><\/a>","8ba9d85d":"# **Preprocessing**<a><\/a>","9448dac7":"<a id=\"Feature-Distribution\"><\/a>","e64fa5da":"<a id=\"Correlation-Matrix\"><\/a>","bbb1f6a1":"**Best Model Results**\n","68231e4f":"Line Plot - Silhouette Score under Various Number of Cluster<a><\/a>","6f16a1f9":"**Correlation Matrix**<a><\/a>\n\nNote:\nTo avoid the top and bottom boxes are cut off, we need to downgrade matplotlib (conda install -c conda-forge matplotlib=3.1.2)","f05ae0f6":"<a id=\"Render-Plot---Features-Value-of-Each-Cluster\"><\/a>","6c6ebd8a":"<a id=\"Feature-Digitization\"><\/a>","020132b9":"Scatter Plot - Cluster Results <a><\/a>","07d4d0d1":"Render Plot - Features Value of Each Cluster<a><\/a>\n\nSplit features into 3 parts based on value reange (low, high, top)","16ed640c":"**Feature Distribution**<a><\/a>\n\nKDE (Kernel Density Estimation) plot for each feature","cd2a1dae":"**Missing Values Treatment**<a><\/a>","31e1bfe4":"**Evaluation Metric - Silhouette Score**<a><\/a>","40c08120":"<a id=\"Missing-Values-Treatment\"><\/a>","87ca1c55":"<a id=\"Scatter-Plot---Cluster-Results\"><\/a>","8ebfbcfb":"<a id=\"Evaluation-Metric---Silhouette-Score\"><\/a>","e7cc3af5":"'MINIMUM_PAYMENTS': \n1. Kernel Density Estimation Plot shows positively skewed distribution, which means there are outliers\n2. Fill missing values with median value ","6e8b9c2c":"Histogram - Features Value of Each Cluster<a><\/a>","f8acc8d3":"<a id=\"Line-Plot---Silhouette-Score-under-Various-Number-of-Cluster\"><\/a>","67f9f00e":"**Create Interaction Features**<a><\/a>\n\nWe can understand clients' purchase level or preference by knowing their average purchase amount, however we only got frequency for one-off and installment purchase or cash advance form this dataset.\n\nFrequency data is reagarded as metric of number of transaction since number of transaction divided by some value will be frequency. (The higher frequency is, the less average purchase amount is)\n\nBelow are the formulas:\n1. Average one-off purchase amount = 'ONEOFF_PURCHASES'\/'ONEOFF_PURCHASES_FREQUENCY'\n2. Average installment purchase amount = 'INSTALLMENTS_PURCHASES'\/'PURCHASES_INSTALLMENTS_FREQUENCY'\n3. Average can advance amount = 'CASH_ADVANCE'\/'CASH_ADVANCE_TRX'","d719998e":"'CREDIT_LIMIT': Drop that row directly since it only has 1 missing value","b2efbd7f":"<a id=\"Outliers-Treatment\"><\/a>","fa4d1504":"**1. Balance**\n* BALANCE: Balance amount left in their account to make purchases (\n* BALANCEFREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\n**2. Purchases**\n* PURCHASES: Amount of purchases made from account\n* ONEOFFPURCHASES: Maximum purchase amount done in one-go\n* INSTALLMENTSPURCHASES: Amount of purchase done in installment\n\n**3. Purchases Frequency**\n* PURCHASESFREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n* ONEOFFPURCHASESFREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n* PURCHASESINSTALLMENTSFREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n* PURCHASESTRX: Number of purchase transactions made\n\n**4. Cash Advance**\n* CASHADVANCE: Cash in advance given by the user\n* CASHADVANCEFREQUENCY: How frequently the cash in advance being paid\n* CASHADVANCETRX: Number of Transactions made with \"Cash in Advanced\"\n\n**5. Payments**\n* PAYMENTS: Amount of Payment done by user\n* MINIMUM_PAYMENTS: Minimum amount of payments made by user\n* PRCFULLPAYMENT: Percent of full payment paid by user\n\n**6. Others**\n* CUSTID: Identification of Credit Card holder (Categorical)\n* CREDITLIMIT: Limit of Credit Card for user\n* TENURE: Tenure of credit card service for user","5c36e23f":"What we do:\n1. Since there is no evidence shows there are observations wrongly recorded, we do not remove outliers.\n2. Z-score is used to identify which features with more than 2% outliers (absolute z-score >3) in themselves, and replace them with thier square root value.","11712aae":"**Feature Digitization**<a><\/a>\n\nDigitize features into 8 ranges except 'TENURE' (due to its meaningful own range)","e431bff6":"**Group Red**: Big spender with high balance, credit limit, and payments who mostly take one-off purchases.\n\n**Group Green**: Medium spender with low balance who take less cash advance and purchase mostly in installments.\n\n**Group Blue**: Small spender with medium balance who take more cash advance and seldom purchase in installments.","fb58a629":"# **Visualization**<a><\/a>","7cc059ef":"High Correlation Coefficient Pairs Analysis:\n\n* PURCHASES & ONEOFFPURCHASES: 0.92\n\nWhen people use one-off purchases, purchase amount is higher than using installment purchases.\n\n* PURCHASESFREQUENCY & PURCHASESINSTALLMENTSFREQUENCY: 0.86\n\nMore people use installment purchases.\n\n* CASHADVANCEFREQUENCY & CASHADVANCETRX: 0.80\n","528594dc":"<a id=\"Histogram---Features-Value-of-Each-Cluster\"><\/a>","7c33d9f1":"<a id=\"Create-Interaction-Features\"><\/a>","93493bb4":"# **Dataset Features**<a><\/a>","e267e122":"Check out feature types","37c49eee":"# **Outline**\n\n[**1. Dataset Features**](#Dataset-Features)\n\n\n[**2. Preprocessing**](#Preprocessing)\n\n> [* Missing Values Treatment](#Missing-Values-Treatment)\n\n> [* Feature Distribution - KDE plot](#Feature-Distribution)\n\n> [* Outliers Treatment](#Outliers-Treatment)\n\n> [* Correlation Matrix](#Correlation-Matrix)\n\n> [* Create Interaction Features](#Create-Interaction-Features)\n\n> [* Feature Digitization](#Feature-Digitization)\n\n\n[**3. Modeling**](#Modeling)\n\n> [* Evaluation Metric - Silhouette Score](#Evaluation-Metric---Silhouette-Score)\n\n> [* Model - KMeans](#Model---KMeans)\n\n\n[**4. Visualization**](#Visualization)\n\n> [* Line Plot - Silhouette Score under Various Number of Cluster](#Line-Plot---Silhouette-Score-under-Various-Number-of-Cluster)\n\n> [* Scatter Plot - Cluster Results](#Scatter-Plot---Cluster-Results)\n\n> [* Histogram - Features Value of Each Cluster](#Histogram---Features-Value-of-Each-Cluster)\n\n> [* Render Plot - Features Value of Each Cluster](#Render-Plot---Features-Value-of-Each-Cluster)\n\n\n[**5. Features of customers in each group**](#Features-of-customers-in-each-group)","f67a9d5d":"Feature Value (Mean Value) of Each Cluster","c729bc57":"# **Features of customers in each group**<a><\/a>","b58c1ddc":"Design pipelines to select numerical and categorical features\n\nNote: Since sklearn cannot directly handle DataFrames, we need to define a function to transform it into numpy\n","3333a4d2":"# **Modeling**<a><\/a>"}}