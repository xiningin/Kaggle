{"cell_type":{"453ec251":"code","10eb61c0":"code","3698aa8a":"code","8ed1ecbf":"code","08c67020":"code","dfeb4e8a":"code","4c3c6b8a":"code","7685442f":"code","4c82ddf8":"code","2ba18059":"code","64971203":"code","2b4aff17":"code","75221ba0":"code","2bfccd6a":"code","7c6d6840":"code","66e73a3c":"code","4a5af13d":"code","a7a45bde":"code","a5c8f79f":"code","0e121170":"code","2aaac1ab":"code","c320cda9":"code","11198756":"code","7920d27b":"code","4d16ee08":"code","bc2a2507":"code","df1605d3":"code","2f695341":"code","83d936e2":"code","9b87f27c":"code","e1226e16":"code","80c44188":"code","dcb1b417":"code","42dc380e":"code","74f0f384":"code","319eb709":"code","16a05105":"code","4257fd66":"code","b1eba59e":"code","e4d86ee4":"code","4a22ab69":"code","bac17a10":"code","fcaac4a4":"code","ce7f4c37":"code","091d273a":"code","3a41cbc1":"code","9e3a6e49":"code","2a19459c":"code","5c3dac4b":"code","1e5fe8fb":"code","1c71b521":"code","ffd9dc1d":"code","d72aa044":"code","3afe003e":"code","6ea186f6":"markdown","bef8634c":"markdown","82f3130e":"markdown","18f90afc":"markdown","94f2d080":"markdown","4bbd988b":"markdown","d6ee5fa2":"markdown","32d08779":"markdown","213fcc26":"markdown","d248d122":"markdown","c5fd2f06":"markdown","2a3dc7f1":"markdown","65719b71":"markdown","25e1314d":"markdown"},"source":{"453ec251":"import os\nimport time\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n#from torchsummary import summary\n\nimport string\nfrom collections import Counter","10eb61c0":"#writer = SummaryWriter(os.path.join(\"runs\", \"baby-names\"))","3698aa8a":"data_path = os.path.join(\"\/kaggle\", \"input\", \"us-baby-names\", \"NationalNames.csv\")\ndata = pd.read_csv(data_path)\nprint(data.shape)\ndata.head()","8ed1ecbf":"data.info()","08c67020":"def clean(name):\n    \n    name = name.lower().strip()\n    name = \"\".join([c for c in name if c in string.ascii_lowercase])\n    name += \".\"\n    return name","dfeb4e8a":"data['Name'] = data['Name'].apply(clean)\ndata.head()","4c3c6b8a":"names = data[['Name', 'Count']].groupby('Name').sum()\ndel names.index.name\nprint(len(names))\nnames.head()","7685442f":"pd.Series(names.index).apply(len).max()","4c82ddf8":"max_length = 11\nlen_filter = pd.Series(names.index).apply(lambda x: len(x)<=max_length).tolist() # max length of 10 excluding '.'\nprint(len_filter[:10])\nprint(names.shape)\nnames = names[len_filter]\nprint(names.shape)","2ba18059":"pd.Series(names.index).apply(len).max()","64971203":"names = names.sort_values(by=['Count'], ascending=False)\nnames.head()","2b4aff17":"names['Count'].describe()","75221ba0":"alpha = 0.8\nnames['Count'].apply(lambda x: np.power(x, alpha)).apply(np.int).describe()","2bfccd6a":"names['count_normalized'] = names['Count'].apply(lambda x: np.power(x, alpha)).apply(np.int)\nnames.head()","7c6d6840":"count_normalized_sum = names['count_normalized'].sum()\nprint(count_normalized_sum)","66e73a3c":"names['p'] = names['count_normalized'] \/ count_normalized_sum\nnames.head()","4a5af13d":"np.random.seed(0)\nnames_list = np.random.choice(names.index, size=10**5, p=names['p'], replace=True)\nprint(len(names_list))\nprint(names_list[:50])","a7a45bde":"pd.Series(names_list).value_counts()","a5c8f79f":"del data, names","0e121170":"chars = \".\" + string.ascii_lowercase\nnum_chars = len(chars)\nprint(chars)\nprint(num_chars)","2aaac1ab":"char_to_id = {c:i for i, c in enumerate(chars)}\nid_to_char = {v:k for k, v in char_to_id.items()}\nprint(char_to_id)\nprint(id_to_char)","c320cda9":"print(max_length)","11198756":"class NamesDataset(Dataset):\n    \n    def __init__(self, names_list):\n        self.names_list = names_list\n        \n    def __len__(self):\n        return len(self.names_list)\n    \n    def __getitem__(self, idx):\n        x_str = self.names_list[idx].ljust(max_length, \".\")[:max_length]\n        y_str = x_str[1:] + \".\"\n        \n        x = torch.zeros((max_length, num_chars))\n        y = torch.zeros(max_length)\n        for i, c in enumerate(x_str):\n            x[i, char_to_id[c]] = 1\n        for i, c in enumerate(y_str):\n            y[i] = char_to_id[c]\n            \n        return x, y","7920d27b":"trainset = NamesDataset(names_list)","4d16ee08":"train_batch_size = 256","bc2a2507":"cpu_count = os.cpu_count()\nprint(cpu_count)","df1605d3":"train_loader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True, num_workers=cpu_count)\nprint(len(train_loader))","2f695341":"train_iter = iter(train_loader)\nX, Y = train_iter.next()\nprint(X.size(), Y.size())","83d936e2":"input_size = num_chars\nhidden_size = 54\noutput_size = num_chars\nnum_layers = 1","9b87f27c":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)","e1226e16":"class Model(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, output_size, num_layers):\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.fc3 = nn.Linear(output_size, output_size)\n        \n    def forward(self, X, states):\n        ht, ct = states\n        batch_size = X.size(0)\n        out, (ht, ct) = self.lstm1(X, (ht, ct))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out, (ht, ct) # out: Size([batch_size, max_length, num_chars])","80c44188":"model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\nmodel = nn.DataParallel(model)\nmodel = model.to(device)","dcb1b417":"#list(model.parameters())","42dc380e":"ht = torch.zeros((num_layers, train_batch_size, hidden_size)).to(device)\nct = torch.zeros((num_layers, train_batch_size, hidden_size)).to(device)\n#writer.add_graph(model, (X, (ht, ct)))\n#writer.close()","74f0f384":"#summary(model, input_size=(max_length, num_chars))","319eb709":"lr = 0.005\nstep_size = len(train_loader) * 1\ngamma = 0.95\nprint(step_size)","16a05105":"criterion = nn.CrossEntropyLoss(reduction='mean')\noptimizer = optim.Adam(model.parameters(), lr=lr)\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=step_size, gamma=gamma)","4257fd66":"def generate_name(model, start='a', k=5):\n    \n    if len(start) >= max_length:\n        return name\n    \n    with torch.no_grad():\n        \n        ht = torch.zeros((num_layers, 1, hidden_size)).to(device)\n        ct = torch.zeros((num_layers, 1, hidden_size)).to(device)\n        length = 0\n        name = start\n        \n        for char in start:\n            X = torch.zeros((1, 1, num_chars)) # [batch_size, timestep, num_chars]\n            X[0, 0, char_to_id[char]] = 1\n            out, (ht, ct) = model(X, (ht, ct))\n            length += 1\n        vals, idxs = torch.topk(out[0], k) # 0 -> first eg in a batch\n        idx = np.random.choice(idxs.cpu().numpy()[0]) # 0 -> first...\n        char = id_to_char[idx]\n        vals, idxs = torch.topk(out[0], k) # 0 -> first eg in a batch\n        idx = np.random.choice(idxs.cpu().numpy()[0]) # 0 -> first...\n        char = id_to_char[idx]\n        \n        while char != \".\" and length <= max_length-1:\n            X = torch.zeros((1, 1, num_chars)) # [batch_size, timestep, num_chars]\n            X[0, 0, char_to_id[char]] = 1\n            out, (ht, ct) = model(X, (ht, ct))\n            vals, idxs = torch.topk(out[0], k) # 0 -> first eg in a batch\n            idx = np.random.choice(idxs.cpu().numpy()[0]) # 0 -> first...\n            char = id_to_char[idx]\n            length += 1\n            name += char\n    \n        if name[-1] != \".\":\n            name += \".\"\n    \n    return name","b1eba59e":"def sampler(model, start='a', n=10, k=5, only_new=False):\n    \n    names = []\n    cnt = 0\n    while cnt <= n:\n        name = generate_name(model=model, start=start, k=k)\n        if only_new: \n            if name not in names_list and name not in names:\n                names.append(name)\n                cnt += 1\n        else:\n            if name not in names:\n                names.append(name)\n                cnt += 1\n    names = [name[:-1].title() for name in names]\n    \n    return names","e4d86ee4":"epochs = 50\nprint_every_n_epochs = epochs \/\/ 10","4a22ab69":"epoch_losses = []\nepoch_lrs = []\niteration_losses = []\niteration_lrs = []\n\nfor epoch in tqdm(range(1, epochs+1), desc=\"Epochs\"):\n    epoch_loss = 0\n    epoch_lr = 0\n    \n    for i, (X, Y) in tqdm(enumerate(train_loader, 1), total=len(train_loader), desc=\"Epoch-{}\".format(epoch)):\n    #for i, (X, Y) in enumerate(train_loader, 1):\n        X, Y = X.to(device), Y.to(device)\n        \n        ht = torch.zeros((num_layers, X.size(0), hidden_size)).to(device)\n        ct = torch.zeros((num_layers, X.size(0), hidden_size)).to(device)\n\n        optimizer.zero_grad()\n        Y_pred_logits, (ht, ct) = model(X, (ht, ct))\n        Y_pred_logits = Y_pred_logits.transpose(1, 2) # Check Loss Doc: [N, d1, C] -> [N, C, d1]\n        loss = criterion(Y_pred_logits, Y.long())\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        lr_scheduler.step()\n        \n        iteration_losses.append(loss.item())\n        iteration_lrs.append(lr_scheduler.get_lr()[0])\n        epoch_loss += loss.item()\n        epoch_lr += lr_scheduler.get_lr()[0]\n        \n    epoch_loss \/= len(train_loader)\n    epoch_lr \/= len(train_loader)\n    epoch_losses.append(epoch_loss)\n    epoch_lrs.append(epoch_lr)\n    \n    if epoch % print_every_n_epochs == 0:    \n        message = \"Epoch:{}    Loss:{}    LR:{}\".format(epoch, epoch_loss, epoch_lr)\n        print(message)\n        names = sampler(model, start='jo', n=10, k=10, only_new=False)\n        print(names)","bac17a10":"fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 8))\nax1.plot(epoch_losses, marker=\"o\", markersize=5)\nax1.set_title(\"Loss\")\nax2.plot(epoch_lrs, marker=\"o\", markersize=5)\nax2.set_title(\"LR\")\nplt.xlabel(\"Epochs\")\nplt.show()","fcaac4a4":"fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 8))\nax1.plot(iteration_losses[::])\nax1.set_title(\"Loss\")\nax2.plot(iteration_lrs[::])\nax2.set_title(\"LR\")\nplt.xlabel(\"Iterations\")\nplt.show()","ce7f4c37":"window = 100\nplt.figure(figsize=(15, 4))\npd.Series(iteration_losses).rolling(window=window).mean().iloc[window-1:].plot()\nplt.show()","091d273a":"path = os.path.join(\"\/kaggle\", \"working\", \"classifier.pth\")\ntorch.save(model.state_dict(), path)","3a41cbc1":"path = os.path.join(\"\/kaggle\", \"working\", \"classifier.pth\")\nmodel = Model(input_size=num_chars, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(path))","9e3a6e49":"names = sampler(model, start='indi', n=10, k=5, only_new=True)\nprint(names)","2a19459c":"names = sampler(model, start='herb', n=10, k=5, only_new=False)\nprint(names)","5c3dac4b":"names = sampler(model, start='su', n=10, k=5, only_new=True)\nprint(names)","1e5fe8fb":"names = sampler(model, start='vis', n=10, k=5, only_new=True)\nprint(names)","1c71b521":"names = sampler(model, start='a', n=10, k=3, only_new=True)\nprint(names)","ffd9dc1d":"names = sampler(model, start='a', n=10, k=8, only_new=True)\nprint(names)","d72aa044":"names = sampler(model, start='a', n=10, k=15, only_new=True)\nprint(names)","3afe003e":"names = sampler(model, start='jam', n=10, k=2, only_new=False)\nprint(names)","6ea186f6":"## 7. Set optimizer","bef8634c":"## 6. Define model","82f3130e":"## 5. Define dataloader","18f90afc":"## 4. Define dataset","94f2d080":"[This Kaggle dataset](https:\/\/www.kaggle.com\/kaggle\/us-baby-names#NationalNames.csv) has names of the child born from 1880 to 2014 along with other features such as Gender and Count. I am going to use this to build a name generator model using sampling of the trained character level LSTM network","4bbd988b":"## 10. Generate new baby names","d6ee5fa2":"## 2. Clean data","32d08779":"## 1. Load data","213fcc26":"## 3. Define utilities","d248d122":"**Project Repository:** https:\/\/github.com\/GokulKarthik\/deep-learning-projects-pytorch","c5fd2f06":"We need a list of names to start building the name generator model. One naive approach for this dataset could be to just take list of unique names. The number of uniques names is 93889, which is large. So, if we sample uniformly from the unique names, the model may learn to generate uncommon and less interesting names. Also if we use the exact counts the model will generate more common names. So we have to sample in between these two. Normalized counts can be used to sample for training.","2a3dc7f1":"## 3. Set training data","65719b71":"## 9. Train model","25e1314d":"## 8. Define sampler"}}