{"cell_type":{"7262e052":"code","8e75272d":"code","0bdd4606":"code","8e0904c3":"code","8a528b60":"code","9ec8a387":"code","429bc610":"code","5d35e71f":"code","e93e08ea":"code","c14a11b7":"code","f162a652":"code","b43f5f0e":"code","84be0cce":"code","6710bd9e":"code","731594d2":"code","550c965a":"code","4a00c5c5":"code","0e29e16d":"code","9ffc6f1e":"code","7c94e780":"code","7ed800ef":"code","76baaf53":"code","276e2d3a":"code","73792a37":"code","9ebefaef":"code","8e74ee7c":"code","16c6f7f9":"code","aaee1b31":"code","5ffe9786":"code","84ec92d5":"code","d26db812":"code","6facdae6":"code","65450943":"code","eed58939":"code","5a74ad34":"code","dc0d587f":"code","8434b9de":"code","57803492":"markdown","ba37234c":"markdown","2ce884dc":"markdown","0f731d90":"markdown","0c7ccd7f":"markdown","35faf639":"markdown","cd1e3078":"markdown","1d5aa714":"markdown","156be28d":"markdown","c477a881":"markdown","88880f0e":"markdown","cd30fcc1":"markdown","836af3f0":"markdown"},"source":{"7262e052":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import auc\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')","8e75272d":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')\n\ndf.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\ndf.totalcharges = df.totalcharges.fillna(0)\n\ndf.churn = (df.churn == 'yes').astype(int)\n\n\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.churn.values\ny_val = df_val.churn.values\ny_test = df_test.churn.values\n\ndel df_train['churn']\ndel df_val['churn']\ndel df_test['churn']\n\nnumerical = ['tenure', 'monthlycharges', 'totalcharges']\n\ncategorical = [\n    'gender',\n    'seniorcitizen',\n    'partner',\n    'dependents',\n    'phoneservice',\n    'multiplelines',\n    'internetservice',\n    'onlinesecurity',\n    'onlinebackup',\n    'deviceprotection',\n    'techsupport',\n    'streamingtv',\n    'streamingmovies',\n    'contract',\n    'paperlessbilling',\n    'paymentmethod',\n]\n\n\ndv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train[categorical + numerical].to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nval_dict = df_val[categorical + numerical].to_dict(orient='records')\nX_val = dv.transform(val_dict)\n\ny_pred = model.predict_proba(X_val)[:, 1]\nchurn_decision = (y_pred >= 0.5)\n(y_val == churn_decision).mean()","0bdd4606":"from sklearn.metrics import accuracy_score","8e0904c3":"accuracy_score(y_val, y_pred >= 0.5)","8a528b60":"\"\"\"Let's check the accuracy with different threshold\"\"\"\nthresholds = np.linspace(0, 1, 21)\n\nscores = []\n\nfor t in thresholds:\n    score = accuracy_score(y_val, y_pred >= t)\n    print('%.2f %.3f' % (t, score))\n    scores.append(score)","9ec8a387":"\"\"\"Let's plot these results\"\"\"\nplt.plot(thresholds, scores);","429bc610":"\"\"\"Also, our data is not balanced\"\"\"\ndf['churn'].mean()","5d35e71f":"actual_positive = (y_val == 1)\nactual_negative = (y_val == 0)\n\nt = 0.5\npredict_positive = (y_pred >= t)\npredict_negative = (y_pred < t)\n\ntp = (predict_positive & actual_positive).sum()\ntn = (predict_negative & actual_negative).sum()\n\nfp = (predict_positive & actual_negative).sum()\nfn = (predict_negative & actual_positive).sum()\n\nconfusion_matrix = np.array([\n    [tn, fp],\n    [fn, tp]\n])\n\"\"\"Confusion Matrix in frequency Form\"\"\"\nconfusion_matrix","e93e08ea":"\"\"\"Confusion Matrix in percentage Form\"\"\"\n(confusion_matrix \/ confusion_matrix.sum()).round(2)","c14a11b7":"\"\"\"Precision\"\"\"\np = tp \/ (tp + fp)\nprint(p)\n\"\"\"Recall\"\"\"\nr = tp \/ (tp + fn)\nprint(r)","f162a652":"\"\"\"TPR\"\"\"\ntpr = tp \/ (tp + fn)\nprint(tpr)\n\"\"\"FRP\"\"\"\nfpr = fp \/ (fp + tn)\nprint(fpr)","b43f5f0e":"scores = []\n\nthresholds = np.linspace(0, 1, 101)\n\nfor t in thresholds:\n    actual_positive = (y_val == 1)\n    actual_negative = (y_val == 0)\n    \n    predict_positive = (y_pred >= t)\n    predict_negative = (y_pred < t)\n\n    tp = (predict_positive & actual_positive).sum()\n    tn = (predict_negative & actual_negative).sum()\n\n    fp = (predict_positive & actual_negative).sum()\n    fn = (predict_negative & actual_positive).sum()\n    \n    scores.append((t, tp, fp, fn, tn))\n    \n\n\ncolumns = ['threshold', 'tp', 'fp', 'fn', 'tn']\ndf_scores = pd.DataFrame(scores, columns=columns)\n\ndf_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\ndf_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)\n\n\n\n\nplt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\nplt.legend();\n    ","84be0cce":"\"\"\"Random model\"\"\"\nnp.random.seed(1)\ny_rand = np.random.uniform(0, 1, size=len(y_val))\n\n((y_rand >= 0.5) == y_val).mean()","6710bd9e":"def tpr_fpr_dataframe(y_val, y_pred):\n    scores = []\n\n    thresholds = np.linspace(0, 1, 101)\n\n    for t in thresholds:\n        actual_positive = (y_val == 1)\n        actual_negative = (y_val == 0)\n\n        predict_positive = (y_pred >= t)\n        predict_negative = (y_pred < t)\n\n        tp = (predict_positive & actual_positive).sum()\n        tn = (predict_negative & actual_negative).sum()\n\n        fp = (predict_positive & actual_negative).sum()\n        fn = (predict_negative & actual_positive).sum()\n\n        scores.append((t, tp, fp, fn, tn))\n\n    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n    df_scores = pd.DataFrame(scores, columns=columns)\n\n    df_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\n    df_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)\n    \n    return df_scores","731594d2":"df_rand = tpr_fpr_dataframe(y_val, y_rand)\n\nplt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\nplt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\nplt.legend();","550c965a":"\"\"\"Ideal model\"\"\"\nnum_neg = (y_val == 0).sum()\nnum_pos = (y_val == 1).sum()\nnum_neg, num_pos\n\ny_ideal = np.repeat([0, 1], [num_neg, num_pos])\n\ny_ideal_pred = np.linspace(0, 1, len(y_val))","4a00c5c5":"1 - y_val.mean()","0e29e16d":"accuracy_score(y_ideal, y_ideal_pred >= 0.726)","9ffc6f1e":"df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\ndf_ideal[::10]","7c94e780":"plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\nplt.legend()","7ed800ef":"\"\"\"Putting everything together\"\"\"\nplt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\n\nplt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\n\n# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\n# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\n\nplt.legend();","76baaf53":"plt.plot(df_scores.fpr, df_scores.tpr, label='Model')\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend();","276e2d3a":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\n\nplt.plot(fpr, tpr, label='Model')\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend();","73792a37":"\"\"\"Area under Curve basically calculates the probability that a randomly selected positive class has higher score than randomly selected negative class\"\"\"\nprint(auc(fpr, tpr))  # Score from implementation\nprint(auc(df_scores.fpr, df_scores.tpr)) #score from sklearn model","9ebefaef":"auc(df_ideal.fpr, df_ideal.tpr)  # score from ideal model","8e74ee7c":"\"\"\"Calculating fpr and tpr using roc_curve from sklearn\"\"\"\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\nauc(fpr, tpr)                # Then calculating auc from fpr and tpr generated from previous step","16c6f7f9":"\"\"\"We can use sklearn roc_auc to calculate area under curve directly\"\"\"\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_val, y_pred)","aaee1b31":"\"\"\"As discussed earlier: Area under Curve basically calculates the probability that a randomly selected positive class has higher score than \nrandomly selected negative class\n\nLet's implement that and see.\"\"\"\n\nneg = y_pred[y_val == 0]   # creating a variable negative to contain all -ve probabilities\npos = y_pred[y_val == 1]   # creating a variable positive to contain all +ve probabilities","5ffe9786":"\"\"\"\nRandomly selecting postive and negetive class and checking. \nIf we get postive probability greater than negative probability we increment the score by 1\nFinally we take the fraction by dividing the success by total number of times we repeated experiment. \nWe did the experiment 100000 times.\n\"\"\"\nimport random\nn = 100000\nsuccess = 0 \n\nfor i in range(n):\n    pos_ind = random.randint(0, len(pos) - 1)\n    neg_ind = random.randint(0, len(neg) - 1)\n\n    if pos[pos_ind] > neg[neg_ind]:\n        success = success + 1\n\nsuccess \/ n","84ec92d5":"\"\"\"Implementing it second time\"\"\"\nn = 50000\n\nnp.random.seed(1)\npos_ind = np.random.randint(0, len(pos), size=n)\nneg_ind = np.random.randint(0, len(neg), size=n)\n\n(pos[pos_ind] > neg[neg_ind]).mean()","d26db812":"\"\"\"\nEvaluating the same model on different subsets of data\nGetting the average prediction and the spread within predictions\n\"\"\"\n\ndef train(df_train, y_train, C=1.0):\n    dicts = df_train[categorical + numerical].to_dict(orient='records')\n\n    dv = DictVectorizer(sparse=False)\n    X_train = dv.fit_transform(dicts)\n\n    model = LogisticRegression(C=C, max_iter=1000)\n    model.fit(X_train, y_train)\n    \n    return dv, model\n\ndef predict(df, dv, model):\n    dicts = df[categorical + numerical].to_dict(orient='records')\n\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:, 1]\n\n    return y_pred","6facdae6":"!pip install tqdm","65450943":"dv, model = train(df_train, y_train, C=0.001)\ny_pred = predict(df_val, dv, model)","eed58939":"from sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm","5a74ad34":"n_splits = 5\n\nfor C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    scores = []\n\n    for train_idx, val_idx in kfold.split(df_full_train):\n        df_train = df_full_train.iloc[train_idx]\n        df_val = df_full_train.iloc[val_idx]\n\n        y_train = df_train.churn.values\n        y_val = df_val.churn.values\n\n        dv, model = train(df_train, y_train, C=C)\n        y_pred = predict(df_val, dv, model)\n\n        auc = roc_auc_score(y_val, y_pred)\n        scores.append(auc)\n\n    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))","dc0d587f":"scores","8434b9de":"\"\"\"Now that we have the best parameter. Let's train the model using that\"\"\"\ndv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\ny_pred = predict(df_test, dv, model)\n\nauc = roc_auc_score(y_test, y_pred)\nauc","57803492":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">4. Precision and Recall\n    \n<\/div>","ba37234c":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>Chapter 4<\/u><\/div>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>Evaluation<\/u>    \n<p style=\"font-family:cursive;font-size:17px; color:white\" >Notebook is a part of FREE ML course by Glexey Grigorev\n<a style=\"font-family:cursive;font-size:17px; color: yellow\" href=\"https:\/\/datatalks.club\/courses\/2021-winter-ml-zoomcamp.html\" target=\"_blank\"> <u>Link for the Course<\/u><\/a><\/p>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#1\">1. Getting ready for Evaluation<\/a><\/li>   \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#2\">2. Accuracy and dummy model<\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#3\">3. Confusion Matrix <\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#4\"> 4. Precision and Recall  <\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5\"> 5. ROC Curves <\/a><\/li>  \n    <ul>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5.1\"> 5.1 Random model<\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5.2\"> 5.2 Ideal model<\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5.3\"> 5.3 Putting everything together<\/a><\/li>             \n<\/ul>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#6\"> 6. ROC AUC<\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#7\">7. Cross-Validation<\/a><\/li>     \n    \n<\/div>    ","2ce884dc":"<a id=\"table-of-contents\"><\/a>","0f731d90":"[back to top](#table-of-contents)\n<a id=\"5.2\"><\/a>\n\n<div style=\"background:#59a1c6;color:black; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">5.2 Ideal model\n    \n<\/div>","0c7ccd7f":"[back to top](#table-of-contents)\n<a id=\"5.1\"><\/a>\n\n<div style=\"background:#59a1c6;color:black; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">5.1 Random model\n    \n<\/div>","35faf639":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">6. AUC\n    \n<\/div>","cd1e3078":"[back to top](#table-of-contents)\n<a id=\"5.3\"><\/a>\n\n<div style=\"background:#59a1c6;color:black; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">5.3 Putting everything together\n    \n<\/div>","1d5aa714":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">3. Confusion Matrix\n    \n<\/div>","156be28d":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">1. Getting ready for Evaluation\n    \n<\/div>","c477a881":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">2. Accuracy and dummy model\n    \n<\/div>","88880f0e":"\n[back to top](#table-of-contents)\n<a id=\"7\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">7. Cross-Validation\n    \n<\/div>","cd30fcc1":"**Observation:**\n- **We can see that when all the predictions are False our model gives accuracy of ``0.726``**\n- **This is because our data is ``not balanced`` . One class is way more than in numbers than the other.**\n ","836af3f0":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px;color:yellow\">5. ROC Curves\n    \n<\/div>\n\n\n"}}