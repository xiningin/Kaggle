{"cell_type":{"28defa3f":"code","aeb5e37b":"code","c602c06b":"code","ddb17ec2":"code","35a5fbdb":"code","9283899f":"code","605ffda7":"code","0ce832fe":"code","82dcf0b2":"code","6b21f935":"code","073e2734":"code","574a2a9b":"code","a17fcd58":"code","746f1141":"code","64329a1f":"code","eade1882":"code","c4907586":"code","4e3fde8e":"code","40a18543":"code","86d785f8":"code","8c5a31a7":"code","3bab0391":"code","edd12572":"code","91166021":"code","df2a2271":"code","8e23ac72":"code","9d7b108d":"code","964e982f":"code","f1ff7273":"code","605a65e5":"code","1b1b94c5":"code","25b4adbc":"code","31c3468a":"code","7aa3d509":"code","f5f1a529":"code","c6cdc221":"code","f2a212da":"markdown"},"source":{"28defa3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aeb5e37b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","c602c06b":"# from google.colab import files\n# uploaded = files.upload()","ddb17ec2":"df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","35a5fbdb":"df.head()","9283899f":"df.describe()","605ffda7":"sns.heatmap(df.corr())\n# We can not find out about the most important factors because the target is binary","0ce832fe":"# for the sake of data exploration we can create a model to predict the most\n#  important factors\nx = df.iloc[:,:-1]\n# getting the features alone\ny = df.iloc[:,-1]\n# the target\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import StackingClassifier\nmodel = ExtraTreesClassifier()\nmodel2 = StackingClassifier\nmodel.fit(x,y)\nfeat_importance = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importance.nlargest(12).plot(kind='barh')\n# plotting using pandas plot command\n# .nlargest() easy way to sort\n# the plot's kind could be 'bar' or 'barh' for horizontal bars","82dcf0b2":"x = df.iloc[:,:-1]\n# getting the features alone\ny = df.iloc[:,-1]\n# the target\nfrom sklearn.ensemble import AdaBoostClassifier\nmodel2 = AdaBoostClassifier(learning_rate=0.1)\nmodel2.fit(x,y)\nfeat_importance = pd.Series(model2.feature_importances_, index=x.columns)\nfeat_importance.nlargest(12).plot(kind='barh')","6b21f935":"# The largest three features in affect on the death are the most important in\n# finding the outliers.","073e2734":"# we should find outliers\n# Boxplots are good for finding outliers","574a2a9b":"sns.boxplot(df['ejection_fraction']);\n# there are two outliers","a17fcd58":"df[df['ejection_fraction']>=70]","746f1141":"df = df[df['ejection_fraction']<70]\n# Removed two outliers from one of the three most important features","64329a1f":"sns.boxplot(x=df['time'],color='red')\n# no outliers","eade1882":"sns.boxplot(x=df['serum_creatinine'])","c4907586":"sns.boxplot(y=df['age'],x=df['DEATH_EVENT'])","4e3fde8e":"sns.boxplot(y=df['time'],x=df['DEATH_EVENT'])","40a18543":"df['sex'].value_counts()","86d785f8":"sns.countplot(x=df['DEATH_EVENT'],hue=df['sex']);\nsns.set()","8c5a31a7":"plt.pie(x=df['sex'].value_counts(),labels=['Male','Female'])","3bab0391":"# male_died = len(df[(df['DEATH_EVENT']==1)&(df['sex']==1)])\n# male_survived = len(df[(df['DEATH_EVENT']==0)&(df['sex']==1)])\n# female_died = len(df[(df['DEATH_EVENT']==1)&(df['sex']==0)])\n# female_survived = len(df[(df['DEATH_EVENT']==0)&(df['sex']==0)])\n\nsex_mortality = []\nsex_mortality.append(len(df[(df['DEATH_EVENT']==1)&(df['sex']==1)]))\nsex_mortality.append(len(df[(df['DEATH_EVENT']==0)&(df['sex']==1)]))\nsex_mortality.append(len(df[(df['DEATH_EVENT']==1)&(df['sex']==0)]))\nsex_mortality.append(len(df[(df['DEATH_EVENT']==0)&(df['sex']==0)]))\nsex_labels = ['male_died','male_survived','female_died','female_survived']\n\nplt.pie(x=sex_mortality,autopct='%.1f',labels=sex_labels);\n# autopct = '%.1f'","edd12572":"smoking_died = len(df[(df['DEATH_EVENT']==1)&(df['smoking']==1)])\nsmoking_survived = len(df[(df['DEATH_EVENT']==0)&(df['smoking']==1)])\nnon_smoking_died = len(df[(df['DEATH_EVENT']==1)&(df['smoking']==0)])\nnon_smoking_survived = len(df[(df['DEATH_EVENT']==0)&(df['smoking']==0)])\n\nsmoking_mortality = []\nsmoking_mortality.append(len(df[(df['DEATH_EVENT']==1)&(df['smoking']==1)]))\nsmoking_mortality.append(len(df[(df['DEATH_EVENT']==0)&(df['smoking']==1)]))\nsmoking_mortality.append(len(df[(df['DEATH_EVENT']==1)&(df['smoking']==0)]))\nsmoking_mortality.append(len(df[(df['DEATH_EVENT']==0)&(df['smoking']==0)]))\n\n\nsmoking_labels = ['smoking_died','smoking_survived','non_smoking_died','non_smoking_survived']\n\nplt.pie(x=smoking_mortality,autopct='%.1f',labels=smoking_labels);","91166021":"df.columns","df2a2271":"# enough of exploration\n# choosing some of the most important features for the learning\nX = df.drop([ 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'high_blood_pressure', 'platelets', 'sex', 'smoking','age','serum_sodium',\n       'DEATH_EVENT'],axis=1)\ny = df['DEATH_EVENT']","8e23ac72":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)","9d7b108d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","964e982f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nlr_ac = accuracy_score(y_test,lr_pred)\nlr_con = confusion_matrix(y_test, lr_pred)\naccuracies = []\naccuracies.append(lr_ac)\nprint(lr_ac)\nprint(lr_con)","f1ff7273":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","605a65e5":"knn = KNeighborsClassifier()\nparam_grid_knn = {'n_neighbors':list(range(3,9)), 'metric':['minkowski']}\nknn_grid = GridSearchCV(estimator=knn,param_grid=param_grid_knn,cv=70,scoring='accuracy')\nknn_grid.fit(X_train,y_train)\nknn_pred = knn_grid.predict(X_test)\n\nknn_ac = accuracy_score(y_test,knn_pred)\nknn_con = confusion_matrix(y_test, knn_pred)\naccuracies.append(knn_ac)\nprint(knn_ac)\nprint(knn_con)","1b1b94c5":"from sklearn.svm import SVC","25b4adbc":"svc = SVC(  )\nparam_grid_svc = {'C':[0.1,0.4,0.5,0.6,1],'degree':[1,2,3],'max_iter':[10000]}\nsvc_grid = GridSearchCV(estimator=svc,param_grid=param_grid_svc,cv=70,scoring='accuracy')\nsvc_grid.fit(X_train,y_train)\nsvc_pred = svc_grid.predict(X_test)\n\nsvc_ac = accuracy_score(y_test,svc_pred)\nsvc_con = confusion_matrix(y_test, svc_pred)\naccuracies.append(svc_ac)\nprint(svc_ac)\nprint(svc_con)\n","31c3468a":"from sklearn.ensemble import RandomForestClassifier\nrndf = RandomForestClassifier(n_estimators = 10, criterion='entropy', random_state=0)\nrndf.fit(X_train,y_train)\nrndf_pred = rndf.predict(X_test)\n\nrndf_ac = accuracy_score(y_test,rndf_pred)\nrndf_con = confusion_matrix(y_test, rndf_pred)\naccuracies.append(rndf_ac)\nprint(rndf_ac)\nprint(rndf_con)","7aa3d509":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nparam_grid_xgb = {'n_estimators' : list(range(8,12)), 'max_depth':list(range(10,15)), 'subsample':[0.2,0.7]}\nxgb_grid = GridSearchCV(estimator=xgb,param_grid=param_grid_xgb,cv=70)\n\nxgb_grid.fit(X_train,y_train)\nxgb_pred = svc_grid.predict(X_test)\n\nxgb_ac = accuracy_score(y_test,xgb_pred)\nxgb_con = confusion_matrix(y_test, xgb_pred)\naccuracies.append(xgb_ac)\nprint(xgb_ac)\nprint(xgb_con)","f5f1a529":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,y_train)\ngb_pred = gb.predict(X_test)\ngb_ac = accuracy_score(y_test,gb_pred)\ngb_con = confusion_matrix(y_test, gb_pred)\naccuracies.append(gb_ac)\nprint(gb_ac)\nprint(gb_con)\n","c6cdc221":"models = [\"Logistic Regression\", \"KNearestNeighbours\",\"SupportVectorClassifier\",\"RandomForest\", \"XGBOOST\", 'GradientBoosting']\nax = sns.barplot(x=accuracies,y=models)\n","f2a212da":"1. Logistic Regression"}}