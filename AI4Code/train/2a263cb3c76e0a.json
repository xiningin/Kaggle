{"cell_type":{"6cfc4b90":"code","1b729776":"code","db6cb0c6":"code","c6a83b0d":"code","4b87aaec":"code","f1972075":"code","354f2859":"code","4651464a":"code","6b58030c":"code","969b0abd":"code","711a942e":"code","3c03eb58":"code","9f76bb99":"code","77bb8d27":"code","0c1d5c5b":"code","8e92aec6":"code","95533247":"code","69ad78b7":"code","e6555e4c":"code","b9aee52d":"code","4f3423e0":"code","93520453":"code","b109f9bc":"code","b936246d":"code","b893a648":"code","7fb36592":"code","c2ff7e1f":"code","c598b946":"code","1226ab67":"code","f9644817":"code","bed87fe1":"code","7ae1c6bb":"code","52c11ce8":"code","6681c28f":"code","4ef41170":"code","05aa4254":"code","72c4356c":"code","5160276e":"code","eb678d8a":"code","6b812864":"code","5b084508":"code","2d8554cc":"code","64dd2728":"code","48064245":"code","c63d56d2":"code","0ee479d1":"code","39532ec1":"code","f5e13f8e":"markdown","f2cf51ea":"markdown","19510fbf":"markdown","39bcdc00":"markdown","d3baba22":"markdown","5b255b03":"markdown","dc908012":"markdown","f7adfcee":"markdown"},"source":{"6cfc4b90":"import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\nimport collections\nimport random\nimport numpy as np\nimport json\n\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.vgg19 import VGG19\n#from tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tqdm import tqdm","1b729776":"train_images = '..\/input\/coco-2017-dataset\/coco2017\/train2017\/'\ntest_images = '..\/input\/coco-2017-dataset\/coco2017\/test2017\/'\nglove_path = '..\/input\/glove6b\/glove.6B.200d.txt'","db6cb0c6":"train_images_len = len(os.listdir(train_images))\ntest_images_len = len(os.listdir(test_images))\nprint(train_images_len)\nprint(test_images_len)","c6a83b0d":"annotation_file = '..\/input\/coco-2017-dataset\/coco2017\/annotations\/captions_train2017.json'\nwith open(annotation_file, 'r') as f:\n    annotations = json.load(f)\nprint(annotations['annotations'][0])","4b87aaec":"# Group all captions together having the same image ID.\nimage_path_to_caption = collections.defaultdict(list)\nfor val in annotations['annotations']:\n  caption = (f\"{val['caption']}\")\n  image_path = train_images + '%012d.jpg' % (val['image_id'])  \n  image_path_to_caption[image_path].append(caption)","f1972075":"print(len(image_path_to_caption))\nimage_path_to_caption['..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000519186.jpg']","354f2859":"# conversion from defaultdict to dict\nimage_path_to_caption = dict(image_path_to_caption)\nprint(type(image_path_to_caption))","4651464a":"import random\n\nlst = list(image_path_to_caption.items())\n#Run only one time ---\nrandom.shuffle(lst)\nx = int(len(lst)*0.5)\nlst  = lst[0:x]\n# ---------\nimage_path_to_caption = dict(lst)\nprint(len(image_path_to_caption))","6b58030c":"def id_caption(image_path_to_caption): \n    image_id_to_caption = collections.defaultdict(list)\n    for (key,val) in image_path_to_caption.items(): \n        for values in val:\n            x = key.split('\/')[-1]\n            x = x.split('.')[0]\n            image_id_to_caption[x].append(values)\n\n    # Convert image_id_to_caption to dict\n    image_id_to_caption = dict(image_id_to_caption)\n    return image_id_to_caption","969b0abd":"# Clean description means remove puncutation and lowercase\n\n#1. image_path_to_caption\ntable = str.maketrans('', '', string.punctuation)\nfor key, desc_list in image_path_to_caption.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","711a942e":"items = list(image_path_to_caption.items())\nprint(items[7])","3c03eb58":"# unique words in our caption data\nvocabulary = set()\nfor key in image_path_to_caption.keys():\n        [vocabulary.update(d.split()) for d in image_path_to_caption[key]]\nprint(len(vocabulary))\n","9f76bb99":"# Now call the id_caption function\nimage_id_to_caption = id_caption(image_path_to_caption)\nprint(len(image_id_to_caption))\nprint(type(image_id_to_caption))\nprint(list(image_id_to_caption.keys())[:5])","77bb8d27":"lines = list()\nfor key, desc_list in image_id_to_caption.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)\n\nprint(type(new_descriptions))\nprint(new_descriptions[:400])","0c1d5c5b":"## Getting image ids\ntrain = list(image_id_to_caption.keys())\nprint(train[0:5])","8e92aec6":"train_img = list(image_path_to_caption.keys())\nprint(len(train_img))","95533247":"train_descriptions = dict()\nfor line in tqdm(new_descriptions.split('\\n')):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)\n\nprint(len(train_descriptions))","69ad78b7":"list(train_descriptions.keys())[0:5]","e6555e4c":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)","b9aee52d":"print(len(all_train_captions)) # contain all the captions   5*11858 = 59173\nprint(all_train_captions[:5])","4f3423e0":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint('Vocabulary = %d' % (len(vocab)))","93520453":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1","b109f9bc":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","b936246d":"embeddings_index = {} \nf = open(glove_path, encoding=\"utf-8\")\nfor line in f:   \n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","b893a648":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","7fb36592":"embedding_matrix[5].shape","c2ff7e1f":"# model = InceptionV3(weights='imagenet')\n# model = VGG19(include_top = False, weights = 'imagenet')\nmodel = InceptionV3(weights='imagenet')\nmodel_new = Model(model.input, model.layers[-2].output)","c598b946":"from keras.preprocessing.image import load_img,img_to_array\ndef preprocess(image_path):\n    img = load_img(image_path, target_size=(299, 299))\n    x = img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","1226ab67":"# train img is array of path value to images\nprint(len(train_img))\nprint(train_img[0:5])","f9644817":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    #print(fea_vec.shape)\n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    #print(fea_vec.shape)\n    return fea_vec","bed87fe1":"encoding_train = {}\nfor img in tqdm(train_img):\n    path = img.split('\/')[-1]\n    encoding_train[path] = encode(img)\ntrain_features = encoding_train\n\n# encoding_test = {}\n# for img in test_img:\n#     encoding_test[img[len(test_images):]] = encode(img)","7ae1c6bb":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n      \ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","52c11ce8":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])","6681c28f":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key+'.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","4ef41170":"epochs = 5\nbatch_size = 32\nsteps = len(train_descriptions)\/\/batch_size\n\ngenerator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","05aa4254":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","72c4356c":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","5160276e":"# Saving Model to disk\nmodel.save(\"image_caption_generator.h5\")\nprint(\"Saved model to disk\")","eb678d8a":"# This function will generate caption for the image \ndef generate_caption(image_path):\n    image = encode(image_path)\n    image = image.reshape((1, 2048))\n    x=plt.imread(image_path)\n    plt.imshow(x)\n    plt.show()\n    print(\"Greedy:\",greedySearch(image))\n    print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n    print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n    print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n    ","6b812864":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000000009.jpg')","5b084508":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000000030.jpg')","2d8554cc":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/val2017\/000000000776.jpg')","64dd2728":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/val2017\/000000000885.jpg')","48064245":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/test2017\/000000000063.jpg')","c63d56d2":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/test2017\/000000000155.jpg')","0ee479d1":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/val2017\/000000000785.jpg')","39532ec1":"generate_caption('..\/input\/coco-2017-dataset\/coco2017\/test2017\/000000000178.jpg')","f5e13f8e":"## Testing of the Model","f2cf51ea":"## Importing Libraries","19510fbf":"# Image Caption Generator (InceptionV3 and COCO-2017 dataset)","39bcdc00":"### Image_id_to_caption Function so that you can convert path_to_caption to id_to_caption whenever you want to","d3baba22":"## Model (InceptionV3)","5b255b03":"## Saving the model","dc908012":"## Importing Dataset and data preprocessing","f7adfcee":"## Dropping of some data from the traning set"}}