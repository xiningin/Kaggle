{"cell_type":{"85fcd664":"code","fe8d1911":"code","ad36b662":"code","92b17281":"code","8857729d":"code","4402057b":"code","a5e26e48":"code","6fbe4ade":"code","92ae9278":"code","3c7bc392":"code","5bb04312":"code","f3ccac0e":"code","341cdcf3":"code","328753b4":"code","2a08ced4":"code","2524e123":"code","97bb8889":"code","6fcf137d":"code","c0d8b1ca":"code","8b607daf":"code","0bcbb69e":"code","ed13fa66":"code","6b21d79a":"code","265d0518":"code","dd57b920":"code","36ff13c1":"code","863223b8":"code","2d3458f5":"code","5d3ebd74":"code","44772763":"code","5ccf0378":"code","e6eab33f":"code","e29c33a8":"code","f8c750ae":"code","79c70ba5":"code","2f79b995":"code","7eac858e":"code","227c4dde":"code","b52c0d6f":"markdown","8e20ec8d":"markdown","765da408":"markdown","cc5681cf":"markdown","4d76ab3d":"markdown","d196dda9":"markdown","043a6027":"markdown","578a496c":"markdown","8c98e2be":"markdown","964f2ad1":"markdown","566c88d1":"markdown","3a5359d5":"markdown","94c14292":"markdown","312d7d25":"markdown","4519f1a0":"markdown","4df9d64c":"markdown","7b6b826b":"markdown","3be97f71":"markdown","d171dc4b":"markdown","626393d3":"markdown","c99f9c11":"markdown","1bbc46b4":"markdown"},"source":{"85fcd664":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  LabelEncoder\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\n\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier","fe8d1911":"train=pd.read_csv(\"..\/input\/gender-neutrality-and-inclusion\/Train.csv\")\ntrain.head()","ad36b662":"train.shape","92b17281":"test=pd.read_csv(\"..\/input\/gender-neutrality-and-inclusion\/Test.csv\")\ntest.head()","8857729d":"test.shape","4402057b":"train.isnull().sum()","a5e26e48":"test.isnull().sum()","6fbe4ade":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","92ae9278":"sns.pairplot(train,hue=\"BiasInfluentialFactor\")","3c7bc392":"train.describe()","5bb04312":"train.corr()","f3ccac0e":"# using .heatmap() of seaborn to understand better relationship of variables \nsns.heatmap(train.corr(), annot=True,cbar=False)","341cdcf3":"# dropping the EmpId and EmpName as well as they are not required\ndf=train.drop(['GraduationYear','EmpID', 'EmpName','CurrentCTC','BiasInfluentialFactor','YearsOfExperince'],axis=1)\n\n# checking the first five rows\ndf.head()","328753b4":"# Encode each object type with a label using one hot encoding\n\n# loop over each column\nfor f in df.columns: \n    # check for object type\n    if df[f].dtype=='object':\n      #label encoder \n        lbl = LabelEncoder() \n        lbl.fit(list(df[f].values)) \n        df[f] = lbl.transform(list(df[f].values))\n\n# check the dataframe after label encoding\ndf.head()","2a08ced4":"# distribute our data to X and Y\nX=df.drop(['FitmentPercent'],axis=1)\nY=df[\"FitmentPercent\"]","2524e123":"# Find the correlation (last check)\nX.corr()","97bb8889":"# split the data to train and test set\nx_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.85,random_state=42)\n\n\nprint(\"training data shape:-{} labels{} \".format(x_train.shape,y_train.shape))\nprint(\"testing data shape:-{} labels{} \".format(x_test.shape,y_test.shape))","6fcf137d":"# train Random forest regressor\n\n# create the model\nmodel_rf = RandomForestRegressor(n_estimators=500, oob_score=True, random_state=100)\n\nprint(\"Training..........\")\n# fitting the model\nmodel_rf.fit(x_train, y_train) \n\n# get original predictions\npred_train_rf= model_rf.predict(x_train)\n\nprint(\"Training Evaluation\")\nprint(\"Mean Square Error\",np.sqrt(mean_squared_error(y_train,pred_train_rf)))\nprint(\"R2 Score\",r2_score(y_train, pred_train_rf))\n\nprint(\"Testing Evaluation\")\n# testing predictions\npred_test_rf = model_rf.predict(x_test)\nprint(\"Mean Square Error\",np.sqrt(mean_squared_error(y_test,pred_test_rf)))\nprint(\"R2 Score\",r2_score(y_test, pred_test_rf))","c0d8b1ca":"# get the dataset ready removing unwanted columns\ndf_test=test.drop(['GraduationYear','EmpID', 'EmpName','CurrentCTC','YearsOfExperince'],axis=1)","8b607daf":"# Encoding each column og object datatype\nfor f in df_test.columns: \n    if df_test[f].dtype=='object': \n        lbl = LabelEncoder() \n        lbl.fit(list(df_test[f].values)) \n        df_test[f] = lbl.transform(list(df_test[f].values))\n\n# scaling down test data\nscaler = StandardScaler()\ndf_test = scaler.fit_transform(df_test)\nprint(df_test[0])","0bcbb69e":"# get the predictions and store them as list\nval=list(model_rf.predict(df_test))\n","ed13fa66":"import matplotlib.pyplot as plt\nplt.title(\"Graph showing training accuracy\")\nplt.plot(sorted(y_train),label=\"original value\")\nplt.plot(sorted(model_rf.predict(x_train)),label=\"Model Predictions\")\nplt.legend()\nplt.show()","6b21d79a":"import matplotlib.pyplot as plt\nplt.title(\"Graph showing testing accuracy\")\nplt.plot(sorted(y_test),label=\"original value\")\nplt.plot(sorted(model_rf.predict(x_test)),label=\"Model Predictions\")\nplt.legend()\nplt.show()","265d0518":"# create the dataset of our requred columns\ndf=train.drop(['GraduationYear','EmpID', 'EmpName','CurrentCTC','FitmentPercent','YearsOfExperince'],axis=1)\ndf.head()","dd57b920":"# encoding the object datatype values in data\nle =LabelEncoder()\ndf['BiasInfluentialFactor']=le.fit_transform(df['BiasInfluentialFactor'])","36ff13c1":"df.isnull().sum()","863223b8":"def assign_labels(df,column):\n  val=df[column].unique().tolist()\n  mydict={}\n  c=0\n  for i in val:\n    mydict[i]=c\n    c+=1\n  print(mydict)\n  return mydict\n\nfor i in df.columns:\n  if df[i].dtype=='object':\n    df[i]=df[i].map(assign_labels(df,i))\n    #print(assign_labels(df,i))\ndf.head()","2d3458f5":"# creating X and Y\nX=df.drop(['BiasInfluentialFactor'],axis=1)\nY=df['BiasInfluentialFactor']","5d3ebd74":"# scaling down the data using Standatd scaler for much accuracy\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nprint(X[0])","44772763":"# split the data to train and test set\nx_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.85,random_state=42)\n\n\nprint(\"training data shape:-{} labels{} \".format(x_train.shape,y_train.shape))\nprint(\"testing data shape:-{} labels{} \".format(x_test.shape,y_test.shape))","5ccf0378":"# build a decision tree classifier\nclf = DecisionTreeClassifier().fit(x_train, y_train)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(x_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(x_test, y_test)))","e6eab33f":"# Make Predictions\npred=clf.predict(np.array(df_test))","e29c33a8":"# Store Predictions\nsol=list(le.inverse_transform(pred))\n","f8c750ae":"from sklearn.metrics import confusion_matrix\n# Make Predictions\npred1=clf.predict(np.array(x_train))\nplt.title(\"Confusion Matrix training data\")\nsns.heatmap(confusion_matrix(y_train,pred1),annot=True,cbar=False)\nplt.legend()","79c70ba5":"from sklearn.metrics import confusion_matrix\n# Make Predictions\npred1=clf.predict(np.array(x_test))\nplt.title(\"Confusion Matrix testing data\")\nsns.heatmap(confusion_matrix(y_test,pred1),annot=True,cbar=False)\nplt.legend()","2f79b995":"# storing the employee ID as list\nEmp=test['EmpID'].to_list()","7eac858e":"\n# intialise data of lists.\ndata = {'EmpID':Emp,\n        'BiasInfluentialFactor':sol,\n        'FitmentPercent':val}\n  \n# Create DataFrame\ndf = pd.DataFrame(data)\n  \n# Print the output.\ndf.head()","227c4dde":"# saving the dataframe as CSV file\ndf.to_csv('Sol_Pulkit.csv',index=False)","b52c0d6f":"Another think to notice is that Expected CTC and Current CTC are positively correlated \n\nAge is also positively correlated with years of experience ","8e20ec8d":"## Check Correlation","765da408":"### Check for null values","cc5681cf":"# let's crate a dataframe with required columns","4d76ab3d":"## This section of the notebook deals with building the bias influential factor classifier","d196dda9":"# Part 1 - Fitment regression","043a6027":"## Reading the data\n","578a496c":"## Test dataset for regression testing","8c98e2be":"# Part 2-Bias classification","964f2ad1":"# Saving the score","566c88d1":"## There are NaN values in the Bias Influential Factor column. But I had some confusion regarding it's relevance, thus I am considering those NaN values as labels. I have started a discussion regarding the same, if some changes are required I'll make them in the notebook after the discussion.","3a5359d5":"From the Heatmap it is clear that Graduation year and age are negatively correlated And Graduatuon year has a negative correlation with Years of experience","94c14292":"## Get the dataset ready","312d7d25":"# Thanks!!","4519f1a0":"## Make the predictions","4df9d64c":"## Label encoding on data with 'object' datatye","7b6b826b":"There are null values in BiasInfluentialFactor in training dataset \nRest all the data is clean","3be97f71":"## Pairplot","d171dc4b":"# Exploratory data analysis","626393d3":"## Let's create a CSV file to store the model predictions in the desired format","c99f9c11":"We can drop columns Graduation year, Current CTC and Years of Experience from our data ","1bbc46b4":"# Gender Neutrality and Inclusion"}}