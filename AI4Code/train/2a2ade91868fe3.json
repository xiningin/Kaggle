{"cell_type":{"7ad65f0e":"code","9c58433c":"code","c080f9a4":"code","a1aee858":"code","02867c56":"code","c65d8574":"code","4464b31a":"code","e13410ea":"code","11119f88":"code","5fd47255":"code","54189b61":"code","8709fe23":"code","2864391b":"code","c81d9164":"code","2479dfce":"code","af32c003":"code","862b2589":"code","c589d91b":"code","f72b4bfa":"code","6e5eb045":"code","f99f06bb":"code","d8ee0d6f":"code","48cdd5c6":"code","65c7c78a":"code","8cec5591":"code","a6dc88ab":"code","5c2ce169":"code","039d343b":"code","f802d420":"code","b5370985":"code","5e5bf2c7":"code","87038177":"code","a4c6fce6":"code","407d6a4b":"code","9e51733c":"code","011bf1b7":"code","d23e5c07":"code","518982f8":"code","96e3b174":"code","58832220":"code","fdc2be8f":"code","ea1f1392":"code","67640c13":"code","c90bfbc4":"code","5caf401d":"code","4a19a039":"code","40303f7c":"code","e6649469":"code","21027bb8":"code","c2df03a3":"code","6469ef6d":"code","3844ef2e":"code","742b50ee":"code","9320f782":"code","d88972a0":"code","eb95d4b3":"code","43b28456":"code","cd1f2599":"code","1399bd01":"markdown","6d0b8b46":"markdown","42c1bf0d":"markdown","948771ea":"markdown","577d7f5c":"markdown","062f4bf9":"markdown","cd8d05b0":"markdown","90e51f65":"markdown","3615043b":"markdown","dedb316b":"markdown","61249efc":"markdown","d387e5fa":"markdown","dab57a49":"markdown","2e7d0e1e":"markdown","d4c157da":"markdown","87c3a3d5":"markdown","36f6b4db":"markdown","c53d3084":"markdown","f823568b":"markdown","fa1273b4":"markdown","7d86c3d1":"markdown","c5d11e41":"markdown","312cff31":"markdown","375e741c":"markdown","77869e38":"markdown","392bff39":"markdown","5d8fe0db":"markdown","ff750fec":"markdown","56fc6336":"markdown","f1d90498":"markdown"},"source":{"7ad65f0e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nsns.set()\nrcParams[\"figure.figsize\"] = (10, 6)","9c58433c":"main_data = pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv\")","c080f9a4":"main_data.shape","a1aee858":"main_data.isna().sum()","02867c56":"main_data.engine.unique()","c65d8574":"main_data.mileage.unique()","4464b31a":"main_data.seats.unique()","e13410ea":"# seats, torque and max_power features are not necessary so let's drop it\n\nmain_data.drop(columns = [\"seats\", \"torque\", \"max_power\"], inplace=True)","11119f88":"# For now let's make NaN value as 0 in mileage and engine column\n\nmain_data.fillna(0, inplace=True)","5fd47255":"# let's see the data where mileage value and engine values are 0\n\nmain_data.loc[main_data.mileage == 0].head(5)","54189b61":"# let's split the numeric values from mileage and engine and convert it into float or int\n\nmain_data.mileage = main_data.mileage.apply(lambda x: float(str(x).split()[0]))\nmain_data.engine = main_data.engine.apply(lambda x: int(str(x).split()[0]))\nmain_data.sample(2)","8709fe23":"# let's separate company and car name from name column\n\nmain_data[\"comp_car_name\"] = main_data.name.apply(lambda x: str(x).split()[0] + \" \" + str(x).split()[1])\nmain_data.sample(2)","2864391b":"# let's check how many records have 0 mileage values\n\nlen(main_data.loc[main_data.mileage == 0])","c81d9164":"# now let's fill the mileage value with mean value of same car name's mileage values wherever it has 0 value\n\ndropable_index = []\nfor i in main_data.loc[main_data.mileage == 0].index:\n    missing_mileage_car_name = main_data.loc[i, \"comp_car_name\"]\n    avg_mileage = main_data.loc[(main_data.name.str.contains(missing_mileage_car_name)) & (main_data.mileage !=0), \"mileage\"].mean()\n    if np.nan is avg_mileage:\n        # here if mean value is nan then it means in our data set there are no other same company name car available\n        # so for that we have one option we have to drop that records\n        dropable_index.append(i)\n    else:\n        main_data.loc[i, \"mileage\"] = avg_mileage\n\n# let's drop those records which i did not able to fill the mileage value because that car name was unique\nmain_data.drop(dropable_index, axis=0, inplace=True)\nmain_data.sample(3)","2479dfce":"# let's see how many records have 0 value in engine column\n\nlen(main_data.loc[main_data.engine == 0])","af32c003":"# now let's fill the engine value with median value of same car name's engine value wherever engine value is 0\n\nfor i in main_data.loc[main_data.engine == 0].index:\n    missing_engine_car_name = main_data.loc[i, \"comp_car_name\"]\n    median_engine = main_data.loc[(main_data.name.str.contains(missing_engine_car_name)) & (main_data.engine != 0), \"engine\"].median()\n    \n    main_data.loc[i, \"engine\"] = median_engine\n\nmain_data.sample(3)","862b2589":"# let's see how many records are remaining with 0 engine value\n\nlen(main_data.loc[main_data.engine == 0])","c589d91b":"# let's find unique values of name columns\n\nmain_data.name.nunique()","f72b4bfa":"# let's find unique values of year columns\n\nmain_data.year.unique()","6e5eb045":"# let's plot it using count plot it see the variation of years\nsns.countplot(main_data.year)\nplt.xticks(rotation=90)\nplt.show()","f99f06bb":"# let's see the data distribution of km_driven\n\nsns.distplot(main_data.km_driven)","d8ee0d6f":"# let's see min and max value\n\nprint(\"Minimum Km Driven:\", main_data.km_driven.min())\nprint(\"Maximum Km Driven:\", main_data.km_driven.max())","48cdd5c6":"# let's see the variation of fuel type\n\nsns.countplot(main_data.fuel)\nplt.title(\"Count of Fuel Type\")\nplt.show()","65c7c78a":"# Let's find avg price of the car according to it's fuel type\n\navg_fuel_selling_price = main_data.groupby(\"fuel\").selling_price.mean().reset_index()\nsns.barplot(avg_fuel_selling_price.fuel, avg_fuel_selling_price.selling_price)\nplt.title(\"Average Selling Price base on Fuel Type\")\nplt.show()","8cec5591":"# let's look at seller_type \n\nsns.countplot(main_data.seller_type)\nplt.title(\"Count of Seller Type\")\nplt.show()","a6dc88ab":"# let's see seller_type feature affect on selling price or not\n\navg_seller_type_selling_price = main_data.groupby(\"seller_type\").selling_price.mean().reset_index()\n\nsns.barplot(avg_seller_type_selling_price.seller_type, avg_seller_type_selling_price.selling_price)\nplt.title(\"Average Selling Price base on Seller Type\")\nplt.show()","5c2ce169":"# let's look at transmission types\n\nsns.countplot(main_data.transmission)\nplt.title(\"Count of Transmission\")\nplt.show()","039d343b":"# let's see the average selling price manual and automatic transmission\n\navg_transmission_selling_price = main_data.groupby(\"transmission\").selling_price.mean().reset_index()\n\nsns.barplot(avg_transmission_selling_price.transmission, avg_transmission_selling_price.selling_price)\nplt.title(\"Average Selling Price base on Transmission\")\nplt.show()","f802d420":"# let's look at owner category\n\nsns.countplot(main_data.owner)\nplt.title(\"Count of Owner\")\nplt.show()","b5370985":"# let's see the average selling price base on owner\n\navg_owner_selling_price = main_data.groupby(\"owner\").selling_price.mean().reset_index()\n\nsns.barplot(avg_owner_selling_price.owner, avg_owner_selling_price.selling_price)\nplt.title(\"Average Selling Price base on Owner\")\nplt.show()","5e5bf2c7":"# Average selling price of owner type test drive car is very high it might be an outlier let's see the data\n# which have owner type is Test Drive Car\n\nmain_data.loc[main_data.owner == \"Test Drive Car\"]","87038177":"main_data.loc[main_data.name == \"Volkswagen Vento 1.5 TDI Highline BSIV\"]","a4c6fce6":"# let's drop records which has Test Drive Car owner type\n\nmain_data.drop(main_data.loc[main_data.owner == \"Test Drive Car\"].index, axis=0, inplace=True)","407d6a4b":"# now again let's see the average selling price base on owner\n\navg_owner_selling_price = main_data.groupby(\"owner\").selling_price.mean().reset_index()\n\nsns.barplot(avg_owner_selling_price.owner, avg_owner_selling_price.selling_price)\nplt.title(\"Average Selling Price base on Owner\")\nplt.show()","9e51733c":"# let's look the data distribution of mileage column\n\nsns.distplot(main_data.mileage)\nplt.show()","011bf1b7":"# let's look at engine\n\nsns.distplot(main_data.engine)\nplt.show()","d23e5c07":"main_data[[\"engine\", \"selling_price\"]].corr()","518982f8":"# let's look at the data distribution of selling_price\n\nsns.distplot(main_data.selling_price)\nplt.show()","96e3b174":"# finding upper and lower limit using percertile \n# here i'm going to use 99.9% for upper limit and 0.1% for lower limit\n\nlower_limit = main_data.selling_price.quantile(0.001)\nupper_limit = main_data.selling_price.quantile(0.999)\n\nprint(\"Lower Limit:\", lower_limit)\nprint(\"Upper Limit:\", upper_limit)","58832220":"# let's see the data which are below the range of lower limit\n\nmain_data.loc[main_data.selling_price < lower_limit]","fdc2be8f":"# let's see the data which are below the limit of upper limit\n\nmain_data.loc[main_data.selling_price > upper_limit]","ea1f1392":"main_data.selling_price.describe()","67640c13":"# let's do target guided label encoding for name columns\n\ncar_name_encoded = main_data.groupby(\"name\").selling_price.mean().sort_values().reset_index().drop(columns=\"selling_price\")\n\n# converting name and index value into dict using zip then map it with origial series\nmain_data.name = main_data.name.map(dict(zip(car_name_encoded.name, car_name_encoded.index)))\nmain_data.sample(3)","c90bfbc4":"# let's do target guided label encoding for fuel type bcz diesel cars has high price compare to other\n\nfuel_encoded = main_data.groupby(\"fuel\").selling_price.mean().sort_values().reset_index().drop(columns=\"selling_price\")\n\n# converting fuel and index value into dict using zip then map it with original series\nmain_data.fuel = main_data.fuel.map(dict(zip(fuel_encoded.fuel, fuel_encoded.index)))\nmain_data.sample(3)","5caf401d":"# let's encode seller_type using target guided encoding\n\nseller_type_encoded = main_data.groupby(\"seller_type\").selling_price.mean().sort_values().reset_index().drop(columns=\"selling_price\")\n\n# creating dict using enumerator and key as count value and value as seller_type_encoded value then map it with original series\nmain_data.seller_type = main_data.seller_type.map({k:v for v, k in enumerate(seller_type_encoded.seller_type, 0)})\nmain_data.sample(3)","4a19a039":"# let's encode transmission using one hot encoding\n# in transmission column we have two values Manual and Automatic\n\ntransmission_encoded = pd.get_dummies(main_data.transmission, prefix=\"transmission\", drop_first=True)\n# here droping first value to prevent dummy variable trap, first value was Automatic so 0 will represent that value\n\n# let's concat encoded transmission series with main_data data frame\n\nmain_data = pd.concat([main_data, transmission_encoded], axis=1)\nmain_data.sample(3)","40303f7c":"# let's split owner value and encode it according to it's number\n\nmain_data.owner = main_data.owner.apply(lambda x: str(x).split()[0])\n\n# now let's do label encoding\n\nmain_data.owner = main_data.owner.map({\"First\": 1, \"Second\": 2, \"Third\": 3, \"Fourth\": 4})\nmain_data.sample(3)","e6649469":"# let's drop transission and comp_car_name and make a final_data data frame\n\nfinal_data = main_data.drop(columns = [\"transmission\", \"comp_car_name\"]).copy()","21027bb8":"final_data.sample(3)","c2df03a3":"# let's create feature matrix X and target vector y\n\nX = final_data.drop(columns=\"selling_price\")\ny = final_data.selling_price","6469ef6d":"sns.distplot(y)","3844ef2e":"# let's split the data into train and test data set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n\n(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)","742b50ee":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\nalgos = {\n    \"Linear Regression\": {\n        \"model\": LinearRegression(),\n        \"params\": {}\n    },\n    \"Ridge\": {\n        \"model\": Ridge(),\n        \"params\": {\n            \"alpha\":np.arange(0, 1, 0.01)\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            \"alpha\": np.arange(0, 1, 0.01)\n        }\n    },\n    \"Decision Tree\": {\n        \"model\": DecisionTreeRegressor(),\n        \"params\": {\n            \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n            \"splitter\": [\"best\", \"random\"],\n            \"max_depth\": [1, 3, 5, 7, 9, 10, 11, 12, 14, 15, 18, 20, 25, 28, 30, 33, 38, 40],\n            \"min_samples_split\": [2, 4, 6, 8, 10, 15, 20],\n            \"min_samples_leaf\": [i for i in range(1, 11)],\n            \"max_leaf_nodes\": [None] + [i for i in range(10, 91, 10)],\n            \"max_features\": [\"auto\", \"log2\", \"sqrt\", None]            \n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": np.linspace(100, 1200, 12).astype(int),\n            \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n            \"max_depth\": [1, 3, 5, 7, 9, 10, 11, 12, 14, 15, 18, 20, 25, 28, 30, 33, 38, 40],\n            \"min_samples_split\": [2, 4, 6, 8, 10, 15, 20],\n            \"min_samples_leaf\": [i for i in range(1, 11)],\n            \"max_leaf_nodes\": [None] + [i for i in range(10, 91, 10)],\n            \"max_features\": [\"auto\", \"log2\", \"sqrt\", None]\n        }\n    },\n    \"Ada Boost\": {\n        \"model\": AdaBoostRegressor(),\n        \"params\": {\n            \"n_estimators\": np.linspace(100, 1200, 12).astype(int),\n            \"learning_rate\": np.arange(0, 1, 0.01),\n            \"loss\": ['linear', 'square', 'exponential']\n        }\n    },\n    \"Gradient Boost\": {\n        \"model\": GradientBoostingRegressor(),\n        \"params\": {\n            \"learning_rate\": np.arange(0, 1, 0.01),\n            \"n_estimators\": np.linspace(100, 1200, 12).astype(int),\n            \"criterion\": ['friedman_mse', 'mse', 'mae'],\n            \"min_samples_split\": [2, 4, 6, 8, 10, 15, 20],\n            \"min_samples_leaf\": [i for i in range(1, 11)],\n            \"max_depth\": [1, 3, 5, 7, 9, 10, 11, 12, 14, 15, 18, 20, 25, 28, 30, 33, 38, 40],\n            \"max_features\": [\"auto\", \"log2\", \"sqrt\", None],\n            \"max_leaf_nodes\": [None] + [i for i in range(10, 91, 10)],\n            \"alpha\": np.arange(0, 1, 0.01)\n        }\n    }\n}","9320f782":"from sklearn.model_selection import RandomizedSearchCV\nimport time\n\nstart_time = time.time()\nbest_model = {}\nbest_model_details = []\n\nfor model_name, values in algos.items():\n    rscv = RandomizedSearchCV(values[\"model\"], values[\"params\"], cv=5, n_iter=10, n_jobs=-1, verbose=2, random_state=4)\n    rscv.fit(X_train, y_train)\n    best_model[model_name] = rscv\n    best_model_details.append({\"Model Name\": model_name, \"Best Score\": rscv.best_score_, \"Best Parameters\": rscv.best_params_})\n\n\nprint(\"--------------------------------------------------------\")\nprint(f\"it takes {(time.time() - start_time) \/ 60} minutes\")\nprint(\"--------------------------------------------------------\")","d88972a0":"pd.set_option('display.max_colwidth', None)\npd.DataFrame(best_model_details)","eb95d4b3":"test_model = []\n\nfor model_name, model in best_model.items():\n    test_model.append({\"Model Name\": model_name, \"Test Score\": model.score(X_test, y_test)})\n\npd.DataFrame(test_model)\n","43b28456":"# Gradient Boost work better here, let's make it as final model\n\nfinal_model = best_model[\"Random Forest\"]\n\n#let's find out mean square error, root mean square error and mean absolute error\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_pred = final_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"Mean Square Error:\", mse)\nprint(\"Root Mean Square Error:\", rmse)\nprint(\"Mean Absolute Error:\", mae)\n","cd1f2599":"import pickle\n# let's save models and appropriate encoding files, it will help me to predict on web app\n\n# let's save encoded data frame as csv files, if save it in json format it will take more memory than csv file\ncar_name_encoded.to_csv(\"car_name_encoded.csv\", index=False)\nfuel_encoded.to_csv(\"fuel_encoded.csv\", index=False)\nseller_type_encoded.to_csv(\"seller_type.csv\", index=False)\n\n# let's save the model into pickle file\nwith open(\"car_price_model.pickle\", \"wb\") as f:\n    pickle.dump(final_model, f)","1399bd01":"### Exploring Years Feature","6d0b8b46":"### Exploring Mileage Feature","42c1bf0d":"### Exploring Transmission Feature","948771ea":"- Here we can see the cars which belongs to Test Drive Car category of owner are unique models\n- Owner Type Test Drive Car are 5 records and 3 unique model so we can easily drop it to remove that outlier","577d7f5c":"## EDA","062f4bf9":"##### Observation:\n- We have cars from year 1983 to 2020","cd8d05b0":"##### Observation:\n- Here selling price data are not normally distributed it's right skewed because most of the car price are high\n- Lower selling price is 29999 of maruti car and Higher price is 10000000 of Volvo car these are normal it's not an outlier","90e51f65":"### Exploring Km Driven Feature","3615043b":"##### Observation:\n- We have more cars from first owner\n- The car selling price is more when owner is first compare to other owner\n- so here owner feature is affect to selling price","dedb316b":"##### Observation:\n- Our Km Driven data is right skewed but it's okay some cars are old so it driven more than lac KM","61249efc":"### Exploring Fuel Feature","d387e5fa":"#### Model training using following algorithms\n- linear regression\n- ridge regression\n- lasso regression\n- decision tree\n- random forest\n- ada boost\n- gradient boost\n- xg boost","dab57a49":"##### Observation:\n- we have 2058 unique cars","2e7d0e1e":"### Exploring Owner Feature","d4c157da":"## <center> Car Valuation Prediction <\/center>\n![car_valuation.png](attachment:d4faf636-4efd-426d-9bd9-ba19b4c5ad19.png)\n- The Car Valuation project is about predicting selling price of used cars\n- In this datset i have 2040 unique cars\n- my model can predict selling price of cars in INR\n- I have used Gradient Boost algorithm gives 97.79% accuracy on training data set and 98.61% accuracy on testing data set, here data set was split on 80:20 ratio\n- Here i have developed end to end application using Flask, Javascript, Bootstrap, CSS and HTML here's the full source code link <br>https:\/\/github.com\/jaysoftic\/car-valuation\n- I have deployed this end to end project on AWS Elastic Beanstalk platform here's the live demo link <br>http:\/\/carvaluationprediction-env-1.eba-pvpbk242.us-east-2.elasticbeanstalk.com\/\n\n- - This notebook is about data analysis, feature engineering and model building","87c3a3d5":"### Exploring Target Feature","36f6b4db":"##### Observation:\n- The data of engine is not normally distributed but engine size is really matter for car\n- We can see engine is highly co-relates with selling price","c53d3084":"### Exploring Seller Type Feature","f823568b":"##### Observation:\n- Here mileage data are noramlly distributed so our regression model will perform better on normal distribution data\n- Mileage feature is important for car so it is affect on selling price","fa1273b4":"##### Observation:\n- Maximum cars are run on Diesel and Petrol fuel and some are on LPG and CNG\n- The cars which has Diesel fuel type it's selling price is high \n- so Fuel type is affect to selling price","7d86c3d1":"##### Observation:\n- Individual Sellers are high than Dealer and Tustmark Delaer\n- Selling Price is high when seller type are Dealer and Trustmark Dealer\n- so Seller Type affect to Selling Price","c5d11e41":"- we can see that Car company and name are same but car version are differnce\n- we can replace that 0 mileage value with mean value of same care name's mileage value\n- we can replace that 0 engine value with median value of same care name's engine value","312cff31":"### Model Building","375e741c":"### Exploring Engine Feature","77869e38":"main_data.sample(5)","392bff39":"- Selling price data are distributed in righ skewed, there may be chance of outliers let's try to find it","5d8fe0db":"## Feature Engineering","ff750fec":"##### Observation:\n- I got around 98% accuracy but RMSE is 105790 and MAE is 49412\n- so on an average i will get 49412 error from this model\n- I got huge error because target value is also high it is around 50-80 lakhs","56fc6336":"##### Observation:\n- We have more cars which have a manual transmission compare to automatic transmission\n- The Selling price of an automatic transmission are high than manual transmission\n- so here transmission types are affect to selling price","f1d90498":"###  Exploring Name Feature"}}