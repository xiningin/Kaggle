{"cell_type":{"5db17796":"code","f14c89e1":"code","2da346ae":"code","e2a6eafd":"code","7d0d08e8":"code","7871394a":"code","47069e77":"code","c9dfd10c":"code","a84979eb":"code","457da2b3":"code","35c67d9f":"code","38213fc2":"code","86d74b6b":"code","9a90f333":"code","be427040":"code","13cbb641":"code","3ae22dce":"code","f699dec0":"code","eb63fd4b":"code","7680a6b7":"code","56ed27c3":"code","5056aa3d":"code","d9e152c6":"code","98225a3b":"code","bdbcb76c":"code","4d078ac7":"code","ad30fea2":"code","cc75c130":"code","764f737c":"code","0f039d75":"code","25326ea9":"code","32ef99a5":"code","da7d2efc":"code","0d1861fd":"code","d6831ef8":"code","a10728c3":"code","462d0387":"markdown","3e5f9cba":"markdown","4a266a92":"markdown","70a9a065":"markdown","228de7b0":"markdown","cb1ab92f":"markdown","cf47d885":"markdown","5ff55dac":"markdown","c6c7ff4d":"markdown","821f4111":"markdown","26ff495d":"markdown","626359e0":"markdown","6b41a516":"markdown","3cd1608b":"markdown"},"source":{"5db17796":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f14c89e1":"!pip install https:\/\/med7.s3.eu-west-2.amazonaws.com\/en_core_med7_lg.tar.gz","2da346ae":"# We read all the medical notes from the directory\ndir = '\/kaggle\/input\/nlp-specialization-data\/Medical_Notes\/Medical_Notes'\nprint (\"Total {} files in directory\".format(len(os.listdir(dir))))","e2a6eafd":"labels = pd.read_csv(\"\/kaggle\/input\/nlp-specialization-data\/Labels_Medical_Notes.csv\",header=None)\nlabels.columns = ['file','label']","7d0d08e8":"labels.head(5)","7871394a":"# read each medical notes and the corresponding label (disease category)\ntexts = []\nclasses = []\n\nfor i in tqdm(range(labels.shape[0])):\n    filename = os.path.join(dir,labels.iloc[i]['file'])\n    text = \" \".join(open(filename,'r',errors='ignore').readlines())\n    texts.append(text)\n    classes.append(labels.iloc[i]['label'])\n    \ndata = pd.DataFrame()\ndata['text'] = texts\ndata['label'] = classes","47069e77":"print (data.shape)","c9dfd10c":"data.head(5)","a84979eb":"sample_text = data.text.iloc[3]\nprint (sample_text)","457da2b3":"import re","35c67d9f":"def remove_html(text):\n    text = text.replace(\"\\n\",\" \")\n    pattern = re.compile('<.*?>') #all the HTML tags\n    return pattern.sub(r'', text)","38213fc2":"sample_text_processed = remove_html(sample_text)\nprint (sample_text_processed)","86d74b6b":"def remove_headings(text):\n    pattern = re.compile('\\w+:')\n    return pattern.sub(r'', text)","9a90f333":"sample_text_processed = remove_headings(sample_text_processed)\nprint (sample_text_processed)","be427040":"def replace_mult_spaces(text):\n    text = text.replace(\"&quot\",\"\")\n    pattern = re.compile(' +')\n    text = pattern.sub(r' ', text)\n    text = text.strip()\n    return text","13cbb641":"sample_text_processed = replace_mult_spaces(sample_text_processed)\nprint (sample_text_processed)","3ae22dce":"def replace_other_chars(text):\n    pattern = re.compile(r'[()!@&;]')\n    text = pattern.sub(r'', text)\n    return text","f699dec0":"sample_text_processed = replace_other_chars(sample_text_processed)\nprint (sample_text_processed)","eb63fd4b":"def clean_text(text):\n    text = remove_html(text)\n    text = remove_headings(text)\n    text = replace_mult_spaces(text)\n    text = replace_other_chars(text)\n    text = text.lower()\n    return text","7680a6b7":"data['clean_text'] = data.text.apply(clean_text)","56ed27c3":"import matplotlib.pyplot as plt\n\ndata.clean_text.apply(len).plot.hist()\ndata.text.apply(len).plot.hist()\nplt.title(\"Distribution of total number of characters in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","5056aa3d":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\ndata.text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","d9e152c6":"sample_text = data.clean_text.iloc[1]\nprint (sample_text)","98225a3b":"import nltk\n\ndef simple_stemmer(text):\n    ps = nltk.stem.SnowballStemmer('english')\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","bdbcb76c":"stemmed_text = simple_stemmer(sample_text)\nprint (stemmed_text)","4d078ac7":"import spacy\nimport en_core_med7_lg #en_core_web_sm\n\nnlp = en_core_med7_lg.load()\n\ndef simple_lemmatizer(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","ad30fea2":"lemmatized_text = simple_lemmatizer(sample_text)\nprint (lemmatized_text)","cc75c130":"sample_text = data.clean_text.iloc[1]\ndoc = nlp(sample_text)\nfor token in doc:\n    print(token.text, token.pos_)","764f737c":"pd.Series(\" \".join(data.clean_text.values).split()).value_counts().head(20)","0f039d75":"stopword_list = nltk.corpus.stopwords.words('english')\n\nprint (stopword_list[:10])","25326ea9":"def lemmatize_and_remove_stopwords(text):\n    doc = nlp(text)\n    tokens = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","32ef99a5":"sample_text","da7d2efc":"sample_text_processed = lemmatize_and_remove_stopwords(sample_text)\nprint (sample_text_processed)","0d1861fd":"data.clean_text = data.clean_text.apply(lemmatize_and_remove_stopwords)","d6831ef8":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the texts\")\nplt.show()","a10728c3":"#data.to_csv(\"clinical_notes_cleaned.csv\",index=False)","462d0387":"We remove all the special characters like - \"\\n\", HTML tags from the texts","3e5f9cba":"<a id='regex'><\/a>\n\n### Basic data cleaning\n\nNatural language in its pure form can bring lot of noise. We need to clean the data in order to use any statistical\/machine learning model. Below are the few techniques for cleaning the text data.\n\n* Using RegEx (regular expressions) to identify the irrelevant text sections for removal\n* Standardizing\/normalizing texts like - abbreviations, spelling mistakes\n* For social media data - remove smileys, email ids if these information are not relevant for downstream analysis\n","4a266a92":"<a id='eda'><\/a>\n\n### Basic descriptive analysis on the texts","70a9a065":"Putting everything together in a function and apply the cleaning on all the texts. Further, convert everything into lower case.","228de7b0":"remove multiple consecutive spaces and replace with single space","cb1ab92f":"## Contents\n\n* [Read the dataset](#datasetreading)\n* [Data Cleaning](#regex)\n* [Stemming & Lemmatization](#stemming)\n* [Tokenization](#tokenization)\n* [Stop word removal](#stopword)","cf47d885":"### References for further reading\n\n<strong> NLP overview - <\/strong> https:\/\/towardsdatascience.com\/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n\n<strong> Regular Expressions - <\/strong> https:\/\/regex101.com\/ \n\n<strong> Spacy - <\/strong> https:\/\/spacy.io\/usage\/spacy-101\n\n<strong> NLTK - <\/strong> https:\/\/www.nltk.org\/book\/\n\n","5ff55dac":"Remove all the headings from text","c6c7ff4d":"Top 10 words based on frequency are english words like - articles, conjuctions, prepositions etc. These words often do not play in significant roles in the downstream applications. We need to remove these words to reduce the model complexity.","821f4111":"Remove &quot marks and other characters. Replace multiple spaces with single space","26ff495d":"<a id='stopword'><\/a>\n\n### Stop word removal\n\nLet us first see the most frequent words in the dataset","626359e0":"<a id='tokenization'><\/a>\n\n### Tokenization\n\nTokenization splits a text into tokens or, words. Typically, words are splitted based on blank spaces. But tokenizations can also split words joined by other characters.","6b41a516":"<a id='datasetreading'><\/a>\n\n### Read the dataset\n\nThe dataset is collected from https:\/\/www.kaggle.com\/c\/medical-notes\/data. It contains 800 anonymised transcribed medical reports with the disease category (specialty). For more information browse original source - https:\/\/www.mtsamples.com\/","3cd1608b":"<a id='stemming'><\/a>\n\n### Stemming and Lemmatization\n\nStemming changes word into its root stem. \n\n<img src = https:\/\/miro.medium.com\/max\/359\/1*l65c30sY9fQsWPKIckqmCQ.png>\n\nHowever, the root stem may not be lexicographically a correct word. Lemmatization on the other hand standardizes a word into its root word. Lemmatization deals with higher level of abstraction.\n\n<img src = https:\/\/devopedia.org\/images\/article\/227\/6785.1570815200.png>\n"}}