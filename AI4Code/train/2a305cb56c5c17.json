{"cell_type":{"72858ef4":"code","39c321ba":"code","9f318f35":"code","71495535":"code","a99e302b":"code","321b90ed":"code","0e6aff15":"code","854fbe6c":"code","33c26fd2":"code","07e83134":"code","643e8db5":"code","e3fc14da":"code","8b8a3753":"code","de78c5a7":"code","a6203fa6":"code","2059a64d":"code","472c43e3":"code","7f88470a":"code","39c5553d":"code","3442847a":"code","f684c0ad":"code","4b7a7a3b":"code","c37d411b":"code","d4378786":"code","fd0700f5":"code","6dde280e":"code","6ff3a888":"code","9f055972":"code","7a387002":"code","bac1302c":"code","c522a5b8":"code","bc1f6859":"code","0dbf02db":"code","4b543d6d":"code","0bd64460":"code","08d8d9ae":"code","3dcae40e":"code","1588c0c9":"code","3bd52f05":"code","34d8e6b6":"code","687f724f":"code","ed38c83b":"code","34f424a0":"code","2b5e146e":"code","6fcb74a3":"code","1f3dc804":"code","8ef4e99c":"code","f1fb573d":"code","d73f05e8":"code","c78de623":"code","3e4d0a43":"code","6fb74821":"code","a978ef9e":"code","d10ad511":"code","863684c4":"code","e3d313ce":"code","8fcc6b4a":"code","e9f81fc4":"code","fac23774":"code","99ede9db":"code","60004dfd":"code","af3c00c3":"code","c6f672eb":"code","f32a9dfd":"code","3cd8339c":"code","15d29ea4":"code","b7b76bdf":"code","f80535da":"code","c8834559":"code","67759ced":"code","a17651c1":"markdown","9cfee261":"markdown","2282c29d":"markdown","a1e8580e":"markdown","7a312a0b":"markdown","17bb2395":"markdown","b0e1f1d1":"markdown","96a2aba8":"markdown","1447f722":"markdown","77b5686b":"markdown","64ae7b19":"markdown","730b65e6":"markdown","81a01fc5":"markdown","45488aab":"markdown","9bd8f578":"markdown","d0bea8c4":"markdown","133beca5":"markdown","f7248095":"markdown","fd9b4a8f":"markdown","5b5c87d8":"markdown","6d5d488c":"markdown","c1eaa629":"markdown","62d14ebe":"markdown"},"source":{"72858ef4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier","39c321ba":"train = pd.read_csv('..\/input\/finance-company-loan-data\/train_ctrUa4K.csv')\ntest = pd.read_csv('..\/input\/finance-company-loan-data\/test_lAUu6dG.csv')\ntrain.head()","9f318f35":"train.head()","71495535":"# Join both the train and test dataset\ntrain['source']='train'\ntest['source']='test'\n\ndataset = pd.concat([train,test], ignore_index = True)\nprint(\"Train dataset shape:\",train.shape)\nprint(\"Test dataset shape:\",test.shape)\nprint(\"Concatenated dataset shape:\",dataset.shape)","a99e302b":"dataset.info()","321b90ed":"dataset.isnull().sum()","0e6aff15":"print(dataset['Gender'].unique())\nprint(dataset['Married'].unique())\nprint(dataset['Dependents'].unique())\nprint(dataset['Self_Employed'].unique())\nprint(dataset['LoanAmount'].unique())\nprint(dataset['Loan_Amount_Term'].unique())\nprint(dataset['Credit_History'].unique())","854fbe6c":"dataset['Gender'].fillna(dataset['Gender'].mode()[0], inplace=True)\ndataset['Married'].fillna(dataset['Married'].mode()[0], inplace=True)\ndataset['Dependents'].fillna(dataset['Dependents'].mode()[0], inplace=True)\ndataset['Self_Employed'].fillna(dataset['Self_Employed'].mode()[0], inplace=True)\ndataset['LoanAmount'].fillna(dataset['LoanAmount'].median(), inplace=True)\ndataset['Loan_Amount_Term'].fillna(dataset['Loan_Amount_Term'].median(), inplace=True)\ndataset['Credit_History'].fillna(dataset['Credit_History'].mode()[0], inplace=True)","33c26fd2":"dataset.isnull().sum()","07e83134":"dataset.info()","643e8db5":"print(len(dataset['Gender'].unique()))\nprint(len(dataset['Married'].unique()))\nprint(len(dataset['Dependents'].unique()))\nprint(len(dataset['Self_Employed'].unique()))\nprint(len(dataset['LoanAmount'].unique()))\nprint(len(dataset['Loan_Amount_Term'].unique()))\nprint(len(dataset['Credit_History'].unique()))\nprint(len(dataset['Loan_ID'].unique()))\nprint(len(dataset['Education'].unique()))\nprint(len(dataset['ApplicantIncome'].unique()))\nprint(len(dataset['CoapplicantIncome'].unique()))\nprint(len(dataset['Property_Area'].unique()))\nprint(len(dataset['source'].unique()))","e3fc14da":"#Divide into test and train:\ntrain = dataset.loc[dataset['source']==\"train\"]\ntest = dataset.loc[dataset['source']==\"test\"]\n#Drop unnecessary columns:\ntest.drop(['source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)","8b8a3753":"train.head()","de78c5a7":"plt.title('Loan Status Bar Plot')\nplt.xlabel('Loan Status Y - Yes or N- No')\nplt.ylabel('Loan Status Count')\n\ntrain['Loan_Status'].value_counts().plot.bar(color=['green', 'red'],edgecolor='blue')","a6203fa6":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\ntrain['Gender'].value_counts(normalize=True).plot.bar(title='Gender')\nplt.subplot(2,2,2)\ntrain['Married'].value_counts(normalize=True).plot.bar(title='Married')\nplt.subplot(2,2,3)\ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(title='Self Employed')\nplt.subplot(2,2,4)\ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title='Credit History')","2059a64d":"fig, ax = plt.subplots(2,4,figsize = (15,15))\nGender = pd.crosstab(train['Gender'],train['Loan_Status'])\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, ax=ax[0,0])\n\nMarried = pd.crosstab(train['Married'],train['Loan_Status'])\nMarried.div(Married.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[0,1])\n\nDependents = pd.crosstab(train['Dependents'],train['Loan_Status'])\nDependents.div(Dependents.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[0,2])\n\nEducation = pd.crosstab(train['Education'],train['Loan_Status'])\nEducation.div(Education.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[0,3])\n\nSelf_Employed = pd.crosstab(train['Self_Employed'],train['Loan_Status'])\nSelf_Employed.div(Self_Employed.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[1,0])\n\nCredit_History = pd.crosstab(train['Credit_History'],train['Loan_Status'])\nCredit_History.div(Credit_History.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[1,1])\n\nProperty_Area = pd.crosstab(train['Property_Area'],train['Loan_Status'])\nProperty_Area.div(Property_Area.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[1,2])","472c43e3":"#cols=['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education','Self_Employed','Property_Area']\n#for label in cols:\n#    dataset[label]=LabelEncoder().fit_transform(dataset[label])\n#dataset.head()","7f88470a":"X=train.drop([\"Loan_Status\",'Loan_ID'],axis=1)\ny=train[\"Loan_Status\"]\n\nX = pd.get_dummies(X,drop_first=True)\nX.head()","39c5553d":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)","3442847a":"logistic_Regression = LogisticRegression(max_iter=1000,random_state=0)\nlogistic_Regression.fit(x_train,y_train)","f684c0ad":"y_pred = logistic_Regression.predict(x_test)","4b7a7a3b":"log = accuracy_score(y_pred,y_test)*100","c37d411b":"print(confusion_matrix(y_pred,y_test))","d4378786":"print(classification_report(y_pred,y_test))","fd0700f5":"knn = KNeighborsClassifier(n_neighbors=200)\nknn.fit(x_train,y_train)","6dde280e":"pred_knn = knn.predict(x_test)","6ff3a888":"KNN = accuracy_score(pred_knn,y_test)*100","9f055972":"print(confusion_matrix(pred_knn,y_test))","7a387002":"print(classification_report(pred_knn,y_test))","bac1302c":"error=[]\nfor i in range(1,50):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred1=knn.predict(x_test)\n    error.append(np.mean(pred1!=y_test))\nprint(error)","c522a5b8":"plt.figure(figsize=(10,6))\nplt.plot(range(1,50),error,color='blue',linestyle='dashed',marker = 'o',markerfacecolor='red',markersize=10)\nplt.title('Error rate vs K value')\nplt.xlabel('k')\nplt.ylabel('error rate')","bc1f6859":"gnb=GaussianNB()\ngnb.fit(x_train,y_train)","0dbf02db":"pred_gnb = gnb.predict(x_test)","4b543d6d":"GNB = accuracy_score(pred_gnb,y_test)*100","0bd64460":"print(confusion_matrix(pred_gnb,y_test))","08d8d9ae":"print(classification_report(pred_gnb,y_test))","3dcae40e":"svc = SVC()\nsvc.fit(x_train,y_train)\npred_svc = svc.predict(x_test)","1588c0c9":"SVC = accuracy_score(pred_svc,y_test)*100","3bd52f05":"print(confusion_matrix(pred_svc,y_test))","34d8e6b6":"print(classification_report(pred_svc,y_test))","687f724f":"dtree_en = DecisionTreeClassifier(criterion='entropy',splitter='random',max_leaf_nodes=5,min_samples_leaf=10,max_depth=3)","ed38c83b":"clf = dtree_en.fit(x_train,y_train)","34f424a0":"pred_dt = clf.predict(x_test)","2b5e146e":"DTREE = accuracy_score(pred_dt,y_test)*100","6fcb74a3":"cm=confusion_matrix(y_test,pred_dt)\nprint(cm)\nprint(classification_report(y_test,pred_dt))","1f3dc804":"dtree = DecisionTreeClassifier(criterion='gini',splitter='random',max_leaf_nodes=5,min_samples_leaf=10,max_depth=5)\ndtree.fit(x_train,y_train)","8ef4e99c":"pred_g = dtree.predict(x_test)","f1fb573d":"DTREE_G = accuracy_score(y_test,pred_g)*100","d73f05e8":"cm=confusion_matrix(y_test,pred_g)\nprint(cm)\nprint(classification_report(y_test,pred_g))","c78de623":"rfc = RandomForestClassifier(criterion='entropy',n_estimators=400)\nrfc.fit(x_train, y_train)","3e4d0a43":"pred_rf= rfc.predict(x_test)","6fb74821":"RFC = accuracy_score(y_test,pred_rf)*100\nRFC","a978ef9e":"print(confusion_matrix(pred_rf,y_test))","d10ad511":"print(classification_report(pred_rf,y_test))","863684c4":"model = DecisionTreeClassifier(criterion='entropy',max_depth=1,random_state=0)\nadaboost = AdaBoostClassifier(n_estimators=80, base_estimator=model,random_state=0)\nadaboost.fit(x_train,y_train)","e3d313ce":"pred = adaboost.predict(x_test)","8fcc6b4a":"ada = accuracy_score(y_test,pred)*100","e9f81fc4":"model_g = DecisionTreeClassifier(criterion='gini',max_depth=1,random_state=0)\nadaboost1 = AdaBoostClassifier(n_estimators=90, base_estimator=model_g,random_state=0)\nadaboost1.fit(x_train,y_train)","fac23774":"pred_gini = adaboost.predict(x_test)","99ede9db":"g = accuracy_score(y_test,pred_gini)*100","60004dfd":"xgb =  XGBClassifier(learning_rate =0.000001,n_estimators=1000,max_depth=5,min_child_weight=1,subsample=0.8,colsample_bytree=0.8,nthread=4,scale_pos_weight=1,seed=27)","af3c00c3":"xgb.fit(x_train, y_train)\npredxg = xgb.predict(x_test)\nxg = accuracy_score(y_test,predxg)*100","c6f672eb":"print(\"1)  Logistic Regression    :\",log)\nprint(\"2)  AdaBoost - Entropy     :\",ada)\nprint(\"3)  AdaBoost - Gini        :\",g)\nprint(\"4)  XGBoost                :\",xg)\nprint(\"5)  Decision Tree - Entropy:\",DTREE)\nprint(\"6)  Decision Tree - Gini   :\",DTREE_G)\nprint(\"7)  Random Forest          :\",RFC)\nprint(\"8)  Naive-Bayes            :\",GNB)\nprint(\"9)  KNN                    :\",KNN)\nprint(\"10) SVC                    :\",SVC)","f32a9dfd":"test.head()","3cd8339c":"Xt = test.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\nXt = pd.get_dummies(Xt,drop_first=True)\n\nXt.head()","15d29ea4":"test_pred = logistic_Regression.predict(Xt)","b7b76bdf":"test[\"Loan_Status\"] = test_pred","f80535da":"test.head()","c8834559":"submission = test[[\"Loan_ID\",\"Loan_Status\"]].copy()","67759ced":"submission.to_csv('testLR.csv')","a17651c1":"# Step 2: Data Visualization\n\n\n<b> Visualizing Loan_Status <\/b>","9cfee261":"# 10) XGBoost ","2282c29d":"# STEP 6: Predicting the values of Loan_Status for the data given in    test_lAUu6dG.csv","a1e8580e":"# 6) Decision Tree - Gini","7a312a0b":"# Importing Libraries","17bb2395":"# 9) AdaBoost (Gini-Decision Tree)","b0e1f1d1":"# STEP 3 : Converting the categorical data into numerical data appropriately\n<b> scikit-learn only accepts numerical variables. Hence, we need to convert all categorical variables into numeric types.\n\n\nI am not using LabelEncoder() because this method proved to give very less Accuracy when compared to get_dummies() method<\/b>","96a2aba8":"# Train\/Test Split","1447f722":"# 7) Random Forest","77b5686b":"# 2) KNN","64ae7b19":"# Predict Loan Eligibility for Dream Housing Finance company\n","730b65e6":"# 3) Naive-Bayes","81a01fc5":"# Applying the Logistic Regression Model to the test dataset","45488aab":"<b> Study of categorical features like Gender, Married, Self_Employed and Credit_History <\/b>","9bd8f578":"# 8) AdaBoost (Entropy-Decision Tree)","d0bea8c4":"<b> Categorical Independent Variable vs Target Variable <\/b>","133beca5":"# 4) SVM","f7248095":"# 5) Decision Tree - Entropy","fd9b4a8f":"# Spliting the train and test set again after replacing all missing values","5b5c87d8":"# b) replacing missing data with substituted values\n<b>Out of the features with missing NaN values, the missing values in 'Loan_Status' are the Values that need to be predicted by our model. Hence, we need not impute the NaN values in Loan_Status column.\n    \n1) Missing values in Integer and Float dtype columns are replaced by their median \n    \n2) Missing values in Object dtype columns are replaced by their mode<\/b>","6d5d488c":"# STEP 1: Data Pre- Processing\n\n# a) Finding the Missing Value Features","c1eaa629":"# Step 5: Deciding Best Fit Model\n\n<b> \n    \n    1)  Logistic Regression    : 83.78 %\n    \n    2)  XGBoost                : 82.70 %\n    \n    3)  Decision Tree - Entropy: 82.70 %\n    \n    4)  Decision Tree - Gini   : 82.70 %\n    \n    5)  Naive-Bayes            : 82.16 %\n    \n    6)  AdaBoost - Entropy     : 81.62 %\n    \n    7)  AdaBoost - Gini        : 81.62 %\n    \n    8)  Random Forest          : 78.37 %\n    \n    9)  KNN                    : 72.43 %\n    \n    10) SVC                    : 72.43 % \n    \nThe best fit model for the given dataset is: <b>LOGISTIC REGRESSION<\/b>","62d14ebe":"# Step 4: Fitting the dataset to various models\n# 1) Logistic Regression"}}