{"cell_type":{"99a20406":"code","a3eb5888":"code","612e5151":"code","a427f56c":"code","856409c4":"code","cd3f0326":"code","a5337cc7":"code","36f4487d":"code","6efbc54a":"code","8570ec3e":"code","15883f8e":"code","e83598ee":"code","65520231":"code","317712c5":"code","10a66a35":"code","de02b587":"code","ab07f553":"code","3aa35b9e":"code","b34e8cc7":"code","bbd4b84c":"code","b995800b":"code","ba0d05fb":"code","fb6af58f":"code","c4d63e19":"code","39b8faab":"code","ddf503ac":"code","01a87268":"code","ca2ef00d":"code","8374418b":"markdown","30ba7626":"markdown","c87ab877":"markdown","53767c79":"markdown","28f7de93":"markdown","60b9604d":"markdown","31eb167d":"markdown","11311c4c":"markdown","5f8519fd":"markdown","0ed44b1b":"markdown","131c41fe":"markdown","e9602b31":"markdown"},"source":{"99a20406":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix","a3eb5888":"INPUT_DIR = '\/kaggle\/input'\nDATA_DIR = os.path.join(INPUT_DIR,\"rsna-pneumonia-detection-challenge\")\nMODEL_RESULTS = os.path.join(INPUT_DIR,\"predict-on-validation\",\"validation_predictions.csv\")","612e5151":"anns = pd.read_csv(os.path.join(DATA_DIR,'stage_1_train_labels.csv'))\nanns.head()","a427f56c":"anns.shape","856409c4":"# I need to group by patient id so that it is easy to join with predictions:\n\ndef gather_gt(patient_df):\n    if np.sum(patient_df['x'].isna()) > 0:\n        return []\n    else:\n        gts = []\n        for index, row in patient_df.iterrows():\n            gts.append({\n                'x':row['x'],\n                'y':row['y'],\n                'width':row['width'],\n                'height':row['height']\n            })\n        return gts\n\ngt_patient = anns.groupby('patientId').apply(gather_gt)\ngt_patient = gt_patient.to_frame(\"gt\").reset_index()\ngt_patient['Target'] = gt_patient['gt'].apply(lambda x: 1 * (x != []))\nlen(gt_patient)","cd3f0326":"results = pd.read_csv(MODEL_RESULTS, header=None, names=['patientId','prediction'])\nresults = results[~results['prediction'].isna()]\n# we will join ground truth only for patients in the results df\ndf = results.merge(gt_patient, on='patientId', how='left')\nprint(df.shape)\ndf.head(n=10)","a5337cc7":"def filter_predictions(min_confidence):\n    def mapper(prediction):\n        exploded = prediction.strip().split(\" \")\n        predictions = [exploded[x:x+5] for x in range(0, len(exploded),5)]\n        filtered_predictions = [p for p in predictions if float(p[0]) >= min_confidence]\n        return \" \".join([\" \".join(p) for p in filtered_predictions])\n    return mapper\ndef minimal_confidence_predictions(bbox_predictions, min_confidence):\n    return bbox_predictions.apply(filter_predictions(min_confidence))\n\ndef rsna_precision(tp, fp, fn):\n    return tp\/(tp+fp+fn)\n\ndef rsna_precision_for(min_conf):\n    # In my results there is no dedicated column for classification prediction -\n    # I will be inferring it from bounding box prediction for various confidence thresholds\n    y_pred = 1 * (minimal_confidence_predictions(df['prediction'], min_conf) != '')\n    tn, fp, fn, tp = confusion_matrix(df['Target'], y_pred).ravel()\n    return rs\n\ndef metrics_for_confidences():\n    for min_conf in [x\/100.0 for x in range(70,100)]:\n        y_pred = 1 * (minimal_confidence_predictions(df['prediction'], min_conf) != '')\n        tn, fp, fn, tp = confusion_matrix(df['Target'], y_pred).ravel()\n        prec = rsna_precision(tp,fp,fn)\n        cnt = tn + fp + fn + tp\n        yield {\"confidence\":min_conf,\"tn\":tn\/cnt,\"fp\":fp\/cnt,\"fn\":fn\/cnt,\"tp\":tp\/cnt,\"prec\":prec}","36f4487d":"metrics = pd.DataFrame(list(metrics_for_confidences()))\nmelted = metrics.melt(id_vars='confidence',value_vars=['tn','fp','fn','tp','prec'])\n\nplt.figure(figsize=(20,10))\n\nsns.lineplot(x='confidence', y='value', hue='variable',data=melted)","6efbc54a":"metrics.iloc[np.argmax(metrics['prec'])]","8570ec3e":"def to_structure(prediction):\n    exploded = prediction.strip().split(\" \")\n    predictions = [exploded[x:x+5] for x in range(0, len(exploded),5)]    \n    return [{'x':float(p[1]), 'y':float(p[2]), 'width': float(p[3]), 'height':float(p[4]), 'confidence':float(p[0])} for p in predictions]\n        \ndf['all_predictions'] = df['prediction'].apply(to_structure)","15883f8e":"# source: https:\/\/www.kaggle.com\/chenyc15\/mean-average-precision-metric\n\n# extended version of metrics per patient giving more information:\n\niouthresholds = np.linspace(0.4,0.75,num=8)\n\n# helper function to calculate IoU\ndef iou(box1, box2):\n    x11, y11, w1, h1 = box1\n    x21, y21, w2, h2 = box2\n    assert w1 * h1 > 0\n    assert w2 * h2 > 0\n    x12, y12 = x11 + w1, y11 + h1\n    x22, y22 = x21 + w2, y21 + h2\n\n    area1, area2 = w1 * h1, w2 * h2\n    xi1, yi1, xi2, yi2 = max([x11, x21]), max([y11, y21]), min([x12, x22]), min([y12, y22])\n    \n    if xi2 <= xi1 or yi2 <= yi1:\n        return 0\n    else:\n        intersect = (xi2-xi1) * (yi2-yi1)\n        union = area1 + area2 - intersect\n        return intersect \/ union\n\ndef map_iou(boxes_true, boxes_pred, scores, thresholds = iouthresholds):\n    \"\"\"\n    Mean average precision at differnet intersection over union (IoU) threshold\n    \n    input:\n        boxes_true: Mx4 numpy array of ground true bounding boxes of one image. \n                    bbox format: (x1, y1, w, h)\n        boxes_pred: Nx4 numpy array of predicted bounding boxes of one image. \n                    bbox format: (x1, y1, w, h)\n        scores:     length N numpy array of scores associated with predicted bboxes\n        thresholds: IoU shresholds to evaluate mean average precision on\n    output: \n        map: mean average precision of the image\n    \"\"\"\n    \n    # According to the introduction, images with no ground truth bboxes will not be \n    # included in the map score unless there is a false positive detection (?)\n    result= {\n        'tp':0,\n        'tn':0,\n        'fp':0,\n        'fn':0,\n        'skipped':0,\n        'predicted_cnt':len(boxes_pred),\n        'gt_cnt':len(boxes_true),\n        'ious':{}\n    }    \n    # return None if both are empty, don't count the image in final evaluation (?)\n    if len(boxes_true) == 0 and len(boxes_pred) == 0:\n        result['skipped'] = 1\n        result['tn'] = 1\n        return result\n    if len(boxes_true) > 0 and len(boxes_pred) == 0:\n        result['prec'] = 0\n        result['fn'] = len(iouthresholds) * (len(boxes_true) - len(boxes_pred))\n        return result\n    if len(boxes_true) == 0 and len(boxes_pred) > 0:\n        result['prec'] = 0\n        result['fp'] = len(iouthresholds) * (len(boxes_pred) - len(boxes_true))\n        return result\n    \n    assert boxes_true.shape[1] == 4 or boxes_pred.shape[1] == 4, \"boxes should be 2D arrays with shape[1]=4\"\n    \n    # I am not doing any sorting just assume that predictions are sorted according to confidence, since I cannot find a way to so\n    if len(boxes_pred):\n        assert len(scores) == len(boxes_pred), \"boxes_pred and scores should be same length\"\n        # sort boxes_pred by scores in decreasing order\n        boxes_pred = boxes_pred[np.argsort(-1 * scores, kind='mergesort'), :]\n    \n    map_total = 0\n    \n    # loop over thresholds\n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n    for t in thresholds:\n        matched_bt = set()\n        tp, fn = 0, 0\n        for i, bt in enumerate(boxes_true):\n            matched = False\n            for j, bp in enumerate(boxes_pred):\n                miou = iou(bt, bp)\n                result['ious'][(i,j)] = miou\n                if miou >= t and not matched and j not in matched_bt:\n                    matched = True\n                    tp += 1 # bt is matched for the first time, count as TP\n                    matched_bt.add(j)                    \n            if not matched:\n                fn += 1 # bt has no match, count as FN\n                \n        fp = len(boxes_pred) - len(matched_bt) # FP is the bp that not matched to any bt\n        m = tp \/ (tp + fn + fp)\n        map_total += m\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    result['prec'] = map_total \/ len(thresholds)\n    result['tp'] = total_tp\n    result['fn'] = total_fn\n    result['fp'] = total_fp\n    \n    return result\n\ndef bbox_to_array(bbox_dict):\n    return [\n                    bbox_dict['x'],\n                    bbox_dict['y'],\n                    bbox_dict['width'],\n                    bbox_dict['height'],\n                ]\n\ndef patient_metrics_off(row):\n    gtboxes = np.array([bbox_to_array(b) for b in row['gt']])\n    predboxes = np.array([bbox_to_array(b) for b in row['predictions']])\n    confidences = np.array([b['confidence'] for b in row['predictions']])\n    return map_iou(gtboxes,predboxes, confidences)","e83598ee":"def filter_by_min_confidence(min_conf):\n    def f(predictions):\n        return [p for p in predictions if p['confidence'] > min_conf]\n    return f\n    \ndef metrics_for_confidences_bbox(df):\n    for min_conf in [x\/100.0 for x in range(70,100)]:\n        df['predictions'] = df['all_predictions'].apply(filter_by_min_confidence(min_conf))\n        patient_metrics_df = df.apply(patient_metrics_off, axis=1,  result_type='expand')\n        tp = np.sum(patient_metrics_df.tp)\n        tn = np.sum(patient_metrics_df.tn)\n        fp = np.sum(patient_metrics_df.fp)\n        fn = np.sum(patient_metrics_df.fn)\n        not_skipped = patient_metrics_df[patient_metrics_df.skipped != 1]\n        prec = np.mean(not_skipped.prec)        \n        cnt = tn + fp + fn + tp\n        yield {\"confidence\":min_conf,\"tn\":tn\/cnt,\"fp\":fp\/cnt,\"fn\":fn\/cnt,\"tp\":tp\/cnt,\"prec\":prec}","65520231":"plt.figure(figsize=(20,10))\n\nmetrics_bbox = pd.DataFrame(list(metrics_for_confidences_bbox(df)))\nmelted_bbox = metrics_bbox.melt(id_vars='confidence',value_vars=['tn','fp','fn','tp','prec'])\n\nsns.lineplot(x='confidence', y='value', hue='variable',data=melted_bbox)","317712c5":"metrics_bbox.iloc[np.argmax(metrics_bbox['prec'])]","10a66a35":"df['predictions'] = df['all_predictions']\npatient_metrics_df = df.join(df.apply(patient_metrics_off, axis=1,  result_type='expand'))","de02b587":"# source: https:\/\/www.kaggle.com\/meaninglesslives\/dataset-visualization-using-opencv\ndef overlay_box(im, box, rgb, stroke=1):\n    \"\"\"\n    Method to overlay single box on image\n\n    \"\"\"\n    # --- Convert coordinates to integers\n    box = [int(b) for b in box]\n    \n    # --- Extract coordinates\n    y1, x1, height, width = box\n    y2 = y1 + height\n    x2 = x1 + width\n\n    im[y1:y1 + stroke, x1:x2] = rgb\n    im[y2:y2 + stroke, x1:x2] = rgb\n    im[y1:y2, x1:x1 + stroke] = rgb\n    im[y1:y2, x2:x2 + stroke] = rgb\n\n    return im\n\n\nimport cv2\nimport pydicom\nfrom IPython.display import display, Image\n\n\ndef cvshow(image, format='.png', rate=255 ):\n    decoded_bytes = cv2.imencode(format, image*rate)[1].tobytes()\n    display(Image(data=decoded_bytes))\n    return\n\ndef visualize(df, patientId):\n    dcm_file = '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/%s.dcm' % patientId\n    dcm_data = pydicom.read_file(dcm_file)\n    img = dcm_data.pixel_array\n    img = np.stack([img] * 3, axis=2)\n    \n    def show_boxes(img,boxes,rgb):\n        for box in boxes:\n            #y,x,h,w\n            box = [box['y'],box['x'],box['height'],box['width']]                \n            overlay_box(img, box, rgb)\n            \n    gt_boxes = df[df['patientId'] == patientId]['gt'].values[0]\n    gt_col = np.array([1,250,1])  \n    pred_boxes = df[df['patientId'] == patientId]['predictions'].values[0]    \n    pred_col = np.array([1,1,250])  \n    \n    show_boxes(img,gt_boxes,gt_col)\n    show_boxes(img,pred_boxes,pred_col)\n    \n    ious = df[df['patientId'] == patientId]['ious'].values[0]\n    offsets = dict([(i,2) for i in range(len(pred_boxes))])\n    for box in pred_boxes:\n        confidence = box['confidence']        \n        img = cv2.putText(img=np.copy(img), text=\"conf = {:.2f}\".format(confidence), org=(int(box['x'])+5,int(box['y']) + 15),\n                          fontFace=1, fontScale=1, color=(0,0,255), thickness=1)\n    for k,iou in ious.items():     \n        if iou > 0.0:            \n            box = pred_boxes[k[1]]            \n            img = cv2.putText(img=np.copy(img), text=\"iou = {:.2f}\".format(iou), org=(int(box['x'])+5,int(box['y']) + 15 * offsets[k[1]]),\n                          fontFace=1, fontScale=1, color=(0,0,255), thickness=1)\n            offsets[k[1]]+=1\n    prec = df[df['patientId'] == patientId]['prec'].values[0]\n    img = cv2.putText(img=np.copy(img), text=\"precision = {:.2f}\".format(prec), org=(10,300),\n                          fontFace=1, fontScale=1, color=(0,0,255), thickness=1)\n    cvshow(img)","ab07f553":"patient_metrics_df_3 = patient_metrics_df[patient_metrics_df['gt_cnt'] == 3]\npatient_metrics_df_2 = patient_metrics_df[patient_metrics_df['gt_cnt'] == 2]\npatient_metrics_df_1 = patient_metrics_df[patient_metrics_df['gt_cnt'] == 1]\npatient_metrics_df_0 = patient_metrics_df[patient_metrics_df['gt_cnt'] == 0]","3aa35b9e":"fns = np.hstack([patient_metrics_df_3.sort_values('fn', ascending=False).patientId.values[:2],\npatient_metrics_df_2.sort_values('fn', ascending=False).patientId.values[:2],\npatient_metrics_df_1.sort_values('fn', ascending=False).patientId.values[:2]])\n\nfor pid in fns:\n    visualize(patient_metrics_df, pid)","b34e8cc7":"fps = np.hstack([patient_metrics_df_3.sort_values('fp', ascending=False).patientId.values[:2],\npatient_metrics_df_2.sort_values('fp', ascending=False).patientId.values[:2],\npatient_metrics_df_1.sort_values('fp', ascending=False).patientId.values[:2]])\n\nfor pid in fps:\n    visualize(patient_metrics_df, pid)\n","bbd4b84c":"best_prec = np.hstack([patient_metrics_df_3.sort_values('prec', ascending=False).patientId.values[:2],\npatient_metrics_df_2.sort_values('prec', ascending=False).patientId.values[:2],\npatient_metrics_df_1.sort_values('prec', ascending=False).patientId.values[:2]])\n\nfor pid in best_prec:\n    visualize(patient_metrics_df, pid)","b995800b":"high_conf_df = df.copy()","ba0d05fb":"high_conf_df['predictions'] = df['all_predictions'].apply(filter_by_min_confidence(0.98))\nhigh_conf = high_conf_df.join(high_conf_df.apply(patient_metrics_off, axis=1, result_type='expand'))","fb6af58f":"high_conf_low_prec = high_conf.sort_values('fp', ascending=False).patientId.values[:6]\n# high_conf.sort_values('fp', ascending=False).head()\nfor pid in high_conf_low_prec:\n    visualize(high_conf, pid)","c4d63e19":"def area(bbox):\n    return bbox['width'] * bbox['height']    \n    \ndef min_bbox_size(bboxes):\n    if len(bboxes)==0:\n        return 0\n    else:\n        return np.min([area(bbox) for bbox in bboxes])\n\ndef max_bbox_size(bboxes):\n    if len(bboxes)==0:\n        return 0\n    else:\n        return np.max([area(bbox) for bbox in bboxes])\n\npatient_metrics_df['min_pred_size'] = patient_metrics_df['all_predictions'].apply(min_bbox_size)\npatient_metrics_df['max_pred_size'] = patient_metrics_df['all_predictions'].apply(max_bbox_size)\npatient_metrics_df['min_gt_size'] = patient_metrics_df['gt'].apply(min_bbox_size)\npatient_metrics_df['max_gt_size'] = patient_metrics_df['gt'].apply(max_bbox_size)","39b8faab":"smallest_pred = patient_metrics_df[patient_metrics_df['min_pred_size'] > 0].sort_values('min_pred_size', ascending=True).patientId.values[:4]\n\nfor pid in smallest_pred:\n    visualize(patient_metrics_df, pid)","ddf503ac":"smallest_gt = patient_metrics_df[patient_metrics_df['min_gt_size'] > 0].sort_values('min_gt_size', ascending=True).patientId.values[:4]\n\nfor pid in smallest_gt:\n    visualize(patient_metrics_df, pid)","01a87268":"largest_pred = patient_metrics_df.sort_values('max_pred_size', ascending=False).patientId.values[:4]\n\nfor pid in largest_pred:\n    visualize(patient_metrics_df, pid)","ca2ef00d":"largest_gt = patient_metrics_df.sort_values('max_gt_size', ascending=False).patientId.values[:4]\n\nfor pid in largest_gt:\n    visualize(patient_metrics_df, pid)","8374418b":"## smallest predicted","30ba7626":"## explore the worst examples at high confidence","c87ab877":"# 2. False Positives - detecting bounding boxes for no real GT:","53767c79":"# In this case I didn't find any significant patterns by looking at those images. For me it is hard to say whether this is indeed a model failure or an annotation misalignment","28f7de93":"# Let's plot Precision and other confusion matrix errors as a function of confidence threshold","60b9604d":"# Let's investigate the biggest errors:\n\n# 1. purple boxes are ground true.\n# 2. light blue are predictions with iou presented\n\n### False Negatives - not detecting bounding box at all","31eb167d":"## explore the smallest\/largest gt\/predicted boxes ","11311c4c":"### in my case the best performance in terns of precision is achieved with confidence >= 0.96.\n\n# Now let's calculate the real metric including bounding box IoU for every confidence threshold","5f8519fd":"I want to analyze model results on part of the training set (usually this will be validation set)\n\nLet's verify and inspect its performance in ways that are criticall in terms of [evaluation metric](https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge#evaluation):\n\n# 1. Classification errors\n Classification errors contribute to the error the most.\n False positive and false negative errors gives one an additional 1 in the metric denominator.\n True negatives are just ignored while true positives can have a precision from 0 to 1 depending on the IoU error.\n In this part I will assume that I only have a classification model and look at the model results as if the bounding boxes for correctly classified images were perfect.\n \n# 2. bounding box errors\nHere I want to calculate what I believe is the real competition metric and visualize it for various confidence thresholds.\nFurthermore I want to visualize those that are the most incorrect, hoping it will give me some insight on model weak points.\n\n","0ed44b1b":"## largest predicted","131c41fe":"# Examples with largest precision:\n","e9602b31":"## smallest gt"}}