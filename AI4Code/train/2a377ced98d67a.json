{"cell_type":{"e35216fe":"code","596ac9b7":"code","108006e8":"code","b74bf8cd":"code","76e435dd":"code","9064d193":"code","8636fae4":"code","cf32313d":"code","ab41dc2a":"code","1fce2da9":"code","beac00c0":"code","863ab722":"code","8cc83a08":"code","979e7bdf":"code","3dedb004":"code","991f6820":"code","302300d7":"code","03837854":"code","c1a542f9":"code","08b299b9":"code","f1bb59bd":"code","ec8c7b5b":"code","02540b0f":"markdown","ea5c316e":"markdown","b2f3b0c0":"markdown","ffaee4e7":"markdown","ac7c34bf":"markdown","9180e527":"markdown","09c12904":"markdown","078fdf89":"markdown","98d6792e":"markdown","12372251":"markdown","5c881456":"markdown","9514ea04":"markdown","8f395a62":"markdown","d49ec5b7":"markdown","ccd522b2":"markdown","3d3c1bfe":"markdown","0ea08aa2":"markdown","dafcaf8b":"markdown","378e0915":"markdown","7f84d28e":"markdown","ded244ae":"markdown","b6a4b23e":"markdown","5e9d78f3":"markdown","aaaf7e61":"markdown","77ee7390":"markdown","7ed139ac":"markdown"},"source":{"e35216fe":"# System\nimport os\nimport sys\n\n# Numerical\nimport numpy as np\nfrom numpy import median\nimport pandas as pd\n\n\n# NLP\nimport re\n\n# Tools\nimport itertools\n\n# Machine Learning - Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning - Model Selection\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Machine Learning - Models\nfrom sklearn import svm, linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n# Machine Learning - Evaluation\nfrom sklearn import metrics \nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(os.listdir(\"..\/input\"))","596ac9b7":"df1 = pd.read_csv('..\/input\/Admission_Predict.csv', sep='\\s*,\\s*', header=0, encoding='ascii', engine='python')\ndf2 = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv', sep='\\s*,\\s*', header=0, encoding='ascii', engine='python')","108006e8":"df = df2\ndf.head()","b74bf8cd":"df.describe()","76e435dd":"def get_plt_params():\n    params = {'legend.fontsize': 'x-large',\n              'figure.figsize' : (18, 8),\n              'axes.labelsize' : 'x-large',\n              'axes.titlesize' : 'x-large',\n              'xtick.labelsize': 'x-large',\n              'ytick.labelsize': 'x-large',\n              'font.size'      :  10}\n    return params","9064d193":"all_columns = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research', 'Chance of Admit']\ncolumns = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research']\ntarget = 'Chance of Admit'","8636fae4":"df['CGPA'] = df['CGPA'].apply(lambda x: ((x\/10)*4))","cf32313d":"# df.describe().plot(table = True)\n# plt.title(\"General Statistics of Admissions\")","ab41dc2a":"# #https:\/\/www.kaggle.com\/biphili\/university-admission-in-era-of-nano-degrees\n\n# f,ax=plt.subplots(1, 2, figsize=(18,6))\n# df['Research'].value_counts().plot.pie(ax=ax[0],shadow=True)\n# ax[0].set_title('Students Research')\n# ax[0].set_ylabel('Student Count')\n# sns.countplot('Research',data=df,ax=ax[1])\n# ax[1].set_title('Students Research')\n# plt.show()","1fce2da9":"fig = plt.figure(figsize=(22, 10))\n\nparams = get_plt_params()\nplt.rcParams.update(params)\n    \nfig.subplots_adjust(hspace=1, wspace=.5)\n\n\nfor i in range(len(columns)-1):\n    plt.subplot(2, 3, i+1)\n    plt.title(\"Distribution\/ Spread\")\n    val = df[columns[i]]\n    ax = sns.boxplot(x=val)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    ax.set_title('Feature Value Spread')\n\nplt.tight_layout()   ","beac00c0":"print(\"\\n\")\nfor i in range(len(columns)-1):\n    print(\" \" + \"*\"*50)\n    val = df[columns[i]]\n    print(\" \" + columns[i])\n    print(\" \" + \"*\"*50)\n    print(\" Minimum                        :{:.2f}\".format(val.min()))\n    print(\" Maximum                        :{:.2f}\".format(val.max()))\n    print(\" Percentile(25%)                :{:.2f}\".format(val.quantile(0.25)))\n    print(\" Percentile(75%)                :{:.2f}\".format(val.quantile(0.75)))\n    print(\" Percentile(50%)\/ Median        :{:.2f}\".format(val.quantile(0.55)))\n    print(\" Mean                           :{:.2f}\".format(val.mean()))\n    print(\" Standard Deviation             :{:.2f}\".format(val.std()))\n    print(\" \" + \"-\"*50)\n    print(\"\\n\")\n    ","863ab722":"fig = plt.figure(figsize=(18, 9))\n\nparams = get_plt_params()\nplt.rcParams.update(params)\n\nfig.subplots_adjust(hspace=0.5)\n\nfor i in range(len(columns)-1):\n    plt.subplot(2, 3, i+1)\n    sns.lineplot(x=columns[i], y=target, data=df)\nplt.tight_layout()   \nplt.plot()","8cc83a08":"fig = plt.figure(figsize=(18, 12))\nfig.subplots_adjust(hspace=0.6)\n\nparams = get_plt_params()\nplt.rcParams.update(params)\n\nfor i in range(len(columns)-1):\n    plt.subplot(3, 3, i+1)\n    sns.regplot(x=columns[i], y=target, data=df)\nplt.tight_layout()   \nplt.plot()","979e7bdf":"fig = plt.figure(figsize=(18, 12))\n\nparams = get_plt_params()\nplt.rcParams.update(params)\n\ng = sns.PairGrid(df[all_columns])\ng.map(sns.lineplot);","3dedb004":"sns.set(style=\"white\")\nfig = plt.figure(figsize=(18, 12))\n\nd = df[all_columns]\n\nparams = get_plt_params()\nplt.rcParams.update(params)\n\n\ncorr = d.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(len(all_columns)*2, len(all_columns)*2))\n\ncmap = sns.diverging_palette(h_neg=220, h_pos=0, s=75, l=50, sep=10, n=len(all_columns), center='light', as_cmap=True)\n\nax = sns.heatmap(\n    corr,\n    cmap=cmap,\n    center=0,\n    robust=True,\n    annot=True,\n    linewidths=0.5,\n    linecolor='white',\n    cbar=True,\n    cbar_kws={\"shrink\": .5},\n    square=True,\n    mask=mask)\n\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)","991f6820":"def print_performance(model, X_test, y_test):\n    preds = model.predict(X_test)\n\n    explained_variance_score = metrics.explained_variance_score(y_test, preds)\n    mean_absolute_error = metrics.mean_absolute_error(y_test, preds)\n    mean_squared_log_error = metrics.mean_squared_log_error(y_test, preds)\n    median_absolute_error = metrics.median_absolute_error(y_test, preds)\n    r2_score = metrics.r2_score(y_test, preds)\n\n    print(\" \" + \"-\"*55)\n    print(\" Performance\")\n    print(\" \" + \"-\"*55)\n    print(\" {} : {:.4f} \".format(\"Explained Variance Score \", explained_variance_score))\n    print(\" {} : {:.4f} \".format(\"Mean Absolute Error      \", mean_absolute_error))\n    print(\" {} : {:.4f} \".format(\"Mean Squared Error       \", mean_squared_log_error))\n    print(\" {} : {:.4f} \".format(\"Median Squared Error     \", median_absolute_error))\n    print(\" {} : {:.4f} \".format(\"R2 Score                 \", r2_score))\n    print(\" \" + \"-\"*55)\n    print(\"\\n\\n\")\n    \n    return preds","302300d7":"X = df[columns].values\ny = df[target].values\n\n# split the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Standardize the features\n# scaler = StandardScaler()\nscaler = preprocessing.MinMaxScaler()\nscaler = scaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","03837854":"print(\"Training\")\nmodel = LinearRegression(normalize=True)\n# model = svm.SVR(kernel='linear')\n# model = linear_model.Ridge()\n# model = BaggingRegressor(model, n_estimators=100)\n\nmodel.fit(X_train, y_train)","c1a542f9":"print(\"\\n Model Score: {:.2f}%\\n\".format(model.score(X_test, y_test)*100))\n\npreds = print_performance(model, X_test, y_test)","08b299b9":"diff = y_test-preds\n\nprint(\"Mean difference in prediction  : {:0.4f}\".format(diff.mean()))\nprint(\"Median difference in prediction: {:0.4f}\".format(median(diff)))\n\nsns.lineplot(data=preds)\nsns.lineplot(data=y_test)\nplt.title(\"Actual vs. Estimated Admision Chance\")\nplt.show()\nsns.lineplot(data=diff)\nplt.title(\"Difference in estimation of admision chance\")\nplt.show()","f1bb59bd":"GRE_Score = 313\nTOEFL_Score = 102\nUniversity_Rating = 5\nSOP = 3\nLOR = 3\nCGPA = 3.80\nResearch = 1\n\nsample1 = [GRE_Score, TOEFL_Score, University_Rating, SOP, LOR, CGPA, Research]\nsample1 = np.array(sample1).reshape(1, -1)\n\nsample1 = scaler.transform(sample1)\n\nprobab = model.predict(sample1)\nprint(\"Chance of admission: {:.2f}%\".format(probab[0]*100))","ec8c7b5b":"# # ! pip install ipywidgets\n# from ipywidgets import widgets\n# from ipywidgets import interact, interactive, fixed, interact_manual\n\n\n# while(True):\n#     print(\"\\n\\nPlease Enter -1 to Exit or Enter to Continue: \")\n#     b = input()\n#     if b=='-1':\n#         print(\"Exiting Admission Calcultor\")\n#         break\n#     print(\"\\nPlease Enter GRE Score: \")\n#     GRE_Score = float(input())\n#     print(\"\\nPlease Enter TOEFL Score: \")\n#     TOEFL_Score = float(input()) \n#     print(\"\\nPlease Enter University Rating:\")\n#     University_Rating = float(input())  \n#     print(\"\\nPlease Enter SOP: \")\n#     SOP = float(input()) \n#     print(\"\\nPlease Enter LOR: \")\n#     LOR = float(input())  \n#     print(\"\\nPlease Enter CGPA: \")\n#     CGPA = float(input())  \n#     print(\"\\nPlease Enter Research: \")\n#     Research = float(input())  \n\n#     sample2 = [GRE_Score, TOEFL_Score, University_Rating, SOP, LOR, CGPA, Research]\n#     print(sample2)\n#     sample2 = np.array(sample1).reshape(1, -1)\n\n#     sample2 = scaler.transform(sample2)\n#     print(sample2)\n#     probab2 = model.predict(sample2)\n#     print(\"Chance of admission: {:.2f}%\".format(probab2[0]*100))\n    ","02540b0f":"### Note: In above Lineplot, it shows linear relation between all the features and target and indicate positive correlation between them.\n***With increase of all the feature  like GRE, TOEFL, SOP AND LOR, chance of admission increases***","ea5c316e":"## 3. Visualization","b2f3b0c0":"### 3.2. Boxplot for showing distribution and spread of all features","ffaee4e7":"#### Note: All the features are highly correleated","ac7c34bf":"# Exploratry Data Analysis and Regression Model for Admission Chance Prediction","9180e527":"![](https:\/\/www.stoodnt.com\/blog\/wp-content\/uploads\/2018\/07\/US-Universities-1024x443.jpg)\n\nCredit: https:\/\/www.stoodnt.com\/blog\/fall-2019-application-deadlines-and-gre-requirements-for-ms-in-us\/","09c12904":"* The Explained variance regression score is around 0.8167, the best score is 1, lower values are worse.\n* Mean Absolute Error in 0.0431 in range of 1, around 0 is best, this was pretty close.\n* The lower the Mean Squared Error and close to 0, the better, this model attained around 0.0013\n* Median Squared Error is around 0.0313.\n* R2 Score close to 1 is best, here this model attained 0.8163.","078fdf89":"### 3.1. Preprocessing","98d6792e":"#### Note: all the features plotted above seem to be positively correlated to each others","12372251":"## 1. Import","5c881456":"Feature scaling helps improve model performance by reducing feature variance. Features with larger values can dominate other features in the dataset and reduce importance of other features in model building. By using feature scaling this feature space can be reduced. This helps reduce training time and helps model perform better. There are couple of feature scaling techniques including normalization and standardization. Here I have used standardization \n\nMinMax Scaler transforms features by scaling each feature to a given range. Specifically, the estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by: \n```math\nX_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min)\n```","9514ea04":"### 3.6. Correlation among features","8f395a62":"## 7. Interactive application for admission likelihood estimation","d49ec5b7":"## 5. Preprocessing ","ccd522b2":"### 3.5. Lineplot for showing relations among all features","3d3c1bfe":"### 3.3. Lineplot for showing relation between all features and target","0ea08aa2":"### 3.4. Regplot for showing relation between all features and target","dafcaf8b":"### 2.1. Show input file stats","378e0915":"As all the features are positively correleated with target, here simple linear regression is experimented.","7f84d28e":"## 2. Read Data","ded244ae":"Selecting latest version of dataset","b6a4b23e":"## 6. Training ","5e9d78f3":"* GRE Score ranged from 290 to 340 with 50th percentile lies in 308-325 region and median is around 318.\n* TOEFL Score ranged from 92 to 120 with 50th percentile lies in 103-112 region and median is around 108.\n* University rating ranged from 1 to 5 and most 50th percentile lies in 2-4 and median is around 3.\n* SOP was evaluated in range 1-5 with most SOP in 2.5-4 range score and median lies in 3.5.\n* Letter of Recommendation (LOR) was scored in range of 1-5 with most LOR in range 3-4 and most median is around 3.5.","aaaf7e61":"## 4. Predict and Evaluation Function","77ee7390":"## 7. Test and Evaluate","7ed139ac":"### 5.1. Test sample case"}}