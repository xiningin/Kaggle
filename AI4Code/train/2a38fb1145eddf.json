{"cell_type":{"4673decf":"code","a1fbad90":"code","0740dc5b":"code","6bd3c220":"code","7d0468bc":"code","8b268e53":"code","728172ce":"code","1de8e8f8":"code","d746eceb":"code","c9c3b0ad":"code","9b45626f":"code","31077ef8":"code","6c6546e3":"code","c82f6529":"code","bff98d98":"code","169b2ce0":"code","ef6f0478":"code","ac8d45d7":"code","2dc63d29":"code","279ad906":"code","cd966f3c":"code","0d08d8ad":"code","e10c4cea":"code","55b483fb":"code","ce681e54":"code","213f1667":"code","ed24877c":"code","a848ddb5":"code","fd584c78":"code","7e9473dc":"code","60986a5c":"code","8db03059":"code","335d1ace":"code","4fdf525c":"code","18979f59":"code","496e062f":"code","a98c5d2b":"code","28971c93":"code","06e53373":"markdown","273ce93f":"markdown","49c126db":"markdown","a3b2a0e6":"markdown","22697130":"markdown","5316ee01":"markdown","9d248fb9":"markdown","89733c56":"markdown","d556bfb7":"markdown"},"source":{"4673decf":"import pandas as pd\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","a1fbad90":"df.shape","0740dc5b":"df.head()","6bd3c220":"df.isnull().sum()","7d0468bc":"df.describe()","8b268e53":"df.var()","728172ce":"from sklearn.preprocessing import StandardScaler","1de8e8f8":"df['Class'].unique()","d746eceb":"df.dropna(inplace =True)","c9c3b0ad":"df['Class'].unique()","9b45626f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrelation = df.corr()\nsns.heatmap(correlation,cmap='magma',linecolor='white')","31077ef8":"sns.countplot(x='Class',data=df,palette='GnBu')","6c6546e3":"sns.boxplot(x=df['Time'])","c82f6529":"sns.lmplot(x='Amount',y='V25', data= df, hue= 'Class',palette= 'Set1')","bff98d98":"cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nsns.scatterplot(x='Amount',y='V1',data= df,hue='Class',palette='PuRd',alpha =0.2)","169b2ce0":"sns.barplot(x='Class',y='Amount',data=df,palette='PuRd')","ef6f0478":"sns.barplot(x='Class',y='Time',data=df,palette='Purples')","ac8d45d7":"sns.stripplot(x=\"Class\", y=\"Time\", data=df , palette= \"Purples\")","2dc63d29":"import numpy as np\nfrom sklearn.decomposition import PCA\nX = df.drop(columns='Class')\ny = df['Class']","279ad906":"X.head()","cd966f3c":"df.shape","0d08d8ad":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n","e10c4cea":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nscaler.transform(X_train)","55b483fb":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred = clf.predict(X_test)","ce681e54":"clf.score(X_test,y_test)","213f1667":"clf.score(X_train,y_train)","ed24877c":" from sklearn.metrics import classification_report\n target_names = ['not_fraud', 'fraud']\nprint(classification_report(y_test, y_pred, target_names=target_names))","a848ddb5":"from sklearn.metrics import log_loss\nlog_loss(y_test,y_pred)","fd584c78":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=42)\ndtc.fit(X_train,y_train)\ny_pred = dtc.predict(X_test)\nprint(\"Test accuracy\",dtc.score(X_test,y_test))\nprint(\"Train accuracy\",dtc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_test,y_pred, target_names=target_names))","7e9473dc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nrfc = RandomForestClassifier(max_depth=2, random_state=42,n_estimators=10)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(\"Test accuracy\",rfc.score(X_test,y_test))\nprint(\"Train accuracy\",rfc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_test,y_pred, target_names=target_names))","60986a5c":"from sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nsvc = make_pipeline(StandardScaler(), SVC(gamma='auto',random_state=42,kernel='sigmoid'))\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint(\"Test accuracy\",svc.score(X_test,y_test))\nprint(\"Train accuracy\",svc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_pred,y_test, target_names=target_names))","8db03059":"#Using neural networks \nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense,Activation,Flatten\nfrom tensorflow.keras import Sequential","335d1ace":"model = Sequential()\nmodel.add(Flatten())\nmodel.add(Dense(2,activation='relu'))\nmodel.add(Dense(2,activation='relu'))\nmodel.add(Dense(2,activation='relu'))\nmodel.add(Dense(2,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\nearlystop = EarlyStopping(monitor='val_loss',patience=2,verbose=0,mode='min')\n\nmodel.compile(optimizer='adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n\nmodel.fit(X_train,y_train,epochs =10,validation_data = (X_test,y_test),callbacks = [earlystop])\n","4fdf525c":"loss = pd.DataFrame(model.history.history)\nloss.plot()","18979f59":"#Since the data is imbalanced ,Resampling :\n \n#Oversampling \n \nfrom sklearn.utils import resample\n\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.Class.value_counts()","496e062f":"\n# trying logistic regression again with the balanced dataset\ny_train = upsampled.Class\nX_train = upsampled.drop('Class', axis=1)\n\nrfc = LogisticRegression()\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(\"Test accuracy\",rfc.score(X_test,y_test))\nprint(\"Train accuracy\",rfc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_test,y_pred, target_names=target_names))","a98c5d2b":"# still using our separated classes fraud and not_fraud from above\n\n# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\ny_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nrfc = RandomForestClassifier(max_depth=2, random_state=42,n_estimators=10)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(\"Test accuracy\",rfc.score(X_test,y_test))\nprint(\"Train accuracy\",rfc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_test,y_pred, target_names=target_names))","28971c93":"from imblearn.over_sampling import SMOTE\n\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_sample(X_train, y_train)\n\n\nrfc = RandomForestClassifier(max_depth=2, random_state=42,n_estimators=10)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(\"Test accuracy\",rfc.score(X_test,y_test))\nprint(\"Train accuracy\",rfc.score(X_train,y_train))\nprint(\"Classification report :\")\nprint(classification_report(y_test,y_pred, target_names=target_names))","06e53373":"# Let's try after resampling data \n# Oversampling the data : \n\n","273ce93f":"<a href=\"https:\/\/colab.research.google.com\/github\/Apurva-tech\/Credit-Card-Fraud-Detection-Dataset-\/blob\/master\/Credit_card_dataset.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","49c126db":"df.info()","a3b2a0e6":"# **Undersampling the data:**","22697130":"# **Generate synthetic samples using SMOTE**","5316ee01":"The data is highly imbalanced. Hence,we train our model before and after resampling .\n\n# Imbalanced Data : ","9d248fb9":"# **Neural Networks**","89733c56":"## **Exploratory Data Analysis :**","d556bfb7":"'''pca = PCA(n_components=2)\npca.fit_transform(X)'''"}}