{"cell_type":{"4d050efd":"code","433e68c0":"code","c1a5baea":"code","2e0e9167":"code","4c19e409":"code","fdc6d489":"code","d60cd9a6":"code","a53d926c":"code","827df7e6":"code","ca2b26a1":"code","df7dbfb9":"code","be977cc1":"code","30f1494b":"code","28781c66":"code","1609d1c5":"code","7bf0851d":"code","05232ed3":"code","6f4887f6":"code","0b4e7316":"code","cb74cbc1":"code","f3da95ba":"code","f3d891ac":"code","4f30615f":"code","65c055fe":"code","61d34e5c":"code","350c7a4e":"code","e03e5c89":"code","a32fcf6f":"code","6be7f1a8":"code","e169d06b":"code","8a44fe8b":"code","a5a92196":"code","052188f6":"code","ca5ff5d6":"code","ddc68ae7":"code","b04fcd70":"code","e6163fcc":"code","35a4e922":"code","67236ea9":"code","2e4a372c":"code","49430615":"code","db96495c":"code","8cba1015":"code","f61ba330":"code","f26252ae":"code","d12a255c":"code","d23919ad":"code","25ba2b2f":"code","b1a3d304":"code","6e33f68b":"code","9d9502b9":"code","2a0893cb":"code","d2345404":"code","396a1949":"code","5b2e55cf":"code","66717639":"code","99032214":"code","22a85be6":"code","bc197072":"code","7f8b1c2c":"code","52787c95":"code","8c51977a":"markdown","0a24ff79":"markdown","c474c563":"markdown","ce13250a":"markdown","503d8221":"markdown","a4c6de42":"markdown","8c0dab4e":"markdown","c591e5b4":"markdown","ac9d732c":"markdown","b9da2fd6":"markdown","5b5a1ece":"markdown","3844cdb8":"markdown","e8f33ccd":"markdown"},"source":{"4d050efd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score, make_scorer, confusion_matrix, classification_report, fbeta_score\n\n# Define seed for repeatability\nSEED = 42\nnp.random.seed(SEED)","433e68c0":"# Read in train data\ntrain_df = pd.read_csv('\/kaggle\/input\/banking-dataset-marketing-targets\/train.csv', \n                       sep=\";\")\ntrain_df.head()","c1a5baea":"# Read in test data\ntest_df = pd.read_csv('\/kaggle\/input\/banking-dataset-marketing-targets\/test.csv', \n                      sep=\";\")\ntest_df.head()","2e0e9167":"train_df.info()","4c19e409":"train_df['age'].describe()","fdc6d489":"train_df['age'].plot.hist(bins=30, density=True)\nplt.show()","d60cd9a6":"train_df['job'].value_counts(normalize=True)","a53d926c":"train_df['marital'].value_counts(normalize=True)","827df7e6":"train_df['education'].value_counts(normalize=True)","ca2b26a1":"train_df['default'].value_counts(normalize=True)","df7dbfb9":"train_df['balance'].describe()","be977cc1":"train_df['balance'].plot.hist(bins=50, density=True)\nplt.show()","30f1494b":"train_df['balance'].plot.box()\nplt.show()","28781c66":"train_df['housing'].value_counts(normalize=True)","1609d1c5":"train_df['loan'].value_counts(normalize=True)","7bf0851d":"train_df['contact'].value_counts(normalize=True)","05232ed3":"train_df['day'].value_counts(normalize=True)","6f4887f6":"train_df['day'].value_counts(normalize=True).describe()","0b4e7316":"train_df['month'].value_counts(normalize=True)","cb74cbc1":"train_df['duration'].describe()","f3da95ba":"train_df['duration'].plot.hist(bins=50, density=True)\nplt.show()","f3d891ac":"train_df['duration'].plot.box()\nplt.show()","4f30615f":"# Investigate duration values that are 0\ntrain_df[train_df['duration'] == 0]","65c055fe":"train_df['campaign'].describe()","61d34e5c":"train_df['campaign'].plot.hist(bins=30, density=True)\nplt.show()","350c7a4e":"train_df['pdays'].describe()","e03e5c89":"# Investigate first time contact\nprint(train_df.loc[train_df['pdays'] == -1, 'pdays'].count())\nprint(train_df.loc[train_df['pdays'] == -1, 'pdays'].count()\/train_df.shape[0]*100)","a32fcf6f":"# Investigate repeat contacts\ntrain_df.loc[train_df['pdays'] != -1, 'pdays'].describe()","6be7f1a8":"train_df.loc[train_df['pdays'] != -1, 'pdays'].plot.hist(bins=20, density=True)\nplt.show()","e169d06b":"train_df['previous'].value_counts()","8a44fe8b":"train_df.loc[train_df['previous'] == 275, 'previous'] = train_df.loc[train_df['previous'] != 0, 'previous'].median()\ntrain_df['previous'].describe()","a5a92196":"np.percentile(train_df.loc[train_df['previous'] != 0, 'previous'], q=95)","052188f6":"train_df.loc[train_df['previous'] >= 9, 'y'].value_counts()","ca5ff5d6":"train_df['poutcome'].value_counts(normalize=True)","ddc68ae7":"train_df.groupby(['poutcome', 'y'])['y'].count()","b04fcd70":"# Create train_x dataframe\ntrain_x = train_df.iloc[:, :-1]\ntrain_x.head()","e6163fcc":"# Create train_y dataframe\ntrain_y = train_df[['y']]\ntrain_y.head()","35a4e922":"train_x.info()","67236ea9":"# Get a list of columns for one-hot encoding\nohe_cols = list(train_x.select_dtypes(include='object').columns.values)\n\n# We want to label encode education\nle_col = ['education']\n\n# Drop education \nohe_cols.remove('education')\nohe_cols","2e4a372c":"train_x = pd.get_dummies(train_x, prefix=ohe_cols, columns=ohe_cols, drop_first=True)\ntrain_x.head()","49430615":"# Perform label encoding on education\ned_cat = {'unknown': 0, \n          'primary': 1,\n          'secondary': 2,\n          'tertiary': 3}\ntrain_x['education'] = train_x['education'].replace(ed_cat)\ntrain_x['education'].value_counts(normalize=True)","db96495c":"train_x.head()","8cba1015":"# Encode target variable\ny_cat = {'no': 0, \n         'yes': 1}\ntrain_y['y'] = train_y['y'].replace(y_cat)\ntrain_y['y'].value_counts(normalize=True)","f61ba330":"# Create the test_x dataframe\ntest_x = test_df.iloc[:, :-1]\n\n# Create train_y dataframe\ntest_y = test_df[['y']]\n\n# One-hot encode columns\ntest_x = pd.get_dummies(test_x, prefix=ohe_cols, columns=ohe_cols, drop_first=True)\n\n# Label encode education\ntest_x['education'] = test_x['education'].replace(ed_cat)\n\n# Encode target variable\ntest_y['y'] = test_y['y'].replace(y_cat)","f26252ae":"test_x.head()","d12a255c":"test_y.head()","d23919ad":"# Define the model\ndc = DecisionTreeClassifier(max_depth=30, min_samples_split=10, min_samples_leaf=10,\n                            random_state=SEED, class_weight=\"balanced\")","25ba2b2f":"# Define a scorer\nrs = make_scorer(recall_score)\n\n# Cross validation\ncv = cross_val_score(dc, train_x, train_y, cv=10, n_jobs=-1, scoring=rs)\nprint(\"Cross validation scores: {}\".format(cv))\nprint(\"%0.2f recall with a standard deviation of %0.2f\" % (cv.mean(), cv.std()))","b1a3d304":"# Fit the model on the complete train dataset\ndc.fit(train_x, train_y)","6e33f68b":"# Get predictions from the train dataset\npred = dc.predict(train_x)\nprint(\"The train recall score is {}\".format(np.round(recall_score(train_y, pred), 2)))","9d9502b9":"plt.title(\"Confusion matrix on Train set\")\nax = sns.heatmap(confusion_matrix(train_y, pred), annot=True, fmt='d')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.yticks(rotation=0)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.show()\nprint(classification_report(train_y, pred))","2a0893cb":"# Get predictions from the test dataset\npred = dc.predict(test_x)\nprint(\"The test recall score is {}\".format(np.round(recall_score(test_y, pred), 2)))","d2345404":"plt.title(\"Confusion matrix on Test set\")\nax = sns.heatmap(confusion_matrix(test_y, pred), annot=True, fmt='d')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.yticks(rotation=0)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.show()\nprint(classification_report(test_y, pred))","396a1949":"rf = RandomForestClassifier(n_jobs=-1, random_state=SEED, class_weight=\"balanced_subsample\")\n\n# Define a scorer\nrs = make_scorer(recall_score)\n\n# Cross validation\ncv = cross_val_score(rf, train_x, train_y, cv=10, n_jobs=-1, scoring=rs)\nprint(\"Cross validation scores: {}\".format(cv))\nprint(\"%0.2f recall with a standard deviation of %0.2f\" % (cv.mean(), cv.std()))\n\n# Fit the model on the complete train dataset\nrf.fit(train_x, train_y)\n\n# Get predictions from the train dataset\npred = rf.predict(train_x)\nprint(\"The train recall score is {}\".format(np.round(recall_score(train_y, pred), 2)))\n\nplt.title(\"Confusion matrix on Train set\")\nax = sns.heatmap(confusion_matrix(train_y, pred), annot=True, fmt='d')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.yticks(rotation=0)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.show()\nprint(classification_report(train_y, pred))\n\n# Get predictions from the test dataset\npred = rf.predict(test_x)\nprint(\"The test recall score is {}\".format(np.round(recall_score(test_y, pred), 2)))\n\nplt.title(\"Confusion matrix on Test set\")\nax = sns.heatmap(confusion_matrix(test_y, pred), annot=True, fmt='d')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.yticks(rotation=0)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.show()\nprint(classification_report(test_y, pred))","5b2e55cf":"# # Determine the maximum permissible balance value\n# balance_thresh = np.percentile(train_df['balance'], q=90)\n\n# # Create a new column that indicates balance > threshold\n# train_df['balance_outliers'] = np.where(train_df['balance'] > balance_thresh, 1, 0)\n# train_df['balance_outliers'].value_counts(normalize=True)","66717639":"# # Clip balance values with the threshold\n# train_df['balance'].clip(upper=balance_thresh, inplace=True)\n# train_df['balance'].describe()","99032214":"# # Check balance values that are negative\n# train_df.loc[train_df.balance < 0, 'balance'].describe()","22a85be6":"# # Determine the maximum permissible value for duration\n# duration_thresh = np.percentile(train_df['duration'], 90)\n\n# # Create a new column that indicates duration > threshold\n# train_df['duration_outliers'] = np.where(train_df['duration'] > duration_thresh, 1, 0)\n# train_df['duration_outliers'].value_counts(normalize=True)","bc197072":"# # Clip the values\n# train_df['duration'].clip(upper=duration_thresh, inplace=True)\n# train_df['duration'].describe()","7f8b1c2c":"# # Determine the maximum permissible value for duration\n# campaign_thresh = np.percentile(train_df['campaign'], 90)\n\n# # Create a new column that indicates duration > threshold\n# train_df['campaign_outliers'] = np.where(train_df['campaign'] > campaign_thresh, 1, 0)\n# train_df['campaign_outliers'].value_counts(normalize=True)","52787c95":"# # Clip the values\n# train_df['campaign'].clip(upper=campaign_thresh, inplace=True)\n# train_df['campaign'].describe()","8c51977a":"Great! We achieved a perfect classifier on the test set! The best part is, we did not have to perform complex feature engineering. Just the basic one-hot encoding and label encoding on categorical variables.","0a24ff79":"# Explore training data","c474c563":"We do have outliers in the balance column. There are a few ways to deal with the outliers:\n- We set a maximum value, say at 90 percentile. Additionally, create a new column that indicates the balance value is greater than the threshold.\n- We let the values be and hope the model is robust to the outliers.","ce13250a":"The distribution of duration is similar to that of balance. We can clip the outliers and create an additional column to record this information.","503d8221":"# Decision Tree Classifier","a4c6de42":"# Random Forest Classifier","8c0dab4e":"- Majority of the values are 0. This matches with what we found with `pdays`. \n- One customer was contacted 275 times. This looks like a data error. We will replace this value with the mean of non-zero values.","c591e5b4":"If the customer was contacted but never answered his\/her phone the `duration` value can be `0`","ac9d732c":"We have no missing information. According to the data dictionary missing values are labeled as `unknown`.","b9da2fd6":"Build a machine learning model using the data as it is. After we set a baseline we apply feature engineering techniques to try and improve the performance of the model.","5b5a1ece":"# Data Dictionary\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\n\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \n\n3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n\n4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n\n5 - default: has credit in default? (binary: \"yes\",\"no\")\n\n6 - balance: average yearly balance, in euros (numeric) \n\n7 - housing: has housing loan? (binary: \"yes\",\"no\")\n\n8 - loan: has personal loan? (binary: \"yes\",\"no\")\n\n**related with the last contact of the current campaign:**\n\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \n\n10 - day: last contact day of the month (numeric)\n\n11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", \u2026, \"nov\", \"dec\")\n\n12 - duration: last contact duration, in seconds (numeric)\n\n**other attributes:**\n\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n\n**Output variable (desired target):**\n\n17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")","3844cdb8":"Customers are rarely contacted in the month of `December`. This makes sense, it is the holiday season and customers would not like to be bothered during this time of the year. But given December is a festive month, are customers more likely to say yes during this time?","e8f33ccd":"81% of customers were contacted for the first time."}}