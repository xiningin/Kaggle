{"cell_type":{"aca23463":"code","f173b521":"code","10978577":"code","e66cad97":"code","047d7701":"code","7d5fec2d":"code","f908c53b":"code","984920f7":"code","a10e3cd6":"code","30fe11ec":"code","8f4caf6a":"code","5dcf2b20":"code","670a5cda":"code","d4a4331a":"code","5f061497":"markdown","6def7da9":"markdown","54a101fe":"markdown","e8593059":"markdown","9cf146f2":"markdown","10380740":"markdown","3f5b422c":"markdown","9b71aee3":"markdown","e1fcef0a":"markdown","e39646bc":"markdown","87b5e6c7":"markdown","beaf627b":"markdown","bd6714c5":"markdown","0192d1a8":"markdown","0f99a9ae":"markdown","170b54d2":"markdown","aa78732c":"markdown","bc0e1cc0":"markdown","12f85dfe":"markdown","003bca18":"markdown","5a782ffd":"markdown","fd8909df":"markdown","97e24126":"markdown","53c782dd":"markdown"},"source":{"aca23463":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","f173b521":"import os\n\nos.chdir('\/kaggle\/')\nos.listdir('\/kaggle\/input')","10978577":"data = pd.read_csv('\/kaggle\/input\/fb_users.csv')\ndf = data.copy()  # To keep the data as backup\n\ndf.head()","e66cad97":"df.shape","047d7701":"df.isnull().sum()","7d5fec2d":"df.describe()","f908c53b":"from sklearn.preprocessing import MinMaxScaler\n\n# Let's instantiate MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Scale\nscaled_array = scaler.fit_transform(X=df)\n\ntype(scaled_array)","984920f7":"df_scaled = pd.DataFrame(data=scaled_array, columns=df.columns)\ndf_scaled.head()","a10e3cd6":"# First, we need to import the sklearn's KMeans library\nfrom sklearn.cluster import KMeans\n\ncost = []\n\nfor k in range(1, 16):\n    kmean = KMeans(n_clusters=k, random_state=0)\n    kmean.fit(df_scaled)\n    cost.append([k, kmean.inertia_])\n\n# Plotting the K's against cost using matplotlib library which we imported at the start\nplt.figure(figsize=(15, 8))\nplt.plot(pd.DataFrame(cost)[0], pd.DataFrame(cost)[1])\nplt.xlabel('K')\nplt.ylabel('Cost')\nplt.title('Elbow Analysis')\nplt.grid(True)\n\nplt.show()","30fe11ec":"# Let's first import the relevant library\nfrom sklearn.metrics import silhouette_score\n\nscore = []\n\nfor k in range(2, 16):\n    kmean = KMeans(n_clusters=k, random_state=0)\n    kmean.fit(df_scaled)\n    score.append([k, silhouette_score(df_scaled, kmean.labels_)])\n\n# Let's plot the Number of K's against Silhouette Score\nplt.figure(figsize=(15, 8))\nplt.plot(pd.DataFrame(score)[0], pd.DataFrame(score)[1])\nplt.xlabel('K')\nplt.ylabel('Silhouette Score')\nplt.grid(True)\nplt.title(\"Silhouette Score against each number of clusters\")\n\nplt.show()","8f4caf6a":"# Let's make a new dataframe named 'predict' and copy the contents of our original df into it\npred = df.copy()\n\n# Let's train the model based on 3 clusters\nkmean_3k = KMeans(n_clusters=3, random_state=0)\nlabels = kmean_3k.fit_predict(df_scaled)\n\npred['clusters'] = labels\npred.head()","5dcf2b20":"pivoted = pred.groupby(['clusters']).median().reset_index()\npivoted","670a5cda":"pred.clusters.value_counts()","d4a4331a":"import seaborn as sns\nsns.set()\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='clusters', data=pred)","5f061497":"First, let's import the relevant python libraries.","6def7da9":"Okay, we can see that on 2 clusters, elbow is being formed. But let's not be hasty in selecting the number 2. Let's also find out the silhouette score to be sure.","54a101fe":"Let's first look at the basic statistics of the data. Let's see what we can find from it.","e8593059":"# Choosing the Machine Learning Algorithm","9cf146f2":"The output we got after scaling the dataframe is a numpy array. We need to convert it into a dataframe.","10380740":"As of now, we have our scaled data ready to be trained. But we have one problem: how many clusters (groups) to put into the data to train? Now, to find out the right number of clusters, we need to decide based on two metrics: <b>Elbow Analysis<\/b> and <b>Silhouette Score<\/b>.","3f5b422c":"Let's check to make sure we don't have any null values in the data.","9b71aee3":"Silhouette score of 3 is pretty high which is a good thing. And by looking at both the elbow analysis and silhouette score. We can decide to choose 3 because the cost of 3 on the Elbow Analysis is quite low and its silhouette score is quite high.\n\nSo, let's choose 3.","e1fcef0a":"By looking at the above graph. We can conclude that in the given limited dataset, most people belong to group 0.","e39646bc":"# Exploratory Data Analysis (EDA)","87b5e6c7":"Let's graph this by using python's popular <b>seaborn<\/b> library.","beaf627b":"I gathered some facebook data from the internet. Let's get this data into a pandas dataframe.","bd6714c5":"## Elbow Analysis","0192d1a8":"Having decided how many clusters to use, we would like to get a better understanding of what values are in those clusters are and interpret them. That's where profiling comes in. We can use profiling for data analysis to get insights. Let's do this to find out the user behavior.","0f99a9ae":"Let's say we want to identify certain groups of facebook users based on their behavior. Some facebook users might not react on the posts. Some people may 'like' react more and use other reactions like 'love' or 'wow' to a very minute extent. Some people may share posts a lot while some may not. Some only react and not comment on posts. Some may comment but react much, etc.\n\nWe are going to identify such groups with the help of <b>machine learning<\/b>.\n\n**Note**: This dataset is Thailand Facebook Live dataset. I have removed the labels to make it an unsupervised problem to demonstrate KMeans Clustering.","170b54d2":"From the pivoted dataframe shown above, we can see that there are three groups of Facebook users:\n1. **Group 0**: Which indicates that the user of this group might not use Facebook a lot or use it only for surfing. Their number of reactions are around 48 and comments only 3. They don't share a lot of posts. And mostly they use 'like' react on posts.\n\n\n2. **Group 1**: This is the group of people who, according to the provided dataset, happen to use Facebook quite a lot. But they are the kind of people who usually give people the 'like' react mostly.\n\n\n3. **Group 2**: This group also shows that people use Facebook a lot. These people tend to comment and share the posts a lot. They also tend to use other reacts on posts besides the 'like' react.\n\nNow, to check the number of people in each group:","aa78732c":"# Profiling","bc0e1cc0":"# Introduction","12f85dfe":"# Clustering","003bca18":"## Silhoutte Score","5a782ffd":"Our basic statistics shows that almost all the labels are right skewed. Also, by looking at the min and max values, we can say that the difference of these values is quite great. Hence, we need to scale our data.\n\nNow, to scale our data, we need to use a great python library 'sklearn' and it's MinMaxScaler() function from sklearn.preprocessing module.","fd8909df":"## Training on the chosen number of K","97e24126":"We are going to use K-Means Clustering for this purpose. K-Means Clustering is the most popular algorithm which is used for unsupervised learning. This algorithm is used when we have un-labelled data.","53c782dd":"We have no null values. So, we need not worry about it!"}}