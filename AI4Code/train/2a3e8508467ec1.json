{"cell_type":{"5f18b76d":"code","a14204e6":"code","fed7147a":"code","6a5c8a57":"code","baa56b1b":"code","c0ebd2f9":"code","a183b757":"code","93148df0":"code","2f7e04ee":"code","8f813faa":"code","556953fd":"code","ff8d3ca6":"code","b312f099":"code","3e0a7f1d":"code","40ce0353":"markdown","3427df84":"markdown","344d50bb":"markdown","91e2f08e":"markdown","4ba0ac1d":"markdown","888ab538":"markdown","255801e7":"markdown","3ee1ca92":"markdown","7b40f671":"markdown","ff02931f":"markdown","01a8a767":"markdown","4404a98e":"markdown"},"source":{"5f18b76d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dropout, Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nimport matplotlib.image as mpimg\nimport cv2\nimport re\nfrom collections import Counter\nimport os","a14204e6":"BASE_DIR = \"..\/input\/dogs-vs-cats\"\nTRAIN_DIR = BASE_DIR + '\/train\/train'\nTEST_DIR = BASE_DIR + '\/test\/test'\nIMAGE_SIZE = (150,150)\nBATCH_SIZE = 20\nEPOCHS = 30\nSPLIT_INDEX = 23000\nRANDOM_SEED = 6587\n\n#ImageDataGenerator\nDATAGEN = ImageDataGenerator(\n                rescale = 1.0\/255\n                )","fed7147a":"def get_images_labels(directory):\n    \n    images = []\n    labels = []\n    \n    for file in os.listdir(directory):\n        \n        #get the label from file name\n        label = file.split('.')[0]\n        images.append(file)\n        labels.append(label)\n        \n    return images, labels","6a5c8a57":"#get image path and label\ntrain_images, train_labels = get_images_labels(TRAIN_DIR)\n\n#store data in pandas dataframe for easy processing\ndf = pd.DataFrame({'image':train_images, 'label':train_labels})\n\n#shuffle data\ndf = df.sample(frac=1, random_state=RANDOM_SEED)\n\n#find count for each class\nprint(df['label'].value_counts())","baa56b1b":"def plot_random_images(nb_rows, nb_cols, df, figsize=(10,10)):\n    nb_images = nb_rows*nb_cols\n    idx = np.random.choice(len(df), size=(nb_images))\n    j=0\n    fig=plt.figure(figsize=figsize)\n    for i in idx:\n            img = mpimg.imread(TRAIN_DIR+'\/'+df['image'][i])\n            fig.add_subplot(nb_rows,nb_cols,j+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(img, cmap=plt.cm.binary)\n            plt.xlabel(df['label'][i])\n            j += 1\n    plt.show()\nplot_random_images(2,3,df)","c0ebd2f9":"df_train = df[:SPLIT_INDEX].reset_index(drop=True)\ndf_val = df[SPLIT_INDEX:].reset_index(drop=True)","a183b757":"train_datagen = DATAGEN.flow_from_dataframe(\n                dataframe = df_train,\n                directory = TRAIN_DIR,\n                x_col = 'image',\n                y_col = 'label',\n                target_size = IMAGE_SIZE,\n                class_mode = 'binary',\n                batch_size = BATCH_SIZE\n                )\n\nval_datagen = DATAGEN.flow_from_dataframe(\n                dataframe = df_val,\n                directory = TRAIN_DIR,\n                x_col = 'image',\n                y_col = 'label',\n                target_size = IMAGE_SIZE,\n                class_mode = 'binary',\n                batch_size = BATCH_SIZE\n                )","93148df0":"model = Sequential()\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.Nadam(lr=2e-5), metrics=['acc'] )","2f7e04ee":"history = model.fit_generator(\n                                train_datagen,\n                                steps_per_epoch = train_datagen.samples \/\/ BATCH_SIZE,\n                                epochs = EPOCHS,\n                                validation_data = val_datagen,\n                                validation_steps = val_datagen.samples \/\/ BATCH_SIZE,\n                             )","8f813faa":"#path to test image\npath = \"..\/input\/dogs-vs-cats\/test\/test\/10103.jpg\"\n\n#preprocess the image\nimg = image.load_img(path, target_size=(150,150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor \/= 255.\n\n#show image\nplt.imshow(img_tensor[0])\nplt.show()","556953fd":"#model that outputs activations\nlayer_outputs = [layer.output for layer in model.layers[:4]]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\n\n#apply model on test image(preprocessed)\nactivations = activation_model.predict(img_tensor)","ff8d3ca6":"# plot 1 channel activations of first layer of trained model\nplt.matshow(activations[0][0,:,:,1], cmap='viridis')","b312f099":"# plot 2 channel activations of second layer of trained model\nplt.matshow(activations[1][0,:,:,2], cmap='viridis')","3e0a7f1d":"layer_names = []\nfor layer in model.layers[:4]:\n    layer_names.append(layer.name)\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations):\n    n_features = layer_activation.shape[-1]\n    \n    size = layer_activation.shape[1]\n    n_cols = n_features\/\/images_per_row\n    display_grid = np.zeros((size*n_cols, images_per_row*size))\n    \n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n            \n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image +=128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col*size: (col+1)*size,\n                         row*size: (row+1)*size] = channel_image\n        \n    scale = 1.\/size\n    plt.figure(figsize=(scale*display_grid.shape[1], scale*display_grid.shape[0]))\n    \n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","40ce0353":"It's often said that deep learning models are black boxes. The representations learned by neural network are not in human readable form. Although it is partially true for certain types of deep-learning models, it's definitely not true for convnets. The presentations learned by convnets are highly amenable to visualization. There are different techniques for visualizing and interpreting these representations, some of them are:\n\n1. Visualizing the intermediate convnet outputs(Intermediate activations).\n2. Visualizing convnet filters.\n3. Visualizing the heatmaps of class activation in an image.\n\nIn this notebook we will build a basic convolutional neural network for single-label binary-classification task and visualize the intermediate activations of CNN.\n\n\nLet's start with importing necessary packages and initializing the parameters.\n","3427df84":"My other kernels on CV:\n\n[Fine tuning VGG16](https:\/\/www.kaggle.com\/flygirl\/intel-image-classification-90-fine-tuning-vgg16)","344d50bb":"We can see that higher representations carry less information about the visual contents of the image and more related to the class of image.","91e2f08e":"Let's start with creating a dataframe with filenames and their corresponding labels. Later we will split this dataframe into train and validation dataframes. We will use ImageDataGenerator with flow_from_dataframe to get the data to our model.","4ba0ac1d":"## Train-Val split","888ab538":"Let's see representataions of initial 8 layers.","255801e7":"Now that we have activation values with us, let's visualize them.","3ee1ca92":"We have trained a model with validation accuracy of 86%, it can be further improved by using pretrained models, fine tuning, augmentation etc. Since here our objective is to visualize the intermediate activations, I will stop the training here. Let's go on to visualization part.\n\nLet's take a random image from test set and proprocess it to pass through the trained model, we will save the activation outputs of initial 4 layers in a list and visualize it.","7b40f671":"## Model","ff02931f":"Let's display some of the images","01a8a767":"Channels of layer 2  has learned a bit more complex representations, looks like a combination of edge and eye detector.","4404a98e":"This channel appears to encode an edge detector."}}