{"cell_type":{"4505d4b2":"code","35b3590b":"code","004449d2":"code","b273a04e":"code","9c7c4cbb":"code","21558739":"code","a1e4e8aa":"code","82029a4f":"code","9bac3bf6":"code","53ccbe4f":"code","a0828bbe":"code","6fd8e246":"code","2fc85139":"code","8bb156a0":"code","2a414d34":"code","4fe5f71a":"code","cd28c996":"code","ba20078e":"code","5578b3f9":"code","37480501":"code","301ac4c2":"code","7bc1dd88":"code","72172e75":"code","72769f4c":"code","f667e27b":"code","9c750c62":"markdown","a85d56e3":"markdown","923faa25":"markdown","6cf59b2d":"markdown","7d2ba860":"markdown","20dfa51f":"markdown","32d01762":"markdown","d3ec97bd":"markdown","6be7d3aa":"markdown"},"source":{"4505d4b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","35b3590b":"\nimport matplotlib.pyplot as plt\n%matplotlib inline","004449d2":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","b273a04e":"train_data.head()","9c7c4cbb":"train_data.shape","21558739":"train_data[~train_data['keyword'].isna()]","a1e4e8aa":"train_data['target'].value_counts()","82029a4f":"from collections import Counter\nimport nltk\nimport re","9bac3bf6":"print('Average length of Disaster tweet is ', train_data[train_data['target'] == 1]['text'].apply(lambda x: len(x)).mean())\nprint('Average length of Not Disaster tweet is ', train_data[train_data['target'] == 0]['text'].apply(lambda x: len(x)).mean())","53ccbe4f":"ax = train_data[train_data['target'] == 1]['text'].apply(lambda x: len(x)).hist()\nplt.title('Lenght of disaster tweets')","a0828bbe":"ax = train_data[train_data['target'] == 0]['text'].apply(lambda x: len(x)).hist()\nplt.title('Lenght of not disaster tweets')","6fd8e246":"ttokenizer = nltk.TweetTokenizer()\nttokenizer.tokenize(train_data.iloc[0].text)\n","2fc85139":"ttokenizer.tokenize(train_data.iloc[1].text)","8bb156a0":"import string\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english') + ['...', '\\x89'])\ndef clean_text(text):\n    \"\"\"\n    1. Lower text\n    2. Remove standard puntuation\n    3. Remove stop words\n    \"\"\"\n    text = str(text).lower()\n    words = ttokenizer.tokenize(text)\n    text = ' '.join([word for word in words if (word not in set(string.punctuation) and word not in stop_words)]) \n    return text","2a414d34":"train_data['clean_text'] = train_data['text'].apply(lambda x: clean_text(x))","4fe5f71a":"counter = Counter(\" \".join(train_data[\"clean_text\"]).split())\ncounter.most_common(100)","cd28c996":"counter = Counter(\" \".join(train_data[train_data['target'] == 1][\"clean_text\"]).split())\ncounter.most_common(100)","ba20078e":"counter = Counter(\" \".join(train_data[train_data['target'] == 0][\"clean_text\"]).split())\ncounter.most_common(100)","5578b3f9":"train_data[train_data['clean_text'].apply(lambda x: 'http' in x)].iloc[0]","37480501":"train_data[(train_data['clean_text'].apply(lambda x: 'http' in x) & (train_data['target'] == 1))]","301ac4c2":"train_data[(train_data['clean_text'].apply(lambda x: 'http' in x) & (train_data['target'] == 0))]","7bc1dd88":"def extract_hashtags(text):\n    hashtags = re.findall(r\"#(\\w+)\", text)\n    if len(hashtags) > 0:\n        return hashtags\n    return None\n\ntrain_data['hashtags'] = train_data['clean_text'].apply(extract_hashtags)","72172e75":"train_data.head(10)","72769f4c":"## Collect all disaster hashtags\ndisaster_hts = \" \".join([ht for hts in train_data[(train_data['target'] == 1) & (~train_data['hashtags'].isna())][\"hashtags\"] for ht in hts])\n## Compute frequency\ndisater_freq = nltk.FreqDist(disaster_hts.split())\n\ndistaster_freq_df = pd.DataFrame({'count': list(disater_freq.values()), 'hashtag': list(disater_freq.keys())})\ndistaster_freq_df = distaster_freq_df.nlargest(columns=\"count\", n = 15) \n\n## Visualize \n\nplt.figure(figsize=(17,7))\nplt.barh(y=distaster_freq_df['hashtag'], width=distaster_freq_df['count'], color=['b', 'g', 'r', 'c', 'm', 'y', 'g'])\nplt.title('Disaster top hashtags')\nplt.xlabel('Count')\nplt.ylabel('Hashtag')\nplt.show()","f667e27b":"notdisaster_hts = \" \".join([ht for hts in train_data[(train_data['target'] == 0) & (~train_data['hashtags'].isna())][\"hashtags\"] for ht in hts])\n\nnotdisater_freq = nltk.FreqDist(notdisaster_hts.split())\nnotdistaster_freq_df = pd.DataFrame({'count': list(notdisater_freq.values()), 'hashtag': list(notdisater_freq.keys())})\nnotdistaster_freq_df = notdistaster_freq_df.nlargest(columns=\"count\", n = 15) \nplt.figure(figsize=(17,7))\nplt.barh(y=notdistaster_freq_df['hashtag'], width=notdistaster_freq_df['count'], color=['b', 'g', 'r', 'c', 'm', 'y', 'g'])\nplt.title('Not disaster top hashtags')\nplt.xlabel('Count')\nplt.ylabel('Hashtag')\nplt.show()","9c750c62":"Most common words in disaster tweets","a85d56e3":"Check if urls in text","923faa25":"It's almost the same","6cf59b2d":"As it's almost the same quantity of data with urls - it's not big deal to extract it and make it as feature","7d2ba860":"Extract hashtags","20dfa51f":"Most common words in tweets","32d01762":"Most common words in not disaster tweets","d3ec97bd":"Now, we can make some transformations to texts\nIn nltk module we have TweetTokenizer","6be7d3aa":"Let's look at the text"}}