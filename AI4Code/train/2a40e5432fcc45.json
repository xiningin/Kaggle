{"cell_type":{"d048c04b":"code","445a332f":"code","91f0eef7":"code","313aba9a":"code","3a70fcee":"code","98d14375":"code","e3c539ac":"code","47372ef2":"code","9d8131e0":"code","07871935":"code","9ffe2f30":"code","5a366989":"code","2c43f3ac":"code","16cf4121":"code","c43f35c7":"code","338643cf":"markdown","3439b051":"markdown","7c31f783":"markdown","f9c2bc23":"markdown","9a3f3900":"markdown","b5ed6d24":"markdown","d3f30af4":"markdown","778666ba":"markdown","45739a81":"markdown","5aacb80a":"markdown","e2f24136":"markdown","513143ed":"markdown","5308eeee":"markdown","ac095c14":"markdown","667d7d9c":"markdown","8b310b45":"markdown","8418aca6":"markdown","e8ac1084":"markdown","938d639b":"markdown","5e323f83":"markdown"},"source":{"d048c04b":"# Imports\nimport os\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom wordcloud import WordCloud","445a332f":"# Read input data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\n# Print data sizes and preview\nprint('Train data size: ', train.shape)\nprint('Test data size: ', test.shape)\ntrain.head(3)","91f0eef7":"print('Number of unique keywords: ', train['keyword'].nunique())\nprint('Percentage of tweets with keywords: ', len(train['keyword'].dropna())\/ len(train) * 100)","313aba9a":"print('Top keywords in disaster tweets:')\nprint(train[train['target'] == 1]['keyword'].value_counts().head(), '\\n')\nprint('Top keywords in non-disaster tweets:')\nprint(train[train['target'] == 0]['keyword'].value_counts().head())","3a70fcee":"# Fix variations of USA and UK\ntrain.loc[train['location'].isin(['US', 'United States', 'United States of America']), 'location'] = 'USA'\ntrain.loc[train['location'].isin(['United Kingdom', 'The UK']), 'location'] = 'UK'\n\n# Apply the same to the test set\ntest.loc[test['location'].isin(['US', 'United States', 'United States of America']), 'location'] = 'USA'\ntest.loc[test['location'].isin(['United Kingdom', 'The UK']), 'location'] = 'UK'\n\nprint('Number of unique locations: ', train['location'].nunique())\nprint('Top tweet locations:')\nlocation_counts = train['location'].value_counts().head()\nlocation_counts","98d14375":"# First, some data cleaning - lowercase all tweet text\ntrain['text'] = train['text'].map(lambda text: text.lower())\ntest['text'] = test['text'].map(lambda text: text.lower())\n\n# Find hastags with regex\ntrain['hashtags'] = train['text'].map(lambda text: re.findall(r\"#(\\w+)\", text))\ntest['hashtags'] = test['text'].map(lambda text: re.findall(r\"#(\\w+)\", text))\n\n# Remove hashtags symbols from the original text but keep the words\ntrain['text'] = train['text'].map(lambda text: text.replace('#', ''))\ntest['text'] = test['text'].map(lambda text: text.replace('#', ''))\n\n# Flatten hastags list and print most common\nhashtags = pd.Series([tag for hashtags in train['hashtags'] for tag in hashtags])\nprint('Most common hashtags:')\nhashtags.value_counts().head()","e3c539ac":"disaster_hashtags = pd.Series([tag for hashtags in train[train['target'] == 1]['hashtags'] for tag in hashtags])\nwordcloud = WordCloud(background_color='white', width=500, height=300).generate(' '.join(disaster_hashtags))\nprint('Hashtags in disaster tweets:')\nwordcloud.to_image()","47372ef2":"non_disaster_hashtags = pd.Series([tag for hashtags in train[train['target'] == 0]['hashtags'] for tag in hashtags])\nwordcloud = WordCloud(background_color='white', width=500, height=300).generate(' '.join(non_disaster_hashtags))\nprint('Hashtags in non-disaster tweets:')\nwordcloud.to_image()","9d8131e0":"# Find mentions with regex\ntrain['mentions'] = train['text'].map(lambda text: re.findall(r\"@(\\w+)\", text))\ntest['mentions'] = test['text'].map(lambda text: re.findall(r\"@(\\w+)\", text))\n\n# Remove mention symbols from the original text but keep the words\ntrain['text'] = train['text'].map(lambda text: text.replace('@', ''))\ntest['text'] = test['text'].map(lambda text: text.replace('@', ''))\n\n# Flatter mentions list and print most common\nmentions = pd.Series([mention for mentionslist in train['mentions'] for mention in mentionslist])\nmentions.value_counts().head()","07871935":"# Find URLs with regex\ntrain['url'] = np.NaN\ntrain['url_search'] = train['text'].map(lambda text: re.search('(?P<url>https?:\/\/[^\\s]+)', text))\ntrain.loc[~train['url_search'].isnull(), 'url'] = train[~train['url_search'].isnull()]['url_search'].map(lambda result: result.group('url'))\ndel train['url_search']\n\ntest['url'] = np.NaN\ntest['url_search'] = test['text'].map(lambda text: re.search('(?P<url>https?:\/\/[^\\s]+)', text))\ntest.loc[~test['url_search'].isnull(), 'url'] = test[~test['url_search'].isnull()]['url_search'].map(lambda result: result.group('url'))\ndel test['url_search']\n\n# Extract hostname from URLs\ntrain.loc[~train['url'].isnull(),'host'] = train['url'].dropna().map(lambda s: s.split(':\/\/')[-1].split('\/')[0].split('?')[0] if s else np.NaN)\ntest.loc[~test['url'].isnull(), 'host'] = test['url'].dropna().map(lambda s: s.split(':\/\/')[-1].split('\/')[0].split('?')[0] if s else np.NaN)\n\n\n# Remove URLs from tweet text\ntrain['text'] = train['text'].map(lambda text: re.sub('(?P<url>https?:\/\/[^\\s]+)', '', text))\ntest['text'] = test['text'].map(lambda text: re.sub('(?P<url>https?:\/\/[^\\s]+)', '', text))\n\n# Print most common\nprint('Most common URLs:')\ntrain['host'].value_counts().head()","9ffe2f30":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Tokenize text, filter out punctuations and stopwords and lemmatize the words\ndef get_words(text):\n    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in word_tokenize(text)]\n    words = [word for word in words if word.isalpha() and word not in stop_words]\n    words = [lemmatizer.lemmatize(word) for word in words]\n    return words\n\n\ntrain['words'] = train['text'].map(lambda text: get_words(text))\ntest['words'] = test['text'].map(lambda text: get_words(text))\n\n# Print top words\nwords = pd.Series([word for wordlists in train['words'] for word in wordlists])\nprint('Most common words (lemmatized) :')\nwords.value_counts().head()","5a366989":"train.head(3)","2c43f3ac":"# Vectorize the training set\nword_vectorizer = CountVectorizer()\nX_train = word_vectorizer.fit_transform(train['words'].map(lambda words: ', '.join(words)))\n\n# Vectorize the testing test\nX_test = word_vectorizer.transform(test['words'].map(lambda words: ', '.join(words)))\n\n# Our output variable \"target\" which indicates whether a tweet is diaster tweet\ny_train = train['target']\n\nX_train.shape","16cf4121":"clf = BernoulliNB()\nscores = cross_val_score(clf, X_train, y_train)\nprint(scores.mean())","c43f35c7":"clf = BernoulliNB()\nclf.fit(X_train, y_train)\ny_test = clf.predict(X_test)\noutput = pd.DataFrame()\noutput['id'] = test['id']\noutput['target'] = y_test\noutput.to_csv('submission.csv', index=False)","338643cf":"Disaster tweets seem to contain more negative sentiments.","3439b051":"Let's see if there's a difference between hashtags on disaster tweets compared to the ones on non-disaster tweets using wordclouds to visualize.","7c31f783":"### Train Final Model\n\nRetrain the classifier using full training set and predict the output for the test set.","f9c2bc23":"## Classification\n\nWe'll be using a Naive Bayes classifier to estimate the likelihood of a tweet being a disaster tweet.\n\nOnly the words in the tweet are used as they provide a decent amount of information about the tweets.","9a3f3900":"### Hashtags\n\nHashtags are commonly used in tweets to tag them a specific theme or topic. It is possible to extract these from the tweet text and look at the trends.","b5ed6d24":"Most tweets are tagged to keywords and there is a relatively small set of keywords being repeated.\n\nLet's compare the top keywords in disaster tweets vs non-disaster tweets.","d3f30af4":"### Keywords\nThe given data contains keywords tagged to the tweets. Let's see how many of these keywords are present and how often they appear.","778666ba":"Most URLs are shortened to t.co and do not provide much valuable information.","45739a81":"### Tweet Texts\n\nIt's now time to further clean the tweet text and prepare for model training.\n\nThe tweet text is already lowercased and has symbols like # and @ removed. We also removed the URLs from the tweet text in the previous step.\n\nNow the tweet text tokenized to break it up into words and all other symbols, punctuations and stopwords are removed.\n\nThe words are then converted to their lemma form using WordNetLemmatizer.","5aacb80a":"Our training set is a sparse matrix of the size 7613 x 14636.","e2f24136":"# Disaster Tweets EDA and Classification\n\nIn this notebook, I try to explore the [Disaster Tweets Dataset](https:\/\/www.kaggle.com\/c\/nlp-getting-started) on Kaggle and try to classify the tweets as disaster or non-disaster tweets.\n\nThe given features like the location and keywords are analyzed along with features extracted from the tweet text like hashtags, mentions and URLs.\n\nThe data is cleaned and preprocessed by using NLP techniques before being fed into a simple classification model.","513143ed":"Even just using the words in the tweet gives us decent accuracy.","5308eeee":"### Mentions\n\nMentions are a way to tag or reply to another user on twitter. These too, can be extracted from the tweet text.","ac095c14":"### URLs\n\nSome tweets contain links to other places on the web. These may not be very useful to use and we need to remove them from the tweet text.","667d7d9c":"### Cross Validation\nNow perform cross validation on the training set to see model accuracy.","8b310b45":"### Vectorization\n\nThe words in the tweet are mapped to numeric vectors using CountVectorizer.","8418aca6":"## EDA and Feature Engineering","e8ac1084":"### Locations\n\nTweet location is another feature that is available. Let's fix the variations in \"USA\" and \"UK\" and look at the most common locations.","938d639b":"Although too early to conclude at this point, the top keywords in both cases seem to carry negative sentiments.","5e323f83":"The cleaned data with the extracted features:\n"}}