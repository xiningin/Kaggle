{"cell_type":{"0441067b":"code","ca391550":"code","78ab2e67":"code","d4036899":"code","50b1cce6":"code","d504faba":"markdown","a063a6ed":"markdown","8567aaa6":"markdown","dbdc646d":"markdown","bf3c773c":"markdown","198d2c29":"markdown","890ae535":"markdown","af35bdf5":"markdown","f8d75978":"markdown"},"source":{"0441067b":"from sklearn import set_config                      # to change the display\nfrom sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\nfrom IPython.core.display import display, HTML      # to visualize pipeline\n\nfrom fit_pipeline_ames import *                     # create & fit pipeine\nset_config(display='diagram')\ndisplay(HTML(estimator_html_repr(pipe)))","ca391550":"from feature_importance import FeatureImportance\nfeature_importance = FeatureImportance(pipe)\nfeature_importance.plot(top_n_features=25)","78ab2e67":"len(feature_importance.discarded_features)","d4036899":"feature_importance.discarded_features","50b1cce6":"\nimport numpy as np  \nimport pandas as pd  \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils.validation import check_is_fitted\nimport plotly.express as px\n\n\nclass FeatureImportance:\n\n    \"\"\"\n    \n    Extract & Plot the Feature Names & Importance Values from a Scikit-Learn Pipeline.\n    \n    The input is a Pipeline that starts with a ColumnTransformer & ends with a regression or classification model. \n    As intermediate steps, the Pipeline can have any number or no instances from sklearn.feature_selection.\n\n    Note: \n    If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, \n    it must come last in the pipeline. For example, OneHotEncoder, MissingIndicator & SimpleImputer(add_indicator=True) add columns \n    to the dataset that didn't exist before, so there should come last in the Pipeline.\n    \n    \n    Parameters\n    ----------\n    pipeline : a Scikit-learn Pipeline class where the a ColumnTransformer is the first element and model estimator is the last element\n    verbose : a boolean. Whether to print all of the diagnostics. Default is False.\n    \n    Attributes\n    __________\n    column_transformer_features :  A list of the feature names created by the ColumnTransformer prior to any selectors being applied\n    transformer_list : A list of the transformer names that correspond with the `column_transformer_features` attribute\n    discarded_features : A list of the features names that were not selected by a sklearn.feature_selection instance.\n    discarding_selectors : A list of the selector names corresponding with the `discarded_features` attribute\n    feature_importance :  A Pandas Series containing the feature importance values and feature names as the index.    \n    plot_importances_df : A Pandas DataFrame containing the subset of features and values that are actually displaced in the plot. \n    feature_info_df : A Pandas DataFrame that aggregates the other attributes. The index is column_transformer_features. The transformer column contains the transformer_list.\n        value contains the feature_importance values. discarding_selector contains discarding_selectors & is_retained is a Boolean indicating whether the feature was retained.\n    \n    \n    \n    \"\"\"\n    def __init__(self, pipeline, verbose=False):\n        self.pipeline = pipeline\n        self.verbose = verbose\n\n\n    def get_feature_names(self, verbose=None):  \n\n        \"\"\"\n\n        Get the column names from the a ColumnTransformer containing transformers & pipelines\n\n        Parameters\n        ----------\n        verbose : a boolean indicating whether to print summaries. \n            default = False\n\n\n        Returns\n        -------\n        a list of the correct feature names\n\n        Note: \n        If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, \n        it must come last in the pipeline. For example, OneHotEncoder, MissingIndicator & SimpleImputer(add_indicator=True) add columns \n        to the dataset that didn't exist before, so there should come last in the Pipeline.\n\n        Inspiration: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12525 \n\n        \"\"\"\n\n        if verbose is None:\n            verbose = self.verbose\n            \n        if verbose: print('''\\n\\n---------\\nRunning get_feature_names\\n---------\\n''')\n        \n        column_transformer = self.pipeline[0]        \n        assert isinstance(column_transformer, ColumnTransformer), \"Input isn't a ColumnTransformer\"\n        check_is_fitted(column_transformer)\n\n        new_feature_names, transformer_list = [], []\n\n        for i, transformer_item in enumerate(column_transformer.transformers_): \n            \n            transformer_name, transformer, orig_feature_names = transformer_item\n            orig_feature_names = list(orig_feature_names)\n            \n            if verbose: \n                print('\\n\\n', i, '. Transformer\/Pipeline: ', transformer_name, ',', \n                      transformer.__class__.__name__, '\\n')\n                print('\\tn_orig_feature_names:', len(orig_feature_names))\n\n            if transformer == 'drop':\n                    \n                continue\n                \n            if isinstance(transformer, Pipeline):\n                # if pipeline, get the last transformer in the Pipeline\n                transformer = transformer.steps[-1][1]\n\n            if hasattr(transformer, 'get_feature_names'):\n\n                if 'input_features' in transformer.get_feature_names.__code__.co_varnames:\n\n                    names = list(transformer.get_feature_names(orig_feature_names))\n\n                else:\n\n                    names = list(transformer.get_feature_names())\n\n            elif hasattr(transformer,'indicator_') and transformer.add_indicator:\n                # is this transformer one of the imputers & did it call the MissingIndicator?\n\n                missing_indicator_indices = transformer.indicator_.features_\n                missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n                                      for idx in missing_indicator_indices]\n                names = orig_feature_names + missing_indicators\n\n            elif hasattr(transformer,'features_'):\n                # is this a MissingIndicator class? \n                missing_indicator_indices = transformer.features_\n                missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n                                      for idx in missing_indicator_indices]\n\n            else:\n\n                names = orig_feature_names\n\n            if verbose: \n                print('\\tn_new_features:', len(names))\n                print('\\tnew_features:\\n', names)\n\n            new_feature_names.extend(names)\n            transformer_list.extend([transformer_name] * len(names))\n        \n        self.transformer_list, self.column_transformer_features = transformer_list,\\\n                                                                    new_feature_names\n\n        return new_feature_names\n\n    \n    def get_selected_features(self, verbose=None):\n        \"\"\"\n\n        Get the Feature Names that were retained after Feature Selection (sklearn.feature_selection)\n\n        Parameters\n        ----------\n        verbose : a boolean indicating whether to print summaries. default = False\n\n        Returns\n        -------\n        a list of the selected feature names\n\n\n        \"\"\"\n\n        if verbose is None:\n            verbose = self.verbose\n\n        assert isinstance(self.pipeline, Pipeline), \"Input isn't a Pipeline\"\n\n        features = self.get_feature_names()\n        \n        if verbose: print('\\n\\n---------\\nRunning get_selected_features\\n---------\\n')\n            \n        all_discarded_features, discarding_selectors = [], []\n\n        for i, step_item in enumerate(self.pipeline.steps[:]):\n            \n            step_name, step = step_item\n\n            if hasattr(step, 'get_support'):\n\n                if verbose: print('\\nStep ', i, \": \", step_name, ',', \n                                  step.__class__.__name__, '\\n')\n                    \n                check_is_fitted(step)\n\n                feature_mask_dict = dict(zip(features, step.get_support()))\n                \n                features = [feature for feature, is_retained in feature_mask_dict.items()\\\n                            if is_retained]\n                                         \n                discarded_features = [feature for feature, is_retained in feature_mask_dict.items()\\\n                                      if not is_retained]\n                \n                all_discarded_features.extend(discarded_features)\n                discarding_selectors.extend([step_name] * len(discarded_features))\n                \n                \n                if verbose: \n                    print(f'\\t{len(features)} retained, {len(discarded_features)} discarded')\n                    if len(discarded_features) > 0:\n                        print('\\n\\tdiscarded_features:\\n\\n', discarded_features)\n\n        self.discarded_features, self.discarding_selectors = all_discarded_features,\\\n                                                                discarding_selectors\n        \n        return features\n\n    def get_feature_importance(self):\n        \n        \"\"\"\n        Creates a Pandas Series where values are the feature importance values from the model and feature names are set as the index. \n        \n        This Series is stored in the `feature_importance` attribute.\n\n        Returns\n        -------\n        A pandas Series containing the feature importance values and feature names as the index.\n        \n        \"\"\"\n        \n        assert isinstance(self.pipeline, Pipeline), \"Input isn't a Pipeline\"\n\n        features = self.get_selected_features()\n             \n        assert hasattr(self.pipeline[-1], 'feature_importances_'),\\\n            \"The last element in the pipeline isn't an estimator with a feature_importances_ attribute\"\n        \n        importance_values = self.pipeline[-1].feature_importances_\n        \n        assert len(features) == len(importance_values),\\\n            \"The number of feature names & importance values doesn't match\"\n        \n        feature_importance = pd.Series(importance_values, index=features)\n        self.feature_importance = feature_importance\n        \n        # create feature_info_df\n        column_transformer_df =\\\n            pd.DataFrame(dict(transformer=self.transformer_list),\n                         index=self.column_transformer_features)\n\n        discarded_features_df =\\\n            pd.DataFrame(dict(discarding_selector=self.discarding_selectors),\n                         index=self.discarded_features)\n\n        importance_df = self.feature_importance.rename('value').to_frame()\n\n        self.feature_info_df = \\\n            column_transformer_df\\\n            .join([importance_df, discarded_features_df])\\\n            .assign(is_retained = lambda df: ~df.value.isna())        \n\n\n        return feature_importance\n        \n    \n    def plot(self, top_n_features=100, rank_features=True, max_scale=True, \n             display_imp_values=True, display_imp_value_decimals=1,\n             height_per_feature=25, orientation='h', width=750, height=None, \n             str_pad_width=15, yaxes_tickfont_family='Courier New', \n             yaxes_tickfont_size=15):\n        \"\"\"\n\n        Plot the Feature Names & Importances \n\n\n        Parameters\n        ----------\n\n        top_n_features : the number of features to plot, default is 100\n        rank_features : whether to rank the features with integers, default is True\n        max_scale : Should the importance values be scaled by the maximum value & mulitplied by 100?  Default is True.\n        display_imp_values : Should the importance values be displayed? Default is True.\n        display_imp_value_decimals : If display_imp_values is True, how many decimal places should be displayed. Default is 1.\n        height_per_feature : if height is None, the plot height is calculated by top_n_features * height_per_feature. \n        This allows all the features enough space to be displayed\n        orientation : the plot orientation, 'h' (default) or 'v'\n        width :  the width of the plot, default is 500\n        height : the height of the plot, the default is top_n_features * height_per_feature\n        str_pad_width : When rank_features=True, this number of spaces to add between the rank integer and feature name. \n            This will enable the rank integers to line up with each other for easier reading. \n            Default is 15. If you have long feature names, you can increase this number to make the integers line up more.\n            It can also be set to 0.\n        yaxes_tickfont_family : the font for the feature names. Default is Courier New.\n        yaxes_tickfont_size : the font size for the feature names. Default is 15.\n\n        Returns\n        -------\n        plot\n\n        \"\"\"\n        if height is None:\n            height = top_n_features * height_per_feature\n            \n        # prep the data\n        \n        all_importances = self.get_feature_importance()\n        n_all_importances = len(all_importances)\n        \n        plot_importances_df =\\\n            all_importances\\\n            .nlargest(top_n_features)\\\n            .sort_values()\\\n            .to_frame('value')\\\n            .rename_axis('feature')\\\n            .reset_index()\n                \n        if max_scale:\n            plot_importances_df['value'] = \\\n                                plot_importances_df.value.abs() \/\\\n                                plot_importances_df.value.abs().max() * 100\n            \n        self.plot_importances_df = plot_importances_df.copy()\n        \n        if len(all_importances) < top_n_features:\n            title_text = 'All Feature Importances'\n        else:\n            title_text = f'Top {top_n_features} (of {n_all_importances}) Feature Importances'       \n        \n        if rank_features:\n            padded_features = \\\n                plot_importances_df.feature\\\n                .str.pad(width=str_pad_width)\\\n                .values\n            \n            ranked_features =\\\n                plot_importances_df.index\\\n                .to_series()\\\n                .sort_values(ascending=False)\\\n                .add(1)\\\n                .astype(str)\\\n                .str.cat(padded_features, sep='. ')\\\n                .values\n\n            plot_importances_df['feature'] = ranked_features\n        \n        if display_imp_values:\n            text = plot_importances_df.value.round(display_imp_value_decimals)\n        else:\n            text = None\n\n        # create the plot \n        \n        fig = px.bar(plot_importances_df, \n                     x='value', \n                     y='feature',\n                     orientation=orientation, \n                     width=width, \n                     height=height,\n                     text=text)\n        fig.update_layout(title_text=title_text, title_x=0.5) \n        fig.update(layout_showlegend=False)\n        fig.update_yaxes(tickfont=dict(family=yaxes_tickfont_family, \n                                       size=yaxes_tickfont_size),\n                         title='')\n        fig.show()","d504faba":"1. This `pipe` instance contains the following 4 steps:\n\n1. The [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html) instance is composed of 3 Pipelines, containing a total of 4 transformer instances, including [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html), [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) & [GLMMEncoder](http:\/\/contrib.scikit-learn.org\/category_encoders\/glmm.html) from the [category_encoders](https:\/\/contrib.scikit-learn.org\/category_encoders\/) package. See my [previous blog post](https:\/\/towardsdatascience.com\/building-columntransformers-dynamically-1-6354bd08aa54) for a full explanation of how I dynamically constructed this particular ColumnTransformer.\n\n2. The [VarianceThreshold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html) uses the default threshold of 0, which removes any features that contain only a single value. Some models will fail if a feature has no variance.\n\n3. The [SelectPercentile](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectPercentile.html) uses the [f_regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.f_regression.html) scoring function with a percentile threshold of 90. These settings retain the top 90% of features and discard the bottom 10%.\n\n4. The [CatBoostRegressor](https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor.html) model is fit to the `SalesPrice` dependent variable using the features created and selected in the preceding steps.","a063a6ed":"## Code\n\nThe original notebook for this blog post can be found [here](https:\/\/www.kaggle.com\/kylegilde\/extracting-scikit-feature-names-importances). The complete code for `FeatureImportance` is shown below and can be found [here](https:\/\/www.kaggle.com\/kylegilde\/feature-importance).\n\nIf you create a Pipeline that you believe should be supported FeatureImportanceby but is not, please provide a reproducible example, and I will consider making the necessary changes.\n\nStay tuned for further posts on training & regularizing models with Scikit-Learn ColumnTransformers and Pipelines. Let me know if you found this post helpful or have any ideas for improvement. Thanks!","8567aaa6":"# Extracting & Plotting  Feature Names & Importance from Scikit-Learn Pipelines","dbdc646d":"## How It Works\n\nThe `FeatureImportance` class should be instantiated using a fitted Pipeline instance. (You can also change the `verbose` argument to `True` if you want to have all of the diagnostics printed to your console.) My class validates that this Pipeline starts with a `ColumnTransformer` instance and ends with a regression or classification model that has the `feature_importance_` attribute. As intermediate steps, the Pipeline can have any number or no instances of classes from [sklearn.feature_selection](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html).\n\nThe `FeatureImportance` class is composed of 4 methods.\n\n1. `get_feature_names` was the hardest method to devise. It iterates through the `ColumnTransformer` transformers, uses the `hasattr` function to discern what type of class we are dealing with and pulls the feature names accordingly. (Special Note: If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, it must come last in the pipeline. For example, OneHotEncoder, [MissingIndicator](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.MissingIndicator.html) & SimpleImputer(add_indicator=True) add columns to the dataset that didn't exist before, so they should come last in the Pipeline.)\n\n2. `get_selected_features` calls `get_feature_names`. Then it tests for whether the main Pipeline contains any classes from sklearn.feature_selection based upon the existence of the `get_support` method. If it does, this method returns only the features names that were retained by the selector class or classes. It stores the features that were not selected in the `discarded_features` attribute. Here are the features that were removed by the selectors in my pipeline:","bf3c773c":"4. `plot` calls `get_feature_importance` and plots the output based upon the specifications.","198d2c29":"If you have ever been tasked with productionalizing a machine learning model, you probably know that Scikit-Learn library offers one of the best ways -- if not the best way -- of creating production-quality machine learning workflows. The ecosystem's [Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html), [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html), [preprocessors](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing), [imputers](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.impute) & [feature selection](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection) classes are powerful tools that transform raw data into model-ready features.\n\nHowever, before anyone is going to let you deploy to production, you are going to want to have some minimal understanding of how the new model works. The most common way to explain how a black-box model works is by plotting feature names and importance values. If you have ever tried to extract the feature names from a heterogeneous dataset processed by ColumnTransformer, you know that this is no easy task. Exhaustive Internet searches have only brought to my attention where others have [asked](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6424) [the](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6431) [same](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/12627) [question](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13307) or offered a [partial answer](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12525), instead of yielding a comprehensive and satisfying solution. \n\nTo remedy this situation, I have developed a class called `FeatureImportance` that will extract feature names and importance values from a Pipeline instance. It then uses the Plotly library to plot the feature importance using only a few lines of code. In this post, I will load a fitted Pipeline, demonstrate how to use my class and then give an overview of how it works. The complete code can be found [here](https:\/\/www.kaggle.com\/kylegilde\/feature-importance) or at the end of this blog post.\n\nThere are two things I should note before continuing:\n\n1. I credit Joey Gao's code on [this thread](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12525#issuecomment-436217100) with showing the way to tackle this problem.\n\n2. My post assumes that you have worked with Scikit-Learn and Pandas before and are familiar with how ColumnTransformer, Pipeline & preprocessing classes facilitate reproducible feature engineering processes. If you need a refresher, check out this [Scikit-Learn example](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html).\n\n## Creating a Pipeline\n\n\nFor the purposes of demonstration, I've written a script called [fit_pipeline_ames.py](https:\/\/www.kaggle.com\/kylegilde\/fit-pipeline-ames). It loads the [Ames housing training data from Kaggle](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) and fits a moderately complex Pipeline. I've plotted its visual representation below.","890ae535":"The `plot` method takes a number of arguments that control the plot's display. The most important ones are the following:\n\n- `top_n_features`: This controls how many features will be plotted. The default value is 100. The plot's title will indicate this value as well as how many features there are in total. To plot all features, just set `top_n_features` to a number larger than the total features. \n\n- `rank_features`: This argument controls whether the integer ranks are displayed in front of the feature names. The default is `True`. I find that this aids with interpretation, especially when comparing the feature importance from multiple models.\n\n- `max_scale`: This determines whether the importance values are scaled by the maximum value & multiplied by 100. The default is `True`. I find that this enables an intuitive way to compare how important other features are vis-a-viz the most important one. For instance, in the plot of above, we can say that `GrLivArea` is about 81% as important to the model as the top feature, `OverallQty`.","af35bdf5":"## Plotting FeatureImportance\n\n\nWith the help of FeatureImportance, we can extract the feature names and importance values and plot them with 3 lines of code.","f8d75978":"3. `get_feature_importance` calls `get_selected_features` and then creates a Pandas Series where values are the feature importance values from the model and its index is the feature names created by the first 2 methods. This Series is then stored in the `feature_importance` attribute."}}