{"cell_type":{"8b3cfe82":"code","8394d689":"code","9ae8b4c7":"code","02b82b17":"code","645a45f8":"code","a8ef1b33":"code","91550df5":"code","9d022e46":"code","18f71ef3":"code","eae202ff":"code","dc9b8407":"code","939a73ee":"code","548a1036":"code","a320ea3e":"code","5ea3a01f":"code","08af0657":"code","da338251":"code","394290ba":"code","37970ad6":"code","e6b0a825":"code","f901d1c5":"code","0f673063":"code","84118e9a":"markdown","2c70e6d1":"markdown","860581be":"markdown","ea0772d3":"markdown","fd539ef7":"markdown","d7b1cd82":"markdown","f92133df":"markdown"},"source":{"8b3cfe82":"!pip install jcopdl","8394d689":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import mobilenet_v2\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\n\nfrom tqdm.auto import tqdm\nfrom jcopdl.callback import Callback, set_config\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom PIL import Image\nimport os\n\nimport pandas as pd\nimport numpy as np\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","9ae8b4c7":"PATH = '..\/input\/hanacaraka\/'\nimages, labels = [], []\nfor dirname, _, filenames in os.walk(PATH):\n  for filename in filenames:\n    images.append(os.path.join(dirname, filename).split('\/')[-1]) # \/folder\/blabla\/xxx.jpg\n    labels.append(os.path.join(dirname, filename).split('\/')[-2])","02b82b17":"label2cat = np.unique(labels)\nlabel2cat","645a45f8":"label2cat = np.delete(label2cat, np.where(label2cat == 'hanacaraka'))\nimages = np.delete(images, [0, 1, 2])\nlabels = np.delete(labels, [0, 1, 2])\nlabel2cat","a8ef1b33":"ints = np.arange(0, len(label2cat))\ndicts = dict(zip(label2cat, ints))\ndicts","91550df5":"datas = pd.DataFrame({'image_id':images, 'label':labels})\ndatas.head(), datas.shape","9d022e46":"index = []\nfor i in range(len(datas)):\n    try:\n        Image.open(PATH + str(datas['label'].values[i]) + '\/' + str(datas['image_id'].values[i]))\n        \n    except PIL.UnidentifiedImageError:\n        index.append(i)\n  \n    except FileNotFoundError:\n        index.append(i)\n\ndatas = datas.drop(index)\ndatas.head(), datas.shape","18f71ef3":"X_train, X_val, y_train, y_val = train_test_split(\n    datas['image_id'].values, datas['label'].values, \n    test_size=0.3, stratify=datas['label'].values, \n    random_state=24)\n\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val,\n    test_size=0.5, stratify=y_val, \n    random_state=24)\n\n\nlen(X_train), len(X_val), len(X_test)","eae202ff":"class customDataset(Dataset):\n    def __init__(self, x, y, path, maps_label=dicts, transform=None):\n        self.X = x\n        self.y = y\n        self.path = path\n        self.dicts = maps_label\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        img = Image.open(self.path + str(self.y[idx]) + '\/' + str(self.X[idx])).convert('RGB')\n        label = self.y[idx]\n        label = self.label_2_ints(label)\n        \n        if self.transform is not None:\n            img = self.transform(img)\n            \n        return img, label\n    \n    def label_2_ints(self, x):\n        label_id = None\n        for key, values in self.dicts.items():\n            if x == key:\n                label_id = values\n        return label_id\n    \n    def __len__(self):\n        return len(self.X)","dc9b8407":"crop_size = 224\nbs = 64\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(10),\n    transforms.RandomResizedCrop(crop_size, scale=(0.9, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\nval_transform = transforms.Compose([\n    transforms.Resize(230),\n    transforms.CenterCrop(crop_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\ntrain_set = customDataset(X_train, y_train, PATH, transform=train_transform)\ntrainloader = DataLoader(train_set, batch_size=bs, shuffle=True, num_workers=2)\n\nval_set = customDataset(X_val, y_val, PATH, transform=val_transform)\nvalloader = DataLoader(val_set, batch_size=bs, shuffle=True)\n\ntest_set = customDataset(X_test, y_test, PATH, transform=val_transform)\ntestloader = DataLoader(test_set, shuffle=True)","939a73ee":"feature, target = next(iter(trainloader))\nfeature.shape, len(trainloader)","548a1036":"fig, axes = plt.subplots(2,5, figsize=(15, 7))\nfor img, label, ax in zip(feature, target, axes.flatten()):\n  ax.imshow(img.permute(1,2,0).cpu())\n  font = {\"color\":'g'}\n  label = label2cat[label.item()]\n  ax.set_title(f\"Label: {label}\", fontdict=font);\n  ax.axis(\"off\");","a320ea3e":"class CustomMobileNetv2(nn.Module):\n  def __init__(self, output_size):\n    super().__init__()\n    self.mnet = mobilenet_v2(pretrained=True)\n    self.freeze()\n\n    self.mnet.classifier = nn.Sequential(\n        nn.Linear(1280, 256),\n        nn.ReLU(),\n        nn.Linear(256, output_size),\n        nn.LogSoftmax(1)\n    )\n\n  def forward(self, x):\n    return self.mnet(x)\n  \n  def freeze(self):\n    for param in self.mnet.parameters():\n      param.requires_grad = False\n\n  def unfreeze(self):\n    for param in self.mnet.parameters():\n      param.requires_grad = True","5ea3a01f":"config = set_config({\n    'batch_size': bs,\n    'crop_size': crop_size,\n    'output_size': len(label2cat)\n})","08af0657":"model = CustomMobileNetv2(config.output_size).to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001)\ncallback = Callback(model, config, early_stop_patience=2, outdir='model')","da338251":"def loop_fn(mode, dataset, dataloader, model, criterion, optimizer, device):\n  if mode == 'train':\n    model.train()\n  elif mode == 'val':\n    model.eval()\n  \n  cost = correct = 0\n  for feature, target in dataloader:\n    feature, target = feature.to(device), target.to(device)\n    output = model(feature)\n    loss = criterion(output, target)\n\n    if mode == 'train':\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n    \n    cost += loss.item() * feature.shape[0]\n    correct += (output.argmax(1) == target).sum().item()\n  cost = cost\/len(dataset)\n  acc = correct\/len(dataset)\n  return cost, acc","394290ba":"while True:\n  train_cost, train_score = loop_fn('train', train_set, trainloader, model, criterion, optimizer, device)\n  with torch.no_grad():\n    test_cost, test_score = loop_fn('val', val_set, valloader, model, criterion, optimizer, device)\n\n  # Logging\n  callback.log(train_cost, test_cost, train_score, test_score)\n\n  # Checkpoint\n  callback.save_checkpoint()\n\n  # Runtime Plotting\n  callback.cost_runtime_plotting()\n  callback.score_runtime_plotting()\n\n  # Early Stopping\n  if callback.early_stopping(model, monitor='test_score'):\n    callback.plot_cost()\n    callback.plot_score()\n    break","37970ad6":"model.unfreeze()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\ncallback.reset_early_stop()\ncallback.early_stop_patience = 6","e6b0a825":"while True:\n  train_cost, train_score = loop_fn('train', train_set, trainloader, model, criterion, optimizer, device)\n  with torch.no_grad():\n    test_cost, test_score = loop_fn('val', val_set, valloader, model, criterion, optimizer, device)\n\n  # Logging\n  callback.log(train_cost, test_cost, train_score, test_score)\n\n  # Checkpoint\n  callback.save_checkpoint()\n\n  # Runtime Plotting\n  callback.cost_runtime_plotting()\n  callback.score_runtime_plotting()\n\n  # Early Stopping\n  if callback.early_stopping(model, monitor='test_score'):\n    callback.plot_cost()\n    callback.plot_score()\n    break","f901d1c5":"for feature, target in valloader:\n  feature, target = feature.to(device), target.to(device)\n  with torch.no_grad():\n    model.eval()\n    output = model(feature)\n    preds = output.argmax(1)\n\nfig, axes = plt.subplots(6, 6, figsize=(24, 24))\nfor img, label, pred, ax in zip(feature, target, preds, axes.flatten()):\n  ax.imshow(img.permute(1,2,0).cpu())\n  font = {\"color\":'r'} if label != pred else {\"color\": 'g'}\n  label, pred = label2cat[label.item()], label2cat[pred.item()]\n  ax.set_title(f\"Label: {label}\\nPred: {pred}\", fontdict=font);\n  ax.axis(\"off\");","0f673063":"accuracy = []\nfor feature, target in testloader:\n  feature, target = feature.to(device), target.to(device)\n  with torch.no_grad():\n    model.eval()\n    output = model(feature)\n    preds = output.argmax(1)\n    if(target == preds):\n      acc = 1\n    else: acc = 0\n    accuracy.append(acc)\naccuracy = np.array(accuracy)\naccuracy.mean()","84118e9a":"# **Adaptation**","2c70e6d1":"# **Predict**","860581be":"# **Dataset & Dataloader**","ea0772d3":"# **Data Preparation**","fd539ef7":"# **Fine-Tuning**","d7b1cd82":"# **Architecture & Config**","f92133df":"# **Test Score**"}}