{"cell_type":{"f84916f4":"code","a149d76d":"code","c09ddd2c":"code","faa7c691":"code","d908bac6":"code","ed67eece":"code","ff10d3bb":"code","ce3c5192":"code","50e0bb32":"code","20135bd2":"code","1d1d41fc":"code","45108772":"code","1157c7c5":"code","45840074":"code","9f24a009":"code","5d6167a6":"code","7c5c153b":"code","83effd52":"code","9ebae3e4":"code","4b66ebb9":"code","82384302":"code","4c299d87":"markdown","59a5fdaf":"markdown","c503691c":"markdown","88ff09b6":"markdown","3be03d3a":"markdown","695960a7":"markdown","fd805461":"markdown","82b2d84c":"markdown","1d784e28":"markdown"},"source":{"f84916f4":"from math import sqrt\nfrom numpy import concatenate\nfrom numpy import datetime64\nfrom numpy import timedelta64\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom pandas import merge\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom pandas import to_datetime\nfrom pandas import DateOffset\nfrom datetime import datetime\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import LSTM\nfrom tensorflow.python.keras.preprocessing.sequence import TimeseriesGenerator\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","a149d76d":"dataset = read_csv('..\/input\/watershed-water-quality-data.csv')\ndataset.head()\n\n\n","c09ddd2c":"\ntwelve_am_data_set = dataset[['Date','Turbidity(NTU) at 12AM','Coliform, Fecal(fc\/100mL)']]\nfour_am_data_set = dataset[['Date','Turbidity(NTU) at 4AM','Coliform, Fecal(fc\/100mL)']]\neight_am_data_set = dataset[['Date','Turbidity(NTU) at 8AM','Coliform, Fecal(fc\/100mL)']]\ntwelve_pm_data_set = dataset[['Date','Turbidity(NTU) at 12PM','Coliform, Fecal(fc\/100mL)']]\nfour_pm_data_set = dataset[['Date','Turbidity(NTU) at 4PM','Coliform, Fecal(fc\/100mL)']]\neight_pm_data_set = dataset[['Date','Turbidity(NTU) at 8PM','Coliform, Fecal(fc\/100mL)']]\n\n#Change Date by adding hour of measurement\n\ntwelve_am_data_set['Date'] = to_datetime(twelve_am_data_set['Date'])\n\nfour_am_data_set['Date'] = to_datetime(four_am_data_set['Date'])\nfour_am_data_set['Date'] = four_am_data_set['Date'] + DateOffset(hours=4)\n\neight_am_data_set['Date'] = to_datetime(eight_am_data_set['Date'])\neight_am_data_set['Date'] = eight_am_data_set['Date'] + DateOffset(hours=8)\n\ntwelve_pm_data_set['Date'] = to_datetime(twelve_pm_data_set['Date'])\ntwelve_pm_data_set['Date'] = twelve_pm_data_set['Date'] + DateOffset(hours=12)\n\nfour_pm_data_set['Date'] = to_datetime(four_pm_data_set['Date'])\nfour_pm_data_set['Date'] = four_pm_data_set['Date'] + DateOffset(hours=16)\n\neight_pm_data_set['Date'] = to_datetime(eight_pm_data_set['Date'])\neight_pm_data_set['Date'] = eight_pm_data_set['Date'] + DateOffset(hours=20)\n\ntwelve_am_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\nfour_am_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\neight_am_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\ntwelve_pm_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\nfour_pm_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\neight_pm_data_set.columns = [\"Date\", \"Turbidity\",'Fecal']\n\ncomplete_data_set= concat([twelve_am_data_set,four_am_data_set, eight_am_data_set,twelve_pm_data_set,four_pm_data_set,eight_pm_data_set], axis=0, join='outer', ignore_index=False)\n\ncomplete_data_set = complete_data_set.sort_values(by=['Date'])\ncomplete_data_set.head(10)\n","faa7c691":"complete_data_set.Turbidity.unique()","d908bac6":"complete_data_set.Fecal.unique()","ed67eece":"pd.value_counts(complete_data_set['Fecal']).plot.bar(figsize=(10,5))","ff10d3bb":"\ncomplete_data_set = complete_data_set.drop(['Fecal'], axis=1)\ncomplete_data_set.info()","ce3c5192":"complete_data_set['Turbidity'].plot.hist(bins=50,figsize = (10,10))","50e0bb32":"df = complete_data_set.set_index('Date')","20135bd2":"df['Turbidity'].plot(figsize = (20,10))","1d1d41fc":"df[:180]['Turbidity'].plot(figsize = (20,10))","45108772":"\nnan_rows = df[df['Turbidity'].isnull()]\nnan_rows","1157c7c5":"df= df.fillna(method='ffill')\ndf.isna().sum()\n","45840074":"#remove first row which is also null\ndf = df.iloc[1:]\ndf.isna().sum()","9f24a009":"values = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(values)\n\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]","5d6167a6":"train_data_gen = TimeseriesGenerator(train, train,\n\tlength=2, sampling_rate=1,stride=1,\n    batch_size=3)\ntest_data_gen = TimeseriesGenerator(test, test,\n\tlength=2, sampling_rate=1,stride=1,\n\tbatch_size=1)","7c5c153b":"model = Sequential()\nmodel.add(LSTM(4, input_shape=(2, 1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory = model.fit_generator(train_data_gen, epochs=50).history","83effd52":"model.evaluate_generator(test_data_gen)\ntrainPredict = model.predict_generator(train_data_gen)\ntrainPredict.shape","9ebae3e4":"testPredict = model.predict_generator(test_data_gen)\ntestPredict.shape","4b66ebb9":"\n#return values to their pre-normalized form\ninv_trainPredict = scaler.inverse_transform(trainPredict)\ninv_testPredict= scaler.inverse_transform(testPredict)\n\n\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[2:len(trainPredict)+2, :] = inv_trainPredict\n\n\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(2*2):len(dataset), :] = inv_testPredict\n\npyplot.figure(figsize = (20, 10))\npyplot.plot(trainPredictPlot, label=\"trainPredict\")\npyplot.plot(testPredictPlot,label=\"testPredict\")\npyplot.plot(df['Turbidity'].values, label=\"real\")\npyplot.xlabel(\"Date\")\npyplot.ylabel(\"Turbidity\")\npyplot.title(\"Comparison \")\npyplot.legend()\npyplot.show()\n\n\n","82384302":"\npyplot.figure(figsize = (20, 10))\npyplot.plot(trainPredictPlot[6600:6630], label=\"trainPredict\")\npyplot.plot(testPredictPlot[6600:6630],label=\"testPredict\")\npyplot.plot(df[6600:6630]['Turbidity'].values, label=\"real\")\npyplot.xlabel(\"4-hour measurements\")\npyplot.ylabel(\"Turbidity\")\npyplot.title(\"Comparison \")\npyplot.legend()\npyplot.grid(True,which='both')\npyplot.show()\n","4c299d87":"\nSince the Fecal column has values that are not numbers, such as 'E1' 'E2' etc., We can't pass it to the LSTM unless we do something such as one-hot coding.\nHowever, this wouldn't make much sense as the number values are measurements. The E values are either Errors or they mean something field-specific.\nWe could omit them in training if they do not amount to much in the total dataset:","59a5fdaf":"<font size=5>Analysis<font>\n1. The model works but could use much more fine-tuning. This model looks back 8 hours in the past to predict the present turbidity.  Looking back 4 hours would probably lead to a better result if we had measurements taken every hour as opposed to in 4 hour increments.\n2. More Data is needed as well as longer training time and \n3. The Fecal Coliform Data would also help in finding patterns that the lstm is missing\n   ","c503691c":"<font size=5>Taking each row and making new rows for each 4 hour time value<\/font>\n\n* Since the Turbidity values are taken every 4 hours, we can turn this dataset into 4 hour segments as opposed to rows that are split by Days.","88ff09b6":"<font size=5>Plotting The Results<\/font>","3be03d3a":"<font size=5>Check for Null Values and replace them with previous result<\/font>","695960a7":"<font size=5>Preparing Data For Keras LSTM Network<\/font>","fd805461":"Since the vast majority of the values are not numbers, we will omit this data for the rest of the project.","82b2d84c":"<font size=5>Testing The Model<\/font>","1d784e28":"<font size=5>Lets take a closer look of the first month <\/font>"}}