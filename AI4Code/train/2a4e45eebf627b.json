{"cell_type":{"3483b74d":"code","411b5e52":"code","3e4b627c":"code","ec42f6b4":"code","a9018497":"code","56d698e3":"code","eebb7ac8":"code","0e8b6a7b":"code","2bceac49":"code","b6070c7c":"code","a0ee30b4":"code","4428f373":"code","bf947f01":"code","cccd1893":"code","2ed6240c":"code","9a3993e7":"code","43c7bebe":"code","0388a836":"code","35d0ed29":"code","2d0b4319":"code","230bab98":"code","88e3fc15":"code","86d7f6c5":"code","3efbbf87":"code","77491fb4":"code","47704913":"code","22a2d9ca":"code","dff097da":"code","5e1aa9ee":"code","6bf320ee":"code","61ad67df":"code","bd4c98ab":"code","99011e47":"code","3edf9449":"code","f346a7f5":"code","17627e38":"code","48437968":"code","0f381470":"code","2c062517":"code","35363201":"code","066e2aa4":"code","45a7bea5":"code","0ba7c279":"code","bf273808":"code","e5461a60":"code","dd4d2592":"code","d9b47900":"code","da97fc38":"code","2bb1db61":"code","94b32750":"code","64ef588c":"code","8d316599":"markdown","73f2f221":"markdown","4932535c":"markdown","da5790ff":"markdown","1b3caeb4":"markdown","f84d62f7":"markdown","b1c737de":"markdown","34130fec":"markdown","1f6b668c":"markdown","e65853fe":"markdown","03d6f990":"markdown","f6a5fa13":"markdown","5fda4835":"markdown","cf1c8f4e":"markdown","16fb5a29":"markdown","88184072":"markdown","e3b282c7":"markdown","e85a18b4":"markdown","ef49594d":"markdown","20dbaa4f":"markdown","5333c98b":"markdown","7245cb84":"markdown","04aafd70":"markdown","33e987ca":"markdown","0870a5bd":"markdown","649320e9":"markdown","dc58d34c":"markdown","ff2426bc":"markdown","5d3154cd":"markdown","7a675368":"markdown","cc610a89":"markdown","38c31170":"markdown","5adafb10":"markdown","c828dae1":"markdown","fb598498":"markdown"},"source":{"3483b74d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","411b5e52":"#import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","3e4b627c":"#import and load dataset\ndata=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","ec42f6b4":"#view the data in summary of top 5 rows with corresponding column names\ndata.head()","a9018497":"#drop ID column as it does not have a corrilation with dignosis which is what we want to classify\ndata.drop('id',axis=1,inplace=True)","56d698e3":"#checking for how our data looks in an informative way\ndata.info()","eebb7ac8":"#vieweing the shape of our data set\ndata.shape","0e8b6a7b":"#vieweing the data type distribution of our data set\ndata.dtypes.value_counts()","2bceac49":"#viewing the column names inside our data\ndata.columns","b6070c7c":"#checking for missing values\ndata.isna().sum()","a0ee30b4":"#drop unnamed column \ndata.drop(['Unnamed: 32'],axis=1,inplace=True)","4428f373":"#our data now has no missing values\ndata.isna().sum()","bf947f01":"#Getting a statistical decription of our data \ndata.describe()","cccd1893":"from sklearn.preprocessing import LabelEncoder\nlbl_enc = LabelEncoder()\ndata['diagnosis_enc']=lbl_enc.fit_transform(data[['diagnosis']])\n","2ed6240c":"#The new column name diagnosis_enc icludes encoded column values from the diagnosis column\ndata.head()\n","9a3993e7":"#Getting to view the corrilation on our data set\nplt.figure(figsize=(20,10))\nsns.heatmap(data.corr(),annot=True, fmt=\".2f\",annot_kws={\"size\":10},linewidths=.7)","43c7bebe":"#getting the correlation values  to our target variable - diagnosis_enc\ncor_target=abs(data.corr()['diagnosis_enc'])","0388a836":"#view all the feature columns values with a correlation to diagnosis column\ncor_target","35d0ed29":"#filtering features that have a strong correlation to our target column i.e >5\ncor_target[cor_target>0.5]","2d0b4319":"feature_cols=data[['radius_mean','perimeter_mean','area_mean','compactness_mean',\n                'concavity_mean','concave points_mean','radius_se','perimeter_se','area_se','radius_worst',\n                'perimeter_worst','area_worst','compactness_worst','concave points_worst','diagnosis_enc','diagnosis']]","230bab98":"\n#in plotting the heatmap to see how the features corrilate to each other \nplt.figure(figsize=(20,10))\nsns.heatmap(feature_cols.corr(),annot=True,fmt=\".2f\", annot_kws={\"size\":10},linewidths=.7)","88e3fc15":"#get to view our our diagnosis column values are distributed\ndata.diagnosis.value_counts()","86d7f6c5":"#plotting the distribution on an histogram\ndata.diagnosis.hist()","3efbbf87":"#plotting subplots of our target column\nf,ax=plt.subplots(1,2,figsize=(10,5))\ndata['diagnosis'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('diagnosis')\nax[0].set_ylabel('')\nsns.countplot('diagnosis',data=data,ax=ax[1])\nax[1].set_title('diagnosis')\nplt.show()","77491fb4":"#plotting radius features of the cell in relation to  the diagnosis type\nsns.pairplot(feature_cols[['radius_mean','radius_se','radius_worst','diagnosis']],hue='diagnosis')","47704913":"#features related to perimeter in relation to the diagnosis type\nsns.pairplot(feature_cols[['perimeter_mean','perimeter_se','perimeter_worst','diagnosis']],hue='diagnosis')","22a2d9ca":"#features related to concave and concativity\nsns.pairplot(feature_cols[['concavity_mean','concave points_mean','concave points_worst','diagnosis']],hue='diagnosis')","dff097da":"#features related to area in relation to the diagnosis type\nsns.pairplot(feature_cols[['area_mean','area_worst','area_se','diagnosis']],hue='diagnosis')","5e1aa9ee":"#instanciate LogisticRegression\nlogreg=LogisticRegression(solver='liblinear')","6bf320ee":"#We shall use our feature cols data set as it showed the columns values with strong correlation to our output variable. \nfeature_cols.columns","61ad67df":"#creating our X feature variables and y outcome variable\nX= feature_cols.drop(['diagnosis','diagnosis_enc'],axis=1)\ny=feature_cols['diagnosis_enc']","bd4c98ab":"#splitting our data into training and testing data set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)","99011e47":"#fitting our training dataset\nlogreg.fit(X_train,y_train)","3edf9449":"#using our model to predict our testing values and training values\npred=logreg.predict(X_test)\ntrain=logreg.predict(X_train)","f346a7f5":"#evaluating our model with data some training data\naccuracy_score(y_train,train)","17627e38":"#evaluating our model with data some testing data\nlogreg_score=accuracy_score(y_test,pred)","48437968":"#defining our DTC model\ndtc_model=DecisionTreeClassifier()","0f381470":"#defining feature and outcome variables \nX= feature_cols.drop(['diagnosis','diagnosis_enc'],axis=1)\ny=feature_cols['diagnosis_enc']","2c062517":"#splitting our data into training and testing data \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)","35363201":"#fitting training values into our model\ndtc_model.fit(X_train,y_train)\n","066e2aa4":"#using our model to predict based on testing data\ndtc_pred=dtc_model.predict(X_test)\ndtc_train=dtc_model.predict(X_train)\n","45a7bea5":"#Evaluating our model  with unseen testing data values\ndtc_score=accuracy_score(y_test,dtc_pred)\n","0ba7c279":"#evaluating our model with training values (seen data)\naccuracy_score(y_train,dtc_train)","bf273808":"#define model\nsvm_model=SVC(kernel='rbf')","e5461a60":"#defining feature and outcome variables \nX= feature_cols.drop(['diagnosis','diagnosis_enc'],axis=1)\ny=feature_cols['diagnosis_enc']","dd4d2592":"#splitting our data into training and testing data \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)","d9b47900":"#fitting our training values into the model\nsvm_model.fit(X_train,y_train)","da97fc38":"#using our model to predcit based on test values\nsvm_pred=svm_model.predict(X_test)\nsvm_train=svm_model.predict(X_train)","2bb1db61":"#checking the accuracy score of our model based on testing data\nsvm_score=accuracy_score(y_test,svm_pred)\n","94b32750":"#checking the accuracy score of our model based on training data\naccuracy_score(y_train,svm_train)","64ef588c":"print(\"The accuracy score of Logistic Regression is:\",round(logreg_score,2))\nprint(\"The accuracy score of Decision Tree Classifier  is:\",round(dtc_score,2))\nprint(\"The accuracy score of Support Vector Machine Classifier is:\",round(svm_score,2))","8d316599":"From the cells in the matrix the darker shades of the color indicate smaller values while brighter shades correspond to larger values near to 1.\n\nFrom our correlation matrix where there is a large positive near to 1.0 it indicates a strong positive correlation.Meaning when a value of one of the variables increases, the value of the other variable will increase as well.\n\nWhere there is a negative value near to -1.0 it  indicates a strong negative correlation that means  the value of one variable will decreases with the increasing\/decreasing of the other.\n\nWhile a value near to 0 whether positive or negative indicates the absence of any correlation between the two variables they are  independent of each other.\n\n","73f2f221":"**3) Splitting our data**","4932535c":"    It returns a score of 0.956 rounded off 0.96.The best score is because we are evaluating it with data it's trained on.\n    The true measure should be on data it has not seen (Testing data)","da5790ff":"# # # # # Evaluating SVM","1b3caeb4":"**plotting different cell features in relation to the diagnosis type**","f84d62f7":"# # # # # # Findings","b1c737de":"# # # # **Data Exploration**","34130fec":"We have breast cancer type of Benign with 357 outcomes and Malignant type with 212 outcomes from our data.\n","1f6b668c":"    Brief information about Support Vector Machine.\nAlso known as SVM is another algorithm used for both classification as well as regression problems but is widely used for classification tasks.\n\nA support vector machine model  is able to generalize between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes.\n\nThere can be many hyperplanes that can do this task but the objective is to find that hyperplane that has the highest margin that means maximum distances between the two classes, so that in future if a new data point comes that is to be classified then it can be classified easily.\n\n\n","e65853fe":"In comparing the features of radius_mean in relation to the diagnosis type we see that Benign type has the highest number of radius_mean values.\n\nWhen it comes to radius_se there are two value outliers in our malignant value as most values fall below 2.0 value.","03d6f990":"    Here we find plot the columns with the strongest correlation greater than 0.5 based on our target           column(diagnosis_enc )to see how they relate with each other.\n    \n    Worth also noting is the compactness_worst column has less positive correlation less than 0.5 to other feauters apart from columns compactness_mean,concavity_mean,concave points_mean and perimeter_worst.Later we can see if this feature has an impact in our overall result to our model based on this.\n\n","f6a5fa13":"    Our data is composed of 31 columns and 569 rows.\n    The dignosis column that we want to classify is of object data type.\n    There is a column called unnamed with missing values (this needs to be removed)","5fda4835":"Decision Tree returned a higher score 94% compared to logistc regression next we shall be using Support Vector Machine model to see how it performs on our data in classifying beast cancer diagnosis.","cf1c8f4e":"A Decision Tree Algorithm Decision can be used for both classification or regression and uses a form of a tree structure. It breaks down a dataset into smaller and smaller subsets beginning from the root (starting point) up to the leaf node(end point)and it gives values based on traversing various nodes based on set conditions until the ideal one is chosen.","16fb5a29":"# # # # # ****Support Vector Machine Model****","88184072":"The notebook consists of exploratoty data analysis based on Breast Cancer Wisconsin diagnosis dataset.\n\nDiagnosis denote (M = malignant, B = benign)\nThe features  computed for each cell nucleus include:\n\n    1) radius (mean of distances from center to points on the perimeter)\n    2) texture (standard deviation of gray-scale values)\n    3) perimeter\n    4) area\n    5) smoothness (local variation in radius lengths)\n    6) compactness (perimeter^2 \/ area - 1.0)\n    7) concavity (severity of concave portions of the contour)\n    8) concave points (number of concave portions of the contour)\n    9) symmetry\n    10)fractal dimension (\"coastline approximation\" - 1)\n\nLater I shall use Logistic regression,Decision Trees and Support Vector Machine algorithms to predict classification of the breast cancer diagnosis.\n\nLooking forward to your feedback in the comment section also kindly UPVOTE and motivate a beginner here!!!","e3b282c7":"# # # # # **Decision Tree Classifer Model**","e85a18b4":"**2) Define Feature and outcome variables**","ef49594d":"The description works for numerical data values to include diagnosis column values we wil need to transform from categorical to numerical values.Also known as label encoding.\n\nLabel encoding: assign each unique category in a categorical variable with an integer. No new columns are created unlike hot encoding where each unique category variable has a new column.\n\nThe reason for us to use label encoding is that we only have two unique values for a categorical variable (M and B) representing Malignant and Benign for more than 2 unique categories it's advicable to use  one-hot encoding though it adds many coulmns in the event of many coulmns something to concider.","20dbaa4f":"# # # # # **1) Logistic Regression**","5333c98b":"# # # # **Model Building**","7245cb84":"SVM with a kernel function of rbf(Radial Basis Function) returned a value of 90% .\n\nIn conclusion Decision Tree Classifier performed better than both logistic regression and SVM at 94%.","04aafd70":"This usally forms a good way in determining the features to use in our feature engineering to give a good result from our models.","33e987ca":"**4) Model Training on data**","0870a5bd":"# # # # # **Distribution of our Target column**","649320e9":"# # # # # #Evaluating Decision Tree Classifier ","dc58d34c":"**6)Model Evaluation**","ff2426bc":"There are two value outliers in our perimeter_se with malignant values","5d3154cd":" If the data is non linearly separable SVM makes use of kernel tricks to make it linearly separable. \n Kernel tricks help in projecting data points to the higher dimensional space by which they became     relatively more easily separable in higher-dimensional space.\n \n While using the svm classifier we can take the kernel as \u2018linear\u2019 , \u2019poly\u2019 , \u2018rbf\u2019(Radial Basis Function) , \u2018sigmoid\u2019.\n \n The default on SVM is rbf","7a675368":"Our Logistic regression model returns a score of 0.91. Which shows our model gets atleast to classify correctly at 90% the diagnosis of breast cancer types.\n\nLater I shall work on Decision Tree Classifier and Support Vector Machine Algorithms to see how it they fair compared to logistc regression.","cc610a89":"# # # # **Data Visualization**","38c31170":"# # # # # **Different Model Scores**","5adafb10":"I welcome critic ,suggestion and atleast an UPVOTE as I keep learning.The classification algorithm used here are the ones I have been learning the past 6 months.","c828dae1":"**Model prediction on data**","fb598498":"# # # # # # **Findings**"}}