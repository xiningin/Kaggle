{"cell_type":{"cd1db320":"code","30e54040":"code","09589176":"code","aead86e1":"code","55c0687b":"code","f53fb2be":"code","da2170d8":"code","c9fc9776":"code","a447298a":"code","1a150407":"code","ed9142cf":"code","bf278b7e":"code","ba6d543d":"code","86bf8803":"code","9af185c1":"code","67a02e9f":"code","6f2c22f7":"code","876b7da9":"code","45f11069":"code","64608396":"code","1b0bcad7":"code","87c405a6":"code","70c80fd2":"code","5562b8d8":"code","b3486328":"code","4d4cfc85":"code","ede55535":"code","097e8d18":"code","4612f2b7":"code","94b32039":"code","a745a0fe":"code","1ac58d69":"code","cc2645f4":"code","9386c9c2":"code","6d635943":"code","df55fbd9":"code","4cd1511a":"code","08a695ca":"code","533b5916":"code","a49a1a46":"code","803532aa":"code","f6a684da":"code","6fe372bf":"code","37b22afe":"code","f5a96c52":"code","4b763c56":"code","c892127f":"code","851fabba":"code","7c613f7c":"code","4135947e":"code","fc1e9913":"code","3da9d561":"code","77c6f312":"code","9766e7ef":"markdown","e46ce4d9":"markdown","7822b4f6":"markdown","bd7424d0":"markdown","384c8cfd":"markdown","de6bb007":"markdown","fd9ab155":"markdown","7d8e2521":"markdown","74e487ab":"markdown","4eaa77b2":"markdown","01bd37ed":"markdown","36154428":"markdown","0262ef7f":"markdown","9309f050":"markdown","4b3ae94f":"markdown","b32888d8":"markdown","9ecc49ea":"markdown","11af5d11":"markdown","cb0711f3":"markdown","298d1495":"markdown","90270394":"markdown","c1f94089":"markdown","86f3e9f2":"markdown","7cba359f":"markdown","3afc4cf9":"markdown","ad94dec5":"markdown","68f68dd9":"markdown","80c905c5":"markdown","f13104bb":"markdown","94570221":"markdown","f37f5adb":"markdown","e724b8e7":"markdown","9ce21834":"markdown","ff493c8e":"markdown","09c22619":"markdown","5d5758a1":"markdown","e676420d":"markdown","2c95e4b5":"markdown","ab0d962a":"markdown","3f68b7a0":"markdown","c3fc066d":"markdown","286f1dd8":"markdown","d3c82456":"markdown","f1fe678e":"markdown","486f0b00":"markdown","13bf2575":"markdown","aae9b6b1":"markdown","b7ea76fc":"markdown","fbabc6b6":"markdown"},"source":{"cd1db320":"#basics\nimport numpy as np \nimport pandas as pd \nimport random as rnd\n\n#plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n#matplotlib inline  is used for for the getting output of the plot, especially in jupiter\n\n#models\nfrom sklearn.linear_model import LogisticRegression #even this project is a classification problem, logistic_regression comes from linear_model library like Perceptron & SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#misc\n\n#stats\n\n#ignore warnings\n\n#location path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","30e54040":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf=[train,test]\ndata_df=pd.concat([train,test])","09589176":"train.head()","aead86e1":"train.columns","55c0687b":"#Finding numerical features\nnumeric_dtypes = train.dtypes[train.dtypes != \"object\"].index\nnumeric_dtypes","f53fb2be":"#Finding categorical features\ncategorical_dtypes = train.dtypes[train.dtypes == \"object\"].index\ncategorical_dtypes","da2170d8":"#Train & Test Set\nTotal = data_df.isnull().sum().sort_values(ascending=False)\nPercent = (data_df.isnull().sum()\/data_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, Percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","c9fc9776":"#Alternative short-view of above stated info\ntrain.info()\nprint('_'*40)\ntest.info()","a447298a":"train.describe()","1a150407":"train[[\"Sex\",\"Survived\"]].groupby([\"Sex\"],as_index=False).mean().sort_values(by='Survived', ascending=False)","ed9142cf":"train[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","bf278b7e":"train[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","ba6d543d":"train[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","86bf8803":"# peaks for survived\/not survived passengers by their age\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\n\n# average survived passengers by age\nfig, axis1 = plt.subplots(1,1,figsize=(40,4))\naverage_age = train[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\nsns.barplot(x='Age', y='Survived', data=average_age)","9af185c1":"# peaks for survived\/not survived passengers by their Fare\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\n\n# average survived passengers by fare\nfig, axis1 = plt.subplots(1,1,figsize=(40,4))\naverage_age = train[[\"Fare\", \"Survived\"]].groupby(['Fare'],as_index=False).mean()\nsns.barplot(x='Fare', y='Survived', data=average_age)","67a02e9f":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","6f2c22f7":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","876b7da9":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","45f11069":"print(\"Before\", train.shape, test.shape, df[0].shape, df[1].shape)\n\ntrain = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\ndf = [train, test]\n\n\"After\", train.shape, test.shape, df[0].shape, df[1].shape","64608396":"for dataset in df:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","1b0bcad7":"for dataset in df:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","87c405a6":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","70c80fd2":"train=train.drop(['Name','PassengerId'],axis=1)\ntest=test.drop(['Name'],axis=1)\ndf=[train,test]\ntrain.shape, test.shape","5562b8d8":"for dataset in df:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain.head()","b3486328":"grid=sns.FacetGrid(train,row='Pclass',col='Sex',size=2.2,aspect=1.6)\ngrid.map(plt.hist,'Age',alpha=.5,bins=20)\ngrid.add_legend()","4d4cfc85":"guess_ages=np.zeros((2,3))\nguess_ages","ede55535":"for dataset in df:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()","097e8d18":"train['AgeBand']=pd.cut(train['Age'],5)\ntrain[['AgeBand','Survived']].groupby(['AgeBand'],as_index=False).mean().sort_values(by='AgeBand',ascending=True)","4612f2b7":"for dataset in df:\n    dataset.loc[dataset['Age'] <16, 'Age']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<32), 'Age']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<48), 'Age']=2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<64), 'Age']=3\n    dataset.loc[dataset['Age']>64,'Age']\ntrain.head()","94b32039":"train=train.drop(['AgeBand'],axis=1)\ndf=[train,test]\ntrain.head()","a745a0fe":"for dataset in df:\n    dataset['FamilySize']=dataset['SibSp']+dataset['Parch']+1\n    \ntrain[['FamilySize','Survived']].groupby(['FamilySize'],as_index=False).mean().sort_values(by='Survived',ascending=False)","1ac58d69":"for dataset in df:\n    dataset['IsAlone']=0\n    dataset.loc[dataset['FamilySize']==1, 'IsAlone']=1\n    \ntrain[['IsAlone','Survived']].groupby(['IsAlone'],as_index=False).mean()","cc2645f4":"train=train.drop(['Parch','SibSp','FamilySize'],axis=1)\ntest=test.drop(['Parch','SibSp','FamilySize'],axis=1)\ndf=[train,test]\n\ntrain.head()","9386c9c2":"for dataset in df:\n    dataset['Age*Class']=dataset.Age*dataset.Pclass\n    \ntrain.loc[:,['Age*Class','Age','Pclass']].head()","6d635943":"freq_port=train.Embarked.dropna().mode()[0]\nfreq_port","df55fbd9":"for dataset in df:\n    dataset['Embarked']=dataset['Embarked'].fillna(freq_port)\n    \ntrain[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending=False)","4cd1511a":"for dataset in df:\n    dataset['Embarked']=dataset['Embarked'].map({'S':0, 'C':1, 'Q':2}).astype(int)\n    \ntrain.head()","08a695ca":"test['Fare'].fillna(test['Fare'].dropna().median(),inplace=True)\ntest.head()","533b5916":"train['FareBand']=pd.qcut(train['Fare'],4)\ntrain[['FareBand','Survived']].groupby(['FareBand'],as_index=False).mean().sort_values(by='FareBand',ascending=True)","a49a1a46":"for dataset in df:\n    dataset.loc[dataset['Fare']<=7.91, 'Fare']=0\n    dataset.loc[(dataset['Fare']>7.91) & (dataset['Fare']<=14.454), 'Fare']=1\n    dataset.loc[(dataset['Fare']>14.454) & (dataset['Fare']<=31.0), 'Fare']=2\n    dataset.loc[dataset['Fare']>31, 'Fare']=3\n    dataset['Fare']=dataset['Fare'].astype(int)\n    \ntrain=train.drop(['FareBand'],axis=1)\ndf=[train,test]\n\ntrain.head()","803532aa":"test.head()","f6a684da":"X_train=train.drop(\"Survived\",axis=1)\nY_train=train[\"Survived\"]\nX_test=test.drop(\"PassengerId\",axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","6fe372bf":"#Logistic Regression\n\nlogreg=LogisticRegression()\nlogreg.fit(X_train,Y_train)\nY_pred=logreg.predict(X_test)\nacc_log=round(logreg.score(X_train,Y_train)*100,2)\nacc_log","37b22afe":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","f5a96c52":"#Support Vector Machines\n\nsvc=SVC()\nsvc.fit(X_train,Y_train)\nY_pred=svc.predict(X_test)\nacc_svc=round(svc.score(X_train,Y_train)*100,2)\nacc_svc","4b763c56":"knn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,Y_train)\nY_pred=knn.predict(X_test)\nacc_knn=round(knn.score(X_train,Y_train)*100,2)\nacc_knn","c892127f":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","851fabba":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","7c613f7c":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","4135947e":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","fc1e9913":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","3da9d561":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","77c6f312":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","9766e7ef":"Achieved titles needs to convert ordinal feature.","e46ce4d9":"Before creating FareBand, first we will analyse correlation of it with Survived","7822b4f6":"Lets replace Age variables with created AgeBand.","bd7424d0":"## Model evaluation\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","384c8cfd":"Many of them will named as ***Rare***.","de6bb007":"## **Reach the data**","fd9ab155":"This early investigation will accelerate understanding dynamics of what we are dealing with it.\n* Firstly, in total sample (train), we have 891 passengers.\n* Survived as categorized as 0 or 1. Also we can say that our avg survived rate is around 0.38.\n* We have 3 different Pclasses, and lots of them located in 2 or 3.\n* Age is ranging with 0-80 (we have babies and elder people, but avg age is around 30.)\n* Fare is ranging 0 and 512, and avg fare is 32.","7d8e2521":"Right now we can remove AgeBand feature","74e487ab":"Usage of Age variable will be Age bands that will reveal correlation with Survived","4eaa77b2":"Numeric features can be varied inside of it as continuous or discrete or timeseries.\n\n* Continuous: Age and Fare.\n* Discrete: SibSp, Parch.","01bd37ed":"### Converting categorical feature to numeric\n\nWe can convert FilledEmbarked feature by creating a new numeric Port feature.","36154428":"**Correlation with numerical and ordinal features**\n\nWe will continue with Age as a numeric feature, and it will combine with ordinal feature: Pclass\n\n* When PClass=3 shows higher death statistics, Pclass=1 indicates higher number of survival. (Classfying #2)\n* Pclass=3 and 2 has higher deaths including lower age band, comparing with Pclass=1. (Classfying #2)","0262ef7f":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n* Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n* So is Title as second highest positive correlation.","9309f050":"Now we will fill values based on Sex (0,1) and PClass (1,2,3), so we need to fill 6 combination, then we created (2,3) matrix above part.","4b3ae94f":"Remaining types are errors or typos.\n\n* Typo: Name may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names).","b32888d8":"### **Converting a categorical feature**\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex for male=0 and female=1.","9ecc49ea":"## Creating new features existing features","11af5d11":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference : Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Perceptron]","cb0711f3":"## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n* **Sex:** We can see that female are highly survived ratio at this accident. (Classfying #1)\n* **Pclass:** We can see that upper class (1) has higher surviver rate. (Classfying #3)\n* **SibSp and Parch:** We can see that small groups can higher survival ratio comparing with large groups. (Creating #1)\n\n\n\n","298d1495":"## Early Investigation","90270394":"## Completing a categorical feature","c1f94089":"**Correcting by dropping features**\n\nAs we mentioned lacking lots of missing values in Ticket and Cabin features, we will drop them for both data sets.","86f3e9f2":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference: Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier#Gaussian_na\u00efve_Bayes]\n\nThe model generated confidence score is one of the lower models among the models evaluated so far.","7cba359f":"##\u00a0**Assumptions based on early investigations**\n\nWe need to analysis and need to take action e.g. missing values before establishing models. Those steps can be states as:\n\n**Correlating:**\n\nWe have correlate items with each other we need to know them. This will also can prevent problems such as multicollinerity. \n\n        Note: Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 1 or \u22121.\n\n**Completing:**\n* We can complete our missing variables such as age, because of the high-related with survival\n* Embarked feature need to be completed because kind of correlation it has with Survived.\n\n**Correcting:**\n* Passenger_id & Cabin are mainly missing in the both sets, can be dropped.\n* Name feature is meaningless, but we can extract information such \"Mr\", \"Miss\" ..etc, then we can drop it.\n\n**Creating:**\n* We can create family variable based on Parch & SibSp features to get number of the family members.\n* We can create Title feature as above mentioned.\n* Essential part, we can create Age & Fare band for getting numerical feature into ordinal categorical feature.\n* We can drop Ticket feature because of the duplicate of it & not having correlation survival.\n\n**Classifying:**\n* Women have higher survival rate comparing with men.\n* Children have higher survival rate comparing older people.\n* Pclass=1 has higher survival rate comparing with other lower classes (2 and 3).\n\nIN FINAL, WE COVER ALL VARIABLES WITH OUR DATA ANALYSIS. :).\n","3afc4cf9":"We can create for only one person as 'IsAlone'","ad94dec5":"### Quick way completing and converting a numeric feature\n\nWe can now complete one remaining Fare feature in the test dataset as using the most occurs of this feature.\n\nAs a note: We will not create a new intermediate new features or doing any further analysis for correlation to guess missing values. The completion goal achieves desired achievement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.","68f68dd9":"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference: Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine]\n\nNote that the model generates a confidence score which is higher than Logistics Regression model.","80c905c5":"## Model, Predict and Solve\n\nNow we are going to train and predict results. The models that we can use can be listed as as under the roof of classification models:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","f13104bb":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference: Wikipedia - (https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).","94570221":"Mix features can be varied inside of it as numeric or alphanumeric or even combination of them.\n\n* Mix-numeric and alphanumeric: Ticket.\n* Mix-alphanumeric: Cabin.","f37f5adb":"We can create a new feature for FamilySize as using Parch and SibSp. This will enable us to drop Parch and SibSp from the dataset. (Creating #1)","e724b8e7":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference: Wikipedia- [https:\/\/en.wikipedia.org\/wiki\/Random_forest]\n\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.","9ce21834":"**Correlating categorical features**\n\n* Women have better surviving rates comparing with men except embarked=C.\n* Males have better rates in Pclass=3 in some point.\n* Port of embarkation can vary upon surviving rates.\n\nIn final,\n* Sex variable will be added to model\n* Even embarked variable show different stats that will be included to model.","ff493c8e":"**Correlating categorical and numerical features**\n\n* Higher fare paying passengers had better survival. Confirms our assumption for fare ranges. (Creating #3)\n* Port of embarkation (S and C) correlates with survival rates. Correlating (#1) and Completing (#2).","09c22619":"Embarked feature takes S,Q,C values based on port of embarkation. Training data has 2 missing values. Then, basically we fill these values as the most common existence.","5d5758a1":"**Creating new feature extracting from existing**\n\n* As we mentioned correcting part, we can extract same values from name feature such as Miss, Mr. Mrs.\n\n* For implementing this value, we need to RegEx *(\\w+\\.)* pattern which will give us with (.) characters.\n\nIn final,\n\nWhen we plot created feature 'Title' with Age and Survived,then 'Title' feature will add to model.\n* Master title mean of 5 years.\n* Survival among title Age band varies slightly.\n* Specific titles comes forward such as Lady, Sir.","e676420d":"### **Completing a numerical continuous feature**\n\nFor estimating or completing null\/missing values, we have 3 different options.\n\n1. Basic method is to assign random values between mean and s.d. of related feature.\n2. As detailed implementation, we can generate values with other features related median in different combinations.\n3. We can combine 1. and 3. points, but it will present random and noisy values to our model.Then we will choose only method 2.","2c95e4b5":"## **Investigation on train set**","ab0d962a":"Additionally, we can create extra feature combining Pclass & Age.","3f68b7a0":"Categorical features can be varied inside of it as nominal, ordinal, ratio or interval.\n- Categorical: Survived, Sex, and Embarked.\n- Ordinal: Pclass.","c3fc066d":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference: Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm]\n\nKNN confidence score is better than Logistics Regression & than SVM.","286f1dd8":"We fill age missing age values based on PClass & Sex, because it can seen that generally higher Pclasses are seen higher number of ages in both gender.","d3c82456":"* In here, we can drop 'Name' and 'PassengerId' feature from both training and test data sets. (Correcting #1 and #2)","f1fe678e":"## Analyze by visualizing data\n\n **Numerical features**\n\nLet's focus relationship between our numeric features and Survived feature\n* Kids and babies (<4 years) has high survival rate\n* Sample age generally cumulated between 15 and 35.\n* As adult-age increases, surviving rate decrease. However, the oldest passengers are survived (age= 80).\n* We cannot say that directly higher fare results with higher surviving rates.\n\nThen,\n* Age variable needs to be banded for the model. (Creating #3)\n* Because of age variable importance, missing values need to be filled in an appropriate way. (Completing #1).","486f0b00":"## Execution time for Correcting, Creating and Completing\n\nEssential both of train and test data set will be affected these edits to stay consistent\n","13bf2575":"We can drop Parch,SibSp and FamilySize features after creating IsAlone.","aae9b6b1":"## Missing values on test & train data set","b7ea76fc":"We need to convert Fare feature based on bands ","fbabc6b6":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference: Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Decision_tree]\n\nThe model confidence score is the highest among models evaluated so far."}}