{"cell_type":{"28560aee":"code","2cd203ca":"code","9672f36b":"code","f68fbcaf":"code","d1af4a1f":"code","8f349f6c":"code","1550fca6":"code","84ad2f1b":"markdown","8180c262":"markdown","f468759d":"markdown","d05ce371":"markdown"},"source":{"28560aee":"!pip install kaggle-environments --upgrade -q","2cd203ca":"%%writefile epsilon_greedy_decay_own.py\n\nimport math\nimport random\nimport numpy as np\n\nepsilon = 0.1\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(42)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward    \n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay = 0.97 ** numbers_of_selections[i]\n                upper_bound = decay * sums_of_reward[i] \/ numbers_of_selections[i]\n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","9672f36b":"%%writefile epsilon_greedy_decay_both.py\n\nimport math\nimport random\nimport numpy as np\n\nepsilon = 0.1\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nnumbers_of_total_selections = None\nrandom.seed(42)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, numbers_of_total_selections, last_bandit, total_reward    \n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        numbers_of_total_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n        for action in observation.lastActions:\n            numbers_of_total_selections[int(action)] += 1\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay = 0.97 ** numbers_of_total_selections[i]\n                upper_bound = decay * sums_of_reward[i] \/ numbers_of_selections[i]\n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","f68fbcaf":"from kaggle_environments import make\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef bo5(file1, file2):\n    env = make(\"mab\", debug=True)\n\n    rewards_1 = []\n    rewards_2 = []\n    for i in range(10):\n        print('.', end='')\n        env.run([file1, file2])\n        rewards_1.append([])\n        rewards_2.append([])\n        for step in env.steps:\n            rewards_1[-1].append(step[0]['reward'])\n            rewards_2[-1].append(step[1]['reward'])\n        env.reset()\n        \n    rewards_1 = np.mean(rewards_1, axis=0)\n    rewards_2 = np.mean(rewards_2, axis=0)\n    \n    fig, ax = plt.subplots(2, 1)\n    ax[0].set_title(\"Rewards\")\n    ax[0].plot(rewards_2, label=f\"{file2}\")\n    ax[0].plot(rewards_1, label=f\"{file1}\")\n    ax[1].plot(rewards_1-rewards_2, label='difference', color='tab:orange')\n    ax[1].axhline(0, ls='--', color='xkcd:dark red')\n    for i in range(2):\n        ax[i].legend()\n            ","d1af4a1f":"print('Default vs epsilon-greedy_decay_own')\nbo5(\"epsilon_greedy_decay_own.py\", \"..\/input\/santa-2020\/submission.py\")","8f349f6c":"print('Default vs epsilon-greedy_decay_both')\nbo5(\"epsilon_greedy_decay_both.py\", \"..\/input\/santa-2020\/submission.py\")","1550fca6":"print('epsilon-greedy_both vs epsilon-greedy_decay_own')\nbo5(\"epsilon_greedy_decay_both.py\", \"epsilon_greedy_decay_own.py\")","84ad2f1b":"> ## Best of 10's","8180c262":"## $\\epsilon$-greedy with decay including own steps only (original version)\n","f468759d":"References:\n* [Santa 2020 starter](https:\/\/www.kaggle.com\/xhlulu\/santa-2020-epsilon-greedy-starter): all lines identical, except for the decay\n\nThe above referenced $\\epsilon$-greedy algorithm accounts for the decay in threshold of bandits by applying a $0.97^{n_i}$ upper bound for the reward probability. Here $n_i$ is the number of pulls **by our agent** on the $i^{th}$ bandit. \n\nIn the simulation both player's selections contribute to the decay, hence when calculating the upper bound, the total number of selection should be used.\n\nHowever, when playing against each other, the second version significantly underperforms. Naively I would expect the opposite, but maybe I'm missing something?\n\n**please discuss**\n","d05ce371":"## $\\epsilon$-greedy with decay including own and opponent steps\n\n"}}