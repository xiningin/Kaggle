{"cell_type":{"0ce88786":"code","a93abc54":"code","f2f8144b":"code","21b219d3":"code","7c54832d":"code","f1fd84b6":"code","4e94b977":"code","81d72dba":"code","6de55d20":"code","5244c82a":"code","e60e515f":"code","b29fa29a":"code","8e66331e":"code","233402a3":"code","7f6c9c0b":"code","cc7eb213":"code","e4825efd":"code","51df15e2":"code","1f997c53":"code","03635b53":"code","018b6117":"code","640901b2":"code","dddba57f":"code","a12be2d5":"code","31a9cf4c":"code","4e50f55a":"code","6cc4aeae":"code","72edbe27":"code","8a41124f":"code","24bae5e6":"code","fe5a0a09":"code","c3607172":"code","31316739":"code","6ec27aee":"code","af3b2a5f":"markdown","4bbcb2d4":"markdown","89f38585":"markdown","a5335361":"markdown","525d8584":"markdown","fbbac45d":"markdown","b7c4fd5e":"markdown","23bd5023":"markdown","6cff01a8":"markdown","3a969812":"markdown","a4c864de":"markdown","1d777298":"markdown","4e138741":"markdown","9d3eea63":"markdown","7bfd7458":"markdown","95f6f521":"markdown","f1b54941":"markdown","10890f46":"markdown","e9603275":"markdown","5f96bf26":"markdown","f30e2840":"markdown","6ea08f2f":"markdown","2a28c547":"markdown","c1cab6a2":"markdown","f221af9f":"markdown","daa8afe0":"markdown","1359668c":"markdown","9a7eec0f":"markdown"},"source":{"0ce88786":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style = 'whitegrid')\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","a93abc54":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f2f8144b":"training_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntesting_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","21b219d3":"training_data.head()","7c54832d":"testing_data.describe()","f1fd84b6":"null_col = (training_data.isnull().sum()\/len(training_data)) * 100\nnull_col = null_col.sort_values(ascending=False)\nnull_col","4e94b977":"corr = training_data.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncorr.SalePrice","81d72dba":"train_data = training_data.drop(['BsmtFinSF2','BsmtHalfBath','MiscVal','Id','LowQualFinSF',\n                                 'YrSold','OverallCond','MSSubClass','EnclosedPorch','GarageType',\n                                 'KitchenAbvGr','PoolQC','MiscFeature','Alley','Fence','FireplaceQu','SalePrice'], axis = 1)\n\ntest_data = testing_data.drop(['BsmtFinSF2','BsmtHalfBath','MiscVal','Id','LowQualFinSF',\n                                 'YrSold','OverallCond','MSSubClass','EnclosedPorch','GarageType',\n                                 'KitchenAbvGr','PoolQC','MiscFeature','Alley','Fence','FireplaceQu'], axis = 1)","6de55d20":"categorical_features = train_data.select_dtypes(include = [\"object\"]).columns\ncategorical_features","5244c82a":"numerical_features = train_data.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features","e60e515f":"sns.distplot(training_data['SalePrice'], color=\"r\", kde=False)\nplt.title(\"Distribution of Sale Price\")\nplt.ylabel(\"Number of Occurences\")\nplt.xlabel(\"Sale Price\");","b29fa29a":"sns.barplot(x='SaleCondition', y='SalePrice', data=training_data)","8e66331e":"plt.scatter(x ='TotalBsmtSF', y = 'SalePrice', data = training_data)\nplt.xlabel('Total Basement in Square Feet')","233402a3":"sns.catplot(x ='Street', y = 'SalePrice', data = training_data)","7f6c9c0b":"def mapping(data):\n    GarageCondM = {'TA':0, 'Fa':1, 'Gd':2, 'Ex':3}\n    data['GarageCond'] = data['GarageCond'].map(GarageCondM)\n\n    GarageQualM = {'TA':0, 'Fa':1, 'Gd':2, 'Ex':3}\n    data['GarageQual'] = data['GarageQual'].map(GarageQualM)\n\n    GarageFinishM = {'RFn':0, 'Unf':1}\n    data['GarageFinish'] = data['GarageFinish'].map(GarageFinishM) \n\n    BsmtFinType2M = {'Unf':0, 'BLQ':1, 'ALQ':2, 'Rec':3, 'LwQ':4, 'GLQ':5}\n    data['BsmtFinType2'] = data['BsmtFinType2'].map(BsmtFinType2M) \n\n    BsmtExposureM = {'No':0, 'Gd':1, 'Mn':2, 'Av':3}\n    data['BsmtExposure'] = data['BsmtExposure'].map(BsmtExposureM) \n\n    BsmtCondM = {'TA':0, 'Fa':1, 'Gd':2}\n    data['BsmtCond'] = data['BsmtCond'].map(BsmtCondM)\n\n    BsmtQualM = {'TA':0, 'Fa':1, 'Gd':2, 'Ex':3}\n    data['BsmtQual'] = data['BsmtQual'].map(BsmtQualM)\n\n    BsmtFinType1M = {'GLQ':0, 'ALQ':1, 'Unf':2, 'Rec':3, 'BLQ':4, 'LwQ':5}\n    data['BsmtFinType1'] = data['BsmtFinType1'].map(BsmtFinType1M)\n\n    MasVnrTypeM = {'BrkFace':0, 'None':1, 'Stone':2, 'BrkCmn':3, 'BLQ':4, 'LwQ':5}\n    data['MasVnrType'] = data['MasVnrType'].map(MasVnrTypeM)\n\n    ElectricalM = {'SBrkr':0, 'FuseA':1, 'FuseF':2, 'BrkCmn':3, 'BLQ':4, 'LwQ':5}\n    data['Electrical'] = data['Electrical'].map(ElectricalM)\n    \n    return data","cc7eb213":"train_data = mapping(train_data)\ntest_data = mapping(test_data)","e4825efd":"train_data = train_data.fillna(train_data.mean())\ntrain_data = pd.DataFrame(train_data)\n\ntest_data = test_data.fillna(test_data.mean())\ntest_data = pd.DataFrame(test_data)\ntrain_data.head()","51df15e2":"test_data.head()","1f997c53":"train_data['train'] = 1\ntest_data['test'] = 0\ncombined = pd.concat([train_data, test_data])\ncombined = pd.get_dummies(combined, prefix_sep='_', columns = list(categorical_features))\ncombined.head()","03635b53":"train_data = combined[combined[\"train\"] == 1]\ntest_data = combined[combined[\"test\"] == 0]\ntrain_data.drop([\"test\",\"train\"], axis = 1, inplace = True)\ntest_data.drop([\"train\",\"test\"], axis = 1, inplace = True)","018b6117":"train_data.head()","640901b2":"test_data.head()","dddba57f":"categorical_features = train_data.select_dtypes(include = [\"object\"]).columns\ncategorical_features","a12be2d5":"categorical_features = test_data.select_dtypes(include = [\"object\"]).columns\ncategorical_features","31a9cf4c":"def null(data):\n    null_col = (data.isnull().sum()\/len(data)) * 100\n    null_col = null_col.sort_values(ascending=False)\n    return null_col","4e50f55a":"null(test_data)","6cc4aeae":"null(train_data)","72edbe27":"X_train = train_data.copy()\nY_train = training_data[\"SalePrice\"].values\nX_test = test_data.copy()\nX_train.shape, Y_train.shape, X_test.shape","8a41124f":"from sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline","24bae5e6":"import xgboost as xgb\ngbm = xgb.XGBRegressor(\n                 colsample_bytree=0.1,   #ratio_of_constructing_each_tree\n                 gamma=0.0,              #loss_reduction_param\n                 learning_rate=0.01,     #for_updating_param\n                 max_depth=3,            #maximum_depth_of_tree\n                 min_child_weight=0,     #minimum_sum_of_child_weight\n                 n_estimators=10000,     #total_no_of_iterations                                                                   \n                 reg_alpha=0.0006,       #updating_coefficient_L1_regularization_term\n                 reg_lambda=0.6,         #updating_coefficient_L2_regularization_term\n                 subsample=0.7,          #sampling_training_data_randomly\n                 seed=30,                #random_number_seed\n                 silent=1)               #occurence_of_message\ngbm_fit = gbm.fit(X_train, Y_train)\ngbm_predictions = gbm.predict(X_test)","fe5a0a09":"import lightgbm as lgb\nlgb = lgb.LGBMRegressor(objective='regression',     \n                        num_leaves=5,                  #num_of_leaves in a tree\n                        learning_rate=0.05,            #updating_weights_to_mimize_loss\n                        n_estimators=5000,             #num_of_iterations\n                        max_bin = 55,                  #num_of_bins_binning_refers_to_continous_unique_value\n                        bagging_fraction = 0.8,        #select_random_data_samples\n                        bagging_freq = 5,              #perform_bagging_at_k_iteration\n                        feature_fraction = 0.2319,     #randomly_select_feature_here_23%\n                        feature_fraction_seed=9,       #random_seed_for_feature_fraction\n                        bagging_seed=9,                #random_seed_for_bagging\n                        min_data_in_leaf =6,           #minimum_no_leaf\n                        min_sum_hessian_in_leaf = 11)\nlgb = lgb.fit(X_train, Y_train)\nlgb_predictions = lgb.predict(X_test)","c3607172":"blend = lgb_predictions*0.5+gbm_predictions*0.5","31316739":"submission = pd.DataFrame({\n        \"Id\": testing_data[\"Id\"],\n        \"SalePrice\": blend\n    })\nsubmission.to_csv('final_submissison.csv', index=False)","6ec27aee":"submission.head()","af3b2a5f":"Finding correlations with respect to Target value i.e SalePrice","4bbcb2d4":"Droping \/ Deleting most missing features and features that have negative correlations for target variable","89f38585":"Importing all the mandatory libraries for data manipulation, data extraction, data visualization and to avoid unneccessary warnings","a5335361":"# Modelling\n","525d8584":"We will blend the two best prediction results of XGB & LGBM together ","fbbac45d":"Separate this **combined** dataset and then drop these extra variables created for concatenation","b7c4fd5e":"# **Data visualisation with important features**","23bd5023":"Filling missing value with mean strategy in both training and testing set","6cff01a8":"**Light GBM (Light Gradient Boosting Machine)** is framework of gradient boosting based on decision tree algorithm. It is not similar to XGBoost as LGBM splits tree leaf wise where XGBoost as level wise this results in reduction of loss function.\u00b6  \nLGBM is new released algorithm available with the key features of faster training speed, compatibility with higher dataset and parallel learning support","3a969812":"Now we will use one of the ensemble technique known as **XGboost(Extreme Boosting)**. Xgboost use basic principle of Decision Tree as boosting method which contain several bags. Suppose first bag contains random training sample then by evaluating with test set if we get wrong predictions then next bag is filled with these misclassified\/ wrong predicted samples and further random data samples and these repetitive process goes on. Which helps avoiding underfitting","a4c864de":"Obtaining standard features like count, mean and min, max values of features to analyse the data","1d777298":"**Heading towards final submissions**","4e138741":"### LightGBM","9d3eea63":"![](https:\/\/miro.medium.com\/max\/792\/1*AZsSoXb8lc5N6mnhqX5JCg.png)","7bfd7458":"No null values are occurred in train and test set hence we can end our feature engineering here","95f6f521":"Loading training and testing set","f1b54941":"**I hope you found this kernel helpful . Feel free to comment for queries or suggestions** <br>\n**Motivate me by upvoting ..** <br>\n**Until Next Time Happy Kaggling : )**","10890f46":"Find categorical and numerical data","e9603275":" Solving [House Price Prediction](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) with ensemble learning techniques in descriptive manner ","5f96bf26":"### XGBoost","f30e2840":"### Blending","6ea08f2f":"Here we can see there is no categorical features avaliable in train and test dataset","2a28c547":"Mapping functions for features missing in training & testing set","c1cab6a2":"To convert categorical data into numerical data we will concatenate both train, test data. By adding unique identifier i.e 1 for train and 0 for test data and then create dummy variables together. This dummy variables will be created with  delimeter ' _ ' ","f221af9f":"Get null values in percentage and sorting that values in descending","daa8afe0":"# **Feature Engineering**","1359668c":"Here we can see SalePrice distribution has occured several times betw 100000 - 200000","9a7eec0f":"![](https:\/\/miro.medium.com\/max\/792\/1*whSa8rY4sgFQj1rEcWr8Ag.png)"}}