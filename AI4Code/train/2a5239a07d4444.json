{"cell_type":{"33d007c9":"code","1718a95b":"code","06a12fbf":"code","449df2bd":"code","67177bb1":"code","67834192":"code","26cd815b":"code","c1192b1b":"code","caecaa1f":"code","fac327dd":"code","8c598b5f":"code","479a97d0":"code","8d4bfc0a":"code","f41d3ecb":"code","d631114e":"code","1f73f782":"code","b4ac2847":"code","f25ac131":"code","c08831bf":"markdown","2c756166":"markdown","e7f9b157":"markdown","0fd8bfb2":"markdown","9a39c956":"markdown","e5e99395":"markdown","94d10ce3":"markdown","1487f0b3":"markdown","8b7e6a5e":"markdown","32bcc23c":"markdown","a60f46a8":"markdown","9283a578":"markdown","d23c2b48":"markdown","dc57505f":"markdown","f41f8ce9":"markdown","289e879f":"markdown","5c6fb8b2":"markdown"},"source":{"33d007c9":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\nimport xgboost\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier","1718a95b":"df = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')","06a12fbf":"df.head()","449df2bd":"df.shape","67177bb1":"df.dtypes","67834192":"df.isnull().sum()","26cd815b":"df[df.columns[2:]].corr()['Exited'][:]","c1192b1b":"co = df[df.columns[2:]].corr()['Exited'][:]\nfeatures = co.index\n\nplt.figure(figsize = (10, 5))\nsns.heatmap(df[features].corr(), annot = True, cmap = 'viridis')\nplt.show()","caecaa1f":"X = df.iloc[:, 3:13]\ny = df.iloc[:, -1]","fac327dd":"X = pd.get_dummies(X, columns = ['Geography', 'Gender'], drop_first = True)\nX.head()","8c598b5f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","479a97d0":"lr = LogisticRegression()\n\n# Train the model\nmodel = lr.fit(X_train, y_train)\n\n# Prediction\ny_pred_test = lr.predict(X_test)\ny_pred_train = lr.predict(X_train)\n\n# Accuracy Score\nprint('Train Accuracy score : {}\\n'.format(accuracy_score(y_train, y_pred_train)))\nprint('Test Accuracy score : {}'.format(accuracy_score(y_test, y_pred_test)))","8d4bfc0a":"cl1 = XGBClassifier()\n\n# Train the model\nmodel1 = cl1.fit(X_train, y_train)\n\n# Prediction\ny1_train_pred = cl1.predict(X_train)\ny1_test_pred = cl1.predict(X_test)\n\n# Accuracy Score\nprint('Train Accuracy score : {}\\n'.format(accuracy_score(y_train, y1_train_pred)))\nprint('Test Accuracy score : {}'.format(accuracy_score(y_test, y1_test_pred)))","f41d3ecb":"params = {\n    'learning_rate'     : [0.05, 0.10, 0.05, 0.20, 0.25, 0.30],\n    'max_depth'         : [3, 4, 5, 6, 8, 10, 12, 15],\n    'min_child_weight'  : [1, 3, 5, 7],\n    'gamma'             : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n    'colsmaple_bytree'  : [0.3, 0.4, 0.5, 0.6, 0.7]\n    }","d631114e":"cl2 = XGBClassifier()\nrandom_search = RandomizedSearchCV(cl2, param_distributions = params,\n                                   n_iter = 5, scoring = 'roc_auc',\n                                   n_jobs = -1,\n                                   cv = 5,\n                                   verbose = 3)\nrandom_search.fit(X_train, y_train)","1f73f782":"random_search.best_estimator_","b4ac2847":"random_search.best_params_","f25ac131":"# Prediction\ny2_train_pred = random_search.predict(X_train)\ny2_test_pred = random_search.predict(X_test)\n\n# Accuracy Score\nprint('Train Accuracy score : {}\\n'.format(accuracy_score(y_train, y2_train_pred)))\nprint('Test Accuracy score : {}'.format(accuracy_score(y_test, y2_test_pred)))","c08831bf":"#### Split into train and test set","2c756166":"#### Split the dataset into explanatory and response variable","e7f9b157":"<br\/>\n\n## 2. Importing necessary modules along with dataset","0fd8bfb2":"<br\/>\n\n# 3. Reconnissance of the dataset and EDA","9a39c956":"#### Checking the missng values","e5e99395":"<br\/>\n\n## 7. The differences","94d10ce3":"**Main Components of this Kernel**  \n1. Problem statement\n2. Importing necessary modules along with dataset  \n3. Reconnissance of the dataset and EDA  \n4. Logistic Regression  \n5. XGBoost Classifier without Hyperparameter tuning  \n6. XGBoost Classifier with Hyperparameter tuning \n7. The differences","1487f0b3":"<br\/>\n\n## 6. XGBoost Classifier with Hyperparameter tuning","8b7e6a5e":"#### Convert the categorical features into dummy variables","32bcc23c":"<br\/>\n\n## 5. XGBoost Classifier without Hyperparameter tuning","a60f46a8":"#### Correlation with Heatmap","9283a578":"**Logistic Regression**  \nTrain Accuracy score : 0.789875  \nTest Accuracy score : 0.789  \n**Conclusion:** Not that bad. But let's try it with XGBoost.\n\n<br\/>\n\n\n**XGBoost Classifier without Hyperparameter tuning**  \nTrain Accuracy score : 0.95775  \nTest Accuracy score : 0.8545  \n**Conclusion:** A way better than Logistic Regression. But it looks like a bit overfitting. Now time to see what hyperparameter tuning can do for us.\n\n<br\/>\n\n**XGBoost Classifier with Hyperparameter tuning**  \nTrain Accuracy score : 0.875  \nTest Accuracy score : 0.867  \n**Conclusion:** Hyperparameter tuning increases the test accuracy along with reducing the overfitting problem.","d23c2b48":"#### Hyperparameters I choose to work with","dc57505f":"<br\/>\n\n## 1. problem Statement\n**We are given some information regarding bank account. Now we will have to predict whether a customer will stay or not.  \nWe will use Kaggles 'Churn Modelling' here.**","f41f8ce9":"#### Observing the correlation","289e879f":"<br\/>\n\n## 4. Logistic Regression","5c6fb8b2":"![xgboos1t.jpg](attachment:xgboos1t.jpg)"}}