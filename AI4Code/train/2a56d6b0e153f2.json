{"cell_type":{"e71c0f83":"code","1046d1f7":"code","50a7c3d1":"code","1003eb7f":"code","13c2e689":"code","3d84e5dd":"code","babbc122":"code","56942edc":"code","e742ecb2":"code","7e847502":"code","204cccea":"code","c667c28d":"code","5d032e5f":"code","b794bb46":"code","b11cd84a":"code","2268229a":"code","cbabcc3c":"code","f91b0ffd":"code","7e315e09":"code","709e6f5c":"code","0152f8c8":"code","13982b38":"code","299e9a66":"code","ba8526b8":"code","0f1df171":"code","25c38b80":"code","36bf9b8f":"code","99b19e71":"code","098481ab":"code","9693a1fa":"code","98382245":"code","31b800fe":"code","8e496362":"code","0fde8a95":"code","0ec9f4b5":"code","40a4697f":"code","9fefa32a":"code","386695ed":"code","ead5f7ea":"code","37407793":"code","c219e517":"code","aeadbb31":"markdown","0def6778":"markdown","95b14af7":"markdown","3b7527f4":"markdown","08d3b579":"markdown","31b1129f":"markdown","25b8ae4a":"markdown","e0f42913":"markdown","d87b9362":"markdown","b1a05efc":"markdown","a1398f28":"markdown","32fa879c":"markdown","05b55970":"markdown","44f28872":"markdown","2f7fbb51":"markdown","32ea37c4":"markdown","4bc0687c":"markdown","2283c812":"markdown"},"source":{"e71c0f83":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1046d1f7":"df = pd.read_csv(r\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head()","50a7c3d1":"df.drop(['sl_no'], axis = 1, inplace = True) #dropping insignificant values\ndf.head()","1003eb7f":"df.info()\n","13c2e689":"\ndf['salary'].fillna(int(df['salary'].mean()), inplace=True)\ndf.head()","3d84e5dd":"df.info()\ndf['degree_t'].value_counts()","babbc122":"df[\"Stat\"] = df[\"status\"]\ndf.head()\ndf.drop(['status'], axis = 1, inplace = True)","56942edc":"df.head()","e742ecb2":"# heat map correlation\n#HEAT MAP CORRELATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True,cmap=\"YlGnBu\")","7e847502":"#count plot genders\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"gender\", palette = \"flare\")\nplt.show()\n","204cccea":"#COUNT PLOT DEGREE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"degree_t\", palette = \"husl\")\nplt.show()\n","c667c28d":"#COUNT PLOT WORK EXPERIENCE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"workex\", palette = \"ch:s=.25,rot=-.25\")\nplt.show()\n","5d032e5f":"#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\", palette = \"Set2\")\nplt.show()\n","b794bb46":"#SPECIALISATION V\/S STATUS\n#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\",hue = 'Stat', palette = \"Set2\")\nplt.title(\"specialisation v\/s Status\")\nplt.show()\n","b11cd84a":"#COUNT PLOT \nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"degree_t\", hue = 'Stat', palette = \"flare\")\nplt.title(\"DEGREE V\/S STATUS\")\nplt.show()\n","2268229a":"#WORK EXP V\/S STATUS\nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"workex\", hue = 'Stat', palette = \"pastel\")\nplt.title(\"WORK EXP V\/S STATUS\")\nplt.show()\n","cbabcc3c":"#WORK EXP V\/S STATUS\nf = plt.subplots(figsize = (12 , 12))\nsns.histplot(data = df, x = \"salary\", hue = 'specialisation', palette = \"pastel\", bins = 30)\nplt.title(\"SALARY V\/S SPECIALISATION\")\nplt.show()\n","f91b0ffd":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"gender\", kde = True, palette = \"flare\")\nplt.title('gender v\/s DEGREE percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","7e315e09":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"pastel\")\nplt.title('DEGREE percentage V\/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","709e6f5c":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"gender\", kde = True)\nplt.title('gender v\/s MBA percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","0152f8c8":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"flare\")\nplt.title(' MBA percentage V\/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","13982b38":"x = df.iloc[:, :13]\ny = df.iloc[:, 13]\nx","299e9a66":"y","ba8526b8":"#ONE HOT ENCODING\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [5, 7])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x.shape)","0f1df171":"#label encoding\nle = LabelEncoder()\nx[:, 6] = le.fit_transform(x[:, 6]) #gender\nx[:, 8] = le.fit_transform(x[:, 8]) #ssc_b\nx[:, 10] = le.fit_transform(x[:, 10]) #hsc_b\nx[:, 12] = le.fit_transform(x[:, 12]) #workexp\nx[:, 14] = le.fit_transform(x[:, 14]) #specialisation\nx[:, 6] = le.fit_transform(x[:, 6])","25c38b80":"y = le.fit_transform(y)\ny","36bf9b8f":"print(x.shape)\nprint(y.shape)","99b19e71":"#splitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)","098481ab":"#scaling\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nx_train = std.fit_transform(x_train)\nx_test = std.fit_transform(x_test)","9693a1fa":"#importing metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","98382245":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nm1 = LogisticRegression()\nm1.fit(x_train, y_train)\n\nm1_pred = m1.predict(x_test)\nm1_pred\nprint(\"logistic regression\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m1_pred, y_test))\nprint(\"precision score: \", precision_score(m1_pred, y_test))\nprint(\"f1 score: \", f1_score(m1_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m1_pred, y_test))\nprint(\"recall score : \", recall_score(m1_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m1_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#svm\nfrom sklearn.svm import SVC\nm2 = SVC()\nm2.fit(x_train, y_train)\n\nm2_pred = m2.predict(x_test)\nm2_pred\nprint(\"SVC\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m2_pred, y_test))\nprint(\"precision score: \", precision_score(m2_pred, y_test))\nprint(\"f1 score: \", f1_score(m2_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m2_pred, y_test))\nprint(\"recall score : \", recall_score(m2_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m2_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#random forest\nfrom sklearn.ensemble import RandomForestClassifier\nm3 = RandomForestClassifier()\nm3.fit(x_train, y_train)\n\nm3_pred = m3.predict(x_test)\nm3_pred\nprint(\"Random Forest\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m3_pred, y_test))\nprint(\"precision score: \", precision_score(m3_pred, y_test))\nprint(\"f1 score: \", f1_score(m3_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m3_pred, y_test))\nprint(\"recall score : \", recall_score(m3_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m3_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nm4 = KNeighborsClassifier()\nm4.fit(x_train, y_train)\n\nm4_pred = m4.predict(x_test)\nm4_pred\nprint(\"Knn\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m4_pred, y_test))\nprint(\"precision score: \", precision_score(m4_pred, y_test))\nprint(\"f1 score: \", f1_score(m4_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m4_pred, y_test))\nprint(\"recall score : \", recall_score(m4_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m4_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nm5 = DecisionTreeClassifier()\nm5.fit(x_train, y_train)\n\nm5_pred = m5.predict(x_test)\nm5_pred\nprint(\"decision Tree\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m5_pred, y_test))\nprint(\"precision score: \", precision_score(m5_pred, y_test))\nprint(\"f1 score: \", f1_score(m5_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m5_pred, y_test))\nprint(\"recall score : \", recall_score(m5_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m5_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#naive bayes\nfrom sklearn.naive_bayes import BernoulliNB\nm6 = BernoulliNB()\nm6.fit(x_train, y_train)\n\nm6_pred = m6.predict(x_test)\nm6_pred\nprint(\"naive bayes\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m6_pred, y_test))\nprint(\"precision score: \", precision_score(m6_pred, y_test))\nprint(\"f1 score: \", f1_score(m6_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m6_pred, y_test))\nprint(\"recall score : \", recall_score(m6_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m6_pred, y_test))\nprint(\"\\n\", \"*\" * 30)","31b800fe":"#parameter tuning\n#logistic regression\nfrom sklearn.model_selection import GridSearchCV \n\nparams = {'C':[5, 10, 15, 20],'random_state':[0]}\ngrid1 = GridSearchCV(estimator = m1, param_grid = params, scoring = 'accuracy', cv = 10)\ngrid1.fit(x_train, y_train)\nbest_acc = grid1.best_score_\nbest_param = grid1.best_params_\nprint(\"best parameters: \", best_param)\n\nprint('best accuracy:', best_acc*100)\n","8e496362":"#svc\nparams ={'C':[10, ],'kernel':['linear', 'rbf'],'random_state':[0]}\ngrid2 = GridSearchCV(estimator = m2, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid2.fit(x_train, y_train)\nbest_acc = grid2.best_score_\nparam = grid2.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","0fde8a95":"#random forest\nparams = {\"n_estimators\": [100, 200, 300], \"criterion\": [\"gini\", \"entropy\"],\"random_state\":[42] }\ngrid3= GridSearchCV(estimator = m3, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid3.fit(x_train, y_train)\nbest_Acc = grid3.best_score_\nbest_param = grid3.best_params_\nprint(\"best accuracy :\", best_Acc*100)\nprint(\"best parameters : \", best_param)","0ec9f4b5":"#knn \nparams = {\n    'n_neighbors' : [5, 25],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid4 = GridSearchCV(estimator = m4 , param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid4.fit(x_train, y_train)\nbest_Acc = grid4.best_score_\nbest_param = grid4.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"best accuracy :\", best_Acc*100)   ","40a4697f":"#decison\nparams = {\"criterion\": [\"gini\", \"entropy\"], \"random_state\": [0] } \ngrid5 = GridSearchCV(estimator = m5, param_grid = params, scoring = \"accuracy\", cv = 10)\ngrid5.fit(x_train, y_train)\nbest_acc = grid5.best_score_\nbest_param = grid5.best_params_\nprint(\"best acuracy: \", best_acc*100)\nprint(\"best parameters : \", best_param )\n","9fefa32a":"#bernoulli naive bayes\nparams = {'alpha': [0.25, 0.5, 1]}\ngrid6 = GridSearchCV(estimator = m6, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid6.fit(x_train, y_train)\nbest_acc = grid6.best_score_\nparam = grid6.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","386695ed":"#selecting random forest and decision tree as it shows the highest accuracies\n#fiiting random forest model\nmodel1 =  RandomForestClassifier(criterion = 'entropy', n_estimators = 300, random_state = 42)\nmodel1.fit(x_train, y_train)\n\nmodel_pred1 = model1.predict(x_test)\nmodel_pred1\nprint(\"accuracy score : \", accuracy_score(model_pred1, y_test))\nprint(\"precision score:\", precision_score(model_pred1, y_test))\nprint(\"recall score: \", recall_score(model_pred1, y_test))\nprint(\"f1_score :\", f1_score(model_pred1, y_test))\nprint(\"auc score : \", roc_auc_score(model_pred1, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred1, y_test))","ead5f7ea":"#fitting decision tree\nmodel2 =  DecisionTreeClassifier(criterion = 'gini', random_state = 0)\nmodel2.fit(x_train, y_train)\n\nmodel_pred2 = model2.predict(x_test)\nmodel_pred2\nprint(\"accuracy score : \", accuracy_score(model_pred2, y_test))\nprint(\"precision score:\", precision_score(model_pred2, y_test))\nprint(\"recall score: \", recall_score(model_pred2, y_test))\nprint(\"f1_score :\", f1_score(model_pred2, y_test))\nprint(\"auc score : \",  roc_auc_score(model_pred2, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred2, y_test))","37407793":"\n#Visualizing Confusion Matrix\ncm = confusion_matrix(model_pred1, y_test)\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues',annot = True, annot_kws= {'Fontsize': 15},cbar = False,  yticklabels = [\"NOT PLACED \", \"PLACED\"], xticklabels = ['predicted NOT PLACED', 'predicted PLACED'])\nplt.yticks(rotation= 0)\nplt.show()","c219e517":"#SHOWING ROU_AUC_CURVE\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, model_pred1)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","aeadbb31":"# EXPLORATORY DATA ANALYSIS","0def6778":"# MODEL SELECTION","95b14af7":"IN THIS GRAPH , MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 62%- 65% IN THEIR DEGREE RESPECTIVELY.","3b7527f4":"AS DEMONSTRATED, A DECENT AMOUNT OF INDIVIUALS ARE SPECIALIZED IN MARKETING & FINANCE","08d3b579":"# TUNING PARAMETER","31b1129f":"IN THE ABOVE GRAPH, WE CAN SEE NUMBER OF MALEs ARE MORE THAN NUMBER OF FEMALES","25b8ae4a":"OVER HERE, WE CAN SEE PERCENTAGE IN RANGE 62% - 90% ARE LIKELY TO BE PLACED.","e0f42913":"AS WE CAN SEE, THE SIGNIFICANT NUMBER OF PEOPLE HAS BEEN GRADUATED THROUGH COMMERCE & MANAGEMENT FIELD, FOLLOWED BY SCIENCE AND TECHNOLOGY AND OTHER STREAMS","d87b9362":"# DATA PREPROCESSING","b1a05efc":"HERE, THE INDIVIUALS WHO HOLDS COMMERCE & MANAGEMENT DEGREE ARE LIKELY TO PLACED.","a1398f28":"HERE, MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 58% - 65% IN THEIR MBA RESPECTIVELY.","32fa879c":"HERE,THE INDIVIUALS ARE NOT EXPERIENCED THOUGH , THEY ARE LIKELY TO BE PLACED.","05b55970":"MARKETING & FINANCE ARE MORE LIKELY TO BE PLACED.","44f28872":"HERE, MARKETING & HR ARE LIKELY TO PLACED WITH SALARY 3L PER ANNUM , IT IS FOLLOWED BY MARKETING & FINANCE","2f7fbb51":"AS DEMONSTRATED, LARGE NUMBER OF PEOPLE DO NOT HAVE WORK EXPERIENCE","32ea37c4":"\n# IMPORTING LIBRARIES","4bc0687c":"after tuning the hyperparameters we can see that, random forest shows the 86% accuracy. \n**Random Forest** model fits the best for this dataset","2283c812":"IN THIS, MBA % IN RANGE 52% - 75% ARE HIGLY LIKED TO BE PLACED"}}