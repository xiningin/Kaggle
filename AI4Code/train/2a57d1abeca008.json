{"cell_type":{"b833b733":"code","30ff1a56":"code","6ac3126f":"code","b4f48bf1":"code","be1dd6a0":"code","c42afe9e":"code","2e224587":"code","7c78d9d8":"code","15e8d002":"code","20798bb9":"code","3f3684dc":"code","0929866c":"code","46a26fa0":"markdown","d9b9bfe9":"markdown","2d9009f2":"markdown","a244dc25":"markdown","e6ce00b0":"markdown","29a749aa":"markdown","3a945a90":"markdown","d20853a5":"markdown","6ce95dcc":"markdown","f7ca5722":"markdown","a8218ad4":"markdown","e190e22b":"markdown","71694315":"markdown","08180b64":"markdown"},"source":{"b833b733":"! \/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install nara_wpe","30ff1a56":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport IPython\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport soundfile as sf\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\n\nfrom nara_wpe.utils import stft, istft, get_stft_center_frequencies\nfrom nara_wpe import project_root","6ac3126f":"try:\n    from nara_wpe.tf_wpe import wpe\nexcept ImportError:\n    print('Import error')\ntry:\n    from nara_wpe.tf_wpe import online_wpe_step, get_power_online\nexcept ImportError:\n    print('Import error')","b4f48bf1":"import sys\nprint(sys.path)\n!pwd\n%rm -rfv .\/tf2_wpe\n%rmdir .\/tf2_wpe\n!git clone https:\/\/github.com\/githubdcw\/tf2_wpe.git\nsys.path.append('\/kaggle\/working\/tf2_wpe')","be1dd6a0":"from tf2_wpe import wpe\nfrom tf2_wpe import online_wpe_step, get_power_online","c42afe9e":"stft_options = dict(\n    size=512,\n    shift=128,\n    window_length=None,\n    fading=True,\n    pad=True,\n    symmetric_window=False\n)","2e224587":"channels = 8\nsampling_rate = 16000\ndelay = 3\nalpha=0.99\ntaps = 10\nfrequency_bins = stft_options['size'] \/\/ 2 + 1","7c78d9d8":"import io\nfrom urllib.request import urlopen\nurl = \"https:\/\/github.com\/fgnt\/nara_wpe\/raw\/master\"\n","15e8d002":"file_template = 'AMI_WSJ20-Array1-{}_T10c0201.wav'\n# signal_list = [\n#     sf.read(str(project_root \/ 'data' \/ file_template.format(d + 1)))[0]\n#     for d in range(channels)\n# ]\nsignal_list = [\n    sf.read(io.BytesIO(urlopen(url+'\/'+'data'+'\/'+file_template.format(d + 1)).read()))[0]\n    for d in range(channels)\n]\n\ny = np.stack(signal_list, axis=0)\nIPython.display.Audio(y[0], rate=sampling_rate)","20798bb9":"Y = stft(y, **stft_options).transpose(1, 2, 0)\nT, _, _ = Y.shape\n\ndef aquire_framebuffer():\n    buffer = list(Y[:taps+delay+1, :, :])\n    for t in range(taps+delay+1, T):\n        yield np.array(buffer)\n        buffer.append(Y[t, :, :])\n        buffer.pop(0)","3f3684dc":"Z_list = []\n\nQ = np.stack([np.identity(channels * taps) for a in range(frequency_bins)])\nG = np.zeros((frequency_bins, channels * taps, channels))\n\n# with tf.Session() as session:\n#     Y_tf = tf.placeholder(tf.complex128, shape=(taps + delay + 1, frequency_bins, channels))\n#     Q_tf = tf.placeholder(tf.complex128, shape=(frequency_bins, channels * taps, channels * taps))\n#     G_tf = tf.placeholder(tf.complex128, shape=(frequency_bins, channels * taps, channels))\n\nwith tf.compat.v1.Session() as session:\n    Y_tf = tf.compat.v1.placeholder(tf.complex128, shape=(taps + delay + 1, frequency_bins, channels))\n    Q_tf = tf.compat.v1.placeholder(tf.complex128, shape=(frequency_bins, channels * taps, channels * taps))\n    G_tf = tf.compat.v1.placeholder(tf.complex128, shape=(frequency_bins, channels * taps, channels))\n\n    results = online_wpe_step(Y_tf, get_power_online(tf.transpose(Y_tf, (1, 0, 2))), Q_tf, G_tf, alpha=alpha, taps=taps, delay=delay)\n    for Y_step in tqdm(aquire_framebuffer()):\n        feed_dict = {Y_tf: Y_step, Q_tf: Q, G_tf: G}\n        Z, Q, G = session.run(results, feed_dict)\n        Z_list.append(Z)\n\nZ_stacked = np.stack(Z_list)\nz = istft(np.asarray(Z_stacked).transpose(2, 0, 1), size=stft_options['size'], shift=stft_options['shift'])\n\nIPython.display.Audio(z[0], rate=sampling_rate)","0929866c":"fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(20, 8))\nim1 = ax1.imshow(20 * np.log10(np.abs(Y[200:400, :, 0])).T, origin='lower')\nax1.set_xlabel('')\n_ = ax1.set_title('reverberated')\nim2 = ax2.imshow(20 * np.log10(np.abs(Z_stacked[200:400, :, 0])).T, origin='lower')\n_ = ax2.set_title('dereverberated')\ncb = fig.colorbar(im1)","46a26fa0":"# There is not local audio data.\n\nWe need to load Audio Data from github.","d9b9bfe9":"! pip install nara_wpe","2d9009f2":"Now try to import wpe online_wpe_step, get_power_online from tf2_wpe.","a244dc25":"nara_wpe is not standard lib\n\npip is required","e6ce00b0":"Modified version of nara_wpe.tf_wpe is on github as tf2_wpe https:\/\/github.com\/githubdcw\/tf2_wpe\n\nWe can clone it here.","29a749aa":"### Audio data","3a945a90":"### Setup","d20853a5":"# Power spectrum\nBefore and after applying WPE.","6ce95dcc":"Original nara_wpe is not compatible to tensorflow 2.\n* There are import errors.\n* No module named 'tensorflow.contrib'","f7ca5722":"# Example with real audio recordings\nThe iterations are dropped in contrast to the offline version. To use past observations the correlation matrix and the correlation vector are calculated recursively with a decaying window. $\\alpha$ is the decay factor.","a8218ad4":"### Non-iterative frame online approach\nA frame online example requires, that certain state variables are kept from frame to frame. That is the inverse correlation matrix $\\text{R}_{t, f}^{-1}$ which is stored in Q and initialized with an identity matrix, as well as filter coefficient matrix that is stored in G and initialized with zeros. \n\nAgain for simplicity the ISTFT is applied in Numpy afterwards.","e190e22b":"### Online buffer\nFor simplicity the STFT is performed before providing the frames.\n\nShape: (frames, frequency bins, channels)\n\nframes: K+delay+1","71694315":"tf.compat.v1 is added.","08180b64":"This notebook is a clone of an example for githubdcw\/nara_wpe forked from fgnt\/nara_wpe\nhttps:\/\/github.com\/githubdcw\/nara_wpe\/blob\/master\/examples\/WPE_Tensorflow_online.ipynb"}}