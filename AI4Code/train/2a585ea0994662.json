{"cell_type":{"af99eede":"code","67c2e877":"code","6af71c48":"code","a8764a71":"code","35b645ef":"code","8e623053":"code","6c2c2cd6":"code","b00be9c0":"code","3da8e21a":"code","a54315a6":"code","c708a6f3":"code","0d6630d8":"code","e71be878":"code","41e7fe9a":"code","e2829d96":"code","2848880b":"code","188d4c41":"code","3df5671b":"code","1df9f243":"code","67e47bea":"code","f2ef7d46":"code","57d21cbd":"code","285189b4":"code","be4fe5fa":"code","1c1b01c7":"code","c1e1006a":"code","6b94be7f":"code","122ceb97":"code","844611a0":"code","1cd31d7e":"code","d71a120c":"markdown","e126062d":"markdown","0d4940fd":"markdown","7ed7f0bd":"markdown","443bbfa5":"markdown","f5aeec3c":"markdown","98eb7374":"markdown","dceb386e":"markdown","20b0dc22":"markdown","bd4bba16":"markdown","54c7d4eb":"markdown","45220772":"markdown","dc1e0665":"markdown","6c35ddfb":"markdown","76d8b654":"markdown","21399903":"markdown","0531fa62":"markdown","986a5219":"markdown","6e64924a":"markdown","0cafda39":"markdown","45e92b94":"markdown"},"source":{"af99eede":"\nimport numpy as np \nimport pandas as pd \nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","67c2e877":"df = pd.read_excel(\"..\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx\")\n\ndf.shape","6af71c48":"df.dtypes","a8764a71":"df.select_dtypes(include=['object']).head()","35b645ef":"print(df.select_dtypes(include=['object']).isnull().sum())","8e623053":"df.select_dtypes(exclude=['object']).isnull().sum()","6c2c2cd6":"df.dropna().shape","b00be9c0":"df_cat = df.select_dtypes(include=['object'])\ndf_numeric = df.select_dtypes(exclude=['object'])\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nidf = pd.DataFrame(imp.fit_transform(df_numeric))\nidf.columns = df_numeric.columns\nidf.index = df_numeric.index\n\n\nidf.isnull().sum()\n","3da8e21a":"idf.drop([\"PATIENT_VISIT_IDENTIFIER\"],1)\nidf = pd.concat([idf,df_cat ], axis=1)\n\ncor = idf.corr()\ncor_target = abs(cor[\"ICU\"])\nrelevant_features = cor_target[cor_target>0.2]\nrelevant_features.index","a54315a6":"data = pd.concat([idf[relevant_features.index],idf[\"WINDOW\"]],1)\n\ndata.ICU.value_counts()","c708a6f3":"plt.figure(figsize=(10,7))\ncount = sns.countplot(x = \"ICU\",data=data)\ncount.set_xticklabels([\"Not Admitted\",\"Admitted\"])\nplt.xlabel(\"ICU Admission\")\nplt.ylabel(\"Patient Count\")\nplt.show()","0d6630d8":"plt.figure(figsize=(10,7))\nage = sns.countplot(data.AGE_ABOVE65, hue='ICU', data=data)\nage.set_xticklabels([\"Under 65\",\"65 and Above\"])\nplt.title(\"COVID-19 ICU Admissions by Age Range\")\nplt.xlabel(\"Patient Age\")\nplt.ylabel(\"Patient Count\")\nplt.xticks(rotation = 0)\nplt.legend(title = \"ICU Admission\",labels=['Not Admitted', 'Admitted'])\nplt.show()","e71be878":"plt.figure(figsize=(10,7))\nwindow = sns.countplot(data.WINDOW, hue='ICU', data=data)\nwindow.set_xticklabels([\"0-2\",\"2-4\",\"4-6\",\"6-12\",\"12+\"])\nplt.xticks(rotation = 45)\nplt.title(\"Patient Event Window and ICU Admission Counts\")\nplt.ylabel(\"Patient Count\")\nplt.xlabel(\"Window (hours)\")\nplt.legend(title = \"ICU Admission\",labels=['Not Admitted', 'Admitted'])","41e7fe9a":"plt.figure(figsize=(15,7))\npercentile = age = sns.countplot(sorted(idf.AGE_PERCENTIL), hue='ICU', data=idf)\nplt.xticks(rotation=40)\nplt.xlabel(\"Age Percentile\")\nplt.ylabel(\"Patient Count\")\nplt.title(\"COVID-19 ICU Admissions by Age Percentile\")\nplt.legend(title = \"ICU Admission\",labels=['Not Admitted', 'Admitted'], loc = 0)","e2829d96":"data = pd.get_dummies(data)\n\ndata.AGE_ABOVE65 = data.AGE_ABOVE65.astype(int)\ndata.ICU = data.ICU.astype(int)\n\ndata.head()","2848880b":"y = data.ICU\nX = data.drop(\"ICU\", 1)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42, shuffle = True)","188d4c41":"LR = LogisticRegression(max_iter = 500)\nLR.fit(X_train,y_train)\n\ny_hat = LR.predict(X_test)","3df5671b":"confusion_matrix = pd.crosstab(y_test, y_hat, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, fmt = 'g', cmap = 'Blues')\n\nprint(classification_report(y_test, y_hat))\nprint(\"AUC = \",roc_auc_score(y_test, y_hat))\n\nyhat_probs = LR.predict_proba(X_test)\nyhat_probs = yhat_probs[:, 1]\nfpr, tpr, _ = roc_curve(y_test, yhat_probs)\n\n\nplt.figure(figsize=(10,7))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label = \"Base\")\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"best\")\nplt.show()","1df9f243":"data2 = pd.concat([idf[relevant_features.index],idf[\"WINDOW\"]],1)\ndata2.AGE_ABOVE65 = data2.AGE_ABOVE65.astype(int)\ndata2.ICU = data2.ICU.astype(int)\n\nX2 = data2.drop(\"ICU\",1)\ny2 = data2.ICU","67e47bea":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nX2.WINDOW = label_encoder.fit_transform(np.array(X2[\"WINDOW\"].astype(str)).reshape((-1,)))\nX2.WINDOW","f2ef7d46":"X2_train,X2_test,y2_train,y2_test = train_test_split(X2,y2,test_size=0.25,random_state=42, shuffle = True)\n\nLR.fit(X2_train,y2_train)\n\ny2_hat = LR.predict(X2_test)","57d21cbd":"confusion_matrix2 = pd.crosstab(y2_test, y2_hat, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix2, annot=True, fmt = 'g', cmap = 'Reds')\n\nprint(\"ORIGINAL\")\nprint(classification_report(y_test, y_hat))\nprint(\"AUC = \",roc_auc_score(y_test, y_hat),'\\n\\n')\nprint(\"LABEL ENCODING\")\nprint(classification_report(y2_test, y2_hat))\nprint(\"AUC = \",roc_auc_score(y2_test, y2_hat))\n\n\ny2hat_probs = LR.predict_proba(X2_test)\ny2hat_probs = y2hat_probs[:, 1]\n\nfpr2, tpr2, _ = roc_curve(y2_test, y2hat_probs)\n\nplt.figure(figsize=(10,7))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label=\"Base\")\nplt.plot(fpr2,tpr2,label=\"Label Encoded\")\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"best\")\nplt.show()","285189b4":"print(\"Original 1:0 ratio =\",y2.value_counts()[1]\/y2.value_counts()[0])\nprint(\"Training 1:0 ratio =\",y2_train.value_counts()[1]\/y2_train.value_counts()[0])\nprint(\"Testing 1:0 ratio =\",y2_test.value_counts()[1]\/y2_test.value_counts()[0])","be4fe5fa":"X3_train,X3_test,y3_train,y3_test = train_test_split(X2,y2,test_size=0.25,random_state=42, stratify = y2, shuffle = True)\n\nprint(\"Original 1:0 ratio =\",y2.value_counts()[1]\/y2.value_counts()[0])\nprint(\"Training 1:0 ratio =\",y3_train.value_counts()[1]\/y3_train.value_counts()[0])\nprint(\"Testing 1:0 ratio =\",y3_test.value_counts()[1]\/y3_test.value_counts()[0])","1c1b01c7":"LR.fit(X3_train,y3_train)\ny3_hat = LR.predict(X3_test)","c1e1006a":"confusion_matrix3 = pd.crosstab(y3_test, y3_hat, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix3, annot=True, fmt = 'g', cmap = 'Greens')\n\nprint(\"LABEL ENCODING\")\nprint(classification_report(y2_test, y2_hat))\nprint(\"AUC = \",roc_auc_score(y2_test, y2_hat),'\\n\\n')\n\nprint(\"LABEL ENCODING + STRATIFY\")\nprint(classification_report(y3_test, y3_hat))\nprint(\"AUC = \",roc_auc_score(y3_test, y3_hat))\n\ny3hat_probs = LR.predict_proba(X3_test)\ny3hat_probs = y3hat_probs[:, 1]\n\nfpr3, tpr3, _ = roc_curve(y3_test, y3hat_probs)\n\nplt.figure(figsize=(10,7))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label=\"Base\")\nplt.plot(fpr2,tpr2,label=\"Label Encoded\")\nplt.plot(fpr3,tpr3,label=\"Stratify\")\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"best\")\nplt.show()","6b94be7f":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 42)\nX_train_res, y_train_res = sm.fit_sample(X3_train,y3_train.ravel())\n","122ceb97":"LR.fit(X_train_res, y_train_res)\ny_res_hat = LR.predict(X3_test)\n\nconfusion_matrix3 = pd.crosstab(y3_test, y_res_hat, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix3, annot=True, fmt = 'g', cmap=\"YlOrBr\")\n\nprint(\"LABEL ENCODING + STRATIFY\")\nprint(classification_report(y3_test, y3_hat))\nprint(\"AUC = \",roc_auc_score(y3_test, y3_hat),'\\n\\n')\n\nprint(\"SMOTE\")\nprint(classification_report(y3_test, y_res_hat))\nprint(\"AUC = \",roc_auc_score(y3_test, y_res_hat))\n\ny_res_hat_probs = LR.predict_proba(X3_test)\ny_res_hat_probs = y_res_hat_probs[:, 1]\n\nfpr_res, tpr_res, _ = roc_curve(y3_test, y_res_hat_probs)\n\nplt.figure(figsize=(10,10))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label=\"Base\")\nplt.plot(fpr2,tpr2,label=\"Label Encoded\")\nplt.plot(fpr3,tpr3,label=\"Stratify\")\nplt.plot(fpr_res,tpr_res,label=\"SMOTE\")\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"best\")\nplt.show()","844611a0":"from sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.metrics import plot_precision_recall_curve\n\nprecision, recall, _ = precision_recall_curve(y3_test, y3hat_probs)\nprecision_sm, recall_sm, _ = precision_recall_curve(y3_test, y_res_hat_probs)\n\nplt.figure(figsize=(10,7))\nplt.plot(recall, precision, label=\"w\/out SMOTE\")\nplt.plot(recall_sm, precision_sm, label=\"SMOTE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curves\")\nplt.legend(loc=\"best\")\n\nauc_score = auc(recall, precision)\nauc_score_sm = auc(recall_sm, precision_sm)\n\nprint(\"P-R AUC:\", auc_score)\nprint(\"P-R AUC (SMOTE):\", auc_score_sm)","1cd31d7e":"#y3hat_probs\n#y_res_hat_probs\nimport matplotlib.patches as mpatches\ny3 = np.asarray(y3_test)\n\npredict_df = pd.DataFrame({'Probability of Admission':y3hat_probs, 'Admission':y3})\npredict_df = predict_df.sort_values('Probability of Admission')\n\npredict_df['Rank'] = np.arange(1,483)\n\nplt.figure(figsize=(12,8))\n\nclasses = ['Not Admitted','Admitted']\nscatter = plt.scatter(predict_df['Rank'],predict_df['Probability of Admission'],\n                      c=predict_df['Admission'], \n                      cmap = 'seismic', \n                      marker = '^')\nplt.title(\"COVID-19 ICU Admission Predictions\\n(Without SMOTE)\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Predicted Probability of ICU Admission\")\nplt.legend(handles=scatter.legend_elements()[0], labels=classes,title=\"Actual ICU Admission\", loc='best')\nplt.show()\n\n\npredict_df2= pd.DataFrame({'SMOTE_prob':y_res_hat_probs, 'Admission':y3})\npredict_df2 = predict_df2.sort_values(\"SMOTE_prob\")\npredict_df2['Rank'] = np.arange(1,483)\n\nplt.figure(figsize=(12,8))\n\nscatter = plt.scatter(predict_df2['Rank'],predict_df2['SMOTE_prob'],\n                      c=predict_df2['Admission'], \n                      cmap = 'seismic', \n                      marker = '^')\nplt.title(\"COVID-19 ICU Admission Predictions\\n(SMOTE)\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Predicted Probability of ICU Admission\")\nplt.legend(handles=scatter.legend_elements()[0], labels=classes,title=\"Actual ICU Admission\", loc='best')\nplt.show()\n\n\n\n","d71a120c":"These are all the features with a correlation coefficient over 0.2, indicating that each of these variables is at least weakly correlated to ICU Admission. Some of these features are clearly correlated with one another (included are the mean, median, max, and min for a single metric), so it may be a good idea to drop even more of these values. For now, though, let's just see what result we get with all of them. The dataframe we'll be analyzing going forward will consist of these features, plus the categorical WINDOW feature. I observed a positive correlation between WINDOW and ICU in some visual data exploration I conducted, which can be seen immediately below.","e126062d":"# Conclusion?\n\nSo we ultimately end up with two models. One without synthesized cases, which has a higher positive precision and overall accuracy, and one with synthesized cases for training, which suffers in positive precision and overall accuracy but has a greater positive recall. In any case, the probability generated by the logistic regression model must always be considered for a given prediction.\n\nFor the main purpose of the model, that is, predicting patient ICU admission, it may be safer to go with higher recall, due to the lower number of False Negatives. However, higher precision may actually be desirable. As I said in the beginning, we are not diagnosing illness, but determining if a person is likely to need an ICU bed. False Positives may lead to beds being occupied unnecessarily. If beds are extremely limited (which they are increasingly becoming), this would not ideal. \n\nSo thus we are left with difficult questions: do we help as many people that need it as possible, with the risk of occupying beds unnecessarily? Or do we allocate ICU beds as efficiently as possible, with a greater risk of someone that needs a bed not getting one? Of course, this is for decisions made by a mathematical model. The final decisions ultimately lie with health care professionals, who will probably know better than a Logistic Regression algorithim implemented by someone who knows little about medicine. The insights may be useful, nonetheless.\n\nI don't feel particularly done here. I want to improve feature selection for starters, something a bit better than looking for linear correlations. I have seen some of my colleagues on Kaggle use libraries like xgboost for this purpose, so I may explore that. I mentioned earlier that I'm also not satisfied with our solution to missing data. I realize there's no perfect solution to accounting for missing information, but I would like to explore other possible strategies. I also would like to look into other classification models, e.g. Random Forest.\n\nBut as it stands, I'd say we have a halfway decent model to start from.","0d4940fd":"There are A LOT of missing values, around 50% of the numeric data. We'll use scikit-learn to fill the null values with imputed means. This is probably not the best approach. If we dropped incomplete columns:","7ed7f0bd":"Now, onto our logistic regression. We'll need to encode our categorical variable, WINDOW. We'll use get_dummies for this purpose. Also, we'll change AG_ABOVE65 and ICU back to int (they became floats during impution), just to make things look nicer.","443bbfa5":"We lose a huge portion of our data! I'd greatly appreciate any other solutions or suggestions, but for now impution will do for this cursory analysis","f5aeec3c":"Positive ICU admission recall jumps significantly, and the AUC increases as well. The precision of the positive case drops significantly, so the positive F1 Score stays about the same. Overall accuracy also drops slightly.\n\nLet's do some final visualizations for the two best approaches, the Label Encoded + Stratified model, and the same model but with SMOTE.","98eb7374":"There is a clear positive correlation between the Window and ICU Admission, hence it's inclusion in the model","dceb386e":"Yeesh, what a difference! All the metrics across the board shot up!\n\nNow, while there may have been some improvement to the model itself, this is largely an improvement in how we evaluate the model. By stratifying, we ensure a fair test and train split.\n\nI'm also interested in using SMOTE. SMOTE (Synthetic Minority Oversampling Technique) synthesizes instances for the minority case to create a more balanced model.","20b0dc22":"# Building a Predictive Model for COVID-19 ICU Admission\n\nAccurate predictions are always desirable, but it is especially important for medical applications. A false negative would not just be a statistical inconvenience, it could very well lead to more severe illness or death.\n\nNow, we aren't exactly attempting to diagnose here, but I'm aiming to at least get a reasonably high recall of positive ICU admission. I'll start by doing a basic logistic regression.","bd4bba16":"Our recall for ICU Admission went slightly up, at the cost of some precision. AUC also increased slightly. For this model, I'd say recall takes precedence over precision, so we'll stick with this change. \n\nI'm curious about the ratio of ICU Admissions:Non-Admissions in the splits compared to the original column","54c7d4eb":"Next, we'll identify the most important numerical features. We'll use the corr method to identify all the features with at least a weak correlation to the dependent variable (ICU Admission)","45220772":"Looks like there are small variations in the distribution of positive:negative cases between the original data set, the training set, and the test set. We'll apply the stratity option in train_test_split in the binary target","dc1e0665":"In contrast, there doesn't seem to be a clear relation between age percentile and ICU admission.","6c35ddfb":"Now for the logisitic regression. It's actually pretty easy. We'll use the logistic regression model to then predict y values in y_hat, which we'll compare to y_test (the actual values)","76d8b654":"Of all the patients, about 26.7% were admitted to the ICU","21399903":"In both models, we see that at very low and very high probabilities the real life results almost always correspond to the prediction. ","0531fa62":"Now the distributions are pretty even. Let's see the effect on our regression","986a5219":"And so here's our Logistic Regression model. Kinda cool, but I'd like to try and improve the metrics for the minority case (positive ICU admission). Let's tune the regression a bit.\n\nFirst, we'll try LabelEncoding WINDOW instead of using dummy variables","6e64924a":"These are the two non-numeric columns in the data set. As we can see, there are no missing pieces of data in this column, which is more than can be said for the rest of the data.","0cafda39":"Individials above 65 appear to be admitted to the ICU more often than those below, corroroborating the statements of public health officials; older patients are at a higher risk for more serious illness","45e92b94":"Now, we create two dataframes, one being the dependent variable, ICU, and the other being all the independent variables. We'll then create a train-test split in order to evaluate the model later."}}