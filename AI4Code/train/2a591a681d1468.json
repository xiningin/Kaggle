{"cell_type":{"79f99cf5":"code","12c80141":"code","94dd0020":"code","645afa4c":"code","d059952d":"code","4c5fb8fb":"code","f2137764":"code","2455d597":"code","40a358f2":"code","4a3922d5":"code","d3b4740c":"code","032e9bf5":"code","2108cb31":"code","44bbc385":"code","0040892c":"code","5842d0a6":"code","4590b942":"code","b0db94d2":"code","6b3bfa72":"code","4137219c":"code","3d06b972":"code","d5610a17":"code","6cea726d":"code","609825d2":"code","32f5fe93":"code","d29b4de6":"code","0ea929ac":"code","ca7b1a03":"markdown","b0b64c52":"markdown","6bc70bf4":"markdown","5157b6b7":"markdown","2a78f72d":"markdown","11d039b2":"markdown","aac47a9f":"markdown","7aa11f3e":"markdown","0661b543":"markdown","19bc7197":"markdown","e1ecc55e":"markdown","1ffcada2":"markdown","5f92f1da":"markdown","0adbb095":"markdown","dd46585f":"markdown","ee5f44c8":"markdown","bf3f0779":"markdown"},"source":{"79f99cf5":"# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\n\npd.set_option(\"display.max_rows\", None,\"display.max_columns\", None)\nwarnings.simplefilter(action='ignore')\nplt.style.use('seaborn')","12c80141":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.drop(\"Unnamed: 32\",axis=1,inplace=True)\ndf.head()","94dd0020":"df.info()","645afa4c":"df.describe().transpose()","d059952d":"# Dropping \"id\" column\ndf.drop(\"id\",axis=1,inplace=True)","4c5fb8fb":"# Replacing \"M\" with 0 and \"B\" with 1\ndf[\"diagnosis\"].replace(\"M\",0,inplace=True)\ndf[\"diagnosis\"].replace(\"B\",1,inplace=True)","f2137764":"df.head()","2455d597":"plt.figure(figsize=(20,18))\nsns.heatmap(df.corr(),annot=True,linewidths=1,cmap=\"YlGnBu\")\nplt.show()","40a358f2":"num_mean = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\ndf[num_mean].corr()['diagnosis'].sort_values()","4a3922d5":"top_corr = ['diagnosis','concave points_mean','perimeter_mean','radius_mean','area_mean','concavity_mean','compactness_mean','texture_mean']\n\nsns.pairplot(df[top_corr],hue='diagnosis')\nplt.show()","d3b4740c":"from sklearn.model_selection import train_test_split\n\nX = df.drop('diagnosis',axis=1)\ny = df['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)","032e9bf5":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","2108cb31":"# Importing keras related libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","44bbc385":"# Shape of input i.e. a 1D array having 30 features\nX_train_scaled.shape[1:]","0040892c":"model = Sequential()\n\n# Adding input layer\nmodel.add(Dense(30,activation='relu',input_shape=X_train_scaled.shape[1:],name=\"input\"))\n# Adding hidden layers\nmodel.add(Dense(30,activation='relu',name=\"hidden_1\"))\nmodel.add(Dense(15,activation='relu',name=\"hidden_2\"))\n# Adding output layer and since this is a binary classification we are using \"sigmoid\" activation function\nmodel.add(Dense(1,activation='sigmoid',name=\"output\"))\n\nmodel.summary()","5842d0a6":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')","4590b942":"%%time\nmodel.fit(x=X_train_scaled,\n          y=y_train,\n          validation_data=(X_test_scaled,y_test),\n          batch_size=128,epochs=500)","b0db94d2":"pd.DataFrame(model.history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.show()","6b3bfa72":"from tensorflow.keras.layers import Dropout","4137219c":"model = Sequential()\n\n# Adding input layer\nmodel.add(Dense(30,activation='relu',input_shape=X_train_scaled.shape[1:],name=\"input\"))\nmodel.add(Dropout(0.5))\n# Adding hidden layers\nmodel.add(Dense(30,activation='relu',name=\"hidden_1\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(15,activation='relu',name=\"hidden_2\"))\nmodel.add(Dropout(0.5))\n# Adding output layer and since this is a binary classification we are using \"sigmoid\" activation function\nmodel.add(Dense(1,activation='sigmoid',name=\"output\"))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')","3d06b972":"%%time\nmodel.fit(x=X_train_scaled,\n          y=y_train,\n          validation_data=(X_test_scaled,y_test),\n          batch_size=128,epochs=500,verbose=0)","d5610a17":"pd.DataFrame(model.history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.show()","6cea726d":"from tensorflow.keras.callbacks import EarlyStopping","609825d2":"model = Sequential()\n\n# Adding input layer\nmodel.add(Dense(30,activation='relu',input_shape=X_train_scaled.shape[1:],name=\"input\"))\n# Adding hidden layers\nmodel.add(Dense(30,activation='relu',name=\"hidden_1\"))\nmodel.add(Dense(15,activation='relu',name=\"hidden_2\"))\n# Adding output layer and since this is a binary classification we are using \"sigmoid\" activation function\nmodel.add(Dense(1,activation='sigmoid',name=\"output\"))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","32f5fe93":"%%time\nmodel.fit(x=X_train_scaled,\n          y=y_train,\n          validation_data=(X_test_scaled,y_test),\n          batch_size=128,epochs=500,\n          callbacks=[early_stop],\n          verbose=0)","d29b4de6":"pd.DataFrame(model.history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.show()","0ea929ac":"from sklearn.metrics import classification_report,confusion_matrix\n\npredictions = model.predict_classes(X_test_scaled)\nprint(classification_report(y_test,predictions),\"\\n\\n\")\nprint(confusion_matrix(y_test,predictions))","ca7b1a03":"### **i) Using Dropout layer to deal with Overfitting**","b0b64c52":"# **2. Data Preprocessing**","6bc70bf4":"## **c) Model Evaluation**","5157b6b7":"**Remark:** We will be using test sample for validation as well.","2a78f72d":"**Inferences:**\n<br>Its clear that letting the model run for 500 epochs has lead to Overfitting. There are various ways using which overfitting can be tackled, among those most commonly used techniques are:\n- Dropout Layer\n- Early Stopping\n\n**Refrence**: https:\/\/www.kdnuggets.com\/2019\/12\/5-techniques-prevent-overfitting-neural-networks.html","11d039b2":"### **ii) Using Early Stopping to deal with Overfitting**","aac47a9f":"# **1. Getting Familier with dataset**","7aa11f3e":"## **b) Model Training**","0661b543":"## **a) Train-Test Split**","19bc7197":"Generally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights.","e1ecc55e":"# **5. Model Creation\/Evaluation**","1ffcada2":"# **3. EDA**","5f92f1da":"# **About the dataset**[<a href=\"https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\">src<\/a>]\n\n### **Description** \nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https:\/\/goo.gl\/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu cd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\n.. topic:: References\n\n    W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T\/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n    O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995.\n    W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171.\n\n\n\n#### **Attribute Information:**\n\n1. ID number\n2. Diagnosis (M = malignant, B = benign)\n3. Ten real-valued features are computed for each cell nucleus:\n  - radius (mean of distances from center to points on the perimeter)\n  - texture (standard deviation of gray-scale values)\n  - perimeter\n  - area\n  - smoothness (local variation in radius lengths)\n  - compactness (perimeter^2 \/ area - 1.0)\n  - concavity (severity of concave portions of the contour)\n  - concave points (number of concave portions of the contour)\n  - symmetry\n  - fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.","0adbb095":"# **4. Data Preparation**","dd46585f":"## **a) Model Creation**","ee5f44c8":"# **Steps Involved**\n1. Getting familier with dataset\n2. Data Preprocessing\n3. EDA\n4. Data Preparation\n5. Model Creation\/Evaluation","bf3f0779":"## **b) Feature Scaling**"}}