{"cell_type":{"f5fb956a":"code","5debccca":"code","6dc1e176":"code","31a060f8":"code","fd068400":"code","4cd7fc15":"code","5ed95e59":"code","ddbd3136":"code","03fdd2f7":"code","d0629529":"code","2b098e04":"code","8726597e":"code","7427040a":"code","fea977d1":"code","59d5da60":"code","5ea9a918":"code","fa8b6dc1":"code","be1f5f50":"code","9d8bcf5d":"code","e4b73edc":"code","b6913643":"code","8613fec4":"code","3586b00f":"code","abfaef55":"code","09beaada":"code","b33b14f4":"code","c34af4a7":"code","d44e0e48":"code","d9567d7e":"markdown","08e25cd0":"markdown","b0e55a2c":"markdown","01b84536":"markdown","bb9affdd":"markdown","0b29cd21":"markdown","2a3591f2":"markdown","d23f4b37":"markdown","1ba025da":"markdown","4e7e9af9":"markdown","1e9bf941":"markdown","571529e7":"markdown","75e63a27":"markdown","1020365a":"markdown","238ec40c":"markdown","23f1cce9":"markdown","094780c9":"markdown","58c1fddb":"markdown","1c09a61d":"markdown","5ddc7927":"markdown","3355caa3":"markdown","818e6242":"markdown","34595fda":"markdown","73984b6d":"markdown","ca7d8d5e":"markdown","e1d140cb":"markdown","3aecc9ec":"markdown","2e657db6":"markdown","913ef708":"markdown","ef94f930":"markdown","b76a8019":"markdown","ab152fcf":"markdown"},"source":{"f5fb956a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5debccca":"# Load data from the csv file, droping columns if provided\ndef load_data(filename:str, columns:'list of strings' = None):\n    result = pd.read_csv(filename)\n    if columns is not None and len(columns) > 1:\n        return result.drop(columns=columns)\n    return result","6dc1e176":"# Print a brief, quick analysis of a dataframe to gain insight\ndef quick_analysis(data_frame:pd.DataFrame):\n    print('\\nAnalysis of dataframe:')\n    print(data_frame.head())\n    print(data_frame.info())\n    print(data_frame.describe())","31a060f8":"from matplotlib import pyplot as plt\n\n%matplotlib inline\n\nraw_data = load_data('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nquick_analysis(raw_data)\n\nplt.hist(raw_data['SalePrice'])\nplt.show()","fd068400":"non_numeric_cols = raw_data.loc[:, raw_data.dtypes == object]\n\nfor col in non_numeric_cols.columns:\n    print(non_numeric_cols[col].value_counts())","4cd7fc15":"corr_matrix = raw_data.corr()\nsale_correl = corr_matrix['SalePrice'].sort_values(ascending=False)\nprint(sale_correl)","5ed95e59":"raw_data['Grade'] = raw_data['OverallQual'] \/ raw_data['OverallCond']","ddbd3136":"raw_data['Age'] = raw_data['YrSold'] - raw_data['YearBuilt']\nraw_data['RemodAge'] = raw_data['YrSold'] - raw_data['YearRemodAdd']","03fdd2f7":"raw_data['TotalSF'] = raw_data['TotalBsmtSF'] + raw_data['1stFlrSF'] + raw_data['2ndFlrSF']","d0629529":"corr_matrix = raw_data.corr()\nsale_correl = corr_matrix['SalePrice'].sort_values(ascending=False)\nprint(sale_correl)","2b098e04":"age_correl = corr_matrix['Age'].sort_values(ascending=False)\nprint('Age correlations:', age_correl, '\\n')\n\nremod_age_correl = corr_matrix['RemodAge'].sort_values(ascending=False)\nprint('RemodAge correlations:', remod_age_correl, '\\n')\n\ngrade_correl = corr_matrix['Grade'].sort_values(ascending=False)\nprint('Grade correlations:', grade_correl, '\\n')\n\ntotalsf_correl = corr_matrix['TotalSF'].sort_values(ascending=False)\nprint('TotalSF correlations:', totalsf_correl, '\\n')","8726597e":"from matplotlib import ticker as tick\n\n# Plot correlations\ndef corr_plot(data :pd.DataFrame, feature :str, threshold=0.5, plot_type :str = 'scatter', y_lower_scale=True, same_fig=True, fig_size=(3, 4)):\n    fig = plt.figure()\n    corr_matrix = data.corr()\n    alpha = 0.3\n    i = 1\n    for feat in corr_matrix.columns:\n        if abs(corr_matrix[feat][feature]) > threshold and feat != feature:\n            if same_fig == True:\n                ax = fig.add_subplot(fig_size[0], fig_size[1], i)\n                if plot_type == 'scatter':\n                    ax.scatter(x=feat, y=feature, data=data, alpha=alpha)\n                elif plot_type == 'hist':\n                    ax.hist(x=feat, data=data)\n                ax.set_xlabel(feat)\n                if y_lower_scale == True:\n                    ax.yaxis.set_major_formatter(tick.FormatStrFormatter('%.e'))\n                plt.yticks(rotation=45)\n                i = i + 1\n            else:\n                if plot_type == 'scatter':\n                    plt.scatter(x=feat, y=feature, data=data, alpha=alpha)\n                elif plot_type == 'hist':\n                    plt.hist(x=feat, data=data)\n                plt.xlabel(feat)\n                plt.show()\n\n    if same_fig == True:\n        fig.tight_layout()\n        plt.show()","7427040a":"corr_plot(raw_data, 'SalePrice', y_lower_scale=False, same_fig=False)","fea977d1":"corr_plot(raw_data, 'SalePrice', plot_type='hist', y_lower_scale=False, same_fig=False)","59d5da60":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n\nclass FeatureCreator(BaseEstimator, TransformerMixin):\n    def __init__(self, features :'list of strings', operation, as_dataframe :bool = False, feat_name :str = 'NewFeature'):\n        self.features = features\n        self.operation = operation\n        self.as_dataframe = as_dataframe\n        self.feat_name = feat_name\n\n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X, y = None):\n        no_feat = len(self.features)\n        prev_feat = self.features[0]\n        for i in range(1, no_feat):\n            new_feature = self.operation(X[prev_feat], X[self.features[i]])\n            prev_feat = self.features[i]\n\n        if self.as_dataframe:\n            X[self.feat_name] = new_feature\n            return X\n\n        return np.c_[X, new_feature]\n\n\nclass FeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, features, as_dataframe :bool = False):\n        self.features = features\n        self.as_dataframe = as_dataframe\n\n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X, y = None):\n        if self.as_dataframe == True:\n            return X.drop(columns=self.features)\n        return np.c_[X.drop(columns=self.features)]\n\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, features, as_dataframe :bool = False):\n        self.features = features\n        self.as_dataframe = as_dataframe\n\n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X, y = None):\n        if self.as_dataframe == True:\n            return X[self.features]\n        return np.c_[X[self.features]]\n\n\nclass CategoricalImputer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, strategy :str = 'most_frequent', value :str = None):\n        self.strategy = strategy\n        self.value = value\n        \n    def fit(self, X, y=None):\n        if self.strategy == 'most_frequent':\n            self.fill = pd.Series([X[col].mode()[0] for col in X], index=X.columns)\n        elif self.strategy == 'nan_to_none':\n            self.fill = pd.Series(['None' for col in X], index=X.columns)\n        elif self.strategy == 'custom_val' and self.value is not None:\n            self.fill = pd.Series([self.value for col in X], index=X.columns)\n            \n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n   \n","5ea9a918":"raw_data['YrSold_C'] = raw_data['YrSold'].copy().astype(str)\nraw_data['MoSold'] = raw_data['MoSold'].astype(str)\nraw_data['MSZoning'] = raw_data['MSZoning'].astype(str)\nraw_data['OverallCond_C'] = raw_data['OverallCond'].copy().astype(str)\n\nnum_cols = [\n    'OverallQual', \n    'OverallCond', \n    'YearBuilt', \n    'YearRemodAdd', \n    'TotalBsmtSF', \n    '1stFlrSF', \n    '2ndFlrSF', \n    'GarageCars',\n    'GarageArea',\n    'FullBath',\n    'YrSold', \n] \ncat_cols = [\n    'MSZoning', \n    'Street', \n    'Utilities', \n    'Neighborhood', \n    'ExterQual', \n    'ExterCond', \n    'BsmtQual', \n    'BsmtCond', \n    'Heating', \n    'CentralAir', \n    'PavedDrive', \n    'SaleType', \n    'SaleCondition',\n    'YrSold_C', \n    'MoSold',\n    'OverallCond_C',\n]","fa8b6dc1":"cat_cols_categs = [raw_data[col].unique() for col in cat_cols]\ncat_cols_categs","be1f5f50":"from sklearn.pipeline import Pipeline, FeatureUnion \nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer as Imputer\n\nnum_pipeline = Pipeline([\n    ('feat_sel', FeatureSelector(num_cols, True)),\n    ('grade', FeatureCreator(['OverallCond', 'OverallQual'], lambda x, y: x \/ y, as_dataframe=True, feat_name='Grade')),\n    ('age', FeatureCreator(['YrSold', 'YearBuilt'], lambda x,y: x - y, as_dataframe=True, feat_name='Age')),\n    ('remod_age', FeatureCreator(['YrSold', 'YearRemodAdd'], lambda x,y: x - y, as_dataframe=True, feat_name='RemodAge')),\n    ('total_sf', FeatureCreator(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], lambda x,y: x + y, as_dataframe=True, feat_name='TotalSF')),\n    ('drop_cat_feat', FeatureDropper(['YrSold', 'OverallCond'], as_dataframe=True)),\n    ('imputer_mean', Imputer(strategy='mean')),\n    ('robust_scalar', RobustScaler())\n]) \n\ncat_pipeline = Pipeline([\n    ('feat_sel', FeatureSelector(cat_cols, True)),\n    ('imputer_most_frequent', CategoricalImputer()),\n    ('encode', OneHotEncoder(categories=cat_cols_categs, sparse=False)),\n])\nfeat_union = FeatureUnion(transformer_list=[\n    ('num_features', num_pipeline),\n    ('cat_features', cat_pipeline),\n])","9d8bcf5d":"train_labels = raw_data['SalePrice'].copy()\ntrain_feat = feat_union.fit_transform(raw_data)","e4b73edc":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","b6913643":"from sklearn.linear_model import LinearRegression\n\n\nlin_reg = LinearRegression()\ngrid_search = GridSearchCV(lin_reg, [{}], cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('Linear regression best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_lr_model = grid_search.best_estimator_","8613fec4":"from sklearn.tree import DecisionTreeRegressor\n\n\nhyperparams_vals = [\n    {'max_features': [6, 10, 12, 16, 18, 20, 24]},\n]\n    \ndt_reg = DecisionTreeRegressor(random_state=42)\ngrid_search = GridSearchCV(dt_reg, hyperparams_vals, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('Decision tree best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_dt_model = grid_search.best_estimator_","3586b00f":"from sklearn.ensemble import RandomForestRegressor\n\n\nhyperparams_vals = [\n    {'n_estimators': [200, 225, 250], 'max_features': [16, 24, 30]},\n    {'bootstrap': [False], 'n_estimators': [220, 225], 'max_features': [24, 28]},\n]\n    \nforest_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\ngrid_search = GridSearchCV(forest_reg, hyperparams_vals, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('Random forest best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_rf_model = grid_search.best_estimator_","abfaef55":"from xgboost import XGBRegressor\n\n\nhyperparams_vals = [\n    {'n_estimators': [450, 500, 750], 'max_features': [2, 4, 8], 'max_depth': [3, 4, None]},\n]\n    \nxgbr_reg = XGBRegressor(learning_rate=0.05, n_threads=-1, random_state=42)\ngrid_search = GridSearchCV(xgbr_reg, hyperparams_vals, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('XGBoost regressor best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_xgb_model = grid_search.best_estimator_","09beaada":"from sklearn.svm import SVR\n\n\nhyperparams_vals = [\n    {'kernel': ['linear', 'sigmoid', 'rbf'], 'gamma': ['auto', 'scale']},\n    {'kernel': ['poly'], 'gamma': ['auto', 'scale'], 'degree': [3, 4, 5]},\n]\n    \nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, hyperparams_vals, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('Support vector machine best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_svm_model = grid_search.best_estimator_","b33b14f4":"from sklearn.linear_model import ElasticNet\n\n\nhyperparams_vals = [\n    {'alpha': [0.0005, 0.005, 0.05, 0.2], 'l1_ratio': [0.1, 0.25, 0.75, 0.9]},\n]\n\nenet_reg = ElasticNet(max_iter=100000000, tol=0.001)\ngrid_search = GridSearchCV(enet_reg, hyperparams_vals, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_feat, train_labels)\nprint('ElasticNet best hyperparameters:')\nprint(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n\nfinal_enet_model = grid_search.best_estimator_","c34af4a7":"rf_feat_imp = final_rf_model.feature_importances_\nxgb_feat_imp = final_xgb_model.feature_importances_\n\nother_feat = ['Grade', 'RemodAge', 'TotalSF']\nall_features = num_cols.copy()\nprint(num_cols)\nfor cat_values in cat_cols_categs.copy():\n    all_features.extend(cat_values)\nall_features.extend(other_feat.copy())\n\nprint('Random forest feature importances:')\nfor feat in sorted(zip(rf_feat_imp, all_features), reverse=True):\n    print(feat)\n    \nprint('\\nXGBoost feature importances:')\nfor feat in zip(xgb_feat_imp, all_features):\n    print(feat)","d44e0e48":"test_data = load_data('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_data['YrSold_C'] = test_data['YrSold'].copy().astype(str).replace('nan', None)\ntest_data['MoSold'] = test_data['MoSold'].astype(str).replace('nan', None)\ntest_data['MSZoning'] = test_data['MSZoning'].astype(str).replace('nan', None)\ntest_data['OverallCond_C'] = test_data['OverallCond'].copy().astype(str).replace('nan', None)\ntest_feat = feat_union.transform(test_data)\n\nrf_predictions = final_rf_model.predict(test_feat)\nxgb_predictions = final_xgb_model.predict(test_feat)\npredictions = rf_predictions * 0.35 + xgb_predictions * 0.65\n\npred_df = pd.DataFrame()\npred_df['Id'] = test_data['Id']\npred_df['SalePrice'] = predictions.flatten()\n\nprint(pred_df)","d9567d7e":"Finally let's remake the correlation matrix, to see the influence our new features exert onto the sale price.","08e25cd0":"Finally, we can now define pipelines for numerical and categorical columns and bring the results together using the ```FeatureUnion``` class provided by ```sklearn```.\n\nI opted for the *mean* imputer strategy for numerical features an the *most frequent* stategy for categorical features, as they seemed to be the most sensible ones to use, based on the density and distribution seen for each feature in the previous plots.\n\nFinally, for numerical features, *standard scaling* seems like a good scaling option, while for categorical features the *one-hot encoding* would be a competent option.","b0e55a2c":"# Model evaluation\n\nTo evaluate the performance of the models we will test, a good option if to use the **RMSE** in combination with **cross-validation**. To enable easier evaluation and hyperparameter tuning, we shall also use the **grid search cross-validation** algorithm.\n\nWe shall test the following algorithms:\n  * **Linear regression:** a classical, common algorithm; probably will not yield great results, but no reason not to test it\n  * **Decision trees:** a better alternative, in some cases; again, by itself will probabily not give greate results, from what I know, decission trees being easily overfit\n  * **Random forest:** a much more useful alternative to *decision trees*, since it makes use of ensemble learning, which in turn is usually a better alternative to using only 1 model\n  * **XGBoost random forest regressor:** a gradient boosting alternative to the previously mentioned random forest models\n  * **Support vector machine:** a completely different approach from the previous models; maybe it will surprize us\n  * **ElasticNet regression:** another commonly used linear model, that could maybe yield good results\n  \nThe grid search cross-validation algorithm is found in ```sklearn```, just like all others, with the exception of ```XGBoost``` ones. We can also import the **MSE** function, in case we will need it later on:","01b84536":"As we can see all our features are moderately correlated to the sale price, with ```Age``` and ```RemodAge``` actually being more strongly correlated to it than either of their components. Granted that the difference is not that big.\n\nThe ```Grade``` seems to be a good indicator as well, but the ```TotalSF``` is the clear winner, being strongly correlated to the price (compared to the aforementioned at least).\n\nNow let us also check the correlation between each of the new features with regards to their components, in order to prevent \"duplicates\".","bb9affdd":"We cannot know if the test data will have all categories that each non-numeric feature can take, thus we should also store them separately, for later encoding:","0b29cd21":"I also thought maybe the total surface of the house could impact significantly its price, this adding the following feature:","2a3591f2":"Before anything else, let's define the columns we want to keep, initially. We shall also separate them into numerical and categorical features.\n\nSince both the year and month during which a house was sold are actually categorical features, we should also convert them. The same need to be done to the ```MSZoning``` and ```OverallCond``` features.","d23f4b37":"We can see a strong negative correlation between ```Age``` and ```YearBuilt```, as well as between ```RemodeAge``` and ```YearRemodAdd```. Thus, we should keep either or, from each pair, since they are so similar.\n\nWhile ```Grade``` is also pretty correlated with its components, I will keep it for now, since it is nowhere near as similar to them, as the previous features were.\n\n```TotalSF``` seems to be strongly correlated to 3 other features (excluding the sale price), 2 of which are its components. The third is most likely correlated to the ```1stFlrSF``` feature and thus I will ignore it for now. We could still keep this feature, since it also takes into account the ```2ndFlrSF```.","1ba025da":"After checking the other low correlation features, with regards to the sale price, I found no other combinations I could make or that had a good correlation (>0.5).","4e7e9af9":"# Brief Data Analysis\n\nI will also define another function, used for quickly analysing dataframes, making use of the following three existing methods, from ```pandas.DataFrame```:\n\n  * ```head``` - to display the first 5 rows of the dataframe, useful for a quick glance\n  * ```info``` - to check out the available features, their types and the amount of null\/NaN values each has\n  * ```describe``` - to have a quick statistical analysis of the dataframe, in order to get an initial idea about it","1e9bf941":"Doing the same but with histograms:","571529e7":"Seems like for the most part, the features engineered have a significant importance in predicting the sale price. We should move on with actually predicting the labels for the test data, as I am pleased with the current results.","75e63a27":"Finally, we can start testing the models:","1020365a":"Next, why not focus on the columns related to the year the house was built, remodelled and sold in. Concretely, we can find the age of the house and the remodelling it suffered, at the time of the sell.","238ec40c":"## Pipelines and transformers\n\nIn order to make it easier to select, drop and create features, as well as deal with missing values and scaling, I will create *pipelines*, which will contain *custom transformers*.\n\nLet's start with the transformers. They are quite basic, one drops given columns, one only keeps specified columns and another creates a new feature from multiple others, given an operation to perfom on the later. I will also add some other parameters, which will allow for the result to be either a ```numpy``` array or a ```pandas``` dataframe.","23f1cce9":"# Reading the data\n\nFirstly, I will start by creating a function to read the data from the files, to simplify the process. I used it before creating my pipelines, so I also added a parameter which allowed given columns to be dropped from the resulting dataframe.","094780c9":"Taking a look at another linear model, the *ElasticNet regression* yields the following results:","58c1fddb":"Seems like the last two are quite similar, but ```XGBoost```'s *random forest* is the best performer, even if by a slight margin.\n\nLets quickly analize the *support vector machine* as well:","1c09a61d":"# Feature Engineering\n## Initial thoughts and approaches\n\nAt this point, we should start and brainstorm combinations of those features that we think might impact the sale price of a house. \n\nSince some features could be more strongly correlated than one would think, we should quickly check the correlation matrix of our dataset.","5ddc7927":"We can now create the train features and labels, by simply doing the folowing:","3355caa3":"There is a large number of categorical features, as shown by the previous insight. We could quickly glance at the different categories each of those features present:","818e6242":"Let's start with the histogram. We can clearly see that the data is skewed right. The highest frequency is around 200k, followed by 300k. The few houses that cost more than 4000k seem to be exceptions from the rule. I will still keep them in mind, but maybe some algorithm should be run to check anomalies\/outliers and give a more clear indication.\n\nFrom the quick analysis, we can see a clue that some features (i.e.: ```Alley```, ```PoolQC``` or ```Fence```) could be useless. In regards to the others, nothing out of the ordinary at a first glance at the output of ```head```.\n\nThis conclusion is confirmed by looking futher on, at the output of ```info```, which shows us that:\n  * **Alley:** *only 91 values*, the rest are null; clearly this one will be of no use\n  * **PoolQC:** this one has *just 7 non-null values*! If there were any doubts in regards to the previous feature, there should be none towards this one's usefulness\n  * **Fence:** 281 values that we could use; still way lower than the most, which have 1460 values\n  * **MiscFeature:** 54 values; once again, we can safely eliminate this feature from our model\n  * **FireplaceQu:** this one has a lot more values, 770 to be exact; maybe it could be engineered in a way which would give it importance, but both due to its low amount of values, as well as a common sense intuition, we can safely assume it is useless\n  * **LotFrontage:** since it has 1201 values, for the time being we can keep it, in hopes that after the use of an imputer it will have meaning to the model\n  * **GarageType:** with 1379 values, it is feasible to keep it and use an imputer to fill in the blanks\n  * **GarageYrBlt:** with 1379 values, we should do the same to it\n  * **GarageFinish:** with 1379 values, again the treatment will be the same","34595fda":"Looking at the correlation coefficients for each feature, I thought maybe I could engineer something using the *unfinished basement surface\/percentage*. That seems to be a dead end, since anything I came up with had its Pearson's *r* less than 0.1.\n\nNext I decied to see if I could create a more comprehensive \"grade\" for a house, using ```OverallCond``` and ```OverallQual```:","73984b6d":"You could make more sense of the data by reading the ```data_description.txt file```, since I will not go in to greate detail regarding the individual meaning of each feature\/category.\n\nI eliminated some based on my common knowledge on houses\/buying a house, since I would not give them much importance (which was later confirmed when checking each feature's importance in the model).","ca7d8d5e":"As expected, *linear regression* is not a suitable algorithm for our problem.","e1d140cb":"A quick look at the scatter plots for the price:","3aecc9ec":"# Feature Selection\n\nIn order to see if more can be found out from our data, with hopes to help in feature selection, I will define the following function:","2e657db6":"*Decision trees* seem like a better alternative to the *linear regression* model, but we probably can do better with *random forests*. ","913ef708":"We can see that most of the features are skewed right, same as with the sale price, with some of the others skewed left, indicative of their positive, negative respectively correlation. \n\nWe can see some clear outliers now, with the scatter plot, as well as with the histograms, giving us a more complete view of how our data is distributed.\n\nI will not do this in the current model, but one could test different strategies to deal with the clear outliers in our data.\n\nThe ```GarageArea``` seems to have a slight upwards trend, but the data is almost entirely concentrated around one spot. Thus, I do not think it will provide great use for predicting the sale price.\n\nThe surfaces seem to have the same distribution and the same trend, with our engineered feature, ```TotalSF``` being less disperse. We might want to only keep that one in our model.","ef94f930":"Finally, it is time to read the data, run our quick analysis and plot a histogram for the label:","b76a8019":"```SKLearn```'s *random forest* algorithm seems to be a good choice so far, having the best score. Let's compare it with ```XGBoost```'s algorithm:","ab152fcf":"The clear winner is ```XGBoost```'s *random forest* model. We should also take into cosideration ```SKLearn```'s version of the algorithm as it was close to the former. I doesn't seem like *ElasticNet* has a low enough error to consider it when combining the algorithms.\n\nNow it is time to analize the feature importance of our best model:"}}