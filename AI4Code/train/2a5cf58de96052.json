{"cell_type":{"44d429dd":"code","36320ddc":"code","bb95e38d":"code","2893c97c":"code","9d45db73":"code","a4e29375":"code","54e424b0":"code","f53a49f8":"code","a7e46033":"code","f13d68c9":"code","4d9429a5":"code","bfa8f479":"code","7fd345a5":"markdown","58f942c4":"markdown","2648c394":"markdown","d4dd82e9":"markdown","f26b491a":"markdown"},"source":{"44d429dd":"import copy\nimport random\nimport time\n\nimport os.path\nimport json\nimport codecs\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import trange\nfrom tqdm import tqdm as tqdm\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon","36320ddc":"TRAIN_IMAGES_PATH = '..\/input\/coco2017\/train2017\/train2017'\nINASANCES_ANN_TRAIN_PATH = \"..\/input\/coco2017\/annotations_trainval2017\/annotations\/instances_train2017.json\"\n\nVALID_IMAGES_PATH = '..\/input\/coco2017\/val2017\/val2017'\nINASANCES_ANN_VALID_PATH = \"..\/input\/coco2017\/annotations_trainval2017\/annotations\/instances_val2017.json\"","bb95e38d":"with codecs.open(INASANCES_ANN_TRAIN_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n    train_ann = json.load(f)\n    \nwith codecs.open(INASANCES_ANN_VALID_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n    valid_ann = json.load(f)","2893c97c":"# Load and mergs annotation\nvalid_df_images = pd.DataFrame(valid_ann['images'], columns=['file_name', 'height', 'width', 'id'])\nvalid_df_images.rename(columns = {'id':'image_id'}, inplace=True)\n\nvalid_df_bbox = pd.DataFrame(valid_ann['annotations'], columns=['bbox', 'category_id', 'image_id', 'iscrowd'])\nvalid_df_bbox = valid_df_bbox[valid_df_bbox.iscrowd==0]\n\nvalid_df_categories = pd.DataFrame(valid_ann['categories'])\nvalid_df_categories.rename(columns = {'id':'category_id'}, inplace=True)\n\nvalid_df_images = valid_df_images.merge(valid_df_bbox, on='image_id', how='inner')\nvalid_df_images = valid_df_images.merge(valid_df_categories, on='category_id', how='inner')\nvalid_df_images.sort_values(by=['file_name'], inplace=True)\n\n# Make valid data map\nvalid_images_box_map = dict()\ncurrent_fname = None\n# for b in tqdm(valid_df_images[valid_df_images.name == 'clock'].values):\nfor b in tqdm(valid_df_images.values):\n    file_name, _, _, _, bbox, _, _,name, _ = b\n    bbox = copy.deepcopy(bbox)\n    bbox.append(name)\n    \n    if file_name != current_fname:\n        valid_images_box_map[file_name] = list()\n        current_fname = file_name\n    valid_images_box_map[file_name].append(bbox)\n# valid_images_box_map","9d45db73":"# Load and mergs annotation\ntrain_df_images = pd.DataFrame(train_ann['images'], columns=['file_name', 'height', 'width', 'id'])\ntrain_df_images.rename(columns = {'id':'image_id'}, inplace=True)\n\ntrain_df_bbox = pd.DataFrame(train_ann['annotations'], columns=['bbox', 'category_id', 'image_id', 'iscrowd'])\ntrain_df_bbox = train_df_bbox[train_df_bbox.iscrowd==0]\n\ntrain_df_categories = pd.DataFrame(train_ann['categories'])\ntrain_df_categories.rename(columns = {'id':'category_id'}, inplace=True)\n\ntrain_df_images = train_df_images.merge(train_df_bbox, on='image_id', how='inner')\ntrain_df_images = train_df_images.merge(train_df_categories, on='category_id', how='inner')\ntrain_df_images.sort_values(by=['file_name'], inplace=True)\n\n# Make valid data map\ntrain_images_box_map = dict()\ncurrent_fname = None\n# for b in tqdm(train_df_images[train_df_images.name == 'clock'].values):\nfor b in tqdm(train_df_images.values):\n    file_name, _, _, _, bbox, _, _,name, _ = b\n    bbox = copy.deepcopy(bbox)\n    bbox.append(name)\n    \n    if file_name != current_fname:\n        train_images_box_map[file_name] = list()\n        current_fname = file_name\n    train_images_box_map[file_name].append(bbox)\n# train_images_box_map","a4e29375":"def read_image(fpath, brg2rgb=True):\n    image = cv2.imread(fpath)\n    if brg2rgb:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n\ndef show_image_with_bboxes(image, bboxes):\n    plt.figure(figsize=(15,15))\n    plt.imshow(image)\n    plt.axis('off')\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for bbox in bboxes:\n        c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n        bbox_x, bbox_y, bbox_w, bbox_h, label = bbox\n        poly = [[bbox_x, bbox_y], [bbox_x, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y]]\n        np_poly = np.array(poly).reshape((4,2))\n        polygon = Polygon(np_poly)\n        polygons.append(polygon)\n        \n        text = plt.text(bbox_x, bbox_y, label, verticalalignment='bottom', fontweight='heavy', color=c)\n        color.append(c)\n\n    p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n    ax.add_collection(p)\n    p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)\n    ax.add_collection(p)\n\n    \ndef show_image_with_bboxes_from_bbox_map(fname, image_path, bbox_map):\n    image = read_image(os.path.join(image_path, fname))\n    bboxes = bbox_map[fname]\n    show_image_with_bboxes(image, bboxes)","54e424b0":"show_image_with_bboxes_from_bbox_map(random.choice([i for i in valid_images_box_map.keys()]), VALID_IMAGES_PATH, valid_images_box_map)","f53a49f8":"!git clone https:\/\/github.com\/ultralytics\/yolov5\n!pip install -r .\/yolov5\/requirements.txt\n\n%cd yolov5\n!weights\/download_weights.sh","a7e46033":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as D\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","f13d68c9":"def letterbox(img, new_shape=(416, 416), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n    # Resize image to a 32-pixel-multiple rectangle https:\/\/github.com\/ultralytics\/yolov3\/issues\/232\n    shape = img.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new \/ old)\n    r = min(new_shape[0] \/ shape[0], new_shape[1] \/ shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = new_shape\n        ratio = new_shape[0] \/ shape[1], new_shape[1] \/ shape[0]  # width, height ratios\n\n    dw \/= 2  # divide padding into 2 sides\n    dh \/= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return img, ratio, (dw, dh)\n\n\ndef scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = max(img1_shape) \/ max(img0_shape)  # gain  = old \/ new\n        pad = (img1_shape[1] - img0_shape[1] * gain) \/ 2, (img1_shape[0] - img0_shape[0] * gain) \/ 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] \/= gain\n    clip_coords(coords, img0_shape)\n    return coords\n\ndef non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, fast=False, classes=None, agnostic=False):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n\n    Returns:\n         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n    \"\"\"\n    if prediction.dtype is torch.float16:\n        prediction = prediction.float()  # to FP32\n\n    nc = prediction[0].shape[1] - 5  # number of classes\n    xc = prediction[..., 4] > conf_thres  # candidates\n\n    # Settings\n    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n    max_det = 300  # maximum number of detections per image\n    time_limit = 10.0  # seconds to quit after\n    redundant = True  # require redundant detections\n    fast |= conf_thres > 0.001  # fast mode\n    multi_label = nc > 1  # multiple labels per box (adds 0.5ms\/img)\n    if fast:\n        merge = False\n    else:\n        merge = True  # merge for best mAP (adds 0.5ms\/img)\n\n    t = time.time()\n    output = [None] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n        x = x[xc[xi]]  # confidence\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Compute conf\n        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n\n        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n        box = xywh2xyxy(x[:, :4])\n\n        # Detections matrix nx6 (xyxy, conf, cls)\n        if multi_label:\n            i, j = (x[:, 5:] > conf_thres).nonzero().t()\n            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n\n        # Filter by class\n        if classes:\n            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n\n        # Apply finite constraint\n        # if not torch.isfinite(x).all():\n        #     x = x[torch.isfinite(x).all(1)]\n\n        # If none remain process next image\n        n = x.shape[0]  # number of boxes\n        if not n:\n            continue\n\n        # Sort by confidence\n        # x = x[x[:, 4].argsort(descending=True)]\n\n        # Batched NMS\n        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n        if i.shape[0] > max_det:  # limit detections\n            i = i[:max_det]\n        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n            try:  # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n                weights = iou * scores[None]  # box weights\n                x[i, :4] = torch.mm(weights, x[:, :4]).float() \/ weights.sum(1, keepdim=True)  # merged boxes\n                if redundant:\n                    i = i[iou.sum(1) > 1]  # require redundancy\n            except:  # possible CUDA error https:\/\/github.com\/ultralytics\/yolov3\/issues\/1139\n                print(x, i, x.shape, i.shape)\n                pass\n\n        output[xi] = x[i]\n        if (time.time() - t) > time_limit:\n            break  # time limit exceeded\n\n    return output\n\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n    y[:, 0] = x[:, 0] - x[:, 2] \/ 2  # top left x\n    y[:, 1] = x[:, 1] - x[:, 3] \/ 2  # top left y\n    y[:, 2] = x[:, 0] + x[:, 2] \/ 2  # bottom right x\n    y[:, 3] = x[:, 1] + x[:, 3] \/ 2  # bottom right y\n    return y\n\n\ndef clip_coords(boxes, img_shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","4d9429a5":"def eval_yolo5(model,\n               image_iterator, \n               imgsz=640,\n               conf_thres=0.4,\n               iou_thres=0.5,\n               classes=None,\n               class_names=None,\n               colors=None,\n               agnostic_nms=False,\n               img_with_boxes=True):\n\n    def process_image(img0):\n        img, _, _ = letterbox(img0, new_shape=640)\n        img = img[:, :, ::-1].transpose(2, 0, 1)\n        img = np.ascontiguousarray(img)\n\n        img = torch.from_numpy(img).to(device)\n        img = img.float()  # uint8 to fp16\/32\n        img \/= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        pred = model(img, augment=False)[0]\n\n        # Apply NMS\n        pred = non_max_suppression(pred, conf_thres, iou_thres, fast=True, classes=classes, agnostic=agnostic_nms)\n        \n        # Process detections\n        det_list = list()\n        for det in pred:  # detections per image\n            s = ''\n            img_with_bboxes = img0.copy() # It is numpy array\n            \n            gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n            if det is not None and len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n\n                # Write results\n                for *xyxy, conf, cls in det:\n                    if class_names:\n                        l = '%s %.2f' % (class_names[int(cls)], conf)\n                    else:\n                        l = 'cls=%s, conf=%.2f' % (str(int(cls)), conf)\n                    \n                    if colors:\n                        c = colors[int(cls)]\n                    else:\n                        c = (int(cls), int(cls), int(cls))\n                        \n                    plot_one_box(xyxy, img_with_bboxes, label=l, color=c, line_thickness=3)\n                \n                det_list.append(det)\n\n        if img_with_boxes:\n            return det_list, img_with_bboxes\n        return det_list\n    \n    return (process_image(i) for i in image_iterator)","bfa8f479":"# Configure device\nCUDA = \"cuda:0\"\nCPU = \"cpu\"\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(CUDA if use_cuda else CPU)\n\n# Load model\nweights='weights\/yolov5s.pt'\n\nmodel = torch.load(weights, map_location=device)['model'].float()  # load to FP32\nmodel.to(device).eval()\n\n# Set names and colors\nnames = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n         'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n         'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n         'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n         'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n         'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n         'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n         'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n         'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\ncolors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n\n\n# Set image iterator\n_VALID_IMAGES_PATH = '..\/..\/input\/coco2017\/val2017\/val2017'\n\ncount = 6\nvalidset_image_fnames = [i for i in valid_images_box_map.keys()]\nvalidset_image_sample = random.sample(validset_image_fnames, count)\nvalidset_image_sample_paths = [os.path.join(_VALID_IMAGES_PATH, p) for p in validset_image_sample]\nimage_iter = (read_image(p, brg2rgb=False) for p in validset_image_sample_paths)\n\n# Setup inference function\nyolo5_inf = eval_yolo5(model,\n                       image_iter, \n                       imgsz=640,\n                       conf_thres=0.4,\n                       iou_thres=0.5,\n                       classes=None,\n                       class_names=names,\n                       colors=colors,\n                       agnostic_nms=False,\n                       img_with_boxes=True)\n\n\n# Run\nfor det, img in yolo5_inf:\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()","7fd345a5":"## Eval sample COCO data","58f942c4":"## Eval function","2648c394":"## Util functions\n\nBased on yolov5.utils","d4dd82e9":"# Init COCO BBox Data","f26b491a":"# Init YOLOV5"}}