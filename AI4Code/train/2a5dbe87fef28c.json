{"cell_type":{"024a0bd2":"code","86a2fa5c":"code","5c953b65":"code","e8370fa9":"code","657ccb77":"code","68d3e878":"code","468f5cf1":"code","b5471a16":"code","b5181484":"code","cb8abb2a":"code","5b511673":"code","a382ab08":"code","af31cf53":"code","5f2965b1":"code","e887fcf8":"code","63d8cec8":"code","d3f763e2":"code","8751c0ad":"code","4d1e24d3":"code","6aa80854":"code","c676605e":"code","ba0c6f43":"code","28d4dd39":"code","d4c4e110":"code","30931442":"code","22bccb95":"code","d00a3e32":"code","62b5cdcd":"code","dc071b37":"code","45f9b265":"code","8b13c3a8":"code","417d8787":"code","554e24ac":"code","da6c518c":"code","8657f05a":"code","c61895db":"code","2834330f":"code","da1fa4d7":"code","931d56e3":"code","9df1eab0":"code","e6037dce":"code","7acf953c":"code","16ee1b9f":"code","4f22f754":"code","c1ed5196":"code","8e653f78":"code","7b93da11":"code","f89773dd":"code","d36b9827":"code","628fcdd0":"code","f58f2de7":"code","130ab8b8":"code","cfaa50a1":"code","1d754c6d":"code","a7ae2926":"markdown","3dc000de":"markdown","aa7e71df":"markdown","00803f89":"markdown","58631bf0":"markdown","766df4ed":"markdown","6d17a27a":"markdown","1a306c82":"markdown","5f59c2e8":"markdown","aa9fef52":"markdown","266bdae0":"markdown","54a2e30c":"markdown","ec5b1012":"markdown","d7a15bd0":"markdown","a3150ae8":"markdown","c6278618":"markdown","5b68ad37":"markdown","ad10f359":"markdown","9d78fe0a":"markdown","0a0099bc":"markdown","5f30f48e":"markdown","f42cff6c":"markdown","e0443aa7":"markdown","a710e432":"markdown","17e8d491":"markdown","0d8ffbd0":"markdown","a93f8809":"markdown","637b91e2":"markdown","3496c406":"markdown","955a9590":"markdown","45858ade":"markdown","6cacd454":"markdown","a459f9f4":"markdown","5df67d9a":"markdown","3f95a50a":"markdown","9877abce":"markdown","a37590f3":"markdown","b77d97b9":"markdown","e7a20342":"markdown","87f7365a":"markdown","059ff469":"markdown","a6856c0c":"markdown","f0d08e1d":"markdown","65e46951":"markdown"},"source":{"024a0bd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86a2fa5c":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom scipy.stats import randint as sp_randint\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import cross_val_score,KFold,RandomizedSearchCV,cross_val_predict\nimport pandas as pd\nimport random\nfrom xgboost import XGBClassifier\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5c953b65":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","e8370fa9":"test_df.head()","657ccb77":"train_df.columns","68d3e878":"train_df.head(10)","468f5cf1":"print(train_df.info())","b5471a16":"test_df.info()","b5181484":"train_df.describe().T","cb8abb2a":"train_df.describe(include=['O']).T","5b511673":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a382ab08":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","af31cf53":"train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5f2965b1":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e887fcf8":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","63d8cec8":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)","d3f763e2":"grid = sns.FacetGrid(train_df, row='Embarked', height=2.2, aspect=2)\ngrid.map(sns.pointplot, 'Pclass', 'Survived','Sex', palette='deep',order = [1,2,3],hue_order = ['male','female'])","8751c0ad":"# Remove ticket and Cabin column\ntrain_df = train_df.drop(['Ticket','PassengerId'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","4d1e24d3":"for data in [train_df,test_df]:\n    mean = data[\"Age\"].mean()\n    std = data[\"Age\"].std()\n    is_null = data[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = data[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    data[\"Age\"] = age_slice\n    data[\"Age\"] = data[\"Age\"].astype(int)","6aa80854":"for data in [train_df,test_df]:\n    data['Fare'].fillna((data['Fare'].median()), inplace=True)","c676605e":"for data in [train_df, test_df]:\n    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])","ba0c6f43":"def covt_to_categorical(cat_val,df,col_name):\n    df[col_name] = df[col_name].map(cat_val).astype(int)\n    df[col_name] = df[col_name].fillna(0).astype(int)    ","28d4dd39":"covt_to_categorical({'S': 0, 'C': 1, 'Q': 2},train_df,\"Embarked\")\ncovt_to_categorical({'S': 0, 'C': 1, 'Q': 2},test_df,\"Embarked\")","d4c4e110":"covt_to_categorical({'female': 1, 'male': 0},train_df,\"Sex\")\ncovt_to_categorical({'female': 1, 'male': 0},test_df,\"Sex\")","30931442":"train_df['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train_df['Title'], train_df['Sex'])","22bccb95":"for data in [train_df,test_df]:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d00a3e32":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\ncovt_to_categorical(title_mapping,train_df,\"Title\")\ncovt_to_categorical(title_mapping,test_df,\"Title\")","62b5cdcd":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","dc071b37":"for data in [train_df,test_df]:\n    data['Family_Count'] = data['SibSp'] + data['Parch'] + 1\n    data.loc[data['Family_Count'] > 1, 'Is_alone'] = 0\n    data.loc[data['Family_Count'] == 1, 'Is_alone'] = 1\n    data[\"Is_alone\"] = data[\"Is_alone\"].astype(\"int\")","45f9b265":"sns.factorplot('Family_Count','Survived',data=train_df, aspect = 2.5 )","8b13c3a8":"train_df['Fare'].isnull().sum()","417d8787":"for data in [train_df, test_df]:\n    data['Age_Class'] = data['Age']* data['Pclass']\n    data['Fare_Per_Person'] = data['Fare']\/(data['Family_Count']).astype(int)    ","554e24ac":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","da6c518c":"for data in [train_df, test_df]:    \n    data.loc[ data['Age'] <= 16, 'Age'] = 0\n    data.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\n    data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n    data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n    data.loc[ data['Age'] > 64, 'Age'] = 5","8657f05a":"train_df.drop([\"AgeBand\",'Parch', 'SibSp'], axis=1,inplace=True)\ntest_df.drop(['Parch', 'SibSp'], axis=1,inplace=True)","c61895db":"# Convert the Fare feature to ordinal value\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","2834330f":"for data in [train_df, test_df]:\n    data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\n    data.loc[ data['Fare'] > 31, 'Fare'] = 3\n\ntrain_df.drop(['FareBand'], axis=1,inplace=True)","da1fa4d7":"train_df['Cabin'].value_counts()","931d56e3":"train_df['Deck'] = train_df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'U')\ntest_df['Deck'] = test_df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'U')\ntrain_df[['Deck', 'Survived']].groupby(['Deck'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9df1eab0":"train_df.Deck.value_counts()","e6037dce":"sns.barplot(x = 'Deck',\n            y = 'Survived',\n            hue='Pclass',\n            data = train_df)","7acf953c":"for data in [train_df, test_df]:        \n    data['Deck'] = LabelEncoder().fit_transform(data['Deck'])","16ee1b9f":"train_df.drop(['Cabin'], axis=1,inplace=True)\ntest_df.drop(['Cabin'], axis=1,inplace=True)","4f22f754":"test_df.head()","c1ed5196":"ss_scaler = StandardScaler()\ntrain_df_ss = pd.DataFrame(data = train_df)\ntest_df_ss = pd.DataFrame(data = test_df)\ntrain_df_ss[['Age_Class','Fare_Per_Person']] = ss_scaler.fit_transform(train_df_ss[['Age_Class','Fare_Per_Person']])\ntest_df_ss[['Age_Class','Fare_Per_Person']] = ss_scaler.fit_transform(test_df_ss[['Age_Class','Fare_Per_Person']])","8e653f78":"test_df_ss.isnull().sum()","7b93da11":"X_train = train_df_ss.drop(\"Survived\", axis=1)\nY_train = train_df_ss[\"Survived\"]\nX_test  = test_df_ss.drop(\"PassengerId\", axis=1).copy()","f89773dd":"def build_model(train,test):\n    models=[]\n    models.append(('LogisticRregression',LogisticRegression()))\n    models.append(('KNearestNeighbors',KNeighborsClassifier(n_neighbors = 3,n_jobs=-1)))\n    models.append(('SupportVectorClassifier-Linear',SVC(kernel='linear')))\n    models.append(('DecisionTree',DecisionTreeClassifier()))\n    models.append(('RandomForestClassifier',RandomForestClassifier()))\n    models.append(('GradientBoostClassifier',GradientBoostingClassifier()))\n    models.append(('AdaBoostClassifier',AdaBoostClassifier(base_estimator=DecisionTreeClassifier())))\n    models.append(('ExtraTreesClassifier',ExtraTreesClassifier()))\n    models.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier())))\n    models.append(('XgBoostClassifier',XGBClassifier(use_label_encoder=False)))\n    models.append(('MLPClassifier',MLPClassifier()))\n    models.append(('LGBMClassifier',LGBMClassifier()))\n\n    model_tran_result,model_val_result,model_name=[],[],[]\n    scoring='accuracy'\n    print(\"Statistical Classification Model Baseline Evaluation\")\n    for name,model in models:\n        kfold=KFold(n_splits=10)\n        results=cross_val_score(model,train,test,cv=kfold)\n        print(\"*\"* 80)\n        print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n        print(\"*\"* 80)\n        predictions=cross_val_predict(model,train,test)\n        accuracy = accuracy_score(predictions,test)\n        model_tran_result.append(results.mean())\n        model_val_result.append(accuracy)\n        model_name.append(name)\n        \n    result_df = pd.DataFrame(\n            {\n             'Model': model_name,\n             'Training Acc': model_tran_result,\n             'Validation Acc': model_val_result\n            }\n        )\n    display(result_df)","d36b9827":"build_model(X_train,Y_train)","628fcdd0":"def randomised_hyperparam_tunning(class_model,param_grid,n_iter_p,train,test):\n    \n    ran_model = RandomizedSearchCV(estimator = class_model, param_distributions = param_grid, n_iter = n_iter_p, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    ran_model.fit(train,test.values.ravel())\n    print(ran_model.best_params_)\n    \n    return ran_model.best_estimator_","f58f2de7":"random_forest_grid = {'n_estimators': sp_randint(50, 500),\n                      'criterion':[\"gini\", \"entropy\"],\n                       'max_features': ['auto', 'sqrt'],\n                       'max_depth': sp_randint(1, 11),\n                       'min_samples_split': sp_randint(2, 11),\n                       'min_samples_leaf':  sp_randint(1, 11),\n                       'max_leaf_nodes':sp_randint(1, 10),\n                       'bootstrap': [True, False],\n                        'oob_score':[True, False]\n                     }\nrft_best = randomised_hyperparam_tunning(RandomForestClassifier(),random_forest_grid,50,X_train,Y_train)\nresult = rft_best.predict(X_test)\ntest_df_ss[\"Survived\"] = result\nresult = test_df_ss[[\"PassengerId\", \"Survived\"]]\nresult.to_csv('titanic_RFClassifier.csv', index=False)","130ab8b8":"param_dist = {\n              \"loss\": ['deviance', 'exponential'],\n              \"learning_rate\":np.around(np.linspace(0.001, 0.5, 40), decimals = 4),\n              \"n_estimators\":sp_randint(25, 400),\n              \"criterion\":['friedman_mse', 'mse', 'mae'],\n              \"max_depth\": sp_randint(1, 11),\n              \"max_features\":['auto', 'sqrt', 'log2'],\n              \"min_samples_split\": sp_randint(2, 11),\n              \"subsample\":np.around(np.linspace(0.5, 1.0, 20), decimals = 2),\n              \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n              \"max_features\":['sqrt','log2','None']\n             }\ngbm_best = randomised_hyperparam_tunning(GradientBoostingClassifier(),param_dist,50,X_train,Y_train)\nresult = gbm_best.predict(X_test)\ntest_df_ss[\"Survived\"] = result\nresult = test_df_ss[[\"PassengerId\", \"Survived\"]]\nresult.to_csv('titanic_GBClassifier.csv', index=False)","cfaa50a1":"LgbmgridParams = {\n    'learning_rate': np.around(np.linspace(0.001, 0.5, 40), decimals = 4),\n    'n_estimators': sp_randint(50, 400),\n    'num_leaves': sp_randint(1, 21),\n    'max_depth': sp_randint(1, 21),\n    'boosting_type' : ['gbdt','dart'],\n    'objective' : ['binary'],\n    'colsample_bytree' : np.around(np.linspace(0.5, 1.0, 20), decimals = 2),\n    'subsample' : np.around(np.linspace(0.5, 1.0, 20), decimals = 2),\n    'reg_alpha' : np.around(np.linspace(1,1.5,20), decimals = 2),\n    'reg_lambda' : np.around(np.linspace(1,1.5,20), decimals = 2),\n    }\n\nLgbm_best = randomised_hyperparam_tunning(LGBMClassifier(),LgbmgridParams,50,X_train,Y_train)\nresult = Lgbm_best.predict(X_test)\ntest_df_ss[\"Survived\"] = result\nresult = test_df_ss[[\"PassengerId\", \"Survived\"]]\nresult.to_csv('titanic_Lgbmlassifier.csv', index=False)","1d754c6d":"XgBoostgridParams={\n        'learning_rate': np.around(np.linspace(0.001, 0.5, 40), decimals = 4),\n        'max_depth': sp_randint(1, 21),\n        'booster ' : ['gbdt','dart'],\n        'tree_method':['auto', 'exact', 'approx', 'hist', 'gpu_hist'],\n        'gamma': sp_randint(1, 11),\n        'max_delta_step':sp_randint(1, 11),\n        'reg_alpha' : np.around(np.linspace(1,1.5,20), decimals = 2),\n        'reg_lambda' :np.around(np.linspace(1,1.5,20), decimals = 2),\n        'colsample_bytree' : np.around(np.linspace(0.5, 1.0, 20), decimals = 2),\n        'min_child_weight' : sp_randint(1, 11),\n        'subsample': np.around(np.linspace(0.5, 1.0, 20), decimals = 2),\n        'n_estimators': sp_randint(50, 400),\n    }\nXgBoost_best = randomised_hyperparam_tunning(XGBClassifier(),XgBoostgridParams,10,X_train,Y_train)\nresult = XgBoost_best.predict(X_test)\ntest_df_ss[\"Survived\"] = result\nresult = test_df_ss[[\"PassengerId\", \"Survived\"]]\nresult.to_csv('titanic_XgBoostClassifier.csv', index=False)","a7ae2926":"* Let's create deck type co,lumn by extracting the first letter Cabin(string s) column U stands for Unknown or missing data\n- It clearly show that people staying in some deck type have higher chances of survival","3dc000de":"1. Age column let's impute the missing value","aa7e71df":"We can create seprate category for few tiltes as other.","00803f89":"# Categorical Variable distribution\n- Names are unique across the dataset\n- Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n- Cabin values have several dupicates across samples which means several passengers shared a cabin.\n- Embarked takes three possible values. S port used by most passengers (top=S)\n- Ticket feature has high ratio (22%) of duplicate values (unique=681).","58631bf0":"* Female has highest survival rate compared to male","766df4ed":"# LightGBM\nLight GBM is another gradient boosting strategy which relies on trees.It has the following advantages:\n\n* Faster training speed and higher efficiency.\n\n* Lower memory usage.\n\n* Better accuracy.\n\n* Support of parallel and GPU learning.\n\n* Capable of handling large-scale data.\n\n* LightGBM grows leaf-best wise and will choose the leaf with maximum max delta loss to grow.\n![image.png](attachment:image.png)","6d17a27a":"# Feature enginnering:\n- Feature engineering is the process of using domain knowledge to extract features from raw data via data mining technique\n- These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.\n- Here we will create some new feature from existing feature by extracting data and creating new pattern","1a306c82":"# Cabin Column:\n- Cabin feature is bit tricky as it contains lot of missing cabin values. Still the feature itself can't be ignored completely because few of cabins type can have higher survival rates. It turns out to be the first letter of the Cabin values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.\n* On the Boat Deck there were 6 rooms labeled as T, U, W, X, Y, Z but only the T cabin is present in the dataset\n* A, B and C decks were only for 1st class passengers\n* D and E decks were for all classes\n* F and G decks were for both 2nd and 3rd class passengers","5f59c2e8":"# References\nThis notebook has been created based some previous Titanic competition solutions and other sources.\n* [titanic-data-science-solutions](http:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [nlp-end-to-end-cll](http:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-2#DistilBERT-Mode)","aa9fef52":"Artificial feature Age * Class","266bdae0":"# Data Preparation","54a2e30c":"![image.png](attachment:image.png)","ec5b1012":"# Missing Value Imputation\n ","d7a15bd0":"# MultiNomial Naive Bayes\nMultiNomial NB is a probabilistic statistical classification model which uses conditional probability to segregate or classify samples. This works well with discrete integer valued features (such as count vectorization) but can also be used with TFIDF vectors. Particularly, this uses the Bayes Theorem which tries to determine conditional probability using prior and posterior probabilities as shown in the figure:\n![image.png](attachment:image.png)","a3150ae8":"# Workflow Overview\nThis notebook invloves below steps while solving the problem:\n\n* Question or problem definition.\n* Acquire training and testing data.\n* Wrangle, prepare, clean the data.\n* Analyze, identify patterns, and explore the data.\n* Model, predict and solve the problem.\n* Submit the result\n\n# The dataset has the following features:\n1. PassengerId: Unique Id of a passenger\n1. survival: Whether a passenger survived or not; 1 if survived and 0 if not.\n1. pclass: Ticket class\n1. sex: Sex\n1. Age: Age in years\n1. sibsp: # of siblings \/ spouses aboard the Titanic\n1. parch: # of parents \/ children aboard the Titanic\n1. ticket: Ticket number\n1. fare: Passenger fare\n1. cabin: Cabin number\n1. embarked: Port of Embarkation","c6278618":"![image.png](attachment:image.png)","5b68ad37":"# Conclusion based on above data analysis:\n* We should impute the Age and Embarked as these variable may be related to the other data variable and target variable Survive\n* PassengerId, Name shoule dropped from training dataset as it does not contribute anything in prediction and all the values are unique so may not contain any hidden pattern\n* In earlier version I dropped Cabin feature and build the model without cabin feature. Let's try to generate new feature using cabin column and see if accuray is increased","ad10f359":"*  Few of the SibSp and Parch category has no relation with Survived","9d78fe0a":"* After running all the classification model,It is observed that GradientBoostClassifier gives highest training and validation accuracy among all other model. Let's perform hyperparametr tunning on Gradient boosting classifier to increase the accuracy of model.","0a0099bc":"# Decision Trees\nDecision Trees is a supervised model for classification\/regression. This works on creating decision branches which evolves a criteria and is often acknowledged as a simplistic classification (white box) model as the stages of decision can be easily derived. A regression tree appears as follows:\n![image.png](attachment:image.png)\nThe algorithms include ID3,C4.5\/C5.0,CART which can be analysed as follows:\n\n* ID3(Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n\n* C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule\u2019s precondition if the accuracy of the rule improves without it.\n\n* C5.0 is Quinlan\u2019s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n\n* CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.","5f30f48e":"# Logistic Regression:\n Logistic Regression Classifier uses a sigmoid kernel for training. In a supervised learning mode , Logistic Regression is one of the standardised models under generalized linear models which tries a convex optimization by passing the cost function through the sigmoid kernel. The sigmoid function is denoted by the formulation:\n ![image.png](attachment:image.png)\n This equation due to its convergence property (+\/- infinity) and due to its differentiability , the sigmoid kernel allows clamping of predicted values to binary labels. The sigmoid curve actually has optima at x=0 and x=1.Now in the case of supervised logistic regression, when we try to optimize the cost function (in this case a linear sum of weights & biases passed through sigmoid kernel), by applying stochastic gradient descent. Since by gradient descent, the steepest slope is considered, the change in derivatives (gradients) at each stage is computed and the weights of the cost function are updated. The effective loss function for logistic regression is E=(|y_predicted -y_actual|^2). This blog provides an idea.","f42cff6c":"Takes lot of time in Hyper parameter tunning hence running with less number of fits","e0443aa7":"# Bi-variate Analysis:\n","a710e432":"* Embarked column missing value imputation\n- As it is an catgorical variable let's replace most commonly occurred term for missing value","17e8d491":"* Convert the new column to categorical","0d8ffbd0":"- A histogram chart is very useful for analyzing numerical variables where banding or ranges will help identify useful hidden patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands.\n\nObservations.\n\n- Infants had high survival rate.\n- Oldest passengers survived.\n- Large number of 15-35 year olds did not survive.\n\nFrom the below analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values","a93f8809":"Let's drop the Name column as we have derived new feature from Name","637b91e2":"*  There is a significant correlation between Pclass = 1 and Survived.","3496c406":"Several Boosting Models can be found under this criteria:\n\n# AdaBoosting:\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Initially, those weights are all set to (1\/N), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\n\n![image.png](attachment:image.png)","955a9590":"## LGBM Hyper parameter tunning","45858ade":"# Variable Summary\n- After looking into the data we can clearly says that Survived, Sex, and Embarked are categorcal variable and Pclass is ordinal variable\n- Age and Fare are continuous numerical variable\n- SibSp and Parch are discrete numerical variable\n- Name,Ticket and Cabin looks to be alphanumeric character variables.These variable may not play any important role in the model building","6cacd454":"# Random Forests\n- Random Forests is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. When splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details).The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n![image.png](attachment:image.png)\n\n","a459f9f4":"The below plot tells that if person is alone and does not how to swim has low chances of survival.\nThe person with family size of 1-3 has higher probability of survival.","5df67d9a":"Let us replace Age with ordinals based on these bands.","3f95a50a":"- Female passengers had higher survival rate than males.\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing","9877abce":"# Visualize the data","a37590f3":"From the below graph we can conclude that:\n* 100% of A, B and C decks are 1st class passengers\n* Deck D has mostly 1st and 2nd class passengers\n* Deck E has high % 3rd class passengers survived\n* Deck F has 1st and 2nd class passengers which are survived\n* G deck has only 3rd class passengers with 50% survived\n* Passengers labeled as U are the missing values in Cabin feature. ","b77d97b9":"# Conclusion:\nMy submission to the competition site Kaggle results in scoring 1258 out of 19813 competition entries. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\n- Even after adding Deck type new feature the accuracy still hovering aroun 78-79 need to try different approach.","e7a20342":"# Gradient Boosting Forests and Trees\nGradient Boosting is a central part of ensemble modelling in sklearn.\n\nThe goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\nIn averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\nExamples: Bagging methods, Forests of randomized trees\n\nBy contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\nExamples: AdaBoost, Gradient Tree Boosting","87f7365a":"# Scaling the numerical data","059ff469":"## Null value check\n- Cabin, Age, Embarked features contains null values in the training dataset.\n- Cabin, Age and Embarked are incomplete in case of test dataset.","a6856c0c":"# XGBoost Hyper parameter tunning","f0d08e1d":"- SibSp and Parch together will give form complete family member list. Let's combine both of them together and create one more feature which will shows if person is alone or not","65e46951":"# 5 point summary\n\nThis helps us determine with early insights, how the numerical value are distributed looking at the summary. If the variable are following bell curve\n\n- Total samples size is 891 \n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers were adults very few child anf elders among samples\n- Fares varied significantly with few passengers paying as high as $512"}}