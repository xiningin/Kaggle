{"cell_type":{"4737fa45":"code","fb1dbe94":"code","ca878442":"code","d28d76ea":"code","38010e11":"code","8a7c757f":"code","32817513":"code","a316cac3":"code","7a322b34":"code","1bf909a2":"code","3af514a9":"code","0edc1c6c":"code","e46ac01c":"code","7554ffa4":"code","d4cef1a4":"code","4f3b1b84":"code","81bfecb1":"code","6013556e":"code","147d84b1":"code","a31271a9":"code","3b3a1a26":"code","5307ff83":"code","ad49224c":"code","607055c1":"code","6b3728d7":"markdown","794db538":"markdown","de204843":"markdown","fa99d9f2":"markdown","b8708240":"markdown","4fe4587f":"markdown","636abe5d":"markdown","a79f8dd3":"markdown","06395d3d":"markdown","cb5f5601":"markdown","b4217675":"markdown","8725fa88":"markdown","416c96d5":"markdown","ecc32347":"markdown","a6ab57a7":"markdown","1cf1ebb0":"markdown","927c26a0":"markdown","60378cbb":"markdown","75379954":"markdown","5eb5ef2b":"markdown","78739d89":"markdown"},"source":{"4737fa45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.special import boxcox1p\n\nimport warnings\nfrom IPython.display import Image\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\n\n# Modeling packages\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","fb1dbe94":"df_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n","ca878442":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d28d76ea":"# Lets see the shape of the data prior to processing\nprint ('Shape of Train set: {}'.format(df_train.shape))\nprint ('Shape of Test set: {}'.format(df_test.shape))\n\n# Id column does not affect the prediction outcome hence can be dropped. However the Id serieses are saved for future modeling\ntest_id = df_test['Id']\ntrain_id = df_train['Id']\n\n# Dropping Id column\ndf_test.drop(\"Id\", axis = 1, inplace = True)\ndf_train.drop(\"Id\", axis = 1, inplace = True)\n\n# Lets see the shape of the data prior to processing\nprint ('\\nShape of Train set after removing Id: {}'.format(df_train.shape))\nprint ('Shape of Test set after removing Id: {}'.format(df_test.shape))\n\n","38010e11":"original = df_train['SalePrice']\ndf_train['SalePrice'] = stats.boxcox(df_train['SalePrice'])[0]\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\n(mu_norm, sigma_norm) = norm.fit(df_train['SalePrice'])\n\n# Get Skewness and Kurtosis \noriginal_skew = original.skew()\noriginal_kurt = original.kurt()\n\ntransf_skew = df_train['SalePrice'].skew()\ntransf_kurt = df_train['SalePrice'].kurt()\n\nplt.figure(figsize=(18,8))\n\n#Now plot the distribution\nplt.subplot(221)\nsns.distplot(original , fit=norm)\nplt.legend(['Normal dist. \\n$\\mu=$ {:.2f} \\n $\\sigma=$ {:.2f} \\n skewness= {:.2f} \\n kurtosis= {:.2f}'.format(mu, sigma,original_skew,original_kurt)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice density distribution')\n\n#Get also the QQ-plot\nplt.subplot(222)\nres = stats.probplot(original, sparams=(2.5,), plot=plt, rvalue=True)\nplt.title('SalePrice Probability Plot')\n\n\n#Now plot Normalized distribution\nplt.subplot(223)\nsns.distplot(df_train['SalePrice'] , fit=norm)\nplt.legend(['Normal dist. \\n$\\mu=$ {:.2f} \\n $\\sigma=$ {:.2f} \\n skewness= {:.2f} \\n kurtosis= {:.2f}'.format(mu_norm, sigma_norm,transf_skew,transf_kurt)],loc='best')\n\nplt.ylabel('Frequency')\nplt.title('Normalized SalePrice density distribution')\n\n#Get also the QQ-plot for Normalized distribution\nplt.subplot(224)\nres = stats.probplot(df_train['SalePrice'], sparams=(2.5,), plot=plt, rvalue=True)\nplt.title('Normalized SalePrice Probability Plot')\nplt.tight_layout()\n","8a7c757f":"plt.figure(figsize=(16,8))\nplt.subplot(221)\nsns.scatterplot(x=df_train['GrLivArea'],y=df_train['SalePrice'])\nplt.title('Original Data:\\nGrLivArea vs SalePrice')\n\n# Removing two outliers with larger living area and low sale price\ndf_train = df_train.drop(df_train.sort_values(by = 'GrLivArea', ascending = False)[:2].index)\n\nplt.subplot(222)\nsns.scatterplot(x=df_train['GrLivArea'],y=df_train['SalePrice'])\nplt.title('Outliers removed:\\nGrLivArea vs SalePrice')\n\nplt.tight_layout()\n\n","32817513":"# Combining Train and Test data sets\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\n\nntrain = df_train.shape[0] # Will use this to generate a new train data set \nntest = df_test.shape[0] # Will use this to generate a new test data set\ny_train = df_train['SalePrice'].values\ny_test = df_train['SalePrice'].values ","a316cac3":"def report_missing_data(df):\n    '''\n    IN: Dataframe \n    OUT: Dataframe with reported count of missing values, % missing per column and per total data\n    '''\n    \n    missing_count_per_column = df.isnull().sum()\n    missing_count_per_column = missing_count_per_column[missing_count_per_column>0]\n    total_count_per_column = df.isnull().count()\n    total_cells = np.product(df.shape)\n    \n    # Percent calculation\n    percent_per_columnn = 100*missing_count_per_column\/total_count_per_column\n    percent_of_total = 100*missing_count_per_column\/total_cells\n    \n    # Creating new dataframe for reporting purposes only\n    missing_data = pd.concat([missing_count_per_column,\n                              percent_per_columnn,\n                              percent_of_total], axis=1, keys=['Total_Missing', 'Percent_per_column','Percent_of_total'])\n    \n    \n    missing_data = missing_data.dropna()\n    missing_data.index.names = ['Feature']\n    missing_data.reset_index(inplace=True)\n\n    \n    \n    return missing_data.sort_values(by ='Total_Missing',ascending=False)\n\ndf = report_missing_data(all_data)\n","7a322b34":"plt.figure(figsize=(18,15))\nplt.subplot(221)\nsns.barplot(y='Feature',x='Total_Missing',data=df)\nplt.title('Missing Data')","1bf909a2":"Image(\"..\/input\/housingpics\/missingdata1.png\",height=800 , width=600)","3af514a9":"# Change Pic Above ( GarageCars 2x)\n\nCat_toNone = ('PoolQC','Fence','MiscFeature','Alley','FireplaceQu','GarageType','GarageFinish',\n              'GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n              'BsmtFinType2','MasVnrType','MSSubClass')\n    \nCat_toMode = ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd',\n              'Utilities','SaleType','Functional')\n\nNum_toZero = ('GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n              'BsmtFullBath','BsmtHalfBath','GarageYrBlt','MasVnrArea')\n\nfor col in Cat_toNone:\n    all_data[col]=all_data[col].fillna('None')\n\nfor col in Cat_toMode:\n    all_data[col]=all_data[col].fillna(all_data[col].mode()[0])\n    \nfor col in Num_toZero:\n    all_data[col]=all_data[col].fillna(0)\n    \n","0edc1c6c":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nreport_missing_data(all_data)","e46ac01c":"all_data._get_numeric_data()\nall_data['Total_SF'] = all_data['1stFlrSF']+all_data['2ndFlrSF']+all_data['TotalBsmtSF']+all_data['GarageArea']+all_data['GrLivArea']+all_data['PoolArea']+all_data['WoodDeckSF']\n            ","7554ffa4":"def bcox_transform(df):\n    '''\n    IN: Original dataframe \n    OUT: Dataframe with box-cox normalized numerical values. \n    Specified skewness threshold 1 and -1 \n    '''\n    lam = 0.15\n    for feat in df._get_numeric_data():\n        if df[feat].skew()>1.0 or df[feat].skew()<-1.0:\n            df[feat]=boxcox1p(df[feat], lam)\n    return df\n\nall_data = bcox_transform(all_data)","d4cef1a4":"#getting dummie variable\nall_data = pd.get_dummies(all_data)","4f3b1b84":"# splitting data back to train and test data sets\ndf_train = all_data[:ntrain]\ndf_test = all_data[ntrain:]","81bfecb1":"X_train = all_data[:df_train.shape[0]]\nX_test = all_data[df_train.shape[0]:]\ny = y_train","6013556e":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\nn_folds = 15\n\n# K-fold Root Mean Square Error Cross Validation\ndef k_rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","147d84b1":"Image(\"..\/input\/housingpics\/bias-variance.png\",height=1400 , width=800)","a31271a9":"Image(\"..\/input\/housingpics\/regequation.png\",height=1600 , width=1000)","3b3a1a26":"# Definning models\nmodel_ridge = Ridge()\nmodel_lasso = Lasso()\nmodel_elasticNet = ElasticNet(l1_ratio=0.5)\n\n# Tunning hyperparameter\n# In this example we use alpha as our hyperparameter lambda. \nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n\n# Evaluating models through k-fold cross-validation defined earlier\ncv_ridge = [k_rmsle_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_lasso = [k_rmsle_cv(Lasso(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_elasticNet = [k_rmsle_cv(ElasticNet(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_elasticNet = pd.Series(cv_elasticNet, index = alphas)","5307ff83":"plt.figure(figsize=(20,10))\n\nplt.subplot(241)\ncv_ridge.plot(title = \"Ridge Validation Curve\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\ncv_lasso = pd.Series(cv_lasso, index = alphas)\nplt.subplot(242)\ncv_lasso.plot(title = \"Lasso Validation Curve\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\ncv_elasticNet = pd.Series(cv_elasticNet, index = alphas)\nplt.subplot(243)\ncv_elasticNet.plot(title = \"ElasticNet Validation Curve\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\nplt.subplot(244)\ncv_ridge.plot()\ncv_lasso.plot()\ncv_elasticNet.plot()\nplt.legend(labels=['Ridge','Lasso','ElasticNet'])\nplt.title('Models Validation Curves')\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\nplt.tight_layout()","ad49224c":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nmodel_ridge.fit(df_train, y_train)\nmodel_ridge_pred = model_ridge.predict(df_train)\nridge_pred = np.expm1(model_ridge.predict(df_test.values))","607055c1":"submission = pd.DataFrame()\nsubmission['Id'] = test_id\nsubmission['SalePrice'] = ridge_pred\nsubmission.to_csv('..\/house_pricing_submission.csv',index=False)\n\n","6b3728d7":"## 1.4 Feature Engineering:\n\n* Total SF - Combining all square footage ","794db538":"This is the time to look at Data Documentation file to choose the proper strategy in dealing with missing values.","de204843":"* **LotFrontage** missing data requires deeper analysis. \n\nSince LotFrontage is the distance of house's front yard, we can make an assumption that it depends on the neighborhood where the house is located. This way we can group LotFrontage values by the neighborhood feature and substitute missing values by the median Lotfrontage distance of the neighborhood. **","fa99d9f2":"## If you find this kernel useful please hit that UPVOTE button :) ","b8708240":"## 1.2 Removing Outliers","4fe4587f":"## 2.3 Defining models ","636abe5d":"# 3. Acknowledgement\n\nKernels:\n* [Bsivavenu's House Price Calculation methods for beginners](https:\/\/www.kaggle.com\/bsivavenu\/house-price-calculation-methods-for-beginners)\n* [Serigne's Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n\nBooks:\n* Raschka Sebastian, Python Machine Learning, Ch 3,5,6,10\n\nArticles:\n* [Regression Analysis: Lasso, Ridge, and Elastic Net](https:\/\/towardsdatascience.com\/regression-analysis-lasso-ridge-and-elastic-net-9e65dc61d6d3)\n* [Regularization: Ridge, Lasso and Elastic Net\n](https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net)\n\n\n\nYoutube videos:\n\n* [Ridge Regression](https:\/\/www.youtube.com\/watch?v=Q81RR3yKn30)\n\n* [Lasso Regression](https:\/\/www.youtube.com\/watch?v=NGf0voTMlcs)","a79f8dd3":"### 2.5 Model Result Submission","06395d3d":"To improve Accuracy (low bias) and Precision (low variance) of our models we introduce Regularization (L1 or L2) terms to our regression equations. Regularization allows us to increase bias and lower the variance of the model by increasing the value of **hyperparameter \u03bb**. Introduction of the initial bias leads to underfitting of the train data set, but optimization of the test data fit. \n\n* **Ridge Regression:**\n\n**L2 penalized model** where the Square Sum of the weights term is added to Ordinary Least Squares cost function. As hyperparameter **\u03bb** increases Ridge regression introduces more bias into the model, so the training data does not fit well initially, yet overfitting of the test data decreases in the long run. Rigde regression minimizes (penalizes) the effect of irrelevant features as the strength of regularization growth. \n\n* **LASSO (Least Absolute Shrinkage and Selection Operator)**:\n\n**L1 penalized model** where Sum of Absolute weights term is added to Ordinary Least Squares cost function. Regularization strength also increases with an increase in **\u03bb**, however in the case of LASSO, irrelevant feature weights become zero. It is also common to think of LASSO as feature selection model. \n\n* **ElasticNet Regression**:\n\n**L2 + L1 penalized model** where both regularization temrs are added to Ordinary Least Squares cost function. This model was introduced to limit sometimes inaccurate feature selection of LASSO, where relevant features are being zeroed. By adding L2 term to the model, the chances of preserving important features increase. Generally in definition of ElasticNet, the ratio of L1\/L2 needs to be specified, yet for many applications it is kept at 0.5. \n\nNow, let's try to use some machine learning notations used in equations below:\n\n* **X** - input variable\n* **w** - weight vector. In a simple line equation this was our slope (coefficient to the input X) . We are trying to estimate our weight vector **w** through regression.\n* **y** - output feature value. Mathematically it is equal to a dot product of **wX**. We are distinguishing **real y** from **predicted y** by adding hat(triangle) symbol to the **predicted y** (See table figure below).\n* **\u03bb** - hyperparameter that determines strengths of Ridge, LASSO, and ElasticNet regression. \n","cb5f5601":"## 1.5 Skewed features transformation\n\nML regression algorithms love normally distributed data, hence skewed features need to be normalized through transformation. You can think of this process as rescaling and shifting your data. \nWe are going to use Box-Cox method with normalization lambda parameter = 0.15.","b4217675":"## 1.3 Combine Train and Test Data Sets","8725fa88":"## 1.2 Target Feature Normalization ","416c96d5":"# 1. Data Preparation\n\n## 1.1 Data Exploration","ecc32347":"## 2.4 Plotting model Validation Curves","a6ab57a7":"### Conclusion\nBased on the results of Validation curves, Ridge regression shows the lowest **RMSE value 0.0452** at alpha = 10, which indicates the highest performance out of three models considered. We can make hypothesis that the reason why Ridge regression outperformed LASSO is due to the fact that all features presented in this data set influence the Saleprice and that LASSO regression potentially turns some of them into zeros. The fact that ElasticNet also outperformed LASSO supports this hypothesis.\n\nThe main purpose of this kernel is to explain the concept of Regularization as well as Ridge, LASSO, and ElasticNet regressions. \n\nHowever, if you like to improve the accuracy, you can try to experiment with the following approaches:\n* Feature selection\n* Holdout cross validation method\n* Grid Search tunning model \n* Other regression algorithms such as: Xgboost, LightGBM, Kernel Ridge, and Stack Regression Method","1cf1ebb0":" # 2. Modeling\n","927c26a0":"## 2.2 Defining cross-validation","60378cbb":"# Preface\n\nThe purpose of this kernel is to explain the concept of Regularization via implementation of Ridge, LASSO, and ElasticNet regression models. \n\nThis kernel is broken down into three sections:\n1. Data preparation \n2. Modeling \n3. Acknowledgement\n\nThe last sections contains useful resources that can clarify certrain concepts for you.\n\nIf you find this kernel useful please click **upvote** :)","75379954":"## 2.1 Splitting data into Train and Test sets","5eb5ef2b":"Before I jump into explanation, I want you to think of a basic line equation we studied in school.\n**y = wx+b**, where **w** - slope of the line and **b** - intercept. \n\nIn machine learning we also deal with line equations, however the dimensionality is no longer 2D, but equal to the number of features(columns) of your data set. To deal with high dimensionality we introduce vector calculus.\n\n*Why using regression in Data Science?*\n\nRegressions allow us to predict output feature values **y** using various linear and nonlinear equations.  \n\nIn the typical Ordinary Least Squares (OLS) regression model (Also called Least Square Cost Function), we are **minimizing the sum of square residuals**. In other words, the sum of squared errors between the true (**y**) and estimate (**y hat** or **wX**) values. Important to note that residuals are **local** errors and not systematic.\n\n*Why do we want to improve Ordinary Least Squares regression and introduce Ridge,LASSO, and ElasticNet regressions?*\n\nThis is where the Bias-Variance Tradeoff (over and underfitting) phenomena becomes critical to understand. Consider the following definitions and picture below: \n\n* **Bias** - systematic prediction error. The key word is **systematic** error (in slope **w**) and not local error(residuals of **y**). You can think of Bias as the accuracy of predictions.\n* **Variance** - uncertainty or measure of the spread of our weight vector **w** estimates.You can think of Variance as the precisions(consistency) of predictions.","78739d89":"## 1.4 Missing Value Imputation"}}