{"cell_type":{"15fcce90":"code","d7b48eb3":"code","7e015db6":"code","07076c15":"code","17f8c529":"code","22ac0b6f":"code","8aa74ad8":"code","f216f700":"code","1c1f5fb8":"code","6374d0cd":"code","814258b5":"code","9ccd1f85":"code","edaa9771":"code","090c2177":"code","914f58ff":"code","5f07e1d3":"code","b81ef34b":"code","b7623bea":"code","bc99cee2":"code","c2d99989":"code","7a94212e":"code","f48a6279":"code","0bdfdabd":"code","b3a4f660":"code","c7e5257b":"code","2173daa9":"code","aa8ad22d":"code","a193f4c7":"code","15c93f00":"code","7eaaeaea":"code","e660ad7b":"code","b6a0d7d8":"code","8916b9c3":"code","7c086ace":"code","aa12753c":"code","12fada0d":"code","7ef6a2f3":"code","b5b69ff8":"code","ceb909c9":"code","e892c6ec":"code","333541cc":"code","ac33311c":"markdown","dd35df43":"markdown","eb98deba":"markdown","c5467809":"markdown","8f978916":"markdown","cc8fdadf":"markdown","64ad34ed":"markdown","38bd239e":"markdown","ee73c6da":"markdown","4a15c65c":"markdown","7c6b3ef8":"markdown","0fbb3419":"markdown","046ac223":"markdown","fcc7dca6":"markdown","0673cd19":"markdown","d4d1fc3b":"markdown","fa59d73a":"markdown","4ae546d3":"markdown","4a135315":"markdown","a0ab87b4":"markdown","75a06b36":"markdown","ec2ac524":"markdown","defba785":"markdown","cf92bd17":"markdown"},"source":{"15fcce90":"# Install missingno to visualize missing data\n#!pip install missingno\n\n# Install xgboost - an algorithm used in this project\n#!pip install xgboost\n\n# Utilities\nimport os\n\n# Numpy & Pandas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n#import pandas_profiling as pp\n\n# Models\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn import metrics\nimport missingno as msno\n\n# Others (warnings etc)\nfrom warnings import simplefilter\n%matplotlib inline\n","d7b48eb3":"# Declare variables required\nDASHES = '-' * 10\nTABS = '\\t' * 8\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.max_columns', 12)\npd.set_option('display.expand_frame_repr', False)\npd.options.display.float_format = '{:,.2f}'.format\n\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\n# Define a function to show values on bar charts\ndef show_values_on_bars(axs, space=0.4):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\", va=\"bottom\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n","7e015db6":"# Read from input file\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        with open(os.path.join(dirname, filename),  encoding='utf-8') as f:\n            %time vehicles_df_full = pd.read_csv(f)\n            f.close()\n        \n# Print read info\nprint(f'Read {len(vehicles_df_full)} lines from the file vehicles.csv\\n\\n')\n\nvehicles_df_full.info()","07076c15":"# Review the completeness of data\n\nmsno.bar(vehicles_df_full.sample(5000))\n","17f8c529":"# Determine and remove the columns to drop based on the above graph\ncols_to_drop = ['id','url', 'region', 'region_url', 'VIN', 'image_url', 'description', \\\n    'county', 'size', 'paint_color', 'drive', 'cylinders', 'state', 'lat','long']\nvehicles_df = vehicles_df_full.drop(columns=cols_to_drop)\n\n# Remove the larger data frame from memory\ndel vehicles_df_full\n\n# Get info of the new data frame\nvehicles_df.info()","22ac0b6f":"# Preview the new dataframe\nvehicles_df\n","8aa74ad8":"# Initial cleaning up\n# Drop NaNs and duplicates\nvehicles_df.dropna(inplace=True)\nvehicles_df.drop_duplicates(inplace=True)\n\n# Update index and change data type of year to string\nvehicles_df.index = range(len(vehicles_df))\nvehicles_df.year = vehicles_df.year.astype(int).astype(str)\n\nvehicles_df","f216f700":"# Describing the dataset to get a basic idea of the non-categorical features\nvehicles_df.describe()","1c1f5fb8":"# Looking at the target column \"price\" first\nf, ax = plt.subplots(figsize=(12, 8))\nax.set_title('Price Distribution', pad=12)\nsns.histplot(vehicles_df, x=\"price\", stat='count', bins=5)\nshow_values_on_bars(ax)","6374d0cd":"vehicles_prc = vehicles_df[(vehicles_df.price >=2000) & (vehicles_df.price <=50000)]\n\n# Then plot the distriution again\nf, ax = plt.subplots(figsize=(12, 8))\nax.set_title('Price Distribution', pad=12)\nsns.histplot(vehicles_prc, x=\"price\", stat='count', bins=20)\nshow_values_on_bars(ax)\n","814258b5":"\n# Check for skewness\nprint(f\"Skewness for odometer: {round(vehicles_prc['odometer'].skew(),2)}\\n\\n\")\nsns.displot(data=vehicles_prc, x=\"odometer\", aspect=2, height=5, kde=True)\n","9ccd1f85":"print(vehicles_prc[(vehicles_prc.odometer == 0)].describe())\nprint('\\n')\nprint(vehicles_prc[(vehicles_prc.odometer > 200000)].describe())","edaa9771":"# Filtering the dataset and verifying again\nvehicles_odo = vehicles_prc[(vehicles_prc.odometer >100) & (vehicles_prc.odometer <=200000)]\n\nprint(pd.DataFrame(vehicles_odo.odometer).describe())\n\nprint(f\"\\n\\nSkewness for odometer: {round(vehicles_odo['odometer'].skew(),2)}\\n\\n\")\nsns.displot(data=vehicles_odo, x=\"odometer\", aspect=2, height=5, kde=True)\n","090c2177":"\n# Log\nodo_log = np.log(vehicles_odo['odometer'])\nprint(f\"Skewness for Log of Odometer Readings: {round(odo_log.skew(),2)}\\n\")\nsns.displot(data=odo_log, aspect=2, height=5, kde=True, legend=True)\n\n\n# Square Root\nodo_sqrt = np.sqrt(vehicles_odo['odometer'])\nprint(f\"Skewness for Square Root of Odometer Readings: {round(odo_sqrt.skew(),2)}\\n\\n\")\nsns.displot(data=odo_sqrt, aspect=2, height=5, kde=True, legend=True)\n\n","914f58ff":"f, ax = plt.subplots(figsize=(20, 10))\nax.set_title('Price vs Year', pad=12)\nfig = sns.boxplot(x=vehicles_odo.year.astype(int), y='price', data=vehicles_odo)\nplt.xticks(rotation=90);","5f07e1d3":"year_list = list(range(2000, 2021))\n\nvehicles_year = vehicles_odo[vehicles_odo.year.astype(int).isin(year_list)]\n\n# Plot again to visualize distribution\nf, ax = plt.subplots(figsize=(12, 8))\nax.set_title('Price vs Year', pad=12)\nfig = sns.boxplot(x=vehicles_year.year.astype(int), y='price', data=vehicles_year)\nplt.xticks(rotation=90);","b81ef34b":"# Calculate age of the posted car using \"posting date\"\n# Convert year and posting date to datetime\nvehicles_year.posting_date = pd.to_datetime(vehicles_year.posting_date, utc=True)\nvehicles_year.posting_date = vehicles_year.posting_date.astype('datetime64[ns]')\n\n# Add a new field for age of cars\nvehicles_year['age'] = vehicles_year.posting_date.dt.year.astype(int) - vehicles_year.year.astype(int)\n\n# Get a preview of the changes\nvehicles_year.head()","b7623bea":"# Get mean of odometer readings by age\ngrp_df = vehicles_year.groupby(by='age').mean()[['price','odometer']].astype(int).reset_index()\n\n# Visualize how odometer average readings vary with price over age of cars\n# Set axes and points \nx = x=grp_df.odometer\ny = grp_df.price\npoints = grp_df.age\ns = [30*n for n in range(len(y))]\n\nf, ax = plt.subplots(figsize=(12, 8))\n# Plot for each year\nplt.title(f\"Mean of Odometer vs Price over age of cars\")\nplt.xlabel(\"Odometer Readings (mean)\")\nplt.ylabel(\"Price (mean) ($)\")\n\n# Add labels for weeks\nfor i, week in enumerate(points):\n    plt.annotate(week, (x[i], y[i]), size=14, va=\"bottom\", ha=\"center\")\n    plt.scatter(x, y, s=s)\n\nplt.show()","bc99cee2":"sns.catplot(x='condition', y ='price', hue='title_status', data=vehicles_year,\n             kind=\"bar\", aspect=2, height=5)","c2d99989":"vehicles_used = vehicles_year[vehicles_year.condition != 'new']\n\nvehicles_used = vehicles_used[vehicles_used.title_status != 'parts only']\n\nsns.catplot(x='condition', y ='price', hue='title_status', data=vehicles_used, \n            kind=\"bar\", aspect=2, height=5)\n\n#del vehicles_year","7a94212e":"# Categorical plot between fuel and price for each type of trasmission \nsns.catplot(x='type', y ='price', hue='fuel', col='transmission', data=vehicles_used, kind=\"bar\", \n            aspect=3, height=4,  palette=\"rocket\", col_wrap=1)\n","f48a6279":"# Remove \"other\" types of fuel\nvehicles_used = vehicles_used[(vehicles_used.fuel != 'other')]\n\n# Remove \"other\" type of trasmissions\nvehicles_used = vehicles_used[(vehicles_used.transmission != 'other')]\n\n# Plot again to visualize\nsns.catplot(x='type', y ='price', hue='fuel', col='transmission', data=vehicles_used, kind=\"bar\", \n            aspect=3, height=4,  palette=\"rocket\", col_wrap=1)\n","0bdfdabd":"# Visualize the relationship of average price by manufacturer\ngrp_man_df = vehicles_used.groupby(by='manufacturer').mean()['price'].reset_index()\n\nx = grp_man_df.manufacturer\ny = grp_man_df.price\ny_mean = [np.median(y)]*len(grp_man_df)\n\n\nf, ax = plt.subplots(figsize=(12, 8))\nax.scatter(x, y, s=y\/100)\nax.plot(x, y_mean, label='Median price', linestyle='--')\n\nplt.title(f\"Mean Prices by Manufacturer\")\nplt.ylabel(\"Price (mean)\")\nplt.xlabel(\"Manufacturer\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n","b3a4f660":"# Add a field for row numbers\nvehicles_used['row_num'] = np.arange(len(vehicles_used))\n\n# Get counts of models\nmodel_df = vehicles_used.groupby(by=\"model\").count()['row_num'].reset_index()\nmodel_df.columns=(['model','count'])\n\n# Get only 10 frequent models and how much the other account to\nlar10_df = model_df.nlargest(10, columns='count')\nlar10_df.index = range(len(lar10_df))\n\n# Get count of all other models and append to the end of data drame\nother_val_sum = model_df[~model_df['model'].isin(lar10_df.model)].sum().T['count']\nlar10_df.loc[10] = ['Other Models',other_val_sum]\n\n# Plot what the counts of models look like\nf, ax = plt.subplots(figsize=(12, 8))\nax.set_title('Count Distribution of Car Models', pad=12)\nsns.barplot(x=\"model\", y=\"count\",  palette=\"icefire\",  data=lar10_df)\nshow_values_on_bars(ax)","c7e5257b":"# Get current information of the dataset\nvehicles_used.info()\n\n# Drop columns populated during clean-up or not required\nvehicles_used.drop(columns=['posting_date','row_num'], inplace=True)","2173daa9":"# Make a copy of the data frame for encoding\nvehicles_used_enc = vehicles_used.copy()\nvehicles_used_enc.info()\n","aa8ad22d":"# Get fields that are categorical and remove only \"model\"\ncat_features = vehicles_used_enc.select_dtypes(exclude=np.number).columns.to_list()\nprint(f'Categorical features: {cat_features}\\n\\n')\n\n# Encode using LabelEncoder\nfor c in cat_features:\n      le = LabelEncoder()\n      le.fit(list(vehicles_used_enc[c].astype(str).values))\n\n      vehicles_used_enc[c] = le.transform(list(vehicles_used_enc[c].astype(str).values))\n\n# Encode \"model\" using OneHotEncoder\n# model_arr = vehicles_used_enc.model.values.reshape(-1,1)\n# oh = OneHotEncoder()\n# model_encoded = oh.fit_transform(model_arr)\n# vehicles_used_enc.model = oh.transform(model_arr)\n\n\nvehicles_used_enc","a193f4c7":"# Get the correlation matrix for the encoded data frame\nf, ax = plt.subplots(figsize=(12, 10))\nax.set_title('Encoded Correlation Heatmap for Used Vehicles Dataset', pad=12)\nsns.heatmap(vehicles_used_enc.corr(), vmin=-1, vmax=1, annot=True, cmap='Spectral')\n","15c93f00":"# X will be all features except price\nfeature_cols = vehicles_used_enc.columns.values.tolist()\nfeature_cols.remove('price')\n#feature_cols.remove('model')\nX = vehicles_used_enc[feature_cols]\n\n# Y will be the target col = price\nY = vehicles_used_enc['price']\n\nprint(f\"X (features):\\n\\n{X}\")\n\nprint(f\"\\nY (target):\\n\\n{Y}\")","7eaaeaea":"#Splitting the dataset into training and testing sets for modeling later\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n\n","e660ad7b":"# Define a function for output statistics\ndef reg_metrics(pred_model, x_train, x_test, y_train, y_test):\n    \"\"\" Function takes in training and testing sets, prediction model, \n    and ouputs the below metrics:\n    1. R\u00b2 or Coefficient of Determination.\n    2. Adjusted R\u00b2\n    3. Mean Squared Error(MSE)\n    4. Root-Mean-Squared-Error(RMSE).\n    5. Mean-Absolute-Error(MAE).\n    \"\"\"\n    # Get predicted values on x_test\n    y_pred = pred_model.predict(x_test)\n\n    #1 & 2 Coefficient of Determination (R\u00b2 & Adjusted R\u00b2)\n    print(\"\\n\\t--- Coefficient of Determination (R\u00b2 & Adjusted R\u00b2) ---\")\n    r2 = metrics.r2_score(y_pred=y_pred, y_true=y_test)\n    adj_r2 = 1 - (1-r2)*(len(y_train)-1)\/(len(y_train)-x_train.shape[1]-1)\n\n    print(f\"R\u00b2\\t\\t: {round(r2, 2)}\")\n    print(f\"Adjusted R\u00b2\\t: {round(adj_r2, 2)}\")\n    \n\n    #3 & 4. MSE and RMSE\n    print(\"\\n\\t--- Mean Squared Error (MSE & RMSE) ---\")\n   \n    mse = metrics.mean_squared_error(y_pred=y_pred, y_true=y_test, squared=True)\n    rmse = metrics.mean_squared_error(y_pred=y_pred, y_true=y_test, squared=False)\n\n    print(f\"MSE\\t: {round(mse, 2)}\")\n    print(f\"RMSE\\t: {round(rmse, 2)}\")\n\n\n    #5. MAE\n    print(\"\\n\\t--- Mean Absolute Error (MAE) ---\")\n    mae = metrics.mean_absolute_error(y_pred=y_pred, y_true=y_test)\n    print(f\"MAE\\t: {round(mae, 2)}\")\n    \n    # Return Accuracy\n    train_acc = round(pred_model.score(x_train, y_train)*100, 2)\n    test_acc = round(pred_model.score(x_test, y_test)*100, 2)\n\n    return (train_acc, test_acc)\n","b6a0d7d8":"# Define a dataframe to summarize accuracies for later\nalgo_list = ['Linear Regression','Decision Trees', 'Bagging', 'Random Forest', 'Adaptive Boosting', 'Gradient Boosting', 'XGBoost']\nacc_cols = ['Training Accuracy', 'Testing Accuracy']\n\nacc_df = pd.DataFrame(columns=acc_cols, index=algo_list)\n\nacc_df.index.name='Algorithm'\n","8916b9c3":"# Linear Regression\nlinear_reg = LinearRegression()\nlinear_reg.fit(x_train, y_train)\nprint(\"\\t------- Linear Regression -------\")\nlinreg_acc = reg_metrics(linear_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Linear Regression'] = linreg_acc","7c086ace":"# Decision Tree \n# A graphical representation of possible solutions to a decision based on certain conditions\n\ndtree_reg = DecisionTreeRegressor()\ndtree_reg.fit(x_train, y_train)\n\nprint(\"\\t------- Decision Tree Regression -------\")\ndtree_acc = reg_metrics(dtree_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Decision Trees'] = dtree_acc","aa12753c":"# Bagging Regression\n# Meta-algorithm combining predictions from multiple-decision\n#  trees through a majority voting mechanism\n\nbag_reg = BaggingRegressor()\nbag_reg.fit(x_train, y_train)\n\nprint(\"\\t------- Bagging Regression -------\")\nbag_acc = reg_metrics(bag_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Bagging'] = bag_acc\n\n","12fada0d":"# Random Forest Regression\n# Bagging-based algorithm where only a subset of features are selected at\n# random to build a forest or collection of decision trees\n\nrf_reg = RandomForestRegressor()\nrf_reg.fit(x_train, y_train)\n\nprint(\"\\t------- Random Forest Regression -------\")\nrf_acc = reg_metrics(rf_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Random Forest'] = rf_acc","7ef6a2f3":"# Adaboost Regression\n# Models are built sequentially by minimizing the errors from previous models while\n# increasing (or boosting) influence ofnigh-performing models\nab_reg = AdaBoostRegressor()\nab_reg.fit(x_train, y_train)\n\n\nprint(\"\\t------- Adaboost Regression -------\")\nab_acc = reg_metrics(ab_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Adaptive Boosting'] = ab_acc\n","b5b69ff8":"# Gradient Boosting Regression\n# Gradient Boosting employs gradient descent algorithm to minimize errors in sequential models\n\ngb_reg = GradientBoostingRegressor()\ngb_reg.fit(x_train, y_train)\n\n\nprint(\"\\t------- Gradient Boosting Regression -------\")\ngb_acc = reg_metrics(gb_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['Gradient Boosting'] = gb_acc\n","ceb909c9":"# XGBoost\n# Optimized Gradient Boosting algorithm through parallel processing, tree-pruning,\n# handling missing values and regularization to avoid overfitting\/bias\n\nxgb_reg = xgb.XGBRegressor() \nxgb_reg.fit(x_train, y_train)\n\nprint(\"\\t------- XGBoost Regression -------\")\nxgb_acc = reg_metrics(xgb_reg, x_train, x_test, y_train, y_test)\n\nacc_df.loc['XGBoost'] = xgb_acc","e892c6ec":"# All accuracies from the above algorithms\nacc_df.astype(str) + '%'","333541cc":"# Rearrange the data frame\nacc_df.columns=['Training','Testing']\nacc_plot_df = acc_df.reset_index().melt(id_vars=['Algorithm'])\nacc_plot_df.columns=['Algorithm','Dataset','Accuracy']\n\n# Plot the final accuracies\nf, ax = plt.subplots(figsize=(12, 8))\nsns.barplot(x=\"Algorithm\", y=\"Accuracy\", hue=\"Dataset\", data=acc_plot_df)\nax.set_title(\"Accuracies by Algorithm\", pad=12)\nshow_values_on_bars(ax)\n","ac33311c":"*That didn't help.. so proceeding without log or sqrt, next step is to see how the age of cars and the odometer readings are related to the price of cars*\n\n\n---","dd35df43":"*With this used 20 year set, next, trying to find how the three features come together and depict real-worl characteristics.*\n*Checking how price varies with mean odometer ratings over the age of the car posted.*\n\n---\n","eb98deba":"\n>**From the above, it's evident that each algorithm has a method of working but they accomplish a common goal.**\n\n\n>**Which one to choose and the best one to use would have to be determined per requirements at hand.**","c5467809":"*It's evident that the distribution is highly skewed and there's some bad data with max odometer readings of 10mil miles etc.*\n\n*Let's work on cleaning up some of that data*\n\n*Doing some research, I found that Americans drive an average of 14,300 miles per year, according to the [Federal Highway Administration](https:\/\/www.thezebra.com\/resources\/driving\/average-miles-driven-per-year\/).*\n\n*Let's look at the entries for odometer = 0 and odometer > 200k.*\n\n","8f978916":"# Encoding Categorical Data\n\n*Since almost all features are categorical in this dataset, we'd have to encode them. I use Label Encoding*\n\n---\n\n\n\n\n\n\n","cc8fdadf":">*With the previews and descrption of continuous variables above, it's immedealtely apparent that some first level cleanup is required. Continuing to do so...*","64ad34ed":"A (good) used car is a better (depreciating) asset to own than a new one\n\nUsing this Craigslist used car dataset and considering the last 20 years' worth of data I use the below algorithms to best predict the value of used cars, based on 10 features\n\n* Linear Regression \n* Decision Trees\n* Bagging \n* Random Forest\n* Adaptive Boosting \n* Gradient Boosting \n* XGBoost\n","38bd239e":"*It appears that the price ranges between 0 and an unrealistic $3.7B*\n\n*To keep things simple and realistic, making a subset of prices between 2k and 50k*\n\n---\n\n","ee73c6da":"*It's evident from the visualization above that cars that have been driven less are more expensive than older cars which have been driven more. There seem to be a good chunk of cars under 10k that have been driven 120k and over and are 12 years and older - this is an interesting insight.*\n\n---\n\n","4a15c65c":"## *Modeling*\n\n*Since the target field is non-catgorical, classifier I use regression rather than classification*.\n*I start with Linear Regression and explore the variants of Decision Tree based models as below:*\n\nDecision Trees --> Bagging --> Random Forest --> Boosting --> Gradient Boosting --> XGBoost\n\n*References*\n1. [Titanic Data Scince Solutions (Models)](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions?scriptVersionId=10431564&cellId=77)\n2. [XGBoost Algorithm](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n\n---","7c6b3ef8":"# Preparing Data and Modeling\n## *Data Prep*\n","0fbb3419":"\n\n# Data Visualization & Cleaning\n  \nVisualizing the data reveals patterns that are not obvious to the human eye when reviewing raw data\n\nCorrelation matrices, histograms, category, scatter & box plots have helped identify relationships\n\nCleaned data based on visualizations:\n* Removed NaNs & duplicates\n* Price b\/w 2k and 50k\n* Odometer b\/w 100 and 200k, etc..\n\nIn the section below, features that would help with better prediction are identified\n\n\n---\n\n\n\n\n","046ac223":"*Next, we see how price is related to different kinds of manufacturers and the models they produce.*\n\n---","fcc7dca6":"**Great Accuracy!**\n\n*Now moving on to summarize and concluding this project..*\n*From the above, it's evident that Random Forest gives the best accuracy of ~90%.*","0673cd19":" *Based on the stats above, I can make a fair assumption that odometer readings be between 100 (CPO) to 200k (20 yo) will be a good dataset to continue with*","d4d1fc3b":"*To start with, define a function for regression metrics*\n\n1. R\u00b2 measures how much variability in dependent variable can be \"explained by the model.\n2. While R\u00b2 is a relative measure of how well the model fits dependent variables, \\nMean Square Error is an absolute measure of the goodness for the fit.\n3. Mean Absolute Error(MAE) is similar to MSE, however, unlike MSE, MAE takes the sum of the ABSOLUTE value of error.\n\n*[Regression Merics Reference](https:\/\/towardsdatascience.com\/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914)*","fa59d73a":"*On to the next, understanding how price of cars is affected by the fuel and trasmission features...*","4ae546d3":"*It appears that there is some inconsistency in the first 2\/3rds of the dataset.*\n\n*Price seems to consistently rise 2000 onwards until about 2021; and there seems to be some bad data for 2022 as well.*\n\n*Filtering the dataset between 2000 and 2020 for further analysis*\n\n---\n\n","4a135315":"*It's obviously evident that luxury brands have a higer price, but except a couple outliers, the median price lies near most points*\n\n*Finally, we explore the \"model\" feature which I imagine has the highest cardinality amongst all the features we've seen so far..*\n\n---\n","a0ab87b4":"*and with that, the skewness comes down from 41.63 to just 0.37 - although still positively skewed, it's worth exploring what log and square root can do..*\n\n---\n\n","75a06b36":"*Since we want to look at only used cars, ignoring new cars for the moment*.\n\n*It also looks like there are only parts being sold - which might affect the price.*\n\n*Removing both these attributes..*\n\n---","ec2ac524":"*As noted above, the \"model\" field has very high cardinality and this would have to be encoded with one of the encoders as they describe [in this (slightly older) article](https:\/\/towardsdatascience.com\/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)*\n\n *I do want to note here that I want to experiment without dropping this high cardinality feature and work with models - if the accuracy turns to be too low, it's worth exploring without this feature*\n \n ---","defba785":"# *Summary & Conclusion*","cf92bd17":"*From the above visualization, it's noted that \"other\" values for type of fuels and trasmissions contribute to a considerable volume of data.*\n\n*These, which are not a lot of value might affect the overall accuracy - hence removing them..*\n\n---\n"}}