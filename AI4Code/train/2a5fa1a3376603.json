{"cell_type":{"d3a20480":"code","d836b35f":"code","1f60a22e":"code","b2b4625f":"code","c9822eed":"code","4efb7a98":"code","a24e6d7a":"code","7c218055":"code","f56343bc":"code","ab0795d0":"code","18546a34":"code","834c3e37":"markdown","6144f686":"markdown","bad7825d":"markdown","fa0e4916":"markdown","3ed46b15":"markdown","f8b3fae8":"markdown","3f54265c":"markdown"},"source":{"d3a20480":"!pip3 install antialiased-cnns","d836b35f":"import torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nimport numpy as np\nnp.random.seed(0)\nimport torchvision\nimport torchvision.transforms as transforms\nimport PIL","1f60a22e":"DC = torchvision.datasets.ImageFolder(\n    root      = \"\/kaggle\/input\/training_set\/training_set\", \n    transform = transforms.Compose([\n        transforms.Resize(200),\n        transforms.Pad(100, padding_mode=\"reflect\"),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.RandomVerticalFlip(),\n#         transforms.RandomRotation(45, resample=PIL.Image.BILINEAR),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std =[0.229, 0.224, 0.225]),\n    ]))\n\ndataloader = torch.utils.data.DataLoader(DC, batch_size=16, shuffle=True, num_workers=8)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","b2b4625f":"# %cd antialiased-cnns\nimport antialiased_cnns\n# import models_lpf.vgg\nmodel = antialiased_cnns.vgg16(filter_size=3, pretrained=True)\n# model = models_lpf.vgg.vgg16(filter_size=3, pretrained=True)\n# model.load_state_dict(torch.load('.\/weights\/vgg16_lpf3.pth.tar')['state_dict'])\n# %cd ..\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nfeature_size = model.classifier[6].in_features\nmodel.classifier[6] =  torch.nn.Linear(feature_size, 2)\n\nmodel = model.to(device)\nweights = []\nfor name, param in model.named_parameters():\n    if param.requires_grad == True:\n        weights.append(param)\n        print(\"\\t\", name)","c9822eed":"model","4efb7a98":"weights","a24e6d7a":"optimizer_ft = torch.optim.Adam(weights, lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\niteration = 0\nnum_epoch = 2","7c218055":"model.train()\nfor epoch in range(num_epoch):\n    for x, y in dataloader:\n        pred = model(x.to(device))\n        loss = loss_fn(pred, y.to(device))\n        optimizer_ft.zero_grad()\n        loss.backward()\n        optimizer_ft.step()\n        iteration = iteration + 1\n        print(f\"\\r[{iteration * 100.0 \/ num_epoch\/len(dataloader):7.3f} % ] loss: {loss.item():5.3f}\", end=\"\")","f56343bc":"torch.save(model.state_dict(), \"vgg16_dogvscat_model.pt\")\n# %rm -r antialiased-cnns","ab0795d0":"DC_test = torchvision.datasets.ImageFolder(\n    root      = \"\/kaggle\/input\/test_set\/test_set\", \n    transform = transforms.Compose([\n        transforms.Resize((224,224)),\n#         transforms.Pad(100, padding_mode=\"reflect\"),\n#         transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std =[0.229, 0.224, 0.225]),\n    ]))\n    \ndef acc(model, dataset):\n    model.eval()\n    hit = 0\n    total = 0\n    dataloader_test = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n\n    for x, y in dataloader_test:\n        pred = model(x.to(device))\n        hit = hit + sum( pred.argmax(dim=-1).cpu() == y ).item()\n        total = total + x.size(0)\n\n    model.train()\n    return hit*100.0 \/ total\n\n\nprint(\"Test accuracy: \", acc(model, DC_test))\n# print(\"Train acc: \", acc(model, DC))","18546a34":"dataloader_test = torch.utils.data.DataLoader(DC_test, batch_size=32, shuffle=True)\n\nbatch = next(iter(dataloader_test))\no = model(batch[0].to(device)).argmax(dim=-1)\nlabels =  [DC_test.classes[i][:-1] for i in o.tolist()]\ntrue_labels =  [DC_test.classes[i] for i in (model(batch[0].to(device)).argmax(dim=-1).tolist())]\nimgs =  batch[0] * torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1) + \\\n                    torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n\nimgs = imgs.permute(0, 2, 3, 1)\nimport matplotlib.pyplot as plt\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\n\nfor i in range(imgs.size(0)):\n    plt.subplot(4, 8, i+1)\n    plt.imshow(imgs[i])\n    c = \"b\" if o[i].item() == batch[1][i].item() else \"r\"\n    plt.text(0,20, labels[i], fontsize=16, color=c,\n                      bbox=dict(boxstyle=\"square\",\n                   ec=(1., 0.5, 0.5),\n                   fc=(1., 0.8, 0.8),\n                   )\n         )\n    plt.axis(\"off\")","834c3e37":"Load the training set","6144f686":"Compute the prediction accuracy on the test set","bad7825d":"Setting the random seeds so we can reproduce the results.","fa0e4916":"Load the pretrained VGG16 model. We replace the last linear classifier layer by a new linear layer with two output neurons.","3ed46b15":"Setting optimizer and loss function.","f8b3fae8":"In this notebook, we will use a vgg16 pretrained model to classify dogs vs cats. Moreover, we use an antialiased model from the paper *Making Convolutional Networks Shift-Invariant Again*, https:\/\/richzhang.github.io\/antialiased-cnns\/\n\n\n","3f54265c":"Train the last layer for 2 epochs."}}