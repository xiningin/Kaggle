{"cell_type":{"b8d00c95":"code","a98c5e51":"code","779190c5":"code","62e13b6e":"code","a01fef1f":"code","94cddb81":"code","e5c9f466":"code","dd9ceebe":"code","cf3e43c8":"code","10b49e69":"code","3d2f397a":"code","46791210":"code","477c43e5":"code","5c698bbd":"code","19a0328a":"code","85f6e208":"code","d0920373":"code","df790571":"code","0f665379":"code","889f3d76":"code","bbbcfde8":"code","4cfd1c10":"code","e089b314":"code","f5df01ed":"code","1cd0960b":"code","a73a733d":"code","6e7769c4":"code","1a5f3d31":"code","ca927f46":"code","5399dcfc":"code","32963892":"code","b5b69415":"code","22d90db9":"code","015af48f":"code","0f770055":"code","8e14057a":"code","bdb0c079":"code","fa048fde":"code","bfd6d236":"code","9e339a82":"code","ecf89254":"code","f5ed4f6f":"code","00df9efa":"code","380a3b9e":"code","189c263a":"code","767abc1e":"code","a525231e":"code","35612422":"markdown","0eb297e7":"markdown","1fc501d6":"markdown","1f993e7f":"markdown","17e72ff2":"markdown","e545fd28":"markdown","75b3d9b9":"markdown","88fa03ca":"markdown","4af2fc92":"markdown","4c96bde0":"markdown","c07d4522":"markdown","e2a1d7b7":"markdown","01a9f8c5":"markdown","39a9cd2f":"markdown","0cd4833a":"markdown","bf0d9784":"markdown","002c1661":"markdown"},"source":{"b8d00c95":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.max_colwidth',None)","a98c5e51":"users = pd.read_csv('..\/input\/book-recommendation-dataset\/Users.csv',low_memory=False)\nratings = pd.read_csv('..\/input\/book-recommendation-dataset\/Ratings.csv',low_memory=False)\nbooks = pd.read_csv('..\/input\/book-recommendation-dataset\/Books.csv',low_memory=False)","779190c5":"print(books.shape)\nbooks.columns=['ISBN','Title','Author','Year_Of_Publication','Publisher','Image_URL_S','Image_URL_M','Image_URL_L']\nbooks.drop(['Image_URL_S','Image_URL_L'],axis=1,inplace=True)\nbooks.head()","62e13b6e":"L=((books.isnull().sum()).sort_values()).to_dict()\nfor i in L:\n    print(i,\"--->\",L[i])  ","a01fef1f":"books.info()","94cddb81":"duplicateRowsDF = books[books.duplicated()]\nduplicateRowsDF.shape","e5c9f466":"print(\"unique isbn: \",len(books[\"ISBN\"].unique()))\nprint(\"total rows: \",books.shape[0])\nprint(\"unique title: \",len(books[\"Title\"].unique()))\nprint(\"total rows: \",books.shape[0])","dd9ceebe":"books['Author'].fillna(\"Unknown\",inplace=True)\nbooks['Publisher'].fillna(\"Unknown\",inplace=True)\nbooks.isnull().sum()","cf3e43c8":"my_dict=(books['Publisher'].value_counts()).to_dict()\ncount= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])\na = count.sort_values(by=['count'], ascending = False)\na.head(7)\nlabels = 'Harlequin','Silhouette','Pocket','Ballantine Books','Bantam Books','Scholastic','Simon & Schuster'\nsizes = [count['count'].iloc[0],count['count'].iloc[1],count['count'].iloc[2],count['count'].iloc[3],count['count'].iloc[4],\n         count['count'].iloc[5],count['count'].iloc[6]]\nexplode = (0.1, 0.1, 0.1, 0.1,0.1, 0.1,0.1 )\nfig1 , ax1 = plt.subplots(figsize=(7,7))\nax1.pie(sizes,\n        explode = explode,\n        labels = labels,\n        autopct = '%1.1f%%',\n        shadow = True,\n        startangle = 0)\nplt.title(\"Top 7 Publishers With the Most Books\")\nax1.axis ('equal')\nplt.show()","10b49e69":"b = count.sort_values(by=['count'], ascending = False)\nb = b.head(20)\nx =['Harlequin','Silhouette','Pocket','Ballantine Books','Bantam Books','Scholastic','Simon &amp; Schuster']\ny = [7537,4220,3905,3783,3646,3160,2971]\nfig=plt.figure(figsize=(10,7))\nax = sns.barplot(x = 'count',y = 'c' , data = b)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90,horizontalalignment='center')\nplt.xlabel(\"No of Books Published\", size=14)\nplt.ylabel(\"Publisher\", size=14)\nplt.title(\" Top 20 Publishers With the Most Books\", size=18)\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_width(), xy=(p.get_width()\/2, p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")\nplt.show()","3d2f397a":"np.set_printoptions(threshold=np.inf)\nbooks['Year_Of_Publication'].unique()","46791210":"index=books.loc[books['Year_Of_Publication']=='DK Publishing Inc'].index\nbooks.drop(index,inplace=True)\nindex=books.loc[books['Year_Of_Publication']=='Gallimard'].index\nbooks.drop(index,inplace=True)\nbooks['Year_Of_Publication'].replace({'0':books['Year_Of_Publication'].value_counts().idxmax()},inplace=True)\nbooks['Year_Of_Publication'] = books['Year_Of_Publication'].astype(str).astype(int)\nbooks['Year_Of_Publication'].unique()","477c43e5":"fig=plt.figure(figsize=(8,5))\ny1 = books[books['Year_Of_Publication'] >= 1960]\ny1 = y1[y1['Year_Of_Publication'] <= 2005]\nsns.distplot(y1['Year_Of_Publication'])\nplt.xlabel('Year Of Publication',size=14)\nplt.title('Histogram of the Year of Publication',size=16)\nplt.show()","5c698bbd":"print(users.shape)\nusers.columns=['UserID','Location','Age']\nusers.head()","19a0328a":"users.info()","85f6e208":"L=((users.isnull().sum()).sort_values()).to_dict()\nfor i in L:\n    print(i,\"--->\",L[i])  ","d0920373":"users['Age'].fillna(users['Age'].value_counts().idxmax(),inplace=True)\nfor i in users['Age'][users['Age']>95]:\n        users['Age'].replace({i:users['Age'].value_counts().idxmax()},inplace=True)\nfor i in users['Age'][users['Age']==0]:\n        users['Age'].replace({i:users['Age'].value_counts().idxmax()},inplace=True)\nusers['Age'] = users['Age'].astype(int) \nusers['Age'].unique()","df790571":"fig=plt.figure(figsize=(8,5))\nsns.distplot(users['Age'])\nplt.xlabel('Age',size=14)\nplt.title('Histogram of Age of Users',size=16)\nplt.show()","0f665379":"users['Location']","889f3d76":"users[['city','state','country','nan','nan','nan','nan','nan','nan']]=users['Location'].apply(lambda x: pd.Series(str(x).split(\",\")))\nusers.drop(['Location','nan'],axis=1,inplace=True)\nusers","bbbcfde8":"print(ratings.shape)\nratings.columns=['UserID','ISBN','Rating']\nratings.head()","4cfd1c10":"ratings['Rating'].unique()","e089b314":"filter1 = ratings[ratings[\"UserID\"].isin(users[\"UserID\"])] \ndf_ratings=filter1[filter1[\"ISBN\"].isin(books[\"ISBN\"])] \ndf=pd.merge(users,df_ratings,on='UserID')\ndf","f5df01ed":"# Pie chart showing countries with most number of users \nmy_dict=(df_ratings['Rating'].value_counts()).to_dict()\ncount= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])\na = count.sort_values(by=['count'], ascending = False)\na.head(7)\nlabels = 'UserID: 153662','UserID: 11671','UserID: 98391','UserID: 198711','UserID: 35859'\nsizes = [count['count'].iloc[0],count['count'].iloc[1],count['count'].iloc[2],count['count'].iloc[3],count['count'].iloc[4]]\nexplode = (0.1, 0.1, 0.1, 0.1,0.1)\n\nfig1 , ax1 = plt.subplots(figsize=(5,5))\n\nax1.pie(sizes,\n        explode = explode,\n        labels = labels,\n        autopct = '%1.1f%%',\n        shadow = True,\n        startangle = 0)\nplt.title(\"UserIDs With Highest Number Of Rating\")\nax1.axis ('equal')\n\nplt.show()","1cd0960b":"my_dict=(users['country'].value_counts()).to_dict()\ncount= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])\nf = count.sort_values(by=['count'], ascending = False)\nf = f.head(15)\nf.drop(7,inplace=True)\nfig=plt.figure(figsize=(10,5))\nax = sns.barplot(y = 'count',x= 'c' , data = f)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90,horizontalalignment='center')\nfor bar in ax.patches: \n    ax.annotate(format(bar.get_height(), '.0f'),  \n                   (bar.get_x() + bar.get_width() \/ 2,  \n                    bar.get_height()), ha='center', va='center', \n                   size=8, xytext=(0,8), \n                   textcoords='offset points') \nplt.xlabel(\"Country\", size=14)\nplt.ylabel(\"No of Users\", size=14)\nplt.title(\"Countries With Most Users\", size=18)\nplt.show()","a73a733d":"# Pie chart showing countries with most number of users \nmy_dict=(users['country'].value_counts()).to_dict()\ncount= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])\na = count.sort_values(by=['count'], ascending = False)\na.head(7)\nlabels = 'United Kingdom','Australia','USA','Germany','Italy','Canada','Spain'\nsizes = [count['count'].iloc[2],count['count'].iloc[5],count['count'].iloc[0],count['count'].iloc[3],count['count'].iloc[6],\n         count['count'].iloc[1],count['count'].iloc[4]]\nexplode = (0.1, 0.1, 0.1, 0.1,0.1, 0.1,0.1 )\n\nfig1 , ax1 = plt.subplots(figsize=(7,7))\n\nax1.pie(sizes,\n        explode = explode,\n        labels = labels,\n        autopct = '%1.1f%%',\n        shadow = True,\n        startangle = 0)\nplt.title(\"Top 7 Countries With the Most Users\")\nax1.axis ('equal')\n\nplt.show()","6e7769c4":"coun=[]\nfor country in df[\"country\"].unique():\n    coun.append(country)\nimport pycountry\ndef do_fuzzy_search(country):\n    result = pycountry.countries.search_fuzzy(country)\n    return result[0].alpha_3\niso_map=[]\nc=[]\nfor i in coun:\n    try:\n        iso_map.append(do_fuzzy_search(i))\n        c.append(i)\n    except:\n        iso_map.append('unknown')\n        c.append(i)\n        continue\ndf1=pd.DataFrame(iso_map,c,columns=['code'])\ndf1","1a5f3d31":"l=list(df1.index)\ncountry_code=[]\nfor i in df['country']:\n    if i in l:\n        country_code.append(df1['code'].loc[df1.index==i][0])\ndf['Country_Code'] = np.array(country_code)","ca927f46":"import pycountry\ngrouped = df.groupby(['Country_Code','country'])\navg=pd.DataFrame(grouped['Rating'].agg(np.mean))\navg.reset_index(inplace=True)\navg.columns=['Country Code','Country','Average Rating']\nimport plotly.express as px\nfig = px.choropleth(avg, locations=avg['Country Code'],color=avg['Average Rating'],hover_name=avg['Country'],\n                    color_continuous_scale=px.colors.sequential.Plasma)\nfig.show()","5399dcfc":"import pycountry\ngrouped = df.groupby(['Country_Code','country'])\navg=pd.DataFrame(grouped['Age'].agg(np.mean))\navg.reset_index(inplace=True)\navg.columns=['Country Code','Country','Average Age']\nimport plotly.express as px\nfig = px.choropleth(avg, locations=avg['Country Code'],color=avg['Average Age'],hover_name=avg['Country'],\n                    color_continuous_scale=px.colors.sequential.Plasma)\nfig.show()","32963892":"a=df[df['Rating']>0]\ngrouped = a.groupby('ISBN')\npopular_books = pd.DataFrame(grouped['Rating'].agg([np.size, np.mean]))\nmost_popular = popular_books.sort_values(['mean'], ascending=False)\npop=most_popular[most_popular['size']>200]\npop_title=list(pop[:10].index)\ntitles=[]\nfor i in pop_title:\n    titles.append(books['Title'].loc[books['ISBN']==i].values[0])\nindex=1\nfor i in titles:\n    print(index,'. ',i)\n    index+=1","b5b69415":"dataset=pd.read_csv(\"..\/input\/dataset\/dataset.csv\")\ndata1=dataset[['title','description']]\ndata1","22d90db9":"data1['description'] =data1['description'].astype(str)\ndata1['len']=data1['description'].apply(lambda x:len(x.split(' ')))\nbookdf=data1[data1['len']>=50]\nbookdf.shape","015af48f":"import spacy as sp\nsp.prefer_gpu()\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nimport re\ndef clean_text(doc):\n    #Clean the document. Remove pronouns, stopwords, lemmatize the words and lowercase them\n    doc = nlp(doc)\n    tokens = []\n    exclusion_list = [\"nan\"]\n    for token in doc:\n        if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum()==False) or token.text in exclusion_list :\n            continue\n        token = str(token.lemma_.lower().strip())\n        tokens.append(token)\n    return \" \".join(tokens) \nbookdf['cleaned_description']=bookdf.apply(lambda row: clean_text(row[\"description\"]) ,axis=1)\nbookdf.reset_index(inplace=True)","0f770055":"newdf=bookdf[['title','description','cleaned_description']]","8e14057a":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nnewdf['title_id']=le.fit_transform(newdf['title'])\ncontentdf=newdf[['title_id','title','description','cleaned_description']]\ncontentdf=contentdf.set_index('title_id')\ncontentdf.reset_index(inplace=True)\n#contentdf.drop(['index'],axis=1,inplace=True)\ncontentdf","bdb0c079":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 3), stop_words='english')\ntfidf_description = tfidf.fit_transform((contentdf[\"cleaned_description\"])) \nprint(\"Each of the %d text is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(tfidf_description.shape))\n# Finding cosine similarity between vectors \nfrom sklearn.metrics.pairwise import cosine_similarity \ncos_sim = linear_kernel(tfidf_description, tfidf_description) ","fa048fde":"indices = pd.Series(contentdf.index) \ndef recommendations(title, cosine_sim = cos_sim): \n    recommended_book = [] \n    index = indices[indices == title].index[0]\n    similarity_scores = pd.Series(cosine_sim[index]).sort_values(ascending = False) \n    top_10_books = list(similarity_scores.iloc[1:11].index)\n    for i in top_10_books: \n        recommended_book.append(list(contentdf.index)[i]) \n    top_5_books=[]\n    for i in recommended_book:\n        if i not in top_5_books and i!=title:\n            top_5_books.append(i)\n    recommendbook=[]\n    for i in top_5_books:\n        recommendbook.append(contentdf['title'][i])\n    return recommendbook","bfd6d236":"book=\"Stairway to Heaven\"\nprint(\"Enter a book you liked: \",book)\na=contentdf.loc[contentdf['title']==book]\nid=a.index[0]\nr=recommendations(id)\nprint(\"*****Here are a few recommendations for you*****\")\nfor i,j in zip(r,range(1,len(r)+1)):\n    print(j,\".\",i)","9e339a82":"df=df[df['Rating']>0]","ecf89254":"counts1 = df['UserID'].value_counts()\ndf= df[df['UserID'].isin(counts1[counts1 > 200].index)]\ndf","f5ed4f6f":"len(df['ISBN'].unique())","00df9efa":"cdf1=df[['UserID','ISBN','Rating']]\ncdf=pd.merge(cdf1,books,on='ISBN')\ncdf","380a3b9e":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncdf['User_ID']=le.fit_transform(cdf['UserID'])\ncdf['title_id']=le.fit_transform(cdf['Title'])\ncdf.drop('UserID',axis=1,inplace=True)\ncdf","189c263a":"from surprise import Reader\nreader = Reader(rating_scale=(1, 10))\ndata = Dataset.load_from_df(cdf[['User_ID','title_id','Rating']], reader)\nsvd = SVD()\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n","767abc1e":"def user_rec(id):\n    user= cdf[['ISBN','Title','Author','Year_Of_Publication','Publisher','title_id']].copy()\n    user = user.reset_index()\n# getting full dataset\n    data = Dataset.load_from_df(cdf[['User_ID','title_id','Rating']], reader)\n    trainset = data.build_full_trainset()\n    svd.fit(trainset)\n    user['Estimate_Score'] = user['title_id'].apply(lambda x: svd.predict(id, x).est)\n    user = user.drop(['index','title_id'], axis = 1)\n    user= user.sort_values('Estimate_Score' , ascending = False)\n    counts1 = user['Estimate_Score'].value_counts()\n    user = user[user['Estimate_Score'].isin(counts1[counts1 == 1].index)]\n    return user.head(10)","a525231e":"pd.set_option('display.max_rows',None) \nUid=3\nprint(\"THE ID OF THE USER: \",Uid)\ndetails=cdf.loc[cdf['User_ID']==Uid]\nid=details['User_ID'].iloc[0]\na=user_rec(id)\na.reset_index(inplace=True)\na.drop(['index'],axis=1,inplace=True)\ndetails.reset_index(inplace=True)\ndetails\ndetails.drop(['index','User_ID','title_id','ISBN'],axis=1,inplace=True)\nprint(\"\\n********************************************USER HAS RATED THESE BOOKs******************************************************\")\ntitle_1=list(details['Title'])\nrat1=list(details['Rating'])\nfor i,j in zip(title_1,rat1):\n    print(i,'=>',j)\nprint(\"\\n****************HERE ARE A FEW RECOMMENDATIONS FOR THE USER WITH THE  MOST ESTIMATED POINTS THE USER WILL GIVE******************\")\n\ndisplay(a)","35612422":"now let us start from books dataframe. first we will check if there are NULL values","0eb297e7":"<h2>Popularity Based Recommendation<\/h2>","1fc501d6":"we can see that the years are object type and we have zeros as well as there is some error by including names in years ","1f993e7f":"here we use a dataset with descriptions of books given to recommend to users new books based on their previous preference ","17e72ff2":"<h2>Collabrative Filtering Based Recommendation<\/h2>","e545fd28":"Now we will analyse users dataset","75b3d9b9":"here we recommend books to users who have already rated a few books ","88fa03ca":"we do not have duplicate rows now we will check if the isbn or the title is repeated","4af2fc92":"we have cleaned the age values  ","4c96bde0":"we can see that some titles are repeated","c07d4522":"the above map shows the average ratings given by users of a country","e2a1d7b7":"we will check for null values","01a9f8c5":"<h2>Content Based Recommendation<\/h2>","39a9cd2f":"here we merged the ratings and users dataset on UserID","0cd4833a":"the above map shows the average age of users of a country","bf0d9784":"now we will analyse the ratings columns","002c1661":"recommendations of the books are made based on their popularity among users"}}