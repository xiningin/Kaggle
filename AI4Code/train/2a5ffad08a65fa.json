{"cell_type":{"89a55574":"code","1d90ac95":"code","dfa7ddfb":"code","485bec8e":"code","d2a8a5de":"code","bdda5317":"code","46557f4c":"code","e5215d6c":"code","1ea5629e":"code","8342f858":"code","0d1fb594":"code","d3ea4bc3":"code","bf09bbd9":"code","34ba0165":"code","4d3de23f":"code","89a73acc":"code","34fddc65":"code","60865fe0":"code","39f1ac12":"markdown","20fa90e7":"markdown","92104c51":"markdown","94bd73a0":"markdown","9b456344":"markdown","110dfbf4":"markdown","09d0441e":"markdown","17f6be54":"markdown","d641f147":"markdown","1f133bf7":"markdown","df397629":"markdown"},"source":{"89a55574":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\n\ndata_path = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        data_path.append(path)\n        #print(path)\ndata_path.sort()","1d90ac95":"train = pd.read_csv(data_path[2])\ntest = pd.read_csv(data_path[1])\nJ_genre = ['Japanese style', \n       'Creative Japanese food', 'Japanese food in general','Japanese cuisine\/Kaiseki', \n       'Okonomiyaki\/Monja\/Teppanyaki',\n       'Shabu-shabu\/Sukiyaki', 'Udon\/Soba','Sushi']\n#area_name \u306e\u6700\u521d\u306e\u6587\u5b57\u306f\u90fd\u9053\u5e9c\u770c\u3092\u8868\u3059\nprefectures = []\nfor c in train[\"area_name\"].unique():\n    c = c.split()\n    prefectures.append(c[0])\nprefectures = set(prefectures)\nprefectures = list(prefectures)\nKantou = [\"T\u014dky\u014d-to\",'Saitama-ken',\"Kanagawa-ken\"]\nKansai = [\"Hy\u014dgo-ken\",\"Osaka\",\"\u014csaka-fu\"]","dfa7ddfb":"def make_data(df):\n    \n    #\u66dc\u65e5\n    df[\"holiday\"] = [1 if x == \"Saturday\" else 1 if x == \"Sunday\" else 2 for x in df[\"day_of_week\"] ]\n    #\u30b8\u30e3\u30f3\u30eb\n    df[\"genre_J\"] = [1 if x in(J_genre) else 2 for x in df[\"genre_name\"]]\n    #\u5730\u57df\n    df_prefectures = []\n    for area in df[\"area_name\"]:\n        #print(\"hege\")\n        area = area.split()\n        for prefecture in prefectures:\n            if(area[0] == prefecture):\n                df_prefectures.append(prefecture)\n    df[\"prefectures\"] = df_prefectures #\u3042\u3068\u3067Onehot\n    #area_Kantou Kansai\u306fOnehot\u5316\n    df[\"area_Kantou\"] = [1 if x in(Kantou) else 2 for x in df[\"prefectures\"]]\n    df[\"area_Kansai\"] = [1 if x in(Kansai) else 2 for x in df[\"prefectures\"]] \n    #\u7def\u5ea6\u3001\u7d4c\u5ea6\n    df[\"latitude\"] = df[\"latitude\"] - df[\"latitude\"].min() + 1 \n    df[\"longitude\"] = df[\"longitude\"] - df[\"longitude\"].min() + 1\n    \n    #store_id\n    \n    \n    # LabelEncoding\n    objects = [\"store_id\",\"day_of_week\"]\n    for c in objects:\n        le = LabelEncoder()\n        df[c] = le.fit_transform(df[c]) + 1\n        \n    # One hot  for prefecture\n    df = pd.get_dummies(df,columns = [\"prefectures\",\"genre_name\", \"area_name\"])\n    #store_id\n    #df[\"store_id\"] = (df[\"store_id\"] - df[\"store_id\"].mean() ) \/ df[\"store_id\"].var()\n    \n    return df\n#even_list = [i if i % 2 == 0 else \"odd\" for i in range(10)]\n","485bec8e":"def determine_alternative_store_id():\n    #t_train = train.drop(\"prefectures\",axis=1)\n    t_train = train.groupby(\"store_id\",as_index = False).mean()\n    t_train = t_train.rank()[\"log_visitors\"].astype(int)\n    return t_train\n\ndef set_alternative_store_id(df):\n    alter_id_ = determine_alternative_store_id()\n    tmp_id = []\n    for i in df[\"store_id\"]:\n        for j, Id in zip( range(len(alter_id_)) ,alter_id_.values):\n            if( i-1 == j  ):\n                Id = Id\/100\n                tmp_id.append(round(Id,0))\n    df[\"alter_id\"] = tmp_id\n    \n    return df","d2a8a5de":"# \u4eca\u56de\u306f\u672a\u4f7f\u7528\ndef generate_character(df):\n    df_tmp = pd.DataFrame()\n    for i,n_i in zip(df.columns,range(len(df.columns))):\n        #if(i != \"store_id\"):\n            \n            #df_tmp[i + \"\/mean\"] = df[i] * df[i].mean()\n            #df_tmp[i + \"\/max\"] = df[i] * df[i].max()\n            #df_tmp[i + \"\/min\"] = df[i] * df[i].min()\n        for j,n_j in zip(df.columns, range(len(df.columns))):\n            if(i!=j and n_i > n_j and i != \"alter_id\" and j != \"alter_id\"):\n                df_tmp[i +\"\/plus\/\"+j] = df[i] + df[j]\n                #df_tmp[i +\"\/minus\/\"+j] = df[i] - df[j]\n                #df_tmp[i +\"\/multi\/\"+j] = df[i] * df[j]\n                #df_tmp[i +\"\/devide\/\"+j] = df[i] \/ df[j]\n    return df_tmp","bdda5317":"%%time\ntrain = make_data(train)\ntest = make_data(test)\n#\u6642\u9593\u304c\u304b\u304b\u308b alter_id\ntrain = set_alternative_store_id(train)\ntest = set_alternative_store_id(test)\n\n#\u5916\u308c\u5024\u9664\u53bb ver1\n#train = train[train[\"alter_id\"] != train[\"alter_id\"].max()]\n#\u5916\u308c\u5024\u9664\u53bb ver2\nfor i in range(10):\n    train = train[train[\"log_visitors\"] != train[\"log_visitors\"].max()]\n    train = train.reset_index(drop = True )\n\ny_train = train[\"log_visitors\"]\n\ntrain.drop([\"id\",\"store_id\",\"log_visitors\"],axis = 1,inplace = True)\ntest.drop([\"id\",\"store_id\"],axis = 1 , inplace = True)\n\n\n\n#leak_train_data = train.groupby(\"alter_id\",as_index = False).mean()\n#leak_test_data = test.groupby(\"alter_id\",as_index = False).mean()\n\n#\u7279\u5fb4\u91cf\u591a\u3044\n#train = pd.concat( [train,generate_character(train)],axis = 1)\n#test = pd.concat( [test,generate_character(test)], axis = 1)\n\n#train = pd.merge(train,leak_train_data,on=\"alter_id\")\n#test = pd.merge(test,leak_test_data)\nX_train = train","46557f4c":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#from lightgbm import LGBMRegressor\n\nfrom catboost import CatBoostRegressor\n\n#from sklearn.linear_model import ElasticNet","e5215d6c":"def rmse_score(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    return rmse\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ndef eval(model):\n    print(cross_val_score(estimator=model,X=X_train,y=y_train,cv=kf,scoring = make_scorer(rmse_score)).mean()))","1ea5629e":"#model_cat = CatBoostRegressor(iterations=3000,loss_function='RMSE')\n#model_cat_cv1 = GridSearchCV(model_cat, {'learning_rate': [0.03], 'depth': [5,6],\"l2_leaf_reg\":[20],\"random_strength\":[2],'bagging_temperature':[2]}, verbose=1)\n#model_cat_cv1.fit(X_train, y_train)\n#print (model_cat_cv1.best_params_) #GridSearch","8342f858":"#model_catx = CatBoostRegressor(learning_rate = 0.03, depth = 5, l2_leaf_reg = 20, random_strength = 2, max_leaves = 64,loss_function='RMSE')\n#Eval(model_catx)","0d1fb594":"#model_catx = CatBoostRegressor(learning_rate = 0.03, depth = 6, l2_leaf_reg = 20, random_strength = 2, max_leaves = 64,loss_function='RMSE')\n#Eval(model_catx)","d3ea4bc3":"#model_cat = CatBoostRegressor(iterations=3000,loss_function='RMSE')\n#model_cat_cv1 = GridSearchCV(model_cat, {'learning_rate': [0.03], 'depth': [6],\"l2_leaf_reg\":[20],\"random_strength\":[2],'bagging_temperature':[2,4,6]}, verbose=1)\n#model_cat_cv1.fit(X_train, y_train)\n#print (model_cat_cv1.best_params_) #GridSearch","bf09bbd9":"#cat\n#model_catx = CatBoostRegressor(learning_rate = 0.03, depth = 6, l2_leaf_reg = 20, random_strength = 2, max_leaves = 64,loss_function='RMSE')\n#eval(model_catx)","34ba0165":"#lgb\n#model_lgb = LGBMRegressor(n_estimators=200, learning_rate=0.15, metric='rmse')\n#eval(model_lgb)\n#model_lgb = LGBMRegressor(n_estimators=200, learning_rate=0.15, metric='rmse')\n#model_lgb.fit(X_train,y_train)\n#result_lgb = pd.DataFrame(model_lgb.feature_importances_,index = X_train.columns,columns = [\"lgb_importance\"]).sort_values('lgb_importance',ascending = False)\n#result_lgb.head(10)","4d3de23f":"#model_cat = CatBoostRegressor(learning_rate=0.05, depth=6, l2_leaf_reg = 5, max_leaves = 64,loss_function='RMSE')\n#model_cat.fit(X_train,y_train)\n#result_cat = pd.DataFrame(model_cat.feature_importances_,index = X_train.columns,columns = [\"cat_importance\"]).sort_values('cat_importance',ascending = False)\n#drop_col = result_cat.tail(50).index\n#X_train.drop(drop_col, axis = 1 , inplace = True)\n#test.drop(drop_col, axis = 1 , inplace = True)\n\n#model_cat = CatBoostRegressor(learning_rate=0.05, depth=6, l2_leaf_reg = 5, max_leaves = 64,loss_function='RMSE')\n#eval(model_cat)\n#test.drop(drop_col, axis = 1 , inplace = True)","89a73acc":"preds_test = []\nfor i_fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n    print(f\"--------fold {i_fold}-------\")\n    ## train data\n    x_tr = X_train.loc[train_idx]\n    y_tr = y_train.loc[train_idx]    \n    model_cat_sub = CatBoostRegressor(learning_rate = 0.03, depth = 6, l2_leaf_reg = 20, random_strength = 2,max_leaves = 64,loss_function='RMSE')\n    model_cat_sub.fit(x_tr, y_tr)\n    ## pred on test\n    pred_test = model_cat_sub.predict(test)\n    preds_test.append(pred_test)\n\n\n#model_cat_sub = CatBoostRegressor(learning_rate = 0.03, depth = 6, l2_leaf_reg = 20, random_strength = 2,max_leaves = 64,loss_function='RMSE')\n#model_cat_sub.fit(X_train, y_train)\n#pred = model_cat_sub.predict(test)\n#pred = pred1 * 0.1 + pred2 * 0.9\nsubmit = pd.read_csv(data_path[0])\nsubmit[\"log_visitors\"] = np.mean(np.vstack(preds_test), axis = 0)\nsubmit.to_csv(\"submission10.csv\", index = False)","34fddc65":"train = pd.read_csv(data_path[2])\n#first data \u3053\u3063\u3061\u306f\u9069\u3057\u3066\u3044\u306a\u3044\u3000\u306a\u305c\u306a\u3089\u5927\u904e\u53bb\u306e\u30c7\u30fc\u30bf\u3092\u904e\u53bb\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\u3057\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u304b\u3089\u3000\u3086\u3048\u306b\u30b9\u30b3\u30a2\u306f\u9ad8\u3044\n#val1 = train.groupby(\"store_id\",as_index = False).first()\n#train1 = train.groupby(\"store_id\").tail(2)\n#second \u3000\u3053\u3063\u3061\u306f\u9069\u3057\u3066\u3044\u308b                                  \nval = train.groupby(\"store_id\",as_index = False).last()\ntrain = train.groupby(\"store_id\").head(2)\n\ntrain = make_data(train)\nval = make_data(val)\ntrain = set_alternative_store_id(train)\nval = set_alternative_store_id(val)\n\n#\u5916\u308c\u5024\u9664\u53bb ver1\n#train = train[train[\"alter_id\"] != train[\"alter_id\"].max()]\n#\u5916\u308c\u5024\u9664\u53bb ver2\nfor i in range(40):\n    train = train[train[\"log_visitors\"] != train[\"log_visitors\"].max()]\n    train = train.reset_index(drop = True )\n\ny_train = train[\"log_visitors\"]\ny_val = val[\"log_visitors\"]\ntrain.drop([\"id\",\"store_id\",\"log_visitors\"],axis = 1,inplace = True)\nval.drop([\"id\",\"store_id\",\"log_visitors\"],axis = 1 , inplace = True)                    \n\n#leak_train_data = train.groupby(\"alter_id\",as_index = False).mean()\n#leak_val_data = val.groupby(\"alter_id\",as_index = False).mean()\n\n#\u7279\u5fb4\u91cf\u591a\u3044\n#train = pd.concat( [train,generate_character(train)],axis = 1)\n#val = pd.concat( [val,generate_character(val)], axis = 1)\n\n#train = pd.merge(train,leak_train_data,on=\"alter_id\")\n#val = pd.merge(val,leak_val_data, on = \"alter_id\")\n\n\nX_train = train\nX_val = val\n\nmodel_cat_val = CatBoostRegressor(learning_rate = 0.03, depth = 6, l2_leaf_reg = 20, max_leaves = 64,loss_function='RMSE')\n\nmodel_cat_val.fit(X_train,y_train)\ny_pred_val = model_cat_val.predict(X_val)\n                       \nrmse_score(y_val,y_pred_val)","60865fe0":"#result_cat = pd.DataFrame(model_cat_val.feature_importances_,index = X_train.columns,columns = [\"cat_importance\"]).sort_values('cat_importance',ascending = False)\n#result_cat.head(30)","39f1ac12":"### \u4e88\u60f3\u3000\u63d0\u51fa","20fa90e7":"## \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","92104c51":" 0.22  ,","94bd73a0":"* \u3000\u6700\u521d\u306b\u7279\u5fb4\u91cf\u3092\u8ffd\u52a0\u3057\u3066PB\u3092\u51fa\u3057\u305f\u304c\u7d50\u679c\u304c0.35\u3067\u3042\u308a,\u4ed6\u306e\u4eba\u3088\u308a\u3082\u7d50\u679c\u304c\u304b\u306a\u308a\u52a3\u3063\u3066\u3044\u305f.\n* \u3000\u65b0\u305f\u306a\u7279\u5fb4\u91cf\u3092\u5927\u91cf\u306b\u8ffd\u52a0\u3057,null importance\u3092\u304b\u3051,\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u3044\u304f\u6642\u9593\u7684,\u7269\u7406\u7684\u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3057\u306a\u3044\n\n\u3068\u3044\u3046\u72b6\u6cc1\u3067\u6b63\u89e3\u30e9\u30d9\u30eb\u3067\u3042\u308b'log_visitors'\u3092\u5909\u63db\u3057\u3066\u7279\u5fb4\u91cf\u306b\u4f7f\u3048\u306a\u3044\u304b\u8003\u3048\u305f.\n\u5e97\u306e\u8a2a\u554f\u8005\u306f\u77ed\u671f\u9593\u3067\u5909\u5316\u3057\u306a\u3044\u306e\u3067\u6709\u52b9\u3067\u3042\u308b\u306f\u305a\u3067\u3042\u308b(\u3053\u306e\u30b3\u30f3\u30da\u3067\u306fPB\u3082PV\u3082\u671f\u9593\u304c\u9650\u3089\u308c\u3066\u3044\u308b)\n\ndata_leak\u3057\u306a\u3044\u3053\u3068\u304c\u5927\u524d\u63d0\u3067\u3042\u308b.(\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3067\u306fdata_leak\u304c\u751f\u3058\u3066\u3044\u308b\u3001\u3001\u3001\u3001)\n\n\u7d50\u679c\u3068\u3057\u30660.35\u21920.29\u3068\u3044\u3046\u98db\u8e8d\u7684\u306a\u30b9\u30b3\u30a2\u306e\u5411\u4e0a\u304c\u898b\u3089\u308c\u305f.\n\n\u5b9f\u969b\u306b\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u6709\u52b9\u306b\u4f7f\u3063\u3066\u3044\u305f\u304b\u306f\u4e0d\u660e\u3067\u3042\u308b\u3001\u3001\u3001(\u4eca\u56de\u306f\u6b63\u89e3\u30e9\u30d9\u30eb\u304b\u3089\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u304b\u3089\u3055\u3089\u306a\u308b\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\u3068\u7cbe\u5ea6\u304c\u30ac\u30af\u30c3\u3068\u843d\u3061\u305f)","9b456344":"### \u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0","110dfbf4":"### calculate_CV","09d0441e":"### \u8a55\u4fa1\u6307\u6a19","17f6be54":"### drop_non_important_col","d641f147":"### \u4e00\u756a\u5927\u4e8b\u306a\u7279\u5fb4\u91cf:alter_id","1f133bf7":"## \u306f\u3058\u3081\u306b","df397629":"## \u6a5f\u68b0\u5b66\u7fd2"}}