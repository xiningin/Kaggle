{"cell_type":{"3d3b40ea":"code","213868ad":"code","360151f5":"code","9719e3e1":"code","d5624abe":"code","7439310c":"code","188f4e28":"code","2699f69c":"code","2c473bb5":"code","f44f9e88":"code","aea71957":"code","80079639":"code","bd8522fd":"code","305b0384":"code","3a2cc978":"code","ff7e1091":"markdown","38c4a614":"markdown","0a122ec3":"markdown","4379d7fa":"markdown","bf9f8804":"markdown","2181d7d6":"markdown","2886b370":"markdown"},"source":{"3d3b40ea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","213868ad":"class PCA:\n    \n    # I will denote components as features here\n    # Though its not mathematically accurate but machine learning it works \n    def __init__(self,number_of_important_features=2):\n        # number of specified features\n        # Default being passed as 2\n        self.number_of_important_features=number_of_important_features\n        # Best possible features\n        self.features=None\n        self._mean=None\n        \n        \n    def fit(self,X):\n        # placing mean to as origin of axis\n        # axis =0 is mean of rows along the column direction \n        self._mean=np.mean(X,axis=0)\n        X=X-self._mean\n        \n        # Co-variance of N,D -->DxD\n        # Also called Autocorrelation as both are X's\n        covariance=np.dot(X.T,X)\/(X.shape[0]-1)\n        print(covariance.shape)\n        \n        # Eigenvalues,eigenvectors detail discussion below\n        # Eigenvector is the vector which doesnot chnage it span(simply, direction) after matrix transformation\n        # So, why eigen importance. Best intuitive way to say\n        # for 3D object, the eigenvector represents its axis of rotation(For earth eigenvector is the axis of rotation)\n        # Formula A(matrix).v(eigenvector)=lambda(eigenvalue).v(eigenvector)\n        # So, Intuitively above formula means, matrix transformation of eigenvector is the eigenvector scaled by eigenvalue\n        # Here we are finding the eigenvector and eigenvalue of the covariance matrix\n        # how to solve is (A-lambda.I(identity matrix))-v=0,  As v is non-zero --> det(A-lambda.I)=0(area under transformation=0)\n        # Here lambda is the knob by tweaking it, we change the det = 0\n        # We can do all this by only one line of code, isnt it awesome!!!\n        # There is very powerful application of eigen's i.e eigenbasis-->diagonalisation()\n        # A gift for the patience\n        # you can say this to your gf or bf --> \"My love for you is like eigenvector\"\n        eigenvalues,eigenvector=np.linalg.eig(covariance)\n        print(\"eigenvalues-->\",eigenvalues.shape)\n        print(\"eigenvalues \\n\",eigenvalues)\n        print(\"eigenvector-->\",eigenvector.shape)\n        print(\"eigenvector \\n\",eigenvector)\n        #sort the eigenvalues from highest to lowest\n        # If we didnt transpose, then applying indexs will require more steps and computation\n        eigenvector=eigenvector.T\n        print(\"eigenvector.T-->\",eigenvector.shape)\n        print(\"eigenvector after Transpose\\n\",eigenvector)\n        indexs=np.argsort(eigenvalues)[::-1]\n        #taking those indices and storing in eigenvalues and eigenvectors accordingly\n        eigenvector=eigenvector[indexs]\n        print(\"eigenvector-indexs-->\",eigenvector.shape)\n        print(\"eigenvector after indexes \\n\",eigenvector)\n        eigenvalues=eigenvalues[indexs]\n        print(\"eigenvalues-indexs-->\",eigenvalues.shape)\n        print(\"eigenvalues \\n\",eigenvalues)\n        \n        ## This below code snippet is for seeing how to determine which feature to be calculated\n        total = sum(eigenvalues)\n        variance_of_each_feature = [(i \/ total)*100 for i in eigenvalues]\n        print(\"variance of each feature-->\",variance_of_each_feature)\n        \n        # Now taking only number of specified componenets\n        self.features=eigenvector[:self.number_of_important_features]\n        print(\"self.features\",self.features.shape)\n        # So, now the we have chosen most significant features componenet\n        \n    def apply(self,X):\n        # Here we project the data onto Principal component line\n        X=X-self._mean\n        # Check the dimensionality with (.shape) to confirm for yourselves\n        # Here X-->(N,4);self.features-->2,4\n        # (X,self.features.T)-->(N,4)x(4,2)==(N,2) i.e N samples with 2 feature vector \n        return np.dot(X,self.features.T)\n        \n        \n        ","360151f5":"from sklearn.datasets import load_iris","9719e3e1":"iris = load_iris()\niris_df = pd.DataFrame(iris.data,columns=[iris.feature_names])\niris_df.head()","d5624abe":"iris_df.columns=iris_df.columns.sort_values()","7439310c":"iris_df.head()","188f4e28":"X = iris_df.iloc[:,:]","2699f69c":"X.shape","2c473bb5":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\nprint(X[0:5])","f44f9e88":"pca=PCA(2)","aea71957":"pca.fit(X)","80079639":"projected=pca.apply(X)","bd8522fd":"x0=projected[:,0]\nx1=projected[:,1]","305b0384":"y=iris.target\n# For coloring the graph, unsupervised method no need to think much","3a2cc978":"plt.scatter(x0,x1,c=y)","ff7e1091":"#### May be thinking where is Y. There is no need for Y as it is unsupervised","38c4a614":"#### As you can see the variance of first two features equals 96. Thus we chose these two as PCA","0a122ec3":"#### Applying the PCA","4379d7fa":"#### Refer to Coursera Mathematics for Machine Learning Linear Algebra week 5\n#### Andrew Ng's Machine Learning course or coursera on Dimensionality Reduction","bf9f8804":"### Testing from sklearn's IRIS dataset\n#### 4 feature and 1 output","2181d7d6":"## If these notebook helped. Give an upvote, adds a lot to Motivation","2886b370":"# PCA \n### PRINCIPAL COMPONENT ANALYSIS\n#### It is Unsupervised dimensionality reduction technique used for selecting best feature among lots lots of features"}}