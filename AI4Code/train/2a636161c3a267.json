{"cell_type":{"c36e3b90":"code","4c61395c":"code","3b968597":"code","a2d8141d":"code","32134319":"code","3aa210d5":"code","49f83851":"code","e33e5d30":"code","37b725d9":"code","ec276317":"code","9dd56275":"code","43ac99e6":"code","4ba43638":"code","0ee7318a":"code","7201ab17":"code","2cb8815a":"code","ad0e9941":"code","bb032815":"code","f3703573":"code","4f742725":"code","b45df325":"code","e54224e5":"code","64b10116":"markdown","a1cc7770":"markdown","afddca01":"markdown","f4eadcda":"markdown","144326ea":"markdown","34d9dede":"markdown","bfe607e4":"markdown","4d0b7a24":"markdown","3fd9a1c9":"markdown","de2878e2":"markdown","8404ff53":"markdown"},"source":{"c36e3b90":"import numpy as np \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\n\nfrom sklearn.metrics import mean_squared_error, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\n\n# import xgboost\n# import lightgbm as lgb\n# from lightgbm import LGBMClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\npd.set_option('max_rows', 300)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 300)\nnp.random.seed(566)\npd.set_option('display.max_rows', 200)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:20,.2f}'.format)\npd.set_option('display.max_colwidth', -1)","4c61395c":"TARGET_COL = \"hospital_death\"","3b968597":"df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/training_v2.csv\")\nprint(df.shape)\ndisplay(df.nunique())\ndf.head()","a2d8141d":"df.isna().sum()","32134319":"df.describe()","3aa210d5":"test = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/unlabeled.csv\")\nprint(test.shape)\ndisplay(test.nunique())\ntest.head()","49f83851":"test.isna().sum()","e33e5d30":"print([c for c in df.columns if 7<df[c].nunique()<800])\n## \n# categorical_cols = ['hospital_id','apache_3j_bodysystem', 'apache_2_bodysystem',\n# \"hospital_admit_source\",\"icu_id\",\"ethnicity\"]","37b725d9":"## print non numeric columns : We may need to\n## define them as categorical \/ encode as numeric with label encoder, depending on ml model used\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","ec276317":"categorical_cols =  ['hospital_id',\n 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']\n\n#['apache_3j_bodysystem', 'apache_2_bodysystem',\n# \"hospital_admit_source\",\"icu_id\",\"ethnicity\"]","9dd56275":"display(df[categorical_cols].dtypes)\ndisplay(df[categorical_cols].tail(3))\ndisplay(df[categorical_cols].isna().sum())","43ac99e6":"df[categorical_cols] = df[categorical_cols].fillna(\"\")\n\n# same transformation for test data\ntest[categorical_cols] = test[categorical_cols].fillna(\"\")\n\ndf[categorical_cols].isna().sum()","4ba43638":"## useful \"hidden\" function - df._get_numeric_data()  - returns only numeric columns from a pandas dataframe. Useful for scikit learn models! \n\nX_train = df.drop([TARGET_COL],axis=1)\ny_train = df[TARGET_COL]","0ee7318a":"## catBoost Pool object\ntrain_pool = Pool(data=X_train,label = y_train,cat_features=categorical_cols,\n#                   baseline= X_train[\"\"], ## \n#                   group_id = X_train['hospital_id']\n                 )\n\n### OPT\/TODO:  do train test split for early stopping then add that as an eval pool object : ","7201ab17":"model_basic = CatBoostClassifier(verbose=False,iterations=50)#,learning_rate=0.1, task_type=\"GPU\",)\nmodel_basic.fit(train_pool, plot=True,silent=True)\nprint(model_basic.get_best_score())","2cb8815a":"### hyperparameter tuning example grid for catboost : \ngrid = {'learning_rate': [0.04, 0.1],\n        'depth': [7, 11],\n#         'l2_leaf_reg': [1, 3,9],\n#        \"iterations\": [500],\n       \"custom_metric\":['Logloss', 'AUC']}\n\nmodel = CatBoostClassifier()\n\n## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`\ngrid_search_result = model.grid_search(grid, \n                                       train_pool,\n                                       plot=True,\n                                       refit = True, #  refit best model on all data\n                                      partition_random_seed=42)\n\nprint(model.get_best_score())","ad0e9941":"print(\"best model params: \\n\",grid_search_result[\"params\"])","bb032815":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score > 0.05:\n        print('{0}: {1:.2f}'.format(name, score))","f3703573":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_pool)\n\n# visualize the training set predictions\n# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \nshap.force_plot(explainer.expected_value,shap_values[0,:400], X_train.iloc[0,:400])","4f742725":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X_train)","b45df325":"test[TARGET_COL] = model.predict(test.drop([TARGET_COL],axis=1),prediction_type='Probability')[:,1]","e54224e5":"test[[\"encounter_id\",\"hospital_death\"]].to_csv(\"submission.csv\",index=False)","64b10116":"## Load train\/test data\n* If doing feature engineering, we could combine the 2 dataframes together then split them after. But we'll go for a naive approach in this kernel","a1cc7770":"* As we would expect, age is important. \n* The top features are precalculated predictors of risk of death (which likely take age into account). \n\n* We see that there's a difference between hospitals , although it's not an especially clear or linear feature. One explanation may be differences in skill of departments\/doctors inside each hospital, with these \"latent\"\/hidden variables interacting with other factors in our dataset","afddca01":"## Build & Tune catboost\/GBM models\n* We could make a validation subset for early stopping - allowing us to more easily tune our models hyperparameters\n*  we can go with a defualt model for now - it gets good results ,as we'll see\n\n* We can run on the GPU, giving a speed boost - to do this modify the kaggle kernel to use the GPU, and in the `fit` parameters set `task_type = \"GPU\"`\n\n* Catboost and lgbm have `Pool`\/`Dataset` objects, that can be used \"internally\" by them for some functions, e.g. to efficienctly CV\n\n","f4eadcda":"# Get predictions on test set and export for submission\n\n* You can \"ensemble\"\/average the predictions from the 2 catboost models as a quick improvement , even if there isn't much diversity added","144326ea":"### Hyperparameter search\n* We can do a gridsearch for best hyperparameters, such as learning rate, etc' \n     * Another improvement: split evaluation set from train , and use it for early stopping + tun\n      * I leave this to the reader :)  ","34d9dede":"### if using cartboost or lgbm, we can define categorical variables\n\n* catboost hyperparam tuning : https:\/\/colab.research.google.com\/github\/catboost\/tutorials\/blob\/master\/python_tutorial.ipynb#scrollTo=nSteluuu_mif\n\n\n* We see that many clearly continous numeric variables have relatively low cardinality (e.g. icu sensor readings) - making it tricky to define them automatically. \n* Categorical columns are not necessarily string columns, could be numerical - e.g. hospital codes.. \n","bfe607e4":"* We Fill in empty string for missing  values in the string columns , otherwise catboost will give an error - \"CatBoostError: Invalid type for cat_feature: cat_features must be integer or string, real number values and NaN values should be converted to string.\"\n","4d0b7a24":"* `encounter_id ,\tpatient_id\t` are unique. We could probably drop, but there may be leaks from them so we'll keep (And we need them for the submission)\n* Their being unique means they are not candidates for generating historical features from them per patient. \n    * We could try the hospital ID or surgery type or apache codes for that purpose","3fd9a1c9":"## Train a basic model","de2878e2":"## Train model(s)","8404ff53":"## Features importances\n\n* What are the most important features for predicting death? \n\n* We also look also at Shapley values : https:\/\/github.com\/slundberg\/shap\n    * Shap + Catboost tutorial :  https:\/\/github.com\/slundberg\/shap\/blob\/master\/notebooks\/tree_explainer\/Catboost%20tutorial.ipynb\n      \n"}}