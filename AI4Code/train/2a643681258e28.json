{"cell_type":{"e6e12671":"code","e208982a":"code","cb5893e2":"code","e5a7112e":"code","c62e2849":"code","1e2d3e41":"code","2d0afea9":"code","b3ccd5ec":"code","6adccd00":"code","2c69c3da":"code","74fea0e2":"code","3c67bfea":"code","9cea29d7":"code","6d7eb4ee":"code","cf3d772c":"code","69b9d472":"code","ba744fcb":"code","a3305958":"code","75aa9a9f":"code","99ad71d1":"code","a4b96c7f":"code","c3e601d3":"code","f5ffb263":"code","1dcd4fac":"code","bf42386e":"code","1e3ec58d":"code","f9c98069":"code","13e4e2e2":"code","7b025f48":"code","8f40ac27":"code","3b060c1e":"code","9139e544":"code","c2d8d97b":"code","9b6739e1":"code","6103d7ad":"code","2108c2c2":"code","1bde205e":"code","8ccb7351":"code","64d591d3":"code","333f34b0":"code","2d6d0bd9":"code","7b1fcc95":"code","1f9e99d7":"code","4278fbba":"markdown","949666e1":"markdown","fbba9ec0":"markdown","072706af":"markdown","b3b67996":"markdown","c4158905":"markdown","dda93d3f":"markdown","8d99a3a1":"markdown","2cc0c7bb":"markdown","cc0683f7":"markdown","4f14a82f":"markdown","8db659a3":"markdown","8a3ec7a2":"markdown","4567a944":"markdown","8d68f7cb":"markdown","821545e2":"markdown","c813ffbe":"markdown","f2313ddd":"markdown","1ea8399c":"markdown","1ce1cc7a":"markdown","154d0ad6":"markdown"},"source":{"e6e12671":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nSEED = 69420\nFILE_PATH = '..\/input\/used-car-dataset-ford-and-mercedes\/ford.csv'\n\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n","e208982a":"df = pd.read_csv(FILE_PATH)","cb5893e2":"df","e5a7112e":"df = df.sort_values('year')","c62e2849":"df","1e2d3e41":"df = df.iloc[:-1, :]","2d0afea9":"df","b3ccd5ec":"nullvaluecheck = pd.DataFrame(df.isna().sum().sort_values(ascending=False)*100\/df.shape[0],columns=['missing %']).head(60)\nnullvaluecheck.style.background_gradient(cmap='PuBu')","6adccd00":"plt.figure(figsize=(10, 8))\nax = sns.boxplot(data = df, x='year',y='price')\n# plt.yscale('log')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)","2c69c3da":"years = df.year.unique()\nfor year in years:\n    print(f\"{year}\")\n    stats = df[df['year']==year].describe()\n    print(f\"Number of Prices for {year} is {stats.iloc[0,1]}\")","74fea0e2":"df = df.iloc[498:]","3c67bfea":"df.year.unique()","9cea29d7":"sns.displot(x=df.price, hue=df.year, kind='kde', palette=sns.color_palette()[:8], aspect=2)\nplt.title('KDEPlot of all Models Prices')\nplt.show()","6d7eb4ee":"ax = sns.countplot(df.model)\nplt.title('Countplot of all Models')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)\n\nprint(df.model.value_counts())","cf3d772c":"models = list(df.model.value_counts().index[:4])\nmodels","69b9d472":"df = df[df.model.isin(models)]","ba744fcb":"df['model'].value_counts()","a3305958":"sns.displot(x=df.price, hue=df.year, kind='kde', palette=sns.color_palette()[:8], aspect=2)\nplt.title('KDEPlot of Selected Model Prices')\nplt.show()","75aa9a9f":"ax = sns.countplot(df.model)\nplt.title('Countplot of all Models')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)\n\nprint(df.model.value_counts())","99ad71d1":"plt.figure(figsize=(10, 8))\nax = sns.boxplot(data = df, x='year',y='price')\nplt.title('BoxPlot of Selected Model Prices Yearly')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)","a4b96c7f":"df = df[df['price'] <= 35000]","c3e601d3":"plt.figure(figsize=(10, 8))\nax = sns.boxplot(data = df, x='year',y='price')\nplt.title('BoxPlot of Selected Model Prices Yearly')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)","f5ffb263":"sns.pairplot(df)","1dcd4fac":"corr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr,\n            mask=mask,\n            cmap='PuBu',\n            square=True,\n            linewidths=.5,\n            annot=True)\nplt.show()","bf42386e":"y = df['price'].values\ndf.drop('price', axis=1, inplace=True)\nX = df","1e3ec58d":"X.shape, y.shape","f9c98069":"df","13e4e2e2":"cols_to_encode = list(X.columns[X.dtypes == 'object'])\ncols_to_encode.append('year')\ncols_to_encode","7b025f48":"# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","8f40ac27":"MCLE = MultiColumnLabelEncoder(columns = cols_to_encode)\n\nX = MCLE.fit_transform(X)","3b060c1e":"from sklearn.model_selection import train_test_split\n\n# 60 20 20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)\n\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","9139e544":"X_train.to_csv('X_train.csv')\nX_test.to_csv('X_test.csv')\nX_val.to_csv('X_val.csv')\n\npd.Series(y_train).to_csv('y_train.csv')\npd.Series(y_test).to_csv('y_test.csv')\npd.Series(y_val).to_csv('y_val.csv')","c2d8d97b":"import optuna \nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom statistics import mean","9b6739e1":"def objective(trial: Trial, X, y) -> float:\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)\n    evals = [(X_val, y_val)]\n    \n    # Assign Parameter Dict\n    param = {\n                \"n_estimators\":trial.suggest_int('n_estimators', 0, 1000),\n                'max_depth':trial.suggest_int('max_depth', 2, 25),\n                'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n                'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n                'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n                'gamma':trial.suggest_int('gamma', 0, 5),\n                'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n                'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01)\n            }\n    \n    # Build Model\n    model = XGBRegressor(**param,\n                         predictor = 'gpu_predictor',\n                         tree_method = 'gpu_hist',\n                         eval_metric = 'rmse',\n                         verbosity=1)\n    \n    # Fit Model\n    model.fit(X_train, y_train, eval_set = evals, eval_metric = 'rmse', early_stopping_rounds = 10)\n    \n    # Predict\n    y_pred = model.predict(X_test)\n    y_valpred = model.predict(X_val)\n    \n    # Compute Metrics\n    test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n    val_rmse = mean_squared_error(y_val, y_valpred, squared=False)\n    \n    return mean((val_rmse, test_rmse))","6103d7ad":"study = optuna.create_study(study_name='CarPriceRegression',\n                            direction='minimize',\n                            sampler=TPESampler())","2108c2c2":"%%time\n# To conserve computing time I have limited the trials here to 10\nstudy.optimize(lambda trial : objective(trial, X, y),\n               n_trials= 100,\n               n_jobs=-1,\n               gc_after_trial=True,\n               show_progress_bar=True)\n\nprint('Best trial: RMSE {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","1bde205e":"hist = study.trials_dataframe()\nhist.head()","8ccb7351":"# Deselect Objective Value to see the curve more clearly!\nplot_optimization_history(study)","64d591d3":"plot_param_importances(study)","333f34b0":"print('Best trial: RMSE {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","2d6d0bd9":"clf = XGBRegressor(**study.best_trial.params,\n                   predictor = 'gpu_predictor',\n                   random_state = SEED,\n                   verbosity=0)\n\nevals = [(X_val, y_val)]","7b1fcc95":"%%time\nclf.fit(X_train, y_train,\n        eval_set = evals,\n        eval_metric = 'rmse',\n        early_stopping_rounds = 10)","1f9e99d7":"# Predict\ny_pred = clf.predict(X_test)\n\n# Compute Metrics\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(f\"Final Model RMSE: {test_rmse}\")","4278fbba":"Yikes! Imagine trying to model an entire years worth of prices based on only 115 sales!\n\nWe are going to only take the years 2013 onwards from hereon","949666e1":"Still we can clearly see outliers! I will clip the data to < 35k to clean them","fbba9ec0":"# **Special Encoder Class**","072706af":"# Next Steps?\n\n* Use Optuna to tune OVR CrossValidated Classifier\n\n* Use different types of Boosting\n\n* User different ML Algorithms\n\n* KNN Feature Generations\n\n* Deep Learning Neural Nets\n\n* AutoML Libraries\n\n**If you enjoyed this please upvote to reach others!**","b3b67996":"This plot shows us that apart from 2019 (when Ford Scandal occured) the data generally belongs to the same distribution just with a greater magnitude","c4158905":"**Visualise Quantiles Over Time**\n\nThis helps us understand how the distribution of price has changed over time","dda93d3f":"# Import Libraries","8d99a3a1":"**How Many Values for each year?**","2cc0c7bb":"# Train Full Model","cc0683f7":"**Yikes again! We are only going to model the top 4 models**","4f14a82f":"# Model Prep","8db659a3":"Obviously 2060 is a misentry so we remove it","8a3ec7a2":"# Final Results...","4567a944":"Lots of outliers! But price has been generally trending upwards (aside from 2008 ofc where there was the financial crisis thus making the wick massive","8d68f7cb":"**THE TEST SETS ARE HOLDOUT - NO PART OF MODELLING, ONLY EVALUATION - USE VAL FOR TUNING**","821545e2":"**Sanity Check!**","c813ffbe":"# Model Building\n\nFinally! I will publish the cleaned X, y sets for those who want to use them for an easy comparison between models","f2313ddd":"**Sanity Check**","1ea8399c":"# EDA on Cleaned DF","1ce1cc7a":"# Optuna Tuning XGBoost","154d0ad6":"# Simple EDA"}}