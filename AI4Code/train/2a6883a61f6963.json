{"cell_type":{"50d0a008":"code","6d66e999":"code","1c9a342e":"code","58409df5":"code","57edc3c0":"code","f1b0ce8a":"code","00fec5a9":"code","523b9d49":"code","c0e3fa95":"code","0796986e":"code","399ba98d":"code","a283182f":"code","a8d9b259":"code","74c9624f":"code","4c4bc709":"code","ecdaac10":"code","4373f63e":"code","07751bcf":"code","a3ab4e10":"code","6ae979c3":"markdown","72b0b504":"markdown","a57b03ca":"markdown","e5273935":"markdown","5f78bd9b":"markdown"},"source":{"50d0a008":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n# !cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n# !cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n# !cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","6d66e999":"import os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\nfrom abc import abstractmethod, ABCMeta\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\n# from cuml import LogisticRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nfrom time import time","1c9a342e":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on = 'sig_id', how = 'left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","58409df5":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss_krr = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss_lr = ss_krr.copy()\n\ncols = [c for c in ss_krr.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","57edc3c0":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 0.5, 72: 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","f1b0ce8a":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(n_quantiles = 100, output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","00fec5a9":"from sklearn.decomposition import PCA\n\n# GENES\nn_comp_genes = 600\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_genes = PCA(n_components=n_comp_genes, random_state = 42)\ndata2 = pca_genes.fit_transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)\n\n#CELLS\nn_comp_cells = 50\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_cells = PCA(n_components=n_comp_cells, random_state = 42)\ndata2 = pca_cells.fit_transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","523b9d49":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)","c0e3fa95":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","0796986e":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","399ba98d":"train.head()","a283182f":"N_SPLITS = 7\nLBS = 0.0008\nfolds = create_folds(1, N_SPLITS)\nprint(folds)","a8d9b259":"res_krr = train_targets.copy()\nss_krr.loc[:, train_targets.columns] = 0\nres_krr.loc[:, train_targets.columns] = 0\n\n# for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True).split(train_targets, train_targets)):\nfor n, foldno in enumerate(set(folds[0])):\n    start_time = time()\n    tr = folds[0] != foldno\n    te = folds[0] == foldno\n\n    x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n    y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n    x_tt = test.values[:, top_feats]\n    \n    # Label Smoothing\n    y_tr = y_tr * (1 - LBS) + 0.5 * LBS\n    \n    model = KernelRidge(alpha = 80, kernel = 'rbf')\n    model.fit(x_tr, y_tr)\n\n    ss_krr.loc[:, train_targets.columns] += model.predict(x_tt) \/ N_SPLITS\n    fold_pred = model.predict(x_val)\n    res_krr.loc[te, train_targets.columns] += fold_pred\n    fold_score = log_loss_metric(train_targets.loc[te].values, fold_pred)\n    print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] KRR: Fold {n}:', fold_score)","74c9624f":"print(f'Model OOF Metric: {log_loss_metric(train_targets.values, res_krr.values)}')","4c4bc709":"X_new = res_krr[cols].values\nx_tt_new = ss_krr[cols].values","ecdaac10":"res_lr = train_targets.copy()\nss_lr.loc[:, train_targets.columns] = 0\nres_lr.loc[:, train_targets.columns] = 0\n\nfor tar in tqdm(range(train_targets.shape[1])):\n\n#     start_time = time()\n    targets = train_targets.values[:, tar]\n\n    if targets.sum() >= N_SPLITS:\n\n        skf = StratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n\n        for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n            x_tr, x_val = X_new[tr, tar].reshape(-1, 1), X_new[te, tar].reshape(-1, 1)\n            y_tr, y_val = targets[tr], targets[te]\n\n            model = LogisticRegression(penalty = 'none', max_iter = 1000)\n            model.fit(x_tr, y_tr)\n            ss_lr.loc[:, train_targets.columns[tar]] += model.predict_proba(x_tt_new[:, tar].reshape(-1, 1))[:, 1] \/ N_SPLITS\n            res_lr.loc[te, train_targets.columns[tar]] += model.predict_proba(x_val)[:, 1]\n\n    score = log_loss(train_targets.loc[:, train_targets.columns[tar]].values, res_lr.loc[:, train_targets.columns[tar]].values)\n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] LR Target {tar}:', score)","4373f63e":"print(f'LR OOF Metric: {log_loss_metric(train_targets.values, res_lr.values)}')","07751bcf":"np.save('klr_oof.npy', res_lr[cols].values)\nnp.save('klr_sub.npy', ss_lr[cols].values)","a3ab4e10":"ss_lr.loc[test['cp_type'] == 1, train_targets.columns] = 0\nss_lr.to_csv('submission.csv', index = False)","6ae979c3":"# Kernel Logistic Regression","72b0b504":"# Kernel Logistic Regression\n\n* **Version 1-3:** Nystroem Kernel Approximation + Logistic Regression\n* **Version 4:** Kernel Ridge Regression + Platt Scaling\n* **Version 5:** Kernel Ridge Regression + Platt Scaling + Tuned Hyperparameter C's\n* **Version 6:** Kernel Ridge Regression + Inter-Target Platt Scaling (CPU) The training time is too long so I cancelled it.\n* **Version 7:** Kernel Ridge Regression + Inter-Target Platt Scaling (GPU)\n* **Version 8:** Kernel Ridge Regression + Platt Scaling (CPU) + Remove Control Group\n* **Version 9-13:** GroupCV + Quantile Transformation + Label Smoothing\n* **Version 14:** Add PCA and VarianceThreshold Feature Selection\n\nIn this example, I play with the kernel logistic regression method. Scikit-Learn does not have kernel logistic regression. Instead, I use kernel ridge regression and platt scaling. According to the [Kernel Ridge Regression][1] document on Scikit-Learn, It should perform as well as SVR.\n\nP.S. The inter-target Platt Scaling means I consider target relationships during Platt Scaling.\n\n[1]: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge","a57b03ca":"# Data Preparation","e5273935":"# Platt Scaling\n\nTrain a Logistic Regression model to calibrate the results of Kernel Ridge Regression.","5f78bd9b":"# Submit"}}