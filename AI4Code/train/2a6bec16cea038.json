{"cell_type":{"89a17853":"code","d34c5028":"code","267cf2ba":"code","ad0bc255":"code","d2deef1f":"code","6f9c439e":"code","cf9bf82f":"code","c3c00948":"code","ccfea4cc":"code","ea2379fe":"code","60c2f1e7":"code","6e289ac8":"code","25621175":"code","ba74ac0a":"code","a896fde3":"code","50f14f07":"code","ae88f902":"code","d0f4d269":"code","39ef7f65":"code","a36d3cb1":"code","8e3494de":"code","dcab33f3":"code","bd85e345":"code","056f81e0":"code","1f993d4b":"code","b1a5da64":"code","c2f411bd":"code","302ee104":"code","aa90bb51":"code","d8fde9a2":"code","fcbf097e":"code","9d5747c3":"code","0f96d77c":"code","1d7eba51":"code","089a5748":"code","baf617fc":"code","1311d0be":"code","1bd5d5b2":"code","1ffeaf16":"markdown","2f28d483":"markdown","c56ec1f7":"markdown","5830cf81":"markdown","1329e5fc":"markdown","8f43e1f6":"markdown","685d8fc5":"markdown","32888ae7":"markdown","a815e12d":"markdown","18cb6360":"markdown","364be2a8":"markdown","37273c92":"markdown","758379d6":"markdown","17cb1cfd":"markdown","98c92662":"markdown","5e60c33e":"markdown","56aa87e8":"markdown","83ce0c2b":"markdown","56d53748":"markdown","91abe59c":"markdown","7be21772":"markdown","83f02aa0":"markdown","aed4f632":"markdown","ff7ea4d6":"markdown","18c475a4":"markdown","401e0c14":"markdown","a04f508f":"markdown","d2249987":"markdown","cad525f5":"markdown","595b18a1":"markdown","c74dc3a9":"markdown","d8bcb367":"markdown","b301a51c":"markdown","c9d4c7c1":"markdown","c1fb636f":"markdown","c1f3a84e":"markdown","9f5ee68d":"markdown","c2525a62":"markdown"},"source":{"89a17853":"import numpy as np\nimport pandas as pd\nimport imblearn #for SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d34c5028":"train = pd.read_csv(\"\/kaggle\/input\/term-deposit-customer-data\/train.csv\")\ntrain.head()","267cf2ba":"train.shape","ad0bc255":"train.isna().sum()","d2deef1f":"train.dtypes","6f9c439e":"train[\"job\"].value_counts()","cf9bf82f":"eduRatio = pd.DataFrame({'Job' : []})\nfor i in train[\"job\"].unique():\n    eduRatio = eduRatio.append(train[(train[\"job\"] == i)][\"education\"].value_counts().to_frame().iloc[0] * 100 \/ train[(train[\"job\"] == i)][\"education\"].value_counts().sum())\neduRatio[\"Job\"] = train[\"job\"].unique()\neduRatio","c3c00948":"train.loc[(train.job == \"unknown\") & (train.education == \"secondary\"),\"job\"] = \"services\"\ntrain.loc[(train.job == \"unknown\") & (train.education == \"primary\"),\"job\"] = \"housemaid\"\ntrain.loc[(train.job == \"unknown\") & (train.education == \"tertiary\"),\"job\"] = \"management\"\ntrain.loc[(train.job == \"unknown\"),\"job\"] = \"blue-collar\"","ccfea4cc":"train[\"job\"].value_counts()","ea2379fe":"train[\"marital\"].value_counts(), train[\"education\"].value_counts()","60c2f1e7":"train.loc[(train.education == \"unknown\") & (train.job == \"admin.\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"management\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"services\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"technician.\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"retired\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"blue-collar\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"housemaid.\"),\"education\"] = \"primary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"self-employed\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"student\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"entrepreneur\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"unemployed\"),\"education\"] = \"secondary\"\n#REST CAN BE SECONDARY\ntrain.loc[(train.education == \"unknown\"),\"education\"] = \"secondary\"","6e289ac8":"train[\"education\"].value_counts()","25621175":"train[\"default\"].value_counts(), train[\"housing\"].value_counts(), train[\"contact\"].value_counts()","ba74ac0a":"train[\"contact\"].replace([\"unknown\"],train[\"contact\"].mode(),inplace = True) # I replace unknown contact values with mode value","a896fde3":"train[\"y\"].value_counts()","50f14f07":"train.drop(columns = [\"day\",\"month\"], inplace = True)","ae88f902":"train.drop(columns = [\"duration\"],inplace = True)","d0f4d269":"#OneHotEncoding of job column\nohe = OneHotEncoder(sparse = False)\ntrain = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"job\"].to_frame()),columns = \"job_\" + np.sort(train[\"job\"].unique()))),axis = 1)\ntrain.drop(columns = [\"job\"],inplace = True)\n\n#Marital column has 3 values lets apply OneHotEncoding again.\ntrain = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"marital\"].to_frame()),columns = \"marital_\" + np.sort(train[\"marital\"].unique()))),axis = 1)\ntrain.drop(columns = [\"marital\"],inplace = True)\n\n#poutcome\ntrain = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"poutcome\"].to_frame()),columns = \"poutcome_\" + np.sort(train[\"poutcome\"].unique()))),axis = 1)\ntrain.drop(columns = [\"poutcome\"],inplace = True)","39ef7f65":"train.head()","a36d3cb1":"train.loc[(train.education == \"tertiary\"),\"education\"] = 2\ntrain.loc[(train.education == \"secondary\") ,\"education\"] = 1\ntrain.loc[(train.education == \"primary\"),\"education\"] = 0","8e3494de":"#Default column\ntrain.loc[(train.default == \"yes\"),\"default\"] = 1\ntrain.loc[(train.default == \"no\") ,\"default\"] = 0\n\n#Housing column\ntrain.loc[(train.housing == \"yes\"),\"housing\"] = 1\ntrain.loc[(train.housing == \"no\") ,\"housing\"] = 0\n\n#Loan column label encoding\ntrain.loc[(train.loan == \"yes\"),\"loan\"] = 1\ntrain.loc[(train.loan == \"no\") ,\"loan\"] = 0\n\n#Contact column label encoding\ntrain.loc[(train.contact == \"telephone\"),\"contact\"] = 1 # 0 means cellular 1 means telephone\ntrain.loc[(train.contact == \"cellular\") ,\"contact\"] = 0","dcab33f3":"train.balance.sort_values()","bd85e345":"upper_quantile = train.balance.quantile(0.99)\nlower_quantile = train.balance.quantile(0.01)\n\n#Remove customers that are above and under\ntrain_filtered = train[(train.balance < upper_quantile) & (train.balance > lower_quantile)]\nlen(train), len(train_filtered)","056f81e0":"max_value = train_filtered[\"balance\"].max()\nmin_value = train_filtered[\"balance\"].min()\ntrain_filtered[\"balance\"] = (train_filtered[\"balance\"] - min_value) \/ (max_value - min_value)\ntrain = train_filtered\ntrain[\"balance\"].head()","1f993d4b":"train.loc[(train.pdays == -1),\"pdays\"] = 999","b1a5da64":"train.loc[(train.y == \"yes\"),\"y\"] = 1 # 0 means subscribed no 1 means yes\ntrain.loc[(train.y == \"no\") ,\"y\"] = 0","c2f411bd":"train.info()","302ee104":"train.education = train.education.astype(int)\ntrain.default = train.default.astype(int)\ntrain.housing = train.housing.astype(int)\ntrain.loan = train.loan.astype(int)\ntrain.contact = train.contact.astype(int)\ntrain.y = train.y.astype(int)\ntrain.info()","aa90bb51":"plt.figure(figsize=(20,10))\nsns.heatmap(train.corr(),annot = True)","d8fde9a2":"y = train.y.to_frame()\nX = train.drop(columns = [\"y\"])\nX_train , X_test , y_train , y_test = train_test_split(X,y, test_size = 0.25, random_state = 10)","fcbf097e":"lr = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr.fit(X_train, y_train.values.ravel())\ny_predlr = lr.predict(X_test)\ncmlr = confusion_matrix(y_test, y_predlr)\nacclr = accuracy_score(y_test, y_predlr)\ncmlr , acclr","9d5747c3":"sm = SMOTE()\nX_sm , y_sm = sm.fit_resample(X, y)\ny_sm.y.value_counts()","0f96d77c":"X_train_sm , X_test_sm , y_train_sm , y_test_sm = train_test_split(X_sm,y_sm, test_size = 0.25, random_state = 10)\nlr2 = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr2.fit(X_train_sm,y_train_sm.values.ravel())\ny_predlr2 = lr2.predict(X_test_sm)\ncmlr2 = confusion_matrix(y_test_sm, y_predlr2)\nacclr2 = accuracy_score(y_test_sm, y_predlr2)\ncmlr2 , acclr2","1d7eba51":"svc = SVC()\nsvc.fit(X_train_sm, y_train_sm.values.ravel())\ny_predsvc = svc.predict(X_test_sm)\ncmsvc = confusion_matrix(y_test_sm, y_predsvc)\naccsvc = accuracy_score(y_test_sm, y_predsvc)\ncmsvc , accsvc","089a5748":"knn = KNeighborsClassifier()\nknn.fit(X_train_sm, y_train_sm.values.ravel())\ny_predknn = knn.predict(X_test_sm)\ncmknn = confusion_matrix(y_test_sm, y_predknn)\naccknn = accuracy_score(y_test_sm, y_predknn)\ncmknn , accknn","baf617fc":"rf = RandomForestClassifier()\nrf.fit(X_train_sm, y_train_sm.values.ravel())\ny_predrf = rf.predict(X_test_sm)\ncmrf = confusion_matrix(y_test_sm, y_predrf)\naccrf = accuracy_score(y_test_sm, y_predrf)\ncmrf , accrf","1311d0be":"print(f\"Logistic Regression accuracy without SMOTE :{acclr * 100} Sensivity :{cmlr[0,0] * 100 \/ (cmlr[0,0] + cmlr[0,1])} Specificity : {cmlr[1,1] * 100 \/ (cmlr[1,1] + cmlr[1,0])}\")\nprint(f\"Logistic Regression accuracy with SMOTE :{acclr2 * 100} Sensivity :{cmlr2[0,0] * 100 \/ (cmlr2[0,0] + cmlr2[0,1])} Specificity : {cmlr2[1,1] * 100 \/ (cmlr2[1,1] + cmlr2[1,0])}\")\nprint(f\"Support Vector Classifier accuracy with SMOTE :{accsvc * 100} Sensivity :{cmsvc[0,0] * 100 \/ (cmsvc[0,0] + cmsvc[0,1])} Specificity : {cmsvc[1,1] * 100 \/ (cmsvc[1,1] + cmsvc[1,0])}\")\nprint(f\"K Nearest Neighbors Classfier accuracy with SMOTE :{accknn * 100} Sensivity :{cmknn[0,0] * 100 \/ (cmknn[0,0] + cmknn[0,1])} Specifictiy : {cmknn[1,1] * 100 \/ (cmknn[1,1] + cmknn[1,0])}\")\nprint(f\"Random Forest Classifier accuracy with SMOTE :{accrf * 100} Sensivity :{cmrf[0,0] * 100 \/ (cmrf[0,0] + cmrf[0,1])} Specificity : {cmrf[1,1] * 100 \/ (cmrf[1,1] + cmrf[1,0])}\")","1bd5d5b2":"import pickle\n\nfilename = 'random_forests.sav'\npickle.dump(rf, open(filename, 'wb'))\n\nfilename2 = 'knn.sav'\npickle.dump(knn, open(filename2, 'wb'))","1ffeaf16":"Let's check the other columns, \"marital\" status and \"education\" level.","2f28d483":"# Saving the Model for Offline Predictions","c56ec1f7":"Finally, we transform our desired output column \"y\" into 1.0, which means subscribed, and 0.0 which means did not subscribe.","5830cf81":"Looking at out \"y\" label column, the highest correlation is 0.31 - which is of moderate correlation. \n\nGenerally speaking, we want to avoid including features that are highly correlated with one another to avoid redundancy and to reduce the number of features we have overall.\n\n## [Slide 18-19: A Recap of the Data Transformations Performed]","1329e5fc":"We see that 288 of the rows are marked as \"unknown\".  We could choose to remove those customer records from our training dataset completely, but instead we are going to **impute** those values, i.e. substitute them with an estimated value.\n\nBut what would be an appropriate way to substitute \"unknown\" values?  We might turn to an SME, but here we are going to replace \"unknown\" with the most common job according to that person's education level.\n\nLet's find out what is the most common job for each education level.","8f43e1f6":"Let's look at the \"job\" column","685d8fc5":"It seems like \"day\" and \"month\" are already captured by the \"pdays\" column, so let's drop them.","32888ae7":"The \"marital\" column is fine.  We can fill in \"unknown\" education levels based on the person's \"job\".","a815e12d":"The \"unknown\" values for \"contact\" will be replaced with \"cellular\".","18cb6360":"## Changing Object Data Types to Numerical Ones\nBefore training we should transform object dtypes to int because some classifiers won't work with object dtype.","364be2a8":"We probably have outlier data - people with much higher and lower balances than normal.  We can get rid of these samples from our training dataset.","37273c92":"# 2. Data Preprocessing\n## Filling NaN (unknown) Values\nLet's look at whether there are any \"empty\" values in our dataset","758379d6":"## Removing Outlier Data\nNow lets look at the numerical value, account \"balance\":","17cb1cfd":"Looking at the \"job\" column again, we no longer have any customers with \"unknown\" jobs.","98c92662":"The duration column is 0 before a call is performed, so if we have never called this person before, we won't have any data.  Drop this column.","5e60c33e":"Without any hyperparameter tuning Random Forest Classifier gave us best result. Before SMOTE our Specificity values was low around 14%. \n\nAfter SMOTE, maybe accuracy of classifications seems a little bit low. But evaluate results with just \"accuracy\" won't prove anything. Sensitivity decreased with SMOTE but specificity increased 10 times.","56aa87e8":"And what about categorical values that only have \"yes\" and \"no\"?  Those can simply be transformed into 1.0 and 0.0 respectively.","83ce0c2b":"## Visualizing Correlation\n\nAfter all of our data transformations, let's plot a heatmap to see what the correlation between all of our features look like.","56d53748":"Notice that now we have 11 \"job\" columns and 3 \"marital\" columns with potential values of 1.0 for \"yes\" and 0.0 for \"no\".\n\nBut what about \"education\", which is also a categorical column?  Education has an inherent ordering, i.e. we must finish primary school before secondary school, followed by tertiary school.  So we can transform these \"ordinal\" columns by using sequentially increasing numbers.","91abe59c":"Looking at the columns \"default\", \"housing\" and \"contact\":","7be21772":"## Logistic Regression\nNext, let's create one of the most basic Machine Learning models - Logistic Regression","83f02aa0":"It seems that we are lucky and that we don't have empty values. However, we do have something similar to empty values, and those are cells marked with the text \"unknown\".","aed4f632":"We can see that the dataset is imbalanced - there are many more examples of \"no\" than \"yes\".  We will address this imbalance later on by augmenting the training data using SMOTE (Synthetic Minority Oversampling TEchnique).","ff7ea4d6":"## Changing Data to Fit our Case \nThe \"pdays\" column indicates how many days have passed since we last contacted this customer.  If we have never contacted this customer before, it is set to -1.  Change this to a large value instead.","18c475a4":"## K-Nearest Neighbours (KNN)","401e0c14":"> ## [Slide 15: Feature Selection]\n\nLet's revisit the data columns and intuitively see whether they make sense to be included in the model of whether someone will subscribe to a term deposit:\n\n1 - age\n\n2 - job : type of job \n\n3 - marital : marital status \n\n4 - education\n\n5 - default: has credit in default? \n\n6 - balance : has money in account? \n\n7 - housing: has housing loan? \n\n8 - loan: has personal loan? \n\n9 - contact: contact communication type (categorical: \"cellular\",\"telephone\")\n\n**10 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")**\n\n**11 - day: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")**\n\n**12 - duration: last contact duration, in seconds (numeric)****\n\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"unknown\",\"other\",\"success\")\n","a04f508f":"It looks like we have ~89 percent accuracy. But this isn't the full picture.\n\n## [Slide 22-27: Confusion Matrix]\n\nOur Sensitivity is low and our Specificity is high.  But we probably want our Sensitivity to be high instead.\n\nThis is happening because we built the model using an imbalanced dataset. Let's balance the dataset with SMOTE.\n\n## Data Augmentation using SMOTE","d2249987":"# 3. Model Training\n\n## Splitting Data into Training and Testing Sets\nFirst, let's split the training dataset into training and test sets.","cad525f5":"* For \"tertiary\" education, the most common job is \"management\"\n* For \"secondary\" education, the most common job is \"services\"\n* For \"primary\" education, the most common job is \"housemaid\"\n* Note that we still have people with \"unknown\" education level. We can assign them to the most common job overall, which is \"blue-collar\"\n\nSo let's assign customers with \"unknown\" job a job according to their education level.","595b18a1":"## Logistic Regression after SMOTE","c74dc3a9":"This time our accuracy is around 76 percent. But as you can see our prediction improved at Specificity in the Confusion Matrix.\n\n## Support Vector Classifier","d8bcb367":"Next, scale down the \"balance\" column so that it is between zero and one.","b301a51c":"# Blue Prism IA Scenario 1\n## Subscribe to Service Classifier","c9d4c7c1":"## Random Forest Classifier","c1fb636f":"# 1. Dataset Description \n## [Slide 14 - Understand the Data]\n\n**Inputs (Bank Client Data):**\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n\n3 - marital : marital status (categorical: \"divorced\",\"married\",\"single\"; note: \"divorced\" means divorced or widowed)\n\n4 - education (categorical: \"primary\",\"secondary\",\"tertiary,\"unknown\")\n\n5 - default: has credit in default? (categorical: \"no\",\"yes\")\n\n6 - balance : has money in account? (numeric)\n\n7 - housing: has housing loan? (categorical: \"no\",\"yes\")\n\n8 - loan: has personal loan? (categorical: \"no\",\"yes\")\n\n9 - contact: contact communication type (categorical: \"cellular\",\"telephone\")\n\n10 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n\n11 - day: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n\n12 - duration: last contact duration, in seconds (numeric)\n\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"unknown\",\"other\",\"success\")\n\n**Output variable (desired target):**\n\n17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")","c1f3a84e":"## [Slide 16: Feature Engineering]\nHere, we will transform existing columns into different columns.\n\n## [Slide 17: Converting \"Text\" into \"Numbers\" using One Hot Encoding]\nCategorical text columns need to be converted into numeric columns. The list of categorical text columns that need transformation include:\n1. Job - which has 11 categories after getting rid of \"unknown\"\n2. Marital - which has 3 categories\n3. poutcome - which has 4 categories\n","9f5ee68d":"# 4. Model Evaluation\nSensitivity : True Positive \/ (True Positive + False Negative) , Specificity : True Negative \/ (True Negative + False Negative)****","c2525a62":"Finally, let's look at the column that we want to predict \"y\"."}}