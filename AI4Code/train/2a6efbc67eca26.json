{"cell_type":{"5e1f7c29":"code","a638da26":"code","9608d1f9":"code","612f0b11":"code","7345ddda":"code","8de66d6d":"code","014462f7":"code","98ce28c2":"code","32c845e6":"code","db76c50e":"code","a0909afd":"code","9559fd06":"code","50689693":"code","a6e645f2":"code","d8c3f7fc":"code","ab70459a":"code","d7646dab":"code","f615eb7c":"code","1eb97017":"code","4efa47c4":"code","3225c8ee":"markdown"},"source":{"5e1f7c29":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a638da26":"df=pd.read_csv('\/kaggle\/input\/travel-insurance\/travel insurance.csv')","9608d1f9":"df.head()","612f0b11":"df.shape","7345ddda":"# Renaming columns\ndf.rename(columns={ 'Agency Type':'Agency_Type', 'Distribution Channel':'Distribution_Channel', 'Product Name':'Product_Name',\n       'Net Sales':'Net_Sales', 'Commision (in value)':'Commision_in_value'\n       },inplace=True)","8de66d6d":"df.head()","014462f7":"#missing value check\ndf.isnull().sum().any","98ce28c2":"df.info()","32c845e6":"#check coorelation\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm')","db76c50e":"df['Agency_Type'].unique()","a0909afd":"\"\"\"\nChecking Distribution for categorical columns in dataset\n\n\"\"\"\n\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.countplot(df['Agency_Type'])\nplt.title('Agency_Type')\nplt.subplot(2,2,2)\nsns.countplot(df['Distribution_Channel'])\nplt.title('Distribution_Channel')\n\nplt.subplot(2,2,3)\nsns.countplot(df['Agency'])\nplt.xticks(rotation=90)\nplt.title('Agency')\nplt.subplot(2,2,4)\nsns.countplot(df['Gender'])\nplt.title('Gender')","9559fd06":"#checking distriubution for destination in dataset, Please ignore the mess at pie we can consider those as others\nplt.figure(figsize=(15,10))\n\nwedges, texts = plt.pie(df['Destination'].value_counts(),    \n                                  labels = df['Destination'].unique(), \n                                  shadow = True,\n                                  textprops = dict(color =\"magenta\")) \n\n\nplt.pie(df['Destination'].value_counts(),labels=df['Destination'].unique())\nplt.title('Destination')\n#plt.xticks(rotation=45)","50689693":"#Distribution of age in dataset\nplt.figure(figsize = (10,4))\nplt.hist(df['Age'],bins=100)\nplt.title('Age')\nplt.xticks(rotation=90)","a6e645f2":"#Drop Gender since more than 50% are missing\ndf.drop(\"Gender\", axis = 1, inplace = True)","d8c3f7fc":"#making categorical column ready for model fitting\ns=(df.dtypes=='object')\nobject_col = (s[s].index).tolist()\nprint(\"categorical columns: \\n {}\".format(object_col))\n\nencoder=LabelEncoder()\nfor col in object_col:\n    try:\n        df[col]=encoder.fit_transform(df[col])\n    except:\n        print(\"{} column is creating a problem\".format(col))\n","ab70459a":"#Droping target variable from train set\ny=df['Claim']\ndf.drop('Claim',axis=1,inplace=True)","d7646dab":"\"\"\"\nWe want to explore different models and didn't want to write code multiple times so we formed these functions\n\n\"\"\"\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,r2_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\n\n#for regression task use r2_score in place of accuracy_score\n\n#Algorithms\n\ndef algorithm1(X,y, n):\n    model=LogisticRegression(C = n)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    return acc_score\n    print(\"accuracy score is : {}\".format(acc_score))\n    \n    \ndef algorithm2(X,y,n):\n    model=KNeighborsClassifier(n_neighbors=n)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    #print(\"accuracy score is : {}\".format(acc_score))\n    return acc_score\n    \ndef algorithm4(X,y,n):\n    model=RandomForestClassifier(n_estimators=n)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    return acc_score\n    print(\"accuracy score is : {}\".format(acc_score))","f615eb7c":"#Searching best hyperparameter for logistic regression\nlist_log_reg=[]\nfor i in [0.1,0.001,0.0001,0.2,0.002,0.3,0.4,0.5,0.005]:\n    list_log_reg.append(algorithm1(df,y,i))\nplt.figure(figsize=(15,10))\nplt.plot([0.1,0.001,0.0001,0.2,0.002,0.3,0.4,0.5,0.005],list_log_reg,marker='o')","1eb97017":"#Searching best hyperparameter for KNearestNeighbor\nlist_knn=[]\nfor i in range(1,21):\n    list_knn.append(algorithm2(df,y,i))\nplt.figure(figsize=(15,10))\nplt.plot(range(20),list_knn,marker='o')","4efa47c4":"\n#It was taking time you can explore it\nlist_randomforest=[]\nfor i in [100,200,500,800,1000,5000]:\n    list_randomforest.append(algorithm4(df,y,i))","3225c8ee":"# Just See these points definitely you will be benefited\n- whenever Dataset comes first thing we have to do is to exlore it from a relatively top view and if your aim is to do analysis than dig it further as deeper as possible.\n- Here main aim is to determine a begginer's approach how you can approach any dataset and can have something to improve not to start from scratch.\n- Now in this we had done very less analysis but still there was some and for machine learning part we had implemented logistic regression and KNearestNeighbor along with hyperparameter tuning.\n- When you will progress further you will come across randomizedsearchcv which is used for selecting best parameters btw we had not implemented here\n- The code is self explanatory and I think will give you a good start and I'm hoping you will be putting your notebook also:)\n- If you upload your notebook it will give you confidence of uploading your work in the community and you will be documenting your code also if your'e going to upload things which is a must thing in the industry.\n- At last there is something which I have left for you ie. you have to check for distribution_channel column whether imbalancement is creating problem or not?\n- Please don't forgot to upvote it.\n\n\"upvote karoge tabhi to bhai ki notebook chalegi\""}}