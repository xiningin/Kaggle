{"cell_type":{"fc7ea3ee":"code","9d50d41a":"code","3f8fd703":"code","262ff40f":"code","11168815":"code","70004edd":"code","3c9d2afc":"code","5e645615":"code","163dd115":"code","d3c56d52":"code","5de48ef4":"code","0b1d2f18":"code","b5595d1e":"code","c1b23a53":"code","a25ed2cc":"code","68da5dd5":"code","a05c4821":"code","0e78c4de":"code","1aa226e7":"code","41d40afd":"code","f6d8f175":"code","4851cd6d":"code","62cd4430":"code","47a25bf2":"code","5e8f173d":"code","fe42e203":"markdown","f4562cb6":"markdown","5f7ea3e8":"markdown","d0bcfbc6":"markdown","41565da8":"markdown","e3049418":"markdown","320a309c":"markdown","360bdab4":"markdown","aa306bba":"markdown","37d8c8fb":"markdown","51676f40":"markdown","101753c1":"markdown","d44b0227":"markdown","7155b7bb":"markdown","283070da":"markdown","b6e39e9b":"markdown","859601a6":"markdown","0527d342":"markdown","cbf5df96":"markdown","761aa6c2":"markdown","2201bc7c":"markdown","6d8abc14":"markdown","a88cf7b3":"markdown","5153dcb1":"markdown","de686af0":"markdown"},"source":{"fc7ea3ee":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport PIL\nimport glob\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","9d50d41a":"import pathlib\n\ndataset_url = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz\"\ndata_dir = tf.keras.utils.get_file('flower_photos', origin = dataset_url, untar = True)\ndata_path = pathlib.Path(data_dir)","3f8fd703":"image_count = len(list(data_path.glob('*\/*.jpg')))\n\nprint(image_count)","262ff40f":"# Tulip images\ntulips = list(data_path.glob('tulips\/*'))\n\nPIL.Image.open(str(tulips[0]))","11168815":"PIL.Image.open(str(tulips[15]))","70004edd":"# Daisy images\ndaisy = list(data_path.glob('daisy\/*'))\n\nPIL.Image.open(str(daisy[100]))","3c9d2afc":"batch_size = 32\nimg_height = 180\nimg_width = 180","5e645615":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_path,\n    validation_split = 0.3,\n    subset = \"training\",\n    seed = 111,\n    image_size = (img_height, img_width),\n    batch_size = batch_size)","163dd115":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_path,\n    validation_split = 0.3,\n    subset = \"validation\",\n    seed = 111,\n    image_size = (img_height, img_width),\n    batch_size = batch_size)","d3c56d52":"# Name of the classes of flowers present\nclass_names = train_ds.class_names\nprint(class_names)","5de48ef4":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n     ax = plt.subplot(3, 3, i + 1)\n     plt.imshow(images[i].numpy().astype(\"uint8\"))\n     plt.title(class_names[labels[i]])\n     plt.axis(\"off\")","0b1d2f18":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","b5595d1e":"num_classes = 5\n\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","c1b23a53":"model.compile(optimizer = 'adam', \n             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n             metrics = 'accuracy')","a25ed2cc":"model.summary()","68da5dd5":"epochs = 10\n\nhistory = model.fit(train_ds, validation_data = val_ds, epochs = epochs)","a05c4821":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nplt.figure(figsize = (8,8))\nplt.subplot(1,2,1)\nplt.plot(range(epochs), acc, label = \"Training Accuracy\")\nplt.plot(range(epochs), val_acc, label = \"Validation Accuracy\")\nplt.legend(loc = \"lower right\")\nplt.title(\"Training and Validation Accuracy\")\n\nplt.subplot(1,2,2)\nplt.plot(range(epochs), loss, label = \"Training Loss\")\nplt.plot(range(epochs), val_loss, label = \"Validation Loss\")\nplt.legend(loc = \"lower right\")\nplt.title(\"Training and Validation Loss\")","0e78c4de":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape = (img_height, \n                                                               img_width,\n                                                               3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","1aa226e7":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n     augmented_images = data_augmentation(images)\n     ax = plt.subplot(3, 3, i + 1)\n     plt.imshow(augmented_images[5].numpy().astype('uint8'))\n     plt.axis(\"off\")","41d40afd":"model = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","f6d8f175":"model.compile(optimizer = 'adam',\n             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n             metrics = 'accuracy')","4851cd6d":"model.summary()","62cd4430":"epochs = 15\n\nhistory = model.fit(train_ds, validation_data = val_ds, epochs = epochs )","47a25bf2":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nplt.figure(figsize = (8,8))\nplt.subplot(1,2,1)\nplt.plot(range(epochs), acc, label = \"Training Accuracy\")\nplt.plot(range(epochs), val_acc, label = \"Validation Accuracy\")\nplt.legend(loc = \"lower right\")\nplt.title(\"Training and Validation Accuracy\")\n\nplt.subplot(1,2,2)\nplt.plot(range(epochs), loss, label = \"Training Loss\")\nplt.plot(range(epochs), val_loss, label = \"Validation Loss\")\nplt.legend(loc = \"lower right\")\nplt.title(\"Training and Validation Loss\")","5e8f173d":"sunflower_url = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/592px-Red_sunflower.jpg\"\nsunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n\nimg = keras.preprocessing.image.load_img(\n    sunflower_path, target_size=(img_height, img_width)\n)\nimg_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nscore = tf.nn.softmax(predictions[0])\n\nprint(\n    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n    .format(class_names[np.argmax(score)], 100 * np.max(score))\n)","fe42e203":"#### Model Summary","f4562cb6":"#### Compile the model","5f7ea3e8":"#### One thing to not here the the dropout and data augmentation layers will be absent during ptediction","d0bcfbc6":"#### Visualize the results","41565da8":"### Visualize the data","e3049418":"#### Data Augmentation\nOverfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting then using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.","320a309c":"### Imrove the model","360bdab4":"***Let's visualize the augmented images***","aa306bba":"#### Again compile and train the model","37d8c8fb":"### Load and Examine the data","51676f40":"#### Let's check out some images","101753c1":"#### Make training and validation set","d44b0227":"There are a total of 3670 images in the dataset.\n\nTarget classes of flowers are 5 :\n* daisy\n* dandelion\n* roses\n* sunflowers\n* tulips","7155b7bb":"### This tutorial shows how to classify images of flower. You will be able to efficiently do classification using tensorflow-keras after going through this notebook\n\n##### Workflow of this notebook\n* Load and Examine the data\n* Build an input pipeline\n* Build the model\n* Train the model\n* Test the model\n* Improve the model and repeat the process","283070da":"#### Importing some libraries","b6e39e9b":"### Build the model","859601a6":"#### Dropout\nAnother technique to reduce overfitting is to introduce Dropout to the network, a form of regularization.","0527d342":"### Build Input Pipeline","cbf5df96":"#### Configure the dataset for performance","761aa6c2":"### If you liked the notebook then let me know by giving an upvote","2201bc7c":"### Predict on new data","6d8abc14":"Here we can see that the training and validation accuracy are off by a large margin. \nAlso, the training accuracy is increasing with each epoch wheresas validation accuracy got stuck around 60%.\nClearly, this is a sign of overfitting. Let's see how to resolve this issue","a88cf7b3":"### Train the model","5153dcb1":"Now, it can be clearly interpreted that we have reduced the overfitting problem of our model and able to increase the validation accuracy","de686af0":"#### Visualize training results"}}