{"cell_type":{"7f166e3a":"code","56ee0c2d":"code","ebf9e54a":"code","1fbe9684":"code","2388d4e6":"code","43ed840a":"code","90cd9c8b":"code","45f66888":"code","0d6ac7a9":"code","e7892491":"code","0d1959c0":"code","6a0023f9":"code","75dc6ff2":"code","59d6d40f":"code","62e14b64":"code","ca031bb3":"code","5e8b4bf2":"code","79f9b656":"code","8cb5bae4":"code","ecd5441a":"code","cf4a4595":"code","3f83b3d6":"code","343bc79b":"code","a8c88445":"code","aa4d6074":"code","a76da0d1":"code","97d741aa":"code","47b647da":"code","753f1800":"code","cceea5a8":"code","90b832eb":"code","d517848c":"code","ce1a09e8":"code","bfeb831a":"code","65f1cdbd":"code","516139bd":"code","cd43e458":"code","a179573f":"code","28a00113":"code","4476f325":"code","b4717a4b":"code","bc6ac151":"code","9fd9875b":"code","412ec916":"code","e4cbf0cb":"code","8f2369d6":"code","9be757a2":"code","0cb0d6a8":"code","ec82115e":"code","dc265b42":"code","f14fac54":"code","ed740f09":"code","d0e2a75b":"code","374ed90c":"code","602801a3":"code","79d86233":"code","2e511048":"code","2da0b56e":"code","fcf64853":"code","65921a55":"code","377f8aa8":"code","37e264d3":"code","64d85f52":"code","76895ecc":"code","ba3f9a0a":"code","3ef66cda":"markdown","b57709cb":"markdown","cfc7051c":"markdown","3e5a20cf":"markdown","1e962710":"markdown","8a5af311":"markdown","3d4910a5":"markdown","205024ee":"markdown","d6122ba9":"markdown","3ea87335":"markdown","9644d4ed":"markdown","216990f6":"markdown","fd31bb95":"markdown","6f60c03a":"markdown","8973dab2":"markdown","c6bae850":"markdown","7c75fe54":"markdown","1551766d":"markdown","4596ff17":"markdown","6bda13d6":"markdown","72dd3b9d":"markdown","13d32114":"markdown","11351be9":"markdown","0e1a7e06":"markdown"},"source":{"7f166e3a":"import pandas as pd\nimport numpy as np\n\n# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\nimport numpy as np","56ee0c2d":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","ebf9e54a":"train.head()","1fbe9684":"test.head()","2388d4e6":"train.describe(include='all')","43ed840a":"test.describe(include='all')","90cd9c8b":"train.info()","45f66888":"test.info()","0d6ac7a9":"train.isnull().sum()","e7892491":"test.isnull().sum()","0d1959c0":"import matplotlib.pyplot as plt # Plot the graphes\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots","6a0023f9":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","75dc6ff2":"bar_chart('Sex')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Sex'].value_counts())\nprint(\"---------------------------\")\nprint(\"Dead:\\n\",train[train['Survived']==0]['Sex'].value_counts())","59d6d40f":"bar_chart('Pclass')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Pclass'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Pclass'].value_counts())","62e14b64":"bar_chart('SibSp')\nprint(\"Survived :\\n\",train[train['Survived']==1]['SibSp'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['SibSp'].value_counts())","ca031bb3":"bar_chart('Parch')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Parch'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Parch'].value_counts())","5e8b4bf2":"bar_chart('Embarked')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Embarked'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Embarked'].value_counts())","79f9b656":"train.head()","8cb5bae4":"train_test_data = [train,test]\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","ecd5441a":"train['Title'].value_counts()","cf4a4595":"test['Title'].value_counts()","3f83b3d6":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset[\"Title\"].map(title_mapping)","343bc79b":"dataset.head()","a8c88445":"test.head()","aa4d6074":"bar_chart('Title')","a76da0d1":"# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","97d741aa":"sex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","47b647da":"bar_chart('Sex')","753f1800":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace= True)\ntest[\"Age\"].fillna(test.groupby('Title')['Age'].transform(\"median\"), inplace= True)","cceea5a8":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.show()\n\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.xlim(10,50);","90b832eb":"for dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4","d517848c":"bar_chart('Age')","ce1a09e8":"Pclass1 = train[train['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass'] == 3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st Class','2nd Class','3rd Class']\ndf.plot(kind = 'bar', stacked =  True, figsize=(10,5))\nplt.show()\nprint(\"Pclass1:\\n\",Pclass1,\"\\n\")\nprint(\"Pclass2:\\n\",Pclass2,\"\\n\")\nprint(\"Pclass3:\\n\",Pclass3,\"\\n\")","bfeb831a":"for dataset in train_test_data:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')\n    \ntrain.head()    ","65f1cdbd":"embarked_mapping = {'S':0,'C':1,'Q':2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","516139bd":"# fill missing Fare with median fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head(20)","cd43e458":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4 )\nfacet.map(sns.kdeplot, 'Fare', shade = True)\nfacet.set(xlim = (0, train['Fare'].max()))\nfacet.add_legend()\nplt.show()","a179573f":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","28a00113":"for dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[dataset['Fare'] >= 100, 'Fare'] = 3\n    \n\ntrain.head()    ","4476f325":"train.Cabin.value_counts()","b4717a4b":"for dataset in train_test_data:\n    dataset['Cabin'] =  dataset['Cabin'].str[:1]","bc6ac151":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5));","9fd9875b":"cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","412ec916":"# fill missing Fare with median fare for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","e4cbf0cb":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","8f2369d6":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","9be757a2":"family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","0cb0d6a8":"train.head()","ec82115e":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","dc265b42":"X_train.drop(['PassengerId','Ticket'],axis=1,inplace=True)\nX_train.head()","f14fac54":"Y_train.head()","ed740f09":"X_test.drop('Ticket',axis=1,inplace=True)","d0e2a75b":"X_test.head()","374ed90c":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","602801a3":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","79d86233":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","2e511048":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","2da0b56e":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","fcf64853":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","65921a55":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","377f8aa8":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","37e264d3":"\nxgb_classifier = XGBClassifier(n_estimators=100)\nxgb_classifier.fit(X_train, Y_train)\nY_pred = xgb_classifier.predict(X_test)\nxgb_classifier.score(X_train, Y_train)\nacc_xgb_classifier = round(xgb_classifier.score(X_train, Y_train) * 100, 2)\nacc_xgb_classifier","64d85f52":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',\n              'Decision Tree','XGB classifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_decision_tree,acc_xgb_classifier]})\nmodels.sort_values(by='Score', ascending=False)","76895ecc":"submission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('gender_submission.csv', index=False)","ba3f9a0a":"submission.head()","3ef66cda":"## k-Nearest Neighbors algorithm \nIn pattern recognition k-NN is a non-parametric method used for classification and regression. \n\nKNN confidence score is better than Logistics Regression but worse than SVM.","b57709cb":"## Decision tree\n\nThe model confidence score is the highest among models evaluated so far.","cfc7051c":"#### Family Size","3e5a20cf":"####  ** combine dataset **","1e962710":"#### Binning\nBinning\/Converting Numerical Age to Categorical Variable\n\nfeature vector map:\n* child: 0\n* young: 1\n* adult: 2\n* mid-age: 3\n* senior: 4","8a5af311":"The Chart confirms a **person aboarded from C** slightly more likely survived.  \nThe Chart confirms a **person aboarded from Q** more likely dead.  \nThe Chart confirms a **person aboarded from S** more likely dead. ","3d4910a5":"Those who were 20 to 30 years old were more dead and more survived.","205024ee":"Ember map:\n* S: 0\n* C: 1\n* Q: 2\n","d6122ba9":"###  Feature engineering","3ea87335":"The Chart confirms ** 1st class ** more likely survivied than **other classes**.  \nThe Chart confirms ** 3rd class ** more likely dead than **other classes**","9644d4ed":"#### Title Map\n\nMr    : 0   \nMiss  : 1  \nMrs   : 2  \nOthers: 3  ","216990f6":"## Naive Bayes classifiers \n\nThe model generated confidence score is the lowest among the models evaluated so far.","fd31bb95":"### Train and Test split","6f60c03a":"more than 50 % of 1st class are from S embark.  \nmore than 50 % of 2st class are from S embark.   \nmore than 50 % of 3st class are from S embark.  \n\n**fill out missing embark with S embark**","8973dab2":"The Chart confirms a **person aboarded with more than 2 siblings or spouse** more likely survived.  \nThe Chart confirms a **person aboarded without siblings or spouse** more likely dead","c6bae850":"The Chart confirms Women more likely survivied than Men.","7c75fe54":"The Chart confirms a **person aboarded with more than 2 parents or children more likely survived.**  \nThe Chart confirms a **person aboarded alone more likely dead**","1551766d":"## Random Forests\nThe model confidence score is the highest among models evaluated so far. So will use this model's output (Y_pred) for creating competition submission of results.","4596ff17":"### Data Visualization using Matplotlib and Seaborn packages","6bda13d6":"- Sex is highest positivie coefficient,\n- Inversely as Pclass increases, probability of Survived=1 decreases the most.\n- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.","72dd3b9d":"### Bar Chart for Categorical Features","13d32114":"### Model evaluation\n\nWhile both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","11351be9":"## Support Vector Machines\n\nThe model generates a confidence score which is higher than Logistics Regression model.","0e1a7e06":"## Logistic regression\n\nIt measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features)\n\nNote the confidence score generated by the model based on the training dataset."}}