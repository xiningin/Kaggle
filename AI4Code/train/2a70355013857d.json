{"cell_type":{"dac0fc87":"code","c9c624a3":"code","f41ab5dd":"code","86a26d45":"code","8afda5f6":"code","41f35161":"code","728faaf7":"code","c6f4901b":"code","67b6e6ed":"code","eed7a184":"code","b0390c74":"code","f8e0cdd8":"code","e80ba068":"code","12801c3e":"code","b266688f":"code","97b45557":"code","8b26f9d3":"code","272899a8":"code","895a7741":"code","7f4901d7":"code","efca1962":"code","1ae8e8b9":"code","0bbc468e":"code","339c6770":"code","dbec65f2":"code","6b409313":"code","5524d4dd":"code","b3caa1f2":"code","c09301ae":"code","8dfad817":"code","be718797":"code","ef6f7b6d":"code","fc0c9813":"code","9a0f1a6d":"code","3e811a60":"code","6d24476f":"code","007ac2f9":"code","f83bd74b":"markdown","8fe5129b":"markdown","e3b04020":"markdown","05b50d73":"markdown","f5f606ec":"markdown","90d884d9":"markdown","0cddc51f":"markdown","948e5494":"markdown","e801e5ed":"markdown","dbfa8bb3":"markdown","f6c5289d":"markdown","bbc4b808":"markdown","b3a3bb02":"markdown","acfa08f0":"markdown","5c0e0435":"markdown","f6ebef54":"markdown","aa6c833e":"markdown","926582d9":"markdown","87470fe7":"markdown","b6cb3151":"markdown","18b23ace":"markdown"},"source":{"dac0fc87":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport bz2 # To open zipped files\nimport re # regular expressions\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nprint(os.listdir(\"..\/input\"))","c9c624a3":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","f41ab5dd":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","86a26d45":"# clean the older variables\ndel train_file, test_file\ngc.collect()","8afda5f6":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","41f35161":"print(type(train_file_lines), type(test_file_lines), \"\\n\")\n\nprint(\"Train Data Volume:\", len(train_file_lines), \"\\n\")\nprint(\"Test Data Volume:\", len(test_file_lines), \"\\n\\n\")\n\nprint(\"Demo: \", \"\\n\")\nfor x in train_file_lines[:2]:\n    print(x, \"\\n\")","728faaf7":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]","c6f4901b":"train_labels[0]","67b6e6ed":"sns.countplot(train_labels)\nplt.title('Train Labels distribution')","eed7a184":"sns.countplot(test_labels)\nplt.title('Test Labels distribution')","b0390c74":"train_sentences = [x.split(' ', 1)[1][:-1] for x in train_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1] for x in test_file_lines]","f8e0cdd8":"train_sentences[0]","e80ba068":"train_sentences_size = list(map(lambda x: len(x.split()), train_sentences))\n\nsns.distplot(train_sentences_size)\nplt.xlabel(\"#words in reviews\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Word Frequency Distribution in Reviews\")","12801c3e":"train_label_len = pd.DataFrame({\"labels\": train_labels, \"len\": train_sentences_size})\ntrain_label_len.head()","b266688f":"neg_mean_len = train_label_len.groupby('labels')['len'].mean().values[0]\npos_mean_len = train_label_len.groupby('labels')['len'].mean().values[1]\n\nprint(f\"Negative mean length: {neg_mean_len:.2f}\")\nprint(f\"Positive mean length: {pos_mean_len:.2f}\")\nprint(f\"Mean Difference: {neg_mean_len-pos_mean_len:.2f}\")\nsns.catplot(x='labels', y='len', data=train_label_len, kind='box')\nplt.xlabel(\"labels (0->negative, 1->positive)\")\nplt.ylabel(\"#words in reviews\")\nplt.title(\"Review Size Categorization\")","97b45557":"del neg_mean_len,pos_mean_len\ngc.collect()","8b26f9d3":"for i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n\nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","272899a8":"list(filter(lambda x: '<url>' in x, train_sentences))[0]","895a7741":"del train_file_lines, test_file_lines\ngc.collect()","7f4901d7":"import nltk\nfrom nltk import pos_tag\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nnltk.download('averaged_perceptron_tagger')\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n    \ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]\n\nlemmatize_sent('He is WALKING walking to school')","efca1962":"# Stopwords from stopwords-json\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords.words('english'))\nstopwords_punct = set(punctuation)\n# Combine the stopwords. Its a lot longer so I'm not printing it out...\nstoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)","1ae8e8b9":"def preprocess_text(text):\n    # Input: str, i.e. document\/sentence\n    # Output: list(str) , i.e. list of lemmas\n    return [word for word in lemmatize_sent(text) \n            if word not in stoplist_combined\n            and not word.isdigit()]","0bbc468e":"train_sentences[10]","339c6770":"preprocess_text(train_sentences[10])","dbec65f2":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(analyzer=preprocess_text)","6b409313":"train_set = count_vect.fit_transform(train_sentences[:10000])","5524d4dd":"train_set.toarray().shape","b3caa1f2":"test_set = count_vect.transform(test_sentences[:1000])","c09301ae":"most_freq_words = pd.DataFrame(count_vect.vocabulary_.items(), columns=['word', 'frequency'])[:100].sort_values(ascending=False, by = \"frequency\")[:20]\nmost_freq_words.plot.bar(x=\"word\", y=\"frequency\", rot=70, title=\"Most Frequent Words\")\n","8dfad817":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()","be718797":"clf.fit(train_set, train_labels[:10000])","ef6f7b6d":"from sklearn.metrics import accuracy_score\n\n# To predict our tags (i.e. whether requesters get their pizza), \n# we feed the vectorized `test_set` to .predict()\npredictions_valid = clf.predict(test_set)\n\nprint('Amazon Sentiment Analysis Accuracy = {}'.format(\n        accuracy_score(predictions_valid, test_labels[:1000]) * 100)\n     )","fc0c9813":"def important_features(vectorizer,classifier,n=40):\n    class_labels = classifier.classes_\n    feature_names =vectorizer.get_feature_names()\n\n    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n\n    class1_frequency_dict = {}\n    class2_frequency_dict = {}\n    \n    for coef, feat in topn_class1:\n        class1_frequency_dict.update( {feat : coef} )\n\n    for coef, feat in topn_class2:\n        class2_frequency_dict.update( {feat : coef} )\n\n    return (class1_frequency_dict, class2_frequency_dict)","9a0f1a6d":"neg_frequency_dict, pos_frequency_dict = important_features(count_vect, clf)","3e811a60":"neg_feature_freq = pd.DataFrame(neg_frequency_dict.items(), columns = [\"feature_word\", \"frequency\"])  \npos_feature_freq = pd.DataFrame(pos_frequency_dict.items(), columns = [\"feature_word\", \"frequency\"])  ","6d24476f":"neg_feature_freq.plot.bar(x=\"feature_word\", y=\"frequency\", rot=70, figsize=(15, 5), title=\"Important Negative Features(words)\")","007ac2f9":"pos_feature_freq.plot.bar(x=\"feature_word\", y=\"frequency\", rot=70, figsize=(15, 5), title=\"Important Positive Features(words)\")","f83bd74b":"## Vectorization with sklearn\n\nIn scikit-learn, there're pre-built functions to do the preprocessing and vectorization.  \n\nIt will be the object that contains the vocabulary (i.e. the first row of our table above) and has the function to convert any sentence into the counts vectors.  \n\nThe input that CountVectorizer is a textfile\/Iterable of strings","8fe5129b":"## Methods\n\n#### What methods did you use to analyze the data and why are they appropriate? Be sure to adequately, but briefly, describe your methods.\n","e3b04020":"## Motivations\n#### Describe the problem you want to solve with the data.  It may relate closely with your research question, but your goal here is to make your audience care about the project\/problem you are trying to solve.  You need to articulate the problem you are exploring and why (and for whom) insight would be valuable.\n\n\nMaking sense out of humongous dirty text data present on the internet. Using it to better understand product consumers, which will eventually help in better products and services. Leading to better and smarter economies.\n\nThese insights will be valuable in Better understanding consumer behaviour and reviews regarding Amazon products, and will directly impact including (not exhaustive):\n- Amazon sellers\n- Academic Researchers","05b50d73":"## Data Preparation and Cleaning\n\n#### At a high-level, what did you need to do to prepare the data for analysis?  Describe what problems, if any, did you encounter with the dataset?\n\nEach review with more than 100 words was imported and tokenized. Afterward, all of the tokens that were punctuations, label, stopword, or not an English word (emoji, special character, foreign languages) were removed.\n\nThe remaining tokens would undergo the process of lemmatizing using WordNetLemmatizer to reduce word to its root form, and Lemmatization using Wordnet Lemmatizer to acquire primal terms before appending into a clean list.\n\n\nFollowing are the process followed:\n\n- Lowercasing\n- Tokenization\n- Stemming and Lemmatization\n- Removing Stopwords\n- Removing Punctuations\n- Removing Digits\n- Removing Url\u2019s\n","f5f606ec":"## Acknowledgements:\n\n#### Where did you get your data?  Did you use other informal analysis to inform your work?  Did you get feedback on your work by friends or colleagues? Etc.  If you had no one give you feedback and you collected the data yourself, say so.\n\n\n- Dataset: https:\/\/www.kaggle.com\/bittlingmayer\/amazonreviews  \n- Nlp with NLTK Tutorial: https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk","90d884d9":"## Stopwords\n\nStopwords are non-content words that primarily has only grammatical function. Often we want to remove stopwords when we want to keep the \"gist\" of the document\/sentence.\n\n## Using a stronger\/longer list of stopwords\n\nAfter applying NLTK Stopwords we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n\nWe can combine the stopwords we have in NLTK with other stopwords list we find online.\n\nPersonally, I like to use stopword-json because it has stopwrds in 50 languages  \nhttps:\/\/github.com\/6\/stopwords-json\n\n\n## Punctuations\nOften, we want to remove the punctuations from the documents too.  \nSince Python comes with \"batteries included\", we have string.punctuation","0cddc51f":"## Word Embeddings\n\n#### From Strings to Vectors\n\nVector is an array of numbers\n\n- **Vector Space Model:** conceptualizing language as a whole lot of numbers\n\n- **Bag-of-Words (BoW):** Counting each document\/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n\n\nVector space model or term vector model is an algebraic model for representing text documents as vectors of identifiers, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings.\n\n\n#### We are using sklearn CountVectorizer to create Vector Space Model","948e5494":"Now we\u2019ll divide it by sentiment and calculate average values","e801e5ed":"## Research Question(s)\n#### What is your research question you aim to answer using the dataset?  Be sure the research question is well defined (see project description for details).\n\n- Extracting information out of dirty review data from Amazon product reviews containing various typos, nonstandard spellings, and other variations that you may not find in curated sets of published text.\n\n\n- Classifying the reviews as positive or negative, understanding their sentiment towards the product.\n\n\n- Understanding Features Importance: Analyzing tokenized keyword vocabulary, which are responsible for predicting positive and negative reviews.\n","dbfa8bb3":"## Multinomial Naive Bayes classifier in sklearn\n\n### Classification\n\nClassification simply means putting our data points into bins\/box. You can also think of it as assigning label to our data points, e.g. given box of fruits, sort them in apples, oranges and others.\n\n### Naive Bayes Classification\n\nIn statistics, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features. They are among the simplest Bayesian network models\n\nThere are different variants of Naive Bayes (NB) classifier in sklearn.  \n\n**Multinomial** is a big word but it just means many classes\/categories\/bins\/boxes that needs to be classified.","f6c5289d":"## Limitations\n\n#### If applicable, describe limitations to your findings.  For example, you might note that these results were true for British Premier league players but may not be applicable to other leagues because of differences in league structures.  \n#### Or you may note that your data has inherent limitations.  For example, you may not have access to the number of Twitter followers per users so you assumed all users are equally influential.  If you had the number of followers, you could weight the impact of their tweet\u2019s sentiment by their influence (# of followers).  \n\n\n\n\n- Model is trained on the Amazon Product Review data, and does not assure guarantee of predicting correct label (positive or negative)for other e-commerce websites.\n\n- The knowledge of categories of products are unknown, so we can to predict accuracy of prediction in product category segments.\n\n- The predictions are always limitied to efficiency of:\n    - Data Cleaning Algorithms\n    - Text Embedding Algorithms\n    - Prediction Algorithm","bbc4b808":"#### Let\u2019s count number of words in reviews and see it distribution","b3a3bb02":"### Clean URLs\nmask url in data with `<url>`","acfa08f0":"## Dataset\n\n#### Describe your dataset(s) here. You should say what data is in the dataset, how much data, and where you found the dataset (if applicable).\n \nOur dataset was provided by Adam Bittlingmayer from Kaggle (https:\/\/www.kaggle.com\/bittlingmayer\/amazonreviews) covering approximately 3,600,000 customer reviews from Amazon. This is a large dataset with only review text as a feature with no other metadata. It is data written by users, so it's like that there are various typos, nonstandard spellings, and other variations that you may not find in curated sets of published text.\n\nThere are 2 categories of labels to classify the review as either positive or negative based on the number of stars given by the writer. \n\n","5c0e0435":"> ### Convert from raw binary strings to strings that can be parsed","f6ebef54":"### Extracting Labels from the data\n`0 -> __label__1 -> 1\/2 star rating`  \n`1 -> __label__2 -> 4\/5 star rating`","aa6c833e":"## Lowercasing\nThe CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n\nWe can simply lowercase them after we do sent_tokenize() and word_tokenize().\nThe tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal.\n\n\n## Tokenization\n\n- **Sentence tokenization** is the process of splitting up strings into \u201csentences\u201d\n- **Word tokenization** is the process of splitting up \u201csentences\u201d into \u201cwords\u201d\n\n\n## Stemming and Lemmatization\n\nOften we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n\nThe stemming and lemmatization process are hand-written regex rules written find the root word.\n\n- **Stemming:** Trying to shorten a word with simple regex rules\n- **Lemmatization:** Trying to find the root word with linguistics rules (with the use of regexes)\n\n(See also: Stemmers vs Lemmatizers question on StackOverflow - https:\/\/stackoverflow.com\/q\/17317418\/610569)\n\n### There are various stemmers and one lemmatizer in NLTK, the most common being:\n\n- Porter Stemmer\n- Wordnet Lemmatizer (Used in our Analysis)\n\n*Note: Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb) (https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk). By default, if you don't specify the part of speech, then all words are considered as nouns.*\n","926582d9":"## Abstract\n#### Summarize your questions and findings in 1 brief paragraph (4-6 sentences max).  Your abstract needs to include: what dataset, what question, what method was used, and findings.\n\nLeveraged 4 Million unstructured product reviews containing various typos, nonstandard spellings, and other variations, to better understand customer sentiment regarding ecommerce products. Leveraging NLTK for data cleaning, matplotlib for plotting statistics, and scikit learn to predict consumer sentiment with an accuracy of 83%, and understanding Feature Importance.","87470fe7":"## Clean Digits","b6cb3151":"### Extracting Reviews from the data\nclean newline character","18b23ace":"## Feature Importance"}}