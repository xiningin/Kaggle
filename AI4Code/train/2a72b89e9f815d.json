{"cell_type":{"17672183":"code","3cd0a28e":"code","32b4319b":"code","f4e80015":"code","e786d64b":"code","6cf84720":"code","7ae80918":"code","ef27941f":"code","47bd98df":"code","52635012":"code","fdbeb4d3":"code","f8296e09":"code","2e6e5897":"code","40b9c1da":"code","82ac8a6b":"code","41f0ae8c":"code","22bf514e":"code","dfd12475":"code","a648770a":"code","9e2cbf20":"code","8a86c1c5":"markdown","92587c59":"markdown","018d4f95":"markdown","f8298dbb":"markdown","5218094a":"markdown","4a88350b":"markdown","cdbf3353":"markdown","b14d8a70":"markdown","85f9722d":"markdown","3723888c":"markdown","e4a2e676":"markdown","e8817ba4":"markdown"},"source":{"17672183":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cd0a28e":"import re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","32b4319b":"data = pd.read_csv(\"..\/input\/appletwittersentimenttexts\/apple-twitter-sentiment-texts.csv\")\ndata.head()","f4e80015":"for i in range(len(data.sentiment)):\n    if data.sentiment[i] == -1:\n        data[\"sentiment\"][i] = \"negative\"\n    elif data.sentiment[i] == 0:\n        data[\"sentiment\"][i] = \"neutral\"\n    else:\n        data[\"sentiment\"][i] = \"positive\"\n        \ndata.head()","e786d64b":"data[\"sentiment\"].value_counts()","6cf84720":"sns.countplot(data[\"sentiment\"]);","7ae80918":"text = \" \".join(review for review in data.text)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","ef27941f":"# Generate a word cloud image\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","47bd98df":"data.iloc[1].text","52635012":"data.iloc[0].text","fdbeb4d3":"def remove_chars():\n    pattern = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|#[a-zA-Z]+|$[a-zA-Z]+|@[a-zA-Z]+|[,.^_$*%-;\u9daf!?:]')\n    for i in range(len(data[\"text\"])):\n        data[\"text\"][i] = pattern.sub('', data[\"text\"][i])\nremove_chars()\n\ndata.head()","f8296e09":"data_test = data.copy()\nstop = stopwords.words('english')\ndata_test[\"text\"] = data_test[\"text\"].str.lower().str.split()\ndata_test[\"text\"] = data_test[\"text\"].apply(lambda x: [item for item in x if item not in stop])\ndata_test.head()","2e6e5897":"def concate_words(data):\n    for i in range(len(data)):\n        #print(train_data.iloc[i])\n        data.iloc[i] = \" \".join(data.iloc[i])\n    return data\n\ndata_test[\"text\"] = concate_words(data_test[\"text\"])","40b9c1da":"data_test.head()","82ac8a6b":"max_words = 500\nmax_len= 20\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', data_test['text'][0])\nX, tokenizer = tokenize_pad_sequences(data_test['text'])\nprint('After Tokenization & Padding \\n', X[0])","41f0ae8c":"y = pd.get_dummies(data_test['sentiment'])\ntrain_data, test_data, train_label, test_label = train_test_split(X, y, test_size=0.10, random_state=1)\n\nprint('Train Set ->', train_data.shape, train_label.shape)\nprint('Test Set ->', test_data.shape, test_label.shape)","22bf514e":"vocab_size = tokenizer.document_count\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 16),\n    tf.keras.layers.LSTM(16, activation=\"relu\"),\n    tf.keras.layers.Dense(3, activation=\"softmax\")\n])\n\nmodel.summary()","dfd12475":"model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\nhistory = model.fit(train_data, train_label, epochs=10, validation_data=(test_data, test_label))","a648770a":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n    \nplot_graphs(history, \"acc\")\nplot_graphs(history, \"loss\")","9e2cbf20":"rf = RandomForestClassifier(n_estimators=100)\nrf.fit(train_data, train_label)\nrf.score(test_data, test_label)","8a86c1c5":"# Bulding Model\n\nWe convert the sentiment part to numeric values \u200b\u200band equalize it to the y variable. Then we divide the data into two as train and test.","92587c59":"First of all, we write the string equivalents of the numbers in the dataframe. Because these numbers cannot be used directly for the model.","018d4f95":"While removing the stopwords, we had to separate the words of each sentence within the list. In order to avoid problems in the next use, we should combine these words and make them a sentence again.","f8298dbb":"# Preprocessing","5218094a":"## Tokenization and Pad Sequences\n\n**Tokenization** : A token is, very simply, a piece of data that stands in for another, more valuable piece of information. Tokens have virtually no value on their own - they are only useful because they represent something bigger. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n\n![Screenshot-from-2019-07-05-13-50-56.png](attachment:41149c9f-acb4-4634-b37e-e1f85af7b8df.png)\n\n**pad_sequences** : The pad_sequences() function in the Keras deep learning library can be used to pad variable length sequences.\n\n","4a88350b":"## Removing Stopwords\n\n**Stop Words:** A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. We would not want these words to take up space in our database, or taking up valuable processing time.","cdbf3353":"# Gathering Data\n\nThis dataset has 1630 rows and 2 columns. In text column, there are user reviews and sentiment column there are -1,0,1 values.\n\n* -1:negative\n* 0:neutral\n* 1:positive","b14d8a70":"**Embedding Layer**: The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n\nIt is a flexible layer that can be used in a variety of ways, such as:\n\n* It can be used alone to learn a word embedding that can be saved and used in another model later.\n* It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n* It can be used to load a pre-trained word embedding model, a type of transfer learning.\n\n\n**LSTM (Long-Short Term Memory)**: Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n\n![0*T_-9HypnVL09rnuo.jpeg](attachment:10673a54-7b2a-4e1a-915f-a24dc1b405c8.jpeg)","85f9722d":"In the regex below, it is specified to find and extract urls starting with http, words starting with @, #, $ sign.","3723888c":"   ![apple-store-fifth-avenue-new-york-city-wallpaper-for-twitter-header-49-697 (1).jpg](attachment:8261e3f8-501a-4ec7-a096-15b9b28a25ca.jpg)","e4a2e676":"# Visualization","e8817ba4":"Below are the texts written in the first two lines of the dataset. These texts also contain unnecessary information such as url and mention. This information needs to be cleared for the model to work better."}}