{"cell_type":{"ebc39a6f":"code","a19fa215":"code","61a81391":"code","cd5220a6":"code","e7a1d02b":"code","4b40860d":"code","a0fb3704":"code","e844792f":"code","cd444da8":"code","4d2058a9":"code","d240d435":"code","ac2ebdaa":"code","6b9fd2be":"code","2f04d24e":"code","12bea34b":"code","c6211c07":"markdown","a72bcfe6":"markdown"},"source":{"ebc39a6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a19fa215":"# This is my first time to practice NLP skills through a Kaggle competition.GOOD luck!!!\n\n#import packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","61a81391":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","cd5220a6":"train_df","e7a1d02b":"#quick look for the data \n\n#positive comment eg.\nprint(train_df[train_df[\"target\"] == 0][\"text\"].values[0])\n\n#negative comment eg.\nprint(train_df[train_df[\"target\"] == 1][\"text\"].values[0])","4b40860d":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n##  let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","a0fb3704":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","e844792f":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","cd444da8":"#model \n\n##\u5cad\u56de\u5f52\n##ridge regression\n\nclf = linear_model.RidgeClassifier()","4d2058a9":"#\u4ea4\u53c9\u9a8c\u8bc1\u6cd5\n#F1 - Score\u4f5c\u4e3a\u8bc4\u4ef7\u6307\u6807\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv = 3, scoring = \"f1\")\nscores","d240d435":"clf.fit(train_vectors, train_df[\"target\"])","ac2ebdaa":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","6b9fd2be":"sample_submission[\"target\"] = clf.predict(test_vectors)","2f04d24e":"sample_submission.head()","12bea34b":"sample_submission.to_csv(\"submission.csv\", index=False)","c6211c07":"The above tells us that:\n\nThere are 54 unique words (or \"tokens\") in the first five tweets.\nThe first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\nNow let's create vectors for all of our tweets.","a72bcfe6":"Let's test our model and see how well it does on the training data. For this we'll use cross-validation - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here."}}