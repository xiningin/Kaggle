{"cell_type":{"ba51e065":"code","c17f527d":"code","01b5b29f":"code","d07522f8":"code","3ff9d9f1":"code","9afd8804":"code","3f14d18f":"code","f1203f33":"code","d96d03f0":"code","e18999d0":"code","610e9cfb":"code","0b1efcdb":"code","eeb8a3b1":"code","ecc75a31":"code","9b0fe9e4":"code","ff257ec3":"code","5a4d2be8":"code","51bdb9f2":"code","d6a56f71":"code","e2a8e47d":"code","4780b891":"code","15870062":"code","22c2282f":"code","777b929e":"code","9ef2b29a":"code","3914647e":"code","f3c8651d":"code","acd1c570":"code","4a8f86bb":"code","e95dc683":"code","d0358b83":"markdown","bab28157":"markdown","2ef37d09":"markdown","bae1b26c":"markdown","bca519ca":"markdown","a4db6a13":"markdown","3b8c0711":"markdown","5d668cf3":"markdown","d2436103":"markdown","faf962bc":"markdown","bdc591c3":"markdown","41a4055f":"markdown","1feadc3f":"markdown","51aaddd7":"markdown","83befe5b":"markdown","1faf57a7":"markdown","483a5565":"markdown","19bb5be2":"markdown","26adfd97":"markdown","7ebcc785":"markdown","892444d6":"markdown","ecc78688":"markdown","6299a7fc":"markdown","ce2bb3a0":"markdown","26af44bb":"markdown","796a91fb":"markdown","fb589fde":"markdown","8f997e01":"markdown","acb67bb6":"markdown","15a1bb27":"markdown","ba149c24":"markdown","c7c2a374":"markdown","970aedfb":"markdown","1a3d38f1":"markdown","38177102":"markdown","46024859":"markdown","064f4da3":"markdown"},"source":{"ba51e065":"# Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Install required packages\n!pip install wheel matplotlib pandas scikit-learn xgboost seaborn hyperopt","c17f527d":"# Import libraries and set settings of seaborn (lib for nicer plotting)\nfrom math import sqrt\n\nfrom hyperopt import hp, tpe, STATUS_OK, Trials\nfrom hyperopt.fmin import fmin\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns;\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tabulate import tabulate\nimport xgboost as xgb\n\nsns.set(rc={'figure.figsize': (11.7, 8.27)})","01b5b29f":"# Specify the filename\nfile_name = \"..\/input\/kc_house_data.csv\"\n\n# Read dataset to pandas dataframe (also parse date to datetime object and set column #0 as index for dataframe)\ndf = pd.read_csv(file_name, parse_dates=[1], date_parser=lambda x: pd.to_datetime(x, format='%Y%m%dT000000'), index_col=0)","d07522f8":"# Brief overview of the dataset\ndf.head()","3ff9d9f1":"# How many examples and features do we have in the dataset?\ndf.shape","9afd8804":"# What are the types of the features?\ndf.dtypes","3f14d18f":"# Check if we need to handle with some NA values in dataframe\ndf.apply(lambda x: sum(x.isnull()) \/ len(df))","f1203f33":"# Show basic statistics about our dataframe's columns (columns are ours features)\ndf.describe()","d96d03f0":"# Plot histograms of the features\ndf.hist(bins=50, figsize=(20, 15));","e18999d0":"# Plot the histogram of pricing dates grouped by years and months\ndf['date'].groupby([df[\"date\"].dt.year, df[\"date\"].dt.month]).count().plot(kind=\"bar\");","610e9cfb":"# Replace date feature with new ones\ndf['pricing_yr'] = df['date'].dt.year\ndf['pricing_month'] = df['date'].dt.month\ndf['pricing_yrmonth'] = df['date'].dt.year.map(str) + df['date'].dt.month.map('{:02d}'.format)\ndf['pricing_yrmonth'] = df['pricing_yrmonth'].astype(int)\ndf = df.drop(['date'], axis=1)","0b1efcdb":"# Create a dataframe for showing purposes with feature name and feature type\nfeature_type = [\"discrete\", \"discrete\", \"continuous\", \"continuous\", \"discrete\",\n                \"dichotomous\", \"ordinal\", \"ordinal\", \"ordinal\", \"continuous\",\n                \"continuous\", \"discrete\", \"discrete\", \"discrete\", \"continuous\", \"continuous\",\n                \"continuous\", \"continuous\", \"discrete\", \"ordinal\", \"discrete*\"]\nfeature_array = df.drop(['price'], axis=1).columns.values\npd.DataFrame({\"feature\": feature_array, \"type\": feature_type})","eeb8a3b1":"# Plot the correlation of every two variables\nsns.pairplot(df);","ecc75a31":"# Remove the outlier\ndf = df[df[\"bedrooms\"] < 20]","9b0fe9e4":"sns.jointplot(x=\"bedrooms\", y=\"price\", data=df, kind=\"reg\");","ff257ec3":"# Count the pearson correlation of features\ncorr_matrix = df.corr()\ncorr_matrix[\"price\"].sort_values(ascending=False)","5a4d2be8":"# Source: https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\n# Compute the correlation matrix\n# Plot figsize\nfig, ax = plt.subplots(figsize=(20, 20))\n# Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n# Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr_matrix, cmap=colormap, annot=True, fmt=\".2f\")\n# Apply xticks\nplt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns);\n# Apply yticks\nplt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n# Show plot\nplt.show()","51bdb9f2":"# Remove pricing and zipcode features\ndf = df.drop(['pricing_yr', 'pricing_yrmonth', 'pricing_month', 'zipcode'], axis=1)","d6a56f71":"# Prepare data for model\nY = df['price'].values\nX = df.drop(['price'], axis=1).values\n\n# Split dataset on: train (for cross-validation) and test (hold-out) sets \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)","e2a8e47d":"# Simple naive model with Cross-Validation on train set\nroot_mean_squared_errors = []\n\n# When we compare models it's good to use CV, because it's going to be robust for randomness of train\/val split.\nkf = KFold(n_splits=10)\nfor train_indices, val_indices in kf.split(X_train):\n    predictions = [Y_train[train_indices].mean()] * Y_train[val_indices].shape[0]\n    actuals = Y_train[val_indices]\n    root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score: {0:.0f}\".format(rmse))","4780b891":"# Train Linear Regression with Cross-Validation on train set\nroot_mean_squared_errors = []\n\nkf = KFold(n_splits=10)\nfor train_indices, val_indices in kf.split(X_train):\n    linreg_model = LinearRegression(n_jobs=-1).fit(X_train[train_indices], Y_train[train_indices])\n    predictions = linreg_model.predict(X_train[val_indices])\n    actuals = Y_train[val_indices]\n    root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score: {0:.0f}\".format(rmse))","15870062":"# Train RandomForest model with Cross-Validation on train set\nroot_mean_squared_errors = []\n\nkf = KFold(n_splits=10)\nfor train_indices, val_indices in kf.split(X_train):\n    rf_model = RandomForestRegressor(n_jobs=-1).fit(X_train[train_indices], Y_train[train_indices])\n    predictions = rf_model.predict(X_train[val_indices])\n    actuals = Y_train[val_indices]\n    root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score: {0:.0f}\".format(rmse))","22c2282f":"# Train RandomForest model with Cross-Validation on train set\nroot_mean_squared_errors = []\n\nkf = KFold(n_splits=10)\nfor train_indices, val_indices in kf.split(X_train):\n    rf_model = RandomForestRegressor(n_jobs=-1, n_estimators=100).fit(X_train[train_indices], Y_train[train_indices])\n    predictions = rf_model.predict(X_train[val_indices])\n    actuals = Y_train[val_indices]\n    root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score: {0:.0f}\".format(rmse))","777b929e":"# Train XGBRegressor model with Cross-Validation on train set\nroot_mean_squared_errors = []\n\nkf = KFold(n_splits=10)\nfor train_indices, val_indices in kf.split(X_train):\n    xgb_model = xgb.XGBRegressor(n_jobs=-1, n_estimators=300).fit(X_train[train_indices], Y_train[train_indices])\n    predictions = xgb_model.predict(X_train[val_indices])\n    actuals = Y_train[val_indices]\n    root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score: {0:.0f}\".format(rmse))","9ef2b29a":"# Train MLP model with Cross-Validation on train set\n\n# Do rescaling as NN models like features from 0-1 range\nscaler = MinMaxScaler(copy=True)\nscaler.fit(X_train)\n\n# Enscapsulate model training and evaluation in function\ndef train_mlp(X_train, Y_train, params={}, n_splits=10):\n    root_mean_squared_errors_train = []\n    root_mean_squared_errors = []\n\n    kf = KFold(n_splits=n_splits)\n    for train_indices, val_indices in kf.split(X_train):\n        mlp_model = MLPRegressor(**params).fit(X_train[train_indices], Y_train[train_indices])\n        predictions = mlp_model.predict(X_train[val_indices])\n        actuals = Y_train[val_indices]\n        root_mean_squared_errors_train.append(sqrt(mlp_model.loss_))\n        root_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\n    rmse_train = np.mean(root_mean_squared_errors_train)\n    rmse_valid = np.mean(root_mean_squared_errors)\n    \n    return rmse_train, rmse_valid\n    \nX_train_scaled = scaler.transform(X_train)\n\nrmse_train, rmse_valid = train_mlp(X_train_scaled, Y_train, n_splits=2)\nprint(\"average of CV rmse score on train set: {0:.0f}\".format(rmse_train))\nprint(\"average of CV rmse score on val set: {0:.0f}\".format(rmse_valid))","3914647e":"def objective(space):\n    hidden_layers = tuple(int(space['hidden_layers']) * [int(space['hidden_neurons'])])\n    \n    rmse_train, rmse_valid = train_mlp(X_train_scaled, \n                                       Y_train, \n                                       params={'solver': 'adam',\n                                               'hidden_layer_sizes': hidden_layers,\n                                               'activation': space['activation'],\n                                               'shuffle': True,\n                                               'max_iter': int(space['max_iter']),\n                                               'learning_rate_init': space['learning_rate_init'],\n                                               'verbose': False},\n                                       n_splits=2)\n\n    return {'loss': rmse_valid, 'status': STATUS_OK}\n\nactivation_functions = ['relu', 'tanh', 'logistic']\nspace = {\n    'activation': hp.choice('activation', activation_functions),\n    'hidden_neurons': hp.quniform('hidden_neurons', 10, 50, 10),\n    'hidden_layers': hp.quniform('hidden_layers', 2, 4, 1),\n    'learning_rate_init': hp.loguniform('learning_rate_init', np.log(0.001), np.log(0.01)),\n    'alpha': hp.loguniform('alpha', np.log(0.01), np.log(0.1)),\n    'max_iter': hp.quniform('max_iter', 100, 750, 50)\n}\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=15,\n            trials=trials)","f3c8651d":"sns.lineplot(x=np.arange(0, len(trials.losses())), y=trials.losses());","acd1c570":"best","4a8f86bb":"best['activation'] = activation_functions[best['activation']]\n\nhidden_layers = tuple(int(best['hidden_layers']) * [int(best['hidden_neurons'])])\nrmse_train, rmse_valid = train_mlp(X_train_scaled, Y_train, \n                                   params={'solver': 'adam',\n                                           'hidden_layer_sizes': hidden_layers,\n                                           'activation': best['activation'],\n                                           'shuffle': True,\n                                           'max_iter': int(best['max_iter']),\n                                           'learning_rate_init': best['learning_rate_init'],\n                                           'verbose': False})\n\nprint(\"average of CV rmse score on train set: {0:.0f}\".format(rmse_train))\nprint(\"average of CV rmse score on val set: {0:.0f}\".format(rmse_valid))","e95dc683":"# Train RandomForest model with Cross-Validation on train set\nroot_mean_squared_errors = []\n\nrf_model = RandomForestRegressor(n_jobs=-1, n_estimators=300).fit(X_train, Y_train)\npredictions = rf_model.predict(X_test)\nactuals = Y_test\nroot_mean_squared_errors.append(sqrt(mean_squared_error(actuals, predictions)))\n\nrmse = np.mean(root_mean_squared_errors)\nprint(\"average of CV rmse score on test set: {0:.0f}\".format(rmse))","d0358b83":"#### Random Forest","bab28157":"As we can see `pricing_*` features have terrible correlation, `bedrooms` ain't so good, too. Let's leave them for now and plot the heatmap of correlations, so maybe some new things will come up.","2ef37d09":"The pricing dates were from 05.2014 to 05.2015.\n\nLet's handle that date feature. We will create three new features based on it: year of pricing, month of pricing and concatenated year and month.","bae1b26c":"Before we categorise the features in four variable types: `continuous`, `nominal`, `dichotomous` and `ordinal`, we will look at the `date` feature, because it wasn't plotted above.","bca519ca":"We will have to deal with that `datetime64` type of `date` feature, but let's leave it for later.","a4db6a13":"Number of almost 22k examples looks good for the future of my model building. That amount of data is good for some basic models (for sure it wouldn't be for deep neural networks w\/o regularization).","3b8c0711":"Let's plot the loss change in time.","5d668cf3":"It looks like we don't have any nominal features (features which type are strings), so there is no need to convert anything to dummy variables (also known as one-hot encoding).","d2436103":"#### Final model selection\n\nWe have worse results from _MLP_ model than others, and similar results from _XGBoost_ and _Random Forest_, so it's always better to select the model with fewer hyperparameters and the one which the data scientist understands better. That's why I will pick the _Random Forest_ model as my best model and we will check how it performs on the test (hold-out) set.","faf962bc":"#### Simple \"train average set\" model","bdc591c3":"The decision of adding `pricing_*` features wasn't good. Also, because `zipcode` and `lat` with `long` can be linked together, let's remove that features, and `pricing_*` features, too.\n","41a4055f":"#### Linear regression","1feadc3f":"Basicly we can see that the MLP model overfits to training set and it would take more time to tune the hyperparameters properly. If some other methods gives reasonable results, then it's sometimes not worth investing time in the ones which don't.","51aaddd7":"## 2.2. Analyze the dataset","83befe5b":"On the test set, which none of the models have seen before, the result is comparable with the one calculated with Cross-Validation.","1faf57a7":"# 1. Intro\n\nThis is kernel with solution for _House Sales in King County, USA_ dataset made by Piotr Podbielski on the 2nd of April, 2019.\n\n# 2. Solution\nSteps taken:\n1. Load the libraries and the dataset\n2. Analyze the dataset\n3. Build a couple of models\n4. Summary of the experiments\n\n## 2.1. Load the libraries and the dataset","483a5565":"It again dropped by ~6.3%. I will also try some other tree model, which XGBoost is. For now I am not familiar with how it works under the hood, but it's always frequent choice in competitions on Kaggle platform.","19bb5be2":"And print the best hiperparameters set.","26adfd97":"There is a lot of small plots, but please open it in a new window and try to find some correlations.\n\nFor sure `bedrooms` and `sqft_living` features correlates well with `price`. `grade` and `sqft_above` features also looks promising.\n\nWhat's alarming is this one lonely point in the second plot of the first row. It looks like outlier, so we're going to remove it.","7ebcc785":"After removal of outlier the correlation doesn't look so awesome.","892444d6":"_XGBoost_ also decreases RMSE a little bit.\n\nLinear regression was linear model, trees are non-linear class of models, but they split space by hyperplanes, so let's try some model with explicitly given non-linearity functions - a MLP model.\n\nBelow there is a drawing how linear regression and trees split space.\n\nLinear Regression | Decision Tree\n- | - \n![](https:\/\/littleml.files.wordpress.com\/2016\/06\/lr_boundary_linear.png?w=244&h=244&crop=1) | ![](https:\/\/littleml.files.wordpress.com\/2016\/06\/model_boundary_linear.png?w=244&h=244&crop=1)\n\n---\nSource: https:\/\/littleml.files.wordpress.com\/2016\/06\/lr_boundary_linear.png?w=244&h=244&crop=1","ecc78688":"**\\*** - some types of features are quite hard to define or could be easily treated as one of two types.\n\nVariable types explanations are below.\n\n> Most variables in a data set can be classified into one of two major types.\n>\n> *Numerical variables*\n>\n> The values of a numerical variable are numbers. They can be further classified into discrete and continuous variables.\n>\n> * A **continuous variable** is a numeric variable. Observations can take any value between a certain set of real numbers.\n> * A **discrete variable** is a numeric variable. Observations can take a value based on a count from a set of distinct whole values. A discrete variable cannot take the value of a fraction between one value and the next closest value.\n>\n> *Categorical variables*\n>\n> The values of a categorical variable are selected from a small group of categories.Categorical variables can be further categorized into ordinal and nominal variables.\n>\n> * An **ordinal variable** is a categorical variable. Observations can take a value that can be logically ordered or ranked. The categories associated with ordinal variables can be ranked higher or lower than another, but do not necessarily establish a numeric difference between each category.\n>\n> * A **nominal variable** is a categorical variable. Observations can take a value that is not able to be organized in a logical sequence.\n\nSource: https:\/\/www.quora.com\/What-are-some-types-of-features-for-machine-learning\/answer\/Pavan-Kumar-Kota-4\n\n\nThere is one more left variable type:\n\nA **dichotomous variable** is one that takes on one of only two possible values when observed or measured.\n\nSource: http:\/\/methods.sagepub.com\/reference\/the-sage-encyclopedia-of-social-science-research-methods\/n239.xml","6299a7fc":"## 2.4. Summary of the experiments\n\nAs part of the kernel we've made a data mining part, model building part, as well as hyperparameter tuning by using `HyperOpt` module. The results seems to be good, but there are some things that could be done as further experiments:\n* feature transformation - as seen at https:\/\/playground.tensorflow.org\/, making squares of features, multiplying them or taking trigonometric functions of them could help model to fit the data,\n* more complicated feature selection - it wasn't necessary in this task, because there is a couple of features, but if we somehow multiply them (in. e. using the method described above or by changing some variables from numerical to one-hot encoded), it would be beneficial to pick the ones with highest pearson correlation with `price` feature,\n* more models could be tried. Usually some datasets are fitted better by one type of models than another.","ce2bb3a0":"RMSE (root mean squared error) is one of the common options of metric for the regression task.\n\nOthers are:\n* R^2\n* Mean Absolute Error\n\nWhat distinguishes RMSE in regard to MAE is that RMSE will make price differences even bigger (squares them), so smaller differences will commit less than bigger ones.\n\nMore info here: https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation, https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error, https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination\n\n","26af44bb":"Now run the MLP classifier again with parameters above and `n_splits=10`, so I could fairly compare it to models I've trained before.","796a91fb":"#### XGBoost Regressor","fb589fde":"Let's imagine a model, which predicts for every input the average of price from train set as a result.\n\nThat simple model can be a basic benchmark for our next models. We do that because we don't have any assumptions how good our model should be. So for sure it shouldn't be worse than this simple model.\n\n","8f997e01":"Take a look if the correlation of bedroom and price has changed.","acb67bb6":"Out of the box _Random Forest regressor_ gives even better results. Almost ~35% better than _linear regression_. The random forest method is ensemble method, so it uses not one decision tree, but plenty of it. Let's change the number of trees (estimators) from default 10 to 100 and see if the result changes.","15a1bb27":"The dataset doesn't have NA values. What a relief! :)","ba149c24":"#### Random Forest with `n_estimators=100`","c7c2a374":"The table presented above is quite raw. The plots are more assimilable way of getting knowledge about data.","970aedfb":"_Linear regression_ reduces RMSE by ~36% versus _simple \"train average set\" model_.\nI know it's quite obvious and expectable. :)\n\nLet's now try some more sophisticated methods like trees.","1a3d38f1":"We did such feature transformation, because pricing date might contain some valuable information.","38177102":"#### MLP model","46024859":"## 2.3. Build a couple of models","064f4da3":"As you can see MLP regressor without hypertuning works terrible. Also, we've started to printing two of the losses - on train and validation set to have some infomation about possible overfitting. Let's use auto hyper-parameters search to find proper hyperparameters.\n\nThe space of hyperparameters will be selected manually."}}