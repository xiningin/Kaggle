{"cell_type":{"d20bd91d":"code","2bbd1761":"code","90732772":"code","042926dd":"code","5d49f354":"code","ddfb7676":"code","979cb7e8":"code","86a2a422":"code","3d30f847":"code","5e8d7c6c":"code","b50d88f4":"code","0230561a":"code","12da78be":"code","395cbe57":"code","1287f1b5":"code","04549146":"code","8e1162c9":"code","3f566d6e":"code","6aabcdec":"code","59c03f73":"code","d7458af3":"code","6f72ed51":"code","9d1a0aee":"markdown","ce42879f":"markdown","ec5d7085":"markdown","83d29c28":"markdown","e42ee5fc":"markdown","59eb3b98":"markdown","b274b333":"markdown","023a3823":"markdown","c5371f45":"markdown","fbad5b57":"markdown","85ff03b7":"markdown","9bd20d0e":"markdown","8d3b7de8":"markdown","bcb34fa3":"markdown","b2514bdc":"markdown","5a8471b4":"markdown"},"source":{"d20bd91d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bbd1761":"data_dir = \"..\/input\/disease-prediction-using-machine-learning\"\ntrain_df = pd.read_csv(data_dir + \"\/Training.csv\")\ntest_df = pd.read_csv(data_dir + \"\/Testing.csv\")\ntrain_df.head()","90732772":"train_df.info()","042926dd":"# display 30-60. indices of columns. \ntrain_df.columns[30:60]","5d49f354":"# all of these are encoded data and types are integer.\ntrain_df[\"indigestion\"]","ddfb7676":"train_df[\"prognosis\"].unique()","979cb7e8":"# the data is balanced for classification training. lets see value counts and also visualize them\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize = (12,30))\nplt.xticks(rotation = 90)\nsns.countplot(y=\"prognosis\", data=train_df)\nprint(train_df[\"prognosis\"].value_counts())","86a2a422":"# lets visualize some of features\nfeatures = ['indigestion', 'headache', 'yellowish_skin', 'dark_urine', 'nausea',\n       'loss_of_appetite', 'pain_behind_the_eyes', 'back_pain', 'constipation',\n       'abdominal_pain', 'diarrhoea', 'mild_fever']\n\nplt.figure(figsize = (17,25))\nfor i, feature in enumerate(features):\n    plt.subplot(4,3,i+1)\n    plt.bar(train_df[feature].value_counts().index.to_numpy(), train_df[feature].value_counts().values)\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\nplt.show()","3d30f847":"# visualize it with seaborn library one more time\nfeatures = ['pain_behind_the_eyes', 'redness_of_eyes', 'sinus_pressure', 'runny_nose', 'congestion',\n       'chest_pain', 'weakness_in_limbs', 'fast_heart_rate',\n       'pain_during_bowel_movements']\n\nplt.figure(figsize = (17,17))\nfor i, feature in enumerate(features):\n    plt.subplot(4,3,i+1)\n    sns.countplot(x = feature, data = train_df)\n    plt.xlabel(feature)\nplt.show()","5e8d7c6c":"# linear relationships between some of features using correlation heatmap: for example which symptoms occur together?\ndf_corr = train_df.iloc[:, 10:40]\nplt.figure(figsize = (30, 30))\nsns.heatmap(df_corr.corr(), annot = True)\nplt.show()","b50d88f4":"# drop unnamed feature from train data\ntrain_df.drop(\"Unnamed: 133\", axis = 1, inplace = True)\n# train_df[\"Unnamed: 133\"]  # it's not here anymore","0230561a":"train_df.info()","12da78be":"train_df.head()","395cbe57":"# Modelling\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)  # k = 5\nx_train, y_train = train_df.loc[:,train_df.columns != \"prognosis\"], train_df.loc[:,\"prognosis\"]\nx_test, y_test = test_df.loc[:,train_df.columns != \"prognosis\"], test_df.loc[:,\"prognosis\"]\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)\nprint(\"Prediction list: {}\".format(prediction[0:20]))\nprint(\"With KNN (K=5) accuracy is: \",knn.score(x_test, y_test))","1287f1b5":"neighbors = np.arange(1,25)   # for k tuning\ntrain_accuracy = []\ntest_accuracy = []\n# Loop for different k values\nfor i, k in enumerate(neighbors):\n    # k from 1 to 25 (excluded)\n    knn = KNeighborsClassifier(n_neighbors = k)\n    # fit the knn\n    knn.fit(x_train, y_train)\n    # train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))","04549146":"# Plot the accuracies\nplt.figure(figsize = (13,8))\nplt.plot(neighbors, train_accuracy, label = \"Training accuracy\")\nplt.plot(neighbors, test_accuracy, label = \"Testing accuracy\")\nplt.legend()\nplt.title(\"Accuracy for both train and test data\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(neighbors)\nplt.show()","8e1162c9":"from sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\nk = 5 # for K-fold cross validation\ncv_result = cross_val_score(knn, x_train, y_train, cv = k) # uses R^2 score\nprint(\"CV scores: \", cv_result)\nprint(\"CV scores average: \", np.sum(cv_result)\/len(cv_result))","3f566d6e":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)\ndt.fit(x_train, y_train)\ndt.predict(x_test)\ndt.score(x_test, y_test)","6aabcdec":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state = 42, n_estimators = 100)\nrfc.fit(x_train, y_train)\nrfc.predict(x_test)\nrfc.score(x_test, y_test)","59c03f73":"from sklearn.svm import SVC\nsvc = SVC(gamma = \"auto\", kernel = \"rbf\" )\nsvc.fit(x_train, y_train)\nsvc.predict(x_test)\nsvc.score(x_test, y_test)","d7458af3":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state = 42)\nlogreg.fit(x_train, y_train)\nlogreg.predict(x_test)\nlogreg.score(x_test, y_test)","6f72ed51":"y_predictions = {\"KNN\": knn.predict(x_test),\n          \"SVC\": svc.predict(x_test),\n          \"DT\": dt.predict(x_test),\n          \"RFC\": rfc.predict(x_test),\n          \"LOGREG\": logreg.predict(x_test)}\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfor classifier, y_pred in y_predictions.items():\n    cm = confusion_matrix(y_test, y_pred)\n    print(classifier,'Confusion matrix: \\n',cm)\n    print(\"------------------\")\n    print(classifier, 'Classification report: \\n',classification_report(y_test,y_pred))","9d1a0aee":"<a id = \"6\"><\/a>\n#### Decision Tree Classifier","ce42879f":"<a id = \"9\"><\/a>\n#### Logistic Regression Classifier","ec5d7085":"<a id = \"4\"><\/a>\n#### Model Complexity","83d29c28":"OK. Now let's create our ML models\n<a id = \"3\"><\/a>\n### KNN","e42ee5fc":"As we can see classification reports for each classifer, knn,svm and logistic regression's success score is 100% while decision tree and random forest's accuracies are approximately 98%.\n\nThat's it for now! Thanks...","59eb3b98":"There are 4920 samples and 134 features in the dataset. First we have to dive into deeper of the data so we can completely understand it. The data is clean and balanced so we dont need to handle with missing values, outliers etc. Just one column will be dropped below stages of this kernel","b274b333":"### Content\n* [Import Data](#1)\n* [Data Cleaning](#2)\n* [KNN](#3)\n    * [Model Complexity](#4)\n    * [Cross Validation](#5)\n* [Decision Tree Classifier](#6)\n* [Random Forest Classifier](#7)\n* [Support Vector Classifier](#8)\n* [Logistic Regression](#9)\n* [Metrics](#10)","023a3823":"In fact, this dataset doesnt requires tuning, but I want to try basics of complex modelling","c5371f45":"<a id = \"1\"><\/a>\n#### Import Data","fbad5b57":"<a id = \"5\"><\/a>\n#### Cross Validation","85ff03b7":"as we look at from correlation map, we can infer these:\n* yellowish skin and abdominal pain have a high correlation coefficent which means these features usually seem together (maybe a liver problem)\n* cough and breathlessness also have high correlation (it's usual because lung diseases give the same symptoms)\n* restlessness and irregular sugar usually seem together","9bd20d0e":"<a id = \"10\"><\/a>\n#### Metrics","8d3b7de8":"<a id = \"7\"><\/a>\n#### Random Forest Classifier","bcb34fa3":"* First look at data\n* We must read our csv files from directory","b2514bdc":"<a id = \"2\"><\/a>\n#### Data Cleaning","5a8471b4":"<a id = \"8\"><\/a>\n#### Support Vector Classifier"}}