{"cell_type":{"ddff0fc6":"code","98389f39":"code","de27c819":"code","da465d5d":"code","f5f56ec6":"code","dc74f69c":"code","dfa59af4":"code","dc5b48e6":"code","f39766ff":"code","1091f74a":"code","ded651df":"code","d4d2e3e7":"code","38ff50da":"code","ee455143":"code","f64a2680":"code","ca801f14":"code","0f4427e5":"code","9e020961":"code","87d0713b":"markdown","ead5ba17":"markdown","430b2ba1":"markdown","14eb192d":"markdown","96e4c774":"markdown","27daa48a":"markdown","7df30527":"markdown","468e7d1f":"markdown","13c2bb03":"markdown","242ac871":"markdown","85623ba9":"markdown","78110389":"markdown","1486c0c1":"markdown","08328261":"markdown","92d36b12":"markdown","58595a87":"markdown","a45a325f":"markdown","3a3fa9ee":"markdown","0f2c8a79":"markdown","86609a1d":"markdown","c6fefcf9":"markdown","84a622e5":"markdown","548cceb9":"markdown","07e4774e":"markdown","92e01a37":"markdown","37b5791c":"markdown","dfd03f5b":"markdown","fd33b587":"markdown"},"source":{"ddff0fc6":"import librosa\nfrom librosa import display\nimport os\nimport glob \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time","98389f39":"path = '\/kaggle\/input\/audio-emotions\/Emotions'\nlst = []\ni = -2\nstart_time = time.time()\n\nfor subdir, dirs, files in os.walk(path):\n  i=i+1\n  print(subdir)\n  print(i)\n  for file in files:\n\n        #Load librosa array, obtain mfcss, add them to array and then to list.\n        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=4096, hop_length=256, n_mfcc=40).T,axis=0) \n        arr = mfccs, i\n        lst.append(arr) #Here we append the MFCCs to our list.\n\nprint(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))","de27c819":"#Add paths and get signals.\n\nfile1='\/kaggle\/input\/audio-emotions\/Emotions\/Neutral\/03-02-01-01-02-02-20.wav'\nsignal1, sample_rate = librosa.load(file1, sr=22050)\n\nfile2='\/kaggle\/input\/audio-emotions\/Emotions\/Neutral\/1007_WSI_NEU_XX.wav'\nsignal2, sample_rate = librosa.load(file2, sr=22050)\n\nfile3='\/kaggle\/input\/audio-emotions\/Emotions\/Neutral\/n01.wav'\nsignal3, sample_rate = librosa.load(file3, sr=22050)\n\nfile4='\/kaggle\/input\/audio-emotions\/Emotions\/Neutral\/YAF_vote_neutral.wav'\nsignal4, sample_rate = librosa.load(file4, sr=22050)\n\nemotion='Neutral'\n","da465d5d":"fig = plt.figure(figsize=(15,8))\n# WAVEFORM\n# display waveform\nplt.subplot(2, 2, 1)\nlibrosa.display.waveplot(signal1,sample_rate, alpha=0.4)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"RAVDESS Waveform \"+emotion)\n\nplt.subplot(2, 2, 2)\nlibrosa.display.waveplot(signal2,sample_rate, alpha=0.4)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"CREMA-D Waveform \"+emotion)\n\n\nfig = plt.figure(figsize=(15,8))\nplt.subplot(2, 2, 3)\nlibrosa.display.waveplot(signal3,sample_rate, alpha=0.4)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"SAVEE Waveform \"+emotion)\n\nplt.subplot(2, 2, 4)\nlibrosa.display.waveplot(signal4,sample_rate, alpha=0.4)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"TESS Waveform \"+emotion)\n","f5f56ec6":"# FFT -> power spectrum\n# perform Fourier transform\nfft1 = np.fft.fft(signal1)\nfft2 = np.fft.fft(signal2)\nfft3 = np.fft.fft(signal3)\nfft4 = np.fft.fft(signal4)\n\n# calculate abs values on complex numbers to get magnitude\nspectrum1 = np.abs(fft1)\nspectrum2 = np.abs(fft2)\nspectrum3 = np.abs(fft3)\nspectrum4 = np.abs(fft4)\n\n# create frequency variable\nf1 = np.linspace(0, sample_rate, len(spectrum1))\nf2 = np.linspace(0, sample_rate, len(spectrum2))\nf3 = np.linspace(0, sample_rate, len(spectrum3))\nf4 = np.linspace(0, sample_rate, len(spectrum4))\n\n# take half of the spectrum and frequency\nleft_spectrum1 = spectrum1[:int(len(spectrum1)\/2)]\nleft_f1 = f1[:int(len(spectrum1)\/2)]\n# take half of the spectrum and frequency\nleft_spectrum2 = spectrum2[:int(len(spectrum2)\/2)]\nleft_f2 = f2[:int(len(spectrum2)\/2)]\n# take half of the spectrum and frequency\nleft_spectrum3 = spectrum3[:int(len(spectrum3)\/2)]\nleft_f3 = f3[:int(len(spectrum3)\/2)]\n# take half of the spectrum and frequency\nleft_spectrum4 = spectrum4[:int(len(spectrum4)\/2)]\nleft_f4 = f4[:int(len(spectrum4)\/2)]\n\nfig = plt.figure(figsize=(8,10))\nplt.subplot(2, 2, 1)\n# plot spectrum\nplt.plot(left_f1, left_spectrum1, alpha=0.4)\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"RAVDESS  Power spectrum \"+emotion)\n\nplt.subplot(2, 2,2)\n# plot spectrum\nplt.plot(left_f2, left_spectrum2, alpha=0.4)\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"CREMA-D Power spectrum \"+emotion)\n\n\nfig = plt.figure(figsize=(8,10))\n\nplt.subplot(2, 2, 3)\nplt.plot(left_f3, left_spectrum3, alpha=0.4)\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"SAVEE Power spectrum \"+emotion)\n\nplt.subplot(2, 2, 4)\nplt.plot(left_f4, left_spectrum4, alpha=0.4)\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"TESS Power spectrum \"+emotion)\n","dc74f69c":"# STFT -> spectrogram\nhop_length =256 # in num. of samples\nn_fft = 4096 # window in num. of samples\n\n# calculate duration hop length and window in seconds\nhop_length_duration = float(hop_length)\/sample_rate\nn_fft_duration = float(n_fft)\/sample_rate\n\nprint(\"STFT hop length duration is: {}s\".format(hop_length_duration))\nprint(\"STFT window duration is: {}s\".format(n_fft_duration))\n\n# perform stft\nstft1 = librosa.stft(signal1, n_fft=n_fft, hop_length=hop_length)\nstft2 = librosa.stft(signal2, n_fft=n_fft, hop_length=hop_length)\nstft3 = librosa.stft(signal3, n_fft=n_fft, hop_length=hop_length)\nstft4 = librosa.stft(signal4, n_fft=n_fft, hop_length=hop_length)\n\n# calculate abs values on complex numbers to get magnitude\nspectrogram1 = np.abs(stft1)\nspectrogram2 = np.abs(stft2)\nspectrogram3 = np.abs(stft3)\nspectrogram4 = np.abs(stft4)\n\n\n# display spectrogram\n\n\nfig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 1)\nlibrosa.display.specshow(spectrogram1, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar()\nplt.title(\"RAVDESS Spectrogram \"+emotion)\n\nplt.subplot(2, 2,2)\nlibrosa.display.specshow(spectrogram2, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar()\nplt.title(\"CREMA-D Spectrogram \"+emotion)\n\n\nfig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 3)\nlibrosa.display.specshow(spectrogram3, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar()\nplt.title(\"SAVEE Spectrogram \"+emotion)\n\nplt.subplot(2, 2, 4)\nlibrosa.display.specshow(spectrogram4, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar()\nplt.title(\"TESS Spectrogram \"+emotion)","dfa59af4":"log_spectrogram1 = librosa.amplitude_to_db(spectrogram1)\nlog_spectrogram2 = librosa.amplitude_to_db(spectrogram2)\nlog_spectrogram3 = librosa.amplitude_to_db(spectrogram3)\nlog_spectrogram4 = librosa.amplitude_to_db(spectrogram4)","dc5b48e6":"fig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 1)\nlibrosa.display.specshow(log_spectrogram1, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar(format=\"%+2.0f dB\")\nplt.title(\"RAVDESS Spectogramm (dB) \"+emotion)\n\nplt.subplot(2, 2,2)\nlibrosa.display.specshow(log_spectrogram2, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar(format=\"%+2.0f dB\")\nplt.title(\"CREMA-D Spectogramm (dB) \"+emotion)\n\n\nfig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 3)\nlibrosa.display.specshow(log_spectrogram3, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar(format=\"%+2.0f dB\")\nplt.title(\"SAVEE Spectogramm (dB) \"+emotion)\n\nplt.subplot(2, 2, 4)\nlibrosa.display.specshow(log_spectrogram4, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.colorbar(format=\"%+2.0f dB\")\nplt.title(\"TESS Spectogramm (dB) \"+emotion)\n","f39766ff":"# MFCCs\n# extract 13 MFCCs\nMFCCs1 = librosa.feature.mfcc(signal1, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\nMFCCs2 = librosa.feature.mfcc(signal2, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\nMFCCs3 = librosa.feature.mfcc(signal3, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\nMFCCs4 = librosa.feature.mfcc(signal4, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\n\n# display MFCCs\nhop_length=256\n\n\nfig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 1)\nlibrosa.display.specshow(MFCCs1, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"MFCC coefficients\")\nplt.colorbar()\nplt.title(\"RAVDESS MFCCs \"+emotion)\n\nplt.subplot(2, 2,2)\nlibrosa.display.specshow(MFCCs2, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"MFCC coefficients\")\nplt.colorbar()\nplt.title(\"CREMA-D MFCCs \"+emotion)\n\n\nfig = plt.figure(figsize=(8,6))\nplt.subplot(2, 2, 3)\nlibrosa.display.specshow(MFCCs3, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"MFCC coefficients\")\nplt.colorbar()\nplt.title(\"SAVEE MFCCs \"+emotion)\n\n\nplt.subplot(2, 2, 4)\nlibrosa.display.specshow(MFCCs4, sr=sample_rate, hop_length=hop_length)\nplt.xlabel(\"Time\")\nplt.ylabel(\"MFCC coefficients\")\nplt.colorbar()\nplt.title(\"TESS MFCCs \"+emotion)\n","1091f74a":"# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\nX, y = zip(*lst)\nimport numpy as np\nX = np.asarray(X)\ny = np.asarray(y)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n#As always we need to expand the dimensions, so we can input the data to NN.\nx_traincnn = np.expand_dims(X_train, axis=2) \nx_testcnn = np.expand_dims(X_test, axis=2)","ded651df":"import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\n","d4d2e3e7":"#Simple model\n\nmodel = keras.Sequential([\n\n        # input layer\n        keras.layers.Flatten(input_shape=(40, 1)),\n\n        # 1st dense layer\n        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n        keras.layers.Dropout(0.5),\n\n        # 2nd dense layer\n        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n        keras.layers.Dropout(0.5),\n\n        # 3rd dense layer\n        keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n        keras.layers.Dropout(0.3),\n\n        # output layer\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    # compile model\noptimiser = keras.optimizers.Adam(learning_rate=0.00001)\nmodel.compile(optimizer=optimiser,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n","38ff50da":"cnnhistory=model.fit(x_traincnn, y_train, batch_size=64, epochs=700, validation_data=(x_testcnn, y_test))","ee455143":"\nplt.plot(cnnhistory.history['loss'])\nplt.plot(cnnhistory.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n\n\nplt.plot(cnnhistory.history['accuracy'])\nplt.plot(cnnhistory.history['val_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f64a2680":"from sklearn.metrics import classification_report\n\npredictions = model.predict_classes(x_testcnn)\ny_test = y_test.astype(int)\nreport = classification_report(y_test, predictions)\nprint(report)","ca801f14":"\nfrom sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(y_test, predictions)\nprint (matrix)","0f4427e5":"import seaborn as sns\nplt.figure(figsize=(10,8))\nsns.heatmap(matrix, annot=True, fmt=\"d\");","9e020961":"model_name = 'EmotionClassificationModel.h5'\nsave_dir = '\/kaggle\/working'\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)","87d0713b":"This doesn't show much does it. Lets try to apply logarhithm to cast amplitude to decibels!","ead5ba17":"Few imports:","430b2ba1":"Some imports! Not sure if we need all of them, but i did need them at some point so maybe they stayed. If they are not needed, let me know, and ill remove them!","14eb192d":"If you want you can also save the model!","96e4c774":"Now lets draw the power spectrums by performing Fourier transformations!","27daa48a":"# Let's build the classificator!","7df30527":"I din't add data augmentation, but that could be a fun thing to add in future versions, to see how results change!","468e7d1f":"This is a notebook that explores the audio data in Audio Emotions dataset and shows a simple model to classify these images. \nI learned most of the code from a dude in youtube, his channel is - Valerio Velardo - The Sound of AI. Go check him out if you are interested in audio classification and sound AI stuff. \n\nThis notebook is mainly just to give some basic information about the data set. I did not go in depth in comparing the emotions and datasets, but mostly just gave you some stuff to get started more easily if you are planning to make audio classifier yourself!\n\nI am new to data science and classification, so this is just basic and im still learning and this notebook might not be up to some people's standards. \nIf you have any ideas and advice on how to upgrade it, feel free to hit me up.","13c2bb03":"Lets display Spectogramms now!","242ac871":"# Simple Audio emotion exploration and classification ","85623ba9":"Lets explore the Neutral path. ","78110389":"Lets make the X and y and lets split them into test and train!","1486c0c1":"Lets fit and visualize our training data!\nThis does take a while.","08328261":"Let's make a classification matrix to see more in depth about classifications.","92d36b12":"# Results!\n\nLets see how we did.","58595a87":"Let's build a simple Artificial neural network with some regularization and dropouts! Im still new to neural networks so this is definetly a sub-optimal solution! Feel free to change all of this.","a45a325f":"Here we can see that between the data sets the power spectrums are very different.","3a3fa9ee":"So that is basically the information that we are going to be inputing in our neural network!\nIf you don't know what MFCCS are here's a wiki page - https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum#:~:text=Mel%2Dfrequency%20cepstral%20coefficients%20(MFCCs,%2Da%2Dspectrum%22).","0f2c8a79":"That looks, too boring. Lets get seaborn heatmap to help us with that!","86609a1d":"Lets assign paths to some variables to paths of files from different datasets, so we can compare them and explore further.","c6fefcf9":"Here we can make a lot of different conclusions, like 2nd class is often classified as 0 and so on.","84a622e5":"Here we can see, that the aplitude difference is quite visible, witch is probably due to different recording environments, text and individual voice characterstics.","548cceb9":"# Exploration\n\nLet's draw the simplest of visualisations WaveForms!\n","07e4774e":"Lets get a classification report and lets see in more detail which emotions are easier and which are harder to classify.","92e01a37":"Ah, much better. I've seen some classificators that use theese images as input to convolutional neural networks, so that might be a fun thing to try!","37b5791c":"Lets draw the spectograms of theese files ","dfd03f5b":"Now, lets actually see how the MFCC look! ","fd33b587":"# Loading files\nFirst we need to convert all the files from .wav to something we can use in our neural network.\nWe can make a list (lst) that contains the arrays of MFCC (40 dimensional vector that contains the information about audio files that we can use in training)\n\nThis does take a while, so if you are planning to run this, feel free to take a break and make yourself a coffe while this loads (Around 900seconds)"}}