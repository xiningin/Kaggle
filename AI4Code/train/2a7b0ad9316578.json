{"cell_type":{"5e4e4e27":"code","1ca6fab4":"code","251f26d8":"code","489bee63":"code","85826772":"code","2b645326":"code","a82ce010":"code","c63104c3":"code","9f4a3eba":"code","dda6155a":"code","c309d215":"code","fede28bf":"code","1b522060":"code","a07d7a93":"code","82bc7e1f":"code","7d73ef52":"code","e3a896f9":"code","df835b7d":"code","452b179c":"code","04cb94ea":"code","0bb713f1":"code","f512fe1e":"code","69f242e1":"code","6a4c36a9":"code","d935ed97":"code","60bc8e3c":"code","adfe709f":"code","afd2628a":"code","5f4235bb":"code","0ca3cc1a":"code","3cf291e0":"code","71919bf8":"code","4454d936":"code","016c8e03":"code","157cdfc8":"code","9d185e52":"code","4f270329":"code","f170c824":"code","737369af":"code","82407f94":"code","1db49c01":"code","04746d0d":"code","f59ada85":"code","88c2b818":"markdown","5f99b444":"markdown","1b097c54":"markdown","e2fa22d2":"markdown","36540964":"markdown","a86e42ed":"markdown","5e60b4ef":"markdown","a3a78308":"markdown","9479bbf1":"markdown","1d7c75d4":"markdown","0096a5d8":"markdown","6680035c":"markdown","b5c04f37":"markdown","e374d6c0":"markdown","0954fa46":"markdown","c8ce1924":"markdown","2e36eccf":"markdown","61a9af13":"markdown","5b632b68":"markdown","f12324c6":"markdown","c5129232":"markdown","50ad8eef":"markdown"},"source":{"5e4e4e27":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport sklearn as sk\n\n# for fast visualization\n%matplotlib inline\n\n#  retina display \n%config InlineBackend.figure_format = 'retina'\n\n# for minus\nmpl.rc('axes', unicode_minus=False)","1ca6fab4":"# install catboost first.\n!pip install catboost","251f26d8":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline #optional\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n# from lightgbm.sklearn import LGBMClassifier\n# from xgboost.sklearn import XGBClassifier\nfrom catboost import CatBoostClassifier","489bee63":"# train data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\n# test data\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n# sample solution\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","85826772":"# let's look into train data\nprint(train.shape)\ntrain.describe()","2b645326":"# let's look into train data\nprint(test.shape)\ntest.describe()","a82ce010":"# source1: https:\/\/www.kaggle.com\/dmitryuarov\/tps-soft-voting-xgb-cb-lgbm#Basic-information\n# source2: https:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage\n\n\ndef save_ram(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    print(f\"After Diet : {round(end_mem, 2)}MB\")\n    print(f\"Reduced: {round(100*(start_mem - end_mem)\/(start_mem), 2)}%\")\n\n    return df","c63104c3":"train = save_ram(train)","9f4a3eba":"test = save_ram(test)","dda6155a":"train.info()","c309d215":"test.info()","fede28bf":"train['miss_cnt'] = train.isnull().sum(axis=1)\ntest['miss_cnt'] = test.isnull().sum(axis=1)","1b522060":"def mc_cat(val):\n    if val == 0:\n        return 0\n    elif val == 1 or val == 14:\n        return 1\n    else:\n        return 2\n    \ntrain['mc_cat'] = train['miss_cnt'].apply(mc_cat)\ntest['mc_cat'] = test['miss_cnt'].apply(mc_cat)","a07d7a93":"train.head(2)","82bc7e1f":"test.head(2)","7d73ef52":"y = train.pop('claim')\nprint(y.shape)","e3a896f9":"X_num = train.iloc[:, 1:119]\nprint(X_num.shape)\nX_num.head(2)","df835b7d":"test_num = test.iloc[:, 1:119]\nprint(test_num.shape)\ntest_num.head(2)","452b179c":"X_cat1 = train['mc_cat']\nX_test1 = test['mc_cat']","04cb94ea":"# Data preprocessing functions\ndef get_stats_per_row(data):\n    features = [x for x in data.columns.values if x[0]==\"f\"] \n    # n_missing is the number of null value in a row\n    # Because of the same as 'miss_cnt', let 'n_missing' exist instead of 'miss_cnt'\n    data['n_missing'] = data[features].isna().sum(axis=1)\n    \n    data['max_row'] = data[features].max(axis=1)\n    data['min_row'] = data[features].min(axis=1)\n    data['std'] = data[features].std(axis=1)\n    # If the difference between the mean and the median was large, the median function was used.\n    # If the difference is small the quantile function was used.\n    # Quantile function has better than mean function.\n    median_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n    for col_name in features:\n        if col_name in median_set:\n            data[col_name].fillna(data[col_name].median(), inplace=True)\n        else:\n            data[col_name].fillna(data[col_name].quantile(0.75), inplace=True)\n            \n    # Multiply feature is the feature that multiply all values in a row\n    data['multiply'] = 1\n    for feature in features:\n        data['multiply'] = data[feature] * data['multiply']\n    \n    return data","0bb713f1":"X_num = get_stats_per_row(X_num)\nprint(f\"Count of null values in each column: {X_num.isnull().sum()}\")\nprint(f\"Count of null values in each row {X_num.isnull().sum(axis=1)}\")","f512fe1e":"test_num = get_stats_per_row(test_num)\nprint(f\"Count of null values in each column: {test_num.isnull().sum()}\")\nprint(f\"Count of null values in each row {test_num.isnull().sum(axis=1)}\")","69f242e1":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n# f1 ~ f118 of train data after Robust scaling\nX_num = pd.DataFrame(scaler.fit_transform(X_num), index = X_num.index, columns = X_num.columns)\nX_num.head(3)","6a4c36a9":"# f1 ~ f118 of test data after Robust scaling\ntest_num = pd.DataFrame(scaler.fit_transform(test_num), index = test_num.index, columns = test_num.columns)\ntest_num.head(3)","d935ed97":"X1 = pd.concat([X_num, X_cat1], axis =1)\nprint(X1.shape)\nX1.head(3)","60bc8e3c":"new_test = pd.concat([test_num, X_test1], axis =1)\nprint(new_test.shape)\nnew_test.head(3)","adfe709f":"%who","afd2628a":"del X_num, X_cat1, test_num, X_test1, train, test","5f4235bb":"# check\n%who","0ca3cc1a":"# garbage collection\nimport gc\ngc.collect()","3cf291e0":"y","71919bf8":"X1.head(2)","4454d936":"new_test.head(2)","016c8e03":"## source: https:\/\/www.kaggle.com\/dmitryuarov\/tps-soft-voting-xgb-cb-lgbm#CatBoost\n## I think I wrote worng source of parameters 'param_cat2' below.\n## I will find right source of parameters below, soon. \n\n\nparam_cat2 = {\n    'depth': 3, \n    'learning_rate': 0.014530866870832323, \n    'iterations': 44204,\n    'max_bin': 265, \n    'min_data_in_leaf': 14, \n    'l2_leaf_reg': 0.004427550682515904, \n    'subsample': 0.5402586792667279, \n    'grow_policy': 'SymmetricTree', \n    'leaf_estimation_method': 'Gradient',\n    'bootstrap_type': 'Bernoulli',\n    'random_seed': 228,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU',\n    'verbose': 0,\n    # 'n_jobs': -1\n    }","157cdfc8":"model_cat = CatBoostClassifier(**param_cat2)","9d185e52":"\ndef cat_run_predict(K, model, X, y, new_test):\n    predictions = np.zeros(len(new_test))\n    scores=[]\n    skf = StratifiedKFold(n_splits = K, shuffle = True, random_state=42)\n    for i, (train_idx, valid_idx) in enumerate(skf.split(X1, y)):\n        X_train, X_test = X1.iloc[train_idx], X1.iloc[valid_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[valid_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_test, y_test)])\n        \n        train_pred = model.predict_proba(X_train)[:, 1]\n        train_score = roc_auc_score(y_train, train_pred)\n\n        test_pred = model.predict_proba(X_test)[:, 1]\n        test_score = roc_auc_score(y_test, test_pred)\n\n        scores.append((train_score, test_score))\n        print(f\"{i+1}th Fold -ing\")\n        print(f\"{i+1}th Fold - AUC of test data(new_test): {test_score}\")\n        predictions += model.predict_proba(new_test)[:,1] \/ K\n        print(f\"{i+1}th Fold - predict_proba of test data(new_test): {predictions * K \/ (i+1)}\")\n\n    scores_data = pd.DataFrame(scores, columns = ['train_score', 'test_score'])\n    return scores_data, predictions\n","4f270329":"scores_10_cat, pred_cat_10 = cat_run_predict(10, model_cat, X1, y, new_test)\nprint(scores_10_cat.shape)\nscores_10_cat","f170c824":"# average of scores\nscores_10_cat.test_score.mean()","737369af":"# important features\nif_list10 = pd.DataFrame( model_cat.feature_importances_, index = X1.columns, columns=['importance'])\nif_list10","82407f94":"# The most important features in this model \nif_list10.importance.sort_values(ascending=False)[:30].index","1db49c01":"# Not important features in this model \nif_list10.importance.sort_values(ascending=True)[:25].index","04746d0d":"# Visualization of important features   \nplt.figure(figsize = (24,12))\nif_list10.importance.sort_values(ascending=False)[:30].plot.barh()\nplt.title(\"K = 10 Catboost important features\", fontsize = 28)\nplt.yticks(fontsize = 14)\nplt.show()","f59ada85":"ss['claim'] = pred_cat_10\nss.to_csv('smtm_after5_cb10.csv', index = False)\nss.head()","88c2b818":"## New column 'mc_cat' for ML. \nWe made 'miss_cnt' by counting null values in each row and made new categorical feature 'mc_cat' based on this. The column 'mc_cat' has strong relation with target column 'claim'. You can check details of this feature in [this note](https:\/\/www.kaggle.com\/heiswicked\/smtm-s-tps-sep-2021-eda). ","5f99b444":"### Visualize feature importance ","1b097c54":"## Imputing null valus of numerical columns on train and test by GMO's\nImputing null values by GMO's insight. However, we need to condider result of this function below for better performance later. This is still amazing and brilliant. I and my team learn from this.","e2fa22d2":"## K = 10\n\nThere is no specific reason for K = 10. I just checked the performances of K = 10, K =5, K = 3. Through this, I wanna know if there is somthing to consider what my team missed. \n  \n- Data size is huge    \n  : Data size is so huge that K = 10 could not be mattered in cross_validation. \n  \n- Always got best scores at K = 10.\n  : I don't know exactly reason for this. ","36540964":"### cat_run_predict\n\nThere are many codes of cross-validation. My team wanna something new, which can do everything in once. As result of our struffle for this, 'can_run_predict'. Nothing special, just cross-validation and prediction into one custom function. \n\n- StratifiedKFold for cross-validation.           \n    : Better than KFold because of keeping the ratio of target's values 0 and 1.\n\n- predict_proba               \n    : We wrote code so that model can get learned and predict(predict_proba) after each fold of cross_validation. After run all cross-validation for given K, we can get prediction datas for test data by getting mean value of predict_probas. We expect that this way can bring us predict_probas keeping apart from Over-fitting.","a86e42ed":"# SMTM's TPS(Sep-2021) CatBoost\n\"SHOW ME THE MEDAL\", My Team. We scored 0.81785 with ideas shared in the 'Code' and 'Discussion' (including Team GMO). In this note, We share ideas of preprocessing and models after [EDA](https:\/\/www.kaggle.com\/heiswicked\/smtm-s-tps-sep-2021-eda).\n\n- Preprocessing, models based on [EDA](https:\/\/www.kaggle.com\/heiswicked\/smtm-s-tps-sep-2021-eda).\n- Hyperparameters based on the 'codes', 'Discussion'.\n- Another Try in this note for better socre.\n\n  : Yeah, I know. Although probability of lower score than 0.81785, we continue to challenge for better.\n\nMy team used Google Colab, but writing this note for sharing. Reading this note, please consider for my writing skills in English. For these several years, I have been apart from English. By the way, I will start this note, and hope this would be helpful.","5e60b4ef":"#### you can also check result of 'save_ram' by info method.","a3a78308":"## Concatenation\nConcatenate X_num and X_cat1 into X1.\n- X_num: train's DataFrame numerical columns(f1 ~ f118) after scaling\n- X_cat1 : train's categorical column 'mc_cat' without scaling\n- X1 : new train data after concatenating X_num and X_cat1\n\nConcatenate test_num and X_test into new_test.\n- test_num: test's DataFrame numerical columns(f1 ~ f118) after scaling\n- X_test1 : test's categorical column 'mc_cat' without scaling\n- new_test : new test data after concatenating X_num and X_test1","9479bbf1":"## del variables\nfor helping RAM, needs to del variables. ","1d7c75d4":"## CatBoostClassifier\n- GPU Accelerated         \n    : CatBoost's preformance is better than LGBM, XGB and others. \n \n- Parameters.            \n    : My team hasn't studied parameters of CatBoost, LGBM and XGB, which makes us to use parameter-set of others' notes considering of rest of this competition. My team tried many parameters-set of CatBoost, 'param_cat2' of these always scores the best.","0096a5d8":"## Check performance.\nlet's check the average score of test(validation data)'s AUC score('test_score') for K = 10. This is the performance of this model. ","6680035c":"### CatBoostClassifier\nMy team used CatBoosClassifier, XGBoostClassifier, LGBMClassifier. CatBoosClassifier of thses three models performed the best score. 0.81785. And then, import CatBoostClassifier and others. \n- Many teams used all of LGBM, Catboost, XGB and run code 'VottingClassifier(?)' with these models. My team is still considering about that. ","b5c04f37":"## Seprate: Numerical and Categorical and target","e374d6c0":"## Submission file\nLet's save submission file and submit.","0954fa46":"## Load Data\nload datas and reduce memory of datas right now.","c8ce1924":"## Importance Features\n\nLet's find out important features by CatBoost's method '.feature_importances_'\n\n- The most important features.\n- Not important features. \n- Visualizaton.","2e36eccf":"### Trying so many times,\nstill the first AUC score is the best. Maybe I and my team still needs study and research speicifically on parameters and model. By the way, the first version of this note will be set as default.\n\nThese below are tries I and My tram did, which brought us lower score than 0.81782\n- fold K = 5, 7 \n- drop column 'n_missing' or 'miss_cnt'\n- drop columns 'max_row', min_row', 'std', 'multiply' \n- CatBoostClassifier - early_stopping_rounds = 300 \n- StratifiedKFold - 'random_state': 42 -> 228\n- eli5 (failed because of Kaggle's error.)\n\nNext Week or on this weekends, we will try again after more study and reasearch. ","61a9af13":"## Last check before running model.","5b632b68":"## Import\nimport basic libraries first.","f12324c6":"### Got errors on my Kaggle Notebooks\n\nRight now, I faced unexpected errors with pop-ip message 'Failed to initialize Firebase client' or 'An error occurred while committing kernel: Firebase: No Firebase App '[DEFAULT]' has been created - call Firebase App.initializeApp() (app\/no-app)'. These seem to be porblems of Kaggle's Backend. Don't know why the exactly reason yet.\n\n","c5129232":"## Robust Scaler\nAccording to last [EDA note](https:\/\/www.kaggle.com\/heiswicked\/smtm-s-tps-sep-2021-eda). We need Robust scaling of numerical columns (X_num, test_num) for better performance and dealing with outliers.","50ad8eef":"### Save RAM\nReducing data for saving RAM. I think this is essential in this Competition. Without this code, you have to continue to restart runtime again and again because of RAM. It doesn't matter if RAM of your PC or laptop is more than 16GB. \n- just changed name of function"}}