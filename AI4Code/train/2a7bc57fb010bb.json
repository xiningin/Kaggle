{"cell_type":{"93272a7d":"code","350b4d49":"code","6b08e98b":"code","aef363e0":"code","e3c92fc8":"code","c0ae434d":"code","ee0b73de":"code","e62c2f9a":"code","a202b4de":"code","1506baea":"code","37d2be48":"code","4c573607":"code","97acef1a":"code","acdaddb8":"code","0fe1c6b0":"code","d1d209ab":"code","2d6e40b8":"code","c4e5e3d4":"code","b882942b":"code","e9d9c6f0":"code","8c738c56":"code","9ffc302c":"code","499dbc77":"code","523936f9":"code","9974f31c":"code","717f7bdc":"code","fc25520d":"code","a12f5d69":"code","6b85ecfb":"code","9f34605b":"code","10817224":"code","8ec30271":"code","77f3f0c3":"code","a94114a5":"code","f8a28503":"code","7d47f996":"code","e839ef8a":"code","39bd63e8":"code","340c4b37":"code","9c1af541":"code","d0381ed6":"code","74cab026":"code","89177d03":"code","451fea77":"code","327ad936":"code","18cfaefe":"code","affd0f9f":"code","665c0e6f":"code","8ee37c97":"code","7c926002":"code","90f2e4ff":"code","8888d295":"code","45a987b5":"code","e22d79dd":"code","7e27dd3d":"code","ae295426":"code","f75183b0":"code","7aea946a":"code","5a9c2e6b":"code","ca32d369":"code","04e2dc9c":"code","47d285ab":"code","87f52791":"code","34d4cd18":"code","90bcdb7a":"markdown","fc087e94":"markdown","80d7ec0b":"markdown","025bd99a":"markdown","e6a58305":"markdown","c53f09bd":"markdown","f6dc071c":"markdown","c07a5c34":"markdown","551b1d47":"markdown","f7015f9b":"markdown","093dc379":"markdown","f4d04dd6":"markdown","a6f83818":"markdown","1b53149e":"markdown","8be3b40b":"markdown","0febeb72":"markdown","3ad35c50":"markdown","27cbc00c":"markdown","f82b8ce3":"markdown","eb748c3f":"markdown"},"source":{"93272a7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nfilenames = check_output([\"ls\", \"..\/input\/\"]).decode(\"utf8\").strip()\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","350b4d49":"# import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec \nfrom scipy import stats\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor,AdaBoostRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.svm import SVR, LinearSVR\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import  make_scorer, mean_squared_error,mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nimport math\n\n\n# pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","6b08e98b":"df = pd.read_csv('..\/input\/consumer-credit-card-prediction\/TRAIN.csv').drop(['id'], axis = 1)\nprint(df.shape)\ndf.head()","aef363e0":"df_test = pd.read_csv('..\/input\/consumer-credit-card-prediction\/TEST.csv')\ntest_id = df_test['id']\ndf_test.drop(['id'], axis = 1, inplace = True)\nprint(df_test.shape)\ndf_test.head()","e3c92fc8":"df_submission = pd.read_csv('..\/input\/consumer-credit-card-prediction\/sample_submission.csv')\nprint(df_submission.shape)\ndf_submission.head()","c0ae434d":"! pip install openpyxl","ee0b73de":"pip install --upgrade pip","e62c2f9a":"pip install xlrd","a202b4de":"df_data_dict = df_submission = pd.read_excel('..\/input\/consumer-credit-card-prediction\/Data_Dictionary.xlsx')\ndf_data_dict","1506baea":"df.info()","37d2be48":"df.describe()","4c573607":"# df = df[(df > 0).all(axis=1)]\n# cat_cols1 = df_test.select_dtypes(include=[object]).columns.to_list()\n\ndf_cat = df.select_dtypes(include=[object])\ndf_num = df.select_dtypes(include=['int64','float64'])\n","97acef1a":"df_num[df_num < 0] = 0\ndf_num","acdaddb8":"df = pd.concat([df_num,df_cat], axis=1)\ndf","0fe1c6b0":"df.describe()","d1d209ab":"def two_chart_plot(df, feature):\n    fig = plt.figure(constrained_layout = True, figsize = (20, 5))\n    grid = gridspec.GridSpec(ncols = 2, nrows = 2, figure = fig)\n    \n    ax1 = fig.add_subplot(grid[0, :1])\n    ax1.set_title('Histogram')\n    sns.distplot(df.loc[:, feature], norm_hist = True, ax = ax1)\n    plt.axvline(x = df[feature].mean(), c = 'red')\n    plt.axvline(x = df[feature].median(), c = 'green')\n    \n    ## Customizing the Box Plot. \n    ax2 = fig.add_subplot(grid[:, 1])\n    ## Set title. \n    ax2.set_title('Box Plot')\n    sns.boxplot(df.loc[:,feature], orient = 'v', ax = ax2 )\n    \ntwo_chart_plot(df, 'cc_cons')","2d6e40b8":"from scipy.stats import skew\nprint(\"Skewness of credit catd consumption is\", df['cc_cons'].skew())","c4e5e3d4":"df.loc[df['cc_cons'] >= 100000 ].shape","b882942b":"df.drop(df[df['cc_cons'] >= 100000].index, inplace = True)","e9d9c6f0":"two_chart_plot(df, 'cc_cons')\nprint(\"Skewness of credit catd consumption is\", df['cc_cons'].skew())","8c738c56":"two_chart_plot(df, 'age')","9ffc302c":"df.loc[df['age'] >= 100 ].shape","499dbc77":"df.drop(df[df['age'] >= 100].index, inplace = True)","523936f9":"two_chart_plot(df, 'age')","9974f31c":"sns.countplot(x=\"gender\", data=df,hue=\"account_type\");\ndf.groupby([\"gender\" , \"account_type\"]).count()[\"age\"]","717f7bdc":"sns.barplot(x=\"gender\" , y=\"cc_cons\",hue=\"account_type\", data=df)","fc25520d":"sns.barplot(x=[\"april\", \"may\", \"june\"],y=[df[\"cc_cons_jun\"].sum(),df['cc_cons_may'].sum(),df[\"cc_cons_jun\"].sum()])","a12f5d69":"import missingno as msno\n%matplotlib inline\nmsno.matrix(df)","6b85ecfb":"def null_table(data):\n    \"\"\"\n    A function which returns the number and percentage of null values in the given dataset.\n    \"\"\"\n    indices = data.isnull().sum().index\n    values = data.isnull().sum().values\n    percentages = []\n    for i in indices:\n        percentages.append((data[i].isnull().sum() \/ data[i].shape[0]) * 100)\n    d = {'Columns' : indices, 'Count of Null Values' : values, 'Null_values_percentage' : percentages}\n    # data = dict(zip(indices, percentages))\n    null_frame = pd.DataFrame(data = d)\n    return null_frame","9f34605b":"null_frame_train = null_table(df)\nnull_frame_train.sort_values(by = 'Null_values_percentage')\n# null_frame_train","10817224":"null_frame_test = null_table(df_test)\nnull_frame_test.sort_values(by = 'Null_values_percentage')","8ec30271":"def msv_1(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh +10, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 3, f'Columns with less than {thresh}% missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\n\nmsv_1(df, 45, color=sns.color_palette('Reds',15))\n","77f3f0c3":"df.columns","a94114a5":"# summing up the columns \ndf[\"investment\"]= df[['investment_1','investment_2','investment_3','investment_4']].sum(axis=1) \ndf[\"totat_active_loan\"]= df[['personal_loan_active','vehicle_loan_active']].sum(axis=1) \ndf[\"total_closed_loan\"]= df[['personal_loan_closed','vehicle_loan_closed']].sum(axis=1)\n\n# taking mean of the columns\ndf['cc_cons_old'] = df[['cc_cons_apr', 'cc_cons_may','cc_cons_jun']].mean(axis=1)\ndf['dc_cons'] = df[['dc_cons_apr', 'dc_cons_may','dc_cons_jun']].mean(axis=1)\ndf['cc_count'] = df[['cc_count_apr', 'cc_count_may','cc_count_jun']].mean(axis=1)\ndf['dc_count'] = df[['dc_count_apr', 'dc_count_may','dc_count_jun']].mean(axis=1)\ndf['debit_amount'] = df[['debit_amount_apr', 'debit_amount_may','debit_amount_jun']].mean(axis=1)\ndf['credit_amount'] = df[['credit_amount_apr', 'credit_amount_may','credit_amount_jun']].mean(axis=1)\ndf['max_credit_amount'] = df[['max_credit_amount_apr', 'max_credit_amount_may','max_credit_amount_jun']].mean(axis=1)\ndf['debit_count'] = df[['debit_count_apr', 'debit_count_may','debit_count_jun']].mean(axis=1)\ndf['credit_count'] = df[['credit_count_apr', 'credit_count_may','credit_count_jun']].mean(axis=1)","f8a28503":"# summing up the columns \ndf_test[\"investment\"]= df_test[['investment_1','investment_2','investment_3','investment_4']].sum(axis=1) \ndf_test[\"totat_active_loan\"]= df_test[['personal_loan_active','vehicle_loan_active']].sum(axis=1) \ndf_test[\"total_closed_loan\"]= df_test[['personal_loan_closed','vehicle_loan_closed']].sum(axis=1)\n\n# taking mean of the columns\ndf_test['cc_cons_old'] = df_test[['cc_cons_apr', 'cc_cons_may','cc_cons_jun']].mean(axis=1)\ndf_test['dc_cons'] = df_test[['dc_cons_apr', 'dc_cons_may','dc_cons_jun']].mean(axis=1)\ndf_test['cc_count'] = df_test[['cc_count_apr', 'cc_count_may','cc_count_jun']].mean(axis=1)\ndf_test['dc_count'] = df_test[['dc_count_apr', 'dc_count_may','dc_count_jun']].mean(axis=1)\ndf_test['debit_amount'] = df_test[['debit_amount_apr', 'debit_amount_may','debit_amount_jun']].mean(axis=1)\ndf_test['credit_amount'] = df_test[['credit_amount_apr', 'credit_amount_may','credit_amount_jun']].mean(axis=1)\ndf_test['max_credit_amount'] = df_test[['max_credit_amount_apr', 'max_credit_amount_may','max_credit_amount_jun']].mean(axis=1)\ndf_test['debit_count'] = df_test[['debit_count_apr', 'debit_count_may','debit_count_jun']].mean(axis=1)\ndf_test['credit_count'] = df_test[['credit_count_apr', 'credit_count_may','credit_count_jun']].mean(axis=1)","7d47f996":"cols_to_delete_1 = ['investment_1','investment_2','investment_3','investment_4','personal_loan_active','vehicle_loan_active',\n                    'personal_loan_closed','vehicle_loan_closed','cc_cons_apr', 'cc_cons_may','cc_cons_jun',\n                    'dc_cons_apr', 'dc_cons_may','dc_cons_jun','cc_count_apr', 'cc_count_may','cc_count_jun',\n                    'dc_count_apr', 'dc_count_may','dc_count_jun','debit_amount_apr', 'debit_amount_may','debit_amount_jun',\n                    'credit_amount_apr', 'credit_amount_may','credit_amount_jun',\n                    'max_credit_amount_apr', 'max_credit_amount_may','max_credit_amount_jun','debit_count_apr',\n                    'debit_count_may','debit_count_jun','credit_count_apr', 'credit_count_may','credit_count_jun']","e839ef8a":"df.drop(cols_to_delete_1, axis = 1, inplace = True)\ndf_test.drop(cols_to_delete_1, axis = 1, inplace = True)","39bd63e8":"df.head()","340c4b37":"msv_1(df, 5, color=sns.color_palette('Reds',15))","9c1af541":"null_frame_train = null_table(df)\nnull_frame_train.sort_values(by = 'Null_values_percentage').tail()","d0381ed6":"df.loan_enq.value_counts()","74cab026":"df['loan_enq'].fillna(0, inplace=True)\ndf['loan_enq'].replace(to_replace=dict(Y = 1, NaN = 0), inplace=True)\n\ndf_test['loan_enq'].fillna(0, inplace=True)\ndf_test['loan_enq'].replace(to_replace=dict(Y = 1, NaN = 0), inplace=True)\n","89177d03":"df.loan_enq.head(10)","451fea77":"na_cols = list(df.columns[df.isnull().any()].values)\nna_cols","327ad936":"df.fillna(0, inplace=True)\ndf_test.fillna(0, inplace=True)","18cfaefe":"df.info()","affd0f9f":"df_test.info()","665c0e6f":"null_frame_train = null_table(df)\nnull_frame_train.sort_values(by = 'Null_values_percentage')","8ee37c97":"msno.matrix(df)","7c926002":"cat_cols1 = df.select_dtypes(include=[object]).columns.to_list()\ncat_cols1","90f2e4ff":"df = pd.get_dummies(df, prefix=['account_type', 'gender'], drop_first=True)\ndf_test = pd.get_dummies(df_test, prefix=['account_type', 'gender'], drop_first=True)","8888d295":"num = df.select_dtypes(exclude = 'object')\ncorr_num = num.corr()\n\nf, ax = plt.subplots(figsize = (17, 1))\n\nsns.heatmap(corr_num.sort_values(by = ['cc_cons'], ascending = False).head(1), cmap='PuRd_r')\n\nplt.title('Correlation of Numerical Features with the Target', weight = 'bold', fontsize = 18)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold', color='dodgerblue', rotation=0)\n\nplt.show()","45a987b5":"correlation = df.corr()['cc_cons'].sort_values(ascending = False).to_frame()\ncmap = sns.light_palette(\"cyan\", as_cmap = True)\ns = correlation.style.background_gradient(cmap = cmap)\ns","e22d79dd":"# split train-test\n\nX = df.drop(columns='cc_cons')\ny = df.loc[:,'cc_cons']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","7e27dd3d":"def rmsle(y, y0):\n    return np.sqrt(np.mean(np.square(np.log1p(y) - np.log1p(y0))))\nrmsle_score = make_scorer(rmsle, greater_is_better=False)","ae295426":"models=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor(),\n       XGBRegressor(),ElasticNet(),GradientBoostingRegressor(),DecisionTreeRegressor(), Lasso(),LinearRegression(),LGBMRegressor(),\n       CatBoostRegressor(),Ridge()]\n\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor','XGBRegressor','ElasticNet'\n            'GradientBoostingRegressor','DecisionTreeRegressor',' Lasso','LinearRegression','LGBMRegressor','CatBoostRegressor',\n            'Ridge']\nrmsle=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    test_pred=clf.predict(X_test)\n    rmsle.append(np.sqrt(np.mean(np.square(np.log1p(test_pred) - np.log1p(y_test)))))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \n# d((key, round(val, 2)) for key, val in d.items()) \n","f75183b0":"rmsle_frame=pd.DataFrame.from_dict(d, orient='index').transpose()\nrmsle_frame","7aea946a":"sns.factorplot(y='Modelling Algo',x='RMSLE',data=rmsle_frame,kind='bar',size=5,aspect=2)\n","5a9c2e6b":"g = sns.factorplot(x='Modelling Algo',y='RMSLE',data=rmsle_frame,kind='point',size=5,aspect=2)\ng.set_xticklabels(rotation=90)\n","ca32d369":"# I will try afterward to run this on google colab to check if the grid search can help us getting a better SVR model. \n\n# parameters = [{'kernel': ['rbf'],\n#                'epsilon': [0.1, 0.2, 0.5],\n#                'gamma': [1e-4, 0.01, 0.1, 0.5],\n#                'C': [1, 10, 100]\n#               },\n#               {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n#              ]\n\n# svr_cv = SVR()\n# regressor_cv = GridSearchCV(svr_cv, param_grid = parameters)\n# regressor_cv.fit(X_train, y_train)","04e2dc9c":"svr = SVR().fit(X_train,y_train)\n\ny_pred_svr = svr.predict(df_test)","47d285ab":"submission_svr = pd.DataFrame({\n        \"id\": test_id,\n        \"cc_cons\": y_pred_svr\n    })","87f52791":"submission_svr.to_csv('submission_svr.csv', index=False)","34d4cd18":"sub_svr = pd.read_csv('.\/submission_svr.csv')\nsub_svr","90bcdb7a":"Red line in histogram indicates the mean of the cc_cons and the Green line indicates the median.\n\nFrom the above plots, we can observe that the consumption is not normally distributed. From the boxplot we can observe that the dataset has too many outliers.\n\nlets check the skewness value of the target vatriable distribution","fc087e94":"We will also fill the other columns along with dc_count and dc_cons with missing valuesconsidering there were no transactions were made by those customers.","80d7ec0b":"Lets see which month has got maximum credit card consumption","025bd99a":"> - As we can see that SVR is having least rmsle score, we will do some hyperparameter tunuing to check if the model improves.","e6a58305":"## Encoding Categorical Variables","c53f09bd":"# Exploratory Data Analysis","f6dc071c":"# Credit Card Consumption Prediction","c07a5c34":"There are only 16 rows with age more than 100 and also they seems to be wrong as having more than 100 years of customer's age sounds unreal so we will drop these outliers","551b1d47":"We can observe that may month has got almost 1.5 times more spending while april and june have almost same amount.","f7015f9b":"## Feature Engineering","093dc379":"The number of males are almost 5 times than that of females and the current accounts are ","f4d04dd6":"Loan Enquiry has stil got 98% NaN values. We will replace Y with 1 and NaN with zero as the NaNs are likely to be people who have not made any enquiry.\n\n","a6f83818":"So lets try removing the outliers from the data and see if the skewness decreases","1b53149e":"## Handling Missing Values\nThere is an amazing library called missingno which helps us to visualize the number of Null values present in each feature.","8be3b40b":"The grid search was taking too much time on my kaggle kernel so I decided to use SVR without any specif parameters.\n","0febeb72":"We can see that the data has got too many null values so let us see the actual percentage of data missing in each feature.","3ad35c50":"The ideal skewness value should be to between -0.5 and 0.5, for data to be fairly symmetrical and If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed. But here the skewness is too high so the data is[](http:\/\/) highly skewed.","27cbc00c":"Though the featute still containss many outliers but the skewness has certainly decreased a lot","f82b8ce3":"### Credit Card Consumption Feature Analysis\n\nLet's Explore the target feature i.e. Credit Card Consumption ('cc_cons') ","eb748c3f":"- We can see that all the four investments have most of the rows null individually soo we wil add all of them and see if it reduces our null values.\n\n- Also same can be by adding  both the active loans and closed loans\n\n- credit and debit card consumptions, credit and debit card counts, debit and credit card amounts, debit and credit counts and maximum credit amount for different months majorly have got more than 45% data as null. So we can either add them all or take an average. As the target column is average consumption of all the 3 months, taking  a mean of all 3 individual month makes more sense. So we will make new features and drop the individual ones"}}