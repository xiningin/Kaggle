{"cell_type":{"e6a9a3d9":"code","115c1442":"code","fcd0da1e":"code","b4d6a450":"code","70207902":"code","347201ef":"code","c035c570":"code","2db5afc5":"code","228bbfa1":"code","563ca094":"code","b9803cf5":"code","2d6b4870":"code","e50687e1":"code","740cad72":"code","3afe80b5":"code","cc32e432":"code","ca866e47":"code","ce2563ad":"code","0a39c0ca":"code","4dd89482":"code","afb9bc96":"code","24f80416":"code","ba15ff2a":"code","0b7c437a":"code","18682710":"code","2f79213d":"code","8201d4c6":"code","c99c5c4c":"code","8d183a85":"code","bf818b5d":"code","92591542":"code","69f6349e":"code","129da981":"code","bea30cb9":"code","815ae212":"code","86d17b4f":"code","39c0387a":"code","190dc233":"code","75cff3a2":"code","dfef1019":"code","f13115fe":"code","c11e5825":"code","86058337":"code","04738daa":"code","86cb7f39":"code","0588462e":"code","98c4724d":"code","cc878e05":"code","0ee041cb":"code","a1de4fe3":"code","d93b4c82":"code","9e5fecf8":"code","739f3308":"code","c3323142":"code","17cb7c81":"code","ffa2ddd3":"code","693cde51":"code","94d130d6":"code","3cff06f5":"code","cea51fe4":"code","1d4a1c27":"code","0bbd7f43":"code","2d4f167e":"code","f01b0f56":"code","a9bdee9f":"code","22a0d342":"code","97cff629":"code","649c6dd4":"code","b9c73d84":"code","9cad5c45":"code","4792775e":"code","dbb9619e":"code","ec5ec9be":"code","37e2ef76":"code","e426cfad":"code","5eba79a2":"code","e5845cc2":"code","f1fa793d":"code","804c74bb":"code","86072899":"code","522448b3":"code","2148fc2b":"code","b42d5e29":"code","c610e0cb":"code","acc5ff95":"code","734fa10a":"code","dbd7b8ab":"code","41978303":"code","c712b21a":"code","38c52f7c":"markdown","1e0d696c":"markdown","3f4870b0":"markdown","f5b21884":"markdown","5031442a":"markdown","e9b636d5":"markdown","a54bdd63":"markdown","5d9b8b9a":"markdown","22c4b813":"markdown","9d3637c2":"markdown","bdfad453":"markdown","6b6982d9":"markdown","8c49bcd1":"markdown","9245b4d2":"markdown","1ff84c8a":"markdown","ae4dbe77":"markdown","2f05558a":"markdown","3dae049a":"markdown","5cbccf3d":"markdown","6ac2c284":"markdown","ee345da9":"markdown","a870bf22":"markdown","a2733c52":"markdown","66cfbc48":"markdown"},"source":{"e6a9a3d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","115c1442":"# Import data analysis tools \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","fcd0da1e":"# import training and validation sets \ndf=pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n              low_memory=False)\ndf.head()","b4d6a450":"df.info()","70207902":"df.isna().sum()","347201ef":"fig,ax= plt.subplots()\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","c035c570":"df['SalePrice'].plot.hist(bins=20)","2db5afc5":"df.saledate[:100]","228bbfa1":"df.saledate.dtype","563ca094":"df=pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n              low_memory=False,\n              parse_dates=['saledate'])","b9803cf5":"# With parse_dates... check dtype of \"saledate\"\ndf.info()","2d6b4870":"df.saledate.dtype","e50687e1":"type('<M8[ns]')==type('datetime64[ns]')","740cad72":"fig,ax=plt.subplots(figsize=[16,6])\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","3afe80b5":"df.head()","cc32e432":"df.head().T","ca866e47":"df.saledate[:10]","ce2563ad":"#Sort Dataframe in date order\n\ndf.sort_values(by=[\"saledate\"], inplace=True, ascending=True)","0a39c0ca":"df.head()","4dd89482":"df.saledate.head(12)","afb9bc96":"#Make a copy of the original Dataframe to preform edits on.\ndf_tmp=df.copy()","24f80416":"df_tmp","ba15ff2a":"# Add datetime parameters for saledate\ndf_tmp[\"saleYear\"]=df_tmp['saledate'].dt.year\ndf_tmp[\"saleMonth\"]=df_tmp['saledate'].dt.month\ndf_tmp[\"saleDay\"]=df_tmp['saledate'].dt.day\ndf_tmp[\"saleDayOfWeek\"]=df_tmp['saledate'].dt.dayofweek\ndf_tmp[\"saleDayOfYear\"]=df_tmp['saledate'].dt.dayofyear","0b7c437a":"df_tmp.drop('saledate',axis=1,inplace=True)","18682710":"df_tmp.head().T","2f79213d":"# Check the different values of different columns\ndf_tmp.state.value_counts()","8201d4c6":"# Check for missing categories and different datatypes\ndf_tmp.info()","c99c5c4c":"# Check for missing values\ndf_tmp.isna().sum()","8d183a85":"pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])","bf818b5d":"# These columns contain strings\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","92591542":"# If you're wondering what df.items() does, let's use a dictionary as an example\nrandom_dict={'key1':'val1',\n            'key2':'vam222'}\nfor key,value in random_dict.items():\n    print('this is the key' ,key,' and this is it\\'s content ',value)","69f6349e":"# This will turn all of the string values into category values\nfor label , content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label]=content.astype('category').cat.as_ordered()","129da981":"df_tmp.info()","bea30cb9":"df_tmp.head().T","815ae212":"df_tmp.state.cat.codes","86d17b4f":"states_code=df_tmp.state.cat.categories\ndict(enumerate(states_code))","39c0387a":"#the percentage of null value in the dataframe \ndf_tmp.isnull().sum()\/len(df_tmp)","190dc233":"# Save preprocessed data\ndf_tmp.to_csv('train_tmp.csv',\n             index=False)","75cff3a2":"# Import preprocessed data\ndf_tmp=pd.read_csv('train_tmp.csv',\n                  low_memory=False)\n","dfef1019":"df_tmp.head().T","f13115fe":"df_tmp.isnull().sum()","c11e5825":"for label , content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","86058337":"# Check for which numeric columns have null values\nfor label ,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)\n        ","04738daa":"# Fill numeric rows with the median\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells if the data was missing our not\n            df_tmp[label+\"_is_missing\"]=pd.isnull(content)\n            # Fill missing numeric values with median since it's more robust than the mean\n            df_tmp[label]=content.fillna(content.median())","86cb7f39":"# Check if there's any null values\nfor label ,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","0588462e":"# Check to see how many examples were missing\ndf_tmp.auctioneerID_is_missing.value_counts()","98c4724d":"# Check columns which *aren't* numeric\ni=1\nfor label , content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(i, label)\n        i+=1","cc878e05":"i=1\nfor label ,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(i, label)\n            i+=1","0ee041cb":"# Turn categorical variables into numbers\nfor label ,content in df_tmp.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df_tmp[label+\"_is_missing\"]=pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df_tmp[label]=pd.Categorical(content).codes+1","a1de4fe3":"pd.Categorical(df_tmp.state).codes","d93b4c82":"df_tmp.info()","9e5fecf8":"df_tmp.isna().sum()","739f3308":"df_tmp.head().T","c3323142":"from sklearn.ensemble import RandomForestRegressor","17cb7c81":"%%time\n# Instantiate model\nmodel=RandomForestRegressor(n_jobs=-1)\n# Fit the model\n","ffa2ddd3":"df_tmp.saleYear.value_counts()","693cde51":"%%time\n# Split data into training and validation\ndf_val = df_tmp[df_tmp['saleYear']==2012]\ndf_train =df_tmp[df_tmp['saleYear'] != 2012]\nlen(df_val), len(df_train)","94d130d6":"#  Split data into X & y\nX_train , y_train= df_train.drop('SalePrice',axis=1),df_train.SalePrice\nX_valid , y_valid = df_val.drop('SalePrice' , axis=1), df_val.SalePrice\n\nX_train.shape , y_train.shape , X_valid.shape , y_valid.shape","3cff06f5":"y_train","cea51fe4":"# Create evaluation function (the competition uses RMSLE)\nfrom sklearn.metrics import mean_absolute_error , mean_squared_log_error ,r2_score\n\ndef rmsle(y_test,y_pred):\n    \"\"\"\n    Caculates root mean squared log error between predictions and\n    true labels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test,y_pred))\n\n# Create function to evaluate model on a few different levels\ndef show_scores(model):\n    train_preds=model.predict(X_train)\n    val_preds=model.predict(X_valid)\n    scores={\"Training MAE\":mean_absolute_error(y_train,train_preds),\n            \"Valid MAE\":mean_absolute_error(y_valid,val_preds),\n            \"Training RMSLE\":rmsle(y_train,train_preds),\n            \"Valid RMSLE\":rmsle(y_valid,val_preds),\n            \"Training R^2\":r2_score(y_train,train_preds),\n            \"Valid R^2\":r2_score(y_valid,val_preds),\n           }\n    return scores","1d4a1c27":"len(X_train)","0bbd7f43":"X_train.info()","2d4f167e":"# Change max_samples value\nmodel=RandomForestRegressor(n_jobs=-1,\n                           random_state=42,\n                           max_samples=10000)","f01b0f56":"%%time\n# Cutting down on the max number of samples each estimator can see improves training time\nmodel.fit(X_train,y_train)\n### Wall time: 18.2 s","a9bdee9f":"y_pi=model.predict(X_valid)","22a0d342":"rmsle(y_pi,y_valid)","97cff629":"%%time\nshow_scores(model)","649c6dd4":"# Find the best model hyperparameter\nrs_mode.best_params_","b9c73d84":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n# Different RandomForestRegressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2),\n           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n\n# Instantiate RandomizedSearchCV model\nrs_mode=RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                 n_estimators=2),\n                           param_distributions=rf_grid,\n                           n_iter=2,\n                           cv=5,\n                           verbose=True)\n# Fit the RandomizedSearchCV model\nrs_mode.fit(X_train,y_train)","9cad5c45":"show_scores(rs_mode)","4792775e":"%%time\n# Most ideal hyperparameters\nideal_model=RandomForestRegressor(n_estimators=90,\n                                    min_samples_leaf=1,\n                                    min_samples_split=14,\n                                    max_features=0.5,\n                                    n_jobs=-1,\n                                    max_samples=None)\nideal_model.fit(X_train,y_train)","dbb9619e":"show_scores(ideal_model)","ec5ec9be":"# Import the test data\ndf_test = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/Test.csv\",\n                      low_memory=False,\n                      parse_dates=[\"saledate\"])\n\ndf_test.head()","37e2ef76":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n    \n    df.drop(\"saledate\", axis=1, inplace=True)\n    \n    # Fill the numeric rows with median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                # Add a binary column which tells us if the data was missing or not\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n    \n        # Filled categorical missing data and turn categories into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add +1 to the category code because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1\n    \n    return df","e426cfad":"# Process the test data \ndf_test = preprocess_data(df_test)\ndf_test.head()","5eba79a2":"X_train.head()","e5845cc2":"# We can find how the columns differ using sets\nset(X_train.columns) - set(df_test.columns)","f1fa793d":"# Manually adjust df_test to have auctioneerID_is_missing column\ndf_test[\"auctioneerID_is_missing\"] = False\ndf_test.head()","804c74bb":"# Make predictions on the test data\ntest_preds = ideal_model.predict(df_test)","86072899":"test_preds","522448b3":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","2148fc2b":"# Export prediction data\ndf_preds.to_csv(\"test_predictions.csv\", index=False)","b42d5e29":"submission=df_preds\nsubmission.to_csv(\"submission.csv\", index = False)","c610e0cb":"# Find feature importance of our best model\nideal_model.feature_importances_","acc5ff95":"import seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","734fa10a":"plot_features(X_train.columns,ideal_model.feature_importances_)","dbd7b8ab":"sum(ideal_model.feature_importances_)","41978303":"df.ProductSize.isna().sum()","c712b21a":"df.ProductSize.value_counts()","38c52f7c":"## Fill missing values\n\nFrom our experience with machine learning models. We know two things:\n\n* 1.All of our data has to be numerical\n* 2.There can't be any missing values\nAnd as we've seen using df_tmp.isna().sum() our data still has plenty of missing values.\n\nLet's fill them.\n\nFilling numerical values first\nWe're going to fill any column with missing values with the median of that column.","1e0d696c":"\nAccording to the Kaggle data page, the validation set and test set are split according to dates.\n\nThis makes sense since we're working on a time series problem.\n\nE.g. using past events to try and predict future events.\n\nKnowing this, randomly splitting our data into train and test sets using something like train_test_split() wouldn't work.\n\nInstead, we split our data into training, validation and test sets using the date each sample occured.\n\nIn our case:\n\nTraining = all samples up until 2011\nValid = all samples form January 1, 2012 - April 30, 2012\nTest = all samples from May 1, 2012 - November 2012\nFor more on making good training, validation and test sets, check out the post How (and why) to create a good validation set by Rachel Thomas.mm","3f4870b0":"### Testing our model on a subset (to tune the hyperparameters)","f5b21884":"Excellent, our processed DataFrame has the columns we added to it but it's still missing values.\n\n","5031442a":"### Make predictions on test data\n","e9b636d5":"All of our data is categorical and thus we can now turn the categories into numbers, however it's still missing values...","a54bdd63":"### Parsing dates\nWhen working with time series data, it's a good idea to make sure any date data is the format of a datetime object (a Python data type which encodes specific information about dates).","5d9b8b9a":"\n### Building an evaluation function\nAccording to Kaggle for the Bluebook for Bulldozers competition, the evaluation function they use is root mean squared log error (RMSLE).\n\nRMSLE = generally you don't care as much if you're off by $10 as much as you'd care if you were off by 10%, you care more about ratios rather than differences. MAE (mean absolute error) is more about exact differences.\n\nIt's important to understand the evaluation metric you're going for.\n\nSince Scikit-Learn doesn't have a function built-in for RMSLE, we'll create our own.\n\nWe can do this by taking the square root of Scikit-Learn's mean_squared_log_error (MSLE). MSLE is the same as taking the log of mean squared error (MSE).\n\nWe'll also calculate the MAE and R^2 for fun.","22c4b813":"### Convert strings to categories\nOne way to help turn all of our data into numbers is to convert the columns with the string datatype into a category datatype.\n\nTo do this we can use the pandas types API which allows us to interact and manipulate the types of data.","9d3637c2":"\n### Train a model with the best parameters\nIn a model I prepared earlier, I tried 100 different combinations of hyperparameters (setting n_iter to 100 in RandomizedSearchCV) and found the best results came from the ones you see below.\n\nNote: This kind of search on my computer (n_iter = 100) took ~2-hours. So it's kind of a set and come back later experiment.\n\nWe'll instantiate a new model with these discovered hyperparameters and reset the max_samples back to its original value.","bdfad453":"Now we've got our tools for data analysis ready, we can import the data and start to explore it.\n\nFor this project, we've downloaded the data from Kaggle","6b6982d9":"### Add datetime parameters for saledate column\nWhy?\n\nSo we can enrich our dataset with as much information as possible.\n\nBecause we imported the data using read_csv() and we asked pandas to parse the dates using parase_dates=[\"saledate\"], we can now access the different datetime attributes of the saledate column.","8c49bcd1":"### Preprocessing the data (getting the test dataset in the same format as our training dataset)","9245b4d2":"## Hyerparameter tuning with RandomizedSearchCV","1ff84c8a":"In the format it's in, it's still good to be worked with, let's save it to file and reimport it so we can continue on.\n\n## Save Processed Data\n","ae4dbe77":"# predection the Sale Price of Bulldozers using Machine Lerning\nin this Notebook we are going to go through an example machine lerning project with a goal of predecting the sale price of bulldozers.\n## 1. Problem definition\nThe goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\n\nFast Iron is creating a \"blue book for bull dozers,\" for customers to value what their heavy equipment fleet is worth at auction.\n## 2.Data\n\nThe data for this competition is split into three parts:\n\n* Train.csv is the training set, which contains data through the end of 2011.\n* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set * throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\n## 3. Evalution \nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\n## 4. Features\nkaggle provides a data dictionary detailling all of the features of the dataset. You can view this data dictionay on google Sheets  or look \"Data Dictionary.xlsx\" ","2f05558a":"### Filling and turning categorical variables to numbers\nNow we've filled the numeric values, we'll do the same with the categorical values at the same time as turning them into numbers.","3dae049a":"### Feature Importance\nFeature importance seeks to figure out which different attributes of the data were most importance when it comes to predicting the target variable (SalePrice).","5cbccf3d":"We've made some predictions but they're not in the same format Kaggle is asking for:","6ac2c284":"With these new hyperparameters as well as using all the samples, we can see an improvement to our models performance.\n\nYou can make a faster model by altering some of the hyperparameters. Particularly by lowering n_estimators since each increase in n_estimators is basically building another small model.\n\nHowever, lowering of n_estimators or altering of other hyperparameters may lead to poorer results.","ee345da9":"Why add a binary column indicating whether the data was missing or not?\n\nWe can easily fill all of the missing numeric values in our dataset with the median. However, a numeric value may be missing for a reason. In other words, absence of evidence may be evidence of absence. Adding a binary column which indicates whether the value was missing or not helps to retain this information.","a870bf22":"#### Make a copy of the original DataFrame\n\nSince we're going to be manipulating the data, we'll make a copy of the original DataFrame and perform our changes there.\n\nThis will keep the original DataFrame in tact if we need it again.","a2733c52":"\nFinally now our test dataframe has the same features as our training dataframe, we can make predictions!","66cfbc48":"#### Sort DataFrame by saledate\nAs we're working on a time series problem and trying to predict future examples given past examples, it makes sense to sort our data by date."}}