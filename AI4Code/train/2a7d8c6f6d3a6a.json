{"cell_type":{"3dc21b00":"code","fa4839ca":"code","05dd3f72":"code","930d6d15":"code","1b84ad1c":"code","a25122e7":"code","a9589576":"code","e171e495":"code","cc1854af":"code","96732ccb":"code","909bcac8":"code","ea6139d9":"code","9a6fc13c":"code","74e01766":"code","02aa394a":"markdown","e6187d48":"markdown","3a61fcc1":"markdown","11cddec2":"markdown","12457d1b":"markdown","d5a555ed":"markdown","d6304966":"markdown","247b5677":"markdown"},"source":{"3dc21b00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa4839ca":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nnp.random.seed(42)","05dd3f72":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf","930d6d15":"df.columns","1b84ad1c":"df[\"target\"].unique()","a25122e7":"df[\"target\"].value_counts()","a9589576":"df.info()\ndf.isna().sum()","e171e495":"x = df.drop(\"target\" , axis =1)\ny = df[\"target\"]","cc1854af":"x_train , x_test , y_train , y_test = train_test_split(x ,y , test_size=0.3)     # test by 30%\n","96732ccb":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(criterion='entropy', max_depth=7, n_estimators=70,\n                       n_jobs=-1)","909bcac8":"# testing\n# train (input , output)\nscalled = model.fit(x_train , y_train )\n\n# validate modle \n#print(model.score(x_train , y_train))\nprint(model.score(x_test , y_test))","ea6139d9":"from sklearn.model_selection import cross_val_score\nvalidator = cross_val_score(model ,x, y ,cv =5)\nprint(np.mean(validator)) ","9a6fc13c":"parameters ={'n_estimators':(10,30,50,70,90,100),\n            'criterion':('gini' , 'entropy'),\n            'max_depth':(3,5,7,9),\n            'max_features':('auto' , 'sqrt'),\n            'min_samples_split':(2,4,6)\n}\ngrid = GridSearchCV(RandomForestClassifier(n_jobs=-1,oob_score=False), param_grid=parameters ,cv =3 , verbose=True)\ntune_grid = grid.fit(x_train , y_train)\nprint(tune_grid.best_estimator_)","74e01766":"from sklearn.metrics import classification_report\ny_predicted = model.predict(x_test)\nprint(classification_report(y_test , y_predicted))","02aa394a":"It is better to compensate with the parameters resulting from the Tuning process in the model","e6187d48":"**report**","3a61fcc1":"# tune modle prams","11cddec2":"# **Training data**","12457d1b":"# **choose modle (estimator)**","d5a555ed":"really score for model","d6304966":"# **handle missing data**","247b5677":"# **Reaging and show data from csv file**"}}