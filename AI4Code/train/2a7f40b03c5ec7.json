{"cell_type":{"983e6e0d":"code","95b73231":"code","187c573d":"code","a5192c26":"code","2e29a69f":"code","b0c2a8f1":"code","5ee0923f":"code","794f7730":"code","5a0eb78b":"code","43659397":"code","70b25735":"code","320e1dc0":"code","471db8bb":"code","63ecbbcd":"code","b7a467c0":"code","829adfbc":"code","ebcea11a":"code","d3b7690b":"code","5b5a83d8":"code","e5f29ea8":"code","7ef514e4":"code","2db9186b":"code","ea75d8b8":"code","8294a688":"code","f0b524f1":"code","63e58a8b":"code","c03fd24c":"code","76ec56ea":"markdown","d5b68589":"markdown","4d7147be":"markdown","dac86660":"markdown","0679fd64":"markdown","5b6832a6":"markdown","f9caa800":"markdown","5f9c8179":"markdown","3d711825":"markdown","5d3dd599":"markdown","b0fc11dc":"markdown","4f09e61c":"markdown","ab8e8048":"markdown","99cffca7":"markdown","283f9d79":"markdown","e20b306a":"markdown"},"source":{"983e6e0d":"# importing mathematical and analytical tools\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# importing classification models\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier\n\n# importing evaluation tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_roc_curve , accuracy_score , precision_score , f1_score , recall_score","95b73231":"# importing the data\ndf = pd.read_csv(\"water_potability.csv\")","187c573d":"# shuffling the data\ndf = df.sample(frac = 1)","a5192c26":"x = df.drop(\"Potability\",axis=1)\ny = df.Potability\nx_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=42)","2e29a69f":"df.info()","b0c2a8f1":"# function to fill missing values\ndef fill_missing(df):\n    for column in df.columns:\n        if df[column].isna().sum():\n            \n            # if found a column with missing values , replace the empty space in it with the median value of the column\n            df[column].fillna(df[column].median() , inplace=True)\n    return df","5ee0923f":"x_train_filled = fill_missing(x_train)\nfor column in x_train_filled.columns:\n    # convert the training set to all integers\n    x_train_filled[column] = x_train_filled[column].astype(int)","794f7730":"# checking for missing values in preprocessed data\nx_train_filled.isna().sum()","5a0eb78b":"model_1 = LinearSVC()\nmodel_2 = LogisticRegression()\nmodel_3 = RandomForestClassifier()\nmodel_4 = KNeighborsClassifier()","43659397":"# fitting and scoring the training set\nmodel_1.fit(x_train_filled,y_train)\nscore_1 = model_1.score(x_train_filled,y_train)\n","70b25735":"model_2.fit(x_train_filled,y_train)\nscore_2 = model_2.score(x_train_filled,y_train)","320e1dc0":"model_3.fit(x_train_filled,y_train)\nscore_3 = model_3.score(x_train_filled,y_train)","471db8bb":"model_4.fit(x_train_filled,y_train)\nscore_4 = model_4.score(x_train_filled,y_train)","63ecbbcd":"score_data = {\"LinearSVC\" : score_1,\n              \"LogisticRegression\" : score_2,\n              \"RandomForesClassifier\" : score_3,\n              \"KNeighborsClassifier\" : score_4}\nfig , ax = plt.subplots(figsize = (10,8))\nax.bar(score_data.keys() , score_data.values())\nax.set(ylabel = \"Score on test data\",\n      title = \"Comparison of the classification models scores on training data\",\n      ylim = (0,1.2));","b7a467c0":"x_test_filled = fill_missing(x_test)\nfor column in x_test_filled.columns:\n    x_test_filled[column] = x_test_filled[column].astype(int)","829adfbc":"model_3_before_tuning = model_3.score(x_test_filled , y_test)\nmodel_3_before_tuning","ebcea11a":"grid = {\"criterion\" : [\"gini\" , \"entropy\"],\n       \"n_estimators\" : [50,70,90,110,130,150],\n       \"min_samples_split\" : np.arange(2,8,2),\n       \"max_features\" : [\"auto\", \"sqrt\", \"log2\"],\n       \"max_depth\": [None, 5, 10, 20, 30],\n       \"min_samples_leaf\": [1, 2, 4]}","d3b7690b":"# defining a new classifier model that uses grid search CV\ngs_clf = GridSearchCV(estimator=model_3,\n                      param_grid=grid, \n                      cv=5,\n                      verbose=2)","5b5a83d8":"gs_clf.fit(x_train_filled , y_train)","e5f29ea8":"gs_clf.best_params_","7ef514e4":"#best parameters = {'criterion': 'entropy',\n#                   'max_depth': 30,\n#                   'max_features': 'log2',\n#                   'min_samples_leaf': 4,\n#                   'min_samples_split': 4,\n#                   'n_estimators': 110}\n\n## this set of parameters will most probably fit your data as well and if it doesn't , sadly you will have to run the exhaustive search above\n## implement it in the gs_clf model and then proceed further....","2db9186b":"# this is the predicted potability values after tuning\ngs_y_preds = gs_clf.predict(x_test_filled)","ea75d8b8":"gs_y_preds","8294a688":"# scoring our model's performance on test data\nmodel_3_after_tuning = gs_clf.score(x_test_filled , y_test)","f0b524f1":"model_3_after_tuning","63e58a8b":"data = {\"before tuning\" : model_3_before_tuning,\n        \"after tuning\" : model_3_after_tuning}\nfig , ax = plt.subplots(figsize = (10,8))\nax.bar(data.keys() , data.values())\nax.set(ylabel = \"Score on test data\",\n      title = \"Comparison before and after tuning\",\n      ylim = (0,1.2));\nplt.axhline(y = model_3_before_tuning , color = \"r\", linestyle = \"--\")\nplt.axhline(y = model_3_after_tuning , color = \"g\", linestyle = \"--\")","c03fd24c":"print(f\"Accuracy: {accuracy_score(y_test, gs_y_preds)*100:.2f}%\")\nprint(f\"Precision: {precision_score(y_test, gs_y_preds)*100:.2f}%\")\nprint(f\"Recall: {recall_score(y_test, gs_y_preds)*100:.2f}%\")\nprint(f\"F1: {f1_score(y_test, gs_y_preds)*100:.2f}%\")","76ec56ea":"## Checking other parameters\nOur last step is to check for other parameters like accuracy , precision , f1 and recall to make sure that our model is correctly classifying the data and is not overfitting or underfitting.","d5b68589":"# Predicting water potability\n\nThe dataset for the following notebook has been taken from the link : https:\/\/www.kaggle.com\/adityakadiwal\/water-potability\n<br>\n\nThe parameters used in the dataset are :\n<br>\n1. pH value\n2. Hardness\n3. Solids (Total dissolved solids - TDS)\n4. Chloramines\n5. Sulfate\n6. Conductivity\n7. Organic_carbon\n8. Trihalomethanes\n9. Turbidity\n10. Potability (0 = Not potable , 1 = Potable)\n\n\nThe purpose of the following notebook is:\n* To analyse the given dataset using various classification models\n* To predict the potability of water on test dataset using the best model\n* To improve the model using exhaustive search by GridSearchCV","4d7147be":"## Hyperparameter tuning\nWe shall now define a grid that contains random values of hyperparameters of random forest model like number of estimators, maximum depth etc. which will be used in grid search CV.","dac86660":"## Woohoo!!\nAs we can see clearly, hyperparameter tuning through grid search CV significantly improved our model's performance on the test data. We can depict is graphically because graphs make us understand better","0679fd64":"## Splitting the data into train and test set\n    80 percent data is for training and rest for testing","5b6832a6":"# Thankyou","f9caa800":"Plotting a bar graph of the scores...","5f9c8179":"## Preprocessing the data\nHere we fill the missing values in our dataset , and since our entire dataset is numerical type, we do not need to perform categorical transformations.","3d711825":"Evaluating test data","5d3dd599":"## Modelling\nNow we start to fit our preprocessed training data into the different models that have been imported in this notebook , i.e.\n1. Linear SVC\n2. Logistic Regression\n3. Random Forest Classifier\n4. K Nearest Neighbor Classifier","b0fc11dc":"Clearly , we can see that the model has performed poorly on the test data as it was only capable of correcty predicting 63 percent of the data , therfore , we need to perform hyperparameter tuning on the random forest classifier.","4f09e61c":"From the above bar graph , it is quite clear that Random Forest Classifier has performed the best on training data . Hence , we shall use it to predict the target on test data.","ab8e8048":"We have got pretty good results for these parameters as well , so now we can rest assured and pat ourselves on the back for drastically improving the accuracy of our model.","99cffca7":"## Predictions on test data\nNow when we have finally chosen a model for fitting the test data , we can go ahead and preprocess the test data as well through the fill_missing function and converting it into integer format.\n","283f9d79":"## Importing the tools needed","e20b306a":"## Warning :\nThe cell below can take hours to run as it is an exhaustive search over 8100 different combinations of hyperparameters (it took almost 2 hours to run in my system with an i5 8th gen processor and 512 GB ssd) , so if you wish to avoid waiting for so much time , please consider the final parameters commented two cells after."}}