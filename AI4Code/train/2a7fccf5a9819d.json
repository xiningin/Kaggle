{"cell_type":{"52d51791":"code","83c00f5c":"code","073592b3":"code","4267fada":"code","080f7c77":"code","b367638e":"code","d2e0fa30":"code","7a113a8b":"code","fb54f7c9":"code","8556d56d":"code","aff76256":"code","dd6c4d54":"code","18e4eb18":"code","22350b50":"code","9b11a360":"code","5dd89602":"code","cc982699":"code","1de161a1":"code","1ff7fe3b":"code","4bd562af":"code","db403738":"code","32129ce2":"code","59f8a345":"code","dd1e0dd6":"code","e728071f":"code","3aa0b0e0":"code","add96070":"code","32ff77d8":"code","82978073":"code","77bee544":"code","3d0a5754":"code","5e805686":"code","664a31c7":"code","2bb69033":"code","32049348":"code","de2200f1":"code","b438c731":"code","b177241f":"code","0d34e42e":"code","59a4c176":"code","1e13bc68":"code","15858fd6":"code","996ba728":"code","baa72410":"code","7dc8ef94":"code","5c91bf03":"code","4b3a3a6d":"markdown","7e965eb4":"markdown","dec4f57d":"markdown","8e308c44":"markdown","f3e8e592":"markdown","80ca9106":"markdown","23b0e432":"markdown","2ab18596":"markdown","0e54cda0":"markdown","cb936815":"markdown","0affcaf1":"markdown"},"source":{"52d51791":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","83c00f5c":"true = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","073592b3":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image \ntext = str\ntext = \" \".join(review for review in true.text)\nmask = np.array(Image.open(\"\/kaggle\/input\/image-file-for-fake-news\/news.png\"))\nwc = WordCloud(background_color=\"white\", max_words=1000, mask=mask,max_font_size=200,contour_color='black')\nwc.generate(text)\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","4267fada":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image \ntext = str\ntext = \" \".join(review for review in fake.text)\nmask = np.array(Image.open(\"\/kaggle\/input\/image-file-for-fake-news\/news.png\"))\nwc = WordCloud(background_color=\"white\", max_words=1000, mask=mask,max_font_size=200,contour_color='black')\nwc.generate(text)\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","080f7c77":"true.head()","b367638e":"true['result'] = [1 for i in range(0,len(true))]","d2e0fa30":"true.shape","7a113a8b":"fake['result'] = [0 for i in range(0,len(fake))]","fb54f7c9":"fake.head()","8556d56d":"fake.shape","aff76256":"# club both the files\ndata = true.append(fake,ignore_index = True,sort=False)","dd6c4d54":"true.shape","18e4eb18":"data.head()","22350b50":"data.tail()","9b11a360":"data.info()","5dd89602":"data['subject'].nunique()\n# this shows we have 8 subjects","cc982699":"data['subject'].unique()\n","1de161a1":"true_politicnews=0\nworld_n=0\ngovt_n =0\nus_n =0\nmiddle_n=0\nfor i,j in zip(data['subject'],data['result']):\n    if i==('politicsNews' or 'politics') and j==1:\n        true_politicnews = true_politicnews +1\n    if i==('worldnews') and j==1:\n        world_n+=1\n    if i==('Government News') and j==1:\n        govt_n+=1\n    if i==('US_News') and j==1:\n        us_n+=1\n    if i==('Middle-east') and j==1:\n        middle_n+=1\nprint(true_politicnews)\nprint(world_n)\nprint(us_n)\nprint(govt_n)\nprint(middle_n)\n","1ff7fe3b":"str =\" \"\nfor i,j in zip(data['subject'],data['text']):\n    if i=='politicsNews':\n        str += j\n","4bd562af":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# list_comp = [review if data[data['subject']==\"politicsnews\"] else ' ' for review in data['text']]\npolitics = str\nwordcloud = WordCloud( background_color=\"white\", max_words=1000).generate(politics)\n\nplt.figure(figsize=[7,7])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","db403738":"# we can have a look at the wordcloud for each of these subjects\n# so lets group the data acc to subjects and then make wordclods for fake and true news.","32129ce2":"# CLEANING THE TEXT COLUMN","59f8a345":"# X = pd.concat(data['title'],data['text'])\ndata['combines'] = data['title']+\" \"+data['text']\n","dd1e0dd6":"df = data[['combines','result']]","e728071f":"data['combines'] = data['combines'].apply(lambda word:word.lower())","3aa0b0e0":"import string\nprint(string.punctuation)","add96070":"def punctuation_removal(str1):\n    list1 = [x for x in str1 if x not in string.punctuation]\n    str2 = ''.join(list1)\n    return str2\ndata['combines'] = data['combines'].apply(lambda word:punctuation_removal(word))","32ff77d8":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nprint(stop)","82978073":"df['combines'].apply(lambda x: [word for word in x if word not in stop])","77bee544":"# seperate each word by white space\ndata['combines'] = data['combines'].apply(lambda word:word.split(','))","3d0a5754":"data['combines'].head(2)","5e805686":"# from nltk.tokenize import word_tokenize\n# def to_words(text):\n#     tokens = word_tokenize(text)\n#     return tokens\n# data['combines'] = data['combines'].apply(lambda s:to_words(s))\n","664a31c7":"def convert(lst):       \n    return ' , '.join(lst)\n\ndata['combines'] = data['combines'].apply(lambda s:convert(s))\n","2bb69033":"# # split into words\n# from nltk.tokenize import word_tokenize\n# def splitter(text):\n#     tokens = word_tokenize(text)\n#     return tokens\n# data['combines'] = data['combines'].apply(lambda x:splitter(x))","32049348":"# def convert(lst):       \n#     return ' , '.join(lst)\n\n# data['combines'] = data['combines'].apply(lambda s:convert(s))\n","de2200f1":"# from nltk.stem.porter import PorterStemmer\n# porter = PorterStemmer()\n# def stem(tokens):\n#     stemmed = [porter.stem(word) for word in tokens]\n#     print(stemmed[:100])\n# data['combines'] = data['combines'].apply(lambda x:stem(x))","b438c731":"data['combines'].head()","b177241f":"from sklearn.feature_extraction.text import CountVectorizer\n\ncombine = CountVectorizer()\ncombine = combine.fit(data['combines'])\n\ncombined_vector = combine.transform(data['combines'])\n","0d34e42e":"\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(combined_vector)\nnews_tfidf = tfidf_transformer.transform(combined_vector)\nprint(news_tfidf.shape)","59a4c176":"y = data['result']","1e13bc68":"xtrain,xtest,ytrain,ytest = train_test_split(news_tfidf,y,test_size =0.25,random_state=7)","15858fd6":"from sklearn.linear_model import SGDClassifier\n\nsvm = SGDClassifier().fit(xtrain, ytrain)\npredict_svm = svm.predict(xtest)\n","996ba728":"from sklearn.metrics import classification_report\nprint (classification_report(ytest, predict_svm))","baa72410":"from sklearn.metrics import confusion_matrix\nfinal = confusion_matrix(predict_svm, ytest)","7dc8ef94":"print(final)","5c91bf03":"from sklearn.metrics import accuracy_score\nprint('Accuracy Score :',accuracy_score(ytest, predict_svm) )\n","4b3a3a6d":"wordcloud for political_news(true)","7e965eb4":"# PUNCTUATIONS","dec4f57d":"# **STEMMING**","8e308c44":"**lets find out that which subject has maximum  true news**\nthis can not be done for fake news as subject for all news is 'news' there","f3e8e592":"info() shows that there is no null object. so this cuts down one of the steps of data preprocessing.\nsince columns are object type(strings) feature scaling is also obviously not required.","80ca9106":"# STOPWORDS","23b0e432":"lets have a look at our data subject wise","2ab18596":"i will concat titles and text columns and that column would be pre processed","0e54cda0":"# SVM","cb936815":"# LOWERCASE","0affcaf1":"**LETS WORK ON TEXT NOW**"}}