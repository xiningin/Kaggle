{"cell_type":{"77f28211":"code","4c4691af":"code","ba47c806":"code","9e18f873":"code","8811eb85":"code","b272ab97":"code","468fabf1":"code","b6c83e66":"code","dfb93169":"code","4e8781c5":"code","13ecebf2":"code","b695c9fe":"code","4ce6b6fd":"code","99347137":"code","7400f4d4":"code","f3f73608":"code","8fbc0ad6":"code","fa241ed8":"code","a386f72b":"code","7729aaf8":"code","e3b87c12":"code","a5764fb9":"code","e5f16de1":"code","9d2e970f":"code","6260654d":"code","0f9685b7":"markdown","edb42724":"markdown","b2b36392":"markdown","64dd3e66":"markdown","2bd5dc69":"markdown","df7f3f24":"markdown","d2bcb127":"markdown","4415b656":"markdown","9a0d8d24":"markdown","95afcbe6":"markdown","14d64450":"markdown","d8df148c":"markdown","aaa5912f":"markdown"},"source":{"77f28211":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import twitter_samples\nnltk.download('stopwords')\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer         # module for stemming\n\nfrom nltk.tokenize import TweetTokenizer,sent_tokenize,word_tokenize   # module for tokenizing strings\n\nfrom sklearn.utils import shuffle","4c4691af":"nltk.download('twitter_samples')","ba47c806":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","9e18f873":"def preprocess_tweets(tweet):\n    tweets_clean = []\n    \n    # Instantiate stemming class\n    Lemma = WordNetLemmatizer()\n    \n    # Create an empty list to store the stems\n    tweets_lemma = []\n\n    stopwords_english = stopwords.words('english') \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n    \n    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet2 = re.sub(r'#', '', tweet2)\n    \n\n    tweet_tokens = tokenizer.tokenize(tweet2)\n    \n    for word_token in tweet_tokens:\n        if(word_token not in stopwords_english and word_token not in string.punctuation):\n            stem_word = Lemma.lemmatize(word_token)\n            tweets_clean.append(stem_word)\n    \n    return tweets_clean","8811eb85":"def build_freq(tweets,label):\n    ys = label\n    yslist = np.squeeze(ys).tolist()\n\n    freq = {}\n\n    for tweet,y in zip(tweets,ys):\n        for word in preprocess_tweets(tweet):\n            pair = (word,y)\n            if pair in freq:\n                freq[pair] += 1\n            else:\n                freq[pair] = 1\n    return freq","b272ab97":"def extract_feature_map(tweet,freq):\n    clean_word = preprocess_tweets(tweet)\n    x = np.zeros((1,3))\n    x[0,0]=1\n\n    for word in clean_word:\n        x[0,1] += freq.get((word,1.0),0)\n        x[0,2] += freq.get((word,0.0),0)\n\n    return x","468fabf1":"# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg","b6c83e66":"train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","dfb93169":"freqs = build_freq(train_x, np.squeeze(train_y))","4e8781c5":"X = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i]=extract_feature_map(train_x[i],freqs)\n\ny = train_y","13ecebf2":"X, y = shuffle(X, y)","b695c9fe":"Logistic_reg = LogisticRegression()\nLogistic_reg.fit(X,y)","4ce6b6fd":"Logistic_reg.predict(extract_feature_map(test_x[0],freqs))","99347137":"Pred_y = np.zeros(len(test_x))\ny_pred_proba = pd.DataFrame()\nfor i in range(0,len(test_x)):\n    Pred_y[i]= Logistic_reg.predict(extract_feature_map(test_x[i],freqs))\n    y_pred_proba=pd.concat([y_pred_proba,pd.DataFrame(Logistic_reg.predict_proba(extract_feature_map(test_x[i],freqs)))])","7400f4d4":"sns.heatmap(confusion_matrix(test_y,Pred_y),annot=True)","f3f73608":"from sklearn.metrics import f1_score,precision_score, recall_score,roc_auc_score,r2_score,roc_curve","8fbc0ad6":"print(\"Precision score\",precision_score(y_true=test_y,y_pred=Pred_y))","fa241ed8":"print(\"recall score\",recall_score(y_true=test_y,y_pred=Pred_y))","a386f72b":"print(\"roc_auc_score\",roc_auc_score(y_true=test_y,y_score=Pred_y))","7729aaf8":"from sklearn.naive_bayes import MultinomialNB\n\nMB = MultinomialNB(alpha=1)\nMB.fit(X,y)","e3b87c12":"Pred_y = np.zeros(len(test_x))\ny_pred_proba = pd.DataFrame()\nfor i in range(0,len(test_x)):\n    Pred_y[i]= MB.predict(extract_feature_map(test_x[i],freqs))\n    y_pred_proba=pd.concat([y_pred_proba,pd.DataFrame(MB.predict_proba(extract_feature_map(test_x[i],freqs)))])","a5764fb9":"sns.heatmap(confusion_matrix(test_y,Pred_y),annot=True)","e5f16de1":"print(\"Precision score\",precision_score(y_true=test_y,y_pred=Pred_y))","9d2e970f":"print(\"recall score\",recall_score(y_true=test_y,y_pred=Pred_y))","6260654d":"print(\"roc_auc_score\",roc_auc_score(y_true=test_y,y_score=Pred_y))","0f9685b7":"**Step2** - Generate positive & negetive count for each word occured in the tweet","edb42724":"**Step4** - Divide data into train & test","b2b36392":"# Lets Try out with Naive Bayes","64dd3e66":"Naive bayes Algorithm is based on the bayesian techique and works on conditional proabability. It also has laplace smoothening (alpha>=1) which means when it calculates the probability ratio of words in terms of positive or negative. If any term does not occur in any class, its probability value would be 0 & hence which doesn't signify a correct intrepretation.so, laplace suggest add +1 in numerater uniformally across all and when we add 1 in numerator it need to be normalized.Hence in denominator it added no of unique keyword in specific selected class & divides   ","2bd5dc69":"# Sentiment Analysis using Logistic Regression & Naive bayes leveraging NLTK library","df7f3f24":"Sentiment analysis is a technique through which you can analyze a piece of text to determine the sentiment behind it. It combines machine learning and natural language processing (NLP) to achieve this. Using basic Sentiment analysis, a program can understand if the sentiment behind a piece of text is positive, negative, or neutral.","d2bcb127":"As in files it has first positive followed by negetive tweets.To create more generalize model, tweets need to be distributed + & - all randomly","4415b656":"**Step1** -Preprocess_tweet function cleans the tweet after removing extra stop words, punctuation etc. After this tweet need to be tokenize into words & then words converted into their respective lemma","9a0d8d24":"**Step3** - Build frequency metrics [word, postive count, negetive count], this will act as our training feature set X and label corresponding to this will be y","95afcbe6":"To perform this sentiment analysis we need very simple 5 steps -\n1. Preprocess the text by removing extra unwanted words, stop words & punctuation\n2. Generate frequency metrics has positive or negetive count with respect to each word\n3. Extract feature map which has [word, postive count, negetive count] \n4. Divide into train and test and classify using any classifier ( Logistic regression )\n5. Predict & Evaluate classifier output","14d64450":"**Step5-** Evaluate the classifier","d8df148c":"![image.png](attachment:image.png)","aaa5912f":"Get the label which as y variable in classifier y=1 is positive where as y=0 is negetive"}}