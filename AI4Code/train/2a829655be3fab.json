{"cell_type":{"c1b523a7":"code","3ac96e59":"code","26be3ac6":"code","f1c7e1b8":"code","356e3cb5":"code","b742e764":"code","f48698ad":"code","2df74f61":"code","f12e35c0":"code","982d35c5":"code","350f7789":"code","a5d41704":"code","24b2da20":"code","d178c9d1":"code","acf348a6":"code","6fd61f57":"code","ce338cc8":"code","d264b798":"code","08fa76ac":"code","63f2f4e7":"code","9f6d55a8":"code","198e53b1":"code","60236e86":"code","6b9e36c5":"code","c890157c":"code","eeb81da7":"code","334e0680":"code","20a31058":"code","af4dab3d":"code","5d0ba701":"code","64986637":"code","570d547c":"code","24f35be8":"code","413913df":"code","a29242e7":"code","ee09e7e1":"code","7fbaf8ba":"code","a8b9419a":"code","3919dbd0":"code","c3d30f98":"code","ef7810da":"code","28dbd096":"code","e4c579db":"code","e013084f":"code","16d3e8b5":"code","c57884cd":"code","f9f34d31":"code","37f9b31c":"code","d0c89ab9":"code","079c3cb2":"code","e5199672":"code","f5a908cb":"code","f98c23ec":"code","e358e06f":"code","b8efd5cb":"code","a27a17ab":"code","7616f0e5":"code","6b03fe91":"code","e87e4c25":"code","84356ccb":"code","e3ee535e":"code","ae191826":"code","f97b830b":"code","dfa66603":"code","2a730f80":"code","c22d20d8":"code","40b3481e":"code","3945765a":"code","f9cb7b89":"code","83926fab":"markdown","68d565c0":"markdown","50109747":"markdown","885ceeac":"markdown","4250e856":"markdown","920229be":"markdown","8c21f3f9":"markdown","a054f9b9":"markdown","7ee4fdc0":"markdown","af2a3b3b":"markdown","e0bec548":"markdown","c15ea761":"markdown","7d792277":"markdown","9e34b041":"markdown","c971e026":"markdown","76cddc46":"markdown","99232dc7":"markdown","439656ce":"markdown","a21835b7":"markdown","f842a811":"markdown","2db90f6a":"markdown","5581013e":"markdown","413d72b6":"markdown","124b93f5":"markdown","b4a2fc1d":"markdown","445208ed":"markdown","c5667c2d":"markdown","8039f4f5":"markdown","d8c730a9":"markdown","917177de":"markdown","7894348d":"markdown","6bd5e0ba":"markdown","ed7ddeaf":"markdown","00933347":"markdown","b29b9226":"markdown","b6482b06":"markdown","a7dc34ce":"markdown","09da4966":"markdown","870d5398":"markdown","3a9ad00c":"markdown","d34257cd":"markdown","d93ada89":"markdown","fed1a4ce":"markdown","c881134c":"markdown","54391525":"markdown","739fb3f1":"markdown","2c593db9":"markdown","80a34197":"markdown","5b8af70c":"markdown","4b6b3b50":"markdown","eb8a6e2f":"markdown","2a0bb033":"markdown","c6704cc0":"markdown","a8c0135d":"markdown","2901e0b4":"markdown","ece71158":"markdown","97e76702":"markdown","f630c02f":"markdown","3f02ef3a":"markdown","136da8f8":"markdown","4fa6ba29":"markdown","584db724":"markdown","6d99b862":"markdown","931835dc":"markdown","6f062868":"markdown","1d1368a7":"markdown","25c5897b":"markdown","36554e24":"markdown","6f6a9d19":"markdown","a3fcc593":"markdown","afd76080":"markdown","d09e00ee":"markdown","8a4e426e":"markdown","bc8d415e":"markdown","e6e58ba0":"markdown","65691696":"markdown"},"source":{"c1b523a7":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import recall_score, roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# Local application\nimport utilidades_practica_2_ordinaria as utils","3ac96e59":"random_state = 27912","26be3ac6":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex_col = \"id\"\ntarget = \"diagnosis\"\n\ndata_Cancer = utils.load_data(filepath, index_col, target)","f1c7e1b8":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\nindex_col = None\ntarget = \"Outcome\"\n\ndata_Diabetes = utils.load_data(filepath, index_col, target)","356e3cb5":"data_Cancer.sample(5, random_state=random_state)","b742e764":"data_Diabetes.sample(5, random_state=random_state)","f48698ad":"target = \"diagnosis\"\n\n(X_Cancer, y_Cancer) = utils.divide_dataset(data_Cancer, target)","2df74f61":"target = \"Outcome\"\n\n(X_Diabetes, y_Diabetes) = utils.divide_dataset(data_Diabetes, target)","f12e35c0":"X_Cancer.sample(5, random_state=random_state)","982d35c5":"X_Diabetes.sample(5, random_state=random_state)","350f7789":"y_Cancer.sample(5, random_state=random_state)","a5d41704":"y_Diabetes.sample(5, random_state=random_state)","24b2da20":"stratify = y_Cancer\ntrain_size = 0.7\n\n(X_train_Cancer, X_test_Cancer, y_train_Cancer, y_test_Cancer) = train_test_split(X_Cancer, y_Cancer,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","d178c9d1":"stratify = y_Diabetes\ntrain_size = 0.7\n\n(X_train_Diabetes, X_test_Diabetes, y_train_Diabetes, y_test_Diabetes) = train_test_split(X_Diabetes, y_Diabetes,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","acf348a6":"X_train_Cancer.sample(5, random_state=random_state)","6fd61f57":"y_train_Cancer.sample(5, random_state=random_state)","ce338cc8":"X_train_Diabetes.sample(5, random_state=random_state)","d264b798":"y_train_Diabetes.sample(5, random_state=random_state)","08fa76ac":"X_test_Cancer.sample(5, random_state=random_state)","63f2f4e7":"y_test_Cancer.sample(5, random_state=random_state)","9f6d55a8":"X_test_Diabetes.sample(5, random_state=random_state)","198e53b1":"y_test_Diabetes.sample(5, random_state=random_state)","60236e86":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","6b9e36c5":"X_Cancer = X_train_Cancer\ny_Cancer = y_train_Cancer","c890157c":"del_columns_wisconsin = utils.QuitarColumnasTransformer(['perimeter_mean', 'area_mean', 'compactness_mean', 'concave points_mean','radius_se','texture_se',\n                                    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                                    'fractal_dimension_se', 'radius_worst', 'smoothness_worst', 'symmetry_worst', 'texture_worst', 'perimeter_worst', \n                                    'area_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'fractal_dimension_worst', 'Unnamed: 32'])\n\nnan_anomalos_wisconsin=utils.AnomalosANanTransformer({'radius_mean':[6.981, 22.27], 'texture_mean':[10.38, 29.97],'smoothness_mean':[0.06251, 0.1326],\n                                     'symmetry_mean':[0.1203, 0.2459], 'fractal_dimension_mean':[0.04996, 0.0795],'concavity_mean':[0, 0.2871] })\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='mean')\n\ndiscretizer_wisconsin = KBinsDiscretizer(n_bins=5, strategy=\"uniform\")","eeb81da7":"X_Diabetes = X_train_Diabetes\ny_Diabetes = y_train_Diabetes","334e0680":"del_columns_pima = utils.QuitarColumnasTransformer(['SkinThickness', 'Insulin'])\n\nnan_anomalos_pima=utils.AnomalosANanTransformer({'Glucose':[56,199], 'BloodPressure':[38,102], 'Pregnancies':[0,13],\n                                      'BMI':[18.2,50], 'DiabetesPedigreeFunction':[0.078,1.182], 'Age':[21,64]})\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='mean')\n\ndiscretizer_pima = KBinsDiscretizer(n_bins=5, strategy=\"kmeans\")","20a31058":"def normalize(dataset):\n    dataNorm=((dataset)\/(dataset.max()))\n    return dataNorm","af4dab3d":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=\"M\")}","5d0ba701":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),('imp', imp),\n                           ('discretizer_wisconsin', discretizer_wisconsin),('knn', estimator)])\n\nX_Cancer_Norm = normalize(X_Cancer)\n\nweights = [\"uniform\", \"distance\"]\n\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n\nmetric = [\"euclidean\", \"manhattan\"]\n#NO UTILIZAMOS MINKOWSKI PORQUE LA MEDIDA MINKOWSKI ES UNA GENERALIZACION DE LA DISTANCIA EUCLIDEA Y MANHATTAN A\u00d1ADIENDO UN PARAMETRO P\n\n#p =[1, 2, 3, 4, 5, 6, 7]\n\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\n\n#n_jobs = default -1\n\nk_neighbors_clf = utils.optimize_params(pipeline,X_Cancer_Norm, y_Cancer, cv,\n                                        scoring=scoring,\n                                        knn__n_neighbors=n_neighbors,\n                                        knn__weights=weights,\n                                        knn__metric=metric)","64986637":"# Should not modify the original model\nestimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),('imp', imp),\n                           ('discretizer_wisconsin', discretizer_wisconsin),('DecisionTree', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\n\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, None]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [10, 15, 20, 30, 40]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nsplitter = [\"best\",\"random\"]\n\n\n\ndecision_tree_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                          scoring=scoring,\n                                          DecisionTree__criterion=criterion,\n                                          DecisionTree__max_depth=max_depth,\n                                          DecisionTree__ccp_alpha=ccp_alpha,\n                                          DecisionTree__splitter=splitter)","570d547c":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('AdaBoost', estimator)])\n\n#algorithm = [\"SAMME\", \"SAMME.R\"]\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nlearning_rate = [0.1, 0.2,0.95, 1.0]\nn_estimators = [100, 120, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\n\n\nadaboost_clf = utils.optimize_params(pipeline, X_Cancer, y_Cancer, cv,\n                                     scoring=scoring,\n                                     AdaBoost__base_estimator=base_estimator,\n                                     AdaBoost__learning_rate=learning_rate,\n                                     AdaBoost__n_estimators=n_estimators,\n                                     AdaBoost__base_estimator__criterion=criterion,\n                                     AdaBoost__base_estimator__max_depth=max_depth)","24f35be8":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('Bagging', estimator)])\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [ 10, 50, 75, 100 ]\nmax_samples = [ 0.2, 1.0]\nbootstrap = [ True, False ]\n\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\n\nbagging_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                    scoring=scoring,\n                                    Bagging__n_estimators=n_estimators,\n                                    Bagging__max_samples=max_samples,\n                                    Bagging__bootstrap=bootstrap,\n                                    Bagging__base_estimator=base_estimator,\n                                    Bagging__base_estimator__criterion=criterion,\n                                    Bagging__base_estimator__max_depth=max_depth)","413913df":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('RandForest', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n#max_depth = [5,6,7,8,9]\n#class_weight = [\"balanced\", None]\nn_estimators = [50, 100, 150, 200]\nrandom_forest_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                          scoring=scoring,\n                                          RandForest__criterion=criterion,\n                                          RandForest__n_estimators=n_estimators,\n                                          RandForest__max_features=max_features)","a29242e7":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('GradientBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nsubsample = [0.2, 0.8, 1.0]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\n\ngradient_boosting_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                              scoring=scoring,\n                                              GradientBoosting__learning_rate=learning_rate,\n                                              GradientBoosting__criterion=criterion,\n                                              GradientBoosting__max_depth=max_depth,\n                                              GradientBoosting__subsample=subsample,\n                                              GradientBoosting__n_estimators = n_estimators)","ee09e7e1":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('HistGradientBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\nmax_bins = [200, 255, 300, 350]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                                   scoring=scoring,\n                                                   HistGradientBoosting__learning_rate=learning_rate,\n                                                   HistGradientBoosting__min_samples_leaf=min_samples_leaf,\n                                                   HistGradientBoosting__loss=loss,\n                                                   HistGradientBoosting__max_iter=max_iter)","7fbaf8ba":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=1)}","a8b9419a":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('knn', estimator)])\n\n\nX_Diabetes_Norm = normalize(X_Diabetes)\n\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n#p =[2, 3, 4, 5, 6, 7]\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\nmetric = [\"euclidean\", \"manhattan\"]\n#n_jobs = default -1\n\nk_neighbors_clf_pima = utils.optimize_params(pipeline,X_Diabetes_Norm, y_Diabetes, cv,\n                                             scoring=scoring,\n                                             knn__weights=weights,\n                                             knn__n_neighbors=n_neighbors)","3919dbd0":"estimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('DecisionTree', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8, 9, 10, None]\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [8, 9, 10, 11, 12]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#splitter = [\"best\",\"random\"]\n\n\n\n\n\ndecision_tree_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                               scoring=scoring,\n                                               DecisionTree__criterion=criterion,\n                                               DecisionTree__max_depth=max_depth,\n                                               DecisionTree__ccp_alpha=ccp_alpha)","c3d30f98":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('AdaBoost', estimator)])\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\n#n_estimators = [100, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\n\nadaboost_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                          scoring=scoring,\n                                          AdaBoost__base_estimator=base_estimator,\n                                          AdaBoost__learning_rate=learning_rate,\n                                          AdaBoost__base_estimator__criterion=criterion,\n                                          AdaBoost__base_estimator__max_depth=max_depth,\n                                          AdaBoost__base_estimator__ccp_alpha=ccp_alpha)","ef7810da":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('Bagging', estimator)])\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\nmin_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n\nbagging_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                         scoring=scoring,\n                                         Bagging__base_estimator=base_estimator,\n                                         Bagging__base_estimator__criterion=criterion,\n                                         Bagging__base_estimator__max_depth=max_depth,\n                                         Bagging__base_estimator__min_samples_leaf=min_samples_leaf)","28dbd096":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('RandForest', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nmax_depth = [5,6,7,8,9]\n\nrandom_forest_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                               scoring=scoring,\n                                               RandForest__criterion=criterion,\n                                               RandForest__max_features=max_features,\n                                               RandForest__max_depth=max_depth)","e4c579db":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('GradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\nsubsample = [0.2, 0.8, 1.0]\n\ngradient_boosting_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                                   scoring=scoring,\n                                                   GradBoosting__learning_rate=learning_rate,\n                                                   GradBoosting__criterion=criterion,\n                                                   GradBoosting__max_depth=max_depth,\n                                                   GradBoosting__subsample=subsample,\n                                                   GradBoosting__n_estimators = n_estimators)","e013084f":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),('HistGradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\nmax_bins = [200, 255, 300, 350]\n\nhist_gradient_boosting_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                                        scoring=scoring,\n                                                        HistGradBoosting__learning_rate=learning_rate,\n                                                        HistGradBoosting__min_samples_leaf=min_samples_leaf,\n                                                        HistGradBoosting__loss=loss,\n                                                        HistGradBoosting__max_iter=max_iter)","16d3e8b5":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf \n}","c57884cd":"X_Cancer = X_test_Cancer\ny_Cancer = y_test_Cancer\n\nutils.evaluate_estimators2(estimators, X_Cancer, y_Cancer, 'M')","f9f34d31":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf_pima,\n    \"Decision tree\": decision_tree_clf_pima,\n    \"AdaBoost\": adaboost_clf_pima,\n    \"Bagging\": bagging_clf_pima,\n    \"Random Forests\": random_forest_clf_pima,\n    \"Gradient Boosting\": gradient_boosting_clf_pima,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_pima\n}","37f9b31c":"X_Diabetes = X_test_Diabetes\ny_Diabetes = y_test_Diabetes\n\nutils.evaluate_estimators2(estimators, X_Diabetes, y_Diabetes,1)","d0c89ab9":"filepath = \"..\/input\/titanic\/train.csv\"\n\nindex = \"PassengerId\"\ntarget = \"Survived\"\n\nTitanic_data = utils.load_data(filepath, index, target)","079c3cb2":"Titanic_data.sample(5, random_state=random_state)","e5199672":"Titanic_data[\"Survived\"]=Titanic_data[\"Survived\"].astype(\"category\")\nTitanic_data.info(memory_usage=False)","f5a908cb":"(X_Titanic_data, y_Titanic_data)=utils.divide_dataset(Titanic_data,target=\"Survived\")","f98c23ec":"X_Titanic_data.sample(5, random_state=random_state)","e358e06f":"y_Titanic_data.sample(5, random_state=random_state)","b8efd5cb":"(X_Titanic_data_train,X_Titanic_data_test,y_Titanic_data_train,y_Titanic_data_test)=train_test_split(X_Titanic_data,\n                                                                                                     y_Titanic_data,\n                                                                                                     stratify=y_Titanic_data,\n                                                                                                     random_state=random_state,\n                                                                                                     train_size=0.7)","a27a17ab":"X_Titanic = X_Titanic_data_train\ny_Titanic = y_Titanic_data_train","7616f0e5":"X_Titanic.sample(5, random_state=random_state)","6b03fe91":"del_columns = utils.QuitarColumnasTransformer(['Cabin', 'Name', 'Ticket', 'Fare'])\n\nstring_int1 = utils.StringAIntTransformer('Sex',['female','male'],[0,1])\nstring_int2 = utils.StringAIntTransformer('Embarked',['Q','C','S'],[1,2,3])\n\nnan_anomalos_titanic = utils.AnomalosANanTransformer({'Pclass':[1,3], 'Sex':[0,1], 'SibSp':[0, 8],\n                                         'Parch':[0, 6], 'Embarked':[1, 3] })\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='most_frequent')\n\ndiscretizer_titanic = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")","e87e4c25":"def normalizeTitanic(dataset):\n    dataNorm=((dataset[\"Pclass\"])\/(dataset[\"Pclass\"].max()))\n    dataNorm=((dataset[\"SibSp\"])\/(dataset[\"SibSp\"].max()))\n    dataNorm=((dataset[\"Parch\"])\/(dataset[\"Parch\"].max()))\n    \n    dataNorm[\"Sex\"]=dataset[\"Sex\"]\n    dataNorm[\"Embarked\"]=dataset[\"Embarked\"]\n    \n    dataNorm[\"Age\"]=dataset[\"Age\"]\n    dataNorm[\"Cabin\"]=dataset[\"Cabin\"]\n    dataNorm[\"Name\"]=dataset[\"Name\"]\n    dataNorm[\"Ticket\"]=dataset[\"Ticket\"]\n    dataNorm[\"Fare\"]=dataset[\"Fare\"]\n    return dataNorm","84356ccb":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=1)}","e3ee535e":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int2),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('knn', estimator)])\n\n\nX_Titanic_Norm = normalizeTitanic(X_Titanic)\n\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n#p =[2, 3, 4, 5, 6, 7]\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\nmetric = [\"euclidean\", \"manhattan\"]\n#n_jobs = default -1\n\nk_neighbors_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             knn__weights=weights,\n                                             knn__n_neighbors=n_neighbors,\n                                             knn__metric=metric)","ae191826":"estimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('DecisionTree', estimator)])\n\n\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8, 9, 10, None]\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [8, 9, 10, 11, 12]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#splitter = [\"best\",\"random\"]\n\ndecision_tree_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             DecisionTree__criterion=criterion,\n                                             DecisionTree__max_depth=max_depth,\n                                             DecisionTree__ccp_alpha=ccp_alpha)","f97b830b":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('adaBoost', estimator)])\n\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\n#n_estimators = [100, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             adaBoost__learning_rate=learning_rate,\n                                             adaBoost__base_estimator=base_estimator,\n                                             adaBoost__base_estimator__criterion=criterion,\n                                             adaBoost__base_estimator__max_depth=max_depth,\n                                             adaBoost__base_estimator__ccp_alpha=ccp_alpha)","dfa66603":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('bagging', estimator)])\n\n\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\nmin_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n\n\nbagging_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             bagging__base_estimator=base_estimator,\n                                             bagging__base_estimator__criterion=criterion,\n                                             bagging__base_estimator__max_depth=max_depth,\n                                             bagging__base_estimator__min_samples_leaf=min_samples_leaf)","2a730f80":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('RFC', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n#max_depth = [5,6,7,8,9]\n\nrandom_forest_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             RFC__criterion=criterion,\n                                             RFC__max_features=max_features)","c22d20d8":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('gradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\nsubsample = [0.2, 0.8, 1.0]\n\ngradient_boosting_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             gradBoosting__learning_rate=learning_rate,\n                                             gradBoosting__criterion=criterion,\n                                             gradBoosting__max_depth=max_depth,\n                                             gradBoosting__n_estimators=n_estimators,\n                                             gradBoosting__subsample=subsample)","40b3481e":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),('histGradBoosting', estimator)])\n\n\n\n\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\n\nhist_gradient_boosting_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             histGradBoosting__learning_rate=learning_rate,\n                                             histGradBoosting__min_samples_leaf=min_samples_leaf,\n                                             histGradBoosting__loss=loss,\n                                             histGradBoosting__max_iter=max_iter)","3945765a":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf_titanic,\n    \"Decision tree\": decision_tree_clf_titanic,\n    \"AdaBoost\": adaboost_clf_titanic,\n    \"Bagging\": bagging_clf_titanic,\n    \"Random Forests\": random_forest_clf_titanic,\n    \"Gradient Boosting\": gradient_boosting_clf_titanic,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_titanic \n}","f9cb7b89":"X_Titanic = X_Titanic_data_test\ny_Titanic = y_Titanic_data_test\n\nutils.evaluate_estimators2(estimators, X_Titanic, y_Titanic,1)","83926fab":"Es un m\u00e9todo que simplemente busca en las observaciones m\u00e1s cercanas a la que se est\u00e1 tratando de predecir y clasifica el punto de inter\u00e9s basado en la mayor\u00eda de datos que le rodean.","68d565c0":"Algunos de los parametros que podriamos utilizar son:\n\n* `criterion`: tipo de cadena, opcional (el valor predeterminado es \"gini\") Mide la calidad de la clasificaci\u00f3n. Los est\u00e1ndares admitidos son \"gini\" para la impureza y \"entropy\" para la ganancia de informaci\u00f3n.\n\n* `splitter`: {\u201cbest\u201d, \u201crandom\u201d}, default=\u201dbest\u201d. Una estrategia utilizada para seleccionar categor\u00edas en nodos. Las estrategias admitidas son \"best\", seleccione la mejor categor\u00eda y \"random\" seleccione la mejor categor\u00eda aleatoria.\n\n* `max_depth`: int, default=None. Profundidad m\u00e1xima que puede alcanzar el \u00e1rbol. Si no se especifica el arbol se expander\u00e1 hasta que todas las hojas sean de una \u00fanica clase o mientras cada hoja contenga menos ejemplos que el parametro `min_samples_leaf` \n\n* `min_samples_split`: int or float, default=2. N\u00famero m\u00ednimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracci\u00f3n del total de observaciones de entrenamiento.\n\n* `min_samples_leaf`: int or float, default=1. n\u00famero m\u00ednimo de muestras que debe haber en un nodo final (hoja). Tambi\u00e9n se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, opcional (el valor predeterminado es 0) La puntuaci\u00f3n m\u00ednima ponderada requerida por la muestra de entrada de un nodo hoja.La puntuaci\u00f3n m\u00ednima ponderada requerida por la muestra de entrada de un nodo hoja\n\n* `max_features`: int, float, string o None opcional (el valor predeterminado es None) La cantidad de caracter\u00edsticas que deben tenerse en cuenta al clasificar.\n    1. Si es un int, las caracter\u00edsticas de max_features deben ser consideradas en cada clasificaci\u00f3n.\n    \n    2. Si es un flotante, entonces max_features es un porcentaje y la cantidad de caracter\u00edsticas que se deben considerar durante la clasificaci\u00f3n es int (max_features * n_features, donde n_features es la cantidad de caracter\u00edsticas enviadas cuando se completa el entrenamiento).\n    \n    3. Si es autom\u00e1tico, max_features = sqrt (n_features)\n    \n    4. Si es sqrt, max_features = sqrt (n_features)\n    \n    5. Si es log2, max_features = log2 (n_features)\n    \n    6. Si es None, max_features = n_features\n\n    Nota: Cuando se encuentre al menos un punto de muestra clasificado, la b\u00fasqueda y clasificaci\u00f3n se detendr\u00e1.\n\n* `random_stateint`: Semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n\n* `max_leaf_nodes`: int, default=None N\u00famero m\u00e1ximo de nodos terminales.\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividir\u00e1 si esta division produce un decremento de la impureza igual o mayor a este valor.\n\n* `min_impurity_split`:float, default=0 Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, ser\u00eda una hoja.\n\n* `ccp_alphanon-negative`: float, default=0.0 Determina el grado de penalizaci\u00f3n por complejidad. Cuanto mayor es este valor, m\u00e1s agresivo el podado y menor el tama\u00f1o del \u00e1rbol resultante.","50109747":"Y lo mismo para la base de datos de diabetes.","885ceeac":"Gracias a lo estudiado en la pr\u00e1ctica anterior, vamos a generar los transformadores con los que crearemos los pipelines con los que trabajaremos m\u00e1s adelante.","4250e856":"Respecto a `HistGradientBoosting` tambien tenemos una gran cantidad de hiperparametros que podriamos utilizar aunque nosotros unicamente hemos utilizado `learning_rate`, `loss`, `max_iter` ,`min_samples_leaf` y `max_bins`","920229be":"# 6. Titanic","8c21f3f9":"### KNeighborsClassifier","a054f9b9":"Empezamos a optimizar los hiperparametros de los modelos y comentaremos los resultados al final de todo.","7ee4fdc0":"## Selecci\u00f3n para Wisconsin","af2a3b3b":"Algunos de los parametros que podriamos utillizar son:\n\n* `loss`: {\u2018deviance\u2019, \u2018exponential\u2019}, default=\u2019deviance\u2019 La funci\u00f3n `loss` a optimizar. \"deviance\" se refiere a la desviaci\u00f3n (regresi\u00f3n log\u00edstica) para la clasificaci\u00f3n con resultados probabil\u00edsticos. \"exponencial\" gradientBoosting recupera el algoritmo AdaBoost.\n\n* `learning_rate`: float, default=0.1 Reduce la contribuci\u00f3n de cada \u00e1rbol multiplicando su influencia original por este valor.\n\n* `n_estimators`: int, default=100 El n\u00famero de etapas de boosting a realizar. Gradient Boosting es bastante robusto ante el sobreajuste, por lo que un gran n\u00famero generalmente da como resultado un mejor rendimiento.\n\n* `subsample`: float, default=1.0 La proporci\u00f3n de submuestras utilizadas en el entrenamiento de cada \u00e1rbol de decisi\u00f3n con respecto al total de muestras, y la selecci\u00f3n de submuestras es aleatoria. El uso de un valor ligeramente inferior a 1 puede hacer que el modelo sea m\u00e1s robusto porque reduce la varianza.\n\n* `criterion`: {\u2018friedman_mse\u2019, \u2018mse\u2019, \u2018mae\u2019}, default=\u2019friedman_mse\u2019 La funci\u00f3n para medir la calidad de una divisi\u00f3n. Los criterios admitidos son \"friedman_mse\" para el error cuadr\u00e1tico medio con puntuaci\u00f3n de mejora de Friedman, \"mse\" para el error cuadr\u00e1tico medio y \"mae\" para el error absoluto medio.\n\n* `min_samples_split`: int or float, default=2 N\u00famero m\u00ednimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracci\u00f3n del total de observaciones de entrenamiento.\n\n* `min_samples_leaf`: int or float, default=1 n\u00famero m\u00ednimo de muestras que debe haber en un nodo final (hoja). Tambi\u00e9n se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, default=0.0 La fracci\u00f3n ponderada m\u00ednima de la suma total de pesos (de todas las muestras de entrada) que se requiere para estar en un nodo hoja. Las muestras tienen el mismo peso cuando no se proporciona sample_weight.\n\n* `max_depth`: int, default=3 La profundidad m\u00e1xima de los estimadores de regresi\u00f3n individuales.\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividir\u00e1 si esta divisi\u00f3n induce una disminuci\u00f3n de la impureza mayor o igual a este valor.\n\n* `min_impurity_split`: float, default=None Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, ser\u00eda una hoja.\n\n* `random_state`: int, RandomState instance or None, default=None Controla la semilla aleatoria dada a cada Tree Estimator en cada iteraci\u00f3n de boosting. Adem\u00e1s, controla la permutaci\u00f3n aleatoria de las caracter\u00edsticas en cada divisi\u00f3n. \n\n* `max_features`: {\u2018auto\u2019, \u2018sqrt\u2019, \u2018log2\u2019}, int or float, default=None La cantidad de caracter\u00edsticas a considerar al buscar la mejor divisi\u00f3n:\n\n    Si es int, entonces considere las caracter\u00edsticas de max_features en cada divisi\u00f3n.\n\n    Si es flotante, max_features es una fracci\u00f3n y las caracter\u00edsticas int (max_features * n_features) se consideran en cada divisi\u00f3n.\n\n    Si es \"auto\", entonces max_features = sqrt (n_features).\n\n    Si es \"sqrt\", entonces max_features = sqrt (n_features).\n\n    Si es \"log2\", entonces max_features = log2 (n_features).\n\n    Si es None, entonces max_features = n_features.\n\n    La elecci\u00f3n de max_features <n_features conduce a una reducci\u00f3n de la varianza y un aumento del sesgo.\n\n    Nota: la b\u00fasqueda de una divisi\u00f3n no se detiene hasta que se encuentra al menos una partici\u00f3n v\u00e1lida de las muestras de nodo, incluso si requiere inspeccionar de manera efectiva m\u00e1s de las caracter\u00edsticas de max_features.\n\n* `verbose`: int, default=0 Habilita la salida detallada. Si es 1, muestra el progreso y el rendimiento de vez en cuando (cuantos m\u00e1s \u00e1rboles, menor es la frecuencia). Si es mayor que 1, imprime el progreso y el rendimiento de cada \u00e1rbol.\n\n* `max_leaf_nodes`: int, default=None Los \u00e1rboles crecen con max_leaf_nodes numero de hojas de la mejor manera. Los mejores nodos se definen como una reducci\u00f3n relativa de la impureza. Si es None, entonces un n\u00famero ilimitado de nodos hoja.\n\n* `warm_start`: bool, default=False Cuando se establece en Verdadero, reutiliza la soluci\u00f3n de la llamada anterior para ajustar y agregar m\u00e1s estimadores al conjunto; de lo contrario, simplemente borra la soluci\u00f3n anterior. \n\n* `validation_fraction`: float, default=0.1 La proporci\u00f3n de datos de entrenamiento que se deben reservar como conjunto de validaci\u00f3n para la detenci\u00f3n anticipada. Debe estar entre 0 y 1.Solo se usa si n_iter_no_change se establece en un n\u00famero entero.\n\n* `n_iter_no_change`: int, default=None n_iter_no_change se utiliza para decidir si la parada anticipada se utilizar\u00e1 para finalizar el entrenamiento cuando la puntuaci\u00f3n de validaci\u00f3n no mejora. De forma predeterminada, est\u00e1 configurado en Ninguno para deshabilitar la parada anticipada. Si se establece en un n\u00famero, dejar\u00e1 de lado el tama\u00f1o de validation_fraction de los datos de entrenamiento como validaci\u00f3n y finalizar\u00e1 el entrenamiento cuando la puntuaci\u00f3n de validaci\u00f3n no est\u00e9 mejorando en todos los n_iter_no_change anteriores n\u00fameros de iteraciones.\n\n* `tol`: float, default=1e-4 Tolerancia a la parada anticipada. Cuando la p\u00e9rdida no mejora en al menos `tol` para `n_iter_no_change` iteraciones (si se establece en un n\u00famero), el entrenamiento se detiene.\n\n* `ccp_alpha`: non-negative float, default=0.0 Par\u00e1metro de complejidad utilizado para la poda Minimal Cost-Complexity. Se elegir\u00e1 el sub\u00e1rbol con la mayor complejidad de costo siendo m\u00e1s peque\u00f1o que ccp_alpha. De forma predeterminada, no se realiza ninguna poda.","e0bec548":"Algunos de los parametros que podriamos utilizar son:\n\n* `n_estimators`: int, default=100 n\u00famero de \u00e1rboles que va a tener el bosque aleatorio. Normalmente cuantos m\u00e1s mejor, pero a partir de cierto punto deja de mejorar y s\u00f3lo hace que vaya m\u00e1s lento. Un buen valor por defecto puede ser el uso de 100 \u00e1rboles.\n\n* `criterion`: {\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d Mide la calidad de la clasificaci\u00f3n. Los est\u00e1ndares admitidos son \"gini\" para la impureza y \"entropy\" para la ganancia de informaci\u00f3n.\n\n* `max_depth`: int, default=None La profundidad m\u00e1xima del \u00e1rbol.\n\n* `min_samples_split`: int or float, default=2 n\u00famero m\u00ednimo de muestras necesarias antes de dividir este nodo.\n\n* `min_samples_leaf`: int or float, default=1 n\u00famero m\u00ednimo de muestras que debe haber en un nodo final (hoja). Tambi\u00e9n se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, default=0.0 La puntuaci\u00f3n m\u00ednima ponderada requerida por la muestra de entrada de un nodo hoja\n\n* `max_features`:{\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, int or float, default=\u201dauto\u201d La cantidad de caracter\u00edsticas que deben tenerse en cuenta al clasificar.\n    1. Si es un int, las caracter\u00edsticas de max_features deben ser consideradas en cada clasificaci\u00f3n.\n    \n    2. Si es un flotante, entonces max_features es un porcentaje y la cantidad de caracter\u00edsticas que se deben considerar durante la clasificaci\u00f3n es int (max_features * n_features, donde n_features es la cantidad de caracter\u00edsticas enviadas cuando se completa el entrenamiento).\n    \n    3. Si es autom\u00e1tico, max_features = sqrt (n_features)\n    \n    4. Si es sqrt, max_features = sqrt (n_features)\n    \n    5. Si es log2, max_features = log2 (n_features)\n    \n    6. Si es None, max_features = n_features\n\n    Nota: Cuando se encuentre al menos un punto de muestra clasificado, la b\u00fasqueda y clasificaci\u00f3n se detendr\u00e1.\n\n* `max_leaf_nodes`: int, default=None n\u00famero m\u00e1ximo de nodos finales\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividir\u00e1 si esta division produce un decremento de la impureza igual o mayor a este valor.\n\n* `min_impurity_split`: float, default=None Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, ser\u00eda una hoja.\n\n* `bootstrap`: bool, default=True Si se utilizan muestras de bootstrap al construir \u00e1rboles. Si es False, se usa todo el conjunto de datos para construir cada \u00e1rbol.\n\n* `oob_score`: bool, default=False Si utilizar muestras out-of-bag para estimar el error de generalizaci\u00f3n. Solo disponible si bootstrap=True.\n\n* `n_jobs`: int, default=None N\u00famero de cores que se pueden usar para entrenar los \u00e1rboles. Cada \u00e1rbol es independiente del resto, as\u00ed que entrenar un bosque aleatorio es una tarea muy paralelizable. Por defecto s\u00f3lo utiliza 1 core de la CPU. Para mejorar el rendimiento puedes usar tantos cores como estimes necesario. Si usas n_jobs = -1, est\u00e1s indicando que quieres usar tantos cores como tenga tu m\u00e1quina.\n\n* `random_state`: int, RandomState instance or None, default=None Controla tanto la aleatoriedad del bootstrapping de las muestras utilizadas al construir \u00e1rboles (si bootstrap = True) como el muestreo de las caracter\u00edsticas a considerar cuando se busca la mejor divisi\u00f3n en cada nodo (si max_features <n_features).\n\n* `verbose`: int, default=0 Controla la verbosidad al ajustar y predecir.\n\n* `warm_start`: bool, default=False Cuando se establece en True, reutilice la soluci\u00f3n de la llamada anterior para ajustar y agregar m\u00e1s estimadores al conjunto; de lo contrario, simplemente ajuste un bosque completamente nuevo.\n\n* `class_weight`: {\u201cbalanced\u201d, \u201cbalanced_subsample\u201d}, diccionario o lista de diccionarios, default=None Pesos asociados con clases en el formato {class_label: weight}. Si no se da, se supone que todas las clases tienen un peso uno. Para problemas de m\u00faltiples salidas, se puede proporcionar una lista de dictados en el mismo orden que las columnas de y.\n\n* `ccp_alpha`: non-negative float, default=0.0 Par\u00e1metro de complejidad utilizado para la poda Minimal Cost-Complexity. Se elegir\u00e1 el sub\u00e1rbol con la mayor complejidad de costo siendo m\u00e1s peque\u00f1o que ccp_alpha. De forma predeterminada, no se realiza ninguna poda.\n    \n* `max_samples`: int or float, default=None Si bootstrap es True, El n\u00famero de muestras a extraer de X para entrenar a cada estimador base.\n\n    Si es None entonces extrae X.shape[0] muestras.\n\n    Si es int entonces extrae `max_samples` muestras.\n\n    Si es float entonces extrae `max_samples` * X.shape[0] muestras. Por lo tanto, max_samples debe estar en el intervalo (0, 1).","c15ea761":"### KNeighborsClassifier","7d792277":"### DecisionTreeClassifier","9e34b041":"Finalmente construimos y validamos el modelo final.","c971e026":"En esta pr\u00e1ctica estudiaremos los modelos m\u00e1s utilizados en `scikit-learn` para conocer los distintos hiperpar\u00e1metros que los configuran y estudiar los clasificadores resultantes. Adem\u00e1s, veremos m\u00e9todos de selecci\u00f3n de modelos orientados a obtener una configuraci\u00f3n \u00f3ptima de hiperpar\u00e1metros.","76cddc46":"### RandomForestClassifier","99232dc7":"Nosotros unicamente vamos a utilizar los hiperparametros `n_estimators`, `learning_rate` y `base_estimator` como `base_estimator` utilizamos arboles de decisi\u00f3n con `max_depth = 1` y `criterion = entropy`","439656ce":"Es un conjunto (ensemble) de `\u00e1rboles de decisi\u00f3n` combinados con `bagging`. Al usar `bagging`, lo que en realidad est\u00e1 pasando, es que distintos \u00e1rboles ven distintas porciones de los datos. Ning\u00fan \u00e1rbol ve todos los datos de entrenamiento. Esto hace que cada \u00e1rbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicci\u00f3n que generaliza mejor.","a21835b7":"## Selecci\u00f3n para Pima","f842a811":"### BaggingClassifier","2db90f6a":"Con los hiperparametros optimizados podemos observar que el mejor modelo es el `Histogram Gradient Boosting` en cuanto a ROC_AUC_Score, en cuanto a Recall_score el mejor modelo es el de `KNN` ","5581013e":"En este caso hemos decidido utilizar `cpp_alpha`, `criterion` , `max_depth` y `splitter` y donde los hiperparametros utilizados son `0.0`, `entropy`, `8` y `random` respectivamente. Con cpp_alpha = 0.0 podemos ver que el podado va a ser minimo permitiendo que el arbol se expanda, con una profundidad maxima de 8. Y vemos que la categoria de los nodos se elige aleatoriamente.","413d72b6":"# Enlace al kernel donde hemos realizado el estudio:\n \n https:\/\/www.kaggle.com\/pablomoreiragarcia\/estudio-kernel","124b93f5":"Antes de comenzar cargamos las librer\u00edas para que est\u00e9n disponibles posteriormente:","b4a2fc1d":"Con scoring estamos eligiendo las metricas que utilizaremos, en este caso vamos a utilizar `recall_score` y `roc_auc`.","445208ed":"# 5. Construcci\u00f3n y validaci\u00f3n del modelo final","c5667c2d":"Los `\u00e1rboles de decisi\u00f3n` son una t\u00e9cnica de aprendizaje autom\u00e1tico supervisado muy utilizada. Como su nombre indica, esta t\u00e9cnica toma una serie de decisiones en forma de \u00e1rbol. Los nodos finales (las hojas) nos dan la predicci\u00f3n que vamos buscando.","8039f4f5":"Nosotros utilizamos los hiperparametros `bootstrap` para extraer las muestras con reemplazo, `max_samples` y `n_estimators`, a parte de algunos hiperparametros para nuesto estimador base que en este caso es un arbol de decisi\u00f3n.","d8c730a9":"### RanfomForestClassifier","917177de":"Elegimos los scorers que vamos a utilizar.","7894348d":"### AdaBoostClassifier","6bd5e0ba":"Y continuamos con la variable clase:","ed7ddeaf":"### BaggingClassifier","00933347":"* `loss`: {\u2018auto\u2019, \u2018binary_crossentropy\u2019, \u2018categorical_crossentropy\u2019}, default=\u2019auto\u2019 La funci\u00f3n de p\u00e9rdida para usar en el proceso de refuerzo. \"Binary_crossentropy\" (tambi\u00e9n conocido como p\u00e9rdida log\u00edstica) se utiliza para la clasificaci\u00f3n binaria y se generaliza a \"categorical_crossentropy\" para la clasificaci\u00f3n multiclase. \"Auto\" elegir\u00e1 autom\u00e1ticamente cualquiera de las p\u00e9rdidas dependiendo de la naturaleza del problema.\n\n* `learning_rate`: float, default=0.1 La tasa de aprendizaje, tambi\u00e9n conocida como contracci\u00f3n. Esto se usa como factor multiplicativo para los valores de las hojas. Utilice 1 para que no se encoja.\n\n* `max_iter`: int, default=100 El n\u00famero m\u00e1ximo de iteraciones del proceso de refuerzo, es decir, el n\u00famero m\u00e1ximo de \u00e1rboles para la clasificaci\u00f3n binaria. Para la clasificaci\u00f3n multiclase, se construyen n \u00e1rboles de clases por iteraci\u00f3n.\n\n* `max_leaf_nodes`: int or None, default=31 El n\u00famero m\u00e1ximo de hojas de cada \u00e1rbol. Debe ser mayor que 1. Si es None, no hay l\u00edmite m\u00e1ximo.\n\n* `max_depth`: int or None, default=None La profundidad m\u00e1xima de cada \u00e1rbol.\n\n* `min_samples_leaf`: int, default=20 El n\u00famero m\u00ednimo de muestras por hoja. Para conjuntos de datos peque\u00f1os con menos de unos pocos cientos de muestras, se recomienda reducir este valor, ya que solo se construir\u00edan \u00e1rboles muy poco profundos.\n\n* `l2_regularization`: float, default=0 El par\u00e1metro de regularizaci\u00f3n L2. Utiliza 0 para no regularizaci\u00f3n.\n\n* `max_bins`: int, default=255 El n\u00famero m\u00e1ximo de contenedores que se utilizar\u00e1n para los valores que no faltan. Antes del entrenamiento, cada caracter\u00edstica de la matriz de entrada X se agrupa en bins con valores enteros, lo que permite una etapa de entrenamiento mucho m\u00e1s r\u00e1pida. Las funciones con una peque\u00f1a cantidad de valores \u00fanicos pueden usar menos de max_bins bins. Adem\u00e1s de los contenedores max_bins, siempre se reserva un contenedor m\u00e1s para los valores perdidos. No debe ser mayor de 255.\n\n* `categorical_features`: array-like of {bool, int} of shape (n_features) or shape (n_categorical_features,), default=None. Indica las caracter\u00edsticas categ\u00f3ricas.\n\n  Ninguno: ninguna caracter\u00edstica se considerar\u00e1 categ\u00f3rica.\n\n  tipo matriz booleana: m\u00e1scara booleana que indica caracter\u00edsticas categ\u00f3ricas.\n\n  tipo matriz de enteros: \u00edndices enteros que indican caracter\u00edsticas categ\u00f3ricas.\n\n    Para cada caracter\u00edstica categ\u00f3rica, debe haber como m\u00e1ximo max_bins categor\u00edas \u00fanicas, y cada valor categ\u00f3rico debe estar en [0, max_bins -1].\n\n* `monotonic_cst`: array-like of int of shape (n_features), default=None Indica la restricci\u00f3n mon\u00f3tona que se debe aplicar a cada funci\u00f3n. -1, 1 y 0 corresponden respectivamente a una restricci\u00f3n negativa, una restricci\u00f3n positiva y ninguna restricci\u00f3n.\n\n* `warm_start`: bool, default=False Cuando se establece en Verdadero, reutiliza la soluci\u00f3n de la llamada anterior para ajustar y agregar m\u00e1s estimadores al conjunto. Para que los resultados sean v\u00e1lidos, el estimador debe volver a entrenarse solo con los mismos datos.\n\n* `early_stopping`: \u2018auto\u2019 or bool, default=\u2019auto\u2019 Si es \"auto\", la parada anticipada est\u00e1 habilitada si el tama\u00f1o de la muestra es mayor que 10000. Si es Verdadero, la parada anticipada est\u00e1 habilitada; de lo contrario, la parada anticipada est\u00e1 deshabilitada.\n\n* `scoring`: str or callable or None, default=\u2019loss\u2019 Par\u00e1metro de puntuaci\u00f3n que se utilizar\u00e1 para la parada anticipada. Puede ser una sola cadena o un invocable. Si es None, se usa el puntaje predeterminado del estimador. Si la puntuaci\u00f3n = 'loss', se verifica la detenci\u00f3n anticipada con el valor de la p\u00e9rdida. Solo se utiliza si se realiza una parada anticipada.\n\n* `validation_fraction`: int or float or None, default=0.1 Proporci\u00f3n (o tama\u00f1o absoluto) de los datos de entrenamiento que se deben reservar como datos de validaci\u00f3n para la detenci\u00f3n anticipada. Si es None, la detenci\u00f3n anticipada se realiza en los datos de entrenamiento. Solo se utiliza si se realiza una parada anticipada.\n\n* `n_iter_no_change`: int, default=10 Utilizado para saber cuando hacer una parada anticipada. El proceso de ajuste se detiene cuando ninguna de las \u00faltimas puntuaciones n_iter_no_change es mejor que la n_iter_no_change - 1 -th-to-last one, hasta cierta tolerancia. Solo se utiliza si se realiza una parada anticipada.\n\n* `tol`: float or None, default=1e-7 La tolerancia absoluta a utilizar al comparar puntuaciones. Cuanto mayor sea la tolerancia, m\u00e1s probabilidades hay de que nos detengamos antes de tiempo: una mayor tolerancia significa que ser\u00e1 m\u00e1s dif\u00edcil que las iteraciones posteriores se consideren una mejora en la puntuaci\u00f3n de referencia.\n\n* `verbose`: int, default=0 El nivel de verbosidad. Si no es cero, imprima alguna informaci\u00f3n sobre el proceso de ajuste.\n\n* `random_state`: int, RandomState instance or None, default=None Generador de n\u00fameros pseudoaleatorios para controlar el submuestreo en el proceso de agrupamiento y la divisi\u00f3n de datos de validaci\u00f3n\/entrenamiento si la parada anticipada est\u00e1 habilitada. Pase un int para una salida reproducible a trav\u00e9s de m\u00faltiples llamadas a funciones.","b29b9226":"Creamos el Pipeline para titanic obtenido de la practica anterior:","b6482b06":"# 2. Carga de datos","a7dc34ce":"Adem\u00e1s, fijamos una semilla para que los experimentos sean reproducibles:","09da4966":"### AdaBoostClassifier","870d5398":"Comprobando que se ha cargado correctamente:","3a9ad00c":"Algoritmo basado en la combinaci\u00f3n de modelos predictivos d\u00e9biles (weak learners) normalmente \u00e1rboles de decisi\u00f3n para crear un modelo predictivo fuerte. La generaci\u00f3n de los \u00e1rboles de decisi\u00f3n d\u00e9biles se realiza de forma secuencial, cre\u00e1ndose cada \u00e1rbol de forma que corrija los errores del \u00e1rbol anterior. Los aprendices suelen ser \u00e1rboles \"poco profundos\", de apenas uno, dos o tres niveles de profundidad","d34257cd":"Algunos de los parametros que podriamos utilizar son:\n\n* `weights`: La forma de calcular los pesos para las predicciones, existen 2 posibilidades.:\n   - uniform : Peso uniforme, donde todos los puntos de cada grupo pesan lo mismo.\n\n   - distance :El peso equivale a la inversa de la distancia entre los puntos, va a valer mas si los puntos est\u00e1n muy cerca que si est\u00e1n alejados.\n\n   - [callable] : Tambien esta la posibilidad donde acepta un array de distancias.\n* `n_neighbors`: Numero de grupos (neighbors) a utilizar, si no se especifica ser\u00eda 5.\n* `algorithm`: Algoritmos para procesar los vecinos m\u00e1s cercanos:\n\n   - \u2018ball_tree\u2019 utilizar\u00e1 BallTree\n    \n   - \u2018kd_tree\u2019 utilizar\u00e1 KDTree\n\n   - \u2018brute\u2019 utilizar\u00e1 busqueda con fuerza bruta.\n\n   - \u2018auto\u2019 elegira de forma automatica el algoritmo m\u00e1s adecuado segun los valores que se le pasen al fit.\n   \n* `metric`: La medida de distancia que se va a utilizar, si no se especifica, se utiliza `Minkowski` donde utilizamos la variable `p`, si la `p` es 2 ser\u00eda equivalente a utilizar `Euclidean`, y si fuera 1, ser\u00eda equivalente a utilizar `Manhattan`.\n\n* `leaf_size`: Tama\u00f1o de las hojas utilizado en los algooritmos `ball_tree` o `kd_tree`.\n\n* `n_jobs`: El n\u00famero de busquedas paralelas que va a realizar el ordenador, cuanto m\u00e1s alto mejor va a ser el resultado pero m\u00e1s costoso.","d93ada89":"### DecisionTreeClassifier","fed1a4ce":"# Pr\u00e1ctica 2: Aprendizaje y selecci\u00f3n de modelos de clasificaci\u00f3n*\n\n### Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n### Alumnos:\n\n* Pablo Moreira Garcia\n* Ruben Martinez Sotoca\n\nAdaptado de las pr\u00e1cticas de Jacinto Arias Mart\u00ednez y Enrique Gonz\u00e1lez Rodrigo","c881134c":"Algunos de los parametros que podriamos utilizar son:\n\n* `base_estimator`: object, default=None El estimador base para ajustarse a subconjuntos aleatorios del conjunto de datos. Si es None, entonces el estimador base es un DecisionTreeClassifier.\n\n* `n_estimators`: int, default=10 El n\u00famero de estimadores de base en el conjunto.\n\n* `max_samples`: int or float, default=1.0 El n\u00famero de muestras a extraer de X para entrenar a cada estimador base.\n\n    Si es int entonces extrae `max_samples` muestras.\n\n    Si es float entonces extrae `max_samples` * X.shape[0] muestras.\n\n* `max_features`: int or float, default=1.0 El n\u00famero de caracter\u00edsticas a extraer de X para entrenar cada estimador base.\n\n    Si es int entonces extrae `max_features` caracteristicas.\n\n    Si es float entonces extrae`max_features` * X.shape[1] caracteristicas.\n\n* `bootstrap`: bool, default=True. Las muestras se extraen con reemplazo. Si es Falso, se realiza un muestreo sin reemplazo.\n\n* `bootstrap_features`: bool, default=False Si los rasgos se dibujan con el reemplazo.\n\n* `oob_score`: bool, default=False Si utilizar muestras out-of-bag para estimar el error de generalizaci\u00f3n. Solo disponible si bootstrap=True.\n\n* `warm_start`: bool, default=False Cuando es True, reutiliza la soluci\u00f3n de la llamada anterior para ajustar y agregar m\u00e1s estimadores al conjunto, de lo contrario, simplemente ajusta un conjunto completamente nuevo.\n\n* `n_jobs`: int, default=None El n\u00famero de busquedas que se ejecutar\u00e1n en paralelo tanto para ajustar como para predecir.\n\n* `random_state`: int, RandomState instance o None, default=None Controla el remuestreo aleatorio del conjunto de datos original (en cuanto a muestras y caracter\u00edsticas). Si el estimador base acepta un atributo de `random_state`, se genera una semilla diferente para cada instancia en el conjunto. Pasar un int para una salida reproducible a trav\u00e9s de m\u00faltiples llamadas a funciones.\n\n* `verbose`: int, default=0 Controla la verbosidad al ajustar y predecir.\n\nA parte de estos tambien se podr\u00edan ajustar los parametros del estimador base.","54391525":"A diferencia de muchos modelos ML que se enfocan en un solo modelo para completar predicciones de alta calidad, el algoritmo `Boosting` intenta mejorar las capacidades de predicci\u00f3n entrenando una serie de modelos d\u00e9biles, cada uno de los cuales puede compensar las debilidades de sus predecesores.\n\n`AdaBoost` es un algoritmo de `Boosting` espec\u00edfico (tambi\u00e9n conocido como `AdaBoost discreto`) desarrollado para problemas de clasificaci\u00f3n. La debilidad est\u00e1 determinada por la tasa de error del estimador d\u00e9bil.","739fb3f1":"Cargamos los datos y obtenemos los conjuntos de variables predictoras X y la variable clase y.","2c593db9":"En este caso hemos utilizado unicamente `metric`, `n_neighbors` y `weighs`, donde los mejores hiperparametros han sido `euclidean`, `3` y `uniform` respectivamente, donde vemos que los resultados en cuanto a score no cambian, cuando se prueba con `euclidean` o `manhattan`, o con `uniform` o `distance`, pero lo que si que vemos que cambia son los tiempos.","80a34197":"Vemos que la particion de la base de datos en variables predictoras y por otro lado la variable clase se ha realizado correctamente, por lo que ahora dividimos en train\/test.","5b8af70c":"Este estimador tiene soporte nativo para valores perdidos (NaN). Durante el entrenamiento, cuando el arbol crece, aprende en cada punto de divisi\u00f3n si las muestras con valores perdidos deben ir al hijo izquierdo o derecho, seg\u00fan la ganancia potencial. Al predecir, las muestras con valores perdidos se asignan al hijo izquierdo o derecho en consecuencia. Si no se encontraron valores perdidos para una caracter\u00edstica determinada durante el entrenamiento, las muestras con valores perdidos se asignan al hijo que tenga m\u00e1s muestras.\n\nComo el algoritmo Histogram Gradient Boosting realiza un discretizado interno, no permite el uso de discretizadores en pasos previos del pipeline, por lo que no utilizamos nuestro discretizador.","4b6b3b50":"Creamos la funcion para normalizar la base de datos de titanic.","eb8a6e2f":"Y finalizamos con el conjunto de datos de prueba:","2a0bb033":"Vamos a utilizar, como en la practica 1, los conjuntos de datos `Breast Cancer Wisconsin` y `Pima Indians Diabetes`","c6704cc0":"### GradientBoostingClassifier","a8c0135d":"### HistGradientBoostingClassifier","2901e0b4":"Para `random_forest` utilizamos los hiperparametros `criterion`, `max_features` y `n_estimators` y `gini`, `sqrt` y `150` respectivamente son los mejores hiperparametros seleccionados.","ece71158":"# 1. Preliminares","97e76702":"Para la evaluaci\u00f3n de los modelos vamos a utilizar validaci\u00f3n cruzada, lo cual nos garantiza que los resultados no van a estar sesgados. Con esto lo que hacemos es dividir el conjunto de datos en `n_splits` particiones, de las cuales se elige una como test y las demas como train. Este proceso se repite `n_repeats` cada vezz con conjuntos de entrenamiento y de test distintos.","f630c02f":"Por \u00faltimo, dividimos el conjunto de datos en entrenamiento y prueba mediante un *holdout* estratificado:","3f02ef3a":"Vamos a comprobar que se ha separado correctamente. Comenzamos con las variables predictoras:","136da8f8":"Para `GradientBoosting` teniamos una gr\u00e1n cantidad de hiperparametros para seleccionar, aunque nosotros unicamente vamos a utilizar `criterion`, `learning_rate`, `max_depth`, `n_estimators` y `subsample`.","4fa6ba29":"# 3. Evaluaci\u00f3n de modelos","584db724":"Y nos aseguramos que se ha realizado adecuadamente. Comenzamos con el conjunto de datos de entrenamiento:","6d99b862":"Ya tenemos todos los clasificadores optimizados, por lo que ahora tendremos que elegir el mejor de todos. \nComo es complicado ver todos los resultados en el apartado anterior, gracias a las funciones de evaluate_estimators2 vamos a poder ver de manera clara los resultados obtenidos por los distintos clasificadores en unas tablas.","931835dc":"### GradientBoostingClassifier","6f062868":"# 4. Selecci\u00f3n de modelos","1d1368a7":"### HistGradientBoostingClassifier","25c5897b":"Viendo estos datos, el modelo que peor resultados brinda es el de `Nearest neighbors`. Pensamos que esto es as\u00ed porque tenemos bastantes variables y KNN se comporta de forma que todas las variables se consideran de igual importancia, lo que hace que una variable que aporte poco al problema le quite importancia a una variable que sea significativa.\n\nPodemos observar que `Histogram Gradient Boosting` obtiene los mejores resultados en Wisconsin pero en Pima quien obtiene los mejores resultados es `Decision Tree Classifier`, esto puede deberse por la gran diferencia que hay entre las dos bases de datos en cuanto a tama\u00f1o y cantidad de informaci\u00f3n.\n\nEn cuanto a los ensembles vemos que generalmente obtienen buenos resultados en general.\n\n\n\n","36554e24":"`BagginClassifier` es un metaestimador de conjunto que ajusta los clasificadores base cada uno en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales (ya sea por votaci\u00f3n o promediando) para formar una predicci\u00f3n final. \nUn metaestimador de este tipo se puede utilizar t\u00edpicamente como una forma de reducir la varianza de un estimador de caja negra (por ejemplo, un \u00e1rbol de decisi\u00f3n), al introducir la aleatorizaci\u00f3n en su procedimiento de construcci\u00f3n y luego hacer un conjunto a partir de \u00e9l.","6f6a9d19":"Vamos a usar la funci\u00f3n `utils.optimize_params` para encontrar los mejores hiperpar\u00e1metros y asi optimizar los distintos algoritmos y al mismo tiempo mostrar los resultados de la validaci\u00f3n cruzada para poder ver los distintos resultados que vamos a ir obteniendo.","a3fcc593":"Algunos de los parametros que podriamos utilizar son:\n\n* `base_estimator`: object, default=None El estimador base a partir del cual se construye el conjunto. Se requiere soporte para la ponderaci\u00f3n de la muestra, as\u00ed como atributos adecuados de classes_ y n_classes_ . Si es None, entonces el estimador base es DecisionTreeClassifier inicializado con max_depth = 1.\n\n* `n_estimators`: int, default=50 El n\u00famero m\u00e1ximo de estimadores en los que finaliza el boosting. En caso de un ajuste perfecto, el procedimiento de aprendizaje se detiene antes de tiempo.\n\n* `learning_rate`: float, default=1. Peso aplicado a cada clasificador en cada iteraci\u00f3n del boosting. Una tasa de aprendizaje m\u00e1s alta aumenta la contribuci\u00f3n de cada clasificador. Existe una compensaci\u00f3n entre los par\u00e1metros learning_rate y n_estimators.\n\n* `algorithm`: {\u2018SAMME\u2019, \u2018SAMME.R\u2019}, default=\u2019SAMME.R\u2019 Si es 'SAMME.R', utiliza el algoritmo de real boosting de SAMME.R. base_estimator debe soportar el c\u00e1lculo de probabilidades de clase. Si es 'SAMME', utiliza el algoritmo de discrete boosting de SAMME. El algoritmo SAMME.R generalmente converge m\u00e1s r\u00e1pido que SAMME, logrando un error de prueba menor con menos iteraciones de impulso.\n\n* `random_state`: int, RandomState instance o None, default=None Controla la semilla aleatoria dada en cada base_estimator en cada iteraci\u00f3n de boosting. Por lo tanto, solo se usa cuando base_estimator expone un random_state.\n\nA parte de estos tambien existirian los parametros del estimador base.","afd76080":"A su vez, lo dividimos en variables predictoras (`X`) y variable clase (`y`):","d09e00ee":"Antes de comenzar a utilizar los modelos, es necesario que almacenemos en el conjunto de entrenamiento de nuestras bases de datos en la variable que vamos a utilizar. De lo contrario, usar\u00edamos todo el conjunto de datos y las pruebas sobre el conjunto test perder\u00edan su eficacia.","8a4e426e":"Tras haber explicado detalladamente los hiperparametros de los distintos algoritmos anteriormente, en la seleccion para Wisconsin, aqui no vamos a explicar mucho, luego comentaremos los resultados en la parte final.","bc8d415e":"Basicamente como explicamos en la practica anterior, borramos las variables que no nos son necesarias como todas las `x_se` o `x_worst`, las que tienen alta correlacion como `perimeter_mean`, `area_mean`... y las que tienen un alto numero de valores anomalos o ruidosos como puede ser la variable `Unnamed: 32`\n\nLos valores anomalos que tengan las distintas variables, que si que utilizamos, los cambiamos a nan gracias al segundo transformador. Y luego creamos un imputador que cambia los valores Nan por la media de esa variable.\n\nPor ultimo utilizamos un discretizador de 5 conjuntos ya que como explicamos en la practica anterior al representar graficamente estas variables en una nube de putos y diferenciar por colores vamos a ver en algunas variables que hay segmentos bien diferenciados con solo un tipo de clase.","e6e58ba0":"En este apartado vamos a poder elegir los mejores hiperparametros gracias a un algoritmo de seleccion de modelos por fuerza bruta `GridSearch` al cual le pasamos un conjunto de hiperparametros (que nosotros seleccionamos) y este hace una evaluacion mediante validacion cruzada de todas las posibles combinaciones entre los hiperparametros para poder seleccionar los mejores vaalores.\n\nUna vez obtenidos los mejores valores, evaluaremos los clasificadores para comprobar si la estimaci\u00f3n es buena.","65691696":"Funcion utilizada para normalizar nuestra base de datos para el algoritmo de los vecinos m\u00e1s cercanos."}}