{"cell_type":{"13fe5daa":"code","5a9bec1f":"code","f96330a6":"code","09574c5b":"code","24ed87ff":"code","d68e2803":"code","f09e8a2c":"code","8a30dcd3":"code","fc74f33b":"code","4db2a0b4":"code","83a7fc06":"code","e884fae2":"code","08f71458":"markdown","827df36f":"markdown","73469a7e":"markdown","bf2ff6e0":"markdown","2e81d054":"markdown","cf381253":"markdown","397a0fa9":"markdown","48a4a7a0":"markdown","f59f7232":"markdown","79d45e3d":"markdown","f49c0c23":"markdown","62d97d60":"markdown"},"source":{"13fe5daa":"# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex4 import *\nprint(\"Setup Complete\")","5a9bec1f":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","f96330a6":"X_train.head()","09574c5b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","24ed87ff":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\n# Preprocessing for numerical data\nnumerical_transformer =  SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer =Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=200, random_state=0)\n\n# Check your answer\nstep_1.a.check()","d68e2803":"# Lines below will give you a hint or solution code\n#step_1.a.hint()\n#step_1.a.solution()","f09e8a2c":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid) \nparam_grid = {\n    'model__max_depth': [80, 90, 100, 110],\n    'model__max_features': [2, 3],\n    'model__min_samples_leaf': [3, 4, 5],\n    'model__min_samples_split': [8, 10, 12],\n    'model__n_estimators': [100, 200, 300, 1000],\n    'model__random_state': [0, 42]\n}\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(  my_pipeline, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n#grid_search.fit(X_train, y_train)\n#print(grid_search.best_params_)\n#best_grid = grid_search.best_estimator_\n#predsGS = grid_search.best_estimator_.predict(X_valid) \n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\n#scoreGS = mean_absolute_error(y_valid, preds)\n\nprint('MAE:', score)\n#print('MAE GSCV:', scoreGS)\n\n# Check your answer\nstep_1.b.check()","8a30dcd3":"###### with xgBoost trying out best combinations\nfrom xgboost import XGBRegressor\ndef getModelAndScore(stops):\n    xgModel = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.01, max_delta_step=0, max_depth=7,\n             min_child_weight=4,  monotone_constraints='()',\n             n_estimators=1400, n_jobs=4, nthread=4, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1,   subsample=0.7, tree_method='exact',\n             validate_parameters=1, verbosity=None)\n    xgModel.fit(preprocessor.transform(X_train), y_train, early_stopping_rounds=stops,   eval_set=[(preprocessor.transform(X_valid), y_valid)],  verbose=False)\n    predsXG = xgModel.predict(preprocessor.transform(X_valid))\n    scorexg = mean_absolute_error(y_valid, predsXG)\n    return (scorexg,stops,xgModel)\nmodels = {getModelAndScore(stops)[0]:getModelAndScore(stops) for stops in range(16,21,3)}\nprint(models)\nmaxScore = max(models,key= models.get)\nprint(\"max score model\",maxScore)\nxgModel = models.get(maxScore)[2] \npredsXG = xgModel.predict(preprocessor.transform(X_valid))\nscorexg = mean_absolute_error(y_valid, predsXG)\nprint('XG GSCV:', scorexg)","fc74f33b":"#random_search.best_estimator_.fit(preprocessor.transform(X_train), y_train, early_stopping_rounds=16,   eval_set=[(preprocessor.transform(X_valid), y_valid)],  verbose=False)\n#predsXG = xgModel.predict(preprocessor.transform(X_valid))\n#random_search.best_estimator_.predict(preprocessor.transform(X_valid))\n#scorexg = mean_absolute_error(y_valid, predsXG)\n#print('XG Opt:', scorexg)\npredsXG = xgModel.predict(preprocessor.transform(X_valid))\nscorexg = mean_absolute_error(y_valid, predsXG)\nprint('XG GSCV:', scorexg)","4db2a0b4":"# Preprocessing of test data, fit model\npreds_test = my_pipeline.predict(X_test)\n# Check your answer\nstep_2.check()","83a7fc06":"# Lines below will give you a hint or solution code\n#step_2.hint()\n#step_2.solution()","e884fae2":"# Save test predictions to file\npreds_test_xg = xgModel.predict(preprocessor.transform(X_test)) # Your code here\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test_xg})\noutput.to_csv('submission.csv', index=False)","08f71458":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/161289) to chat with other Learners.*","827df36f":"## trying gridsearch and cv for tuning","73469a7e":"In this exercise, you will use **pipelines** to improve the efficiency of your machine learning code.\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system.","bf2ff6e0":"# Step 2: Generate test predictions\n\nNow, you'll use your trained model to generate predictions with the test data.","2e81d054":"### Part B\n\nRun the code cell below without changes.\n\nTo pass this step, you need to have defined a pipeline in **Part A** that achieves lower MAE than the code above.  You're encouraged to take your time here and try out many different approaches, to see how low you can get the MAE!  (_If your code does not pass, please amend the preprocessing steps and model in Part A._)","cf381253":"# Using XGboost to find out best model hyperparams\ntuning by dump looping :)","397a0fa9":"The next code cell uses code from the tutorial to preprocess the data and train a model.  Run this code without changes.","48a4a7a0":"The code yields a value around 17862 for the mean absolute error (MAE).  In the next step, you will amend the code to do better.\n\n# Step 1: Improve the performance\n\n### Part A\n\nNow, it's your turn!  In the code cell below, define your own preprocessing steps and random forest model.  Fill in values for the following variables:\n- `numerical_transformer`\n- `categorical_transformer`\n- `model`\n\nTo pass this part of the exercise, you need only define valid preprocessing steps and a random forest model.","f59f7232":"**This notebook is an exercise in the [Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) course.  You can reference the tutorial at [this link](https:\/\/www.kaggle.com\/alexisbcook\/pipelines).**\n\n---\n","79d45e3d":"Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition.","f49c0c23":"You will work with data from the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course). \n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","62d97d60":"```python\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nxgModelTune = XGBRegressor()\nparams = {\n        'n_estimators': [1000, 1200,1300,1400],\n        'learning_rate': [0.01, 0.05] ,\n            'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n        }\nfolds = 3\nparam_comb = 5\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)\nX = preprocessor.transform(X_full[my_cols].copy())\nrandom_search = RandomizedSearchCV(xgModelTune, param_distributions=params, n_iter=param_comb, scoring='neg_mean_absolute_error', n_jobs=6, \n                                   cv=skf.split(X,y),\n                                   verbose=3, random_state=42 )\nrandom_search.fit(X, y)\n\nprint('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\n```\n#**This gave best settings as n_estimators=1600, learning_rate=0.01** commented as took lot of time\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.01, max_delta_step=0, max_depth=7,\n             min_child_weight=4, missing=nan, monotone_constraints='()',\n             n_estimators=1400, n_jobs=4, nthread=4, num_parallel_tree=1,\n             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, silent=1, subsample=0.7, tree_method='exact',\n             validate_parameters=1, verbosity=None)\n\n Best normalized gini score for 3-fold search with 5 parameter combinations:\n-30984.431160842043\n\n Best hyperparameters:\n{'subsample': 0.7, 'silent': 1, 'objective': 'reg:linear', 'nthread': 4, 'n_estimators': 1400, 'min_child_weight': 4, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n"}}