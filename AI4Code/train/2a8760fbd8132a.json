{"cell_type":{"6a230a8e":"code","6d6232d9":"code","01780f3f":"code","8ae04eb7":"code","7285b7fc":"code","e60f383a":"code","63f753a3":"code","a54c7628":"code","b96ad985":"code","8960f281":"code","68fbd594":"code","6d5fb994":"code","705e847f":"markdown","e7dce624":"markdown","17bcd95a":"markdown","0c4c7cb2":"markdown","2c197be2":"markdown","0ce8f912":"markdown","c9c36673":"markdown","4f5075c3":"markdown","472c05d1":"markdown","32080c82":"markdown","790ef887":"markdown"},"source":{"6a230a8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6d6232d9":"iris=pd.read_csv('..\/input\/iris\/Iris.csv',index_col='Id')\niris.head()","01780f3f":"iris.info()","8ae04eb7":"iris.Species.value_counts()","7285b7fc":"plt.pie(iris.Species.value_counts(),labels=iris.Species.unique(),autopct = '%1.2f%%')","e60f383a":"y = iris.pop('Species') #Target\nX = iris  #DataFrame with features\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10)\nprint(y_train.value_counts())\nprint(y_test.value_counts())","63f753a3":"plt.figure(figsize=(10,15))\nplt.subplot('121')\nplt.pie(y_train.value_counts(),labels=y_train.unique(),autopct = '%1.2f%%',shadow = True)\nplt.title('Training Dataset')\n\nplt.subplot('122')\nplt.pie(y_test.value_counts(),labels=y_test.unique(),autopct = '%1.2f%%', shadow =True)\nplt.title('Test Dataset')\n\nplt.tight_layout()","a54c7628":"from sklearn.model_selection import StratifiedShuffleSplit \n\nsplitter=StratifiedShuffleSplit(n_splits=1,random_state=12) #we can make a number of combinations of split\n#But we are interested in only one.\n\nfor train,test in splitter.split(X,y):     #this will splits the index\n    X_train_SS = X.iloc[train]\n    y_train_SS = y.iloc[train]\n    X_test_SS = X.iloc[test]\n    y_test_SS = y.iloc[test]\nprint(y_train_SS.value_counts())  \nprint(y_test_SS.value_counts())","b96ad985":"plt.figure(figsize=(10,15))\n\nplt.subplot('121')\nplt.pie(y_train_SS.value_counts(),labels=y_train_SS.unique(),autopct = '%1.2f%%')\nplt.title('Training Dataset')\n\nplt.subplot('122')\nplt.pie(y_test_SS.value_counts(),labels=y_test_SS.unique(),autopct = '%1.2f%%')\nplt.title('Test Dataset')\n\nplt.tight_layout()","8960f281":"# Model 1 with stratified sample\nmodel1=LogisticRegression()\nmodel1.fit(X_train,y_train)\ny_pred_m1=model1.predict(X_test)\nacc_m1=accuracy_score(y_pred_m1,y_test)\n\nprint(acc_m1)","68fbd594":"# Model 2 with stratified sample\nmodel2=LogisticRegression()\nmodel2.fit(X_train_SS,y_train_SS)\ny_pred_m2=model1.predict(X_test_SS)\nacc_m2=accuracy_score(y_pred_m2,y_test_SS)\n\nprint(acc_m2)","6d5fb994":"#visualizing result\nplt.bar(['Random Split','Stratified split'],[acc_m1,acc_m2])\nplt.title('Random vs Stratified split')","705e847f":"We see a clear difference in the way the training data and test data are distributed!\n\nWhy this happens? What does train_test_split do?\n\nThe train_test_split function randomly splits the training set and testing set. But the problem here is the less amount of data(150 only). If we have only 6 data,two from each class and we split it into train and test we cannot expect a random split that contains equal proportions both in test and train. \n\nSo lesser the data, more the chance that your test and train set are not stratified!  ","e7dce624":"\n   This is where StratifiedShuffleSplit helps us. Let us built a logistic regressions with train_test_split and StrstifiedShuffleSplit and analyze the difference.\n   \n   This notebook is inspired from this article \"https:\/\/blog.usejournal.com\/creating-an-unbiased-test-set-for-your-model-using-stratified-sampling-technique-672b778022d5\"","17bcd95a":"# Why Stratification?\n\n   For model evaluation we often use train_test_split from sklearn to split the data into train set and test set. This is fine if the sample size is large. If the sample size is small we cannot expect the same. \n   \n   For iris dataset, the data is equally distributed between the three species of flowers. Have you ever given a thought that your train_test_split on the data gives equal proportion of these classes?\n   \n   If it is not so then we are training on one distribution and predicting from another distribution!\n   ","0c4c7cb2":"![StratifiedRandomSampling.jpg](attachment:StratifiedRandomSampling.jpg)\nImage from Wikipedia","2c197be2":"Here we see a clear difference between the stratified and unstratified samples! The same Logistic regression has different accuracies! \n\nThis is because the way they are tested are the way they are trained in case of stratified split. ","0ce8f912":"Here we note that the given dataset has equal distribution of target variables. If we use train_test_split will it get equal proportion from all species? i.e., will it be a strtified sample?\n\nLets look!","c9c36673":"\nNow thats the end. We have made a stratified split! We found we could actually improve our accuracy!\n\nNote : It is crucial as the data gets limited!\n\nFeedbacks are welcomed!\n\nIf you like please **Upvote!**","4f5075c3":"# The Models","472c05d1":"# StratifiedShuffleSplit\nHow do we achieve stratified split on a small amount of data such as this?\n\nHere stands tall the StratifiedShuffleSplit of sklearn. It actually forces the data to be stratified.  Lets split and see..","32080c82":"Thats Great! We have made a stratified sampling!\n\nNow lets build two logistic regression models and compare them!","790ef887":"**More on Iris:**\nIris data set is collection of length and width of sepal and petal of iris flower and there species. The dataset has three species of Iris.\n\nWe can classify the flower species by using the four features. So it is a four feature three class classification problem.\n\nLets analyze how our target variable is distributed in the data.[](http:\/\/)"}}