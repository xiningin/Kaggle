{"cell_type":{"8d211fe6":"code","6856b866":"code","2ab9234c":"code","e8323425":"code","d9ed7839":"code","1def1fee":"code","19c51bd5":"code","eb1ead64":"code","afb84fb3":"code","83b51d8b":"code","2db58969":"code","44f23295":"code","34bc695c":"code","6b7f778e":"code","93ba014e":"code","4aaca984":"code","4877adaf":"code","38532933":"code","cdd627cd":"code","7b2044ca":"code","05fb8d68":"code","586e9050":"code","bace2113":"code","e247afb3":"code","e133c355":"code","ba49bb8f":"markdown"},"source":{"8d211fe6":"import pickle\nimport pandas as pd\nhpa_df = pd.read_pickle(\"..\/input\/partial-dataframes-with-paths-for-training\/dataset_partial_224\")","6856b866":"hpa_df","2ab9234c":"LABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","e8323425":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt","d9ed7839":"IMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = 32\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","1def1fee":"#%%script echo skipping\n#set an amount of folds to split dataframe into --> k-fold cross validation\n# explanation: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\nN_FOLDS=5\n#choose which one of the 5 folds will be used as validation set this time\ni_VAL_FOLD=1\nhpa_df=np.array_split(hpa_df, N_FOLDS+1) #add one extra part for testing set\ndf_test_split=hpa_df[-1]\nhpa_df=hpa_df[:-1]\ni_training = [i for i in range(N_FOLDS)]\ni_training.pop(i_VAL_FOLD-1)\ni_validation=i_VAL_FOLD-1\ndf_train_split=list()\nfor i in i_training:\n    df_train_split.append(hpa_df[i])\ndf_train_split=pd.concat(df_train_split)\ndf_val_split=hpa_df[i_validation]","19c51bd5":"print(len(df_train_split))\nprint(len(df_val_split))\nprint(len(df_test_split))","eb1ead64":"#%%script echo skipping\n#analyze class imbalance and set up class weights here\n#https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/improve-class-imbalance-class-weights\/\ny_train=df_train_split[\"Label\"].apply(lambda x:list(map(int, x.split(\"|\"))))\ny_train=y_train.values\ny_train=np.concatenate(y_train)\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","afb84fb3":"#%%script echo skipping\ntmp_dict={}\nfor i in range(len(LABELS)):\n    tmp_dict[i]=class_weights[i]\nclass_weights=tmp_dict\nclass_weights","83b51d8b":"#%%script echo skipping\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n    return one_hot_enc_tensor\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    rgb = tf.io.read_file(df_dict['path'])\n    image = tf.image.decode_png(rgb, channels=3)\n    #https:\/\/medium.com\/@kyawsawhtoon\/a-tutorial-to-histogram-equalization-497600f270e2\n    image=tf.image.per_image_standardization(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","2db58969":"#%%script echo skipping\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","44f23295":"#%%script echo skipping\ndef get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(10):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","34bc695c":"#%%script echo skipping\n# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n#print(label_batch)","6b7f778e":"#%%script echo skipping\n\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainable = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","93ba014e":"#%%script echo skipping\ntime_stopping_callback = tfa.callbacks.TimeStopping(seconds=int(round(60*60*8.5)), verbose=1) #8.5h\n\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\nlronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=5, verbose=0,\n    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0\n)","4aaca984":"#%%script echo skipping\n#set up checkpoint save\n#source:https:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load\n!pip install -q pyyaml h5py\nimport os\ncheckpoint_path_input = \"..\/input\/train-data-dropout03-effnetb0-subset-of-hpa\/cp.ckpt\"\ncheckpoint_path = \".\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","4877adaf":"#%%script echo skipping\nimport keras.backend as K\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef f1_loss(y_true, y_pred):\n    \n    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","38532933":"#%%script echo skipping\nimport tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","cdd627cd":"#%%script echo skipping\n# Initialize model\ntf.keras.backend.clear_session()\n#model = get_model()\n\nmodel.load_weights(checkpoint_path_input)\n\n# Compile model\nmodel.compile(optimizer='adam', loss=f1_loss, metrics=f1)\n\n# Train\nhistory=model.fit(train_ds,\n                  epochs=100,\n                  validation_data=val_ds,\n                  class_weight=class_weights,\n                  callbacks=[cp_callback,earlystopper,time_stopping_callback])","7b2044ca":"#%%script echo skipping\nhistory.history","05fb8d68":"#%%script echo skipping\n#source: https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n# list all data in history\nprint(history.history.keys())\n# summarize history for f1\nplt.plot(history.history['f1'])\nplt.plot(history.history['val_f1'])\nplt.title('model f1 score')\nplt.ylabel('f1 score')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","586e9050":"#%%script echo skipping\nfor element in train_ds.as_numpy_iterator():\n    x=element[0]\n    y_true=element[1]\n    break\ny_pred = model.predict(x)\nprint(x.shape)\nprint(y_true.shape)\nprint(y_pred.shape)\n#print(\"X:\")\n#print(x)\nprint(\"y_true:\")\nprint(y_true)\nprint(\"y_pred:\")\nprint(y_pred)\n#y_pred=np.argmax(y_pred,axis=1)\ny_pred[y_pred>0.5] = 1\ny_pred[y_pred<0.5] = 0\nprint(\"y_pred:\")\nprint(y_pred)\n#y_pred=np.eye(len(LABELS))[y_pred]\n#print(\"y_pred:\")\n#print(y_pred)","bace2113":"#%%script echo skipping\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.multilabel_confusion_matrix.html\nimport sklearn.metrics as skm\n\ncm = skm.multilabel_confusion_matrix(y_true, y_pred)\nprint(cm)\nprint(skm.classification_report(y_true,y_pred))","e247afb3":"with open('.\/historyhistory.pkl', 'wb') as handle:\n    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)","e133c355":"with open('.\/history.pkl', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","ba49bb8f":"#### this notebook is part of the documentation on my HPA approach  \n    -> main notebook: https:\/\/www.kaggle.com\/philipjamessullivan\/0-hpa-approach-summary\n\n\n## 8: Batch size optimization\n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/8-batchsize-optim-from-ckpt-b0-bs-32-do-0-3  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/8-batchsize-optim-from-ckpt-b1-bs-32-do-0-3  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/8-batchsize-optim-from-ckpt-b2-bs-32-do-0-3  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/8-batchsize-optim-from-ckpt-b3-bs-32-do-0-3  \n    \n##### Network Architecture: Efficientnet (B0 to B3) with a Dropout layer of 0.3 & Batch size 32  \n\n#### Goal: further refine model by using batch size 32  \n\n##### INPUT: \n**resized cropped RGB cell images**:  \n\nEfficientnetB0 -> 224x224  \nEfficientnetB1 -> 240x240  \nEfficientnetB2 -> 260x260  \nEfficientnetB3 -> 300x300  \n\n**specific subset of ground truth dataframe**:  \n\nColums: img_id, Label, Cell#, bbox coords, path  \n\n##### OUTPUT:\n**model checkpoints**:  \n\none after each epoch\n\n**training history**:  \n\nshown as printed array on screen with f1 loss and f1 score per step"}}