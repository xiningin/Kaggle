{"cell_type":{"57de81eb":"code","8c15531c":"code","f8593fa8":"code","c7c13a0d":"code","fb2264f1":"code","109e937b":"code","f51e5c45":"code","c438d0cc":"code","f41f02d0":"code","d1f9e8b1":"code","1bcd19f6":"code","5ee023c3":"code","b8b16760":"code","d0b46fa7":"code","ae74e73f":"code","040ff1cb":"code","c1865f09":"code","c88cf072":"code","e716d6e7":"code","2a969fd6":"code","6f8aa4eb":"code","e8a4df6d":"code","21271a3a":"markdown","3a6b236f":"markdown","4c62a56f":"markdown","b88cc393":"markdown","a00a14e6":"markdown","642bbcbe":"markdown","be1fdcb8":"markdown","67a0cf80":"markdown"},"source":{"57de81eb":"import numpy as np\nimport pandas as pd\nfrom time import time\nimport os\nimport cupy\nfrom PIL import Image\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom cuml.metrics import confusion_matrix\n\ntrain = []\nval = []\ntest = []\ny_train = []\ny_val = []\ny_test = []\nWIDTH, HEIGHT = 200, 230 # Using higher dimensions require CUDA 11.0\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\/'):\n    category = 0 if dirname.endswith('NORMAL') else 1\n    y_train = y_train + [category for _ in filenames]\n    for filename in filenames:\n        train.append(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/'):\n    category = 0 if dirname.endswith('NORMAL') else 1\n    y_val = y_val + [category for _ in filenames]\n    for filename in filenames:\n        val.append(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test\/'):\n    category = 0 if dirname.endswith('NORMAL') else 1\n    y_test = y_test + [category for _ in filenames]\n    for filename in filenames:\n        test.append(os.path.join(dirname, filename))\n\n\nprint('Train size: ' + str(len(train)))\nprint('Val size: ' + str(len(val)))\nprint('Test size: ' + str(len(test)))","8c15531c":"train_index = np.arange(len(train))\nnp.random.shuffle(train_index)\ntrain = np.array(train)[train_index]\ny_train = np.array(y_train)[train_index]\n\nval_index = np.arange(len(val))\nnp.random.shuffle(val_index)\nval = np.array(val)[val_index]\ny_val = np.array(y_val)[val_index]\n\ntest_index = np.arange(len(test))\nnp.random.shuffle(test_index)\ntest = np.array(test)[test_index]\ny_test = np.array(y_test)[test_index]","f8593fa8":"import matplotlib.pyplot as plt\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    plt.figure(figsize=(3.8 * n_col, 3.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i], cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n        \n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    if y_pred is not None:\n        pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n        return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n    else:\n        return '%s' % (true_name)","c7c13a0d":"target_names = {0:'Normal',1:'Pneumonia'}\nprediction_titles = [title(None, y_val, target_names, i)\n                     for i in range(y_val.shape[0])]\n\nval_images = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in val]\nplot_gallery(val_images, prediction_titles, h=HEIGHT, w=WIDTH)\n\nplt.show()","fb2264f1":"import numbers\n\ndef _gen_batches(n, batch_size, min_batch_size=0):\n    \"\"\"\n    Generator to create slices containing batch_size elements, from 0 to n.\n    The last slice may contain less than batch_size elements, when batch_size\n    does not divide n.\n    Parameters\n    ----------\n    n : int\n    batch_size : int\n        Number of element in each batch\n    min_batch_size : int, default=0\n        Minimum batch size to produce.\n    Yields\n    ------\n    slice of batch_size elements\n    \"\"\"\n\n    if not isinstance(batch_size, numbers.Integral):\n        raise TypeError(\"gen_batches got batch_size=%s, must be an\"\n                        \" integer\" % batch_size)\n    if batch_size <= 0:\n        raise ValueError(\"gen_batches got batch_size=%s, must be\"\n                         \" positive\" % batch_size)\n    start = 0\n    for _ in range(int(n \/\/ batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)","109e937b":"from cuml.experimental.decomposition import IncrementalPCA as IPCA\n\nn_components = 95\nBATCH_SIZE = 512\n#WIDTH, HEIGHT = 750, 930\nipca = IPCA(n_components=n_components, batch_size=BATCH_SIZE)\ntotal_time = 0\n\nfor batch in _gen_batches(n=len(train), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = train[batch]\n    X_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_batch = [cupy.array(img) for img in X_batch]\n    # Normalize image\n    X_batch = (((cupy.array(X_batch) \/ 255) - 0.5) * 2)\n    X_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n    \n    t0 = time()\n    ipca.partial_fit(X_batch)\n    total_time += time() - t0\n    \nprint('Total time spent on IPCA partial fit: %0.2f sec' % total_time)","f51e5c45":"h,w = HEIGHT, WIDTH\ncomponents = (ipca.components_.reshape((n_components, WIDTH, HEIGHT)).get() \/ 2 + 0.5) # Uncenter the values\n\ncomponent_titles = [\"Component %d\" % i for i in range(components.shape[0])]\nplot_gallery(components, component_titles, h, w)","c438d0cc":"X_train_pca = cupy.zeros((len(train), n_components))\ntotal_time = 0\nfor batch in _gen_batches(n=len(train), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = train[batch]\n    X_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_batch = [cupy.array(img) for img in X_batch]\n    # Normalize image\n    X_batch = (((cupy.array(X_batch) \/ 255) - 0.5) * 2)\n    X_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n    \n    t0 = time()\n    X_train_pca[batch] = ipca.transform(X_batch)\n    total_time += time() - t0\n\nprint('Total time spent on IPCA train transform: %0.2f sec' % total_time)","f41f02d0":"X_val_pca = cupy.zeros((len(val), n_components))\n# Load Image\nX_batch = val\nX_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\nX_batch = [cupy.array(img) for img in X_batch]\n# Normalize image\nX_batch = (((cupy.array(X_batch) \/ 255) - 0.5) * 2)\nX_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n\nt0 = time()\nX_val_pca = ipca.transform(X_batch)\ntotal_time += time() - t0","d1f9e8b1":"X_test_pca = cupy.zeros((len(test), n_components))\nfor batch in _gen_batches(n=len(test), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = test[batch]\n    X_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_batch = [cupy.array(img) for img in X_batch]\n    # Normalize image\n    X_batch = (((cupy.array(X_batch) \/ 255) - 0.5) * 2)\n    X_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n    \n    t0 = time()\n    X_test_pca[batch] = ipca.transform(X_batch)\n    total_time += time() - t0\n    \nprint('Total time spent on IPCA transform: %0.2f sec' % total_time)","1bcd19f6":"from cuml.svm import SVC\n\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\nparam_grid = {'C': [1e0, 5e0, 1e1,5e1,1e2,5e2,1e3, 5e3, 1e4, 5e4],\n              'gamma': [1e-6,5e-5,1e-5,5e-4,1e-4, 5e-3, 1e-3], }\nclf = GridSearchCV(\n    SVC(kernel='rbf', class_weight='balanced'), param_grid\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)","5ee023c3":"train_pred = clf.predict(X_train_pca)\nprint(classification_report(y_train, train_pred.get()))\nprint(confusion_matrix(y_train, train_pred))","b8b16760":"val_pred = clf.predict(X_val_pca)\nprint(classification_report(y_val, val_pred.get()))\nprint(confusion_matrix(y_val, val_pred))","d0b46fa7":"test_pred = clf.predict(X_test_pca)\nprint(classification_report(y_test, test_pred.get()))\nprint(confusion_matrix(y_test, test_pred))","ae74e73f":"from sklearn.decomposition import IncrementalPCA as IPCA\n\nn_components = 95\nBATCH_SIZE = 512\nipca = IPCA(n_components=n_components, batch_size=BATCH_SIZE)\ntotal_time = 0\n\nfor batch in _gen_batches(n=len(train), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = train[batch]\n    X_train = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_train = [np.array(img) for img in X_train]\n    # Normalize image\n    X_train = (((np.array(X_train) \/ 255) - 0.5) * 2)\n    X_train = X_train.reshape((-1, WIDTH * HEIGHT))\n    t0 = time()\n    ipca.partial_fit(X_train)\n    total_time += time() - t0\n    \nprint('Total time spent on IPCA partial fit (sklearn): %0.2f sec' % total_time)","040ff1cb":"X_train_pca = np.zeros((len(train), n_components))\ntotal_time = 0\nfor batch in _gen_batches(n=len(train), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = train[batch]\n    X_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_batch = [np.array(img) for img in X_batch]\n    # Normalize image\n    X_batch = (((np.array(X_batch) \/ 255) - 0.5) * 2)\n    X_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n    \n    t0 = time()\n    X_train_pca[batch] = ipca.transform(X_batch)\n    total_time += time() - t0\n\nprint('Time spent on IPCA train transform (sklearn): %0.2f sec' % total_time)","c1865f09":"X_val_pca = np.zeros((len(val), n_components))\n# Load Image\nX_batch = val\nX_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\nX_batch = [np.array(img) for img in X_batch]\n# Normalize image\nX_batch = (((np.array(X_batch) \/ 255) - 0.5) * 2)\nX_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n\nt0 = time()\nX_val_pca = ipca.transform(X_batch)\ntotal_time += time() - t0","c88cf072":"X_test_pca = np.zeros((len(test), n_components))\nfor batch in _gen_batches(n=len(test), batch_size=BATCH_SIZE):\n    # Load Image\n    X_batch = test[batch]\n    X_batch = [Image.open(img_path).resize((HEIGHT, WIDTH)).convert('L') for img_path in X_batch]\n    X_batch = [np.array(img) for img in X_batch]\n    # Normalize image\n    X_batch = (((np.array(X_batch) \/ 255) - 0.5) * 2)\n    X_batch = X_batch.reshape((-1, WIDTH * HEIGHT))\n    \n    t0 = time()\n    X_test_pca[batch] = ipca.transform(X_batch)\n    total_time += time() - t0\nprint('Total time spent on IPCA transform (sklearn): %0.2f sec' % total_time)","e716d6e7":"from sklearn.svm import SVC\n\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\nparam_grid = {'C': [1e0, 5e0, 1e1,5e1,1e2,5e2,1e3, 5e3, 1e4, 5e4],\n              'gamma': [1e-6,5e-5,1e-5,5e-4,1e-4, 5e-3, 1e-3], }\nclf = GridSearchCV(\n    SVC(kernel='rbf', class_weight='balanced'), param_grid\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)","2a969fd6":"train_pred = clf.predict(X_train_pca)\nprint(classification_report(y_train, train_pred))\nprint(confusion_matrix(y_train, train_pred))","6f8aa4eb":"val_pred = clf.predict(X_val_pca)\nprint(classification_report(y_val, val_pred))\nprint(confusion_matrix(y_val, val_pred))","e8a4df6d":"test_pred = clf.predict(X_test_pca)\nprint(classification_report(y_test, test_pred))\nprint(confusion_matrix(y_test, test_pred))","21271a3a":"# Comparison with sklearn","3a6b236f":"## Transform\nOnce fitted with all the training data loaded by batch, we can transform the data to the final format, on which we can run a classification algorithm with all the data","4c62a56f":"# Conclusion\n\nThe algorithms from RAPIDS allow us to get an immediate speed boost:\n- IPCA speed-up: x4.5 faster on the partial fits\n- SVM speed-up: x10 faster on the grid search\n\nOur pipeline went from minutes in sklearn to seconds with RAPIDS.","b88cc393":"## Visualization of the principal components","a00a14e6":"# Chest X-Ray pneumonia dataset\n\nThe chest X-ray dataset is a Kaggle dataset, that classify 1.15 Gb of X-ray images into either 'Normal' or 'Pneumonia' cases.\n\nIt is available through the following link: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia\n\nOr with the kaggle API command: `kaggle datasets download -d paultimothymooney\/chest-xray-pneumonia`\n\n## Collect and organize the data","642bbcbe":"# Classification with SVM\n\nhttps:\/\/docs.rapids.ai\/api\/cuml\/nightly\/api.html#cuml.svm.SVC\n\nBy using the reduced dataset, we can run fast SVM to classify our data.\n\nWe can even do an hyper-parameter grid-search in less than a minute !","be1fdcb8":"# Incremental PCA\n\nhttps:\/\/docs.rapids.ai\/api\/cuml\/nightly\/api.html#cuml.experimental.decomposition.IncrementalPCA\n\nThe incremental PCA algorithm is a modification of the regular Principal Component Analysis (PCA) to allow batch treatment for larger dataset.\nThis is a very important feature when you can't load the whole dataset in your memory.\nBy using Incremental PCA, we will be able to load a batch of images, run a partial fit on it, then we can unload it from the memory to load the next batch.","67a0cf80":"## Vizualize the data"}}