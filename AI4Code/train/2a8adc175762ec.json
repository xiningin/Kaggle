{"cell_type":{"9a6593e6":"code","1b636c09":"code","60207205":"code","130c35dc":"code","521e2614":"code","4ce4291d":"code","884356c3":"code","cdaf9a61":"code","55aa1ded":"code","598ef9d7":"code","31d88349":"code","cc803513":"code","8a1634b0":"code","5199c97c":"code","0b2ac058":"code","5cd2979f":"code","7ac763e5":"code","99102d86":"code","57fc05b8":"code","050d4ae3":"code","fded921c":"code","eec47ae8":"code","c0806ec2":"code","53e68379":"code","d18dd3fa":"code","e7bb463f":"code","33753989":"code","6d0e1116":"code","0fcb3cef":"code","0cd825e9":"code","c75bd322":"code","32c95b31":"code","c8226a86":"code","62dafec4":"code","6ae73394":"code","cd48b68b":"code","17cc8053":"code","20113728":"code","fbf812c8":"code","a94c6a4e":"code","5b64bab7":"code","d9c951cd":"code","0918622e":"code","eb172137":"code","e7ca4607":"code","74766bdb":"code","55172bad":"code","58d5b9df":"code","c054507d":"code","d946956a":"code","827cccca":"code","7ba6fe8f":"code","d1f1ceee":"code","1351d9df":"code","77d50939":"code","8a77abb5":"code","7b6f29e2":"code","07ab1e5d":"code","2c85ee3a":"code","c096542e":"code","51387c34":"code","1c5a61db":"code","29fea570":"markdown","43b7a149":"markdown","7aa1164b":"markdown","f0fd69e2":"markdown","2afeaeef":"markdown","2974a7fa":"markdown","819a0cba":"markdown","1a3f3709":"markdown","b018d3ca":"markdown","a9601a31":"markdown","9a80db25":"markdown","4ab5563e":"markdown","d3a20240":"markdown","21aabdf2":"markdown","641e0f0e":"markdown"},"source":{"9a6593e6":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n#import keras\n#from keras import backend as K\n\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# os.listdir('..\/input')\nos.listdir('..\/input\/skin-cancer-mnist-ham10000')","1b636c09":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n # val_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)\n\n","60207205":"# View the new directory structure\n# print(os.listdir('base_dir'))\n# print(os.listdir('base_dir\/train_dir'))\n# print(os.listdir('base_dir\/val_dir'))","130c35dc":"df_data = pd.read_csv('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_metadata.csv')\n\ndf_data.head()","521e2614":"# \u663e\u793a\u51fa\u53ea\u6709\u4e00\u5f20\u56fe\u7247\u7684\u75c5\u53d8id\n# this will tell us how many images are associated with each lesion_id\ndf = df_data.groupby('lesion_id').count() # \u540c\u4e00\u4e2a\u75c5\u53d8id\u6709\u51e0\u5f20\u56fe\u7247\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]# \u627e\u51fa\u4e00\u4e2a\u53ea\u6709\u4e00\u5f20\u56fe\u7247\u7684\u75c5\u53d8id,\u6b64\u65f6\u662f\u4ee5lesion_id\u4e3a\u7d22\u5f15\n\ndf.reset_index(inplace=True)# \u8fd8\u539f\u6210\u4ee5\u9ed8\u8ba4\u7684\u6574\u578b\u6570\u5b57\u4f5c\u4e3a\u7d22\u5f15\n\ndf.head()# df\u50a8\u5b58\u7684\u5c31\u662f\u75c5\u53d8id\u53ea\u6709\u4e00\u5f20\u56fe\u7247\u7684\u8868","4ce4291d":"# \u65b0\u5efa\u4e00\u5217\u7528\u6765\u6807\u8bb0\u8fd9\u6bcf\u5f20\u56fe\u7247\u6709\u6ca1\u6709\u8ddf\u5b83\u91cd\u590d\u7684\u75c5\u53d8id\uff0c\u6709\u91cd\u590d\u7684\u6807\u8bb0\u4e3ahas_duplicates\uff0c\u6ca1\u6709\u91cd\u590d\u7684\u6807\u8bb0\u4e3ano_duplicates\n# here we identify lesion_id's that have duplicate images and those that have only\n# one image.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\ndf_data['duplicates'] = df_data['lesion_id']# \u590d\u5236lesion_id\u5217\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7c7b\u53ebduplicates\u5217\n# apply the function to this new column\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)# \n\ndf_data.head()","884356c3":"# \u6709\u91cd\u590d\u75c5\u53d8id\u7684\u67094501\u5f20\uff0c\u6ca1\u6709\u91cd\u590d\u75c5\u53d8id\u7684\u67095514\u5f20\ndf_data['duplicates'].value_counts()","cdaf9a61":"# \u7559\u4e0b\u6ca1\u6709\u91cd\u590d\u75c5\u53d8id\u7684\u56fe\u7247\u53bb\u505a\u9a8c\u8bc1\u96c6\n# now we filter out images that don't have duplicates\ndf = df_data[df_data['duplicates'] == 'no_duplicates']\n\nprint(df.shape)\ndf.head()","55aa1ded":"# df['dx']","598ef9d7":"# \u4ece\u6ca1\u6709\u91cd\u590d\u75c5\u53d8id\u76845514\u5f20\u56fe\u7247\u4e2d\u5206\u51fa17%\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\uff0c\u4e5f\u5c31\u662f\u9a8c\u8bc1\u96c6\u6709938\u5f20\uff0c\u5e76\u628a\u8fd9938\u5f20\u56fe\u7247\u6309\u75c5\u53d8\u7c7b\u578b\u4f5c\u4e3a\u6807\u7b7e\u5206\u5c42\n# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\ny = df['dx']\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n\nprint(df_val.shape)\ndf_val.head()","31d88349":"# \u9a8c\u8bc1\u96c6\u4e2d\u5404\u4e2a\u75c5\u53d8\u7c7b\u578b\u7684\u6570\u91cf\ndf_val['dx'].value_counts()","cc803513":"# \u65b0\u5efa\u4e00\u5217\u6807\u8bb0\u662f\u9a8c\u8bc1\u96c6\u8fd8\u662f\u8bad\u7ec3\u96c6\uff0c\u5e76\u7559\u4e0b\u662f\u8bad\u7ec3\u96c6\u56fe\u7247\u7684\u4fe1\u606f\n# This set will be df_data excluding all rows that are in the val set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# \u7528\u56fe\u7247id\u53bb\u4e4b\u524d\u7684df_val\u8868\u4e2d\u67e5\u662f\u5426\u5c5e\u4e8e\u9a8c\u8bc1\u96c6\uff0c\u662f\u9a8c\u8bc1\u96c6\u6807\u8bb0\u4e3aval\uff0c\u4e0d\u662f\u9a8c\u8bc1\u96c6\u5219\u6807\u8bb0\u4e3atrain\n# create a new colum that is a copy of the image_id column\ndf_data['train_or_val'] = df_data['image_id']\n# apply the function to this new column\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n# \u6700\u7ec8\u8bad\u7ec3\u96c6\u67099007\u5f20\uff0c\u9a8c\u8bc1\u96c6\u6709938\u5f20\n# print(len(df_train))\n# print(len(df_val))","8a1634b0":"# \u8bad\u7ec3\u96c6\u5404\u75c5\u53d8\u7c7b\u578b\u7684\u6570\u91cf\ndf_train['dx'].value_counts()","5199c97c":"# \u9a8c\u8bc1\u96c6\u5404\u75c5\u53d8\u7c7b\u578b\u7684\u6570\u91cf\ndf_val['dx'].value_counts()","0b2ac058":"df_data.head()","5cd2979f":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","7ac763e5":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1')\nfolder_2 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        ","99102d86":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","57fc05b8":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","050d4ae3":"# \u6570\u636e\u589e\u5f3a\uff1a\u9664\u4e86\u6570\u91cf\u6700\u591a\u7684nv\u7c7b\uff0c\u628a\u5176\u4ed6\u7c7b\u90fd\u4f9d\u6b21\u590d\u5236\u5230\u4e00\u4e2a\u4e34\u65f6\u6587\u4ef6\u5939aug_dir\/img_dir\u4e2d\uff0c\n# note that we are not augmenting class 'nv'\nclass_list = ['mel','bkl','bcc','akiec','vasc','df']\n\n# \u9009\u62e9\u4e00\u4e2a\u6570\u91cf\u5c11\u7684\u7c7b\u590d\u5236\u5230\u4e00\u4e2a\u4e34\u65f6\u6587\u4ef6\u5939\u4e2d\uff0c\u6bcf\u6b21\u53d650\u5f20\u56fe\u7247\uff0c\u7ecf\u8fc7\u6570\u636e\u751f\u6210\u5668\u5904\u7406\u6210\u65b0\u7684\u56fe\u7247\u6dfb\u52a0\u5230\u8bad\u7ec3\u96c6\u4e2d\u7684\u5bf9\u5e94\u7c7b\u7684\u6587\u4ef6\u5939\u4e2d\n# \u76f4\u5230\u589e\u52a0\u5230\u5c06\u8fd16000\u5f20\u65f6\uff0c\u5c31\u518d\u53bb\u66fe\u5e7f\u4e0b\u4e00\u4e2a\u7c7b\nfor item in class_list:\n    \n    # \u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u6587\u4ef6\u5939aug_dir\/img_dir\uff0c\u5e76\u628a\u9664\u4e86ve\u7c7b\u4ee5\u5916\u7684\u6240\u6709\u7c7b\u90fd\u7684\u56fe\u7247\u90fd\u590d\u5236\u5230\u8fd9\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    \n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir\/train_dir\/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image \u539f\u59cb\u8def\u5f84\uff1abase_dir\/train_dir\/\u7c7b\u540d\/\u56fe\u7247\u540d\n            src = os.path.join('base_dir\/train_dir\/' + img_class, fname)\n            # destination path to image \u76ee\u7684\u8def\u5f84\uff1aaug_dir\/img_dir\/\u56fe\u7247\u540d\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n            \n    # point to a dir containing the images and not to the images themselves\n    # \u6307\u5411\u6570\u636e\u589e\u5f3a\u56fe\u7247\u548c\u8bad\u7ec3\u96c6\u56fe\u7247\u7684\u6587\u4ef6\u5939\u8def\u5f84\uff0c\u800c\u4e0d\u662f\u56fe\u7247\u672c\u8eab\n    path = aug_dir\n    save_path = 'base_dir\/train_dir\/' + img_class\n\n    # Create a data generator(keras\u5e93)\n    datagen = ImageDataGenerator(\n        rotation_range=180,#\u65cb\u8f6c\u8303\u56f4\n        width_shift_range=0.1,#\u6c34\u5e73\u5e73\u79fb\u8303\u56f4\n        height_shift_range=0.1,#\u5782\u76f4\u5e73\u79fb\u8303\u56f4\n        zoom_range=0.1,#\u7f29\u653e\u8303\u56f4\n        horizontal_flip=True,#\u6c34\u5e73\u7ffb\u8f6c\n        vertical_flip=True,#\u5782\u76f4\u7ffb\u8f6c\n        fill_mode='nearest')#\u586b\u5145\u6a21\u5f0f\n\n    batch_size = 50\n\n    # \u628a\u4e34\u65f6\u8def\u5f84\u7684\u56fe\u7247\u5904\u7406\u540e\u5b58\u5230\u8bad\u7ec3\u96c6\u4e2d\u5bf9\u5e94\u7684\u7c7b\u7684\u8def\u5f84\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    \n    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)\/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","fded921c":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","eec47ae8":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","c0806ec2":"# plots images with labels within jupyter notebook\n# source: https:\/\/github.com\/smileservices\/keras_utils\/blob\/master\/utils.py\n\ndef plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","53e68379":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","d18dd3fa":"train_path = 'base_dir\/train_dir'#\u66fe\u5e7f\u8fc7\u7684\u8bad\u7ec3\u96c6\u8def\u5f84\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)#9077\nnum_val_samples = len(df_val)#938\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)#908\nval_steps = np.ceil(num_val_samples \/ val_batch_size)#94\n\n# print(num_train_samples)","e7bb463f":"# \u5236\u4f5c\u56fe\u7247\u751f\u6210\u5668\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\ndatagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,# \u53d6\u8bad\u7ec3\u96c6\u7684\u56fe\u7247\n                                            target_size=(image_size,image_size),# 224*224\n                                            batch_size=train_batch_size)# 10\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\n# \u6d4b\u8bd5\u96c6\u4e0d\u9700\u8981\u6253\u4e71\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","33753989":"\"\"\"\n\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 MobileNet \u6a21\u578b\n\u8be5\u6a21\u578b\u76ee\u524d\u53ea\u652f\u6301 channels_last \u7684\u7ef4\u5ea6\u987a\u5e8f\uff08\u9ad8\u5ea6\u3001\u5bbd\u5ea6\u3001\u901a\u9053\uff09\u3002\n\u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002\n\"\"\"\n\n# create a copy of a mobilenet model\n# MobileNet \u6a21\u578b\u662f Google \u53d1\u5e03\u7528\u4e8e\u79fb\u52a8\u7aef\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\n# \u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\uff0cMobileNet \u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e4b\u95f4\u8fdb\u884c\u4e86\u5747\u8861\uff0c\n# \u53ef\u5e7f\u6cdb\u5e94\u7528\u4e0e\u5bf9\u8c61\u68c0\u6d4b\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u9762\u90e8\u8bc6\u522b\u4ee5\u53ca\u5927\u89c4\u6a21\u5730\u7406\u5b9a\u4f4d\u7b49\u65b9\u9762\n# mobile = tensorflow.keras.applications.mobilenet.MobileNet()\n# mobile = tensorflow.keras.applications.mobilenet.MobileNet(include_top=False,#\u662f\u5426\u4fdd\u7559\u9876\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc\n#                                                            input_shape=(224,224,3),#\u6307\u660e\u8f93\u5165\u56fe\u7247\u7684shape\uff0c\u56fe\u7247\u7684\u5bbd\u9ad8\u5fc5\u987b\u5927\u4e8e197\uff0c\u5982(200,200,3)\uff0c\u4ec5\u5f53include_top=False\u6709\u6548\uff0c\u5e94\u4e3a\u957f\u4e3a3\u7684tuple\n#                                                            alpha=1.0, #\u63a7\u5236\u7f51\u7edc\u7684\u5bbd\u5ea6\uff1aalpha<1\u6216>1\uff0c\u540c\u6bd4\u51cf\u5c11\u6216\u589e\u5927\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\uff1balpha=1\uff0c\u4f7f\u7528\u9ed8\u8ba4\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\n#                                                            depth_multiplier=1, #depthwise\u5377\u79ef\u7684\u6df1\u5ea6\u4e58\u5b50\uff0c\u4e5f\u79f0\u4e3a\uff08\u5206\u8fa8\u7387\u4e58\u5b50\uff09\n#                                                            dropout=1e-3, #dropout\u6bd4\u4f8b\n#                                                            weights='imagenet', #\u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd.'imagenet\u2019\u4ee3\u8868\u52a0\u8f7d,None\u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\n#                                                            input_tensor=None, #\u53ef\u586b\u5165Keras tensor\u4f5c\u4e3a\u6a21\u578b\u7684\u56fe\u50cf\u8f93\u51fatensor\n#                                                            pooling='avg', #\u5f53include_top=False\u65f6\uff0c\u6307\u5b9a\u6c60\u5316\u65b9\u5f0f\u3002None\u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u6700\u540e\u4e00\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4e3a4D\u5f20\u91cf\u3002\u2018avg\u2019\u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff0c\u2018max\u2019\u4ee3\u8868\u5168\u5c40\u6700\u5927\u503c\u6c60\u5316\u3002\n#                                                            )#classes=7\n\n\n# keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n# keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n# keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n# keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n# keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n","6d0e1116":"mobile = tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False,\n                                                            input_shape=(224,224,3), \n                                                            weights='imagenet', \n                                                            pooling='avg')","0fcb3cef":"mobile.summary()","0cd825e9":"# How many layers does MobileNet have?\nlen(mobile.layers)","c75bd322":"# \u7528\u4e0a\u8ff0\u6a21\u578b\u76f4\u63a5\u8f93\u51fa\nx = mobile.output\n# x = Dropout(0.25)(x)\n# x = Dense(1024, activation='relu')(x)\npredictions = Dense(7, activation='softmax')(x)\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","32c95b31":"model.summary()","c8226a86":"len(model.layers)","62dafec4":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-206]:\n    layer.trainable = False","6ae73394":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","cd48b68b":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n\n","17cc8053":"# Get the labels that are associated with each index\n# print(valid_batches.class_indices)","20113728":"# Add weights to try to make the model more sensitive to melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 1.0, # mel # Try to make the model more sensitive to Melanoma.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","fbf812c8":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)\n","a94c6a4e":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","5b64bab7":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","d9c951cd":"# Here the best epoch will be used.\n# \u6700\u597d\u7684epoch\u5c06\u88ab\u7528\uff0c\u7528\u8be5\u7528\u6700\u7ec8\u7684\u5427\uff1f\uff1f\uff1f\uff1f\n\nmodel.load_weights('model.h5')\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","0918622e":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\nplt.title('Training and validation top3 accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n\nplt.show()","eb172137":"# Get the labels of the test images.\n\ntest_labels = test_batches.classes","e7ca4607":"# We need these to plot the confusion matrix.\nprint(test_labels.shape)\n","74766bdb":"# Print the label associated with each class\ntest_batches.class_indices","55172bad":"# make a prediction\npredictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)","58d5b9df":"predictions.shape#\u6bcf\u5f20\u56fe\u7247\u662f\u6bcf\u4e2a\u7c7b\u7684\u6982\u7387","c054507d":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=True,#False\n                          title='Confusion matrix for InceptionResNetV2',\n                          cmap=plt.cm.viridis):#Blues magma\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"white\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n\n","d946956a":"test_labels.shape","827cccca":"# argmax returns the index of the max value in a row\u8fd4\u56de\u4e00\u884c\u4e2d\u6700\u5927\u503c\u7684\u7d22\u5f15\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","7ba6fe8f":"test_batches.class_indices","d1f1ceee":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","1351d9df":"predictions.shape","77d50939":"# ROC\u66f2\u7ebf\u51c6\u5907\n# \u6d4b\u8bd5\u96c6\u7684\u6807\u7b7e\uff08onehot\u7c7b\u578b\uff09test_labels\nlable_onehot = np.zeros([len(df_val),7],dtype=np.int)\nfor i in range(len(test_labels)):\n    for t in range(7):\n        if(test_labels[i]==t):\n            lable_onehot[i][t] = 1\n        else:\n            lable_onehot[i][t] = 0\n# print(lable_onehot)\n\n# \u6a21\u578b\u8f93\u51fa\u7684\u6d4b\u8bd5\u96c6\u6837\u672c\u5bf9\u5404\u7c7b\u7684\u6982\u7387predictions938*7\nscores_val = predictions","8a77abb5":"# \u753bROC\u66f2\u7ebf\n\nimport sklearn\nfrom sklearn import metrics\n\n#\u8f93\u5165\uff1aonehot_val\u662f\u6d4b\u8bd5\u96c6\u7684\u6807\u7b7e\uff0cscores_val\u662f\u6a21\u578b\u9884\u6d4b\u7684\u6d4b\u8bd5\u96c6\u6837\u672c\u7684\u6982\u7387\n#\u8f93\u51fa\uff1aFPR\uff08\u5047\u9633\u6027\u7387\uff09,TPR\uff08\u771f\u9633\u6027\u7387\uff09\u548c\u9608\u503c\nfpr, tpr, thresholds = metrics.roc_curve(lable_onehot.ravel(),scores_val.ravel())\nauc = metrics.auc(fpr, tpr)\nplt.plot(fpr, tpr, c = 'b', lw = 2, alpha = 0.7, label = u'front view, AUC=%.5f' % auc)\nplt.title('InceptionResNetV2')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')","7b6f29e2":"#Compute ROC curve for each classes \u753b\u6bcf\u4e00\u7c7b\u7684ROC\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(7):\n    fpr[i], tpr[i], _ = metrics.roc_curve(lable_onehot[:, i], scores_val[:, i])\n    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n    \n    \n# Compute micro-average ROC curve and ROC area \u8ba1\u7b97\u5b8f\u89c2\u5e73\u5747ROC\nfpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(lable_onehot.ravel(), scores_val.ravel())\nroc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n","07ab1e5d":"roc_auc","2c85ee3a":"# Plot all ROC curves\nlw=2\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (AUC = {0:0.3f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='black', linestyle='--', linewidth=2)\n\ncolors = itertools.cycle(['blue', 'green', 'red','yellow', 'magenta', 'cyan','deeppink'])\nclass_labels = {0:'akiec', 1:'bcc', 2:'bkl', 3:'df', 4:'mel',5:'nv', 6:'vasc'}\n# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(7), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (AUC = {1:0.3f})'\n             ''.format(class_labels[i], roc_auc[i]))\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curves of InceptionResNetV2 for each classes and micro-average')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}","c096542e":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","51387c34":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","1c5a61db":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","29fea570":"### Create a Confusion Matrix","43b7a149":"### Modify MobileNet Model","7aa1164b":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. ","f0fd69e2":"### Transfer the Images into the Folders","2afeaeef":"### Visualize 50 augmented images","2974a7fa":"### Plot the Training Curves","819a0cba":"### Train the Model","1a3f3709":"### Set Up the Generators","b018d3ca":"### Generate the Classification Report","a9601a31":"### Create a train set that excludes images that are in the val set","9a80db25":"### Create a stratified val set","4ab5563e":"### Create a ROC","d3a20240":"### Copy the train images  into aug_dir","21aabdf2":"### Create Train and Val Sets","641e0f0e":"### Evaluate the model using the val set"}}