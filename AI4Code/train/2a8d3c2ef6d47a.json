{"cell_type":{"cf41b9ad":"code","b3fbd282":"code","24996833":"code","9d21db1a":"code","589e3f0d":"code","6ec5a37c":"code","ba70f199":"code","63bffbeb":"code","65300f03":"code","22826779":"code","75998c7c":"code","f34de646":"code","9eb60046":"code","64ef3c44":"code","5309fb5a":"code","7aabb66f":"code","ac0b1d34":"code","e7afe15c":"code","df191448":"code","bf45bd07":"code","46745d47":"code","94c47e7c":"code","ae5582dc":"code","fad85c79":"code","3663f613":"code","a8af4612":"code","ea0d5250":"code","8e1aa268":"code","80bdb991":"code","9ca53295":"code","88facf56":"code","bbc146b8":"code","266b0f74":"code","58d7c582":"code","eecf2228":"code","013e4936":"code","5574fea4":"code","6d031066":"code","80631f12":"code","09e217c3":"code","dadfc508":"code","8d2fe928":"code","6fd4e76c":"code","4cc471b8":"code","a8c8853b":"code","ef48e0b2":"code","9330fd8c":"code","41f60e09":"code","53291045":"code","4a10699d":"code","c2442f77":"code","f410b0e0":"code","edba9d78":"code","3af55349":"code","7d7a41f6":"markdown","aa8b0914":"markdown","dadc29f2":"markdown","8a5ae707":"markdown","60c4b60a":"markdown","e6fd8c0b":"markdown","a3502704":"markdown","4ea9b824":"markdown","46f50273":"markdown","e367e411":"markdown","8fcd572e":"markdown","a97dc1f5":"markdown","3638538d":"markdown","4d9e0ba5":"markdown","ef4ffccf":"markdown","6408ed21":"markdown","b1c2978e":"markdown","d247e9fb":"markdown","a17c61f0":"markdown","01cb00a2":"markdown","e0316337":"markdown","44eb95af":"markdown","4b0b66aa":"markdown","2b6056f9":"markdown"},"source":{"cf41b9ad":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport gc # \u0441\u0431\u043e\u0440\u0449\u0438\u043a \u043c\u0443\u0441\u043e\u0440\u0430\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport seaborn as sns\n%matplotlib inline","b3fbd282":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24996833":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntrain.head()","9d21db1a":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntrain.head()","589e3f0d":"train.shape","6ec5a37c":"test.shape","ba70f199":"train.isna().sum().sum()\n","63bffbeb":"test.isna().sum().sum()","65300f03":"train['target'].value_counts()","22826779":"for i in train.dtypes:\n      print(i)","75998c7c":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","f34de646":"train = reduce_mem_usage(train)","9eb60046":"test = reduce_mem_usage(test)","64ef3c44":"def plot_feature_importances(importances, X):\n    \n    indices = np.argsort(importances)[::-1]\n\n    plt.figure(figsize = (20, 6))\n    plt.title(\"Feature importances\", fontsize=16)\n    plt.bar(range(X.shape[1]), importances[indices] \/ importances.sum(),\n           color=\"darkblue\", align=\"center\")\n    plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 90, fontsize=14)\n    plt.xlim([-1, X.shape[1]])\n\n    plt.tight_layout()\n    # plt.savefig('fe.jpg')\n    plt.show()","5309fb5a":"train.info()","7aabb66f":"X.hist(figsize=(16,16), bins=100);","ac0b1d34":"plt.figure(figsize = (15,10))\n\nsns.set(font_scale=1.4)\n\ncorr_matrix = train.corr()\n#print(X.corr())\ncorr_matrix = np.round(corr_matrix, 2)\ncorr_matrix[np.abs(corr_matrix) < 0.1] = 0  # \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0435\u0441\u043b\u0438 \u0443\u0431\u0440\u0430\u0442\u044c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438\n\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, cmap='coolwarm')\n\nplt.title('Correlation matrix')\nplt.show()","e7afe15c":"corr_with_target = train.corr().iloc[:-1, -1].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 8))\n\nsns.barplot(x=corr_with_target.values, y=corr_with_target.index)\n\nplt.title('Correlation with target variable')\nplt.show()","df191448":"drop_features = ['f9', 'f16', 'f19', 'f20', 'f23', 'f24', 'f30', 'f31', 'f32', 'f33', 'f42', 'f46', 'f48', 'f51', 'f56', 'f58', 'f59',\n                'f61', 'f62', 'f64', 'f69', 'f72', 'f75', 'f76', 'f78', 'f88', 'f89', 'f90', 'f92', 'f93', 'f94', 'f98', 'f99']","bf45bd07":"train_dropped = train.drop(drop_features, axis=1)","46745d47":"train_dropped.head","94c47e7c":"test_dropped = test.drop(drop_features, axis=1)","ae5582dc":"# Get train data without the target and ids\nX = train_dropped.iloc[:, 1:-1].copy()\n# Get the target\ny = train_dropped.target.copy()\n\n# Create test X, drop ids.\ntest_X = test_dropped.iloc[:, 1:].copy()","fad85c79":"# \u0412\u044b\u0434\u0435\u043b\u0438\u043c \u0432\u0435\u043a\u043e\u0442\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0432\u0435\u043a\u0442\u043e\u0440 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\nX = train.drop(['target', 'id'], axis=1)\ny = train['target']\ntest = test.drop(['id'], axis=1)\n\nX","3663f613":"y","a8af4612":"#\u0421\u0422\u0410\u041d\u0414\u0410\u0420\u0422\u0418\u0417\u0410\u0426\u0418\u042f\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = StandardScaler()\n# Fit on the training data\nscaler.fit(X_important)\n# Transform both the training and testing data\nX_standar = scaler.transform(X_important)\nX_standar_f = scaler.transform(X_important_f)\nprint(X_standar)","ea0d5250":"#\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create the scaler object with a range of 0-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n# Fit on the training data\nscaler.fit(X_standar)\n# Transform both the training and testing data\nX_norm = scaler.transform(X_standar)\nX_norm_f = scaler.transform(X_standar_f)\nprint(X_norm)","8e1aa268":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","80bdb991":"from sklearn.svm import SVC\nSVC_model = SVC()\nSVC_model.fit(X_train, y_train)\nSCV_predict = SVC_model.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))","9ca53295":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nclf = RandomForestClassifier(n_estimators=1000)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))","88facf56":"y_pred = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))","bbc146b8":"clf_f = clf.RandomForestClassifier(n_estimators=1000)\nclf_f.fit(X, y) #X_norm\ny_pred_test = clf_f.predict(X_standar_f) # X_norm_f","266b0f74":"#%%time\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\n\nxgb1 = xgb.XGBClassifier(max_depth = 3, min_child_weight = 0.05, learning_rate = 0.607)\nxgb1.fit(X_train, y_train)\ny_pred_test = xgb1.predict(X_test)\nprint(accuracy_score(y_test, y_pred_test))","58d7c582":"xgb_f = xgb.XGBClassifier(max_depth = 3, min_child_weight = 0.05, learning_rate = 0.607)\nxgb_f.fit(X, y) #X_norm\ny_pred_test = xgb_f.predict(test) # X_norm_f","eecf2228":"!pip install catboost","013e4936":"#%%time\nfrom catboost import CatBoostClassifier\ncatboost = CatBoostClassifier(task_type='GPU', n_estimators=200, learning_rate=0.1, #l2_leaf_reg = 2.5, \n                              depth = 3)#500 0.1\ncatboost.fit(X_train, y_train)\ny_pred_test = catboost.predict(X_test)\n#y_test.columns\nprint(accuracy_score(y_test, y_pred_test))","5574fea4":"catboost_f = CatBoostClassifier(task_type='GPU', n_estimators=10000)\ncatboost_f.fit(X, y)\ny_pred_test = catboost_f.predict(test)","6d031066":"X_important.shape","80631f12":"X_important_f.shape","09e217c3":"from lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(#max_depth=2,\n                      max_depth=11,#11 \n                             n_estimators=326,#326\n                             random_state=53,\n                            #objective = 'gamma',#gamma\n                           # min_data_in_leaf = 27)#27)\n)\nmodel.fit(X_train, y_train)\ny_pred_test = model.predict(X_test)\nprint(accuracy_score(y_test, y_pred_test))","dadfc508":"model_f = LGBMClassifier(#max_depth=2,\n                      max_depth=11,#11 \n                             n_estimators=326,#326\n                             random_state=53,\n                            #objective = 'gamma',#gamma\n                            #min_data_in_leaf = 27)#27)\nmodel_f.fit(X_norm, y)\ny_pred_test = model_f.predict(X_norm_f)","8d2fe928":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\ny_pred_test = gbc.predict(X_test)\nprint(accuracy_score(y_test, y_pred_test))\n","6fd4e76c":"plot_feature_importances(importances = catboost.feature_importances_, X=X_important)","4cc471b8":"model = catboost\nX_important = X.copy()\nX_important_f = test.copy()\nk = 0\ncol = []\nfor a in X.columns:\n    col.append(a)\n#print(col)\n#os_data_X_1 = os_data_X\nfor i in model.feature_importances_:\n    if i < 0.5:\n        #print(col[k])\n        print(i)\n        X_important = X_important.drop([col[k]], axis=1)\n        X_important_f = X_important_f.drop([col[k]], axis=1)\n    \n    k += 1","a8c8853b":"from keras import models\nfrom keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV","ef48e0b2":"network = models.Sequential()\nnetwork.add(layers.Dense(units=15, activation=\"relu\", input_shape=(100,)))\nnetwork.add(layers.Dense(units=15, activation=\"selu\"))\n#network.add(layers.Dense(units=10, activation=\"relu\"))\nnetwork.add(layers.Dense(units=4, activation=\"selu\"))\n#network.add(layers.Dense(units=1, activation=\"tanh\"))\nnetwork.add(layers.Dense(units=1, activation=\"sigmoid\"))","9330fd8c":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearly_stopping = EarlyStopping(\n    monitor='accuracy', \n    min_delta=0, \n    patience=100, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='accuracy', \n    factor=0.2,\n    patience=200,\n    mode='min'\n)","41f60e09":"network.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)","53291045":"history = network.fit(\n    X_train,\n    y_train,\n    epochs=18,\n    verbose=1,\n    batch_size=100,\n    validation_data=(X_test, y_test),\n        callbacks=[\n            early_stopping,\n            reduce_lr,\n        ])","4a10699d":"history = network.fit(\n    X,\n    y,\n    epochs=18,\n    verbose=1,\n    batch_size=100,\n        callbacks=[\n            early_stopping,\n            reduce_lr,\n        ])","c2442f77":"y_pred_test = network.predict(test)","f410b0e0":"indexs = [i for i in range(600000, 1140000)]","edba9d78":"test_pred = pd.DataFrame(indexs, columns=['id'])\ntest_pred['target'] = y_pred_test\ntest_pred","3af55349":"test_pred.to_csv(r'211130_Vasenkov_16.csv', index=None)","7d7a41f6":"### \u041d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c","aa8b0914":"### \u0422\u0438\u043f\u044b \u0434\u0430\u043d\u043d\u044b\u0445","dadc29f2":"## Reading Data","8a5ae707":"#### Catboost","60c4b60a":"#### \u042d\u043a\u0441\u0442\u0440\u0435\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433","e6fd8c0b":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043f\u043e\u0434\u0431\u043e\u0440 \u043c\u043e\u0434\u0435\u043b\u0438","a3502704":"#### \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441","4ea9b824":"### \u0411\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0430 \u043a\u043b\u0430\u0441\u0441\u043e\u0432","46f50273":"## \u0423\u0431\u0438\u0440\u0430\u0435\u043c \u043c\u0443\u0441\u043e\u0440","e367e411":"####\u041c\u0435\u0442\u043e\u0434 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432","8fcd572e":"#### \u0423\u0431\u0438\u0440\u0430\u0435\u043c \u043d\u0435\u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","a97dc1f5":"### \u0414\u0440\u043e\u043f\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","3638538d":"# NOVEMBER COMPETITION ON KAGGLE","4d9e0ba5":"### \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","ef4ffccf":"#### \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f","6408ed21":"### \u041a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438","b1c2978e":"# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f","d247e9fb":"### \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0434\u0430\u043d\u043d\u044b\u0445","a17c61f0":"### \u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445","01cb00a2":"#### \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433","e0316337":"### \u0421\u0442\u0440\u043e\u0438\u043c \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","44eb95af":"### \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445","4b0b66aa":"### \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c","2b6056f9":"### \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043e\u0442\u0432\u0435\u0442"}}