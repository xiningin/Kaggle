{"cell_type":{"e8fe4bbe":"code","8c140075":"code","d70f05d4":"code","e2707686":"code","acbb239d":"code","261de0b2":"code","aa8f3709":"code","4b9f7910":"code","ae3ae307":"code","b6af0fc6":"code","19cc77f9":"code","e53ad2d4":"code","ec313ea8":"code","dafd1f95":"code","084664a9":"code","880d8a91":"code","3aa75f12":"code","3a6eef6f":"code","96287e6e":"code","e3a110a2":"code","a505ec2f":"code","65f749de":"code","f46cd10a":"code","548d1860":"code","2bc917cb":"code","40cb4ee8":"code","3a714004":"code","b0ed69df":"code","615acc15":"markdown","16aa795b":"markdown","5266dd11":"markdown","efcf744b":"markdown","4194be51":"markdown","9f10c85f":"markdown","2f047ec3":"markdown","7178ba55":"markdown","70e660e9":"markdown","8359fd15":"markdown"},"source":{"e8fe4bbe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 104)\npd.set_option('display.max_columns',104)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm as lgb\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score\nimport warnings\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nwarnings.filterwarnings(\"ignore\")","8c140075":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d70f05d4":"df = pd.read_csv('train.csv', index_col=0)\ntest = pd.read_csv('test.csv', index_col=0)","e2707686":"df['Credit_Product']  =df['Credit_Product'].fillna('Maybe')\ndf.isna().sum()","acbb239d":"df['Is_Lead'].value_counts() # 1:3 ratio moderate imbalance","261de0b2":"numerical_features = df.select_dtypes(include=np.number)\ncategorical_features = df.select_dtypes(include=np.object)\nprint(\"numeric_features: \", numerical_features.shape, numerical_features.columns)\nprint(\"categorical_features: \", categorical_features.shape, categorical_features.columns)","aa8f3709":"numerical_features.describe()","4b9f7910":"\n# skewness & kurtosis\npd.DataFrame({\"Skewness\": df.skew(), \"Kurtosis\": df.kurt()})","ae3ae307":"fig, axs = plt.subplots(2,2, figsize=(16, 6), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace = .5, wspace=.001)\naxs = axs.ravel()\n\nfor i,j in zip(numerical_features.columns.to_list(),range(4)):\n    axs[j].hist(numerical_features[i])\n    axs[j].set_title(i+': '+str(np.round(numerical_features[i].skew(),2)))","b6af0fc6":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14,20))\nAX = [ax1, ax2, ax3, ax4]\n\nfor i,j in zip(numerical_features.columns.to_list(),AX):\n    sns.boxplot(x = 'Is_Lead', y = i, data =numerical_features,ax=j)","19cc77f9":"# Correlation matrix\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(numerical_features.corr())","e53ad2d4":"vif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(numerical_features.values, i) for i in range(numerical_features.shape[1])]\nvif[\"features\"] = numerical_features.columns\nvif","ec313ea8":"for i in categorical_features.columns.to_list():\n    print(\"Total unique values for\",i,len(categorical_features[i].unique()))\n    print(\"Value Counts for\",i,'\\n',categorical_features[i].value_counts(),'\\n')","dafd1f95":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nAX = [ax1, ax2, ax3, ax4, ax5, ax6]\n\nfor i,j in zip(categorical_features.columns.to_list(),AX):\n    temp = pd.crosstab(df[i],df['Is_Lead'])\n    temp.plot(kind='bar',stacked=True,color=['red','green'],grid=False,ax=j)","084664a9":"for i in categorical_features.columns.to_list():\n    print(pd.crosstab(df[i],df['Is_Lead']))","880d8a91":"df = pd.read_csv('\/kaggle\/input\/analytics-vidhya-jobathon-may2021\/job_may_train.csv',index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/analytics-vidhya-jobathon-may2021\/job_may_test.csv',index_col=0)\ndf['source'] = 'train'\ntest['source'] = 'test'\ndata = pd.concat([df,test],ignore_index=True)\ndata['Credit_Product'].replace(np.nan,'Maybe',inplace=True)","3aa75f12":"sns.distplot(data['Avg_Account_Balance'])\nplt.show()","3a6eef6f":"data['Avg_Account_Balance'] = np.log(data['Avg_Account_Balance'])\nsns.distplot(data['Avg_Account_Balance'])\nplt.show()","96287e6e":"train = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Is_Lead','source'],axis=1,inplace=True)\ntrain.drop('source',axis=1,inplace=True)","e3a110a2":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\nfor i in var_mod:\n    train[i] = le.fit_transform(train[i])\n    test[i] = le.fit_transform(test[i])","a505ec2f":"X = train.drop('Is_Lead',axis=1)\ny = train['Is_Lead']","65f749de":"def simple_model(alg, X,y):\n\n    # splitting data into training and validation set\n    xtrain, xtest, ytrain, ytest = train_test_split(X,y, random_state=42, test_size=0.2)\n    model = alg\n    model.fit(xtrain, ytrain) # training the model\n\n    print(\"roc_score:\",roc_auc_score(ytest, model.predict(xtest))) # calculating f1 score\n    print(\"Accuracy on train data:\",model.score(xtrain,ytrain))\n    print(\"Accuracy on test data:\",model.score(xtest,ytest))","f46cd10a":"algs = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),AdaBoostClassifier(), XGBClassifier(),lgb.LGBMClassifier()]\nalgs_lst = ['LR','DTC','RFC','ABC', 'XGB', 'LGB']\nfor alg,l in zip(algs,algs_lst):\n    print(l)\n    simple_model(alg, X, y)","548d1860":"# using a strtified fold function\n\n\ndef Strat_Kcross_validation(X, y, model, params, folds=5):\n    stkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_index, test_index) in enumerate(stkf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict_proba(x_test)[:, 1]\n        roc_score = roc_auc_score(y_test, pred)\n        print(f\"roc_auc_score: {roc_score}\")\n        print(\"-\"*50)\n    \n    return alg\n","2bc917cb":"lgb_params= {'learning_rate': 0.07, \n             'n_estimators': 20000, \n             'max_bin': 86,\n             'num_leaves': 10, \n             'max_depth': 27, \n             'reg_alpha': 8.5, \n             'reg_lambda': 6.4, \n             \n             'scale_pos_weight':1.3\n            }\n\nlgb_model = Strat_Kcross_validation(X, y, LGBMClassifier, lgb_params)","40cb4ee8":"pred_test_lgb = lgb_model.predict_proba(test)[:,1]\nsample_submission = pd.read_csv(r'submission.csv')\nsample_submission['Is_Lead'] = pred_test_lgb\nsample_submission.to_csv(f'pred_test_final.csv',index=False)","3a714004":"lgb_params= {'learning_rate': 0.07, \n             'n_estimators': 50000, \n             'max_bin': 86,\n             'num_leaves': 10, \n             'max_depth': 27, \n             'reg_alpha': 8.5, \n             'reg_lambda': 6.4, \n             \n             'scale_pos_weight':1.35\n            }\n\nlgb_model = Strat_Kcross_validation(X, y, LGBMClassifier, lgb_params)","b0ed69df":"pred_test_lgb = lgb_model.predict_proba(test)[:,1]\nsample_submission = pd.read_csv(r'\/kaggle\/input\/analytics-vidhya-jobathon-may2021\/job_may_submission.csv')\nsample_submission['Is_Lead'] = pred_test_lgb\nsample_submission.to_csv(f'pred_test_fin.csv',index=False)","615acc15":"# Numercial Data Anlysis","16aa795b":"If skewness is less than \u22121 or greater than +1, the distribution is highly skewed. If skewness is between \u22121 and \u2212\u00bd or between +\u00bd and +1, the distribution is moderately skewed. If skewness is between \u2212\u00bd and +\u00bd, the distribution of Avg account balance is highly skewed hence we need to do rectify this","5266dd11":"# Feature Engineering ","efcf744b":"\nIf age and vintage is high. credit card lead is high","4194be51":"# Categorical Analysis","9f10c85f":"Choosing LGB since getting similar roc score and higher test accuracy and it is faster then xgboost","2f047ec3":"Data is +vely skewed hence doing log multiplication","7178ba55":"ageand vintage is slightly correlated","70e660e9":"# Model Selection","8359fd15":"If we dont have credit product info then there is higher chance of crdit card lead and some location have higher chances of lead"}}