{"cell_type":{"be975dd3":"code","3b7146fd":"code","d3d5053d":"code","9744f0df":"code","3ed63c29":"code","100259d1":"code","6b8baa08":"code","ee3e18cc":"code","50deb741":"code","2568e121":"code","8d20a918":"code","2849a5ff":"code","400593f1":"code","e89be271":"code","625e70c2":"code","e34713bf":"code","3db37ff5":"code","f8c79ce4":"code","1d501d9a":"code","2231489c":"markdown","007315fe":"markdown","3d1eef31":"markdown","bf965348":"markdown","0606a277":"markdown","b5015bd2":"markdown","d7708b38":"markdown","0a0efaa9":"markdown","b65e1b1e":"markdown"},"source":{"be975dd3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nprint('-------------------------')\nprint('all files:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint('-------------------------')","3b7146fd":"input_dim = 10\nnn_depth = 4\nnn_width = 4\noutput_dim = 1\n\nleaky_relu_alpha = 0.3\ninputs = keras.Input(shape=(None, input_dim), name='input')\n\nx = layers.Dense(nn_width, name='FC_1')(inputs)\nx = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_1')(x)\n\nfor k in range(nn_depth-1):\n    x = layers.Dense(nn_width, name='FC_%d' %(k+2))(x)\n    x = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_%d' %(k+2))(x)\n\noutputs = layers.Dense(output_dim, name='output')(x)\n\nmodel_name = 'FCN_%dx%d_GT_model' %(nn_depth, nn_width)\nGT_model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\nGT_model.summary()\n\nGT_model.compile(optimizer=\"adam\", loss=\"mse\")","d3d5053d":"class UniformDataGenerator(keras.utils.Sequence):\n    def __init__(self, teacher_model, input_dim=10, num_batches_per_epoch=1, batch_size=64):\n        self.teacher_model = teacher_model\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.num_batches_per_epoch = num_batches_per_epoch\n\n    def __len__(self):\n        return self.num_batches_per_epoch\n\n    def __getitem__(self, idx):\n        batch_x = -1.0 + 2.0 * np.random.rand(self.batch_size, self.input_dim)\n        batch_y = self.teacher_model.predict(batch_x)\n        return batch_x, batch_y\n    \nnum_batches_per_epoch = 1\nbatch_size = 32\n\ntrain_datagen = UniformDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=num_batches_per_epoch, batch_size=batch_size)\nvalid_datagen = UniformDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=num_batches_per_epoch, batch_size=batch_size)","9744f0df":"test_generator = False\n\nif test_generator:\n    batch_1 = train_datagen[0]\n    batch_2 = train_datagen[42]\n\n    print('batch 1 shapes: (X.shape = %s, y.shape = %s)' %(batch_1[0].shape, batch_1[1].shape))\n    #print('batch 1 X: \\n%s' %(batch_1[0]))\n    #print('batch 1 y: \\n%s' %(batch_1[1]))\n\n    print('X batch 1 (mean, std) = \\n (%s, \\n  %s)' %(batch_1[0].mean(axis=0), batch_1[0].std(axis=0)))\n    print('y batch 1 (mean, std) = \\n (%s, %s)' %(batch_1[1].mean(axis=0), batch_1[1].std(axis=0)))\n    \n    print('batch 2 shapes: (X.shape = %s, y.shape = %s)' %(batch_2[0].shape, batch_2[1].shape))\n    #print('batch 2 X: \\n%s' %(batch_2[0]))\n    #print('batch 2 y: \\n%s' %(batch_2[1]))\n    \n    print('X batch 2 (mean, std) = \\n (%s, \\n  %s)' %(batch_2[0].mean(axis=0), batch_2[0].std(axis=0)))\n    print('y batch 2 (mean, std) = \\n (%s, %s)' %(batch_2[1].mean(axis=0), batch_2[1].std(axis=0)))\n","3ed63c29":"student_nn_depth = 1\nstudent_nn_width = 4\n\nleaky_relu_alpha = 0.3\ninputs = keras.Input(shape=(None, input_dim), name='input')\n\nx = layers.Dense(student_nn_width, name='FC_1')(inputs)\nx = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_1')(x)\n\nfor k in range(student_nn_depth-1):\n    x = layers.Dense(student_nn_width, name='FC_%d' %(k+2))(x)\n    x = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_%d' %(k+2))(x)\n\noutputs = layers.Dense(nn_width, name='output')(x)\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\nstudent_model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\nstudent_model.summary()\n\nstudent_model.compile(optimizer=\"adam\", loss=\"mse\")","100259d1":"num_train_iterations = 1000\n\nhistory = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)","6b8baa08":"train_loss = history.history['loss']\nvalid_loss = history.history['val_loss']\nbatch_index = np.arange(len(train_loss))\n\nplt.figure(figsize=(20,8))\nplt.plot(batch_index, train_loss, color='k')\nplt.plot(batch_index, valid_loss, color='b')\nplt.xlabel('iteration', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(['train', 'valid'], fontsize=20)","ee3e18cc":"def create_FCN(input_dim, output_dim, nn_depth, nn_width, model_name, leaky_relu_alpha=0.3):\n    \n    # input\n    inputs = keras.Input(shape=(None, input_dim), name='input')\n    \n    # first hiddent layer\n    x = layers.Dense(nn_width, name='FC_1')(inputs)\n    x = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_1')(x)\n    \n    # rest of hidden layers\n    for k in range(nn_depth-1):\n        x = layers.Dense(nn_width, name='FC_%d' %(k+2))(x)\n        x = layers.LeakyReLU(alpha=leaky_relu_alpha, name='LReLU_%d' %(k+2))(x)\n\n    # output\n    outputs = layers.Dense(output_dim, name='output')(x)\n\n    # assemble the model\n    FCN_model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n    \n    return FCN_model","50deb741":"student_nn_depth = 2\nstudent_nn_width = 2\n\nnum_random_inits = 20\nnum_train_iterations = 500\n\ntraining_results = {}\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)","2568e121":"# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\nylim_max = 1.1 * learning_curve_matrix.max()\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);\n","8d20a918":"student_nn_depth = 2\nstudent_nn_width = 3\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","2849a5ff":"student_nn_depth = 3\nstudent_nn_width = 2\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","400593f1":"student_nn_depth = 3\nstudent_nn_width = 3\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);\n","e89be271":"student_nn_depth = 3\nstudent_nn_width = 4\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","625e70c2":"student_nn_depth = 4\nstudent_nn_width = 3\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","e34713bf":"student_nn_depth = 4\nstudent_nn_width = 4\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","3db37ff5":"student_nn_depth = 5\nstudent_nn_width = 5\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","f8c79ce4":"student_nn_depth = 6\nstudent_nn_width = 6\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","1d501d9a":"student_nn_depth = 8\nstudent_nn_width = 8\n\nmodel_name = 'FCN_%dx%d_student_model' %(student_nn_depth, student_nn_width)\ntraining_results[model_name] = []\n\nfor k in range(num_random_inits):\n    curr_model_name = '%s_%d' %(model_name, k+1)\n    print('training \"%s\"' %(curr_model_name))\n    \n    # create model\n    student_model = create_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name)\n    #student_model.summary()\n    student_model.compile(optimizer=\"adam\", loss=\"mse\")\n    \n    # train model\n    history = student_model.fit(train_datagen, epochs=num_train_iterations, validation_data=valid_datagen, verbose=0)\n    \n    # store learning curves\n    curr_learning_curves = {}\n    curr_learning_curves['num_batches'] = num_batches_per_epoch * np.arange(num_train_iterations)\n    curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n    curr_learning_curves['train_loss']  = np.array(history.history['loss'])\n    curr_learning_curves['valid_loss']  = np.array(history.history['val_loss'])\n\n    training_results[model_name].append(curr_learning_curves)\n    \n    \n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(training_results[model_name]), training_results[model_name][0]['valid_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(training_results[model_name]):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_loss']\n    \nnum_batches_vec = training_results[model_name][0]['num_batches']\nnum_samples_vec = training_results[model_name][0]['num_samples']\n\n# show all learning curves\nplt.figure(figsize=(15,25))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\nplt.suptitle(model_name, fontsize=24)\n\nplt.subplot(4,1,1)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\nplt.ylim(0,ylim_max)\n\nplt.subplot(4,1,2)\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,3)\nplt.plot(num_batches_vec[:100], learning_curve_matrix[:,:100].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[:100], learning_curve_matrix.mean(axis=0)[:100], color='k', label='average')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.max(axis=0)[:100], color='r', label='worst')\nplt.plot(num_batches_vec[:100], learning_curve_matrix.min(axis=0)[:100], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)\n\nplt.subplot(4,1,4)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix[:,-100:].T, color='b', alpha=0.6)\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.mean(axis=0)[-100:], color='k', label='average')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.max(axis=0)[-100:], color='r', label='worst')\nplt.plot(num_batches_vec[-100:], learning_curve_matrix.min(axis=0)[-100:], color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20);","2231489c":"# Train many large student networks","007315fe":"# fit the student network and plot learning curve","3d1eef31":"# Train many small student networks","bf965348":"# create a generator that will create data based on teacher GT network","0606a277":"# Create a student network","b5015bd2":"# train many \"correct size\" student networks","d7708b38":"# Show all learning curves overlaid ","0a0efaa9":"# Create a GT Teacher network","b65e1b1e":"# Test the generator"}}