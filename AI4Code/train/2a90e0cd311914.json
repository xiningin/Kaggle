{"cell_type":{"76258ed7":"code","6b5b5517":"code","def6b9f7":"code","b84af1d2":"code","2640640c":"code","630d9ea4":"code","8c488b50":"code","74f236f3":"code","0e00018f":"code","81ac9236":"code","a0d9e37e":"code","ae200027":"code","c6bf4f97":"code","3910abd6":"code","ce435757":"code","469795cf":"code","4d561c8c":"code","ea684489":"code","ddf51d72":"code","f5b2d4e6":"code","4f538382":"code","06bec973":"code","cbcf3d55":"code","4bf8ff0d":"code","8027729e":"code","fb928ff7":"code","4905d8cf":"code","89ed29f0":"code","762b2604":"code","a76e590c":"code","acf58498":"code","2a313d9e":"code","76effd7f":"code","196009e9":"code","54701076":"code","4bcd8749":"code","4cd674c1":"code","a4d4199c":"code","bb57e29a":"code","195a1efe":"code","deba74f6":"code","e45d6bc0":"code","d5fe024e":"code","5fc683cc":"code","34f33325":"code","014fa76a":"code","c2a0b169":"code","2259cb75":"code","daa886da":"code","45a7118b":"markdown","5c297dec":"markdown","c3abcbf3":"markdown","cb945b6a":"markdown","5e9b53a8":"markdown","68b6d050":"markdown","94821c3d":"markdown","d80a85b0":"markdown","26cd669f":"markdown","76ebdf05":"markdown","a922b0a3":"markdown","48ee0a5e":"markdown","fb5ba0c9":"markdown","cfc3237c":"markdown","0fa7efa3":"markdown","286adacc":"markdown","626c51fb":"markdown","58afb140":"markdown","76462067":"markdown","a096d6ed":"markdown","5bf6d6fe":"markdown","4572eaa0":"markdown"},"source":{"76258ed7":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","6b5b5517":"def create_normalize_feature(df, feature, ordered_categories):\n    weight = 1\/(len(ordered_categories)-1)\n    for i in range(len(ordered_categories)):\n        df[feature].replace(ordered_categories[i], round(weight*(i),3), \n                            inplace= True)","def6b9f7":"def merge_categories_by_threshold(df, label, new_col_name, threshold):\n    value_counts_label = df.value_counts(df[label])\n    mask = (value_counts_label \/ value_counts_label.sum() * 100).lt(threshold)\n    new_df = df.assign(new_label = np.where(df[label].isin(\n                          value_counts_label[mask].index), 'Other', df[label]))\n    new_df.rename(columns={'new_label': new_col_name}, inplace=True)\n    new_df.drop(columns = label, inplace=True)\n    return new_df","b84af1d2":"def garage_features(df):\n    df.loc[df['GarageType'].notnull(), 'GarageType'] = 1\n    df.GarageType.fillna(0, inplace=True)\n    df.rename(columns={'GarageType':'Garage'}, inplace=True)\n\ndef lot_frontage_feature(df):\n    df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)\n\ndef veneer_features(df):\n    df.loc[df['MasVnrType'].notnull(), 'MasVnrType'] = 1\n    df.MasVnrType.fillna(0, inplace=True)\n    df['MasVnrType'][df.MasVnrArea == 0] = 0\n    df.rename(columns={'MasVnrType':'MasVnr'}, inplace=True)\n    df.MasVnrArea.fillna(0, inplace=True)\n\ndef fill_nan_categories(df):\n    df.Functional.fillna('Typ', inplace=True)\n    df.SaleType.fillna('WD', inplace=True)\n    df.Exterior1st.fillna('VinylSd', inplace=True)\n    df.Exterior2nd.fillna('VinylSd', inplace=True)\n\ndef electrical_feature(df):\n    df.Electrical.fillna('SBrkr', inplace=True)\n    ordered_categories = ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']\n    create_normalize_feature(df, 'Electrical', ordered_categories)\n\ndef exterior_feature(df):\n    df.Exterior1st.fillna('None', inplace=True)\n    df.Exterior2nd.fillna('None', inplace=True)\n    df['Exterior_combined'] = df.apply(lambda x: x.Exterior1st + '_' + \n                                             x.Exterior2nd, axis=1)\n    df.drop(columns=['Exterior1st', 'Exterior2nd'], inplace=True)","2640640c":"def fillna_continues_features(df, continues_features):\n    for feature in continues_features:\n        df.loc[np.isnan(df[feature]), feature] = 0\n\ndef encoding_ordinal_categories(df, categorial_dict):\n    for feature, ordered_categories in categorial_dict.items():\n        create_normalize_feature(df, feature, ordered_categories)\n        df.loc[np.isnan(df[feature]), feature] = 0\n\ndef special_categories_treatment(df):\n    fill_nan_categories(df)\n    garage_features(df)\n    lot_frontage_feature(df)\n    veneer_features(df)\n    electrical_feature(df)\n    exterior_feature(df)\n\ndef remove_categorical_features(df, columns):\n    df_columns = df.columns\n    for feature in columns:\n        if feature in df_columns:\n            df.drop(columns = [feature], inplace=True)\n\ndef merge_categories(df, merged_features_dict, threshold):\n    for feature, new_feature in merged_features_dict.items():\n        df = merge_categories_by_threshold(df, feature, new_feature, threshold)\n    return df\n\ndef unicode_features(df, unicode_dict):\n    for feature, new_unicode_dict in unicode_dict.items():\n        df[feature] = df[feature].map(new_unicode_dict) \n\ndef create_onehot_features(df, onehot_columns):\n  data = pd.get_dummies(df, prefix=onehot_columns, columns=onehot_columns)\n  return data\n","630d9ea4":"continues_features = ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n                      'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', \n                      'GarageYrBlt']\n\ncategory_ordered = {'PoolQC': ['Na', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'Fence': ['Na', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n                    'FireplaceQu': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'GarageFinish': ['Na', 'Unf', 'RFn', 'Fin'],\n                    'GarageQual': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'GarageCond': ['Na', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtQual': ['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtCond': ['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtExposure': ['NA','No', 'Mn', 'Av', 'Gd'],\n                    'BsmtFinType1': ['NA','Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                    'BsmtFinType2': ['NA','Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n                    'LandContour': ['Low', 'HLS', 'Bnk', 'Lvl'],\n                    'LandSlope': ['Sev', 'Mod', 'Gtl'],\n                    'BldgType': ['Twnhs', 'TwnhsE', 'Duplex', '2fmCon', '1Fam'],\n                    'PavedDrive': ['N', 'P', 'Y'],\n                    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub']}\n            \nmerged_features_dict = {'RoofMatl': 'RoofMatl_CompShg', \n                        'Heating': 'Heating_GasA',\n                        'Functional': 'Functional_typical',\n                        'Foundation': 'Foundation_merged',\n                        'SaleCondition': 'SaleCondition_merged',\n                        'Exterior_combined': 'Exterior_combined_merged',\n                        'Condition2': 'Condition2_norm'}\n                                            \nunicode_features_dict = {'Street': {'Grvl': 1, 'Pave': 0},\n                          'CentralAir': {'Y': 1, 'N': 0},\n                          'RoofMatl_CompShg': {'CompShg': 1, 'Other': 0},\n                          'Heating_GasA': {'GasA': 1, 'Other': 0},\n                          'Functional_typical': {'Typ': 1, 'Other': 0},\n                          'Condition2_norm': {'Norm': 1, 'Other': 0}}\n\nonehot_columns = ['SaleCondition_merged']\n\nremove_features_list = ['LotConfig', 'Condition1', 'MoSold', 'MiscVal',\n                        'SaleCondition_merged_Other']","8c488b50":"def preprocessing(df, continues_features, category_ordered, remove_features_list, \n                  merged_features_dict, unicode_features_dict, onehot_columns):\n    fillna_continues_features(df, continues_features)\n    encoding_ordinal_categories(df, category_ordered)\n    special_categories_treatment(df)\n    df = merge_categories(df, merged_features_dict, 5)\n    unicode_features(df, unicode_features_dict)\n    data = create_onehot_features(df, onehot_columns)\n    remove_categorical_features(data, remove_features_list)\n    data.set_index(['Id'], inplace=True)\n    return data","74f236f3":"def target_encoding(df, features):\n  group_by_avg = {}\n  df_columns = df.columns\n  for feature in features:\n    feature_means = df.groupby(feature)['SalePrice'].mean()\n    group_by_avg[feature] = feature_means\n    df[feature + \"_by_avg\"] = df[feature].map(feature_means)\n    df.drop(columns = [feature], inplace=True)\n  return group_by_avg","0e00018f":"test = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')","81ac9236":"#running on kaggle:\ntest = pd.read_csv('..\/input\/house-price-feature-eng\/test.csv')\ntrain = pd.read_csv('..\/input\/house-price-feature-eng\/train.csv')","a0d9e37e":"processed_train = preprocessing(train, continues_features, category_ordered, \n                                remove_features_list, merged_features_dict, \n                                unicode_features_dict, onehot_columns)\n\nfeatures = ['Neighborhood', 'HouseStyle', 'RoofStyle', 'SaleType', \n            'Foundation_merged', 'Exterior_combined_merged']\n                    \ntarget_encoding(processed_train, features)\n\ny = processed_train['SalePrice']\ny = np.log1p(y)\nX = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']","ae200027":"#example:\nprocessed_train['RoofStyle_by_avg'].value_counts()","c6bf4f97":"#basic model score\nclf = LinearRegression()\nscores = cross_val_score(clf, X, y, cv=5)\nprint(scores)\nprint(np.mean(scores))","3910abd6":"neighborhoods_dict = {'Blmngtn': 'Bloomington Heights, Ames, IA',\n                      'Blueste': 'Bluestem, Ames, IA',\n                      'BrDale': 'Briardale, Ames, IA',\n                      'BrkSide': 'Brookside, Ames, IA',\n                      'ClearCr': 'Clear Creek, Ames, IA',\n                      'CollgCr': 'College Creek, Ames, IA',\n                      'Crawfor': 'Crawford, Ames, IA',\n                      'Edwards': 'Edwards, Ames, IA',\n                      'Gilbert': 'Gilbert, IA',\n                      'IDOTRR': 'Iowa Department of Transportation, Ames, IA',\n                      'MeadowV': 'Meadow Place, Ames, IA',\n                      'Mitchel': 'Mitchell, Ames, IA',\n                      'NAmes': 'North Ames, Ames, IA',\n                      'NoRidge': 'Northridge, Ames, IA',\n                      'NPkVill': 'parkview, Ames, IA',\n                      'NridgHt': 'Northridge Heights, Ames, IA',\n                      'NWAmes': 'Northwest Ames, Ames, IA',\n                      'OldTown': 'Old Town, Ames, IA',\n                      'SWISU': 'Iowa State University, Ames, IA',\n                      'Sawyer': 'Garfield Cir, Ames, IA',\n                      'SawyerW': 'Illinois Ave, Ames, IA',\n                      'Somerst': 'Somerset, Ames, IA',\n                      'StoneBr': 'Stone Brooke, Ames, IA',\n                      'Timber': 'Timberland, Ames, IA',\n                      'Veenker': 'Veenker, Ames, IA'}","ce435757":"from geopy.geocoders import Nominatim\nfrom geopy.distance import geodesic\n\ndef find_geolocation(neighborhoods_dict):\n  location = {}\n  geolocator = Nominatim(user_agent='my_request')\n  for key in neighborhoods_dict:\n      loc = geolocator.geocode(neighborhoods_dict[key])\n      location[key] = (loc.latitude, loc.longitude)\n  return location\n\ndef find_distance_from_university():\n  neighborhood_locations = find_geolocation(neighborhoods_dict)\n  university = neighborhood_locations['SWISU']\n  distance_from_uni = {}\n  for key in neighborhood_locations:\n      distance_from_uni[key] = geodesic(neighborhood_locations[key], university).km\n  return distance_from_uni","469795cf":"#running on kaggle:\ntest = pd.read_csv('..\/input\/house-price-feature-eng\/test.csv')\ntrain = pd.read_csv('..\/input\/house-price-feature-eng\/train.csv')","4d561c8c":"test = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')","ea684489":"distance_from_uni = find_distance_from_university()\ntrain['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n\nprocessed_train = preprocessing(train, continues_features, category_ordered, \n                                remove_features_list, merged_features_dict, \n                                unicode_features_dict, onehot_columns)\n\ntarget_encoding(processed_train, features)\n\ny = processed_train['SalePrice']\ny = np.log1p(y)\nX = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']","ddf51d72":"#basic model score\nclf = LinearRegression()\nscores = cross_val_score(clf, X, y, cv=5)\nprint(scores)\nprint(np.mean(scores))","f5b2d4e6":"X_copy = X.copy()\n\n#quality*size basement finish1\nX_copy.loc[X_copy['BsmtFinSF1'] > 0, \"BsmtFinSF1\"] = (X_copy[\"BsmtFinSF1\"] * \n                                                      X_copy[\"BsmtFinType1\"])\n#quality*size basement finish2 \nX_copy.loc[X_copy['BsmtFinSF2'] > 0, \"BsmtFinSF2\"] = (X_copy[\"BsmtFinSF2\"] *\n                                                      X_copy[\"BsmtFinType2\"])\n#basement quality*size \nX_copy.loc[X_copy['TotalBsmtSF'] > 0, \"TotalBsmtSF\"] = (X_copy[\"TotalBsmtSF\"] *\n                                                        X_copy[\"BsmtCond\"])\n\n#vaneer quality*size \nX_copy.loc[X_copy['MasVnrArea'] > 0, \"MasVnrArea\"] = (X_copy[\"MasVnrArea\"] *\n                                                  X_copy[\"MasVnr\"])\n\n#Exterior quality*size\nX_copy.loc[X_copy['ExterQual'] > 0, \"ExterQual\"] = (X_copy[\"ExterQual\"] *\n                                                   X_copy[\"ExterCond\"])\n\n#Garage quality*size\nX_copy.loc[X_copy['GarageArea'] > 0, \"GarageArea\"] = (X_copy[\"GarageArea\"] *\n                                                   X_copy[\"GarageCond\"])\n\n#Fireplace quality*size\nX_copy.loc[X_copy['Fireplaces'] > 0, \"Fireplaces\"] = (X_copy[\"Fireplaces\"] *\n                                                   X_copy[\"FireplaceQu\"])\n\nX_copy.rename(columns={'TotalBsmtSF':'BsmtSizeQual', 'ExterQual': 'Exterior'}, \n              inplace=True)\n\nX_copy.drop(columns = [\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\",\n                       \"MasVnr\", 'ExterCond', \"FireplaceQu\"], inplace=True)","4f538382":"#percent finish1 basement\nX_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtFinSF1\"] = (X_copy[\"BsmtFinSF1\"]\/\n                                                        X_copy[\"BsmtSizeQual\"])\n#percent finish2 basement \nX_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtFinSF2\"] = (X_copy[\"BsmtFinSF2\"]\/\n                                                        X_copy[\"BsmtSizeQual\"])\n#percent unfinished basement \nX_copy.loc[X_copy['BsmtSizeQual'] > 0, \"BsmtUnfSF\"] = (X_copy[\"BsmtUnfSF\"]\/\n                                                       X_copy[\"BsmtSizeQual\"])\n\n#percent bedrooms above grade from all rooms above grade  \nX_copy.loc[X_copy['TotRmsAbvGrd'] > 0, \"BedroomAbvGr\"] = (X_copy[\"BedroomAbvGr\"]\/\n                                                       X_copy[\"TotRmsAbvGrd\"])\n\n#percent kitchens above grade from all rooms above grade  \nX_copy.loc[X_copy['TotRmsAbvGrd'] > 0, \"KitchenAbvGr\"] = (X_copy[\"KitchenAbvGr\"]\/\n                                                       X_copy[\"TotRmsAbvGrd\"])","06bec973":"X_copy[\"BsmtBaths\"] = (X_copy[\"BsmtHalfBath\"]*0.5) + X_copy[\"BsmtFullBath\"]\nX_copy.drop(columns = [\"BsmtHalfBath\", \"BsmtFullBath\"], inplace=True)","cbcf3d55":"#basic model score\nclf = LinearRegression()\nscores = cross_val_score(clf, X_copy, y, cv=5)\nprint(scores)\nprint(np.mean(scores))","4bf8ff0d":"years_features = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\nmax_year = X_copy[years_features].max()\nX_copy[years_features] = max_year - X_copy[years_features]","8027729e":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nfeatures_to_normalize = ['OverallQual', 'OverallCond', \"LotArea\", \"LotFrontage\",\n                         'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', \n                         'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n                         'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n                         'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n                         'Neighborhood_by_avg', 'HouseStyle_by_avg', \n                         'BsmtSizeQual','RoofStyle_by_avg', 'SaleType_by_avg',\n                         'Foundation_merged_by_avg', 'Exterior_combined_merged_by_avg']\n\nscaler.fit(X_copy[features_to_normalize])\nX_copy[features_to_normalize] = scaler.transform(X_copy[features_to_normalize])","fb928ff7":"#basic model score\nclf = LinearRegression()\nscores = cross_val_score(clf, X_copy, y, cv=5)\nprint(scores)\nprint(np.mean(scores))","4905d8cf":"from geopy.geocoders import Nominatim\nfrom geopy.distance import geodesic\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef find_geolocation(neighborhoods_dict):\n  location = {}\n  geolocator = Nominatim(user_agent='my_request')\n  for key in neighborhoods_dict:\n      loc = geolocator.geocode(neighborhoods_dict[key])\n      location[key] = (loc.latitude, loc.longitude)\n  return location\n\ndef find_distance_from_university(neighborhoods_dict):\n  neighborhood_locations = find_geolocation(neighborhoods_dict)\n  university = neighborhood_locations['SWISU']\n  distance_from_uni = {}\n  for key in neighborhood_locations:\n      distance_from_uni[key] = geodesic(neighborhood_locations[key], \n                                        university).km\n  return distance_from_uni\n\ndef quality_size_mult(df, features):\n  for paired_features in features:\n    size_feature = paired_features[0]\n    quality_feature = paired_features[1]\n    df.loc[df[size_feature] > 0, size_feature] = (df[size_feature] * \n                                                      df[quality_feature])\n    \ndef size_from_total_divition(df, features):\n  for paired_features in features:\n    size_feature = paired_features[0]\n    total_feature = paired_features[1]\n    df.loc[df[total_feature] > 0, size_feature] = (df[size_feature]\/\n                                                        df[total_feature])\ndef rename_columns(df, names_dict):\n  df.rename(columns= names_dict, inplace=True)\n\ndef remove_features(df, columns):\n  df_columns = df.columns\n  for feature in columns:\n      if feature in df_columns:\n          df.drop(columns = [feature], inplace=True)\n\ndef bsmt_bath(df):\n  df[\"BsmtBaths\"] = (df[\"BsmtHalfBath\"]*0.5) + df[\"BsmtFullBath\"]\n\ndef year_feature_by_max(df):\n  years_features = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n  max_year = df[years_features].max()\n  df[years_features] = max_year - df[years_features]\n\ndef target_encoding(df, features):\n  group_by_avg = {}\n  df_columns = df.columns\n  for feature in features:\n    feature_means = df.groupby(feature)['SalePrice'].mean()\n    group_by_avg[feature] = feature_means\n    df[feature + \"_by_avg\"] = df[feature].map(feature_means)\n    df.drop(columns = [feature], inplace=True)\n  return group_by_avg","89ed29f0":"def feature_engineering(df, neighborhoods_dict, qual_size_pairs,\n                        size_total_pairs, remove_of_feature_engineering, \n                        names_dict):\n  quality_size_mult(df, qual_size_pairs)\n  rename_columns(df, names_dict)\n  size_from_total_divition(df, size_total_pairs)\n  bsmt_bath(df)\n  remove_features(df, remove_of_feature_engineering)\n  year_feature_by_max(df)","762b2604":"neighborhoods_dict = {'Blmngtn': 'Bloomington Heights, Ames, IA',\n                      'Blueste': 'Bluestem, Ames, IA',\n                      'BrDale': 'Briardale, Ames, IA',\n                      'BrkSide': 'Brookside, Ames, IA',\n                      'ClearCr': 'Clear Creek, Ames, IA',\n                      'CollgCr': 'College Creek, Ames, IA',\n                      'Crawfor': 'Crawford, Ames, IA',\n                      'Edwards': 'Edwards, Ames, IA',\n                      'Gilbert': 'Gilbert, IA',\n                      'IDOTRR': 'Iowa Department of Transportation, Ames, IA',\n                      'MeadowV': 'Meadow Place, Ames, IA',\n                      'Mitchel': 'Mitchell, Ames, IA',\n                      'NAmes': 'North Ames, Ames, IA',\n                      'NoRidge': 'Northridge, Ames, IA',\n                      'NPkVill': 'parkview, Ames, IA',\n                      'NridgHt': 'Northridge Heights, Ames, IA',\n                      'NWAmes': 'Northwest Ames, Ames, IA',\n                      'OldTown': 'Old Town, Ames, IA',\n                      'SWISU': 'Iowa State University, Ames, IA',\n                      'Sawyer': 'Garfield Cir, Ames, IA',\n                      'SawyerW': 'Illinois Ave, Ames, IA',\n                      'Somerst': 'Somerset, Ames, IA',\n                      'StoneBr': 'Stone Brooke, Ames, IA',\n                      'Timber': 'Timberland, Ames, IA',\n                      'Veenker': 'Veenker, Ames, IA'}\n\nqual_size_pairs = [('BsmtFinSF1', \"BsmtFinType1\"), ('BsmtFinSF2', \"BsmtFinSF2\"), \n                   (\"TotalBsmtSF\",\"BsmtCond\"), (\"MasVnrArea\", \"MasVnr\"), \n                   (\"GarageArea\", \"GarageCond\"), (\"Fireplaces\", \"FireplaceQu\")]\n\nsize_total_pairs = [(\"BsmtFinSF1\", \"BsmtSizeQual\"), \n                    (\"BsmtFinSF2\", \"BsmtSizeQual\"),\n                    (\"BedroomAbvGr\", \"TotRmsAbvGrd\"), \n                    (\"KitchenAbvGr\", \"TotRmsAbvGrd\")]\n                    \nremove_of_feature_engineering = [\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\",\n                  \"MasVnr\", 'ExterCond', \"FireplaceQu\", \"BsmtHalfBath\",\n                  \"BsmtFullBath\"]\n\nnames_dict = {'TotalBsmtSF':'BsmtSizeQual', 'ExterQual': 'Exterior'}\n\nfeatures_to_normalize = ['OverallQual', 'OverallCond', \"LotArea\", \"LotFrontage\",\n                         'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', \n                         'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n                         'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n                         'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n                         'Neighborhood_by_avg', 'HouseStyle_by_avg', \n                         'BsmtSizeQual','RoofStyle_by_avg', 'SaleType_by_avg',\n                         'Foundation_merged_by_avg', 'Exterior_combined_merged_by_avg']\n\ntarget_encoding_features = ['Neighborhood', 'HouseStyle', 'RoofStyle', 'SaleType', \n                            'Foundation_merged', 'Exterior_combined_merged']","a76e590c":"#train fit\ntrain = pd.read_csv('train.csv')\n\ndistance_from_uni = find_distance_from_university(neighborhoods_dict)\ntrain['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n\nprocessed_train = preprocessing(train, continues_features, category_ordered, \n                                remove_features_list, merged_features_dict, \n                                unicode_features_dict, onehot_columns)\n\ntarget_encoding_dict = target_encoding(processed_train, target_encoding_features)\n\nfeature_engineering(processed_train, neighborhoods_dict, \n                                       qual_size_pairs, size_total_pairs, \n                                       remove_of_feature_engineering, names_dict)\n\ny_train = processed_train['SalePrice']\ny_train = np.log1p(y_train)\nX_train = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']\n\nscaler = MinMaxScaler()\nscaler.fit(X_train[features_to_normalize])\nX_train[features_to_normalize] = scaler.transform(X_train[features_to_normalize])","acf58498":"#basic model score\nclf = LinearRegression()\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","2a313d9e":"clf = Ridge(random_state=0)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","76effd7f":"\nclf = RandomForestRegressor(random_state=0)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","196009e9":"\nclf = XGBRegressor(objective ='reg:squarederror', random_state=0)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","54701076":"\nclf = GradientBoostingRegressor(random_state=0)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","4bcd8749":"#train fit\ntrain = pd.read_csv('train.csv')\n\ndistance_from_uni = find_distance_from_university(neighborhoods_dict)\ntrain['dist_from_uni'] = train.Neighborhood.map(distance_from_uni)\n\nprocessed_train = preprocessing(train, continues_features, category_ordered, \n                                remove_features_list, merged_features_dict, \n                                unicode_features_dict, onehot_columns)\n\ntarget_encoding_dict = target_encoding(processed_train, target_encoding_features)\n\nfeature_engineering(processed_train, neighborhoods_dict, \n                                       qual_size_pairs, size_total_pairs, \n                                       remove_of_feature_engineering, names_dict)\n\ny_train = processed_train['SalePrice']\ny_train = np.log1p(y_train)\nX_train = processed_train.copy().loc[:, processed_train.columns != 'SalePrice']\n\nscaler = MinMaxScaler()\nscaler.fit(X_train[features_to_normalize])\nX_train[features_to_normalize] = scaler.transform(X_train[features_to_normalize])","4cd674c1":"#test fit\ntest = pd.read_csv('test.csv')\n\ntest['dist_from_uni'] = test.Neighborhood.map(distance_from_uni)\n\nX_test = preprocessing(test, continues_features, category_ordered, \n                                remove_features_list, merged_features_dict, \n                                unicode_features_dict, onehot_columns)\n\nfor feature, feature_values in target_encoding_dict.items():\n    X_test[feature + \"_by_avg\"] = X_test[feature].map(feature_values)\n    X_test.drop(columns = [feature], inplace=True)\n\nfeature_engineering(X_test, neighborhoods_dict, qual_size_pairs, \n                    size_total_pairs, remove_of_feature_engineering, names_dict)\n\nX_test[features_to_normalize] = scaler.transform(X_test[features_to_normalize])","a4d4199c":"def multiply_new_features(df):\n  df[\"Lot\"] = (df[\"LotArea\"] * df[\"LotFrontage\"])\n  df[\"Pool\"] = (df[\"PoolArea\"] * df[\"PoolQC\"])\n  df[\"Overall\"] = (df[\"OverallQual\"] * df[\"OverallCond\"])\n  df.drop(columns = [\"LotArea\", \"LotFrontage\", \"PoolArea\", \"PoolQC\", \n                        \"OverallQual\", \"OverallCond\"], inplace=True)","bb57e29a":"multiply_new_features(X_train)\n\nscaler2 = MinMaxScaler()\nnormalize_list = [\"Overall\", \"Lot\", \"Pool\"]\nscaler2.fit(X_train[normalize_list])\nX_train[normalize_list] = scaler2.transform(X_train[normalize_list])\n\nmultiply_new_features(X_test)\nX_test[normalize_list] = scaler2.transform(X_test[normalize_list])","195a1efe":"X_train.drop(columns=['Utilities', 'MSSubClass'], inplace=True)\nX_test.drop(columns=[ 'Utilities', 'MSSubClass'], inplace=True)","deba74f6":"params_xgb = {'max_depth': [3,6,10],\n                   'n_estimators': [100, 500, 1000],\n                   'learning_rate': [0.01, 0.05, 0.1],\n                   'colsample_bytree': [0.3, 0.7, 1.0],\n              'subsample': [0.5, 0.7, 1.0]}\n\nparams_gb = {'max_depth': [3,6,10],\n                   'n_estimators': [100, 500, 1000],\n                   'learning_rate': [0.01, 0.05, 0.1],\n                   'max_features': [0.3, 0.7, 1.0],\n             'subsample': [0.5, 0.7, 1.0]}\n","e45d6bc0":"xgb = XGBRegressor(objective ='reg:squarederror', random_state=0)\ngb = GradientBoostingRegressor(random_state=0)\n\nclf_gb = GridSearchCV(estimator=gb, param_grid=params_gb)\nclf_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, scoring='neg_mean_squared_error')\n\nclf_xgb.fit(X_train, y_train)\nclf_gb.fit(X_train, y_train)\n\nprint('Best parameters for XGBoost: ', clf_xgb.best_params_)\nprint('Best parameters for GradientBoosting: ', clf_gb.best_params_)","d5fe024e":"params_xgb = {'max_depth': [2,3,4],\n                   'n_estimators': [400, 500, 600],\n                   'learning_rate': [0.04, 0.05, 0.6],\n                   'colsample_bytree': [0.2, 0.3, 0.4],\n              'subsample': [0.6, 0.7, 0.8]}\n\nparams_gb = {'max_depth': [5,6,7],\n                   'n_estimators': [900, 1000, 1100],\n                   'learning_rate': [0.05, 0.1, 0.15],\n                   'max_features': [0.2, 0.3, 0.4],\n             'subsample': [0.4, 0.5, 0.6]}\n","5fc683cc":"xgb = XGBRegressor(objective ='reg:squarederror', random_state=0)\ngb = GradientBoostingRegressor(random_state=0)\n\nclf_gb = GridSearchCV(estimator=gb, param_grid=params_gb)\nclf_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, scoring='neg_mean_squared_error')\n\nclf_xgb.fit(X_train, y_train)\nclf_gb.fit(X_train, y_train)\n\nprint('Best parameters for XGBoost: ', clf_xgb.best_params_)\nprint('Best parameters for GradientBoosting: ', clf_gb.best_params_)","34f33325":"clf = XGBRegressor(objective ='reg:squarederror', random_state=0, \n                   colsample_bytree= 0.3, learning_rate= 0.05, \n                   max_depth= 4, n_estimators= 500, subsample= 0.6)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","014fa76a":"\nclf = GradientBoostingRegressor(random_state=0, learning_rate= 0.05, \n                                max_depth= 5, max_features= 0.3, n_estimators= 900, subsample= 0.6)\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)\nprint(np.mean(scores))","c2a0b169":"from sklearn.ensemble import StackingRegressor\n\nestimators = [('lr', LinearRegression(n_jobs = -1)),\n              ('rd', Ridge(random_state=0, alpha = 4.84)),\n              ('rf', RandomForestRegressor(random_state=0)),\n              ('gb', GradientBoostingRegressor(random_state=0, learning_rate= 0.05, \n                               max_depth= 5, max_features= 0.3,\n                               n_estimators= 900, subsample= 0.6)),\n             ('xgb', XGBRegressor(objective ='reg:squarederror', random_state=0, \n                   colsample_bytree= 0.3, learning_rate= 0.05, \n                   max_depth= 4, n_estimators= 500, subsample= 0.6))]\n\nmodel = StackingRegressor(estimators=estimators, final_estimator=Ridge(random_state=0, alpha = 4.84), cv=5)\nmodel.fit(X_train, y_train)\nscore = model.score(X_train, y_train)\nprint(score)","2259cb75":"estimators = [('lr', LinearRegression(n_jobs = -1)),\n              ('rd', Ridge(random_state=0, alpha = 4.84)),\n              ('gb', GradientBoostingRegressor(random_state=0))]\n\nmodel = StackingRegressor(estimators=estimators, final_estimator=Ridge(random_state=0, alpha = 4.84), cv=5)\nmodel.fit(X_train, y_train)\nscore = model.score(X_train, y_train)\nprint(score)","daa886da":"y_pred = model.predict(X_test)\ny_pred_final = np.expm1(y_pred)\nsubmission = pd.DataFrame({'Id':test.Id, 'Predicted': y_pred_final})\nsubmission.to_csv('submission.csv', index=False)","45a7118b":"## New feature- distance from university\n\nWe found out thata the neighborhoods locations are around an university, so we decided to define a new feature that will represent the distance from the university because we saw it's influence on house price.","5c297dec":"### Years features\n\nWe want to normalize the data but first we need to deal with the various years categories. We do it by measuring the difference from the maximal year so we get a measure of how recent is this year. Then we can apply MinMax normalization as usual.\n","c3abcbf3":"# Feature engineering A-Z","cb945b6a":"## Changes in preprocess\n\nWe decided to find a better way to preprocess those features:\n- Neighborhood \n- HouseStyle \n- RoofStyle \n- SaleType \n- Foundation\n- Exterior\n\nOn the basic model we used \"onehot encoding\", it's created a massive number of new features because the features above has more then 2 categories. Now we decided to use \"target encoding\" which replaces a categorical value with the average value of the output (ie. target) for that value of the feature.\n * On 2 features: Foundation, Exterior we decided first to merge categories that are under 5% of the data because we saw from our EDA that they are not contibute a lot.\n * On the other features we decided not to merge categories under 5% because we saw that thier correlation with Y is pretty high.\n\nWe saw that it's improve our results so we add it to our preprocess and remove the original feature.\n\n* The only category that we decided to use as onehot feature is \"SaleCondition\" that was better as onehot then as avg values. we merge the catefories that are less then 5% from our data and remove 1 onehot column because to represent all the categories is inufe to use num_categories-1 of the columns.","5e9b53a8":"**Proportion by feature**- we tried to score the half bath in a diffrent way from the baths to united them to one feature that show the number of baths but more evaluation full baths then half baths ","68b6d050":"### Multiply new features \n- We noticed that there are some feature engineering that contribute to those models specific so we decided to add them and normalize:\n \n $Lot = LotArea \\cdot LotFrontage$\n\n $Pool = PoolArea \\cdot PoolQC$\n\n $Overall = OverallQual \\cdot OverallCond$","94821c3d":"# Feature Engineering","d80a85b0":"# Preprocess","26cd669f":"* **Percent of total size**- when we show the percent of parameter from the hole we can see the bigger picture and compare propertly between diffrent records.","76ebdf05":"## Features Interaction\nWe want to emphasize the differences of components in houses, to do that we are using features interactions.","a922b0a3":"### Main functions","48ee0a5e":"The score we got shows us that we are overfitting so we decided to remove:\n\n- Random forest- because we learned that it's tend to overfitting\n- XGBoost- because we suspec that with the gradient boosting they contribute to the overfitting of our model so we decided to stay with one of them\n- On gradient boost we used the default params to avoid from overfitting.","fb5ba0c9":"## Normalization","cfc3237c":"# Final model predictions- stack model\n\nAfter the grid we saw that we get almost the same results with XGBoost and Gradient Boosting. We decided to use our models with stack ensemble method, we did a basic fine-tuning to each model which is not our main model, and used our results from the grid search. \n\nThe models for stacking:","0fa7efa3":"* **Quality * Size**- when we combine the quality with the area we might get some differences between components because the combination creates new \"score\" that combine the 2 parameters and helps to evaluation.","286adacc":"## Fine-tuning the parameters","626c51fb":"# Models evaluation\n\nWe check some models long the way. To find the best hyper parameters we used gridSearchCV. The models we tested:\n- Ridge regression\n- XGboost\n- Random forest\n- Gradient boost","58afb140":"# GridSearch - Note: This takes a long time to run!\n\nIn order to tune the hyperparameters of the models we chose, we decided to go with sklearn's GridSearchCV.\nOf course, there are a lot of hyperparameters but we decided to focus on some of them:\n\n* max_depth\n* n_estimators\n* learning_rate\n* colsample_bytree \/ max_features\n* subsample\n\nWe decided to first do a rough grid search and then fine-tune the search near the best values.","76462067":"So these are the final hyperparameters for our models:\n\n* Best parameters for XGBoost:  {'colsample_bytree': 0.3, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.6}\n\n\n* Best parameters for GradientBoosting:  {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 0.3, 'n_estimators': 900, 'subsample': 0.6}","a096d6ed":"### MinMax normalization","5bf6d6fe":"# Improve models\nWe saw that the gradient boosting and the XGBoost are our best models and close with thier scores, so we decided to try to improve them both.","4572eaa0":"### Drop more features\nWe decided to drop Utilities feature because it's gives no added information about the test data as all the test data has the same utility."}}