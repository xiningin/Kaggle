{"cell_type":{"9bf33e18":"code","71990e16":"code","312488be":"code","7bd0de0f":"code","3752cfcb":"code","46cdb20f":"code","9ee7c9b6":"code","051d9c9a":"code","e4d0f63d":"code","06709506":"code","ca364434":"code","b7ce98e5":"code","6343bb29":"code","6ddb4dd5":"code","40375f23":"code","7fa935a5":"code","dd0b115d":"code","3960caa9":"code","fe9720bf":"code","372d2d05":"code","47cbe8db":"code","35b15886":"code","fb1f7943":"code","3b55e057":"code","bd7ba7e2":"code","b1f2c3e3":"code","d1203ed0":"code","31689722":"code","548d76d1":"code","e67891a5":"code","ddd70a42":"code","9f9ca3ed":"code","b4d65ae3":"code","df8bfcdf":"code","0da442ce":"code","7fb14279":"code","df804d97":"code","621c0b8c":"code","ed47cab4":"code","e45da31c":"code","7ef6131c":"code","8b1f80ec":"code","cc3ad416":"code","51aaef54":"code","c61603ab":"code","77c02051":"code","d7aca706":"code","88ab759e":"code","d6e27f92":"code","022f82ac":"code","8d7aa261":"code","b1d65655":"code","f2cc0939":"code","59c75c46":"code","a28d0834":"code","fac9e180":"code","badf9d42":"markdown","a613e296":"markdown","05c69b89":"markdown","d603f7cd":"markdown","c235d51f":"markdown","e3201c1d":"markdown","ea6db856":"markdown","4f87af63":"markdown","4b6bcb7c":"markdown","6e75c30b":"markdown","1bada0bc":"markdown","59fee6d9":"markdown","c4b26ae0":"markdown","d2ed7ca4":"markdown","cce0f138":"markdown"},"source":{"9bf33e18":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","71990e16":"import matplotlib.pyplot as plt\nimport seaborn as sns","312488be":"data = pd.read_csv(\"..\/input\/ks-projects-201801.csv\")","7bd0de0f":"data.head()","3752cfcb":"from sklearn.preprocessing import FunctionTransformer, LabelEncoder, LabelBinarizer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import TransformerMixin\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, auc\nfrom sklearn.metrics import accuracy_score, roc_curve\nfrom collections import defaultdict","46cdb20f":"class MultiColumnLabelEncoder(TransformerMixin):  \n    def __init__(self):\n        self.d = defaultdict(LabelEncoder)\n\n    def transform(self, X, **transform_params):\n\n        X = X.fillna('NaN')  \n        transformed = X.apply(self._transform_func)\n        return transformed\n\n    def fit(self, X, y=None, **fit_params):\n        X = X.fillna('NaN')  \n        X.apply(self._fit_func)\n        return self\n    \n    \n    def _transform_func(self, x):\n        return self.d[x.name].transform(x)\n    \n    def _fit_func(self, x):\n        return self.d[x.name].fit(x)","9ee7c9b6":"def get_categorical_data(x): \n    return x[['category', 'main_category', 'currency', 'country']]\n\ndef get_name_lenght_feature(x): \n    return x['name'].str.len().fillna(0).to_frame()\n    \ndef get_duration_feature(x): \n    return (pd.to_datetime(x['deadline']) - pd.to_datetime(x['launched'])).dt.days.to_frame()\n\ndef get_deadline_month_feature(x): \n    return pd.to_datetime(x['deadline']).dt.month.to_frame()\n    \ndef get_deadline_weekday_feature(x): \n    return pd.to_datetime(x['deadline']).dt.weekday.to_frame()\n\ndef get_launched_month_feature(x): \n    return pd.to_datetime(x['launched']).dt.month.to_frame()\n\ndef get_launched_weekday_feature(x): \n    return pd.to_datetime(x['launched']).dt.weekday.to_frame()\n\n\npreprocess_base_pipeline = FeatureUnion(\n         transformer_list = \n         [\n            ('name_length', Pipeline([\n                ('selector', FunctionTransformer(get_name_lenght_feature, validate=False))\n            ])),\n            ('duration_feature', Pipeline([\n                ('selector', FunctionTransformer(get_duration_feature, validate=False))\n            ])),  \n            ('deadline_month', Pipeline([\n                ('selector', FunctionTransformer(get_deadline_month_feature, validate=False))\n            ])),\n            ('deadline_weekday', Pipeline([\n                ('selector', FunctionTransformer(get_deadline_weekday_feature, validate=False))\n            ])),  \n            ('launched_month', Pipeline([\n                ('selector', FunctionTransformer(get_launched_month_feature, validate=False))\n            ])),\n            ('launched_weekday', Pipeline([\n                ('selector', FunctionTransformer(get_launched_weekday_feature, validate=False))\n            ]))                    \n        ])","051d9c9a":"preprocess_pipeline = FeatureUnion(\n         transformer_list = \n         [\n            ('cat_features', Pipeline([\n                ('selector', FunctionTransformer(get_categorical_data, validate=False)),\n                ('encoder', MultiColumnLabelEncoder())\n            ])),\n\n            ('name_length', Pipeline([\n                ('preprocess_base_pipeline', preprocess_base_pipeline)\n            ])),\n                   \n        ])","e4d0f63d":"def report_results(model, X, y):\n    pred_proba = model.predict_proba(X)[:, 1]\n    pred = model.predict(X)        \n\n\n    auc = roc_auc_score(y, pred_proba)\n    acc = accuracy_score(y, pred)\n    f1 = f1_score(y, pred)\n    prec = precision_score(y, pred)\n    rec = recall_score(y, pred)\n    result = {'auc': auc, 'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n    return result","06709506":"def get_roc_curve(model, X, y):\n    pred_proba = model.predict_proba(X)[:, 1]\n    fpr, tpr, _ = roc_curve(y, pred_proba)\n\n    return fpr, tpr","ca364434":"X = preprocess_pipeline.fit_transform(data)\ny = (data['state'] == 'successful').astype('int')","b7ce98e5":"sns.heatmap(pd.DataFrame(X).corr(), cmap='Blues')\nplt.title('Feature Correlations')\nplt.show()","6343bb29":"X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, stratify=y, random_state=1)","6ddb4dd5":"model_lr = Pipeline([('preprocess', preprocess_pipeline), \n                     ('estimator', LogisticRegression(solver='liblinear', random_state=0))]) \n\nmodel_lr.fit(X_train, y_train)","40375f23":"model_lr.score(X_test, y_test)","7fa935a5":"y_pred_lr = model_lr.predict(X_test)\ny_pred_proba_lr = model_lr.predict_proba(X_test)[:,1]","dd0b115d":"all_models = {}\nall_models['lr'] = {} \nall_models['lr']['model'] = model_lr\nall_models['lr']['train_preds'] = model_lr.predict_proba(X_train)[:, 1]\nall_models['lr']['result'] = report_results(all_models['lr']['model'], X_test, y_test)\nall_models['lr']['roc_curve'] = get_roc_curve(all_models['lr']['model'], X_test, y_test)","3960caa9":"print(classification_report(y_test, y_pred_lr))","fe9720bf":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_lr)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","372d2d05":"from sklearn.naive_bayes import MultinomialNB\nnp.random.seed(1)\n\nmodel_nb = Pipeline([('preprocess', preprocess_pipeline), \n                     ('estimator', MultinomialNB())]) ","47cbe8db":"model_nb.fit(X_train, y_train)","35b15886":"model_nb.score(X_test, y_test)","fb1f7943":"all_models['nb'] = {} \nall_models['nb']['model'] = model_nb\nall_models['nb']['train_preds'] = model_nb.predict_proba(X_train)[:, 1]\nall_models['nb']['result'] = report_results(all_models['nb']['model'], X_test, y_test)\nall_models['nb']['roc_curve'] = get_roc_curve(all_models['nb']['model'], X_test, y_test)","3b55e057":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = Pipeline([('preprocess', preprocess_pipeline), \n                     ('estimator', RandomForestClassifier(n_estimators=200, random_state=0, n_jobs=-1))])\n\nmodel_rf.fit(X_train, y_train)","bd7ba7e2":"model_rf.score(X_test, y_test)","b1f2c3e3":"all_models['rf'] = {} \nall_models['rf']['model'] = model_rf\nall_models['rf']['train_preds'] = model_rf.predict_proba(X_train)[:, 1]\nall_models['rf']['result'] = report_results(all_models['rf']['model'], X_test, y_test)\nall_models['rf']['roc_curve'] = get_roc_curve(all_models['rf']['model'], X_test, y_test)","d1203ed0":"from sklearn.neighbors import KNeighborsClassifier\nnp.random.seed(1)\n\nmodel_knn = Pipeline([('preprocess', preprocess_pipeline), \n                     ('estimator', KNeighborsClassifier(n_neighbors=3, n_jobs=-1))])\n\nmodel_knn.fit(X_train, y_train)","31689722":"model_knn.score(X_test, y_test)","548d76d1":"all_models['knn'] = {} \nall_models['knn']['model'] = model_knn\nall_models['knn']['train_preds'] = model_knn.predict_proba(X_train)[:, 1]\nall_models['knn']['result'] = report_results(all_models['knn']['model'], X_test, y_test)\nall_models['knn']['roc_curve'] = get_roc_curve(all_models['knn']['model'], X_test, y_test)","e67891a5":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\nen_stopwords = set(stopwords.words(\"english\"))  \n\npreprocess_nlp_pipeline = Pipeline(\n    [('selector', FunctionTransformer(lambda x: x['name'].fillna(''), validate=False)),\n     ('vectorizer', CountVectorizer(stop_words = en_stopwords))\n    ]\n)\n\npreprocess_full_nlp_pipeline = FeatureUnion(\n         transformer_list = [('preprocess_nlp_pipeline', preprocess_nlp_pipeline),\n                             ('preprocess_base_pipeline', preprocess_base_pipeline)])","ddd70a42":"model_nlp = Pipeline([('preprocess', preprocess_nlp_pipeline), \n                     ('estimator', LogisticRegression(random_state=0))])\n","9f9ca3ed":"model_nlp.fit(X_train, y_train)","b4d65ae3":"model_nlp.score(X_test, y_test)","df8bfcdf":"all_models['nlp'] = {} \nall_models['nlp']['model'] = model_nlp\nall_models['nlp']['train_preds'] = model_nlp.predict_proba(X_train)[:, 1]\nall_models['nlp']['result'] = report_results(all_models['nlp']['model'], X_test, y_test)\nall_models['nlp']['roc_curve'] = get_roc_curve(all_models['nlp']['model'], X_test, y_test)","0da442ce":"model_mix = Pipeline([('preprocess', preprocess_full_nlp_pipeline), \n                     ('estimator', LogisticRegression(random_state=0))])","7fb14279":"model_mix.fit(X_train, y_train)","df804d97":"model_mix.score(X_test, y_test)","621c0b8c":"all_models['mix'] = {} \nall_models['mix']['model'] = model_mix\nall_models['mix']['train_preds'] = model_mix.predict_proba(X_train)[:, 1]\nall_models['mix']['result'] = report_results(all_models['mix']['model'], X_test, y_test)\nall_models['mix']['roc_curve'] = get_roc_curve(all_models['mix']['model'], X_test, y_test)","ed47cab4":"all_models_name = all_models.keys()\n\ntmp_list = []\nfor mo in all_models_name:\n    tmp_list.append(all_models[mo]['result'])\nmodels_results = pd.DataFrame(dict(zip(all_models_name, tmp_list))).transpose()","e45da31c":"models_results = models_results.sort_values(['auc'], ascending=False)\nmodels_results","7ef6131c":"from matplotlib import cm\n\ntmp_models = models_results.index\n\ncolors = cm.rainbow(np.linspace(0.0, 1.0, len(tmp_models)))\n\n\nplt.figure(figsize=(14,8))\nlw = 2\n\nfor mo, color in zip(tmp_models, colors):\n    fpr, tpr = all_models[mo]['roc_curve']\n    plt.plot(fpr, tpr, color=color,\n         lw=lw, label='{} (auc = {:.4f})'.format(mo, all_models[mo]['result']['auc']))\n\n\n\nplt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Roc curve')\nplt.legend(loc=\"lower right\")\n\nplt.show()","8b1f80ec":"corr_dict = {}\nfor mo in tmp_models:\n    corr_dict[mo] = all_models[mo]['train_preds']","cc3ad416":"kdata_proba = pd.DataFrame(corr_dict) ","51aaef54":"corr = kdata_proba.corr()\ncorr","c61603ab":"plt.figure(figsize=(14,8))\nsns.heatmap(corr, cmap=\"YlGnBu\")\nplt.show()","77c02051":"np.random.seed(1)\n\nfrom mlxtend.classifier import StackingClassifier\n\nlr_stack = LogisticRegression(random_state=0)\n#lr_stack = RandomForestClassifier()\n\nclf_stack = StackingClassifier(\n    classifiers = [model_nlp, model_lr],\n    use_probas = True, \n    average_probas = False,    \n    meta_classifier = lr_stack, verbose=1)","d7aca706":"clf_stack.fit(X_train, y_train)","88ab759e":"clf_stack.score(X_test, y_test)","d6e27f92":"report_results(clf_stack, X_test, y_test)","022f82ac":"from sklearn.model_selection import GridSearchCV\nparams = {'meta-logisticregression__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=clf_stack, \n                    param_grid=params, \n                    cv=3,\n                    refit=True)\ngrid.fit(X_train, y_train)","8d7aa261":"print('Best parameters: %s' % grid.best_params_)\nprint('Accuracy: %f' % grid.best_score_)","b1d65655":"report_results(grid, X_test, y_test)","f2cc0939":"from sklearn.ensemble import VotingClassifier","59c75c46":"eclf1 = VotingClassifier(estimators=[('lr', model_lr), \n                                     ('rf', model_rf), \n                                     ('nb', model_nb),\n                                     ('knn', model_knn),\n                                     ('nlp', model_nlp)], voting='soft')","a28d0834":"eclf1.fit(X_train, y_train)","fac9e180":"report_results(eclf1, X_test, y_test)","badf9d42":"### Naive Bayes","a613e296":"### k-nearest neighbors","05c69b89":"## Combination of models","d603f7cd":"Let's try a votting classifier:","c235d51f":"### Random Forest","e3201c1d":"Let's see the correlation between models:","ea6db856":"### Bag of words\nIn this case, we'll obtain features only from the name of the capaign.","4f87af63":"Here is the correlation between features:","4b6bcb7c":"In this case we get a good result, but not better than a simple model like model_mix ","6e75c30b":"### Logistic Regression","1bada0bc":"Let's try to optimize the stacked model:","59fee6d9":"## Models","c4b26ae0":"### Bag of words with additional features\n","d2ed7ca4":"## Summary\nWe have used different models to predict when a Kickstarter campaign will be successful. For that, we have build pipelines to preprocess the data and get the features we need per each model. We have studied the correlation between models and calculated their performance. At the end, we have created some stacks of these models to try to get a better solution and a voting clasiffier.","cce0f138":"Comparasion of all the results"}}