{"cell_type":{"54f1af77":"code","ed404d2a":"code","339f8dc8":"code","3181abdb":"code","59f53253":"code","80eec234":"code","11f18963":"code","6ea717b7":"markdown"},"source":{"54f1af77":"from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom matplotlib import pyplot as plt","ed404d2a":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output","339f8dc8":"def train(model, device, train_loader, optimizer, epoch, log_interval=1):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n            \ndef train_exp(model, device, train_loader, optimizer, epoch, log_interval=1):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = torch.exp(F.nll_loss(output, target))\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, prefix=\"\"):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss \/= len(test_loader.dataset)\n\n    print('[ ' + prefix + ' ]\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))\n    return 100. * correct \/ len(test_loader.dataset)","3181abdb":"def drawgraph(losslist1, losslist2):\n    plt.plot(range(len(losslist1)), losslist1, color='red', label='EXP Loss')\n    plt.plot(range(len(losslist2)), losslist2, color='blue', label='Default Loss')\n    \n    plt.title('Accurate of Default Loss and Exp Loss')\n    plt.xlabel('Epcoh')\n    plt.ylabel('Accurate')\n    \n    plt.legend() \n    \n    plt.show()","59f53253":"default_loss = []\nexp_loss = []\n\ndef main():\n    global exp_loss, default_loss\n    \n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_kwargs = {'batch_size': 50}\n    test_kwargs = {'batch_size': 50}\n    \n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    \n    dataset1 = datasets.MNIST('..\/data', train=True, download=True,transform=transform)\n    dataset2 = datasets.MNIST('..\/data', train=False,transform=transform)\n    \n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    Expmodel = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    Expoptimizer = optim.Adadelta(Expmodel.parameters(), lr=1.0)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n    Expscheduler = StepLR(Expoptimizer, step_size=1, gamma=0.7)\n    \n    for epoch in range(1, 30 + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        default_loss.append(test(model, device, test_loader, prefix=\"\uae30\uc874 \ubc29\uc2dd\"))\n        scheduler.step()\n        \n    for epoch in range(1, 30 + 1):\n        train_exp(Expmodel, device, train_loader, Expoptimizer, epoch)\n        exp_loss.append(test(Expmodel, device, test_loader, prefix=\"exp \ubc29\uc2dd\"))\n        Expscheduler.step()\n        ","80eec234":"if __name__ == '__main__':\n    main()","11f18963":"drawgraph(exp_loss, default_loss)","6ea717b7":"# Exp \uaf34\uc758 \uc190\uc2e4 \ud568\uc218\n----------------------------------------\n\uc190\uc2e4 \ud568\uc218\uac00 \uc74c\uc218 \uac12\uc774 \ub098\uc62c \uacbd\uc6b0\uc5d0, \uac00\ub3c5\uc131\uc774 \ub5a8\uc5b4\uc9c0\ub294 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud568.\n\n\ubc11\uc774 \uc790\uc5f0\ub300\uc218(e)\uc778 \uc9c0\uc218\ud568\uc218\uc5d0 \uae30\uc874 \uc190\uc2e4\uac12\uc744 \ub300\uc785\ud55c \uc190\uc2e4 \ud568\uc218\uc640, \uae30\uc874 \uc190\uc2e4 \ud568\uc218\uc758 \uc131\ub2a5 \ube44\uad50."}}