{"cell_type":{"7c1bfd19":"code","f7d5523e":"code","90299ca3":"code","22561d97":"code","7caf821c":"code","8d3e2135":"code","92289391":"code","610f5ca0":"code","e407f14d":"code","eea7ef0f":"code","4bbb505f":"code","34b141a8":"code","a083f575":"code","e7569d5b":"code","6cd22b8a":"code","f9eb73de":"code","bed7ca34":"code","8c96335b":"code","0dab35fa":"code","17ed3a53":"code","82a2cbb9":"markdown","15b91186":"markdown"},"source":{"7c1bfd19":"import sys\nsys.path.append('..\/input\/fasthugs')","f7d5523e":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport warnings\nimport joblib\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_colwidth=300","90299ca3":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string and make string lower\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n    return text","22561d97":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","7caf821c":"def P1_train(dataset='class', cleaned=False, df_muls=None, n_folds=7, frac_factor=1.5):\n    if dataset=='class':\n        df = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\n        cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    elif dataset=='bias':\n        df = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n        cols = ['toxic', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n    elif dataset=='ruddit':\n        df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n    else:\n        assert 1==0 # use aformentioned datasets.\n\n    if df_muls is None:\n        df_muls = [1, 2, 1, 1, 1, 1]\n    df['y'] = 0\n    for col, mul in zip(cols, df_muls):\n        df['y'] = df['y'] + df[col] * mul\n    df['y'] = df['y']\/df['y'].max()\n    df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n    if cleaned:\n        df = clean(df, 'text')\n        tqdm.pandas()\n        df['text'] = df['text'].progress_apply(text_cleaning)\n\n    frac = 0.4\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        tmp_df = pd.concat([df[df.y>0].sample(frac=frac, random_state = 10*(fld+1)) , \n                            df[df.y==0].sample(n=int(len(df[df.y>0])*frac*frac_factor) , random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n        tmp_df.to_csv(f'\/kaggle\/working\/df_fld{fld}.csv', index=False)\n\n    for fld in range(n_folds):\n        print(\"\\nTrain:\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'\/kaggle\/working\/df_fld{fld}.csv')\n        print(df.shape)\n\n        features = FeatureUnion([\n            #('vect1', LengthTransformer()),\n            #('vect2', LengthUpperTransformer()),\n            (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n            #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n        ])\n        pipeline = Pipeline([\n                (\"features\", features),  #(\"clf\", RandomForestRegressor(n_esatimators = 5, min_sample_leaf=3)),\n                (\"clf\", Ridge()),  #(\"clf\",LinearRegression())\n            ])\n        # Train the pipeline\n        pipeline.fit(df['text'].values.astype('U'), df['y'])\n\n        # What are the important features for toxicity\n        if cleaned:\n            joblib.dump(pipeline, f'{dataset}_c_{fld}.pkl')\n        else:\n            joblib.dump(pipeline, f'{dataset}_d_{fld}.pkl')","8d3e2135":"#P1_train(dataset='class', cleaned=True , frac_factor=1.5, n_folds=7)\n#P1_train(dataset='bias' , cleaned=True,  frac_factor=0.3, n_folds=7)\nif False:   # trained pipelines are saved\n    P1_train(dataset='class', cleaned=False, frac_factor=1.5, n_folds=7)\n    P1_train(dataset='class', cleaned=True , frac_factor=1.5, n_folds=7)\n\n    P1_train(dataset='bias' , cleaned=False, frac_factor=0.3, n_folds=7)\n    P1_train(dataset='bias' , cleaned=True,  frac_factor=0.3, n_folds=7)","92289391":"def P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=False):\n\n    \"\"\"\n    #print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n    #pprint(feature_wts[:30])\n    \"\"\"\n    # Validation and Evaluation\n    df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n    if cleaned:\n        tqdm.pandas()\n        df_sub = clean(df_sub, 'text')\n        df_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n        if not test_only:\n            df_val = clean(df_val, 'less_toxic')\n            df_val = clean(df_val, 'more_toxic')\n            df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n            df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\n\n                \n    for fld in range(n_folds):\n        print(\"\\nEval:\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n\n        if cleaned:\n            pipeline = joblib.load(f'..\/input\/jrstc-linear-model\/{dataset}_c_{fld}.pkl')\n        else:\n            pipeline = joblib.load(f'..\/input\/jrstc-linear-comp\/{dataset}_d_{fld}.pkl')\n            \n        if not test_only:\n            print(\"\\npredict validation data \")\n            val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n            val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n        print(\"\\npredict test data \")\n        test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])\n    return val_preds_arr1, val_preds_arr2, test_preds_arr","610f5ca0":"if False:\n    c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7)\n    b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7)\n    def sort_good_weight(L, R, L_, R_):\n        wts_acc = []\n        for i in range(1, 100, 1):\n            alpha = i\/100\n            L_wt = alpha * L + (1-alpha) * L_\n            R_wt = alpha * R + (1-alpha) * R_\n            wts_acc.append( (alpha, 1-alpha, \n                                 np.round((L_wt < R_wt).mean() * 100,2))\n                              )\n        print(sorted(wts_acc, key=lambda x:x[2], reverse=True)[:5])\n","e407f14d":"#print(np.round((L_1<R_1).mean()*100, 2)) # --> 68.37\n#print(np.round((L_3<R_3).mean()*100, 2)) # --> 68.64\n# sort_good_weight(L_1, R_1, L_3, R_3) --> [(0.47, 0.53, 69.21), (0.48, 0.52, 69.2), (0.49, 0.51, 69.19), (0.39, 0.61, 69.16), (0.4, 0.6, 69.16)]\n# 69.18?","eea7ef0f":"def sort_good_weight(L, R, L_, R_):\n    wts_acc = []\n    for i in range(1, 100, 1):\n        alpha = i\/100\n        L_wt = alpha * L + (1-alpha) * L_\n        R_wt = alpha * R + (1-alpha) * R_\n        wts_acc.append( (alpha, 1-alpha, \n                             np.round((L_wt < R_wt).mean() * 100,2))\n                          )\n    print(sorted(wts_acc, key=lambda x:x[2], reverse=True)[:5])","4bbb505f":"#c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=True)\n#b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7, test_only=True)\n#T_1 = c_d_t.mean(axis=1)\n#T_3 = b_d_t.mean(axis=1)\n\ntest_only = True\nif True:\n    c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=test_only)\n    c_c_l, c_c_r, c_c_t = P1_eval(dataset='class', cleaned=True , n_folds=7, test_only=test_only)\n\n    b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7, test_only=test_only)\n    b_c_l, b_c_r, b_c_t = P1_eval(dataset='bias' , cleaned=True , n_folds=7, test_only=test_only)\n\n    L_1 = c_d_l.mean(axis=1)\n    R_1 = c_d_r.mean(axis=1)\n    L_2 = c_c_l.mean(axis=1)\n    R_2 = c_c_r.mean(axis=1)\n\n    L_3 = b_d_l.mean(axis=1)\n    R_3 = b_d_r.mean(axis=1)\n    L_4 = b_c_l.mean(axis=1)\n    R_4 = b_c_r.mean(axis=1)\n\n    T_1 = c_d_t.mean(axis=1)\n    T_2 = c_c_t.mean(axis=1)\n    T_3 = b_d_t.mean(axis=1)\n    T_4 = b_c_t.mean(axis=1)\n    \n    x = 0.45\n    y = 0.68\n    z = 0.39\n    \n    L_c = x * L_1 + (1-x) * L_2\n    R_c = x * R_1 + (1-x) * R_2\n    L_b = y * L_3 + (1-y) * L_4\n    R_b = y * R_3 + (1-y) * R_4\n    L_y = z * L_c + (1-z) * L_b\n    R_y = z * R_c + (1-z) * R_b\n    \n    T_c = x * T_1 + (1-x) * T_2\n    T_b = y * T_3 + (1-y) * T_4\n    T_y = z * T_c + (1-z) * T_b","34b141a8":"import sys\nsys.path.append('..\/input\/fasthugs')","a083f575":"from fastai.text.all import *\nfrom fasthugs.data import TransformersTextBlock, TextGetter\nfrom fasthugs.learner import TransLearner\n\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport pandas as pd\nfrom bayes_opt import BayesianOptimization\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nSEED=2021","e7569d5b":"def valid_model(model_name='distilroberta-base', bs=16, idx=1):\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    df_sub['comment_text'] = df_sub.text\n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n    df['all_tox'] = 0\n    model_path = f'..\/input\/roberta-transformers-pytorch\/{model_name}'\n\n    dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_path), RegressionBlock(n_out=6)],\n               get_x=TextGetter('comment_text'),\n               get_y=ItemGetter('all_tox'))\n\n    dls = dblock.dataloaders(df, bs=bs, val_bs=bs*1, num_workers=1)\n    p_hdrop = 0.1\n    #learn.fit_one_cycle(10, 1e-5, cbs=[SaveModelCallback(), EarlyStoppingCallback(comp=np.less, patience=3)])\n    preds_all = []\n    for i in range(5):\n        model = AutoModelForSequenceClassification.from_pretrained(f'..\/input\/jrstc-test\/{model_name}_df{idx}_{i}', \n                    num_labels=6, hidden_dropout_prob=p_hdrop)\n        metrics = [rmse, R2Score(), PearsonCorrCoef(), SpearmanCorrCoef()]\n        opt_func = Adam\n        learn = TransLearner(dls, model, loss_func=BCEWithLogitsLossFlat(), metrics=metrics, opt_func=opt_func)\n\n        test_dl = dls.test_dl(df_sub)\n        preds_, _ = learn.tta(dl=test_dl, n=1, beta=0)\n        preds_all.append(preds_)\n        #preds_l, _ = learn.get_preds(dl=test_dl, n=1, beta=0)\n        if False:\n            df_sub['comment_text'] = df_val.more_toxic\n            test_dl = dls.test_dl(df_val)\n            preds_r, _ = learn.tta(dl=test_dl, n=1, beta=0)\n            preds_l_all.append(preds_l)\n            preds_r_all.append(preds_r)\n        learn = None\n        gc.collect()\n        torch.cuda.empty_cache()\n    return preds_all","6cd22b8a":"if True:\n    T_c_1 = valid_model(model_name='distilroberta-base', idx=1, bs=16)\n    T_b_1 = valid_model(model_name='distilroberta-base', idx=2, bs=16)\n\n    T_c_2 = valid_model(model_name='roberta-base', idx=1, bs=4)\n    T_b_2 = valid_model(model_name='roberta-base', idx=2, bs=4)\n\n    T_c_3 = valid_model(model_name='roberta-large', idx=1, bs=4)\n    T_b_3 = valid_model(model_name='roberta-large', idx=2, bs=4)\n    \n    T_c_1 = np.array([ll.numpy() for ll in T_c_1])\n    T_b_1 = np.array([ll.numpy() for ll in T_b_1])\n    T_c_2 = np.array([ll.numpy() for ll in T_c_2])\n    T_b_2 = np.array([ll.numpy() for ll in T_b_2])\n    T_c_3 = np.array([ll.numpy() for ll in T_c_3])\n    T_b_3 = np.array([ll.numpy() for ll in T_b_3])\n    \n    \n    w = np.array([1, 2, 1, 1, 1, 1])\n\n    T_c_1 = np.einsum('ijk, k->j', T_c_1, w)\n    T_b_1 = np.einsum('ijk, k->j', T_b_1, w)\n    T_c_2 = np.einsum('ijk, k->j', T_c_2, w)\n    T_b_2 = np.einsum('ijk, k->j', T_b_2, w)\n    T_c_3 = np.einsum('ijk, k->j', T_c_3, w)\n    T_b_3 = np.einsum('ijk, k->j', T_b_3, w)","f9eb73de":"if True:\n    w = np.array([1, 2, 1, 1, 1, 1])\n    L_c_1 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_c_1.npy'), w)\n    L_c_2 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_c_2.npy'), w)\n    L_c_3 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_c_3.npy'), w)\n    L_b_1 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_b_1.npy'), w)\n    L_b_2 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_b_2.npy'), w)\n    L_b_3 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/L_b_3.npy'), w)\n\n    R_c_1 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_c_1.npy'), w)\n    R_c_2 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_c_2.npy'), w)\n    R_c_3 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_c_3.npy'), w)\n    R_b_1 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_b_1.npy'), w)\n    R_b_2 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_b_2.npy'), w)\n    R_b_3 = np.einsum('ijk, k->j', np.load('..\/input\/jrstc-test\/R_b_3.npy'), w)\n    sort_good_weight(L_b_1, R_b_1, L_c_1, R_c_1) # [(0.57, 0.43000000000000005, 70.49), (0.48, 0.52, 70.48), (0.51, 0.49, 70.48), (0.52, 0.48, 70.48), (0.53, 0.47, 70.48)]\n    sort_good_weight(L_b_2, R_b_2, L_c_2, R_c_2) # [(0.61, 0.39, 70.43), (0.64, 0.36, 70.41), (0.6, 0.4, 70.39), (0.62, 0.38, 70.39), (0.63, 0.37, 70.39)]\n    sort_good_weight(L_b_3, R_b_3, L_c_3, R_c_3) # [(0.09, 0.91, 69.94), (0.08, 0.92, 69.93), (0.1, 0.9, 69.93), (0.25, 0.75, 69.93), (0.07, 0.9299999999999999, 69.92)]\n    x = 0.5\n    L__1 = x * L_b_1 + (1-x) * L_c_1\n    R__1 = x * R_b_1 + (1-x) * R_c_1\n\n    x = 0.6\n    L__2 = x * L_b_2 + (1-x) * L_c_2\n    R__2 = x * R_b_2 + (1-x) * R_c_2\n\n    x = 0.1\n    L__3 = x * L_b_3 + (1-x) * L_c_3\n    R__3 = x * R_b_3 + (1-x) * R_c_3\n    #sort_good_weight(L_1, R_1, L_2, R_2) # [(0.85, 0.15000000000000002, 70.53), (0.86, 0.14, 70.53), (0.76, 0.24, 70.52), (0.77, 0.22999999999999998, 70.52), (0.78, 0.21999999999999997, 70.52)]\n    #sort_good_weight(L_x, R_x, L_3, R_3) # [(0.59, 0.41000000000000003, 70.56), (0.56, 0.43999999999999995, 70.55), (0.57, 0.43000000000000005, 70.55), (0.61, 0.39, 70.55), (0.58, 0.42000000000000004, 70.54)]\n    yy = 0.8\n    L_x = L__1 * yy + (1-yy) * L__2\n    R_x = R__1 * yy + (1-yy) * R__2\n    yy = 0.6\n    L_x = L_x * yy + (1-yy) * L__3\n    R_x = R_x * yy + (1-yy) * R__3","bed7ca34":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nx, y, z = 0.5, 0.5, 0.5 \nT_y = z*(x*T_1+(1-x)*T_2) + (1-z)*(y*T_3+(1-y)*T_4)\nx_, y_, z_ = 0.5, 0.5, 0.1\nxx_, yy_ = 0.5, 0.6\nT_x = (xx_*(x_*T_b_1+(1-x_)*(T_c_1))+(1-xx_)*(y_*T_b_2+(1-y_)*(T_c_2))) * yy_ + (1-yy_)*(z_*T_b_3+(1-z_)*(T_c_3))\nll_ = 0.5\nT_ = T_x * ll_ + (1-ll_) * T_y\ndf_sub['score'] = T_\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\ndf_sub[['comment_id', 'score']].head()","8c96335b":"if False:\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    x, y, z = 0.45, 0.68, 0.39 \n    T_y = z*(x*T_1+(1-x)*T_2) + (1-z)*(y*T_3+(1-y)*T_4)\n    x_, y_, z_ = 0.5, 0.6, 0.1\n    xx_, yy_ = 0.8, 0.6\n    T_x = (xx_*(x_*T_b_1+(1-x_)*(T_c_1))+(1-xx_)*(y_*T_b_2+(1-y_)*(T_c_2))) * yy_ + (1-yy_)*(z_*T_b_3+(1-z_)*(T_c_3))\n    ll_ = 0.4\n    T_ = T_x * ll_ + (1-ll_) * T_y\n    df_sub['score'] = T_\n    df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n    df_sub[['comment_id', 'score']].head()","0dab35fa":"if False:\n    # Predict using pipeline\n    df_sub['score'] = test_preds_arr.mean(axis=1)\n    # Cases with duplicates scores\n\n    df_sub['score'].count() - df_sub['score'].nunique()\n    df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n    df_sub[['comment_id', 'score']].head()","17ed3a53":"if False:\n    wts_acc = []\n    for i in range(30,70,1):\n        for j in range(0,20,1):\n            w1 = i\/100\n            w2 = (100 - i - j)\/100\n            w3 = (1 - w1 - w2 )\n            p1_wt = w1*p1 + w2*p3 + w3*p5\n            p2_wt = w1*p2 + w2*p4 + w3*p6\n            wts_acc.append( (w1,w2,w3, \n                             np.round((p1_wt < p2_wt).mean() * 100,2))\n                          )\n    sorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]","82a2cbb9":"# Imports","15b91186":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)"}}