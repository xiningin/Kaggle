{"cell_type":{"aa272546":"code","ed25992b":"code","f37cd8d1":"code","d969081d":"code","49cbce80":"code","26f55938":"code","9077b12b":"code","7a3936bd":"code","a17a0084":"code","63e516bf":"code","1b9215fe":"code","f1538ed7":"code","f2771699":"code","b1177184":"code","04517e2b":"code","6f4c95c3":"code","cde96038":"code","fc6afacf":"code","9e922b2d":"code","25d80dd8":"code","33851292":"code","de70459b":"code","b010a647":"code","c1158013":"code","2b752ee3":"markdown","b8108f75":"markdown","c90b88de":"markdown","e4085c68":"markdown","63151c58":"markdown","f679d862":"markdown","0d5189a4":"markdown","cd804c7a":"markdown","d73923a6":"markdown","8f4e6b56":"markdown","66761c97":"markdown","2a72e5f6":"markdown","360e7f03":"markdown","cb7b0c3e":"markdown","6519fa0c":"markdown","7a425617":"markdown","9fcbeb7f":"markdown"},"source":{"aa272546":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom  tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed25992b":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\nprint(\"Train Shape:\", train.shape)\nprint(\"Test Shape:\", test.shape)","f37cd8d1":"X_train = (train.iloc[:,1:].values).astype('float32') # Convert Pixel values to floating point\ny_train = train.iloc[:,0].values.astype('int32') # Convert Labels to integers\n\nX_test = test.values.astype('float32') # Convert Pixel values to floating point","d969081d":"# Reshape the data into (m, 28, 28) for plotting\nX_train = X_train.reshape(X_train.shape[0], 28, 28)\nX_test = X_test.reshape(X_test.shape[0], 28, 28)\n\nprint(X_train.shape)\nprint(X_test.shape)","49cbce80":"plt.title(\"Label: {}\".format(y_train[300]))\nplt.imshow(X_train[300])","26f55938":"plt.title(\"Label: {}\".format(y_train[50]))\nplt.imshow(X_train[50])","9077b12b":"# Reshape into (m, 28, 28, 1) for the NN\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\nprint(X_train.shape)\nprint(X_test.shape)","7a3936bd":"X_train = X_train \/ 255\nX_test = X_test \/ 255","a17a0084":"y_train = tf.keras.utils.to_categorical(y_train)\n\nprint(y_train.shape)","63e516bf":"# Visualize one of the labels\nplt.title(\"Num {}: {}\".format(np.argmax(y_train[16]), y_train[16]))\nplt.plot(y_train[16])\nplt.xticks(range(y_train.shape[1]))","1b9215fe":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.90, test_size=0.10, random_state=16)\n\nprint(X_train.shape)\nprint(X_val.shape)","f1538ed7":"train_num = X_train.shape[0] # Number of training examples\nval_num = X_val.shape[0] # Number of validation examples\n\nprint(train_num)\nprint(val_num)","f2771699":"generator = ImageDataGenerator()\nbatch_size = 64\n\ntrain_set = generator.flow(X_train, y_train, batch_size = batch_size)\nval_set = generator.flow(X_val, y_val, batch_size = batch_size)","b1177184":"model = Sequential()\n\nmodel.add(Conv2D(32, 3, input_shape=(28, 28, 1), activation='relu'))\nmodel.add(MaxPool2D(strides=2))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(64, 3, padding='same', activation='relu'))\nmodel.add(MaxPool2D(strides=2))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(64, 3, padding='same', activation='relu'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(128, 3, padding='same', activation='relu'))\nmodel.add(MaxPool2D(strides=2))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1152, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10, activation='softmax'))","04517e2b":"model.summary()","6f4c95c3":"# Use 'categorical_crossentropy' because the labels are one-hot encoded\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","cde96038":"# Monitor and early-stop after 2 epochs with no improvements\nearly_stop = EarlyStopping(monitor='val_loss', patience=2, mode='min')","fc6afacf":"epochs = 10\n\nhistory = model.fit(\n    train_set,\n    epochs = epochs,\n    steps_per_epoch = train_num \/\/ batch_size,\n    validation_data = val_set,\n    validation_steps = val_num \/\/ batch_size,\n    callbacks = [early_stop]\n)","9e922b2d":"# Plot the graph\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n# plt.ioff()\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.figure()\n# #loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\nplt.close()","25d80dd8":"# Run the model on the test data for predictions\npredictions = model.predict(X_test)\n# Convert the output to corresponding class (0..9)\npredictions = np.argmax(predictions, axis = -1)","33851292":"print(predictions)","de70459b":"# Plot the 1st test data\nfirst_test_img = X_test[0]\nplt.title(\"Predicted Label: {}\".format(predictions[0]))\nplt.imshow(first_test_img.reshape(28, 28 * 1))","b010a647":"# Plot the 3rd test data\nthird_test_img = X_test[2]\nplt.title(\"Predicted Label: {}\".format(predictions[2]))\nplt.imshow(third_test_img.reshape(28, 28 * 1))","c1158013":"submissions = pd.DataFrame({\n    \"ImageId\": range(1, len(predictions)+1),\n    \"Label\": predictions\n})\nsubmissions.to_csv(\"DKB_Mnist.csv\", index=False, header=True)","2b752ee3":"## Build the NN Model","b8108f75":"### Create a generator for feeding data to the model","c90b88de":"## Submit results","e4085c68":"## Get the train\/test Data","63151c58":"### One-hot encode the labels\n\nEach label is represented as a binary array with the corresponding index = 1 and all others  = 0.  \nEg. number 4 will be represented as `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`.\n\n[One Hot Encoding](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/)","f679d862":"## Visualize the data","0d5189a4":"### It is 2 just as the model predicted.","cd804c7a":"## Visualize the 1st and 3rd Test data","d73923a6":"## Make predictions on Test data","8f4e6b56":"### It is 9 just as the model predicted.","66761c97":"### Normalize the pixels\n\nScale down the pixel values into (0..1) using the Rescale (Min-Max Normalization) technique.  \n*Z_Score Standardization can also be used.*\n\n[Feature Scaling](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling)","2a72e5f6":"## Compile the model\n\n[RMSProp](https:\/\/towardsdatascience.com\/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)","360e7f03":"## Visualize the training process\n\nPlot the accuracy and loss agains the epochs","cb7b0c3e":"**We only divided by 255 becauses the min value is 0. Thus `(x-0)\/(255-0) == x\/255`**","6519fa0c":"### Split the train set into Train and Validation Sets for cross validation\n\nIn Cross Validation, we evaluate the model's performance on the validation set (unseen data) after each epoch.\n\n[Cross Validation](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/)","7a425617":"## Image preprocessing","9fcbeb7f":"## Train the model"}}