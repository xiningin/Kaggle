{"cell_type":{"4e834d8a":"code","a9988d89":"code","5d38c8ab":"code","8b06846e":"code","4d59b66b":"code","82ad2f1c":"code","6d4efdca":"code","261db515":"code","a7afc4ad":"code","9af8cb81":"code","f1f6dd2a":"code","575fe93c":"code","16db8c1a":"code","80d8c66e":"code","8e9cf98b":"code","aa0e0e94":"code","97ce11cf":"code","7195510d":"code","e1498534":"code","62b2dc91":"code","f62f7242":"code","62a4e0f6":"code","8d1616fb":"code","62ee0152":"code","69128d77":"code","84ad1b0b":"code","9327a400":"code","6595b12e":"code","577df878":"code","a1df9918":"code","44d7cd64":"code","bb637b19":"code","c90bddb0":"code","e302bbff":"code","29e40817":"code","9b5b4e08":"code","aa3a5613":"code","dd0a4a34":"code","24a8bfea":"code","366681dd":"code","32d770c0":"code","5723d860":"code","0bb6a961":"code","4011a3d1":"code","be314cbc":"code","73d28331":"code","e0b7ce78":"code","2fbdb593":"code","a1ee9354":"code","722c3ef6":"code","ea0c52b0":"code","672b8f0a":"code","3ff83c13":"code","9e855c7d":"code","4dd58c10":"code","5c28e5ea":"code","1fc57514":"code","9201f201":"code","8473fc93":"code","222c9abb":"code","e5ec2023":"code","fdd84af9":"code","ba1b8cfa":"code","c1bb4acb":"code","036eab81":"code","2b737e63":"markdown","b119a948":"markdown","8c0573c6":"markdown","311526ef":"markdown","ab9167ee":"markdown","194afbef":"markdown","de9f0f5f":"markdown","bbd46b13":"markdown","b41da889":"markdown","c1cc369c":"markdown","7daff010":"markdown","224965c5":"markdown","5b65dc5e":"markdown","eb8134ef":"markdown","97ab7618":"markdown","ed9e47ce":"markdown","859c7f98":"markdown","d5bd4dc9":"markdown","5f8a6a72":"markdown","39783bd7":"markdown","f4613f18":"markdown","d159e8f5":"markdown","d472d5b7":"markdown","9d7f4dfe":"markdown","c13ee526":"markdown","9ae0c2d2":"markdown","f41024e4":"markdown","c504bb7d":"markdown","d22f9fcb":"markdown","8702ec6d":"markdown","66290129":"markdown","153394f8":"markdown","ce240cab":"markdown","364869d1":"markdown","64e94e14":"markdown","5bbde8a1":"markdown","80e31045":"markdown","ea505862":"markdown","d3fc6e7b":"markdown","95723b93":"markdown","a1d31827":"markdown","994b87c9":"markdown","47c34bc1":"markdown","e42e81e2":"markdown","1da0f2f6":"markdown","aaf2b148":"markdown"},"source":{"4e834d8a":"#Paquetes Clasicos\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Paquetes para Preprocesamiento\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords \nimport unicodedata\n\n#Vectorizacion\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Pipeline\nfrom sklearn.pipeline import Pipeline\n\n#Tuning de Parametros\nfrom sklearn.model_selection import GridSearchCV\n\n#Clasificadores\nfrom sklearn.linear_model import RidgeClassifier # Classifier using Ridge regression.\nfrom sklearn.linear_model import LogisticRegression # Classifier using Logistic Regression.\nfrom sklearn.linear_model import Perceptron # Classifier using Perceptron.\nfrom sklearn.naive_bayes import MultinomialNB # Classifier using Naive Bayes classifier multinomial\nfrom sklearn.svm import LinearSVC #Classifier using Linear Support Vector\nfrom sklearn.svm import SVC #C-Support Vector\nfrom sklearn.ensemble import GradientBoostingClassifier #Classifier using Gradient Boosting classifier ","a9988d89":"#Dataset de entranamiento\ndf_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","5d38c8ab":"#Dataset de test\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","8b06846e":"#Observo la constitucion general del Dataset de entrenamiento\n\ndf_train.sample(10)","4d59b66b":"#Dimensiones del dataset\n\ndf_train.shape","82ad2f1c":"#Cantidad de tweets catalogados como Verdaderos(1) y Falsos(0)\n\ndf_train['target'].value_counts()","6d4efdca":"#Reemplazo los NaN por el string 'None'\n\ndf_train.location.fillna('None', inplace=True)\ndf_train.keyword.fillna('None', inplace=True)","261db515":"#Observo como queda el dataframe\n\ndf_train.head()","a7afc4ad":"#Cuento la cantidad de 'text' que se encuentran repetidos\n\nduplicados_train = df_train['text'].duplicated(keep=False)\nduplicados_train.sum()","9af8cb81":"#Elimino las filas donde 'text' esta repetido\n\ndup_train = df_train[['text','target']][duplicados_train]\ndup_train","f1f6dd2a":"#Verifico que el tama\u00f1o del dataset disminuyo la cantidad de repetidos\n\ndf_train.shape","575fe93c":"#Elimino las filas donde 'text' esta repetido\n\ndf_train = df_train.drop_duplicates('text',keep=False)\n\ndf_train.head()","16db8c1a":"#Verifico que el tama\u00f1o del dataset disminuyo la cantidad de repetidos\n\ndf_train.shape","80d8c66e":"#Observo la constitucion general del Dataset de test\n\ndf_test.head()","8e9cf98b":"#Dimensiones del dataset\n\ndf_test.shape","aa0e0e94":"#Reemplazo los NaN por el string 'None'\n\ndf_test.location.fillna('None', inplace=True)\ndf_test.keyword.fillna('None', inplace=True)","97ce11cf":"#Observo como queda el dataframe\n\ndf_test.head()","7195510d":"duplicados_test = df_test['text'].duplicated(keep=False)\nduplicados_test.sum()","e1498534":"dup_test = df_test[['text']][duplicados_test]\ndup_test","62b2dc91":"# Transformar el texto a minuscula - OK\n\ndef minuscula(texto):\n    return texto.lower()","f62f7242":"#Remover URL - OK\n\ndef remover_url(texto):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', texto)","62a4e0f6":"#Remover Usuarios (''@usuario') - OK\n#Si aparecen usuarios combinados ('@usuario__usuario') queda '_usuario' y la funcion remover_no_alfabeto(texto) lo elimina\n\ndef remover_usuario(texto):\n    text = re.sub(r\"\\@[A-Za-z0-9]+\", \"\", texto)\n    return texto","8d1616fb":"# Remover Emoji - OK\n\ndef remover_emoji(texto):\n    emoji_patrones = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F' \n        u'\\U0001F300-\\U0001F5FF' \n        u'\\U0001F680-\\U0001F6FF' \n        u'\\U0001F1E0-\\U0001F1FF' \n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_patrones.sub(r'', texto)","62ee0152":"#Expandir las Abreviaturas - OK\n\nabreviaturas = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", \n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef expandir_abreviatura(texto,mapping = abreviaturas):\n    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n    return texto","69128d77":"#Expandir las Contracciones - OK\n\ncontracciones_mapeo = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef expandir_contraccion(texto,mapping = contracciones_mapeo):\n    specials =[\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        texto = texto.replace(s,\"'\")\n    \n    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n    return texto","84ad1b0b":"# Remover los tags HTML - OK\n\ndef remover_tag_html(texto):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', texto)","9327a400":"#Remover acentos - OK\n\ndef remover_acento(texto):\n    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return texto","6595b12e":"#Remover Puntos - OK\n\ndef remover_punto(texto):\n    import string\n    texto = ''.join([c for c in texto if c not in string.punctuation])\n    return texto","577df878":"#Remover Numeros - OK\n\ndef remover_numero(texto):\n    texto = ''.join([i for i in texto if not i.isdigit()])\n    return texto","a1df9918":"#Remover espacios en Blanco (extras\/tabs) - OK\n\ndef remover_espacio_extra(texto):\n    import re\n    pattern = r'^\\s*|\\s\\s*'\n    return re.sub(pattern, ' ', texto).strip()","44d7cd64":"#Remueve todo lo que se sea del alfabeto - OK\n\ndef remover_no_alfabeto(texto):\n    return ' '.join([i for i in texto.split() if i.isalpha() == True])","bb637b19":"#Remover Stop-Word - OK\n\ndef remover_stop_word(texto):\n    return \" \".join ([word for word in word_tokenize(texto) if not word in stopwords.words('english')])","c90bddb0":"#Lematizar - OK\n\ndef lematizar(texto):\n    lemma = WordNetLemmatizer()\n    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(texto)])","e302bbff":"def preprocesar_df(df, col_name, clean_col_name):\n    df[clean_col_name] = df[col_name].apply(lambda x: minuscula (x))\\\n                                    .apply(lambda x: remover_url(x))\\\n                                    .apply(lambda x: remover_usuario(x))\\\n                                    .apply(lambda x: remover_emoji(x))\\\n                                    .apply(lambda x: expandir_abreviatura(x))\\\n                                    .apply(lambda x: expandir_contraccion(x))\\\n                                    .apply(lambda x: remover_tag_html(x))\\\n                                    .apply(lambda x: remover_acento(x))\\\n                                    .apply(lambda x: remover_punto(x))\\\n                                    .apply(lambda x: remover_numero(x))\\\n                                    .apply(lambda x: remover_espacio_extra(x))\\\n                                    .apply(lambda x: remover_no_alfabeto(x))\\\n                                    .apply(lambda x: remover_stop_word(x))\\\n                                    .apply(lambda x: lematizar(x))","29e40817":"#Llamo a la funcion que realiza el preprocesamiento\n\npreprocesar_df(df_train,'text', 'texto_preprocesado')","9b5b4e08":"#Observo las 5 primeras filas\n\ndf_train.head(30)","aa3a5613":"#Dimensiones del dataset\n\ndf_train.shape","dd0a4a34":"x_train_original = df_train['text']","24a8bfea":"x_train_preprocesado = df_train['texto_preprocesado']","366681dd":"y_train = df_train['target']","32d770c0":"preprocesar_df(df_test,'text', 'texto_preprocesado')","5723d860":"#Observo las 5 primeras filas\n\ndf_test.head()","0bb6a961":"df_test.shape","4011a3d1":"x_test_original = df_test['text']","be314cbc":"x_test_preprocesado = df_test['texto_preprocesado']","73d28331":"#Parametros para el vectorizador BOW - CountVectorizer()\n\nparametros_bow = {'vectorizador__strip_accents':'unicode',\n                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                  'vectorizador__max_df':[10,20,30],\n                  'vectorizador__binary':[False,True]}","e0b7ce78":"#Parametros para el vectorizador TFIDF - TfidfVectorizer()\n\nparametros_tfidf = {'vectorizador__strip_accents':'unicode',\n                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                  'vectorizador__max_df':[10,20,30],\n                  'vectorizador__binary':[False,True],\n                  'vectorizador__use_idf':[True,False]}","2fbdb593":"#Pipeline\n\npipeline_bow_RR = Pipeline([('vectorizador', CountVectorizer()),('clf', RidgeClassifier())])","a1ee9354":"#Dataset sin preprocesamiento\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_1 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__min_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_bow_original = GridSearchCV(pipeline_bow_RR, hiperparametros_1,cv=5, n_jobs=-1,verbose=2)\nclf_RR_bow_original.fit(x_train_original, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_bow_original.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_bow_original.best_params_)","722c3ef6":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","ea0c52b0":"submission['target'] = clf_RR_bow_original.predict(x_test_original)","672b8f0a":"submission.to_csv(\"original_bow_hiper3_ridge_regression.csv\", index=False)","3ff83c13":"#Dataset Preprocesado\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_2 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__min_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_bow_preprocesado = GridSearchCV(pipeline_bow_RR, hiperparametros_2,cv=5, n_jobs=-1,verbose=2)\nclf_RR_bow_preprocesado.fit(x_train_preprocesado, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_bow_preprocesado.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_bow_preprocesado.best_params_)","9e855c7d":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","4dd58c10":"submission['target'] = clf_RR_bow_preprocesado.predict(x_test_preprocesado)","5c28e5ea":"submission.to_csv(\"preprocesado_bow_hiper3_ridge_regression.csv\", index=False)","1fc57514":"#Pipeline\n\npipeline_tfidf_RR = Pipeline([('vectorizador', TfidfVectorizer()),('clf', RidgeClassifier())])","9201f201":"#Dataset sin preprocesamiento\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_3 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__max_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'vectorizador__use_idf':[True,False],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_tfidf_original = GridSearchCV(pipeline_tfidf_RR, hiperparametros_3,cv=5, n_jobs=-1,verbose=2)\nclf_RR_tfidf_original.fit(x_train_original, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_tfidf_original.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_tfidf_original.best_params_)","8473fc93":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","222c9abb":"submission['target'] = clf_RR_tfidf_original.predict(x_test_original)","e5ec2023":"submission.to_csv(\"original_tfidf_hiper3_ridge_regression.csv\", index=False)","fdd84af9":"#Dataset Preprocesado\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_4 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__max_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'vectorizador__use_idf':[True,False],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_RR, hiperparametros_4,cv=5, n_jobs=-1,verbose=2)\nclf_RR_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_tfidf_preprocesado.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_tfidf_preprocesado.best_params_)","ba1b8cfa":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","c1bb4acb":"submission['target'] = clf_RR_tfidf_preprocesado.predict(x_test_preprocesado)","036eab81":"submission.to_csv(\"preprocesado_tfidf_hiper3_ridge_regression.csv\", index=False)","2b737e63":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle","b119a948":"**Datos Preprocesados**","8c0573c6":"Creo una nueva columna con los valores que predice el modelo","311526ef":"Se definen funciones especificas para realizar el preprocesamiento de los datos:","ab9167ee":"# NLP with Disaster Tweets","194afbef":"Guardo el archivo .csv para realizar el submit a Kaggle","de9f0f5f":"Creo una nueva columna con los valores que predice el modelo","bbd46b13":"## 3.1 Preprocesamiento del Dataset 'train' ","b41da889":"# Limpieza de datos \/ Vectorizacion (BOW \/ TFIDF) \/ Ridge Regresion Classifier\n\n## Score 0.78","c1cc369c":"Defino los vectores que se utilizaran para el entrenamiento de los modelos","7daff010":"Generacion del SUBMIT","224965c5":"Guardo el archivo .csv para realizar el submit a Kaggle","5b65dc5e":"# 1. Librerias\n","eb8134ef":"## 2.1 Dataset 'train'","97ab7618":"Generacion del SUBMIT","ed9e47ce":"Generacion del SUBMIT","859c7f98":"En este caso no es necesario eliminar los tweets. Seria conveniente verificar que los mismos fueron catalogados de igual forma al finalizar el analisis","d5bd4dc9":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle","5f8a6a72":"**Tratamiento de NaN**","39783bd7":"## 2.2 Dataset 'test'","f4613f18":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle","d159e8f5":"Generacion del SUBMIT","d472d5b7":"## 3.2 Preprocesamiento del Dataset 'test' ","9d7f4dfe":"**Datos sin Preprocesar**","c13ee526":"**Tratamiento de NaN**","9ae0c2d2":"**Datos Preprocesados**","f41024e4":"Defino los vectores que se utilizaran para el entrenamiento de los modelos","c504bb7d":"Creo una nueva columna con los valores que predice el modelo","d22f9fcb":"Una vez definidias todas las funciones que se utilizaran para realizar la limpieza se define una funcion de ejecuta todas estas a una columna del dataset","8702ec6d":"**Tweets Repetidos**","66290129":"## 5.1 Ridge Regresion","153394f8":"Creo una nueva columna con los valores que predice el modelo","ce240cab":"A simple vista se observa que alguno de los tweets repetidos tienen como target diferentes valores, lo cual en principio es confuso. Se decide eliminar todos los tweets repetidos","364869d1":"### 5.1.2 TFIDF ","64e94e14":"Guardo el archivo .csv para realizar el submit a Kaggle","5bbde8a1":"Guardo el archivo .csv para realizar el submit a Kaggle","80e31045":"# 5. Tuning de Parametros (GridSearch) y Resultados","ea505862":"**Datos sin Preprocesar**","d3fc6e7b":"### 5.1.1 BOW","95723b93":"# 2. Datos","a1d31827":"Realizo el preprocesamiento de la comlumna **'text'** del dataset **'test'**","994b87c9":"# 3. Preprocesamiento","47c34bc1":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle","e42e81e2":"Realizo el preprocesamiento de la comlumna **'text'** del dataset **'train'**","1da0f2f6":"# 4. Hiperparametros de Vectorizadores\n\nSe definen los Hiperparametros que se utilizaran en GridSearchCV para realizar el tuning","aaf2b148":"**Tweets Repetidos**"}}