{"cell_type":{"61a12b8e":"code","8681f024":"code","78a3cf57":"code","6a945bcb":"code","6143c09b":"code","0febff54":"code","ff7cb685":"code","fb9c9796":"code","65203962":"code","c782d8e2":"code","3308dbf7":"code","eaea30c6":"code","139008a3":"code","53f4817d":"code","e0a893cd":"code","4f3c2f69":"code","c063b1a8":"code","76bed2a3":"code","d6ad8e5e":"code","ee291679":"code","80def46a":"code","dc4a569d":"code","1ca3e9d0":"code","9e4c1926":"code","93ecaffc":"code","c6dc9a85":"code","a0f4bf6c":"code","067ddc28":"code","b329e014":"code","1d6e2dcb":"code","f7b16c79":"code","8f76c49b":"code","cc448cc8":"code","1a57b3be":"code","f72d476c":"code","3f1a7439":"code","1a36de4a":"code","5c546bbd":"code","eb4772d2":"code","903c9703":"code","1c1a5b7e":"code","51267942":"code","f81b3f8f":"code","e5beedb6":"code","5230b2bb":"code","48186ae5":"code","39c3f200":"code","ba18a5ec":"code","cb831f7e":"code","0c5fe799":"code","cfadac6a":"code","dac3adf0":"markdown","b9ac2199":"markdown","68b8fc9f":"markdown","9d89379b":"markdown","cde23013":"markdown","4d6800c9":"markdown","70c0ebac":"markdown","efa3bb51":"markdown","0b9b5b8f":"markdown","af9b3baf":"markdown","4e0a350f":"markdown"},"source":{"61a12b8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8681f024":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","78a3cf57":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\nprint(\"Shape of the training set : \" ,train_df.shape)\ntrain_df.head()","6a945bcb":"train_df.count()","6143c09b":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(\"Shape of the test set : \" ,test_df.shape)\ntest_df.head()","0febff54":"#sort the ages into logical categories\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_df)","ff7cb685":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (15,5))\nsns.heatmap(train_df.isnull(), cmap = 'magma', ax = ax1)\nsns.heatmap(test_df.isnull(), cmap = 'mako_r', ax = ax2)","fb9c9796":"fig, (ax1, ax2 ) = plt.subplots(ncols=2 , figsize=(20, 6))\nfor s in [1,2,3]:\n    train_df.Age[train_df.Pclass == s].plot(kind='kde',ax = ax1,title = \"Age wrt class\" )\nplt.legend(('1st','2nd','3rd'))\ntrain_df.Embarked.value_counts(normalize=True).plot(kind='bar',alpha=0.5,ax = ax2, color = \"chocolate\")   \nplt.title(\"Embarked\")","65203962":"sns.countplot(train_df['Embarked'],palette = \"Accent\")","c782d8e2":"\n#visualise gender\nfig, (ax1, ax2, ax3 , ax4 ) = plt.subplots(ncols=4 , figsize=(20, 6))\n\ntrain_df.Survived.value_counts(normalize=True).plot(kind='bar',alpha=0.5 ,ax = ax1, color = \"olive\",title = \"Survived\")\n\ntrain_df.Survived[train_df.Sex == 'male'].value_counts(normalize=True).plot(kind='bar',alpha=0.5 ,ax = ax2, color = \"slateblue\",title = \"Male survivors\")\n\ntrain_df.Survived[train_df.Sex == 'female'].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='fuchsia' ,ax = ax3,title = \"Female survivors\")\n\ntrain_df.Sex[train_df.Survived == 1].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='coral' ,ax = ax4,title = \"Sex Survived\")\n","3308dbf7":"plt.subplot2grid((2,3),(1,0),colspan=4)\nfor s in [1,2,3]:\n    train_df.Survived[train_df.Pclass == s].plot(kind='kde')\nplt.title(\"Survived wrt class\")\nplt.legend(('1st','2nd','3rd'))\nplt.show()","eaea30c6":"fig, (ax1, ax2, ax3 , ax4 ) = plt.subplots(ncols=4 , figsize=(20, 6))\n\ntrain_df.Survived[(train_df.Sex == 'male') & (train_df.Pclass ==1)].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='olive',title = \"Rich men survivors\",ax = ax1)\n\ntrain_df.Survived[(train_df.Sex == 'male') & (train_df.Pclass ==3)].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='springgreen',title = \"Poor men survivors\",ax = ax2)\n\ntrain_df.Survived[(train_df.Sex == 'female') & (train_df.Pclass ==1)].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='hotpink',title = \"Rich women survivors\",ax = ax3)\n\ntrain_df.Survived[(train_df.Sex == 'female') & (train_df.Pclass ==3)].value_counts(normalize=True).plot(kind='bar',alpha=0.5,color='tomato',title = \"Poor women survivors\",ax = ax4)\n","139008a3":"df_m = train_df[train_df['Sex'] == 'male']\ndf_f = train_df[train_df['Sex'] == 'female']\ndf_m = df_m['Survived'].value_counts()\ndf_f = df_f['Survived'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Male', marker = dict(color = 'slateblue'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Female', marker = dict(color = 'salmon'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Survival Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","53f4817d":"df_m = train_df[train_df['Sex'] == 'male']\ndf_f = train_df[train_df['Sex'] == 'female']\ndf_m = df_m['Embarked'].value_counts()\ndf_f = df_f['Embarked'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Male', marker = dict(color = 'lime'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Female', marker = dict(color = 'orangered'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Embarked Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","e0a893cd":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Parch'].value_counts()\ndf_f = df_f['Parch'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'burlywood'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'darkolivegreen'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Parch Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","4f3c2f69":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Pclass'].value_counts()\ndf_f = df_f['Pclass'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'firebrick'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'gold'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Pclass Distribution', )\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","c063b1a8":"import pandas as pd\ntrain= pd.read_csv('..\/input\/titanic\/train.csv')\ntrain[\"Hyp\"] = 0\ntrain.loc[train.Sex == \"female\",\"Hyp\"]=1\n\ntrain[\"Result\"]=0\ntrain.loc[train.Survived == train[\"Hyp\"],\"Result\"]=1\nprint(\"The percentage of survivors :\")\nprint( train[\"Result\"].value_counts(normalize  = True))","76bed2a3":"df = [train_df, test_df]\nfor data in df:\n    data['Title'] = data['Name'].str.extract(r', (\\w+)\\.', expand=False)\npd.crosstab(train_df['Title'], train_df['Sex']).transpose()","d6ad8e5e":"for data in df:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title']).mean()\n\nlabels = {'Mr':1, 'Mrs':2, 'Master':3, 'Miss':4, 'Rare':5}\ntest_df.replace({'Title':labels}, inplace = True)\ntrain_df.replace({'Title':labels}, inplace = True)\ntrain_df['Title'] = train_df['Title'].fillna(0)\ntrain_df['Title'] = train_df['Title'].astype(int)                     # this is performed beacuse it was giving float values of title","ee291679":"train_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)\nmr_age = train_df[train_df[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train_df[train_df[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train_df[train_df[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train_df[train_df[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nrare_age = train_df[train_df[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\"}\n\nfor x in range(len(train_df[\"AgeGroup\"])):\n    if train_df[\"AgeGroup\"][x] == \"Unknown\":\n        train_df[\"AgeGroup\"][x] = age_title_mapping[train_df[\"Title\"][x]]\n        \nfor x in range(len(test_df[\"AgeGroup\"])):\n    if test_df[\"AgeGroup\"][x] == \"Unknown\":\n        test_df[\"AgeGroup\"][x] = age_title_mapping[test_df[\"Title\"][x]]\n\n\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain_df['AgeGroup'] = train_df['AgeGroup'].map(age_mapping).astype(int)\ntest_df['AgeGroup'] = test_df['AgeGroup'].map(age_mapping).astype(int)","80def46a":"train_df['Embarked'].fillna('S', inplace = True)\n\nlabel = {'S':1, 'C':2, 'Q':3}\ntrain_df.replace({'Embarked':label}, inplace = True)\ntest_df.replace({'Embarked':label}, inplace = True)","dc4a569d":"train_df[\"Cabin\"] = (train_df[\"Cabin\"].notnull().astype('int'))\ntest_df[\"Cabin\"] = (test_df[\"Cabin\"].notnull().astype('int'))","1ca3e9d0":"#if we check the data info then fare feature is a category not int, so to convert we are performing this: \ntrain_df['Fare'] = pd.to_numeric(train_df['Fare'])","9e4c1926":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)\n\ntrain_df['Fare'] = pd.qcut(train_df['Fare'], 4, labels = [1, 2, 3, 4])\ntest_df['Fare'] = pd.qcut(test_df['Fare'], 4, labels = [1, 2, 3, 4])","93ecaffc":"label = {'male':1, 'female':0}\ntrain_df.replace({'Sex':label}, inplace = True)\ntest_df.replace({'Sex':label}, inplace = True)","c6dc9a85":"for data in df:\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a0f4bf6c":"for data in df:\n    data['IsAlone'] = 0\n    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","067ddc28":"train_df.drop(['PassengerId', 'Name', 'Ticket', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis = 1, inplace = True)\ntest_df.drop(['Name', 'Ticket', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis = 1, inplace = True)","b329e014":"train_df.head()","1d6e2dcb":"test_df.head()","f7b16c79":"pd.DataFrame({'Train':train_df.isnull().sum(), 'Test':test_df.isnull().sum()})","8f76c49b":"train_df.info()","cc448cc8":"train_df.describe()","1a57b3be":"train_df.info()","f72d476c":"train_df.head()","3f1a7439":"\nX = train_df.drop('Survived', axis = 1)\ny = train_df['Survived']\n","1a36de4a":"X.dropna(inplace = True)","5c546bbd":"X.isnull().sum()","eb4772d2":"from sklearn import linear_model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)\n\n#Create classifier\n\nLR = linear_model.LogisticRegression()\nLR_ = LR.fit(X_train,y_train )\n\ny_pred = LR_.predict(X_test)\n\nprint(\"Train score: \" , LR_.score(X_train,y_train))\nprint(\"Test score: \" , LR_.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n\n","903c9703":"from sklearn import model_selection\ndc_tree= DecisionTreeClassifier(random_state = 42)\ndc_ = dc_tree.fit(X_train,y_train)\nprint(dc_.score(X_train,y_train))\n\nscore = model_selection.cross_val_score(dc_tree ,X_train,y_train,scoring='accuracy' , cv=50)\nprint(\"After removing overfitting score is \" + str(score.mean()))\n\n\ndc_tree=DecisionTreeClassifier(random_state = 1 , max_depth =7 , min_samples_split=2)\ndc_ = dc_tree.fit(X_train,y_train)\nprint(dc_.score(X_train,y_train))\n\nscore = model_selection.cross_val_score(dc_tree ,X_train,y_train,scoring='accuracy' , cv=50)\nprint(\"After removing overfitting score is \" + str(score.mean()))\n\n\ny_pred = dc_tree.predict(X_test)\n\nprint(\"Train score: \" , dc_tree.score(X_train,y_train))\nprint(\"Test score: \" , dc_tree.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report :\")\nprint(classification_report(y_test, y_pred))\n","1c1a5b7e":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nprint(\"Train score: \" , classifier.score(X_train,y_train))\nprint(\"Test score: \" , classifier.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report :\")\nprint(classification_report(y_test, y_pred))\n\n","51267942":"error_rate = []\nfor i in range(1,30):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize = (8,6))\nplt.plot(range(1,30), error_rate, color='salmon', linestyle='dashed', marker='o', markerfacecolor='b', markersize=10)","f81b3f8f":"from sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Train score: \" , clf.score(X_train,y_train))\nprint(\"Test score: \" , clf.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report :\")\nprint(classification_report(y_test, y_pred))\n","e5beedb6":"from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nrfc = RandomForestClassifier(max_depth=10 , random_state=0)\nrfc.fit(X_train , y_train)\n\ny_pred = rfc.predict(X_test)\n\nprint(\"Train score: \" , rfc.score(X_train,y_train))\nprint(\"Test score: \" , rfc.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report :\")\nprint(classification_report(y_test, y_pred))","5230b2bb":"adb = AdaBoostClassifier(rfc, n_estimators = 200)\nadb.fit(X_train, y_train)\ny_pred = adb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nadb_train_acc = round(adb.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', adb_train_acc)\nadb_test_acc = round(adb.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', adb_test_acc)","48186ae5":"gdb = GradientBoostingClassifier()\nparams = {'learning_rate':[0.01,0.1,1,10],'n_estimators':[100,150,200,300],'subsample':[0.6,0.8,1.0],'max_depth':[2,3,4,6],'min_samples_leaf':[1,2,4,6]}\ngcv = GridSearchCV(estimator=gdb, param_grid=params, cv=5, n_jobs=-1)\ngcv.fit(X_train, y_train)\ngcv.best_params_\n\ngdb = GradientBoostingClassifier(max_depth = 2, n_estimators = 300, subsample = 0.6)\ngdb.fit(X_train, y_train)\ny_pred = gdb.predict(X_test)\n\n\nprint(\"Train score: \" , gdb.score(X_train,y_train))\nprint(\"Test score: \" , gdb.score(X_test,y_test))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report :\")\nprint(classification_report(y_test, y_pred))","39c3f200":"# Compare Algorithms\nimport pandas\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('Random Forest', rfc))\nmodels.append(('ADA booster', adb))\nmodels.append(('GradientBoostingClassifier', gdb))\n\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","ba18a5ec":"# boxplot algorithm comparison\nfig = plt.figure(figsize =(20,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","cb831f7e":"X_train.head()","0c5fe799":"test_df.head()","cfadac6a":"test_df['Fare'] = pd.to_numeric(test_df['Fare'])\ntest_df['Survived'] = gdb.predict(test_df.drop(['PassengerId',\"Survived\"], axis = 1))\ntest_df[['PassengerId', 'Survived']].to_csv('MySubmission.csv', index = False)","dac3adf0":"# KNeighborsClassifier","b9ac2199":"# Predictive Models used :\n\n* Logistic Regression\n* Support Vector Machines\n* KNN or K-Nearest Neighbors\n* Decision Trees\n* Random Forest","68b8fc9f":"# Algorithm comparision","9d89379b":"# AdaBoostClassifier","cde23013":"# Logistic Regression","4d6800c9":"# Decision Tree Classifier ","70c0ebac":"# Support Vector Classifier","efa3bb51":"# GradientBoostingClassifier","0b9b5b8f":"# Random Forest Classifier","af9b3baf":"# Variable description\n\n* Survived: Survived (1) or died (0)\n* Pclass: Passenger\u2019s class\n* Name: Passenger\u2019s name\n* Sex: Passenger\u2019s sex\n* Age: Passenger\u2019s age\n* SibSp: Number of siblings\/spouses aboard\n* Parch: Number of parents\/children aboard\n* Ticket: Ticket number\n* Fare: Fare\n* Cabin: Cabin\n* Embarked: Port of embarkation","4e0a350f":"* # Best algorithm is Random forest Classifier"}}