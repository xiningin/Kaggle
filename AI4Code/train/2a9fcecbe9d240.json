{"cell_type":{"d60d6576":"code","aa8b581f":"code","50a3905e":"code","815dac9f":"code","1aa79317":"code","b337e5fe":"code","9e1a695a":"code","e48b1c50":"code","2698eab4":"code","77ee3793":"code","89280b01":"code","c1cbeb85":"code","0fc4c583":"code","9c538270":"code","49578ee5":"code","7866b0cc":"code","0957e0f4":"code","c13ffc6e":"code","52fb1b41":"code","8aa4ba99":"code","890f5d0c":"code","150043ca":"code","2ee2bd62":"code","3100c7d8":"code","798ea344":"code","943ddc81":"code","b85c66d8":"code","253688f0":"code","8cce373f":"code","c1ab3bcb":"code","2f6ec174":"code","cf685769":"code","2b594ff1":"code","f5ac166f":"code","113560b0":"code","8a6960b7":"code","0cb92f38":"code","e91ac512":"markdown","09406620":"markdown","644c0702":"markdown","251bad61":"markdown","6bec8d1c":"markdown","56df4169":"markdown","a7523a65":"markdown","08c53ae5":"markdown","fb8dcfe6":"markdown","3ecaf242":"markdown","7e17db86":"markdown","76068eb4":"markdown","be2a2786":"markdown","ef073c7b":"markdown","8190e862":"markdown","8e6bf3ef":"markdown","b06d90e2":"markdown"},"source":{"d60d6576":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa8b581f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","50a3905e":"# lets install extra liberary\n! pip install autocorrect","815dac9f":"\"\"\" ---- NLP text cleaning ----\"\"\"\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n# import en_core_web_sm\n# nlp = en_core_web_sm.load()\nimport re\nimport string\nimport unidecode\n# from pycontractions import Contractions\nfrom autocorrect import Speller\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n","1aa79317":"from sklearn.feature_extraction.text import CountVectorizer","b337e5fe":"messages=pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding=\"latin-1\")\nmessages.head()","9e1a695a":"messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace=True)\nmessages.columns = [\"category\", \"text\"]\nprint(messages.shape)\nmessages.head()","e48b1c50":"# # let's add some columns feature\ndef get_avg_word_len(x):\n    words=x.split()\n    word_len=0\n    for word in words:\n        word_len=word_len+len(word)\n    return word_len\/len(words)\n\nmessages['msg_len'] = messages['text'].apply(len)\nmessages['word_count'] = messages['text'].apply(lambda x :  len(x.split())  )\nmessages['avg_word_len'] = messages['text'].apply(lambda x :  get_avg_word_len(x)  )\nmessages['class']=messages['category'].apply(lambda x: 1 if x=='ham' else 0) # let's give ham=1 and spam=0 class\n# # messages['class']=np.where(messages['category']=='spam',0,1)","2698eab4":"messages.head()","77ee3793":"stop_words = set(stopwords.words(\"english\"))\nspell_check = Speller(lang='en')","89280b01":"\"\"\"remove accented characters, to lower case, Remove numerical\"\"\"\ndef text_process_1(text):\n    # remove accented characters from text, e.g. caf\u00e9 \n    text = unidecode.unidecode(text)\n    # change to lower case\n    text = text.lower()\n    # remove tags\n    text=re.sub('<[^<]+?>','', text)\n    # Remove numerical like  1996, 6 ,6df\n    text=''.join(c for c in text if not c.isdigit())\n    # return\n    return text\n\n\n\"\"\" token & lemmatizer -- using spaCy liberary \"\"\" \n\"\"\" spacy lematizer  also Expand Contractions words\"\"\"\ndef text_process_2(text):\n    text=nlp(text)\n    text=[token.lemma_ if token.lemma_ != \"-PRON-\" else token.lower_ for token in text ]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\" Remove stopword & punctuation & single character\"\"\"\ndef text_process_3(text):\n    # Check characters to see if they are in punctuation then remove them\n    text=''.join([char for char in text if char not in string.punctuation])\n    # Remove stopword and single character \n    text = [word for word in word_tokenize(text) if word not in stop_words and len(word )>1]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\"autocorrect\"\"\"\ndef text_process_4(text):\n    # spell check autocorrect\n    text=[spell_check(w) for w in text.split() ]\n    # Again\n    # Remove stopword and single character if generated\n    text = [word for word in text if word not in stop_words and len(word )>1]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\"Detect number in word if present and remove Eg: five, three  \"\"\"\n\"\"\" using spacy \"\"\"\ndef text_process_5(text):\n    text = nlp(text)\n    text = [token.text for token in text if token.pos_ != 'NUM'  ]\n    #text = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token.text for token in text]\n    return ' '.join(text)\n\n","c1cbeb85":"msg=\" ...18u..  \u00e2\u00f1 don't <h1>HELLO!!<\/h1> the??\/ him he functions fna is a great  going go 66s ain\u2019t wif ac acc early Available otw fiev hundrade \"\nmsg1=text_process_1(msg)\nmsg2=text_process_2(msg1)\nmsg3=text_process_3(msg2)\nmsg4=text_process_4(msg3)\nmsg5=text_process_5(msg4)\n\n\nprint(msg1)\nprint(msg2)\nprint(msg3)\nprint(msg4)\nprint(msg5)","0fc4c583":"# text_process_1 : remove accented characters, to lower case, Remove numerical\nmessages[\"text\"] = messages[\"text\"].apply(text_process_1)\n","9c538270":"# text_process_2 : token & lemmatizer & Expand Contractions words\nmessages[\"text\"] = messages[\"text\"].apply(text_process_2)","49578ee5":"# text_process_3 : Remove stopword & punctuation & single character\nmessages[\"text\"] = messages[\"text\"].apply(text_process_3)","7866b0cc":"# text_process_4 : autocorrect words\nmessages[\"text\"] = messages[\"text\"].apply(text_process_4)","0957e0f4":"# text_process_5 : Detect number in word if present and remove Eg: five, three\nmessages[\"text\"] = messages[\"text\"].apply(text_process_5)","c13ffc6e":"messages.head()","52fb1b41":"messages.isnull().sum()","8aa4ba99":"messages[\"category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), \n                                     autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","890f5d0c":"# lets see on bar graph\nmessages[\"category\"].value_counts().plot(kind = 'bar')\nplt.show()","150043ca":"plt.figure(figsize=(10,5))\nplt.hist(messages['msg_len'], bins=40)\nplt.show()","2ee2bd62":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == \"spam\"][\"msg_len\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word Length\")\n\nsns.distplot(messages[messages[\"category\"] == \"ham\"][\"msg_len\"], bins = 20, ax = ax[1])\nax[1].set_xlabel(\"Ham Message Word Length\")\n\nplt.show()","3100c7d8":"plt.figure(figsize=(10,5))\nplt.hist(messages['word_count'], bins=40)\nplt.title(\"word count on messgaes\")\nplt.show()","798ea344":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == \"spam\"][\"word_count\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word count\")\n\nsns.distplot(messages[messages[\"category\"] == \"ham\"][\"word_count\"], bins = 20, ax = ax[1])\nax[1].set_xlabel(\"Ham Message Word count\")\n\nplt.show()","943ddc81":"# function to draw the wordCloud from the text msg-paragraph \ndef show_word_cloud(Msg):\n  text=' '\n  for words in Msg:\n    text+=\" \"+words\n\n  #word cloud\n  wordcloud = WordCloud(width=600, \n                        height=400,\n                        background_color = 'black'\n                        ).generate(text.lower())\n  plt.figure( figsize=(10,8),\n             facecolor='k')\n  plt.imshow(wordcloud, interpolation = 'bilinear')\n  plt.axis(\"off\")\n  plt.tight_layout(pad=0)\n  plt.show()\n  del text","b85c66d8":"show_word_cloud(messages['text'].values)","253688f0":"# function to return the top 10 common words with freq\ndef feature_bow(msg):\n    cv=CountVectorizer()\n    bow=cv.fit_transform(msg)\n    features_df=pd.DataFrame(bow.toarray(), columns=cv.get_feature_names())\n    words = cv.get_feature_names()\n    feature_df = pd.DataFrame(\n        data =list(zip(words, features_df[words].sum())),\n        columns = ['feature','freq']\n        )\n    #sort the df according to freq\n    feature_df.sort_values(by='freq',ascending=False, inplace=True)\n    feature_df.reset_index(drop=True, inplace=True)\n    # most occuring 10 words\n    return feature_df.head(10)\n    ","8cce373f":"feature_freq=feature_bow(messages['text'])\nfeature_freq","c1ab3bcb":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from whole dataset\")\nplt.show()","2f6ec174":"#  Lets study individual Spam\/ham words\nspam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\nham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\nprint(f\"spam len:{len(spam_messages)}\")\nprint(f\"ham len:{len(ham_messages)}\")\nprint(f\"spam+ham: {len(spam_messages)+len(ham_messages)}\")\nprint(f\"total len:{messages.shape[0]}\")","cf685769":"show_word_cloud(spam_messages)","2b594ff1":"feature_freq=feature_bow(spam_messages)\nfeature_freq\n","f5ac166f":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from spam_messages\")\nplt.show()\n    ","113560b0":"show_word_cloud(ham_messages)","8a6960b7":"feature_freq=feature_bow(ham_messages)\nfeature_freq\n","0cb92f38":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from ham_messages\")\nplt.show()\n    ","e91ac512":"**Apply Text Process**","09406620":"## **Load DataSet**","644c0702":" let's see the msg length from both spam and ham msg","251bad61":"Number of words counts for each msg","6bec8d1c":"##  ---- To be continue ---","56df4169":"Thus, 86.6% of data are spam and remaining 13.4% are only spam","a7523a65":"## **BASIC Text Cleaning**\n\n\n1. Remove tags, accented_chars and white space.\n2. Expand Contractions\n3. Make text all lower case\n4. Remove punctuation and numerical values\n5. Remove common non-sensical text (\/n)\n6. Tokenize text, Lemmatize \/ Stemming text\n7. Spell check\n8. Remove stop words","08c53ae5":"**let's see ham_messages**","fb8dcfe6":"## **Liberary Import**","3ecaf242":"## EDA","7e17db86":"**Let's see Spam Messages**","76068eb4":"msg word counts for both spam and ham ","be2a2786":"show the msg_len ","ef073c7b":" let's know how many of the data are spam and ham","8190e862":"**Let's see the WordCloud diagream**","8e6bf3ef":"Thus, \n1. In spam message the are call, free, txt words present mostly\n2. In ham message there are get, go , come  words present mostly","b06d90e2":"Reference:\n1. https:\/\/blog.ekbana.com\/pre-processing-text-in-python-ad13ea544dae [pre-process ekbana]\n2. https:\/\/towardsdatascience.com\/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79 [Pre-Process towardsdatascience]"}}