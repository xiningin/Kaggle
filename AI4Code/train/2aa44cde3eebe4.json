{"cell_type":{"95a1bd0b":"code","9478fad8":"code","93e22280":"code","0d56ad75":"code","5eb1c927":"code","f58b0864":"code","b6af3fe1":"code","fc115319":"code","f5745658":"code","c997a8db":"code","6a700986":"code","c32b87bd":"code","e2aa5724":"code","ddad97b7":"code","2829ca97":"code","6ff4b699":"code","4c7e3bd2":"code","e15f1bf6":"code","55e616bc":"code","9b6dba7d":"code","882b986b":"code","182d50e6":"code","1afb81ce":"code","355631ac":"code","80be65bf":"code","2565f237":"code","a48a8a73":"code","b9156702":"code","482ea5c7":"code","a6d11e5e":"code","3babaab0":"code","b1916f53":"code","804dc758":"code","d8307545":"code","9379f6e6":"code","3aff4a81":"code","99031a56":"code","cd953327":"code","135a444d":"code","a885c001":"code","4613ddb1":"code","a2439540":"code","20a640bd":"code","d22a5886":"code","0bf7f8d6":"code","1c901c67":"code","88c44701":"code","8c2c916d":"markdown","a4a74a59":"markdown","c2006252":"markdown","3e8b3d41":"markdown","07e5b335":"markdown","18b50a61":"markdown","ce3bd8bf":"markdown","88ffc11b":"markdown"},"source":{"95a1bd0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9478fad8":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, VotingClassifier\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_roc_curve\n\ntrain_main_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_main_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_df=train_main_df\ntest_df=test_main_df","93e22280":"def AgetoNum(df):\n    AgeChange={'Unknown': 0, 'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\n    for key, item in AgeChange.items():\n        row_indexes=df[df['Age']==key].index\n        df.loc[row_indexes,'NewAge']=item\n    df.Age=df.NewAge\n    df.drop(['NewAge'], axis=1, inplace=True)\n    df.Age=df.Age.astype(int)\n    \n##################\n#from Scikit-Learn ML from Start to Finish Kernel for titanic: Machine Learning From Disaster\n\ndef simplify_ages(df):\n    \n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n################\n\ndef feature_importance(estimator, ax, title):\n    \n    feat_importances = pd.Series(estimator.feature_importances_, index=X.columns)\n    feat_importances.nlargest(20).plot(kind='barh', ax=ax, title=title)\n    plt.title(title)","0d56ad75":"train_df.info()","5eb1c927":"test_df.info()","f58b0864":"#Deck column code is from : Kaggle Notbook: Titanic - Advanced Feature Engineering TutorialDeck\n\n#Deck column\ntrain_df['Deck'] = train_df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntest_df['Deck'] = test_df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n####################################################################################\n#New feature: Class_Deck\ntrain_df['Class_Deck']=train_df['Pclass'].astype(str) + train_df['Deck']\ntest_df['Class_Deck']=test_df['Pclass'].astype(str) + test_df['Deck']\n\n####################################################################################\n#Once we have used Cabin to extract Deck, Cabin can be dropped. \n\ntrain_df.drop(['Cabin'], axis=1, inplace=True)\ntest_df.drop(['Cabin'], axis=1, inplace=True)\n\n####################################################################################\n#Passenger ID could be treated as index as they all have unique numbers. Will have no bearing on prediction so dropping it. \n#Name also has no influence on the prediction model. Droping Name too. Same goes for Ticket\n\ntrain_df.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)\ntest_df.drop(['Name','Ticket'], axis=1, inplace=True)\n\n####################################################################################\n\n#Embarked\n#Droping these two Null entried from train_df. \n\ntrain_df.dropna(subset=['Embarked'], inplace=True)\n\n##################################################################################\n\n#Age\ntrain_df[train_df.Age.isnull()]\ntest_df[test_df.Age.isnull()]\n\n#Replacing Age with categories ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n\ntrain_df.Age.fillna(- 0.5, inplace=True)\ntest_df.Age.fillna(- 0.5, inplace=True)\n\ntrain_df=simplify_ages(train_df)\ntest_df=simplify_ages(test_df)\n\n#################################################################################\n","b6af3fe1":"#Lets see how fare is distributed:\n\nsns.distplot(train_df.Fare, kde=False, rug=True);\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Frequency\")\n#Fare is left skewed, with majority of the data is between 0 and 100.  ","fc115319":"#box plot of Pclass and Fare for train_df and test_df\nfig, axes=plt.subplots(1,2, figsize=(10,6))\nsns.boxplot(x='Pclass',y='Fare',data=train_df, ax=axes[0])\nsns.boxplot(x='Pclass',y='Fare',data=test_df, ax=axes[1])\n\n\n#Looks like there are a few outliers in the fare. Those might be the reason to skew data a bit. BUT I am going to keep them for now as I think those\n#might be helpful in not overfitting the model on the training data ","f5745658":"#How is class related to Fare and did males paid different in each class from woman \nplt.figure(figsize=(8,6))\nsns.boxplot(x='Pclass',y='Fare', hue='Sex',data=train_df)\n\n#Females paid more in fare in Pclass1 then males. The median price females paid in Pclass1 is around 90 and \n# median price for males in the same class is around 50\n\n","c997a8db":"#Visualizing pairwise relationship between variables in train_df\n\nsns.pairplot(train_df, height=2)","6a700986":"#MORE SNS PLOTS","c32b87bd":"sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = train_df)\n#As seen before, Females in all three class have survived more than man. The most survived people were in Class 1. ","e2aa5724":"sns.barplot(y = 'Class_Deck', x = 'Survived', order=['1A','1B','1C','1D','1E','1M','2D','2E','2F','2M', '3E','3F','3G','3M','1T'], ci=None, data = train_df)\n","ddad97b7":"#How many males and females were on each Deck\ntemp=train_df[['Sex','Deck', 'Pclass']]\n\nfig, ax = plt.subplots(figsize=(5,5))\n\n\n#temp.groupby(['Deck','Sex']).count()['Pclass'].unstack()\ntemp=temp.groupby(['Deck','Sex']).count()['Pclass'].unstack().plot(ax=ax, kind='bar', colormap='RdBu',alpha=0.7)\n\n#The following graph shows that the missing information on the deck might be very crucial in our prediction model since many people are missing that data\n#More of male data for the deck information is missing than for the females. \n","2829ca97":"#How many total people per class survived or died?\ntemp=train_df[['Deck', 'Pclass', 'Sex','Survived']]\n\n#temp.groupby(['Deck','Pclass']).count().unstack()\nfig, ax = plt.subplots(figsize=(20,10))\n\n\n#temp.groupby(['Deck','Sex']).count()['Pclass'].unstack()\ntemp.groupby(['Deck','Pclass','Survived'])['Sex'].count().unstack(['Deck','Survived']).plot(ax=ax, kind='bar')\nplt.ylim([0,180]) \n#chaning ylim to see more clearly where majority of the people were in different classes.\n\n#The most people died are from class3 (M,0). (M,1) looks large however, combining all survivors in class 1 makes it largest class to survive. \n#This is proven in next graph\n\n","6ff4b699":"temp=train_df[['Pclass', 'Sex','Survived']]\n\n#temp.groupby(['Deck','Pclass']).count().unstack()\nfig, ax = plt.subplots(figsize=(8,8))\n\n\n#temp.groupby(['Deck','Sex']).count()['Pclass'].unstack()\ntemp.groupby(['Pclass','Survived'])['Sex'].count().unstack(['Survived']).plot(ax=ax, kind='bar')\n\n#Most survival was in class 1\n#Most deaths were in class 3","4c7e3bd2":"train_df.shape, test_df.shape","e15f1bf6":"##################### Preprocessing \n\nenc = preprocessing.LabelEncoder()\ndf_combined=pd.concat([train_df, test_df])\nenc = enc.fit(df_combined['Sex'])\ntrain_df['Sex'] = enc.transform(train_df['Sex'])\ntest_df['Sex'] = enc.transform(test_df['Sex'])\n#Changing the Age Labels (Categories) to values as the Classifiers can't work with categorical values\nAgetoNum(train_df)\nAgetoNum(test_df)\n\n#########################\n","55e616bc":"enc = preprocessing.LabelEncoder()\ndf_combined=pd.concat([train_df, test_df])\nenc = enc.fit(df_combined['Class_Deck'])\ntrain_df['Class_Deck'] = enc.transform(train_df['Class_Deck'])\ntest_df['Class_Deck'] = enc.transform(test_df['Class_Deck'])","9b6dba7d":"enc = preprocessing.LabelEncoder()\ndf_combined=pd.concat([train_df, test_df])\nenc = enc.fit(df_combined['Embarked'])\ntrain_df['Embarked'] = enc.transform(train_df['Embarked'])\ntest_df['Embarked'] = enc.transform(test_df['Embarked'])","882b986b":"#Since EDA is done I can get rid of Deck here as well.\ntrain_df.drop(['Deck'], axis=1, inplace=True)\ntest_df.drop(['Deck'], axis=1, inplace=True)","182d50e6":"##### Modeling ######","1afb81ce":"X=train_df.drop(['Survived'], axis=1)\ny=train_df['Survived']\n\nX_hold=test_df.drop(['PassengerId'], axis=1)\n#y_hold -> predict model for this??\n\nX.shape, X_hold.shape, y.shape","355631ac":"X_hold['Fare'].fillna(method='ffill', inplace=True)\nX_hold.info()","80be65bf":"### K Nearest Neighbors Classifiers\n\nparam_grid={'n_neighbors': np.arange(1,50)}\n\nknn=KNeighborsClassifier()\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n\nknn_cv=GridSearchCV(knn,param_grid)\n\nknn_cv.fit(X_train, y_train)\ny_pred=knn_cv.predict(X_test)\n\n","2565f237":"knn_score_test=knn_cv.score(X_test, y_test)\n#knn_score_hold=knn_cv.score(X_hold, y_pred_hold)\nknn_score_test","a48a8a73":"knn_cv.best_score_, knn_cv.best_params_, knn_cv.best_estimator_","b9156702":"#LogisticRegression\n\n\nparam_grid={'penalty':['l1','l2'], \n            'C': np.arange(0.1,10),\n           'solver' : ['liblinear']}\nlogReg=LogisticRegression()\n\n\n\nlogReg_cv=GridSearchCV(logReg,param_grid,cv=5)\n\nlogReg_cv.fit(X_train, y_train)\ny_pred=logReg_cv.predict(X_test)\n\nlogReg_score=logReg_cv.score(X_test, y_test)","482ea5c7":"logReg_cv.best_params_, logReg_cv.best_score_, logReg_cv.best_estimator_","a6d11e5e":"#Random Forest Classifier\nparam_grid = [\n    {\n    'n_estimators' : list(range(10,101,10)),\n    'max_features': list(range(6,8,2)),\n    'max_depth'    : [2, 3, 5, 10], \n    } \n]\n\nRF=RandomForestClassifier()\nRF_cv=GridSearchCV(RF, param_grid, cv=5)\nRF_cv.fit(X_train, y_train)\ny_pred=RF_cv.predict(X_test)\n\nRF_score=RF_cv.score(X_test, y_test)\n","3babaab0":"RF_score, RF_cv.best_score_, RF_cv.best_estimator_, RF_cv.best_params_,","b1916f53":"#Support Vector Machine Classifier\n\n\nparam_grid = {'C': [0.1,1,10, 100], \n              'gamma': [1,0.1,0.01,0.001]}\n\nsvm_clf=svm.SVC()\nsvm_clf_cv=GridSearchCV(svm_clf,param_grid)\n\nsvm_clf_cv.fit(X_train, y_train)\ny_pred=svm_clf_cv.predict(X_test)\n\nsvm_score=svm_clf_cv.score(X_test, y_test)","804dc758":"svm_clf_cv.score(X_test, y_test), svm_clf_cv.best_params_","d8307545":"#Gradient Boosting Classifier \n\n\nparam_grid=[{'n_estimators':list(range(20,100,10))}]\ngb=GradientBoostingClassifier()\n\ngb_cv=GridSearchCV(gb,param_grid)\ngb_cv.fit(X_train, y_train)\ny_pred=gb_cv.predict(X_test)\ngb_cv_score=gb_cv.score(X_test, y_test)\n\n","9379f6e6":"gb_cv_score,gb_cv.best_params_, gb_cv.best_score_","3aff4a81":"#AdaBoost Classifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nparam_grid=[{'n_estimators':list(range(20,100,10))}]\n\n\nab=AdaBoostClassifier()\nab_cv=GridSearchCV(ab,param_grid)\nab_cv.fit(X_train, y_train)\ny_pred=ab_cv.predict(X_test)\n\n","99031a56":"ab_cv.best_score_, ab_cv.best_estimator_","cd953327":"#SVM classifier with scaled values\n\n\n\nsteps=[('scalar', StandardScaler()),\n      ('svm', svm.SVC())]\n\nparameters={'svm__kernel':['rbf', 'linear'],\n           'svm__C':[1,10,100,1000],\n           'svm__gamma':[1e-3,1e-4]}\n\npipeline=Pipeline(steps)\n\nsvm_tuned_cv=GridSearchCV(pipeline, param_grid=parameters)\n\nsvm_tuned_cv.fit(X_train, y_train)\ny_pred=svm_tuned_cv.predict(X_test)\n","135a444d":"svm_tuned_score=svm_tuned_cv.score(X_test,y_test)\nsvm_tuned_score, svm_tuned_cv.best_score_, svm_tuned_cv.best_params_,","a885c001":"#Ensemble VotingClassifer - Not sure what this will do but trying here as it is supposedly better option :) \n\n#vclass=VotingClassifier(estimators=[('rf',RF_cv), ('ab',ab_cv)], voting='hard') #0.805\n#vclass=VotingClassifier(estimators=[('knn',knn_cv), ('ab',ab_cv), ('rf',RF_cv)], voting='soft', weights=[4,1,5])\n#vclass=VotingClassifier(estimators=[('gb',gb_cv),('rf',RF_cv), ], voting='soft', weights=[1,2,1])\n\n\n\n#Several combinations between estimators were chosen and ran with voting classifier, this combination gave best result\nvclass=VotingClassifier(estimators=[('rf',RF_cv), ('gb',gb_cv)], voting='hard') #.808\nvclass.fit(X_train,y_train)\ny_pred=vclass.predict(X_test)","4613ddb1":"vclass.score(X_test, y_test)","a2439540":"#ROC Curves\n# I am not including votingclassifier results here as they are clearly not greater than RF, or GB or AB\n\nknn_disp=plot_roc_curve(knn_cv, X_test, y_test, label=\"KNN\")\nlogreg_disp=plot_roc_curve(logReg_cv, X_test, y_test, ax=knn_disp.ax_, label=\"LogReg\")\nrfc_disp= plot_roc_curve(RF_cv, X_test, y_test, ax=knn_disp.ax_, label=\"RandomForest\")\nsvm_disp= plot_roc_curve(svm_clf_cv, X_test, y_test, ax=knn_disp.ax_, label=\"SVM\")\ngb_disp=plot_roc_curve(gb_cv, X_test, y_test,ax=knn_disp.ax_, label='GradientBoost')\nab_disp=plot_roc_curve(ab_cv, X_test, y_test,ax=knn_disp.ax_, label='AdaBoost')\nsvm_tuned_disp=plot_roc_curve(svm_tuned_cv, X_test, y_test,ax=knn_disp.ax_, label='SVM - ScaledValues')\n\n\nsvm_tuned_disp.figure_.suptitle(\"ROC curve comparison\")","20a640bd":"#Accuracy Scores\n\nmy_dict={'KNN':'{:.2f}'.format(knn_cv.best_score_*100), \n         'LogReg':'{:.2f}'.format(logReg_cv.best_score_*100),\n         'RandomForest': '{:.2f}'.format(RF_cv.best_score_*100), \n         'SVM':'{:.2f}'.format(svm_clf_cv.best_score_*100), \n         'Gradient Boosting': '{:.2f}'.format(gb_cv.best_score_*100),\n         'AdaBoost': '{:.2f}'.format(ab_cv.best_score_*100),\n         'SVM - Scaled': '{:.2f}'.format(svm_tuned_cv.best_score_*100)\n         \n        }\nscore_df=pd.DataFrame(list(my_dict.items()),\n                      columns=['Model','Best_Score'])\n\n","d22a5886":"score_df.sort_values(by='Best_Score', ascending=False)","0bf7f8d6":"#Feature Importances of RF, AB and GB. \n\nfig, axes=plt.subplots(3,1, figsize=(8,7))\nfeature_importance(RF_cv.best_estimator_, axes[0], 'RF_cv')\nfeature_importance(ab_cv.best_estimator_, axes[1], 'AB_cv')\nfeature_importance(gb_cv.best_estimator_, axes[2], 'GB_cv')\nplt.tight_layout()\n\n\n\n\n#Predictor 'Sex' has most importance in RF and GB. AB gives most importance to Fare (perhaps bias towards continous variable?)","1c901c67":"#Decided to use the best scored classifier i.e. Random Forest. (RF_cv)\n\ny_pred_RF=RF_cv.predict(X_hold)\n\n\nPassengerid = test_df['PassengerId']\n\nfinal = pd.DataFrame(y_pred_RF,columns=['Survived'])\n\nsubmit = pd.concat([Passengerid, final], axis=1, sort=False)\nsubmit.to_csv('sub1.csv', header=True, index=False)\nsubmit\n","88c44701":"#Second Submission with GB\n\ny_pred_GB=gb_cv.predict(X_hold)\n\n\nPassengerid = test_df['PassengerId']\n\nfinal2 = pd.DataFrame(y_pred_GB,columns=['Survived'])\n\nsubmit2 = pd.concat([Passengerid, final2], axis=1, sort=False)\nsubmit2.to_csv('sub2.csv', header=True, index=False)\n\nsubmit2","8c2c916d":"### Classification","a4a74a59":"### Exploratory Data Analysis ","c2006252":"#### First Submission on Kaggle\n#### I went through many Kaggle notebooks and learned from them and applied to my model.\n#### I would love to get feedback and any tips and tricks that have worked for others","3e8b3d41":"#### The End","07e5b335":"#### Lastly just for fun here is how each forest based method gave importance to different features. ","18b50a61":"#### Data Wrangling \n* From .info() from both dfs, I can see there is a significant number of values missing from Cabin column. A quick search on google shows that alot of information was misplaced and\/or added later after Titanic Sunk by talking to surviovors. Not very reliable. \n* Also, from [wikipedia](https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic#:~:text=All%20three%20of%20the%20Olympic,which%20the%20lifeboats%20were%20housed) I can see that there were total of 10 decks on the Titanic. A was exclusive to first class passengers. B, C, D were majority of first\/sec and a few third class; E, F were mostly for third class but other classes were on those decks too. \n\n* I will be taking info from Cabin col i.e. first letter of the Cabin to determine the Deck of the person. Since alot of this information is missing, I will be adding M to those rows in the Deck col. I will then combine pclass and deck to create another feature Pclass_Deck. \n\n* Fare col has one NaN in test data. Dropping that row.\n\n* Similarly \"embarked\" have 2 NaN values in train data; I will be dropping those two rows\n\n* Age column has 177 NaN values in train data and 86 NaN in test data. In order to not lose alot of information, I will categorize the ages into ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']. Then for the classification, I will replace each category with a number since sklearn classifiers can't work with categorical variables. \n\n* Lastly PassengerId, Name and Ticket columns will be removed as survival dependency on those three is not very high\n","ce3bd8bf":"### Variable Description\n\n* PassengerId: unique id number to each passanger\n* Survived: passenger survive(1) or died(0)\n* Pclass: passenger class\n* Name: name\n* Sex: gender of passenger\n* Age: age of passenger\n* SibSp: number of siblings\/spouses\n* Parch: number of parents\/children\n* Ticket: ticket number\n* Fare: amount of money for ticket\n* Cabin: cabin category\n* Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","88ffc11b":"#########################################################################################################################\n#########################################################################################################################"}}