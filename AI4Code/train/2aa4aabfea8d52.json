{"cell_type":{"ce428323":"code","509641bd":"code","08fee3f2":"code","016e8236":"code","614b80e3":"code","8c08b148":"code","31c71ac6":"code","866694ea":"code","07c9c460":"code","bbe4fc4c":"code","95ec3e75":"code","79ba3677":"code","6cbf8109":"code","0a7a6dd1":"code","61bcc3cf":"code","796b9ba2":"code","6ad47e13":"code","663fe44b":"code","e477d062":"code","82f69e97":"code","b359d554":"code","435e5f82":"code","5d831cf2":"code","3aedc73d":"code","90cfef85":"markdown","3330ac77":"markdown","f072df88":"markdown","e957af6f":"markdown","bb2f2fd2":"markdown","0cd96366":"markdown","5ea90d0b":"markdown","a4e3d2be":"markdown","e7e37673":"markdown","0cf03240":"markdown"},"source":{"ce428323":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n\nsns.set(style=\"whitegrid\")","509641bd":"# Leitura dos datasets:\ndataset = pd.read_csv('..\/input\/dataset_5secondWindow%5B1%5D.csv')\ndataset.head()","08fee3f2":"# Verifica\u00e7\u00e3o de valores nulos:\ndataset.info()","016e8236":"# Verifica\u00e7\u00e3o de linhas duplicadas:\ndataset.duplicated().sum()","614b80e3":"# Excluir as vari\u00e1veis de desvio padr\u00e3o:\nto_drop = [c for c in dataset.columns if '#std' in c]\ndataset.drop(to_drop, axis=1, inplace=True)\ndataset.head()","8c08b148":"# Renomear as colunas para nomes mais leg\u00edveis:\ndataset.columns = dataset.columns.str.replace('android.sensor.','').str.replace('#','_')\ndataset.head()","31c71ac6":"# Verifica\u00e7\u00e3o da distribui\u00e7\u00e3o das vari\u00e1veis restantes por sensor:\n\nplt.figure(figsize=(10,15))\n\nplt.subplot(4,1,1)\nsns.distplot(dataset.iloc[:,0])\nplt.xlabel('Time')\n\nplt.subplot(4,1,2)\nfor i in range(1,4):\n    sns.distplot(dataset.iloc[:,i])\nplt.legend(dataset.iloc[:,1:4].columns)\nplt.xlabel('Accelerometer')\n\nplt.subplot(4,1,3)\nfor i in range(4,7):\n    sns.distplot(dataset.iloc[:,i])\nplt.legend(dataset.iloc[:,4:7].columns)\nplt.xlabel('Gyroscope')\n\nplt.subplot(4,1,4)\nfor i in range(7,10):\n    sns.distplot(dataset.iloc[:,i])\nplt.legend(dataset.iloc[:,7:10].columns)\nplt.xlabel('Sound')\n\nplt.show()","866694ea":"# Exclus\u00e3o das vari\u00e1veis sound_min e sound_max, que apresentaram correla\u00e7\u00e3o m\u00e1xima com sound_mean:\ndataset.drop(['sound_max','sound_min'],axis=1,inplace=True)\ndataset.head()","07c9c460":"# Verifica\u00e7\u00e3o dos valores das vari\u00e1veis restantes atrav\u00e9s de um boxplot:\n\nplt.figure(figsize=(17,4))\n\nplt.subplot(141)\nplt.boxplot(dataset.iloc[:,0])\nplt.xlabel('Time')\n\nplt.subplot(142)\nplt.boxplot([dataset.iloc[:,1],dataset.iloc[:,2],dataset.iloc[:,3]])\nplt.xlabel('Accelerometer')\n\nplt.subplot(143)\nplt.boxplot([dataset.iloc[:,4],dataset.iloc[:,5],dataset.iloc[:,6]])\nplt.xlabel('Gyroscope')\n\nplt.subplot(144)\nplt.boxplot(dataset.iloc[:,7]);\nplt.xlabel('Sound')\n\nplt.show()","bbe4fc4c":"# Verifica\u00e7\u00e3o da representatividade da vari\u00e1vel target no dataset:\nsns.barplot(x=dataset.target.value_counts().index,y=dataset.target.value_counts(),color=\"salmon\",saturation=.5);","95ec3e75":"# Separa\u00e7\u00e3o do Dataset (tamb\u00e9m ser\u00e3o utilizados em todos os modelos posteriores a este):\nX = dataset.drop(['target'],axis=1)\ny = dataset.target\n\n# Separa\u00e7\u00e3o de datasets para treino e teste:\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)","79ba3677":"# Desempenho do algor\u00edtmo KNN, medido pela acur\u00e1cia, em fun\u00e7\u00e3o do hiperpar\u00e2metro k:\n\nplt.figure(figsize=(8,5))\n\n# Valores de K\nK = range(1,11)\n\n# Teste dos modelos\nscores = []\nfor k in K:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train,y_train)\n    scores.append(knn.score(X_test,y_test))\n\n# Resultados\nsns.lineplot(K, scores)\nplt.ylabel('Score')\nplt.xlabel('K')\nplt.show()","6cbf8109":"# Classificador de Decis\u00e3o Bayesiana:\n\nclass DecisaoBayesiana:\n    \n    def fit(self, train_x, train_y):\n        from scipy.stats import multivariate_normal\n        import numpy as np\n        import pandas as pd\n        \n        if type(train_x) != np.ndarray:\n            self.train_x = train_x.values\n        if type(train_y) != np.ndarray:\n            self.train_y = train_y.values\n        \n        return self\n        \n    def score(self, test_x, test_y):\n        from sklearn.metrics import accuracy_score\n        from scipy.stats import multivariate_normal\n        \n        if type(test_x) != np.ndarray:\n            test_x = test_x.values\n        if type(test_y) != np.ndarray:\n            test_y = test_y.values\n        \n        classes = y_train.unique()\n        train_x = self.train_x\n        train_y = self.train_y\n        P = pd.DataFrame(data=np.zeros((test_x.shape[0], len(classes))), columns=classes)\n        \n        for i in np.arange(0, len(classes)):\n            elements = tuple(np.where(train_y == classes[i]))\n            Z = train_x[elements,:][0]\n            m = np.mean(Z, axis = 0)\n            cv = np.cov(np.transpose(Z))\n            for j in np.arange(0,test_x.shape[0]):\n                x = test_x[j,:]\n                pj = multivariate_normal.pdf(x, mean=m, cov=cv)\n                P[classes[i]][j] = pj\n        pred_y = []\n        \n        for i in np.arange(0, test_x.shape[0]):\n            c = np.argmax(np.array(P.iloc[[i]]))\n            pred_y.append(classes[c])\n\n        return accuracy_score(test_y, pred_y)","0a7a6dd1":"# Sele\u00e7\u00e3o de modelos a serem testados:\nmodels = [\n    KNeighborsClassifier(n_neighbors=1),\n    GaussianNB(),\n    DecisaoBayesiana()\n]","61bcc3cf":"# Desempenho dos algoritmos testados, medido pela acur\u00e1cia:\nscores = []\nfor model in models:\n    model.fit(X_train,y_train)\n    scores.append(model.score(X_test,y_test))\n\n# Resultados\nsns.barplot(x=[type(m).__name__ for m in models], y=scores, color=\"salmon\", saturation=.5);\nplt.ylabel('Score')\nplt.show()","796b9ba2":"# Desempenho dos modelos testados, medido pela acur\u00e1cia, com o mesmo conjunto de dados com diferentes pr\u00e9-processamentos:\n\n# Normaliza\u00e7\u00e3o (min=0 max=1)\nX_norm = MinMaxScaler().fit_transform(X)\n\n# Padroniza\u00e7\u00e3o (mean=0 std=1)\nX_padr = StandardScaler().fit_transform(X)\n\n# Teste dos modelos\nscores = []\nfor X_currrent,dataset_name in zip([X, X_norm, X_padr],['Sem processamento','Normalizados','Padronizados']):\n    X_current_train, X_current_test, y_current_train, y_current_test = train_test_split(X_currrent,y,random_state=42)\n    for model in models:\n        model.fit(X_current_train,y_current_train)\n        score = model.score(X_current_test,y_current_test)\n        scores.append({'Model':type(model).__name__, 'dataset':dataset_name,'score':score})\nscores_df = pd.DataFrame(scores)\n\n# Resultados\nplt.figure(figsize=(10,6))\nsns.barplot(x=scores_df.dataset, y=scores_df.score.values, hue=scores_df.Model);\nplt.xlabel('')\nplt.show()","6ad47e13":"# Transforma\u00e7\u00e3o de target para vari\u00e1vel num\u00e9rica:\ndataset_ex6 = dataset.copy()\ndataset_ex6.target = LabelEncoder().fit_transform(dataset_ex6.target)\ndataset_ex6.sample(5)","663fe44b":"# Correla\u00e7\u00f5es com target:\ncorrelations = dataset_ex6.corr()\ncorrelations[['target']]","e477d062":"# Matriz de correla\u00e7\u00e3o entre novas as vari\u00e1veis:\n\nX_low_corr = dataset_ex6.drop(['target'],axis=1)\ny_low_corr = dataset_ex6.target\n\nX_low_corr = X_low_corr[correlations[correlations.target<0.4].index]\n\nplt.figure(figsize=(5,4))\nplt.title('Matriz de vari\u00e1veis de correla\u00e7\u00e3o < 0.5 com Target')\nsns.heatmap(X_low_corr.corr(), cmap='PuBu',annot=True)\n\nplt.show()","82f69e97":"# Normaliza\u00e7\u00e3o (min=0 max=1)\nX_low_corr_norm = MinMaxScaler().fit_transform(X_low_corr)\n\n# Padroniza\u00e7\u00e3o (mean=0 std=1)\nX_low_corr_padr = StandardScaler().fit_transform(X_low_corr)\n\n# Teste dos modelos\nscores = []\nfor X_currrent,base in zip([X_low_corr, X_low_corr_norm, X_low_corr_padr],\n                           ['Sem processamento','Normalizados','Padronizados']):\n    X_current_train, X_current_test, y_current_train, y_current_test = train_test_split(X_currrent,y,random_state=42)\n    for model in models:\n        model.fit(X_current_train,y_current_train)\n        score = model.score(X_current_test,y_current_test)\n        scores.append({'Model':type(model).__name__, 'dataset':base,'score':score})\nscores_df_low_corr = pd.DataFrame(scores)\n\n# Resultado\nplt.figure(figsize=(20,6))\n\nax = plt.subplot(121)\nsns.barplot(x=scores_df.dataset, y=scores_df.score.values, hue=scores_df.Model);\nplt.xlabel('')\nplt.ylabel('Score')\nplt.title('Classifica\u00e7\u00e3o com todas as vari\u00e1veis')\n\nplt.subplot(122, sharey=ax)\nsns.barplot(x=scores_df_low_corr.dataset, y=scores_df_low_corr.score.values, hue=scores_df.Model);\nplt.xlabel('')\nplt.ylabel('Score')\nplt.title('Classifica\u00e7\u00e3o com vari\u00e1ves menos correlacionadas')\n\nplt.show()","b359d554":"# Normaliza\u00e7\u00e3o\n\n# Ru\u00eddo","435e5f82":"# Desempenho dos modelos gausianos, medido pela acur\u00e1cia, com o mesmo conjunto de dados com diferentes pr\u00e9-processamentos:\n\n# Normaliza\u00e7\u00e3o (min=0 max=1)\nX_norm = MinMaxScaler().fit_transform(X) \n                                         \n# Sele\u00e7\u00e3o de modelos gausianos\ngausian_models = [\n    GaussianNB(),\n    MultinomialNB(),\n    BernoulliNB()\n]\n\n# Teste dos modelos\nscores = []\nfor X_currrent,base in zip([X, X_norm],['Sem processamento','Normalizados']):\n    X_current_train, X_current_test, y_current_train, y_current_test = train_test_split(X_currrent,y,random_state=42)\n    for model in gausian_models:\n        model.fit(X_current_train,y_current_train)\n        score = model.score(X_current_test,y_current_test)\n        scores.append({'Model':type(model).__name__, 'dataset':base,'score':score})\nscores_df = pd.DataFrame(scores)\n\n# Resultado\nplt.figure(figsize=(10,6))\nsns.barplot(x=scores_df.dataset, y=scores_df.score.values, hue=scores_df.Model);\nplt.xlabel('')\nplt.show()","5d831cf2":"# Desempenho dos modelo KNN, medido pela acur\u00e1cia, em fun\u00e7\u00e3o do hiperpar\u00e2metro k, com diferentes dist\u00e2ncias:\n\nplt.figure(figsize=(15,7))\n\n# Ks a serem utilizados pela curva\nK = range(1,11)\n\n# Dist\u00e2ncias\nmetrics = ['euclidean','manhattan','chebyshev']\n\n# Testes dos modelos\nfor m in metrics:\n    scores = []\n    for k in K:\n        knn = KNeighborsClassifier(n_neighbors=k, metric=m)\n        knn.fit(X_train,y_train)\n        scores.append(knn.score(X_test,y_test))\n    sns.lineplot(K, scores)\n    \nfor p in [1.5,3,5]:\n    scores = []\n    for k in K:\n        knn = KNeighborsClassifier(n_neighbors=k,metric='minkowski',p=p)\n        knn.fit(X_train,y_train)\n        scores.append(knn.score(X_test,y_test))\n    metrics.append('minkowski, p={}'.format(p))\n    sns.lineplot(K, scores)\n\n# Resultados\nplt.legend(metrics)\nplt.ylabel('Score')\nplt.xlabel('K')\nplt.show()","3aedc73d":"# Desempenho dos modelos testados, medido pela acur\u00e1cia, em fun\u00e7\u00e3o do matanho do dataset de treinamento:\n\nplt.figure(figsize=(8,5))\n\n# Valores para test_size\nP = [i\/10 for i in range(1,10)]\n\n# Teste dos modelos\nfor model in models:\n    scores = []\n    for p in P:\n        X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(X,y,random_state=42,test_size=p)\n        model.fit(X_p_train,y_p_train)\n        scores.append(model.score(X_p_test,y_p_test))\n    sns.lineplot(P, scores)\n\n# Resultados\nplt.legend([type(m).__name__ for m in models])\nplt.ylabel('Score')\nplt.xlabel('Test Size')\nplt.show()","90cfef85":"### 7\t\u2013 Verifique\tqual\tdos\tclassificadores\t\u00e9\tmais\trobusto\tcom\trela\u00e7\u00e3o\t\u00e0\tpresen\u00e7a\tde\tru\u00eddos.\tPara\tisso:\t\n* Aplique\ta\tnormaliza\u00e7\u00e3o\tdos\tdados\tpara\tque\tos\tatributos\tapresentem\tm\u00e9dia\tigual\ta\tzero\te\tvari\u00e2ncia\tigual\ta\t1.\n* Inclua\tem\tX%\tdos\tatributos,\tum\tvalor\tnormalmente\tdistribu\u00eddo\tcom\tm\u00e9dia\t0\te\tvari\u00e2ncia 1. Considere\ttoda\ta\tmatriz\tdos\tdados,\tsorteando uma\tposi\u00e7\u00e3o\tda\tmatriz\tde\tforma\taleat\u00f3ria.\n\n* Varie\to\tn\u00edvel\tde\tru\u00eddo,\tde\t0\ta\t50%\t(em\tpassos\tde\t5%)\te\tavalie\tcomo\tmuda a\tclassifica\u00e7\u00e3o.\tConstrua\tum\tgr\u00e1fico\tde\tX% de\tru\u00eddo versus\tporcentagem\tde\tclassifica\u00e7\u00e3o\tcorreta.\tColoque\ta\tm\u00e9dia\te\to\tdesvio\tpadr\u00e3o\tcalculados\ta partir\tde\tao\tmenos\t10\tsimula\u00e7\u00f5es.\tConsidere\t70%\tdos\tdados\tno\tconjunto de\ttreinamento.\n* Discuta os\tresultados.","3330ac77":"### 10\t\u2013 Fa\u00e7a\tum\tgr\u00e1fico\tda\tfra\u00e7\u00e3o\tde\telementos\tno\tconjunto\tde\ttreinamento\t(10% at\u00e9\t90%\tem\tpassos\tde\t10%)\tversus\tacur\u00e1cia\tpara\tos\tclassificadores:\n* knn\t(melhor\tk\tobservado\tanteriormente)\n* Naive\tBayes\n* Decis\u00e3o\tBayesiana\n### Considere\tos\tcasos\tcom\te\tsem\tpadroniza\u00e7\u00e3o.","f072df88":"### 9 \u2013 No\tcaso\tdo\tKnn,\tcompare\tas\tclassifica\u00e7\u00f5es\tusando\tdiferentes\tm\u00e9tricas.\tVarie\tk\te\tmostre\tas\tcurvas\t(em\tum\tmesmo\tplot)\tpara\tas\tdist\u00e2ncias\teuclidiana,\tManhattan,\tChebyshev e\tMinkowsky\t(p=0.5,\tp=1.5,\tp\t=\t3).\n","e957af6f":"### 6\t\u2013 Mostre\ta\tmatriz\tde\tcorrela\u00e7\u00e3o\tentre\tos\tatributos.\tConsidere\tos\tatributos\tcom\tmenor orrela\u00e7\u00e3o\t(por\texemplo,\tmenor\tdo\tque\t0.5).\tRealize\ta\tclassifica\u00e7\u00e3o novamente\tapenas\tcom\tesses\tatributos.\tA acur\u00e1cia\tmelhora?","bb2f2fd2":"### 2 \u2013 Realize o pr\u00e9-processamento dos\tdados:\nVerifque se h\u00e1 NaN\tou\toutros\terros no dados.<br>Selecione  apenas os atributos relevantes\te num\u00e9ricos.","0cd96366":"### 1 \u2013 Considere\ta\tbase\tde\tdados:\nhttps:\/\/www.kaggle.com\/fschwartzer\/tmd-dataset-5-seconds-sliding-window","5ea90d0b":"### 5 \u2013 Verifique\to\tefeito\tda\tnormaliza\u00e7\u00e3o (atributos\tem\t[0,1])\te\tpadroniza\u00e7\u00e3o (atributos\tcom\tm\u00e9dia\t0\te\tvari\u00e2ncia\t1)\tdos\tdados.\tCompare\tos\tcasos\tsem\tprocessamento,\tcom\tpadroniza\u00e7\u00e3o\te\tcom\tnormaliza\u00e7\u00e3o\tpara\tos\tclassificadores:\n* knn\t(melhor\tk\tobservado\tno\titem\tanterior)\n* Naive\tBayes\n* Decis\u00e3o\tBayesiana","a4e3d2be":"### 3 \u2013 No\tcaso\tdo\tclassificador\tKnn,\tverifique\to\tefeito\tdo\tpar\u00e2metro\tk\tna\tclassifica\u00e7\u00e3o.","e7e37673":"### 8 \u2013 No\tcaso\tdo\tclassificador\tNaive\tBayes,\t\u00e9\tposs\u00edvel\tconsiderar\tdiferentes fun\u00e7\u00f5es\tpara\testimar\tas\tprobabilidades.\tCompare\tos\tcasos:\n1. Gaussian\tNaive Bayes\n2. multinomial\tNaive\tBayes\n3. Bernoulli\tNaive\tBayes.\n#### Considere os casos\tcom\te\tsem\tpadroniza\u00e7\u00e3o.","0cf03240":"### 4 \u2013 Compare\tos classificadores:\n* knn (melhor\tk\tobservado\tno\titem\tanterior)\n* Naive\tBayes\n* Decis\u00e3o\tBayesiana"}}