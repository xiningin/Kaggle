{"cell_type":{"10dc65d0":"code","d0b39179":"code","3ca44d34":"code","56e1a69b":"code","0a8ae226":"code","54fa3f62":"code","b106788c":"code","b7a9f3bf":"code","19acaf1f":"markdown","b7c248eb":"markdown"},"source":{"10dc65d0":"import requests\nimport numpy as np\nimport plotly.graph_objects as go\nfrom ipywidgets import interact","d0b39179":"EPISODE_ID = 7604169\n\nurl = f\"https:\/\/www.kaggleusercontent.com\/episodes\/{EPISODE_ID}.json\"\nreplay = requests.get(url).json()","3ca44d34":"!pip install --upgrade kaggle_environments\nimport kaggle_environments","56e1a69b":"env = kaggle_environments.make(\n    \"mab\",\n    configuration=replay[\"configuration\"],\n    steps=replay[\"steps\"],\n    info=replay[\"info\"]\n)\n\n# # or instead of a replay you could simulate a new episode, e.g:\n# env.run([\"\/kaggle\/input\/santa-2020\/submission.py\", \"round_robin\"])\n\nprint(\"rewards:\", [agent.reward for agent in env.state])","0a8ae226":"def make_traces(env, data, step):\n    \n    initial_thresholds = env.steps[0][0][\"observation\"][\"thresholds\"]\n    current_thresholds = env.steps[step][0][\"observation\"][\"thresholds\"]\n    actions = [s[\"action\"] for s in env.steps[step]]\n    rewards = [s[\"reward\"] for s in env.steps[step]]\n\n    if step > 0:\n        last_rewards = [s[\"reward\"] for s in env.steps[step - 1]]\n    else:\n        last_rewards = [0, 0]\n        \n    success = [1 if rewards[i] > last_rewards[i] else 0 for i in range(2)]\n    \n    text_offset = 5 if step < 85 else 2\n    red = \"red\"\n    blue = \"blue\"\n\n    traces = [\n        go.Scatter(\n            name=\"agent0_pulls\",\n            x=list(range(100)),\n            y=data[\"action_histogram\"][0, step],\n            mode=\"markers\",\n            marker=dict(symbol=\"line-ew\", line=dict(width=1, color=blue)),\n        ),\n        go.Scatter(\n            name=\"agent1_pulls\",\n            x=list(range(100)),\n            y=data[\"action_histogram\"][1, step],\n            mode=\"markers\",\n            marker=dict(symbol=\"line-ew\", line=dict(width=1, color=red)),\n        ),\n        go.Scatter(\n            name=\"agent0_line\",\n            x=[actions[0]] * 2,\n            y=[0, 108],\n            mode=\"lines\",\n            line=dict(color=blue, dash=\"solid\" if success[0] else \"dot\", width=1),\n        ),\n        go.Scatter(\n            name=\"agent1_line\",\n            x=[actions[1]] * 2,\n            y=[0, 122],\n            mode=\"lines\",\n            line=dict(color=red, dash=\"solid\" if success[1] else \"dot\", width=1),\n        ),\n        go.Scatter(\n            name=\"agent_indicators\",\n            x=actions,\n            y=[108, 122],\n            text=[\"0\", \"1\"],\n            textposition=\"top left\",\n            textfont=dict(color=[blue, red]),\n            mode=\"markers+text\",\n            marker=dict(\n                color=[blue, red],\n                size=16,\n                symbol=[\"arrow-down\" if s else \"arrow-down-open\" for s in success],\n            ),\n        ),\n        go.Bar(\n            name=\"initial_thresholds\",\n            x=list(range(100)),\n            y=initial_thresholds,\n            marker_line_width=0,\n            marker_color=\"grey\",\n        ),\n        go.Scatter(\n            name=\"current_thresholds\",\n            x=list(range(100)),\n            y=current_thresholds,\n            mode=\"markers\",\n            marker_line_width=0,\n            marker_color=\"orange\"\n        ),\n        go.Bar(\n            name=\"rewards\",\n            x=[r \/ 8 for r in rewards],\n            y=[180, 195],\n            text=rewards,\n            textposition=\"auto\",\n            marker_line_width=1.5,\n            marker_line_color=[blue, red],\n            marker_color=[blue, red],\n            orientation=\"h\",\n        ),\n        go.Scatter(\n            name=\"team_names\",\n            x=[r \/ 8 + text_offset for r in rewards],\n            y=[180, 195],\n            text=env.info.get(\"TeamNames\", [\"\"] * 2),\n            textposition=\"bottom right\",\n            textfont=dict(color=[blue, red]),\n            mode=\"text\",\n        ),\n        go.Bar(\n            name=\"expected_rewards\",\n            x=[r \/ 8 for r in data[\"expected_rewards\"][:, step]],\n            y=[150, 165],\n            text=[\"{:.2f}\".format(x) for x in data[\"expected_rewards\"][:, step]],\n            textposition=\"auto\",\n            textfont=dict(color=[blue, red]),\n            marker_line_width=1.5,\n            marker_line_color=[blue, red],\n            marker_color=\"silver\",\n            orientation=\"h\",\n        ),\n        go.Scatter(\n            name=\"expected_label\",\n            x=[r \/ 8 + text_offset for r in data[\"expected_rewards\"][:, step]],\n            y=[150, 165],\n            text=\"(expected)\",\n            textposition=\"bottom right\",\n            textfont=dict(color=[blue, red]),\n            mode=\"text\",\n        ),\n    ]\n    \n    return traces","54fa3f62":"def make_figure_widget(env, n_traces=30):\n\n    fig = go.FigureWidget()\n\n    fig.update_xaxes(\n        range=[-2, 102],\n        zeroline=False,\n        showgrid=False,\n    )\n    fig.update_yaxes(\n        range=[-2, 208],\n        zerolinecolor=\"white\",\n        showgrid=False,\n    )\n    fig.update_layout(\n        showlegend=False,\n        plot_bgcolor=\"silver\",\n        autosize=False,\n        width=900,\n        height=400,\n        margin=dict(l=10, r=10, b=10, t=30, pad=1),\n    )\n    \n    # create a couple extra datasets\n    action_histogram = np.zeros((2, 2000, 100), dtype=int)\n    expected_rewards = np.zeros((2, 2000), dtype=float)\n    for step_idx, step in enumerate(env.steps):\n        if step_idx == 0:\n            continue\n        for agent_idx, agent in enumerate(step):\n            action = agent[\"action\"]\n            thresholds = env.steps[step_idx - 1][0][\"observation\"][\"thresholds\"]\n            expected_reward = np.ceil(thresholds[action]) \/ 101\n            action_histogram[agent_idx, step_idx:, action] += 1\n            expected_rewards[agent_idx, step_idx:] += expected_reward\n            \n    data = {\n        \"action_histogram\": action_histogram,\n        \"expected_rewards\": expected_rewards,\n    }\n\n    fig.add_traces(make_traces(env, data, 0))\n\n    return fig, data","b106788c":"fig, data = make_figure_widget(env)","b7a9f3bf":"@interact(step=(0, len(env.steps) - 1))\ndef interactive_display(step=0):\n    fig.update(data=make_traces(env, data, step))\nfig","19acaf1f":"![visualiser-screenshot.png](attachment:visualiser-screenshot.png)","b7c248eb":"## Santa 2020 replay visualizer\n\nThis notebook sets up an interactive plotly widget that can be used to visualise episodes. The widget shows:\n\n- initial thresholds\n- current thresholds\n- current agent moves and whether they were successful\n- cumulative agent rewards\n- expected rewards (i.e. adding up the probabilities of success for each pull)\n- cumulative agent pulls for each lever\n\nThe visualizer requires a running notebook to render each step. To use it, plug in an episode ID below, run all notebook cells, and then use the slider in the final cell to step through the episode."}}