{"cell_type":{"febe2c42":"code","842d5497":"code","5f926767":"code","eaccda0c":"code","f51dd20f":"code","b0c3c786":"code","40ee2af2":"code","ef516e49":"code","38087cbc":"code","39ce0e69":"code","f74cd6fd":"code","05b2788a":"code","e707d0db":"code","736d2082":"code","8e525bee":"code","85a9ad44":"code","27e97de8":"code","2e7cb4b0":"code","a0643e4b":"code","a0ad6916":"code","aa0decc1":"code","aa1ce664":"code","a2466181":"code","b12a5f26":"code","bab4ddcb":"code","d4a6fe7f":"code","eabdbd7b":"code","f8b270e8":"code","8af346dd":"code","37997bbd":"code","a7bf4c62":"code","7cb11290":"code","b46d29ac":"code","3d100594":"code","24ad2ffc":"code","e8b86302":"code","7991b02e":"code","7cb33a40":"code","01bc5dde":"code","49c7d8c5":"code","5fb5b770":"code","e9612fb1":"code","3368d66a":"code","af44f85d":"code","68dc9715":"code","e2e57d76":"code","fe85ec4a":"code","d59b979c":"code","f1d3ba13":"code","5634ef01":"code","58109346":"code","ebb18121":"code","d3b506b3":"code","895e3ca1":"code","d07c654d":"code","d93ba824":"code","11250ab2":"code","6947891d":"code","f208b61c":"code","3b7f0127":"code","01cda5fd":"code","43ca7b06":"code","f1205aa4":"code","5af5c40a":"code","d6d717fb":"code","33714ec9":"code","f96828e5":"code","58456ca9":"code","d29117d6":"markdown","70234782":"markdown","d9395fd4":"markdown","9746aa36":"markdown","121e66bb":"markdown","ef176525":"markdown","8889d037":"markdown","c59db624":"markdown","5cdf6b5d":"markdown","86f38799":"markdown","a585c11c":"markdown","473cea18":"markdown","d2741112":"markdown","56bd5594":"markdown","363eca6d":"markdown","1e7abffa":"markdown","de732266":"markdown","35246852":"markdown","41e542b4":"markdown","4c48d630":"markdown","6b9a164b":"markdown","3243f3df":"markdown","09b99451":"markdown","fa3d2ab4":"markdown","ec2d35db":"markdown","a2900227":"markdown","285e0f7e":"markdown","ca1a4317":"markdown","32528037":"markdown","cefbfff3":"markdown","ca339c7e":"markdown","c6f79fbd":"markdown","61e51284":"markdown","dbaf7c2c":"markdown","6965b940":"markdown","4cdd1f07":"markdown","d29f04a8":"markdown","73656592":"markdown","572fdbda":"markdown","7da4d832":"markdown","35364268":"markdown"},"source":{"febe2c42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","842d5497":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nsns.set_style('darkgrid')\n\nfrom termcolor import cprint      # For making colorful printing texts","5f926767":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier, plot_importance\n\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.preprocessing import StandardScaler","eaccda0c":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","f51dd20f":"display(train.head())\ndisplay(test.head())\ndisplay(submission.head())","b0c3c786":"cprint(\"Shape of the train set :\", 'green')\ncprint('*'*25, 'green')\nprint('\\nShape of the train set:', train.shape)\n\ncprint(\"\\nShape of the test set :\", 'green')\ncprint('*'*25, 'green')\nprint('\\nShape of the test set:', test.shape)\n\ncprint(\"\\nShape of the Submission set :\", 'green')\ncprint('*'*25, 'green')\nprint('\\nShape of the Submission set:', submission.shape)","40ee2af2":"cprint(\"Dtypes in train :\", 'green')\ncprint('*'*25, 'green')\nprint('\\n', train.info())\ncprint(\"Dtypes in test :\", 'green')\ncprint('*'*25, 'green')\nprint('\\n', test.info())","ef516e49":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","38087cbc":"train.columns","39ce0e69":"train.target.unique()","f74cd6fd":"!pip install dataprep","05b2788a":"from dataprep.eda import *\nfrom dataprep.eda import plot","e707d0db":"# plots the distribution of each column and calculates dataset statistics\nplot(train)","736d2082":"# plots the distribution of column x in various ways and calculates column statistics\nplot(train, 'target')","8e525bee":"# generates plots depicting the relationship between columns x and y\nplot(train, 'feature_72', 'feature_73')","85a9ad44":"from dataprep.eda import plot_correlation","27e97de8":"plot_correlation(train)","2e7cb4b0":"# plots the most correlated columns to column \"target\"\nplot_correlation(train, 'feature_1')","a0643e4b":"from dataprep.eda import plot_missing","a0ad6916":"plot_missing(train)","aa0decc1":"create_report(train)","aa1ce664":"cprint(\"Missing Values in train :\", 'green')\ncprint('*'*25, 'green')\nprint(train.isnull().sum().sort_values(ascending=False))\ncprint(\"\\nMissing Values in test :\", 'green')\ncprint('*'*25, 'green')\nprint(test.isnull().sum().sort_values(ascending=False))","a2466181":"print('\\nMissing values in train:', train.isnull().sum().sum())\nprint('\\nMissing values in train:',test.isnull().sum().sum())","b12a5f26":"train.describe().T.style.bar(subset=['mean'], color='#20c8f2')\\\n                   .background_gradient(subset=['std'], cmap='YlGn')","bab4ddcb":"plt.figure(figsize=(18,25))\nsns.boxplot(data=train, orient=\"h\");","d4a6fe7f":"plt.figure(figsize=(18,25))\nsns.boxplot(data=test.iloc[:,1:], orient=\"h\");","eabdbd7b":"# Pearson Correlation\nplt.figure(figsize=(18,10))\nsns.heatmap(train.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","f8b270e8":"# Spearman Correlation\nplt.figure(figsize=(24,15))\nsns.heatmap(train.corr(method='spearman'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","8af346dd":"fig, ax = plt.subplots(figsize=(18, 12))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nax.text(-1.1, -0.7, 'Correlation between the Features', fontsize=20, fontweight='bold', fontfamily='serif')\nsns.heatmap(corr, mask=mask, annot=False, fmt='.2f', linewidth=0.2, cbar=True, cmap='coolwarm');","37997bbd":"# kendall\nfig, ax = plt.subplots(1, 3, figsize=(17 , 5))\n\nfeature_lst = ['feature_0', 'feature_1', 'feature_2','feature_3','feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9']\n\ncorr = train[feature_lst].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nfor idx, method in enumerate(['pearson', 'kendall', 'spearman']):\n    sns.heatmap(train[feature_lst].corr(method=method), ax=ax[idx],\n            square=True, annot=True, fmt='.1f', center=0, linewidth=2,\n            cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            mask=mask\n           ) \n    ax[idx].set_title(f'{method.capitalize()} Correlation', loc='left', fontweight='bold')     \n\nplt.show()","a7bf4c62":"train.skew()","7cb11290":"test.skew()","b46d29ac":"features = [feature for feature in train.columns if feature not in ['id', 'target']]\nfeatures = features[:36]","3d100594":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew \")\nsns.distplot(train[features].skew(),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].skew(),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","24ad2ffc":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis \")\nsns.distplot(train[features].kurtosis(),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].kurtosis(),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","e8b86302":"plt.figure(figsize=(10,7))\nsns.countplot(x=\"target\", data=train);","7991b02e":"import plotly.graph_objects as go\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=train.target, hole=.3)])\nfig.show()","7cb33a40":"feature_cols = [col for col in train.columns if col != \"target\"]\ntarget_cat = train[\"target\"]\ndf = train.drop(\"target\", axis=1)","01bc5dde":"df.head()","49c7d8c5":"le = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])\n#target = le.fit_transform(target_cat)","5fb5b770":"print(\"-\"*30)\nprint(\"Before label encoding, \")\nprint(target_cat[:10])\nprint(\"-\"*30)\nprint(\"After label encoding, \")\nprint(train['target'][:10])\nprint(\"-\"*30)","e9612fb1":"X = train.drop('target', axis = 1)\ny = train['target']","3368d66a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","af44f85d":"X_train = X_train.applymap(lambda p: np.log(p+1))\nX_test = X_test.applymap(lambda p: np.log(p+1))","68dc9715":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","e2e57d76":"from sklearn.decomposition import PCA\npca = PCA().fit(X_train)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 76, step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 75, step=2)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","fe85ec4a":"xgb_params= {'n_estimators': 25000, \n             'max_depth': 8, \n             'learning_rate': 0.0320, \n             'reg_lambda': 29.326, \n             'subsample': 0.918, \n             'colsample_bytree': 0.235, \n             'colsample_bynode': 0.820, \n             'colsample_bylevel': 0.453}","d59b979c":"def cross_val(X, y, model, params, folds=9):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400,                    # verbose =400 --> every 400 iterations it will tell logloss score; verbose = False --> No output\n                eval_metric='mlogloss')\n\n        pred = alg.predict_proba(x_test)\n        loss = log_loss(y_test, pred)\n        print(f\"Log loss: {loss}\")\n        print(\"-\"*50)\n    \n    return alg","f1d3ba13":"xgb_model = cross_val(X, y, XGBClassifier, xgb_params)","5634ef01":"y_pred_xgb1 = xgb_model.predict_proba(X_test)\ny_pred_xgb  = xgb_model.predict(X_test)\nxgb_acc     = accuracy_score(y_test, y_pred_xgb)\n\ny_pred_xgb_test = xgb_model.predict_proba(test)","58109346":"submission[['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = y_pred_xgb_test\nsubmission.to_csv(f'xgb.csv',index=False)","ebb18121":"print(\"Accuracy : Train Score {:.2f} & Test Score {:.2f}\".format(xgb_model.score(X_train, y_train), xgb_model.score(X_test, y_test)))\ncprint('-'*70, 'green')\nprint(\"ROC_AUC_Score: \", roc_auc_score(y_test, y_pred_xgb1, multi_class = 'ovr'))\nprint(\"\\n\\nConfusion matrix: \\n\\n\",confusion_matrix(y_test, y_pred_xgb))\ncprint('-'*70, 'green')\nprint(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, y_pred_xgb))","d3b506b3":"plt.figure(figsize=(9,7))\nsns.heatmap(confusion_matrix(y_test, y_pred_xgb), fmt='.1f', annot=True, square=True, linewidth=0.2, cbar=False);","895e3ca1":"xgb_model.get_params(deep=True)","d07c654d":"xgb_model.get_xgb_params()","d93ba824":"xgb_model.feature_importances_","11250ab2":"# To have even better plot, let\u2019s sort the features based on importance value:\n\nplt.figure(figsize=(12,18))\nsorted_idx = xgb_model.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb_model.feature_importances_[sorted_idx]);\nplt.title(\"Xgboost Feature Importance\", size=20)\nplt.show()","6947891d":"# Plot_tree allows to visualize the trees that were built by XGBoost\nplt.figure(figsize=(35,40))\nxgb.plot_tree(xgb_model, ax=plt.gca());","f208b61c":"LGB = LGBMClassifier(random_state=42, use_label_encoder=True)\nLGB.fit(X_train, y_train)","3b7f0127":"y_pred_LGB = LGB.predict(X_test)\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred_LGB))","01cda5fd":"print(\"XGBoost : Train Score {:.2f} & Test Score {:.2f}\".format(LGB.score(X_train, y_train), LGB.score(X_test, y_test)))\nprint('-'*70)\nprint(\"\\n\\nConfusion matrix \\n\\n\",confusion_matrix(y_test, y_pred_LGB))\nprint('-'*70)\nprint(\"\\n\\nClassification Report\\n\\n\",classification_report(y_test, y_pred_LGB))","43ca7b06":"plt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix(y_test, y_pred_LGB), fmt='.1f', annot=True, linewidth=0.2, square=True, cbar=False);","f1205aa4":"# plt.rcParams[\"figure.figsize\"] = (12, 22)\n# lightgbm.plot_importance(lgbm_model, max_num_features = 60, height=.9)\n\nplot_importance(LGB, figsize=(15, 19));","5af5c40a":"y_pred_LGB_test = LGB.predict(X_test)","d6d717fb":"rfc = RandomForestClassifier(max_depth=10, min_samples_split=9)\nrfc.fit(X_train, y_train)","33714ec9":"y_pred_rfc = rfc.predict(X_test)\nrf_acc = accuracy_score(y_test, y_pred_rfc)","f96828e5":"print(\"Random Forest : Train Score {:.2f} & Test Score {:.2f}\".format(rfc.score(X_train, y_train), rfc.score(X_test, y_test)))\nprint('-'*70)\nprint(\"\\n\\nConfusion matrix \\n\\n\",confusion_matrix(y_test, y_pred_rfc))\nprint('-'*70)\nprint(\"\\n\\nClassification Report\\n\\n\",classification_report(y_test, y_pred_rfc))","58456ca9":"plt.figure(figsize=(15,18))\nfeature_imp = pd.Series(rfc.feature_importances_, index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\", size=20)\nplt.legend()\nplt.show()","d29117d6":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 5.1) Principal Component Analysis (PCA) <\/h1>","70234782":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 6) LightAutoML <\/h1>","d9395fd4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.3) Outliers <\/h1>","9746aa36":"import shap\n\nshap_tree = shap.TreeExplainer(xgb_model)\n\nshap_values = shap_tree.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test)","121e66bb":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1.2) Analyze correlations with plot_correlation() <\/h1>\n\n- The function **plot_correlation()** explores the correlation between columns in various ways and using multiple correlation metrics. It generates correlation matrices using Pearson, Spearman, and KendallTau correlation coefficients\n\n- **plot_correlation(df):** plots correlation matrices (correlations between all pairs of columns)\n\n- **plot_correlation(df, x):** plots the most correlated columns to column x\n\n- **plot_correlation(df, x, y):** plots the joint distribution of column x and column y and computes a regression line","ef176525":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.4) Relation between Features <\/h1>","8889d037":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom sklearn.metrics import log_loss\nfrom lightautoml.dataset.roles import NumericRole","c59db624":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 5.3) LGBM Classifier <\/h1>","5cdf6b5d":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.4.1) The correlation between the continuos variables <\/h1>\n\na. Pearson Correlation\n\nb. Spearman Correlation\n\nc. kendall","86f38799":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 5.5) CatBoost Classifier <\/h1>","a585c11c":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) Load Required Libraries <\/h1>","473cea18":"* [1) Load Required Libraries](#1)\n\n* [2) Read Data](#2)\n\n* [3) EDA (Exploratory Data Analysis)](#3)\n\n  * [3.1) DataPrep (AutoEDA)](#3.1)\n  \n    * [3.1.1) Analyze distributions with plot()](#3.1.1)\n    \n    * [3.1.2) Analyze correlations with plot_correlation()](#3.1.2)\n    \n    * [3.1.3) Analyze missing values with plot_missing()](#3.1.3)\n    \n    * [3.1.4) Create a profile report with create_report()](#3.1.4)\n  \n  * [3.2) Missing Values](#3.2)\n  \n  * [3.3) Outliers](#3.3)\n  \n  * [3.4) Relation between Features](#3.4)\n\n* [4) Data Preprocessing](#4)\n\n* [5) Model Building and Evaluation](#5)\n\n  * [5.1) XGBoost Classifier](#5.1)\n  \n  * [5.2) LGBM Classifier](#5.2)\n  \n  * [5.3) Random Forest Classifier](#5.3)\n\n* [6) LightAutoML](#6)","d2741112":"submission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv('lightautoml_2lvl_8hours_with_nn_oofs.csv', index = False)","56bd5594":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Data Preprocessing <\/h1>","363eca6d":"model = CatBoostClassifier(iterations = 4000, reg_lambda=100, learning_rate = 0.02,          # task_type = 'GPU'\n                           bootstrap_type='Bernoulli', random_strength = 5, depth = 8,\n                           loss_function='MultiClass')\n\nmodel.fit(X_train, y_train)","1e7abffa":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1.3) Analyze missing values with plot_missing() <\/h1>\n\n- The function **plot_missing()** enables thorough analysis of the missing values and their impact on the dataset. The following describes the functionality of plot_missing() for a given dataframe df.\n\n- **plot_missing(df):** plots the amount and position of missing values, and their relationship between columns (\u201cI want to understand the missing values of the dataset\u201d)\n\n- **plot_missing(df, x):** plots the impact of the missing values in column x on all other columns\n\n- **plot_missing(df, x, y):** plots the impact of the missing values from column x on column y in various ways.","de732266":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntrain_data[TARGET_NAME] = train_data[TARGET_NAME].str.slice(start=6).astype(int) - 1\ntrain_data.head()","35246852":"columns = ['PREDS_Linear_' + str(i) for i in range(1, 10)] + \\\n          ['PREDS_LGBM_' + str(i) for i in range(1, 10)] + \\\n          ['PREDS_CB_' + str(i) for i in range(1, 10)] + \\\n          ['PREDS_NN_' + str(i) for i in range(1, 10)]\ncolumns","41e542b4":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Submission <\/h1>","4c48d630":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.4.3) Skewness and Kurtosis <\/h1>","6b9a164b":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 4.1) target <\/h1>","3243f3df":"%%time\n\ntask = Task('multiclass',)","09b99451":"test_data = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\ntest_data.head()","fa3d2ab4":"%%time\n\ntest_pred = automl.predict(test)\nprint('Prediction for test set:\\n{}\\nShape = {}'.format(test_pred[:5], test_pred.shape))","ec2d35db":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 5.2) XGBoost Classifier <\/h1>","a2900227":"#### How to deal with Skewness in data?\n\nFor skewness the best way is to handle it by standardizing data by applying **Transformation, Scaling or both Transformation and Scaling**.\n\n##### I have used Logarithmic Transformation in combination with Standard Scaler (between 0-1) to standardize my data.","285e0f7e":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Table of Contents <\/h1>","ca1a4317":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 5.4) Random Forest Classifier <\/h1>","32528037":"%%time \n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id'],\n    NumericRole(np.float32, prob = True): columns\n}\n\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {\n                           'use_algos': [['lgb_tuned', 'cb_tuned']],\n                       },\n                       tuning_params = {'max_tuning_time': 1800},\n                       reader_params = {'n_jobs': N_THREADS},\n                       #configs_list = ['..\/input\/lightautoml-configs\/conf_1_sel_type_1.yml'],\n                       max_runs_per_config=1\n                       )\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","cefbfff3":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) EDA (Exploratory Data Analysis) <\/h1>","ca339c7e":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.2) Missing Values <\/h1>","c6f79fbd":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) Read Data <\/h1>","61e51284":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1.4) Create a profile report with create_report() <\/h1>\n\n- The function **create_report()** generates a comprehensive profile report of the dataset. create_report() combines the individual components of the dataprep.eda package and outputs them into a nicely formatted HTML document. The document contains the following information:\n\n- **Overview:** detect the types of columns in a dataframe\n\n- **Variables:** variable type, unique values, distint count, missing values\n\n- **Quantile statistics** like minimum value, Q1, median, Q3, maximum, range, interquartile range\n\n- **Descriptive statistics** like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n\n- **Text analysis** for length, sample and letter\n\n- **Correlations:** highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n\n- **Missing Values:** bar chart, heatmap and spectrum of missing values","dbaf7c2c":"pip install -U lightautoml","6965b940":"### Data Description\n\n- The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the **category on an eCommerce product** given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\n- This competition dataset is **similar** to the **Tabular Playground Series - May 2021 dataset**, but with **increased observations, increased features, and increased class labels.**\n\n- Submissions are **evaluated using multi-class logarithmic loss**. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:","4cdd1f07":"submission","d29f04a8":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 8 * 3600 # Time in seconds for automl run\nTARGET_NAME = 'target'","73656592":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) Model Building and Evaluation <\/h1>","572fdbda":"submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\nsubmission.head()","7da4d832":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1.1) Analyze distributions with plot() <\/h1>\n\n- a) **The function plot()** explores the distributions and statistics of the dataset. The following describes the functionality of plot() for a given dataframe df.\n\n- b) **plot(df):** plots the distribution of each column and calculates dataset statistics (\u201cI want to see an overview of the dataset\u201d )\n\n- c) **plot(df, x):** plots the distribution of column x in various ways and calculates column statistics (\u201cI want to understand the column x\u201d)\n\n- d) **plot(df, x, y):** generates plots depicting the relationship between columns x and y. (\u201cI want to understand the relationship between x and y\u201d)","35364268":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1) DataPrep (AutoEDA) <\/h1>"}}