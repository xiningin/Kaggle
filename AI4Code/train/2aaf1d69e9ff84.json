{"cell_type":{"8b4678fe":"code","ebddc71c":"code","0842cefd":"code","43585273":"code","88632c43":"code","b8afae0a":"code","aff574d2":"code","ee8dca18":"code","a1871ab1":"code","0e8b38a9":"code","ab1ffe95":"code","6f1e7579":"code","a4898296":"code","60f1bf78":"code","f36d0cc3":"code","ce33d813":"code","6f88bbb3":"code","652a514c":"code","65ce81c0":"code","09da1842":"code","9e40e231":"code","d78eeb32":"code","3e122681":"code","a2b0f7df":"code","bcf0c4ff":"code","cd1203f7":"code","5bbfcdde":"code","2d25d63b":"code","8cfef833":"code","42a16490":"code","5c1d3c77":"code","fbe74dd9":"code","7e8c390c":"code","cdf5166a":"code","3a48491c":"code","dd0b609b":"code","d02f96be":"code","5f02f385":"code","091cf791":"code","99f8c1ad":"code","1ac29e69":"code","3d1ad90b":"code","4df0f451":"code","e89c67c3":"code","fe41a3b5":"code","480818fd":"code","4876928c":"code","ffdd71e1":"code","265369a3":"code","194e7e36":"code","e95ec950":"code","a5a3866b":"code","49ca5e18":"code","7352c5e3":"code","de9353c8":"code","802a2993":"code","c6853978":"code","02a69e90":"code","47d0d723":"code","e42ada1b":"code","a3c9bd69":"code","f703a545":"code","c353ac28":"code","83e60a03":"code","2af78e44":"code","32f1d7fd":"code","04c504d9":"code","5b798569":"code","53f96a38":"code","b7b55487":"code","4f2ab6b2":"code","cb846671":"code","f7ef0c78":"code","9aecb9b2":"code","7c83b486":"code","3e474f55":"code","b9436bea":"code","222ec8bd":"code","4a49575e":"code","11a8ac01":"code","4430efd5":"code","167b93a2":"code","13c56137":"code","7b4384f2":"code","591623b6":"code","bdaf3716":"code","0b69ed40":"code","91869ea3":"code","2371b747":"code","8678d84e":"code","da43f6b4":"code","bdd4643e":"code","5791b127":"code","65157765":"code","e1601332":"code","e3f14ccc":"code","e18f6a9e":"code","89d3e0a3":"code","0e1d0cb0":"code","8ba95538":"code","66261728":"code","5de35a9e":"code","8e1ff93f":"code","dcd540f4":"code","a6c6e052":"code","6df20a61":"code","d234e088":"code","548f79a8":"code","048fd167":"code","3d16fc86":"code","d91a182b":"code","570f19a3":"code","aadb23bf":"code","f3deef98":"code","fc795c31":"code","46cc1d2f":"code","ef10662e":"code","d88a862e":"code","1dbb714b":"code","b016d05a":"code","e200e6ff":"code","7e9ec6a5":"code","07a7af20":"code","c4013431":"code","fbc2c90b":"code","bbe6c2ac":"code","7d464b16":"code","36705563":"code","98951c02":"code","f010f249":"code","0e3c9b4c":"code","9e4b51dc":"code","950914a4":"code","86962614":"code","72a1f8f3":"code","1dfc9a6e":"code","6943f855":"code","7d1aab61":"code","a13dd882":"code","fb5bd2f9":"code","28e0980b":"code","01723aa1":"code","de70d02d":"code","a11f4680":"code","4e8789ab":"code","54c33290":"code","80c21e9a":"code","40a8e277":"code","70d31c3b":"code","b3628830":"code","1b324f19":"code","767b40f8":"markdown","1e77bd05":"markdown","521384b1":"markdown","236b204a":"markdown","62341dbc":"markdown","9646ef27":"markdown","d9a83c42":"markdown","4f6389c2":"markdown","1ef27db4":"markdown","7d73caa5":"markdown","430a3b93":"markdown","50591911":"markdown","7b8bc9c3":"markdown","99079661":"markdown","74777a13":"markdown","a1ab4f77":"markdown","d2fda587":"markdown","03907aa2":"markdown","2444d6f9":"markdown","af318802":"markdown","9b161718":"markdown","42fc7995":"markdown","f8fd7fb6":"markdown","3237c324":"markdown","d8b59402":"markdown","462e55be":"markdown","5a0ef80c":"markdown","1fb2bfdf":"markdown"},"source":{"8b4678fe":"#import libraries \nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport altair as alt  #Altair is a declarative statistical visualization library for Python\n\nimport statsmodels.api as sm \n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor","ebddc71c":"#ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0842cefd":"! pip install yfinance --upgrade --no-cache-dir","43585273":"import yfinance as yf\nbse = yf.download('^BSESN', start='2015-01-01', end='2020-06-30')\n#since our Textual Analysis dataset containing news from Times of India News Headlines is only till 30th June 2020. \n#So we will assume today is 29th June 2020 and tomorrow is 30th June 2020. And we have to predict the stock price ((high+low+close)\/3) and closing price of BSE index \n#for tomorrow 30th June 2020.\nunseenbse_data = yf.download('^BSESN', start='2020-06-30', end='2020-07-01')","88632c43":"bse.columns","b8afae0a":"unseenbse_data.columns","aff574d2":"bse.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Adj Close': 'adjclose', 'Volume': 'volume'}, inplace = True)","ee8dca18":"unseenbse_data.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Adj Close': 'adjclose', 'Volume': 'volume'}, inplace = True)","a1871ab1":"bse.info()","0e8b38a9":"unseenbse_data.info()","ab1ffe95":"bse.head()","6f1e7579":"bse.tail()","a4898296":"unseenbse_data.head()","60f1bf78":"bse.reset_index(inplace=True)","f36d0cc3":"bse.rename(columns={'Date': 'date'}, inplace = True)","ce33d813":"bse.head()","6f88bbb3":"unseenbse_data.reset_index(inplace=True)","652a514c":"unseenbse_data.rename(columns={'Date': 'date'}, inplace = True)","65ce81c0":"unseenbse_data.head()","09da1842":"bse['date'] = pd.to_datetime(bse['date'], format = '%Y%m%d')","9e40e231":"unseenbse_data['date'] = pd.to_datetime(unseenbse_data['date'], format = '%Y%m%d')","d78eeb32":"#before moving forward let us calculate first the actual price\nunseenbsedata_price = round((unseenbse_data['high'] + unseenbse_data['low'] + unseenbse_data['close'])\/ 3, 2)\nunseenbsedata_price  #actual price","3e122681":"def stock_weekmovingavg(wks, df):\n  dateclose_data = pd.DataFrame({'date': df['date'], 'close':df['close']})\n  dateclose_data.set_index('date', inplace=True)\n  num = wks * 5                                 #calculating the number of days in the week. 5 days because BSE is open for 5 days \/ week\n  dateclose_data['movingavg'] = dateclose_data['close'].rolling(window=num).mean().shift()\n  return dateclose_data.dropna()","a2b0f7df":"stock_weekmovingavg(4, bse).head()","bcf0c4ff":"stock_weekmovingavg(4, bse).plot()","cd1203f7":"altdata_fourweek = stock_weekmovingavg(4, bse)\naltdata_fourweek.reset_index(inplace=True)\naltdata_fourweek.rename(columns={list(altdata_fourweek)[0]:'date'}, inplace=True)","5bbfcdde":"alt.Chart(altdata_fourweek).mark_point().encode(\n    x='date',\n    y='movingavg'\n)","2d25d63b":"plotfourweek = altdata_fourweek.filter(['date', 'movingavg'], axis=1) #df.copy()\nplotfourweek.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_fourweek['date']))) \ndel plotfourweek['date']\nsm.graphics.tsa.plot_pacf(plotfourweek.values.squeeze())\nplt.show()","8cfef833":"stock_weekmovingavg(16, bse).head()","42a16490":"stock_weekmovingavg(16, bse).plot()","5c1d3c77":"altdata_sixteenweek = stock_weekmovingavg(16, bse)\naltdata_sixteenweek.reset_index(inplace=True)\naltdata_sixteenweek.rename(columns={list(altdata_sixteenweek)[0]:'date'}, inplace=True)","fbe74dd9":"alt.Chart(altdata_sixteenweek).mark_point().encode(\n    x='date',\n    y='movingavg'\n)","7e8c390c":"plotsixteenweek = altdata_sixteenweek.filter(['date', 'movingavg'], axis=1) #df.copy()\nplotsixteenweek.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_sixteenweek['date']))) \ndel plotsixteenweek['date']\nsm.graphics.tsa.plot_pacf(plotsixteenweek.values.squeeze())\nplt.show()","cdf5166a":"stock_weekmovingavg(52, bse).head()","3a48491c":"stock_weekmovingavg(52, bse).plot()","dd0b609b":"altdata_fiftytwoweek = stock_weekmovingavg(52, bse)\naltdata_fiftytwoweek.reset_index(inplace=True)\naltdata_fiftytwoweek.rename(columns={list(altdata_fiftytwoweek)[0]:'date'}, inplace=True)","d02f96be":"alt.Chart(altdata_fiftytwoweek).mark_point().encode(\n    x='date',\n    y='movingavg'\n)","5f02f385":"plotfiftytwoweek = altdata_fiftytwoweek.filter(['date', 'movingavg'], axis=1) #df.copy()\nplotfiftytwoweek.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_fiftytwoweek['date']))) \ndel plotfiftytwoweek['date']\nsm.graphics.tsa.plot_pacf(plotfiftytwoweek.values.squeeze())\nplt.show()","091cf791":"def rollingwindows(days, df):\n  data = df.filter(['date','open','high','low','close'], axis=1) #df.copy()\n  data.set_index('date', inplace=True)\n  rollingwindows_data = data.rolling(window=days).mean().shift()\n  return rollingwindows_data.dropna()","99f8c1ad":"rollingwindows(10, bse).head()","1ac29e69":"rollingwindows(10, bse).plot()","3d1ad90b":"altdata_tendays = rollingwindows(10, bse)\naltdata_tendays.reset_index(inplace=True)\naltdata_tendays.rename(columns={list(altdata_tendays)[0]:'date'}, inplace=True)","4df0f451":"alt.Chart(altdata_tendays).mark_point().encode(\n    x ='date',\n    y = 'close'\n)","e89c67c3":"rollingwindows(50, bse).head()","fe41a3b5":"rollingwindows(50, bse).plot()","480818fd":"altdata_fiftydays = rollingwindows(50, bse)\naltdata_fiftydays.reset_index(inplace=True)\naltdata_fiftydays.rename(columns={list(altdata_fiftydays)[0]:'date'}, inplace=True)","4876928c":"alt.Chart(altdata_fiftydays).mark_point().encode(\n    x='date',\n    y='close'\n)","ffdd71e1":"def boolean_shock(percent, df, col):\n  data = df.filter(['date', col], axis=1) #df.copy()\n  data.set_index('date', inplace=True)\n  data['percentchg'] = (data[col].pct_change()) * 100  #percentage change compare to previous volume using pct_change() function\n  data['shock'] = data['percentchg'].apply(lambda x: 1 if x >= percent else 0)\n  data.drop(col, axis = 1, inplace = True)\n  return data.dropna()","265369a3":"boolean_shock(10, bse, 'volume')","194e7e36":"altdata_volpercentchg = boolean_shock(10, bse, 'volume')\naltdata_volpercentchg.reset_index(inplace=True)\naltdata_volpercentchg.rename(columns={list(altdata_volpercentchg)[0]:'date'}, inplace=True)","e95ec950":"alt.Chart(altdata_volpercentchg).mark_point().encode(\n    x='date',\n    y='percentchg'\n)","a5a3866b":"plotvolpercentchg = altdata_volpercentchg.filter(['date', 'percentchg'], axis=1) #df.copy()\nplotvolpercentchg.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_volpercentchg['date']))) \ndel plotvolpercentchg['date']\nsm.graphics.tsa.plot_pacf(plotvolpercentchg.values.squeeze())\nplt.show()","49ca5e18":"boolean_shock(2, bse, 'close')","7352c5e3":"altdata_closepercentchg2 = boolean_shock(2, bse, 'close')\naltdata_closepercentchg2.reset_index(inplace=True)\naltdata_closepercentchg2.rename(columns={list(altdata_closepercentchg2)[0]:'date'}, inplace=True)","de9353c8":"alt.Chart(altdata_closepercentchg2).mark_point().encode(\n    x='date',\n    y='percentchg'\n)","802a2993":"plotclosepercentchg2 = altdata_closepercentchg2.filter(['date', 'percentchg'], axis=1) #df.copy()\nplotclosepercentchg2.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_closepercentchg2['date']))) \ndel plotclosepercentchg2['date']\nsm.graphics.tsa.plot_pacf(plotclosepercentchg2.values.squeeze())\nplt.show()","c6853978":"boolean_shock(10, bse, 'close')","02a69e90":"altdata_closepercentchg10 = boolean_shock(10, bse, 'close')\naltdata_closepercentchg10.reset_index(inplace=True)\naltdata_closepercentchg10.rename(columns={list(altdata_closepercentchg10)[0]:'date'}, inplace=True)","47d0d723":"alt.Chart(altdata_closepercentchg10).mark_point().encode(\n    x='date',\n    y='percentchg'\n)","e42ada1b":"plotclosepercentchg10 = altdata_closepercentchg10.filter(['date', 'percentchg'], axis=1) #df.copy()\nplotclosepercentchg10.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_closepercentchg10['date']))) \ndel plotclosepercentchg10['date']\nsm.graphics.tsa.plot_pacf(plotclosepercentchg10.values.squeeze())\nplt.show()","a3c9bd69":"def priceboolean_shock(percent, df):\n  df['date'] = pd.to_datetime(df['date'])\n  data = df.filter(['date', 'high', 'low','close'], axis=1) #df.copy()\n  data.set_index('date', inplace=True)\n  data['priceavg'] = (data['high'] + data['low'] + data['close']) \/ 3\n  data['shock'] = (data['priceavg'].pct_change()) * 100\n  data['shock'] = data['shock'].apply(lambda x: 1 if x >= percent else 0)\n  data.drop(['high', 'low', 'close'], axis = 1, inplace = True)\n  return data","f703a545":"priceboolean_shock(10, bse)","c353ac28":"altdata_pricepercentchg = priceboolean_shock(10, bse)\naltdata_pricepercentchg.reset_index(inplace=True)\naltdata_pricepercentchg.rename(columns={list(altdata_pricepercentchg)[0]:'date'}, inplace=True)","83e60a03":"alt.Chart(altdata_pricepercentchg).mark_point().encode(\n    x='date',\n    y='priceavg'\n)","2af78e44":"plotpricepercentchg = altdata_pricepercentchg.filter(['date', 'priceavg'], axis=1) #df.copy()\nplotpricepercentchg.index = pd.Index(sm.tsa.datetools.dates_from_range('2015', length=len(altdata_pricepercentchg['date']))) \ndel plotpricepercentchg['date']\nsm.graphics.tsa.plot_pacf(plotpricepercentchg.values.squeeze())\nplt.show()","32f1d7fd":"def reverseboolean_shock(percent, df, col):\n  data = df.filter(['date', col], axis=1) #df.copy()\n  data.set_index('date', inplace=True)\n  data = data.reindex(index=data.index[::-1])\n  data['percentchg'] = (data[col].pct_change()) * 100\n  data['shock'] = data['percentchg'].apply(lambda x: 1 if x > percent else 0)\n  data.drop(col, axis = 1, inplace = True)\n  data = data.reindex(index=data.index[::-1])\n  return data.dropna()","04c504d9":"reverseboolean_shock(2, bse, 'close')","5b798569":"altdata_closepercentchg = reverseboolean_shock(2, bse, 'close')\naltdata_closepercentchg.reset_index(inplace=True)\naltdata_closepercentchg.rename(columns={list(altdata_closepercentchg)[0]:'date'}, inplace=True)","53f96a38":"alt.Chart(altdata_closepercentchg).mark_point().encode(\n    x='date',\n    y='percentchg'\n)","b7b55487":"reverseboolean_shock(5, bse, 'close')","4f2ab6b2":"altdata_closepercentchg5 = reverseboolean_shock(5, bse, 'close')\naltdata_closepercentchg5.reset_index(inplace=True)\naltdata_closepercentchg5.rename(columns={list(altdata_closepercentchg5)[0]:'date'}, inplace=True)","cb846671":"alt.Chart(altdata_closepercentchg5).mark_point().encode(\n    x='date',\n    y='percentchg'\n)","f7ef0c78":"def pricereverseboolean_shock(percent, df):\n  data = df.filter(['date', 'high', 'low','close'], axis=1) #df.copy()\n  data.set_index('date', inplace=True)\n  data = data.reindex(index=data.index[::-1])\n  data['reversepriceavg'] = (data['high'] + data['low'] + data['close']) \/ 3\n  data['shock'] = (data['reversepriceavg'].pct_change()) * 100\n  data['shock'] = data['shock'].apply(lambda x: 1 if x >= percent else 0)\n  data.drop(['high', 'low', 'close'], axis = 1, inplace = True)\n  data = data.reindex(index=data.index[::-1])\n  return data.dropna()","9aecb9b2":"pricereverseboolean_shock(2, bse)","7c83b486":"altdata_reversepricepercentchg = pricereverseboolean_shock(2, bse)\naltdata_reversepricepercentchg.reset_index(inplace=True)\naltdata_reversepricepercentchg.rename(columns={list(altdata_reversepricepercentchg)[0]:'date'}, inplace=True)","3e474f55":"alt.Chart(altdata_reversepricepercentchg).mark_point().encode(\n    x='date',\n    y='reversepriceavg'\n)","b9436bea":"#reading the uploaded csv file and assigning to news variable\nnews  = pd.read_csv('india-news-headlines.csv')","222ec8bd":"#finding the total rows and columns of news dataset\nnews.shape","4a49575e":"#first 5 rows content of the dataset\nnews.head()","11a8ac01":"#converting publish_date column to \nnews['publish_date'] = pd.to_datetime(news['publish_date'], format = '%Y%m%d')","4430efd5":"#first 5 rows content of the dataset\nnews.head()","167b93a2":"#last 5 rows content of the dataset\nnews.tail()","13c56137":"#getting brief overview of the dataset - number of columns and rows (shape of dataset), columns names and its dtype, how many non-null values it has and memory usage.\nnews.info()","7b4384f2":"#finding unique values in headline_category\nnews['headline_category'].unique()","591623b6":"#checking all the values count (unique values total count)\nnews['headline_category'].value_counts()","bdaf3716":"#total unique values count\nnews['headline_category'].value_counts().count()","0b69ed40":"#checking all the values count (unique values total count)\nnews['headline_text'].value_counts()","91869ea3":"#total unique values count\nnews['headline_text'].value_counts().count()","2371b747":"#finding if any null values are present\nnews.isnull().sum().sum()","8678d84e":"#finding if any duplicate values are present\nnews.duplicated().sum()","da43f6b4":"#rough checking by marking all duplicates as True. Default is first which marks duplicates as True except for the first occurrence.\nnews.duplicated(keep=False).sum()","bdd4643e":"#sorting the dataset to delete the duplicates, to make duplicates come together one after another. The sorted dataset index values are also changed\ncols = list(news.columns)\nnews.sort_values(by=cols, inplace=True, ignore_index=True)","5791b127":"news[news.duplicated(keep=False)]","65157765":"#dropping the duplicates only keeping the last value (ordinally last row from sorted) of each duplicates\nnews.drop_duplicates(keep='last', inplace=True, ignore_index=True)","e1601332":"#re-checking everything worked well with drop_duplicates() carried out earlier on the dataset\nnews.duplicated().sum()","e3f14ccc":"from textblob import TextBlob","e18f6a9e":"#getting a list of unique dates in publish_date column\nlst = news['publish_date'].value_counts().index.tolist()","89d3e0a3":"#concatenating all the headline_text column values of same date in publish_date column\nnew = []\nfor x in lst:\n  df = news.loc[news['publish_date'] == x]\n  headlinetext = ''\n  publishdate = str(x)\n  headlinetext = df['headline_text'].iloc[0]\n  for i in range(1 , len(df)):\n    headlinetext = headlinetext + '. '+ df['headline_text'].iloc[i]  \n  new.append(headlinetext)","0e1d0cb0":"#creating a new dataset\nnewsdf = pd.DataFrame({'publish_date': lst, 'headline_text' : new})","8ba95538":"newsdf","66261728":"#sorting the dataset based on dates\nnewsdf.sort_values(by='publish_date', inplace=True, ignore_index=True)","5de35a9e":"newsdf.head()","8e1ff93f":"newsdf.tail()","dcd540f4":"newsdf.info()","a6c6e052":"polarity = []\nsubjectivity = []\nfor idx, row in newsdf.iterrows():\n  polarity.append(TextBlob(row['headline_text']).sentiment[0])\n  subjectivity.append(TextBlob(row['headline_text']).sentiment[1])","6df20a61":"newsdf['polarity'] = polarity\nnewsdf['subjectivity'] = subjectivity","d234e088":"newsdf.head()","548f79a8":"newsdf.tail()","048fd167":"#finding if any null values are present\nnewsdf.isnull().sum().sum()","3d16fc86":"#renameing the publish_date to date so it will help us during joining this dataset with bse_data dataset\nnewsdf.rename(columns={'publish_date': 'date'}, inplace = True)","d91a182b":"#selecting required columns\nnewsdf = newsdf.filter(['date', 'polarity', 'subjectivity'], axis=1)","570f19a3":"newsdf.shape","aadb23bf":"newsdf['date'].duplicated().sum()","f3deef98":"bse.shape","fc795c31":"bse['date'].duplicated().sum()","46cc1d2f":"bse = pd.merge(bse, newsdf, how='left', on=['date'])","ef10662e":"bse.shape","d88a862e":"bse.head()","1dbb714b":"bse.tail()","b016d05a":"#finding if any null values are present\nbse.isnull().sum().sum()","e200e6ff":"#adding new row for 30th June 2020 (price to be predicted of this day) to main dataset to get average values of all the columns for this day\n#taking average because we don't know the values of all the columns for tomorrow so to predict we need average for independent variable.\n#We will separate this row later from this main dataset so we can use this as prediction of unseen data for tomorrow. \n#And then tally it with actual data from unseenbse_data dataset which we have downloaded too for 30th June 2020 actual values\nbse.loc[len(bse)] = ['2020-06-30', bse['open'].mean(), bse['high'].mean(), bse['low'].mean(),\n                       bse['close'].mean(), bse['adjclose'].mean(), bse['volume'].median(), newsdf['polarity'].mean(), newsdf['subjectivity'].mean() ]","7e9ec6a5":"#converting date from object dtype to datetime dtype\nbse['date'] = pd.to_datetime(bse['date'], format=\"%Y-%m-%d\")","07a7af20":"bse.tail()","c4013431":"bse[\"month\"] = bse['date'].dt.month\nbse[\"day\"] = bse['date'].dt.day\nbse[\"dayofweek\"] = bse['date'].dt.dayofweek\nbse[\"week\"] = bse['date'].dt.week\nbse['movingavg4weeks'] = round(bse['close'].rolling(window=(4*5), min_periods = 1).mean().shift(),2)\nbse['movingavg16weeks'] = round(bse['close'].rolling(window=(16*5), min_periods = 1).mean().shift(),2) #add 12 weeks to 4 weeks \nbse['movingavg28weeks'] = round(bse['close'].rolling(window=(28*5), min_periods = 1).mean().shift(),2) #add 12 weeks to 16 weeks\nbse['movingavg40weeks'] = round(bse['close'].rolling(window=(40*5), min_periods = 1).mean().shift(),2) #add 12 weeks to 28 weeks\nbse['movingavg52weeks'] = round(bse['close'].rolling(window=(52*5), min_periods = 1).mean().shift(),2)  #add 12 weeks to 40 weeks\nbse['window10days'] = round(bse['close'].rolling(window = 10, min_periods = 1).mean().shift(),2)  \nbse['window50days'] = round(bse['close'].rolling(window = 50, min_periods = 1).mean().shift(),2)\nbse['volumeshock'] = round(boolean_shock(10, bse, 'volume').reset_index()['shock'], 2)\nbse['closeshock2'] = round(reverseboolean_shock(2, bse, 'close').reset_index()['shock'], 2)\nbse['closeshock5'] = round(reverseboolean_shock(5, bse, 'close').reset_index()['shock'],2)\nbse['closeshock10'] = round(reverseboolean_shock(10, bse, 'close').reset_index()['shock'], 2)\nbse['priceshock'] = round(priceboolean_shock(10, bse).reset_index()['shock'], 2)\nbse['reversebooleanshock2'] = round(reverseboolean_shock(2, bse, 'close').reset_index()['shock'], 2)\nbse['reversebooleanshock5'] = round(reverseboolean_shock(5, bse, 'close').reset_index()['shock'], 2)\nbse['pricereverseshock2'] = round(pricereverseboolean_shock(2, bse).reset_index()['shock'], 2)\nbse['polarity'] = round(bse['polarity'] , 2)\nbse['subjectivity'] = round(bse['subjectivity'] , 2)\nbse['price'] = round((bse['high'] + bse['low'] + bse['close']) \/ 3 , 2)\nbse['close'] = round(bse['close'] , 2)","fbc2c90b":"bse.columns","bbe6c2ac":"bse","7d464b16":"#fillinf the null columns\nbse.fillna(method = 'bfill', inplace = True)","36705563":"#fillinf the null columns\nbse.fillna(method = 'ffill', inplace = True)","98951c02":"#finding if any null values are present\nbse.isnull().sum().sum()","f010f249":"#selecting specific columns\nbse = bse.filter(['month', 'day', 'dayofweek', 'week',\n       'movingavg4weeks', 'movingavg16weeks', 'movingavg28weeks',\n       'movingavg40weeks', 'movingavg52weeks', 'window10days', 'window50days',\n       'volumeshock', 'closeshock2', 'closeshock5', 'closeshock10',\n       'priceshock', 'reversebooleanshock2', 'reversebooleanshock5',\n       'pricereverseshock2', 'polarity', 'subjectivity', 'price', 'close'], axis=1)","0e3c9b4c":"bse","9e4b51dc":"#separating the predicted date row from main dataset after getting all the calculated average values\nmain_bsedata = bse.iloc[:1345,:].reset_index()  \nnewtestunseen_bsedata = bse.iloc[1345:,:].reset_index()","950914a4":"main_bsedata.shape","86962614":"main_bsedata.tail()","72a1f8f3":"newtestunseen_bsedata.shape","1dfc9a6e":"newtestunseen_bsedata.head()","6943f855":"X = main_bsedata.drop(['price','close'], axis = 1)\ny = main_bsedata[['price','close']]","7d1aab61":"Xnewtestunseen = newtestunseen_bsedata.drop(['price','close'], axis = 1)\nynewtestunseen_ans = newtestunseen_bsedata[['price','close']]","a13dd882":"X.shape, y.shape","fb5bd2f9":"Xnewtestunseen.shape, ynewtestunseen_ans.shape","28e0980b":"split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(X, y):\n  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n  y_train, y_test = y.iloc[train_index], y.iloc[test_index]","01723aa1":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","de70d02d":"rfg = RandomForestRegressor(random_state = 42, n_estimators = 500, criterion='mse', max_depth = 30, min_samples_leaf=2, min_samples_split=5, n_jobs=1)","a11f4680":"chainedmodel = RegressorChain(rfg)\nchainedmodel.fit(X_train, y_train)","4e8789ab":"pred = chainedmodel.predict(X_test)\nroundpred = []\nfor x in range(len(pred)):\n  roundpred.append([round(pred[x][0], 2),round(pred[x][1], 2) ])","54c33290":"r2_score(y_test, roundpred)","80c21e9a":"#evaluating the performance of the model\n#MAE\nprint('MAE')\nprint(mean_absolute_error(y_test, roundpred), end='\\n')\n#MSE\nprint('MSE')\nprint(mean_squared_error(y_test, roundpred), end='\\n')\n#RMSE\nprint('RMSE')\nprint(np.sqrt(mean_squared_error(y_test, roundpred)))","40a8e277":"pred_newtestunseen = chainedmodel.predict(Xnewtestunseen)","70d31c3b":"[(round(pred_newtestunseen[0][0], 2)),(round(pred_newtestunseen[0][1], 2))]","b3628830":"ynewtestunseen_ans  #used average of high, low, close, volume to calculate price ((high+low+close)\/3) and close value","1b324f19":"[unseenbsedata_price[0] , round(unseenbse_data['close'],2)[0]] #actual price ((high+low+close)\/3) calculated earlier and the closing price","767b40f8":"There are two ways to predict values of two columns one is\n\n1) Direct Multioutput Regression:\n\nIt involves seperating each target variable as independent regression problem, that is here it presumably assumes the outputs to be independent of each other.\n\n2) Chained Multioutput Regression:\n\nIt involves creating a series of regression models from single output regression model, that is the first model in the sequence uses the input and predicts one output then the second model uses the input and the output from the first model to make a prediction and it goes on depending on the number of target variables.","1e77bd05":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 4 and 32. Since it is less than 0 and more than -1 so 4 and 32 represents a perfect negative correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions (which is not vividly seen in the above plot)","521384b1":"# THE SPARKS FOUNDATION","236b204a":"Pricing shock without volume shock : Now we will be creating a time series for pricing shock without volume shock based on whether price at T vs T+1 has a difference > 2%. ( 0\/1 dummy-coding is for direction of shock). This will be reverse of pct_change()","62341dbc":"## Textual Analysis of news from Times of India News Headlines","9646ef27":"## Objective:\n\n> Create a hybrid model for stock price\/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines\n\n> Stock to analyze and predict - SENSEX (S&P BSE SENSEX)","d9a83c42":"In this case Chained Multioutput Regression will be more appropriate option as the stock price ((high+low+close)\/3) and closing price are interdependent.","4f6389c2":"We can calculate the sentiment using TextBlob. Based on the polarity, we determine whether it is a positive text or negative or neutral. For TextBlog, if the polarity is more than 0, it is considered positive, if it is less than 0 then it is considered negative and if it ia=s equal to 0 is considered neutral. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.","1ef27db4":"## Rolling window analysis of time series","7d73caa5":"## Training the model and predicting the price of 30th June 2020","430a3b93":"## Creating the reverse dummy time series:","50591911":"## Creating the dummy time series:","7b8bc9c3":"## Preparing the dataset for machine learning","99079661":"## Task 7- Stock Market Prediction using Numerical and Textual Analysis","74777a13":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28. Where 0, 1, 19 represents a perfect positive correlation and 20 represents a perfect negative correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions (which is not vividly seen in the above plot)","a1ab4f77":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 5, 6, 7, 10, 11, 24. And lag value 0 represents a perfect positive correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions","d2fda587":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 3, 4, 5, 8, 9. 10, 12, 13, 15, 16, 18, 19, 20, 22, 23, 29, 30, 32. And lag value 0 represents a perfect positive correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions","03907aa2":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 1 representing a perfect positive correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions","2444d6f9":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 1, 2, 4, 6, 7, 8, 15, 16, 21, 22, 25, 26. And lag values 0, 1 represents a perfect positive correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions","af318802":"## Creating a rolling window of size 10 and 50 of the BSE index","9b161718":"Pricing black swan : we will be creating a 0\/1 dummy-coded boolean time series for shock, based on whether closing price at T vs T+1 has a difference > 5%. ( 0\/1 dummy-coding is for direction of shock). This will be reverse of pct_change()","42fc7995":"Price shocks : we will be creating a 0\/1 dummy-coded boolean time series for shock, based on whether closing price at T vs T+1 has a difference > 2%. ( 0\/1 dummy-coding is for direction of shock). This will be reverse of pct_change()","f8fd7fb6":"## Pricing shock without volume shock","3237c324":"# Author - Deepak Kaura (DK)\n\n## GRIPJAN2021 - Data Analysis & Business Analytics Intern","d8b59402":"Volume shocks : we will be creating a 0\/1 dummy-coded boolean time series for shock, based on whether volume traded is 10% higher\/lower than previous day. ( 0\/1 dummy-coding is for direction of shock)","462e55be":"In the partial autocorrelation plot above, we have statistically significant partial autocorrelations at lag values 0, 5, 6, 7, 10, 11, 24. And lag value 0 represents a perfect positive correlation. While the rest of values are very close to 0 and under the confidence intervals, which are represented as blue shaded regions","5a0ef80c":"The model predicted for 30th June 2020 the price ((high+low+close)\/3) i.e the average of high, low, close of BSE index to be 35099.15 and closing price to be 34987.5\n\nAnd the actual price ((high+low+close)\/3) i.e the average of high, low, close of BSE index on day 30th June 2020 was 34987.5, and closing price was 34915.8\n\nSo it maybe vary due to yahoo finance data or news data that is used in our model but overall as above our model has done a very good prediction","1fb2bfdf":"Creating 4,16, 52 week moving average of closing price of BSE index"}}