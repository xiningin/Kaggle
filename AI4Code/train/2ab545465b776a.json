{"cell_type":{"75f1acf4":"code","abe57af0":"code","58b2a71b":"code","099cf4bb":"code","7bfb2a32":"code","0f722bd3":"code","fd28bb4e":"code","a386aaf5":"code","c86ba6b2":"code","15870fc7":"code","f6bb7d7b":"code","17da0cef":"code","5dce72b0":"code","c19a39f4":"code","f124f89c":"code","d1a5a6b8":"code","431172ff":"code","ac5ca790":"code","68cbeb64":"code","923d5897":"code","88c42e53":"code","5854839b":"markdown","b7823705":"markdown","216bb4ea":"markdown","03dbc590":"markdown","776372e7":"markdown","304209d6":"markdown","1dd1330d":"markdown","dc7adcaa":"markdown","8666aa5a":"markdown","4da05f98":"markdown","265401a3":"markdown","beed8182":"markdown","86620759":"markdown","d704a8cb":"markdown","4b3c40a0":"markdown","4065a8df":"markdown","de81463f":"markdown","e8810f39":"markdown","97831214":"markdown","4ab8ce47":"markdown","eaff61ab":"markdown","ac4d0c44":"markdown","ed4120d2":"markdown","c130ad99":"markdown","c3e4b761":"markdown","81428a4f":"markdown","c9ff3c66":"markdown","465f319a":"markdown","865be875":"markdown","b8b588ea":"markdown","07187694":"markdown","1f8008a2":"markdown","4068600c":"markdown","dd70ce36":"markdown"},"source":{"75f1acf4":"import pandas as pd\nimport torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch.utils.data import Dataset","abe57af0":"DATA_DIR = '..\/input\/jovian-pytorch-z2g\/Human protein atlas'\n\nTRAIN_DIR = DATA_DIR + '\/train'                           # Contains training images\nTEST_DIR = DATA_DIR + '\/test'                             # Contains test images\n\nTRAIN_CSV = DATA_DIR + '\/train.csv'                       # Contains real labels for training images\nTEST_CSV = '..\/input\/jovian-pytorch-z2g\/submission.csv'   # Contains dummy labels for test image","58b2a71b":"labels = {\n    0: 'Mitochondria',\n    1: 'Nuclear bodies',\n    2: 'Nucleoli',\n    3: 'Golgi apparatus',\n    4: 'Nucleoplasm',\n    5: 'Nucleoli fibrillar center',\n    6: 'Cytosol',\n    7: 'Plasma membrane',\n    8: 'Centrosome',\n    9: 'Nuclear speckles'\n}","099cf4bb":"## used for label encoding \n## converts 2,3 -> [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\ndef encode_label(label):\n    #create an initial target which is all zeros\n    target = torch.zeros(10)\n    for l in str(label).split(' '):\n        target[int(l)] = 1.\n    return target","7bfb2a32":"## used for label decoding\n## converts converts [0, 0, 1, 1, 0, 0, 0, 0, 0, 0] ->  2,3 \ndef decode_target(target, text_labels=False, threshold=0.5):\n    result = []\n    for i, x in enumerate(target):\n        if (x >= threshold):\n            if text_labels:\n                result.append(labels[i] + \"(\" + str(i) + \")\")\n            else:\n                result.append(str(i))\n    return ' '.join(result)","0f722bd3":"## used to show a single image and its label\ndef show_sample(img, target, invert=True):\n    plt.figsize=(8,4)\n    if invert:\n        plt.imshow(1 - img.permute((1, 2, 0)))\n    else:\n        plt.imshow(img.permute(1, 2, 0))\n    print('Labels:', decode_target(target, text_labels=True))","fd28bb4e":"## used to show orginal images and its transformation\ndef show_difference(img1, target1, img2, target2, invert=True):\n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,4))\n    ax1.set_title('Before Transformation')\n    ax2.set_title('After Transformation')\n    if invert:\n        ax1.imshow(1 - img1.permute((1, 2, 0)))\n        ax2.imshow(1 - img2.permute((1, 2, 0)))\n    else:\n        ax1.imshow(img1.permute((1, 2, 0)))\n        ax2.imshow(img2.permute((1, 2, 0)))","a386aaf5":"class HumanProteinDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img_id, img_label = row['Image'], row['Label']\n        img_fname = self.root_dir + \"\/\" + str(img_id) + \".png\"\n        img = Image.open(img_fname)\n        if self.transform:\n            img = self.transform(img)\n        return img, encode_label(img_label)","c86ba6b2":"transform = transforms.Compose([transforms.ToTensor()])\ndataset = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)","15870fc7":"show_sample(*dataset[0])","f6bb7d7b":"transform = transforms.Compose([transforms.CenterCrop(256),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","17da0cef":"transform = transforms.Compose([transforms.ColorJitter(brightness=(0,10), contrast=(10,20), saturation=(30,40), hue=(-.5,.5)),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","5dce72b0":"transform = transforms.Compose([transforms.Grayscale(3),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","c19a39f4":"transform = transforms.Compose([transforms.Pad(padding=(10,20,30,40), fill=(255,0,0)),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","f124f89c":"transform = transforms.Compose([transforms.RandomAffine(degrees=(-45,+45), scale=(1,2)),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","d1a5a6b8":"random_apply = [transforms.RandomAffine(degrees=(-45,+45), scale=(1,2)), transforms.Grayscale(3), transforms.ColorJitter(brightness=(0,10), contrast=(10,20), saturation=(10,15), hue=(-.5,.5))]\ntransform = transforms.Compose([transforms.RandomApply(random_apply, p=0.8),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","431172ff":"random_apply = [transforms.RandomAffine(degrees=(-45,+45), scale=(1,2)), transforms.Grayscale(3), transforms.ColorJitter(brightness=(0,10), contrast=(10,20), saturation=(10,15), hue=(-.5,.5))]\ntransform = transforms.Compose([transforms.RandomChoice(random_apply),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","ac5ca790":"transform = transforms.Compose([transforms.RandomCrop(128),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","68cbeb64":"transform = transforms.Compose([transforms.RandomResizedCrop(size=512, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=2),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","923d5897":"transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.9),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","88c42e53":"transform = transforms.Compose([transforms.RandomVerticalFlip(p=0.9),transforms.ToTensor()])\ndataset1 = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)\nshow_difference(*dataset[0], *dataset1[0])","5854839b":"Here the transformed image will have a random rotation between -45 to +45 degree and a scaling between 1 and 2.","b7823705":"We can see that the image is vertically flipped.","216bb4ea":"We can see that the transformed image is a random crop of the orginal image.","03dbc590":"<a id=\"dataset\"><\/a>\n## Creating torch dataset\n\nWe wraps the input tensor and the encoded label as a dataset","776372e7":"<a id=\"reference\"><\/a>\n## Reference\n* https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html\n* https:\/\/www.kaggle.com\/aakashns\/simple-cnn-starter","304209d6":"<a id=\"RandomCrop\"><\/a>\n### RandomCrop\n\nCrop the given PIL Image at a random location. Lets see an example.","1dd1330d":"<a id=\"conclusion\"><\/a>\n## Conclusion\n\nWe have seen a small subset of transformations that are available for image augmentation in pytorch. The deep learning models tend to perform better with more training data. Using some or all of the above mentioned techinques, you can increase the training data and can score good results.","dc7adcaa":"<a id=\"introduction\"><\/a>\n## Introduction","8666aa5a":"Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nHere I am trying to introduce few of the image augmentation techniques that are available in pytorch.","4da05f98":"<a id=\"RandomChoice\"><\/a>\n### RandomChoice\n\nApply single transformation randomly picked from a list. Lets see an example.","265401a3":"<a id=\"Grayscale\"><\/a>\n### Grayscale\n\nConvert image to grayscale. Lets see an example.","beed8182":"![image.png](attachment:image.png)","86620759":"<a id=\"transforms\"><\/a>\n## Transforms\n\nTransforms are common image transformations. They can be chained together using Compose. ","d704a8cb":"<a id=\"utility_functions\"><\/a>\n## Utility functions","4b3c40a0":"<a id=\"RandomHorizontalFlip\"><\/a>\n### RandomHorizontalFlip\n\nHorizontally flip the given PIL Image randomly with a given probability. Lets see an example.","4065a8df":"All the inputs are images. We are using a single transform called ToTensor() to convert the images to tensors. ","de81463f":"<a id=\"lib-import\"><\/a>\n## Imporing libraries and data","e8810f39":"We have created a dataset contining the tensor representation of all images. Lets take a look into first image.","97831214":"<a id=\"RandomVerticalFlip\"><\/a>\n### RandomVerticalFlip\n\nVertically flip the given PIL Image randomly with a given probability. Lets see an example.","4ab8ce47":"<a id=\"Pad\"><\/a>\n### Pad\n\nPad the given PIL Image on all sides with the given \u201cpad\u201d value. Lets see an example.","eaff61ab":"The resultant image is transformed using one transformation specified in the list of transformations.","ac4d0c44":"The resultant image is transformed using transformation specified in the list of transformations.","ed4120d2":"## Table of contents\n* [Introduction](#introduction)\n* [Importing libraries and data](#lib-import)\n* [Utility functions](#utility_functions)\n* [Creating torch dataset](#dataset)\n* [Transforms](#transforms)\n\t* [CenterCrop](#CenterCrop)\n\t* [ColorJitter](#ColorJitter)\n\t* [Grayscale](#Grayscale)\n\t* [Pad](#Pad)\n\t* [RandomAffine](#RandomAffine)\n\t* [RandomApply](#RandomApply)\n\t* [RandomChoice](#RandomChoice)\n\t* [RandomCrop](#RandomCrop)\n\t* [RandomResizedCrop](#RandomResizedCrop)\n\t* [RandomHorizontalFlip](#RandomHorizontalFlip)\n\t* [RandomVerticalFlip](#RandomVerticalFlip)\n* [Conclusion](#conclusion)\n* [Reference](#reference)","c130ad99":"We can see that this transformation creates a grayscale image of the actual input image. Also we can sepcify the channels to apply grayscale.","c3e4b761":"<a id=\"RandomResizedCrop\"><\/a>\n### RandomResizedCrop\n\nCrop the given PIL Image to random size and aspect ratio. Lets see an example.","81428a4f":"<a id=\"ColorJitter\"><\/a>\n### ColorJitter\n\nRandomly change the brightness, contrast and saturation of an image. Lets see an example.","c9ff3c66":"<a id=\"RandomApply\"><\/a>\n### RandomApply\n\nApply randomly a list of transformations with a given probability. Lets see an example.","465f319a":"The training data contain images of cell and its components. We use the below labels to identify the various parts.","865be875":"We can see that the right side image is horizontally flipped.","b8b588ea":"In the above image we can see that the left image is 512x512 but right size one is 256x256. This was the result of calling the transform CenterCrop(256). The size of image is changed by this transformation.","07187694":"<a id=\"RandomAffine\"><\/a>\n### RandomAffine\n\nAn affine transformation, or an affinity, is a geometric transformation that preserves lines and parallelism. Random affine transformation of the image keeping center invariant. Lets see an example.","1f8008a2":"<a id=\"CenterCrop\"><\/a>\n### CenterCrop\n\nCrops the given PIL Image at the center. Lets see an example.","4068600c":"From the above images we can see that changing the brightness, contrast, saturation and hue can bring out some details that are not visible in regular image.","dd70ce36":"We can see that the right image has padding. We can specify the amount of padding needed in each size and the color required. "}}