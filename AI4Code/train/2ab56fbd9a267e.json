{"cell_type":{"1daa59f5":"code","6ab51f95":"code","6ad77159":"code","06b09050":"code","fae620e3":"code","ccb85d8d":"code","3896580a":"markdown","8902f39e":"markdown","ce568078":"markdown","4262bf1e":"markdown"},"source":{"1daa59f5":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder, minmax_scale\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Dropout, Input, InputLayer, BatchNormalization \n","6ab51f95":"train_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n\n# Drop some columns, add features\nfor df in [train_df, test_df]:\n    df.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True) # These features are always zero\n    df.drop(columns=['Soil_Type1'], inplace=True) # Feature is useless according to permutation feature importance\n    #df['Aspect180'] = (df['Aspect'] + 180) % 360 # makes cv worse\n    df[\"Hillshade_9am\"] = df[\"Hillshade_9am\"].clip(0, 255)\n    df[\"Hillshade_Noon\"] = df[\"Hillshade_Noon\"].clip(0, 255)\n    df[\"Hillshade_3pm\"] = df[\"Hillshade_3pm\"].clip(0, 255)\n    df[\"Aspect\"] = df[\"Aspect\"].clip(0, 360)\nfeatures = [f for f in test_df.columns if f != 'Id' and f != 'Cover_Type']\n\n# Reduce memory size\ntrain_df = train_df.astype(np.float32)\ntest_df = test_df.astype(np.float32)\ntrain_df['Id'] = train_df['Id'].astype(np.int32)\ntest_df['Id'] = test_df['Id'].astype(np.int32)\ntrain_df['Cover_Type'] = train_df['Cover_Type'].astype(np.int32)\n\n# Show the imbalanced class distribution\nprint(\"The imbalanced class distribution:\")\nprint((train_df.groupby('Cover_Type').Id.count() \/ len(train_df)).apply(lambda p: f\"{p:.3%}\"))\n\n# Drop Cover_Type 5 (the class with only one element can be ignored)\ntrain_df = train_df[train_df.Cover_Type != 5]\n\n# Prepare for multiclass classification\nle = LabelEncoder()\ntarget = le.fit_transform(train_df.Cover_Type) # renumbers the 6 classes from 0 to 5\n","6ad77159":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last unique n_epochs epochs of) the training history\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else len(history['loss']) - n_epochs\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot validation metrics\n    if plot_acc:\n        best_epoch = np.argmax(np.array(history['val_acc']))\n        best_val_acc = history['val_acc'][best_epoch]\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['val_acc'][from_epoch:]), color='r', label='Validation accuracy')\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_acc], c='r', label=f'Best val_acc = {best_val_acc:.5f}')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend(loc='center right')\n        \n    # Plot learning rate\n    if plot_lr:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","06b09050":"#%%time\nEPOCHS = 90 # increase the number of epochs if the training curve indicates that a better result is possible\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nSINGLE_FOLD = False # set to True for a quick experiment and to False for full cross-validation\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nBATCH_SIZE = 1024 # if you set this too high, the notebook will crash (out of memory)\nFOLDS = 10\n\ndef my_model(X):\n    \"\"\"Return a compiled Keras model\"\"\"\n    model = Sequential()\n    model.add(InputLayer(input_shape=(X.shape[-1])))\n    \n    # Add the hidden layers\n    for size in [128, 64, 64]:\n        model.add(Dense(size, kernel_initializer='lecun_normal', activation='selu'))\n        #model.add(BatchNormalization())\n        #model.add(LayerNormalization()) # LayerNormalization gives a similar score increase as BatchNormalization, but is slower\n        #model.add(Dropout(rate=0.1)) # When I tried dropout, accuracy became worse.\n        \n    # Add the final layer with the correct activation function\n    # Adding kernel_regularizer=tf.keras.regularizers.l2(l2=0.03) didn't make a difference\n    model.add(Dense(len(le.classes_), activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n    return model\n\n# Make the results reproducible\nnp.random.seed(202100)\ntf.random.set_seed(202100)\n\ntotal_start_time = datetime.now()\nscore_list, test_pred_list, history_list = [], [], []\noof_list = [np.full((len(train_df), len(le.classes_)), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df.Cover_Type)):\n        print(f\"Fold {run}.{fold}\")\n        start_time = datetime.now()\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        y_tr = le.transform(X_tr.Cover_Type)\n        y_va = le.transform(X_va.Cover_Type)\n        X_tr = X_tr[features]\n        X_va = X_va[features]\n\n        # Scale\n        preproc = StandardScaler() # I tried QuantileTransformer, but StandardScaler seems to be better by 0.005\n        X_tr = preproc.fit_transform(X_tr)\n        X_va = preproc.transform(X_va)\n\n        # Define two callbacks: ReduceLROnPlateau, EarlyStopping\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n                               patience=4, verbose=1)\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=10, \n                           verbose=VERBOSE, mode=\"min\", \n                           restore_best_weights=True)\n\n        # Train and save the model\n        model = my_model(X_tr)\n        history = model.fit(X_tr, y_tr, \n                            validation_data=(X_va, y_va), \n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE, \n                            validation_batch_size=len(X_va),\n                            shuffle=True,\n                            callbacks=[lr, es])\n        history_list.append(history.history)\n        model.save(f\"model{run}.{fold}\")\n        \n        # Inference for validation after last epoch of fold\n        y_va_pred = model.predict(X_va, batch_size=len(X_va))\n        oof_list[run][val_idx] = y_va_pred\n        y_va_pred = np.argmax(y_va_pred, axis=1)\n\n        # Evaluation\n        accuracy = accuracy_score(y_va, y_va_pred)\n        score_list.append((accuracy, datetime.now() - start_time))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]} | Epochs: {len(history_list[-1]['loss'])} | Accuracy: {accuracy:.5f}\")\n        if run == 0: plot_history(history_list[-1], title=f\"Accuracy: {accuracy:.5f}\")\n\n        # Inference for test: keep the predicted probabilities\n        test_pred_list.append(model.predict(preproc.transform(test_df[features]), batch_size=BATCH_SIZE))\n        \n        # Clean up the memory (it seems that Keras doesn't clean up everything at keyboard interrupts)\n        del model, y_va_pred\n        gc.collect()\n        \n        if SINGLE_FOLD: break\n\n# Save all oof and test predictions to later determine ensemble weights\nwith open('oof_list.pickle', 'wb') as handle: pickle.dump(oof_list, handle)\nwith open('test_pred_list.pickle', 'wb') as handle: pickle.dump(test_pred_list, handle)\n    \ntotal_time = datetime.now() - total_start_time\n","fae620e3":"# Overall evaluation\nif oof_list[0].min() >= 0: # Can only evaluate if all folds have been done (set SINGLE_FOLD to False)\n    \n    # Evaluate the overall cv score\n    print(f\"Single-model Accuracy: {sum([accuracy_score(train_df.Cover_Type, le.inverse_transform(np.argmax(oof, axis=1))) for oof in oof_list]) \/ len(oof_list):.5f}\")\n\n    # Evaluate the number of epochs and the time taken\n    print(f\"Average epochs: {sum([len(h['loss']) for h in history_list]) \/ len(history_list):.0f}\")\n    print(f\"Maximum epochs: {max([len(h['loss']) for h in history_list])}\")\n    print(f\"Stopped early in {sum([len(h['loss']) < EPOCHS for h in history_list]) \/ len(history_list):.0%} of runs\")\n    print(f\"Total elapsed time: {str(total_time)[-14:-7]} for {len(history_list)} trainings\") \n    print()\n\n    # Show the confusion matrix\n    def plot_confusion_matrix(cm, classes, cm_type='recall'):\n        if cm_type == 'recall':\n            cm = cm \/ cm.sum(axis=1).reshape(-1, 1)\n            colors = cm\n            cell_format = '.0%'\n            plt.title('Confusion matrix (sum of every row is 100 %, diagonal shows recall)', fontweight='bold', pad=15)\n        elif cm_type == 'precision':\n            cm = cm \/ cm.sum(axis=0).reshape(1, -1)\n            colors = cm\n            cell_format = '.0%'\n            plt.title('Confusion matrix (sum of every column is 100 %, diagonal shows precision)', fontweight='bold', pad=15)\n        elif cm_type == 'accuracy':\n            cm = cm \/ cm.sum()\n            colors = minmax_scale(cm.reshape(-1, 1)).reshape(cm.shape[0], cm.shape[1]) ** 0.3 # make the low-to-medium cells darker\n            cell_format = '.2%'\n            plt.title('Confusion matrix (sum of matrix is 100 %, sum of diagonal shows accuracy)', fontweight='bold', pad=15)\n        elif cm_type == 'count':\n            colors = minmax_scale(cm.reshape(-1, 1)).reshape(cm.shape[0], cm.shape[1]) ** 0.3 # make the low-to-medium cells darker\n            cell_format = 'd'\n            plt.title('Confusion matrix (sample counts)', fontweight='bold', pad=15)\n        else: raise ValueError(f'Illegal value for parameter cm_type: {cm_type}')\n        plt.imshow(colors, interpolation='nearest', cmap=plt.cm.Blues) # or cmap='hot'\n        #plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=0)\n        plt.yticks(tick_marks, classes)\n\n        thresh = colors.max() \/ 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            value = cm[i, j]\n            plt.text(j, i, format(value, cell_format),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if colors[i, j] > thresh else \"black\")\n\n        plt.ylabel('True label', fontweight='bold')\n        plt.xlabel('Predicted label', fontweight='bold')\n        plt.tight_layout()\n\n    cm = confusion_matrix(train_df.Cover_Type, le.inverse_transform(np.argmax(oof_list[0], axis=1)))\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='precision')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='recall')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='accuracy')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='count')\n    plt.show()\n    \n    # Print the classification report\n    print(classification_report(train_df.Cover_Type, le.inverse_transform(np.argmax(oof_list[0], axis=1))))","ccb85d8d":"# Create the submission file\nsub = test_df[['Id']].copy()\nsub['Cover_Type'] = le.inverse_transform(np.argmax(sum(test_pred_list), axis=1)) # soft voting by adding the probabilities of all models in the ensemble\nsub.to_csv('submission.csv', index=False)\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(10,3))\nplt.hist(train_df['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, label='Train labels')\nplt.hist(sub['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\nplt.xlabel('Cover_Type')\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()\n\nsub.head()\n","3896580a":"# Evaluation","8902f39e":"# Training","ce568078":"Now it's your turn: Change the model architecture, add features, ... and see what happens!","4262bf1e":"# Keras Quickstart\n\nThis notebook shows\n- how to use Keras for this competition\n- how to correctly cross-validate the model\n- how to set a decreasing learning rate and early stopping\n- how to plot the training curves\n- how to ensemble the five models by soft voting\n- how to save the models and the oof predictions for later use\n\nYou can enable GPU acceleration for this notebook to get the results faster, but you don't need the GPU.\n\nRelease notes:\n- V1: -> lb 0.94821\n- V2: Other network architecture (added one layer), 60 epochs\n- V3: Hidden layers \\[128, 64, 16\\]\n- V4: Hidden layers \\[128, 64, 64, 16\\] -> lb 0.95468\n  - no real difference to V3\n- V5: Fixed the voting classifier which was missing in earlier versions, added L2 regularization, LabelEncoder, drop Cover_Type 5 -> lb 0.95598\n  - Voting makes a big difference\n  - L2 regularization doesn't matter\n- V6: Hidden layers \\[128, 64, 64\\], selu activation, 3 runs -> lb 0.95619\n  - It seems that the architecture is somewhat better than before, but the two additional runs don't improve the lb score.\n- V7: BatchNormalization improves the cv but not the lb, and almost doubles the running time. -> lb 0.95598\n- V8: 10 folds, 90 epochs -> lb 0.95626\n- V9: drop Soil_Type1 -> lb 0.95635\n- V10: add Aspect180\n  - cv score became worse\n- V11: \\[256, 128, 64\\], other callback parameters\n- V12: =V9 + oversampling -> lb 0.95559\n- V13: oversampling after train-test split (failed)\n- V14: clip some features -> lb 0.95615\n- V15: early stopping on loss -> lb 0.95635 (same score as V9)\n- V16: lecun_normal -> lb 0.95615\n- V17: lecun_normal without BatchNormalization -> lb 0.95650\n- V18: let's see the effect of a different seed\n- V19: again another seed"}}