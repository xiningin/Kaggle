{"cell_type":{"00b10dbd":"code","234a9ccd":"code","9c69debd":"code","d3fee9c9":"code","9b817c48":"code","08d5006e":"code","2cd14bbe":"code","1b37a76d":"code","781ebe8c":"code","1ed184c3":"code","bb844862":"code","0641484f":"code","9dac9829":"code","bd917626":"code","0914247e":"code","2c17dcbd":"code","968b7cba":"code","e3a2c11c":"code","9a1d1792":"code","7d57707b":"code","7dfc072d":"code","95725520":"code","70b08991":"code","069fd5b7":"code","6e5bb8b5":"code","5f5e7849":"code","99c1b844":"code","e9ae0b7a":"code","63fc3e8c":"code","64aee869":"code","b0d47939":"code","485d7685":"code","6315bc15":"code","7b3ca899":"code","38255386":"code","1c40b162":"code","629e7314":"code","46e17c7a":"code","e50197ab":"code","9ecc16d3":"code","d32ec3aa":"code","c1dc06b2":"code","84f41ded":"code","3e82ff3f":"code","ff7c4cf0":"code","bb7f496b":"code","d87eec9a":"markdown","f3de2e9c":"markdown","e705b1eb":"markdown"},"source":{"00b10dbd":"import numpy as np\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_curve, auc","234a9ccd":"heart_df=pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\")","9c69debd":"heart_no_na_df=pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\")\nheart_df.head()","d3fee9c9":"sns.countplot(data = heart_df ,x='TenYearCHD');","9b817c48":"sns.countplot(data = heart_df ,x='TenYearCHD',hue='male');","08d5006e":"sns.countplot(data = heart_df ,x='TenYearCHD',hue='BPMeds');","2cd14bbe":"sns.countplot(data = heart_df ,x='TenYearCHD',hue='currentSmoker');","1b37a76d":"sns.countplot(data = heart_df ,x='TenYearCHD',hue='prevalentStroke');","781ebe8c":"sns.countplot(data = heart_df ,x='TenYearCHD',hue='diabetes');","1ed184c3":"sns.boxplot(data= heart_df, x='TenYearCHD', y='age')","bb844862":"sns.boxplot(data= heart_df, x='TenYearCHD', y='glucose');","0641484f":"sns.boxplot(data= heart_df, x='TenYearCHD', y='heartRate');","9dac9829":"sns.boxplot(data= heart_df, x='TenYearCHD', y='BMI');","bd917626":"sns.boxplot(data= heart_df, x='TenYearCHD', y='totChol');","0914247e":"sns.boxplot(data= heart_df, x='TenYearCHD', y='education');","2c17dcbd":"heart_df.isnull().sum()","968b7cba":"def fill_based_on_sex(df,i):\n    df.loc[(df[i].isnull()) & (df.male==0),i] = df.groupby('male')[i].mean()[0]\n    df.loc[(df[i].isnull()) & (df.male==1),i] = df.groupby('male')[i].mean()[1]\n    return df[i]","e3a2c11c":"heart_df.BPMeds=fill_based_on_sex(heart_df,'BPMeds')\nheart_df.BMI=fill_based_on_sex(heart_df,'BMI')\nheart_df.glucose=fill_based_on_sex(heart_df,'glucose')\nheart_df.heartRate=fill_based_on_sex(heart_df,'heartRate')\nheart_df.totChol=fill_based_on_sex(heart_df,'totChol')","9a1d1792":"#fillna based on other class (like no education)\nheart_df.education.fillna(0,inplace=True)","7d57707b":"heart_df[heart_df.cigsPerDay.isnull()]['currentSmoker'].value_counts()\n#Okay, we can fill cigsperday mean\/median with currentSmoker == 1 \nheart_df.cigsPerDay = heart_df.cigsPerDay.fillna(heart_df[heart_df.currentSmoker==1]['currentSmoker'].mean())","7dfc072d":"heart_df.isnull().sum()","95725520":"heart_df","70b08991":"num_col = heart_df.columns[:-1:]\nnum_col","069fd5b7":"#okay time to work!","6e5bb8b5":"X_train, X_test, y_train, y_test = train_test_split(heart_df.drop(['TenYearCHD'], axis=1), \n                                                    heart_df['TenYearCHD'], \n                                                    test_size=0.2, \n                                                    random_state=42)","5f5e7849":"sc = StandardScaler()\nminmax = MinMaxScaler()\nX_train_sc , X_test_sc = pd.DataFrame(sc.fit_transform(X_train),columns=X_train.columns), pd.DataFrame(sc.transform(X_test),columns=X_test.columns)\nX_train_minmax , X_test_minmax = pd.DataFrame(minmax.fit_transform(X_train),columns=X_train.columns), pd.DataFrame(minmax.transform(X_test),columns=X_test.columns)\n\nclf = LogisticRegression()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","99c1b844":"clf = LogisticRegression(penalty='l1',solver='liblinear')\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","e9ae0b7a":"clf = GaussianNB()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","63fc3e8c":"clf = BernoulliNB()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","64aee869":"clf = BernoulliNB()\nclf.fit(X_train_minmax, y_train)\n\ny_preds = clf.predict_proba(X_test_minmax)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","b0d47939":"from sklearn.ensemble import RandomForestClassifier","485d7685":"clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train_minmax, y_train)\n\ny_preds = clf.predict_proba(X_test_minmax)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","6315bc15":"imp = pd.DataFrame({'Feature':X_train_minmax.columns, 'Importance': clf.feature_importances_})\nprint(imp.sort_values(by=['Importance'],ascending=False))","7b3ca899":"plt.figure(figsize=(12,5))\nplt.xticks(range(len(imp)), imp['Feature'])\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.title('Feature importance')\nplt.bar(range(len(imp)), imp['Importance'])\nplt.xticks(rotation=90);","38255386":"print(imp.sort_values(by=['Importance'],ascending=False).head(9))\nimp_features =  list(imp.sort_values(by=['Importance'],ascending=False)['Feature'].head(9)) ","1c40b162":"imp_features","629e7314":"X_train, X_test, y_train, y_test = train_test_split(heart_df[imp_features], \n                                                    heart_df['TenYearCHD'], \n                                                    test_size=0.2, \n                                                    random_state=42)","46e17c7a":"sc = StandardScaler()\nminmax = MinMaxScaler()\nX_train_sc , X_test_sc = pd.DataFrame(sc.fit_transform(X_train),columns=X_train.columns), pd.DataFrame(sc.transform(X_test),columns=X_test.columns)\nX_train_minmax , X_test_minmax = pd.DataFrame(minmax.fit_transform(X_train),columns=X_train.columns), pd.DataFrame(minmax.transform(X_test),columns=X_test.columns)\n\nclf = LogisticRegression()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","e50197ab":"clf = LogisticRegression(penalty='l1',solver='liblinear')\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","9ecc16d3":"clf = GaussianNB()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","d32ec3aa":"clf = BernoulliNB()\nclf.fit(X_train_sc, y_train)\n\ny_preds = clf.predict_proba(X_test_sc)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","c1dc06b2":"clf = BernoulliNB()\nclf.fit(X_train_minmax, y_train)\n\ny_preds = clf.predict_proba(X_test_minmax)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","84f41ded":"from sklearn.ensemble import RandomForestClassifier","3e82ff3f":"clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train_minmax, y_train)\n\ny_preds = clf.predict_proba(X_test_minmax)\n\n# take the second column because the classifier outputs scores for\n# the 0 class as well\npreds = y_preds[:,1]\n\n# fpr means false-positive-rate\n# tpr means true-positive-rate\nfpr, tpr, _ = roc_curve(y_test, preds)\n\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","ff7c4cf0":"imp = pd.DataFrame({'Feature':X_train_minmax.columns, 'Importance': clf.feature_importances_})\nprint(imp.sort_values(by=['Importance'],ascending=False))","bb7f496b":"plt.figure(figsize=(12,5))\nplt.xticks(range(len(imp)), imp['Feature'])\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.title('Feature importance')\nplt.bar(range(len(imp)), imp['Importance'])\nplt.xticks(rotation=90);","d87eec9a":"# Task - 1","f3de2e9c":"## Tasks\n\n1. Explore the data and build a Logistic Regression classifier to predict the `TenYearCHD` variable. \n1. Try a combination of variables in order to build the best classifier.\n1. Build ROC curves and calculate the AUC to compare the classifiers.\n1. Compare with Naive Bayes classifiers","e705b1eb":"# Task 2,3,4\n## Runing models with selected features"}}