{"cell_type":{"4f284414":"code","0935faed":"code","600d601b":"code","18b0e3e2":"code","53ef58a8":"code","8a97da4e":"code","41059f5c":"code","6f7d0579":"code","f6809ffb":"code","68ccda3a":"code","4cb70243":"code","70461dcd":"code","c7028836":"code","59edbeca":"code","0d31ef3a":"code","4d4bbdf9":"code","7ff88a50":"code","ad12cce9":"code","64b971ba":"code","5e4cf773":"code","5c652757":"code","a56480ef":"code","0acb70b5":"code","17059877":"code","8f349240":"code","72254b09":"code","d9f03a2a":"code","87332e4b":"code","8e14c6d6":"code","ac5e5bcc":"code","39e7165e":"code","bc6378e8":"code","9628e294":"code","147b003e":"code","af23edfe":"code","bb2fdf67":"code","51bdf3ec":"code","34425d8b":"code","a9db239d":"code","34717871":"code","c22b8a84":"code","4c92cb7f":"code","70b2e96b":"code","9f7ff413":"code","6b9b02f8":"code","c5c4c643":"code","1776aa5f":"code","f9f7f9f1":"code","270564b6":"code","ef52796d":"code","38038d95":"markdown","45664294":"markdown","a1e0b644":"markdown","8e7b8ad3":"markdown","c000c9ce":"markdown","9f55f894":"markdown","12cd511b":"markdown","1b132e74":"markdown","fe9e9109":"markdown","b57fcaae":"markdown","66b382f2":"markdown","4e4803f0":"markdown","eb165ed7":"markdown","92fe2f54":"markdown","c1603184":"markdown","51e5d1b0":"markdown","ceb5642c":"markdown","03f4e145":"markdown","9aa7d415":"markdown","dfd6cb7a":"markdown","c0a45e9d":"markdown","b2f56a93":"markdown","7815f145":"markdown","73b8da63":"markdown","b3123ccc":"markdown","bf87935f":"markdown","fa63ac9a":"markdown","5b32109a":"markdown","07dea9a4":"markdown"},"source":{"4f284414":"import matplotlib.pylab as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers","0935faed":"classifier_url =\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/classification\/2\"","600d601b":"IMAGE_SHAPE = (224, 224)\n\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))\n])","18b0e3e2":"import numpy as np\nimport PIL.Image as Image\n\nceleb_picture = '..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000137.jpg'\nceleb_picture = Image.open(celeb_picture).resize(IMAGE_SHAPE)\nceleb_picture","53ef58a8":"celeb_picture = np.array(celeb_picture)\/255.0\nresult = classifier.predict(celeb_picture[np.newaxis, ...])\nceleb_predicted_class = np.argmax(result[0], axis=-1)\nceleb_predicted_class","8a97da4e":"labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nplt.imshow(celeb_picture)\nplt.axis('off')\npredicted_class_name = imagenet_labels[celeb_predicted_class]\n_ = plt.title(\"Prediction: \" + predicted_class_name.title(),fontsize=20)","41059f5c":"dog_picture = '..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/dog.10.jpg'\ndog_picture = Image.open(dog_picture).resize(IMAGE_SHAPE)\ndog_picture","6f7d0579":"dog_picture = np.array(dog_picture)\/255.0\nresult = classifier.predict(dog_picture[np.newaxis, ...])\ndog_predicted_class = np.argmax(result[0], axis=-1)\ndog_predicted_class","f6809ffb":"labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nplt.imshow(dog_picture)\nplt.axis('off')\npredicted_class_name = imagenet_labels[dog_predicted_class]\n_ = plt.title(\"Prediction: \" + predicted_class_name.title(),fontsize=20)","68ccda3a":"img_dict = {\n    'Golf player': celeb_picture,\n    'Blue tick': dog_picture,\n}\n\nimg_name_tensors = {name: img.astype(np.float32) for (name, img) in img_dict.items()}","4cb70243":"plt.figure(figsize=(8, 8))\nfor n, (name, img_tensors) in enumerate(img_name_tensors.items()):\n  ax = plt.subplot(1, 2, n+1)\n  ax.imshow(img_tensors)\n  ax.set_title(name,fontsize=20)\n  ax.axis('off')\nplt.tight_layout()","70461dcd":"baseline = tf.zeros(shape=(224,224,3))","c7028836":"alphas = tf.linspace(start=0.0, stop=1.0, num=51)","59edbeca":"def interpolate_images(baseline,\n                       image,\n                       alphas):\n  alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n  baseline_x = tf.expand_dims(baseline, axis=0)\n  input_x = tf.expand_dims(image, axis=0)\n  delta = input_x - baseline_x\n  images = baseline_x +  alphas_x * delta\n  return images","0d31ef3a":"interpolated_images = interpolate_images(\n    baseline=baseline,\n    image=img_name_tensors['Golf player'],\n    alphas=alphas)","4d4bbdf9":"def compute_gradients(images, target_class_idx):\n  with tf.GradientTape() as tape:\n    tape.watch(images)\n    logits = classifier(images)\n    probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n  return tape.gradient(probs, images)","7ff88a50":"@tf.function\ndef integrated_gradients(baseline,\n                         image,\n                         target_class_idx,\n                         m_steps=300,\n                         batch_size=32):\n  # 1. Generate alphas\n  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps)\n\n  # Accumulate gradients across batches\n  integrated_gradients = 0.0\n\n  # Batch alpha images\n  ds = tf.data.Dataset.from_tensor_slices(alphas).batch(batch_size)\n\n  for batch in ds:\n\n    # 2. Generate interpolated images\n    batch_interpolated_inputs = interpolate_images(baseline=baseline,\n                                                   image=image,\n                                                   alphas=batch)\n\n    # 3. Compute gradients between model outputs and interpolated inputs\n    batch_gradients = compute_gradients(images=batch_interpolated_inputs,\n                                        target_class_idx=target_class_idx)\n\n    # 4. Average integral approximation. Summing integrated gradients across batches.\n    integrated_gradients += integral_approximation(gradients=batch_gradients)\n\n  # 5. Scale integrated gradients with respect to input\n  scaled_integrated_gradients = (image - baseline) * integrated_gradients\n  return scaled_integrated_gradients","ad12cce9":"def read_image(file_name):\n  image = tf.io.read_file(file_name)\n  image = tf.image.decode_jpeg(image, channels=3)\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n  return image","64b971ba":"def integral_approximation(gradients):\n  # riemann_trapezoidal\n  grads = (gradients[:-1] + gradients[1:]) \/ tf.constant(2.0)\n  integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n  return integrated_gradients","5e4cf773":"ig_attributions = integrated_gradients(baseline=baseline,\n                                       image=img_name_tensors['Golf player'],\n                                       target_class_idx=555)","5c652757":"path_gradients = compute_gradients(\n    images=interpolated_images,\n    target_class_idx=tf.constant(555))","a56480ef":"ig = integral_approximation(\n    gradients=path_gradients)","0acb70b5":"def plot_img_attributions(baseline,\n                          image,\n                          target_class_idx,\n                          m_steps=tf.constant(50),\n                          cmap=None,\n                          overlay_alpha=0.4):\n\n  attributions = integrated_gradients(baseline=baseline,\n                                      image=image,\n                                      target_class_idx=target_class_idx,\n                                      m_steps=m_steps)\n\n  # Sum of the attributions across color channels for visualization.\n  # The attribution mask shape is a grayscale image with height and width\n  # equal to the original image.\n  attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n\n  fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n  axs[0, 0].set_title('Baseline image')\n  axs[0, 0].imshow(baseline)\n  axs[0, 0].axis('off')\n\n  axs[0, 1].set_title('Original image')\n  axs[0, 1].imshow(image)\n  axs[0, 1].axis('off')\n    \n  axs[1, 0].set_title('Attribution mask')\n  axs[1, 0].imshow(attribution_mask, cmap=cmap)\n  axs[1, 0].axis('off')\n\n  axs[1, 1].set_title('Overlay')\n  axs[1, 1].imshow(attribution_mask, cmap=cmap)\n  axs[1, 1].imshow(image, alpha=overlay_alpha)\n  axs[1, 1].axis('off')\n  plt.tight_layout()\n  return fig","17059877":"_ = plot_img_attributions(image=img_name_tensors['Golf player'],\n                          baseline=baseline,\n                          target_class_idx=celeb_predicted_class,\n                          m_steps=2400,\n                          cmap=plt.cm.inferno,\n                          overlay_alpha=0.4)","8f349240":"_ = plot_img_attributions(image=img_name_tensors['Blue tick'],\n                          baseline=baseline,\n                          target_class_idx=dog_predicted_class,\n                          m_steps=1100,\n                          cmap=plt.cm.viridis,\n                          overlay_alpha=0.5)","72254b09":"data_root = tf.keras.utils.get_file('flower_photos',\n                                    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz', \n                                    untar=True)","d9f03a2a":"image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255)\nimage_data = image_generator.flow_from_directory(str(data_root), target_size=IMAGE_SHAPE)","87332e4b":"for image_batch, label_batch in image_data:\n  print(\"Image batch shape: \", image_batch.shape)\n  print(\"Label batch shape: \", label_batch.shape)\n  break","8e14c6d6":"result_batch = classifier.predict(image_batch)\nresult_batch.shape","ac5e5bcc":"predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=-1)]\npredicted_class_names","39e7165e":"plt.figure(figsize=(25,30))\nfor n in range(10):\n  plt.subplot(6,5,n+1)\n  plt.imshow(image_batch[n])\n  plt.title(predicted_class_names[n],fontsize=30)\n  plt.axis('off')","bc6378e8":"feature_extractor_url = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/2\"","9628e294":"feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n                                         input_shape=(224,224,3))","147b003e":"feature_batch = feature_extractor_layer(image_batch)\nprint(feature_batch.shape)","af23edfe":"feature_extractor_layer.trainable = False","bb2fdf67":"model = tf.keras.Sequential([\n  feature_extractor_layer,\n  layers.Dense(image_data.num_classes, activation='softmax')\n])\n\nmodel.summary()","51bdf3ec":"predictions = model(image_batch)","34425d8b":"model.compile(\n  optimizer=tf.keras.optimizers.Adam(),\n  loss='categorical_crossentropy',\n  metrics=['acc'])","a9db239d":"class CollectBatchStats(tf.keras.callbacks.Callback):\n  def __init__(self):\n    self.batch_losses = []\n    self.batch_acc = []\n\n  def on_train_batch_end(self, batch, logs=None):\n    self.batch_losses.append(logs['loss'])\n    self.batch_acc.append(logs['acc'])\n    self.model.reset_metrics()","34717871":"steps_per_epoch = np.ceil(image_data.samples\/image_data.batch_size)\nbatch_stats_callback = CollectBatchStats()\nhistory = model.fit_generator(image_data, epochs=2,\n                              steps_per_epoch=steps_per_epoch,\n                              callbacks = [batch_stats_callback])","c22b8a84":"import seaborn as sns\nplt.figure()\nplt.figure(figsize=(8,6))\nplt.ylabel(\"Loss\",fontsize=20)\nplt.xlabel(\"Training Steps\",fontsize=20)\nplt.ylim([0,2])\nplt.plot(batch_stats_callback.batch_losses)","4c92cb7f":"plt.figure()\nplt.figure(figsize=(8,6))\nplt.ylabel(\"Accuracy\",fontsize=20)\nplt.xlabel(\"Training Steps\",fontsize=20)\nplt.ylim([0,1])\nplt.plot(batch_stats_callback.batch_acc)","70b2e96b":"class_names = sorted(image_data.class_indices.items(), key=lambda pair:pair[1])\nprint(class_names)\nclass_names = np.array([key.title() for key, value in class_names])\nclass_names","9f7ff413":"predicted_batch = model.predict(image_batch)\n# predicted_batch.shape\npredicted_id = np.argmax(predicted_batch, axis=-1)\npredicted_label_batch = class_names[predicted_id]\nprint(predicted_label_batch)","6b9b02f8":"label_id = np.argmax(label_batch, axis=-1)","c5c4c643":"plt.figure(figsize=(25,30))\nfor n in range(10):\n  plt.subplot(6,5,n+1)\n  plt.imshow(image_batch[n])\n  color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\n  plt.title(predicted_label_batch[n].title(), color=color,fontsize=30)\n  plt.axis('off')","1776aa5f":"import time\nt = time.time()\n\nexport_path = \"\/tmp\/saved_models\/{}\".format(int(t))\nmodel.save(export_path, save_format='tf')","f9f7f9f1":"reloaded = tf.keras.models.load_model(export_path)","270564b6":"result_batch = model.predict(image_batch)\nreloaded_result_batch = reloaded.predict(image_batch)","ef52796d":"abs(reloaded_result_batch - result_batch).max()","38038d95":"<hr style=\"border: solid 3px blue;\">","45664294":"----------------------------------------------\n# Saving Model","a1e0b644":"---------------------------------------------------------\n# Downloading Pretrained Model\n\nIn this case, we will use mobilenet as followings.","8e7b8ad3":"### Making a feature extractor","c000c9ce":"<span style=\"color:Blue\"> **Observation**:    \n* Great! It is right prediction!","9f55f894":"<span style=\"color:Blue\"> **Observation**:    \n* Oh my god! The golf player was judged to be a bathing cap.","12cd511b":"<span style=\"color:Blue\"> **Observation**:    \n* Looking at the attribution mask picture, pixels judged to be important are very distracting. The model seems to be able to judge the person wearing the bathing cap, and it seems to be able to make the final judgment with the bathing cap.","1b132e74":"# Downloading the headless model for the fine tuning","fe9e9109":"----------------------------------------------------------------\n# Loading Model","b57fcaae":"------------------------------------------------------------------------------\n# Checking the the predict result","66b382f2":"-------------------------------------------\n# Training Model","4e4803f0":"**We try to understand transfer learning in this notebook. \nI would like to proceed with the notebook in the following order.**\n1. Download mobilenet v2 learned by ImageNet through TF hub.\n2. Predict with two images without funing tuning of mobilenet v2.\n3. Compare the correctly predicted image with the erroneously predicted image, and check the reason for this difference. Here, we use Integrated Gradients to understand the behavior of the model.\n4. Perform transfer learning using the flower_photos dataset, measure performance, and check the predicted results.\n5. Save the model and load the saved model to make predictions and check whether the model is saved properly.","eb165ed7":"**There is a bluetick at label 165 on ImageNet. That is, the pretrained model can distinguish blueticks.**","92fe2f54":"-------------------------------------\n# Checking results","c1603184":"## Predicting using Celeb image","51e5d1b0":"<span style=\"color:Blue\"> **Observation**:    \n* Looking at the attribution mask picture, the pretrained model considers the dog's stains, paws, and eyes as important. We already know which pixels are important to our model by training.","ceb5642c":"-------------------------------------------------------------------------------------------------------------\n# Checking images without fine tuning","03f4e145":"-------------------------------------------------------------\n# Predicting by classifier","9aa7d415":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![Transfer Learning](https:\/\/miro.medium.com\/max\/1838\/1*w8c3SbLq8b6wzsDgxPknzA.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\n\n**Why transfer learning?**\n\n* Training a convolution network from scratch is time-consuming and resource-intensive.\n* The more complex the model, the harder it is to train.\n* It takes a lot of effort to actually learn from scratch.\n* Finally, everything except the fully connected layer is used as a fixed feature extractor.\n* Define a new final classifier layer and apply it to training.\n\n","dfd6cb7a":"<hr style=\"border: solid 3px blue;\">\n\n# Understanding Models\n\n![](https:\/\/i.pinimg.com\/originals\/98\/3b\/63\/983b63592a42295d638b4b3120df9cc8.gif)\n\nPicture Credit: https:\/\/i.pinimg.com\n\n\n> **Integrated Gradient(IG)** computes the gradient of the model\u2019s prediction output to its input features and requires no modification to the original deep neural network.\nIG can be applied to any differentiable model like image, text, or structured data.\nIG can be used for\nUnderstanding feature importance by extracting rules from the network\nDebugging deep learning models performance\nIdentifying data skew by understanding the important features contributing to the prediction\n\nRef: https:\/\/towardsdatascience.com\/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf\n\n**We use Integrated Gradient to understand which pixels of the image play an important role, and based on this, we want to understand the previous results.**","c0a45e9d":"## Predicting using Dog image","b2f56a93":"By fixing the variables in the feature extractor layer, training can only change the new classification layer.","7815f145":"# Defining model\n\nUsing feature_extractor and the simple dense layers, model can be defined as followings.","73b8da63":"## Loading dataset","b3123ccc":"<hr style=\"border: solid 3px blue;\">\n\n# Doing Transfer Learning\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*lUyfhgm9mzEmtXWfxbp17w.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\n","bf87935f":"We can confirm that the results predicted by the reloaded model match the results predicted by the model before being saved.\n\nNow, we have successfully completed transfer learning and saved the model as well.\nOur stored model can be used for another transfer learning, or where a model predicting the follower type is needed.","fa63ac9a":"--------------------------------------------------","5b32109a":"There is no image labeled golf player in ImageNet. That is, the pretrained model does not learn golf players and is also unpredictable.\n\nImageNet has two golf related labels as shown below.\n*  574: 'golf ball',\n*  575: 'golfcart, golf cart'","07dea9a4":"The green title is the correct answer and the red title is the incorrect answer."}}