{"cell_type":{"e746a935":"code","e9d937cd":"code","41f0a41c":"code","1acad424":"code","7daf86ea":"code","90f9b863":"code","3a97de94":"code","2943774e":"code","f9b86b38":"code","09cc6d53":"code","8a430d68":"code","f35289fd":"code","607aab3d":"code","6ca61d41":"code","31b0bb7a":"code","2595177b":"code","ca0228e8":"markdown","183dd115":"markdown","c2c36cdb":"markdown","cd1128e9":"markdown","fa0e6c83":"markdown","5b61321f":"markdown","99842254":"markdown"},"source":{"e746a935":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install scikit-optimize==0.8.1\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom collections import Counter\n!pip install kneed\n# kneed is not installed in kaggle. uncomment the above line.\nfrom kneed import KneeLocator\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom skopt import space, gp_minimize","e9d937cd":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","41f0a41c":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","1acad424":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)","7daf86ea":"# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier(nthreads = -1)\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","90f9b863":"classifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist')\n# Defining the parameter grid for the Random Search.\nparam_grid = {\n    \"n_estimators\" : np.arange(100, 1000, 100),\n    \"max_depth\" : np.arange(1, 20, 2),\n    \"colsample_bytree\": np.arange(0.5,1, 0.1),\n    \"learning_rate\" : [0.0001, 0.001, 0.01, 0.1],\n    \"criterion\": [\"gini\",'entropy']\n}","3a97de94":"model = RandomizedSearchCV(estimator = classifier,\n                          param_distributions = param_grid,\n                          n_iter = 10,\n                          scoring = \"accuracy\",\n                          verbose = 10,\n                          n_jobs = -1,\n                          cv = 5)\nmodel.fit(X, label_encoder_y)\nmodel.best_score_","2943774e":"print(model.best_estimator_.get_params())","f9b86b38":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 600, \n                              max_depth = 5, \n                              colsample_bytree = 0.8,\n                              learning_rate = 0.1,\n                              criterion = \"entropy\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","09cc6d53":"dataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 600, \n                              max_depth = 5, \n                              colsample_bytree = 0.8,\n                              learning_rate = 0.1,\n                              criterion = \"entropy\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","8a430d68":"# optimize function for gp_minimize\ndef optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params))\n    model = xg.XGBClassifier(**params)\n    kf = StratifiedKFold(n_splits = 5)\n    accuracies = []\n    for idx in kf.split(X = x, y = y):\n        train_idx , test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n    \n    return -1.0 * np.mean(accuracies)","f35289fd":"# Parameter Space for XGBoost\nparam_space = [\n    space.Integer(3,15, name = 'max_depth'),\n    space.Integer(100, 600, name = 'n_estimators'),\n    space.Categorical(['gini', 'entropy'], name = 'criterion'),\n    space.Real(0.01,1, prior = 'uniform', name = 'colsample_bytree'),\n    space.Real(0.001,1, prior = 'uniform', name = 'learning_rate') \n]\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"colsample_bytree\",\n    \"learning_rate\"\n]\n","607aab3d":"# Optimization Function\noptimization_function = partial(\n    optimize,\n    param_names = param_names,\n    x = X,\n    y = label_encoder_y\n)","6ca61d41":"result = gp_minimize(optimization_function,\n                    dimensions = param_space,\n                    n_calls = 10,\n                    n_random_starts = 10,\n                    verbose = 10, \n                    n_jobs = -1\n)\nprint(dict(zip(param_names, result.x)))","31b0bb7a":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 171, \n                              max_depth = 12, \n                              colsample_bytree = 0.9444262241947871,\n                              learning_rate = 0.253008978,\n                              criterion = \"entropy\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","2595177b":"dataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 171, \n                              max_depth = 12, \n                              colsample_bytree = 0.9444262241947871,\n                              learning_rate = 0.253008978,\n                              criterion = \"entropy\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","ca0228e8":"Let's fit a model using above paramters.","183dd115":"### Hyper parameter Optimization using Random Search","c2c36cdb":"### Trying Bayesian Optimization using using Gaussian Process","cd1128e9":"XGBoost Model with default settings","fa0e6c83":"### Loading the dataset and data preprocessing","5b61321f":"Let's fit a model using above paramters.","99842254":"Making a submission using this model."}}