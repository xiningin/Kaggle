{"cell_type":{"94593450":"code","0f6a8c68":"code","12093a2e":"code","fd43dd97":"code","a85ff3d4":"code","a2909155":"code","629962fe":"code","28106372":"code","d0fc341d":"code","6b18b6c5":"code","2d8f69e2":"code","59bf95af":"code","d65d64f6":"code","d7cd4308":"code","af89dc4f":"code","2349b286":"code","a45a40c6":"code","0f2ac553":"code","3d207bb7":"code","a8f0cc74":"code","a470ad2e":"code","1b5f9289":"code","208feb28":"code","bd908cf8":"code","36615788":"code","a02bfb87":"markdown","b556082a":"markdown","19650b90":"markdown","603e7ac7":"markdown","1aeaf6cb":"markdown"},"source":{"94593450":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f6a8c68":"#Importing familiar libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.style.use('classic')\n%matplotlib inline\n\n# For ordinal encoding of the categorical variables, splitting data\nimport sklearn as sk\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,OrdinalEncoder, FunctionTransformer\nfrom sklearn.model_selection import train_test_split\n\n#To Pipeline the process \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold\n\n#Grid Search\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom xgboost import XGBRegressor\nfrom xgboost import cv\nfrom xgboost import plot_importance\nfrom sklearn.decomposition import PCA\n\n#from yellowbrick.target import FeatureCorrelation\n\nimport tensorflow as tf\nimport matplotlib as mpl\nimport xgboost as xgb\n\npd.set_option('max_columns', None)","12093a2e":"#We set index_col=0 in the code cell below to use the id column to index the DataFrame.\nX_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\nX_full.head()","fd43dd97":"# check for datatypes and missing data\n\nX_full.info()\nprint('*'*50)\nX_full.isnull().sum()","a85ff3d4":"X_full.describe(include='all')","a2909155":"categorical_features=[col for col in X_full.columns if X_full[col].nunique() <= 15  and X_full[col].dtype==\"object\"]\nnumeric_features = [col for col in X_full.columns if X_full[col].dtype in ['int64','float64']]\n\nmy_features = categorical_features+numeric_features\n\nprint('categorical_features:', categorical_features)\nprint('numeric_features:', numeric_features)\nprint('my_features:', my_features)\n\nnumeric_features.remove('target')\nprint('numeric_features minus target column:', numeric_features)","629962fe":"correlations = X_full[my_features].corr()\nf,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(correlations,square=True,cbar=True,annot=True,vmax=0.9)","28106372":"X_full[numeric_features].hist(figsize=(24,12))","d0fc341d":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns', fontsize=16);","6b18b6c5":"sns.boxplot(data=X_full[['target']],orient='h',palette=\"Set2\")\nplt.xticks(fontsize=14)\nplt.title('Box plot of target column', fontsize=16);","2d8f69e2":"from scipy import stats\ndef treatoutliers(df=None,columns=None,factor=1.5, method='IQR', treatment='cap'):\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n#         print(floor, ceil)\n        if treatment == 'remove':\n            print(treatment, column)\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            print(treatment, column)\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n\n\n#Quantile-based Flooring and Capping\nfor colName in [['target','cont0','cont6','cont8']]:\n    X_full = treatoutliers(df=X_full,columns=colName, treatment='cap')      \n    \nX_full.info()","59bf95af":"sns.boxplot(data=X_full[['target']], orient=\"h\", palette=\"Set2\" );\nplt.xticks(fontsize= 14)\nplt.title('Box plot of target column after handling Outliers', fontsize=16);","d65d64f6":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns after handling Outliers', fontsize=16);","d7cd4308":"# seperating target and features\nX_full.dropna(axis=0,subset=['target'],inplace=True)\ny=X_full['target']\nX_full.drop(['target'],axis=1,inplace=True)\n\nX_full.head()","af89dc4f":"y.head()","2349b286":"X_train_full,X_valid_full,y_train,y_valid = train_test_split(X_full,y,train_size=0.9,test_size=0.1,random_state=0)","a45a40c6":"categorical_features  = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() <= 15 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\n#\nprint('categorical_features', categorical_features)\nprint('numeric_features', numeric_features)\nprint('my_features', my_features)","0f2ac553":"X_train = X_train_full[my_features]\nX_valid = X_valid_full[my_features]\nX_test=X_test_full[my_features]","3d207bb7":"X_train.head()","a8f0cc74":"X_train.describe(include='all')\n","a470ad2e":"X_train.shape","1b5f9289":"%%time\n\nrans = 42\n\ndef log_transform(x):\n    return np.log(x + 1)\n\n\ntransformer = FunctionTransformer(log_transform)\n\nnumerical_transformer=Pipeline(steps=[('imputer',SimpleImputer(strategy='mean')),('transformer',transformer),('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    #('imputer', SimpleImputer(strategy='constant'))\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    ,('onehot', OneHotEncoder(handle_unknown='ignore'))\n    #,('scaler', OrdinalEncoder())\n])    \n\npreprocessor = ColumnTransformer(transformers=[\n        ('num', numerical_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder=\"passthrough\"\n  )","208feb28":"xgb_params = {'n_estimators': 4575, 'max_depth': 3, 'learning_rate': 0.03928410410450762, \n              'gamma': 0.30000000000000004, 'min_child_weight': 3, \n              'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 0.2, 'reg_lambda': 0.5}\n\nxgb_params = {'n_estimators': 4575, 'learning_rate': 0.07853392035787837, 'reg_lambda': 1.7549293092194938e-05, 'reg_alpha': 14.68267919457715, \n              'subsample': 0.8031450486786944, 'colsample_bytree': 0.170759104940733, \n              'max_depth': 3}\n\nmodel = XGBRegressor(**xgb_params,tree_method='gpu_hist')","bd908cf8":"clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      #('pca',pca),\n                      ('model', model)\n                     ])\n\nfinal_model = clf.fit(X_train, y_train)    \npreds_valid = final_model.predict(X_valid)\n\nprint('MAE:',mean_absolute_error(y_valid, preds_valid))\nprint('RMSE:',mean_squared_error(y_valid, preds_valid, squared=False))","36615788":"predictions = final_model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","a02bfb87":"## Treating the Outliers","b556082a":"Looks like few outliers in Cont0, Cont6, Cont8, target columns. Lets check the ouliers in target column now.\n\n","19650b90":"## Train the model","603e7ac7":"### Correlation ","1aeaf6cb":"## **LOAD THE DATA**"}}