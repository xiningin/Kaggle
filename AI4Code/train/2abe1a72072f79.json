{"cell_type":{"d0a77246":"code","28f7c20c":"code","7c59b025":"code","f2e53924":"code","e6beecf7":"code","0c481cea":"code","de5aef42":"code","43d8d36c":"code","721db3c1":"code","043172d1":"code","cdc755b4":"code","a10f4322":"code","455c892c":"code","709f0dce":"code","5d32e924":"code","e11805e5":"code","04890d0e":"code","5723114c":"code","c1fe3425":"code","9d307b65":"code","ee400922":"code","c02406c1":"code","a2855b47":"code","cfe0beeb":"code","83676876":"code","13a0d149":"code","2b3b0b5e":"code","2ac8c1c3":"code","bf31b2dc":"code","1eaf1307":"code","c9953bf0":"code","d91aa503":"code","a57af818":"code","478eb072":"code","732a324d":"code","80326659":"code","13366456":"code","3339c601":"code","3f322929":"code","b1c95bcf":"code","cca9a532":"code","4b9c96d6":"code","a86690f7":"code","75190126":"code","1945575c":"code","73bad967":"code","6be7e1c1":"code","5cfc8e78":"code","db115a96":"code","3108b963":"code","9625fd81":"code","0de7cb5c":"code","556d0882":"code","724acfef":"code","a111a0ba":"code","af960135":"code","54d3c3dd":"code","ce7b9116":"code","2af84243":"code","8e8ced91":"code","9e49baf0":"code","32528ab5":"code","8ca015d6":"code","eee656e2":"code","3d10c17a":"markdown","c2e5a8d7":"markdown","6f6c85a4":"markdown","15daf76d":"markdown","5339f58d":"markdown","8f80bf40":"markdown","d4bd6fe2":"markdown","cf9377c5":"markdown","b88d9b30":"markdown","9619870b":"markdown","68541164":"markdown","4e7c43bc":"markdown","aef40df2":"markdown","dc8b0a29":"markdown","0b25311f":"markdown","cf6bea12":"markdown","02c510bc":"markdown","ddc07956":"markdown","0cec2bda":"markdown"},"source":{"d0a77246":"import os\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)","28f7c20c":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","7c59b025":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f2e53924":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 2    \nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","e6beecf7":"# setting up datasets directory\nbasedir = '..\/input\/shopee-code-league-20\/_DS_Sentiment_Analysis_'","0c481cea":"train_df = pd.read_csv(os.path.join(basedir,'modified_sentiments','train (add leak).csv'))\noriginal_df = train_df.copy()\n\n# Credits to Tony NG's Shopee website's scraping dataset to improve model training\/fitting\nadded_df = pd.read_csv(os.path.join(basedir,'shopee_review_scraped','shopee_reviews.csv'))\n\ntest_df = pd.read_csv(os.path.join(basedir,'modified_sentiments','test.csv'))\n\n# Preprocessing\ntrain_df.drop('review_id', axis=1, inplace=True)\n\nprint('Train shape:', train_df.shape)\nprint('Test shape:', test_df.shape)","de5aef42":"review_train = train_df['review'].tolist()\nreview_test = test_df['review'].tolist()\n\nprint(len(set(review_train).intersection(set(review_test))))\n\nsame_data_list = list(set(review_train).intersection(set(review_test)))\nsame_data_list[0:5]","43d8d36c":"added_df = added_df.rename(columns={'label': 'rating','text':'review'})\n\nadded_df.iloc[1431262]","721db3c1":"# Drop the trash data\nadded_df = added_df.drop(1431262)","043172d1":"# Use this to use both shopee data and scraped data\ntrain_df = train_df.append(added_df,ignore_index = True)","cdc755b4":"# change type from string `object` to integer\ntrain_df['rating'] = train_df['rating'].astype(int)","a10f4322":"train_df['rating'].value_counts()","455c892c":"review_train = added_df['review'].tolist()\nreview_test = test_df['review'].tolist()\n#Inspect data leak (after adding scraped data)\nmatched_reviews = set(review_train).intersection(set(review_test))\nprint('Matched reviews from scraped data and the test set:', len(matched_reviews))","709f0dce":"import emoji\ndef emoji_cleaning(text):\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","5d32e924":"have_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(train_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)","e11805e5":"train_emoji_percentage = round(len(have_emoji_train_idx) \/ train_df.shape[0] * 100, 2)\nprint(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) \/ test_df.shape[0] * 100, 2)\nprint(f'Test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total')","04890d0e":"train_df_original = train_df.copy()\ntest_df_original = test_df.copy()\n\n# emoji_cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'] = train_df.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)\ntest_df.loc[have_emoji_test_idx, 'review'] = test_df.loc[have_emoji_test_idx, 'review'].apply(emoji_cleaning)","5723114c":"# before cleaning\ntrain_df_original.loc[have_emoji_train_idx, 'review'].tail()","c1fe3425":"# after cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'].tail()","9d307b65":"import string\nstring.punctuation","ee400922":"# Prints the distribution of the train set\noriginal_df['rating'].value_counts(normalize = True)","c02406c1":"for punc in string.punctuation:\n    print(punc)\n    print(original_df[original_df['review'].str.contains(punc,regex=False)].rating.value_counts(normalize = True))\n    print('------------------------------------------------------------')","a2855b47":"import re\ndef review_cleaning(text):\n    \n    # delete lowercase and newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    text = re.sub('([.,!?()])', r' \\1 ', text)\n    text = re.sub('\\s{2,}', ' ', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    # We decide to include punctuation in the model so we comment this line out!\n    # text = re.sub('[^a-z0-9! ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])","cfe0beeb":"train_df['review'] = train_df['review'].apply(review_cleaning)\ntest_df['review'] = test_df['review'].apply(review_cleaning)","83676876":"repeated_rows_train = []\nrepeated_rows_test = []\n\nfor idx, review in enumerate(train_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_test.append(idx)","13a0d149":"def delete_repeated_char(text):\n    \n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text","2b3b0b5e":"train_df.loc[repeated_rows_train, 'review'] = train_df.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)\ntest_df.loc[repeated_rows_test, 'review'] = test_df.loc[repeated_rows_test, 'review'].apply(delete_repeated_char)","2ac8c1c3":"print('Before: ', train_df_original.loc[92129, 'review'])\nprint('After: ', train_df.loc[92129, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[56938, 'review'])\nprint('After: ', train_df.loc[56938, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[72677, 'review'])\nprint('After: ', train_df.loc[72677, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[36558, 'review'])\nprint('After: ', train_df.loc[36558, 'review'])","bf31b2dc":"def recover_shortened_words(text):\n    \n    # put \\b (boundary) for avoid the characters in the word to be replaced\n    # I only make a few examples here, you can add if you're interested :)\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","1eaf1307":"%%time\ntrain_df['review'] = train_df['review'].apply(recover_shortened_words)","c9953bf0":"rating_mapper_encode = {1: 0,\n                        2: 1,\n                        3: 2,\n                        4: 3,\n                        5: 4}\n\n# convert back to original rating after prediction later(dont forget!!)\nrating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)","d91aa503":"train_df['rating'].value_counts()","a57af818":"from sklearn.utils import resample\ndf_majority = train_df[train_df.rating==4]\ndf_other = train_df[train_df.rating!=4]\n\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=500000,     # to match minority class\n                                 random_state=123) # reproducible results\n\ntrain_df = pd.concat([df_majority_downsampled, df_other])","478eb072":"# zero,one,two,three,four = np.bincount(train_df['rating'])\n# total = zero + one + two + three + four\n\n\n# weight_for_0 = (1 \/ zero)*(total)\/5 \n# weight_for_1 = (1 \/ one)*(total)\/5\n# weight_for_2 = (1 \/ two)*(total)\/5\n# weight_for_3 = (1 \/ three)*(total)\/5\n# weight_for_4 = (1 \/ four)*(total)\/5\n\n# class_weight = {0: weight_for_0, 1: weight_for_1, 2:weight_for_2,3:weight_for_3,4:weight_for_4}\n# class_weight","732a324d":"# Dropping some duplicates\ntrain_df = train_df.drop_duplicates(subset =\"review\")","80326659":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train_df['rating'], num_classes=5)","13366456":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=2020)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","3339c601":"MODEL = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFAutoModel.from_pretrained('bert-base-uncased')","3f322929":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.distplot(train_df['review'].str.len())\nMAX_LEN = 320\nplt.show()","b1c95bcf":"X_train_encode = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val_encode = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test_encode = regular_encode(test_df['review'].values, tokenizer, maxlen=MAX_LEN)","cca9a532":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_encode, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_encode, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test_encode)\n    .batch(BATCH_SIZE)\n)","4b9c96d6":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    sequence_output = tf.keras.layers.Dropout(0.2)(sequence_output)   \n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token) # 5 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","a86690f7":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","75190126":"n_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","1945575c":"import matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\n\n# Get training and test loss histories\ntraining_loss = train_history.history['loss']\ntest_loss = train_history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","73bad967":"pred = model.predict(test_dataset, verbose=1)","6be7e1c1":"# Check if this works\n# saving the temporary model as pickle\n\nimport pickle\nwith open('pred_bert.pkl','wb') as f:\n    pickle.dump(pred, f)","5cfc8e78":"pred_sentiment = np.argmax(pred,axis = 1)","db115a96":"import pickle\nimport os","3108b963":"bert_cased_model = os.path.join(basedir, 'bert', 'bert-cased-500k.pkl')\nwith open(bert_cased_model,'rb') as f:\n    pred_bert500_cased = pickle.load(f)\n    print(pred_bert500_cased.shape)","9625fd81":"bert_uncased_model = os.path.join(basedir, 'bert', 'bert-uncased-500k.pkl')\nwith open(bert_uncased_model,'rb') as f:\n    pred_bert500_uncased = pickle.load(f)\n    print(pred_bert500_uncased.shape)","0de7cb5c":"bert_320_model = os.path.join(basedir, 'bert', 'bert-based-500k-320length.pkl')\nwith open(bert_320_model,'rb') as f:\n    pred_bert500_cased_len320 = pickle.load(f)\n    print(pred_bert500_cased_len320.shape)","556d0882":"pred_bert = (pred_bert500_cased + pred_bert500_uncased + pred_bert500_cased_len320)\/3","724acfef":"gpt_500_model = os.path.join(basedir, 'bert', 'gpt2_pred_500k_len250.pkl')\nwith open(gpt_500_model,'rb') as f:\n    pred_gpt500 = pickle.load(f)\n    print(pred_gpt500.shape)","a111a0ba":"gpt_400_model = os.path.join(basedir, 'roberta', 'gpt2_pred_4epoch_400.pkl')\nwith open(gpt_400_model,'rb') as f:\n    pred_gpt = pickle.load(f)\n    print(pred_gpt.shape)","af960135":"pred_gpt = (pred_gpt500 + pred_gpt)\/2","54d3c3dd":"roberta_model = os.path.join(basedir, 'roberta', 'ROBERTA_pred_4epoch_subscore66.pkl')\nwith open(roberta_model,'rb') as f:\n    pred_roberta = pickle.load(f)\n    print(pred_roberta.shape)","ce7b9116":"pred = (pred_bert + pred_gpt + pred_roberta ) \/ 3","2af84243":"final_pred = []\nconfident_3_index = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 2 and p[2]>0.438: \n#         print(idx)\n        confident_3_index.append(idx)\n        final_pred.append(2)  #because it's 0-4\n    else:\n        p[2] = 0\n        final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)","8e8ced91":"final_pred = []\nconfident_4_index = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 3 and p[3]>0.34: \n#         print(idx)\n        confident_3_index.append(idx)\n        final_pred.append(3)  #because it's 0-4\n    else:\n        p[3] = 0\n        final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)\n# [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","9e49baf0":"final_pred = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 4 and p[4]>0.2: \n#         print(idx)\n        final_pred.append(4)  #because it's 0-4\n    else:\n        p[4] = 0\n        if p[0] > 0.14:\n            final_pred.append(0)\n        else:\n            final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)\n# [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","32528ab5":"rating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\nsubmission['rating'] = submission['rating'].map(rating_mapper_decode)\n\n# 3-models-ensemble\nsubmission.to_csv('submission.csv', index=False)","8ca015d6":"!head submission.csv","eee656e2":"# Public test set distribution  : [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]\nsubmission.rating.value_counts(normalize = True)","3d10c17a":"## BERT","c2e5a8d7":"## Ensembling and Distribution Adjustments\n\nInspired by [garyong](https:\/\/www.kaggle.com\/garyongguanjie\/lb-dist-hacking-final)'s work and [huikang](https:\/\/www.kaggle.com\/huikang\/week6-process-gpu-tpu-output)'s solution\n\nThe method we use is to keep confident predictions and throw out inconfident predictions to other class by adjusting probabilities threshold.\n\n#### Note that the public distribution is [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","6f6c85a4":"## Cleaning\n\nThe cleaning below is mostly based on Indra Lin`s Notebook with some adjustments","15daf76d":"### Credits to [Wen San Xu](https:\/\/www.kaggle.com\/sanxuwen\/shopee-sentiment-analysis-2nd-place-solution)\n\n#### Additional reference from [Indra Lin](https:\/\/www.kaggle.com\/indralin\/text-processing-augmentation-tpu-baseline-0-4544)\n\nP.S. The notebook ensembles Bert-base, GPT-2, and ROBERTa with some distribution adjustments.","5339f58d":"## EDA some effects of punctuation","8f80bf40":"## ROBERTA","d4bd6fe2":"We tried to include class weights in our model but it seems it doesnt help with our predictions. (~2% worse)","cf9377c5":"## Review some train data that matches the test set","b88d9b30":"## Append original train with the scraped training data","9619870b":"## Training\n\nThanks to Indra Lin :D","68541164":"Since the scraped data mostly contains rating = 5 (4 in a 0-4 scale). Sadly, we did not have much time to try to choose what to include so we have to downsampled to about ~500k of rating = 5 to account for class imbalance and reduce training time.","4e7c43bc":"### Observations\n\nSeems like reviews with different punctuation has different target distribution which may help our model predict better!\n\nFortunately, pre-trained models has ids representing most punctuation so this is good for us!","aef40df2":"It seems that most of the reviews are about 300words. We have experiemented MAX_LEN = 512 but the run time takes a little too long so we go with 320 just to be safe.","dc8b0a29":"### Adding a dropout layer have improved our performance ~2%","0b25311f":"## Utilizing Pre-trained Models\n\nWe use the code above to create multiple BERT, GPT-2, and ROBERTA models and the final result is a simple average of the models (GPT-2,BERT, and ROBERTA)","cf6bea12":"Seems like there're some leakage but most of them are general review like \"Awesome product\", \"Fast respond\" etc...","02c510bc":"## Inspecting Data Leak\n\nThe data scraped may match the test set reviews so we need to verify that the most of the scraped data doesn't match the reviews in the test set\n\n[Wen SanXu](https:\/\/www.kaggle.com\/sanxuwen): \"I do not want to feel guilty about using too much leaked data.\"","ddc07956":"## GPT-2","0cec2bda":"After a number of experiment trials of epochs training:\noptimal validation score configurations:\n* BERT: 2 Epochs\n* GPT-2 : 4 Epochs\n* ROBERTA: 4 Epochs"}}