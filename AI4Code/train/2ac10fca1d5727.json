{"cell_type":{"845a4eac":"code","9d761e18":"code","75da9fc6":"code","dceb433d":"code","6bfa324f":"code","05280bad":"code","56ee7e9c":"code","efb7e7ea":"code","41c232c3":"code","06f3a816":"code","76adcc73":"code","dc9a0047":"code","9ea89ff5":"code","7fb168d6":"code","782742ce":"code","6b682c4c":"code","3b71f0cb":"code","04dff32b":"markdown","bf1b1d68":"markdown","47d60e44":"markdown","d1142a1c":"markdown","97a6e256":"markdown","99b7b8a2":"markdown","3452b920":"markdown","b4d60ec4":"markdown","12e2c9f2":"markdown","0c8c0170":"markdown","77213d80":"markdown","462df178":"markdown","74142e7f":"markdown","db87c42a":"markdown","0da5eee5":"markdown","7b97c48e":"markdown"},"source":{"845a4eac":"import pandas as pd\nimport numpy as np\nimport keras\nfrom google.cloud import bigquery","9d761e18":"query = '''\n#standardSQL\nSELECT\n  id,\n  title,\n  REGEXP_REPLACE(NET.HOST(url), 'www.', '') AS domain,\n  FORMAT_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", timestamp, \"America\/New_York\") AS created_at,\n  score,\n  TIMESTAMP_DIFF(LEAD(timestamp, 30) OVER (ORDER BY timestamp), timestamp, SECOND) as time_on_new\nFROM\n  `bigquery-public-data.hacker_news.full`\nWHERE\n  DATETIME(timestamp, \"America\/New_York\") BETWEEN '2017-01-01 00:00:00' AND '2018-08-01 00:00:00'\n  AND type = \"story\"\n  AND url != ''\n  AND deleted IS NULL\n  AND dead IS NULL\nORDER BY\n  created_at DESC\n'''\n\nclient = bigquery.Client()\n\nquery_job = client.query(query)\n\niterator = query_job.result(timeout=30)\nrows = list(iterator)\n\ndf = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n\ndf = df.sample(frac=1, random_state=123).dropna().reset_index(drop=True)\ndf.head(10)","75da9fc6":"from keras.preprocessing import sequence\nfrom keras.preprocessing.text import text_to_word_sequence, Tokenizer\n\nnum_words = 20000\n\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(df['title'].values)","dceb433d":"maxlen = 15\n\ntitles = tokenizer.texts_to_sequences(df['title'].values)\ntitles = sequence.pad_sequences(titles, maxlen=maxlen)\nprint(titles[0:5,])","6bfa324f":"num_domains = 100\n\ndomain_counts = df['domain'].value_counts()[0:num_domains]\n\nprint(domain_counts)","05280bad":"from sklearn.preprocessing import LabelBinarizer\n\ntop_domains = np.array(domain_counts.index, dtype=object)\n\ndomain_encoder = LabelBinarizer()\ndomain_encoder.fit(top_domains)\n\ndomains = domain_encoder.transform(df['domain'].values.astype(str))\ndomains[0]","56ee7e9c":"from keras.utils import to_categorical\n\ndayofweeks = to_categorical(pd.to_datetime(df['created_at']).dt.dayofweek)\nhours = to_categorical(pd.to_datetime(df['created_at']).dt.hour)\n\nprint(dayofweeks[0:5])\nprint(hours[0:5])","efb7e7ea":"weights = np.where(df['score'].values == 1, 0.5, 1.0)\nprint(weights[0:5])","41c232c3":"from sklearn.preprocessing import MinMaxScaler\n\ntrend_encoder = MinMaxScaler()\ntrends = trend_encoder.fit_transform(pd.to_datetime(df['created_at']).values.reshape(-1, 1))\ntrends[0:5]","06f3a816":"newtime_encoder = MinMaxScaler()\nnewtimes = trend_encoder.fit_transform(df['time_on_new'].values.reshape(-1, 1))\nnewtimes[0:5]","76adcc73":"from keras import backend as K\n\ndef r_2(y_true, y_pred):\n    SS_res =  K.sum(K.square( y_true - y_pred )) \n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )","dc9a0047":"def hybrid_loss(y_true, y_pred):\n    weight_mae = 0.1\n    weight_msle = 1.\n    weight_poisson = 0.1\n    \n    mae_loss = weight_mae * K.mean(K.abs(y_pred - y_true), axis=-1)\n    \n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    msle_loss = weight_msle * K.mean(K.square(first_log - second_log), axis=-1)\n    \n    poisson_loss = weight_poisson * K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)\n    return mae_loss + msle_loss + poisson_loss","9ea89ff5":"from keras.models import Input, Model\nfrom keras.layers import Dense, Embedding, CuDNNGRU, CuDNNLSTM, LSTM, concatenate, Activation, BatchNormalization\nfrom keras.layers.core import Masking, Dropout, Reshape, SpatialDropout1D\nfrom keras.regularizers import l1, l2\n\ninput_titles = Input(shape=(maxlen,), name='input_titles')\ninput_domains = Input(shape=(num_domains,), name='input_domains')\ninput_dayofweeks = Input(shape=(7,), name='input_dayofweeks')\ninput_hours = Input(shape=(24,), name='input_hours')\n# input_trend = Input(shape=(1,), name='input_trend')\n# input_newtime = Input(shape=(1,), name='input_newtime')\n\nembedding_titles = Embedding(num_words + 1, 50, name='embedding_titles', mask_zero=False)(input_titles)\nspatial_dropout = SpatialDropout1D(0.2, name='spatial_dropout')(embedding_titles)\nrnn_titles = CuDNNLSTM(128, name='rnn_titles')(spatial_dropout)\n\nconcat = concatenate([rnn_titles, input_domains, input_dayofweeks, input_hours], name='concat')\n\nnum_hidden_layers = 3\n\nhidden = Dense(128, activation='relu', name='hidden_1', kernel_regularizer=l2(1e-2))(concat)\nhidden = BatchNormalization(name=\"bn_1\")(hidden)\nhidden = Dropout(0.5, name=\"dropout_1\")(hidden)\n\nfor i in range(num_hidden_layers-1):\n    hidden = Dense(256, activation='relu', name='hidden_{}'.format(i+2), kernel_regularizer=l2(1e-2))(hidden)\n    hidden = BatchNormalization(name=\"bn_{}\".format(i+2))(hidden)\n    hidden = Dropout(0.5, name=\"dropout_{}\".format(i+2))(hidden)\n    \noutput = Dense(1, activation='relu', name='output', kernel_regularizer=l2(1e-2))(hidden)\n\nmodel = Model(inputs=[input_titles,\n                      input_domains,\n                      input_dayofweeks,\n                      input_hours],\n                      outputs=[output])\n\nmodel.compile(loss=hybrid_loss,\n              optimizer='adam',\n              metrics=['mse', 'mae', r_2])\n\nmodel.summary()","7fb168d6":"from keras.callbacks import LearningRateScheduler, Callback\n\nbase_lr = 1e-3\nnum_epochs = 25\nsplit_prop = 0.2\n\ndef lr_linear_decay(epoch):\n            return (base_lr * (1 - (epoch \/ num_epochs)))\n    \nmodel.fit([titles, domains, dayofweeks, hours], [df['score'].values],\n          batch_size=1024,\n          epochs=num_epochs,\n          validation_split=split_prop,\n          callbacks=[LearningRateScheduler(lr_linear_decay)],\n          sample_weight=weights)","782742ce":"val_size = int(split_prop * df.shape[0])\n\npredictions = model.predict([titles[-val_size:],\n                             domains[-val_size:],\n                             dayofweeks[-val_size:],\n                             hours[-val_size:]])[:, 0]\n\npredictions","6b682c4c":"df_preds = pd.concat([pd.Series(df['title'].values[-val_size:]),\n                      pd.Series(df['score'].values[-val_size:]),\n                      pd.Series(predictions)],\n                     axis=1)\ndf_preds.columns = ['title', 'actual', 'predicted']\n# df_preds.to_csv('hn_val.csv', index=False)\ndf_preds.head(50)","3b71f0cb":"train_size = int((1-split_prop) * df.shape[0])\n\npredictions = model.predict([titles[:train_size],\n                             domains[:train_size],\n                             dayofweeks[:train_size],\n                             hours[:train_size]])[:, 0]\n\ndf_preds = pd.concat([pd.Series(df['title'].values[:train_size]),\n                      pd.Series(df['score'].values[:train_size]),\n                      pd.Series(predictions)],\n                     axis=1)\ndf_preds.columns = ['title', 'actual', 'predicted']\n# df_preds.to_csv('hn_train.csv', index=False)\ndf_preds.head(50)","04dff32b":"### Top Domains\n\nIdentify the top *n* domains by count (in this case *n* = 100), then transform it to a *n*D vector for each post.","bf1b1d68":"Add R^2 as a performance metric: https:\/\/jmlb.github.io\/ml\/2017\/03\/20\/CoeffDetermination_CustomMetric4Keras\/","47d60e44":"### Text\n\nUse a RNN to encode the title. Since we'll be using an unmasked RNN, length of the submission can be implied from the number of padding characters.","d1142a1c":"# Hacker News Submission Score Predictor w\/ Keras and TensorFlow\n\nby Max Woolf ([@minimaxir](https:\/\/minimaxir.com))\n\nA model of a Hacker News post predictor, using a large number of Keras tricks with a TensorFlow backend.\n\nThis notebook requires a GPU instance. (for the very-fast `CuDNNLSTM` to handle text data)","97a6e256":"### Day-of-Week and Hour\n\nConvert day-of-week to a 7D vector and hours to a 24D vector. Both pandas and keras have useful functions for this workflow.","99b7b8a2":"## Trend and Time on New\n\nUnused in final model, but kept here for reference.","3452b920":"## Feature Engineering\n\n* Text, w\/ sequences of length 15 (HN titles can be from 3 - 80 characters; since words are 5-6 characters)\n* Post domain (if in Top 100 by count; 0 otherwise)\n* Day of Week of Submission\n* Hour of Submission\n\nOther features I tried but did not use (since using them prevents forecasting, and they did not help improve the model):\n\n* Trend (time from first submission, scaled to `[0-1]`)\n* Time on `\/new` page (scaled to `[0-1]`)\n\nScore is unmodified. Normally you'd `log` transform a skewed independent variable for a OLS, but that's not necessary for deep learning.","b4d60ec4":"# LICENSE\n\nMIT License\n\nCopyright (c) 2018 Max Woolf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","12e2c9f2":"The model uses a linear learning rate decay to allow it to learn better once it starts converging.\n\nNote: in this Kaggle Notebook, the training times out after 33 epochs when committing, so I set it to 25 here. You should probably train for longer. (50+ epochs)","0c8c0170":"## Check Predictions Against Validation Set\n\nPredicting against data that was not trained in the model: the model does this poorly. :(","77213d80":"Minimizing `mse` loss as typical for regression problems will not work, as the model will realize that selecting 1 unilaterally accomplishes this task the best.\n\nInstead, create a hybrid loss of `mae`, `msle`, and `poisson` (see Keras's docs for more info: https:\/\/github.com\/keras-team\/keras\/blob\/master\/keras\/losses.py) The latter two losses can account for very high values much better; perfect for the hyper-skewed data.","462df178":"## Build the Model Prototype","74142e7f":"## Check Predictions Against Training Set\n\nThe model should be able to predict these better.","db87c42a":"BigQuery:\n\n```sql\n#standardSQL\nSELECT\n  id,\n  title,\n  REGEXP_REPLACE(NET.HOST(url), 'www.', '') AS domain,\n  FORMAT_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", timestamp, \"America\/New_York\") AS created_at,\n  score,\n  TIMESTAMP_DIFF(LEAD(timestamp, 30) OVER (ORDER BY timestamp), timestamp, SECOND) as time_on_new\nFROM\n  `bigquery-public-data.hacker_news.full`\nWHERE\n  DATETIME(timestamp, \"America\/New_York\") BETWEEN '2017-01-01 00:00:00' AND '2018-08-01 00:00:00'\n  AND type = \"story\"\n  AND url != ''\n  AND deleted IS NULL\n  AND dead IS NULL\nORDER BY\n  created_at DESC\n```","0da5eee5":"Use the query above to get it from BigQuery. (via Kaggle tutorial: https:\/\/www.kaggle.com\/mrisdal\/mentions-of-kaggle-on-hacker-news) Outside of Kaggle, you can get the data using `pandas-gbq`.\n\nThe return data is also randomized; this allows us to use the last 20% as a test set without introducing temporal dependencies.","7b97c48e":"## Sample Weights\n\nWeight `score=1` samples lower so model places a higher importance on atypical submissions."}}