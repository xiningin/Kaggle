{"cell_type":{"f6ab5029":"code","aa047ea3":"code","97aec9da":"code","9a4bccb0":"code","ef8792b8":"code","ea270bf2":"code","82b926a5":"code","4aaa2b90":"code","a1b5c148":"code","f4ef5276":"code","58fe4e17":"code","1234c9ca":"code","be08e27e":"code","064493e2":"code","58cc8181":"code","e3b50eee":"code","cbb442ae":"code","39b789ed":"code","26938d38":"code","6e851d14":"code","dd9c8ca6":"code","8267081b":"code","bc328a64":"code","06908338":"code","75cd9df9":"code","cf521c62":"code","99a558eb":"code","a637617a":"code","6b78fcd6":"code","7b615fd2":"code","194d9435":"code","5781bbbc":"code","4f2eac2d":"code","1db46b84":"code","93b9b44a":"markdown","b52cee62":"markdown","6f7bda72":"markdown","f032607d":"markdown","34eaf5d7":"markdown"},"source":{"f6ab5029":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom nltk import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aa047ea3":"train_df = pd.read_csv(r'..\/input\/train.csv')","97aec9da":"train_df.head()","9a4bccb0":"train_df.shape","ef8792b8":"docs = train_df[\"question_text\"].values\nlabels = train_df[\"target\"].values","ea270bf2":"np.unique(labels,return_counts=True)","82b926a5":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(vocab_size)","4aaa2b90":"# pad documents to a max length of 20 words\nmax_length = 20\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(len(padded_docs))","a1b5c148":"padded_docs.shape","f4ef5276":"rus = RandomUnderSampler(random_state=42)\npadded_docs_rus,labels_rus = rus.fit_resample(padded_docs,labels)","58fe4e17":"padded_docs_rus.shape,labels_rus.shape,padded_docs.shape,labels.shape","1234c9ca":"np.unique(labels_rus,return_counts=True)","be08e27e":"# load the whole embedding into memory\nEMBEDDING_FILE = r\"..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt\"\nEMBEDDING_DIM = 100\n\ndef get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE,'r', errors = 'ignore', encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n        except:\n            pass\n            \n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()","064493e2":"# create a weight matrix for words in training docs\n\ndef create_embedding_weights(vocab_size,t):\n    embedding_matrix = np.zeros((vocab_size, 300))\n    for word, i in t.word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    print(embedding_matrix.shape)\n    return embedding_matrix\n\n","58cc8181":"embedding_matrix = create_embedding_weights(vocab_size,t)","e3b50eee":"## create model\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size, 300, input_length=20, weights=[embedding_matrix], trainable=False))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","cbb442ae":"model_glove.summary()","39b789ed":"# split dataset into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(padded_docs_rus, labels_rus, test_size=0.1, random_state=42)","26938d38":"filepath=\"\/weights-{epoch:02d}-{val_acc:.4f}.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=False, save_weights_only=True, \n                             mode='auto', period=10)","6e851d14":"earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5,\n                          verbose=1, mode='auto')","dd9c8ca6":"# fit the model\nmodel_glove.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),\n          verbose=1,callbacks=[checkpoint,earlystop])","8267081b":"# evaluate the model\nloss, accuracy = model_glove.evaluate(X_test, y_test, verbose=1)\nprint('Accuracy: %f' % (accuracy*100))","bc328a64":"model_glove.predict_classes(X_test)","06908338":"ques = ['Has the United States become the largest dictatorship in the world?','Why do so many women become so rude and arrogant when they get just a little bit of wealth and power?','How should I prepare for IIT K\/IIM C\/ ISI K PGDBA course exam and interview?']","75cd9df9":"t.fit_on_texts(ques)\n# integer encode the documents\nencoded_ques = t.texts_to_sequences(ques)\nmax_length = 20\npadded_ques = pad_sequences(encoded_ques, maxlen=max_length, padding='post')\nprint(len(padded_ques))","cf521c62":"model_glove.predict_classes(padded_ques)","99a558eb":"test_file = r\"..\/input\/test.csv\"\ntest_df = pd.read_csv(test_file)","a637617a":"test_df.head()","6b78fcd6":"test_docs = test_df['question_text'].values","7b615fd2":"t.fit_on_texts(test_docs)\n# integer encode the documents\nencoded_ques_test = t.texts_to_sequences(test_docs)\nvocab_size_test = len(t.word_index) + 1\nprint(vocab_size_test)\nmax_length = 20\npadded_ques_test = pad_sequences(encoded_ques_test, maxlen=max_length, padding='post')\nprint(len(padded_ques_test))","194d9435":"predicted_output = model_glove.predict_classes(padded_ques_test)","5781bbbc":"test_df['predicted_labels'] = predicted_output","4f2eac2d":"test_df[test_df['predicted_labels'] == 1].head()","1db46b84":"test_df.to_csv(r'submission.csv',index=False)","93b9b44a":"### 4. Network architecture","b52cee62":"### 2. Preprocessing\nTokenize text, convert words \/ tokens to indexed integers. Take each document and convert to a sequence of max length 20 (pad with zeroes if shorter).","6f7bda72":"### 3. Import embeddings\nThe clever part: import a dictionary of word embeddings that translates each word into a 300 dimensional vector.","f032607d":"###### 1. Read data\nRead the data from CSV and apply some basic pre-processing (remove non-ascii characters, convert our target variable to an integer label).","34eaf5d7":"### 5. Training and Evaluation\nIs it any good? Let's find out.\nDivide our dataset using a holdout strategy:"}}