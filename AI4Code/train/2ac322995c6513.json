{"cell_type":{"c12b512d":"code","f2ad8563":"code","47dae248":"code","e0bfd269":"code","12f3aef5":"code","7e4b3367":"code","e767b831":"code","a17f3fd4":"code","f4e52ec7":"code","bc09b2ce":"code","0437956b":"code","96b12110":"code","128885a3":"code","83d99aa5":"code","29b58a06":"code","ca4a0b14":"code","61a9efbd":"code","c6008970":"code","2b51c81e":"code","fca0ec9f":"code","15f11c09":"code","00a002b0":"code","66b58f36":"code","2959662e":"code","3fec5914":"code","9192b8dd":"code","abf09402":"code","57fe9baf":"code","8b39ed39":"markdown","ad2d45ab":"markdown","182cf196":"markdown","4898028e":"markdown","a76b0579":"markdown","6886a7a8":"markdown","69b52002":"markdown","24a6be11":"markdown","bd15a750":"markdown","4e20b497":"markdown","79e8fef9":"markdown","82498830":"markdown","c1b3ba42":"markdown","87b8f0ea":"markdown","a5d8a4ca":"markdown","ca1741e9":"markdown","b68dcadd":"markdown","3e1da682":"markdown"},"source":{"c12b512d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Seaborn visualization library\nsns.set(style=\"darkgrid\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n\nimport gc\n# Any results you write to the current directory are saved as output.","f2ad8563":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","47dae248":"# Look at first 10 records of the train dataset\ntrain.head(n=10).T","e0bfd269":"# Check out the shape of the train and test sets\nprint('Train:', train.shape)\nprint('Test:', test.shape)","12f3aef5":"# Check the target variable destribution\ntrain['target'].value_counts()","7e4b3367":"# Imports for Modeling\n\n#from sklearn.preprocessing import Imputer, MinMaxScaler\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb","e767b831":"# Target variable from the Training Set\nTarget = train['target']\n\n# Input dataset for Train and Test \ntrain_inp = train.drop(columns = ['target', 'ID_code'])\ntest_inp = test.drop(columns = ['ID_code'])\n\n# List of feature names\nfeatures = list(train_inp.columns)","a17f3fd4":"# Split the Train Dataset into training and validation sets for model building. \n# The training set now has 140K records and validation set has 60K records\n\nX_train, X_test, Y_train, Y_test = train_test_split(train_inp, Target, \n                                                    test_size= 0.3, random_state = 2019)","f4e52ec7":"# check the split of train and validation\nprint('Train:',X_train.shape)\nprint('Test:',X_test.shape)","bc09b2ce":"# Create an object of Logistic Regression with parameters C and class_weight\nlogist = LogisticRegression(C=0.001, class_weight='balanced')\n\n# Fit the training data on this object\nlogist.fit(X_train, Y_train)","0437956b":"# Predict the Target for validation dataset \nlogist_pred = logist.predict_proba(X_test)[:,1]","96b12110":"logist_pred","128885a3":"def performance(Y_test, logist_pred):\n    logist_pred_var = [0 if i < 0.5 else 1 for i in logist_pred]\n    print('Confusion Matrix:')\n    print(confusion_matrix(Y_test, logist_pred_var)) \n      \n    #print(classification_report(Y_test, logist_pred)) \n\n    fpr, tpr, thresholds = roc_curve(Y_test, logist_pred, pos_label=1)\n    print('AUC:')\n    print(auc(fpr, tpr))","83d99aa5":"performance(Y_test, logist_pred)","29b58a06":"# Submission dataframe\nlogist_pred_test = logist.predict_proba(test_inp)[:,1]\n\nsubmit = test[['ID_code']]\nsubmit['target'] = logist_pred_test\n\nsubmit.head()","ca4a0b14":"# Create the Submission File using logistic regression model\nsubmit.to_csv('log_reg_baseline.csv', index = False)","61a9efbd":"# Create Decision Tree Classifier object with few parameters\ntree_clf = DecisionTreeClassifier(class_weight='balanced', random_state = 2019, \n                                  max_features = 0.7, min_samples_leaf = 80)\n\n# Fit the object on training data\ntree_clf.fit(X_train, Y_train)","c6008970":"# Predict for validation set and check the performance\ntree_preds = tree_clf.predict_proba(X_test)[:, 1]\nperformance(Y_test, tree_preds)","2b51c81e":"# Submission dataframe\ntree_pred_test = tree_clf.predict_proba(test_inp)[:, 1]\n\nsubmitTree = test[['ID_code']]\nsubmitTree['target'] = tree_pred_test\n\n# Create the Submission File using logistic regression model\nsubmitTree.to_csv('Decision_Tree.csv', index = False)","fca0ec9f":"# Extract feature importances\nfeature_importance_values = tree_clf.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\nfeature_importances.sort_values(by='importance', ascending=False).head(n=10)","15f11c09":"plt.figure(figsize=(20,8))\nsns.boxplot(data=train[['var_81', 'var_139', 'var_12', 'var_26', 'var_146', 'var_110',\n                        'var_109', 'var_53', 'var_6', 'var_166']])","00a002b0":"# Create random Forest Object using the mentioned parameters\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=2019, verbose=1,\n                                      class_weight='balanced', max_features = 0.5, \n                                       min_samples_leaf = 100)\n\n# Fit the object on training set \nrandom_forest.fit(X_train, Y_train)","66b58f36":"# Predict the validation set target and check the performance\nforest_preds = random_forest.predict_proba(X_test)[:, 1]\nperformance(Y_test, forest_preds)","2959662e":"# Submission dataframe\nforest_pred_test = random_forest.predict_proba(test_inp)[:, 1]\n\nsubmitForest = test[['ID_code']]\nsubmitForest['target'] = forest_pred_test\n\n# Create the Submission File using logistic regression model\nsubmitForest.to_csv('Random_Forest.csv', index = False)","3fec5914":"# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\nfeature_importances.sort_values(by='importance', ascending=False).head(n=10)","9192b8dd":"#custom function to build the LightGBM model.\ndef run_lgb(X_train, Y_train, X_test, Y_test, test_inp):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        \"num_leaves\" : 1000,\n        \"learning_rate\" : 0.01,\n        \"bagging_fraction\" : 0.8,\n        \"feature_fraction\" : 0.8,\n        \"bagging_freq\" : 5,\n        \"reg_alpha\" : 1.728910519108444,\n        \"reg_lambda\" : 4.9847051755586085,\n        \"random_state\" : 42,\n        \"bagging_seed\" : 2019,\n        \"verbosity\" : -1,\n        \"max_depth\": 18,\n        \"min_child_samples\":100\n       # ,\"boosting\":\"rf\"\n    }\n    \n    lgtrain = lgb.Dataset(X_train, label=Y_train)\n    lgval = lgb.Dataset(X_test, label=Y_test)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 2500, valid_sets=[lgval], \n                      early_stopping_rounds=50, verbose_eval=50, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_inp, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\n# Training the model #\npred_test, model, evals_result = run_lgb(X_train, Y_train, X_test, Y_test, test_inp)","abf09402":"# Extract feature importances\nfeature_importance_values = model.feature_importance()\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\nfeature_importances.sort_values(by='importance', ascending=False).head(n=10)","57fe9baf":"# Submission dataframe\npred_test[pred_test>1] = 1\npred_test[pred_test<0] = 0\n\nsubmitLGB = test[['ID_code']]\nsubmitLGB[\"target\"] = pred_test\n\n# Create the Submission File using Light GBM\nsubmitLGB.to_csv('LightGBM.csv', index = False)\n\nsubmitLGB.head()","8b39ed39":"This is my first attempt to write the notebook from scratch. I have been playing around with exisitng kernels for competition until now. Comments, suggestions, recommendations are all very welcomed. \n\nI will be modeling using the following algorithms -\n* Logistic Regression\n* Decision Tree Classifier\n* Random Forest Classifier\n* Light Gradient Boosting Method\n\nLet's start by importing necessary packages -","ad2d45ab":"The feature importance we get from random forest is very similar to the list we got from decision trees ","182cf196":"### Import Datasets","4898028e":"## Logistic Regression\nWe start with most basic algorithm used for classification problems. Initial model with defining only the regularization paramenter (C) yielded 0.6 AUC. Since this is an unbalanced dataset, we need to define another **paramenter 'class_weight = balanced'** which will give equal weights to both the targets irrespective of their reperesentation in the training dataset. We can even define classwise weights using this parameter, if needed ","a76b0579":"## Decision Trees\nMoving on to a slightly advanced algorithm, decision trees. Again, the parameters here are class_weight to deal with unbalanced target variable, random_state for reproducability of same trees. The feature max_features and min_sample_leaf are used to prune the tree and avoid overfitting to the training data. \n\n**Max_features** defines what proportion of available input features will be used to create tree. \n\n**Min_sample_leaf** restricts the minimum number of samples in a leaf node, making sure none of the leaf nodes has less than 80 samples in it. If leaf nodes have less samples it implies we have grown the tree too much and trying to predict each sample very precisely, thus leading to overfitting.  ","6886a7a8":"### Logistic Regresssion Result \nThis model gave out an **AUC of 0.854** on validation set and 0.855 on Public Leaderboard for the test file","69b52002":"Let's take a look at these features and plot them on a box and whiskrers chart","24a6be11":"## Modeling\nI would be trying a few algorithms, starting from the most simple Logistic Regression, followed by Decision Tree, Random Forest and finally, Light GBM. To build the modeling pipeline, let's import all the necessary packages we would need ","bd15a750":"## Performance Function\nSince we will be building multiple models, it is advisable to create a function that can be called with different outputs of each model. This is a simple function which takes in the Predicted Validation Target and Actual Validation Target. It then gives out classification summary like **confusion matrix and AUC score **","4e20b497":"### Light GBM Results:\nThe AUC Score drastically improves from 0.650 in our Decision Tree model to **an AUC score of 0.89** in our ensemble of trees, Light GBM model. The public leaderboard scores after submitting the test predictions come out to be 0.891\n\nThe feature importance though, it has some variables similar to those we saw in the tree models but majority of them are new in the top 10 most important variable list","79e8fef9":"## Ensemble Learning\n[Ensemble Learning](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning) refers to the algorithms that created using ensembles of variour learning algorithms. So, to give you an example, random forests are ensembles of many decision tree estimators. \n\nThere are 2 types of ensemble learning algorithms -\n**1. Bagging Algorithms:** Bagging involves having each model in the ensemble vote with equal weight for the final output. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set\n**2. Boosting Algorithms:** As Wikipedia defines, boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified.\n\n## Random Forest\nLet's start with building a random forest, with parameters like class_weight, random_state, and hyperparameters like max_features and min_sample_leaf as earlier. We have also defined the n_estimators which is a compulsory parameter. This defines the number of decision trees that will be present in the forest. ","82498830":"This is an unbalanced classification problem with only 10% records having target variable = 1. ","c1b3ba42":" ### Random Forest Results:\n Basic random forest is giving us **0.787 AUC score** on the validation set and 0.789 AUC score on the test set submitted on public leaderboard","87b8f0ea":"Let's separate input variables and target variable. Have also created a features list with all input variable names. ","a5d8a4ca":"### Decision Tree Results:\nBasic decision tree is giving us **0.651 AUC score** on the validation set and 0.650 AUC score on the test set submitted on public leaderboard ","ca1741e9":"## Light Gradient Boosting Method\n\n**WHAT IS IT? **\n\nLight GBM is a gradient boosting framework that uses tree based learning algorithm. It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\n**WHY USE LGB?**\n\nIt is \u2018Light\u2019 because of its high speed. It can handle large data, requires low memory to run and focuses on accuracy of results. Also supports GPU learning and thus data scientists\/ Kagglers are widely using LGBM for data science application development.\n\n**TIPS & TRICKS**\n\n* The algorithm easily overfits and thus, should not be used with small (< 10K rows) datasets.\n* Deal with overfitting using these parameters:\n    1. Small Maximum Depth\n    2. Large Minimum Data in a Leaf\n    3. Small Feature and Bagging Fraction\n* Improve the training speed\n    1. Small Bagging Fraction\n    2. Early Stopping Round \n* Use small\u00a0learning_rate\u00a0with large\u00a0num_iterations for better accuracy\n* Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting\n* **If you have a big enough dataset, use this algorithm at least once. It\u2019s accuracy has challenged other boosting algorithms**","b68dcadd":"The dataset consists of an ID_code, 200 input variables (all numeric) and a binary target variable representing the transaction-happened. Since the entire dataset is masked, cannot do much of exploratory data analysis","3e1da682":"## Next Stpes:\nNow that we have a considerably good AUC score to start with, we can improve on it. A very promising approach is to create new features based on the domain knowledge or based on the EDA we usually do as the first step. Tuning the model or creating a more sophisticated stacked architecture helps improve the score too.\n\n"}}