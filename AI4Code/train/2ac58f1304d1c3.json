{"cell_type":{"68a51400":"code","8fbfa244":"code","f12ee012":"code","c89184b9":"code","f92dfc72":"code","96dddcb0":"code","9a8e4338":"code","71f971b2":"code","3a09a15c":"code","5a2fb7a8":"code","14ea9f1b":"code","6acf76ee":"code","6073710b":"code","d36812e5":"code","e2f96529":"code","00dddb0b":"code","f31db432":"code","959a26af":"code","4199a6a0":"code","819f9a5e":"code","0545fc74":"markdown","58835dc5":"markdown","4f05f65c":"markdown","2e734438":"markdown","df840758":"markdown","4815e70d":"markdown","e2991325":"markdown","40d00e07":"markdown","6fad1436":"markdown","2743d3e1":"markdown"},"source":{"68a51400":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer,IterativeImputer\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json\nimport gc\n\n","8fbfa244":"\ndef reduce_mem_usage(df, verbose=True):\n    #reduce memory of data uesd\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: \n        print(f'Memory usage of dataframe is {start_mem} MB --> {end_mem} MB (Decreased by {100 * (start_mem - end_mem) \/ start_mem})')\n    return df\n\n","f12ee012":"%%time\nfolder_path = '..\/input\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\nidentity_col=list(train_identity.columns)\ntrasac_col=list(train_transaction.columns)\n\ndel train_identity, train_transaction, test_identity, test_transaction\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)\ngc.collect()","c89184b9":"#3.transactionDT\n\ndef datetime_trans(train,start_date='2017-11-30'):\n    startdate=datetime.datetime.strptime(start_date,\"%Y-%m-%d\")\n    train['TransactionDT']=train['TransactionDT'].fillna(train['TransactionDT'].mean())\n    train['date']=train['TransactionDT'].apply(lambda x : datetime.timedelta(seconds=x)+startdate)\n    train['weekday']=train['date'].apply(lambda x :x.weekday())#\u4e0d\u9002\u5408\u5355\u72ec\u4f7f\u7528\n    train['month']=(train['date'].dt.year-2017)*12+train['date'].dt.month\n    train['hour']=train['date'].apply(lambda x :x.hour)#\u53ef\u4ee5\u4f7f\u7528\n    train['day']=(train['date'].dt.year-2017)*365+train['date'].dt.dayofyear\n    train['year_weekday']=train['date'].apply(lambda x : str(x.year)+'_'+str(x.weekday()))#\u6709\u4e00\u5b9a\u7684\u504f\u5ea6\uff0c\u4f46\u8f83\u4e3a\u5e73\u5766\n    train['weekday_hour']=train['date'].apply(lambda x :str(x.weekday())+'_'+str(x.hour))#\u6ce2\u52a8\u6027\u8d28\u8f83\u597d\ndate_col=['weekday','month','day','hour','year_weekday','weekday_hour']\ndatetime_trans(train)\ndatetime_trans(test)\n","f92dfc72":"#it's most important trick ,card1-card3 and card5 seems like some serial number\ndef addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    data['uid4'] = data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain = addNewFeatures(train)\ntest = addNewFeatures(test)","96dddcb0":"agg_cols = ['card1','card2','card3','card5','uid','uid2','uid3','uid4']\ndef add_agg_col(col_prefix,agg_col,col_suffix='TransactionAmt'):\n    if isinstance(agg_col,list):\n        temp_df=pd.concat([train[[col_prefix,col_suffix]],test[[col_prefix,col_suffix]]])\n        temp_df=temp_df.groupby(col_prefix)[col_suffix].agg(agg_col)\n        for c in agg_col:\n            new_col=col_prefix+'_'+c+'_'+col_suffix\n            train[new_col]=train[col_prefix].map(temp_df[c])#problem is here temp_df.columns\n            test[new_col]=test[col_prefix].map(temp_df[c])\n    else:\n        raise TypeError('agg_col must be List')\n\nfor i in agg_cols:\n    add_agg_col(i,['mean','std'])\n    print(f'{i} for [\\'mean\\',\\'std\\'] aggregate is done!')\n","9a8e4338":"\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')\n\ntrain=train.replace([np.inf,-np.inf],np.nan)\ntest=test.replace([np.inf,-np.inf],np.nan)","71f971b2":"#P and R emaildomain's np.nan is float type\ndef p_r_domain(train,test,col):\n    train[col]=train[col].fillna('Nan').apply(lambda x : x.split('.')[0])\n    test[col]=test[col].fillna('Nan').apply(lambda x : x.split('.')[0])\n    \nfor c in ['P_emaildomain','R_emaildomain']:\n    p_r_domain(train,test,c)","3a09a15c":"#add 'others' mark threshold=0.95 exclude 'na's\ndef add_others_mark(train,test,categ_col):\n    temp_df=pd.concat([train[[categ_col]],test[[categ_col]]])\n    series=temp_df[categ_col].value_counts(normalize=True).cumsum()\n    others_index=list(series[series>0.95].index)\n    if len(others_index)!=0:\n        train[categ_col]=train[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        test[categ_col]=test[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        print(f'{categ_col}:{len(others_index)} of {len(series)} feature values has been replaced to \\'others\\'')\nmail_col=['P_emaildomain', 'R_emaildomain',\n          'DeviceInfo',\n          'id_30','id_33']       \n\nfor c in mail_col:\n    add_others_mark(train,test,c)\nadd_others_mark(train,test,mail_col[3])\n","5a2fb7a8":"#set frequency\nfreq_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','DeviceType',\n          'id_30','id_33',\n          'uid','uid2','uid3','uid4'\n         ]+date_col\n\ndef set_freq_col(train,test,col):\n    prefix='_fq'\n    temp_df=pd.concat([train[[col]],test[[col]]])\n    fq=temp_df[col].value_counts(dropna=False)\n    train[col+prefix]=train[col].map(fq)\n    test[col+prefix]=test[col].map(fq)\n    \nfor c in freq_cols:\n    set_freq_col(train,test,c)\n    \n\nperiods = ['month','year_weekday','weekday_hour']\nuids = ['uid','uid2','uid3','uid4']\ndef set_uid_period(train,test,periods,uids):\n    for period in periods:\n        for col in uids:\n            new_column = col + '_' + period\n\n            temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n            temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n            fq_encode = temp_df[new_column].value_counts()\n\n            train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n            test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n\n            train[new_column] \/= train[period+'_fq']\n            test[new_column]  \/= test[period+'_fq']\n            \nset_uid_period(train,test,periods,uids)","14ea9f1b":"#\u53bb\u6389na\u8f83\u591a\uff0c\u5355\u503c\u5360\u6bd4\u8f83\u5927\u7684\u5217\u3002threshold=0.85\ntr_na_count=train.isnull().sum()\/len(train)\ntr_drop_cols=[c for c in train.columns if tr_na_count[c]>0.85]\ntr_big_cols=[c for c in train.columns if train[c].value_counts(normalize=True,dropna=False).values[0]>0.85]\ndrop_cols=list(set(tr_drop_cols+tr_big_cols))\ndrop_cols.remove('isFraud')\ny_=train['isFraud']\ntrain.drop(columns=drop_cols+['isFraud'],inplace=True)\ntest.drop(columns=drop_cols,inplace=True)\n#15\u53bb\u6389\u591a\u4f59\u7684\u5217\u3002\nexcess_col=['date','TransactionDT','TransactionID']\n\ntrain.drop(columns=excess_col,inplace=True)\ntest.drop(columns=excess_col,inplace=True)","6acf76ee":"#use sklearn.imputer object instead of pd.fillna()\n\ndef imputing_na(train,col):\n    #use sklearn.imputer object instead of pd.fillna()\n    if train[col].dtypes == object:\n        imp=SimpleImputer(strategy='constant',fill_value='Nan').fit_transform(train[col].values.reshape(-1,1))\n        train[col]=pd.Series(imp[:,0])\n    else:\n        imp=SimpleImputer(strategy='constant',fill_value=-999).fit_transform(train[col].values.reshape(-1,1))\n        train[col]=pd.Series(imp[:,0])\nfor c in train.columns:\n    imputing_na(train,c)\n    imputing_na(test,c)\n","6073710b":"#label encoder categorical columns\nnumerical_cols = train.select_dtypes(exclude = 'object').columns\ncategorical_cols = train.select_dtypes(include = 'object').columns\n\ndef labelencoder(train,test,col):\n    cod=list(train[col].values)+list(test[col].values)\n    le=LabelEncoder().fit(cod)\n    train[col]=le.transform(train[col])\n    test[col]=le.transform(test[col])\n    \nfor c in categorical_cols:\n    labelencoder(train,test,c)\n    \n","d36812e5":"print(train.shape,test.shape)\n","e2f96529":"#training LGBM\nparams = {'num_leaves': int((2**10)*0.72),\n          'min_child_weight': 0.17,\n          'feature_fraction': 0.72,\n          'bagging_fraction': 0.72,\n          'min_data_in_leaf': 179,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 13,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3299927210061127,\n          'reg_lambda': 0.3885237330340494,\n          'random_state': 4,\n}","00dddb0b":"%%time\n\nNFOLDS = 7\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = train.columns\nsplits = folds.split(train, y_)\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(train.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = train[columns].iloc[train_index], train[columns].iloc[valid_index]\n    y_train, y_valid = y_.iloc[train_index], y_.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=300)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")","f31db432":"\nsub['isFraud']= y_preds","959a26af":"sub.to_csv('submission.csv', index=False)\n","4199a6a0":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(NFOLDS));","819f9a5e":"lgb.create_tree_digraph(clf)","0545fc74":"\"\"\"\n#many columns\n\"\"\"amount_col=['TransactionAmt']\nid_cols=[c for c in train.columns if c[0]=='i']\nid_numerical_col=[c for c in id_cols if train[c].dtypes != object]\nid_category_col=[c for c in id_cols if c not in id_numerical_col]\ndevice=['DeviceType','DeviceInfo']\ndist_col=['dist1']\naddr_col=['addr1','addr2']\nv_col=[c for c in train.columns if c[0]=='V']\nm_col=[c for c in train.columns if c[0]=='M']\nd_col=[c for c in train.columns if c[0]=='D' ]\nd_col=list(set(d_col)^set(device))\nc_col=[c for c in train.columns if c[0]=='C']\ncard_cols=['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\ncard_numerical_col=[c for c in card_cols if train[c].dtypes != object]\ncard_category_col=[c for c in card_cols if c not in card_numerical_col]\n\"\"\"\nss_col=list(set(numerical_cols)^set(v_col))\ndef standarscaler_(train,test,cols):\n    sca=StandardScaler()\n    sca.fit(pd.concat([train[cols],test[cols]]))\n    train[cols]=sca.transform(train[cols])\n    test[cols]=sca.transform(test[cols])\n\nstandarscaler_(train,test,ss_col)\ngc.collect()\n\"\"\"","58835dc5":"## Data Exploration\nCategorical Features - Transaction\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\nCategorical Features - Identity\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\u6839\u636edata\u6587\u6863\uff0c\u4e0a\u8ff0\u7279\u5f81\u975econtinuous\u3002\n\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical.","4f05f65c":"* \u4ee5R\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['R_emaildomain']\n* \u4ee5T\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['TransactionID', 'TransactionDT', 'TransactionAmt']\n* \u4ee5a\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['addr1', 'addr2']\n* \u4ee5D\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'DeviceType', 'DeviceInfo']\n* \u4ee5d\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['dist1', 'dist2']\n* \u4ee5c\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n* \u4ee5C\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n* \u4ee5V\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['V1'--'V339']\n* \u4ee5M\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['M1'-- 'M9']\n* \u4ee5i\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['id_01'-- 'id_38']\n* \u4ee5P\u5f00\u5934\u7684\u5217\u540d\u6709\uff1a['ProductCD', 'P_emaildomain']","2e734438":"## General information\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nIn this kernel I work with IEEE Fraud Detection competition.\n\n\u76ee\u6807\u5c31\u662f\u5bf9\u6d4b\u8bd5\u96c6\u7684isFraud\u4f5c\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528AUC\u503c\u4f5c\u4e3a\u5f97\u5206\u7684\u8bc4\u4ef7\n","df840758":"# Feature engenering","4815e70d":"## Training With LGBM","e2991325":"\u5148\u4ece\u7b80\u5355\u7684\u5730\u65b9\u5f00\u59cbTransaction_DT,Transaction_Amt,ProductCD,DeviceType,DeviceInfo,P_emaildomain,R_emaildomain\n\nProductCD\u6ca1\u6709\u7f3a\u5931\u503c\u3002\n\nProductCD: product code, the product for each transaction","40d00e07":"## Data loading and overview\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features.\n\n\u8fd9\u91cc\u53ea\u662f\u7b80\u5355\u5bf9\u4e24\u90e8\u5206\u6570\u636e\u6309\u7167transactionid\u4f5cmerge\uff0chow=left\u3002","6fad1436":"\u4ece\u6b64\u524d\u7684\u7ed3\u679c\u770b\uff0c\u5728numerical features\u4e2d\u4e3b\u8981\u5229\u7528\u5230\u7684\u8fd8\u662famount\u548c\u5176\u884d\u751f\u51fa\u6765\u7684mean,std,freq\u7b49\u4fe1\u606f\u3002\u50cfV\u7cfb\u5217\u57fa\u672c\u6ca1\u7528\u5230\uff0cD\u7cfb\u5217\u8fd8\u80fd\u6df1\u5ea6\u6316\u6398\uff0cC\u7cfb\u5217\u4e5f\u53ef\u4ee5\u3002\nLGBM\u7684\u53c2\u6570\u8c03\u6574\u4e00\u5934\u96fe\u6c34\u3002\u8c8c\u4f3c\u65e9\u505c\u57281200-1600\u4e4b\u95f4\u5c31\u505c\u4e86\uff0c\u8fd9\u4e2a\u610f\u5473\u7740\u4ec0\u4e48\uff0c\u662f\u4e0d\u662f\u597d\u4e8b\uff0c\u4e0d\u5f97\u800c\u77e5\u3002\n\n\u8fd9\u4e24\u70b9\u5c31\u662f\u540e\u9762\u7684\u4e3b\u8981\u95ee\u9898\u3002","2743d3e1":"## Data Preprossesing"}}