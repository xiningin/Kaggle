{"cell_type":{"4c2d72fe":"code","a16ceca8":"code","f99d48bb":"code","e402fdb5":"code","efa71f0b":"code","9882906c":"code","3af5bec6":"code","a5f2bc21":"code","ddf5f0cc":"code","0e106c77":"code","a6c27c01":"code","bc74e3b4":"code","7803d909":"code","ef0af020":"code","a6fedcc7":"code","3541ec18":"code","57760099":"code","1511ad4f":"markdown","900d4d29":"markdown","8e6a7808":"markdown","f0ffddd0":"markdown","5157a212":"markdown","2f3eaba1":"markdown","100b75ce":"markdown","c252d079":"markdown","39a7712f":"markdown"},"source":{"4c2d72fe":"import tensorflow as tf\n# tf.enable_eager_execution()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport pandas as pd\nimport time\nfrom IPython import display\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D, BatchNormalization, LeakyReLU, Conv2DTranspose, Reshape, Flatten\n\n\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ninit = RandomNormal(mean=0.0, stddev=0.02)","a16ceca8":"class DCGAN(tf.keras.Model):\n    def __init__(self, image_size, output_path, num_channels=1, z_dim=100,\n                 G_h_size=128, D_h_size=128):\n        \"\"\"\n          image_size - the size of the side of the square image\n          output_path - path to save training artifacts. at the root - pictures from different iterations, in the model folder - model\n          num_channels - number of image channels\n          z_dim - dimension of the latent vector\n          G_h_size - minimum size of filters with generator convolutional layers\n          D_h_size - minimum size of filters with convolutional discriminator layers\n        \"\"\"\n        super().__init__()\n        self.image_size = image_size\n        self.num_channels = num_channels\n        self.z_dim = z_dim\n\n        self.multiply = int(np.log2(self.image_size \/ 8)) # \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u044c \u0430\u043f\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u0438\u043b\u0438 \u0434\u0430\u0443\u043d\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\n                                                          # \u0447\u0442\u043e\u0431\u044b \u0438\u0437 (4,4) \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c (image_size\/2, image_size\/2) \u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442\n                                                \n        self.output_path =  Path(output_path)\n        (self.output_path \/ \"model\").mkdir(exist_ok=True)\n\n        self.G_h_size = G_h_size\n        self.D_h_size = D_h_size\n\n        self.generator = self._build_generator()\n        self.discriminator = self._build_discriminator()\n\n        self.optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.999)\n         \n        self.gen_loss_hist = []\n        self.disc_loss_hist = []\n        self._vis_h = 5\n        self._vis_w = 5\n        self._vis_noise = np.random.normal(0, 1, (self._vis_h* self._vis_w, self.z_dim)).astype(np.float32)\n        self.start_iteration = 0\n\n    def discriminator_loss(self, real_output, fake_output):\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n        total_loss = real_loss + fake_loss\n        return total_loss\n\n    def generator_loss(self, fake_output):\n        return cross_entropy(tf.ones_like(fake_output), fake_output)\n    \n    def _conv_bn_leaky(self, kernel_size, channels, stride=1):\n        \"\"\"\n         We will use this block often - so we will move it into a separate function.\n         It contains Conv + BatchNorm + LeakyReLU\n         If you specify stride = 2, it will halve the size.\n        \"\"\"\n        model = tf.keras.Sequential()\n        model.add(Conv2D(channels,\n                         kernel_size=kernel_size, padding=\"same\",\n                         use_bias=False, kernel_initializer=init,\n                         strides=(stride, stride))) # use_bias=False, \u0442.\u043a. BatchNorm \u0438 \u0442\u0430\u043a \u0432\u044b\u0447\u0442\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435\n        model.add(BatchNormalization())\n        model.add(LeakyReLU())\n        return model\n        \n\n    def _build_generator(self):\n        \"\"\"\n         The generator should convert the vector of length self.z_dim to\n         picture image_size x image_size x num_channels\n        \"\"\"\n        \n        model = tf.keras.Sequential()\n        \n        model.add(Reshape((1, 1, self.z_dim), input_shape=(self.z_dim,)))\n\n       \n        model.add(Conv2DTranspose(self.G_h_size * 2**self.multiply,\n                                  kernel_size=4, use_bias=False, \n                                  kernel_initializer=init))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU())\n\n        \n        for i in range(self.multiply):\n            model.add(UpSampling2D()) \n            model.add(self._conv_bn_leaky(4, self.G_h_size * 2**self.multiply \/\/ 2**(i+1))) \n        \n        assert model.output_shape[1:] == (self.image_size \/\/ 2, self.image_size \/\/ 2, self.D_h_size), f\"{model.output_shape, self.D_h_size}\"\n        \n        model.add(UpSampling2D())\n        model.add(Conv2D(self.num_channels,\n                         kernel_size=4, strides=(1, 1),\n                         activation=\"tanh\", padding=\"same\", \n                         kernel_initializer=init))\n        return model\n\n    def _build_discriminator(self):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.InputLayer(\n            input_shape=((self.image_size, self.image_size, self.num_channels))))\n        model.add(self._conv_bn_leaky(kernel_size=4, \n                                      channels=self.D_h_size,\n                                      stride=2,\n                                      ))\n        \n        for i in range(self.multiply):\n            model.add(self._conv_bn_leaky(kernel_size=4, \n                                          channels=self.D_h_size * (2 ** (i+1)),\n                                          stride=2)) \n        assert model.output_shape[1:] == (4, 4, self.D_h_size * 2**self.multiply), f\"{model.output_shape}\"\n        model.add(Conv2D(1, kernel_size=4, kernel_initializer=init, use_bias=False)) # \u0431\u0435\u0437 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 !\n        model.add(Flatten())\n        return model\n    \n    @tf.function\n    def train_step(self, images):\n        \n        noise = tf.random.normal([tf.cast(images.shape[0], tf.int32), self.z_dim])\n        \n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(noise, training=True)\n\n            real_output = self.discriminator(images, training=True)\n            fake_output = self.discriminator(generated_images, training=True)\n\n            gen_loss = self.generator_loss(fake_output)\n            disc_loss = self.discriminator_loss(real_output, fake_output)\n            \n        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n\n        self.optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n        self.optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n        return gen_loss, disc_loss\n\n    def save_imgs(self, epoch):\n    \n        gen_imgs = self.generator(self._vis_noise, training=False)\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        fig, axs = plt.subplots(self._vis_h, self._vis_w, figsize=(6,6))\n        cnt = 0\n        for i in range(self._vis_h):\n            for j in range(self._vis_w):\n                if self.num_channels == 1:\n                    axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n                else:\n                    axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n                axs[i, j].axis('off')\n                cnt += 1\n        fig.savefig(self.output_path \/ f\"{epoch}.png\")\n        plt.show()\n    \n    def train(self, dataset, num_iters=2000, show_every=25):\n    \n        start = time.time()\n        iters = self.start_iteration\n        for image_batch in dataset:\n            print(\".\", end='')\n            gen_loss, disc_loss = self.train_step(image_batch)\n            \n            self.disc_loss_hist.append(disc_loss.numpy())\n            self.gen_loss_hist.append(gen_loss.numpy())    \n            \n            if iters % show_every == 0:\n                display.clear_output(wait=True)\n                plt.figure()\n                plt.plot(self.disc_loss_hist, label=\"Discriminator loss\")\n                plt.plot(self.gen_loss_hist, label=\"Generator loss\")\n                plt.legend(loc=\"best\")\n                plt.figure()\n                self.save_imgs(f\"{iters}\")\n                self.save_weights(str(self.output_path \/ \"model\" \/ \"dcgan_model\"), save_format='tf')\n                \n                print(f\"\\n{iters}\/{num_iters}\")\n                print(f'Time elapsed from start {time.time() - start} sec')\n                \n            iters += 1\n            if iters > num_iters:\n                print(f'Finished. Time elapsed from start {time.time() - start} sec')\n                return\n        \n","f99d48bb":"! pip install gdown\nimport gdown\n\nurl = 'https:\/\/drive.google.com\/uc?id=0BxYys69jI14kYVM3aVhKS1VhRUk'\noutput = '\/tmp\/UTKFace.tar.gz'\ngdown.download(url, output, quiet=False)\n! tar -xzf \/tmp\/UTKFace.tar.gz -C \/tmp\/","e402fdb5":"BATCH_SIZE = 128\nIMAGE_SIZE = 32","efa71f0b":"def prep_fn(img):\n    img = img.astype(np.float32) \/ 255.0\n    img = (img - 0.5) * 2\n    return img\n\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=prep_fn)\n\nimage_generator = generator.flow_from_directory(\n    directory=str(Path(\"\/tmp\/\")),\n    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode=None,\n    )\n","9882906c":"sample = next(image_generator)\nassert sample.shape == (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), f\"\u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c: {(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3)}.  \u041f\u043e\u043b\u0443\u0447\u0435\u043d {sample.shape}\"\nplt.imshow(((sample[0] + 1.) \/ 2))","3af5bec6":"output = Path(\".\/gan_utk_32\")\noutput.mkdir(exist_ok=True)\n(output \/ \"model\").mkdir(exist_ok=True)\n\ngan = DCGAN(image_size=IMAGE_SIZE, num_channels=3, output_path=output, \n           z_dim=100, D_h_size=128, G_h_size=128)","a5f2bc21":"images = list(output.glob(\"*.png\"))\nif images: \n    iters = list(map(lambda x: int(x.name.split(\".\")[0]), images))\n    last_iter = sorted(iters)[-1]\n    gan.start_iteration = last_iter\n    print(f\"Resuming model from {last_iter} iteration\")\n    \ngan.train(image_generator, 20500, 50)","ddf5f0cc":"def generate_data(latent_vector, generator):\n\n    gen_imgs = generator(latent_vector, training=False)\n    gen_imgs = 0.5 * gen_imgs + 0.5\n    return gen_imgs","0e106c77":"v1 = tf.random.normal([1, 100]) # random vector\nprint(\"\u0412\u0435\u043a\u0442\u043e\u0440: \", v1.numpy()[0, :10]) # print the first 10 elements\n_ = plt.imshow(generate_data(v1, gan.generator)[0]) # generated face, try generating multiple.\n                                                     # they should be realistic enough","a6c27c01":"def generate_many(generator, n):\n    vis_noise = np.random.normal(0, 1, (n, 100)).astype(np.float32)\n    gen_imgs = generator(vis_noise, training=False)\n    show_many(gen_imgs, \"Generated images\")\n    return vis_noise\n\ndef show_many(images, title=\"\"):\n    w = h = int(np.sqrt(len(images)))\n    images = (np.clip(images, -1, 1) + 1.) \/ 2. \n    \n    fig, axs = plt.subplots(w, h, figsize=(w, h))\n    if title != \"\":\n        fig.suptitle(title)\n\n    cnt = 0\n    for i in range(h):\n        for j in range(w):\n            axs[i, j].imshow(images[cnt, :, :, :])\n            axs[i, j].set_title(f\"{cnt}\")\n            axs[i, j].axis('off')\n            cnt += 1\n    plt.subplots_adjust(wspace=.5)\n    ","bc74e3b4":"def show_interpolation(v_1, v_2, generator, n=20):\n    \n    fig, axs = plt.subplots(1, n, figsize=(n,1))\n    for i, alpha in enumerate(np.linspace(0, 1, n)):\n        curr_vec = v_1 * (1-alpha) - v_2 * alpha\n        image = generate_data(curr_vec, gan.generator)[0]\n        axs[i].imshow(image)\n        axs[i].axis('off')","7803d909":"v1 = tf.random.normal([1, 100])\nv2 = tf.random.normal([1, 100])\n\nshow_interpolation(v1, v2, image_generator)","ef0af020":"faces = generate_many(gan.generator, 100)\nfaces","a6fedcc7":"with_smile = (faces[6]+faces[21]+faces[22]+faces[27]+faces[42]+faces[44]+faces[57]+faces[66]+faces[65]+faces[69])\/10\nwithout_smile = (faces[2]+faces[3]+faces[4]+faces[5]+faces[8]+faces[11]+faces[14]+faces[19]+faces[20]+faces[26])\/10","3541ec18":"with_smile-without_smile","57760099":"with_smile = tf.reshape(with_smile, [1,100])\nwithout_smile = tf.reshape(without_smile, [1,100])","1511ad4f":"## b) Finding a smile vector\n\n","900d4d29":"## Finding the vector of a smile\n \n\nWhat does this mean? As we already know, GAN generates data from random vectors from a specific distribution. Such vectors form a latent space. Let me remind you an analogy with sliders in a computer game - in this case, the latent space is set by all the positions of the sliders, which correspond to realistic faces.\n\nIn our case, GAN also learned something similar, because 100 numbers are enough for him to create a face - these are his sliders. You can see that among the generated faces there are faces with and without a smile. This means that we can assume that some specific location of these sliders (specific values \u200b\u200bof the latent vector) are responsible for the smile. Then we can assume that there is a certain combination of these 100 numbers, adding which to the vector of the corresponding neutral face, you can get the same face, only with a smile. We call these 100 numbers the \u201cvector of a smile\u201d.","8e6a7808":"## Creating data generators","f0ffddd0":"## Loading dataset","5157a212":"### a) Interpolation","2f3eaba1":"#### I made the size of the generated image small so that the training time did not take so much, it is easy to fix it - it just takes longer to train at a larger size, about 4-6 hours","100b75ce":"![alt text](https:\/\/github.com\/znxlwm\/tensorflow-MNIST-GAN-DCGAN\/raw\/master\/tensorflow_DCGAN.png)","c252d079":"<img alt = \"smile.png\" src = \"https:\/\/sun9-51.userapi.com\/c857228\/v857228713\/aea82\/t93eL7L3zbU.jpg\">","39a7712f":"## DCGAN training"}}