{"cell_type":{"f9047af1":"code","8691ed71":"code","97ea4b45":"code","f4db439b":"code","a8a7f8ef":"code","bfb6d125":"code","29163114":"code","16d522a4":"code","3525e103":"code","f14a741e":"code","406e09e6":"code","547112b5":"code","8cf022c5":"code","ecd3973f":"code","2190dd51":"code","146aa549":"code","1e5a350f":"code","dd6a51e8":"code","4105349c":"code","bd95e31e":"code","7f0f25ca":"code","5c0c9b3a":"code","5d530969":"code","9f446693":"code","f24b1637":"code","3a92dc3e":"code","92a18e9a":"code","7646f455":"code","87435dd5":"code","73a83052":"code","e10bb93f":"code","79d4514f":"code","9e347037":"code","4e6efe61":"code","e70aa0ca":"code","8cf23f0e":"code","00f86366":"code","e15fa119":"code","0c944355":"code","c44822a0":"code","d88ae651":"code","e85c2cec":"code","9a55fe27":"markdown","9800e777":"markdown","8a549a52":"markdown","cc12caca":"markdown","7b15229d":"markdown","355cec53":"markdown","cbadb9d0":"markdown","dd6caf71":"markdown","277ec457":"markdown","f2e45c35":"markdown","401f9cd5":"markdown","44fdd6a9":"markdown","8826ee37":"markdown","6ab0c37f":"markdown","20a69576":"markdown","2643db85":"markdown"},"source":{"f9047af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom pathlib import Path\nimport os\nimport os, gc\nimport random\nimport datetime\n\nfrom tqdm import tqdm_notebook as tqdm\n\n# matplotlib and seaborn for plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8691ed71":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","97ea4b45":"def dataset_reader():\n    list=['weather_test.csv'\n          ,'weather_train.csv'\n          ,'test.csv'\n          ,'train.csv'\n          ,'building_metadata.csv']\n    input = Path('\/kaggle\/input\/ashrae-energy-prediction')\n    #list= [c for c in os.listdir(input)]\n    \n    wtest = pd.read_csv(input\/list[0],parse_dates=['timestamp'])\n    wtrain = pd.read_csv(input\/list[1],parse_dates=['timestamp'])\n    test = pd.read_csv(input\/list[2],parse_dates=['timestamp'])\n    train = pd.read_csv(input\/list[3],parse_dates=['timestamp'])\n    bmdata = pd.read_csv(input\/list[4])\n\n    train['is_train'] = 1\n    test['is_train'] = 0\n    \n    # Concatenate and Merge\n    full = pd.concat([train,test],sort=True,ignore_index = True)\n    mean_mr = train.groupby('building_id').meter_reading.mean().reset_index()\n    bmdata = bmdata.merge(mean_mr,on='building_id',how='left')\n    wfull = pd.concat([wtrain,wtest],sort=True,ignore_index = True)\n    return full,wfull,bmdata\n\nfull,wfull,bmdata = dataset_reader()","f4db439b":"bmdata.groupby('primary_use').meter_reading.mean().sort_values().plot(kind='bar')","a8a7f8ef":"bmdata.groupby('primary_use').meter_reading.mean().sort_values().reset_index()","bfb6d125":"primary_use = {0: 'Religious worship',\n  1: 'Warehouse\/storage',\n  2: 'Technology\/science',\n  3: 'Other',\n  4: 'Retail',\n  5: 'Parking',\n  6: 'Lodging\/residential',\n  7: 'Manufacturing\/industrial',\n  8: 'Public services',\n  9: 'Food sales and service',\n  10: 'Entertainment\/public assembly',\n  11: 'Utility',\n  12: 'Office',\n  13: 'Healthcare',\n  14: 'Services',\n  15: 'Education'}\ninv_map = {v: k for k, v in primary_use.items()}\n","29163114":"def primary_use_encoding(x):\n    \n    for use in inv_map.keys():\n        if use == x:\n            return inv_map[use]\n        \nbmdata['pu_label'] = bmdata['primary_use'].apply(primary_use_encoding)","16d522a4":"bmdata['log_sqf'] = np.log(bmdata.square_feet)\n#from scipy import stats\n#bmdata['bcx_sqf'] = stats.boxcox(bmdata.square_feet)","3525e103":"'''\nfull['diff_pp'] = full.loc[(~full.precip_depth_1_hr.isnull())&(full.dew_temperature.isnull() | full.air_temperature.isnull())].precip_depth_1_hr.apply(pp_encoding)\nfull['diff_cd'] = full.loc[(~full.cloud_coverage.isnull())&(full.dew_temperature.isnull() | full.air_temperature.isnull())].cloud_coverage.apply(cd_encoding)\ndef pp_encoding(x):\n    \n    if np.isnan(x):\n        return x\n    \n    else:\n        for diff in diff_precip['mean'].keys():\n\n            if diff == x:\n                return round(float(np.random.normal(diff_precip['mean'][diff], diff_precip['std'][diff], 1)),2)\n        \n    return x\ndef cd_encoding(x):\n    \n    if np.isnan(x):\n        return x\n    else:\n        for diff in diff_cloud['mean'].keys():\n\n            if diff == x:\n                return round(float(np.random.normal(diff_cloud['mean'][diff], diff_cloud['std'][diff], 1)),2)\n    return x\n    \n    full['est_dew_p'] = full.air_temperature.loc[~full.diff_pp.isnull()] - full.diff_pp\nfull['est_air_p'] = full.dew_temperature.loc[~full.diff_pp.isnull()] + full.diff_pp\nfull['est_dew_c'] = full.air_temperature.loc[~full.diff_cd.isnull()] - full.diff_cd\nfull['est_air_c'] = full.dew_temperature.loc[~full.diff_cd.isnull()] + full.diff_cd\n\n#precipitation first\nfull.air_temperature.fillna(full.est_air_p, inplace=True)\nfull.dew_temperature.fillna(full.est_dew_p, inplace=True)\n#cloud next\nfull.air_temperature.fillna(full.est_air_c, inplace=True)\nfull.dew_temperature.fillna(full.est_dew_c, inplace=True)\nfull['diff_pp'] = full.loc[(~full.precip_depth_1_hr.isnull())\n                           &(full.dew_temperature.isnull() | full.air_temperature.isnull())].precip_depth_1_hr.apply(pp_encoding)\nfull['diff_cd'] = full.loc[(~full.cloud_coverage.isnull())\n                           &(full.dew_temperature.isnull() | full.air_temperature.isnull())].cloud_coverage.apply(cd_encoding)\n\nfull['est_dew_p'] = full.air_temperature.loc[~full.diff_pp.isnull()] - full.diff_pp\nfull['est_air_p'] = full.dew_temperature.loc[~full.diff_pp.isnull()] + full.diff_pp\nfull['est_dew_c'] = full.air_temperature.loc[~full.diff_cd.isnull()] - full.diff_cd\nfull['est_air_c'] = full.dew_temperature.loc[~full.diff_cd.isnull()] + full.diff_cd\n\n#precipitation first\nfull.air_temperature.fillna(full.est_air_p, inplace=True)\nfull.dew_temperature.fillna(full.est_dew_p, inplace=True)\n#cloud next\nfull.air_temperature.fillna(full.est_air_c, inplace=True)\nfull.dew_temperature.fillna(full.est_dew_c, inplace=True)\n'''\n","f14a741e":"bmdata.groupby('primary_use').floor_count.mean().apply(np.ceil).plot(kind='bar')","406e09e6":"floor_avg = bmdata.groupby('primary_use').floor_count.mean().apply(np.ceil).fillna(1).to_dict()","547112b5":"floor_avg = bmdata.groupby('primary_use').floor_count.mean().apply(np.ceil).fillna(1).to_dict()\ndef floor_encoding(x):\n    \n    if pd.isna(x):\n        return np.nan\n    else:\n        for floor in floor_avg.keys():\n            if floor in x:\n                return floor_avg[floor]\n    return np.nan\n\nnew = bmdata.loc[bmdata.floor_count.isnull()].primary_use.apply(floor_encoding)\nbmdata['floor_count'].fillna(new, inplace=True)","8cf022c5":"from sklearn import preprocessing\nencoder = LabelEncoder()\nbmdata['primary_use'] = encoder.fit_transform(bmdata['primary_use'])","ecd3973f":"bmdata['year_built']=bmdata['year_built'].fillna(bmdata['year_built'].mean())\nbmdata['year_built']=bmdata['year_built']-1900\nbmdata.isnull().sum()","2190dd51":"#from tqdm import tqdm\n#lists = ['air_temperature','dew_temperature','cloud_coverage','sea_level_pressure','wind_direction','wind_speed','precip_depth_1_hr']\n#size = full.building_id.nunique()\n#for li in lists:\n    #print(li)\n    #for i in tqdm(range(size)):\n        #full[li].update(full.loc[full.building_id==i][li].interpolate(method='pchip',limit_direction='both'))","146aa549":"from tqdm import tqdm\nlists = ['air_temperature','dew_temperature','cloud_coverage','sea_level_pressure','wind_direction','wind_speed','precip_depth_1_hr']\nsize = wfull.site_id.nunique()\nfor li in lists:\n    print(li)\n    for i in tqdm(range(size)):\n        wfull[li].update(wfull.loc[wfull.site_id==i][li].interpolate(method='ffill'))\n        wfull[li].update(wfull.loc[wfull.site_id==i][li].interpolate(method='bfill'))\nwfull.isnull().sum()","1e5a350f":"wfull.columns","dd6a51e8":"wfull.groupby('site_id')[['air_temperature', 'cloud_coverage', 'dew_temperature',\n       'precip_depth_1_hr', 'sea_level_pressure', 'site_id', 'timestamp',\n       'wind_direction', 'wind_speed']].mean()","4105349c":"wfull['Month']= wfull.timestamp.dt.month\nwfull['Day']= wfull.timestamp.dt.day\nwfull['Hour'] = wfull.timestamp.dt.hour\nwfull['Weekday'] = wfull.timestamp.dt.weekday","bd95e31e":"for i in tqdm(range(12)):\n    wfull.update(wfull[wfull.Month==i+1].fillna(wfull[wfull.Month==i+1].mean()))","7f0f25ca":"dt = wfull.dew_temperature\nat = wfull.air_temperature\nws = wfull.wind_speed\n#Relative Humidity\nwfull['RH'] = 100*(0.6108*np.exp((17.27*dt)\/(dt+237.3)))\/(0.6108*np.exp((17.27*at)\/(at+237.3)))","5c0c9b3a":"wfull.sea_level_pressure = wfull.sea_level_pressure - 1000\nwfull.wind_direction = wfull.wind_direction%360\nwfull.isnull().sum()","5d530969":"wfull.groupby('Month')[['air_temperature','dew_temperature','cloud_coverage','sea_level_pressure','wind_direction','wind_speed','precip_depth_1_hr']].mean()","9f446693":"import gc\ngc.collect()","f24b1637":"full = reduce_mem_usage(full)\nwfull = reduce_mem_usage(wfull)\nbmdata = reduce_mem_usage(bmdata)","3a92dc3e":"del bmdata['meter_reading']\nfull = full.merge(bmdata, on='building_id', how='left')\nfull = full.merge(wfull, on=['site_id', 'timestamp'], how='left')\ndel bmdata\ndel wfull\ngc.collect()","92a18e9a":"full.sample(5)","7646f455":"train = full.loc[(full.is_train==1)&(full.building_id<15)]\ntest = full[full.is_train==0]\nprint('done')","87435dd5":"train.shape","73a83052":"train[\"meter_reading\"]=np.log1p(train[\"meter_reading\"])\nprint('done here')","e10bb93f":"target= 'meter_reading'\ndo_not_use = ['meter_reading'\n                 ,'is_train'\n                ,'row_id'\n                ,'square_feet'\n                ,'timestamp'\n                ,'primary_use'\n                ,'random']\n\nfeature_columns = [c for c in full.columns if c not in do_not_use ]\nfeature_columns","79d4514f":"gc.collect()\ngc.collect()","9e347037":"## Training(LGBM)","4e6efe61":"import lightgbm as lgb\nfolds = 4\nseed = 777\nmodels=[]\nfeature_importance = pd.DataFrame()\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\nfor train_idx, valid_idx in tqdm(kf.split(train,train['building_id']),total=folds):\n    print(f'Training and predicting for target {target}')\n    Xtr = train[feature_columns].iloc[train_idx]\n    Xv = train[feature_columns].iloc[valid_idx]\n    ytr = train[target].iloc[train_idx].values\n    yv = train[target].iloc[valid_idx].values\n    print('Train_size: ',Xtr.shape[0],'Validation_size: ', ytr.shape[0])\n    \n    dtrain = lgb.Dataset(Xtr, label=ytr)\n    dvalid = lgb.Dataset(Xv, label=yv)\n    \n    params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'learning_rate': 0.5,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq' : 5\n            }\n    model = lgb.train(params,\n                dtrain,\n                num_boost_round=2000,\n                valid_sets=(dtrain, dvalid),\n               early_stopping_rounds=20,\n               verbose_eval = 20)\n    \n    \n    #feature importance\n    #f_imp = pd.DataFrame()\n    #f_imp['feature'] = feature_columns\n    #f_imp[\"importance\"] = model.feature_importances_\n    #f_imp[\"fold\"] = nfold\n    #nfold += 1\n    #feature_importance = pd.concat([feature_importance, f_imp],axis=0,ignore_index=True)\n    models.append(model)\n    gc.collect()","e70aa0ca":"import matplotlib.pyplot as plt\nfeature_imp = pd.DataFrame(sorted(zip(models[0].feature_importance(), models[0].feature_name()),reverse = True), columns=['Value','Feature'])\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","8cf23f0e":"# split test data into batches\nset_size = len(test)\niterations = 50\nbatch_size = set_size \/\/ iterations\n\nprint(set_size, iterations, batch_size)\nassert set_size == iterations * batch_size","00f86366":"meter_reading = []\nfor i in tqdm(range(iterations)):\n    pos = i*batch_size\n    fold_preds = [np.expm1(model.predict(test[feature_columns].iloc[pos : pos+batch_size])) for model in models]\n    meter_reading.extend(np.mean(fold_preds, axis=0))\n\nprint(len(meter_reading))\nassert len(meter_reading) == set_size","e15fa119":"submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\nsubmission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None)","0c944355":"submission.to_csv('submission.csv', index=False)\nsubmission.head(9)","c44822a0":"#explainer = shap.TreeExplainer(models[0])\n#shap_values = explainer.shap_values(train[feature_columns])","d88ae651":"#shap.force_plot(explainer.expected_value,shap_values[0,:] ,train[feature_columns].iloc[0,:], matplotlib=True)","e85c2cec":"#shap.summary_plot(shap_values, train[feature_columns], plot_type=\"bar\")","9a55fe27":"## Merge all datasets","9800e777":"Overfitting :D","8a549a52":"### rescale some features\n\n$sea\\ level\\ pressure \\rightarrow sea\\ level\\ pressure - 1000$\n\n$wind\\ direction \\rightarrow wind\\ direction \\mod 360$","cc12caca":"Reference: https:\/\/www.kaggle.com\/hmendonca\/shapley-values-for-feature-selection-ashrae by Henrique Mendon\u00e7a","7b15229d":"## Building metadata","355cec53":"#### SHAP Explainer(Practice)","cbadb9d0":"* air_temperature\n* dew_temperature\n* cloud_coverage\n* sea_level_pressure wind_direction\n* wind_speed\n* precip_depth_1_hr \n\nTentative estimation\n($forward\\ fill \\rightarrow backward\\ fill$)","dd6caf71":"## weather data","277ec457":"### Add Relative Humidity Approx\n\n$$100*\\frac{(\\exp\\frac{17.27*dew}{dew+237.3})}{(\\exp\\frac{17.27*air}{air+237.3})}$$","f2e45c35":"#### year_built","401f9cd5":"### Mean filling w.r.t Month","44fdd6a9":"### floor_count \n(mean filling\/\/ if NaN , 1)","8826ee37":"## Read Datasets","6ab0c37f":"###  square_feet (log-scale)","20a69576":"#### Observe : Some sites are having etirely missing feature.","2643db85":"### primary usage"}}