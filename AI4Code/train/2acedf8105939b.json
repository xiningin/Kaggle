{"cell_type":{"5a9a6235":"code","08563704":"code","90d1a701":"code","dc13b3e6":"code","2044566b":"code","f498d75e":"code","c340fdef":"code","f90b991c":"code","aeae4b16":"code","5e813ddf":"code","ab3244ef":"code","e370189d":"code","4d564cfe":"code","df3a712e":"code","b7011c04":"code","07232f9e":"code","b197fa46":"code","69eaadb3":"code","f00e86dc":"code","e991e01e":"code","9e22572a":"code","d56ed4b1":"code","190ee000":"code","8f1d1c0a":"code","81d332f1":"code","7c6d621d":"markdown","d9480914":"markdown","32a07b51":"markdown","193d60ce":"markdown","a97b0f8b":"markdown","b6eb683d":"markdown","91d9ba0e":"markdown","89601cdd":"markdown","713f9cd4":"markdown","a62973e4":"markdown","efee28de":"markdown","9eac2279":"markdown","41ffbec6":"markdown","0c100cc0":"markdown","6c8bc987":"markdown","4131d9fc":"markdown","ef2dc193":"markdown","66b182ef":"markdown","27e7249a":"markdown","223e178b":"markdown","fc9738fa":"markdown","072b3d14":"markdown","c261cdbe":"markdown","c42738ee":"markdown","c043d894":"markdown","83d8e657":"markdown","a1badf84":"markdown","14967953":"markdown","ef7a230a":"markdown","db3bd841":"markdown","ccb65da7":"markdown"},"source":{"5a9a6235":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08563704":"data = pd.read_csv('\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndata","90d1a701":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nsplit = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n\nfor train_index,test_index in split.split(data,data[\"race\/ethnicity\"]):\n    train_set = data.loc[train_index]\n    test_set = data.loc[test_index]","dc13b3e6":"def add_attr_trace(fig, dataset, attr, r, c):\n    fig.add_trace(go.Histogram(x=dataset[attr].sort_values(), histnorm='probability', name=attr), row=r, col=c)","2044566b":"fig = make_subplots(rows=2, cols=5, shared_yaxes=True,\n                    subplot_titles=(\"entire data\",\"entire data\", \"entire data\", \"entire data\", \"entire data\",\n                                   \"testing data\",\"testing data\", \"testing data\", \"testing data\", \"testing data\"))\n\nattrs = data.columns[:5]\n\nfor i in range(len(attrs)):\n    add_attr_trace(fig, data, attrs[i], 1, i + 1)\n    add_attr_trace(fig, test_set, attrs[i], 2, i + 1)\n\nfig.update_layout(showlegend=False, height=900)\nfig.show()","f498d75e":"def add_score_trace(fig, dataset, impact_attr, attr_value, score_type, r, c):\n    fig.add_trace(\n        go.Histogram(x=dataset[dataset[impact_attr] == attr_value][score_type],\n                     histnorm='probability',\n                     name=attr_value + ' - ' + score_type),\n        row=r, col=c)","c340fdef":"fig = make_subplots(rows=5, cols=3, shared_yaxes=True,\n                    subplot_titles=(\"gender - math\", \"gender - reading\", \"gender - writing\",\n                                    \"race\/ethnicity - math\", \"race\/ethnicity - reading\", \"race\/ethnicity - writing\", \n                                    \"parental level - math\", \"parental level - reading\", \"parental level - writing\",\n                                    \"lunch - math\", \"lunch - reading\", \"lunch - writing\",\n                                    \"test preparation - math\", \"test preparation - reading\", \"test preparation - writing\"))\n\nscore_types = data.columns[5:8]\n\nfor i in range(len(attrs)):\n    attr_values = data[attrs[i]].unique()\n    for j in range(len(attr_values)):\n        for k in range(len(score_types)):\n            add_score_trace(fig, data, attrs[i], attr_values[j], score_types[k], i + 1, k + 1)\n            \nfig.update_layout(showlegend=False, barmode='stack', height=1500)\nfig.show()","f90b991c":"X_train = train_set.drop(['math score', 'reading score', 'writing score'], axis=1)\ny_train_math = train_set['math score'].copy()\ny_train_reading = train_set['reading score'].copy()\ny_train_writing = train_set['writing score'].copy()","aeae4b16":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncategory_attrs = attrs\n\nfull_pipeline = ColumnTransformer([('category', OneHotEncoder(), category_attrs)])\n\nX_train = full_pipeline.fit_transform(X_train)","5e813ddf":"from sklearn.linear_model import LinearRegression\n\nlin_regr_math = LinearRegression()\nlin_regr_math.fit(X_train, y_train_math)\n\nlin_regr_reading = LinearRegression()\nlin_regr_reading.fit(X_train, y_train_reading)\n\nlin_regr_writing = LinearRegression()\nlin_regr_writing.fit(X_train, y_train_writing)","ab3244ef":"from sklearn.tree import DecisionTreeRegressor\n\ntree_regr_math = DecisionTreeRegressor(random_state=42)\ntree_regr_math.fit(X_train, y_train_math)\n\ntree_regr_reading = DecisionTreeRegressor(random_state=42)\ntree_regr_reading.fit(X_train, y_train_reading)\n\ntree_regr_writing = DecisionTreeRegressor(random_state=42)\ntree_regr_writing.fit(X_train, y_train_writing)","e370189d":"# plot tree\n# from sklearn import tree\n# tree.plot_tree(tree_regr_math)","4d564cfe":"# export tree\n# import graphviz \n# dot_data = tree.export_graphviz(tree_regr_math, out_file=None) \n# graph = graphviz.Source(dot_data)\n# graph.render(\"math\")","df3a712e":"from sklearn.ensemble import RandomForestRegressor\n\nforest_regr_math = RandomForestRegressor(random_state=42)\nforest_regr_math.fit(X_train, y_train_math)\n\nforest_regr_reading = RandomForestRegressor(random_state=42)\nforest_regr_reading.fit(X_train, y_train_math)\n\nforest_regr_writing = RandomForestRegressor(random_state=42)\nforest_regr_writing.fit(X_train, y_train_math)","b7011c04":"from sklearn.metrics import mean_squared_error\n\ndef predict(model, X, y, tag):\n    predictions = model.predict(X)\n    mse = mean_squared_error(y, predictions)\n    rmse = np.sqrt(mse)\n    print('prediction for ' + tag + ': rmse = ', rmse)","07232f9e":"print('Linear Regression ----------------')\npredict(lin_regr_math, X_train, y_train_math, 'math score')\npredict(lin_regr_reading, X_train, y_train_reading, 'reading score')\npredict(lin_regr_writing, X_train, y_train_writing, 'writing score')","b197fa46":"print('Decision Tree Regressor  ----------------')\npredict(tree_regr_math, X_train, y_train_math, 'math score')\npredict(tree_regr_reading, X_train, y_train_reading, 'reading score')\npredict(tree_regr_writing, X_train, y_train_writing, 'writing score')","69eaadb3":"print('Random Forest Regressor ----------------')\npredict(forest_regr_math, X_train, y_train_math, 'math score')\npredict(forest_regr_reading, X_train, y_train_reading, 'reading score')\npredict(forest_regr_writing, X_train, y_train_writing, 'writing score')","f00e86dc":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard deviation:', scores.std())","e991e01e":"def apply_cross_validation(estimator, X, y, tag):\n    scores = cross_val_score(estimator, X, y, scoring='neg_mean_squared_error', cv=10)\n    rmse_scores = np.sqrt(-scores)\n    print()\n    print('********** ' + tag + ' **********')\n    display_scores(rmse_scores)","9e22572a":"from sklearn.model_selection import cross_val_score\n\nprint()\nprint('cross validation for linear regressions -----------------------------')\napply_cross_validation(lin_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(lin_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(lin_regr_writing, X_train, y_train_writing, 'writing score')\n\nprint()\nprint('cross validation for decision tree regressors -----------------------------')\napply_cross_validation(tree_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(tree_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(tree_regr_writing, X_train, y_train_writing, 'writing score')\n\nprint()\nprint('cross validation for random forest regressors -----------------------------')\napply_cross_validation(forest_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(forest_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(forest_regr_writing, X_train, y_train_writing, 'writing score')","d56ed4b1":"from sklearn.model_selection import GridSearchCV\n\ndef grid_search_cv(estimator, param_grid, X, y, tag):\n    print()\n    print(tag + ' ----------------------------------')\n    \n    grid_search = GridSearchCV(estimator, param_grid, verbose=1, cv=10,\n                              scoring='neg_mean_squared_error',\n                              return_train_score=True, refit=True)\n\n    grid_search.fit(X, y)\n    print()\n    print('best_params_: ')\n    print(grid_search.best_params_)\n    \n    results = grid_search.cv_results_\n    for mean_score, params in zip(results['mean_test_score'], results['params']):\n        print(np.sqrt(-mean_score), params)\n        \n    feature_importances = grid_search.best_estimator_.feature_importances_\n    print()\n    print('feature_importances: ')\n    print(feature_importances)\n    \n    return grid_search","190ee000":"print('Grid search for decision tree regressors -------------------------------')\nparam_grid = {'max_depth': list(range(2, 10)), 'min_samples_split': [2, 3, 4, 5, 6]}\ngrid_search_tree_math = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_math, 'math score')\ngrid_search_tree_reading = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_reading, 'reading score')\ngrid_search_tree_writing = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_writing, 'writing score')\n\n\nprint()\nprint('Grid search for random forest regressors -------------------------------')\nparam_grid = {'max_depth': list(range(2, 10)), 'min_samples_split': [2, 3, 4, 5, 6]}\ngrid_search_forest_math = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_math, 'math score')\ngrid_search_forest_reading = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_reading, 'reading score')\ngrid_search_forest_writing = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_writing, 'writing score')","8f1d1c0a":"def final_predict(grid_search, X, y, tag):\n    predictions = grid_search.best_estimator_.predict(X)\n    mse = mean_squared_error(y, predictions)\n    rmse = np.sqrt(mse)\n    print('final predict for ' + tag + ': rmse = ', rmse)","81d332f1":"X_test = test_set.drop(['math score', 'reading score', 'writing score'], axis=1)\ny_test_math = test_set['math score'].copy()\ny_test_reading = test_set['reading score'].copy()\ny_test_writing = test_set['writing score'].copy()\nX_test = full_pipeline.transform(X_test)\n\nprint('Linear Regression  ----------------')\npredict(lin_regr_math, X_test, y_test_math, 'math score')\npredict(lin_regr_reading, X_test, y_test_reading, 'reading score')\npredict(lin_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Decision Tree Regressor  ----------------')\npredict(tree_regr_math, X_test, y_test_math, 'math score')\npredict(tree_regr_reading, X_test, y_test_reading, 'reading score')\npredict(tree_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Fine-tuned Decision Tree Regressor  ----------------')\nfinal_predict(grid_search_tree_math, X_test, y_test_math, 'math score')\nfinal_predict(grid_search_tree_reading, X_test, y_test_reading, 'reading score')\nfinal_predict(grid_search_tree_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Random Forest Regressor ----------------')\npredict(forest_regr_math, X_test, y_test_math, 'math score')\npredict(forest_regr_reading, X_test, y_test_reading, 'reading score')\npredict(forest_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Fine-tuned Random Forest Regressor  ----------------')\nfinal_predict(grid_search_forest_math, X_test, y_test_math, 'math score')\nfinal_predict(grid_search_forest_reading, X_test, y_test_reading, 'reading score')\nfinal_predict(grid_search_forest_writing, X_test, y_test_writing, 'writing score')","7c6d621d":"Impaction of feature values on scores.","d9480914":"Try to find how does each feature impact the scores.","32a07b51":"Split the dataset to two parts - training set and testing set, the training set is used for training and cross validating, the testing set is used for the final testing.","193d60ce":"# Reference","a97b0f8b":"# Training Models","b6eb683d":"# Visualize Data","91d9ba0e":"## Random Forest Regressor","89601cdd":"Training decision tree regression models.","713f9cd4":"## Decision Tree Regressor","a62973e4":"1. https:\/\/github.com\/ageron\/handson-ml2\n2. https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%2C%20sometimes%20called%20rotation,to%20an%20independent%20data%20set.","efee28de":"Decision tree models","9eac2279":"Training linear regression models.","41ffbec6":"Linear models","0c100cc0":"# Conclusion","6c8bc987":"Training random forest regression models.","4131d9fc":"We explored three different models:\n* Linear Regression\n* Decision Tree\n* Random Forest\n\nIt seems like linear regression is better than decision tree regressor and random forest regression for this dataset.","ef2dc193":"# Prepare Data","66b182ef":"# Split Dataset","27e7249a":"Random forest models","223e178b":"## Linear Regression","fc9738fa":"# Fine Turning","072b3d14":"# Cross validate","c261cdbe":"Transform the category data to numerics.","c42738ee":"# Final Testing","c043d894":"Cross validation is used to test a model's ability to predict new data that was not used in training it.","83d8e657":"Get better models by using Grid Search CV method. ","a1badf84":"# Load Data","14967953":"# Students Performance in Exams\n","ef7a230a":"This notebook mainly describes the process of training models and fine-tuning models.","db3bd841":"# Predict Training Data","ccb65da7":"Distribution of feature values."}}