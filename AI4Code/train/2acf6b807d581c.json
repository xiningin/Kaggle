{"cell_type":{"5260fda2":"code","2f248f73":"code","8790ebc9":"code","2da946b1":"code","2bbe8d64":"code","473eb23c":"code","61f18602":"code","cf83e388":"code","8f027642":"code","78287fcb":"code","7bc9cd4d":"code","642d1476":"code","37f1913b":"code","1c2b6348":"code","b71e734e":"code","2af5641f":"code","c522e92c":"code","b97926ae":"code","99e13804":"code","cea65983":"code","71a404ad":"markdown","1c9f6938":"markdown","296506a6":"markdown","3ac0b4ee":"markdown","87c29a61":"markdown","81b6c0c5":"markdown","80d66fa0":"markdown","33ce2060":"markdown","43d2d833":"markdown","5cedd9aa":"markdown","d88a2352":"markdown","42978e77":"markdown","a3caafd7":"markdown","5e6694ec":"markdown","2b18f765":"markdown","40cf380b":"markdown","1d4d0ec0":"markdown","9362cf41":"markdown","23e03fb5":"markdown","c89b71e8":"markdown","10432535":"markdown","c0426aa1":"markdown"},"source":{"5260fda2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nimport warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\npd.pandas.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","2f248f73":"def data():\n    train_ = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n    test_ = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n    dataframe = pd.concat([train_, test_], ignore_index=True)\n    return dataframe, train_, test_\ndf, train, test = data()\n\n\ndf.head()","8790ebc9":"df.isnull().any().sum()","2da946b1":"cat_cols = [col for col in df.columns if df[col].dtypes == 'O']\nprint('Kategorik De\u011fi\u015fken Say\u0131s\u0131: ', len(cat_cols))\n\n\ndef cat_summary(data, categorical_cols, target, number_of_classes=10):\n    var_count = 0\n    vars_more_classes = []\n    for var in categorical_cols:\n        if len(df[var].value_counts()) <= number_of_classes:  # s\u0131n\u0131f say\u0131s\u0131na g\u00f6re se\u00e7\n            print(pd.DataFrame({var: data[var].value_counts(),\n                                \"Ratio\": 100 * data[var].value_counts() \/ len(data),\n                                \"TARGET_MEDIAN\": data.groupby(var)[target].median()}), end=\"\\n\\n\\n\")\n            var_count += 1\n        else:\n            vars_more_classes.append(data[var].name)\n    print('%d categorical variables have been described' % var_count, end=\"\\n\\n\")\n    print('There are', len(vars_more_classes), \"variables have more than\", number_of_classes, \"classes\", end=\"\\n\\n\")\n    print('Variable names have more than %d classes:' % number_of_classes, end=\"\\n\\n\")\n    print(vars_more_classes)\n\n\n\ncat_summary(df, cat_cols, \"SalePrice\")","2bbe8d64":"\nfor col in ['Neighborhood', 'Exterior1st', 'Exterior2nd']:\n    print(df[col].value_counts())","473eb23c":"num_cols = [col for col in df.columns if df[col].dtypes != 'O' and col not in \"Id\"]\nprint('Say\u0131sal de\u011fi\u015fken say\u0131s\u0131: ', len(num_cols))\n\n\ndef hist_for_nums(data, numeric_cols):\n    col_counter = 0\n    data = data.copy()\n    for col in numeric_cols:\n        data[col].hist(bins=20)\n        plt.xlabel(col)\n        plt.title(col)\n        plt.show()\n        col_counter += 1\n    print(col_counter, \"variables have been plotted\")\n\n\nhist_for_nums(df, num_cols)","61f18602":"df[\"SalePrice\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99])","cf83e388":"def find_correlation(dataframe, corr_limit=0.60):          # our limit is 0.60 if you want you can change it\n    high_correlations = []\n    low_correlations = []\n    for col in num_cols:\n        if col == \"SalePrice\":\n            pass\n\n        else:\n            correlation = dataframe[[col, \"SalePrice\"]].corr().loc[col, \"SalePrice\"]\n            print(col, correlation)\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \": \" + str(correlation))\n            else:\n                low_correlations.append(col + \": \" + str(correlation))\n    return low_correlations, high_correlations\n\n\nlow_corrs, high_corrs = find_correlation(df)","8f027642":"def rare_analyser(dataframe, target, rare_perc):\n    rare_columns = [col for col in df.columns if len(df[col].value_counts()) <= 20\n                    and (df[col].value_counts() \/ len(df) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        print(var, \":\", len(dataframe[var].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}), end=\"\\n\\n\\n\")\n\n\nrare_analyser(df, \"SalePrice\", 0.01)\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n    return temp_df\n\n\ndf = rare_encoder(df, 0.01)\nrare_analyser(df, \"SalePrice\", 0.01)","78287fcb":"\ndrop_list = [\"Street\", \"Utilities\", \"LandSlope\", \"PoolQC\", \"MiscFeature\"]\ncat_cols = [col for col in df.columns if df[col].dtypes == 'O'\n            and col not in drop_list]\n\nfor col in drop_list:\n    df.drop(col, axis=1, inplace=True)\n\nrare_analyser(df, \"SalePrice\", 0.01)","7bc9cd4d":"def one_hot_encoder(dataframe, categorical_cols, nan_as_category=True):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, dummy_na=nan_as_category, drop_first=True)\n    new_columns = [c for c in dataframe.columns if c not in original_columns]\n    return dataframe, new_columns\n\n\ndf, new_cols_ohe = one_hot_encoder(df, cat_cols)\ncat_summary(df, new_cols_ohe, \"SalePrice\")","642d1476":"def missing_values_table(dataframe):\n    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df)\n    return variables_with_na\n\n\nmissing_values_table(df)\n","37f1913b":"df = df.apply(lambda x: x.fillna(x.median()), axis=0)\nmissing_values_table(df)","1c2b6348":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.05)\n    quartile3 = dataframe[variable].quantile(0.95)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef has_outliers(dataframe, num_col_names, plot=False):\n    variable_names = []\n    for col in num_col_names:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n            print(col, \":\", number_of_outliers)\n            variable_names.append(col)\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n    return variable_names\n\n\nhas_outliers(df, num_cols)","b71e734e":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n\nhas_outliers(df, num_cols)","2af5641f":"like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 20]\n\n\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Id\"\n                   and col not in \"SalePrice\"\n                   and col not in like_num]\n\ndf[cols_need_scale].head()\ndf[cols_need_scale].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T\nhist_for_nums(df, cols_need_scale)\n\n\ndef robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)\n\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])\n\n\ndf[cols_need_scale].head()\ndf[cols_need_scale].describe().T\nhist_for_nums(df, cols_need_scale)","c522e92c":"# Adding total sqfootage feature\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n# Overall quality of the house\ndf[\"OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n\ndf['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\ndf['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n\ndf.loc[df[\"OverallQual\"] < 2, \"OverallQual\"] = 2\ndf.loc[df[\"GarageCars\"] > 4, \"GarageCars\"] = 4\ndf.loc[(df[\"OverallQual\"] == 2) | (df[\"OverallQual\"] == 3), \"OverallQual\"] = 3\n\n\n# SalePrice 163k, Generally, 163k salepricants garage built value 2005\ndf.loc[df[\"GarageYrBlt\"] == 2207, \"GarageYrBlt\"] = 2005\ndf.loc[df[\"Fireplaces\"] == 4, \"Fireplaces\"] = 0\ndf.loc[df[\"Fireplaces\"] == 3, \"Fireplaces\"] = 2\n\n\n# NEW_1\ndf[\"new_area\"] = df[\"GrLivArea\"] + df[\"GarageArea\"]\n\n# NEW_2 \ndf[\"new_home\"] = df[\"YearBuilt\"]\ndf.loc[df[\"new_home\"] == df[\"YearRemodAdd\"], \"new_home\"] = 0\ndf.loc[df[\"new_home\"] != df[\"YearRemodAdd\"], \"new_home\"] = 1\n\n# NEW_3  #Sum of Bath numbers\ndf[\"new_bath\"] = df[\"FullBath\"] + (df[\"HalfBath\"] * 0.5)\n\ndf.head()","b97926ae":"missing_values_table(df)\nhas_outliers(df, num_cols)","99e13804":"train_df = df[df['SalePrice'].notnull()]\ntest_df = df[df['SalePrice'].isnull()]\n\nX = train_df.drop('SalePrice', axis=1)\ny = train_df[[\"SalePrice\"]]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)\n\n\n\nmodels = [('LinearRegression', LinearRegression()),\n          ('Ridge', Ridge()),\n          ('Lasso', Lasso()),\n          ('ElasticNet', ElasticNet())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    result = np.sqrt(mean_squared_error(y_test, y_pred))\n    results.append(result)\n    names.append(name)\n    msg = \"%s: %f\" % (name, result)\n    print(msg)\n\ndf[\"SalePrice\"].mean()","cea65983":"from catboost import CatBoostRegressor\n\nX = df.loc[:1459, :].drop([\"SalePrice\", \"Id\"], axis=1)\ny = df.loc[:1459, \"SalePrice\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)\n\ncatboost_model = CatBoostRegressor()\ncatboost_model = catboost_model.fit(X_train, y_train)\ny_pred = catboost_model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_pred, y_test)))\n\n\n# TO CSV , Load to Kaggle\n\ndf1 = pd.DataFrame({\"Id\": df.loc[1460:, \"Id\"],\n                      \"SalePrice\": catboost_model.predict(df.loc[1460:, :].drop([\"SalePrice\", \"Id\"], axis=1))})\n\ndf1.to_csv('submission_catboost.csv', index=False)","71a404ad":"Now, set thresholds(low and up limits) for outliers then use for outliers:","1c9f6938":"Last control before bridge :)","296506a6":"### 2.3. Numerical Variable Analysis","3ac0b4ee":"### 3.5. Standardization","87c29a61":"## Feature Engineering","81b6c0c5":"Variables with more than 10 classes:","80d66fa0":"### 5.2. CatBoost Model","33ce2060":"Then drop some values after Rare analysis and observe what you have:","43d2d833":"## 3. Data Prepcoressing & Feature Engineering\n### 3.1. Rare Analysis\nFirst, observe the Rare values and perform operations.","5cedd9aa":"## 5. Modelling\n### 5.1. Liear, Ridge, Lasso and ElasticNet Regression Models","d88a2352":"### 2.2. Categorical Variable Analysis","42978e77":"Fill the missing values by \"median\" then apply all data:","a3caafd7":"Then look at the correlations between target and independent variables.","5e6694ec":"### 3.2. Label Encoding & One-Hot Encoding","2b18f765":"### 3.4. Outliers","40cf380b":"## 2. EDA (Exploratory of Data Analysis)\n### 2.1. Data Preperation","1d4d0ec0":"Let's check NaN values:","9362cf41":"\n## Conclusion\n\nWe got 19K in this model estimation evaluation. A very successful modeling has been realized.\n\n\n#### Note:\n\n   - After this notebook, my aim is to prepare 'kernel' which is 'not clear' data set.\n\n   - If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n\n   - Thank you for your suggestion and votes ;)","23e03fb5":"### 3.3. Missing Values","c89b71e8":"# **Data Description**\n\n![housesbanner.png](attachment:housesbanner.png)\n\n\n## File descriptions\n\n   - train.csv - the training set\n   - test.csv - the test set\n   - data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n   - sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n## Data fields\n\nHere's a brief version of what you'll find in the data description file.\n\n   - SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n   - MSSubClass: The building class\n   - MSZoning: The general zoning classification\n   - LotFrontage: Linear feet of street connected to property\n   - LotArea: Lot size in square feet\n   - Street: Type of road access\n   - Alley: Type of alley access\n   - LotShape: General shape of property\n   - LandContour: Flatness of the property\n   - Utilities: Type of utilities available\n   - LotConfig: Lot configuration\n   - LandSlope: Slope of property\n   - Neighborhood: Physical locations within Ames city limits\n   - Condition1: Proximity to main road or railroad\n   - Condition2: Proximity to main road or railroad (if a second is present)\n   - BldgType: Type of dwelling\n   - HouseStyle: Style of dwelling\n   - OverallQual: Overall material and finish quality\n   - OverallCond: Overall condition rating\n   - YearBuilt: Original construction date\n   - YearRemodAdd: Remodel date\n   - RoofStyle: Type of roof\n   - RoofMatl: Roof material\n   - Exterior1st: Exterior covering on house\n   - Exterior2nd: Exterior covering on house (if more than one material)\n   - MasVnrType: Masonry veneer type\n   - MasVnrArea: Masonry veneer area in square feet\n   - ExterQual: Exterior material quality\n   - ExterCond: Present condition of the material on the exterior\n   - Foundation: Type of foundation\n   - BsmtQual: Height of the basement\n   - BsmtCond: General condition of the basement\n   - BsmtExposure: Walkout or garden level basement walls\n   - BsmtFinType1: Quality of basement finished area\n   - BsmtFinSF1: Type 1 finished square feet\n   - BsmtFinType2: Quality of second finished area (if present)\n   - BsmtFinSF2: Type 2 finished square feet\n   - BsmtUnfSF: Unfinished square feet of basement area\n   - TotalBsmtSF: Total square feet of basement area\n   - Heating: Type of heating\n   - HeatingQC: Heating quality and condition\n   - CentralAir: Central air conditioning\n   - Electrical: Electrical system\n   - 1stFlrSF: First Floor square feet\n   - 2ndFlrSF: Second floor square feet\n   - LowQualFinSF: Low quality finished square feet (all floors)\n   - GrLivArea: Above grade (ground) living area square feet\n   - BsmtFullBath: Basement full bathrooms\n   - BsmtHalfBath: Basement half bathrooms\n   - FullBath: Full bathrooms above grade\n   - HalfBath: Half baths above grade\n   - Bedroom: Number of bedrooms above basement level\n   - Kitchen: Number of kitchens\n   - KitchenQual: Kitchen quality\n   - TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n   - Functional: Home functionality rating\n   - Fireplaces: Number of fireplaces\n   - FireplaceQu: Fireplace quality\n   - GarageType: Garage location\n   - GarageYrBlt: Year garage was built\n   - GarageFinish: Interior finish of the garage\n   - GarageCars: Size of garage in car capacity\n   - GarageArea: Size of garage in square feet\n   - GarageQual: Garage quality\n   - GarageCond: Garage condition\n   - PavedDrive: Paved driveway\n   - WoodDeckSF: Wood deck area in square feet\n   - OpenPorchSF: Open porch area in square feet\n   - EnclosedPorch: Enclosed porch area in square feet\n   - 3SsnPorch: Three season porch area in square feet\n   - ScreenPorch: Screen porch area in square feet\n   - PoolArea: Pool area in square feet\n   - PoolQC: Pool quality\n   - Fence: Fence quality\n   - MiscFeature: Miscellaneous feature not covered in other categories\n   - MiscVal: $Value of miscellaneous feature\n   - MoSold: Month Sold\n   - YrSold: Year Sold\n   - SaleType: Type of sale\n   - SaleCondition: Condition of sale\n","10432535":"### 2.4. Target Analysis\nLet's see statistical table of Target variable:","c0426aa1":"## 1. Installing "}}