{"cell_type":{"2ec29170":"code","d4e2fe13":"code","7385c2a2":"code","47d32964":"code","470cbf71":"code","093672bc":"code","caf367db":"code","21a95f19":"code","e6f12670":"code","598a3ec9":"code","db91d9a7":"code","77f2f7f2":"code","a527fc67":"code","9b884ea8":"code","d30997c0":"markdown","47bbbe3b":"markdown","288e4536":"markdown","ac77a7bb":"markdown"},"source":{"2ec29170":"# Authors: dtad & jcre\n# Inspiration: https:\/\/machinelearningmastery.com\/time-series-forecasting-long-short-term-memory-network-python\/","d4e2fe13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom IPython.display import clear_output\nclear_output()\nimport time,csv\nimport matplotlib.pyplot as plt\nfrom dateutil.tz import tzlocal\nfrom datetime import datetime\nimport seaborn as sns\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","7385c2a2":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        clear_output()\n\n# could've done this better, but hey...\n# for kaggle\n# rootdir = '\/kaggle\/input\/g-research-crypto-forecasting\/'\n\n# for home\nrootdir = '\/kaggle\/input\/g-research-crypto-forecasting\/'\n\n# data pull\nexample_sample_submission = pd.read_csv(rootdir+'example_sample_submission.csv')\nasset_details = pd.read_csv(rootdir+'asset_details.csv')\nexample_test = pd.read_csv(rootdir+'example_test.csv')\ntrain = pd.read_csv(rootdir+'train.csv')\nsupplemental_train = pd.read_csv(rootdir+'supplemental_train.csv')\n\nprint('csv\/files loaded')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47d32964":"# timestamping \nsupplemental_train['timestamp'] = supplemental_train.timestamp.astype('datetime64[s]')\nexample_test['timestamp'] = example_test.timestamp.astype('datetime64[s]')\ntrain['timestamp'] = train.timestamp.astype('datetime64[s]')\nprint('timestamping complete')","470cbf71":"import numpy\nimport matplotlib.pyplot as plt\nimport math\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","093672bc":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n# fix random seed for reproducibility\nnumpy.random.seed(7)","caf367db":"# load the dataset\n# isolate to only one asset, for now...\n# this also assumes only one variable - & we do not expect to output a complete \"submission.csv\"\ndataframe = supplemental_train[supplemental_train.Asset_ID == 8][['Close']].reset_index(drop=True).head(100)\n\ndataset = dataframe.values\ndataset = dataset.astype('float32')","21a95f19":"# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)","e6f12670":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\n# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))","598a3ec9":"# create and fit the LSTM network\n# could change batch_size\nbatch_size = 1\n\nmodel = Sequential()\nmodel.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\nmodel.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nclear_output()","db91d9a7":"# trains _ samples 10x \nfor i in range(10):\n    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n    model.reset_states()\n\nclear_output()\n\n# make predictions\ntrainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)\n\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","77f2f7f2":"# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.6f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.6f RMSE' % (testScore))\n# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.figure(figsize=(18,7))\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","a527fc67":"#import gresearch_crypto\n#env = gresearch_crypto.make_env()\n\n# Training data is in the competition dataset as usual\n#train_df = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/train.csv', low_memory=False)\n\n# load model\n#model.fit(train_df)\n#model.fit(train_df)\n\n#iter_test = env.iter_test()\n\n#for (test_df, sample_prediction_df) in iter_test:\n#    sample_prediction_df['Target'] = model.predict(test_df)\n#    env.predict(sample_prediction_df)\n\n\n# submission\n#sample_prediction_df['Target'].to_csv('submission.csv', index=False)\nprint(\"project complete, for now.\")","9b884ea8":"# still need output: submission.csv\n\n# to be continued...","d30997c0":"# Submission","47bbbe3b":"# load kaggle data","288e4536":"[](http:\/\/)","ac77a7bb":"# begin tensorflow application"}}