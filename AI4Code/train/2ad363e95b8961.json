{"cell_type":{"b8de8e9d":"code","33995715":"code","983a117d":"code","d4e518cf":"code","d2b586d4":"code","a85db11d":"code","f7c5a421":"code","46043ee0":"code","f26c9a40":"code","c753fe40":"code","164b0f60":"code","b4ae970a":"code","21989c49":"code","80ebc497":"code","2897ee0e":"code","05d1c57b":"code","b6f7eaa7":"code","8ea3af16":"code","37b7a9f2":"code","e7797e14":"code","c80e383d":"code","fa70af25":"code","8684b2f3":"code","01a11d12":"code","b7e9d015":"code","5e09e249":"code","42c0f575":"code","84f53188":"code","512e48c1":"code","fd118b5d":"code","99d26971":"code","67359b74":"code","d9ea772b":"code","6a7301a5":"code","baab554c":"code","16c0be3f":"code","f8813f53":"code","a8ac8a30":"code","eb58a07b":"code","39110512":"code","64729914":"code","058f10e0":"code","d86a3da9":"code","23652d12":"code","99a3c57e":"code","dde3fc41":"code","4a4770c9":"code","e51e09d2":"code","5ed4cce2":"code","f7a69869":"code","c1d7bc89":"code","981db0ca":"code","e2fa8cef":"code","172716fc":"code","c9d77718":"code","f256f298":"code","1de92e23":"code","8e52bd2c":"code","371ad41f":"code","1cead2d9":"code","131e0b7f":"code","3d6139fd":"code","12e0b19b":"code","07438d8d":"code","66d00986":"code","c88d12f0":"code","ddc9026f":"code","549ba6cb":"code","b3f4bde1":"code","7f45ede7":"code","3d6d8b2c":"code","9390d021":"code","1bee3d3c":"code","edf9c16f":"code","86e85368":"code","64982a51":"markdown"},"source":{"b8de8e9d":"# 1.Importing the basic working libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","33995715":"# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n# Working with os module - os is a module in Python 3.\n# Its main purpose is to interact with the operating system. \n# It provides functionalities to manipulate files and folders.\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","983a117d":"print('# File sizes')\nfor f in os.listdir('..\/input'):\n    print(f.ljust(30) + str(round(os.path.getsize('..\/input\/' + f) \/ 1000000, 2)) + 'MB')","d4e518cf":"# 2.Importing the dataset using pandas library\nincome = pd.read_csv(\"..\/input\/income.csv\")\nincome_c = pd.read_csv(\"..\/input\/income.csv\")","d2b586d4":"income\n# 3.By looking into the dataset, there may not be required to remove any columns in this present scenario. After preprocessing we may check if any is needed.\n# education_num, fnlwgt may not be useful for model building\n# education_num is for indicating number of years of education\n# fnlwgt - The fnlwgt which is the final weight determined by the Census Organization\n# is of no use in any of the analysis that we are doing henceforth and is removed.\n#The educationnum if a repetitive variable which recodes the categorical variable education as a \n#numeric variable but will be used in the analysis for decision trees, hence is not being removed.\n","a85db11d":"# 4.Basic Analysis: identifying the datatypes, finding basicstats for each column.\nincome.info()\n# There are 9 object datatypes and 6 int datatypes","f7c5a421":"income.columns\n# names of all the varaibles.","46043ee0":"# finding no of missing values for each variable\nincome.isnull().sum()\n# there is no missing values in this dataset","f26c9a40":"import pandas_profiling as pp\npp.ProfileReport(income)","c753fe40":"income.corr()","164b0f60":"# Data Visualization - univariate and bivariate analysis \n# Univariate analysis\n#age\n#sns.distplot(income.Age,kde=False,rug=True)\nsns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\nsns.distplot(\n    income['Age'], norm_hist=False, kde=True, bins=20, hist_kws={\"alpha\": 1}\n).set(xlabel='Age', ylabel='Count');\n# kde = kernel density estimate\n# rugplot = At every observation, it will draw a small vertical stick.\n# from the below graph,concluded that age between late twenties and early thirties are present more ","b4ae970a":"income.dtypes","21989c49":"numerical_d = income.select_dtypes(include='int64')\nnumerical_d","80ebc497":"categorical_d = income.select_dtypes(include ='object')\ncategorical_d","2897ee0e":"numerical_d.hist(bins=15,figsize=(15,10),layout=(3,2))\n# From the below graphs on numerical data, Age is +vely skewed, kurtosis\n# More no of people aged late 30's are present\n# Most no of people worked for 35 - 40 hrs in the dataset\n# More people educated for 9-10 years\n# Capital_gain and Capital_loss have more no of zeros","05d1c57b":"fig, ax = plt.subplots(2, 5, figsize=(50, 30))\nfor variable, subplot in zip(categorical_d, ax.flatten()):\n    sns.countplot(income[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n# From the beolw graphs :\n# 1.More no of people working for private sector\n# 2.More no of people educated upto HS-grad\n# 3.More no of people are Married-civ-spouse\n# 4.More no of people are in occupations - prof-speciality, craft repair, Exec-managerial\n# 5.More no of people are white, male and belong to united states of america","b6f7eaa7":"fig, ax = plt.subplots(3, 2, figsize=(30, 30))\nfor variable, subplot in zip(numerical_d, ax.flatten()):\n    sns.boxplot(income[variable],income['Income'], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n# More no of people earning >50k are aged between 35-50 and working for 35-40 hours\n# More no of people earning <=50k are aged between 25-45 and working for 40-50 hours \n# finalwgt, capital_gain, capital_loss doesn't gave any information required","8ea3af16":"fig, ax = plt.subplots(3, 2, figsize=(30, 30))\nfor variable, subplot in zip(categorical_d, ax.flatten()):\n    sns.boxplot(income['Age' ],income[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n        ","37b7a9f2":"sns.relplot(x ='Hours_per_week', y='Occupation',hue='Income',style='Sex',data=income)\n# Transport-moving and Craft-repair occupations are ones in which some males are earning more than 50k working for less than 12 hours\n# In Priv-house-serv occupation more no of females are present and are earning less than 50k\n# In Protective-serv, tech-support,Machine-op-inspect,Farming-fishing,Craft-repair,Transport-moving,Adm-clerical\n# occupation more no of males are present and are earning more than 50k\n# In Exec-managerial occupation males working for 40-60 hours are earning more than 50k\n# In Prof-speciality some of the females earning more than 50k while working for lesser hours","e7797e14":"sns.relplot(x='Age',y='Race',hue='Income',style='Sex',data=income)\n# In race of Asian-Pac-Islander aged between 45-55 earning more than 50k","c80e383d":"sns.relplot(x='Age',y='Education',hue='Sex',style='Income',data=income)\n# More no of males who are educated upto Prof-school, Doctorate and bachelors are earning more than 50k","fa70af25":"sns.relplot(x='Age',y='Relationship',hue='Income',style='Sex',data=income)","8684b2f3":"sns.relplot(x='Age',y='Work_class',hue='Sex',style='Income',data=income)\n# Self-emp-inc and Self-emp-not-inc work_class contain more no of males earning more than 50k\n# Local-gov has males and females earning more than 50k","01a11d12":"income.dtypes","b7e9d015":"# Age, Education_num, Capital_gain, Capital_loss, Hours_per_week - numerical dtypes\n# data cleaning - no missing values, treating outliers\n# Age\n\ncount_age = income.Age.value_counts()\nprint('count: ',count_age)\nq_age1 = income.Age.quantile(0.25)\nq_age2 = income.Age.quantile(0.75)\niqr_age = q_age2 -q_age1\nlower_age = q_age1 - (1.5*iqr_age)\nupper_age = q_age2 + (1.5*iqr_age)\nincome.Age[income.Age < lower_age] = lower_age\nincome.Age[income.Age > upper_age] = upper_age","5e09e249":"income.Age.value_counts().sort_index()\n","42c0f575":"\nincome.Age.unique()#17,78,#16-30,25-4,40-60,60-80\nincome.Age[(income.Age > 16) & (income.Age <= 20)] = 0\nincome.Age[(income.Age > 20) & (income.Age <= 25)] = 1\nincome.Age[(income.Age > 25) & (income.Age <= 30)] = 2\nincome.Age[(income.Age > 30) & (income.Age <= 35)] = 3\nincome.Age[(income.Age > 35) & (income.Age <= 40)] = 4\nincome.Age[(income.Age > 40) & (income.Age <= 45)] = 5\nincome.Age[(income.Age > 45) & (income.Age <= 50)] = 6\nincome.Age[(income.Age > 50) & (income.Age <= 55)] = 7\nincome.Age[(income.Age > 55) & (income.Age <= 60)] = 8\nincome.Age[(income.Age > 60) & (income.Age <= 65)] = 9\nincome.Age[(income.Age > 65) & (income.Age <= 70)] = 10\nincome.Age[(income.Age > 70) & (income.Age <= 80)] = 11\n\n\n","84f53188":"\n# Education_num is total number of years they are educated, it is already mentioned in education\n#del income['Education_num']\n\n# Education_num\ncount_edu = income.Education_num.value_counts()\nprint('count: ',count_edu)\nq_edu1 = income.Education_num.quantile(0.25)\nq_edu2 = income.Education_num.quantile(0.75)\niqr_edu = q_edu2 -q_edu1\nlower_edu = q_edu1 - (1.5*iqr_edu)\nupper_edu = q_edu2 + (1.5*iqr_edu)\n#income.Education_num[income.Education_num < lower_edu] = lower_edu\n#income.Education_num[income.Education_num > upper_edu] = upper_edu\nprint('lower: ',lower_edu,'upper: ',upper_edu)\n\n\n#del income['Education']\n\n","512e48c1":"#income['Education_num'].value_counts()","fd118b5d":"\n# Capital_gain\ncount_capg = income.Capital_gain.value_counts()\nprint('count: ',count_capg)\nq_capg1 = income.Capital_gain.quantile(0.25)\nq_capg2 = income.Capital_gain.quantile(0.75)\niqr_capg = q_capg2 -q_capg1\nlower_capg = q_capg1 - (1.5*iqr_capg)\nupper_capg = q_capg2 + (1.5*iqr_capg)\nunique = income.Capital_gain.unique()\nprint('unique: ',unique)\nunique_med = pd.DataFrame(unique).median()\nunique_med\nincome.Capital_gain[income.Capital_gain < lower_capg] = unique_med[0]\n#income.Capital_gain[income.Capital_gain > upper_capg] = unique_med[0]\n","99d26971":"income.Capital_gain.value_counts()","67359b74":"\n# Capital_loss\ncount_capl = income.Capital_loss.value_counts()\nprint('count: ',count_capl)\nq_capl1 = income.Capital_loss.quantile(0.25)\nq_capl2 = income.Capital_loss.quantile(0.75)\niqr_capl = q_capl2 -q_capl1\nlower_capl = q_capl1 - (1.5*iqr_capl)\nupper_capl = q_capl2 + (1.5*iqr_capl)\nunique_l = income.Capital_loss.unique()\nprint('unique: ',unique_l)\nunique_medl = pd.DataFrame(unique_l).median()\nprint('unique_medl:',unique_medl)\nincome.Capital_loss[income.Capital_loss < lower_capl] = unique_medl[0]\n#income.Capital_loss[income.Capital_loss > upper_capl] = unique_medl[0]\n","d9ea772b":"income.Capital_loss.value_counts()","6a7301a5":"\n#Hours_per_week\ncount_hour = income.Hours_per_week.value_counts()\nprint('count: ',count_hour)\nq_hour1 = income.Hours_per_week.quantile(0.25)\nq_hour2 = income.Hours_per_week.quantile(0.75)\nprint('quantile1: ',q_hour1,'quantile2: ',q_hour2)\niqr_hour = q_hour2 -q_hour1\nprint('iqr:',iqr_hour)\nlower_hour = q_hour1 - (1.5*iqr_hour)\nupper_hour = q_hour2 + (1.5*iqr_hour)\nprint('lower: ',lower_hour ,'upper: ',upper_hour)\nincome.Hours_per_week[income.Hours_per_week < lower_hour] = lower_hour\nincome.Hours_per_week[income.Hours_per_week > upper_hour] = upper_hour\n","baab554c":"income.Hours_per_week.value_counts()","16c0be3f":"# dealing with numerical data is completed\n# next dealing with categorical_data\ncategorical_d.dtypes","f8813f53":"# dealing with categorical data\n# the columns Race and Sex are very less important when compared to other variables in dataset\n# removing both columns -> accuracy - 85.3% - test_size=0.35\n# removing Race column  -> accuracy - 85.88%\n# removing Sex column   -> accuracy - 85.08%\n# After these columns, Capital_loss and Education are less important, followed by Work_class\n#print(income_c.Race.value_counts())\n#print(income.Race.value_counts())\n#sns.boxplot(income.Race)\n#del income['Race']\n#del income['Sex']\n#del income['Education']\n#del income['Work_class']\n#income_c.Native_country.value_counts().sort_index()\n#income_c.Native_country[income_c.Native_country.value_counts() < 100] \n#del income['Fnlwgt']\n# removing fnlwgt is reducing the accuracy of model, may be useful for model building\n# no column is removed in this dataset\n","a8ac8a30":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nincome['Work_class']=le.fit_transform(income['Work_class'].astype('str'))\nincome['Education']=le.fit_transform(income['Education'].astype('str'))\nincome['Maritial_Status']=le.fit_transform(income['Maritial_Status'].astype('str'))\nincome['Occupation']=le.fit_transform(income['Occupation'].astype('str'))\nincome['Relationship']=le.fit_transform(income['Relationship'].astype('str'))\nincome['Race']=le.fit_transform(income['Race'].astype('str'))\nincome['Sex']=le.fit_transform(income['Sex'].astype('str'))\nincome['Native_country']=le.fit_transform(income['Native_country'].astype('str'))\nincome['Income']=le.fit_transform(income['Income'].astype('str'))","eb58a07b":"income['Education_num'] = income['Education_num'].astype(int)\nincome['Capital_loss'] = income['Capital_loss'].astype(int)\nincome['Hours_per_week'] = income['Hours_per_week'].astype(int)","39110512":"income.dtypes","64729914":"# After conversions, scaling should be appllied on input varaibles\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","058f10e0":"input = income.iloc[:,:14]\noutput = income.iloc[:,14:]\n","d86a3da9":"output","23652d12":"input","99a3c57e":"income_n = pd.DataFrame(scaler.fit_transform(input),columns = input.columns)","dde3fc41":"income_n","4a4770c9":"income_n['Income'] = output['Income']","e51e09d2":"income_n\n# 0 -   <=50k\n# 1 -   >50k\n","5ed4cce2":"x_labels = income_n.iloc[:,:14]\ny_labels = income_n.iloc[:,14:]\n","f7a69869":"y_labels","c1d7bc89":"x_labels","981db0ca":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_labels,y_labels,test_size = 0.25)\n#x_train","e2fa8cef":"#x_test\n#y_train\n#y_test","172716fc":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_labels,y_labels)\nlr_pred = lr.predict(x_labels)\nlr_pred","c9d77718":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix_lr = confusion_matrix(y_labels,lr_pred)\nprint(confusion_matrix_lr)\naccuracy_score_lr = accuracy_score(y_labels,lr_pred)\nprint(accuracy_score_lr)","f256f298":"# using splitting data\nlr.fit(x_train,y_train)\nlr_pred1 = lr.predict(x_test)\nlr_pred1","1de92e23":"confusion_matrix_lr1 = confusion_matrix(y_test,lr_pred1)\nprint('confusion_matrix of linear regression is:',confusion_matrix_lr1)\nprint('\\n')\naccuracy_score_lr1 = accuracy_score(y_test,lr_pred1)\nprint('accuracy score of linear regression is:',accuracy_score_lr1)","8e52bd2c":"# Applying supervised algorithms \n# Knn\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_k = KNeighborsClassifier(n_neighbors=3)\nclassifier_k.fit(x_train,y_train)\nknn_pred = classifier_k.predict(x_test)\nprint(knn_pred)\nconfusion_matrix_knn = confusion_matrix(y_test,knn_pred)\nprint('confusion_matrix for knn is:  ' ,confusion_matrix_lr1)\nprint('')\naccuracy_score_knn = accuracy_score(y_test,knn_pred)\nprint('accuracy_score for knn is: ',accuracy_score_knn)\n","371ad41f":"# naive-bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_n = GaussianNB()\nclassifier_n.fit(x_train,y_train)\nnb_pred = classifier_n.predict(x_test)\nprint(nb_pred)\nconfusion_matrix_nb = confusion_matrix(y_test,nb_pred)\nprint('confusion_matrix for naive bayes is:  ' ,confusion_matrix_nb)\nprint('')\naccuracy_score_nb = accuracy_score(y_test,nb_pred)\nprint('accuracy_score for naive bayes is: ',accuracy_score_nb)\n","1cead2d9":"# support vector machine\nfrom sklearn.svm import SVC\nclassifier_s = SVC(kernel = 'linear')\nclassifier_s.fit(x_train,y_train)\nsvm_pred = classifier_s.predict(x_test)\nprint(svm_pred)\nconfusion_matrix_svm = confusion_matrix(y_test,svm_pred)\nprint('confusion_matrix for support vector machine is:  ' ,confusion_matrix_svm)\nprint('')\naccuracy_score_svm = accuracy_score(y_test,svm_pred)\nprint('accuracy_score for support vector machine is: ',accuracy_score_svm)\n","131e0b7f":"# svm - rbf\nfrom sklearn.svm import SVC\nclassifier_sr = SVC(kernel = 'rbf')\nclassifier_sr.fit(x_train,y_train)\nsvmr_pred = classifier_sr.predict(x_test)\nprint(svmr_pred)\nconfusion_matrix_svmr = confusion_matrix(y_test,svmr_pred)\nprint('confusion_matrix for support vector machine is:  ' ,confusion_matrix_svmr)\nprint('')\naccuracy_score_svmr= accuracy_score(y_test,svmr_pred)\nprint('accuracy_score for support vector machine is: ',accuracy_score_svmr)\n","3d6139fd":"# svm - poly\nfrom sklearn.svm import SVC\nclassifier_sp = SVC(kernel = 'poly')\nclassifier_sp.fit(x_train,y_train)\nsvmp_pred = classifier_sp.predict(x_test)\nprint(svmp_pred)\nconfusion_matrix_svmp = confusion_matrix(y_test,svmp_pred)\nprint('confusion_matrix for support vector machine is:  ' ,confusion_matrix_svmp)\nprint('')\naccuracy_score_svmp= accuracy_score(y_test,svmp_pred)\nprint('accuracy_score for support vector machine is: ',accuracy_score_svmp)\n","12e0b19b":"# svm - sigmoid\n# svm - rbf\nfrom sklearn.svm import SVC\nclassifier_ss = SVC(kernel = 'sigmoid')\nclassifier_ss.fit(x_train,y_train)\nsvms_pred = classifier_ss.predict(x_test)\nprint(svms_pred)\nconfusion_matrix_svms = confusion_matrix(y_test,svms_pred)\nprint('confusion_matrix for support vector machine is:  ' ,confusion_matrix_svms)\nprint('')\naccuracy_score_svms= accuracy_score(y_test,svms_pred)\nprint('accuracy_score for support vector machine is: ',accuracy_score_svms)\n","07438d8d":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_d = DecisionTreeClassifier(criterion ='entropy',random_state = 0)\nclassifier_d.fit(x_train,y_train)\ndeci_pred = classifier_d.predict(x_test)\nprint(deci_pred)\nconfusion_matrix_deci = confusion_matrix(y_test,deci_pred)\nprint('confusion_matrix for Decision Tree is:  ' ,confusion_matrix_deci)\nprint('')\naccuracy_score_deci= accuracy_score(y_test,deci_pred)\nprint('accuracy_score for Decision Tree is: ',accuracy_score_deci)\n","66d00986":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_r = RandomForestClassifier(n_estimators=80,criterion ='entropy',random_state = 42)\nclassifier_r.fit(x_train,y_train)\nrand_pred = classifier_r.predict(x_test)\nprint(rand_pred)\nconfusion_matrix_rand = confusion_matrix(y_test,rand_pred)\nprint('confusion_matrix for Random Forest is:  ' ,confusion_matrix_rand)\nprint('')\naccuracy_score_rand= accuracy_score(y_test,rand_pred)\nprint('accuracy_score for Random Forest is: ',accuracy_score_rand)\n","c88d12f0":"\n# Application of PCA\nfrom sklearn.decomposition import PCA\npca1 = PCA(n_components =10 )\npca = pca1.fit_transform(x_labels)\npca\nincome_pca = pd.DataFrame(pca)\nincome_pca\n\n","ddc9026f":"\n# The amount of variance that each PCA explains is \nvar = pca1.explained_variance_ratio_\nvar\nvar1 = np.cumsum(np.round(var,decimals = 4)*100)\nvar1\n\n","549ba6cb":"\nincome_pca['Income'] = income_n['Income']\nincome_pca\npca_x = income_pca.iloc[:,:10]\npca_y = income_pca.iloc[:,10:]\npca_y\npca_xtrain,pca_xtest,pca_ytrain,pca_ytest = train_test_split(pca_x,pca_y,test_size=0.25)\n#pca_ytrain\n#pca_ytest\n#pca_xtrain\n#pca_xtest\n","b3f4bde1":"\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rp = RandomForestClassifier(n_estimators=100,criterion ='entropy',random_state = 0)\nclassifier_rp.fit(pca_xtrain,pca_ytrain)\nrandp_pred = classifier_rp.predict(pca_xtest)\nprint(randp_pred)\nconfusion_matrix_randp = confusion_matrix(pca_ytest,randp_pred)\nprint('confusion_matrix for Random Forest is:  ' ,confusion_matrix_randp)\nprint('')\naccuracy_score_randp= accuracy_score(pca_ytest,randp_pred)\nprint('accuracy_score for Random Forest is: ',accuracy_score_randp)\n","7f45ede7":"# May be applying pca is a bad idea, using pca is not useful\nimp_var = pd.DataFrame(classifier_r.feature_importances_,[input.columns])","3d6d8b2c":"imp_var","9390d021":"income_n","1bee3d3c":"income_c","edf9c16f":"print('accuracy of random forest: ',accuracy_score_rand)\nprint('accuracy of decision tree: ',accuracy_score_deci)\nprint('accuracy of linear regression: ',accuracy_score_lr)\nprint('accuracy of knn: ',accuracy_score_knn)\nprint('accuracy of naive bayes: ',accuracy_score_nb)\nprint('accuracy of svm: ',accuracy_score_svm)\nprint('accuracy of svm-rbf: ',accuracy_score_svmr)\nprint('accuracy of svm-poly : ',accuracy_score_svmp)\nprint('accuracy of svm-sigmoid: ',accuracy_score_svms)\n","86e85368":"# Best model -> random forest\n# May be increasing the test_size will increase accuracy\n# May be by removing columns like Race, Sex, Education, Capital_loss, Capital_gain, accuracy may be increased, \n","64982a51":" Pandas-profiling report for the dataset:"}}