{"cell_type":{"c287e65b":"code","fd3c5699":"code","bbace720":"code","4d9ed89a":"code","915f6b53":"code","4f79f90f":"code","c4008682":"code","ac84a08e":"code","b21b19aa":"code","f6544bc1":"code","8fd24b07":"code","ff6be905":"code","a4552ecf":"code","2221771e":"code","9e8fca6a":"code","c5ab8699":"code","096c31b3":"code","21f40315":"code","f3682fd1":"code","6eb0776f":"code","d6c97af2":"code","e984acfa":"code","02fde875":"code","22c48bec":"code","bd89e5b6":"code","5d71c19f":"code","27bbaee7":"code","2c8239f3":"code","aab17e71":"code","99bb5d02":"code","228043b3":"code","69396f25":"code","87efc09e":"code","03432b6f":"code","41110204":"code","793a4bdc":"code","85a49db6":"code","1bb6aaed":"code","98ccae27":"code","e51d4e22":"code","14833b8e":"code","6924aa9a":"code","b096368f":"code","0d216cde":"code","6e7e5b79":"code","9448d12d":"code","9352672a":"code","f0bc914e":"code","a25a6b86":"code","00f06fda":"code","dabdbc12":"code","b9decaf5":"code","fdd1cdba":"code","9599053c":"code","19cc2b1d":"code","1d09e74d":"code","dcc1128a":"code","4eaa658b":"code","0eb0c5b0":"code","7d0e574b":"code","519575f1":"code","23d9e80f":"code","d47ccb68":"code","27f732f6":"code","8b1bb94d":"code","66d97522":"code","ebf3059d":"code","97515f04":"code","f139eecc":"code","d636f312":"code","1ffed076":"code","4af6f914":"code","281813e1":"code","a4ae6f08":"code","bd85e6db":"code","6ffcbddd":"code","93c195be":"code","36893cde":"code","183dea00":"code","e8c9aab3":"code","c082626e":"code","992bfe91":"code","e570f381":"code","1a6c3825":"code","9df7415b":"code","953c5551":"code","b29f4137":"code","a24c1a93":"code","876b6843":"code","c8acabfd":"code","27d5a120":"markdown","2907f79e":"markdown","dcb9cf99":"markdown","6ae9ed90":"markdown","6a68a897":"markdown","6d8d0fef":"markdown","40134697":"markdown","a8bea477":"markdown","fcf65226":"markdown","65746149":"markdown","f8f5ecdb":"markdown","937295c8":"markdown","35238eb4":"markdown","dc9cdfd2":"markdown","ec2e8fea":"markdown","82782eda":"markdown","65a37197":"markdown","c35e85d8":"markdown","18a3c7b3":"markdown","55d710b7":"markdown","7f96ab71":"markdown","7368f3de":"markdown","1c9b3048":"markdown","753da517":"markdown","68945430":"markdown","2c8e400e":"markdown","5ed0c916":"markdown","7f17e160":"markdown","c883b1a8":"markdown","31b67199":"markdown","b029506a":"markdown","96c50b56":"markdown","c7bddbf1":"markdown","95a6efda":"markdown","c935fb29":"markdown","89fbccac":"markdown","56c84264":"markdown","4f1b9809":"markdown","261393f7":"markdown","4a323a56":"markdown","27ced48e":"markdown","51e09dbe":"markdown","fb11919f":"markdown","65f9b268":"markdown","8eda4951":"markdown","fdff814d":"markdown","f0fb251f":"markdown","0192bd7d":"markdown","7ff02e9e":"markdown","e2b3dc90":"markdown","c0c4dceb":"markdown","18b4b471":"markdown","e66138b8":"markdown","acce03d8":"markdown","0a6c46e2":"markdown","32e7bf81":"markdown","cfeee551":"markdown","2f99091f":"markdown","db2b3c0c":"markdown","708a9cac":"markdown","64256702":"markdown","ca6103ea":"markdown","3f1317a0":"markdown","0f877dca":"markdown","571211f7":"markdown","85502961":"markdown","4788ef4c":"markdown"},"source":{"c287e65b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import pyplot\nsns.set(style='white', context='notebook', palette='deep')\n\n# Import scikit_learn module for the algorithm\/model: Linear Regression\nfrom sklearn.linear_model import LogisticRegression\n# Import scikit_learn module to split the dataset into train.test sub-datasets\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# import the metrics class\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score,precision_score, recall_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n","fd3c5699":"df = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\ndf.head()","bbace720":"print(df.shape)","4d9ed89a":"print(df.columns)","915f6b53":"print(df.info())","4f79f90f":"df.describe()","c4008682":"df.describe(include=['object'])","ac84a08e":"df.sex.value_counts()","b21b19aa":"sns.countplot(x=\"sex\", data=df)","f6544bc1":"df.race.value_counts()\n","8fd24b07":"sns.countplot(x=\"race\", data=df)","ff6be905":"df['income']=df['income'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\ndf.head(4)\n","a4552ecf":"df.income.value_counts()","2221771e":"sns.countplot(x=\"income\", data=df)","9e8fca6a":"# Identify Numeric features\nnumeric_features = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week','income']\n\n# Identify Categorical features\ncat_features = ['workclass','education','marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']","c5ab8699":"\ndf.age.plot.hist(grid=True)","096c31b3":"df.fnlwgt.plot.hist(grid=True)","21f40315":"df[numeric_features].hist()","f3682fd1":"cor_mat = sns.heatmap(df[numeric_features].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","6eb0776f":"df['education.num'].value_counts()","d6c97af2":"sns.countplot(x=\"education.num\", data=df)","e984acfa":" sns.countplot(x=\"sex\", hue=\"income\", data=df)","02fde875":"ed_inc= sns.factorplot(x=\"education.num\",y=\"income\",data=df,kind=\"bar\",size = 6,palette = \"muted\")\ned_inc.despine(left=True)\ned_inc = ed_inc.set_ylabels(\">50K probability\")","22c48bec":"hour_inc  = sns.factorplot(x=\"hours.per.week\",y=\"income\",data=df,kind=\"bar\",size = 6,palette = \"muted\")\nhour_inc.despine(left=True)\nhour_inc = hour_inc.set_ylabels(\">50K probability\")","bd89e5b6":"rel_inc= sns.factorplot(x=\"relationship\",y=\"income\",data=df,kind=\"bar\", size = 6 ,\npalette = \"muted\")\nrel_inc.despine(left=True)\nrel_inc = rel_inc.set_ylabels(\"Income >50K Probability\")\n\n","5d71c19f":"df.isna().sum()","27bbaee7":"# Fill Missing Category Entries\ndf[\"workclass\"] = df[\"workclass\"].fillna(\"X\")\ndf[\"occupation\"] = df[\"occupation\"].fillna(\"X\")\ndf[\"native.country\"] = df[\"native.country\"].fillna(\"United-States\")\n\n# Confirm All Missing Data is Handled\ndf.isnull().sum()","2c8239f3":"#Finding the special characters in the data frame \ndf.isin(['?']).sum(axis=0)","aab17e71":"# code will replace the special character to nan and then drop the columns \ndf['native.country'] = df['native.country'].replace('?',np.nan)\ndf['workclass'] = df['workclass'].replace('?',np.nan)\ndf['occupation'] = df['occupation'].replace('?',np.nan)\n","99bb5d02":"df.isna().sum()","228043b3":"print(df.shape)","69396f25":"df_new = df.copy()","87efc09e":"#dropping the NaN rows now \ndf_new.dropna(how='any',inplace=True)","03432b6f":"print(df_new.shape)","41110204":"for c in df_new[cat_features]:\n    print (\"---- %s ---\" % c)\n    print (df[c].value_counts())","793a4bdc":"for i in df_new:\n    print (\"---- %s ---\" % i)\n    print (df[i].nunique())","85a49db6":"#dropping based on uniquness of data from the dataset \ndf_new.drop(['age', 'fnlwgt', 'capital.gain','capital.loss', 'native.country','education.num'], axis=1, inplace=True)","1bb6aaed":"list(df_new.columns)","98ccae27":"df_new.shape","e51d4e22":"df_new.dtypes","14833b8e":"#gender\ndf_new['sex'] = df_new['sex'].map({'Male': 0, 'Female': 1}).astype(int)\ndf_new['race'] = df_new['race'].map({'Black': 0, 'Asian-Pac-Islander': 1,'Other': 2, 'White': 3, 'Amer-Indian-Eskimo': 4}).astype(int)\ndf_new['marital.status'] = df_new['marital.status'].map({'Married-spouse-absent': 0, 'Widowed': 1, 'Married-civ-spouse': 2, 'Separated': 3, 'Divorced': 4,'Never-married': 5, 'Married-AF-spouse': 6}).astype(int)\ndf_new['workclass']= df_new['workclass'].map({'Self-emp-inc': 0, 'State-gov': 1,'Federal-gov': 2, 'Without-pay': 3, 'Local-gov': 4,'Private': 5, 'Self-emp-not-inc': 6}).astype(int)\ndf_new['relationship'] = df_new['relationship'].map({'Not-in-family': 0, 'Wife': 1, 'Other-relative': 2, 'Unmarried': 3,'Husband': 4,'Own-child': 5}).astype(int)\ndf_new['education']= df_new['education'].map({'Some-college': 0, 'Preschool': 1, '5th-6th': 2, 'HS-grad': 3, 'Masters': 4, '12th': 5, '7th-8th': 6, 'Prof-school': 7,'1st-4th': 8, 'Assoc-acdm': 9, 'Doctorate': 10, '11th': 11,'Bachelors': 12, '10th': 13,'Assoc-voc': 14,'9th': 15}).astype(int)\ndf_new['occupation'] = df_new['occupation'].map({ 'Farming-fishing': 0, 'Tech-support': 1, 'Adm-clerical': 2, 'Handlers-cleaners': 3, \n 'Prof-specialty': 4,'Machine-op-inspct': 5, 'Exec-managerial': 6,'Priv-house-serv': 7,'Craft-repair': 8,'Sales': 9, 'Transport-moving': 10, 'Armed-Forces': 11, 'Other-service': 12,'Protective-serv':13}).astype(int)\n\n","6924aa9a":"df_new.race.value_counts()","b096368f":"df_new.sex.value_counts()","0d216cde":"df_new['marital.status'].value_counts()","6e7e5b79":"df_new['workclass'].value_counts()","9448d12d":"df_new['education'].value_counts()","9352672a":"df_new['occupation'].value_counts()","f0bc914e":"df_new['relationship'].value_counts()","a25a6b86":"df_new.dtypes","00f06fda":"df_new.head(10)","dabdbc12":"#plotting a bar graph for Education against Income to see the co-relation between these columns \ndf_new.groupby('education').income.mean().plot(kind='bar')\n","b9decaf5":"df_new.groupby('occupation').income.mean().plot(kind='bar')","fdd1cdba":"df_new.groupby('sex').income.mean().plot(kind='bar')","9599053c":"df_new.groupby('relationship').income.mean().plot(kind='bar')","19cc2b1d":"df_new.groupby('race').income.mean().plot(kind='bar')","1d09e74d":"df_new.groupby('workclass').income.mean().plot(kind='bar')","dcc1128a":"df_new.columns","4eaa658b":"X = df_new.drop('income',axis=1)\ny = df_new.income","0eb0c5b0":"print(\"X shape : \", X.shape)\nprint(\"y shape : \", y.shape)","7d0e574b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,shuffle=True)","519575f1":"print(\"X_train shape : \", X_train.shape)\nprint(\"X_test shape : \", X_test.shape)\nprint(\"===============================\")\nprint(\"y_train shape : \", y_train.shape)\nprint(\"y_test shape : \", y_test.shape)","23d9e80f":"y_train.value_counts()","d47ccb68":"y_test.value_counts()","27f732f6":"log_reg = LogisticRegression()\n#Train our model with the training data\nlog_reg.fit(X_train, y_train)","8b1bb94d":"#print our price predictions on our test data\npred_log = log_reg.predict(X_test)","66d97522":"accuracy_log_reg = metrics.accuracy_score(y_test, pred_log)","ebf3059d":"print(f\"The accuracy of the model is {round(metrics.accuracy_score(y_test,pred_log),3)*100} %\")","97515f04":"auc_log = roc_auc_score(y_test, log_reg.predict_proba(X_test)[:,1])\nprint(f\"The AUC Score  is {round(auc_log,3)*100} %\")\n","f139eecc":"model = GaussianNB()\n\n# Train the model using the training sets\ngnb = model.fit(X_train,y_train)\npredictions = gnb.predict(X_test)","d636f312":"accuracy_log_naive_bayes = metrics.accuracy_score(y_test, predictions)","1ffed076":"#printing the accuracy values \nprint(f\"The accuracy of the model is {round(metrics.accuracy_score(y_test,predictions),3)*100} %\")\n\n","4af6f914":"auc_nb = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\nprint(f\"The AUC Score  is {round(auc_nb,3)*100} %\")\n\n","281813e1":"clf = DecisionTreeClassifier(criterion='entropy',min_samples_split=8,max_depth=10)","a4ae6f08":"# Train Decision Tree Classifer\nclf.fit(X_train,y_train)\nprediction_clf = clf.predict(X_test)","bd85e6db":"accuracy_dec_tree = metrics.accuracy_score(y_test, prediction_clf)","6ffcbddd":"print(f\"The accuracy of the model is {round(metrics.accuracy_score(y_test,prediction_clf),3)*100} %\")\n","93c195be":"auc_clf = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\nprint(f\"The AUC Score  is {round(auc_clf,3)*100} %\")\n\n","36893cde":"rf=RandomForestClassifier(min_samples_split=30)\n# Train the model using the training sets\nrf.fit(X_train,y_train)\npredictions_rf =rf.predict(X_test)","183dea00":"accuracy_rf = metrics.accuracy_score(y_test, predictions_rf)","e8c9aab3":"print(f\"The accuracy of the model is {round(metrics.accuracy_score(y_test,predictions_rf),3)*100} %\")\n","c082626e":"auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])\nprint(f\"The AUC Score  is {round(auc_rf,3)*100} %\")","992bfe91":"model = ['LR',\"NB\",\"DT\",\"RF\"]","e570f381":"result = {'Model':['Logistic Regression',\"Naive Bayes\",\"Decision Tree\",\"Random Forest\"],\n          'Accuracy':[accuracy_log_reg,accuracy_log_naive_bayes,accuracy_dec_tree,accuracy_rf],\n         'AUC':[accuracy_log_reg,auc_nb,auc_clf,auc_rf]}","1a6c3825":"result_df = pd.DataFrame(data=result,index=model)\nresult_df","9df7415b":"train_probs = rf.predict_proba(X_train)[:,1] \nprobs = rf.predict_proba(X_test)[:, 1]\ntrain_predictions = rf.predict(X_train)","953c5551":"def evaluate_model(y_pred, probs,train_predictions, train_probs):\n   \n     # Calculate false positive rates and true positive rates\n    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n    model_fpr, model_tpr, _ = roc_curve(y_test, probs)\n    plt.figure(figsize = (8, 6))\n    plt.rcParams['font.size'] = 16\n    # Plot both curves\n    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n    plt.legend()\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves')\n    plt.show()\n","b29f4137":"evaluate_model(predictions_rf,probs,train_predictions,train_probs)","a24c1a93":"import itertools\ndef plot_confusion_matrix(cm, classes, normalize = False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Greens): # can change color \n    plt.figure(figsize = (5, 5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, size = 24)\n    plt.colorbar(aspect=4)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n    plt.yticks(tick_marks, classes, size = 14)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    # Label the plot\n    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n             plt.text(j, i, format(cm[i, j], fmt), \n             fontsize = 20,\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.ylabel('True label', size = 18)\n    plt.xlabel('Predicted label', size = 18)\n\n# Let's plot it out\ncm = confusion_matrix(y_test, predictions_rf)\nplot_confusion_matrix(cm, classes = ['0 - <50k', '1 - >50k'],\n                      title = 'Confusion Matrix')","876b6843":"feature_importances = list(zip(X_train, rf.feature_importances_))\n# Then sort the feature importances by most important first\nfeature_importances_ranked = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances\n[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances_ranked];","c8acabfd":"feature_names_8 = [i[0] for i in feature_importances_ranked[:8]]\ny_ticks = np.arange(0, len(feature_names_8))\nx_axis = [i[1] for i in feature_importances_ranked[:8]]\nplt.figure(figsize = (10, 14))\nplt.barh(feature_names_8, x_axis)   #horizontal barplot\nplt.title('Random Forest Feature Importance (Top 25)',\n          fontdict= {'fontname':'Comic Sans MS','fontsize' : 20})\nplt.xlabel('Features',fontdict= {'fontsize' : 16})\nplt.show()","27d5a120":"Adults with an educational background of Prof-school (7) and Doctorate (10) will have a better income and it is likely possible that their income is higher than 50K.","2907f79e":"Let's check the race's repartition","dcb9cf99":"*Let's have a look at our target*","6ae9ed90":"Machine Learning model requires input data in numerical notations to extract patterns from it and make predictions. But, not all the data provided in our source dataset is numerical. Some of the data provided are Categorical data like WorkClass, Education, Marital-Status, Occupation, Relationship, etc. we need to convert these into numerical notations.\nHere data is nothing but a feature that our model uses as an input. So, we perform Feature Engineering on our data to create meaningful numerical data out of the source dataset.\n","6a68a897":"### Decision Tree","6d8d0fef":"From the output, we can see that the table contains 32561 rows and 15 columns.\n\n","40134697":"The gender bar chart provides us some useful insight into the data that Men (0) are more likely to have a higher income.","a8bea477":"#### Analyzing numerical features","fcf65226":"#### END","65746149":"#### Checking missing values","f8f5ecdb":"Let\u2019s have a look at data dimensionality, feature names, and feature types","937295c8":"The dataset named Adult Census Income is available in kaggle and UCI repository. This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics).","35238eb4":"Let's resume all the result in a dataframe","dc9cdfd2":"#### Correlation","ec2e8fea":"## Exploratory data analysis with Pandas","82782eda":"Now that we have just continuous variables we can analyze the correlation between them","65a37197":"### Naive Bayes","c35e85d8":"# If you find this notebook useful then please upvote","18a3c7b3":"#### Exploring Relationship vs Income","55d710b7":"Logistic Regression is one of the easiest and most commonly used supervised Machine learning algorithms for categorical classification. The basic fundamental concepts of Logistic Regression are easy to understand and can be used as a baseline algorithm for any binary (0 or 1) classification problem.","7f96ab71":"Let's see statistics on non-numerical features.","7368f3de":"Printing the new columns","1c9b3048":"We will make a copy of the dataset in order to deal with this issue","753da517":"**value_counts for categorical features**","68945430":"We have handled all categorical variables","2c8e400e":"### The new dataframe look like this","5ed0c916":"relationship chart shows us that wife (1) and husband (4) has a higher income. A married couple would most likely earn >50K.","7f17e160":"Taking a glance at the data provided, we can see that there are some special characters in the data like \u2018?\u2019. So, let\u2019s get the count of special characters present in the data.","c883b1a8":"We have 24720 people with incomes below 50k and 7841 with incomes above 50k","31b67199":"#### Exploring Education Num vs Income","b029506a":"Age ","96c50b56":"**The prediction task is to determine whether a person makes over $50K a year or not.**","c7bddbf1":"#### Exploring Hours Per Week vs Income\n","95a6efda":"Let's dive into the random forest's metrics","c935fb29":"Now let's try printing out column names using columns:","89fbccac":"Our data suggest that people with occupation Prof-specialty (5) and Exec-managerial (7) will have a better chance of earning an income of more than 50K.","56c84264":"#### Reformating the target columns","4f1b9809":"Self-emp-in (0), Federal-gov(2) workclass groups have a higher chance of earning more than 50K.","261393f7":"We have a lot of different ways to handle this kind of issue. Let's use the map function, we can convert all the other categorical data in the dataset to numerical data.\n","4a323a56":"### Handling categorical features","27ced48e":"As we said previously, we are more likely to have a salary above 50k if we are married and wife, and if we have a high level of education.","51e09dbe":"## Feature engineering","fb11919f":"Let's have a look at the income according to the gender","65f9b268":"A decision tree is a branched flowchart showing multiple pathways for potential decisions and outcomes. The tree starts with what is called a decision node, which signifies that a decision must be made. From the decision node, a branch is created for each of the alternative choices under consideration.","8eda4951":"## Train test split","fdff814d":"Our best model is random forest !","f0fb251f":"### AUC Random Forest\n","0192bd7d":"### Feature importance of our best model","7ff02e9e":"**Our best model is the random forest.**\n\n\n","e2b3dc90":"The dataset provides 14 input variables that are a mixture of categorical, ordinal, and numerical data types. The complete list of variables is as follows:\n\n- age: continuous.\n- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n- fnlwgt: continuous.\n- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\neducation-num: continuous.\n- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n- sex: Female, Male.\n- capital-gain: continuous.\n- capital-loss: continuous.\n- hours-per-week: continuous.\n- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n- salary: >50K,<=50K[](http:\/\/)","c0c4dceb":"Random Forests are a combination of tree predictors where each tree depends on the values of a random vector sampled independently with the same distribution for all trees in the forest. The basic principle is that a group of \u201cweak learners\u201d can come together to form a \u201cstrong learner\u201d.","18b4b471":"A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. Basically, it\u2019s \u201cnaive\u201d because it makes assumptions that may or may not turn out to be correct.\n","e66138b8":"AUC stands for Area under the curve. AUC gives the rate of successful classification by the logistic model. The AUC makes it easy to compare the ROC curve of one model to another.\n","acce03d8":"Let's check the repartition between male and female","0a6c46e2":"### Import dataset","32e7bf81":"**I hope you find this kernel useful**\n# Your UPVOTES would be highly appreciated","cfeee551":"The describe method shows basic statistical characteristics of each numerical feature (int64 and float64 types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles.","2f99091f":"### Import libraries","db2b3c0c":"Here we ran a for loop over all the columns using the .value_counts() function of Pandas which gets us the count of unique values. We can see that some of the data provided are unique like the \u2018workclass\u2019 attribute which has only 7 distinct values and some columns have a lot of distinct values.","708a9cac":"We can use the info() method to output some general information about the dataframe:","64256702":"### Logistic Regression","ca6103ea":"## Conclusion","3f1317a0":"As per the data, an Asian-Pac-Islander (1) or a white (3) have more chances of earning more than 50K.\n","0f877dca":"# Adult Census Income dataset","571211f7":"### Random Forest\n","85502961":"*Printing the number of uniques values in each categories*","4788ef4c":"## Model Selection"}}