{"cell_type":{"de12ab0d":"code","9e0f4e8e":"code","fa0834c1":"code","3dffd509":"code","36291d54":"code","e34e94ee":"code","05c3847f":"code","2dc99839":"code","a011a1fa":"code","6a77017f":"code","f12e3ea2":"code","73596963":"code","e4421474":"markdown","df9b9a90":"markdown","54683a6e":"markdown","6dbfbed7":"markdown","52ab8e2c":"markdown","329c10f4":"markdown","2d51719e":"markdown","34ead22a":"markdown","53ab886a":"markdown","5dbd0259":"markdown","65a2d69e":"markdown","8b337d7d":"markdown","632de202":"markdown"},"source":{"de12ab0d":"import os\nimport numpy as np\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import layers","9e0f4e8e":"input_dir = \"..\/input\/oxford-pets\/images\/images\"\ntarget_dir = \"..\/input\/oxford-pets\/annotations\/annotations\/trimaps\"\nimg_size = (160, 160)\nnum_classes = 3\nbatch_size = 16\n\ninput_img_paths = sorted(\n    [\n        os.path.join(input_dir, fname)\n        for fname in os.listdir(input_dir)\n        if fname.endswith(\".jpg\")\n    ]\n)\ntarget_img_paths = sorted(\n    [\n        os.path.join(target_dir, fname)\n        for fname in os.listdir(target_dir)\n        if fname.endswith(\".png\") and not fname.startswith(\".\")\n    ]\n)\n\nprint(\"Number of samples:\", len(input_img_paths))","fa0834c1":"# Display image #7\ni = 7\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","3dffd509":"class PetsDataset(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n            y[j] -= 1\n        return x, y","36291d54":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","e34e94ee":"val_samples = 1108 # 85% Training -- 15% Validation\nrandom.Random(1822).shuffle(input_img_paths)\nrandom.Random(1822).shuffle(target_img_paths)\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = PetsDataset(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)","05c3847f":"# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\nmodel.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"pets_segmentation.h5\", save_best_only=True)\n]\n\nepochs = 30\nmodelunet=model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","2dc99839":"# summarize history for accuracy\nplt.plot(modelunet.history['accuracy'])\nplt.plot(modelunet.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()\n# summarize history for loss\nplt.plot(modelunet.history['loss'])\nplt.plot(modelunet.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()","a011a1fa":"# Generate predictions for all images in the validation set\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)\nval_preds = model.predict(val_gen)","6a77017f":"def display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    return img","f12e3ea2":"# Display image #120\ni = 120\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","73596963":"# Display image #180\ni = 180\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","e4421474":"## 3-loading Image Dataset","df9b9a90":"## 4-Display sample of Image Dataset","54683a6e":"![u-net-architecture.png](attachment:u-net-architecture.png)\n> architecture (example for 32x32 pixels in the lowest resolution).","6dbfbed7":"## 8-Training","52ab8e2c":"## 2-Import Libraries","329c10f4":"## 9-Inference","2d51719e":"## 5-Pets Dataset","34ead22a":"# Image Segmentation Using  U-Net","53ab886a":"## 7-Split Dataset into a training and a validation set","5dbd0259":"## 6-Build the U-Net Model Architecture","65a2d69e":"## 1-U-Net\nU-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.","8b337d7d":"## 10-Display results for validation image","632de202":"## 11- References\n\n1. https:\/\/en.wikipedia.org\/wiki\/U-Net\n\n1. https:\/\/keras.io\/examples\/vision\/oxford_pets_image_segmentation\/\n\n1. https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/"}}