{"cell_type":{"111354e4":"code","df8ee85f":"code","ae565cf7":"code","67803b0a":"code","23913d65":"code","ff80670f":"code","d25442e9":"code","85ff4ad5":"code","53005f52":"code","816cdad6":"code","18de5b10":"code","0207d114":"code","079155df":"code","6f5e7a64":"code","0ee26d97":"code","e875b175":"code","e5fb3091":"code","4b73686b":"code","3beae2b1":"code","3a82a52d":"code","0adbc3c9":"code","316c11b6":"code","9e856652":"code","cada5d77":"code","8e59eca5":"code","0b8897ec":"code","38932830":"code","5a70da63":"markdown","522fb7f6":"markdown","a0a1605b":"markdown","06fc335d":"markdown","bad63b14":"markdown","944921da":"markdown","0f0d2cb5":"markdown","d6bdc190":"markdown","3b8aa169":"markdown","a8cb0ba3":"markdown","e24bea88":"markdown","36b0087f":"markdown","88d96b70":"markdown","40a4de9a":"markdown","fdae56cc":"markdown","02466665":"markdown","bb82da2c":"markdown","1f9af32b":"markdown","ab50ce56":"markdown","7191980d":"markdown","2e023c9e":"markdown"},"source":{"111354e4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df8ee85f":"# import both the dataset\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# shapes of both the dataset\nprint(f'train shape : {train_data.shape} , test data : {test_data.shape}')","ae565cf7":"# Run some test on our Hero\ntrain_data.SalePrice.describe()","67803b0a":"# let's visualize and see what the sale price distribution looks like\nplt.figure(figsize=(7,5))\nsns.distplot(train_data['SalePrice'],bins = 25)\nplt.ticklabel_format(style='sci', axis='x', scilimits=(0,1))\nplt.xlabel(\"House Sales Price (10^5) in USD\")\nplt.ylabel(\"Number of Houses\")\nplt.title(\"House Sales Price Distribution\")\n\n# the skewness and the kurtosis\nprint(f'Skewness : {train_data.SalePrice.skew()}')\nprint(f'Kurtosis : {train_data.SalePrice.kurt()}')","23913d65":"# pairplot\nsns.set()\nnumerical_cols = ['LotArea','TotalBsmtSF','GrLivArea','TotRmsAbvGrd','PoolArea','MiscVal','SalePrice']\nsns.pairplot(train_data[numerical_cols], height= 2.5)\nplt.show()","ff80670f":"#boxplot\ncategorical_col = ['YearBuilt','OverallQual','MSZoning','Neighborhood','BsmtQual']\n\nfor col in categorical_col:\n    f, ax = plt.subplots(figsize=(12, 5))\n    sns.boxenplot(x=col, y='SalePrice', data = train_data)\n    plt.show()","d25442e9":"# correlation heatmap\ncorr = train_data.corr()\nplt.figure(figsize = (40, 30))\nsns.heatmap(corr, annot = True)\nplt.show()","85ff4ad5":"# scatter plot for outliar's detection\nvar = train_data['TotalBsmtSF']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"Before Removing Outliar's\")\nplt.show()\n\n# remove outliar from TotalBsmtSF\nIndx = train_data[((train_data.TotalBsmtSF>5000))].index\ntrain_data.drop(Indx,inplace=True)\n\n# see weather outliar gone\nvar = train_data['TotalBsmtSF']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"After Removing Outliar's\")\nplt.show()","53005f52":"# scatter plot for outliar's detection\nvar = train_data['GrLivArea']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"Before Removing Outliar's\")\nplt.show()\n\n# remove outliar from GrLivArea\nIndex = train_data[((train_data.GrLivArea>4000))].index\nfor ind in Index:\n    train_data.drop(ind, inplace=True)\n\n# scatter plot for outliar's detection\nvar = train_data['GrLivArea']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"After Removing Outliar's\")\nplt.show()","816cdad6":"# scatter plot for outliar's detection\nvar = train_data['GarageCars']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.show()","18de5b10":"# scatter plot for outliar's detection\nvar = train_data['OverallQual']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.show()","0207d114":"# scatter plot for outliar's detection\nvar = train_data['YearBuilt']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"Before Removing Outliar's\")\nplt.show()\n\n# remove outliar from YearBuilt\nIndx = train_data[((train_data.SalePrice>400000)&(train_data.SalePrice<500000)&(train_data.YearBuilt<1900))].index\ntrain_data.drop(Indx,inplace=True)\n\n# scatter plot for outliar's detection\nvar = train_data['YearBuilt']\nsns.set()\nsns.scatterplot(train_data['SalePrice'], var)\nplt.title(\"After Removing Outliar's\")\nplt.show()","079155df":"# split dependent and independent feature from dataset\nx = train_data.iloc[:,0:80]\ny = train_data.iloc[:,-1]\n\n# combine train and test data\ndf = pd.concat([x,test_data],axis = 0)\n\n# drop the Id colum from df\ndf = df.drop(['Id'], axis=1)","6f5e7a64":"# dropping all the unnecessary features\ndrop_features = ['MSSubClass','OverallCond','BsmtFinType2','BsmtFinSF2','LowQualFinSF','BsmtHalfBath','PoolArea','MiscVal','MoSold', 'YrSold','GarageArea',\n                '1stFlrSF','TotRmsAbvGrd','GarageYrBlt','LotArea','Alley','LandContour','LandSlope','Heating','Electrical','BsmtExposure','BldgType',\n                'Utilities','Functional']\ndf.drop(drop_features,axis=1,inplace=True)\n","0ee26d97":"# search for missing data\ndef missing_values(data):\n    values = data.isnull().sum().sort_values(ascending = False)\n    percentage = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    missing_df = pd.concat([values,percentage], axis=1, keys=['Values', 'Percentage%'])\n    return missing_df\n\nmissing_df  = missing_values(df)\nmissing_df.head(25)","e875b175":"# plot the missing data\nsns.set()\nplt.figure(figsize = (10, 5))\nsns.barplot(missing_df.index[0:26],missing_df['Percentage%'].head(26))\nplt.xticks(rotation=90)\nplt.show()","e5fb3091":"# remove the features with more than 50% missing data\ndf.drop([ 'PoolQC','MiscFeature','Fence','FireplaceQu'], axis=1, inplace=True)\n\n# numerical column imputation\nnum_colums = ['LotFrontage','MasVnrArea','BsmtFullBath','BsmtFinSF1','TotalBsmtSF','BsmtUnfSF','GarageCars']\nfor col in num_colums:\n    df[col].fillna(df[col].mean(), inplace = True)\n    \n# categorical column imputation\ncatg_colums = ['GarageCond','GarageFinish','GarageQual','GarageType','BsmtCond','BsmtQual','BsmtFinType1','MasVnrType','MSZoning','Exterior1st','Exterior2nd',\n              'SaleType','KitchenQual']\nfor col in catg_colums:\n    df[col].fillna(df[col].value_counts().index[0], inplace=True)","4b73686b":"# normalize the target variable\nfrom scipy import stats\nfrom scipy.stats import norm\n\n# visualize the taarget variable\nsns.distplot(y, fit =norm);\nfig = plt.figure()\nprob = stats.probplot(y, plot = plt)\nplt.show()\n\nprint(\"so we see that the SalePrice is show some positive skewness and peakedness also in probability plot we clearly see that it is deviate from it's diagnol line \\n but we can handle it by doing the log transformation, so let's see how it handle it\")\n\n# apply log transformation\ny = np.log(y)\n\n# again visualize it\nsns.distplot(y, fit =norm);\nfig = plt.figure()\nprob = stats.probplot(y, plot = plt)\nplt.show()","3beae2b1":"# normalize the GrLivArea\ncol = 'GrLivArea'\n    \n# visualize the taarget variable\nsns.distplot(df[col], fit =norm);\nfig = plt.figure()\nprob = stats.probplot(df[col], plot = plt)\nplt.show()\n\nprint(\"so we see that the GrLivArea is show some positive skewness and peakedness also in probability plot we clearly see that it is deviate from it's diagnol line \\n but we can handle it by doing the log transformation, so let's see how it handle it\")\n\n# apply log transformation\ndf[col] = np.log(df[col])\n    \n# again visualize it\nsns.distplot(df[col], fit =norm);\nfig = plt.figure()\nprob = stats.probplot(df[col], plot = plt)\nplt.show()\n    ","3a82a52d":"# get dummy variables\ndf = pd.get_dummies(df, drop_first=True)\ndf.head()","0adbc3c9":"# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\nnormal_df = pd.DataFrame(minmax.fit_transform(df))","316c11b6":"# split the train and test data\ntrain = normal_df.iloc[:1455,:].values\ntest = normal_df.iloc[1455:,:].values","9e856652":"# import all necessary libraries\nfrom sklearn.linear_model import LinearRegression,BayesianRidge,Ridge,Lasso,HuberRegressor,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nimport tensorflow as tf\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error,make_scorer,r2_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# DL libraries\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten","cada5d77":"# apply multiple Algo\nimport time\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n@ignore_warnings(category=ConvergenceWarning)\ndef apply_algorithm(train,label):\n\n    start_time = time.time()\n    model_score = {} # list that contain the model scores\n        \n    # build the model and return error\n    def model(reg):\n        '''we will pass our regressor in this function and '''\n        cross_val = KFold(n_splits=10 ,shuffle= True , random_state= 0)\n        rmsle = make_scorer(r2_score)\n        validation = cross_val_score(reg ,train, label, cv=cross_val, scoring=rmsle,n_jobs=-1)\n        return validation.mean()\n    \n    # Linear Regression\n    linearReg = LinearRegression()\n    model_score['LinearReg'] = model(linearReg)\n    \n    # randomForest\n    randForest = RandomForestRegressor(n_estimators=150,n_jobs=-1)\n    model_score['Rndom Forest'] = model(randForest)\n    \n    # adboost\n    adaboost = AdaBoostRegressor(n_estimators=100)\n    model_score['AdaBoost'] = model(adaboost)\n    \n    # bayseian Ridge\n    bayaeRidge = BayesianRidge()\n    model_score['Bayesian Ridge'] = model(bayaeRidge)\n    \n    # Ridge\n    ridge = Ridge()\n    model_score['Ridge'] = model(ridge)\n    \n    # lasso\n    lasso = Lasso()\n    model_score['Lasso'] = model(lasso)\n    \n    # Hubber\n    hubber = HuberRegressor()\n    model_score['Hubber'] = model(hubber)\n    \n    # SVM\n    svr = SVR()\n    model_score['SVR'] = model(svr)\n    \n    # Elastic-net\n    elastinet = ElasticNet()\n    model_score['Elatic-Net'] = model(elastinet)\n    \n    # gradient boosting\n    GDboost = GradientBoostingRegressor()\n    model_score['Gradient Boost'] = model(GDboost)\n    \n    # LightGBM\n    lightgb = LGBMRegressor()\n    model_score['Light GBM'] = model(lightgb)\n    \n    # Xgboost\n    xgboost = XGBRegressor()\n    model_score['XG Boost'] = model(xgboost)\n    \n    # CatBoost\n    catboost = CatBoostRegressor()\n    model_score['CatBoost'] = model(catboost)\n    \n    # visualize in Dataframe\n    final_scores = pd.DataFrame.from_dict(model_score,orient='index')\n    final_scores.columns = ['R2 Scores']\n    final_scores = final_scores.sort_values('R2 Scores',ascending=False)\n    \n    # visualize in plots\n    final_scores.plot(kind = 'bar',title = 'Model Accoracies')\n    axes = plt.gca()\n    axes.set_ylim([0 ,1])\n    \n    # time taken\n    end_time = time.time()\n    time_taken = end_time - start_time\n    print(f'Time taken to apply all Algorithms  {time_taken} Seconds')\n    \n    return final_scores\n\napply_algorithm(train,y)","8e59eca5":"# initialize the base models\n\n# apply hyperParameter optimization on ridge regressiom\nparameters = {'alpha':[0.001,0.01,0.1,1,2,3,40,50,100,200, 230, 250,265, 270, 275, 290, 300, 500]}\nridge_1 = Ridge()\n\n# apply randomized searchcv\nrandomSearch = RandomizedSearchCV(ridge_1, param_distributions=parameters, n_jobs=-1, cv=10)\nrandomSearch.fit(train,y)\n\n# best parametrs \nbestpara = randomSearch.best_params_\n\n# build the model and return error\ndef model(reg,train,label):\n    '''we will pass our regressor in this function and '''\n    cross_val = KFold(n_splits=10 ,shuffle= True , random_state= 0)\n    rmsle = make_scorer(r2_score)\n    validation = cross_val_score(reg ,train, label, cv=cross_val, scoring=rmsle,n_jobs=-1)\n    return validation.mean()\n\n# base model 1\nmodel_1 = BayesianRidge()\nscore_1 = model(model_1,train,y)\n\n\n# base model 2\nmodel_2 = CatBoostRegressor()\nscore_2 = model(model_2,train,y)\n\n\n# meta regressor\nridge = Ridge(alpha=1)\nscore = model(ridge,train,y)\n\n# apply stacking \n''' Stack up models and optimize using Ridge'''\nstack_model = StackingCVRegressor(regressors= (model_1,model_2), meta_regressor=ridge)\nfinal_score = model(stack_model,train,y)\nstack_gen = stack_model.fit(train,y)","0b8897ec":"# accuracies\nprint('Bayesian Ridge Accuracy = {0:.2f}%'.format(score_1*100))\nprint('CatBoost Accuracy = {0:.2f}%'.format(score_2*100))\nprint('Ridge Accuracy = {0:.2f}%'.format(score*100))\nprint('Stack Accuracy = {0:.2f}%'.format(final_score*100))","38932830":"# predict the scores\ntest_pred = stack_gen.predict(test)\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = np.floor(np.expm1(test_pred)) # inversing and flooring log scaled saleprice to see orignal prediction\nsubmission = submission[['Id', 'SalePrice']]\n\n# create submission file\nsubmission.to_csv('blend_submission.csv',index=False)\nprint('submission file is created')","5a70da63":"***We Know that Some Of the Features Are Numerical And Others Are Categorical So We Have Different Types Of Plots To Observe Their Behaviour in the Data***","522fb7f6":"### A). **Drop the features selected from EDA**","a0a1605b":"### 2.) Categorical Feature Analysis","06fc335d":"* *you see \"**OverallQual**\" is positively increasing relationship with the \"**SalePrice**\"(second no. plot) so our theoretical assumption is right about OverallQual it affects the sale price (checked)*","bad63b14":"# **STEP 4 : STACKING - The climax of the movie**\nstacking is also known as stack generalization in which a meta-model is trained from the pool of base model let see this how it works\n* **STEP 1**: we have our train and test data split the train data in k-fold  using k-fold cross-validation, here k=3 so our train data splits into three parts\n* **STEP 2**: we take our first base model and feed the train data the model will take the first twofold (part_1 and part_2) and predict for the third fold (part_3), similarly it will do a prediction for part_1 and part_2, the prediction from each fold will create a new feature for new training data.\n\n![](https:\/\/miro.medium.com\/max\/1318\/1*9uCwjY5uRkRrX2VNST7R0w.gif)\n\n* **STEP 3**:  first base model then fitted to test data (without any folds), the predictions from test data are creating a new feature for new test data.\n* **STEP 4**: step 2 and step 3 are repeated for every base model which is in our pool and they will create a whole new set of train and test dataset  \n* **STEP 5**: now we trained our meta-model on new train data \n* **STEP 6**: final predictions on test data are made by the trained meta -model\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*GHYCJIjkkrP5ZgPh.png)","944921da":"# **STEP 3 : MODEL BUILDING**","0f0d2cb5":"### **D.) Handle Categorical Variable & Feature Scaling**\n\nwe have to convert the categorical features into the numerical features because we can't directly use them until and unless they are **Decision tree** or **forest algorithms** so we have to create **dummy variables** \nif a feature has two categories then it creates two dummy variables\n![](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2015\/03\/Dummy.png)\n\nin the above example, you see that a categorical column name **gender** contains two categories (male and female) and it will be encoded in two different columns(**var_male and var_female**) which we called **dummy variables** (features) there are some techniques called **label encoding** \nand then **one-hot encoding** is used to encode the categorical features in dummy variables but pandas has a built-in function called **get_dummies** which we use here \n\n**Dummy variables:** variables containing values such as 1 or 0 representing the presence or absence of the categorical value.\n\n**NOTE: you observe that here I pass an argument called \"drop_first  = True\"\nit means if a categorical feature has two categories then it will encode in\ntwo features but we drop one feature to avoid the dummy variable trap**\n\n**Dummy varaiable trap:** a scenario in which the independent variables are multicollinear and we also say that two or more variables are highly correlated; in simple terms one variable can be predicted from the others.","d6bdc190":"### 4.) **Meet the villain Of Movie**\n\n**OUTLIER'S**: In statistics, an outlier is an observation point that is distant from other observations. means they are separated from the crowd and contain special information that significantly affects the models so we have to detect and remove them\n* **Detection of Outliers** -> there are two visualization methods two detect them \n    1. **BoxPlot** in this all the point outside the  **interquartile range (IQR)** are outliers\n![](https:\/\/lsc.studysixsigma.com\/wp-content\/uploads\/sites\/6\/2015\/12\/1435.png)    \n    2. **scatterPlot** in this all the points separated from the **cluster** are the outliers\n![](https:\/\/statistics.laerd.com\/spss-tutorials\/img\/lr\/outliers.png)    \n    \n\n* **Removing of Outliers** -> we simply remove that rows from the dataset that contain oultliar","3b8aa169":"# **STEP 2 : FEATURE ENGINEERING**","a8cb0ba3":"**feature scaling** -> we don't want that our hero is going to biased towards some bunch of friends (features) and ignore others huh... movie aside we do feature engineering to **scale down the features at the same scale** it is not necessary for all algorithms like **tree-based algorithms** didn't affect by unscaled features but on the other hands, **distance-based algorithms** (those algo's which calculate distances like **euclidian** in their background) are affected because they **give priority to the features that contain a high magnitude of numerical value over the features which have low magnitude numerical value.** there are two methods of feature scaling\n\n* **Standardization**: in this, we scaled down the features such that **the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation** we use a function called **StandardScaler** which is present in **sklearn.preprocessing** library formula for Standardization\n![](https:\/\/study.com\/cimages\/multimages\/16\/ZFormula.jpg)                                            \n* **Normalization**:  in this, we scaled down the features in the range of  **(0, 1)** we use a function called **MinMaxScaler** which is also present in **sklearn.preprocessing** library formula for Normalization\n![](https:\/\/beyondbacktesting.files.wordpress.com\/2017\/07\/normalization.png)\n\n**I suggest that You can always start by fitting your model to raw, normalized, and standardized data and compare the performance for best results.  here I use the only normalization but in my previous notebook, I will try this.**\n\n**Note: I read somewhere that standardization is performed best for those algorithms which use gradient descent in their background** ","e24bea88":"In this part of the movie we found out how the relations are between our hero (**SalePrice**) and other movie casts (**features**) or between one member of the cast and other members(**one feature with other features**) movie aside, we do this because if two features are highly correlated then the problem of **multicollinearity** arises so to avoid this we remove one of the features and kept one so, the interesting relations that we see here are\n\n**Note: if two features are highly correlated then we kept the feature which is highly correlated with our SalePrice**\n\n* \"**GarageCars**\"  and  \"**GarageArea**\" are highly correlated  and we  kept \"GarageCars\" the relation between them is high because you know more the area is then more cars we parked in Garage\n* \"**TotalBsmtSF**\" and \"**1stFlrSF**\" are highly correlated and we kept TotalBsmtSF (you know the reason why)\n* \"**TotRmsAbvGrd**\" and '**GrLivArea**\" are highly correlated so we kept GrLivArea\n* \"**GarageYrBlt**\" and \"**YearBuilt**\" are highly correlated so our theoretical assumption is right and the reason is simply that most of the garage is built with their house and we kept YearBuilt","36b0087f":"### **B.) Handle Missing Data**\n\nATTENTION! -> this scene of the movie is most important because we have to be very careful we can't blindly  drop the features with a high percentage of missing values because in this problem if you read the description file of the data then you realize that some features like **Alley**, **BsmtQual**, and many others, etc. that they don't contain the missing value the \"**NaN*\" and \"**None**\" here don't represent the missing values they are categories means\n* in **BsmtQual the NaN means No Basement in the house it's category not missing value similarly in Alley the NaN means no Alley in property it's a category**\n\n\nthere are many other features like these so in these features we don't do the deletion and imputation we simply remove NaN with a string like \"Not in Property\" so it became a category\nand if features have real missing values then we either remove it or impute them depends on the percentage of values missing in them","88d96b70":"### 3.) Correlation Analysis","40a4de9a":"# **Conclusion And Acknowledgements**\nwe reached the end of our exercise, I think this is a great dataset for beginners to learn new methods and techniques I hope you like the notebooks and feel free to give any suggestions regarding this notebook thanks to the **Kaggle community** and special thanks to \n* [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#COMPREHENSIVE-DATA-EXPLORATION-WITH-PYTHON) : Comprehensive data exploration with Python\n* [Lavanya Shukla](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition) : How I made top 0.3% on a Kaggle competition","fdae56cc":"**we remove those features which have very less correlation with the SalePrice or features that we don't consider them when we are going to  buy a house generally**","02466665":"# **THE 25% CLUB : 4 simple Steps**\n  \n   ![](https:\/\/blog.policyexpert.co.uk\/wp-content\/uploads\/2018\/01\/iStock-653905536.jpg)\n\n\n\n                                       \"COMPUTERS ARE ABLE TO SEE, HEAR,AND \n                                           LEARN. WELCOME TO THE FUTURE.\"\n                                                             -DAVE WATERS \n>  **NOTE** : *I am a beginner in the field of data science and machine learning and I love to learn new things and teach someone                   who needs it. so here I am this is my first Kaggle notebook if I made a mistake or something in advance I                           apologize for it and feel free to correct them in the comments, please give your feedback and if you liked it then                   please upvote and appreciate. ahhhh.......enough crap let's jump to the notebook for which you are here.*               \n\n> **THE 4 SIMPLE STEPS ARE :**\n1. *Exploratory Data Analysis*\n2. *feature engineering*\n3. *Model Building*\n4. *Stacking*","bb82da2c":"# **STEP 1 :EXPLORATORY DATA ANALYSIS**\n\n**OPENING SCENE :** Introducing **SalePrice** our hero is born with some disabilities like skewness and peakedness, but don't worry                    we had a vaccine (not for COVID -19) called log transformation but the time of the dose is when we do feature                      engineering","1f9af32b":"### 1.) Numerical Features Relationship's","ab50ce56":"### **c.) Time For Vaccine**\nyou know at the beginning of the movie we see that our hero is born with some disabilities but during the movie, we realize that not only the hero but his buddies also suffer from disabilities so we use a vaccine called **log transformation** (many other vaccines are also out there like log1 you can also try that) first we run some test on our heroes and his buddies and then give them a vaccine and again run test to see whether disabilities were cured or not there are two visualizations that we use to observe the test results\n* **Distribution Plot**: we observe the **skewness** and **kurtosis**\n* **Probability Plot**:  **distribution of data**(the blue one) should closely follow the **diagonal line**(the red one) which represents the normal distribution\n\nthis treatment is called **NORMALIZATION**","7191980d":"### **firstly umm.....Don't Act Like Noobs**\n*You know Out there most of the machine learning courses they are just taught steps like how to do data preprocessing like handling missing values, categorical features, feature scaling, and model building, etc. but before all these things there is one most important step is \"**Exploratory Data Analysis** a.k.a **EDA**\"  in this we explore the data and find some insights about features, test our insight's, Basically we prepare our data for feature engineering and model building because you know \"**Well Prepared Data Beats Fancier Algorithm**\". so most of the time when people learn ML from online courses and when they start at Kaggle competitions like '**Titanic Data**' or '**House price prediction**' they just start applying standard step's which they learn and ended up getting bad models, they didn't know \"I don't write again what you don't know just read above\". so the point is I face similar problems a few months ago when I joined and I learn many new things including Statistics and Probability, EDA and Some new algorithms or techniques thanks to Kaggle notebooks and community and you can do it also ohhh...enough self obsession let's come to the problem and go ahead.*\n\n**A TALE ABOUT YOU**\n>*Imagine in a situation you are with a broker and you are going to buy a house (set yourself in a meditating state)see all features which are in the data and using common sense observe which feature is matter most to you when you are going to buy a house example like \"**LandSlope**\" we don't care about land slope like you didn't hear that the house is perfect but someone didn't buy it because **LandSlope** is slightly high or low you know these features are not very important to predict the **SalePrice** so similarly, you gonna do the same process with other features by doing some field research along with it make some assumptions about features grab some important features and test your assumptions in EDA so sounds fun ha.. ha.. ha.. :) let's do it  here are some assumptions that I made*\n   * when we are going to buy a house we don't care about **\"Alley\", \"LandContour\", \"LandSlope\"** so we are going to drop them \n   * \"**BldgType**\" is also explained in \"**MSSubClass**\" we drop one of them according to their relationship with **SalePrice**\n   * \"**Heating**\" and \"**Electrical**\" are the features that we don't care about it we want electricity we don't want know which kind of electrical system installed ya we check the quality but not which kind of system it is in \"terminology\"\n   * I think correlation of \"**GarageYrBlt**\" and \"**YearBuilt**\" is high because most of the garages are built with the houses so value(year) will be the same in those variables that's why the correlation should be high \n   * the important features that affect the sale price are:-\n        1. **OverallQual** : Represents the overall quality of the house\n        2. **GrLivArea** : Total living Area above ground\n        3. **TotalBsmtSF**: Total basement surface Area\n        4. **GarageArea**: Total Area of garage\n        5. **YearBuilt**: Year in which house is built\n        \nLet's start the movie **ROLL CAMERA ACTION**","2e023c9e":"* *we see that \"**PoolArea**\", \"**MiscVal**\" and \"**LotArea**\" have almost a constant relationship with SalePrice it means on increasing and decreasing the SalePrice these are not affected so we are gonna drop these features*\n\n**NOTE: if we remove the outlier's from \"LotArea\" maybe it's relation will improve with the sale price you can do that I drop it because I observe when I am writing this so maybe you don't drop it rather you remove outliers and try it if results improve then let me know in the comment's**  \n\n* *you see the \"**TotalBsmtSF**\", \"**GrLivArea**\" and \"**TotRmsAbvGrd**\" are positively increasing relationship with our hero \"SalePrice\" so our assumption about TotalBsmtSF and GrLivArea are right (Checked) but to make relation we have to remove the outliers you see that they suppress the relation so we treat them well in Feature engineering "}}