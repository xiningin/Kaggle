{"cell_type":{"4930115e":"code","a88f1333":"code","5aa85888":"code","9847e131":"code","09151d97":"code","eac14a2d":"code","245ae1b6":"code","6b2bc771":"code","4ded77d5":"code","67298b19":"code","e954f7da":"code","657ef3ef":"code","009b76f4":"code","483eb2ba":"code","1caa1821":"code","267d9c40":"code","50dcaee9":"code","1e3d5f4c":"code","ece5d87d":"code","b3b1b962":"code","034f6d10":"code","a037c54a":"code","4ac2e9ad":"code","f7615336":"code","8e1674db":"code","025c75ce":"code","c2528111":"code","21e66b89":"code","264e1f39":"code","286d6ebe":"code","db9efbe4":"code","f29af602":"code","23d9a934":"code","4dc1085a":"code","48eb5312":"code","f549a8cf":"code","22fc94e7":"code","ed72514e":"code","1b6ee8ab":"code","195fa832":"code","b6f2a763":"code","37fa8626":"code","52ad516d":"code","59ca9215":"code","3150eb72":"code","f3844510":"code","67ce0845":"code","6cf2acf1":"markdown","d73ed7d1":"markdown","5f46b930":"markdown","1624f031":"markdown","59d6621f":"markdown","2fc60727":"markdown","5941cf4a":"markdown","ebcc67f4":"markdown","6543a529":"markdown","092d70ff":"markdown","2f74611f":"markdown","e2ff09a3":"markdown","3e55f57c":"markdown","48d7f17f":"markdown","294658c5":"markdown","2db02b1b":"markdown","e2cce416":"markdown","2f0e87a8":"markdown","a53a730d":"markdown","cabe5faa":"markdown","ba0432d7":"markdown"},"source":{"4930115e":"import pandas as pd\nimport numpy as np\nfrom collections import Counter \nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport math\nimport torch.nn.functional as F\nimport pickle\nimport gc\nimport random\npd.set_option('display.max_colwidth', None)","a88f1333":"df = pd.read_csv(\"..\/input\/flickr8k\/captions.txt\", sep=',')\nprint(len(df))\ndisplay(df.head(3))","5aa85888":"def remove_single_char_word(word_list):\n    lst = []\n    for word in word_list:\n        if len(word)>1:\n            lst.append(word)\n\n    return lst","9847e131":"df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\ndf['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))","09151d97":"df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\nmax_seq_len = df['seq_len'].max()\nprint(max_seq_len)","eac14a2d":"df.drop(['seq_len'], axis = 1, inplace = True)\ndf['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )","245ae1b6":"display(df.head(2))","6b2bc771":"word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\nword_dict = Counter(word_list)\nword_dict =  sorted(word_dict, key=word_dict.get, reverse=True)","4ded77d5":"print(len(word_dict))\nprint(word_dict[:5])","67298b19":"vocab_size = len(word_dict)\nprint(vocab_size)","e954f7da":"index_to_word = {index: word for index, word in enumerate(word_dict)}\nword_to_index = {word: index for index, word in enumerate(word_dict)}\nprint(len(index_to_word), len(word_to_index))","657ef3ef":"df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )","009b76f4":"display(df.head(2))","483eb2ba":"df = df.sort_values(by = 'image')\ntrain = df.iloc[:int(0.9*len(df))]\nvalid = df.iloc[int(0.9*len(df)):]","1caa1821":"print(len(train), train['image'].nunique())\nprint(len(valid), valid['image'].nunique())","267d9c40":"train_samples = len(train)\nprint(train_samples)","50dcaee9":"unq_train_imgs = train[['image']].drop_duplicates()\nunq_valid_imgs = valid[['image']].drop_duplicates()\nprint(len(unq_train_imgs), len(unq_valid_imgs))","1e3d5f4c":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","ece5d87d":"class extractImageFeatureResNetDataSet():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['image']\n        img_loc = '..\/input\/flickr8k\/Images\/'+str(image_name)\n\n        img = Image.open(img_loc)\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","b3b1b962":"train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\ntrain_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)","034f6d10":"valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\nvalid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)","a037c54a":"resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\nresnet18.eval()\nlist(resnet18._modules)","4ac2e9ad":"resNet18Layer4 = resnet18._modules.get('layer4').to(device)","f7615336":"def get_vector(t_img):\n    \n    t_img = Variable(t_img)\n    my_embedding = torch.zeros(1, 512, 7, 7)\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    \n    h = resNet18Layer4.register_forward_hook(copy_data)\n    resnet18(t_img)\n    \n    h.remove()\n    return my_embedding","8e1674db":"extract_imgFtr_ResNet_train = {}\nfor image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_train[image_name[0]] = embdg\n    ","025c75ce":"a_file = open(\".\/EncodedImageTrainResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_train, a_file)\na_file.close()","c2528111":"extract_imgFtr_ResNet_valid = {}\nfor image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n \n    extract_imgFtr_ResNet_valid[image_name[0]] = embdg","21e66b89":"a_file = open(\".\/EncodedImageValidResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_valid, a_file)\na_file.close()","264e1f39":"class FlickerDataSetResnet():\n    def __init__(self, data, pkl_file):\n        self.data = data\n        self.encodedImgs = pd.read_pickle(pkl_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n    \n        caption_seq = self.data.iloc[idx]['text_seq']\n        target_seq = caption_seq[1:]+[0]\n\n        image_name = self.data.iloc[idx]['image']\n        image_tensor = self.encodedImgs[image_name]\n        image_tensor = image_tensor.permute(0,2,3,1)\n        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n\n        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view","286d6ebe":"train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\ntrain_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)","db9efbe4":"valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\nvalid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)","f29af602":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n\n    def forward(self, x):\n        if self.pe.size(0) < x.size(0):\n            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n        self.pe = self.pe[:x.size(0), : , : ]\n        \n        x = x + self.pe\n        return self.dropout(x)","23d9a934":"class ImageCaptionModel(nn.Module):\n    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n        super(ImageCaptionModel, self).__init__()\n        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(vocab_size , embedding_size)\n        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.last_linear_layer.bias.data.zero_()\n        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n\n    def generate_Mask(self, size, decoder_inp):\n        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n\n        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n        decoder_input_pad_mask_bool = decoder_inp == 0\n\n        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n\n    def forward(self, encoded_image, decoder_inp):\n        encoded_image = encoded_image.permute(1,0,2)\n        \n\n        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n        \n        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n        \n\n        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n        decoder_input_mask = decoder_input_mask.to(device)\n        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n        \n\n        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n        \n        final_output = self.last_linear_layer(decoder_output)\n\n        return final_output,  decoder_input_pad_mask\n","4dc1085a":"EPOCH = 30","48eb5312":"ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\noptimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\nmin_val_loss = np.float('Inf')","f549a8cf":"for epoch in tqdm(range(EPOCH)):\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n    total_train_words = 0\n    total_valid_words = 0\n    ictModel.train()\n\n    ### Train Loop\n    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n\n        optimizer.zero_grad()\n\n        image_embed = image_embed.squeeze(1).to(device)\n        caption_seq = caption_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n        output = output.permute(1, 2, 0)\n\n        loss = criterion(output,target_seq)\n\n        loss_masked = torch.mul(loss, padding_mask)\n\n        final_batch_loss = torch.sum(loss_masked)\/torch.sum(padding_mask)\n\n        final_batch_loss.backward()\n        optimizer.step()\n        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n        total_train_words += torch.sum(padding_mask)\n\n \n    total_epoch_train_loss = total_epoch_train_loss\/total_train_words\n  \n\n    ### Eval Loop\n    ictModel.eval()\n    with torch.no_grad():\n        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n\n            image_embed = image_embed.squeeze(1).to(device)\n            caption_seq = caption_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n            output = output.permute(1, 2, 0)\n\n            loss = criterion(output,target_seq)\n\n            loss_masked = torch.mul(loss, padding_mask)\n\n            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n            total_valid_words += torch.sum(padding_mask)\n\n    total_epoch_valid_loss = total_epoch_valid_loss\/total_valid_words\n  \n    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n  \n    if min_val_loss > total_epoch_valid_loss:\n        print(\"Writing Model at epoch \", epoch)\n        torch.save(ictModel, '.\/BestModel')\n        min_val_loss = total_epoch_valid_loss\n  \n\n    scheduler.step(total_epoch_valid_loss.item())\n","22fc94e7":"model = torch.load('.\/BestModel')\nstart_token = word_to_index['<start>']\nend_token = word_to_index['<end>']\npad_token = word_to_index['<pad>']\nmax_seq_len = 33\nprint(start_token, end_token, pad_token)","ed72514e":"valid_img_embed = pd.read_pickle('EncodedImageValidResNet.pkl')","1b6ee8ab":"def generate_caption(K, img_nm): \n    img_loc = '..\/input\/flickr8k\/Images\/'+str(img_nm)\n    image = Image.open(img_loc).convert(\"RGB\")\n    plt.imshow(image)\n\n    model.eval() \n    valid_img_df = valid[valid['image']==img_nm]\n    print(\"Actual Caption : \")\n    print(valid_img_df['caption'].tolist())\n    img_embed = valid_img_embed[img_nm].to(device)\n\n\n    img_embed = img_embed.permute(0,2,3,1)\n    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n\n\n    input_seq = [pad_token]*max_seq_len\n    input_seq[0] = start_token\n\n    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n    predicted_sentence = []\n    with torch.no_grad():\n        for eval_iter in range(0, max_seq_len):\n\n            output, padding_mask = model.forward(img_embed, input_seq)\n\n            output = output[eval_iter, 0, :]\n\n            values = torch.topk(output, K).values.tolist()\n            indices = torch.topk(output, K).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = index_to_word[next_word_index]\n\n            input_seq[:, eval_iter+1] = next_word_index\n\n\n            if next_word == '<end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    print(\"\\n\")\n    print(\"Predicted caption : \")\n    print(\" \".join(predicted_sentence+['.']))","195fa832":"generate_caption(1, unq_valid_imgs.iloc[50]['image'])","b6f2a763":"generate_caption(2, unq_valid_imgs.iloc[50]['image'])","37fa8626":"generate_caption(1, unq_valid_imgs.iloc[100]['image'])","52ad516d":"generate_caption(2, unq_valid_imgs.iloc[100]['image'])","59ca9215":"generate_caption(1, unq_valid_imgs.iloc[500]['image'])","3150eb72":"generate_caption(2, unq_valid_imgs.iloc[500]['image'])","f3844510":"generate_caption(1, unq_valid_imgs.iloc[600]['image'])","67ce0845":"generate_caption(2, unq_valid_imgs.iloc[600]['image'])","6cf2acf1":"### Vocab size is 8360","d73ed7d1":"# In this  Notebook, I have Created Image Caption Generator using ResNet and Transformer Decoder Model. It can be divided into 2 steps :\n## **Step 1** : Create features for Images Using Resnet\n## **Step 2** : Train Transformer Decoder Model which predicts next word given a sequence of tokens and Image Features from Step1\n## Please do UpVote this notebook if you liked the content","5f46b930":"##  Train the Model","1624f031":"### 2nd Example","59d6621f":"## Transformer Decoder","2fc60727":"## Lets Generate Captions !!!","5941cf4a":"## Create Vocab and mapping of token to ID","ebcc67f4":"### 1st Example ","6543a529":"### 3rd Example","092d70ff":"### Covert sequence of tokens to IDs","2f74611f":"### Here in the below function,we are generating caption in beam search. K defines the topK token to look at each time step","e2ff09a3":"### The cross entropy loss has been masked at time steps where input token is <'pad'>.","3e55f57c":"### Position Embedding","48d7f17f":"### 4rth Example","294658c5":"## Split In Train and validation data. Same Image should not be present in both training and validation data ","2db02b1b":"## Preprocessing -> Remove Single Character and non alpha Words. Add <Start>, <end> and <pad> tokens. <pad> token is appended such that length in max_seq_len (maximum length across all captions which is 33 in our case)  ","e2cce416":"## Read Data.","2f0e87a8":"## Create DataLoader which will be used to load data into Transformer Model.\n## FlickerDataSetResnet will return caption sequence, 1 timestep left shifted caption sequence which model will predict and Stored Image features from ResNet.","a53a730d":"## Extract features from Images Using Resnet","cabe5faa":"## Thanks for going through the whole work. Please do upvote the notebook if you liked it.","ba0432d7":"## Create Transformer Decoder Model. This Model will take caption sequence and the extracted resnet image features as input and ouput 1 timestep shifted (left) caption sequence. \n## In the Transformer decoder, lookAhead and padding mask has also been applied"}}