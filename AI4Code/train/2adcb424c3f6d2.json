{"cell_type":{"f8f98e0b":"code","39d6af54":"code","4c26421f":"code","6c25e780":"code","ee109299":"code","4addf776":"code","f8f75447":"code","14d78905":"code","8379cf46":"code","766e6782":"code","db197bb2":"code","f19e0d52":"code","8fcc6bf4":"code","ab5b207a":"code","6e2cacfe":"code","56b11b0c":"code","020d5960":"code","a4f82dc9":"code","b55eb37c":"code","1ae4c084":"code","2ed0e434":"code","18181353":"code","2a6c723b":"code","239ee09a":"code","a9fcc6e1":"code","8a2d257d":"code","bf3e909a":"code","a3c4bbf3":"code","833cf0da":"code","5dee3f90":"markdown","4924fe1c":"markdown","789ab956":"markdown","d6c53161":"markdown","e4c46b7c":"markdown","9be7faa4":"markdown","9962db12":"markdown","79f66a62":"markdown","fd19b0f8":"markdown","1b8970d3":"markdown","54fcd241":"markdown","36140cd4":"markdown","23654d37":"markdown","61278b20":"markdown","f8c42735":"markdown","b77a2f02":"markdown","c091f781":"markdown","c4ea6204":"markdown"},"source":{"f8f98e0b":"#importing Numpy and PyTorch Libraries\nimport numpy as np\nimport torch","39d6af54":"# Input(temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')\n\n# Output(apples, orange)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')\n\n\n# inputs and targets into tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(\"Input in tensor form : \\n\",inputs ,\"\\nTarget in tensor form: \\n\",targets)","4c26421f":"# weights and biases\nw = torch.randn(2,3, requires_grad = True)\nb = torch.randn(2, requires_grad = True)\nprint(w,b)","6c25e780":"# model function\ndef model(x):\n    return x @ w.t() +b  # @ represents matrix multiplication in Pytorch.\n\n# prediction \npreds = model(inputs)\npreds","ee109299":"#compare preds with the targets\nprint(targets)","4addf776":"# Meas Squarred Error (Loss Function)\ndef mse(t1,t2):\n    diff = (t1-t2)\n    return torch.sum(diff**2)\/diff.numel() #Returns the total number of elements in the input tensor.\n\n#Calculate loss b\/wprediction and targets\nloss = mse(preds,targets)\nloss","f8f75447":"#Compute gradients\nloss.backward()\n\n# Gradiants for weoghts\nprint(w.grad)\nprint(b.grad)","14d78905":"# weights and biases\nprint(w)\nprint(b)","8379cf46":"# adjust weights and reset gradiants\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()\n    \n    \n# new weights and biases\nprint(w)\nprint(b)\n    ","766e6782":"# calculate loss\npreds= model(inputs)\nloss = mse(preds,targets)\nloss","db197bb2":"# Training for some number of epochs to reduce the loss.\nfor i in range(3000):\n    preds = model(inputs)\n    loss = mse(preds,targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()\n        \n        \n#Calculate loss after the running model for some epochs\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","f19e0d52":"# compare the predicted values with the actual values.\nprint(preds)\nprint(targets)","8fcc6bf4":"#importing the torch.nn package from PyTorch, which contains utility classes for building neural networks\nimport torch.nn as nn\n\n# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n                  dtype='float32')\n\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], \n                    [22, 37], [103, 119], [56, 70], \n                    [81, 101], [119, 133], [22, 37], \n                    [103, 119], [56, 70], [81, 101], \n                    [119, 133], [22, 37], [103, 119]], \n                   dtype='float32')\n\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","ab5b207a":"from torch.utils.data import TensorDataset, DataLoader\n\n# Dataset \ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[0:3]","6e2cacfe":"#DataLoader\nbatchsize = 5\ntrain_dl = DataLoader(train_ds, batchsize ,shuffle= True)\ntrain_dl","56b11b0c":"#Look into one of the batch\nfor xb, yb in train_dl:\n    print(xb)\n    print(yb)\n    break","020d5960":"# define Model\nmodel = nn.Linear(3,2)   # 3 input variables amd 2 output variables.\nprint(\"Weights: \\n\",model.weight)\nprint(\"Biases: \\n\",model.bias)","a4f82dc9":"# Parameters\nlist(model.parameters())","b55eb37c":"preds = model(inputs)\npreds","1ae4c084":"import torch.nn.functional  as F\n\n# loss function\nloss_fn= F.mse_loss\n\n#compute loss\nloss = loss_fn(preds,targets)\nloss","2ed0e434":"#define optimizer\nopt= torch.optim.SGD(model.parameters(), lr = 1e-5)","18181353":"def fit(epoches, model, loss_fn, opt, train_dl):\n    for epoch in range(epoches):\n        for xb,yb in train_dl:\n            pred=model(xb)\n            \n            # compute loss\n            loss=loss_fn(pred,yb)\n            \n            # compute gradients\n            loss.backward()\n            \n            # update parameter using gradients\n            opt.step()\n            \n            #reset gradient to zero\n            opt.zero_grad()\n            \n        if (epoch+1) %10 ==0:\n            print(f\"Epoch[{epoch+1}\/{epoches}], Loss: {loss.item()} \")","2a6c723b":"# fit the model for 200 epoches\nfit(200, model, loss_fn, opt,train_dl)","239ee09a":"preds = model(inputs)\npreds[0:5]","a9fcc6e1":"targets[0:5]","8a2d257d":"torch.save(model.state_dict(), 'lin_reg.pth')","bf3e909a":"model.state_dict()","a3c4bbf3":"#create a new model \nmodel2 = nn.Linear(3,2)\nprint(\"weights and biases for the model\")\nmodel2.state_dict()","833cf0da":"model2.load_state_dict(torch.load('lin_reg.pth'))\nprint(\"weights and biases of model 2 after loading weights and biases from saved model\")\nmodel2.state_dict()","5dee3f90":"## Linear Regression With Pytorch Builtins","4924fe1c":"## Saving and loading the model\nSince we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights and bias matrices to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model.","789ab956":"### Optimizer\nInstead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. SGD stands for `stochastic gradient descent`. It is called stochastic because samples are selected in batches (often with random shuffling) instead of as a single group.","d6c53161":"PyTorch models also have a helpful `.parameters` method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix","e4c46b7c":"### Dataset and DataLoader\nWe'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch.","9be7faa4":"**Loss Function**","9962db12":"**Reference** :- I am practicing this notebook from Lecture 1 [Deep Laering with Pytorch Zero to GANS] on jovian.ml  https:\/\/jovian.ml\/forum\/c\/pytorch-zero-to-gans\/18","79f66a62":"**Compute gradients**","fd19b0f8":"The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model.","1b8970d3":"**Training the model for more epochs**","54fcd241":"### nn.Linear\nInstead of initializing the weights & biases manually, we can define the model using the `nn.Linear` class from PyTorch, which does it automatically.","36140cd4":"## Linear Regression with Gradiant descent method.","23654d37":"### Train the model\nWe are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n\n- Generate predictions\n- Calculate the loss\n- Compute gradients w.r.t the weights and biases\n- Adjust the weights by subtracting a small quantity proportional to the gradient\n- Reset the gradients to zero","61278b20":"### Loading the model","f8c42735":"**Training Data**","b77a2f02":"### Linear Regression\nWeights and biases initialized as random values.\nFirst row of weigths and first element of bias are used to predict first target variable.","c091f781":"These weights and biases are same for both 'model' and 'model2'.","c4ea6204":"Loss Function"}}