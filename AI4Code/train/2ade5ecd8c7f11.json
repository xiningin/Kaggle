{"cell_type":{"b553a41b":"code","8862a90f":"code","a09c1b15":"code","adc9fe32":"code","f3f3dd56":"code","088cf32e":"code","21fe01cd":"code","66b8bd11":"code","d9b40bec":"code","015f6379":"code","ba01f5a4":"code","4cb44f85":"code","d88dc75a":"code","28e0c2d7":"code","51f603ff":"code","ce3c8688":"code","a4b85856":"code","358020a2":"code","23f1b0a1":"code","718efa3f":"code","33936145":"code","d66743e1":"code","9fa8012d":"code","d3d99b61":"code","374707c2":"code","3094a1ed":"code","e9dbe7ce":"markdown","0674c25b":"markdown","a7a408ba":"markdown","31a66801":"markdown","8aec930d":"markdown","69a92a51":"markdown","bd5e4704":"markdown","613ecff9":"markdown","b63f7b54":"markdown","2d646f3f":"markdown","23230922":"markdown","15589c34":"markdown","b6498b76":"markdown"},"source":{"b553a41b":"import os\nimport math\nimport random\nimport time\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nimport gc\ngc.enable()","8862a90f":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"\/kaggle\/input\/roberta-base\"\nTOKENIZER_PATH = \"\/kaggle\/input\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","a09c1b15":"test_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","adc9fe32":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","f3f3dd56":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","088cf32e":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","21fe01cd":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","66b8bd11":"test_dataset = LitDataset(test_df, inference_only=True)","d9b40bec":"%%time\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","015f6379":"model1_predictions = all_predictions.mean(axis=0)","ba01f5a4":"class LitModel1_2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 1024),  # (768, 512)\n            nn.SELU(),    # Tanh\n            nn.Linear(1024, 512),  #  added\n            nn.SELU(),     # added   PReLU, ReLU, RELU6, RReLU, SELU, CELU, GELU, SiLU\n            nn.Linear(512, 1), # (512, 1)\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","4cb44f85":"%%time\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/model17681024512\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel1_2()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","d88dc75a":"model2_predictions = all_predictions.mean(axis=0)","28e0c2d7":"class LitModelTabular(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n#         return self.regressor(context_vector) # original\n        return context_vector","51f603ff":"def get_embeddings(df, path = \"..\/input\/commonlit-roberta-0467\/model_2.pth\", plot_losses=True, verbose=True):\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"{device} is used\")\n            \n    model = LitModelTabular()\n    model.load_state_dict(torch.load(path, map_location=DEVICE)) # path\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH) # '..\/input\/roberta-base'\n    \n    ds = LitDataset(df, inference_only=True)\n    dl = DataLoader(ds,\n                  batch_size = BATCH_SIZE,\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(dl):\n            inputs = input_ids.to(device)\n            att = attention_mask.to(device)\n#             tgt = target[i].to(device)\n            # {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(inputs, att)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","ce3c8688":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain_data = train_data.loc[:, ['excerpt', 'target']]\ntarget = train_data['target'].to_numpy()\n\n# train_data","a4b85856":"%%time\n\ntrain_embeddings1 =  get_embeddings(train_data,'..\/input\/commonlit-roberta-0467\/model_1.pth')\ntest_embeddings1 = get_embeddings(test_df,'..\/input\/commonlit-roberta-0467\/model_1.pth')\n\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/commonlit-roberta-0467\/model_2.pth')\ntest_embeddings2 = get_embeddings(test_df,'..\/input\/commonlit-roberta-0467\/model_2.pth')\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/commonlit-roberta-0467\/model_3.pth')\ntest_embeddings3 = get_embeddings(test_df,'..\/input\/commonlit-roberta-0467\/model_3.pth')\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/commonlit-roberta-0467\/model_4.pth')\ntest_embeddings4 = get_embeddings(test_df,'..\/input\/commonlit-roberta-0467\/model_4.pth')\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/commonlit-roberta-0467\/model_5.pth')\ntest_embeddings5 = get_embeddings(test_df,'..\/input\/commonlit-roberta-0467\/model_5.pth')","358020a2":"train_embeddings1.shape ,test_embeddings1.shape","23f1b0a1":"%%time\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef get_preds_etr(X, y, X_test, seed, nfolds=5):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    # Don't scale targets, only features \n    scaler = MinMaxScaler() # RobustScaler()\n    scaler.fit(X)\n    X_scaled = scaler.transform(X)\n    scaler.fit(X_test)\n    X_test_scaled = scaler.transform(X_test)\n    \n    X, X_test = X_scaled, X_test_scaled\n    \n    kfold = KFold(n_splits=nfolds, shuffle=True, random_state=seed)\n    for k, (train_idx, valid_idx) in enumerate(kfold.split(X, y)):\n        model = ExtraTreesRegressor(random_state=seed)\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_valid, y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds\n\n\netr_preds1 = get_preds_etr(train_embeddings1, target, test_embeddings1, 123)\netr_preds2 = get_preds_etr(train_embeddings2, target, test_embeddings2, 123)\netr_preds3 = get_preds_etr(train_embeddings3, target, test_embeddings3, 123)\netr_preds4 = get_preds_etr(train_embeddings4, target, test_embeddings4, 123)\netr_preds5 = get_preds_etr(train_embeddings5, target, test_embeddings5, 123)\n\n# other seed\netr_preds6 = get_preds_etr(train_embeddings1, target, test_embeddings1, 234)\netr_preds7 = get_preds_etr(train_embeddings2, target, test_embeddings2, 234)\netr_preds8 = get_preds_etr(train_embeddings3, target, test_embeddings3, 234)\netr_preds9 = get_preds_etr(train_embeddings4, target, test_embeddings4, 234)\netr_preds10 = get_preds_etr(train_embeddings5, target, test_embeddings5, 234)\n\n#### 1 seed\n# etr_preds = (etr_preds1 + etr_preds2 + etr_preds3 + etr_preds4 + etr_preds5)\/5\n\n#### 2 seeds\netr_preds = (etr_preds1 + etr_preds2 + etr_preds3 + etr_preds4 + etr_preds5 + etr_preds6 + etr_preds7 + etr_preds8 + etr_preds9 + etr_preds10)\/10\n\netr_preds","718efa3f":"%%time\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.svm import SVR\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\ndef get_preds_svm(X, y, X_test, seed, nfolds=5):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    # Don't scale targets, only features \n    scaler = MinMaxScaler() # RobustScaler()\n    scaler.fit(X)\n    X_scaled = scaler.transform(X)\n    scaler.fit(X_test)\n    X_test_scaled = scaler.transform(X_test)\n    \n    kfold = KFold(n_splits=nfolds, shuffle=True, random_state=seed)\n    for k, (train_idx, valid_idx) in enumerate(kfold.split(X, y)):\n        model = SVR(C=10, kernel='rbf', gamma='auto')\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_valid, y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds\n\n\nsvm_preds1 = get_preds_svm(train_embeddings1, target, test_embeddings1, 678)\nsvm_preds2 = get_preds_svm(train_embeddings2, target, test_embeddings2, 678)\nsvm_preds3 = get_preds_svm(train_embeddings3, target, test_embeddings3, 678)\nsvm_preds4 = get_preds_svm(train_embeddings4, target, test_embeddings4, 678)\nsvm_preds5 = get_preds_svm(train_embeddings5, target, test_embeddings5, 678)\n\n# other seed\nsvm_preds6 = get_preds_svm(train_embeddings1, target, test_embeddings1, 789)\nsvm_preds7 = get_preds_svm(train_embeddings2, target, test_embeddings2, 789)\nsvm_preds8 = get_preds_svm(train_embeddings3, target, test_embeddings3, 789)\nsvm_preds9 = get_preds_svm(train_embeddings4, target, test_embeddings4, 789)\nsvm_preds10 = get_preds_svm(train_embeddings5, target, test_embeddings5, 789)\n\n\n#### 1 seed\n# svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)\/5\n\n#### 2 seeds\nsvm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5 + svm_preds6 + svm_preds7 + svm_preds8 + svm_preds9 + svm_preds10)\/10\n\nsvm_preds","33936145":"test = test_df\n\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange\n\ndef convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n\nclass CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output\n\ndef make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader\n\nclass Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\n\ndef config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )\n\ndef run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds\n\npred_df1 = pd.DataFrame()\npred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold, '..\/input\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n    pred_df2[f'fold{fold+5}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/roberta-large-itptfit\/')\n    pred_df3[f'fold{fold+10}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/commonlit-roberta-large-ii\/')","d66743e1":"pred_df1 = np.array(pred_df1)\npred_df2 = np.array(pred_df2)\npred_df3 = np.array(pred_df3)\nmodel4_predictions = ((pred_df1.mean(axis=1)*0.3) + (pred_df2.mean(axis=1)*0.5) + (pred_df3.mean(axis=1) * 0.2))","9fa8012d":"%%time\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\nCHECKPOINT_DIR = '..\/input\/clrp-mean-pooling\/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nMAX_LENGTH = 248\nTEST_BATCH_SIZE = 1\nHIDDEN_SIZE = 1024\n\nNUM_FOLDS = 5\nSEEDS = [113, 71]\n\ntest = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n\ndef get_test_loader(data):\n\n    x_test = data.excerpt.tolist()\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n    encoded_test = tokenizer.batch_encode_plus(\n        x_test, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n\n    dataset_test = TensorDataset(\n        encoded_test['input_ids'],\n        encoded_test['attention_mask']\n    )\n\n    dataloader_test = DataLoader(\n        dataset_test,\n        sampler = SequentialSampler(dataset_test),\n        batch_size=TEST_BATCH_SIZE\n    )\n    \n    return dataloader_test\n\ntest_dataloader = get_test_loader(test)\n\nall_predictions = []\nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n        \n        print(f\"\\nUsing {model_path}\")\n        \n        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path)) \n        model.to(DEVICE)\n        model.eval()\n\n        predictions = []\n        for batch in test_dataloader:\n\n            batch = tuple(b.to(DEVICE) for b in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'labels':         None,\n                     }\n\n     \n            preds = model(**inputs).item()\n            predictions.append(preds)\n            \n        del model \n        gc.collect()\n            \n        fold_predictions.append(predictions)\n    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \nmodel5_predictions = np.mean(all_predictions,axis=0)\n\n\n# predictions = model1_predictions * 0.5 + model2_predictions * 0.3 + model3_predictions * 0.2  \n# predictions\n\n# results = pd.DataFrame(np.vstack((model1_predictions, model2_predictions, model3_predictions, predictions)).transpose(), \n#                        columns=['model1','model2','model3','ensemble'])\n\n# results","d3d99b61":"model5_predictions","374707c2":"predictions = model1_predictions * 0.15 + \\\n              model2_predictions * 0.25 + \\\n              etr_preds * 0.10 + \\\n              svm_preds * 0.00 + \\\n              pred_df1.mean(axis=1) * 0.00 + \\\n              pred_df2.mean(axis=1) * 0.10 + \\\n              pred_df3.mean(axis=1) * 0.00 + \\\n              model5_predictions * 0.40","3094a1ed":"submission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","e9dbe7ce":"## Model 1 Inference","0674c25b":"## ExtraTreesRegressor","a7a408ba":"# TL;DR\n\nThis solution is an ensemble of a few models:\n\n- Model 1: roberta-base, further pre-trained by [Maunish dave](https:\/\/www.kaggle.com\/maunish), and available in [this dataset](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-base). Inference code was shared in [this notebook](https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch), by [Andrey Tuganov](https:\/\/www.kaggle.com\/andretugan).\n\n- Model 2: Almost the same code as Model 1, roberta-base. I changed the architecture: changed the number of neurons, added one Linear layer, and changed the activation function. This is the best CV I got. Training was done in another notebook.\n\n- Model 3: Inspired by [this work](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm), also from [Maunish dave](https:\/\/www.kaggle.com\/maunish), I extracted the embeddings from Model 1 and used ExtraTreesRegressor to predict the target. 10 seeds were used here.\n\n- Model 4: Code is from [this notebook](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3), published by [torch](https:\/\/www.kaggle.com\/rhtsingh). He used an ensemble of three models, but I only used his second checkpoint, which is a roberta-large, also further pre-trained.\n\n- Model 5: roberta-large with MeanPooling shared in [this notebook](https:\/\/www.kaggle.com\/rajat95gupta\/clrp-ensemble-3x-inference) by [Rajat Gupta](https:\/\/www.kaggle.com\/rajat95gupta). Two seeds were used in this inference.\n\nIf you like this solution, please don't forget to upvote the all the works mentioned above!","31a66801":"[ExtraTreesRegressor docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesRegressor.html)","8aec930d":"# Model 3\n\nInspired by [this work](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm), also from [Maunish dave](https:\/\/www.kaggle.com\/maunish), I extracted the embeddings from Model 1 and used ExtraTreesRegressor to predict the target. 15 seeds were used here.","69a92a51":"# Model 5\n\nRoberta-large with MeanPooling shared in [this notebook](https:\/\/www.kaggle.com\/rajat95gupta\/clrp-ensemble-3x-inference) by [Rajat Gupta](https:\/\/www.kaggle.com\/rajat95gupta).","bd5e4704":"# Model 1\n\nRoberta-base, further pre-trained by [Maunish dave](https:\/\/www.kaggle.com\/maunish), and available in [this dataset](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-base). Inference code was shared in [this notebook](https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch), by [Andrey Tuganov](https:\/\/www.kaggle.com\/andretugan).","613ecff9":"## Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","b63f7b54":"# Model 2\n\nAlmost the same code as Model 1, roberta-base. I changed the architecture: changed the number of neurons, added one Linear layer, and changed the activation function. This is the best CV I got.","2d646f3f":"## Dataset","23230922":"# Model 4\n\nCode is from [this notebook](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3), published by [torch](https:\/\/www.kaggle.com\/rhtsingh). He used an ensemble of three models, but I only used his second checkpoint, which is a roberta-large, also further pre-trained.","15589c34":"## SVM\n\n[SVR docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html#sklearn.svm.SVR)","b6498b76":"# Ensemble"}}