{"cell_type":{"b0beff70":"code","4202756f":"code","9a8bbee3":"code","62940ba8":"code","a62d3b4f":"code","2d33272f":"code","b49fd368":"code","97261ea3":"code","32413a70":"code","2cbc9038":"code","59e3d0d2":"code","11c90041":"markdown","9fe30e89":"markdown","f01cb94f":"markdown","63163ce2":"markdown","46b09dbc":"markdown","2446c4bd":"markdown"},"source":{"b0beff70":"# Load libraries\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier","4202756f":"# Loading our MNIST dataset\ndata = load_digits()\nX = data['data']\ny = data['target']","9a8bbee3":"fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(10,6))\nfor i in range(10):\n    img = np.reshape(X[y == i][5],(8,8))\n    ax[int(i\/5)][i%5].imshow(img, cmap='gray')","62940ba8":"# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X\/16, y, test_size=0.3)","a62d3b4f":"# The class Layer, uniquely defines the attributes and functionalities of a layer e.g. size, activation function, weights etc.\nclass Layer:\n    # Initializes the basic attributes of a single layer\n    def __init__(self, size, index, activation='sigmoid'):\n        self.size = size # defines the number of stacked neurons in the layer\n        self.activation_func = activation # defines the activation function that each neuron in the layer uses\n        self.layer_index = index # defines the location of layer in the network\n        self.a = 0 # defines the output of the layer after running through activation\n        self.z = 0 # defines the input of layer to the activation function\n        \n    # Weight initialization is very important or the we might see vanishing\/exploding gradient problem\n    # We use Glroot Initialization technique to intialize our weights and biases\n    def init_weight(self, input_shape, output_shape):\n        mean = 0\n        std = np.sqrt(2\/(input_shape + output_shape))\n        self.weights = np.float32(np.random.uniform(-std, std, (output_shape, input_shape)))\n        self.bias = np.float32(np.random.uniform(-std, std, (output_shape,1)))\n    \n    # return the output of layer after running the input through selected activation function\n    def activation(self, inputs):\n        self.z = np.dot(self.weights,inputs) + self.bias\n        self.a = 1\/(1 + np.exp(-self.z))\n        return self.a\n    \n    # provides the derivative of activation function for the current output of the layer\n    def activation_grad(self):\n        return self.a * (1 - self.a)       ","2d33272f":"# Class Neural Network, defines multiple layers and runs forwad and back propogation to train the network.\nclass NeuralNetwork:\n    \n    # defines the shape of Network and initializes layers\n    def __init__(self, shape):\n        self.shape = shape\n        self.layers = list()\n        self.log_loss_hist = list()\n        j = 0\n        for i in shape:\n            self.layers.append(Layer(i,j))\n            j = j + 1\n        \n    # Initializes the weights and biases of our network for each layer\n    def initialize(self, X, y):\n        for i in range(len(self.layers)):\n            if i == 0:\n                self.layers[i].init_weight(X.shape[1], self.layers[i].size)\n            else:\n                self.layers[i].init_weight(self.layers[i-1].size, self.layers[i].size)\n       \n    # performs forward propogation\n    def forward_propogation(self, X):\n        a = X.T\n        for layer in self.layers:\n            a = layer.activation(a)\n    # calculates log logs and return the result\n    def loss(self, outputs, y):\n        lb = LabelBinarizer()\n        y_lb = lb.fit_transform(y)\n        outputs_lb = lb.transform(outputs)\n        loss = log_loss(y_lb, outputs_lb)\n        return loss\n    \n    # Performs the most critical, Backpropogation to calculate delta values for each layer\n    def backpropogate(self, X, y):\n        delta = list()\n        delta_w = [0 for _ in range(len(self.layers))]\n        delta_b = [0 for _ in range(len(self.layers))]\n        error_o = (self.layers[-1].z - y.T)\n        for i in reversed(range(len(self.layers) - 1)):\n            error_i = np.multiply(self.layers[i+1].weights.T.dot(error_o), self.layers[i].activation_grad())\n            delta_w[i+1] = error_o.dot(self.layers[i].a.T)\/len(y)\n            delta_b[i+1] = np.sum(error_o, axis=1, keepdims=True)\/len(y)\n            error_o = error_i\n        delta_w[0] = error_o.dot(X)\n        delta_b[0] = np.sum(error_o, axis=1, keepdims=True)\/len(y)\n        return (delta_w, delta_b)\n        \n    # Uses the delta values to update weights and biases\n    def update_weights_bias(self, delta_w, delta_b, lr):\n        #print(self.layers[0].bias.shape)\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            layer.weights = layer.weights - (lr*delta_w[i])\n            layer.bias = layer.bias - (lr*delta_b[i]) \n    \n    # Used to orchestrate the training of network, given a certain epoch and learning rate\n    def train(self, X, y, epochs, batch_size, lr):\n        self.initialize(X, y)\n        lb = LabelBinarizer()\n        y_lb = lb.fit_transform(y)\n        y_lb = lb.fit_transform(y)\n        for i in range(epochs):\n            low = 0\n            high = low + batch_size\n            self.log_loss_hist.append(self.loss(np.argmax(self.predict(X), axis=0), y))\n            while(low < X.shape[0]):\n                X_bat = X[low:high,:]\n                y_bat = y_lb[low:high]\n                self.forward_propogation(X_bat)\n                outputs = self.layers[-1].a\n                delta_w, delta_b = self.backpropogate(X_bat, y_bat)\n                self.update_weights_bias(delta_w, delta_b, lr)\n                low = high\n                if (low + batch_size) > X.shape[0]:\n                    high = X.shape[0]\n                else:\n                    high = low + batch_size\n\n    # Runs the input through the network and returns\n    def predict(self, X):\n        a = X.T\n        for layer in self.layers:\n            a = layer.activation(a)\n        return a","b49fd368":"# Define the model and its architecture\nmodel = NeuralNetwork((64,32,16,10))","97261ea3":"# Train our model using the hyper-parameters\nmodel.train(X_train, y_train, 1000, 64, 0.1)","32413a70":"# Plot the log loss over epochs\nfig = sns.lineplot(x=list(range(len(model.log_loss_hist))), y=model.log_loss_hist)\nfig = plt.xlabel('Epochs')\nfig = plt.ylabel('Log Loss')","2cbc9038":"# Make the prediction on test data and measure performance\ny_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=0)\nprint(f\"Test Accuracy Score: {np.round(accuracy_score(y_test, y_pred)*100,2)}\")","59e3d0d2":"# Plot the wrongly predicted images, to get an idea of mistakes model is making\nfig, ax = plt.subplots(nrows=1, ncols=8, figsize=(16,2))\nwrongs = y_pred != y_test\nsamp = X_test[wrongs]\ny_samp_true = y_test[wrongs]\ny_samp_pred = y_pred[wrongs]\nn = len(samp)\nfor i in range(8):\n    idx = np.random.randint(0, n)\n    img = np.reshape(samp[idx]*16, (8,8))\n    ax[i].imshow(img, cmap='gray_r')\n    ax[i].set_title(f\"Acutual: {y_samp_true[idx]}\\nPrediction: {y_samp_pred[idx]}\")","11c90041":"### Theory\n\nI will try to document as much theory I can, but i suggest going through below  Lectures and Notes to get a good ideas into the maths going on behind the algo.\n\n<ul>\n    <li><b><a href='https:\/\/www.youtube.com\/watch?v=MfIjxPh6Pys'>Introduction to Neural Networks: <\/a><\/b><\/li>\n    <li><b><a href='https:\/\/www.youtube.com\/watch?v=zUazLXZZA2U&t=1872s'>Backpropogation: <\/a><\/b><\/li>\n    <li><b><a href='http:\/\/cs229.stanford.edu\/notes2020spring\/cs229-notes-deep_learning.pdf'>Deep Learning Notes<\/a><\/b><\/li>\n<\/ul>","9fe30e89":"### Thanks for Reading","f01cb94f":"# Conclusion","63163ce2":"So I have been working with neural networks for quite some time now, but I always felt I had a knowledge gap in terms of my understanding of Neural Networks and Backpropogation. This gave me some major Imposter Syndrome issues over time.\n\n\nSo this time I decided to go all the way through and learn the maths behind the algorithms and implement a rudimentry form of neural network, which can make useful predictions.\n\nI know this is not even close to perfect and I welcome any suggestions or feedback to improve upon it.","46b09dbc":"# Building a Neural Net from Scratch using Numpy","2446c4bd":"### I always thought that building something like this, was perhaps to much for me, until.... I built it."}}