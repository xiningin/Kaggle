{"cell_type":{"d8e5c9fa":"code","4921d7de":"code","dd299c74":"code","b55e0585":"code","46d3f19a":"code","ba8d2c7c":"code","a0e99aa3":"code","e49e110a":"code","26a87dc0":"code","8d29d9c7":"code","b113fa76":"code","71665a0c":"code","6bf7f4dd":"code","93659bfb":"code","dfeecf61":"code","a29cb446":"code","7931f0d5":"code","d5581e69":"code","2fdf661f":"code","f29861cf":"code","98c044d5":"code","34b4359f":"code","aa69e563":"code","5e1885b0":"code","51ebe6c2":"code","bbedb1d6":"code","fa173d1c":"code","66d1abda":"code","fb42b5a6":"markdown","d00d9f0a":"markdown","af2d16e1":"markdown","a167074b":"markdown","d0ac3914":"markdown","a1e749c6":"markdown","09ee3939":"markdown","52f86330":"markdown"},"source":{"d8e5c9fa":"%%capture\n\n# Dirty code to make it work\n\nimport sys\n!cp -r ..\/input\/openai-clip\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')\n\n!pip install ..\/input\/openai-clip\/ftfy-5.9\/ftfy-5.9 \\\n             ..\/input\/openai-clip\/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/openai-clip\/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/faiss-163\/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","4921d7de":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\nfrom triplet_loss import TripletLoss\n\n%matplotlib inline","dd299c74":"df_test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv', index_col='posting_id')","b55e0585":"# Run train only for commit\nRUN_ON_TRAIN = len(df_test) == 3","46d3f19a":"_tokenizer = SimpleTokenizer()\n\n# Copied from https:\/\/github.com\/openai\/CLIP\/blob\/beba48f35392a73c6c47ae67ddffced81ad1916d\/clip\/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","ba8d2c7c":"# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9.\/]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","a0e99aa3":"class RollingMean():\n    def __init__(self):\n        self.n = 0\n        self.mean = 0\n        \n    def update(self, value):\n        self.mean = (self.mean * self.n + value) \/ (self.n+1)\n        self.n += 1\n        \n    def result(self):\n        return self.mean","e49e110a":"class SameGroupSampler(Sampler):\n    def __init__(self, df ,ds):\n        super().__init__(ds)\n        \n        # Create a dictionary of posting_id -> index in dataset\n        self.index_to_position = dict(zip(df.index, range(len(df))))\n        \n        # Create a Series of label_group -> set(posting_id)\n        self.label_group = df.reset_index().groupby('label_group')['posting_id'].apply(set).map(sorted).map(np.array)\n        \n        # Keep original dataset probability to sample one example\n        self.label_group_proba = self.label_group.map(len)\n        self.label_group_proba \/= self.label_group_proba.sum()\n\n\n    def __len__(self):\n        return len(self.label_group)\n        \n    def __iter__(self):\n        for _ in range(len(self)):\n            # Sample one label_group\n            label_group_sample = self.label_group.sample(1, weights=self.label_group_proba).iloc[0]\n            \n            # Sample two posting_id's\n            sample1, sample2 = np.random.choice(label_group_sample, 2, replace=False)\n            \n            yield self.index_to_position[sample1]\n            yield self.index_to_position[sample2]            ","26a87dc0":"class MyDataset(Dataset):\n    def __init__(self, df, images_path, preprocess):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.preprocess = preprocess\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = self.preprocess(Image.open(self.images_path \/ row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","8d29d9c7":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","b113fa76":"def l2_normalize(features):\n    return features \/ features.norm(2, dim=1, keepdim=True)","71665a0c":"class CLIPClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.clip, self.preprocess = clip.load(\"..\/input\/openai-clip\/RN50x4.pt\", device='cpu', jit=False)\n        \n        self.embed_dim = self.clip.text_projection.shape[1]\n    \n        self.classification_head = nn.Linear(self.embed_dim, n_classes)\n    \n    def forward(self, images, texts, return_classification=False):\n        images_features = self.clip.encode_image(images)\n        texts_features = self.clip.encode_text(texts)\n\n        # Average images and text features, because CLIP was trained to align them\n        features = l2_normalize(images_features + texts_features)\n\n        if return_classification:\n            classification_output = self.classification_head(features)\n            \n            return features, classification_output\n        else:\n            return features","6bf7f4dd":"# Load train data\ntrain_images_path = Path('..\/input\/shopee-product-matching\/train_images')\n\ndf_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv', index_col='posting_id')\n\n# Convert label_group to categorical id\ndf_train['label_group'] = df_train['label_group'].astype('category').cat.codes.astype(np.int64)\nn_classes = df_train['label_group'].max()+1","93659bfb":"model = CLIPClassifier(n_classes).to(device)","dfeecf61":"dstrain = MyDataset(df_train, train_images_path, model.preprocess)\ndltrain = DataLoader(dstrain, batch_size=32, num_workers=2, sampler=SameGroupSampler(df_train, dstrain))","a29cb446":"n_epochs = 1","7931f0d5":"# optim = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-2)\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.2)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optim, 1e-3, total_steps=n_epochs * (2*len(dltrain)-1),\n                                               base_momentum=0.0, max_momentum=0.5, pct_start=0.1, div_factor=1e2, final_div_factor=1e4)\ncriterion = TripletLoss(device)\ncriterion_classification = nn.CrossEntropyLoss()","d5581e69":"for epoch in range(n_epochs):\n    with tqdm(total=2*len(dltrain)-1) as bar:\n        loss_mean = RollingMean()\n        for images, texts, targets in dltrain:\n            images = images.to(device)\n            texts = texts.to(device)\n            targets = targets.to(device)\n            \n            optim.zero_grad()\n            \n            # Generate features and classification targets\n            features, outputs = model(images, texts, return_classification=True)\n\n            # Apply Triplet SemiHardLoss\n            triplet_loss = criterion(features, targets)\n\n            # Apply CE\n            ce_loss = criterion_classification(outputs, targets)\n            \n            # Combine losses\n            loss = triplet_loss + 0.1 * ce_loss\n\n            loss.backward()\n            optim.step()\n            scheduler.step()\n\n            # Update metric and progress bar\n            loss_mean.update(loss.item())\n            bar.update()\n            bar.set_description('{:.4f}'.format(loss_mean.result()))","2fdf661f":"def find_similarities_and_indexes(df, images_path, top_n=100, features_file=None):\n    # Create pytorch Dataset\/DataLoader\n    ds = MyDataset(df, images_path, model.preprocess)\n    dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2)\n\n    # Allocate memory for features\n    features = np.empty((len(df), model.embed_dim), dtype=np.float32)\n\n    # Begin predict\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            batch_features = model(images.to(device), texts.to(device), return_classification=False)\n\n        # Average images and text features, because CLIP was trained to align them\n        features[i:i+n] = batch_features.cpu()\n\n        i += n\n\n    # Option to save these features (may be usefull to tune cut value)\n    if features_file is not None:\n        np.save(features_file, features)\n\n    # Create index\n    index = faiss.IndexFlatIP(features.shape[1])\n    index.add(features)\n\n    # Search index\n    return index.search(features, top_n)\n\n    # TODO: try range_search\n    # lims, similarities, indexes = index_test.range_search(test_features, GROUP_CUT)","f29861cf":"if RUN_ON_TRAIN:\n    # Perform search of similiar items\n    similarities, indexes = find_similarities_and_indexes(df_train, train_images_path, features_file='features-no-norm.npy')\n    \n    # `similarities` will have shape (n, 100) and will have the similarites scores for closest matches\n    # `indexes` will have shape (n, 100) and have the index closest matches.\n    # Both arrays are aligned\n\n    # Convert index to groups, will have shape (n, 100)\n    found_groups = df_train['label_group'].values[indexes]\n\n    # Check if matches are from same group. Will create a boolean vector of (n, 100)\n    is_same_group = (found_groups == df_train['label_group'].values[:, np.newaxis])\n\n    # Plot similarities score from same group and different groups\n    plt.figure(figsize=(10, 5))\n    plt.hist([similarities[is_same_group], similarities[~is_same_group]], density=False, bins=51,\n         label=['Same group', 'Different group'], histtype='stepfilled', alpha=0.75)\n    plt.xlim(0, 1)\n    plt.xlabel('Similarity score')\n    plt.legend();","98c044d5":"# SRC: https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/224782#1233338\n# With some adaptation\ndef row_wise_f1_score(y_true, y_pred):\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = y_pred.apply(lambda x: len(x)).values - tp\n    fn = y_true.apply(lambda x: len(x)).values - tp\n\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f1 = 2 * ((precision * recall) \/ (precision + recall))\n    return f1\n\n\ndef calc_score(cut_value):\n    # Apply cutoff of similarities\n    groups_are_same = (similarities > cut_value)\n\n    # Build results\n    results = []\n    for i, (group_is_same, index_result) in enumerate(zip(groups_are_same, indexes)):\n        row_results = df_train.index[index_result[group_is_same]]\n\n        # Keep found matches as a `set`\n        results.append(set(row_results))\n\n    df_results = pd.Series(results, index=df_answer.index)\n    \n    # Evaluate results\n    return row_wise_f1_score(df_answer, df_results).mean()","34b4359f":"if RUN_ON_TRAIN:\n    # Create answer dataframe. This will have posting_id on index and a set of label_group as values \n    groups = df_train.reset_index().groupby('label_group')['posting_id'].apply(set)\n    df_answer = df_train['label_group'].map(groups)\n\n    # Cut values to evaluate\n    cuts = np.linspace(0.5, 0.95, 51)\n    scores = [calc_score(c) for c in tqdm(cuts)]\n\n    # Plot curve\n    plt.plot(cuts, scores)\n    plt.xlabel('Cutoff value')\n    plt.ylabel('F1 score')\n\n    print('Best cutoff is {:.2f} with expected F1 score of {:.4f}'.format(cuts[np.argmax(scores)], max(scores)))","aa69e563":"GROUP_CUT = 0.77  # Use option `RUN_ON_TRAIN` to find this number. It will be desabled at submission time","5e1885b0":"test_images_path = Path('..\/input\/shopee-product-matching\/test_images')","51ebe6c2":"# Find similar matches\nsimilarities, indexes = find_similarities_and_indexes(df_test, test_images_path)","bbedb1d6":"# Apply cutoff of similiarites\ntest_are_same_groups = (similarities > GROUP_CUT)","fa173d1c":"# Build submission\nresults = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = set(df_test.index[index_result[test_is_same_group]])\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","66d1abda":"df_sub.to_csv('submission.csv', index=False)","fb42b5a6":"\u2600\ufe0f\u2600\ufe0f\u2600\ufe0f Have a nice day! \u2600\ufe0f\u2600\ufe0f\u2600\ufe0f","d00d9f0a":"### Sampler and dataset\n\nWe implement a sampler that ensures that in every batch, two samples of the same group are always present.\n\nThis is important in order to use Triplet SemiHardLoss (I'm using [this implementation](https:\/\/github.com\/alfonmedela\/triplet-loss-pytorch\/blob\/master\/loss_functions\/triplet_loss.py))","af2d16e1":"## Run on test","a167074b":"## Finetune CLIP on train data\n\nHere we use the triplet loss principe to ajust CLIP:\n\n![Triplet loss](https:\/\/user-images.githubusercontent.com\/18154355\/61485418-1cbb1f00-a96f-11e9-8de8-3c46eef5a7dc.png)\n\n\nWe didn't provide a validation set (yet!), so we are deliberating overfiting.","d0ac3914":"### Utility classes and functions","a1e749c6":"### Tune CUT\n\nIn this last step we will move the `cut_value` to find optimal F1-score.","09ee3939":"## Run on train\n\nIn this section we will generate features using CLIP and perform a similiarity search to find the closest matches.\n\nWe create the final set by taking away those results bellow a threshold similiarity (less 0.7)","52f86330":"## OpenAI CLIP with train\n\nThis notebook uses [OpenAI CLIP](https:\/\/github.com\/openai\/CLIP) to generate images and text features."}}