{"cell_type":{"aed2547e":"code","94cc58b9":"code","e04ee7a2":"code","27c176d3":"code","ac2c2a1d":"code","2b88dd81":"code","5a59b5b5":"code","9a9116e8":"code","9f5c64dc":"code","117022fc":"code","bfcb0988":"code","96638c75":"code","8d7b8818":"code","dbeb0b35":"code","afc9d71c":"code","569fedcc":"code","2db3b08e":"code","f3c80f71":"code","c4788178":"code","5bc17ea5":"code","c4e4406d":"code","b8f7f20e":"code","2978155c":"code","b274d4b5":"code","2eb63a43":"code","4b20c427":"code","ff2676f5":"code","e754e84e":"code","cff26821":"code","030809d3":"code","c3a73366":"code","0fae140c":"code","8a4e81b0":"code","8c1a6ebc":"code","10d8b029":"code","8f233e45":"code","43506457":"code","82c7e3a0":"code","8d7cd406":"code","6c682070":"code","f087b46b":"code","44e4438d":"code","7dc7b033":"code","b0e1ddcd":"code","318f13b3":"code","0db708bf":"code","67fbe38b":"code","403c4f90":"code","6fd11d68":"markdown","bdbdeddd":"markdown","433bd4eb":"markdown","459a1aa0":"markdown","ca58a0ad":"markdown","c515570d":"markdown","0cfa0370":"markdown","99249fda":"markdown","f5b159c1":"markdown","38fca2b7":"markdown","a6fd8c3a":"markdown","f6662b5d":"markdown","e93dd333":"markdown","330b46f1":"markdown","b52864a5":"markdown","e33a8a0e":"markdown","80147f4b":"markdown","d85b3703":"markdown","d2b98e94":"markdown","2df83f18":"markdown","3858e0ce":"markdown"},"source":{"aed2547e":"import pandas as pd\nimport numpy as np\nimport gc\nimport os","94cc58b9":"# Kaggle input path\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e04ee7a2":"# Read train data\ntrain_trans = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n\n# Read test data\ntest_trans = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")","27c176d3":"# Train data (Combine 'train_identity' and 'train_trans')\ndf_train = train_trans.merge(train_identity, how='left', left_index=True, right_index=True, on='TransactionID')\n\n# Test data (Combine 'test_identity' and 'test_trans')\ndf_test =  test_trans.merge(test_identity, how='left', left_index=True, right_index=True, on='TransactionID')","ac2c2a1d":"del train_trans, train_identity, test_trans, test_identity; x = gc.collect()","2b88dd81":"print('train data memory in MB:', df_train.memory_usage().sum() \/ 1024**2) \nprint('test data memory in MB:', df_test.memory_usage().sum() \/ 1024**2) ","5a59b5b5":"# Most often used function to reduce dataset memory\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                      props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64) \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            #print(\"dtype after: \",props[col].dtype)\n            #print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props","9a9116e8":"reduce_mem_usage(df_train)\nreduce_mem_usage(df_test)","9f5c64dc":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis = 0, sort=False)\ndf = df.reset_index()\ndf.drop('index', axis=1, inplace = True)","117022fc":"del df_train, df_test; x = gc.collect()","bfcb0988":"df.head(2)","96638c75":"# Transaction columns\ndf.columns[0:5]","8d7b8818":"# Card related columns\ndf.columns[5:11]","dbeb0b35":"#  addr, dist, emaildomain related columns\ndf.columns[11:17]","afc9d71c":"# C columns\ndf.columns[17:31]","569fedcc":"# D columns\ndf.columns[31:46]","2db3b08e":"# M columns\ndf.columns[46:55]","f3c80f71":"# V columns\ndf.columns[55:394]","c4788178":"# Identity columns\ndf.columns[394:]","5bc17ea5":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n           'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}","c4e4406d":"col = ['P_emaildomain', 'R_emaildomain']\n\nfor x in col:\n    df[x + '_bin'] = df[x].map(emails)","b8f7f20e":"from sklearn import preprocessing","2978155c":"for col in df.drop('isFraud', axis = 1).columns:    \n    if df[col].dtype == 'object':\n        le = preprocessing.LabelEncoder()\n        le.fit(list(df[col].values))\n        df[col] = le.transform(list(df[col].values))","b274d4b5":"df.memory_usage().sum() \/ 1024**2","2eb63a43":"reduce_mem_usage(df)","4b20c427":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","ff2676f5":"del df; x = gc.collect()","e754e84e":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import minmax_scale","cff26821":"v_columns = df_train.columns[55:394]\nv_columns","030809d3":"# fill NaN values and scale the data using scalar function\n\n# for train data\nfor col in v_columns:\n    df_train[col] = df_train[col].fillna((df_train[col].min() - 2))\n    df_train[col] = (minmax_scale(df_train[col], feature_range=(0,1)))\n\n# for test data\nfor col in v_columns:\n    df_test[col] = df_test[col].fillna((df_test[col].min() - 2))\n    df_test[col] = (minmax_scale(df_test[col], feature_range=(0,1)))","c3a73366":"def func_pca(df, v_columns, prefix):\n    \n    pca = PCA(n_components = 30, random_state = 1)\n    pca = pca.fit_transform(df[v_columns])\n    pca_df = pd.DataFrame(pca)\n    df.drop(v_columns, axis=1, inplace=True)\n    pca_df.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n    df = pd.concat([df, pca_df], axis=1)\n    \n    return df","0fae140c":"train = func_pca(df_train, v_columns = v_columns, prefix = 'PCA_V_')","8a4e81b0":"del df_train; x= gc.collect()","8c1a6ebc":"test = func_pca(df_test, v_columns = v_columns, prefix = 'PCA_V_')","10d8b029":"del df_test; x= gc.collect()","8f233e45":"train.info()","43506457":"test.info()","82c7e3a0":"for col in test.columns:\n    if test[col].dtype=='float64': test[col] = test[col].astype('float32')    ","8d7cd406":"for col in train.columns:\n    if train[col].dtype=='float64': train[col] = train[col].astype('float32')\n    if train[col].dtype=='int64': train[col] = train[col].astype('int32')      ","6c682070":"test.info()","f087b46b":"train.head(2)","44e4438d":"# spilt the train data for 'training' and 'validation'.\n\n# train index\nidxT = train.index[:3*len(train)\/\/4]\n\n# Validation index\nidxV = train.index[3*len(train)\/\/4:]","7dc7b033":"# only X columns\ncols = train.columns.difference(['isFraud'])\ncols","b0e1ddcd":"# Model\nimport xgboost as xgb","318f13b3":"# xgb.XGBClassifier?","0db708bf":"clf = xgb.XGBClassifier(n_estimators = 300, eval_metric = 'auc')","67fbe38b":"X_train = train.loc[idxT, cols]\ny_train = train['isFraud'][idxT]\n\nX_val = train.loc[idxV, cols]\ny_val = train['isFraud'][idxV]","403c4f90":" clf.fit(X_train, y_train,eval_set=[(X_val, y_val)],verbose=50, early_stopping_rounds=100)","6fd11d68":"### Summary\n**It's been observed that, V columns are in large number (around 340). So we can either ignore all V columns or apply PCA for all V columns in order to reduce the columns\/memory.**\n\n**In this kernel, we will apply PCA for V columns in order not to lose any information.**","bdbdeddd":"### References for feature engineering, EDA and some advance techniques.\n\nhttps:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\n\nhttps:\/\/www.kaggle.com\/alijs1\/ieee-transaction-columns-reference\n\nhttps:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\/comments\n\n#### Thank you for reading the kernel, hope you find it useful:)\n","433bd4eb":"#### Email Mappings","459a1aa0":"### Build the model","ca58a0ad":"### In this kernel, we will delete temporary storage to provide space to RAM or else session crashes due to shortage of RAM","c515570d":"### 'IEEE_CIS' Dataset is of high memory and we will explore how to handle high memory datasets in this kernel","0cfa0370":"### Understanding the columns\n1. transaction related columns\n2. card related columns\n3. addr,dist,domain related columns\n4. C columns\n5. D columns\n6. M columns\n7. V columns\n8. others ('identity' columns along with 'device' information)","99249fda":"## Combine train and test data","f5b159c1":"### As the memory of train and test dataset is high and our RAM space is low, we will try to reduce the dataset memory","38fca2b7":"### 'train' and 'test' dataset Memory","a6fd8c3a":"### Conclusion: \n1. Train dataset memory reduced from '1955' MB to '546' MB\n2. Test  dataset memory reduced from '1673' MB to '459' MB","f6662b5d":" # 'IEEE-CIS' Fraud Detection Data\n \n \n \n #### The data is broken into two files identity and transaction, which are joined by TransactionID.\n \n #### Categorical Features - Transaction\n\n1. ProductCD\n2. emaildomain\n3. card1 - card6\n4. addr1, addr2\n5. P_emaildomain\n6. R_emaildomain\n7. M1 - M9\n\n#### Categorical Features - Identity\n\n1. DeviceType\n2. DeviceInfo\n3. id_12 - id_38\n \n \n \n \n","e93dd333":"### Combine 'transaction' and 'identity' data","330b46f1":"### Label Encoding","b52864a5":"### Read train(transaction and identity) and test(transaction and identity) data","e33a8a0e":"## Conclusion: \n\n#### Without using any feature engineering or optimizing the model, we have acheived validation accuracy around 90% (which is not bad). \n\n#### Now the model is ready and we can predict the 'Y' for test data.","80147f4b":"### I have tried to apply PCA for V columns using the entire dataset 'df' at once, but session crashed due to low RAM.\n\n### Kaggle has RAM usage of 13GB. So i need to split the data and perform PCA","d85b3703":"### Split the data back into train and test ","d2b98e94":"### Applying PCA for V columns","2df83f18":"### Summary\n\n**Still test dataset is having 1.2 GB memory. Function that we have used to reduce the memory is not effective.**\n\n**There is a simple approach to reduce dataset memory. Just convert float64 into float32 and int64 into int32**","3858e0ce":"### Conclusion: Almost 50% of test dataset memory is reduced with simple approach"}}