{"cell_type":{"c58ffd9f":"code","b78d4d85":"code","0b3c3f20":"code","08a12dd7":"code","9b470173":"code","d45c03ac":"code","58b5e791":"code","93f62be7":"code","f98285a0":"code","573f6ea6":"code","9520dee2":"code","9735c88d":"code","df28ea1d":"code","2c1c6545":"code","7860992a":"code","a7ee202f":"code","2d688328":"code","bba0d6a6":"code","f619a573":"code","0b5eea43":"code","d664e844":"code","fef3ac38":"code","8cc3d609":"code","edd3174d":"code","9d30befd":"code","c4d98958":"code","c29e2c08":"code","04396657":"code","414d572f":"code","1ba66797":"code","4babb05f":"code","5fc12928":"code","49be6ab6":"code","0d46bc44":"code","b5287ce8":"code","d284607f":"code","6a4de48f":"code","06f6823b":"code","1c79daf8":"code","0d192176":"code","35eefff0":"code","0eec7b85":"code","ff62dda7":"code","b71ab559":"code","a81c3341":"code","bd4863fa":"code","fd3e997a":"code","ab98c735":"code","8ed53bd4":"code","ca77af12":"code","abd7ef1b":"code","4129e16b":"code","c0162bb7":"code","6b1b1785":"code","cf251eac":"code","2dbde8c8":"code","29d424c8":"code","16e4dfc3":"code","f9694a45":"code","39bd1e9c":"code","c2b7358e":"code","cef82de2":"code","159874d5":"code","a845d04a":"code","497571d1":"code","d61bd6ee":"code","cb2b25fc":"code","025848eb":"code","519de15c":"code","e7f4ff67":"code","9a7cecdb":"code","009dad4f":"code","87237b2e":"code","520f9dba":"code","db7acf0b":"code","bea66ec5":"code","c046b5c4":"code","d7661c13":"code","00acb9aa":"code","28b35f3a":"code","3dabd214":"code","379ebe20":"code","083ece86":"code","c814ee77":"markdown","52c26d34":"markdown"},"source":{"c58ffd9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b78d4d85":"counntries_df = pd.read_csv(\"\/kaggle\/input\/countries-data\/enriched_covid_19.csv\")","0b3c3f20":"train_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/submission.csv\")","08a12dd7":"train_df['Date'].max()","9b470173":"train_df.describe()\n#train_df.info()","d45c03ac":"train_df['ConfirmedCases']=train_df['ConfirmedCases'].replace([-1], 0)\ntrain_df['Fatalities']=train_df['Fatalities'].replace([-1], 0)","58b5e791":"train_df[train_df['ConfirmedCases']==-1]","93f62be7":"train_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest_df[\"key\"]=test_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\nkey=\"key\"\ndef rate(frame, key, target, new_target_name=\"rate\"):\n   \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value\/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)","f98285a0":"all_data_train_test=train_df","573f6ea6":"all_data_train_test['mean_rate_case_last7']=1.00000000\nall_data_train_test['mean_rate_case_last3']=1.00000000\n\nall_data_train_test['mean_rate_case_last10']=1.00000000\nall_data_train_test['mean_rate_fat_last10']=1.00000000\n\nall_data_train_test['mean_rate_case_last5']=1.00000000\nall_data_train_test['mean_rate_fat_last5']=1.00000000\n\nall_data_train_test['mean_rate_case_last14']=1.00000000\nall_data_train_test['mean_rate_fat_last14']=1.00000000\n\nall_data_train_test['mean_rate_case_last20']=1.00000000\nall_data_train_test['mean_rate_fat_last20']=1.00000000\n\nall_data_train_test['mean_rate_case_each7']=1.00000000\nall_data_train_test['mean_rate_case_each3']=1.00000000\nall_data_train_test['mean_rate_case_each9']=1.00000000\nall_data_train_test['mean_rate_case_each10']=1.00000000\nall_data_train_test['mean_rate_case_each13']=1.00000000\n\nall_data_train_test['mean_rate_fat_last7']=1.00000000\nall_data_train_test['mean_rate_fat_last3']=1.00000000\nall_data_train_test['mean_rate_fat_each7']=1.00000000\nall_data_train_test['mean_rate_fat_each3']=1.00000000\nall_data_train_test['mean_rate_fat_each9']=1.00000000\nall_data_train_test['mean_rate_fat_each10']=1.00000000\nall_data_train_test['mean_rate_fat_each13']=1.00000000\n\nall_data_train_test['max_rate_case']=1.00000000\nall_data_train_test['min_rate_case']=1.00000000\nall_data_train_test['std_rate_case']=1.00000000\nall_data_train_test['mode_rate_case']=1.00000000\nall_data_train_test['range_rate_case']=1.00000000\nall_data_train_test['max_to_min_rate_case']=1.00000000\nall_data_train_test['max_rate_fat']=1.00000000\nall_data_train_test['min_rate_fat']=1.00000000\nall_data_train_test['std_rate_fat']=1.00000000\nall_data_train_test['mode_rate_fat']=1.00000000\nall_data_train_test['mean_rate_case']=1.00000000\nall_data_train_test['mean_rate_fat']=1.00000000\nall_data_train_test['range_rate_fat']=1.00000000       \nall_data_train_test['max_to_min_rate_fat']=1.00000000   \nall_data_train_test['cases_prev']=0  \nall_data_train_test['fat_prev']=0   \nall_data_train_test['mean_rate_fat_each10']=1.00000000","9520dee2":"rate(all_data_train_test, key, target1, new_target_name=\"rate_\" +target1)\nrate(all_data_train_test, key, target2, new_target_name=\"rate_\" +target2)\nall_data_train_test.head()\nall_data_train_test.info()","9735c88d":"def mean_rate_each(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each7=[1.0 for k in range (len(target1))]\n    j=6\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3],target1[i+j -4],target1[i+j -5],target1[i+j -6],target1[i+j -7]]\n           \n            mean_rate_case_each7[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each7)\ndef mean_rate_each3(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each3=[1.0 for k in range (len(target1))]\n    j=2\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3]]\n           \n            mean_rate_case_each3[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each3)            \n    \ndef mean_rate_each10(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each10=[1.0 for k in range (len(target1))]\n    j=9\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3],target1[i+j -4],target1[i+j -5],target1[i+j -6],target1[i+j -7],target1[i+j -8],target1[i+j -9],target1[i+j -10]]\n           \n            mean_rate_case_each10[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each10)  \ndef mean_rate_each13(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each13=[1.0 for k in range (len(target1))]\n    j=12\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3],target1[i+j -4],target1[i+j -5],target1[i+j -6],target1[i+j -7],target1[i+j -8],target1[i+j -9],target1[i+j -10],target1[i+j -11],target1[i+j -12],target1[i+j -13]]\n           \n            mean_rate_case_each13[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each13)\n    \ndef mean_rate_each9(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each9=[1.0 for k in range (len(target1))]\n    j=8\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3],target1[i+j -4],target1[i+j -5],target1[i+j -6],target1[i+j -7],target1[i+j -8],target1[i+j -9]]\n           \n            mean_rate_case_each9[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each9)","df28ea1d":"group_keys = all_data_train_test['key'].values.tolist()\nfor coc in group_keys:\n    mean_rate_each(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each7')\n    mean_rate_each(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each7')\n    mean_rate_each3(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each3')\n    mean_rate_each3(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each3')\n    mean_rate_each13(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each13')\n    mean_rate_each13(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each13')\n    mean_rate_each9(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each9')\n    mean_rate_each9(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each9')\n    mean_rate_each10(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each10')\n    mean_rate_each10(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each10')\n    ","2c1c6545":"all_data_train_test['Province_State'].fillna('NONE', inplace=True)\nall_data_train_test['ConfirmedCases'].fillna(0, inplace=True)\nall_data_train_test['Fatalities'].fillna(0, inplace=True)\nall_data_train_test['Id'].fillna(-1, inplace=True)\n#all_data_train_test['ForecastId'].fillna(-1, inplace=True)","7860992a":"def infection_from_first_confirmed_cases(train_df,country):\n    \n    confirmed_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\n    total_cases = confirmed_cases.join(fatalities_cases)\n    country_cases = [i for i in total_cases.ConfirmedCases['sum'].values]\n    country_cases_filter = country_cases[0:70] \n       \n    return country_cases_filter ","a7ee202f":"#get the mean foe last seven vlaues for fatalities and confirmed cases\ngroup_keys = all_data_train_test['key'].values.tolist()\nfor k in group_keys:\n    all_data_train_test['mean_rate_case_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(7).mean()\n    all_data_train_test['mean_rate_fat_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(7).mean()\n    all_data_train_test['mean_rate_case_last3'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(3).mean()\n    all_data_train_test['mean_rate_fat_last3'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(3).mean()\n    all_data_train_test['std_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].std()\n    all_data_train_test['max_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].max()\n    all_data_train_test['min_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].min()\n    all_data_train_test['mode_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].mode()\n    all_data_train_test['mean_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].mean()\n    all_data_train_test['std_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].std()\n    all_data_train_test['max_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].max()\n    all_data_train_test['min_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].min()\n    all_data_train_test['mode_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].mode()\n    all_data_train_test['mean_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].mean()\n   \n    all_data_train_test['mean_rate_case_last10'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(10).mean()\n    all_data_train_test['mean_rate_fat_last10'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(10).mean()\n    all_data_train_test['mean_rate_case_last5'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(5).mean()\n    all_data_train_test['mean_rate_fat_last5'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(5).mean()\n\n    all_data_train_test['mean_rate_case_last14'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(14).mean()\n    all_data_train_test['mean_rate_fat_last14'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(14).mean()\n    all_data_train_test['mean_rate_case_last20'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(20).mean()\n    all_data_train_test['mean_rate_fat_last20'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(20).mean()\n    \n    \n    \n   # all_data_train_test['max_to_min_rate_case'][all_data_train_test['key']==k]=(all_data_train_test['max_rate_case'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)])\/(all_data_train_test['min_rate_case'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)])\n    #all_data_train_test['max_to_min_rate_fat'][all_data_train_test['key']==k]=(all_data_train_test['max_rate_fat'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)])\/(all_data_train_test['min_rate_fat'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)])\n    \n#all_data_train_test['cases_prev']=all_data_train_test['ConfirmedCases'].shift()\n#all_data_train_test['fat_prev']=all_data_train_test['Fatalities'].shift()                   \n\nall_data_train_test['max_to_min_rate_case']=all_data_train_test['max_rate_case']\/all_data_train_test['min_rate_case']\nall_data_train_test['max_to_min_rate_fat']=all_data_train_test['max_rate_fat']\/all_data_train_test['min_rate_fat']","2d688328":"all_data_train_test['ConfirmedCases'][all_data_train_test['ConfirmedCases']==-1]","bba0d6a6":"train_df['Date'].max()","f619a573":"all_data_train_test['cases_prev'] = all_data_train_test.groupby(\"key\")[\"ConfirmedCases\"].shift()\nall_data_train_test['fat_prev'] = all_data_train_test.groupby(\"key\")[\"Fatalities\"].shift()","0b5eea43":"for lag in range(1, ((train_df['Date'].max()-train_df['Date'].min()).days)):            \n    all_data_train_test[f\"lag_{lag}_cc\"] = all_data_train_test.groupby(\"key\")[\"ConfirmedCases\"].shift(lag)\n    all_data_train_test[f\"lag_{lag}_ft\"] = all_data_train_test.groupby(\"key\")[\"Fatalities\"].shift(lag)\n ","d664e844":"all_data_train_test[['mean_rate_case_last7','ConfirmedCases','rate_ConfirmedCases']][all_data_train_test['Country_Region']=='Qatar']","fef3ac38":"\nall_data_train_test[all_data_train_test['Country_Region']=='Qatar'].tail(50)","8cc3d609":"counntries_df.rename(columns={'Country\/Region':'Country_Region','Province\/State':'Province_State'},inplace = True)\ncounntries_df[\"key\"]=counntries_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n","edd3174d":"counntries_df.columns","9d30befd":"contries_data_columns=['Province_State', 'Country_Region', 'Lat', 'Long', 'Date',\n       'ConfirmedCases', 'Fatalities', 'age_0-4', 'age_5-9', 'age_10-14','key',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']","c4d98958":"grouped_countries=counntries_df.groupby(['Country_Region'], as_index=False).agg({\"Lat\": \"max\",\"Long\": \"max\",'total_pop': \"max\",\n                                                                                         'smokers_perc': \"max\",'density': \"max\",'urbanpop': \"max\",\n                                                                                         'hospibed': \"max\",'lung': \"max\",'femalelung': \"max\",\n                                                                                         'malelung': \"max\",'restrictions': \"max\",'quarantine': \"max\",\n                                                                                         'schools': \"max\",'age_0-4': \"max\",'age_100+': \"max\",'age_5-9': \"max\",\n                                                                                         'age_95-99': \"max\",'age_90-94': \"max\",'age_85-89': \"max\",\n                                                                                         'age_80-84': \"max\",'age_75-79': \"max\",'age_70-74': \"max\",\n                                                                                         'age_65-69': \"max\",'age_60-64': \"max\",'age_55-59': \"max\",\n                                                                                         'age_50-54': \"max\",'age_45-49': \"max\",'age_40-44': \"max\",\n                                                                                         'age_35-39': \"max\",'age_30-34': \"max\",'age_25-29': \"max\",\n                                                                                         'age_20-24': \"max\",'age_15-19': \"max\",'age_10-14': \"max\",\n                                                                                         })","c29e2c08":"grouped_countries=counntries_df.groupby(['key'], as_index=False).agg({\"Lat\": \"max\",\"Long\": \"max\",'total_pop': \"max\",\n                                                                                         'smokers_perc': \"max\",'density': \"max\",'urbanpop': \"max\",\n                                                                                         'hospibed': \"max\",'lung': \"max\",'femalelung': \"max\",\n                                                                                         'malelung': \"max\",'restrictions': \"max\",'quarantine': \"max\",\n                                                                                         'schools': \"max\",'age_0-4': \"max\",'age_100+': \"max\",'age_5-9': \"max\",\n                                                                                         'age_95-99': \"max\",'age_90-94': \"max\",'age_85-89': \"max\",\n                                                                                         'age_80-84': \"max\",'age_75-79': \"max\",'age_70-74': \"max\",\n                                                                                         'age_65-69': \"max\",'age_60-64': \"max\",'age_55-59': \"max\",\n                                                                                         'age_50-54': \"max\",'age_45-49': \"max\",'age_40-44': \"max\",\n                                                                                         'age_35-39': \"max\",'age_30-34': \"max\",'age_25-29': \"max\",\n                                                                                         'age_20-24': \"max\",'age_15-19': \"max\",'age_10-14': \"max\",\n                                                                                         })","04396657":"merged_train_df = pd.merge(all_data_train_test,\n                 grouped_countries[['Lat', 'Long',\n        'age_0-4', 'age_5-9', 'age_10-14','key',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']],\n                 on='key',how='left')","414d572f":"merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) ","1ba66797":"merged_train_df['confirmed_to_pop']=merged_train_df['ConfirmedCases']\/(merged_train_df['total_pop']*1000)\nmerged_train_df['fat_to_pop']=merged_train_df['Fatalities']\/(merged_train_df['total_pop']*1000)","4babb05f":"merged_train_df[['confirmed_to_pop','ConfirmedCases','total_pop']][merged_train_df['Country_Region']=='Saudi Arabia']","5fc12928":"merged_train_df.info()","49be6ab6":"merged_train_df.columns","0d46bc44":"merged_train_df[merged_train_df.isnull().any(axis=1)]","b5287ce8":"merged_train_df['Province_State'].fillna('NONE', inplace=True)","d284607f":"merged_train_df.fillna(0, inplace=True)","6a4de48f":"merged_train_df.head()","06f6823b":"number_c = merged_train_df['Country_Region']\ncountries = (merged_train_df['Country_Region'])\ncountry_dict = dict(zip(countries, number_c)) \ncountry_dict","1c79daf8":"merged_train_df.columns","0d192176":"merged_train_df['Date_num'] = pd.to_datetime(merged_train_df['Date'])\nmerged_train_df['Date_num'] = merged_train_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_train_df['Date_num']","35eefff0":"merged_test_df['Date_num'] = pd.to_datetime(merged_test_df['Date'])\nmerged_test_df['Date_num'] = merged_test_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_test_df['Date_num']","0eec7b85":"merged_train_df['Date_num']=merged_train_df['Date_num'].astype('int')\nmerged_test_df['Date_num']=merged_test_df['Date_num'].astype('int')\nmerged_train_df.info()","ff62dda7":"merged_train_df.columns","b71ab559":"#'rate_ConfirmedCases','rate_Fatalities',\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmerged_train_df[['mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'cases_prev', 'fat_prev', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10', 'age_0-4',\n       'age_5-9', 'age_10-14', 'age_15-19', 'age_20-24', 'age_25-29',\n       'age_30-34', 'age_35-39', 'age_40-44', 'age_45-49', 'age_50-54',\n       'age_55-59', 'age_60-64', 'age_65-69', 'age_70-74', 'age_75-79',\n       'age_80-84', 'age_85-89', 'age_90-94', 'age_95-99', 'age_100+',\n       'total_pop', 'smokers_perc', 'density', 'urbanpop', 'hospibed', 'lung',\n       'femalelung', 'malelung', 'restrictions', 'quarantine', 'schools']] = mms.fit_transform(merged_train_df[['mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'cases_prev', 'fat_prev', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10', 'age_0-4',\n       'age_5-9', 'age_10-14', 'age_15-19', 'age_20-24', 'age_25-29',\n       'age_30-34', 'age_35-39', 'age_40-44', 'age_45-49', 'age_50-54',\n       'age_55-59', 'age_60-64', 'age_65-69', 'age_70-74', 'age_75-79',\n       'age_80-84', 'age_85-89', 'age_90-94', 'age_95-99', 'age_100+',\n       'total_pop', 'smokers_perc', 'density', 'urbanpop', 'hospibed', 'lung',\n       'femalelung', 'malelung', 'restrictions', 'quarantine', 'schools']])","a81c3341":"train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv')","bd4863fa":"train.info()","fd3e997a":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train.fillna(0, inplace=True)\n    train['ConfirmedCases']=train['ConfirmedCases'].replace([-1], 0)\n    train['Fatalities']=train['Fatalities'].replace([-1], 0)\n    train\n\n    test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test.fillna(0, inplace=True)\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period): #63,5,14\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)#(59,1,64)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n#14,6,5,14\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 7\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)  #14\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n#'2020-03-27', '2020-03-24', '2020-03-21', '2020-03-18'\n#'2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13'\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('..\/submissions\/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta  #64\n        last_train = start_val - 1   #63\n        num_val = max_test_val_day - start_val + 1 #11\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14  #14\n        train_data = get_dataset(last_train, num_train, lag_period)#63,5,14\n        valid_data = get_dataset(start_val, 1, lag_period) #64,1,14\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub","ab98c735":"merged_train_df.head()\n","8ed53bd4":"from datetime import datetime as dt, timedelta\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n","ca77af12":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","abd7ef1b":"SEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }\n","4129e16b":"'mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'cases_prev', 'fat_prev', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10', 'age_0-4',\n     \n       'total_pop', 'smokers_perc', 'density'","c0162bb7":"def get_nn_sub():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    df['ConfirmedCases']=df['ConfirmedCases'].replace([-1], 0)\n    df['Fatalities']=df['Fatalities'].replace([-1], 0)\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n    sub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n    df[\"key\"]=df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n    features_columns = ['Id', 'Province_State', 'Country_Region', 'Date', 'ConfirmedCases',\n       'Fatalities','mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','Lat','Long',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'cases_prev', 'fat_prev', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10',  'total_pop', 'smokers_perc', 'density']\n\n    '''coo_df = merged_train_df.groupby(\"Country_Region\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','cases_prev', 'fat_prev',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()'''\n    \n    coo_df = merged_train_df.groupby(\"Country_Region\")[['mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','Lat','Long',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'cases_prev', 'fat_prev', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10', 'total_pop', 'smokers_perc', 'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n    #loc_group = [\"key\"]\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n  #  df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        #df = df.merge(coo_df, how=\"left\", on=\"key\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n    merged_train_df2=pd.DataFrame(columns=features_columns)\n    merged_train_df2=merged_train_df[features_columns].copy()\n    #df = merged_train_df\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\n\n \n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 7\n#'1-smokers_perc','density'\n#2-'smokers_perc','density','mean_rate_case_last7'\n#3-'smokers_perc','density','mean_rate_case_last7','mean_rate_fat_last7'\n#4-'smokers_perc','density','mean_rate_fat_last7'\n#5-'mean_rate_case_last7','mean_rate_fat_last7'\n#6- Lat and Long\n#7-'mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case','std_rate_fat','density','Lat','Long'\n#8----'mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case', 'std_rate_fat','mean_rate_case', 'mean_rate_fat','rate_ConfirmedCases','rate_Fatalities'\n#9-'std_rate_case','std_rate_fat','mean_rate_case', 'mean_rate_fat','rate_ConfirmedCases','rate_Fatalities'\n#10--------'mean_rate_case_last7', 'mean_rate_case_each7',  'mean_rate_fat_last7', 'mean_rate_fat_each7',   'rate_ConfirmedCases', 'rate_Fatalities'\n#11    'mean_rate_case_last7', 'mean_rate_case_each7','std_rate_case','std_rate_fat', 'max_to_min_rate_case', 'max_to_min_rate_fat',       'mean_rate_fat_last7', 'mean_rate_fat_each7'\n#12-----'mean_rate_case_last7', 'mean_rate_case_each7','mean_rate_case_last3','mean_rate_fat_last3', 'max_to_min_rate_case','max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities', 'mean_rate_case', 'mean_rate_fat','std_rate_fat','std_rate_case',              'mean_rate_fat_last7', 'mean_rate_fat_each7   \n    # we have to tune\n    \n    features = ['mean_rate_case_last7', 'mean_rate_case_last3',\n       'mean_rate_case_last10', 'mean_rate_fat_last10', 'mean_rate_case_last5',\n       'mean_rate_fat_last5', 'mean_rate_case_last14', 'mean_rate_fat_last14',\n       'mean_rate_case_last20', 'mean_rate_fat_last20', 'mean_rate_case_each7',\n       'mean_rate_case_each3', 'mean_rate_case_each9', 'mean_rate_case_each10',\n       'mean_rate_case_each13', 'mean_rate_fat_last7', 'mean_rate_fat_last3',\n       'mean_rate_fat_each7', 'mean_rate_fat_each3', 'mean_rate_fat_each9',\n       'mean_rate_fat_each20', 'mean_rate_fat_each13', \n      'rate_ConfirmedCases',\n       'rate_Fatalities', 'mean_rate_fat_each10']\n  \n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 256,0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,128, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer,64, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer,128, 0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,64, 0.1, \"relu\") #0.05\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 128,0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer,32, 0.0, \"sigmoid\")\n\n        hidden_layer = nn_block(hidden_layer,32, 0.2, \"relu\") #0.05\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 256, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 64, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 64, 0, \"relu\")\n        hidden_layer = nn_block(hidden_layer, 64, 0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")#64\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")#32\n        hidden_layer = nn_block(hidden_layer, 32, 0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]","6b1b1785":"sub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')","cf251eac":"sub2 = get_nn_sub()","2dbde8c8":"sub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)","29d424c8":"from sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]","16e4dfc3":"sub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.00+ np.log1p(sub2[t].values)*1).astype('int')\n    \nsub_df.to_csv(\"submission.csv\", index=False)","f9694a45":"Predicted_data = pd.merge(sub_df ,\n                 test_df[['ForecastId','Province_State','Country_Region','Date']],\n                 on='ForecastId',how='left')","39bd1e9c":"target1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\nkey=\"Country_Region\"\ndef trend(frame, key, target, new_target_name=\"trend\"):\n   \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    trend=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     trend[i]=current_value-previous_value\n\n                 \n        trend[i] =max(1,trend[i] )#correct negative values\n\n    frame[new_target_name] = np.array(trend)\ntrend(Predicted_data, key, target1, new_target_name=\"trend_\" +target1)\ntrend(Predicted_data, key, target2, new_target_name=\"trend_\" +target2)    ","c2b7358e":"Predicted_data[Predicted_data['Country_Region']=='Qatar']\n","cef82de2":"confirmed_total_date = Predicted_data.groupby(['Date']).agg({'trend_ConfirmedCases':'sum','ConfirmedCases':'sum'})\nfatalities_total_date = Predicted_data.groupby(['Date']).agg({'trend_Fatalities':'sum','Fatalities':'sum'})\nprint(confirmed_total_date)\nprint(fatalities_total_date)","159874d5":"confirmed_date = train_df.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_date = train_df.groupby(['Date']).agg({'Fatalities':['sum']})\nprint(confirmed_date)\nprint(fatalities_date)","a845d04a":"train_df[train_df['Country_Region']=='Qatar'].tail(10)","497571d1":"def countryplot(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(country + \" Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(country+\" Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","d61bd6ee":"countrylist=['Qatar','Italy','Spain','United Arab Emirates','Egypt','Germany','France','Korea, South','Turkey','US','Saudi Arabia','United Kingdom']\nfor co in countrylist:\n    countryplot(Predicted_data,co)\n    #country='Italy'\n","cb2b25fc":"def countryplottotal(train_df):\n      \n    confirmed_total = train_df.groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df.groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(\"World Wide Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(\"World Wide Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","025848eb":"countryplottotal(Predicted_data)","519de15c":"def countryplothist(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'trend_ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'trend_Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(kind='bar',ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(country + \" Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(kind='bar',ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(country+\" Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","e7f4ff67":"countrylist=['Qatar','Italy','Spain','United Arab Emirates','Egypt','Germany','France','Korea, South','Turkey','US','Saudi Arabia','United Kingdom']\nfor co in countrylist:\n    countryplothist(Predicted_data,co)","9a7cecdb":"featureslist","009dad4f":"def func(x):\n    try:\n        x_new = x['Country_Region'] + \"_\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new","87237b2e":"train_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/submission.csv\")\n\ntrain_df['ConfirmedCases']=train_df['ConfirmedCases'].replace([-1], 0)\ntrain_df['Fatalities']=train_df['Fatalities'].replace([-1], 0)\ntrain_df['key'] = train_df.apply(lambda x: func(x), axis=1)\n#train_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"\" + str(row[1]),axis=1)\n\n#train_df['cases_prev']=train_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n#train_df['fat_prev']=train_df.groupby(\"key\")[\"Fatalities\"].shift()\n\ntrain_df['cases_prev'] = train_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\ntrain_df['fat_prev'] = train_df.groupby(\"key\")[\"Fatalities\"].shift()\n\nsub_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n#sub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"\" + str(row[1]),axis=1)\nsub_df['key'] = sub_df.apply(lambda x: func(x), axis=1)\ncoo_d = merged_train_df.groupby('key')[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\nloc_group = ['key']\ndef preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n     #   df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_d, how=\"left\", on='key')\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n        key_dummies = pd.get_dummies(df['key'])\n        df = pd.concat([df,key_dummies],axis=1)\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n\n#x_train = merged_train_df2\nsub_df=preprocess(sub_df)\nx_train = preprocess(train_df)\nx_test = sub_df\nx_train.loc[x_train[\"Date\"]<x_test['Date'].min(),\"split\"] = \"train\"\nx_train.loc[x_train[\"Date\"]>=x_test['Date'].min(),\"split\"] = \"test\"       \n\nfeatureslist=x_train.columns.unique().tolist()\nfeatures=['mean_rate_case_last7',\n 'mean_rate_case_each7',\n 'mean_rate_fat_last7',\n 'mean_rate_fat_each7',\n 'max_rate_case',\n 'min_rate_case',\n 'std_rate_case',\n 'mode_rate_case',\n 'range_rate_case',\n 'mean_rate_case_last3',\n 'mean_rate_fat_last3',\n 'max_to_min_rate_case',\n 'max_rate_fat',\n 'min_rate_fat',\n 'std_rate_fat',\n 'mode_rate_fat',\n 'mean_rate_case',\n 'mean_rate_fat',\n 'range_rate_fat',\n 'Lat',\n 'Long',\n 'max_to_min_rate_fat',\n 'rate_ConfirmedCases',\n 'rate_Fatalities',\n 'total_pop',\n 'smokers_perc',\n 'density',\n ]\ndef train_model(df, label, base_label, features=features, **kwargs):\n    X_train = df.loc[df[\"split\"] == \"train\"][features]\n    y_train = np.log(df.loc[df[\"split\"] == \"train\"][label] + 1)\n    b_train = np.log(df.loc[df[\"split\"] == \"train\"][base_label] + 1)\n    X_test = df.loc[df[\"split\"] == \"test\"][features]\n    y_test = np.log(df.loc[df[\"split\"] == \"test\", label] + 1)\n    b_test = np.log(df.loc[df[\"split\"] == \"test\", base_label] + 1)\n    print(kwargs)\n    model = lgb.LGBMRegressor(**kwargs)\n    model.fit(X_train, y_train, init_score = b_train)\n    y_pred = model.predict(X_test)\n    print(np.sqrt(mean_squared_error(y_test, y_pred + b_test)))\n   # print(len(X_test))\n    return model\n\n\n\n\n\n","520f9dba":"lgb_model_cases = train_model(x_train,\"ConfirmedCases\",'cases_prev',features,\n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8)","db7acf0b":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_cases.feature_importances_,x_train[features])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n\n\n\nlgb_model_fatalities = train_model(x_train, \"Fatalities\",'fat_prev',features, \n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                                  )\n\n\n","bea66ec5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_fatalities.feature_importances_,x_train[features])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n\n\n\n\n","c046b5c4":"X_train = x_train[features]\ny_train = np.log(x_train[\"ConfirmedCases\"] + 1)\nb_train = np.log(x_train[\"cases_prev\"] + 1)\ncases_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                               )\ncases_model.fit(X_train, y_train, init_score = b_train)\n\nX_train = x_train[features]\ny_train = np.log(x_train[\"Fatalities\"] + 1)\nb_train = np.log(x_train[\"fat_prev\"] + 1)\nfatalities_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8)\nfatalities_model.fit(X_train, y_train, init_score = b_train)","d7661c13":"\n\ndf = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\nsub_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\ndf[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\nsub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n\ncoo_df = merged_train_df.groupby(\"key\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\nloc_group = [\"key\"]\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\nkey_list=df['key'].unique()\ndef preprocess(df):\n    df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n  #  df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n    df = df.merge(coo_df, how=\"left\", on=\"key\")\n    df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n    df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n    key_dummies = pd.get_dummies(df['key'])\n    df = pd.concat([df,key_dummies],axis=1)\n\n    for col in loc_group:\n        df[col].fillna(\"none\", inplace=True)\n    return df\n\ndf = preprocess(df)\nsub_df = preprocess(sub_df)\n\n\n#train_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\n#test_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n#train_df['Date']= pd.to_datetime(train_df['Date']) \ncountry_list = train_df ['Country_Region'].unique()\nkey_list=df['key'].unique()\n#train_df['Date']= pd.to_datetime(train_df['Date']) \n\n\nscoring_dates = test_df['Date'].unique()\npred_df = pd.DataFrame(columns=train_df.columns)\n\nsub = []\nfor date in scoring_dates.tolist():\n    print(date)\n    start=time.time()\n    \n    if date < train_df['Date'].max():\n            \n        new_df = df.loc[df[\"Date\"] < date].copy()\n        curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n        curr_date_df[\"ConfirmedCases\"] = 0\n        curr_date_df[\"Fatalities\"] = 0\n        new_df = new_df.append(curr_date_df).reset_index(drop=True)\n        new_df['cases_prev']=new_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n        new_df['fat_prev']=new_df.groupby(\"key\")[\"Fatalities\"].shift()\n        \n        \n    \n\n        predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n        new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n        predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n        new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n        new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n        new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n        new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n        pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n        \n       \n    else:\n            #new_df = new_df.copy()\n        curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n        curr_date_df[\"ConfirmedCases\"] = 0\n        curr_date_df[\"Fatalities\"] = 0\n        new_df = new_df.append(curr_date_df).reset_index(drop=True)\n        new_df['cases_prev']=new_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n        new_df['fat_prev']=new_df.groupby(\"key\")[\"Fatalities\"].shift()\n        \n        predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n        new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n        predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n        new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n        new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n        new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n        new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n        pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n    print(time.time()-start)  \nfor key in key_list:\n    \n    X_forecastId = sub_df.loc[(sub_df['key'] == key), ['ForecastId']]\n    X_forecastId = X_forecastId.values.tolist()\nX_forecastId = [v[0] for v in X_forecastId]\nfor j in range(len(sub_df['ForecastId'])):\n          \n    dic = { 'ForecastId': sub_df['ForecastId'][j], 'ConfirmedCases': pred_df['ConfirmedCases'][j], 'Fatalities': pred_df['Fatalities'][j]}\n    sub.append(dic)\n\nsubmission = pd.DataFrame(sub)\nsubmission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False) \nfrom Gaussiandistribution import Gaussian","00acb9aa":"len(pred_df['ConfirmedCases']),len(sub_df['ForecastId'])","28b35f3a":"range(len(sub_df['ForecastId']))","3dabd214":"len(X_forecastId)","379ebe20":"len(key_list)","083ece86":"pred_df[pred_df['Country_Region']=='Qatar']","c814ee77":"## Combined","52c26d34":"# LGB"}}