{"cell_type":{"bf2473dc":"code","9e035fea":"code","8b974327":"code","5462b482":"code","4ed0b84f":"code","21b0ff36":"code","534522be":"code","8bab39a2":"code","43cf504b":"code","a44d9543":"code","c88da4a0":"code","3882498f":"code","13cae2a5":"code","d4024f14":"code","97db7725":"code","65e34935":"code","0ef732b7":"code","e5d09050":"code","be6ee858":"code","5b198864":"code","326e0733":"code","1cf1a951":"code","c581032b":"code","f4fcbd38":"code","4bc72197":"code","3765da0a":"code","ea30f1b7":"code","bede7b05":"code","688a1484":"markdown","08132c0e":"markdown","f65eb2f7":"markdown","e3ac4267":"markdown","0231ed60":"markdown","a1da4523":"markdown","a4f24f1f":"markdown","cf88613b":"markdown","31fb3fca":"markdown","af0a2f6e":"markdown","54ca44b7":"markdown"},"source":{"bf2473dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e035fea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport regex as re","8b974327":"titanic_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")","5462b482":"titanic_data.head()","4ed0b84f":"titanic_data.info()","21b0ff36":"# What % passenger survived ,just to check whether class imbalance problem\ntitanic_data[\"Survived\"].sum()\/titanic_data[\"Survived\"].count()","534522be":"print(titanic_data.groupby([\"Pclass\"])['Survived'].sum()\/titanic_data.groupby([\"Pclass\"])['Survived'].count())","8bab39a2":"fig=plt.figure(figsize=(15,5))\nax1=fig.add_subplot(231)\ng=sns.countplot(x=\"Pclass\",data=titanic_data,ax=ax1)\nax2=fig.add_subplot(232)\ng=sns.countplot(x=\"Sex\",data=titanic_data,ax=ax2)\nax3=fig.add_subplot(233)\ng=sns.countplot(x=\"Embarked\",data=titanic_data,ax=ax3)\nax4=fig.add_subplot(234)\ng=sns.countplot(x=\"SibSp\",data=titanic_data,ax=ax4)\nax5=fig.add_subplot(235)\ng=sns.countplot(x=\"Parch\",data=titanic_data,ax=ax5)\nplt.tight_layout()","43cf504b":"fig=plt.figure(figsize=(15,5))\nax1=fig.add_subplot(231)\ng=sns.barplot(x=\"Pclass\",y=\"Survived\",data=titanic_data,ax=ax1)\nax1=fig.add_subplot(232)\ng=sns.barplot(x=\"Sex\",y=\"Survived\",data=titanic_data,ax=ax1)\nax2=fig.add_subplot(233)\ng=sns.barplot(x=\"Embarked\",y=\"Survived\",data=titanic_data,ax=ax2)\nax3=fig.add_subplot(234)\ng=sns.barplot(x=\"SibSp\",y=\"Survived\",data=titanic_data,ax=ax3)\nax4=fig.add_subplot(235)\ng=sns.barplot(x=\"Parch\",y=\"Survived\",data=titanic_data,ax=ax4)\n# plt.close(\"all\")\n# plt.close(3)\n# plt.close(1)\nplt.tight_layout()","a44d9543":"fig=plt.figure()\nax1=fig.add_subplot(1,2,1)\ng=sns.distplot(titanic_data[\"Age\"],bins=100,kde=False,ax=ax1)\nplt.ylabel(\"Frequency\")\nax2=fig.add_subplot(1,2,2)\ng=sns.distplot(titanic_data[\"Fare\"],bins=100,kde=False)\nplt.ylabel(\"Frequency\")\nplt.tight_layout()","c88da4a0":"\n# Age & Fare wise Survival Rate (Plot)\ntitanic_data[\"Age_bins\"]=pd.cut(x=titanic_data[\"Age\"],bins=[0,5,11,21,31,41,51,61,71,81])\ntitanic_data[\"Fare_bins\"]=pd.cut(x=titanic_data[\"Fare\"],bins=[0,8,15,50,500])\n\nfig=plt.figure(figsize=(15,5))\nax1=fig.add_subplot(221)\nsns.countplot(x=\"Age_bins\",data=titanic_data,ax=ax1)\nax2=fig.add_subplot(222)\nsns.barplot(x=\"Age_bins\",y=\"Survived\",data=titanic_data,ax=ax2)\nax3=fig.add_subplot(223)\nsns.countplot(x=\"Fare_bins\",data=titanic_data,ax=ax3)\nax4=fig.add_subplot(224)\nsns.barplot(x=\"Fare_bins\",y=\"Survived\",data=titanic_data,ax=ax4)\nplt.tight_layout()","3882498f":"# Why Cabing is only Available for 20% passengers\n# As we can see Pclass is saying the same thing as PClass, so we can drop this column\n# QUESTION - How can we check correlation between Categorical Variable","13cae2a5":"print(\"Class wise passenger count\\n\",titanic_data[\"Pclass\"].value_counts(),\"\\n Percentage passenger who were staying in Cabins \\n\",titanic_data[titanic_data[\"Cabin\"].isnull()==False][\"Pclass\"].value_counts()\/titanic_data[\"Pclass\"].value_counts())","d4024f14":"corr=titanic_data.corr()\nsns.heatmap(corr)","97db7725":"# Importing Libraries V2\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.svm import SVC","65e34935":"# we can impute cat variables with most_frequent if the test data has also very little missing data else then replace null values\n# with \"missing\ntitanic_test=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntitanic_test.info()\n# Looking at the data we can see that there is no much missing in Cat so we can use \"Most Frequent\"","0ef732b7":"# Creating Pipeline for Categorical variable\ncat_features=[\"Pclass\",\"Sex\",\"Embarked\"]\ncat_transformer=Pipeline([('imputer_cat',SimpleImputer(strategy=\"most_frequent\")),\n                          ('onehot_cat',OneHotEncoder(handle_unknown=\"ignore\"))])","e5d09050":"# Creating Pipeline for Numerical  variable\nnum_features=[\"SibSp\",\"Parch\",\"Age\",\"Fare\"]\nnum_transformer=Pipeline([('imputer_num',SimpleImputer(strategy=\"mean\")),\n                          ('scaling_num',StandardScaler())])","be6ee858":"# combining both transformers\npreprocessor=ColumnTransformer([('categoricals',cat_transformer,cat_features),\n                                ('numericals',num_transformer,num_features)],\n                              remainder=\"drop\")","5b198864":"# Defining Param_grid & Pipeline for Logistic Regression,Random Forest,SVC & Gradient Boost Classifier \n#defining a random state\nrs=111\n\n#Logistic Regression\npipeline_lr=Pipeline([('preprocessing',preprocessor),('clf',LogisticRegression(random_state=rs))])\nparam_grid_lr={'clf__solver':['liblinear'],\n              'clf__penalty':['l1','l2'],\n              'clf__C':[0.1,0.5,1,10,100]\n              }\n\n#Random Forest\n\npipeline_rf=Pipeline([('preprocessing',preprocessor),('rf',RandomForestClassifier(random_state=rs))])\nparam_grid_rf= {'rf__criterion':[\"gini\"],\n                'rf__max_depth':[4,8],\n               'rf__min_samples_split':[2,3,10],\n               'rf__min_samples_leaf':[2,3,10]\n               }\n# SVC\npipeline_svc=Pipeline([('preprocessing',preprocessor),('svc',SVC(random_state=rs))])\nparam_grid_svc={'svc__C':[1,10,50,100],\n                'svc__gamma': [ 0.001, 0.01, 0.1, 1],\n               'svc__kernel':['linear','rbf']\n               }\n#GBC\npipeline_gbc=Pipeline([('preprocessing',preprocessor),('gbc',GradientBoostingClassifier(random_state=rs))])\nparam_grid_gbc={'gbc__learning_rate':[0.01,0.05,0.1],\n               'gbc__max_depth':[4,8],\n               'gbc__n_estimators':[100,200,300],\n               'gbc__min_samples_leaf':[100,500],\n                \"gbc__max_features\":[\"sqrt\",\"None\"]\n               }","326e0733":"gs_lr=GridSearchCV(estimator= pipeline_lr,param_grid=param_grid_lr,scoring=\"accuracy\",cv=5)\ngs_rf=GridSearchCV(estimator= pipeline_rf,param_grid=param_grid_rf,scoring=\"accuracy\",cv=5,n_jobs=-1)\ngs_svc=GridSearchCV(estimator= pipeline_svc,param_grid=param_grid_svc,scoring=\"accuracy\",cv=5,n_jobs=-1)\ngs_gbc=GridSearchCV(estimator= pipeline_gbc,param_grid=param_grid_gbc,scoring=\"accuracy\",cv=5,n_jobs=-1)","1cf1a951":"# Train Validation Split\ntitanic_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")\nX=titanic_data.drop(\"Survived\",axis=1)\ny=titanic_data[\"Survived\"]\n\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.15,stratify=y,random_state=111)\n\nrs=111","c581032b":"\n#Choosing the best classifier\nprint(\"Performing Model Optimization\")\nmodels_dict={0:'Logistic_Regression',1:'Random Forest',2:'Support Vector Classifier',3:'Gradient Boost Classifier'}\nmodels_list=[gs_lr,gs_rf,gs_svc,gs_gbc]\n\nfor idx,clf in enumerate(models_list):\n    print(\"Estimator :{}\".format(models_dict[idx]))\n    clf.fit(X_train,y_train)\n    print(\"Estimator Best params:{} \\n Best Accuracy(training) Score :{}\".format(clf.best_params_,clf.best_score_))\n    y_pred=clf.predict(X_val)\n    print(\"Validation Score using best parameter {}\".format(accuracy_score(y_val,y_pred)))","f4fcbd38":"%%time\n# Xgboost\nfrom xgboost import XGBClassifier\npipeline_xgb=Pipeline([('preprocessing',preprocessor),('xgb',XGBClassifier(random_state=rs))])\nparam_grid_xgb={'xgb__eta':[0.01,0.1,1],\n                'xgb__max_depth':[4,8],\n                'xgb__min_child_weight':[1,5,10,20]}\n\ngs_xgb=GridSearchCV(estimator= pipeline_xgb,param_grid=param_grid_xgb,scoring=\"accuracy\",cv=5)\ngs_xgb.fit(X_train,y_train)\nprint(\"Best Parameter :{}\\n Best Accuracy(training) :{}\".format(gs_xgb.best_params_,gs_xgb.best_score_))\ny_pred=gs_xgb.predict(X_val)\nprint(\"Validation Score using best parameter {}\".format(accuracy_score(y_val,y_pred)))","4bc72197":"%%time\n#lightGBM\nfrom lightgbm import LGBMClassifier\npipeline_lgb=Pipeline([('preprocessing',preprocessor),('lgb',LGBMClassifier(random_state=rs))])\nparam_grid_lgb={'lgb__learning_rate':[0.1],\n                'lgb__boosting_type': [\"binary\"],\n                'lgb__boosting_type':[\"gbdt\"],\n                'lgb__num_leaves':[4,5,6,7],\n                'lgb__max_depth':[5,7,9,11]}\n\n\ngs_lgb=GridSearchCV(estimator= pipeline_lgb,param_grid=param_grid_lgb,scoring=\"accuracy\",cv=5)\ngs_lgb.fit(X_train,y_train)\nprint(\"Best Parameter :{}\\n Best Accuracy(training) :{}\".format(gs_lgb.best_params_,gs_lgb.best_score_))\ny_pred=gs_lgb.predict(X_val)\nprint(\"Validation Score using best parameter {}\".format(accuracy_score(y_val,y_pred)))","3765da0a":"titanic_test=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ny_test_pred=gs_xgb.predict(titanic_test)\n","ea30f1b7":"titanic_test.head()","bede7b05":"# Save Prediction against PassengerID\nmy_submission=pd.DataFrame({\"PassengerId\":titanic_test[\"PassengerId\"],\"Survived\":y_test_pred})\nmy_submission.to_csv(\"Submission_20201112.csv\",index=False)","688a1484":"Correlation Heatmap - Pclass & Fare are highly correlated, Parch & Sibsp are correlated , Fare & survivied is correlated.","08132c0e":"### From EDA of Categorical Variables we can see that\n### INSIGHT : Survival Rate is high PClass =1 and Sex=\"Female\", & Those who embarked from \"C\"(CherBourg\") & SibSp(Sibling\/Spouse =1) &  Parch(Parent\/Child=3)","f65eb2f7":"## 3.Building Pipelines","e3ac4267":"## 4.Split the Data & figure out the best Classifier","0231ed60":"# Steps :\n1.  ### Problem Statement \n2.  ### EDA - distribution of features  & survival rate for different values of different features & treat correlated features\n3.  ### Build Pipelines for handling missing values,One-Hot-Encoder,Scaling & trying different Algorithms\n4.  ### Split the Data in Train -Validation set & then run GridSearchCV on Train Data to figure out the best parameter & best classifier. (If Gradient Boost is giving highest accuracy , then can also try XGboost,LightGBM,CatBoost).\n5. ### Using the best Classifier & parameters ,predict for the Test(no label present) set.","a1da4523":"### From the above run - RF then GBC gives the best score & RF,GBC takes a lot of time & some param were on the border of range (rf_max_depth=10) the best value can be greater than 10 but we will focus on XGboost,Lightgbm,Catboost (as they will reduce the run training time & improve the score a bit). ","a4f24f1f":"## 2.EDA\nEDA -Exploratory Data Analysis\nSome Insights:\n    1. Age(177),Cabin(687),Emarked(2) has null values . \n    2. Check relation of predictor with target, just to get an idea of their importance.\n    3. Fill null values (using Mean value, or Mode).\n    4. Create new Variables (combination of variables ,One-Hot Encoding)","cf88613b":"### INSIGHT :For Continuous Variables - Distribution & survival rate - Child & Old People have higher survival rate & those who paid higher fare had higher survival rate","31fb3fca":"## 1. Problem Statement :\n###  Basis all the data & Features available try to predict who will Survive the titanic crash.\n","af0a2f6e":"Next Step for  all Columns: (Do all the steps using Pipeline just to avoid test data leakage & is replicable\n* Drop PassengerID,Cabin,Ticket\n* Filling Null Values (Age with Mean & Embarked with Mode)\n* Extract info from Name (title) & one-Hot Encoding # **Droping for now create own Pipeline for Title Extractio**n\n* Impute+Scaling -Numerical-Sibsp,Parch,Age,Fare -can add Sibsp+Parch\n* Impute + One-Hot Encode Categorical- Pclass,Sex,Embarked","54ca44b7":"## XGBoost is giving the highest validation score so using the best parameter on test set"}}