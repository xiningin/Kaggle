{"cell_type":{"087aaadb":"code","cd51af88":"code","1276b0d1":"code","017b1512":"code","fbb77c26":"code","e3b75905":"code","e537779a":"code","bd057c0f":"code","079ad36e":"code","9069808a":"code","fcfa13a7":"code","8f179eea":"code","75190fd7":"code","199baa6b":"code","04a54f51":"code","0164a9d4":"code","baab39d6":"code","28e233dc":"code","7aa87c0c":"code","5300f182":"code","b58e3c1f":"code","36928890":"code","265ef60d":"code","71b122b9":"code","d3398741":"code","7e98bfae":"code","b4ed3998":"code","329fb30f":"code","d19d96f8":"code","6248b9b8":"code","e3d197e8":"code","dfd03495":"code","e6f1c4db":"code","70db53d1":"code","fc65a76b":"code","c64138e5":"code","65a32287":"code","05d8ccbc":"code","5e4dcad2":"code","a75ddad3":"code","88f30331":"code","602b3875":"code","90316e10":"code","f1bd47e7":"code","0758591b":"code","8cdadda2":"code","3fb31c64":"code","f4cdce3f":"markdown","b2c0d61b":"markdown","9c5808c0":"markdown","95c53337":"markdown","7ce6f19f":"markdown","e1fcd6c0":"markdown","dc31e4ed":"markdown","ab5ce3e4":"markdown","7cebb6d9":"markdown","d7d9a856":"markdown","d57aeaaf":"markdown","570d4d45":"markdown","557cf798":"markdown","96149a81":"markdown","704f7dba":"markdown","c0f52d2d":"markdown","e4d80723":"markdown","2d43f23b":"markdown","d8e45d6d":"markdown","fcfaa360":"markdown"},"source":{"087aaadb":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","cd51af88":"train=pd.read_csv(r'..\/input\/train.csv')\n#train.head()\ntest=pd.read_csv(r'..\/input\/test.csv')\n#test.head()\ndf=train.copy()\n#df.head()\ntest_df=test.copy()\n#test_df.head()","1276b0d1":"# now data exploration begins.\n\ndf.head()\ndf.index\ndf.columns\ndf.shape","017b1512":"df.describe()  # displays different descriptive measures of the numerical features.","fbb77c26":"df.info() # age and cabin both have missing values. also emabarked has some nan values.\n# can also use .isnull().sum() to get the count of missing values","e3b75905":"df.head()","e537779a":" df.groupby('Survived').Survived.count() # of the given examples 549 people died while only 342 survived.","bd057c0f":"sns.factorplot(x='Survived',data=df,kind='count',palette=['#ff4125','#006400'],size=5,aspect=1)","079ad36e":"# consider 'Sex' feature.\ndf[df.Survived==1].groupby('Sex').Survived.count()\npd.crosstab(index=[df.Sex],columns=[df.Survived],margins=True) # set normalize=True to view %.","9069808a":"sns.factorplot(x='Survived',data=df,hue='Sex',palette=['#0000ff','#FFB6C1'],kind='count',size=5,aspect=1)","fcfa13a7":"pd.crosstab(index=[df.Sex],columns=[df.Survived],margins=True,normalize='index')  ","8f179eea":"sns.factorplot(x='Sex',y='Survived',kind='point',data=df,palette=['#ff4125'],size=5,aspect=1)","75190fd7":"sns.factorplot(x='Sex',y='Survived',data=df,kind='bar',palette=['#0000ff','#FFB6C1'],size=5,aspect=1)","199baa6b":"# consider 'Pclass' feature.\ndf[df.Survived==1].groupby('Pclass').Survived.count()\npd.crosstab(index=[df.Pclass],columns=[df.Survived],margins=True) # set normalize=index to view rowwise %.","04a54f51":"sns.factorplot(x='Survived',y=None,hue='Pclass',kind='count',data=df,size=5,aspect=1,palette=['#ff0000','#006400','#0000ff'])","0164a9d4":"# consider 'Pclass' feature.\ndf[df.Survived==1].groupby('Pclass').Survived.count()\npd.crosstab(index=[df.Pclass],columns=[df.Survived],margins=True,normalize=True) # set normalize=index to view rowwise %.","baab39d6":"pd.crosstab(index=[df.Pclass],columns=[df.Survived],margins=True,normalize='index') ","28e233dc":"sns.factorplot(x='Pclass',y='Survived',kind='point',data=df,size=5,aspect=1,palette=['#ff0000'])","7aa87c0c":"sns.factorplot(x='Pclass',y='Survived',data=df,kind='bar',palette=['#ff0000','#006400','#0000ff'],size=5,aspect=1)","5300f182":"# grouping by both male or female and the respective Pclasses\npd.crosstab(index=[df.Sex,df.Pclass],columns=[df.Survived],margins=True) \n# the result clearly shows that most of the male in class 2 and 3 died and most of the females in class 1 and 2 survived.\n# see the tabulation below.","b58e3c1f":"sns.factorplot(x='Sex',hue='Pclass',data=df,col='Survived',kind='count',palette=['#ff0000','#006400','#0000ff'],size=5,aspect=1)","36928890":" #now let us see how survival varies with 'Embarked'\ndf.groupby('Embarked').Survived.count() # most of the people were embarked with 'S'\n\npd.crosstab(index=[df.Embarked],columns=[df.Survived],margins=True,normalize='index')\n\n","265ef60d":"sns.factorplot(x='Survived',data=df,hue='Embarked',kind='count',palette=['#ff0000','#006400','#0000ff'],size=5,aspect=1)","71b122b9":"pd.crosstab(index=[df.Sex,df.Embarked],columns=[df.Survived],margins=True)","d3398741":"sns.factorplot(x='Sex',data=df,kind='count',hue='Embarked',col='Survived',palette=['#ff0000','#006400','#0000ff'],size=5,aspect=1)","7e98bfae":"pd.crosstab(index=[df.Pclass,df.Embarked],columns=[df.Survived],margins=True)","b4ed3998":"sns.factorplot(x='Survived',col='Embarked',data=df,hue='Pclass',kind='count',palette=['#ff0000','#006400','#0000ff'],size=5,aspect=1)","329fb30f":" # now we need to convert categorical variables into numerical for modelling.\n # can use labels or sep col using get_dummies()\n\n#sex\nfor frame in [train,test,df,test_df]:\n    frame.loc[frame.Sex=='male','Sex']=0\n    frame.loc[frame.Sex=='female','Sex']=1\n    \n#embarked    \nfor frame in [train,test,df,test_df]:\n    frame.loc[frame.Embarked=='C','Embarked']=0\n    frame.loc[frame.Embarked=='S','Embarked']=1\n    frame.loc[frame.Embarked=='Q','Embarked']=2\n#df.head(10)\n        ","d19d96f8":"#now age and fare are continuous variables.\n#we can convert them to discrete intervals.\n\n#age\ndf.Age.describe()   # age varies from 0.42 to 80.00\nfor frame in [train,test,df,test_df]:\n    frame['bin_age']=np.nan\n    frame['bin_age']=np.floor(frame['Age'])\/\/10\n    frame['bin_fare']=np.nan\n    frame['bin_fare']=np.floor(frame['Fare'])\/\/50\n    \ndf.head(10)[['Fare','bin_fare','Age','bin_age']] \n# df.bin_age.unique()\n# df.bin_fare.unique()\n ","6248b9b8":"#can drop Age and Fare columns\nfor frame in [train,df,test_df,test]:\n    frame.drop(['Age','Fare'],axis=1,inplace=True)\n# df.head()\ntest.head()","e3d197e8":"#now we can see how survival varies with bin_age and bin_fare.\ndf.groupby('bin_age').Survived.count()\npd.crosstab(index=[df.bin_age],columns=[df.Survived],margins=True)\npd.crosstab(index=[df.Sex,df.Survived],columns=[df.bin_age,df.Pclass],margins=True)","dfd03495":"sns.factorplot(x='bin_age',hue='Survived',kind='count',data=df,palette=['#ff4125','#006400'],size=5,aspect=1)","e6f1c4db":"#similarly for bin_fare\ndf.groupby('bin_fare').Survived.count()\npd.crosstab(index=[df.bin_fare],columns=[df.Survived],margins=True)\npd.crosstab(index=[df.Sex,df.Survived],columns=[df.bin_fare,df.Pclass],margins=True)","70db53d1":"sns.factorplot(x='bin_fare',hue='Survived',kind='count',data=df,palette=['#ff4125','#006400'],size=5,aspect=1)","fc65a76b":"df.info()\n# embarked and bin_age still have null values.\ndf.describe(include=[np.number])","c64138e5":"for frame in [test,train,df,test_df]:\n    frame.bin_age.fillna(frame.bin_age.median(),inplace=True)\n    frame.Embarked.fillna(frame.Embarked.median(),inplace=True)\n    frame.bin_fare.fillna(frame.bin_fare.median(),inplace=True)\n# just to check \ndf.info()\n    ","65a32287":"for frame in [train,df,test]:\n    frame.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin'],axis=1,inplace=True)\nfor frame in [test_df]:\n    frame.drop(['Name','SibSp','Parch','Ticket','Cabin'],axis=1,inplace=True)\n#df.head()\n\n     ","05d8ccbc":"x_train,x_test,y_train,y_test=train_test_split(df.drop('Survived',axis=1),df.Survived,test_size=0.25,random_state=42)","5e4dcad2":"models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd","a75ddad3":"acc_frame=pd.DataFrame(d)\nacc_frame","88f30331":"sns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)","602b3875":"sns.factorplot(x='Modelling Algo',y='Accuracy',data=acc_frame,kind='point',size=4,aspect=3.5)","90316e10":"# first lets try to tune logistic regression.\n# here we tune the parameters 'C' and 'penalty'\nparams_dict={'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],'penalty':['l1','l2']}\nclf_lr=GridSearchCV(estimator=LogisticRegression(),param_grid=params_dict,scoring='accuracy')\nclf_lr.fit(x_train,y_train)\npred=clf_lr.predict(x_test)\nprint(accuracy_score(pred,y_test))\n","f1bd47e7":"#now lets try KNN.\n#lets try to tune n_neighbors. the default value is 5. so let us vary from say 1 to 50.\nno_of_test=[i+1 for i in range(50)]\n#no_of_test\nparams_dict={'n_neighbors':no_of_test}\nclf_knn=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=params_dict,scoring='accuracy')\nclf_knn.fit(x_train,y_train)\npred=clf_knn.predict(x_test)\nprint(accuracy_score(pred,y_test))\n","0758591b":"#lets RandomForest also. the default value of estimators is 10. so lets vary from 1 to 100.\nno_of_test=[]\nfor i in range(0,101,10):\n     if(i!=0):\n        no_of_test.append(i)\nno_of_test\nparams_dict={'n_estimators':no_of_test}\nclf_rf=GridSearchCV(estimator=RandomForestClassifier(),param_grid=params_dict,scoring='accuracy')\nclf_rf.fit(x_train,y_train)\npred=clf_rf.predict(x_test)\nprint(accuracy_score(pred,y_test))","8cdadda2":"#lets GradientBoosting also. the default value of estimators is 100. so lets vary from 1 to 1000.\nno_of_test=[]\nfor i in range(0,1001,50):\n     if(i!=0):\n        no_of_test.append(i)\nno_of_test\nparams_dict={'n_estimators':no_of_test}\nclf_gb=GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=params_dict,scoring='accuracy',cv=5)\nclf_gb.fit(x_train,y_train)\npred=clf_gb.predict(x_test)\nprint(accuracy_score(pred,y_test))  # results same as before.","3fb31c64":"pred=clf_gb.predict(test)\n#pred\ndict={'PassengerId':test_df['PassengerId'],'Survived':pred}\nans=pd.DataFrame(dict)\nans.to_csv('answer.csv',index=False) # saving to a csv file for predictions on kaggle.","f4cdce3f":"######  after tuning gradient boosting gives the best accuracy on model.","b2c0d61b":"The training dataset has 891 rows or training examples and 12 columns or features. Out of these the 'Survived' is our target variable.","9c5808c0":"###### all the males and females in Pclass 2 and first bin_age survived the accident which hints that children were given preference. also note that in Pclass 1 and bin_age 2,3,4 almost all the people survived which again shows that people in Pclass 1 had better facilities.\n\n","95c53337":"###### we can now tune the models to increase the accuracies.   ","7ce6f19f":"######  # around 18% of all males survived whereas around 75% of females survived.  This again shows that females survived in greater number.","e1fcd6c0":"## [please upvote \/ star if you liked the kernel ]","dc31e4ed":"######  now both of these columns have some missing values and so we need to fill the missing values. for now we impute with the median.\n","ab5ce3e4":"######  data exploration, cleaning and preprocessing ends. Now can move onto modelling algorithms.","7cebb6d9":"# THE END.\n","d7d9a856":"###### the graph highlights the picture very clearly. majority of the males in class 2 and 3 died. this is bcoz they were males and wre traveling in a lower class and hence this makes sense. on the other hand most of the females in class 1 and 2 survived which again makes sense as females were given prioroty and also they were traveling in higher classes","d57aeaaf":"###### Some key points about features\n\nsurvived is the target variable that we have to predict. 0 means die and 1 means survival.\n\nNow some of the more relevant features that I will focus on include --\n\nPclass:  \n\nSex:\n\nAge:\n\nFare:\n\nEmbarked:","570d4d45":"###### 233 female survived while only 109 males. This clearly shows that more females survived than males did. the following graph clearly shows the picture.","557cf798":"## [please star\/upvote if u like it.]","96149a81":" \n######  Some Observations --\n\nage has less than 891 implies that it has some missing(Nan) values.\n\nthe mean of survived indicates that only 38% people survived and rest died.    \n\nalso the age varies from 0.42 to 80. Age less than 1 yr is represented as decimal.\n\n50% denotes the median value of features.","704f7dba":"######  ALSO NOTE THAT ON THIS MODEL KNN & SVM WITH rbf KERNEL GIVES THE HIGHEST ACCURACY.","c0f52d2d":"###### this again shows that 38% of people survived that accident. also this highlights that only 9% of total passengers who traveled in Pclass 2 survived and rest died and similarly 15% of passengers in Pclass 1 survived and rest died.","e4d80723":"###### now this shows an even better picture. 75 % of people died in class 3 and only 24% survived. similarly for class 2.for class 1 only 37% died and rest survived probably bcoz of better facilities.in a nutshell most of the people in pclass 1 survived and most of the people in plass 3 died.","2d43f23b":"######  most of the algorithms have accuracy b\/w 78 to around 81%. this is when we have test size of 0.2 with the given preprocessed data .\n","d8e45d6d":"# TITANIC SURVIVOR PREDICTION.","fcfaa360":"###### the graph clearly shows that majority of the males embarked with S died . also very few females died who were embarked C or Q."}}