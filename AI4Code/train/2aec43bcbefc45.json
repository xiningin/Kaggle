{"cell_type":{"323ddd2d":"code","29727ffb":"code","10771888":"code","f0b79509":"code","aeed1bc5":"code","deae8c6f":"code","45710d9d":"code","c805b2f7":"code","72f7bc35":"code","f2d12de8":"code","f61684a9":"code","a9b9c176":"code","fad91939":"code","6f98fb8e":"code","a93691ba":"code","95bd5357":"code","29e73024":"code","159d264d":"code","567c2afa":"code","fcfa3912":"code","b7ad128f":"code","a14a26aa":"code","1c14cc55":"code","007dc955":"code","ea220e4a":"code","b4cfa5ba":"code","fdf82283":"code","32166f45":"code","41c6855f":"code","56477ded":"code","aa1cb4f3":"code","f75904a9":"code","4781fe48":"code","ee5d245c":"code","d6d915e6":"code","19be0c44":"code","bf2be648":"code","619a4c57":"code","65fc0b68":"code","285e1fb8":"code","d3bda44c":"code","c2a7abb3":"code","995121b9":"code","3bd12f03":"code","5cf41597":"code","bc90cbee":"code","945368bd":"code","c2bcdb2a":"code","02bb4263":"markdown","6d5dee00":"markdown","e4a27cd6":"markdown","ccc86744":"markdown","10cf7706":"markdown","e4dd820c":"markdown"},"source":{"323ddd2d":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nimport xgboost\nimport sklearn\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport time\nimport sys\nimport gc\nimport pickle\n","29727ffb":"train_path = \"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\"\ntest_path = \"..\/input\/competitive-data-science-predict-future-sales\/test.csv\"\nitems_path = \"..\/input\/competitive-data-science-predict-future-sales\/items.csv\"\nshops_path = \"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\"\nitem_cat_path = \"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\"","10771888":"items = pd.read_csv(items_path)\ncats = pd.read_csv(item_cat_path)\nshops = pd.read_csv(shops_path)\ntrain = pd.read_csv( train_path )\ntest = pd.read_csv( test_path )","f0b79509":"monthly_sales = train.groupby('date_block_num')['item_cnt_day'].sum()\nmonthly_sales.plot()","aeed1bc5":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (10,4))\nplt.xlim(-100, 3000)\nsns.boxplot( x= train.item_cnt_day )\n\nplt.figure( figsize = (10,4) )\nplt.xlim(train.item_price.min(), train.item_price.max())\nsns.boxplot( x = train.item_price )\nplt.show()\n\n","deae8c6f":"# Removing outliers based on boxplots\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1100]\n\n#Fix negative price for a item 2973\n\nmedian = np.median(train[train['item_id']==2973].item_price)\ntrain.loc[train['item_price']<0,'item_price'] = median","45710d9d":"sales= train.copy()","c805b2f7":"from sklearn.preprocessing import LabelEncoder\n\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","72f7bc35":"def lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","f2d12de8":"def add_feature(df,grp_cols,feature):\n        new_df = df.groupby(grp_cols).agg({'item_cnt_month': ['mean']})\n        new_df.columns = [feature]\n        new_df.reset_index(inplace = True)\n        df = pd.merge(df,new_df,on = grp_cols,how='left')\n        return(df)","f61684a9":"matrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    mat = sales[sales.date_block_num == i]\n    matrix.append( np.array(list( product( [i], mat.shop_id.unique(), mat.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )","a9b9c176":"test['date_block_num'] = 34\n#Concatenate train and test dataframes\nmatrix = pd.concat([matrix,test], ignore_index = True)\nmatrix.drop('ID',axis = 1, inplace = True)\nmatrix.fillna(0,inplace = True)","fad91939":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","6f98fb8e":"matrix.head()","a93691ba":"\nmatrix[\"month\"] = matrix[\"date_block_num\"] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)\n","95bd5357":"group = sales.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)","29e73024":"gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':[('target_shop','sum')]})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nmatrix = pd.merge(matrix, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n","159d264d":"gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':[('target_item','sum')]})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nmatrix = pd.merge(matrix, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n","567c2afa":"gb = sales.groupby(['shop_id','item_id', 'date_block_num'],as_index=False).agg({'item_price':[('item_price_mean','mean')]})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nmatrix = pd.merge(matrix, gb, how='left', on=['shop_id', 'item_id','date_block_num']).fillna(0)","fcfa3912":"matrix[\"revenue\"] = matrix[\"item_price_mean\"]*matrix[\"item_cnt_month\"]","b7ad128f":"matrix.head()","a14a26aa":"matrix = lag_feature( matrix, [1,2,3,6,12], [\"item_cnt_month\"] )\nmatrix = lag_feature( matrix, [1,2,3,6,12], [\"target_shop\"] )\nmatrix = lag_feature( matrix, [1,2,3,6,12], [\"target_item\"] )\nmatrix = lag_feature( matrix, [1,2,3,6,12], [\"revenue\"] )\nmatrix = lag_feature( matrix, [1,2,3,6,12], [\"item_price_mean\"] )\n\n\n\n\n","1c14cc55":"matrix = add_feature(matrix,['date_block_num', 'shop_id'],'date_shop_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,6,12],['date_shop_avg_cnt'])\n\nmatrix = add_feature(matrix,['date_block_num', 'item_category_id'],'date_cat_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,6,12],['date_cat_avg_cnt'])\n\nmatrix = add_feature(matrix,['date_block_num'],'date_avg_item_cnt')\nmatrix = lag_feature(matrix,[1,2,6,12],['date_avg_item_cnt'])\n\nmatrix = add_feature(matrix,['date_block_num', 'item_id'],'date_item_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,6,12],['date_item_avg_cnt'])\n\nmatrix = add_feature(matrix,['date_block_num', 'city_code'],'date_city_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,6,12],['date_city_avg_cnt'])","007dc955":"def optimize_memory(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    integers = ['int8','int16','int32','int64']\n    floats   = ['float32','float64']\n    int_cols  = [c for c in df if df[c].dtype in integers]\n    float_cols  = [c for c in df if df[c].dtype in floats]\n    for i in int_cols:\n        df[i] = pd.to_numeric(df[i], downcast='integer')\n    for i in float_cols:\n        df[i] = pd.to_numeric(df[i], downcast='float')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","ea220e4a":"import gc\n\nmatrix.fillna(0,inplace = True)\nmatrix = optimize_memory(matrix)\ngc.collect()","b4cfa5ba":"matrix = add_feature(matrix,['date_block_num', 'type_code'],'date_type_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,3,12],['date_type_avg_cnt'])\n\nmatrix = add_feature(matrix,['date_block_num', 'subtype_code'],'date_subtype_avg_cnt')\nmatrix = lag_feature(matrix,[1,2,3,12],['date_subtype_avg_cnt'])\n\n\n\nmatrix.fillna(0,inplace = True)\nmatrix = optimize_memory(matrix)\ngc.collect()","fdf82283":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days)\n\nmatrix['date_item_day'] = matrix['item_cnt_month'] \/ matrix['days']\nmatrix = lag_feature(matrix,[1,2,3,12],['date_item_day'])\n\n","32166f45":"matrix.fillna(0,inplace = True)\nmatrix = optimize_memory(matrix)\ngc.collect()","41c6855f":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, ['date_item_avg_item_price'])\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n\nfeatures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    features_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    features_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(features_to_drop, axis=1, inplace=True)","56477ded":"train['revenue'] = train['item_cnt_day']*train['item_price']\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], ['delta_revenue'])\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","aa1cb4f3":"matrix.head()","f75904a9":"matrix = matrix[matrix['date_block_num'] > 12]\ndata= matrix.copy()","4781fe48":"colsdrop= ['date_item_day','date_item_avg_cnt','date_shop_avg_cnt','date_cat_avg_cnt',\n          'date_avg_item_cnt']\ncolsdrop1= ['target_shop','target_item','item_cnt_month','revenue','item_price_mean',\n           'date_city_avg_cnt','date_type_avg_cnt','date_subtype_avg_cnt']\n\ndropcols= colsdrop+colsdrop1\n#data.drop(colsdrop1, axis=1, inplace=True)","ee5d245c":"data.tail()","d6d915e6":"data.shape","19be0c44":"X_train = data[data.date_block_num < 33].drop(dropcols, axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(dropcols, axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(dropcols, axis=1)","bf2be648":"X_valid.head()","619a4c57":"import lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score","65fc0b68":"lr = LinearRegression()\nlr.fit(X_train.values, Y_train)\npred_lr = lr.predict(X_valid.values)\n\nprint('Test R-squared for linreg is %f' % r2_score(Y_valid, pred_lr))","285e1fb8":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=Y_train), 100)\npred_lgb = model.predict(X_valid)\n\nprint('Test R-squared for LightGBM is %f' % r2_score(Y_valid, pred_lgb))","d3bda44c":"del data\ngc.collect();","c2a7abb3":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    subsample=0.8,\n    colsample_bytree=0.8,\n    eta = 0.3,\n    seed=42)\n\nxgb.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 20)","995121b9":"import matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb, (10,14))","3bd12f03":"pred_xgb = xgb.predict(X_valid)\n\nprint('Test R-squared for XGBoost is %f' % r2_score(Y_valid, pred_xgb))","5cf41597":"from sklearn.metrics import mean_squared_error\n\ny_pred = xgb.predict(X_valid).clip(0,20)\ny_pred_tr = xgb.predict(X_train).clip(0,20)\nrmse_tr = mean_squared_error(Y_train, y_pred_tr,squared=False)\nrmse_val = mean_squared_error(Y_valid, y_pred,squared=False)\nprint(\"RMSE Validation: %.5f\" % rmse_val)\nprint(\"RMSE Training: %.5f\" % rmse_tr)","bc90cbee":"test_pred_lr= lr.predict(X_test)\ntest_pred_lgb= model.predict(X_test)\ntest_pred_xgb= xgb.predict(X_test)\n\nstacked_valid_predictions= np.column_stack((pred_lr, pred_lgb, pred_xgb))\nstacked_test_predictions= np.column_stack((test_pred_lr, test_pred_lgb, test_pred_xgb))","945368bd":"meta_model= LinearRegression()\n\nmeta_model.fit(stacked_valid_predictions, Y_valid)\n\nfinal_predictions= meta_model.predict(stacked_test_predictions)","c2bcdb2a":"Y_test = final_predictions.clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('1c_submission4.csv', index=False)\n","02bb4263":"# **Mean Encoding**","6d5dee00":"# **Feature Engineering**","e4a27cd6":"This notebook has been created as part of the Coursera project. Features and ideas have been\ntaken from multiple sources. \n\nEnsembling has been done by stacking linear regression, lightgbm and xgboost predictions.","ccc86744":"# **EDA**","10cf7706":"# **Model Training**","e4dd820c":"# **Ensembling**"}}