{"cell_type":{"e7501a63":"code","dbba972c":"code","84b80c0b":"code","45b7f83c":"code","71a226d2":"code","21a9714a":"code","02aa21a7":"code","d4831f33":"code","7fe5e849":"code","a6130a71":"code","2c2e368e":"code","1d727f5d":"code","f952c1ea":"code","67dc754a":"code","de180965":"code","2c07f4de":"code","9f9b94ab":"code","ca123438":"code","49b5a66d":"code","94f91fec":"code","ebcb69b8":"code","224106ca":"code","3cf88a68":"code","ee532c23":"code","63a6cca9":"code","a2938182":"code","9710a3ee":"code","e03dc7f9":"code","6f743aa3":"code","1e74ac02":"code","7435264c":"code","d1284c88":"code","6c2fe573":"code","b00cd74a":"code","45b4ed20":"code","17bae13d":"code","c8f6fbe1":"code","10c64bf9":"code","0155c85b":"code","baafc0f7":"code","15d55f09":"code","546fc332":"code","5408245a":"code","49181c4a":"code","dfe32df0":"code","98639951":"code","203f6518":"markdown","59045cc4":"markdown","4c58cf98":"markdown","9c443e89":"markdown","093df0c3":"markdown","646f15db":"markdown","3bcb270b":"markdown","68227bbb":"markdown","489ad6aa":"markdown","566b08f7":"markdown"},"source":{"e7501a63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dbba972c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm_notebook\n\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# sklearn libs compatilhadas\nimport sklearn\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, train_test_split, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## Tratando dados desbalanceados\nfrom imblearn.over_sampling import SMOTE\n\n\n\n\n### Clusteniza\u00e7\u00e3o 1\nfrom sklearn.cluster import KMeans \nfrom sklearn import metrics \nfrom scipy.spatial.distance import cdist \n\n\n\n## Classificacao 1\nfrom sklearn.linear_model import LogisticRegression\n\n## Classifica\u00e7\u00e3o 2\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom lightgbm import LGBMClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\n","84b80c0b":"\ndef plot_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n    df = pd.DataFrame(cm, index=[\"no\", \"yes\"], columns=[\"no\", \"yes\"])\n    ax = sns.heatmap(df, annot=True)\n    ax.set_xlabel(\"Predicted label\")\n    ax.set_ylabel(\"True label\")\n    return ax\n\ndef plot_roc(y_true, y_score, figsize=(8, 8)):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=figsize)\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=2, label=f'ROC curve (AUC = {100*roc_auc:.2f}%)')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return roc_auc\n\n\ndef plot_precision_recall(precisions, recalls, thresholds):\n    fig, ax = plt.subplots(figsize=(12,8))\n    ax.plot(thresholds, precisions[:-1], \"r--\", label=\"Precisions\")\n    ax.plot(thresholds, recalls[:-1], \"#424242\", label=\"Recalls\")\n    ax.set_title(\"Precision and Recall \\n Tradeoff\", fontsize=18)\n    ax.set_ylabel(\"Level of Precision and Recall\", fontsize=16)\n    ax.set_xlabel(\"Thresholds\", fontsize=16)\n    ax.legend(loc=\"best\", fontsize=14)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    return ax","45b7f83c":"df = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')","71a226d2":"df.columns = ['Patient ID',\n 'Patient_age_quantile',\n 'SARS-Cov-2_result',\n 'Patient_addmited_regular_ward_care_bool',\n 'Patient_addmited_semi-intensive_care_bool',\n 'Patient_addmited_intensive_care_bool',\n 'Hematocrit',\n 'Hemoglobin',\n 'Platelets',\n 'Mean_platelet_volume',\n 'Red_blood_Cells',\n 'Lymphocytes',\n 'MCHC',\n 'Leukocytes',\n 'Basophils',\n 'MCH',\n 'Eosinophils',\n 'MCV',\n 'Monocytes',\n 'RDW',\n 'Serum Glucose',\n 'Respiratory_Syncytial Virus',\n 'Influenza_A',\n 'Influenza_B',\n 'Parainfluenza_1',\n 'CoronavirusNL63',\n 'Rhinovirus_Enterovirus',\n 'Mycoplasma_pneumoniae',\n 'Coronavirus_HKU1',\n 'Parainfluenza_3',\n 'Chlamydophila_pneumoniae',\n 'Adenovirus',\n 'Parainfluenza 4',\n 'Coronavirus229E',\n 'CoronavirusOC43',\n 'Inf_A_H1N1_2009',\n 'Bordetella_ertussis',\n 'Metapneumovirus',\n 'Parainfluenza_2',\n 'Neutrophils',\n 'Urea',\n 'Proteina_C',\n 'Creatinine',\n 'Potassium',\n 'Sodium',\n 'Influenza_B_test',\n 'Influenza_A_test',\n 'Alanine_ransaminase',\n 'Aspartate_transaminase',\n 'Gamma-glutamyltransferase',\n 'Total_Bilirubin',\n 'Direct_Bilirubin',\n 'Indirect_Bilirubin',\n 'Alkaline_phosphatase',\n 'Ionized_calcium',\n 'Strepto_A',\n 'Magnesium_',\n 'pCO2_',\n 'Hb_saturation_',\n 'Bas_excess_',\n 'pO2_',\n 'Fio2_',\n 'Total_CO2_',\n 'pH_',\n 'HCO3_venon',\n 'Rods',\n 'Segmented',\n 'Promyelocytes',\n 'Metamyelocytes',\n 'Myelocytes',\n 'Myeloblasts',\n 'Urine_Esterase',\n 'Urine_Aspect',\n 'Urine_pH',\n 'Urine_Hemoglobin',\n 'Urine_Bile pigments',\n 'Urine_Ketone Bodies',\n 'Urine_Nitrite',\n 'Urine_Density',\n 'Urine_Urobilinogen',\n 'Urine_Protein',\n 'Urine_Sugar',\n 'Urine_Leukocytes',\n 'Urine_Crystals',\n 'Urine_Red blood cells',\n 'Urine_Hyaline cylinders',\n 'Urine_Granular cylinders',\n 'Urine_Yeasts',\n 'Urine_Color',\n 'Partial_thromboplastin_time',\n 'Relationship',\n 'INR',\n 'Lactic_Dehydrogenase',\n 'Prothrombin_time (PT)',\n 'Vitamin_B12',\n 'Creatine_phosphokinase',\n 'Ferritin',\n 'Lactic_Acid',\n 'Lipase_dosage',\n 'D-Dimer',\n 'Albumin',\n 'Hb_saturation',\n 'pCO2',\n 'Base_excess',\n 'pH',\n 'Total_CO2',\n 'HCO3_artery',\n 'pO2',\n 'Arteiral_Fio2',\n 'Phosphor',\n 'ctO2']","21a9714a":"# sorted([[df.shape[0]-j,i] for i,j in df.isna().sum().items() if j > 0])\n\n\n# [0, 'D-Dimer'],\n#  [0, 'Mycoplasma_pneumoniae'],\n#  [0, 'Partial_thromboplastin_time'],\n#  [0, 'Prothrombin_time (PT)'],\n#  [0, 'Urine_Sugar'],\n#  [1, 'Fio2_'],\n#  [1, 'Urine_Nitrite'],\n#  [3, 'Vitamin_B12'],\n#  [8, 'Lipase_dosage'],","02aa21a7":"## \ndf.drop(['Prothrombin_time (PT)', 'D-Dimer', 'Mycoplasma_pneumoniae', 'Urine_Sugar', 'Partial_thromboplastin_time', 'Fio2_', 'Urine_Nitrite', 'Vitamin_B12'], axis = 1, inplace = True)","d4831f33":"## Preenchendo as colunas  dos testes que n\u00e3o foram feitos.\ncolumns_to_fill = pd.DataFrame(df.isna().sum()\/df.shape[0], columns=['Missing'])\ncolumns_to_fill = columns_to_fill[(columns_to_fill.Missing < 0.87)].index\n\nfor col in columns_to_fill:\n    df[col] = df[col].fillna('not_done')\n\n# df['null_cols'] = df.isna().sum(axis=1)\nnull_col = df.isna().sum(axis=1)","7fe5e849":"## Map CATEGORICO\nfullMapper={'negative': 0, 'positive': 1,\n           'not_detected': 0, 'detected': 1,\n            'not_done': -1, 'absent': -1,\n            'N\u00e3o Realizado': -1,\n               ## Urine Aspects\n              'clear': 0,\n              'cloudy': 1,\n              'lightly_cloudy': 2,\n              'altered_coloring': 3,\n               #Urine_Leukocytes\n               '<1000': 1000,\n                #Urine_urobilinogen\n                'normal':0,\n            #'Urine_Crystals': {\n              'Ausentes': 0,\n              'Urato Amorfo --+': 1,\n              'Urato Amorfo +++': 2,\n              'Oxalato de C\u00e1lcio +++': 3,\n              'Oxalato de C\u00e1lcio -++': 4,\n            # Urine_Color\n              'yellow': 0,\n              'light_yellow': 1,\n              'orange': 2,\n              'citrus_yellow': 3,\n            # Urine_Hemoglobin\n            'present': 1,\n               }\n\ndf.replace(fullMapper, inplace=True)","a6130a71":"## Preenchendo os exames de sangue com base na m\u00e9dia por idade\ncolumns_to_fill = pd.DataFrame(df.isna().sum()\/df.shape[0], columns=['Missing'])\ncolumns_to_fill = columns_to_fill[columns_to_fill.Missing > 0.87].index\n\n#Prenche quando tiver o dado da idade\nfor value in df.Patient_age_quantile.unique():\n    df_aux = df[df.Patient_age_quantile == value].copy()\n    \n    for col in columns_to_fill:\n        df_aux[col] = df_aux[col].fillna(df_aux[col].median())\n        \n    df.loc[df_aux.index] = df_aux\n\n#Preenche com a media geral\nfor col in columns_to_fill:\n    df[col] = df[col].fillna(df[col].median())","2c2e368e":"try:\n    df[['Urine_Aspect', 'Urine_Color', 'Urine_Crystals','null_cols']]  = df[['Urine_Aspect', 'Urine_Color', 'Urine_Crystals', 'null_cols']].astype('category')\nexcept:\n    df[['Urine_Aspect', 'Urine_Color', 'Urine_Crystals']]  = df[['Urine_Aspect', 'Urine_Color', 'Urine_Crystals']].astype('category')","1d727f5d":"df.isna().sum().sum()","f952c1ea":"use_sample = False\nif use_sample:\n    df_samp = df.sample(frac=0.8,random_state=76)\nelse:\n    df_samp = df\n\nX = df_samp.drop(['Patient ID','SARS-Cov-2_result', 'Patient_addmited_regular_ward_care_bool',\n       'Patient_addmited_semi-intensive_care_bool',\n       'Patient_addmited_intensive_care_bool'], axis=1)\ny = df_samp['SARS-Cov-2_result']\nX_scaled = StandardScaler().fit_transform(X)\n\nbest_cluster = 0\nbest_total = 0\n\nfor r in tqdm_notebook(range(50)):\n    kmodel = KMeans(n_clusters=20).fit(X_scaled)\n    X['cluster'] = kmodel.predict(X_scaled)\n\n# for num in X.cluster.unique():\n#     print(str(num) + '   |   ' + str(y[X.cluster == num].sum())  + '    |     ' + str(y[X.cluster == num].sum()\/X[X.cluster == num].shape[0]))\n    \n    for num in X.cluster.unique():\n#         print(str(num) + '   |   ' + str(y[X.cluster == num].sum())  + '    |     ' + str(y[X.cluster == num].sum()\/X[X.cluster == num].shape[0]))\n        if (y[X.cluster == num].sum() <= 2) & (X[X.cluster == num].shape[0] > 10):\n#             print('--Replace')\n            X['cluster'].replace(num, -1, inplace = True)\n    \n    if (y[X.cluster == -1].sum() >= best_cluster) & (X[X.cluster == -1].shape[0] > best_total):\n        best_kmodel = kmodel\n        best_cluster = y[X.cluster == -1].sum()\n        best_total = X[X.cluster == -1].shape[0]\n\n# X_scaled = StandardScaler().fit_transform(X)\n\n\n# X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_scaled, y, test_size=0.3, random_state=0)","67dc754a":"df['null_cols'] = null_col\n    \nX = df.drop(['Patient ID','SARS-Cov-2_result', 'Patient_addmited_regular_ward_care_bool',\n       'Patient_addmited_semi-intensive_care_bool',\n       'Patient_addmited_intensive_care_bool'], axis=1)\ny = df['SARS-Cov-2_result']\ntry:\n    X_scaled = StandardScaler().fit_transform(X)\n    X['cluster'] = best_kmodel.predict(X_scaled)\nexcept:\n    X_scaled = StandardScaler().fit_transform(X.drop(['null_cols'], axis=1))\n    X['cluster'] = best_kmodel.predict(X_scaled)\n    \nfor num in X.cluster.unique():\n    print(str(num) + '   |   ' + str(y[X.cluster == num].sum())  + '\/' + str(X[X.cluster == num].shape[0]) + '    |     ' + str(y[X.cluster == num].sum()\/X[X.cluster == num].shape[0]))\n    if (y[X.cluster == num].sum() <= 2) & (X[X.cluster == num].shape[0] > 15):\n        print('--Replace')\n        X['cluster'].replace(num, -1, inplace = True)\n\nX_scaled = StandardScaler().fit_transform(X)","de180965":"X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_scaled[X.cluster != -1], y[X.cluster != -1], test_size=0.3, random_state=0)\n\nuse_smote = False\n\nif use_smote:\n    os = SMOTE(random_state=131)\n    columns = X.columns\n\n    os_data_X, os_data_y = os.fit_sample(X_train_all, y_train_all)\n    os_data_X = pd.DataFrame(data=os_data_X,columns=columns)\n    os_data_y = pd.DataFrame(data=os_data_y)\n\n    X_train_all = os_data_X\n    y_train_all = os_data_y['SARS-Cov-2_result']\n    \n    X_test_all = pd.DataFrame(data=X_test_all,columns=columns)\n\nelse:\n    columns = X.columns\n    X_train_all = pd.DataFrame(data=X_train_all,columns=columns)\n    X_test_all = pd.DataFrame(data=X_test_all,columns=columns)\n\n\nprint(f\"\"\"\ntamanho do dataset de treino:\nX_train:{X_train_all.shape}\ny_train:{y_train_all.shape}\n~~~~~~~\ntamanho do dataset de teste:\nX_test:{X_test_all.shape}\ny_test:{y_test_all.shape}\n\"\"\")\n\nX_train = X_train_all.copy()\ny_train = y_train_all.copy()\nX_test = X_test_all.copy()\ny_test = y_test_all.copy()\n\n# params = dict(\n#     n_estimators=[150,500,1000],\n#     max_depth=[3, 5, 10],\n#     min_samples_split=[2,50],\n#     min_samples_leaf=[1,5,10],\n# )\n# model = RandomForestClassifier(n_jobs=-1, random_state=42)\n# grid = GridSearchCV(model, param_grid = params,verbose=True, n_jobs=-1, return_train_score= True)\n# grid.fit(X_train, y_train)\n","2c07f4de":"best_params = {'max_depth': 10,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'n_estimators': 500}\n\nmodel = RandomForestClassifier(**best_params,n_jobs=-1,verbose=0, random_state=42).fit(X_train, y_train)\ny_pred_rf = model.predict_proba(X_test)\n\ny_test = y_test.append(y[X.cluster == -1])\ny_pred_rf = np.append(y_pred_rf, np.repeat([[1,0]], y[X.cluster == -1].shape[0], axis=0), axis=0)","9f9b94ab":"pred_train = model.predict(X_train)\nscores = sklearn.metrics.accuracy_score(y_train, pred_train)\nprint('Accuracy on training data: {:.2f}%'.format(scores))   \n \npred_test = y_pred_rf[:,1] > 0.5\nscores2 = sklearn.metrics.accuracy_score(y_test, pred_test)\nprint('Accuracy on test data: {:.2f}%'.format(scores2))    ","ca123438":"_ =plot_roc(y_test, y_pred_rf[:,1])","49b5a66d":"precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_rf[:,1])\n_ =plot_precision_recall(precisions, recalls, thresholds)","94f91fec":"for i in range(0,5000):\n    thr = i\/5000\n    y_pred_ = y_pred_rf[:,1] > thr\n    \n    if confusion_matrix(y_test, y_pred_)[1,0] > 9:\n        thr -= 1\/5000\n        break\n\nprint(thr)\n\nthreshold = thr\n\ny_pred_ = y_pred_rf[:,1] > threshold\n\n_ =plot_confusion_matrix(y_test, y_pred_)\nconfusion_matrix(y_test, y_pred_)","ebcb69b8":"shap.initjs()","224106ca":"plt.figure(figsize=(20,10))\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.title(\"Top 20 important features\")\nplt.show()","3cf88a68":"explainer = shap.TreeExplainer(model, X_train.sample(100), model_output='probability', feature_dependence='independent')\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","ee532c23":"def force_plot(explainer, patient):\n    shap_values = explainer.shap_values(patient)\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient)","63a6cca9":"y_pred_[0:50]","a2938182":"# explainer = shap.TreeExplainer(model, X_train.sample(100), model_output='probability', feature_dependence='independent')\nforce_plot(explainer, patient=X_test.iloc[[5], :])","9710a3ee":"force_plot(explainer, patient=X_test.iloc[[7], :])","e03dc7f9":"from sklearn.feature_selection import RFECV\nfrom scipy.stats import randint, uniform\n\nX_train = X_train_all.copy()\ny_train = y_train_all.copy()\nX_test = X_test_all.copy()\ny_test = y_test_all.copy()\n\n\nclf1 = LGBMClassifier(n_estimators=100, min_data=1, random_state=0, is_unbalance=True)\nselector = RFECV(clf1, min_features_to_select=5, cv=5, scoring='roc_auc')\n\nparam_test ={\n    'num_leaves': randint(2, 50), \n    'min_child_samples': randint(50, 500), \n    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n    'subsample': uniform(loc=0.2, scale=0.8), \n    'colsample_bytree': uniform(loc=0.4, scale=0.6),\n    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n    'learning_rate': uniform(0.01, 0.2) }\n    \n#     'num_leaves': randint(3, 10), \n#     'min_child_samples': randint(50, 150),\n#     'learning_rate': uniform(0.01, 0.1)\n\n# clf2 = LGBMClassifier(n_estimators=100, random_state=77, min_data=1, silent=True, is_unbalance=True)\n# model = RandomizedSearchCV(\n#     estimator=clf2,\n#     param_distributions=param_test, \n#     n_iter=200,\n#     scoring='roc_auc',\n#     cv=4,\n#     refit=True,\n#     random_state=0,\n#     verbose=2,\n#     n_jobs=-1\n# )\n\n# pipeline = make_pipeline(selector, model)\n\n# pipeline.fit(X_train, y_train)\n\n# model.best_estimator_","6f743aa3":"model = LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.5386420305589861, importance_type='split',\n               is_unbalance=True, learning_rate=0.1177737006790299,\n               max_depth=-1, min_child_samples=258, min_child_weight=1e-05,\n               min_data=1, min_split_gain=0.0, n_estimators=100, n_jobs=4,\n               num_leaves=4, objective=None, random_state=77, reg_alpha=0.1,\n               reg_lambda=10, silent=True, subsample=0.3552182525036206,\n               subsample_for_bin=200000, subsample_freq=0)\n\npipeline = make_pipeline(selector, model)\n\npipeline.fit(X_train, y_train)","1e74ac02":"y_pred_boost = pipeline.predict_proba(X_test)\n\ny_test = y_test.append(y[X.cluster == -1])\ny_pred_boost = np.append(y_pred_boost, np.repeat([[1,0]], y[X.cluster == -1].shape[0], axis=0), axis=0)","7435264c":"_ =plot_roc(y_test, y_pred_boost[:,1])","d1284c88":"precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_boost[:,1])\n_ =plot_precision_recall(precisions, recalls, thresholds)","6c2fe573":"for i in range(0,5000):\n    thr = i\/5000\n    y_pred_ = y_pred_boost[:,1] > thr\n    \n    if confusion_matrix(y_test, y_pred_)[1,0] > 9:\n        thr -= 1\/5000\n        break\n\nprint(thr)\n\nthreshold = thr\n\ny_pred_ = y_pred_boost[:,1] > threshold\n\n_ =plot_confusion_matrix(y_test, y_pred_)\nconfusion_matrix(y_test, y_pred_)","b00cd74a":"X_train = X_train_all.copy()\ny_train = y_train_all.copy()\nX_test = X_test_all.copy()\ny_test = y_test_all.copy()\n\n# Create smote object\nsmt = SMOTE(k_neighbors=5, random_state=1206)\n\n# Do the process\nX_train, y_train = smt.fit_sample(X_train, y_train)\n\n# Defining parameter range to grid search\n# param_gridSVM = {'C': [0.1, 1, 10, 100, 1000],\n#                  'shrinking':[True, False],\n#                  'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001, 0.0001], \n#                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \n\n# Best result found after grid search! This trick is to improve commit speed\n# param_gridSVM = {'C': [10], 'gamma': [0.1], 'kernel': ['rbf'], 'shrinking': [True]}\n\n# gridSVM = GridSearchCV(cv=3, estimator=SVC(class_weight='balanced', random_state=101, probability=True), param_grid=param_gridSVM, refit = True, verbose = 2, scoring='roc_auc', n_jobs=3)\n\n# Define grid instance\ngridSVM = SVC(class_weight='balanced', random_state=101, probability=True, C=10, gamma=0.1, kernel = 'rbf', shrinking = True) \n\n# Initialize grid search, fitting the best model\ngridSVM.fit(X_train, y_train);\n\n# print(gridSVM.best_params_)","45b4ed20":"y_pred_svm = gridSVM.predict_proba(X_test)\n\ny_test = y_test.append(y[X.cluster == -1])\ny_pred_svm = np.append(y_pred_svm, np.repeat([[1,0]], y[X.cluster == -1].shape[0], axis=0), axis=0)","17bae13d":"_ =plot_roc(y_test, y_pred_svm[:,1])","c8f6fbe1":"precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_svm[:,1])\n_ =plot_precision_recall(precisions, recalls, thresholds)","10c64bf9":"for i in range(0,5000):\n    thr = i\/5000\n    y_pred_ = y_pred_svm[:,1] > thr\n    \n    if confusion_matrix(y_test, y_pred_)[1,0] > 9:\n        thr -= 1\/5000\n        break\n\nprint(thr)\n\nthreshold = thr\n\ny_pred_ = y_pred_svm[:,1] > threshold\n\n_ =plot_confusion_matrix(y_test, y_pred_)\nconfusion_matrix(y_test, y_pred_)","0155c85b":"best_alpha = 0\nbest_thr = 0\nnum_false_pos = 100000\n\nfor alpha in tqdm_notebook(range(100)):\n    \n    y_pred_combined = (alpha\/100)*(y_pred_rf) + (1-alpha\/100)*y_pred_boost\n    \n    for i in range(0,500):\n        thr = i\/500\n        y_pred_ = y_pred_combined[:,1] > thr\n\n        if confusion_matrix(y_test, y_pred_)[1,0] > 9:\n            thr -= 1\/500\n            break\n\n    threshold = thr\n\n    y_pred_ = y_pred_combined[:,1] > threshold\n\n    if confusion_matrix(y_test, y_pred_)[0,1] < num_false_pos:\n        best_alpha = alpha\/100\n        best_thr = thr\n        num_false_pos = confusion_matrix(y_test, y_pred_)[0,1]\n    \n\nprint(best_alpha)\nprint(best_thr)\n    \ny_pred_combined = best_alpha*(y_pred_rf) + (1-best_alpha)*y_pred_boost\ny_pred_ = y_pred_combined[:,1] > best_thr\n\n_ =plot_confusion_matrix(y_test, y_pred_)\nconfusion_matrix(y_test, y_pred_)","baafc0f7":"X_scaled = StandardScaler().fit_transform(X)\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_scaled, y, test_size=0.3, random_state=0)\n\nos = SMOTE(random_state=131)\ncolumns = X.columns\n\nos_data_X, os_data_y = os.fit_sample(X_train_all, y_train_all)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns)\nos_data_y = pd.DataFrame(data=os_data_y)\n\nX_train_all = os_data_X\ny_train_all = os_data_y['SARS-Cov-2_result']\n\nX_test_all = pd.DataFrame(data=X_test_all,columns=columns)\n\nX_train = X_train_all.copy()\ny_train = y_train_all.copy()\nX_test = X_test_all.copy()\ny_test_ =y_test_all.copy()\n","15d55f09":"# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test_)\n\ncount_classes = y_test.shape[1]\nprint(count_classes)","546fc332":"# build the model\nmodel_rn = Sequential()\nmodel_rn.add(Dense(250, activation='relu', input_dim=(X_train.shape[1])))\nmodel_rn.add(Dropout(.2))\nmodel_rn.add(Dense(200, activation='relu'))\nmodel_rn.add(Dropout(.2))\nmodel_rn.add(Dense(200, activation='tanh'))\nmodel_rn.add(Dropout(.2))\nmodel_rn.add(Dense(100, activation='relu'))\nmodel_rn.add(Dropout(.3))\nmodel_rn.add(Dense(50, activation='relu'))\nmodel_rn.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel_rn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#Fitting The Mdoel\n\n# model_rn.fit(X_train, y_train, sample_weight= y_train[:,1]*5 + 1, validation_data = (X_test, y_test, y_test[:,1]*5 + 1), batch_size=None, epochs=300, verbose = 1, workers=-1, use_multiprocessing=True)\nmodel_rn.fit(X_train, y_train, batch_size=None, epochs=300, verbose = 0, workers=-1, use_multiprocessing=True)","5408245a":"pred_train = model_rn.predict(X_train)\nscores = model_rn.evaluate(X_train, y_train, verbose=0)\nprint('Accuracy on training data: {:.2f}% \\n Error on training data: {:.2f}'.format(scores[1], 1 - scores[1]))   \n \npred_test = model_rn.predict(X_test)\nscores2 = model_rn.evaluate(X_test, y_test, verbose=0)\nprint('Accuracy on test data: {:.2f}% \\n Error on test data: {:.2f}'.format(scores2[1], 1 - scores2[1]))    ","49181c4a":"y_pred_rn =  model_rn.predict(X_test)\n\n_ =plot_roc(y_test[:,1], y_pred_rn[:,1])","dfe32df0":"precisions, recalls, thresholds = precision_recall_curve(y_test_, y_pred_rn[:,1])\n_ =plot_precision_recall(precisions, recalls, thresholds)","98639951":"for i in range(0,5000):\n    thr = i\/5000\n    y_pred_ = y_pred_rn[:,1] > thr\n    \n    if confusion_matrix(y_test_, y_pred_)[1,0] > 17:\n        thr -= 1\/5000\n        break\n\nprint(thr)\n\nthreshold = thr\n\ny_pred_ = y_pred_rn[:,1] > threshold\n\n_ =plot_confusion_matrix(y_test_, y_pred_)\nconfusion_matrix(y_test_, y_pred_)","203f6518":"## Modelo DeepLearning","59045cc4":"## MODELO 04 - Combinando os modelos (Random Forest e Boosting)","4c58cf98":"# Imports","9c443e89":"# Libs","093df0c3":"# Cluster","646f15db":"# Tratando Nulls","3bcb270b":"# Funcs","68227bbb":"## MODELO 01 - Random Forest","489ad6aa":"## MODELO 02 - Boosting","566b08f7":"## MODELO 04 - SVM"}}