{"cell_type":{"4dd6fde8":"code","ce6a1809":"code","1d321ed8":"code","72c9d35e":"code","4dec4e69":"code","faef4f36":"code","3fd7258f":"code","c534acc3":"code","e8f0d25c":"code","a27d8bf1":"code","4eb1e287":"code","79444459":"code","a09f5c57":"code","998fe6ed":"code","7a697816":"markdown","931ea536":"markdown","b27655c7":"markdown","c8809a00":"markdown","cf72c2b7":"markdown","98f3f746":"markdown","728c790f":"markdown","f576715f":"markdown","812ff779":"markdown","cbae49d6":"markdown"},"source":{"4dd6fde8":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, TimeDistributed\nfrom keras.optimizers import RMSprop\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras import optimizers\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n","ce6a1809":"with pd.HDFStore('..\/input\/madrid.h5') as data:\n    df = data['28079016']\n\n\ndf = df.sort_index()\nprint(df.columns.values)\nx_label = ['CO','NO_2','PM10','SO_2','NOx']\ny_label = ['O_3']\ndf = df[y_label + x_label] \ndf = df.dropna() # There are quite a few nans so lets just remove them. We have enough data for our purposes\ndf.describe()\n","1d321ed8":"# Pair plot\nsns.pairplot(df)","72c9d35e":"# Split the data into test and training sets.\nnp.random.seed(100)\nX_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.1)\n# Print the dimensions\nprint('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\nprint('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))","4dec4e69":"# Define regression model in Keras\ndef regression_model():\n    # Define model\n    model = Sequential()\n    model.add(Dense(5, input_dim=5, activation='relu'))\n    model.add(Dense(5, activation='relu'))\n    model.add(Dense(2, activation='relu'))\n    model.add(Dense(1, activation='linear'))\n    # Compile model\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(loss='mean_squared_error', optimizer=adam,metrics=['accuracy'])\n    \n    return model\n\n# Use KerasRegressor wrapper (from Keras to sklearn)\n# The packages we use are meant to be run with sklearn models\nestimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\nhistory = estimator.fit(X_train, y_train)","faef4f36":"# summarize history loss\nprint(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'dev'], loc='upper left')\nplt.show()","3fd7258f":"fitted = estimator.predict(X_train)\nresiduals = y_train['O_3'] - fitted","c534acc3":"# Two plots\nfig, (ax1, ax2) = plt.subplots(ncols=2,figsize=(12,6))\n\n# 1. Histogram of residuals\nsns.distplot(residuals, ax=ax1)\nax1.set_title('Histogram of residuals')\n\n# Fitted vs residuals\nx1 = pd.Series(fitted, name='Fitted O_3')\nx2 = pd.Series(y_train['O_3'], name=\"O_3 values\")\nsns.kdeplot(x1, x2, n_levels=40,ax = ax2)\nsns.regplot(x=x1,y=x2, scatter=False, ax = ax2)\nax2.set_title('Fitted vs actual values')\nax2.set_xlim([0,120])\nax2.set_ylim([0,120])\nax2.set_aspect('equal')\n","e8f0d25c":"from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())","a27d8bf1":"from pdpbox import pdp, get_dataset, info_plots\n\n# Gather pdp data\npdp_goals_NOx = pdp.pdp_isolate(model = estimator, \n                                dataset = X_train, \n                                model_features = x_label,\n                                feature='NOx')","4eb1e287":"# plot NOX pdp\npdp.pdp_plot(pdp_goals_NOx, 'NOx', \n             x_quantile=False, \n            plot_pts_dist=False)\nplt.show()","79444459":"# Gather pdp data\npdp_goals_NO2 = pdp.pdp_isolate(model = estimator, \n                                dataset = X_train, \n                                model_features = x_label,\n                                feature='NO_2')","a09f5c57":"# plot NO_2 pdp\npdp.pdp_plot(pdp_goals_NO2, 'NO_2',\n            x_quantile=False, \n            plot_pts_dist=False)\nplt.show()","998fe6ed":"import shap\n\n# SHAP expects model functions to take a 2D numpy array as input, so we define a wrapper function around the original Keras predict function.\ndef f_wrapper(X):\n    return estimator.predict(X).flatten()\n\n# Too many input data - use a random slice\n# rather than use the whole training set to estimate expected values, we summarize with\n# a set of weighted kmeans, each weighted by the number of points they represent.\nX_train_summary = shap.kmeans(X_train, 20)\n\n# Compute Shap values\nexplainer = shap.KernelExplainer(f_wrapper,X_train_summary)\n\n# Make plot with combined shap values\n# The training set is too big so let's sample it. We get enough point to draw conclusions\nX_train_sample = X_train.sample(400)\nshap_values  = explainer.shap_values(X_train_sample)\nshap.summary_plot(shap_values, X_train_sample)\n","7a697816":"There are seems to be some sort of linear-ish patterns so we can expect our model to work decently, at least for the purpose of this work.\n\n# Regression model: a neural network with several layers\nNext we split the dataset into a training and test set and afterwards define the model. The model we have chosen here is a neural network with 3 layers stacked on top of each other. ","931ea536":"The model seems to be fitted after a couple of epochs. Let's have a quick look at the residuals","b27655c7":"There is nothing too strange that stands out so let's keep on going. There are several points one would explore until this model can be considered satisfactory:\n\n1. Time dependencies should be considered. The Ozone exhibits a really strong diurnal seasonality.\n2. Hyperparameter tuning: shall we add\/remove layers? What about increasing\/decreasing the bath size or the learning rate? [Grid search](https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/) on the hyperparameters is the way to go\n3. Can we find more data, for example, related to traffic (like the author of the dataset did in the [last](https:\/\/www.kaggle.com\/diegovicente\/particle-levels-prediction-using-lstm) section), temperature or solar irradiance, that help explaining the Ozone levels?\n4. Data could be normalized. I decided to skip this step in order to make the interpretations of the model easier.\n\nThe aim of this notebook is not to build the best possible model but instead to get some insights from it, especially when the model is a neural network and its effects are rather _\"black-box\"_.\n\nOnce we are satisfied with our model search, let's have a look at what we can learn from it.","c8809a00":"Each row corresponds to a feature, the color represents the feature value (red high, blue low) , and each dot corresponds to a training sample.\n\nFrom the combined Shap plot values we observe the following:\n\n- NOx is the feature with the highest impact. We already found this out when calculating the permutation importance. Nevertheless, it is always a good idea to double-check your conclusions. The higher NOx (red dots at the first row), the lower the Ozone predictions are.\n- The effect of NO_2 is reverse to the NOx: higher NO_2 levels imply more Ozone\n- PM10 seems somewhat relevant even though less important than the levels of nitrogen\n- The CO and SO_2 are way less relevant and they could easily be removed from the model\n\n\n# Conclusion\n\nWe have seen three ways of exploring the effect of different features on the predicted values of a model. These techniques are model-independent and are specially useful for black-box models like neural networks and random forests. **Shap values** is the **most useful** of all with the ability to show the positive\/negative effect of each feature on the predicted variable in a very compact format. The drawback of the Shap values is their computational complexity. A deep analysis of any black-box model should include also a table of **permutation importance** and a set of **partial dependence plots**.\n\n### Further reading\n\n- Interpretable machine learning [book, chapter 6](https:\/\/christophm.github.io\/interpretable-ml-book\/pdp.html)\n- ML for Insights [Challenge](https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values) from Kaggle\n- Basic introduction to [pdp](https:\/\/towardsdatascience.com\/introducing-pdpbox-2aa820afd312) from the author of the python library PDPBox\n- Python library for calculating [Shap](https:\/\/github.com\/slundberg\/shap) values.","cf72c2b7":"# Partial dependence plots\nPermutation importance allowed us to find out which variable are most important in terms of predicting the Ozone levels. The next question comes naturally: _What is the effect of such variables in the Ozone concentrations?_ In the world of linear models this question is answered by looking at the coefficients. In the black-box world, we look at the [partial dependence](https:\/\/towardsdatascience.com\/introducing-pdpbox-2aa820afd312) plots (PDP).\n\n\nThe underlying idea behind these plots is to marginalize the effect of one or two variables over the predicted values. When a neural network has several features and layers, it is really hard or even impossible to asses the impact of a single feature on the outcome by simply looking at coefficients. Instead, the marginal effect is approximated by a Monte Carlo approach. Generally speaking, we run predictions for a set of features and then average them out over the features we are interested in knowing their effects.\n\n\n","98f3f746":"# Learning insights from neural networks: a summary with code\n\nThe goal of this notebook is to compile and share some specialized techniques to extract real-world insights from black-box models. It gives a concise summary with some code so that hopefully you will get the main ideas of the course. Also, so that I don't forget about them!\n\nI have been strongly inspired by the challenge named [\"Machine learning of insights\"](https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values) from Kaggle.\n\nThe presented techniques have several characteristics in common:\n\n1. They are used to gain insights from machine learning models. They are not used (generally speaking) for model building\n2. They are model independent, hence can be used with black-box models.\n3. The model needs to be used for predicting numerous times, hence these techniques are mostly useful when the computational time of running the model is relatively low. We do not need to re-train the model more than once, so they are especially useful in cases when training the model is time-consuming.\n4. These techniques are meant to be run on tabulated data (i.e., regression or classification). In theory, they can accept pixel data as inputs even though the interpretation will be less obvious.\n5. The conclusions are clear and concise. Beautiful plots can be easily generated.\n6. There exists libraries in Python to achieve all of this without coding much\n\nIn the following chapters I will briefly introduce the dataset and cover the following topics:\n\n1. Feature importance\n2. Partial dependence plots\n3. SHAP values\n\n\n\n# The data: Air quality in Madrid\n\nThe data has been collected from [Kaggle datasets](https:\/\/www.kaggle.com\/decide-soluciones\/air-quality-madrid). It consists of a list of measurements of air quality in the city of Madrid. We will explore the following components:\n\n- O_3: ozone level measured in \u03bcg\/m\u00b3. High levels can produce asthma, bronchytis or other chronic pulmonary diseases in sensitive groups or outdoor workers.\n- SO_2: sulphur dioxide level measured in \u03bcg\/m\u00b3. High levels of sulphur dioxide can produce irritation in the skin and membranes, and worsen asthma or heart diseases in sensitive groups.\n- CO: carbon monoxide level measured in mg\/m\u00b3. Carbon monoxide poisoning involves headaches, dizziness and confusion in short exposures and can result in loss of consciousness, arrhythmias, seizures or even death in the long term.\n- NO: nitric oxide level measured in \u03bcg\/m\u00b3. This is a highly corrosive gas generated among others by motor vehicles and fuel burning processes.\n- NO_2: nitrogen dioxide level measured in \u03bcg\/m\u00b3. Long-term exposure is a cause of chronic lung diseases, and are harmful for the vegetation.\n- PM10: particles smaller than 10 \u03bcm. Even though the cannot penetrate the alveolus, they can still penetrate through the lungs and affect other organs. Long term exposure can result in lung cancer and cardiovascular complications.\n- NOx: nitrous oxides level measured in \u03bcg\/m\u00b3. Affect the human respiratory system worsening asthma or other diseases, and are responsible of the yellowish-brown color of photochemical smog.\n\nThe aim is to predict the Ozone levels given measurements from the rest of the variables. Let's get to it!\n","728c790f":"From the figures above we can draw some interesting conclusions, for example:\n\n- Higher levels of Ozone are predicted for lower levels of NOx and higher levels of NO_2\n- The change in NO_2 has, generally speaking, a similar impact than the NOx but with opposite sign. There are some cases with extremely high NO_2 where the relationship with Ozone is more than 3 times the usual.\n- The impact of NOx on the Ozone stabilizes after a values of NOx greater than 200\n\nOf course, these conclusions should be confirmed with an expert in air quality. The conclusions can change if we change the model.\n\nTo finalize the example, we could have a a look at the combined effect of both NOx and NO_2. In the linear-model world, this would be equivalent to looking at the coefficient of the interaction between NOx and NO_2. In this case the effect is more complex so it is not enough to look at one number - instead we look at a 3d plot. In this case there was not much to show, below I paste the code for completeness.\n\n```python\ninter1 = pdp.pdp_interact( \n    model = estimator,\n    dataset=X_train,\n    model_features = x_label,\n    features = ['NOx', 'NO_2'],\n    num_grid_points = [10, 10])\n    \nfig, axes = pdp.pdp_interact_plot(\n    pdp_interact_out = inter1,\n    feature_names=['NOx', 'NO_2'],\n    plot_type='contour',\n    plot_pdp=False,\n    x_quantile=False    \n)\n\n```","f576715f":"# Permutation importance\nWe answer the following questions: _which of the explanatory variables is most relevant when predicting the O3 levels? And which one is not significant at all, and shall be removed?_\n\nThis question is almost as old as the field of statistics itself. When it comes to linear model and other white-box approaches, the straight-forward answer is given in traditional statistical books. What happens when the model is not as interpretable as a simple linear model? It is now not enough to look at the coefficients themselves. Instead, we make use of the computation power of our computers and calculate the so-called **Permutation importance**.\n\nThe intuition behind permutation importance in quite simple. The only requirement is to have fitted a model, either for regression or classification. If a feature is considered not important by the model, we could shuffle it (re-arrange the rows) and the performance of the model would not be altered much. On the other hand, if the feature is relevant, shuffling its rows with affect negatively the prediction accuracy.\n\nBelow we calculate the permutation importance for our model. Clearly there are two most importance features that should be studied more carefully. The least importance features, namely, SO_2 could be safely removed from the model.\n","812ff779":"# Exploratory analysis\n \nWe start by loading the dataset in _.h5_ format. A very helpful explanation if given by the author of the dataset in [this](https:\/\/www.kaggle.com\/diegovicente\/a-short-introduction-to-hdf5-files) kernel. We will use the records of a single station out of the 18 available. Moreover, we select three features only as explanatory variables for the O3 concentration. Recall that this analysis does not focus on the model building itself, but instead on the model interpretation - gathering insights from a black-box model.","cbae49d6":"# Shapely values\n\nShap values contribute understanding the model in a analogous way to coefficients from a linear model. _Given a prediction: how much of it is affected by the explanatory variables? What features contribute positively and negatively?_\n\n\nThe Shapley value is the average marginal contribution of a feature value over all possible combination of the other features ([wiki page](odesays.com\/solutions-to-training-by-codility\/) and an easier to read [book chapter](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html)). They are useful for understanding the contributions the features of the model when we produce a prediction. Shapely values answer \"why\" the prediction is different than the mean prediction.\n"}}