{"cell_type":{"9f086a25":"code","60b4ed2f":"code","b349ff54":"code","73b5b996":"code","55d11a82":"code","a0883646":"code","9f8c0519":"code","ab4c1eea":"code","8d27529d":"code","6e81592f":"code","b61915e3":"code","88d0a4e4":"code","d50e78ac":"code","fac1ddbb":"code","07764561":"code","b4b1d6b8":"code","99896af9":"code","938216aa":"code","3a31c5a4":"code","acd959fc":"code","8cd6f69b":"code","d62240b6":"code","1c06935e":"code","845d2788":"code","633ab976":"code","7008ecc1":"code","5c762efe":"code","a098bd2c":"code","dee625dc":"code","8b63b5d3":"code","7a9cef9a":"code","29f9204a":"code","c97876e6":"code","413c9324":"code","1c93c86e":"code","eb914031":"code","db7c743e":"code","46586988":"code","09aafa80":"code","a3c878fe":"code","d8ee0ad4":"code","9d62160a":"code","f81e9d0c":"code","786bc40c":"code","ecee75d2":"code","7d4096a8":"code","6fc7dca2":"code","35fc6090":"code","cfcad7c2":"code","051c6a48":"code","87f62f3d":"code","e8f8d817":"code","f07939bc":"code","f0967e84":"code","25fb0a52":"code","6c8783d8":"code","68be30ab":"code","f4800941":"code","71ef88d6":"code","d700d1de":"code","14da5b5d":"code","09164031":"code","d1575b6f":"code","5a8d9393":"code","0cc23d40":"code","aca34dac":"code","888de01f":"code","14f7d5f3":"code","c8a31f36":"code","51275b38":"code","8b6b7e92":"code","ef5f1453":"code","d9207909":"code","275beeb9":"code","602b52a4":"code","91236edf":"markdown","c3d62b73":"markdown","d988087c":"markdown","4a8dfd26":"markdown","bc5cd4f9":"markdown","b7336d64":"markdown","55c5e3df":"markdown","82f542e7":"markdown","0d874cc1":"markdown","747ef6fc":"markdown","77b822aa":"markdown","49a0ff4a":"markdown","a795973d":"markdown","1133e022":"markdown","71c7b2a0":"markdown","3e34e888":"markdown","e9df752c":"markdown","f55cd0dc":"markdown","b5779ffa":"markdown","72ce70d6":"markdown","029e81ab":"markdown","613ed768":"markdown","dbbde819":"markdown","b579cac6":"markdown","ec0e248a":"markdown","7843ea7f":"markdown","94277cc5":"markdown","9cb2d06b":"markdown","02d0de07":"markdown","b1304a21":"markdown","60f2cbfc":"markdown","925c47c7":"markdown","6c8f7cd4":"markdown","6207b02a":"markdown","d6e37e33":"markdown","7d305538":"markdown","de155d97":"markdown","1a3cf42e":"markdown","95c52bde":"markdown","4fb6bafa":"markdown","2865c010":"markdown","1666eddb":"markdown","a45ef523":"markdown","02c2c263":"markdown","65bb88c1":"markdown","4b372696":"markdown","cdbb90e8":"markdown","e334911d":"markdown","b3fa4c32":"markdown","376bbdff":"markdown","1ebbd2bb":"markdown","821d2788":"markdown","6f78f7bf":"markdown","14538646":"markdown","e01ee453":"markdown","b8bd741b":"markdown","849ce814":"markdown","6d865fd0":"markdown","5838ff50":"markdown","ea9d087b":"markdown","55af1908":"markdown","7a3c0364":"markdown","88be27e8":"markdown","b4272452":"markdown","b8364bcd":"markdown","24785a0c":"markdown","5ff7c39f":"markdown"},"source":{"9f086a25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling \nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\") #it is forgraphics,gives grid view\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","60b4ed2f":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","b349ff54":" test_df.profile_report()","73b5b996":"train_df.info()","55d11a82":"def bar_plot(variable):\n    \"\"\"\n        input variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    #get feature\n    var = train_df[variable]\n    #count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index,varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","a0883646":"category1 = [\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\",\"Parch\"]\nfor c in category1:\n    bar_plot(c)","9f8c0519":"category2 = [\"Cabin\", \"Name\", \"Ticket\"]\nfor c in category2:\n    print(\"{} \\n\".format(train_df[c].value_counts()))","ab4c1eea":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable],bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","8d27529d":"numericVar = [\"Fare\", \"Age\", \"PassengerId\"]\nfor n in numericVar:\n    plot_hist(n)","6e81592f":"#Pclass vs Survived\n#Effect of class on surviving\ntrain_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","b61915e3":"#Sex vs Survived\n#Effect of gender on surviving\ntrain_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","88d0a4e4":"#SibSp vs Survived\n#Effect of having siblings or spouses on surviving\ntrain_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","d50e78ac":"#Parch vs Survived\n#Effect of having parents or children on surviving\ntrain_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","fac1ddbb":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        #1st quartile\n        Q1 = np.percentile(df[c],25)\n        #3rd quartile\n        Q3 = np.percentile(df[c],75)\n        #IQR\n        IQR = Q3-Q1\n        #Outlier step\n        outlier_step = IQR * 1.5\n        #Detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #Store indices\n        outlier_indices.extend(outlier_list_col)\n    #Counting numbers of each element\n    outlier_indices = Counter(outlier_indices)\n    #Take if there are more than 1 outliers,not just 1 outlier\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v > 2)\n    return multiple_outliers","07764561":"train_df.loc[detect_outliers(train_df, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","b4b1d6b8":"#Drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]), axis = 0).reset_index(drop = True)","99896af9":"train_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)","938216aa":"train_df.columns[train_df.isnull().any()]","3a31c5a4":"train_df[train_df[\"Embarked\"].isnull()]","acd959fc":"train_df.boxplot(column = \"Fare\",by = \"Embarked\")\nplt.show()","8cd6f69b":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")","d62240b6":"train_df[train_df[\"Fare\"].isnull()]","1c06935e":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"]))","845d2788":"list1 = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot = True, fmt = \".2f\")","633ab976":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survive Probability\")\nplt.show()","7008ecc1":"g = sns.factorplot(x = \"Parch\", y = \"Survived\",data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survive probability\")\nplt.show()","5c762efe":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survive Probability\")\nplt.show()","a098bd2c":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","dee625dc":"g = sns.FacetGrid(train_df, col = \"Survived\", row = \"Pclass\", size = 2)\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","8b63b5d3":"g = sns.FacetGrid(train_df, row = \"Embarked\", size = 2)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\ng.add_legend()\nplt.show()","7a9cef9a":"g = sns.FacetGrid(train_df, row = \"Embarked\",col = \"Survived\", size =2)\ng.map(sns.barplot,\"Sex\",\"Fare\")\ng.add_legend()\nplt.show()","29f9204a":"train_df[train_df[\"Age\"].isnull()]","c97876e6":"sns.factorplot(x = \"Sex\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","413c9324":"sns.factorplot(x = \"Sex\", y = \"Age\",hue = \"Pclass\", data = train_df, kind = \"box\")\nplt.show()","1c93c86e":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","eb914031":"train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]]","db7c743e":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),annot = True)\nplt.show()","46586988":"#index_nan_age includes the indexes of NaN values in Age feature\nindex_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    #age_pred is where age prediction is done\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) & (train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"]) & (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    #age_med is median of Age feature\n    age_med = train_df[\"Age\"].median()\n    #if age_pred is not NaN then age_pred is the new value of Age feature's ith index\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    #if age_pred is NaN then age_med is the new value of Age feature's ith index\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","09aafa80":"train_df[train_df[\"Age\"].isnull()]","a3c878fe":"train_df[\"Name\"].head(10)","d8ee0ad4":"train_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in train_df[\"Name\"]]\ntrain_df[\"Title\"].head(10)","9d62160a":"sns.countplot(x = \"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","f81e9d0c":"train_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Ms\" or i == \"Miss\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]","786bc40c":"sns.countplot(x = \"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","ecee75d2":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","7d4096a8":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)","6fc7dca2":"#Extracting columns from Title feature\ntrain_df = pd.get_dummies(train_df,columns = [\"Title\"])\ntrain_df.head(10)","35fc6090":"# I have added 1 because even if both features' values are 0, 1 person can be counted as family.\ntrain_df[\"Fsize\"] = train_df[\"Parch\"] + train_df[\"SibSp\"] + 1","cfcad7c2":"f = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = train_df, kind = \"bar\")\nf.set_ylabels(\"Survival\")","051c6a48":"train_df[\"Fsize\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]]","87f62f3d":"train_df.head(10)","e8f8d817":"sns.countplot(x = \"Fsize\", data = train_df)\nplt.show()","f07939bc":"f = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = train_df, kind = \"bar\")\nf.set_ylabels(\"Survival\")\nplt.show()","f0967e84":"train_df = pd.get_dummies(train_df,columns = [\"Fsize\"])\ntrain_df.head(10)","25fb0a52":"train_df[\"Embarked\"].head(10)","6c8783d8":"train_df = pd.get_dummies(train_df,columns = [\"Embarked\"])\ntrain_df.head(10)","68be30ab":"train_df[\"Ticket\"].head(20)","f4800941":"tickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ntrain_df[\"Ticket\"] = tickets","71ef88d6":"# Reason to use prefix is to use T instead of Ticket in title.\ntrain_df = pd.get_dummies(train_df, columns = [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)","d700d1de":"sns.countplot(x = \"Pclass\", data = train_df)\nplt.show()","14da5b5d":"train_df = pd.get_dummies(train_df, columns = [\"Pclass\"])\ntrain_df.head(10)","09164031":"train_df[\"Sex\"]","d1575b6f":"\ntrain_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","5a8d9393":"train_df.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)\ntrain_df.head()","0cc23d40":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","aca34dac":"# Combined data size\ntrain_df_len","888de01f":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"], axis = 1, inplace = True)","14f7d5f3":"test.head()","c8a31f36":"train = train_df[:train_df_len]\nX_train = train.drop(labels = \"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test))","51275b38":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train,y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: {}\".format(acc_log_train))\nprint(\"Testing Accuracy: {}\".format(acc_log_test))","8b6b7e92":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                 \"max_depth\" : range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                  \"gamma\": [0.001, 0.01, 0.1, 1],\n                  \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]\n","ef5f1453":"cv_result = []\nbest_estimator = []\n\nfor i in range(len(classifier)):\n    # n_jobs = -1\n    clf = GridSearchCV(classifier[i], param_grid = classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1, verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimator.append(clf.best_estimator_)\n    print(\"cv_results : \", cv_result[i])","d9207909":"cv_results = pd.DataFrame({\"Cross Validation Means\" : cv_result, \"ML Models\" : [\"DecisionTreeClassifier\",\n                                              \"SVM\",\n                                              \"RandomForestClassifier\",\n                                              \"LogisticRegression\",\n                                              \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","275beeb9":"votingC = VotingClassifier(estimators = [(\"dt\", best_estimator[0]),(\"rfc\", best_estimator[2]), (\"lr\",best_estimator[3])], voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","602b52a4":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived], axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","91236edf":"## Fill Missing: Age Feature <a id = \"20\"><\/a><br>","c3d62b73":"## Hyperparameter Tuning -- Grid Search -- Cross Validation <a id = \"32\"><\/a><br>","d988087c":"<a id = \"18\"><\/a><br>\n## Embarked -- Sex -- Pclass -- Survived","4a8dfd26":"<a id = \"7\"><\/a><br>\n# Outlier Detection","bc5cd4f9":"# Introduction\nThe sinking of Titanic is a shipwreck which is known by all world.In 1912,during her voyage,The Titanic sank  after colliding with an iceberg killing 1502 out of 2224 passengers and crew.Our job is to handle Titanic Dataset. \n\n<font color = 'turquoise'>\nContent:\n<font color = 'turquoise'>\n1. [Load and Check Data](#1) \n    <font color = 'turquoise'>\n       \n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable Analysis](#4)\n        * [Numerical Variable Analysis](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Correlation Between SibSp -- Parch -- Age -- Fare -- Survived](#12)\n    * [SibSp -- Survived](#13)\n    * [Parch -- Survived](#14)\n    * [Pclass -- Survived](#15)\n    * [Age -- Survived](#16)   \n    * [Pclass -- Survived -- Age](#17)\n    * [Embarked -- Sex -- Pclass -- Survived](#18)\n    * [Embarked -- Sex -- Fare -- Survived](#19)\n    * [Fill Missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop PassengerId and Cabin](#28)\n1. [Modeling](#29)\n    * [Train Test Split](#30)\n    * [Simple Logistic Regression](#31)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#32)\n    * [Ensemble Modeling](#33)\n    * [Prediction and Submission](#34)","b7336d64":"### There are 3 values in Embarked feature and we will extract them as new features.","55c5e3df":"### Sex feature values were string so values are converted to numbers to see Sex feature on heatmap","82f542e7":"<a id = \"16\"><\/a><br>\n## Age -- Survived","0d874cc1":"### Age is not correlated with sex but it is correlated with parch, sibsp and pclass.","747ef6fc":"* age <= 10 has a high survival rate,\n* oldest passengers (80) survived,\n* large number of 20 years old did not survive,\n* most passengers are in 15-35 age range,\n* use age feature in training\n* use age distribution for missing value of age","77b822aa":"<a id = \"8\"><\/a><br>\n# Missing Value\n* Find Missing Value\n* Fill Missing Value\n","49a0ff4a":"It seems that these 2 passengers got into the Titanic from Cherbourg Port so I will fill Embarked values with C.","a795973d":"### We will extract 3 class features from Pclass feature.","1133e022":"## Feature Engineering <a id = \"21\"><\/a><br>","71c7b2a0":"### We should convert Sex feature to categorical to extract new features from it then extract features.","3e34e888":"## Modeling <a id = \"29\"><\/a><br>","e9df752c":"## Family Size <a id = \"23\"><\/a><br>","f55cd0dc":"## Simple Logistic Regression <a id = \"31\"><\/a><br>","b5779ffa":"* Female passengers have much better survival rate than males.\n* males have better survIval rate in pclass 3 in C.\n* embarked and sex will be used in training.","72ce70d6":"### We will have a new feature with combining SibSp and Parch features.","029e81ab":"### Firstly we should seperate combined test and train data.","613ed768":"## Embarked <a id = \"24\"><\/a><br>","dbbde819":"*Sibsp and Parch can be used for new feature extraction with threshold = 3\n*Small families have more chance to survive\n*There is a std in survival of passenger with parch = 3","b579cac6":"## Ticket <a id = \"25\"><\/a><br>","ec0e248a":"### Titles of people can be good for training. We will create Title features from Name feature.","7843ea7f":"* float64(2) : Fare and Age\n* int64(5): Pclass,Sibsp,Parch,PassengerId and Survived\n* object(5): Cabin,Embarked,Ticket,Name and Sex","94277cc5":"### There can be string before unique numbers or nothing. We will use these strings and if there is no string then we will count this one as 'x' ","9cb2d06b":"## Pclass <a id = \"26\"><\/a><br>","02d0de07":"*Having a lot of SibSp have less chance to survive\n*if sibsp == 0 or 1 or 2, passenger has more chance to survive\n*we can consider a new feature describing these categories","b1304a21":"### We don't want to use these features on training so we will drop them.","60f2cbfc":"<a id = \"14\"><\/a><br>\n## Parch -- Survived","925c47c7":"### There is no missing value in Age feature after filling","6c8f7cd4":"<a id = \"15\"><\/a><br>\n## Pclass - Survived","6207b02a":"### We will compare 5 ml classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression","d6e37e33":"### We have extracted 4 features from Name feature so there is no need to Name feature anymore, let's drop it.","7d305538":"I'm going to fill Fare value with the average of 3rd class prices because Mr.Thomas has a 3rd class ticket.","de155d97":"* [Univariate Variable Analysis](#3)\n    * [Categorical Variable](#4)\n    * [Numerical Variable](#5)","1a3cf42e":"<a id = \"6\"><\/a><br>\n# Basic Data Analysis\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","95c52bde":"<a id = \"5\"><\/a><br>\n## Numerical Variable","4fb6bafa":"## Train Test Split <a id = \"30\"><\/a><br>","2865c010":"Fare feature seems to have correlation with survived feature(0.26)","1666eddb":"### Let's extract new features from Ticket feature.","a45ef523":"<a id = \"1\"><\/a><br>\n# Load and Check Data\n","02c2c263":"<a id = \"4\"><\/a><br>\n## Categorical Variable","65bb88c1":"## Drop PassengerId and Cabin <a id = \"28\"><\/a><br>","4b372696":"<a id = \"10\"><\/a><br>\n## Fill Missing Value\n* Embarked has 2 missing value\n* Fare has only 1 missing value","cdbb90e8":"### Passsengers who pay higher fare have better survival. Fare can be used as categorical for training.","e334911d":"<a id = \"19\"><\/a><br>\n## Embarked -- Sex -- Fare -- Survived","b3fa4c32":"* pclass is important feature for model training.","376bbdff":"### Except first 4 titles, other titles don't have much frequency so let's name them as other and also make categorical variables numerical variables.","1ebbd2bb":"### Small familes have more chance to survive than large families. Now let's extract features from Fsize feature.\n","821d2788":"<a id = \"2\"><\/a><br>\n# Variable Description\n1. PassengerId: Unique id number to each passenger\n1. Survived: Passenger survived(1) or died(0)\n1. Pclass: Passenger class\n1. Name\n1. Sex\n1. Age\n1. SibSp: Number of siblings\/spouses\n1. Parch: Number of parents\/children\n1. Ticket: Ticket number\n1. Fare: Ticket price\n1. Cabin: Cabin category\n1. Embarked: Boarding Port(C = Cherbourg, Q = Queenstown, S = Southampton)\n\n","6f78f7bf":"<a id = \"3\"><\/a><br>\n# Univariate Variable Analysis\n* Categorical Variable: Survived,Sex,Pclass,Embarked,Cabin,Name,Ticket,Sibsp and Parch\n* Numerical Variable: Fare,Age and PassengerId","14538646":"## Prediction and Submission <a id = \"34\"><\/a><br>","e01ee453":"### 1st class passengers are older than 2nd, and 2nd is older than 3rd class.","b8bd741b":"### We seperate test data and drop Survived feature because there shouldn't be Survived feature in test data.","849ce814":"### Here we will use 3 classifier models which has given greater accuracy than 0.8","6d865fd0":"<a id = \"17\"><\/a><br>\n## Pclass -- Survived -- Age","5838ff50":"<a id = \"13\"><\/a><br>\n## SibSp -- Survived","ea9d087b":"## Bu k\u0131s\u0131mda verilerin g\u00f6rsel hallerinden verilerle ilgili \u00e7\u0131kar\u0131mlar yaparak model e\u011fitimi i\u00e7in vs. hangi verilerin kullan\u0131laca\u011f\u0131na karar verebiliriz.Kodlar\u0131n alt\u0131ndaki notlar, g\u00f6rselle\u015fmi\u015f verilerden yap\u0131lan \u00e7\u0131kar\u0131mlard\u0131r.","55af1908":"<a id = \"9\"><\/a><br>\n## Find Missing Value","7a3c0364":"<a id = \"12\"><\/a><br>\n## Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived","88be27e8":"## Ensemble Modeling <a id = \"33\"><\/a><br>","b4272452":"## Sex <a id = \"27\"><\/a><br>","b8364bcd":"<a id = \"11\"><\/a><br>\n# Visualization","24785a0c":"## Name -- Title <a id = \"22\"><\/a><br>","5ff7c39f":"### Sex is not informative for age prediction, age distribution seems to be same."}}