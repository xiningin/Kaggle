{"cell_type":{"32dca629":"code","a4c5eaa8":"code","ea0b6695":"code","2fbdb03f":"code","e688c3a4":"code","c63a6c51":"code","54dd54cb":"code","36a28379":"code","bcb8e1a6":"code","0f813d63":"code","0518efaf":"code","674cdd22":"code","a1082036":"code","594f2432":"code","210bb7b1":"code","293cb34a":"code","22a6675b":"code","f4bd5c35":"code","36eb1924":"code","0af14a07":"code","05f123c8":"code","57eb0144":"code","68d738d9":"code","68171e85":"code","4544669e":"code","601d3dc5":"code","d118411b":"code","df9d9c3b":"code","eb4acf18":"code","d28d0d5c":"code","54280db7":"markdown","c290ee46":"markdown"},"source":{"32dca629":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a4c5eaa8":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain.sample(5)","ea0b6695":"train.describe(include = \"all\")","2fbdb03f":"pd.isnull(train).sum()","e688c3a4":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train)","c63a6c51":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","54dd54cb":"#drop features not useful\ntrain.drop(['Cabin'], axis = 1, inplace = True)\ntest.drop(['Cabin'], axis = 1, inplace = True)\n\ntrain.drop(['Ticket'], axis = 1, inplace = True)\ntest.drop(['Ticket'], axis = 1, inplace = True)\n\ntrain.head()","36a28379":"train.fillna({'Embarked' : 'S'}, inplace = True)\n\ntrain.describe(include = 'all')","bcb8e1a6":"combine = [train, test]\n# TODO: using lambda\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\n    \npd.crosstab(train['Title'], train['Sex'])","0f813d63":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index = False).mean()","0518efaf":"title_mapping = {\"Mr\":1, \"Miss\":2, \"Mrs\":3, \"Master\":4, \"Royal\":5, \"Rare\":6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ntrain.head()","674cdd22":"age_title_mapping = {1:'Young Adult', 2:'Student', 3:'Adult', 4:'Baby', 5:'Adult', 6:'Adult'}\n\nfor x in range(len(train['AgeGroup'])):\n    if train['AgeGroup'][x] == \"Unknown\":\n        train['AgeGroup'][x] = age_title_mapping[train['Title'][x]]\n        \nfor x in range(len(test['AgeGroup'])):\n    if test['AgeGroup'][x] == \"Unknown\":\n        test['AgeGroup'][x] = age_title_mapping[test['Title'][x]]\n        \nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.drop(['Age'], axis = 1, inplace = True)\ntest.drop(['Age'], axis = 1, inplace = True)\n\ntrain.head()","a1082036":"train.drop(['Name'], axis = 1, inplace = True)\ntest.drop(['Name'], axis = 1, inplace = True)","594f2432":"sex_mapping = {'male':0, 'female':1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\ntrain.head()","210bb7b1":"embarked_mapping = {'S':1, 'C':2, 'Q':3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\ntrain.head()","293cb34a":"for x in range(len(test['Fare'])):\n    if pd.isnull(test['Fare'][x]):\n        pclass = test['Pclass'][x]\n        test['Fare'][x] = round(train[train['Pclass'] == pclass]['Fare'].mean(), 4)\n        \ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1,2,3,4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1,2,3,4])\n\ntrain.drop(['Fare'], axis = 1, inplace = True)\ntest.drop(['Fare'], axis = 1, inplace = True)\n\ntrain.head()","22a6675b":"test.head()","f4bd5c35":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train['Survived']\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","36eb1924":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","0af14a07":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","05f123c8":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","57eb0144":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","68d738d9":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","68171e85":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","4544669e":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","601d3dc5":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","d118411b":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","df9d9c3b":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","eb4acf18":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","d28d0d5c":"ids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","54280db7":"## Explore Data","c290ee46":"# Hello Titanic\nThis is my first training.\nAnd this is based on '[Titanic Survival Predictions (Beginner) by Nadin Tamer](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)'."}}